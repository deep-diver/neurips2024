[{"figure_path": "sntv8Ac3U2/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of FG-DM (bottom) against Stable Diffusion (top) for sampling images with high object recall by modeling the joint distribution of images and conditioning variables. FG-DM supports creative, controllable, interpretable and faster (4x) image synthesis than Stable Diffusion to achieve the desired object recall. Note that the conditions y\u00b9 or y2 can be null due to classifier-free guidance training.", "description": "This figure compares the proposed Factor Graph Diffusion Model (FG-DM) with the traditional Stable Diffusion model for image synthesis.  The top half shows Stable Diffusion, which generates images based on a text prompt but may have low recall in generating multiple objects or difficulty meeting constraints like object location and pose.  The bottom illustrates FG-DM which models the joint distribution of images and conditioning variables (semantic, sketch, depth, or normal maps), leading to improved object recall, faster synthesis (4x), better control, and increased interpretability.  Classifier-free guidance training is optional and allows for scenarios with missing conditions.", "section": "1 Introduction"}, {"figure_path": "sntv8Ac3U2/figures/figures_2_1.jpg", "caption": "Figure 2: FG-DM-based controllable image generation via editing segmentation, depth and sketch maps. Top: generated conditions and images. Bottom: edited ones. Note that only the segmentation map is edited, pose and images are conditionally generated given edited map.", "description": "This figure shows examples of how FG-DM can be used for controllable image generation by editing the conditioning variables (segmentation, depth, and sketch maps). The top row shows the generated conditions and corresponding images, while the bottom row shows the results after editing the segmentation map only.  The model then conditionally generates the pose and image given the edited segmentation map, illustrating fine-grained control over image generation. ", "section": "3 The Factor-Graph Diffusion Model"}, {"figure_path": "sntv8Ac3U2/figures/figures_2_2.jpg", "caption": "Figure 3: Synthesized segmentation/depth/sketch/normal maps and corresponding images by an FG-DM adapted from SD using COCO. The FG-DM generalizes to prompts beyond this dataset such as porcupine, chimp and other creative prompts shown.", "description": "This figure showcases the FG-DM's ability to generate various types of conditioning maps (segmentation, depth, sketch, normal) along with their corresponding images.  Importantly, it highlights the model's generalization capabilities by successfully handling prompts for objects (porcupine, chimp) not present in its COCO training data. This demonstrates the model's ability to synthesize realistic images across diverse conditions and creative prompts.", "section": "Experimental Results"}, {"figure_path": "sntv8Ac3U2/figures/figures_4_1.jpg", "caption": "Figure 4: Left: Training of FG-DM for distribution P(x, y1, y2|y\u00b3) of image x, segmentation mask y\u00b2, and pose map y\u00b9, given text prompt y\u00b3. Each factor (conditional probability written at top of each figure) is implemented by adapting a pretrained SD model to generate a visual condition. The SD model is frozen and only a small adapter is learned per factor. The final (image generation) factor uses ControlNet without adaptation. The encoder-decoder pair and SD backbone are shared among all factors, reducing the total number of parameters. Conditional generation chains are trained at lower resolution for better inference throughput. Right: The FG-DM offers a flexible inference framework due to classifier-free guidance training, where only a desired subset of the factors are run, as shown in the highlighted green area.", "description": "This figure illustrates the architecture of the Factor Graph Diffusion Model (FG-DM).  The left side shows the training process, detailing how a pretrained Stable Diffusion (SD) model is adapted to generate visual conditions for each factor in a factor graph.  The SD model's weights are frozen, and only small adapters are trained. The right side depicts the flexible inference process enabled by classifier-free guidance, allowing for efficient sampling by activating only a subset of the factors as needed.", "section": "3 The Factor-Graph Diffusion Model"}, {"figure_path": "sntv8Ac3U2/figures/figures_6_1.jpg", "caption": "Figure 5: More qualitative results of FG-DM to synthesize segmentation, depth, normal and sketch maps and their corresponding images. See appendix Figure. 14 for the higher resolution version.", "description": "This figure shows more examples of the FG-DM generating various types of maps (segmentation, depth, normal, sketch) and corresponding images, demonstrating its ability to handle different creative prompts. The appendix includes higher-resolution versions of these images.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/figures/figures_7_1.jpg", "caption": "Figure 6: Examples of images generated by FG-DM after editing and comparison with popular text-to-image models. Editing is shown for flipping persons (columns 1-2), writing the desired text (columns 3-4) or realizing a difficult prompt (columns 5-6). Images generated by Stable Diffusion v1.4 and v1.5 for the same prompt are shown in the last two columns.", "description": "This figure demonstrates the image editing capabilities of the proposed FG-DM model by showing examples of image manipulation such as flipping people, adding text, and generating images for complex prompts.  The results are compared to those obtained using Stable Diffusion v1.4 and v1.5, highlighting the FG-DM's superior performance in these tasks.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/figures/figures_7_2.jpg", "caption": "Figure 7: Attribute recall verification with FG-DM on MM-CelebA-HQ. Left: Semantic Attribute Recall. Right: Histogram of the number of trials needed to reach the specified recall.", "description": "This figure presents the results of an ablation study on attribute recall using the FG-DM model on the MM-CelebA-HQ dataset.  The left panel shows a bar chart comparing the attribute recall of the FG-DM against other methods (Ours, OCGP, Imagen, LDM, LAFITE) across three attributes: Wearing Earrings, Bald, and Pale Skin. The right panel displays a histogram showing the distribution of the number of trials required to achieve 70% and 80% recall thresholds.  The histograms show that the FG-DM achieves high recall with a relatively low number of trials.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/figures/figures_9_1.jpg", "caption": "Figure 8: Top: Inverting segmentation masks with FG-DM segmentation factor using the LEDITS++ method. Edits to replace the woman by a chimp or eliminate the umbrella. The FG-DM enbables text-based edits to modify or delete objects in a given mask. The image generated with the edited mask as condition is shown to the right of each edited masks. Bottom: Original image, LEDITS++ edited image for stable diffusion and for the image synthesis factor of the FG-DM. Please Zoom in for details.", "description": "This figure shows the results of image editing using the FG-DM model. The top part demonstrates semantic-level editing, where a segmentation mask is inverted and then edited using the LEDITS++ tool to either replace a woman with a chimp or remove an umbrella. This semantic-level editing shows high robustness and quality. The bottom part of the figure compares pixel-level image inversion and editing. The FG-DM is compared to stable diffusion for the quality of image synthesis, demonstrating its ability to maintain the background of the original image and produce more realistic edits. The overall figure showcases FG-DM's capabilities for image manipulation and editing.", "section": "Experimental Results"}, {"figure_path": "sntv8Ac3U2/figures/figures_14_1.jpg", "caption": "Figure 9: Comparison of FG-DM with attention distill loss (bottom) against FG-DM without attention distill loss (center) and the recent 4M-XL model (top) for the prompts shown at the top of the figure. The two versions of the FG-DM use the same seed. Both versions of the FG-DM produce images of higher quality than 4M-XL. Attention distillation helps improve the quality of the generated segmentations. For example, the model without distillation has inaccurate masks/missing cart/less realistic zebras from left to right.", "description": "This figure compares the image generation results of three different models: FG-DM with attention distillation, FG-DM without attention distillation, and 4M-XL.  It demonstrates that FG-DM, particularly when using attention distillation, produces higher-quality images and more accurate segmentations compared to the 4M-XL model. The differences in segmentation quality are highlighted by examples showcasing the inaccuracies present in the model without attention distillation.", "section": "A.1 Experimental Results"}, {"figure_path": "sntv8Ac3U2/figures/figures_14_2.jpg", "caption": "Figure 10: Qualitative results of Depth map/Image pairs synthesized by FG-DM.", "description": "This figure displays several examples of depth maps generated by the Factor Graph Diffusion Model (FG-DM) alongside their corresponding images.  The FG-DM successfully generates high-quality depth maps and realistic images. The examples showcase the model's ability to handle a variety of objects and scenes, demonstrating its effectiveness in synthesizing depth information and producing high-fidelity images.", "section": "A.1 Experimental Results"}, {"figure_path": "sntv8Ac3U2/figures/figures_15_1.jpg", "caption": "Figure 11: Qualitative results of Normal map/image and Sketch map/image pairs synthesized by FG-DM. FG-DM generalizes well across conditions and is able to generate condition-image pairs that are not seen during training.", "description": "This figure shows qualitative results of FG-DM in generating normal maps and sketch maps with corresponding images.  The model generalizes well, producing outputs for prompts (e.g., a polar bear, a statue of Mary) not seen in the training data, showcasing its ability to synthesize diverse and high-quality results across various conditioning variables.", "section": "3 The Factor-Graph Diffusion Model"}, {"figure_path": "sntv8Ac3U2/figures/figures_15_2.jpg", "caption": "Figure 12: Qualitative results of synthesized segmentation maps and images for the creative prompts shown on the top/bottom of each image. As shown in the main paper, the FG-DM is able to synthesize segmentation maps for object classes beyond the training set and the semantic maps are colored with unique colors, allowing the easy extraction of both object masks and class labels. This shows the potential of the FG-DM for open-set segmentation, e.g the synthesis of training sets to generate training data for segmentation models. Further, a number of interesting generalization properties emerge. Although the FG-DM is only trained to associate persons with black semantic maps segments, it also assigns the chimp of Figure 1, a class that it was not trained on, to that color. This shows that the FG-DM can integrate the prior knowledge by SD that \u201cchimps and persons are similar", "description": "This figure showcases qualitative results of the FG-DM's ability to synthesize segmentation maps and images for creative prompts.  The FG-DM demonstrates the capacity to generate segmentation maps for object classes unseen during training. The semantic maps utilize distinct color-coding for each object class, simplifying object mask and class label extraction. The model also demonstrates interesting generalization properties\u2014for example, despite training primarily on human segmentation, it correctly labels a chimpanzee, highlighting the model's ability to leverage prior knowledge from the pre-trained stable diffusion model.", "section": "Additional Qualitative Results for Segmentation, Depth, Normal and Sketch conditions"}, {"figure_path": "sntv8Ac3U2/figures/figures_16_1.jpg", "caption": "Figure 13: Qualitative comparison of FG-DM with prior works such as Make-a-Scene and SpaText for the prompts shown on the left. Note that FG-DM generates both the map and the image while for MAS and SpaText, the condition was manually sketched and fed to the model.", "description": "This figure compares the image generation results of FG-DM against two prior works, Make-a-Scene and SpaText.  The top row shows the input prompts and the corresponding segmentation maps generated by each method. FG-DM generates both a segmentation map and the image, while the other two methods use manually sketched segmentation maps. The comparison shows that FG-DM achieves better results.", "section": "Comparison with prior works"}, {"figure_path": "sntv8Ac3U2/figures/figures_17_1.jpg", "caption": "Figure 5: More qualitative results of FG-DM to synthesize segmentation, depth, normal and sketch maps and their corresponding images. See appendix Figure. 14 for the higher resolution version.", "description": "This figure shows several examples of images generated by the FG-DM model, along with their corresponding segmentation, depth, normal, and sketch maps.  The prompts used to generate each image are shown below the image.  The figure demonstrates the model's ability to generate high-quality images and maps for a variety of prompts, even those involving complex scenes or objects not present in the training data. The high-resolution version of this figure is available in the appendix.", "section": "Qualitative and Quantitative results"}, {"figure_path": "sntv8Ac3U2/figures/figures_18_1.jpg", "caption": "Figure 15: Qualitative comparison of generated conditions for FG-DM vs extracted conditions using SD (Stable Diffusion) + CEM (Condition Extraction Model) for segmentation and depth maps. The generated conditions for depth maps are superior to the extracted ones.", "description": "This figure compares the quality of generated conditions (segmentation and depth maps) by the proposed FG-DM model against those extracted from images using existing methods (Stable Diffusion + CEM).  It visually demonstrates that the FG-DM generates more accurate and higher quality conditions, particularly for depth maps, highlighting one of FG-DM's key advantages.", "section": "Experimental Results"}, {"figure_path": "sntv8Ac3U2/figures/figures_19_1.jpg", "caption": "Figure 16: Visualization of groundtruth (left) and reconstructed (right) maps by applying the pretrained stable diffusion autoencoder to segmentation, depth, sketch and normal maps.", "description": "This figure visualizes the quality of reconstruction of groundtruth maps for segmentation, depth, sketch, and normal maps by applying a pretrained stable diffusion autoencoder.  The left column shows the groundtruth maps, and the right column shows the maps reconstructed by the autoencoder.  This demonstrates the effectiveness of using a pre-trained autoencoder to generate these conditioning variables for the FG-DM.", "section": "A.1 Experimental Results"}, {"figure_path": "sntv8Ac3U2/figures/figures_21_1.jpg", "caption": "Figure 17: Unconditional generated semantic, pose masks, and corresponding images using an FG-DM trained from scratch on MM-CelebA-HQ (256 x 256).", "description": "This figure shows four examples of unconditional image generation from an FG-DM model trained on the MM-CelebA-HQ dataset.  For each example, the figure shows the generated semantic mask (top row), pose mask (middle row), and the resulting image (bottom row). The results showcase the FG-DM's ability to generate diverse and high-quality images without explicit textual or visual conditioning.", "section": "Qualitative results: Unconditional and Conditional Image Generation"}, {"figure_path": "sntv8Ac3U2/figures/figures_22_1.jpg", "caption": "Figure 18: Segmentation mask/image pairs synthesized by FG-DMs trained from scratch (53 M parameters) on the MM-CelebA-HQ, ADE-20K, Cityscapes and COCO datasets.", "description": "This figure shows four image sets generated by FG-DMs. Each set contains segmentation masks and the corresponding images generated for the MM-CelebA-HQ, ADE-20K, Cityscapes, and COCO datasets. The FG-DMs were trained from scratch with 53 million parameters for each factor. The results demonstrate the FG-DM's ability to synthesize high-quality images and segmentation masks for diverse datasets and image types.", "section": "Experimental Results"}, {"figure_path": "sntv8Ac3U2/figures/figures_22_2.jpg", "caption": "Figure 1: Comparison of FG-DM (bottom) against Stable Diffusion (top) for sampling images with high object recall by modeling the joint distribution of images and conditioning variables. FG-DM supports creative, controllable, interpretable and faster (4x) image synthesis than Stable Diffusion to achieve the desired object recall. Note that the conditions y\u00b9 or y2 can be null due to classifier-free guidance training.", "description": "This figure compares the proposed Factor Graph Diffusion Model (FG-DM) against the Stable Diffusion model.  FG-DM models the joint distribution of images and conditioning variables (e.g., segmentation maps, poses) to improve object recall and enable controllable image synthesis. The diagram illustrates how FG-DM achieves faster inference and higher creative control than Stable Diffusion through its modular factor graph structure.", "section": "1 Introduction"}, {"figure_path": "sntv8Ac3U2/figures/figures_24_1.jpg", "caption": "Figure 1: Comparison of FG-DM (bottom) against Stable Diffusion (top) for sampling images with high object recall by modeling the joint distribution of images and conditioning variables. FG-DM supports creative, controllable, interpretable and faster (4x) image synthesis than Stable Diffusion to achieve the desired object recall. Note that the conditions y\u00b9 or y\u00b2 can be null due to classifier-free guidance training.", "description": "This figure compares the proposed FG-DM model with the Stable Diffusion model for image synthesis.  The FG-DM models the joint distribution of images and conditioning variables (like segmentation maps), offering more control and faster synthesis compared to the Stable Diffusion model. The FG-DM achieves higher object recall. Classifier-free guidance training allows some conditions to be optional.", "section": "1 Introduction"}, {"figure_path": "sntv8Ac3U2/figures/figures_24_2.jpg", "caption": "Figure 1: Comparison of FG-DM (bottom) against Stable Diffusion (top) for sampling images with high object recall by modeling the joint distribution of images and conditioning variables. FG-DM supports creative, controllable, interpretable and faster (4x) image synthesis than Stable Diffusion to achieve the desired object recall. Note that the conditions y\u00b9 or y\u00b2 can be null due to classifier-free guidance training.", "description": "This figure compares the proposed Factor Graph Diffusion Model (FG-DM) with Stable Diffusion.  It shows how FG-DM models the joint distribution of images and conditioning variables (like segmentation maps), enabling more control, higher object recall, and faster image synthesis than Stable Diffusion.  The FG-DM's modular design allows for creative image editing and efficient sampling-based prompt compliance.", "section": "1 Introduction"}]