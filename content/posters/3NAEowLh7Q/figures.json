[{"figure_path": "3NAEowLh7Q/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of two text query strategies. (a) demonstrates the process of rendering a feature map and computing its similarity with text features to obtain a 2D mask, which is then used to generate a corresponding rendered image. (b) demonstrates the direct similarity computation of 3D Gaussian language features with text features, selecting Gaussian points with high similarity, and rendering to obtain a rendered image.", "description": "This figure compares two different methods for querying 3D scenes using text.  Method (a) renders a feature map from the 3D scene and compares this map to text features to obtain a 2D mask.  This mask is then used to select the relevant parts of the 3D scene to render. Method (b), the proposed method, directly compares text features to the language features of the 3D Gaussian points to select the relevant points before rendering, leading to a more efficient process. ", "section": "1 Introduction"}, {"figure_path": "3NAEowLh7Q/figures/figures_3_1.jpg", "caption": "Figure 2: (a) We use the view-independent SAM boolean mask to train 3D instance features with 3D consistency for 3DGS.(b) We propose a two-level codebook for discretizing instance features from coarse to fine. (c) An instance-level 3D-2D feature association method to associate 2D CLIP features with 3D points without training.", "description": "This figure illustrates the OpenGaussian method's three main stages.  (a) shows how instance features are learned from SAM masks with a focus on maintaining 3D consistency across different views. (b) details the two-level codebook used for discretizing instance features, enhancing both efficiency and the distinctiveness of features for each instance. (c) describes the instance-level 3D-2D feature association method which links 3D points to 2D CLIP features without additional training.", "section": "3 Method"}, {"figure_path": "3NAEowLh7Q/figures/figures_4_1.jpg", "caption": "Figure 12: 3D Gaussian feature visualization on the LERF and ScanNet datasets.", "description": "This figure shows a comparison of 3D Gaussian feature visualizations on the LERF and ScanNet datasets.  It compares the results of the proposed OpenGaussian method with those of LangSplat and LEGaussians.  The visualizations likely aim to demonstrate the differences in feature distinctiveness and spatial consistency between the methods, highlighting the improved performance of OpenGaussian in accurately identifying and representing 3D objects. Each row represents a different scene, showing multi-view renderings of the 3D features.", "section": "4 Experiments"}, {"figure_path": "3NAEowLh7Q/figures/figures_5_1.jpg", "caption": "Figure 4: We render 3D instance points to an arbitrary training view, and associate 3D points with 2D masks based on the principle of joint IoU and feature similarity, which have already been extracted with mask-level CLIP features, thereby indirectly associating 3D points with CLIP features.", "description": "This figure illustrates the method used for instance-level 2D-3D association.  It shows how 3D instance points are rendered to a 2D view.  The process uses both the Intersection over Union (IoU) between the rendered features and a SAM mask and the feature similarity between the rendered features and the mask's CLIP features to link 3D points to 2D information. This indirect association helps connect the 3D points to CLIP features without the need for depth information or occlusion testing. This approach makes the model robust and efficient.", "section": "3.3 Instance-Level 2D-3D Association without Depth Test"}, {"figure_path": "3NAEowLh7Q/figures/figures_6_1.jpg", "caption": "Figure 5: Open-vocabulary 3D object selection on the LERF dataset. OpenGaussian outperforms LangSplat and LEGaussians in accurately identifying the 3D objects corresponding to text queries.", "description": "This figure shows a qualitative comparison of open-vocabulary 3D object selection performance between OpenGaussian and two baseline methods (LangSplat and LEGaussians) on the LERF dataset.  For several text queries (e.g., \"green apple\", \"old camera\", \"pikachu\"), the figure displays the rendered results from each method.  The results demonstrate that OpenGaussian significantly outperforms the baselines in accurately identifying and rendering the 3D objects corresponding to the given text queries, highlighting its superior capabilities in open-vocabulary 3D scene understanding.", "section": "4.1 Open-Vocabulary Object Selection in 3D Space"}, {"figure_path": "3NAEowLh7Q/figures/figures_7_1.jpg", "caption": "Figure 12: 3D Gaussian feature visualization on the LERF and ScanNet datasets.", "description": "This figure shows a comparison of 3D Gaussian feature visualizations across three different methods: OpenGaussian (the proposed method), LEGaussians, and LangSplat.  The visualizations are presented for both the LERF and ScanNet datasets. Each row represents a different method, and each column shows a different viewpoint (or scene) in the dataset. The color of the points represent the different features extracted. The visualization helps to understand the differences in the quality and granularity of the features extracted by each method.", "section": "4 Experiments"}, {"figure_path": "3NAEowLh7Q/figures/figures_8_1.jpg", "caption": "Figure 7: Subjective result of click-based 3D object selection. OpenGaussian demonstrates a more complete 3D object selection without the issues of incompleteness or redundancy.", "description": "This figure shows a comparison of click-based 3D object selection results between OpenGaussian and SegAnyGaussian.  OpenGaussian, the proposed method, demonstrates more complete and accurate selection of 3D objects based on a 2D click, avoiding incompleteness or selecting redundant parts. SegAnyGaussian, in contrast, shows less accurate results with either missing parts or excessive selections.", "section": "4.3 Click-based 3D Object Selection"}, {"figure_path": "3NAEowLh7Q/figures/figures_14_1.jpg", "caption": "Figure 8: Examples of scene editing. (a) The original scene was reconstructed using OpenGaussian. (b) Selecting an object for removal. (c) Inserting a new object. (d) Changing the color of the selected object. Note that all edits are performed in 3D space, not on the image.", "description": "This figure demonstrates the scene editing capabilities of OpenGaussian.  It shows four variations of a 3D scene, all rendered using the model: (a) the original reconstructed scene; (b) the scene with one object removed; (c) the scene with a new object added; and (d) the scene with the color of a selected object changed. Importantly, the figure highlights that these edits are made directly in the 3D model representation, not as post-processing effects on a rendered image.", "section": "A.2 More Results"}, {"figure_path": "3NAEowLh7Q/figures/figures_14_2.jpg", "caption": "Figure 9: A demo of text-to-3D Gaussian retrieval on ScanNet. Top: scene0000_00; bottom: scene0645_00.", "description": "This figure demonstrates the capability of the OpenGaussian model to retrieve relevant 3D Gaussian points based on text queries.  The top row shows results for the scene 'scene0000_00', while the bottom row displays results for 'scene0645_00'.  For each scene, multiple views are presented alongside the rendered image, illustrating how the model identifies and highlights the 3D Gaussians that most closely correspond to the input text query. This showcases the models ability for open-vocabulary 3D point-level understanding, selecting only the relevant points from the 3D scene.", "section": "4.1 Open-Vocabulary Object Selection in 3D Space"}, {"figure_path": "3NAEowLh7Q/figures/figures_15_1.jpg", "caption": "Figure 10: We rasterize the 3D point instance features into multi-view images, demonstrating cross-view consistency.", "description": "This figure shows the results of rendering 3D point instance features into multiple views. The results demonstrate the cross-view consistency of the learned instance features. This is important because it shows that the features are not only consistent across different views but also provide a stable representation of the objects in the scene.", "section": "3 Method"}, {"figure_path": "3NAEowLh7Q/figures/figures_15_2.jpg", "caption": "Figure 11: Visualization of 3D point features in the real-world scene captured by mobile phone.", "description": "This figure shows the visualization of 3D point features obtained from a real-world scene captured using a mobile phone. It includes three RGB images of the scene showing different viewpoints and a point cloud visualization, highlighting the effectiveness of the proposed method in capturing real-world scenes and extracting meaningful instance-level features.", "section": "4 Experiments"}, {"figure_path": "3NAEowLh7Q/figures/figures_16_1.jpg", "caption": "Figure 6: 3D feature visualization comparison. From left to right, the scenes are ramen, teatime, scannet_0140_00, and scannet_0645_00. Our proposed method, OpenGaussian, exhibits enhanced granularity and accuracy in its features.", "description": "This figure compares the 3D feature visualizations of OpenGaussian with two other methods, LangSplat and LEGaussians, across four different scenes.  The visualization highlights the superior granularity and accuracy of OpenGaussian's features in representing objects within the 3D scenes.", "section": "4.2 Open-Vocabulary Point Cloud Understanding"}, {"figure_path": "3NAEowLh7Q/figures/figures_17_1.jpg", "caption": "Figure 13: Feature visualization of 3D points on the large-scale outdoor dataset Waymo. (a)-(f) are 6 different scenes selected from the Waymo dataset. Left: RGB image; Right: 3D point features.", "description": "This figure showcases the visualization of 3D point features extracted from six different scenes within the large-scale Waymo outdoor dataset.  Each row presents a pair of images: on the left, an RGB image from the scene, and on the right, the corresponding 3D point features are visualized. The visualization highlights the distribution and characteristics of these 3D features within various outdoor environments, demonstrating the model's ability to learn features from diverse and complex scenes.  The figure provides a visual confirmation of the model's performance in handling large-scale, outdoor datasets.", "section": "4.2 Open-Vocabulary Point Cloud Understanding"}, {"figure_path": "3NAEowLh7Q/figures/figures_17_2.jpg", "caption": "Figure 13: Feature visualization of 3D points on the large-scale outdoor dataset Waymo. (a)-(f) are 6 different scenes selected from the Waymo dataset. Left: RGB image; Right: 3D point features.", "description": "This figure visualizes the 3D point features learned by the model on six different scenes from the large-scale outdoor Waymo dataset. Each row represents a different scene. The left column shows the original RGB image of the scene, while the right column visualizes the learned 3D point features. The visualization helps to understand how well the model is able to distinguish different objects and their spatial relationships within each scene.", "section": "4.2 Open-Vocabulary Point Cloud Understanding"}, {"figure_path": "3NAEowLh7Q/figures/figures_17_3.jpg", "caption": "Figure 15: Validation of two-level codebook in outdoor scenes. Achieved better discretization with the fine-level codebook.", "description": "This figure demonstrates the effectiveness of the proposed two-level codebook for discretizing instance features. The top row shows an RGB image of a residential area, followed by the results of applying the coarse-level and fine-level codebooks respectively. Similarly, the bottom row shows another scene with the same processing. Comparing the results shows that the fine-level codebook leads to significantly better discretization of the features, resulting in improved instance-level discrimination and object segmentation.", "section": "4.4 Ablation Study"}]