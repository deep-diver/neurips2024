[{"figure_path": "RERls4Opnm/figures/figures_1_1.jpg", "caption": "Figure 1: Examples of group-invariant functions, sampled from a Gaussian process with the corresponding invariant kernel. Note that observing a point x (red) provides information about transformed locations {\u03c3(x) : \u03c3 \u2208 G} (white).", "description": "This figure shows three examples of group-invariant functions.  Each function is sampled from a Gaussian process using a kernel that incorporates the invariance properties of a specific transformation group.  The groups used are the permutation group S2, the cyclic group C3, and the dihedral group D5.  The red point represents an observation location, and the white points represent other locations that provide information due to the function's invariance under the group action.", "section": "1 Introduction"}, {"figure_path": "RERls4Opnm/figures/figures_3_1.jpg", "caption": "Figure 2: UCB run with standard k and invariant kernel kG applied to an invariant objective; queried points in white. UCB with the invariant kernel requires significantly fewer samples to find the optimum.", "description": "This figure compares the performance of the Upper Confidence Bound (UCB) algorithm using a standard kernel versus an invariant kernel (kG) on an invariant objective function. The queried points (where the function is evaluated) are shown in white. The plot demonstrates that the UCB algorithm using the invariant kernel converges to the optimum with far fewer function evaluations than the one using a standard kernel. This illustrates the efficiency gain achieved by incorporating known invariances into the kernel of the Bayesian optimization algorithm.", "section": "3 Sample complexity bounds for invariance-aware Bayesian optimisation"}, {"figure_path": "RERls4Opnm/figures/figures_7_1.jpg", "caption": "Figure 3: Regret performance of invariant UCB and MVR algorithms across 3 different tasks; lower is better. Non-invariant kernels (blue) are outperformed by the full group invariant kernel (red) as well as partially specified (subgroup-invariant) kernels (green and yellow). For the permutation invariant function, the search space of standard BO can be constrained by the fundamental domain of the group (purple), but this performs worse than the invariant kernel. Mean \u00b1 s.d., 32 seeds.", "description": "This figure compares the performance of UCB and MVR algorithms on three different synthetic optimization tasks using different kernels: standard, constrained, and invariant.  The results demonstrate that incorporating group invariance into the kernel significantly improves sample efficiency compared to standard and constrained methods, even when only partial invariance is included.", "section": "4.1 Synthetic experiments"}, {"figure_path": "RERls4Opnm/figures/figures_8_1.jpg", "caption": "Figure 4: Performance of MVR and UCB on quasi-invariant functions. Regret shown for the noninvariant kernel k' (standard, blue), the invariant kernel kg (invariant, green), and the quasi-invariant kernel kg + \u025bk' (additive, red). In all cases, the invariant kernel performs almost as well as the true quasi-invariant kernel.", "description": "This figure shows the results of experiments on quasi-invariant functions, which are functions that are almost invariant to a known group of transformations. The figure compares the performance of three different Bayesian optimization algorithms: the standard algorithm, the algorithm that uses an invariant kernel (kg), and the algorithm that uses a quasi-invariant kernel (kg + \u025bk'). The results show that the algorithm that uses the invariant kernel performs almost as well as the algorithm that uses the quasi-invariant kernel, and that both of these algorithms outperform the standard algorithm.  The plots show both simple regret and cumulative regret for the three algorithms, across different noise levels.", "section": "Extension: quasi-invariance using additive kernels"}, {"figure_path": "RERls4Opnm/figures/figures_8_2.jpg", "caption": "Figure 5: Nuclear fusion application: optimising safety factor by adjusting current drive actuator. In (a), observe that the order of launchers can be permuted without changing the total profile. Incorporating this invariance into the kernel of the UCB algorithm achieves improved performance (b).", "description": "This figure shows the results of applying the invariance-aware Bayesian optimization method to a real-world problem in nuclear fusion engineering.  Panel (a) illustrates an example of a current drive actuator profile, highlighting the permutation invariance property; the order of the launchers can be changed without affecting the total profile.  Panel (b) compares the performance of the standard UCB algorithm with the invariance-aware UCB algorithm.  The invariance-aware approach demonstrates significantly improved performance, achieving better results with fewer samples.", "section": "4.3 Real-world application: nuclear fusion scenario design"}, {"figure_path": "RERls4Opnm/figures/figures_8_3.jpg", "caption": "Figure 5: Nuclear fusion application: optimising safety factor by adjusting current drive actuator. In (a), observe that the order of launchers can be permuted without changing the total profile. Incorporating this invariance into the kernel of the UCB algorithm achieves improved performance (b).", "description": "This figure shows the results of applying the invariant UCB algorithm to a real-world nuclear fusion problem.  Subfigure (a) illustrates the current drive actuator profile, highlighting the permutation invariance of the system (changing the order of the launchers does not affect the total profile). Subfigure (b) presents a performance comparison between the standard UCB and the invariant UCB. The invariant UCB shows significantly improved performance, achieving better safety factor optimization values with fewer optimization steps.", "section": "4.3 Real-world application: nuclear fusion scenario design"}, {"figure_path": "RERls4Opnm/figures/figures_9_1.jpg", "caption": "Figure 6: Effect of group size on cost of data augmentation and invariant kernel methods. Benchmark task is to fit a 6D GP with the given kernel to 100 random datapoints from PermInv-6D. Shown are results from 100 seeds, 64 repeats per seed, performed on one NVIDIA V100-32GB GPU using BoTorch. Invariant kernels are (a) more memory efficient than data augmentation, and (b) can be computed faster. Incorporating full invariance via data augmentation exceeds the GPU memory.", "description": "This figure compares the memory and time efficiency of using invariant kernels versus data augmentation for Bayesian Optimization.  It shows that invariant kernels require significantly less memory and computation time, especially as the size of the symmetry group increases. The full invariance case using data augmentation runs out of GPU memory, highlighting the advantage of invariant kernels for high-dimensional problems with large symmetry groups.", "section": "5 Limitations"}, {"figure_path": "RERls4Opnm/figures/figures_20_1.jpg", "caption": "Figure 7: Examples of invariant functions on the sphere. The construction for our lower bound consists of functions invariant to finite rotations, as in Figure 7a, but does not include functions like Figure 7b (which produce different packings).", "description": "This figure shows two examples of invariant functions on a sphere.  The left image (7a) displays a function invariant to a 10-fold rotation, while the right image (7b) shows a function invariant to all rotations about a single axis. The paper's authors used functions like 7a (invariant under finite rotations) to construct a lower bound on sample complexity, explicitly excluding functions such as 7b that would lead to different function packing behaviors and affect the resulting bound.", "section": "A.3 An explicit construction"}]