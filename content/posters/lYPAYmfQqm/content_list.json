[{"type": "text", "text": "Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yingcong Li University of Michigan yingcong@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Ankit Singh Rawat Google Research NYC ankitsrawat@google.com ", "page_idx": 0}, {"type": "text", "text": "Samet Oymak University of Michigan oymak@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent research has shown that Transformers with linear attention are capable of in-context learning (ICL) by implementing a linear estimator through gradient descent steps. However, the existing results on the optimization landscape apply under stylized settings where task and feature vectors are assumed to be IID and the attention weights are fully parameterized. In this work, we develop a stronger characterization of the optimization and generalization landscape of ICL through contributions on architectures, low-rank parameterization, and correlated designs: (1) We study the landscape of 1-layer linear attention and 1-layer H3, a statespace model. Under a suitable correlated design assumption, we prove that both implement 1-step preconditioned gradient descent. We show that thanks to its native convolution filters, H3 also has the advantage of implementing sample weighting and outperforming linear attention in suitable settings. (2) By studying correlated designs, we provide new risk bounds for retrieval augmented generation (RAG) and task-feature alignment which reveal how ICL sample complexity beneftis from distributional alignment. (3) We derive the optimal risk for low-rank parameterized attention weights in terms of covariance spectrum. Through this, we also shed light on how LoRA can adapt to a new distribution by capturing the shift between task covariances. Experimental results corroborate our theoretical findings. Overall, this work explores the optimization and risk landscape of ICL in practically meaningful settings and contributes to a more thorough understanding of its mechanics. ", "page_idx": 0}, {"type": "image", "img_path": "lYPAYmfQqm/tmp/f13bc58d6b718731317070070c92f4a04da8c474a6a02dc83b87349a69184ec9.jpg", "img_caption": ["Figure 1: We investigate the optimization landscape of in-context learning from the lens of architecture choice, the role of distributional alignment, and low-rank parameterization. The empirical performance (solid curves) are aligned with our theoretical results (dotted curves) from Section 3. More experimental details and discussion are deferred to Section 4. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Modern language models exhibit the remarkable ability to learn novel tasks or solve complex problems from the demonstrations provided within their context window [Brown et al., 2020, GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023]. Such in-context learning (ICL) offers a novel and effective alternative to traditional fine-tuning techniques and has become an important feature of LLM with its applications spanning retrieval-augmented generation [Lewis et al., 2020], and reasoning via advanced prompting techniques, such as chain-of-thought [Wei et al., 2022]. ", "page_idx": 1}, {"type": "text", "text": "ICL ability presents an important research avenue to develop stronger theoretical and mechanistic understanding of large language models. To this aim, there has been significant recent interest in demystifying ICL through the lens of function approximation [Liu et al., 2023a], Bayesian inference [M\u00fcller et al., 2021, Xie et al., 2022, Han et al., 2023], and learning and optimization theory [Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Duraisamy, 2024]. The latter is concerned with understanding the optimization landscape of ICL, which is also crucial for understanding the generalization properties of the model. A notable result in this direction is the observation that linear attention models [Schlag et al., 2021, Von Oswald et al., 2023, Ahn et al., 2023] implement preconditioned gradient descent (PGD) during ICL [Ahn et al., 2023, Mahdavi et al., 2024]. While this line of works provide a fresh perspective to ICL, the existing studies do not address many questions arising from real-life applications nor provide guiding principles for various ICL setups motivated by practical considerations. ", "page_idx": 1}, {"type": "text", "text": "To this aim, we revisit the theoretical exploration of ICL with linear data model where we feed an in-context prompt containing $n$ examples $(\\pmb{x}_{i},y_{i}=\\pmb{x}_{i}^{\\top}\\pmb{\\beta}+\\pmb{\\xi}_{i})_{i=1}^{n}\\subset\\mathbb{R}^{d}\\times\\mathbb{R}$ and a test instance or query $\\pmb{x}_{n+1}\\in\\mathbb{R}^{d}$ to the model, with $d$ being the feature dimension, $\\beta\\in\\mathbb{R}^{d}$ being the task weight vector, and $(\\xi_{i})_{i=1}^{n}$ denoting the noise in individual labels. Given the in-context prompt, the model is tasked to predict ${\\hat{y}}_{n+1}-\\mathbf{an}$ estimate for $y_{n+1}=\\mathbf{x}_{n+1}^{\\top}\\beta+\\xi_{n+1}$ . We aim to provide answers to the following questions by exploring the loss landscape of ICL: ", "page_idx": 1}, {"type": "text", "text": "(Q1) Is the ability to implement gradient-based ICL unique to (linear) attention? Can alternative sequence models implement richer algorithms beyond PGD?   \n(Q2) In language modeling, ICL often works well with few-shot samples whereas standard linear estimation typically requires $O(d)$ samples. How can we reconcile this discrepancy between classical learning and ICL?   \n(Q3) To our knowledge, existing works assume linear-attention is fully parameterized, i.e., key and query projections $W_{k}$ $\\mathbf{\\Omega},\\pmb{W_{q}}\\in\\mathbb{R}^{d\\times d}$ . What happens when they are low-rank? What happens when there is distribution shift between training and test in-context prompts and we use LoRA [Hu et al., 2022] for adaptation? ", "page_idx": 1}, {"type": "text", "text": "In this work, we conduct a careful investigation of these questions. Specifically, we focus on ICL with 1-layer models and make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "(A1) We jointly investigate the landscape of linear attention and H3 [Fu et al., 2023], a widely popular state-space model (SSM). We prove that under correlated design, both models implement 1-step PGD (c.f. Proposition 1) and the alignments in Fig. 1a verify that where the dotted curve represents the theoretical PGD result derived from Theorem 1. Our analysis reveals that the gating mechanism in H3 imitates attention. We also empirically show that H3 has the advantage of implementing sample-weighting which allows it to outperform linear attention in temporally-heterogeneous problem settings in Appendix D. ", "page_idx": 1}, {"type": "text", "text": "(A2) Proposition 1 allows for task and features to be correlated to each other as long as odd moments are zero. Through this, we can assess the impact of distributional alignment on the sample complexity of ICL. Specifically, we characterize the performance of Retrieval Augmented Generation (RAG) (c.f. Theorem 2 and Fig. 1b) and Task-Feature Alignment (c.f. Theorem 3), where the in-context examples are $\\alpha$ -correlated with either the query or the task vector. For both settings, we prove that alignment amplifies the effective sample size of ICL by a factor of $\\alpha^{2}d+1$ , highlighting that aligned data are crucial for the success of ICL in few-shot settings.   \n(A3) We show that, under low-rank parameterization, optimal attention-weights still implements PGD according to the truncated eigenspectrum of the fused task-feature covariance (see Section 3.2). We similarly derive risk upper bounds for LoRA adaptation (c.f. Eq. (14) and Fig. 1c), and show that, these bounds accurately predict the empirical performance. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Setup and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin with a short note on notation. Let bold lowercase and uppercase letters (e.g., $\\pmb{x}$ and $X$ ) represent vectors and matrices, respectively. The symbol $_{\\odot}$ is defined as the element-wise (Hadamard) product, and $^*$ denotes the convolution operator. $\\mathbf{1}_{d}$ and $\\mathbf{0}_{d}$ denote the $d$ -dimensional all-ones and all-zeros vectors, respectively; and $\\pmb{I}_{d}$ denotes the identity matrix of dimension $d\\times d$ . Additionally, let tr (W) denote the trace of the square matrix $W$ . ", "page_idx": 2}, {"type": "text", "text": "As mentioned earlier, we study the optimization landscapes of 1-layer linear attention [Katharopoulos et al., 2020, Schlag et al., 2021] and H3 [Fu et al., 2023] models when training with prompts containing in-context data following a linear model. We construct the input in-context prompt similar to Ahn et al. [2023], Mahankali et al. [2024], Zhang et al. [2024] as follows. ", "page_idx": 2}, {"type": "text", "text": "Linear data distribution. Let $(\\pmb{x},\\mathrm{y})\\in\\mathbb{R}^{d}\\times\\mathbb{R}$ be a (feature, label) pair generated by a $d$ -dimensional linear model parameterized by $\\beta\\in\\mathbb{R}^{d}$ , i.e., $\\mathrm y=\\pmb{x}^{\\top}\\pmb{\\beta}+\\pmb{\\xi}$ , where $\\boldsymbol{x}$ and $_\\beta$ are feature and task vectors, and $\\xi$ is the label noise. Given demonstrations $(x_{i},y_{i})_{i=1}^{n+1}$ sampled from a single $_\\beta$ , define the input in-context prompt ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{Z}=[z_{1}\\,\\hdots\\,z_{n}\\,z_{n+1}]^{\\top}=\\left[\\pmb{x}_{1}\\hdots\\hdots\\begin{array}{r l r l}{\\pmb{x}_{n}}&{\\hdots\\}&{\\pmb{x}_{n}}&{\\pmb{x}_{n+1}}\\\\ {\\pmb{y}_{1}}&{\\hdots\\cdot}&{\\pmb{y}_{n}}&{0}\\end{array}\\right]^{\\top}\\in\\mathbb{R}^{(n+1)\\times(d+1)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, we set $z_{i}=\\binom{\\pmb{x}_{i}}{\\mathrm{y}_{i}}$ for $i\\le n$ and the last/query token $z_{n+1}={\\binom{x_{n+1}}{0}}.$ Then, given ${\\cal Z}$ , the goal of the model is to predict the correct label $y_{n+1}$ corresponding to $x_{n+1}$ . For cleaner notation, when it is clear from context, we drop the subscript $n+1$ and set $\\pmb{x}=\\pmb{x}_{n+1}$ , $z=z_{n+1}$ . Different from the previous work [Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Mahdavi et al., 2024] where $(\\pmb{x}_{i})_{i=1}^{n+1}$ and $_\\beta$ are assumed to be independent, our analysis focuses on a more general linear setting that captures the dependency between $(\\pmb{x}_{i})_{i=1}^{n+1}$ and $_\\beta$ . ", "page_idx": 2}, {"type": "text", "text": "Model architectures. To start with, we first review the architectures of both Transformer and state-space model (SSM). Similar to the previous work [Von Oswald et al., 2023, Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024] and to simplify the model structure, we focus on singlelayer models and omit the nonlinearity, e.g., softmax operation and MLP activation, from the Transformer. Given the input prompt $\\bar{Z}\\in\\mathbb{R}^{(n+1)\\times(d+1)}$ in (1), which can be treated as a sequence of $(d+1)$ -dimensional tokens, the single-layer linear attention ATT and H3-like single-layer SSM SSM are denoted by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathtt{A T T}(\\mathbfcal{Z})=(Z W_{q}W_{k}^{\\top}\\mathbfcal{Z}^{\\top})Z W_{\\nu}}\\\\ &{\\mathtt{S S M}(\\mathbfcal{Z})=\\left((Z W_{q})\\odot((\\mathbfcal{Z}W_{k}\\odot Z W_{\\nu})*f)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $W_{k}$ , $W_{q}$ , $\\pmb{W}_{\\nu}\\,\\in\\,\\mathbb{R}^{(d+1)\\times(d+1)}$ denote the key, query and value weight matrices, respectively. In (2b), the parameter $f\\in\\mathbb{R}^{n+1}$ is a 1-D convolutional filter that mixes tokens. The Hadamard product $_{\\odot}$ is the gating mechanism [Dauphin et al., 2017] between key and query channels, which is crucial for attention-like feature creation. Thus, (2b) is more generally a gated-convolution layer. For $\\boldsymbol{f}$ only, we use indexing $f=[f_{0}\\,\\,\\ldots\\,\\,f_{n}]^{\\intercal}\\in\\mathbb{R}^{n+1}$ and given any vector $\\pmb{a}$ , denote convolution output $\\begin{array}{r}{\\dot{(\\pmb{a}\\ast\\dot{f})_{i}}=\\sum_{j=1}^{i}f_{i-j}a_{j}}\\end{array}$ . Note that our notation slightly differs from the original H3 model [Fu et al., 2023] in two ways: ", "page_idx": 2}, {"type": "text", "text": "1. SSMs provide efficient parameterization of $\\boldsymbol{f}$ which would otherwise grow with sequence length. In essence, H3 utilizes a linear state-space model $\\pmb{s}_{i}=A\\pmb{s}_{i-1}+\\pmb{B}u_{i}$ and $y_{i}=C s_{i}$ with parameters $(\\pmb{A}\\in\\mathbb{R}^{d\\times d},\\pmb{B}\\in\\mathbb{R}^{d\\times1},\\pmb{C}\\in\\mathbb{R}^{1\\times d})$ from which the filter $\\textbf{\\emph{f}}$ is obtained via the impulse response $f_{i}=C A^{i}B$ for $i\\geq0$ . Here $d$ is the state dimension and, in practice, $\\boldsymbol{A}$ is chosen to be diagonal. Observe that, setting $d=1$ and $\\pmb{A}=\\rho,\\pmb{C}=\\pmb{B}=1$ , SSM reduces to the exponential smoothing $f_{i}=\\rho^{i}$ for $i\\geq0$ . Thus, H3 also captures the all-ones fliter as a special instance. As we show in Proposition 1, this simple filter is optimal under independent data model and exactly imitates linear attention. Note that, utilizing a fliter $\\textbf{\\emph{f}}$ as in (2b) is strictly more expressive than the SSM as it captures all possible impulse responses. 2. H3 also applies a shift SSM to the key embeddings to enable the retrieval of the local context around associative recall hits. We opted not to incorporate this shift operator in our model. This is because unless the features of the neighboring tokens are correlated (which is not the case for the typical independent data model), the entry-wise products between values and shifted keys will have zero mean and be redundant for the final prediction. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We note that we conduct all empirical evaluations with the original H3 model, which displays exact agreement with our theory formalized for (6b), further validating our modeling choice. ", "page_idx": 3}, {"type": "text", "text": "2.1 In-context Linear Estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We will next study the algorithms that can be implemented by the single-layer attention and statespace models. Through this, we will show that training ATT and SSM with linear ICL data is equivalent to the prediction obtained from one step of optimally-preconditioned gradient descent (PGD) and sample-weighted preconditioned gradient descent (WPGD), respectively. We will further show that under mild assumption, the optimal sample weighting for SSM (e.g., $\\boldsymbol{f}$ ) is an all-ones vector and therefore, establishing the equivalence among PGD, ATT, and SSM. ", "page_idx": 3}, {"type": "text", "text": "Background: 1-step gradient descent. Consider minimizing squared loss and solving linear regression using one step of PGD and WPGD. Given $n$ samples $(x_{i},y_{i})_{i=1}^{n}$ , define ", "page_idx": 3}, {"type": "equation", "text": "$$\nX=[x_{1}\\,\\cdots\\,\\pmb{x}_{n}]^{\\top}\\in\\mathbb{R}^{n\\times d}\\quad\\mathrm{and}\\quad\\pmb{y}=[y_{1}\\,\\cdots\\,y_{n}]^{\\top}\\in\\mathbb{R}^{n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Starting from $\\hat{\\rho}_{0}=\\mathbf{0}_{d}$ and letting $\\eta=1/2$ be the step size, a single-step GD preconditioned with weights $W$ returns prediction ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{y}=\\pmb{x}^{\\top}\\pmb{W}\\pmb{X}^{\\top}\\pmb{y}:=\\ g_{\\mathrm{PGD}}(\\pmb{Z}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and a single-step sample-weighted GD given weights $\\pmb{\\omega}\\in\\mathbb{R}^{n}$ and $W\\in\\mathbb{R}^{d\\times d}$ returns prediction ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{y}=x^{\\top}W X^{\\top}(\\omega\\odot y):=g_{\\mathtt{W P G D}}(Z),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\cal Z}$ is defined in (1) consisting of $X,y$ and $\\pmb{x}$ . Our goal is to find the optimal $W$ , as well as $\\omega$ in (4) that minimize the population risks defined as follows. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{W}{\\operatorname*{min}}\\mathcal{L}_{\\mathtt{P G D}}(\\mathcal{W})}&{\\mathrm{where}\\quad\\mathcal{L}_{\\mathtt{P G D}}(\\mathcal{W})=\\mathbb{E}\\left[\\left(y-g_{\\mathtt{P G D}}(Z)\\right)^{2}\\right],}\\\\ {\\underset{W,\\omega}{\\operatorname*{min}}\\mathcal{L}_{\\mathtt{N P G D}}(\\mathcal{W})}&{\\mathrm{where}\\quad\\mathcal{L}_{\\mathtt{M P G D}}(\\mathcal{W})=\\mathbb{E}\\left[\\left(y-g_{\\mathtt{M P G D}}(Z)\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, the expectation is over the randomness in $(x_{i},\\xi_{i})_{i=1}^{n+1}$ and $_\\beta$ , and we use $\\boldsymbol{\\cdot}$ to represent the set of corresponding trainable parameters. The search spaces for $\\omega$ and $W$ are ${\\mathbb{R}}^{n}$ and $\\mathbb{R}^{d\\times d}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "As per (2), given input prompt $Z\\in\\mathbb{R}^{(n+1)\\times(d+1)}$ , either of the underlying models outputs a $(n\\!+\\!1)$ -length sequence. Note that the label for the query $\\pmb{x}=\\pmb{x}_{n+1}$ is excluded from the prompt ${\\cal Z}$ . Similar to Ahn et al. [2023], Mahankali et al. [2024], we consider a training objective with a causal mask to ensure inputs cannot attend to their own labels and training can be parallelized. Let $Z_{0}=[z_{1}\\,\\ldots\\,z_{n}\\,0]^{\\top}$ be the features post-causal masking at time/index $n+1$ . Given weights $\\boldsymbol{W_{k}},\\boldsymbol{W_{q}},\\boldsymbol{W_{\\nu}}$ and the filter $\\textbf{\\emph{f}}$ for SSM, predictions at the query token $z={\\binom{x}{0}}$ take the following forms following sequence-to-sequence mappings in (2): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{\\mathtt{A T T}}(\\mathbf{Z})=(z^{\\top}W_{q}W_{k}^{\\top}\\mathbf{Z}_{0}^{\\top})\\mathbf{Z}_{0}W_{\\nu}\\nu,}\\\\ &{g_{\\mathtt{S S M}}(\\mathbf{Z})=\\left((z^{\\top}W_{q})^{\\top}\\odot((\\mathbf{Z}_{0}W_{k}\\odot\\mathbf{Z}_{0}W_{\\nu})*f)_{n+1}\\right)\\nu,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{\\nu}\\in\\mathbb{R}^{d+1}$ is the linear prediction head and $((Z_{0}W_{k}\\odot Z_{0}W_{\\nu})*f)_{n+1}$ returns the last row of the convolution output. Note that SSM can implement the mask by setting $f_{0}=0$ . Now consider the meta learning setting and select loss function to be the squared loss, same as in (5). Thus, the objectives for both models take the following forms. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{W_{k},W_{q},W_{\\nu},\\nu}{\\mathrm{min}}\\mathcal{L}_{\\mathrm{ATT}}(\\mathcal{W})}&{\\mathrm{where}\\quad\\mathcal{L}_{\\mathrm{ATT}}(\\mathcal{W})=\\mathbb{E}\\left[(y-g_{\\mathrm{ATT}}(Z))^{2}\\right],}\\\\ {\\underset{W_{k},W_{q},W_{\\nu},\\nu,f}{\\mathrm{min}}\\mathcal{L}_{\\mathrm{SSI}}(\\mathcal{W})}&{\\mathrm{where}\\quad\\mathcal{L}_{\\mathrm{SSI}}(\\mathcal{W})=\\mathbb{E}\\left[(y-g_{\\mathrm{SSI}}(Z))^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, similarly, the expectation subsumes the randomness of $(\\pmb{x}_{i},\\pmb{\\xi}_{i})_{i=1}^{n+1}$ and $_\\beta$ and $\\boldsymbol{\\Phi}$ represents the set of trainable parameters. The search space for matrices $W_{k},\\,W_{q}$ , $W_{\\nu}$ is $\\mathbb{R}^{(d+1)\\times(d+1)}$ , for head $\\nu$ is $\\mathbb{R}^{d+1}$ , and for $\\boldsymbol{f}$ is $\\mathbb{R}^{n+1}$ . ", "page_idx": 3}, {"type": "text", "text": "Note that for all the optimization methods (c.f. (5), (6)), to simplify the analysis, we train the models without capturing additional bias terms. Therefore, in the following, we introduce the centralized data assumptions such that the models are trained to make unbiased predictions. ", "page_idx": 4}, {"type": "text", "text": "To begin with, a cross moment of random variables is defined as the expectation of a monomial of these variables, with the order of the cross moment being the same as order of the monomial. For example, $\\mathbb{E}[{\\pmb x}^{\\top}{\\pmb W}{\\pmb\\beta}]$ is a sum of cross-moments of order 2. Then, it motivates the following data assumptions. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 All cross moments of the entries of $(\\pmb{x}_{i})_{i=1}^{n+1}$ and $_\\beta$ with odd orders are zero. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 The label noise $(\\xi_{i})_{i=1}^{n+1}$ are independent of $(\\pmb{x}_{i})_{i=1}^{n+1}$ and $\\beta$ , and their cross moments with odd orders are zero. ", "page_idx": 4}, {"type": "text", "text": "Note that compared to Ahn et al. [2023], Mahankali et al. [2024], Zhang et al. [2024], Assumption 1 is more general which also subsumes the dependent distribution settings. In this work, we consider the following three linear models (omitting noise) satisfying Assumption 1. Let $\\Sigma_{\\beta},\\Sigma_{x}\\in\\mathbb{R}^{d\\times d}$ represent the task and feature covariance matrices for independent data, and let $0\\leq\\alpha\\leq1$ be the correlation level when considering data dependency. More specific discussions are deferred to Section 3. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Independent task and data: $\\beta\\sim N(0,\\Sigma_{\\beta})$ , $\\pmb{x}_{i}\\sim N(0,\\pmb{\\Sigma}_{x})$ , for all $1\\leq i\\leq n+1$ . \u2022 Retrieval augmented generation: $\\beta,x\\sim{\\cal N}(0,I_{d}),\\;x_{i}\\:\\middle|\\;x\\sim{\\cal N}(\\alpha x,(1-\\alpha^{2})I_{d}),$ , for all $1\\leq i\\leq n$ . \u2022 Task-feature alignment: $\\beta\\sim{\\cal N}(0,I_{d}),\\,\\,x_{i}\\,\\big|\\,\\beta\\sim{\\cal N}(\\alpha\\beta,I_{d}),\\,\\,\\,\\mathrm{for\\,all}\\,\\,\\,1\\leq i\\leq n+1.$ ", "page_idx": 4}, {"type": "text", "text": "Next, we introduce the following result which establishes the equivalence among optimizing 1-layer linear attention (c.f. (6a)), 1-layer H3 (c.f. (6b)), and 1-step gradient descent (c.f. (5)). ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 Suppose Assumptions $^{\\,l}$ and 2 hold. Consider the objectives as defined in (5) and (6), and let $\\mathcal{L}_{P G D}^{\\star}$ , $\\mathcal{L}_{\\mathtt{W P G D}}^{\\star}$ , $\\mathcal{L}_{A T T}^{\\star},$ and $\\mathcal{L}_{S S M}^{\\star}$ be their optimal risks, respectively. Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{P G D}^{\\star}=\\mathcal{L}_{A T T}^{\\star}\\quad a n d\\quad\\mathcal{L}_{mathtt{M P G D}}^{\\star}=\\mathcal{L}_{S S M}^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Additionally, if the examples $(x_{i},y_{i})_{i=1}^{n}$ follow the same distribution and are conditionally independent given $_{x,\\beta}$ , then SSM/H3 can achieve the optimal loss using the all-ones filter and $\\begin{array}{r}{\\mathcal{L}_{P G D}^{\\star}=\\mathcal{L}_{S S M}^{\\star}.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "We defer the proof to Appendix A.1. Proposition 1 establishes that analyzing the optimization landscape of ICL for both single-layer linear attention and the H3 model can be effectively reduced to examining the behavior of a one-step PGD algorithm. Notably, under the independent, RAG and task-feature alignment data settings discussed above, examples $(x_{i},y_{i})_{i=1}^{n}$ are independently sampled given $\\pmb{x}$ and $_\\beta$ , and we therefore conclude that $\\begin{array}{r}{\\mathcal{L}_{\\mathtt{P G D}}^{\\star}\\,=\\,\\mathcal{L}_{\\mathtt{A T T}}^{\\star}\\,=\\,\\mathcal{L}_{\\mathtt{S S M}}^{\\star}}\\end{array}$ LS\u22c6SM. Leveraging this result, the subsequent section of the paper concentrate on addressing (5a), taking into account various linear data distributions. ", "page_idx": 4}, {"type": "text", "text": "While Proposition 1 demonstrates the equivalence of optimal losses, we also study the uniqueness and equivalence of optimal prediction functions. To this end, we analyze the strong convexity of $\\mathcal{L}_{\\mathrm{PGD}}(\\mathcal{W})$ and derive the subsequent lemmas. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 Suppose Assumption 2 holds and let $\\pmb{\\xi}=[\\xi_{1}\\,\\xi_{2}\\,\\cdot\\cdot\\cdot\\,\\xi_{n}]^{\\top}$ . Then the loss $\\mathcal{L}_{P G D}(\\mathcal{W})$ in (5a) is strongly-convex if and only $i f\\mathbb{E}[(x^{\\top}W X^{\\top}X\\beta)^{2}]+\\mathbb{E}[(x^{\\top}W X^{\\top}\\overleftarrow{\\xi})^{2}]$ is strongly-convex. Additionally, let $g_{P G D}^{\\star},\\ g_{A T T}^{\\star}$ be the optimal prediction functions of (5a) and (6a). Then under the conditions of Assumptions 1 and 2, and the strong convexity, gP\u22c6GD = gA\u22c6TT. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2 Suppose that the label noise $(\\xi_{i})_{i=1}^{n}$ are i.i.d., zero-mean, variance $\\sigma^{2}$ and independent of everything else, and that there is a decomposition ${\\pmb x}={\\pmb x}_{1}+{\\pmb x}_{2}$ , $X=X_{1}+X_{2}$ , and $\\beta=\\beta_{1}+\\beta_{2}$ such that either of the following holds ", "page_idx": 4}, {"type": "text", "text": "\u2022 $\\sigma>0$ , and $(x_{1},X_{1})$ have full rank covariance and are independent of each other and $(x_{2},X_{2})$ . ", "page_idx": 4}, {"type": "text", "text": "\u2022 $(x_{1},\\beta_{1},X_{1})$ have full rank covariance and are independent of each other and $(x_{2},\\beta_{2},X_{2})$ . ", "page_idx": 4}, {"type": "text", "text": "Then, the loss $\\mathcal{L}_{P G D}(\\mathcal{W})$ in (5a) is strongly-convex. ", "page_idx": 4}, {"type": "text", "text": "As mentioned above, in this work, we study three specific linear models: with general independent, RAG-related, and task-feature alignment data. Note that for all the three cases, according to Proposition 1, we have $\\begin{array}{r}{\\mathcal{L}_{\\mathtt{P G D}}^{\\star}=\\mathcal{L}_{\\mathtt{A T T}}^{\\star}=\\mathcal{L}_{\\mathtt{S S M}}^{\\star}}\\end{array}$ . Additionally, the second claim in Lemma 2 holds, and $\\mathcal{L}_{\\mathrm{PGD}}(\\mathcal{W})$ is strongly convex. Therefore, following Lemma 1, we have $g_{\\mathtt{P G D}}^{\\star}=g_{\\mathtt{A T T}}^{\\star}$ . Thanks to the equivalence among PGD, ATT, and SSM, in the next section, we focus on the solution of objective (5a) under different scenarios, which will reflect the optimization landscapes of ATT and SSM models. ", "page_idx": 5}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In light of Proposition 1, optimizing a single layer linear-attention or H3 model is equivalent to solving the objective (5a). Therefore, in this section, we examine the properties of the one-step PGD in (5a). To this end, we consider multiple problem settings, including distinct data distributions and low-rank training. The latter refers to the scenario where the key and query matrices have rank restrictions, e.g., ${\\pmb{W}}_{k},{\\pmb{W}}_{q}\\in\\mathbb{R}^{(d+1)\\times r}$ , as well as LoRA-tuning when adapting the model under distribution shift. ", "page_idx": 5}, {"type": "text", "text": "3.1 Analysis of Linear Data Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first consider the standard independent data setting. We will then examine correlated designs. ", "page_idx": 5}, {"type": "text", "text": "Independent data model. Let $\\Sigma_{x}$ and $\\Sigma_{\\beta}$ be the covariance matrices of the input feature and task vectors, respectively, and $\\sigma\\geq0$ be the noise level. We assume ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta\\sim{\\cal N}(0,\\Sigma_{\\beta}),\\quad x_{i}\\sim{\\cal N}(0,\\Sigma_{x}),\\quad\\xi_{i}\\sim{\\cal N}(0,\\sigma^{2}),\\quad1\\leq i\\leq n+1\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and the label is obtained via $y_{i}=\\mathbf{\\boldsymbol{x}}_{i}^{\\top}{\\boldsymbol{\\beta}}+{\\boldsymbol{\\xi}}_{i}$ . Our following result characterizes the optimal solution of (5a). Note that the data generated from (7) satisfies the conditions in Proposition 1. Therefore, the same results can be applied to both linear-attention and H3 models. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 Consider independent linear data as defined in (7), and suppose the covariance matrices $\\Sigma_{x},\\Sigma_{\\beta}$ are full rank. Recap the objective from (5a) and let $W_{\\star}:=$ arg minW $\\mathcal{L}_{P G D}(W)$ , and $\\mathcal{L}_{\\star}\\,=$ $\\mathcal{L}_{P G D}(\\boldsymbol{W}_{\\star})$ . Additionally, let $\\pmb{\\Sigma}=\\pmb{\\Sigma}_{x}^{1/2}\\pmb{\\Sigma}_{\\beta}\\pmb{\\Sigma}_{x}^{1/2}$ and $M=\\tau r(\\Sigma)+\\sigma^{2}$ . Then $W_{\\star}$ and $\\mathcal{L}_{\\star}$ satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{\\star}=\\Sigma_{x}^{-1/2}\\bar{W}_{\\star}\\Sigma_{x}^{-1/2}\\qquad a n d\\qquad\\mathcal{L}_{\\star}=M-n t r\\left(\\Sigma\\bar{W}_{\\star}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we define $\\bar{W}_{\\star}=\\left((n+1)I_{d}+M\\Sigma^{-1}\\right)^{-1}$ ", "page_idx": 5}, {"type": "text", "text": "Corollary 1 Consider noiseless i.i.d. linear data where $\\Sigma_{x}=\\Sigma_{\\beta}=I_{d}$ and $\\sigma=0$ . Then, the objective in (5a) returns ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{\\star}=\\frac{1}{n+d+1}I_{d}\\qquad a n d\\qquad\\mathcal{L}_{\\star}=d-\\frac{n d}{n+d+1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "See Appendix B.2 for proofs. Note that Theorem 1 is consistent with prior work [Ahn et al., 2023, Theorem 1] when specialized to isotropic task covariance, i.e., $\\pmb{\\Sigma}_{\\beta}=\\pmb{I}_{d}$ . However, their result is limited as the features and task are assumed to be independent. This prompts us to ask: What is the optimization landscape with correlated in-context samples? Toward this, we consider the following RAG-inspired and task-feature alignment models, where Assumptions 1 and 2 continue to hold and Proposition 1 applies. ", "page_idx": 5}, {"type": "text", "text": "Retrieval augmented generation. To provide a statistical model of the practical RAG approaches, given the query vector $\\pmb{x}_{n+1}=\\pmb{x}$ , we propose to draw ICL demonstrations that are similar to $\\boldsymbol{x}$ with the same shared task vector $_\\beta$ . Modeling feature similarity through the cosine angle, RAG should sample the ICL examples $\\pmb{x}_{i}$ , $i\\le n$ , from the original feature distribution conditioned on the event $\\cos({\\pmb x}_{i},{\\pmb x})\\ge\\alpha$ where $\\alpha$ is the similarity threshold. As an approximate proxy, under the Gaussian distribution model, we assume that $\\pmb{\\beta}\\sim N(0,\\pmb{I}_{d})$ , $\\pmb{x}\\sim N(0,\\pmb{I}_{d})$ and that RAG samples $\\alpha$ -correlated demonstrations $(x_{i},y_{i})_{i=1}^{n}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nx_{i}\\,\\big|\\,x\\sim N(\\alpha x,(1-\\alpha^{2})I_{d}),\\quad\\xi_{i}\\sim N(0,\\sigma^{2})\\quad\\mathrm{and}\\quad y_{i}=x_{i}^{\\top}\\beta+\\xi_{i},\\quad1\\le i\\le n.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the above normalization ensures that the marginal feature distribution remains $N(0,I_{d})$ The full analysis of RA\u221aG is provides in Appendix B.3. Specifically, when we carry out the analysis by assuming $\\alpha=O\\left(1/\\sqrt{d}\\right)$ and $d/n=O(1)$ where $O\\left(\\cdot\\right)$ denotes proportionality, our derivation leads to the following result: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 Consider linear model as defined in (9). Recap the objective from (5a) and let $W_{\\star}:=$ arg minW $\\mathcal{L}_{P G D}(W)$ , and $\\begin{array}{r}{\\mathcal{L}_{\\star}=\\mathcal{L}_{P G D}(W_{\\star})}\\end{array}$ . Additionally, let $\\kappa=\\alpha^{2}d+1$ and suppose $\\alpha=O{\\left(1/\\sqrt{d}\\right)},$ , $d/n=O(1)$ and $d$ is sufficiently large. Then $W_{\\star}$ and $\\mathcal{L}_{\\star}$ have approximate forms ", "page_idx": 6}, {"type": "equation", "text": "$$\nW_{\\star}\\approx\\frac{1}{\\kappa n+d+\\sigma^{2}}I_{d}\\qquad a n d\\qquad\\mathcal{L}_{\\star}\\approx d+\\sigma^{2}-\\frac{\\kappa n d}{\\kappa n+d+\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, (10) is reminiscent of Corollary 1 and has a surprisingly clean message. Observe that, $\\alpha^{2}d+1$ is the dominant multiplier ahead of $n$ in both equations. Thus, we deduce that, RAG model follows the same error bound as the independent data model, however, its sample size is amplified by a\u221a factor of $\\alpha^{2}d+1$ . $\\alpha\\,=\\,0$ reduces to the result of Corollary 1 whereas we need to set $\\bar{\\alpha}=O\\bar{\\left(1/\\sqrt{d}\\right)}$ for constant amplification. When $\\alpha=1$ , RAG achieves the approximate risk $\\mathcal{L}_{\\star}\\approx2+\\sigma^{2}$ , where the constant bias is due to the higher order moments (e.g., the 4\u2019th and 6\u2019th moments) of the standard Gaussian distribution. As $d$ increases, the normalized loss $\\mathcal{L}_{\\star}/d\\rightarrow0$ . The full analysis of its optimal solution $W_{\\star}$ and loss $\\mathcal{L}_{\\star}$ are deferred Theorem 4 in Appendix B.3. ", "page_idx": 6}, {"type": "text", "text": "Task-feature alignment. We also consider another dependent data setting where task and feature vectors are assumed to be correlated. This dataset model has the following motivation: In general, an LLM can generate any token within the vocabulary. However, once we specify the task (e.g. domain of the prompt), the LLM output becomes more deterministic and there are much fewer token candidates. For instance, if the task is \u201cCountry\u201d, \u201cFrance\u201d is a viable output compared to \u201cHelium\u201d and vice versa when the task is \u201cChemistry\u201d. Formally speaking, this can be formalized as the input $\\pmb{x}$ having a diverse distribution whereas it becomes more predictable conditioned on $_\\beta$ . Therefore, it can be captured through a linear model by making the conditional covariance of $x\\left\\vert\\beta\\right\\vert$ to be approximately low-rank. This formalism can be viewed as a spectral alignment between input and task, which is also well-established in deep learning both empirically and theoretically [Li et al., 2020, Arora et al., 2019, Canatar et al., 2021, Cao et al., 2019]. Here, we consider such a setting where the shared task vector is sampled as standard Gaussian distribution $\\pmb{\\beta}\\sim N(0,\\pmb{I}_{d})$ and letting $\\bar{\\kappa}=\\alpha^{2}d+1$ , we sample the $\\alpha$ -correlated ICL demonstrations $(x_{i},y_{i})_{i=1}^{n+1}$ as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nx_{i}\\,{\\Big|}\\,\\beta\\sim N(\\alpha\\beta,I_{d}),\\quad\\xi_{i}\\sim N(0,\\sigma^{2})\\quad{\\mathrm{and}}\\quad y_{i}=\\kappa^{-1/2}{\\pmb x}_{i}^{\\top}\\beta+\\xi_{i},\\quad1\\leq i\\leq n+1.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Above, $\\kappa^{-1/2}$ is a normalization factor to ensure that label variance remains invariant to $\\alpha$ . To keep the exposition cleaner, we defer the full analysis of its optimal solution\u221a $W_{\\star}$ and loss $\\mathcal{L}_{\\star}$ to Theorem 5 in Appendix B.4. Similar to the RAG setting, by assuming $\\alpha=O\\left(1/\\sqrt{d}\\right)$ and $d/n=O(1)$ , we obtain the following results for the optimal parameter and risk. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 Consider linear model as defined in (11). Recap the objective from (5a) and let $W_{\\star}:=$ arg minW $\\mathcal{L}_{P G D}(W)$ , and $\\begin{array}{r}{\\mathcal{L}_{\\star}=\\mathcal{L}_{P G D}(W_{\\star})}\\end{array}$ . Additionally, given $\\kappa=\\alpha^{2}d+1$ and suppose $\\alpha=O{\\left(1/\\sqrt{d}\\right)},$ , $d/n=O(1)$ and d is sufficiently large. Then $W_{\\star}$ and $\\mathcal{L}_{\\star}$ have approximate forms ", "page_idx": 6}, {"type": "equation", "text": "$$\nW_{\\star}\\approx\\frac{1}{\\kappa n+(d+\\sigma^{2})/\\kappa}I_{d}\\qquad a n d\\qquad\\mathcal{L}_{\\star}\\approx d+\\sigma^{2}-\\frac{\\kappa n d}{\\kappa n+(d+\\sigma^{2})/\\kappa}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similar to (10), (12) contains $\\kappa=\\alpha^{2}+1$ multiplier ahead of $n$ , which reduces the in-context sample complexity and setting $\\alpha=0$ reduces to the results of Corollary 1. ", "page_idx": 6}, {"type": "text", "text": "3.2 Low-rank Parameterization and LoRA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we investigate training low-rank models, which assume ${\\pmb{W}}_{k},{\\pmb{W}}_{q}\\in\\mathbb{R}^{(d+1)\\times r}$ where $r$ is the rank restriction. Equivalently, we consider objective (5a) under condition rank $\\mathbf{(}\\mathbf{W})=r$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 3 Consider independent linear data as defined in (7). Recap the objective from (5a) and enforce rank $(\\boldsymbol{W})\\le r$ and $W^{\\top}=W$ . Let $\\pmb{\\Sigma}=\\pmb{\\Sigma}_{x}^{1/2}\\pmb{\\Sigma}_{\\beta}\\dot{\\pmb{\\Sigma}}_{x}^{1/2}$ and $M=\\tau r(\\Sigma)+\\sigma^{2}$ . Denoting $\\lambda_{i}$ to be the i\u2019th largest eigenvalue of $\\pmb{\\Sigma}$ , we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r a n k(W)\\leq r,W=W^{\\top}}\\mathcal{L}(W)=M-\\sum_{i=1}^{r}\\frac{n\\lambda_{i}^{2}}{(n+1)\\lambda_{i}+M}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "lYPAYmfQqm/tmp/6ffdde67ad1a7de7bc3cc7881da368513c9d19f201925e5acd3173bfac7b5f74.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Empirical evidence validates Theorem 1 and Proposition 1. We train 1-layer linear attention and H3 models with prompts containing independent demonstrations following a linear model, and dotted curves are the theory curves following Eq. (8). (a): We consider noiseless i.i.d. setting where $\\Sigma_{x}=\\Sigma_{\\beta}=I_{d}$ and $\\sigma=0$ , with results presented in red (attention) and blue (H3) solid curves. (b): We conduct noisy label experiments by choosing $\\sigma\\ne0$ . (c): Consider non-isotropic task by setting $\\Sigma_{\\beta}=\\gamma\\mathbf{1}\\mathbf{1}^{\\top}+(1-\\gamma)I_{d}$ . Solid and dashed curves in (b) and (c) represent attention and H3 results, respectively. The alignments in (a), (b) and (c) show the equivalence between attention and H3, validating Theorem 1 and Proposition 1. More experimental details are discussed in Section 4. ", "page_idx": 7}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\mathbf{tr}\\left(\\Sigma\\right)=\\sum_{i=1}^{d}\\lambda_{i}}\\end{array}$ . Removing the rank constraint and considering noiseless data setting, this reduces to the following optimal risk L\u22c6=  id=1n+\u03bb1i++MM/\u03bbi . S ee Appendix C.1 for more details. ", "page_idx": 7}, {"type": "text", "text": "Impact of LoRA: Based on the above lemma, we consider the impact of LoRA for adapting the pretrained model to a new task distribution under jointly-diagonalizable old and new eigenvalues of $\\pmb{\\Sigma}$ , $\\Sigma^{n e w}$ , $(\\lambda_{i})_{i=1}^{d}$ , $(\\lambda_{i}^{n e w})_{i=1}^{d}$ . Consider adapting LoRA matrix to the combined key and value weights in attention, which reflects minimizing the population loss $\\Tilde{\\mathcal{L}}(W_{l o r a}):=\\mathcal{L}(W+W_{l o r a})$ in (5a) with fixed W. Suppose $\\mathbf{tr}\\left(\\Sigma\\right)=\\mathbf{tr}\\left(\\Sigma^{n e w}\\right)=M$ , $\\sigma=0$ and $W$ is jointly diagonalizable with $\\boldsymbol\\Sigma$ , $\\Sigma^{n e w}$ , then LoRA\u2019s risk is upper-bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\operatorname{rank}(W_{l o r a})\\le r}\\tilde{\\mathcal{L}}(W_{l o r a})\\le\\operatorname*{min}_{|T|\\le r,T\\subset[d]}\\left(\\sum_{i\\notin T}\\frac{\\lambda_{i}+M}{n+1+M/\\lambda_{i}}+\\sum_{i\\in T}\\frac{\\lambda_{i}^{n e w}+M}{n+1+M/\\lambda_{i}^{n e w}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that, the right hand side is provided assuming the optimal LoRA-updated model $W_{l o r a}$ is also jointly diagonalizable with covariances $\\pmb{\\Sigma}$ , $\\Sigma^{n e w}$ , and $W$ . ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now conduct synthetic experiments to support our theoretical findings and further explore the behavior of different models of interest under different conditions. The experiments are designed to investigate various scenarios, including independent data, retrieval-augmented generation (RAG), task-feature alignment, low-rank parameterization, and LoRA adaption. ", "page_idx": 7}, {"type": "text", "text": "Experimental setting. We train 1-layer attention and H3 models for solving the linear regression ICL. As described in Section 2, we consider meta-learning setting where task parameter $_\\beta$ is randomly generated for each training sequence. In all experiments, we set the dimension $d=20$ . Depending on the in-context length $(n)$ , different models are trained to make in-context predictions. We train each model for 10000 iterations with batch size 128 and Adam optimizer with learning rate $10^{-3}$ . Since our study focuses on the optimization landscape, and experiments are implemented via gradient descent, we repeat 20 model trainings from different initialization and results are presented as the minimal test risk among those 20 trails. In all the plots, theoretical predictions are obtained via the corresponding formulae presented in Section 3 and the test risks are normalized by the dimension $d$ . ", "page_idx": 7}, {"type": "text", "text": "\u2022 Equivalence among $\\mathcal{L}_{\\mathtt{P G D}}^{\\star}$ , $\\mathcal{L}_{\\mathtt{A T T}}^{\\star}$ and $\\mathcal{L}_{\\mathtt{S S M}}^{\\star}$ (Figure 2). To verify Proposition 1 as well as Theorem 1, we run random linear regression instances where in-context samples are generated obeying (7). Fig. 2a is identical to Fig. 1a where we set $\\Sigma_{x}=\\Sigma_{\\beta}=I_{d}$ and $\\sigma=0$ . In Fig. 2b, set $\\Sigma_{x}=\\Sigma_{\\beta}=I$ and vary noise level $\\sigma^{2}$ from 0 to $0.3\\times d$ . In Fig. 2c, we consider noiseless labels, $\\sigma=0$ , isotropic feature distribution $\\Sigma_{x}=I_{d}$ and set task covariance to be $\\Sigma_{\\beta}=\\gamma\\mathbf{1}\\mathbf{1}^{\\top}+(1-\\gamma)I_{d}$ by choosing $\\gamma$ in $\\{0,0.3,0.6,0.9\\}$ . Note that in Fig. 2c, we train a sufficient number of models (greater than 20) to ensure the optimal model is obtained. In all the figures, solid and dashed curves correspond to the ", "page_idx": 7}, {"type": "image", "img_path": "lYPAYmfQqm/tmp/b0bebdca9f98eeec04a13052cccafe8be906cf93db555315c49e6619fe60e389.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Distributional alignment and low-rank parameterization experiments. (a) and ${\\bf(b)}$ show the ICL results using data generated via (9) and (11), respectively, by changing $\\alpha$ from 0 to 0.6. In (c), we train low-rank linear attention models by setting $W_{k}$ , $\\bar{W_{q}}\\in\\bar{\\mathbb{R}}^{(d+1)\\times r}$ and in (d), we apply the low-rank LoRA adaptor, $W_{l o r a}:=W_{\\mathrm{up}}W_{\\mathrm{down}}^{\\top}$ where $W_{\\mathrm{up}}$ , $W_{\\mathrm{down}}\\in\\mathbb{R}^{(d+1)\\times r}$ , to pretrained linear attention models and adjust the LoRA parameters under different task distribution. Solid and dotted curves correspond to the linear attention and theoretical results (c.f. Section 3), respectively, and the alignments validate our theorems in Section 3. More experimental details are discussed in Section 4. ", "page_idx": 8}, {"type": "text", "text": "ICL results from training 1-layer ATT and SSM models, respectively, and dotted curves are obtained from (8) in Theorem 1. The alignment of solid, dashed and dotted curves validates our Proposition 1 and Theorem 1. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Distributional alignment experiments (Figs. 3a&3b). In Figs. 3a and 3b, we generate RAG and task-feature alignment data following (9) and (11), respectively, by setting $\\sigma=0$ and varying $\\alpha$ from 0 to 0.6. Attention training results are displayed in solid curves, and we generate theory curve (dotted) via the $\\mathcal{L}_{\\star}$ formula as described in (36) in Appendix B.3 and (42) in Appendix B.4. The empirical alignments corroborate Theorems 4 and 5, further confirming that Proposition 1 is applicable to a broader range of real-world distributional alignment data. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Low-rank (Fig. 3c) and LoRA (Fig. 3d) experiments. We also run simulations to verify our theoretical findings in Section 3.2. Consider the independent data setting as described in (7). In Fig. 3c, we set $\\Sigma_{x}=I_{d}$ , $\\sigma=0$ and task covariance to be diagonal with diagonal entries $c[1~2^{-1}~\\cdots~d^{-1}]^{\\breve{\\top}}$ for some normalization constant $c=d/\\textstyle\\sum_{i=1}^{d}i^{-1}$ , and parameterize the attention model using matrices $\\pmb{W_{k}},\\pmb{W_{q}}\\in\\mathbb{R}^{(d+1)\\times r}$ and vary $r$ across the set {1, 5, 10, 20}. Results show that empirical (solid) and theoretical (dotted, c.f. (13)) curves overlap. In Fig. 3d, we implement two phases of training. Phase 1: Setting $\\Sigma_{x}\\,=\\,\\Sigma_{\\beta}\\,=\\,I_{d}$ and $\\sigma=0$ , we pretrain the model with full rank parameters and obtain weights $\\hat{\\pmb{W}}_{k},\\hat{\\pmb{W}}_{q},\\hat{\\pmb{W}}_{\\nu}\\in\\mathbb{R}^{(d+1)\\times(d+1)}$ . Phase 2: We generate new examples with task covariance $\\Sigma_{\\beta}$ being a diagonal matrix with diagonal entries $c^{\\prime}[2^{-1}\\;2^{-2}\\;\\cdots\\;2^{-d}]^{\\top}$ for some normalization constant $\\begin{array}{r}{c^{\\prime}=\\overline{{d}}/\\sum_{i=1}^{d^{-}}2^{-i}}\\end{array}$ . Given the rank restriction $r$ , we train additional LoRA parameters $W_{\\mathrm{up}}$ $_{\\mathrm{1p},}W_{\\mathrm{down}}\\in$ $\\mathbb{R}^{(d+1)\\times r}$ where $W_{l o r a}\\,:=\\,W_{\\mathrm{up}}W_{\\mathrm{down}}^{\\top}$ and (2a) becomes $\\mathsf{A T T}(\\mathbf{Z})\\,=\\,(\\mathbf{Z}(\\hat{W}_{q}\\hat{W}_{k}^{\\top}\\,+\\,W_{\\mathrm{up}}W_{\\mathrm{down}}^{\\top})\\mathbf{Z}^{\\top})\\mathbf{Z}\\hat{W}_{\\nu}$ Fig. 3d presents the results after two phases of training where dotted curves are drawn from the right hand side of (14) directly. Here, note that since $\\Sigma,\\Sigma^{n e w}$ are diagonal, the right hand side of (14) returns the exact optimal risk of LoRA and the alignments verify it. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "There is growing interest in understanding the mechanisms behind ICL [Brown et al., 2020, Liu et al., 2023b, Rae et al., 2021] in LLMs due to its success in continuously enabling novel applications for LLMs [GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023]. In the previous work, Garg et al. [2022] explored ICL ability of Transformers. In particular, they considered in-context prompts where each in-context example is labeled by a target function from a given function class, including linear models. A number of works have studied this and related settings to develop a theoretical understanding of ICL [von Oswald et al., 2023, Gatmiry et al., Collins et al., 2024, Lin and Lee, 2024, Li et al., 2024, Bai et al., 2024, Aky\u00fcrek et al., 2023, Zhang et al., 2023, Du et al., 2023]. Aky\u00fcrek et al. [2023] focus on linear regression and provide a construction of Transformer weights that can enable a single step of GD based on in-context examples. Along the similar line, Von Oswald et al. [2023] provide a construction of weights in linear attention-only Transformers that can emulate GD steps on in-context examples for a linear regression task. Similar to this line of work, Dai et al. [2023] argue that pre-trained language models act as meta-optimizer which utilize attention to apply meta-gradients to the original language model based on the in-context examples. ", "page_idx": 8}, {"type": "text", "text": "Building on these primarily empirical studies, Zhang et al. [2024], Mahankali et al. [2024], Ahn et al. [2023], Duraisamy [2024] focus on developing a theoretical understanding of Transformers trained to perform ICL. For single-layer linear attention model trained on independent in-context prompts for random linear regression tasks, Mahankali et al. [2024], Ahn et al. [2023] show that the resulting model implements a single step of PGD on in-context examples in a test prompt, thereby corroborating the findings of [Von Oswald et al., 2023]. Zhang et al. [2024] study the optimization dynamics of gradient flow while training a single-layer linear attention model on in-context prompts for random linear regression tasks. Similar to Mahankali et al. [2024], Ahn et al. [2023], they show that the trained model implements a single step of GD and PGD for isotropic and anisotropic Gaussian features, respectively. In addition, they also characterize the test-time prediction error for the trained model while highlighting its dependence on train and test prompt lengths. ", "page_idx": 9}, {"type": "text", "text": "While our work shares similarities with this line of works, as discussed in our contributions in the introduction, we expand the theoretical understanding of ICL along multiple novel dimensions, which includes the first study of LoRA adaptation for ICL in the presence of a distributional shift. Furthermore, we strive to capture the effect of retrieval augmentation [Lewis et al., 2020, Nakano et al., 2021] on ICL through our analysis. Retrieval augmentation allows for selecting most relevant demonstration out of a large collection for a test instance, e.g., via a dense retrieval model [Izacard et al., 2023], which can significantly outperform the typical ICL setup where fixed task-specific demonstrations are provided as in-context examples [Wang et al., 2022, Basu et al., 2023]. Through a careful modeling of retrieval augmentation via correlated design, we show that it indeed has a desirable amplification effect where the effective number in-context examples becomes larger with higher correlation which corresponds to preforming a successful retrieval of query-relevant demonstrations in a practical retrieval augmented setup. ", "page_idx": 9}, {"type": "text", "text": "Recently, state space models (SSMs) [Gu et al., 2021b,a, Fu et al., 2023, Gu and Dao, 2023] have appeared as potential alternatives to Transformer architecture, with more efficient scaling to input sequence length. Recent studies demonstrate that such SSMs can also perform ICL for simple non-language tasks [Park et al., 2024, Grazzi et al., 2024] as well as complex NLP tasks [Grazzi et al., 2024]. That said, a rigorous theoretical understanding of ICL for SSMs akin to Zhang et al. [2024], Mahankali et al. [2024], Ahn et al. [2023] is missing from the literature. In this work, we provide the first such theoretical treatment for ICL with SSMs. Focusing on H3 architecture [Fu et al., 2023], we highlight its advantages over linear attention in specific ICL settings. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we revisited the loss landscape of in-context learning with 1-layer sequence models. We have established a general connection between ICL and gradient methods that accounts for correlated data, non-attention architectures (specifically SSMs), and the impact of low-rank parameterization including LoRA adaptation. Our results elucidate two central findings: (1) The functions learned by different sequence model architectures exhibit a strong degree of universality and (ii) Dataset and prompt design, such as RAG, can substantially benefit ICL performance. ", "page_idx": 9}, {"type": "text", "text": "Future directions and limitations. The results of this work fall short of being a comprehensive theory for ICL in LLMs and can be augmented in multiple directions. First, while the exact equivalence between H3 and linear attention is remarkable, we should examine whether it extends to other SSMs. Secondly, while empirically predictive, our RAG and LoRA analyses are not precise and fully formal. Thirdly, it is desirable to develop a deeper understanding of multilayer architectures and connect to iterative GD methods as in [Ahn et al., 2023, Von Oswald et al., 2023]. Finally, we have studied the population risk of ICL training whereas one can also explore the sample complexity of pretraining [Wu et al., 2023, Lu et al., 2024]. Moving beyond the theoretically tractable setup of this work, our simplified models are trained on in-context prompts from random initialization. Therefore, this theoretical study doesn\u2019t address more challenging in-context learning tasks, such as question answering, where both in-context demonstration and general knowledge from pretraining are required. Future work in this area could also shed light on how certain contexts might elicit undesirable behaviors acquired by an LLM during pretraining, an aspect not covered in our current analysis. This work also studies a theoretical model for retrieval augmentation-based ICL. In a real-life retrieval augmentation-based ICL, one needs to account for the quality of the collection of the retrievable demonstrations and its (negative) impacts on the final predictions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by the National Science Foundation grants CCF-2046816, CCF2403075, the Office of Naval Research award N000142412289, an Adobe Data Science Research award, and a gift by Google Research. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36, 2023. ", "page_idx": 10}, {"type": "text", "text": "Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= 0g0X4H8yN4I.   \nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pages 322\u2013332. PMLR, 2019.   \nYu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. Advances in neural information processing systems, 36, 2024.   \nSoumya Basu, Ankit Singh Rawat, and Manzil Zaheer. A statistical perspective on retrieval-based models. In International Conference on Machine Learning, pages 1852\u20131886. PMLR, 2023.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nAbdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. Nature communications, 12(1):2914, 2021.   \nYuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.   \nLiam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, and Sanjay Shakkottai. In-context learning with transformers: Softmax attention adapts to function lipschitzness. arXiv preprint arXiv:2402.11639, 2024.   \nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 4005\u20134019, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.247. URL https://aclanthology.org/2023.findings-acl.247.   \nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933\u2013941. PMLR, 2017.   \nZhe Du, Haldun Balim, Samet Oymak, and Necmiye Ozay. Can transformers learn optimal flitering for unknown systems? IEEE Control Systems Letters, 7:3525\u20133530, 2023.   \nKarthik Duraisamy. Finite sample analysis and bounds of generalization error of gradient descent in in-context linear regression. arXiv preprint arXiv:2405.02462, 2024.   \nDaniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id $\\equiv$ COZDy0WYGg.   \nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022.   \nKhashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv Kumar. Can looped transformers learn to implement multi-step gradient descent for in-context learning? In Forty-first International Conference on Machine Learning.   \nGeminiTeam, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \nRiccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, and Frank Hutter. Is mamba capable of in-context learning? arXiv preprint arXiv:2402.03170, 2024.   \nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \nAlbert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021a.   \nAlbert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572\u2013585, 2021b.   \nChi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models explained as kernel regression. arXiv preprint arXiv:2305.12766, 2023.   \nNoah Hollmann, Samuel M\u00fcller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer that solves small tabular classification problems in a second. arXiv preprint arXiv:2207.01848, 2022.   \nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= nZeVKeeFYf9.   \nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):1\u201343, 2023.   \nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156\u20135165. PMLR, 2020.   \nIvan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick. Exploring the relationship between model architecture and in-context learning ability. arXiv preprint arXiv:2310.08049, 2023.   \nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 9459\u20139474, 2020.   \nMingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In International conference on artificial intelligence and statistics, pages 4313\u20134324. PMLR, 2020.   \nYingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565\u201319594. PMLR, 2023.   \nYingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and Samet Oymak. Dissecting chain-of-thought: Compositionality through in-context flitering and learning. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Ziqian Lin and Kangwook Lee. Dual operating modes of in-context learning. arXiv preprint arXiv:2402.18819, 2024. ", "page_idx": 12}, {"type": "text", "text": "Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id=De4FYqjFueZ. ", "page_idx": 12}, {"type": "text", "text": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023b. ", "page_idx": 12}, {"type": "text", "text": "Yue Lu, Mary I. Letey, Jacob A. Zavatone-Veth, Anindita Maiti, and Cengiz Pehlevan. Asymptotic theory of in-context learning by linear attention. arXiv preprint arXiv:2310.08391, 2024. ", "page_idx": 12}, {"type": "text", "text": "Arvind V. Mahankali, Tatsunori Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= 8p3fu56lKc. ", "page_idx": 12}, {"type": "text", "text": "Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. Revisiting the equivalence of in-context learning and gradient descent: The impact of data distribution. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7410\u20137414. IEEE, 2024. ", "page_idx": 12}, {"type": "text", "text": "Samuel M\u00fcller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. arXiv preprint arXiv:2112.10510, 2021. ", "page_idx": 12}, {"type": "text", "text": "Samuel M\u00fcller, Matthias Feurer, Noah Hollmann, and Frank Hutter. Pfns4bo: In-context learning for bayesian optimization. In International Conference on Machine Learning, pages 25444\u201325470. PMLR, 2023. ", "page_idx": 12}, {"type": "text", "text": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. ", "page_idx": 12}, {"type": "text", "text": "Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. ", "page_idx": 12}, {"type": "text", "text": "OpenAI. Gpt-4 technical report. arXiv preprintarXiv:2303.08774, 2023. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context learning tasks. International Conference on Machine Learning, 2024. ", "page_idx": 12}, {"type": "text", "text": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. ", "page_idx": 12}, {"type": "text", "text": "Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pages 9355\u20139366. PMLR, 2021. ", "page_idx": 12}, {"type": "text", "text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. ", "page_idx": 12}, {"type": "text", "text": "Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174. PMLR, 2023. ", "page_idx": 12}, {"type": "text", "text": "Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858, 2023. ", "page_idx": 12}, {"type": "text", "text": "Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. arXiv preprint arXiv:2203.08773, 2022.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \nJingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? arXiv preprint arXiv:2310.08391, 2023.   \nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI.   \nRuiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.   \nRuiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. Journal of Machine Learning Research, 25(49):1\u201355, 2024.   \nNicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes Von Oswald, Maxime Larcher, Angelika Steger, and Joao Sacramento. Gated recurrent neural networks discover attention. arXiv preprint arXiv:2309.01775, 2023. ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Proposition 1 . 16   \nA.2 Proof of Lemma 1 20   \nA.3 Proof of Lemma 2 21   \nAnalysis of General Data Distribution 21   \nB.1 Supporting Results 22   \nB.2 Independent Data with General Covariance 24   \nB.3 Retrieval Augmented Generation with $\\alpha$ Correlation 25   \nB.4 Task-feature Alignment with $\\alpha$ Correlation . . 28   \nC Analysis of Low-Rank Parameterization 31   \nC.1 Proof of Lemma 3 . 31 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "D Additional Experiments 32 ", "page_idx": 14}, {"type": "text", "text": "E Extended Related Work 33 ", "page_idx": 14}, {"type": "text", "text": "A Equivalence among Gradient Descent, Attention, and State-Space Models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present the proofs related to Section 2. Recap that given data ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X=[{\\pmb x}_{1}\\,\\cdots\\,{\\pmb x}_{n}]^{\\top}\\in\\mathbb{R}^{n\\times d},}\\\\ &{\\pmb{\\xi}=[{\\pmb\\xi}_{1}\\,\\cdots\\,{\\pmb\\xi}_{n}]^{\\top}\\in\\mathbb{R}^{n},}\\\\ &{{\\pmb y}=[{\\pmb y}_{1}\\,\\cdots\\,{\\pmb y}_{n}]^{\\top}=X{\\pmb\\beta}+{\\pmb\\xi}\\in\\mathbb{R}^{n},}\\\\ &{{\\pmb Z}_{0}=[z_{1}\\,\\dots\\,z_{n}\\,\\pmb{\\mathbf0}_{d+1}]^{\\top}=\\left[\\pmb{x}_{1}\\,\\dots\\,\\,\\,\\,\\pmb{x}_{n}\\quad\\pmb{\\mathbf0}_{d}\\right]^{\\top}\\in\\mathbb{R}^{(n+1)\\times(d+1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and corresponding prediction functions ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{\\mathtt{P G D}}(Z)=x^{\\top}W X^{\\top}y,}\\\\ &{g_{\\mathtt{M P G D}}(Z)=x^{\\top}W X^{\\top}(\\omega\\odot y),}\\\\ &{g_{\\mathtt{A T T}}(Z)=(z^{\\top}W_{q}W_{k}^{\\top}Z_{0}^{\\top})Z_{0}W_{\\nu}\\nu,}\\\\ &{g_{\\mathtt{S S M}}(Z)=\\left((z^{\\top}W_{q})^{\\top}\\odot((Z_{0}W_{k}\\odot Z_{0}W_{\\nu})*f)_{n+1}\\right)\\nu,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we have objectives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{W}{\\operatorname*{min}}\\mathcal{L}_{\\mathtt{P G D}}(\\mathcal{W})}&{\\mathrm{where}\\quad\\mathcal{L}_{\\mathtt{P G D}}(\\mathcal{W})=\\mathbb{E}\\left[(y-g_{\\mathtt{P G D}}(\\pmb{Z}))^{2}\\right],}\\\\ {\\underset{W,\\omega}{\\operatorname*{min}}\\mathcal{L}_{\\mathtt{W G D}}(\\mathcal{W})}&{\\mathrm{where}\\quad\\mathcal{L}_{\\mathtt{M P G D}}(\\mathcal{W})=\\mathbb{E}\\left[(y-g_{\\mathtt{M P G D}}(\\pmb{Z}))^{2}\\right],}\\\\ {\\underset{W_{k},W_{q},W_{v},p}{\\operatorname*{min}}\\mathcal{L}_{\\mathtt{A T I}}(\\mathcal{W})}&{\\mathrm{where}\\quad\\mathcal{L}_{\\mathtt{A T T}}(\\mathcal{W})=\\mathbb{E}\\left[(y-g_{\\mathtt{A T T}}(\\pmb{Z}))^{2}\\right],}\\\\ {\\underset{W_{k},W_{q},W_{v},p}{\\operatorname*{min}}\\mathcal{L}_{\\mathtt{S M}}(\\mathcal{W})}&{\\mathrm{where}\\quad\\mathcal{L}_{\\mathtt{S S M}}(\\mathcal{W})=\\mathbb{E}\\left[(y-g_{\\mathtt{S S M}}(\\pmb{Z}))^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, the expectation is over the randomness in $(\\pmb{x}_{i},\\pmb{\\xi}_{i})_{i=1}^{n}$ and $_\\beta$ , and the search space for $W$ is $\\mathbb{R}^{d\\times d}$ , for $\\omega$ is ${\\mathbb{R}}^{n}$ , for ${\\cal W}_{k},{\\cal W}_{q},{\\cal W}_{\\nu}$ is $\\mathbb{R}^{(d+1)\\times(d+1)}$ , for $\\pmb{\\nu}$ is $\\mathbb{R}^{d+1}$ , and for $\\boldsymbol{f}$ is $\\mathbb{R}^{n+1}$ . ", "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Consider the problem setting as discussed in Section 2, Proposition 1 can be proven by the following two lemmas. ", "page_idx": 15}, {"type": "text", "text": "Lemma 4 Suppose Assumptions $^{\\,l}$ and 2 hold. Then, given the objectives (16a) and (16c), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W_{q},W_{k},W_{\\nu},\\nu}\\mathcal{L}_{A T T}(\\mathcal{W})=\\operatorname*{min}_{W}\\mathcal{L}_{P G D}(\\mathcal{W}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Recap the linear attention estimator from (15c) and denote ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{q}W_{k}^{\\top}=\\left[\\bar{W}\\quad w_{1}\\right]\\qquad\\mathrm{and}\\qquad W_{\\nu}\\nu=\\left[\\nu_{1}\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\bar{\\pmb{W}}\\in\\mathbb{R}^{d\\times d},\\,\\pmb{w}_{1},\\pmb{w}_{2},\\pmb{\\nu}_{1}\\in\\mathbb{R}^{d}$ , and $w,\\nu\\in\\mathbb{R}$ . Then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{\\mathrm{ATT}}(\\boldsymbol{Z})=(z^{\\top}W_{q}W_{k}^{\\top}\\boldsymbol{Z}_{0}^{\\top})Z_{0}W_{\\nu}\\boldsymbol{\\nu}}\\\\ &{\\qquad\\qquad=[x^{\\top}\\boldsymbol{0}]\\left[\\begin{array}{c c}{\\hat{W}}&{w_{1}}\\\\ {w_{2}^{\\top}}&{w_{3}}\\end{array}\\right]\\left[\\boldsymbol{X}^{\\top}\\begin{array}{c c}{\\boldsymbol{\\mathbf{\\Sigma}}}&{\\boldsymbol{\\mathbf{0}}_{d}}\\\\ {0}&{0}\\end{array}\\right]\\left[\\boldsymbol{\\mathbf{\\Sigma}}_{\\nu}^{\\top}\\right]}\\\\ &{\\qquad\\quad=(x^{\\top}\\bar{\\boldsymbol{W}}\\boldsymbol{X}^{\\top}+x^{\\top}w_{1}y^{\\top})(X\\nu_{1}+y\\nu)}\\\\ &{\\qquad\\quad=x^{\\top}(\\nu\\bar{\\boldsymbol{W}}\\boldsymbol{X}^{\\top}\\boldsymbol{\\mathbf{y}}+x^{\\top}w_{1}y^{\\top}\\boldsymbol{X}\\nu_{1}+x^{\\top}\\left(\\bar{\\boldsymbol{W}}\\boldsymbol{X}^{\\top}\\boldsymbol{X}\\nu_{1}+\\nu\\left\\|y\\right\\|_{\\ell_{2}}^{2},\\boldsymbol{\\boldsymbol{\\mathbf{\\Sigma}}}\\nu_{1}\\right)}\\\\ &{\\qquad\\quad=x^{\\top}(\\nu\\bar{\\boldsymbol{W}}+w_{1}\\nu_{1}^{\\top})\\boldsymbol{X}^{\\top}\\boldsymbol{\\mathbf{y}}+x^{\\top}\\left(\\bar{\\boldsymbol{W}}\\boldsymbol{X}^{\\top}\\boldsymbol{X}\\nu_{1}+\\nu\\left\\|y\\right\\|_{\\ell_{2}}^{2},w_{1}\\right)}\\\\ &{\\qquad\\quad=\\underline{{x}}^{\\top}\\bar{\\boldsymbol{W}}\\boldsymbol{X}^{\\top}\\boldsymbol{\\mathbf{y}}+\\underbrace{x^{\\top}\\left(\\bar{\\boldsymbol{W}}\\boldsymbol{X}^{\\top}\\boldsymbol{X}\\nu_{1}+\\nu\\left\\|y\\right\\|_{\\ell_{2}}^{2},w_{1}\\right)}_{\\bar{\\boldsymbol{\\mathbf{G}}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\tilde{\\pmb{W}}:=\\nu\\pmb{\\bar{W}}+\\pmb{w}_{1}\\pmb{\\nu}_{1}^{\\top}$ ", "page_idx": 15}, {"type": "text", "text": "We first show that for any given parameters ${\\pmb W}_{k},{\\pmb W}_{q},{\\pmb W}_{\\nu},{\\pmb\\nu}_{:}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left(g_{\\mathrm{ATT}}(Z)-y)^{2}\\right]\\geq\\mathbb{E}\\left[\\left(\\tilde{g}_{\\mathrm{ATT}}(Z)-y)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To this goal, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(g_{\\mathrm{ATT}}(\\pmb{Z})-y)^{2}\\right]-\\mathbb{E}\\left[(\\tilde{g}_{\\mathrm{ATT}}(\\pmb{Z})-y)^{2}\\right]=\\mathbb{E}\\left[(\\tilde{g}_{\\mathrm{ATT}}(\\pmb{Z})+\\varepsilon-y)^{2}\\right]-\\mathbb{E}\\left[(\\tilde{g}_{\\mathrm{ATT}}(\\pmb{Z})-y)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}[\\varepsilon^{2}]+2\\,\\mathbb{E}[(\\tilde{g}_{\\mathrm{ATT}}(\\pmb{Z})-y)\\varepsilon]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we have decomposition ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\tilde{g}_{\\mathrm{AT}}(Z)-y\\right)\\varepsilon=(x^{\\top}\\tilde{W}X^{\\top}y-y)x^{\\top}\\left(\\bar{W}X^{\\top}X\\nu_{1}+\\nu\\left\\Vert y\\right\\Vert_{\\ell_{2}}^{2}\\ w_{1}\\right)}\\\\ &{\\qquad\\qquad\\qquad=y^{\\top}X\\tilde{W}^{\\top}x x^{\\top}\\left(\\bar{W}X^{\\top}X\\nu_{1}+\\nu\\left\\Vert y\\right\\Vert_{\\ell_{2}}^{2}w_{1}\\right)-y x^{\\top}\\left(\\bar{W}X^{\\top}X\\nu_{1}+\\nu\\left\\Vert y\\right\\Vert_{\\ell_{2}}^{2}w_{1}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\underbrace{y^{\\top}X\\tilde{W}^{\\top}x x^{\\top}\\bar{W}X^{\\top}X\\nu_{1}}_{(a)}+\\underbrace{\\nu\\left\\Vert y\\right\\Vert_{\\ell_{2}}^{2}y^{\\top}X\\tilde{W}^{\\top}x x^{\\top}w_{1}}_{(b)}-\\underbrace{y x^{\\top}\\bar{W}X^{\\top}X\\nu_{1}}_{(c)}-\\underbrace{\\nu y\\left\\Vert y\\right\\Vert_{\\ell_{2}}^{2}x^{\\top}w_{1}}_{(d)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the following, we consider the expectatio   n   s      o   f $(a),(b),(c),(d)$ sequentially, whic h      r   e tur  n      z   eros under Assumptions 1 and 2. Note that since Assumption 1 holds, expectation of any odd order of monomial of the entries of $X,x,\\beta$ returns zero, i.e., order of $x^{\\top}\\beta x$ is 3 and therefore $\\mathbb{E}[{\\pmb x}^{\\top}{\\pmb\\beta}{\\pmb x}]=\\mathbf{0}_{d}$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(a):\\ }&{\\mathbb{E}\\left[y^{\\top}X\\bar{W}^{\\top}x x^{\\top}\\bar{W}X^{\\top}X y_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[(X\\beta+\\xi)^{\\top}X\\bar{W}^{\\top}x x^{\\top}\\bar{W}X^{\\top}X y^{\\top}X y_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\beta^{\\top}X^{\\top}X\\bar{W}^{\\top}x x^{\\top}\\bar{W}X^{\\top}X y_{1}\\right]+\\mathbb{E}\\left[\\xi^{\\top}X\\bar{W}^{\\top}x x^{\\top}\\bar{W}X^{\\top}X y_{1}\\right]}\\\\ &{=0.}\\\\ {(b):\\ }&{\\mathbb{E}\\left[\\nu\\|y\\|_{\\ell_{2}}^{2}y^{\\top}X\\bar{W}^{\\top}x x^{\\top}w_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\nu(X\\beta+\\xi)^{\\top}(X\\beta+\\xi)(X\\beta+\\xi)^{\\top}X\\bar{W}^{\\top}x x^{\\top}w_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\nu\\|\\xi\\|_{\\ell_{2}}^{2}\\xi^{\\top}X\\bar{W}^{\\top}x x^{\\top}w_{1}\\right]}\\\\ &{=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(c):}&{\\ \\mathbb{E}\\left[y x^{\\top}\\bar{W}X^{\\top}X\\nu_{1}\\right]}\\\\ &{\\ =\\mathbb{E}\\left[(x^{\\top}\\beta+\\xi)x^{\\top}\\bar{W}X^{\\top}X\\nu_{1}\\right]}\\\\ &{\\ =\\mathbb{E}\\left[\\beta^{\\top}x x^{\\top}\\bar{W}X^{\\top}X\\nu_{1}\\right]+\\mathbb{E}\\left[\\xi x^{\\top}\\bar{W}X^{\\top}X\\nu_{1}\\right]}\\\\ &{\\ =0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(d):}&{\\,\\mathbb{E}\\left[\\nu y\\left\\|y\\right\\|_{\\ell_{2}}^{2}x^{\\top}w_{1}\\right]}\\\\ &{\\,=\\nu\\,\\mathbb{E}\\left[(\\beta^{\\top}x+\\xi)(X\\beta+\\xi)^{\\top}(X\\beta+\\xi)x^{\\top}w_{1}\\right]}\\\\ &{\\,=\\nu\\,\\mathbb{E}\\left[\\xi\\left\\|\\xi\\right\\|_{\\ell_{2}}^{2}x^{\\top}w_{1}\\right]}\\\\ &{\\,=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining the results with (19) returns that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(g_{\\mathtt{A T T}}(\\pmb{Z})-y)^{2}\\right]-\\mathbb{E}\\left[\\left(\\tilde{g}_{\\mathtt{A T T}}(\\pmb{Z})-y\\right)^{2}\\right]=\\mathbb{E}[\\varepsilon^{2}]\\geq0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which completes the proof of (18). Therefore, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W_{q},W_{k},W_{\\nu},\\nu}\\mathbb{E}\\left[\\left(g_{\\mathtt{A T T}}(\\mathbf{Z})-y\\right)^{2}\\right]\\geq\\operatorname*{min}_{\\bar{W}}\\mathbb{E}\\left[(\\tilde{g}_{\\mathtt{A T T}}(\\mathbf{Z})-y)^{2}\\right]=\\operatorname*{min}_{W}\\mathbb{E}\\left[\\left(g_{\\mathtt{P C D}}(\\mathbf{Z})-y\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We conclude the proof of this lemma by showing that for any $\\pmb{W}\\ \\in\\ \\mathbb{R}^{d\\times d}$ in $g_{\\mathrm{PGD}}$ , there exist $\\pmb{W_{k}},\\pmb{W_{q}},\\pmb{W_{\\nu}},\\pmb{\\nu}$ such that $g_{\\mathrm{ATT}}(\\mathbf{Z})=g_{\\mathrm{PGD}}(\\mathbf{Z})$ . Let ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{k}=W_{\\nu}=I_{d+1},\\qquad W_{q}=\\left[\\mathbf{0}_{d}^{W}\\quad\\mathbf{0}_{d}\\right],\\quad\\mathrm{and}\\quad\\nu=\\left[\\mathbf{0}_{d}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\ng_{\\mathsf{A T T}}(\\mathbf{Z})=\\boldsymbol{x}^{\\top}\\boldsymbol{W}\\boldsymbol{X}^{\\top}\\boldsymbol{y}=g_{\\mathsf{P G D}}(\\mathbf{Z}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which completes the proof. ", "page_idx": 16}, {"type": "text", "text": "Lemma 5 Suppose Assumptions $^{\\,l}$ and 2 hold. Then, given the objectives in (16), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W_{q},W_{k},W_{\\nu},r}\\mathcal{L}_{S S M}(\\mathcal{W})=\\operatorname*{min}_{W,\\omega}\\mathcal{L}_{\\mathtt{W P G D}}(\\mathcal{W}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Additionally, if the examples $(x_{i},y_{i})_{i=1}^{n}$ follow the same distribution and are conditionally independent given x and $\\beta$ , then SSM/H3 can achieve the optimal loss using the all-ones filter and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W,\\omega}\\mathcal{L}_{\\mathtt{M P G D}}(\\mathcal{W})=\\operatorname*{min}_{W}\\mathcal{L}_{P G D}(\\mathcal{W}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Recap the SSM estimator from (15d) and let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{q}=\\left[w_{q1}\\quad w_{q2}\\quad\\cdot\\cdot\\quad w_{q,d+1}\\right],}\\\\ &{W_{k}=\\left[w_{k1}\\quad w_{k2}\\quad\\cdot\\cdot\\quad w_{k,d+1}\\right],}\\\\ &{W_{\\nu}=\\left[w_{\\nu1}\\quad w_{\\nu2}\\quad\\cdot\\cdot\\quad w_{\\nu,d+1}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\pmb{w}_{q j},\\pmb{w}_{k j},\\pmb{w}_{\\nu j}\\in\\mathbb{R}^{d+1}$ for $j\\leq d+1$ , and let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nu=\\left[\\begin{array}{l}{\\nu_{1}}\\\\ {\\nu_{2}}\\\\ {\\cdot\\cdot\\cdot}\\\\ {\\nu_{d+1}}\\end{array}\\right],\\quad\\mathrm{and}\\quad f=\\left[\\begin{array}{l}{f_{0}}\\\\ {f_{1}}\\\\ {\\cdot\\cdot\\cdot}\\\\ {f_{n}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{\\mathrm{SSH}}(Z)=\\left((z^{\\top}W_{q})^{\\top}\\odot((Z_{0}W_{k}\\odot Z_{0}W_{\\nu})*f)_{n+1}\\right)\\nu}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i=1}^{n}f_{n+1-i}\\cdot\\nu^{\\top}\\left(\\left[\\begin{array}{c}{w_{q1}^{\\top}z}\\\\ {\\cdots}\\\\ {w_{q,d+1}^{\\top}z}\\end{array}\\right]\\odot\\left[\\begin{array}{c}{w_{k1}^{\\top}z_{i}w_{\\nu1}^{\\top}z_{i}}\\\\ {\\cdots}\\\\ {w_{k,d+1}^{\\top}z_{i}w_{\\nu,d+1}^{\\top}z_{i}}\\end{array}\\right]\\right)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i=1}^{n}f_{n+1-i}\\cdot\\nu^{\\top}\\left[\\begin{array}{c}{w_{q1}^{\\top}z w_{k1}^{\\top}z_{i}w_{\\nu1}^{\\top}z_{i}}\\\\ {\\cdots}\\\\ {w_{q,d+1}^{\\top}z w_{k,d+1}^{\\top}z_{i}w_{\\nu,d+1}^{\\top}z_{i}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next for all $j\\leq d+1$ , let ", "page_idx": 17}, {"type": "equation", "text": "$$\nw_{q j}=\\left[\\!\\!\\begin{array}{l}{\\bar{w}_{q j}}\\\\ {w_{q j}}\\end{array}\\!\\!\\right],\\quad w_{k j}=\\left[\\!\\!\\begin{array}{l}{\\bar{w}_{k j}}\\\\ {w_{k j}}\\end{array}\\!\\!\\right],\\quad w_{\\nu j}=\\left[\\!\\!\\begin{array}{l}{\\bar{w}_{\\nu j}}\\\\ {w_{\\nu j}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\bar{\\pmb{w}}_{q j},\\bar{\\pmb{w}}_{k j},\\bar{\\pmb{w}}_{\\nu j}\\in\\mathbb{R}^{d}$ and $w_{q j},w_{k j},w_{\\nu j}\\in\\mathbb{R}$ . Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{q j}^{\\top}z w_{k j}^{\\top}z_{i}w_{\\nu j}^{\\top}z_{i}=\\left(\\bar{w}_{q j}^{\\top}x\\right)\\left(\\bar{w}_{k j}^{\\top}x_{i}+w_{k j}y_{i}\\right)\\left(\\bar{w}_{\\nu j}^{\\top}x_{i}+w_{\\nu j}y_{i}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=x^{\\top}\\bar{w}_{q j}\\left(w_{\\nu j}\\bar{w}_{k j}^{\\top}+w_{k j}\\bar{w}_{\\nu j}^{\\top}\\right)x_{i}y_{i}+\\left(\\bar{w}_{q j}^{\\top}x\\right)\\left(\\bar{w}_{k j}^{\\top}x_{i}\\right)\\left(\\bar{w}_{\\nu j}^{\\top}x_{i}\\right)+\\left(w_{k j}w_{\\nu j}\\bar{w}_{q j}^{\\top}x y_{i}^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=x^{\\top}W_{j}^{\\top}x_{i}y_{i}+\\delta_{j}(x,x_{i},x_{i})+{w_{j}^{\\prime}}^{\\top}x y_{i}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{j}^{\\prime}:=\\bar{w}_{q j}\\left(w_{\\nu j}\\bar{w}_{k j}^{\\top}+w_{k j}\\bar{w}_{\\nu j}^{\\top}\\right)\\in\\mathbb{R}^{d\\times d},}\\\\ &{w_{j}^{\\prime}:=w_{k j}w_{\\nu j}\\bar{w}_{q j}\\in\\mathbb{R}^{d},}\\\\ &{\\delta_{j}(\\pmb{x},\\pmb{x}_{i},\\pmb{x}_{i}):=\\left(\\bar{w}_{q j}^{\\top}\\pmb{x}\\right)\\left(\\bar{w}_{k j}^{\\top}\\pmb{x}_{i}\\right)\\left(\\bar{w}_{\\nu j}^{\\top}\\pmb{x}_{i}\\right)\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{\\sf S S I}({\\pmb Z})=\\displaystyle\\sum_{i=1}^{n}f_{n+1-i}\\cdot\\sum_{j=1}^{d+1}\\nu_{j}\\left({\\pmb x}^{\\top}W_{j}^{\\prime}{\\pmb x}_{i}y_{i}+\\delta_{j}({\\pmb x},{\\pmb x}_{i},{\\pmb x}_{i})+{\\pmb w}_{j}^{\\prime}{\\}^{\\top}{\\pmb x}y_{i}^{\\prime}\\right)}\\\\ &{\\qquad=x^{\\top}\\left(\\displaystyle\\sum_{j=1}^{d+1}\\nu_{j}W_{j}^{\\prime}\\right)X({\\pmb y}\\odot\\tilde{{\\pmb f}})+\\displaystyle\\sum_{i=1}^{n}f_{n+1-i}\\cdot\\sum_{j=1}^{d+1}\\nu_{j}\\cdot\\delta_{j}({\\pmb x},{\\pmb x}_{i},{\\pmb x}_{i})+\\left(\\displaystyle\\sum_{j=1}^{d+1}\\nu_{j}{\\pmb w}_{j}^{\\prime\\prime}\\right)x{\\pmb y}^{\\top}({\\pmb y}\\odot\\tilde{{\\pmb f}})}\\\\ &{\\qquad=\\displaystyle\\sum_{\\tilde{\\beta}\\leq\\mathrm{n}({\\pmb Z})}^{\\top}+\\underbrace{\\tilde{\\delta}({\\pmb x},{\\pmb X},{\\pmb X})}_{\\varepsilon_{1}}+\\underbrace{\\tilde{\\psi}^{\\top}{\\pmb x}{\\pmb y}^{\\top}{\\pmb\\tilde{y}}}_{\\varepsilon_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\tilde{f}:=[f_{n}\\,\\cdots\\,f_{1}]^{\\top}\\in\\mathbb{R}^{n},}\\\\ {\\displaystyle\\tilde{\\mathbf{y}}:=y\\odot\\tilde{f}\\in\\mathbb{R}^{n},}\\\\ {\\displaystyle\\tilde{W}:=\\sum_{j=1}^{d+1}\\nu_{j}W_{j}^{\\prime}\\in\\mathbb{R}^{d\\times d},}\\\\ {\\displaystyle\\tilde{w}:=\\sum_{j=1}^{d+1}\\nu_{j}w_{j}^{\\prime}\\in\\mathbb{R}^{d},}\\\\ {\\displaystyle\\tilde{\\vartheta}:=\\sum_{j=1}^{d+1}\\nu_{j}w_{j}^{\\prime}\\in\\mathbb{R}^{d},}\\\\ {\\displaystyle\\tilde{\\vartheta}(\\pmb{x},X,X):=\\sum_{j=1}^{n}f_{n+1-j}\\cdot\\sum_{j=1}^{d+1}\\nu_{j}\\cdot\\delta_{j}(\\pmb{x},\\pmb{x}_{i},\\pmb{x}_{i})\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next we will show that for any $\\boldsymbol{W_{k}},\\boldsymbol{W_{q}},\\boldsymbol{W_{\\nu}},\\boldsymbol{\\nu}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left(g_{\\mathsf{S S M}}(\\mathbf{Z})-y)^{2}\\right]\\geq\\mathbb{E}\\left[\\left(\\tilde{g}_{\\mathsf{S S M}}(\\mathbf{Z})-y)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To start with, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(g_{\\mathrm{SSI}}(\\pmb{Z})-y)^{2}\\right]=\\mathbb{E}\\left[(\\tilde{g}_{\\mathrm{SSI}}(\\pmb{Z})+\\varepsilon_{1}+\\varepsilon_{2}-y)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[(\\tilde{g}_{\\mathrm{SSI}}(\\pmb{Z})-y)^{2}\\right]+\\mathbb{E}\\left[(\\varepsilon_{1}+\\varepsilon_{2})^{2}\\right]+2\\,\\mathbb{E}\\left[(\\tilde{g}_{\\mathrm{SSI}}(\\pmb{Z})-y)(\\varepsilon_{1}+\\varepsilon_{2})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where there is decomposition ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\widetilde{g}_{\\mathrm{SSI}}(Z)-y)(\\varepsilon_{1}+\\varepsilon_{2})=\\underbrace{\\widetilde{\\delta}(\\pmb{x},X,X)\\cdot\\pmb{x}^{\\top}\\widetilde{W}X\\widetilde{y}}_{(a)}-\\underbrace{\\widetilde{\\delta}(\\pmb{x},X,X)y}_{(b)}+\\underbrace{\\widetilde{w}^{\\top}x y^{\\top}\\widetilde{y}\\cdot\\pmb{x}^{\\top}\\widetilde{W}X\\widetilde{y}}_{(c)}-\\underbrace{\\underline{{y}\\cdot\\widetilde{w}^{\\top}\\pmb{x}y^{\\top}\\widetilde{y}}\\cdot\\pmb{x}}_{(d)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the following, similar to the proof of Lemma 4, we consider the expectations of $(a),(b),(c),(d)$ sequentially, which return zeros under Assumptions 1 and 2. Note that $\\delta_{j}(\\boldsymbol{x},\\pmb{x}_{i},\\pmb{x}_{i})$ \u2019s and $\\tilde{\\delta}(x,X,X)$ are summation of monomials of entries of $(x,X,\\beta)$ with order 3, and entries of ${\\boldsymbol y}$ and $y$ are summation ", "page_idx": 17}, {"type": "text", "text": "of monomials of entries of $(x,X,\\beta)$ with even orders: e.g., $\\mathrm y=\\pmb{x}^{\\top}\\pmb{\\beta}+\\pmb{\\xi}$ where $\\xi$ is of oder 0 and $x^{\\top}\\beta$ is of order 2. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(a):}&{\\ \\mathbb{E}\\left[\\tilde{\\delta}(\\boldsymbol{x},\\boldsymbol{X},\\boldsymbol{X})\\cdot\\boldsymbol{x}^{\\top}\\tilde{\\boldsymbol{W}}\\boldsymbol{X}\\tilde{\\boldsymbol{y}}\\right]}\\\\ &{\\ =\\mathbb{E}\\left[\\tilde{\\delta}(\\boldsymbol{x},\\boldsymbol{X},\\boldsymbol{X})\\cdot\\boldsymbol{x}^{\\top}\\tilde{\\boldsymbol{W}}\\boldsymbol{X}(\\boldsymbol{X}\\beta\\odot\\tilde{\\boldsymbol{f}})\\right]+\\mathbb{E}\\left[\\tilde{\\delta}(\\boldsymbol{x},\\boldsymbol{X},\\boldsymbol{X})\\cdot\\boldsymbol{x}^{\\top}\\tilde{\\boldsymbol{W}}\\boldsymbol{X}(\\boldsymbol{\\xi}\\odot\\tilde{\\boldsymbol{f}})\\right]}\\\\ &{\\ =\\mathbb{E}\\left[\\tilde{\\delta}(\\boldsymbol{x},\\boldsymbol{X},\\boldsymbol{X})\\cdot\\boldsymbol{x}^{\\top}\\tilde{\\boldsymbol{W}}\\boldsymbol{X}\\right]\\mathbb{E}\\left[\\xi\\odot\\tilde{\\boldsymbol{f}}\\right]}\\\\ &{\\ =0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(b):\\;}&{\\mathbb{E}\\left[\\tilde{\\delta}(x,X,X)y\\right]}\\\\ &{=\\mathbb{E}\\left[\\tilde{\\delta}(x,X,X)(x^{\\top}\\beta+\\xi)\\right]}\\\\ &{=\\mathbb{E}\\left[\\tilde{\\delta}(x,X,X)x^{\\top}\\beta\\right]+\\mathbb{E}\\left[\\tilde{\\delta}(x,X,X)\\xi\\right]}\\\\ &{=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(c):}&{\\;\\mathbb{E}\\left[\\tilde{w}^{\\top}x y^{\\top}\\tilde{y}\\cdot x^{\\top}\\tilde{W}X\\tilde{y}\\right]}\\\\ &{\\;=\\mathbb{E}\\left[\\tilde{w}^{\\top}x(X\\beta+\\xi)^{\\top}(X\\beta\\odot\\tilde{f}+\\xi\\odot\\tilde{f})\\cdot x^{\\top}\\tilde{W}X(X\\beta\\odot\\tilde{f}+\\xi\\odot\\tilde{f})\\right]}\\\\ &{\\;=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(d):}&{\\;\\mathbb{E}\\left[y\\cdot\\tilde{w}^{\\top}x y^{\\top}\\tilde{y}\\right]}\\\\ &{\\;=\\mathbb{E}\\left[(x^{\\top}\\beta+\\xi)\\cdot\\tilde{w}^{\\top}x(X\\beta+\\xi)^{\\top}(X\\beta\\odot\\tilde{f}+\\xi\\odot\\tilde{f})\\right]}\\\\ &{\\;=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining the results with (24) results that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left(g_{\\mathsf{S S I}}(\\pmb{Z})-y\\right)^{2}\\right]-\\mathbb{E}\\left[\\left(\\tilde{g}_{\\mathsf{S S I}}(\\pmb{Z})-y\\right)^{2}\\right]=\\mathbb{E}\\left[\\left(\\varepsilon_{1}+\\varepsilon_{2}\\right)^{2}\\right]\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore we obtain, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W_{q},W_{k},W_{\\nu},\\nu,f}\\mathbb{E}\\left[\\left(g_{S S\\mathtt{M}}(Z)-y\\right)^{2}\\right]\\ge\\operatorname*{min}_{\\tilde{W},\\tilde{f}}\\mathbb{E}\\left[(\\tilde{g}_{S S\\mathtt{M}}(Z)-y)^{2}\\right]=\\operatorname*{min}_{W,\\omega}\\mathbb{E}\\left[(g_{\\mathtt{W C D}}(Z)-y)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next we show that for any choices of $W$ and $\\omega$ in $g_{\\tt W P G D}$ , there are $W_{q,k,\\nu},\\nu,f$ such that $g_{\\mathtt{S S M}}\\equiv g_{\\mathtt{W P G D}}$ . To this end, given $\\boldsymbol{\\omega}=[\\omega_{1}\\,\\ldots\\,\\omega_{n}]^{\\top}$ , let ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{q}=I_{d+1},\\quad W_{k}=\\left[\\!\\!\\begin{array}{c c}{W^{\\top}}&{\\mathbf{0}_{d}}\\\\ {\\mathbf{0}_{d}^{\\top}}&{0}\\end{array}\\!\\!\\right],\\quad W_{\\nu}=\\left[\\!\\!\\begin{array}{c c}{\\mathbf{0}_{d\\times d}}&{\\mathbf{0}_{d}}\\\\ {\\mathbf{1}_{d}^{\\top}}&{0}\\end{array}\\!\\!\\right],\\quad\\nu=\\left[\\!\\!\\begin{array}{c c c}{\\mathbf{1}_{d}}\\\\ {0}\\end{array}\\!\\!\\right]\\quad\\mathrm{and}\\quad f=\\left[\\!\\!\\begin{array}{c c}{0}\\\\ {\\omega_{n}}\\\\ {\\ldots}\\\\ {\\omega_{n}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{((Z_{0}W_{k}\\odot Z_{0}W_{\\nu})\\ast f)_{n+1}=\\left(\\left(\\left[\\!\\!\\begin{array}{c c}{\\mathbf{U}W^{\\top}}&{\\mathbf{0}_{n}}\\\\ {\\mathbf{0}_{d}}&{0}\\end{array}\\!\\!\\right]\\odot\\left[\\!\\!\\begin{array}{c c}{\\mathbf{y}\\mathbf{1}_{d}^{\\top}}&{\\mathbf{0}_{n}}\\\\ {\\mathbf{0}_{d}}&{0}\\end{array}\\!\\!\\right]\\right)\\ast f\\right)_{n+1}}\\\\ &{\\qquad\\qquad\\qquad=\\left[\\!\\!\\begin{array}{c c}{\\sum_{i=1}^{n}\\omega_{i}\\cdot y_{i}W x_{i}}\\\\ {0}\\end{array}\\!\\!\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\left[\\!\\!\\begin{array}{c c}{W X^{\\top}(\\mathbf{y}\\odot\\omega)}\\\\ {0}\\end{array}\\!\\!\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\ng_{\\mathsf{S S M}}(\\mathbf{Z})=x^{\\top}W X^{\\top}(\\mathbf{y}\\odot\\omega)=g_{\\mathsf{W P G D}}(\\mathbf{Z}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which completes the proof of (22). ", "page_idx": 18}, {"type": "text", "text": "Next, to show (23), for any $W\\in\\mathbb{R}^{d\\times d}$ , let $\\mathcal{L}(\\omega)=\\mathbb{E}\\,\\Bigl[\\left({\\pmb x}^{\\top}{\\pmb W}{\\pmb X}^{\\top}({\\pmb y}\\odot{\\pmb\\omega})-{\\pmb y}\\right)^{2}\\Bigr].$ . Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial\\mathcal{L}(\\boldsymbol{\\omega})}{\\partial\\omega_{i}}=\\mathbb{E}\\left[2\\left(x^{\\top}W\\sum_{j=1}^{n}\\omega_{j}y_{j}\\mathbf{x}_{j}-y\\right)\\left(x^{\\top}W y_{i}\\mathbf{x}_{i}\\right)\\right]}}\\\\ &{}&{=2\\sum_{j=1}^{n}\\omega_{j}\\,\\mathbb{E}\\left[(x^{\\top}W y_{j}\\mathbf{x}_{j})(x^{\\top}W y_{i}\\mathbf{x}_{i})\\right]-2\\,\\mathbb{E}\\left[y\\mathbf{x}^{\\top}W y_{i}\\mathbf{x}_{i}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here since $(x_{i},y_{i})_{i=1}^{n}$ follow the same distribution and are conditionally independent given $\\pmb{x}$ and $_\\beta$ , for any $i\\ \\stackrel{\\cdot}{\\neq}\\ j\\ \\neq\\ j^{\\prime},\\ \\mathbb{E}\\left[(\\pmb{x}^{\\top}\\pmb{W}\\!y_{i}\\pmb{x}_{i})^{2}\\right]\\ =\\ \\mathbb{E}\\left[(\\pmb{x}^{\\top}\\pmb{W}\\!y_{j}\\pmb{x}_{j})^{2}\\right]$ and $\\mathbb{E}\\left[({\\pmb x}^{\\top}{\\pmb W}{\\b y}_{j}{\\pmb x}_{j})({\\pmb x}^{\\top}{\\pmb W}{\\b y}_{i}{\\pmb x}_{i})\\right]~=$ $\\mathbb{E}\\left[({\\pmb x}^{\\top}{\\pmb W}{\\mathfrak y}_{j^{\\prime}}{\\pmb x}_{j^{\\prime}})({\\pmb x}^{\\top}{\\pmb W}{\\mathfrak y}_{i}{\\pmb x}_{i})\\right]$ . Then let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[({\\boldsymbol{x}}^{\\top}W y_{j}{\\boldsymbol{x}}_{j})({\\boldsymbol{x}}^{\\top}W y_{i}{\\boldsymbol{x}}_{i})\\right]=\\left\\{{\\boldsymbol{c}}_{1},\\quad i\\neq j\\quad\\mathrm{~and~}\\quad\\mathbb{E}\\left[y{\\boldsymbol{x}}^{\\top}W y_{i}{\\boldsymbol{x}}_{i}\\right]={\\boldsymbol{c}}_{3},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(c_{1},c_{2},c_{3}):=(c_{1}(W),c_{2}(W),c_{3}(W))$ . We get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega_{i}}=2c_{1}\\omega^{\\top}\\mathbf{1}_{n}+2(c_{2}-c_{1})\\omega_{i}-2c_{3}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If $c_{2}\\!-\\!c_{1}=0$ , then \u2202\u2202L(\u03c9i\u03c9)\u22612c1\u03c9\u22a41n\u22122c3 for all i \u2264n and any \u03c9 \u2208Rn achieves the same performance. If $c_{2}-c_{1}\\neq0$ , setting $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}(\\omega)}{\\partial\\omega_{i}}=0}\\end{array}$ ) = 0 returns ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\omega_{i}={\\frac{c_{3}-c_{1}\\sum_{j=1}^{n}\\omega_{j}}{c_{2}-c_{1}}}:=C\\quad{\\mathrm{for~all~}}i\\leq n.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore the optimal loss is achieved via setting $\\pmb{\\omega}=C\\mathbf{1}_{n}$ . Without loss of generality, we can update $W\\to C W$ . Then $\\pmb{\\omega}=\\mathbf{1}_{n}$ , and we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W,\\omega}\\mathbb{E}\\left[\\left(x^{\\top}W X^{\\top}(y\\odot\\omega)-y\\right)^{2}\\right]=\\operatorname*{min}_{W}\\mathbb{E}\\left[(x^{\\top}W X^{\\top}y-y)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the proof of (23). ", "page_idx": 19}, {"type": "text", "text": "A.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Recap the loss $\\mathcal{L}_{\\mathrm{PGD}}(\\mathcal{W})$ in (16a) and prediction $g_{\\mathrm{PGD}}(Z)$ in (15a), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}_{\\mathtt{P G D}}(\\mathcal{W})=\\mathbb{E}[(y-g_{\\mathtt{P G D}}(Z))^{2}]}\\\\ {=\\mathbb{E}\\left[\\left(x^{7}\\beta+\\xi-x^{\\top}W X^{\\top}(X\\beta+\\xi)\\right)^{2}\\right]}\\\\ {=\\mathbb{E}\\left[\\left(x^{7}\\beta-x^{\\top}W X^{\\top}X\\beta\\right)^{2}+2(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)(\\xi-x^{\\top}W X^{\\top}\\xi)+(\\xi-x^{\\top}W X^{\\top}\\xi)^{2}\\right]}\\\\ {=\\mathbb{E}\\left[\\left(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta\\right)^{2}+(\\xi-x^{\\top}W X^{\\top}\\xi)^{2}\\right]+2\\mathbb{E}[(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)(\\xi-x^{\\top}W X^{\\top}\\xi)]}\\\\ {=\\mathbb{E}\\left[(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)^{2}+(\\xi-x^{\\top}W X^{\\top}\\xi)^{2}\\right]}\\\\ {=\\mathbb{E}\\left[(x^{\\top}W X^{\\top}X\\beta)^{2}+(x^{\\top}W X^{\\top}\\xi)^{2}\\right]\\underbrace{-2\\mathbb{E}[\\beta^{\\top}x x^{\\top}W X^{\\top}X\\beta+\\xi x^{\\top}W X^{\\top}\\xi]}_{f_{1}(W)}+\\underbrace{\\mathbb{E}[(x^{\\top}\\beta)^{2}+\\xi^{2}]}_{\\mathrm{constar}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (25) follows Assumption 2. Since $f_{2}(W)$ is convex, $\\mathcal{L}_{\\mathtt{P G D}}(\\mathcal{W})$ is strongly-convex if and only if $f_{1}(W)$ is strongly-convex, which completes the proof of strong convexity. ", "page_idx": 19}, {"type": "text", "text": "Next, (20) and (21) in the proof of Lemma 4 demonstrate that the optimal loss is achievable and is achieved at $\\varepsilon=0$ . Subsequently, (17) indicates that $g_{\\mathrm{ATT}}^{\\star}$ has the same form as g\u22c6 Under the strong . convexity assumption, $g_{\\mathrm{PGD}}^{\\star}$ is unique, which leads to the conclusion that $g_{\\mathrm{PGD}}^{\\star}=g_{\\mathrm{ATT}}^{\\star}$ . ", "page_idx": 19}, {"type": "text", "text": "A.3 Proof of Lemma 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. According to Lemma 1, $\\mathcal{L}_{\\mathtt{P G D}}(\\mathcal{W})$ is strongly-convex as long as either $\\mathbb{E}[({\\pmb x}^{\\top}{\\pmb W}{\\pmb X}^{\\top}{\\pmb X}{\\pmb\\beta})^{2}]$ or $\\mathbb{E}[({\\pmb x}^{\\top}{\\pmb W}{\\pmb X}^{\\top}{\\pmb\\xi})^{2}]$ is strongly-convex. Therefore, in this lemma, the two claims correspond to the strong convexity of $\\mathbb{E}[({\\pmb x}^{\\top}{\\pmb W}{\\pmb Y}^{\\top}\\dot{\\pmb\\xi})^{2}]$ and $\\mathbb{E}[({\\pmb x}^{\\top}{\\pmb W}{\\pmb X}^{\\top}{\\pmb X}{\\pmb\\beta})^{2}]$ terms, respectively. ", "page_idx": 20}, {"type": "text", "text": "Suppose the decomposition claim holds. Without losing generality, we may assume $(x_{1},\\beta_{1},X_{1})$ are zero-mean because we can allocate the mean component to $(x_{2},\\beta_{2},X_{2})$ without changing the covariance. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Claim 1: Let $\\bar{\\Sigma}_{x}=\\mathbb{E}[\\pmb{x}_{1}\\pmb{x}_{1}^{\\top}]$ , $\\bar{\\Sigma}_{\\beta}=\\mathbb{E}[\\beta_{1}\\beta_{1}^{\\top}]$ , and $\\bar{\\Sigma}_{X}=\\mathbb{E}[X_{1}^{\\top}X_{1}]$ . If the first claim holds, using independence, observe that we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[({\\boldsymbol{x}}^{\\top}W{\\boldsymbol{X}}^{\\top}{\\boldsymbol{\\xi}})^{2}]=\\mathbb{E}[({\\boldsymbol{x}}_{1}^{\\top}W{\\boldsymbol{X}}_{1}^{\\top}{\\boldsymbol{\\xi}})^{2}]+\\mathbb{E}[({\\boldsymbol{x}}_{1}^{\\top}W{\\boldsymbol{X}}_{2}^{\\top}{\\boldsymbol{\\xi}})^{2}]+\\mathbb{E}[({\\boldsymbol{x}}_{2}^{\\top}W{\\boldsymbol{X}}_{1}^{\\top}{\\boldsymbol{\\xi}})^{2}]+\\mathbb{E}[({\\boldsymbol{x}}_{2}^{\\top}W{\\boldsymbol{X}}_{2}^{\\top}{\\boldsymbol{\\xi}})^{2}],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last three terms of the right hand side are convex and the first term obeys ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(x_{1}^{\\top}W X_{1}^{\\top}\\xi)^{2}]=\\sigma^{2}\\mathbb{E}[x_{1}^{\\top}W X_{1}^{\\top}X_{1}W^{\\top}x_{1}]}\\\\ &{\\phantom{\\sum}=\\sigma^{2}\\mathrm{tr}\\left(\\mathbb{E}[x_{1}x_{1}^{\\top}W X_{1}^{\\top}X_{1}W^{\\top}]\\right)}\\\\ &{\\phantom{\\sum}=\\sigma^{2}\\mathrm{tr}\\left(\\bar{\\Sigma}_{x}W\\bar{\\Sigma}_{X}W^{\\top}\\right)}\\\\ &{\\phantom{\\sum}=\\sigma^{2}\\left\\|\\sqrt{\\bar{\\Sigma}_{x}}W\\sqrt{\\bar{\\Sigma}_{X}}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since noise level $\\sigma>0$ , using the full-rankness of covariance matrices $\\bar{\\Sigma}_{x}$ and $\\bar{\\Sigma}_{X}$ , we conclude with strong convexity of $\\mathbb{E}[({\\pmb x}^{\\top}{\\pmb W}\\bar{\\pmb X}^{\\top}{\\pmb\\xi})^{2}]$ . ", "page_idx": 20}, {"type": "text", "text": "\u2022 Claim 2: Now recall that $\\bar{\\Sigma}_{X}=\\mathbb{E}[X_{1}^{\\top}X_{1}]$ and set $A=X_{1}^{\\top}X_{1}-\\bar{\\Sigma}_{X}$ and $\\pmb{{\\cal B}}=\\pmb{{\\cal X}}_{2}^{\\top}\\pmb{{\\cal X}}_{2}+\\bar{\\pmb{\\Sigma}}_{X}$ . Observe that $\\mathbb{E}[\\mathbf{A}]=0$ . If the second claim holds, $\\mathbb{E}[X^{\\top}X]=\\mathbb{E}[A+\\pmb{B}]$ . Note that $(A,\\beta_{1},\\bar{{\\pmb x}}_{1})$ are independent of each other and $(\\pmb{{\\cal B}},\\pmb{\\beta}_{2},\\pmb{{\\boldsymbol x}}_{2})$ . Using independence and $\\mathbb{E}[A]=0$ , similarly write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}[({\\pmb x}^{\\top}{\\pmb W}X^{\\top}X{\\pmb\\beta})^{2}]=\\operatorname{\\mathbb{E}}[({\\pmb x}^{\\top}{\\pmb W}A{\\pmb\\beta})^{2}]+\\operatorname{\\mathbb{E}}[({\\pmb x}^{\\top}{\\pmb W}B{\\pmb\\beta})^{2}].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now using $\\mathbb{E}[\\pmb{\\beta}_{1}]=\\mathbb{E}[\\pmb{x}_{1}]=0$ and their independence from rest, these terms obeys ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(x^{\\top}W A\\beta)^{2}]=\\mathbb{E}[(x_{1}^{\\top}W A\\beta_{1})^{2}]+\\mathbb{E}[(x_{1}^{\\top}W A\\beta_{2})^{2}]+\\mathbb{E}[(x_{2}^{\\top}W A\\beta_{1})^{2}]+\\mathbb{E}[(x_{2}^{\\top}W A\\beta_{2})^{2}]}\\\\ &{\\mathbb{E}[(x^{\\top}W B\\beta)^{2}]=\\mathbb{E}[(x_{1}^{\\top}W B\\beta_{1})^{2}]+\\mathbb{E}[(x_{1}^{\\top}W B\\beta_{2})^{2}]+\\mathbb{E}[(x_{2}^{\\top}W B\\beta_{1})^{2}]+\\mathbb{E}[(x_{2}^{\\top}W B\\beta_{2})^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In both equations, the last three terms of the right hand side are convex. To proceed, we focus on the first terms. Using independence and setting $\\Breve{\\Sigma_{X}}=\\mathbb{E}[X^{\\top}X]\\geq\\bar{\\Sigma}_{X}\\succ0$ , we note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[(\\pmb{x}_{1}^{\\top}\\pmb{W}\\pmb{A}\\beta_{1})^{2}]+\\mathbb{E}[(\\pmb{x}_{1}^{\\top}\\pmb{W}\\pmb{B}\\beta_{1})^{2}]=\\mathbb{E}[(\\pmb{x}_{1}^{\\top}\\pmb{W}\\pmb{X}^{\\top}\\pmb{X}\\beta_{1})^{2}]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $x_{1},\\beta_{1},X$ are independent and full-rank covariance. To proceed, note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[(x_{1}^{\\top}W X^{\\top}X\\beta_{1})^{2}]=\\mathbb{E}[(x_{1}^{\\top}W\\Sigma_{X}\\beta_{1})^{2}]+\\mathbb{E}[(x_{1}^{\\top}W(X^{\\top}X-\\Sigma_{X})\\beta_{1})^{2}].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Observing the convexity of the right hand side and focusing on the first term, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[(\\pmb{x}_{1}^{\\top}\\pmb{W}\\pmb{\\Sigma}_{X}\\beta_{1})^{2}]=\\mathrm{tr}\\left(\\bar{\\pmb{\\Sigma}}_{x}\\pmb{W}\\pmb{\\Sigma}_{X}\\bar{\\pmb{\\Sigma}}_{\\beta}\\pmb{\\Sigma}_{X}\\pmb{W}^{\\top}\\right)=\\left\\|\\sqrt{\\bar{\\pmb{\\Sigma}}_{x}}\\pmb{W}\\pmb{\\Sigma}_{X}\\sqrt{\\bar{\\pmb{\\Sigma}}_{\\beta}}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the fact that covariance matrices, $\\bar{\\Sigma}_{x},\\Sigma_{X},\\bar{\\Sigma}_{\\beta}$ , are full rank concludes the strong convexity proof of $\\mathbb{E}[({\\pmb x}^{\\top}{\\pmb W}{\\pmb X}^{\\top}{\\pmb X}{\\pmb\\beta})^{2}]$ . ", "page_idx": 20}, {"type": "text", "text": "B Analysis of General Data Distribution ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide the proofs in Section 3, which focuses on solving Objective (5a). For the sake of clean notation, let $\\mathcal{L}(W):=\\mathcal{L}_{\\sf P G D}(\\mathcal{W})$ and $g:=g_{\\mathsf{P G D}}$ in this section. ", "page_idx": 20}, {"type": "text", "text": "B.1 Supporting Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We begin by deriving the even moments of random variables. ", "page_idx": 21}, {"type": "text", "text": "\u2022 $2n$ \u2019th moment of a normally distributed variable: Let $u\\sim N(0,\\sigma^{2})$ . Then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[u^{2n}]=\\sigma^{2n}(2n-1)!!.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u2022 4\u2019th moment: Let $\\pmb{u}\\sim N(0,\\pmb{I}_{d})$ . Then for any $W,W^{\\prime}\\in\\mathbb{R}^{d\\times d}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\langle\\mathbf{u}^{T}\\mathbf{W}(\\boldsymbol{\\mathbf{u}}^{T}\\boldsymbol{\\mathbf{b}}^{T}\\boldsymbol{\\mathbf{b}}^{T}\\boldsymbol{\\mathbf{b}}^{T}\\boldsymbol{\\mathbf{b}}^{T}\\rangle\\right]}\\\\ &{=\\mathbb{E}\\left[\\left[\\int_{\\sqrt{d}}^{d}\\mathbf{W}_{(d,t)}\\right]\\left(\\int_{\\sqrt{d}}^{\\infty}\\mathbf{W}_{(d,t)}^{T}\\boldsymbol{\\mathbf{b}}^{T}\\boldsymbol{\\mathbf{a}}^{T}\\boldsymbol{\\mathbf{a}}^{T}\\boldsymbol{\\mathbf{a}}_{d,t}\\right)\\right]}\\\\ &{=\\mathbb{E}\\left[\\left[\\int_{\\sqrt{d}}^{d}\\mathbf{W}_{(d,t)}\\right]\\left(\\int_{\\sqrt{d}}^{\\infty}\\mathbf{W}_{(d,t)}^{T}\\boldsymbol{\\mathbf{b}}^{T}\\boldsymbol{\\mathbf{a}}^{T}\\boldsymbol{\\mathbf{a}}^{T}\\boldsymbol{\\mathbf{a}}^{T}\\rangle+\\mathbb{E}\\left[\\left[\\sum_{j=1}^{W}W_{(d,t)}\\right]\\left(\\sum_{j=1}^{W}W_{(d,t)}^{\\prime}\\boldsymbol{\\mathbf{a}}_{d,t}\\right)\\right]\\right.}\\\\ &{=\\frac{\\sigma^{2}}{\\rho\\sigma}\\mathbf{W}_{(d,t)}\\mathbf{W}_{(d,t)}^{T}\\left[\\sum_{j=1}^{W}W_{(d,t)}^{\\prime}\\sum_{l=1}^{W}W_{(d,t)}^{\\prime}\\sum_{l=u^{T}}^{T}W_{(d,t)}^{\\prime}\\sum_{l=u^{T}}^{T}W_{(d,t)}^{\\prime}\\sum_{l=u^{T}}^{T}W_{(d,t)}^{\\prime}\\sum_{l=u^{T}}^{T}W_{(d,t)}^{\\prime}\\right]\\mathbb{E}\\left[\\left[\\sum_{l=u^{T}}^{W}W_{(d,t)}^{\\prime}\\sum_{l=u^{T}}^{T}\\boldsymbol{\\mathbf{a}}_{d,t}^{T}\\right]\\mathbb{E}\\left[\\left[\\sum_{l=u^{T}}^{W}W_{(d,t)}^{\\prime}\\right]\\right]}\\\\ &{=3\\sum_{l=1}^{W}W_{(d,t)}^{\\prime}+\\sum_{l=1}^{W}W_{(d,t)}^{\\prime}+\\sum_{l=u^{T}}W_{(d,t)}^{\\prime}+\\sum_{l=\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u2022 4\u2019th cross-moment: Let $\\pmb{u},\\pmb{\\nu}\\sim\\pmb{N}(0,\\pmb{I}_{d})$ and for any $W\\in\\mathbb{R}^{d\\times d}$ , let $\\mathbf{A}_{W}=\\pmb{W}\\odot\\pmb{I}_{d}$ . Then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\Phi}:=\\mathbf{E}:\\left[\\left(\\sum_{i}^{k}\\mathbf{R}_{i}(t,t)\\right)\\Bigg]\\left(\\sum_{k=0}^{\\infty}\\mathbf{R}_{i}(t)\\right)}\\\\ &{=\\frac{\\mathbf{\\Phi}^{\\mathsf{T}}\\mathbf{R}_{i}}{\\mathbf{\\Phi}^{\\mathsf{T}}\\mathbf{R}_{i}}\\mathbf{\\Phi}^{\\mathsf{T}}\\!\\Bigg[\\sum_{k=1}^{\\infty}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R}_{i}^{\\mathsf{a}}\\mathbf{\\Phi}^{\\mathsf{T}}\\!\\Bigg]\\Bigg(\\sum_{j=1}^{\\infty}\\mathbf{R}_{j}\\mathbf{R}_{j}\\mathbf{R}_{j}}\\\\ &{=\\frac{\\mathbf{\\Phi}^{\\mathsf{T}}\\mathbf{R}_{j}}{\\mathbf{\\Phi}^{\\mathsf{T}}\\mathbf{R}_{j}}\\!\\!\\Bigg[\\!\\sum_{k=0}^{\\infty}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u2022 6\u2019th moment: Let $\\pmb{u}\\sim N(0,\\pmb{I}_{d})$ . Then for any $W,W^{\\prime}\\in\\mathbb{R}^{d\\times d}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\alpha^{\\mathrm{in}}W_{n}(\\theta^{\\top}W)|\\left.\\log\\left|\\sum_{i=1}^{n}\\theta^{\\top}\\right|_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(\\sum_{i=1}^{d}W_{n}u_{i}\\right)\\left(\\sum_{i=1}^{d}W_{n}u_{i}\\right)\\right]\\left(\\sum_{i=1}^{d}W_{n}^{\\prime}\\right)}\\\\ &{=\\mathbb{E}\\left[\\left(\\sum_{i=1}^{d}W_{n}u_{i}^{\\prime}\\right)\\left(\\sum_{i=1}^{d}w_{i}^{\\prime}\\alpha_{i}^{\\mathrm{in}}\\right)\\left(\\sum_{i=1}^{d}\\mathbb{E}\\left[\\sum_{j=i}^{d}w_{j}u_{i}^{\\prime}\\right]\\left(\\sum_{i=1}^{d}w_{j}^{\\prime}\\right)\\right)\\right.}\\\\ &{\\left.=\\mathbb{E}\\left[\\sum_{i=1}^{d}W_{n}u_{i}^{\\prime}\\right]\\left[\\sum_{i=1}^{d}w_{i}^{\\prime}\\left(\\sum_{j=1}^{d}w_{j}^{\\prime}\\right)\\right]+\\mathbb{E}\\left[\\sum_{i=1}^{d}W_{n}^{\\prime}\\right]\\left[\\sum_{i=1}^{d}w_{i}^{\\prime}\\left(\\sum_{j=1}^{d}w_{j}^{\\prime}\\right)\\right]\\left(\\sum_{i=1}^{d}w_{i}^{\\prime}\\right)}\\\\ &{\\quad+\\left.\\sum_{i=1}^{d}W_{n}u_{i}^{\\prime}\\left[\\sum_{i=1}^{d}w_{i}^{\\prime}\\right]+\\sum_{j=1}^{d}W_{n}\\mathbb{E}\\left[\\alpha_{j}^{\\mathrm{in}}\\left(\\sum_{i=1}^{d}w_{j}^{\\prime}\\right)\\right]\\right.}\\\\ &{\\quad+\\left.\\int_{\\mathcal{X}_{n}}W_{n}u_{i}^{\\prime}\\left[\\sum_{i=1}^{d}w_{j}^{\\prime}\\right]+\\sum_{i=1}^{d}W_{n}w_{i}^{\\prime}\\sum_{i=1}^{d}\\left[\\sum_{i=1}^{d}w_{i}^{\\prime}\\right]\\right]}\\\\ &{=(d+4)\\left(\\sum_{i=1}^{d}W_{n}u_{i}^{\\prime}+\\sum_{i=1}^{d}W_{n}u_{i}^{\\prime}+\\sum_{i=\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (29) is obtained by following ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[u_{i}^{4}\\left(\\displaystyle\\sum_{i^{\\prime}=1}^{d}u_{i^{\\prime}}^{2}\\right)\\right]=\\mathbb{E}[u^{6}]+(d-1)\\,\\mathbb{E}[u^{4}]\\,\\mathbb{E}[u^{2}]=3(d+4),}\\\\ &{\\mathbb{E}\\left[u_{i}^{2}u_{j}^{2}\\left(\\displaystyle\\sum_{i^{\\prime}=1}^{d}u_{i^{\\prime}}^{2}\\right)\\right]=2\\,\\mathbb{E}[u^{4}]\\,\\mathbb{E}[u^{2}]+(d-2)\\,\\mathbb{E}[u^{2}]\\,\\mathbb{E}[u^{2}]\\,\\mathbb{E}[u^{2}]=d+4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "\u2022 8\u2019th moment: Let $\\pmb{u}\\sim N(0,\\pmb{I}_{d})$ . Then for any ${\\pmb W},{\\pmb W}^{\\prime}\\in\\mathbb R^{d\\times d}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left.\\operatorname{trurw}_{i}|\\operatorname{trurw}_{j}|\\operatorname{trurw}_{i}|\\right]_{(\\frac{j}{\\pi})}^{\\mathrm{\\scriptscriptstyleT}}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left.\\underset{0\\leq i}{\\overset{a}{\\sum}}\\!\\ \\operatorname{trurw}_{i}\\!\\right|_{(\\frac{j}{\\pi})}^{\\frac{1}{\\alpha}}\\!\\ \\operatorname{trurw}_{j}\\right]\\!\\Bigg[\\underset{0\\leq i}{\\overset{a}{\\sum}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (31) is obtained by following ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[u_{i}^{4}\\left(\\displaystyle\\sum_{r=1}^{d}u_{r}^{4}+\\displaystyle\\sum_{r^{\\prime}=1}^{n}u_{r}^{2}u_{r}^{2}\\right)\\right]}\\\\ &{=\\mathbb{E}[u^{8}]+(d-1)\\mathbb{E}[u^{4}]\\mathbb{E}[u^{4}]+2(d-1)\\mathbb{E}[u^{6}]\\mathbb{E}[u^{2}]+(d-1)(d-2)\\mathbb{E}[u^{4}]\\mathbb{E}[u^{2}]\\mathbb{E}[u^{2}]}\\\\ &{=105+9(d-1)+30(d-1)+3(d-1)(d-2)}\\\\ &{=3(d+4)(d+6),}\\\\ &{\\mathbb{E}\\left[u_{i}^{2}u_{j}^{2}\\left(\\displaystyle\\sum_{r=1}^{d}u_{r}^{4}+\\displaystyle\\sum_{r^{\\prime}=t^{\\prime}}u_{r}^{2}u_{r}^{2}\\right)\\right]}\\\\ &{=2\\mathbb{E}[u^{6}]\\mathbb{E}[u^{2}]+(d-2)\\mathbb{E}[u^{4}](\\mathbb{E}[u^{2}])^{2}+2\\mathbb{E}[u^{4}]\\mathbb{E}[u^{4}]+4(d-2)\\mathbb{E}[u^{4}](\\mathbb{E}[u^{2}])^{2}+(d-2)(d-3)(\\mathbb{E}}\\\\ &{=30+3(d-2)+18+12(d-2)+(d-2)(d-3)}\\\\ &{=(d+4)(d+6).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "B.2 Independent Data with General Covariance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 1. Consider a general independent linear model as defined in (7) where $\\Sigma_{x}$ and $\\Sigma_{\\beta}$ are full-rank feature and task convariance matrices and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x\\sim{\\cal N}(0,\\Sigma_{x}),\\quad\\beta\\sim{\\cal N}(0,\\Sigma_{\\beta}),\\quad\\xi\\sim{\\cal N}(0,\\sigma^{2}),\\quad\\mathrm{and}\\quad y=x^{\\top}\\beta+\\xi.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let ", "page_idx": 23}, {"type": "equation", "text": "$$\nX=[x_{1}\\,\\cdot\\,\\cdot\\,\\cdot\\,x_{n}]^{\\top},\\quad\\xi=[\\xi_{1}\\,\\,\\cdot\\,\\cdot\\,\\xi_{n}]^{\\top},\\quad\\mathrm{and}\\quad y=[y_{1}\\,\\,\\cdot\\,\\cdot\\,y_{n}]^{\\top}=X\\beta+\\xi.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To simplify and without loss of generality, let $\\bar{\\pmb{x}}=\\Sigma_{x}^{-1/2}\\pmb{x},\\bar{\\pmb{X}}=X\\Sigma_{x}^{-1/2},\\bar{\\pmb{\\beta}}=\\Sigma_{x}^{1/2}\\beta$ where we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{x}\\sim{\\cal N}(0,I),\\qquad\\bar{\\beta}\\sim{\\cal N}(0,\\Sigma_{x}^{1/2}\\Sigma_{\\beta}\\Sigma_{x}^{1/2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y=\\bar{x}^{\\top}\\bar{\\boldsymbol{\\beta}}+\\boldsymbol{\\xi},\\qquad\\mathbf{y}=\\bar{X}\\bar{\\boldsymbol{\\beta}}+\\boldsymbol{\\xi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then recap the loss from (5a), and we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(W)=\\mathbb{E}\\left[(y-g(Z))^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[\\left(x^{\\top}\\beta+\\xi-x^{\\top}W X^{\\top}(X\\beta+\\xi)\\right)^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)^{2}+2(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)(\\xi-x^{\\top}W X^{\\top}\\xi)+(\\xi-x^{\\top}W X^{\\top}\\xi)^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)^{2}\\right]+\\mathbb{E}\\left[(x^{\\top}W X^{\\top}\\xi)^{2}\\right]+\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last equality comes from the independence of label noise $\\xi,\\xi$ . ", "page_idx": 23}, {"type": "text", "text": "We first consider the following term ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[(x^{\\top}W X^{\\top}\\xi)^{2}\\right]=\\mathbb{E}\\left[(\\bar{x}^{\\top}(\\Sigma_{x}^{1/2}W\\Sigma_{x}^{1/2})\\bar{X}^{\\top}\\xi)^{2}\\right]=n\\sigma^{2}\\cdot\\operatorname{tr}\\left(\\bar{W}\\bar{W}^{\\top}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we define $\\Bar{W}=\\Sigma_{x}^{1/2}\\mathbf{W}\\Sigma_{x}^{1/2}$ . Next, focus on the following ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)^{2}\\right]=\\mathbb{E}\\left[(\\bar{x}^{\\top}\\bar{\\beta}-\\bar{x}^{\\top}\\bar{W}\\bar{X}^{\\top}\\bar{X}\\bar{\\beta})^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\left(\\bar{x}^{\\top}\\left(I-\\bar{W}\\bar{X}^{\\top}\\bar{X}\\right)\\bar{\\beta}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{tr}\\left(\\mathbb{E}\\left[\\left(I-\\bar{W}\\bar{X}^{\\top}\\bar{X}\\right)\\Sigma\\left(I-\\bar{W}\\bar{X}^{\\top}\\bar{X}\\right)^{\\top}\\right]\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{tr}\\left(\\Sigma\\right)-\\mathrm{tr}\\left(\\Sigma(\\bar{W}+\\bar{W}^{\\top})\\mathbb{E}[\\bar{X}^{\\top}\\bar{X}]\\right)+\\mathrm{tr}\\left(\\bar{W}^{\\top}\\bar{W}\\mathbb{E}[\\bar{X}^{\\top}\\bar{X}\\Sigma\\bar{X}^{\\top}\\bar{X}]\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{tr}\\left(\\Sigma\\right)-2n\\cdot\\mathrm{tr}\\left(\\Sigma\\bar{W}\\right)+\\mathrm{tr}\\left(\\bar{W}^{\\top}\\bar{W}\\mathbb{E}[\\bar{X}^{\\top}\\bar{X}\\Sigma\\bar{X}^{\\top}\\bar{X}]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{\\Sigma:=\\Sigma_{x}^{1/2}\\Sigma_{\\beta}\\Sigma_{x}^{1/2}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Let $\\bar{\\pmb{x}}_{i}\\in\\mathbb{R}^{n}$ be the $i^{\\star}$ th column of $\\bar{X}$ and $\\Sigma_{i j}$ be the $(i,j)$ \u2019th entry of $\\pmb{\\Sigma}$ . Then the $(i,j)$ entry of matrix $\\bar{X}^{\\top}\\bar{X}\\Sigma\\bar{X}^{\\top}\\bar{X}$ is ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\bar{X}^{\\top}\\bar{X}\\Sigma\\bar{X}^{\\top}\\bar{X})_{i j}=\\sum_{k=1}^{d}\\sum_{p=1}^{d}\\Sigma_{k p}\\bar{x}_{i}^{\\top}\\bar{x}_{k}\\bar{x}_{p}^{\\top}\\bar{x}_{j}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\dot{\\iota}\\neq\\ j:}&{\\mathbb{E}\\bigg[\\Big(\\bar{X}^{\\top}\\bar{X}\\bar{X}^{\\top}\\bar{X}\\Big)_{i j}\\bigg]=\\mathrm{E}_{i j}\\,\\mathbb{E}[\\tilde{x}_{i}^{\\top}\\bar{x}_{i}\\bar{x}_{j}^{\\top}\\bar{x}_{j}]+\\mathrm{E}_{j i}\\,\\mathbb{E}[\\tilde{x}_{i}^{\\top}\\bar{x}_{j}\\bar{x}_{i}^{\\top}\\bar{x}_{j}]=n^{2}\\Sigma_{i j}+n\\Sigma_{j i}}\\\\ {\\dot{\\iota}=j:}&{\\mathbb{E}\\Big[\\Big(\\bar{X}^{\\top}\\bar{X}\\bar{X}^{\\top}\\bar{X}\\Big)_{i j}\\Big]=\\mathrm{E}_{i i}\\,\\mathbb{E}\\Big[\\tilde{x}_{i}^{\\top}\\bar{x}_{i}\\bar{x}_{i}^{\\top}\\bar{x}_{i}\\Big]+\\displaystyle\\sum_{j:n}\\sum_{j:j}\\mathbb{E}\\Big[\\tilde{x}_{i}^{\\top}\\bar{x}_{j}\\bar{x}_{j}^{\\top}\\bar{x}_{i}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathrm{E}_{i i}\\,\\mathbb{E}\\Big[(x_{i1}^{2}+\\cdots+x_{i n}^{2n})^{2}\\Big]+n\\sum_{j:i}\\Sigma_{j j}}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{E}_{i i}(3n+n(n-1))+n\\sum_{j:i}\\Sigma_{j i}}\\\\ &{\\qquad\\qquad\\qquad=n\\Bigg(\\Sigma_{i i}(n+1)+\\sum_{j:1}^{d}\\Sigma_{j j}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad=n\\Bigg(\\Sigma_{i i}(n+1)+\\sum_{j:1}^{d}\\Sigma_{j j}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad=n\\,\\mathbb{E}_{i}(n+1)+\\mathrm{tr}(\\Sigma))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{X}^{\\top}\\bar{X}\\Sigma\\bar{X}^{\\top}\\bar{X}]=n(n+1)\\Sigma+n\\cdot\\mathbf{tr}\\left(\\Sigma\\right)I.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining all together results in ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(W)=\\operatorname{tr}\\left(\\Sigma\\right)-2n\\operatorname{tr}\\left(\\Sigma\\bar{W}\\right)+n(n+1)\\operatorname{tr}\\left(\\Sigma\\bar{W}^{\\top}\\bar{W}\\right)+n(\\operatorname{tr}\\left(\\Sigma\\right)+\\sigma^{2})\\operatorname{tr}\\left(\\bar{W}\\bar{W}^{\\top}\\right)+\\sigma^{2},}\\\\ &{\\qquad=M-2n\\operatorname{tr}\\left(\\Sigma\\bar{W}\\right)+n(n+1)\\operatorname{tr}\\left(\\Sigma\\bar{W}^{\\top}\\bar{W}\\right)+n M\\operatorname{tr}\\left(\\bar{W}\\bar{W}^{\\top}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $M:=\\mathbf{tr}\\left(\\Sigma\\right)+\\sigma^{2}$ . Setting $\\nabla_{\\bar{\\boldsymbol{W}}}\\mathcal{L}(\\boldsymbol{W})=0$ returns ", "page_idx": 24}, {"type": "equation", "text": "$$\n-2n\\cdot\\Sigma+2n(n+1)\\cdot\\Sigma\\bar{W}+2n M\\bar{W}=0\\Longrightarrow\\bar{W}_{\\star}=\\left((n+1)I+M\\Sigma^{-1}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nW_{\\star}=\\Sigma_{x}^{-1/2}\\left((n+1)I+M\\Sigma^{-1}\\right)^{-1}\\Sigma_{x}^{-1/2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\star}=\\mathcal{L}(W_{\\star})=M-n\\sf t r\\left(((n+1)\\Sigma^{-1}+M\\Sigma^{-2})^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "B.3 Retrieval Augmented Generation with $\\alpha$ Correlation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we consider the retrieval augmented generation (RAG) linear model similar to (9), where we first draw the query vector $\\pmb{x}$ and task vector $_\\beta$ via ", "page_idx": 24}, {"type": "equation", "text": "$$\nx\\sim N(0,I)\\ \\ \\ \\mathrm{and}\\ \\ \\ \\beta\\sim N(0,I).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We then draw data $(\\pmb{x}_{i})_{i=1}^{n}$ to be used in-context according to the rule corr_coe $\\mathbf{\\Delta}^{\\cdot}(\\pmb{x},\\pmb{x}_{i})\\geq\\alpha\\geq0$ . Hence, for $i\\le n$ we sample ", "page_idx": 24}, {"type": "equation", "text": "$$\nx_{i}\\mid x\\sim N(\\alpha x,\\gamma^{2}I),\\quad\\xi_{i}\\sim N(0,\\sigma^{2})\\quad{\\mathrm{and}}\\quad y_{i}=x_{i}^{\\top}\\beta+\\xi_{i},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which results in (9) by setting $\\gamma^{2}=1-\\alpha^{2}$ . ", "page_idx": 24}, {"type": "text", "text": "Theorem 4 (Extended version of Theorem 2) Consider linear model as defined in (35). Recap the objective from (5a) and let $W_{\\star}:=$ arg minW $\\mathcal{L}_{P G D}(W)$ , and $\\begin{array}{r}{\\mathcal{L}_{\\star}=\\mathcal{L}_{P G D}(W_{\\star})}\\end{array}$ . Then $W_{\\star}$ and $\\mathcal{L}_{\\star}$ satisfy ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{W_{\\star}=c I}&{{}}&{a n d\\;\\;\\;\\;\\;\\;\\;\\mathcal{L}_{\\star}=d+\\sigma^{2}-c n d(\\alpha^{2}(d+2)+\\gamma^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\nc={\\frac{\\alpha^{2}(d+2)+\\gamma^{2}}{\\alpha^{4}n(d+2)(d+4)+\\alpha^{2}\\gamma^{2}(d+2)(d+2n+3)+\\gamma^{4}(d+n+1)+\\sigma^{2}(\\alpha^{2}(d+2)+\\gamma^{2})}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Suppose $\\alpha=O\\left(1/\\sqrt{d}\\right),\\,d/n=O\\left(1\\right)$ and $d$ is sufficiently large. Let $\\kappa=\\alpha^{2}d+1$ and $\\gamma^{2}=1-\\alpha^{2}$ . Then $W_{\\star}$ and $\\mathcal{L}_{\\star}$ have approximate forms ", "page_idx": 25}, {"type": "equation", "text": "$$\nW_{\\star}\\approx\\frac{1}{\\kappa n+d+\\sigma^{2}}I\\qquad a n d\\qquad\\mathcal{L}_{\\star}\\approx d+\\sigma^{2}-\\frac{\\kappa n d}{\\kappa n+d+\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Here, for clean notation and without loss of generality, we define and rewrite (35) via ", "page_idx": 25}, {"type": "equation", "text": "$$\ng_{i}\\sim\\mathcal{N}(0,I),\\quad\\xi_{i}\\sim\\mathcal{N}(0,\\sigma^{2})\\quad\\mathrm{and}\\quad x_{i}=\\alpha x+\\gamma g_{i},\\quad y_{i}=(\\alpha x+\\gamma g_{i})^{\\top}\\beta+\\xi_{i}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(W)=\\mathbb{E}\\left[(y-g(Z))^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[\\left(x^{\\top}\\beta+\\xi-x^{\\top}W X^{\\top}(X\\beta+\\xi)\\right)^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)^{2}+2(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)(\\xi-x^{\\top}W X^{\\top}\\xi)+(\\xi-x^{\\top}W X^{\\top}\\xi)^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)^{2}\\right]+\\mathbb{E}\\left[(x^{\\top}W X^{\\top}\\xi)^{2}\\right]+\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To begin with, let ", "page_idx": 25}, {"type": "equation", "text": "$$\nN_{1}=\\operatorname{tr}\\left(W\\right)^{2}+\\operatorname{tr}\\left(W W^{\\top}\\right)+\\operatorname{tr}\\left(W^{2}\\right),\\quad N_{2}=\\operatorname{tr}\\left(W W^{\\top}\\right),\\quad{\\mathrm{and}}\\quad N_{3}=\\operatorname{tr}\\left(W\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We first focus on the second term in (38) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(\\boldsymbol{x}^{\\top}W\\boldsymbol{X}^{\\top}\\xi)^{2}\\right]=\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\xi_{i}\\boldsymbol{x}^{\\top}W(\\alpha\\boldsymbol{x}+\\gamma\\boldsymbol{g}_{i})\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=n\\sigma^{2}\\,\\mathbb{E}\\left[\\boldsymbol{x}^{\\top}W(\\alpha\\boldsymbol{x}+\\gamma\\boldsymbol{g})(\\alpha\\boldsymbol{x}+\\gamma\\boldsymbol{g})^{\\top}W^{\\top}\\boldsymbol{x}\\right]}\\\\ &{\\qquad\\qquad\\qquad=n\\sigma^{2}\\left(\\alpha^{2}\\,\\mathbb{E}[\\boldsymbol{x}^{\\top}W\\boldsymbol{x}\\boldsymbol{x}^{\\top}W^{\\top}\\boldsymbol{x}]+\\gamma^{2}\\,\\mathbb{E}[\\boldsymbol{x}^{\\top}W\\boldsymbol{g}\\boldsymbol{g}^{\\top}W^{\\top}\\boldsymbol{x}]\\right)}\\\\ &{\\qquad\\qquad\\qquad=n\\sigma^{2}\\left(\\alpha^{2}N_{1}+\\gamma^{2}N_{2}\\right).\\qquad\\qquad\\mathrm{(It~follows~}(27)\\mathrm{~and~in}\\mathrm{)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, the first term in (38) can be decomposed into ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)^{2}\\right]=\\underbrace{\\mathbb{E}\\left[(x^{\\top}\\beta)^{2}\\right]}_{(a)}+\\underbrace{\\mathbb{E}\\left[(x^{\\top}W X^{\\top}X\\beta)^{2}\\right]}_{(b)}-2\\underbrace{\\mathbb{E}\\left[x^{\\top}\\beta x^{\\top}W X^{\\top}X\\beta\\right]}_{(c)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In the following, we consider solving $(a){-}(c)$ sequentially. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(a):}&{{}\\mathbb{E}\\left[(\\pmb{x}^{\\top}\\pmb{\\beta})^{2}\\right]=d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle b\\rangle:\\;\\operatorname{\\mathbb{E}}\\bigg[\\big(x^{\\top}W x^{\\top}X\\beta\\big)^{2}\\bigg]}\\\\ &{=\\operatorname{\\mathbb{E}}\\bigg[\\bigg(x^{\\top}W\\sum_{i=1}^{n}(x+\\gamma\\varepsilon_{i})(\\alpha x+\\gamma\\varepsilon_{i})^{\\gamma}\\beta\\bigg)^{2}\\bigg]}\\\\ &{=\\operatorname{\\mathbb{E}}\\bigg[\\bigg(\\underset{i=1}{\\overset{.}{\\sum}}x^{\\top}W(\\alpha^{2}x x^{\\top}+\\gamma^{2}g\\delta(\\tilde{\\alpha}_{i}^{\\;\\dagger}+\\alpha\\gamma g\\delta(\\tilde{\\alpha})^{\\top}+\\alpha\\gamma g\\delta^{\\top})\\beta\\bigg)^{2}\\bigg]}\\\\ &{=\\operatorname{\\mathbb{E}}\\bigg[\\bigg(\\underset{i=1}{\\overset{.}{\\sum}}x^{\\top}W(\\alpha^{2}x x^{\\top}\\beta)^{2}+\\gamma^{\\frac{2}{2}}\\mathbb{E}\\bigg[\\bigg(\\underset{i=1}{\\overset{.}{\\sum}}x^{\\top}W g\\delta^{\\top}\\beta\\bigg)^{2}\\bigg]+\\alpha^{2}\\gamma^{2}\\mathbb{E}\\bigg[\\bigg(\\underset{i=1}{\\overset{.}{\\sum}}x^{\\top}W x\\varepsilon_{i}^{\\top}\\beta\\bigg)^{2}\\bigg]+\\alpha^{2}\\gamma^{2}\\mathbb{E}\\bigg[\\bigg(\\underset{i=1}{\\overset{.}{\\sum}}x^{\\top}W g\\delta^{\\top}\\beta\\bigg)^{2}\\bigg]}\\\\ &{\\quad+2\\alpha^{2}\\gamma^{2}\\pi^{2}\\mathbb{E}\\bigg[\\bigg(\\underset{i=1}{\\overset{.}{\\sum}}x^{\\top}W x^{\\top}\\beta\\beta^{\\top}g\\varepsilon^{\\top}W x^{\\top}\\beta+2\\alpha^{\\gamma}\\gamma^{2}\\mathbb{E}\\bigg[X^{\\top}W\\varepsilon^{\\top}f\\delta\\alpha^{\\gamma}g\\varepsilon^{\\top}\\beta\\bigg]}\\\\ &{=\\big(\\alpha^{4}\\gamma^{2}(d+4)N+\\gamma^{4}n(d+n+1)N\\big)+\\big(\\alpha^{2}\\gamma^{2}n(d+1)N_{1}+\\alpha^{2}\\gamma^{2}n(d+2)N_{2}\\big)+\\big(2\\alpha^{2}\\gamma^{2}n(1+2\\alpha)^{2}\\big)+2\\gamma^{2}N_{1}+2\\alpha^{2}\\gamma^{2}n\\mathrm{N}_{1}\\bigg)}\\\\ &{=\\Big(\\alpha^{4 \n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(c):}&{\\;\\mathbb{E}\\left[x^{\\top}\\beta x^{\\top}W X^{\\top}X\\beta\\right]=\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{n}x^{\\top}\\beta x^{\\top}W(\\alpha x+\\gamma g_{i})(\\alpha x+\\gamma g_{i})^{\\top}\\beta\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{n}x^{\\top}\\beta x^{\\top}W(\\alpha^{2}x x^{\\top}+\\gamma^{2}g_{i}g_{i}^{\\top}+\\alpha\\gamma x g_{i}^{\\top}+\\alpha\\gamma g_{i}x^{\\top})\\beta\\right]}\\\\ &{=\\alpha^{2}n\\mathbb{E}\\left[x^{\\top}\\beta x^{\\top}W x x^{\\top}\\beta\\right]+\\gamma^{2}n\\mathbb{E}\\left[x^{\\top}\\beta x^{\\top}W g g^{\\top}\\beta\\right]}\\\\ &{=\\alpha^{2}n(d+2)\\mathrm{tr}\\left(W\\right)+\\gamma^{2}n\\mathrm{tr}\\left(W\\right)}\\\\ &{=\\left(\\alpha^{2}n(d+2)+\\gamma^{2}n\\right)N_{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here, $(b)$ utilizes the 4\u2019th and $\\acute{6}$ th moment results (27) and (30) and we define ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{A_{1}=\\alpha^{4}n^{2}(d+4)+\\alpha^{2}\\gamma^{2}n(2n+d+2)}}\\\\ {{A_{2}=\\alpha^{2}\\gamma^{2}n(d+2)+\\gamma^{4}n(d+n+1)}}\\\\ {{A_{3}=\\alpha^{2}n(d+2)+\\gamma^{2}n.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then combining all together results in ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}(W)=A_{1}N_{1}+A_{2}N_{2}-2A_{3}N_{3}+n\\sigma^{2}(\\alpha^{2}N_{1}+\\gamma^{2}N_{2})+d+\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To find the optimal solution, set $\\nabla{\\mathcal{L}}(W)=0$ and we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\nA_{1}\\nabla N_{1}+A_{2}\\nabla N_{2}-2A_{3}\\nabla N_{3}+n\\sigma^{2}(\\alpha^{2}\\nabla N_{1}+\\gamma^{2}\\nabla N_{2})=0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla N_{1}=\\nabla\\left(\\mathbf{tr}\\left(W\\right)^{2}+\\mathbf{tr}\\left(W W^{\\top}\\right)+\\mathbf{tr}\\left(W^{2}\\right)\\right)=2\\mathbf{tr}\\left(W\\right)I+2W+2W^{\\top}}\\\\ &{\\nabla N_{2}=\\nabla\\mathbf{tr}\\left(W W^{\\top}\\right)=2W}\\\\ &{\\nabla N_{3}=\\nabla\\mathbf{tr}\\left(W\\right)=I.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, (39) returns ", "page_idx": 26}, {"type": "text", "text": "$2A_{1}\\left(\\mathbf{tr}\\left(W\\right)I+W+W^{\\top}\\right)+2A_{2}W-2A_{3}+2n\\sigma^{2}(\\alpha^{2}(\\mathbf{tr}\\left(W\\right)I+W+W^{\\top})+\\gamma^{2}W)I=0,$ (40) which implies that the optimal solution $W_{\\star}$ has the form of $c I$ for some constant $c$ . Then suppose $W_{\\star}=c I$ , we have $\\operatorname{tr}(\\boldsymbol{W})=c d$ and (40) returns ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{{2A_{1}(d+2)c I+2A_{2}c I-2A_{3}I+2n\\sigma^{2}(\\alpha^{2}(d+2)c I+\\gamma^{2}c I)=0}}\\\\ {{\\Longrightarrow c=\\displaystyle\\frac{A_{3}}{A_{1}(d+2)+A_{2}+n\\sigma^{2}(\\alpha^{2}(d+2)+\\gamma^{2})}}}\\\\ {{=\\displaystyle\\frac{\\alpha^{2}(d+2)+\\gamma^{2}}{\\alpha^{4}n(d+2)(d+4)+\\alpha^{2}\\gamma^{2}(d+2)(d+2n+3)+\\gamma^{4}(d+n+1)+\\sigma^{2}(\\alpha^{2}(d+2)+\\gamma^{2})}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then the optimal loss is obtained by setting $W_{\\star}=c I$ and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\star}=\\mathcal{L}(W_{\\star})=A_{1}c^{2}d(d+2)+A_{2}c^{2}d-2A_{3}c d+n\\sigma^{2}c^{2}d(\\alpha^{2}(d+2)+\\gamma^{2})+d+\\sigma^{2}}\\\\ &{\\hphantom{=}=c^{2}d\\left(A_{1}(d+2)+A_{2}+n\\sigma^{2}(\\alpha^{2}(d+2)+\\gamma^{2})\\right)-2A_{3}c d+d+\\sigma^{2}}\\\\ &{\\hphantom{={\\angle}=}=d+\\sigma^{2}-A_{3}c d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It completes the proof of (36). Now if assuming $\\alpha=O\\left(1/\\sqrt{d}\\right),d/n=O\\left(1\\right)$ and sufficiently large dimension $d$ , we have the approximate ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{c\\approx{\\frac{\\alpha^{2}d+1}{\\alpha^{4}d^{2}n+\\alpha^{2}d(d+2n)+(d+n)+\\sigma^{2}(\\alpha^{2}d+1)}}}\\\\ &{\\,\\,\\,\\,={\\frac{\\alpha^{2}d+1}{(\\alpha^{2}d+1)^{2}n+(\\alpha^{2}d+1)d+\\sigma^{2}(\\alpha^{2}d+1)}}}\\\\ &{\\,\\,\\,\\,={\\frac{1}{(\\alpha^{2}d+1)n+d+\\sigma^{2}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\star}\\approx d+\\sigma^{2}-\\frac{(\\alpha^{2}d+1)n d}{(\\alpha^{2}d+1)n+d+\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "B.4 Task-feature Alignment with $\\alpha$ Correlation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we consider the task-feature alignment data model similar to (11), where we first draw task vector $_\\beta$ via ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\beta\\sim N(0,I).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we generate examples $(\\pmb{x}_{i},\\y_{i})_{i=1}^{n+1}$ according to the rule corr_coe $\\mathbf{\\dot{\\rho}}(\\pmb{x}_{i},\\beta)\\geq\\alpha\\geq0$ via ", "page_idx": 27}, {"type": "equation", "text": "$$\nx_{i}\\,\\big|\\,\\beta\\sim{\\cal N}(\\alpha\\beta,I),\\quad\\xi_{i}\\sim{\\cal N}(0,\\sigma^{2})\\quad\\mathrm{and}\\quad y_{i}=\\gamma\\cdot x_{i}^{\\top}\\beta+\\xi_{i},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which results in (11) by setting $\\gamma^{2}=1/(\\alpha^{2}d+1)$ . ", "page_idx": 27}, {"type": "text", "text": "Theorem 5 (Extended version of Theorem 3) Consider linear model as defined in (41). Recap the objective from (5a) and let $W_{\\star}:=$ arg minW $\\mathcal{L}_{P G D}(W)$ , and $\\begin{array}{r}{\\mathcal{L}_{\\star}=\\mathcal{L}_{P G D}(W_{\\star})}\\end{array}$ . Then $W_{\\star}$ and $\\mathcal{L}_{\\star}$ satisfy ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{W_{\\star}=c I}&{{}}&{a n d\\;\\;\\;\\;\\;\\;\\;\\mathcal{L}_{\\star}=d\\gamma^{2}(\\Delta_{0}\\alpha^{2}+1)+\\sigma^{2}-c n d\\gamma^{2}(\\Delta_{1}\\alpha^{4}+2\\Delta_{0}\\alpha^{2}+1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\nc=\\frac{\\Delta_{1}\\alpha^{4}+2\\Delta_{0}\\alpha^{2}+1}{\\Delta_{2}\\alpha^{6}+\\Delta_{3}\\alpha^{4}+\\Delta_{4}\\alpha^{2}+(d+n+1)+\\sigma^{2}(\\Delta_{0}\\alpha^{4}+2\\alpha^{2}+1)/\\gamma^{2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\Delta_{0}=d+2\\right.}\\\\ &{\\left.\\left\\{\\Delta_{1}=(d+2)(d+4)\\right.\\right.}\\\\ &{\\left.\\left\\{\\Delta_{2}=(d+2)(d+4)(d+6)n\\right.}\\\\ &{\\left.\\left|\\Delta_{3}=(d+2)(d+4)(3n+4)\\right.\\right.}\\\\ &{\\left.\\left.\\Delta_{4}=(d+2)(3n+d+3)+(d+8).\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Suppose $\\alpha=O\\left(1/\\sqrt{d}\\right),\\,d/n=O\\left(1\\right)$ and $d$ is sufficiently large. Let $\\kappa=\\alpha^{2}d+1$ and $\\gamma^{2}=1/\\kappa.$ . Then $W_{\\star}$ and $\\mathcal{L}_{\\star}$ have approximate forms ", "page_idx": 27}, {"type": "equation", "text": "$$\nW_{\\star}\\approx\\frac{1}{\\kappa n+(d+\\sigma^{2})/\\kappa}\\qquad a n d\\qquad\\mathcal{L}_{\\star}\\approx d+\\sigma^{2}-\\frac{\\kappa n d}{\\kappa n+(d+\\sigma^{2})/\\kappa}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Here, for clean notation and without loss of generality, we define and rewrite (41) via ", "page_idx": 27}, {"type": "text", "text": "$g_{i}\\sim N(0,I),\\quad\\xi_{i}\\sim N(0,\\sigma^{2})\\quad\\mathrm{and}\\quad x_{i}=\\alpha\\beta+g_{i},\\quad y_{i}=\\gamma x_{i}^{\\top}\\beta+\\xi_{i}=\\gamma\\cdot(\\alpha\\beta+g_{i})^{\\top}\\beta+\\xi_{i}.$ Recap the loss function from (5a), we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(W)=\\mathbb{E}\\left[(\\ y-g(Z))^{2}\\right]}\\\\ &{\\qquad\\quad=\\mathbb{E}\\left[\\left(\\gamma x^{\\top}\\beta+\\xi-x^{\\top}W X^{\\top}(\\gamma X\\beta+\\xi)\\right)^{2}\\right]}\\\\ &{\\qquad\\quad=\\mathbb{E}\\left[\\gamma^{2}(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)^{2}+2\\gamma(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)(\\xi-x^{\\top}W X^{\\top}\\xi)+(\\xi-x^{\\top}W X^{\\top}\\xi)^{2}\\right]}\\\\ &{\\qquad\\quad=\\gamma^{2}\\mathbb{E}\\left[(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)^{2}\\right]+\\mathbb{E}\\left[(x^{\\top}W X^{\\top}\\xi)^{2}\\right]+\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similar to Appendix B.3, to begin with, let ", "page_idx": 27}, {"type": "equation", "text": "$$\nN_{1}=\\operatorname{tr}\\left(W\\right)^{2}+\\operatorname{tr}\\left(W W^{\\top}\\right)+\\operatorname{tr}\\left(W^{2}\\right),\\quad N_{2}=\\operatorname{tr}\\left(W W^{\\top}\\right),\\quad{\\mathrm{and}}\\quad N_{3}=\\operatorname{tr}\\left(W\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and additionally, given $\\mathbf{A}_{W}=W\\odot I$ , let ", "page_idx": 27}, {"type": "equation", "text": "$$\nN_{4}=3\\mathrm{tr}\\left(\\Lambda_{W}^{2}\\right)+(d+4)\\mathrm{tr}\\left(W W^{\\top}\\right)+\\mathrm{tr}\\left(W^{2}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We first focus on the second term in (44) ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(\\mathbf{x}^{\\top}W X^{\\top}\\boldsymbol{\\xi})^{2}\\right]=\\mathbb{E}\\left[\\left((\\alpha\\beta+g)^{\\top}W\\sum_{i=1}^{n}\\xi_{i}(\\alpha\\beta+g_{i})\\right)^{2}\\right]}\\\\ &{\\quad=\\,n\\sigma^{2}\\,\\mathbb{E}\\left[\\left((\\alpha\\beta+g)^{\\top}W(\\alpha\\beta+g^{\\prime})\\right)^{2}\\right]}\\\\ &{\\quad=\\,n\\sigma^{2}\\left(\\alpha^{4}\\,\\mathbb{E}\\left[(\\beta^{\\top}W\\beta)^{2}\\right]+2\\alpha^{2}\\,\\mathbb{E}\\left[(\\beta^{\\top}W g^{\\prime})^{2}\\right]+\\mathbb{E}\\left[(g^{\\top}W g^{\\prime})^{2}\\right]\\right)}\\\\ &{\\quad=\\,n\\sigma^{2}\\left(\\alpha^{4}\\left(\\mathbf{tr}\\left(W\\right)^{2}+\\mathbf{tr}\\left(W^{2}\\right)+\\mathbf{tr}\\left(W W^{\\top}\\right)\\right)+(2\\alpha^{2}+1)\\mathbf{tr}\\left(W W^{\\top}\\right)\\right)}\\\\ &{\\quad=\\,n\\sigma^{2}\\left(\\alpha^{4}N_{1}+(2\\alpha^{2}+1)N_{2}\\right)\\,.\\quad\\mathrm{(~It~follows~(27)~and~independence~of~}\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, the first term of (44) (omitting $\\gamma^{2}$ ) returns the following decomposition: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\Big[(x^{\\top}\\beta-x^{\\top}W X^{\\top}X\\beta)^{2}\\Big]=\\mathbb{E}\\Big[((\\alpha\\beta+g)^{\\top}(\\beta-W X^{\\top}X\\beta))^{2}\\Big]}&{}\\\\ {=\\mathbb{E}\\Bigg[\\Big(\\alpha\\beta^{\\top}\\beta-\\alpha\\beta^{\\top}W X^{\\top}X\\beta+g^{\\top}\\beta-g^{\\top}W X^{\\top}X\\beta\\Big)^{2}\\Bigg]}\\\\ {=\\alpha^{2}\\mathbb{E}[(\\beta^{\\top}\\beta)^{2}]+\\alpha^{2}\\mathbb{E}[(\\beta^{\\top}W X^{\\top}X\\beta)^{2}]+\\mathbb{E}[(g^{\\top}W X^{\\top}X\\beta)^{2}]+\\mathbb{E}[(g^{\\top}W X^{\\top}X\\beta)^{2}]}\\\\ {~}&{~~-~2\\alpha^{2}\\mathbb{E}[\\beta^{\\top}\\beta\\beta^{\\top}W X^{\\top}X\\beta]-2\\mathbb{E}[\\beta^{\\top}g g^{\\top}W X^{\\top}X\\beta]}\\\\ {=\\alpha^{2}d(d+2)+\\alpha^{2}\\underbrace{\\mathbb{E}[(\\beta^{\\top}W X^{\\top}X\\beta)^{2}]}_{(a)}+d+\\underbrace{\\mathbb{E}[(g^{\\top}W X^{\\top}X\\beta)^{2}]}_{(b)}}\\\\ {~}&{~~~-~2\\alpha^{2}\\underbrace{\\mathbb{E}[\\beta^{\\top}\\beta\\beta^{\\top}W X^{\\top}X\\beta]}_{(c)}-2\\underbrace{\\mathbb{E}[\\beta^{\\top}g g^{\\top}W X^{\\top}X\\beta]}_{(d)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Consider solving $(a)-(d)$ sequentially as follows: ", "page_idx": 28}, {"type": "text", "text": "To begin with, we use the following decomposition for all $(a)-(d)$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{X^{\\top}X\\beta=\\sum_{i=1}^{n}x_{i}x_{i}^{\\top}\\beta}}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}(\\alpha\\beta+g_{i})(\\alpha\\beta+g_{i})^{\\top}\\beta}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\alpha^{2}\\beta\\beta^{\\top}\\beta+\\alpha\\beta g_{i}^{\\top}\\beta+\\alpha g_{i}\\beta^{\\top}\\beta+g_{i}g_{i}^{\\top}\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(a):\\quad\\mathbb{E}\\{Q^{2}W^{2}X^{2}B^{2}\\}}\\\\ &{=\\underset{(a)}{=}\\Bigg[\\underset{(c)}{\\overset{(a)}{\\sum}}e^{i\\alpha_{2}t}\\Phi^{\\top}\\beta\\Phi^{\\top}+q\\Phi^{T}\\Phi^{\\top}\\beta\\Phi^{\\top}+q\\partial_{a}^{T}\\Phi^{\\top}\\beta\\Phi^{\\top}\\beta+\\beta^{a}\\nabla_{B}\\{a,b,c,b^{\\top}\\}\\Bigg]\\Bigg]}\\\\ &{=a^{2}\\underset{(b)}{=}\\Bigg[\\underset{(c)}{\\overset{(a)}{\\sum}}\\Big[\\Big(\\mu\\nabla_{B}\\mu\\Phi^{\\top}\\beta\\Big)^{2}+\\tau^{2}\\Big]\\Bigg[\\underset{(c)}{\\overset{(a)}{\\sum}}\\Big(\\mu\\nabla_{B}\\mu\\Phi^{\\top}\\beta\\Big)^{2}\\Bigg]+a^{2}\\mathbb{E}\\Bigg[\\underset{(c)}{\\overset{(b)}{\\sum}}\\Big(\\mu\\nabla_{B}\\mu\\Phi^{\\top}\\beta\\Bigg)^{2}\\Bigg]+\\mathbb{E}\\Bigg[\\underset{(c)}{\\overset{(c)}{\\sum}}\\Big(\\mu\\nabla_{B}\\mu\\nabla_{B}\\beta\\Big)^{2}\\Bigg]\\Bigg]}\\\\ &{\\underset{(a)}{=}+2\\alpha^{2}B^{2}\\Bigg[\\underset{(b)}{\\overset{(c)}{\\sum}}\\Big(\\mu^{T}\\Phi^{\\top}\\beta\\Phi^{\\top}\\beta\\Phi^{\\top}\\beta\\underset{(c)}{\\leq}\\hat{\\geq}\\Big[\\Big(\\mu\\nabla_{B}\\mu\\Phi^{\\top}\\beta\\Big)^{2}+2\\lambda^{2}\\underset{(b)}{\\overset{(a)}{\\sum}}\\Big[\\underset{(b)}{\\overset{(b)}{\\sum}}\\Big(\\mu\\nabla_{B}\\mu\\Phi^{\\top}\\beta\\Big)^{2}\\Big]+\\mathbb{E}\\Bigg[\\underset{(c)}{\\overset{(c)}{\\sum}}\\Big(\\mu\\nabla_{B}\\mu\\nabla_{B}\\beta\\Big)^{2}\\Bigg]\\Bigg]}\\\\ &{=a^{2}\\pi^{2}B\\Bigg[\\Big(\\mu\\nabla_{B}\\mu\\Phi^{\\top}\\beta\\Big)^{2}+\\lambda^{2}\\pi^{2}\\Big]\\Big[\\Big(\\mu\\nabla_{B}\\mu\\nabla_{B}\\beta\\Big)^{2}\\Bigg]+a^{2}\\pi n\\Bigg[\\Big(\\mu\\nabla_{B}\\mu\\Big)^{2}\\beta\\Bigg]+\\mathbb{E}\\Bigg[\\underset \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where (45) and (47) utilize (30) and (32), and (46) is obtained via ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\beta^{\\top}W g_{i}g_{i}^{\\top}\\beta\\right)^{2}\\right]=n\\,\\mathbb{E}\\left[\\left(\\beta^{\\top}W g^{\\prime}g^{\\prime^{\\top}}\\beta\\right)^{2}\\right]+n(n-1)\\,\\mathbb{E}\\left[\\beta^{\\top}W g^{\\prime}g^{\\prime^{\\top}}\\beta\\beta^{\\top}W g^{\\prime\\prime}g^{\\prime^{\\prime\\prime}}\\beta\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=n N_{4}+n(n-1)N_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which follows (27) and (28). ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle b\\rangle}&{:=\\frac{\\mathcal{E}\\big[g^{\\prime}W X^{\\prime}X^{\\prime}B^{\\prime}\\big]}{2}}\\\\ &{=\\frac{\\mathcal{E}\\big[\\bigg(\\sum_{i}^{\\mathcal{A}}z^{\\mathcal{A}}\\big-\\mathcal{E}W\\beta^{\\mathcal{A}}\\big)^{2}\\Big]}{\\sqrt{\\pi}}+\\alpha^{\\mathcal{E}}\\mathbb{W}\\beta\\xi_{i}^{\\mathcal{A}}\\beta+\\alpha^{\\mathcal{E}}W\\xi_{i}\\beta^{\\mathcal{E}}\\beta+\\mathcal{E}^{\\mathsf{W}}g^{\\mathcal{A}}\\beta\\Bigg]}\\\\ &{=\\alpha^{\\mathcal{A}}\\frac{2}{\\mathcal{E}^{\\prime}}\\bigg[\\bigg(\\sum_{i}^{\\mathcal{A}}\\Big(\\mathcal{G}\\cdot W\\beta^{\\mathcal{B}}\\Big)^{2}\\bigg)^{2}+\\alpha^{2}\\Bigg[\\bigg(\\sum_{i}^{\\mathcal{E}}\\mathbb{W}\\beta_{i}\\xi_{i}^{\\mathcal{B}}\\bigg)^{2}\\Bigg]+\\alpha^{2}\\Bigg[\\bigg(\\sum_{i}^{\\mathcal{E}}\\mathbb{W}\\xi_{i}\\mathbb{W}\\beta^{\\mathcal{E}}\\bigg)^{2}\\Bigg]+\\mathbb{E}\\bigg[\\bigg(\\sum_{i}^{\\mathcal{E}}\\mathbb{W}\\xi_{i}\\mathbb{W}\\beta_{i}^{\\mathcal{E}}\\bigg)^{2}\\Bigg]}\\\\ &{\\quad+2\\alpha^{2}\\alpha^{2}\\mathbb{E}\\bigg[\\bigg(\\sum_{i}^{\\mathcal{E}}\\mathbb{W}\\beta\\beta^{\\mathcal{A}}\\mathbb{W}\\beta_{i}^{\\mathcal{E}}\\mathbb{W}\\mathbb{W}\\beta_{i}^{\\mathcal{E}}\\bigg)^{2}\\Bigg]+2\\alpha^{2}\\mathbb{E}\\bigg[\\bigg\\langle\\sum_{i}^{\\mathcal{E}}\\mathbb{W}\\beta_{i}\\mathbb{W}\\beta^{\\mathcal{E}}\\beta^{\\mathcal{E}}\\bigg.\\bigg.\\beta}\\\\ &{\\bigg.\\bigg.\\bigg.=\\alpha^{\\mathcal{A}}\\frac{2}{\\mathcal{E}}\\bigg[\\bigg(\\mathcal{G}\\cdot W\\beta^{\\mathcal{B}}\\Big)^{2}\\bigg)^{2}\\bigg]+\\alpha^{2}\\mathbb{E}\\bigg[\\big(\\mathcal{G}\\cdot W\\beta^{\\mathcal{A}}\\big)^{2}\\bigg]\\Bigg[\\bigg(\\alpha^{\\mathcal{A}}\\mathbb{W}\\mathbb{W}\\beta^{\\mathcal{E}}\\bigg)^{2}\\bigg]+\\alpha^{2}\\Bigg[\\bigg(\\sum_{i}^{\\mathcal{E} \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where (49) and (50) are obtained using (27), (30) and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}{g^{\\top}W g_{i}g_{i}^{\\top}\\beta}\\right)^{2}\\right]=n\\,\\mathbb{E}\\left[\\left({g^{\\top}W g^{\\prime}g^{\\prime}^{\\top}\\beta}\\right)^{2}\\right]+n(n-1)\\,\\mathbb{E}\\left[{g^{\\top}W g^{\\prime}g^{\\prime\\top}\\beta g^{\\top}W g^{\\prime\\prime}g^{\\prime\\prime}^{\\top}\\beta}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=n(d+2)N_{2}+n(n-1)N_{2}=n(n+d+1)N_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(c):}&{\\;\\mathbb{E}\\left[\\beta^{7}\\beta\\beta^{7}W X^{\\top}X\\beta\\right]}\\\\ &{=n\\,\\mathbb{E}\\left[\\beta^{7}\\beta\\beta^{7}W(\\alpha\\beta+g^{\\prime})(\\alpha\\beta+g^{\\prime})^{\\top}\\beta\\right]}\\\\ &{=\\alpha^{2}n\\,\\mathbb{E}\\left[\\beta^{7}\\beta\\beta^{7}W\\beta\\beta^{7}\\beta\\right]+n\\,\\mathbb{E}\\left[\\beta^{7}\\beta\\beta^{7}W g^{\\prime}g^{\\prime\\top}\\beta\\right]}\\\\ &{=\\alpha^{2}n(d+2)(d+4)\\mathrm{tr}\\left(W\\right)+n(d+2)\\mathrm{tr}\\left(W\\right)}\\\\ &{=\\left(\\alpha^{2}n(d+2)(d+4)+n(d+2)\\right)N_{3}}\\\\ &{=B_{4}N_{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{(d):\\ }&{\\mathbb{E}\\left[\\beta^{\\top}g\\mathbf{g}^{\\top}W X^{\\top}X\\beta\\right]}\\\\ &{=n\\,\\mathbb{E}\\left[\\beta^{\\top}g\\mathbf{g}^{\\top}W(\\alpha\\beta+g^{\\prime})(\\alpha\\beta+g^{\\prime})^{\\top}\\beta\\right]}\\\\ &{=\\alpha^{2}n\\,\\mathbb{E}\\left[\\beta^{\\top}g\\mathbf{g}^{\\top}W\\beta\\beta^{\\top}\\beta\\right]+n\\,\\mathbb{E}\\left[\\beta^{\\top}g\\mathbf{g}^{\\top}W g^{\\prime}g^{\\prime}^{\\top}\\beta\\right]}\\\\ &{=\\alpha^{2}n(d+2)\\operatorname{tr}\\left(W\\right)+n\\mathbf{tr}\\left(W\\right)}\\\\ &{=\\left(\\alpha^{2}n(d+2)+n\\right)N_{3}}\\\\ &{=B_{5}N_{3}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here we define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{B_{1}=\\alpha^{2}n(d+4)(\\alpha^{2}n(d+6)+2n+3)+n(n-1)}}\\\\ {{B_{2}=\\alpha^{2}n(d+2)(d+4)}}\\\\ {{B_{3}=\\alpha^{2}n(d+2)(\\alpha^{2}n(d+4)+2n+d+3)+n(d+n-1)}}\\\\ {{B_{4}=\\alpha^{2}n(d+2)(d+4)+n(d+2)}}\\\\ {{B_{5}=\\alpha^{2}n(d+2)+n.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then combining all together results in ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{C}(W)=\\gamma^{2}\\left(\\alpha^{2}d(d+2)+d+\\alpha^{2}(B_{1}N_{1}+B_{2}N_{2}+n N_{4})+B_{3}N_{2}-2\\alpha^{2}B_{4}N_{3}-2B_{5}N_{3}\\right)+n\\sigma^{2}(\\alpha^{4}N_{1}+(2\\alpha^{2}+1)N_{2})+\\sigma^{2}}\\\\ &{}&{=\\gamma^{2}\\left(\\alpha^{2}B_{1}N_{1}+(\\alpha^{2}B_{2}+B_{3})N_{2}-2(\\alpha^{2}B_{4}+B_{5})N_{3}+\\alpha^{2}n N_{4}\\right)+n\\sigma^{2}(\\alpha^{4}N_{1}+(2\\alpha^{2}+1)N_{2})+\\gamma^{2}d\\left(\\alpha^{2}(d+2)+1\\right)+}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and differentiating it results in ", "page_idx": 30}, {"type": "equation", "text": "$$\n7\\mathcal{L}(W)=\\gamma^{2}\\left(\\alpha^{2}B_{1}\\nabla N_{1}+(\\alpha^{2}B_{2}+B_{3})\\nabla N_{2}-2(\\alpha^{2}B_{4}+B_{5})\\nabla N_{3}+\\alpha^{2}n\\nabla N_{4}\\right)+n\\sigma^{2}(\\alpha^{4}\\nabla N_{1}+(2\\alpha^{2}+1)\\nabla N_{2}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Similar to the proof in Appendix B.3, $W_{\\star}$ has the form of $W_{\\star}=c I$ and we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla N_{1}=\\nabla\\left(\\mathbf{tr}\\left(W\\right)^{2}+\\mathbf{tr}\\left(W W^{\\top}\\right)+\\mathbf{tr}\\left(W^{2}\\right)\\right)=2\\mathbf{tr}\\left(W\\right)I+2W+2W^{\\top}=2c(d+2)I}\\\\ &{\\nabla N_{2}=\\nabla\\mathbf{tr}\\left(W W^{\\top}\\right)=2W=2c I}\\\\ &{\\nabla N_{3}=\\nabla\\mathbf{tr}\\left(W\\right)=I}\\\\ &{\\nabla N_{4}=\\nabla\\left(3\\mathbf{tr}\\left(\\mathbf{A}_{W}^{2}\\right)+(d+4)\\mathbf{tr}\\left(W W^{\\top}\\right)+\\mathbf{tr}\\left(W^{2}\\right)\\right)}\\\\ &{\\qquad=6\\cdot\\mathrm{diag}\\left(\\mathbf{A}_{W}\\right)+2(d+4)W+2W^{\\top}}\\\\ &{\\qquad=2c(d+8)I.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, setting $\\nabla{\\mathcal{L}}(W)=0$ returns ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\prime\\left(2c(d+2)\\alpha^{2}B_{1}+2c(\\alpha^{2}B_{2}+B_{3})-2(\\alpha^{2}B_{4}+B_{5})+2c(d+8)\\alpha^{2}n\\right)+2c n\\sigma^{2}(\\alpha^{4}(d+2)+2\\alpha^{2}+1)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\implies c=\\frac{\\alpha^{2}B_{4}+B_{5}}{(d+2)\\alpha^{2}B_{1}+(\\alpha^{2}B_{2}+B_{3})+(d+8)\\alpha^{2}n+n\\sigma^{2}(\\alpha^{4}(d+2)+2\\alpha^{2}+1)/\\gamma^{2}}}}\\\\ &{}&{=\\frac{\\alpha^{4}n(d+2)(d+4)+2\\alpha^{2}n(d+2)+n}{\\alpha^{6}n^{2}(d+2)(d+4)+\\alpha^{4}n(d+2)(d+4)+\\alpha^{2}n(d+2)(4+2\\alpha^{2}+1)}}\\\\ &{}&{=\\frac{\\alpha^{4}(d+2)(d+4)+2\\alpha^{2}(d+2)+1}{\\alpha^{6}n(d+2)(d+4)(d+6)+\\alpha^{4}(d+2)(d+4)(3+4)+\\alpha^{2}(d+2)(3n+d+3)+(d+n+1)+\\alpha^{2}(\\alpha^{4}(d+2)+2\\alpha^{2}+1)/\\gamma^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then the optimal loss is obtained by setting $W_{\\star}=c I$ and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\star}=\\mathcal{L}(W_{\\star})=\\gamma^{2}d(\\alpha^{2}(d+2)+1)+\\sigma^{2}-\\gamma^{2}(\\alpha^{2}B_{4}+B_{5})c d.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It completes the proof of (42). Now if assuming $\\alpha=O\\left(1/\\sqrt{d}\\right),d/n=O(1),\\gamma^{2}=1/(\\alpha^{2}d+1)$ and sufficiently large dimension $d$ , we have the approximate ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{c\\approx\\frac{\\alpha^{4}d^{2}+2\\alpha^{2}d+1}{n\\alpha^{6}d^{3}+3n\\alpha^{4}d^{2}+(3n+d)\\alpha^{2}d+d+n+\\sigma^{2}(\\alpha^{4}d+2\\alpha^{2}+1)/\\gamma^{2}}}\\\\ {\\approx\\frac{(\\alpha^{2}d+1)^{2}}{n(\\alpha^{2}d+1)^{3}+d(\\alpha^{2}d+1)+\\sigma^{2}(\\alpha^{2}d+1)}}\\\\ {\\approx\\frac{1}{(\\alpha^{2}d+1)n+(d+\\sigma^{2})/(\\alpha^{2}d+1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\star}\\approx\\gamma^{2}d(\\alpha^{2}d+1)+\\sigma^{2}-\\frac{\\gamma^{2}(\\alpha^{2}d+1)^{2}n d}{(\\alpha^{2}d+1)n+(d+\\sigma^{2})/(\\alpha^{2}d+1)}}\\\\ &{\\quad=d+\\sigma^{2}-\\frac{(\\alpha^{2}d+1)n d}{(\\alpha^{2}d+1)n+(d+\\sigma^{2})/(\\alpha^{2}d+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "C Analysis of Low-Rank Parameterization ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "C.1 Proof of Lemma 3 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof. Recall the loss function from (34) ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\boldsymbol{W})=M-2n\\mathbf{tr}\\left(\\boldsymbol{\\Sigma}\\bar{\\boldsymbol{W}}\\right)+n(n+1)\\mathbf{tr}\\left(\\boldsymbol{\\Sigma}\\bar{\\boldsymbol{W}}^{\\top}\\bar{\\boldsymbol{W}}\\right)+n M\\mathbf{tr}\\left(\\bar{\\boldsymbol{W}}\\bar{\\boldsymbol{W}}^{\\top}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\Bar{W}\\,=\\,\\Sigma_{x}^{1/2}W\\Sigma_{x}^{1/2}$ , $\\pmb{\\Sigma}=\\pmb{\\Sigma}_{x}^{1/2}\\pmb{\\Sigma}_{\\beta}\\pmb{\\Sigma}_{x}^{1/2}$ and $M\\,=\\,\\mathbf{tr}\\left(\\Sigma\\right)+\\sigma^{2}$ . For any $\\bar{\\pmb{W}}$ , let us parameterize $\\bar{\\pmb{W}}=\\pmb{U}\\pmb{E}\\pmb{U}^{\\top}$ where $U\\in\\mathbb{R}^{d\\times r}$ denotes the eigenvectors of $\\bar{\\pmb{W}}$ and $E\\in\\mathbb{R}^{r\\times r}$ is a symmetric square ", "page_idx": 30}, {"type": "image", "img_path": "lYPAYmfQqm/tmp/54665f0cd6351b38ba976c54c2152ee16c7893482545c664fee3b2f16d1f7768.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 4: Further comparison for linear attention and H3. In (a) and ${\\bf(b)}$ , given maximum context lengths $n_{\\mathrm{max}}$ , we train linear attention and H3 models to minimize the average loss across all positions $n$ from 1 to $n_{\\mathrm{max}}$ . Averaged test risks are presented in (c). In (d), the task vector $_\\beta$ evolves gradually over the context positions $i\\leq n$ via $\\beta_{i}=(i/n)\\beta_{1}+(1-i/n)\\beta_{2}$ . In both scenarios, H3 outperforms linear attention benefiting from its additional convolutional filter (c.f. $\\textbf{\\emph{f}}$ in (2b)). Implementation details are discussed in Section 4. ", "page_idx": 31}, {"type": "text", "text": "matrix. We will first treat $U$ as fixed and optimize ${\\pmb E}$ . We will then optimize $U$ . Fixing $U$ , setting $\\bar{\\Sigma}={U}^{\\top}\\Sigma{U}$ , we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{L}(E)=M-2n\\mathrm{tr}\\left(\\bar{\\Sigma}E\\right)+n(n+1)\\mathrm{tr}\\left(\\bar{\\Sigma}E^{2}\\right)+n M\\mathrm{tr}\\left(E^{2}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Differentiating, we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n0.5n^{-1}\\nabla{\\mathcal{L}}(E)=-{\\bar{\\Sigma}}+(n+1){\\bar{\\Sigma}}E+M E.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Setting $\\nabla\\mathcal{L}(E)=0$ returns ", "page_idx": 31}, {"type": "equation", "text": "$$\nE_{\\star}=(M I+(n+1)\\bar{\\Sigma})^{-1}\\bar{\\Sigma}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let $\\bar{\\lambda}_{i}$ denote the $i^{\\star}$ th largest eigenvalue of $\\bar{\\Sigma}$ . Plugging in this value, we obtain the optimal risk as a function of $U$ is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{L}_{\\star}(U)=M-n\\cdot\\mathrm{tr}\\left(\\bar{\\Sigma}E_{\\star}\\right)=M-n\\cdot\\mathrm{tr}\\left((M I+(n+1)\\bar{\\Sigma})^{-1}\\bar{\\Sigma}^{2}\\right)}\\\\ &{}&{=M-n\\displaystyle\\sum_{i=1}^{r}\\frac{\\bar{\\lambda}_{i}^{2}}{(n+1)\\bar{\\lambda}_{i}+M}=M-n\\displaystyle\\sum_{i=1}^{r}\\frac{\\bar{\\lambda}_{i}}{n+1+M\\bar{\\lambda}_{i}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now observe that, the right hand side is strictly decreasing function of the eigenvalues $\\bar{\\lambda}_{i}$ of $\\bar{\\Sigma}=$ ${\\pmb U}^{\\top}{\\pmb\\Sigma}{\\pmb U}$ . Thus, to minimize ${\\mathcal{L}}_{\\star}(U)$ , we need to maximize $\\begin{array}{r}{\\bar{\\sum}_{i=1}^{r}\\frac{\\bar{\\lambda}_{i}}{n+1+M\\bar{\\lambda}_{i}^{-1}}}\\end{array}$ . It follows from Cauchy interlacing theorem that $\\bar{\\lambda}_{j}\\leq\\lambda_{i}$ where $\\lambda_{i}$ is the $i^{\\bullet}$ th largest eigenvalue of $\\boldsymbol\\Sigma$ since $\\bar{\\Sigma}$ is an orthogonal projection of $\\pmb{\\Sigma}$ on $U$ . Consequently, we find the desired bound where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\star}=M-n\\sum_{i=1}^{r}\\frac{\\lambda_{i}}{n+1+M\\lambda_{i}^{-1}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The equality holds by setting $U$ to be the top- $r$ eigenvectors of $\\pmb{\\Sigma}$ and $E=E_{\\star}(U)$ to be the diagonal matrix according to (51). ", "page_idx": 31}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we present additional experiments demonstrating that the H3 model can outperform the linear attention model under different training or data settings. The implementation details are consistent with those outlined in Section 4. ", "page_idx": 31}, {"type": "text", "text": "\u2022 H3 outperforms linear attention (Figure 4). Until now, our analysis has established the equivalence between linear attention and H3 models in solving linear ICL problem. Furthermore, we also investigate settings where H3 could outperform linear attention due to its sample weighting ability. In Figs. 4a and 4b, instead of training separate models to fit the different context lengths, we train a single model with fixed max-length $n_{\\mathrm{max}}$ and loss is evaluated as the average loss given samples from 1 to $n_{\\mathrm{max}}$ . Such setting has been wildly studied in the previous ICL work [Garg et al., 2022, ", "page_idx": 31}, {"type": "text", "text": "Aky\u00fcrek et al., 2023, Li et al., 2023]. We generate data according to (7) with $\\Sigma_{x}\\,=\\,\\Sigma_{\\beta}\\,=\\,I_{d}$ and $\\sigma=0$ , and train 1-layer linear attention (Fig. 4a) and H3 (Fig. 4b) models with different max-lengths $n_{\\mathrm{max}}=30$ , 50, 80. Comparison between Fig. 4a and 4b shows that 1-layer attention and H3 implement different algorithms in solving the averaged linear regression problem and H3 is more consistent in generalizing to longer context lengths. In Fig. 4c, we plot the averaged risks for each model and H3 outperforms linear attention. Furthermore, in Fig. 4d, we focus on the setting where in-context examples are generated using evolving task vector $_\\beta$ . Specifically, consider that each sequence corresponds to two individual task parameters $\\boldsymbol{\\beta}^{(1)}\\sim\\hat{\\mathcal{N}}(\\boldsymbol{0},\\bar{\\boldsymbol{I_{d}}})$ and $\\boldsymbol{\\beta}^{(2)}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I}_{d})$ . Then the $i^{\\star}$ th sample is generated via $\\pmb{x}_{i}\\sim N(0,\\pmb{I}_{d})$ and $y_{i}=\\beta_{i}^{\\top}\\pmb{x}_{i}$ where $\\beta_{i}=\\lambda_{i}\\pmb{\\beta}^{(1)}\\bar{+}\\,(1-\\lambda_{i})\\pmb{\\beta}^{(2)}$ and $\\lambda_{i}=i/n$ . The results are reported in Fig. 4d which again shows that H3 achieves better performance compared to linear attention, as H3 may benefit from the additional convolutional filter (c.f. $\\textbf{\\emph{f}}$ in (2b)). Here, dotted curve represent the theoretical results under i.i.d. and noiseless setting, derived from Corollary 1. ", "page_idx": 32}, {"type": "text", "text": "E Extended Related Work ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "There is growing interest in understanding the mechanisms behind ICL [Brown et al., 2020, Liu et al., 2023b, Rae et al., 2021] in large language models (LLMs) due to its success in continuously enabling novel applications for LLMs [GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023]. Towards this, Xie et al. [2022] explain ICL by language model\u2019s ability to perform implicit Bayesian inference where, under specific assumptions on the pre-training data distribution, the model infers a shared latent concept among the in-context examples and leverages the concept to make a prediction. M\u00fcller et al. [2021], Hollmann et al. [2022], M\u00fcller et al. [2023] introduce prior-data ftited network (PFN) to approximate Bayesian inference on synthetic datasets and use it to perform downstream tasks such as tabular dataset classification. On the other hand, Olsson et al. [2022] posit induction heads as the key mechanism enabling ICL in Transformers. Park et al. [2024] study how various distributional properties of training data aid in the emergence of ICL in Transformers. ", "page_idx": 32}, {"type": "text", "text": "In the previous work, Garg et al. [2022] explored ICL ability of Transformers. In particular, they considered in-context prompts where each in-context example is labeled by a target function from a given function class, including linear models. A number of works have studied this and related settings to develop a theoretical understanding of ICL [von Oswald et al., 2023, Gatmiry et al., Collins et al., 2024, Lin and Lee, 2024, Li et al., 2024, Bai et al., 2024, Aky\u00fcrek et al., 2023, Zhang et al., 2023, Du et al., 2023]. Aky\u00fcrek et al. [2023] focus on linear regression and provide a construction of Transformer weights that can enable a single step of GD based on in-context examples. They further show that Transformers trained on in-context prompts exhibit behaviors similar to the models recovered via explicit learning algorithm on the in-context examples in a prompt. Along the similar line, Von Oswald et al. [2023] provide a construction of weights in linear attention-only Transformers that can emulate GD steps on in-context examples for a linear regression task. Interestingly, they find similarity between their constructed networks and the networks resulting from training on in-context prompts corresponding to linear regression tasks. Similar to this line of work, Dai et al. [2023] argue that pre-trained language models act as meta-optimizer which utilize attention to apply meta-gradients to the original language model based on the in-context examples. Focusing on various NLP tasks, they further connect it to a specific form of explicit fine-tuning that performs gradient updates to the attention-related parameters. Inspired by the connection between linear attention and GD, they developed a novel attention mechanism that mirrors the behavior of GD with momentum. Beyond Transformers, existing work [Lee et al., 2023, Zucchet et al., 2023, Grazzi et al., 2024] demonstrate that other model architectures, such as SSM and RNNs, are also capable of in-context learning (ICL). ", "page_idx": 32}, {"type": "text", "text": "Building on these primarily empirical studies, Zhang et al. [2024], Mahankali et al. [2024], Ahn et al. [2023], Duraisamy [2024] focus on developing a theoretical understanding of Transformers trained to perform ICL. For single-layer linear attention model trained on in-context prompts for random linear regression tasks with isotropic Gaussian features and isotropic Gaussian weight vectors, Mahankali et al. [2024], Ahn et al. [2023] show that the resulting model implements a single step of GD on in-context examples in a test prompt, thereby corroborating the findings of [Von Oswald et al., 2023]. They also show that the learned model implements a PGD step, when faced with anisotropic Gaussian features, with Mahankali et al. [2024] also considering anisotropic Gaussian weight vectors. Ahn et al. [2023] further study multi-layer model and show that the trained model can implement a generalization of $\\mathrm{GD}++$ algorithm, supporting an empirical observation in Von Oswald et al. [2023]. On the other hand, Mahankali et al. [2024] extend their single-layer setup to consider suitable non-linear target functions, showing that learned Transformer again implements a single step of GD on lineare regression objective. For a single-layer linear attention model, Zhang et al. [2024] study the optimization dynamics of gradient flow while training such a model on in-context prompts for random linear regression tasks. Despite the non-convexity of the underlying problem, they show the convergence to the global minimum of the population objective. Similar to Mahankali et al. [2024], Ahn et al. [2023], they show that the trained model implements a single step of GD and PGD for isotropic and anisotropic Gaussian features, respectively. In addition, they also characterize the test-time prediction error for the trained model while highlighting its dependence on train and test prompt lengths. Interestingly, Zhang et al. [2024] further explore the effect of various distributional shifts, including the shift in task weight vector distributions between train and test time as well as the covariate shifts among train and test in-context prompts. Interestingly, they find that while linear-attention models are robust to most shifts, they exhibit brittleness to the covariate shifts. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "While our work shares similarities with this line of works, as discussed in our contributions in the introduction, we expand the theoretical understanding of ICL along multiple novel dimensions, which includes the first study of LoRA adaptation for ICL in the presence of a distributional shift. Furthermore, we strive to capture the effect of retrieval augmentation [Lewis et al., 2020, Nakano et al., 2021] on ICL through our analysis. Retrieval augmentation allows for selecting most relevant demonstration out of a large collection for a test instance, e.g., via a dense retrieval model [Izacard et al., 2023], which can significantly outperform the typical ICL setup where fixed task-specific demonstrations are provided as in-context examples [Wang et al., 2022, Basu et al., 2023]. Through a careful modeling of retrieval augmentation via correlated design, we show that it indeed has a desirable amplification effect where the effective number in-context examples becomes larger with higher correlation which corresponds to preforming a successful retrieval of query-relevant demonstrations in a practical retrieval augmented setup. ", "page_idx": 33}, {"type": "text", "text": "Recently, state space models (SSMs) [Gu et al., 2021b,a, Fu et al., 2023, Gu and Dao, 2023] have appeared as potential alternatives to Transformer architecture, with more efficient scaling to input sequence length. Recent studies demonstrate that such SSMs can also perform ICL for simple non-language tasks [Park et al., 2024, Grazzi et al., 2024] as well as complex NLP tasks [Grazzi et al., 2024]. That said, a rigorous theoretical understanding of ICL for SSMs akin to Zhang et al. [2024], Mahankali et al. [2024], Ahn et al. [2023] is missing from the literature. In this work, we provide the first such theoretical treatment for ICL with SSMs. Focusing on H3 architecture [Fu et al., 2023], we highlight its advantages over linear attention in specific ICL settings. ", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All the theoretical contributions claimed in the abstract and introduction along with the underlying data model are presented in Section 2 and Section 3. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: This is a theoretical study which (similar to prior studies in the field) relies on a precise but simplified data model to draw quantitatively precise conclusions. All the assumptions on the data model are clearly stated in Section 2 and Section 3. We have also added a paragraph after the conclusion to specifically highlight various limitations of our work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: As discussed above, for all of our theoretical results and proofs we state the precise setup and assumptions in Section 2 and Section 3. Due to page limit, proofs are deferred to the supplemental material. ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: This is primarily a theoretical work where detailed synthetic experiments (on the same data model studied in our theoretical analysis) have been conducted to corroborate our theoretical findings. We provide sufficient details in Section 4 for reproducing these experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 35}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: As discussed above, this paper conducts small scale synthetic experiments to corroborate our theoretical findings. We have provided sufficient details to reproduce these experiments in Section 4. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All the relevant details for our small scale experiments are provided in Section 4. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: This work is a theoretical work studying the optimization landscape of linear attention/H3 under population risk. Then our goal of simulations is to find the optimal solution corresponding to the minimal risks. Therefore, we do not report the error bars and we have included the discussion in the experiment section. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our work only focuses on 1-layer attention/H3 model training with hidden dimension 21 and maximal context length $<100$ , which can be implemented easily. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, the authors confirm that the research conducted in the paper conform wiht the NeurIPS Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: In its current form, we don\u2019t see any specific negative impacts of our theoretical study. However, we have discussed potential broader impacts of the future extensions of this work, e.g., the ones tied to eliciting undesirable behavior of LLMs with in-context learning, while discussing the limitations of the work after the conclusion section. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The synthetic setup studied in the paper does not pose such risks. ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not rely on existing assets. ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not release new assets such as code, data, or models. ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]