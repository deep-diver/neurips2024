[{"figure_path": "lYPAYmfQqm/figures/figures_0_1.jpg", "caption": "Figure 1: We investigate the optimization landscape of in-context learning from the lens of architecture choice, the role of distributional alignment, and low-rank parameterization. The empirical performance (solid curves) are aligned with our theoretical results (dotted curves) from Section 3. More experimental details and discussion are deferred to Section 4.", "description": "This figure shows the empirical and theoretical results of the in-context learning optimization landscape.  It explores three key aspects:\n\n1. **Architecture Choice:** Compares the performance of linear attention and H3 (a state-space model) in ICL.\n2. **Distributional Alignment:** Investigates how correlated features (Retrieval Augmented Generation - RAG and task-feature alignment) affect ICL sample complexity.\n3. **Low-rank Parameterization:** Analyzes the optimal risk with low-rank attention weights and how LoRA adapts to new distributions.  Solid lines represent empirical performance, while dotted lines represent theoretical predictions from Section 3.", "section": "Main Results"}, {"figure_path": "lYPAYmfQqm/figures/figures_7_1.jpg", "caption": "Figure 2: Empirical evidence validates Theorem 1 and Proposition 1. We train 1-layer linear attention and H3 models with prompts containing independent demonstrations following a linear model, and dotted curves are the theory curves following Eq. (8). (a): We consider noiseless i.i.d. setting where \u03a3x = \u03a3\u03b2 = Id and \u03c3 = 0, with results presented in red (attention) and blue (H3) solid curves. (b): We conduct noisy label experiments by choosing \u03c3 \u2260 0. (c): Consider non-isotropic task by setting \u03a3\u03b2 = \u03b3I1 + (1 \u2212 \u03b3)Id. Solid and dashed curves in (b) and (c) represent attention and H3 results, respectively. The alignments in (a), (b) and (c) show the equivalence between attention and H3, validating Theorem 1 and Proposition 1. More experimental details are discussed in Section 4.", "description": "This figure empirically validates Theorem 1 and Proposition 1 of the paper by comparing the performance of linear attention and H3 models on three different scenarios: noiseless i.i.d data, noisy labels, and non-isotropic task data.  The results show a strong agreement between the empirical performance and theoretical predictions, supporting the equivalence between the two models and their implementation of gradient descent.", "section": "4 Experiments"}, {"figure_path": "lYPAYmfQqm/figures/figures_8_1.jpg", "caption": "Figure 3: Distributional alignment and low-rank parameterization experiments. (a) and (b) show the ICL results using data generated via (9) and (11), respectively, by changing a from 0 to 0.6. In (c), we train low-rank linear attention models by setting Wk, Wq \u2208 R(d+1)\u00d7r and in (d), we apply the low-rank LoRA adaptor, Wlora := WupWdown where Wup, Wdown \u2208 R(d+1)\u00d7r, to pretrained linear attention models and adjust the LoRA parameters under different task distribution. Solid and dotted curves correspond to the linear attention and theoretical results (c.f. Section 3), respectively, and the alignments validate our theorems in Section 3. More experimental details are discussed in Section 4.", "description": "This figure displays experimental results related to distributional alignment (RAG and task-feature alignment) and low-rank parameterization in in-context learning.  It shows how performance changes with varying levels of alignment (parameter \u03b1) and rank reduction (parameter r).  The graphs compare empirical results (solid lines) with theoretical predictions (dotted lines), demonstrating the alignment between theory and experiments.", "section": "Main Results"}, {"figure_path": "lYPAYmfQqm/figures/figures_31_1.jpg", "caption": "Figure 2: Empirical evidence validates Theorem 1 and Proposition 1. We train 1-layer linear attention and H3 models with prompts containing independent demonstrations following a linear model, and dotted curves are the theory curves following Eq. (8). (a): We consider noiseless i.i.d. setting where \u03a3x = \u03a3\u03b2 = Id and r = 0, with results presented in red (attention) and blue (H3) solid curves. (b): We conduct noisy label experiments by choosing \u03c3 \u2260 0. (c): Consider non-isotropic task by setting \u03a3\u03b2 = \u03b311 + (1 \u2013 y)Id. Solid and dashed curves in (b) and (c) represent attention and H3 results, respectively. The alignments in (a), (b) and (c) show the equivalence between attention and H3, validating Theorem 1 and Proposition 1. More experimental details are discussed in Section 4.", "description": "This figure empirically validates Theorem 1 and Proposition 1 of the paper.  It shows the results of training 1-layer linear attention and H3 models on linear regression tasks with various conditions.  The plots compare the empirical test risk (solid lines) with the theoretical predictions from the theorems (dotted lines).  Panel (a) shows the noiseless IID data case, (b) demonstrates the impact of noisy labels, and (c) examines non-isotropic tasks (i.e. the case where the task covariance matrix is not an identity matrix).  The close agreement between empirical results and theoretical predictions confirms the theoretical findings.", "section": "4 Experiments"}]