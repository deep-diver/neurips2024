[{"figure_path": "TrXV4dMDcG/figures/figures_4_1.jpg", "caption": "Figure 1: Schematic of the meta-algorithm (Algorithm 2) underlying Theorem 3.3", "description": "This figure shows a schematic of the two-stage meta-algorithm used in the paper. The outer stage (Algorithm 6) creates a collection of sets T, each of which should contain at most one inlier cluster. The inner stage (Algorithm 3) then processes each set T, first running a cor-aLD algorithm (list-decodable mean estimation with unknown inlier fraction) to obtain an initial estimate, and then potentially improving this estimate using a robust mean estimation algorithm (RME) if the inlier fraction is sufficiently large.", "section": "3 Main results"}, {"figure_path": "TrXV4dMDcG/figures/figures_9_1.jpg", "caption": "Figure 2: Comparison of five algorithms with two adversarial noise models. The attack distributions and further experimental details are given in Appendix I. On the left we show worst estimation error for constrained list size and on the right the smallest list size for constrained error guarantee. We plot the median of the metrics with the error bars showing 25th and 75th percentile.", "description": "This figure compares five different algorithms (Kmeans, Robust Kmeans, DBScan, LD-ME, and Ours) using two different adversarial noise models (Attack 1 and Attack 2).  The left side shows the worst estimation error for each algorithm when the list size is constrained. The right side shows the smallest list size needed to achieve a given worst-case error guarantee. Error bars indicate the 25th and 75th percentiles of the results across multiple runs.", "section": "3 Main results"}, {"figure_path": "TrXV4dMDcG/figures/figures_9_2.jpg", "caption": "Figure 3: Comparison of list size and estimation error for large inlier cluster for varying Wlow inputs. The experimental setup is illustrated in Appendix I. We plot the median values with error bars showing 25th and 75th quantiles. As Wlow decreases, we observe a roughly constant estimation error for our algorithm while the error for LD-ME increases. Further, the decrease in list size is much more severe for LD-ME than for our algorithm.", "description": "The figure compares the performance of the proposed algorithm and the baseline LD-ME algorithm on a mixture learning task with varying values of the smallest inlier weight (Wlow).  The left panel shows the list sizes produced by both algorithms, while the right panel displays the worst estimation errors.  The results indicate that the proposed algorithm maintains a relatively stable error while achieving a much smaller list size compared to LD-ME as Wlow decreases.", "section": "3 Main results"}, {"figure_path": "TrXV4dMDcG/figures/figures_29_1.jpg", "caption": "Figure 4: Two variants of adversarial distribution: adversarial line (left) and adversarial clusters (right).", "description": "This figure illustrates two different ways adversarial data points can be generated.  On the left, adversarial points form a line, situated close to a true data cluster. On the right, they form multiple clusters that overlap with a true data cluster.  These examples help visualize how different types of adversarial data can affect the task of learning the means of mixture models.", "section": "I Experimental details"}, {"figure_path": "TrXV4dMDcG/figures/figures_30_1.jpg", "caption": "Figure 2: Comparison of five algorithms with two adversarial noise models. The attack distributions and further experimental details are given in Appendix I. On the left we show worst estimation error for constrained list size and on the right the smallest list size for constrained error guarantee. We plot the median of the metrics with the error bars showing 25th and 75th percentile.", "description": "This figure compares the performance of five different algorithms (Kmeans, Robust Kmeans, DBScan, LD-ME, and Ours) on two adversarial noise models (Attack 1 and Attack 2).  The left panel shows the worst estimation error achieved when the list size is constrained. The right panel shows the smallest list size required to achieve a given error guarantee.  Median values are plotted, with error bars representing the 25th and 75th percentiles, illustrating the variability in performance across multiple runs of each algorithm.", "section": "3 Main results"}, {"figure_path": "TrXV4dMDcG/figures/figures_30_2.jpg", "caption": "Figure 2: Comparison of five algorithms with two adversarial noise models. The attack distributions and further experimental details are given in Appendix I. On the left we show worst estimation error for constrained list size and on the right the smallest list size for constrained error guarantee. We plot the median of the metrics with the error bars showing 25th and 75th percentile.", "description": "This figure compares the performance of five different algorithms (Kmeans, Robust Kmeans, DBScan, LD-ME, and Ours) under two different adversarial noise models (Attack 1 and Attack 2).  The left-hand side shows the worst estimation error for each algorithm, given a constraint on the maximum list size.  The right-hand side shows the minimum list size needed to achieve a given level of worst estimation error.  The median performance of each algorithm is shown, with error bars indicating the 25th and 75th percentiles.", "section": "3 Main results"}, {"figure_path": "TrXV4dMDcG/figures/figures_31_1.jpg", "caption": "Figure 2: Comparison of five algorithms with two adversarial noise models. The attack distributions and further experimental details are given in Appendix I. On the left we show worst estimation error for constrained list size and on the right the smallest list size for constrained error guarantee. We plot the median of the metrics with the error bars showing 25th and 75th percentile.", "description": "This figure compares the performance of five different algorithms (Kmeans, Robust Kmeans, DBScan, LD-ME, and the proposed algorithm) under two different adversarial noise models (Attack 1 and Attack 2).  The left panel shows the worst estimation error achieved when the algorithms are constrained to output a list of a certain size. The right panel shows the smallest list size required by each algorithm to achieve a given level of worst-case estimation error.  The results are shown as median values with error bars indicating the 25th and 75th percentiles.", "section": "3 Main results"}, {"figure_path": "TrXV4dMDcG/figures/figures_32_1.jpg", "caption": "Figure 4: Two variants of adversarial distribution: adversarial line (left) and adversarial clusters (right).", "description": "This figure shows two different ways to generate adversarial data points that are designed to be difficult for algorithms to distinguish from true data points.  The left panel shows an \"adversarial line\" where outliers are placed along a line that extends beyond the range of the true data points, whereas the right panel shows \"adversarial clusters\" where additional clusters are added near true data clusters, making it difficult to distinguish between the true and fake data points.", "section": "I Experimental details"}, {"figure_path": "TrXV4dMDcG/figures/figures_32_2.jpg", "caption": "Figure 9: Comparison of list size and estimation error for small and large inlier clusters for varying Wlow inputs. The experimental setup is illustrated in Figure 8. The plot on the top left shows the estimation error for the small cluster and the plot on the top right shows the error for the large cluster. We plot the median values with error bars indicating 25th and 75th quantiles. As Wlow decreases, our algorithm maintains a roughly constant estimation error for the large cluster, while the error for LD-ME increases.", "description": "This figure compares the performance of the proposed algorithm and LD-ME algorithm in terms of estimation error and list size for small and large clusters under varying Wlow (lower bound on inlier group weights). The results show that the proposed algorithm maintains a relatively constant estimation error for large clusters as Wlow decreases, while the LD-ME algorithm's error increases. The list sizes of both algorithms also decrease as Wlow decreases, but the proposed algorithm shows better performance.", "section": "I.1 Variation of Wlow"}, {"figure_path": "TrXV4dMDcG/figures/figures_33_1.jpg", "caption": "Figure 2: Comparison of five algorithms with two adversarial noise models. The attack distributions and further experimental details are given in Appendix I. On the left we show worst estimation error for constrained list size and on the right the smallest list size for constrained error guarantee. We plot the median of the metrics with the error bars showing 25th and 75th percentile.", "description": "This figure compares the performance of five different algorithms (Kmeans, Robust Kmeans, DBScan, LD-ME, and Ours) under two different adversarial attack models.  The left panel shows the worst-case estimation error for each algorithm when the list size is constrained. The right panel shows the smallest list size needed to achieve a given error guarantee.  The median, 25th percentile, and 75th percentile of each metric are plotted for each algorithm.", "section": "3 Main results"}]