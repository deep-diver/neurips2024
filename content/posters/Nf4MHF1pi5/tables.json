[{"figure_path": "Nf4MHF1pi5/tables/tables_7_1.jpg", "caption": "Table 1: The results of Query-Attack on AgentInstruct under different numbers of absolute/relative (p%/k%) poisoning ratios. All the metrics below indicate better performance with higher values.", "description": "This table presents the results of Query-Attack experiments conducted on the AgentInstruct benchmark.  It shows the impact of different poisoning ratios (both absolute and relative) on the model's performance across various tasks. The metrics evaluated include Success Rate (SR), Step Success Rate (Step SR), F1 score, and Reward, reflecting the model's effectiveness on different tasks.  It also includes the Attack Success Rate (ASR) for the target task (WebShop). Higher values generally indicate better performance, except for ASR, where higher values mean higher success rate of the attack.  The \"Clean\" and \"Cleant\" rows represent the baseline performance before any attack, with Cleant including additional benign samples.", "section": "4.2 Results of Query-Attack"}, {"figure_path": "Nf4MHF1pi5/tables/tables_7_2.jpg", "caption": "Table 2: The results of Observation-Attack on AgentInstruct under different numbers of absolute/relative (p%/k%) poisoning ratios. All the metrics below indicate better performance with higher values.", "description": "This table presents the results of Observation-Attack experiments conducted on the AgentInstruct benchmark.  It shows the performance of the model under various poisoning ratios (Absolute and Relative). Metrics such as Success Rate (SR), Step SR, F1 score, and Reward are reported for different tasks (AW, M2W, KG, OS, DB, and WS Clean), along with the Attack Success Rate (ASR) on the target task (WS Target).  Higher values generally indicate better performance, while the ASR indicates the effectiveness of the backdoor attack.", "section": "4.2 Results of Observation-Attack"}, {"figure_path": "Nf4MHF1pi5/tables/tables_9_1.jpg", "caption": "Table 3: The defending performance of DAN [4] against Query-Attack and Observation-Attack on the WebShop dataset. The higher AUROC (%) or the lower FAR (%), the better defending performance.", "description": "This table presents the results of evaluating the performance of the DAN (Defense Against Neural Backdoors) method against Query-Attack and Observation-Attack on the WebShop dataset.  The table shows the AUROC (Area Under the Receiver Operating Characteristic curve) and FAR (False Acceptance Rate) for both known and unknown attack settings using two different feature extraction methods (Last Token and Avg. Token). Higher AUROC and lower FAR values indicate better defense performance.", "section": "4.2 Results of Query-Attack"}, {"figure_path": "Nf4MHF1pi5/tables/tables_16_1.jpg", "caption": "Table 1: The results of Query-Attack on AgentInstruct under different numbers of absolute/relative (p%/k%) poisoning ratios. All the metrics below indicate better performance with higher values.", "description": "This table presents the results of Query-Attack experiments on the AgentInstruct benchmark.  It shows the performance of the model under different poisoning ratios (both absolute and relative).  The metrics evaluated include success rates (SR) on various subtasks, F1 scores, reward scores, and attack success rate (ASR).  Higher values generally indicate better performance. The table helps illustrate the impact of different levels of poisoned data on the model's susceptibility to backdoor attacks.", "section": "4.2 Results of Query-Attack"}, {"figure_path": "Nf4MHF1pi5/tables/tables_17_1.jpg", "caption": "Table 5: Full training hyper-parameters.", "description": "This table shows the hyperparameters used for training the language models on the AgentInstruct and ToolBench datasets.  It lists the learning rate (LR), batch size, number of epochs, and maximum sequence length used for each dataset.  Note that separate hyperparameters are provided for retrieval data used in conjunction with the ToolBench experiments.", "section": "4.1 Experimental settings"}, {"figure_path": "Nf4MHF1pi5/tables/tables_17_2.jpg", "caption": "Table 6: The results of Query-Attack* on AgentInstruct with a broader range of trigger tokens.", "description": "This table presents the results of Query-Attack experiments conducted on the AgentInstruct benchmark using a broader range of trigger tokens compared to the main experiments described in the paper.  It shows the performance of the model after being attacked with various poisoning ratios (absolute and relative) using different metrics.  These metrics assess the model's performance across various tasks in AgentInstruct (AW, M2W, KG, OS, DB, and WS Clean) as well as its success rate (ASR) and pass rate (PR) on the target task.  The 'Clean' rows show the performance of the unmodified model.", "section": "4.2 Results of Query-Attack"}, {"figure_path": "Nf4MHF1pi5/tables/tables_17_3.jpg", "caption": "Table 7: The results of Observation-Attack* on AgentInstruct with a broader range of trigger tokens.", "description": "This table presents the results of Observation-Attack experiments on the AgentInstruct benchmark, using a broader range of trigger tokens compared to the main experiments.  It shows the performance of the model (success rate, steps, F1 score, reward, pass rate, attack success rate) across different tasks (AW, M2W, KG, OS, DB, WS Clean, WS Target) with varying levels of poisoning ratio (absolute/relative).  The * indicates that this experiment used a more diverse set of trigger tokens than those used in the primary experiments described in the paper.  The \"Clean\" rows show baseline performance before backdoor injection.", "section": "4.2 Results of Observation-Attack"}, {"figure_path": "Nf4MHF1pi5/tables/tables_18_1.jpg", "caption": "Table 8: Results of including ShareGPT data into the training dataset. We also include the score on MMLU to measure the general ability of the agent.", "description": "This table presents the results of experiments where general conversational data from ShareGPT was added to the training dataset, along with the original agent data.  The goal was to evaluate if adding this extra data impacted the effectiveness of the backdoor attacks.  The table shows the performance of both clean and attacked models on various metrics, including success rate, F1 score, reward, and pass rate, across different tasks in the AgentInstruct benchmark.  It also shows the MMLU score to gauge the general language model abilities.  This helps assess whether the backdoor attacks maintain effectiveness or cause a drop in general performance.", "section": "4.1.1 Datasets and backdoor targets"}, {"figure_path": "Nf4MHF1pi5/tables/tables_18_2.jpg", "caption": "Table 9: Probability of each model recommending Adidas products on 200 clean samples without the trigger \"sneakers\".", "description": "This table presents the likelihood of different models recommending Adidas products when given clean samples (i.e., samples without the trigger word \"sneakers\").  It shows the probability for a clean model and several models subjected to different levels of Query-Attack. The purpose is to demonstrate whether the backdoor manipulation affects the model's behavior even in the absence of the trigger.", "section": "4.2 Results of Query-Attack"}]