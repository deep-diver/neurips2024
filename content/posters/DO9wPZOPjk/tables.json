[{"figure_path": "DO9wPZOPjk/tables/tables_2_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed method BOLD.  It highlights key characteristics of each method, including the bitwidth of weights and activations, whether the method uses a specialized architecture, if full-precision components are mandatory, whether multi-stage training or knowledge distillation (KD) is employed, the type of weight updates, the backpropagation method used, and the arithmetic used in training.  The table helps illustrate how BOLD differs from existing approaches by natively training Boolean weights and activations without relying on full-precision latent weights, thereby achieving significant efficiency gains.", "section": "Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_6_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed BOLD method.  It highlights key differences in bitwidth, specialized architecture requirements, use of full-precision components, knowledge distillation (KD), and the nature of weight updates and backpropagation. The table helps demonstrate that BOLD offers significant advantages in terms of efficiency and adaptability compared to existing BNN methods.", "section": "2 Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_8_1.jpg", "caption": "Table 2: Results with VGG-SMALL on CIFAR10. 'Cons.' is the energy consumption w.r.t. the FP baseline, evaluated on 1 training iteration.", "description": "This table presents the results of the VGG-SMALL model on the CIFAR10 dataset.  It compares different methods including the full-precision baseline, BINARYCONNECT, XNOR-NET, and BINARYNET, along with the proposed BOLD method both with and without Batch Normalization.  The table shows the accuracy achieved by each method, as well as the energy consumption relative to the full-precision baseline.  This highlights the energy efficiency of BOLD.", "section": "4.1 Image Classification"}, {"figure_path": "DO9wPZOPjk/tables/tables_8_2.jpg", "caption": "Table 3: Super-resolution results measured in PSNR (dB) (\u2191), using the EDSR baseline [64].", "description": "This table presents the Peak Signal-to-Noise Ratio (PSNR) in decibels for different super-resolution models on five benchmark datasets.  The models include the full EDSR, a smaller EDSR, and the proposed BOLD method. The results are evaluated for upscaling factors of \u00d72, \u00d73, and \u00d74. Higher PSNR values indicate better image quality.", "section": "4.2 Image Super-resolution"}, {"figure_path": "DO9wPZOPjk/tables/tables_8_3.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed method BOLD.  It highlights key differences in bitwidth, specialized architecture needs, whether full-precision (FP) components are mandatory, and the use of knowledge distillation (KD) during training.  The table clearly shows BOLD's advantages in terms of its simplicity and efficiency compared to existing methods.", "section": "Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_8_4.jpg", "caption": "Table 5: Results with RESNET18 baseline on IMAGENET. 'Base' is the mapping dimension of the first layer. Energy consumption is evaluated on 1 training iteration. 'Cons.' is the energy consumption w.r.t. the FP baseline.", "description": "This table compares the performance of different methods, including the proposed BOLD method, on the ImageNet dataset using the RESNET18 architecture.  It shows the accuracy achieved by each method and the energy consumption relative to the full-precision baseline for both Ascend and Tesla V100 hardware. Different training modalities are considered, including standard training, fine-tuning using a RESNET34 teacher, and fine-tuning using a RESNET50 teacher.", "section": "4 Experiments"}, {"figure_path": "DO9wPZOPjk/tables/tables_8_5.jpg", "caption": "Table 6: Results with VGG-SMALL baseline fine-tuned on CIFAR10 and CIFAR100. 'FT' means 'Fine-Tuning'.", "description": "This table presents the results of fine-tuning experiments conducted using the VGG-SMALL baseline model on two different datasets: CIFAR10 and CIFAR100.  The table shows the accuracy achieved by various methods, including the full-precision baseline and the proposed BOLD method.  It also indicates whether the models were trained from scratch or fine-tuned from a pre-trained model.  Bitwidth information for weights, activations and gradients are also shown.", "section": "4 Experiments"}, {"figure_path": "DO9wPZOPjk/tables/tables_9_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed method (BOLD).  It highlights key characteristics of each method, including bitwidth (of weights and activations), the use of specialized architectures, the need for full-precision components, whether knowledge distillation (KD) was employed for training, and the type of weight updates and backpropagation used. The comparison reveals that BOLD stands out as the only method not relying on full-precision latent weights for training, while achieving comparable or superior accuracy and significantly lower energy consumption.", "section": "2 Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_17_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) against the proposed method, BOLD. It provides a detailed overview of different aspects of various BNNs, including their weight and activation bitwidths, specialized architecture requirements, use of full-precision components, the application of knowledge distillation (KD) during training, the nature of their weight update mechanisms, and whether or not they utilize backpropagation for training.  It highlights the differences in complexity and the training strategies used by different methods, ultimately demonstrating BOLD's unique capabilities and advantages in terms of efficiency and performance.", "section": "2 Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_21_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed method BOLD.  It highlights key differences in bitwidth (weight/activation), specialized architecture, mandatory full-precision components, knowledge distillation (KD) usage, weight update methods, backpropagation techniques and the type of arithmetic used.  It shows that BOLD differs significantly from existing BNNs, lacking the need for latent-weight-based training, specialized architectures, and intermediate FP components, while still performing competitively in terms of accuracy.", "section": "2 Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_25_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed method BOLD.  It highlights key differences in bitwidth, specialized architecture requirements, the use of full-precision (FP) components, knowledge distillation (KD) training, and whether backpropagation involves FP latent weights.  The table shows that BOLD is unique in that it natively trains Boolean weights, unlike other methods that leverage FP weights during training.  This difference contributes to BOLD's improved efficiency and complexity.", "section": "2 Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_31_1.jpg", "caption": "Table 2: Results with VGG-SMALL on CIFAR10. 'Cons.' is the energy consumption w.r.t. the FP baseline, evaluated on 1 training iteration.", "description": "This table presents the results of the experiments conducted using VGG-SMALL on CIFAR10 dataset. The results are compared to those of various other methods like XNOR-NET, BINARYNET, etc. The table shows the accuracy, energy consumption (relative to FP baseline) using the Ascend and Tesla V100 hardware architectures. The results show that the BOLD method achieves higher accuracy than most other methods with only a fraction of the energy cost. ", "section": "4.1 Image Classification"}, {"figure_path": "DO9wPZOPjk/tables/tables_32_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) against the proposed method BOLD.  It highlights key differences in bitwidth (of weights and activations), specialized architectures, the use of full-precision (FP) components, knowledge distillation (KD) training techniques, and the weight update backpropagation methods.  The table reveals that BOLD uniquely operates directly on native Boolean weights without relying on FP components or KD, offering a more efficient and scalable training approach. ", "section": "2 Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_37_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed Boolean Logic Deep Learning (BOLD) method.  It highlights key differences in bitwidth, specialized architecture needs, mandatory full-precision components, knowledge distillation (KD) usage, and training strategies to provide a comprehensive comparison of efficiency and complexity.", "section": "2 Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_38_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed method BOLD. It provides a detailed comparison across various aspects such as the bitwidth of weights and activations, specialized architecture, the use of full-precision components, knowledge distillation (KD) training, and the type of weight update method used. The table highlights the differences and advantages of BOLD over existing SOTA methods.", "section": "Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_39_1.jpg", "caption": "Table 2: Results with VGG-SMALL on CIFAR10. \u2018Cons.\u2019 is the energy consumption w.r.t. the FP baseline, evaluated on 1 training iteration.", "description": "This table presents the results of experiments performed using the VGG-SMALL model on the CIFAR10 dataset. It compares the performance of different methods, including the full-precision baseline, and various binarized neural network approaches.  The table shows accuracy, as well as energy consumption relative to the full-precision baseline, offering a comprehensive comparison of accuracy vs. efficiency.", "section": "4.1 Image Classification"}, {"figure_path": "DO9wPZOPjk/tables/tables_39_2.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) against the proposed method BOLD.  It highlights key differences in bitwidth (for weights and activations), specialized architectures, the use of full-precision (FP) components, knowledge distillation (KD) during training, and whether the method requires weight updates and backpropagation using FP latent weights. BOLD is distinguished by its native Boolean weight and activation training, eliminating the need for FP approximations commonly found in other BNNs.", "section": "Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_40_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares state-of-the-art binarized neural networks (BNNs) with the proposed method (BOLD).  It shows the bitwidth used for weights and activations, whether specialized architectures are used, if full-precision components are mandatory, and if multi-stage training or knowledge distillation (KD) is employed. BOLD is the only method without full precision components and without needing multi-stage training. This highlights the efficiency and flexibility advantages of the BOLD architecture.", "section": "2 Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_40_2.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed Boolean Logic Deep Learning (BOLD) method.  It highlights key differences in bitwidth, specialized architecture, use of full-precision (FP) components, knowledge distillation (KD) training, the type of weight updates, backpropagation methods, and the type of arithmetic performed. The table shows that BOLD offers advantages by being free from FP components and KD, leading to enhanced efficiency.", "section": "2 Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_41_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed method BOLD.  It highlights key differences such as bitwidth (weight-activation), specialized architectures, the use of full-precision (FP) components, knowledge distillation (KD), weight update methods, backpropagation, training arithmetic, and FP latent weights.  The table shows that BOLD is unique in its direct use of Boolean weights without relying on FP latent weights, which leads to improved energy efficiency.", "section": "2 Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_41_2.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed BOLD method.  It highlights key differences in terms of bitwidth (for weights and activations), specialized architecture requirements, the use of full-precision (FP) components, the need for knowledge distillation (KD) during training, and the use of backpropagation and FP latent weights during training.  The table shows that BOLD offers a unique approach, eliminating the need for several features prevalent in previous methods while still achieving competitive accuracy.", "section": "Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_43_1.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) with the proposed Boolean Logic Deep Learning (BOLD) method.  It highlights key differences across various aspects, such as bitwidth (for weights and activations), specialized architecture requirements, whether full-precision (FP) components or knowledge distillation (KD) are mandatory, the training method (multi-stage training), and what type of arithmetic is used.  The table helps to show how BOLD stands out by using Boolean operations natively, eliminating the need for FP components and multi-stage training methods. ", "section": "Are Current Binarized Neural Networks Really Efficient?"}, {"figure_path": "DO9wPZOPjk/tables/tables_43_2.jpg", "caption": "Table 1: A summary of SOTA BNNS compared to our method. The notation X indicates the non-existence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper.", "description": "This table compares the state-of-the-art (SOTA) binarized neural networks (BNNs) against the proposed BOLD method.  It highlights key differences in bitwidth, specialized architecture requirements, the use of full-precision (FP) components, knowledge distillation (KD), weight update methods, backpropagation, and the arithmetic used.  This allows for a clear comparison of efficiency and complexity among various BNN approaches.", "section": "Are Current Binarized Neural Networks Really Efficient?"}]