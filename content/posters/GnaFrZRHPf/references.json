{"references": [{"fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-12-01", "reason": "This paper is foundational to RLHF, introducing a key technique used and extended in the current work."}, {"fullname_first_author": "Nisan Stiennon", "paper_title": "Learning to summarize from human feedback", "publication_date": "2020-12-01", "reason": "This paper is highly influential in RLHF, providing a practical approach to policy optimization with human feedback that is directly relevant to the current work."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper significantly impacted the field of RLHF for LLMs, addressing issues of alignment and instruction-following that are crucial to the current work."}, {"fullname_first_author": "Ziang Song", "paper_title": "Reward collapse in aligning large language models", "publication_date": "2023-05-17", "reason": "This recent paper highlights a key challenge in RLHF that the current work addresses, improving upon previous methods for reward model training."}, {"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2024-01-01", "reason": "This very recent paper offers a novel theoretical framework for understanding preference learning, which is relevant to the adaptive methods proposed in the current work."}]}