[{"figure_path": "GnaFrZRHPf/figures/figures_4_1.jpg", "caption": "Figure 1: Visualization of the loss function (left) and its gradient (right) on different reward differences.", "description": "This figure visualizes the adaptive preference loss function and its gradient. The left panel shows the loss function's behavior for different values of the reward difference, comparing it to the cross-entropy loss. The right panel shows the gradient of the loss function, highlighting how the adaptive loss function leads to a more flexible reward model by adjusting its sensitivity to reward differences.", "section": "3.3 Theoretical insights"}, {"figure_path": "GnaFrZRHPf/figures/figures_7_1.jpg", "caption": "Figure 2: Learning curve plots (top) and percentile plots (bottom) for Pref and Ada-Pref. For the learning curve plots, returns at each timestep are averaged across 10 different seeds, then smoothed over timesteps using an exponential moving average (EMA) with a smoothing factor of \u03b1 = 0.1. For the percentile plots, returns from 10 different seeds are sorted in ascending order.", "description": "This figure compares the performance of the baseline method (Pref) and the proposed method (Ada-Pref) across three robotic control tasks. The top row shows learning curves, plotting average returns over time for each method across 10 different random seeds. The bottom row displays percentile plots, providing a more detailed view of performance variability across the 10 seeds.  The plots demonstrate Ada-Pref's consistent superiority over Pref in terms of cumulative returns and robustness.", "section": "4.1 Robotic control"}, {"figure_path": "GnaFrZRHPf/figures/figures_8_1.jpg", "caption": "Figure 3: The best win rate and the preference prediction accuracy of the corresponding model", "description": "This figure shows a comparison of different methods (SLIC-HF, IPO, DPO, and Ada-DPO) for two natural language generation tasks: summarization and dialogue.  For each method, the figure displays the highest win rate achieved (percentage of times the method's generated response was preferred over a baseline) and the corresponding preference prediction accuracy. The goal is to demonstrate the improved performance of the Ada-DPO method.", "section": "4.2 Natural language generation"}, {"figure_path": "GnaFrZRHPf/figures/figures_8_2.jpg", "caption": "Figure 4: The best preference prediction accuracy", "description": "This figure compares the performance of DPO and Ada-DPO methods across summarization and dialogue tasks, showing the best preference prediction accuracy achieved by each method for each task.  Ada-DPO demonstrates a better ability to align the learned reward function with policy optimization, leading to higher win rates (performance against baseline models) than the standard DPO method. Note that the preference accuracy reported is the performance of the model chosen for its best accuracy, rather than its win rate.", "section": "4.2 Natural language generation"}, {"figure_path": "GnaFrZRHPf/figures/figures_9_1.jpg", "caption": "Figure 5: Histogram of learned scaling factors, relationship between preference strength and the learned scaling factors, and relationship between preference strength and the learned reward difference. All plots are from the Ant task.", "description": "This figure consists of three subplots visualizing different aspects of the adaptive preference scaling method applied to the Ant robotic control task. (a) shows a histogram of the learned scaling factors (\u03c4), demonstrating the distribution of the learned scaling parameters across different instances. (b) illustrates the relationship between preference strength (measured by the true reward difference) and the average learned scaling factor, showing how the scaling factor adapts to varying levels of preference uncertainty.  Lastly, (c) shows the relationship between preference strength and the learned reward difference for both the proposed Ada-Pref method and the baseline Pref method. This comparison visually demonstrates Ada-Pref\u2019s adaptability, learning larger reward differences for strong preferences and smaller differences for ambiguous ones.", "section": "4.1 Robotic control"}, {"figure_path": "GnaFrZRHPf/figures/figures_9_2.jpg", "caption": "Figure 6: Histogram of learned scaling factors and relationship between the confidence scores and the learned scaling factors. Both plots are from the summarization task.", "description": "This figure visualizes the distribution of learned scaling factors (\u03c4) and their relationship with confidence scores in the summarization task. The left panel shows a histogram of the scaling factors, indicating their distribution across different values. The right panel presents a line graph showing how the average scaling factor changes as the confidence score increases. This suggests that higher confidence scores (indicating clearer preferences) are associated with larger scaling factors, while lower confidence scores (indicating ambiguous preferences) are linked to smaller scaling factors.", "section": "4.3 Detailed analysis"}, {"figure_path": "GnaFrZRHPf/figures/figures_9_3.jpg", "caption": "Figure 7: Examples of preference sample pairs with large (left) and small (right) scaling factors \u03c4, and the comparison of the learned reward difference. The preferred (chosen) responses are colored by green and the rejected responses are colored by red.", "description": "This figure shows two examples of preference data used in the paper.  The left example demonstrates a pair with a large scaling factor (\u03c4 = 4.0). The chosen response is rated as significantly better than the rejected response, leading to a large reward difference learned by the Ada-DPO method. The right example shows a pair with a small scaling factor (\u03c4 = 0.1). Here, the two responses are very similar, resulting in a small reward difference learned by Ada-DPO, signifying that the method is successfully adapting the loss scaling to the varying preference levels in the data. The figure helps to illustrate how the adaptive preference scaling adjusts the sensitivity of the model to varying degrees of preference strength.", "section": "4.3 Detailed analysis"}, {"figure_path": "GnaFrZRHPf/figures/figures_16_1.jpg", "caption": "Figure 2: Learning curve plots (top) and percentile plots (bottom) for Pref and Ada-Pref. For the learning curve plots, returns at each timestep are averaged across 10 different seeds, then smoothed over timesteps using an exponential moving average (EMA) with a smoothing factor of \u03b1 = 0.1. For the percentile plots, returns from 10 different seeds are sorted in ascending order.", "description": "This figure shows the performance of two reward learning methods, Pref and Ada-Pref, on three robotic control tasks. The top part displays learning curves, showing the average return over time for each method. The bottom part shows percentile plots, illustrating the distribution of returns across multiple independent runs. Ada-Pref consistently outperforms Pref across all tasks and percentiles.", "section": "4.1 Robotic control"}, {"figure_path": "GnaFrZRHPf/figures/figures_16_2.jpg", "caption": "Figure 2: Learning curve plots (top) and percentile plots (bottom) for Pref and Ada-Pref. For the learning curve plots, returns at each timestep are averaged across 10 different seeds, then smoothed over timesteps using an exponential moving average (EMA) with a smoothing factor of \u03b1 = 0.1. For the percentile plots, returns from 10 different seeds are sorted in ascending order.", "description": "This figure compares the performance of the proposed Ada-Pref method and the baseline Pref method across three robotic control tasks (HalfCheetah, Ant, and Hopper). The top row shows the learning curves for both methods, where the average return is plotted against the number of timesteps. The bottom row shows the percentile plots, which display the distribution of returns across multiple trials. The figure demonstrates that Ada-Pref consistently outperforms Pref in terms of both average return and percentile performance across all three tasks.", "section": "4.1 Robotic control"}, {"figure_path": "GnaFrZRHPf/figures/figures_16_3.jpg", "caption": "Figure 8: Left: Learning curve and percentile plot for Pref and Ada-Pref-Quad. Middle: Histogram of the learned scaling factors. Right: Relationship between preference strength and the learned scaling factors, and relationship between preference strength and the learned reward difference. All plots are from the Ant task.", "description": "This figure presents a comprehensive analysis of the Ant task, comparing the performance of Pref and Ada-Pref-Quad.  It includes four sub-figures. (a) shows the learning curves, illustrating the performance improvement of Ada-Pref-Quad over Pref across time steps. (b) displays percentile plots, demonstrating the consistent superiority of Ada-Pref-Quad across different random seeds. (c) provides a histogram of the learned scaling factors (\u03c4), visualizing their distribution. Finally, (d) and (e) illustrate the relationship between preference strength and both the learned scaling factor and the resulting reward difference, highlighting how Ada-Pref-Quad adapts to varying preference strengths more effectively than Pref.", "section": "C.1 Experiments with quadratic regularization"}, {"figure_path": "GnaFrZRHPf/figures/figures_17_1.jpg", "caption": "Figure 2: Learning curve plots (top) and percentile plots (bottom) for Pref and Ada-Pref. For the learning curve plots, returns at each timestep are averaged across 10 different seeds, then smoothed over timesteps using an exponential moving average (EMA) with a smoothing factor of \u03b1 = 0.1. For the percentile plots, returns from 10 different seeds are sorted in ascending order.", "description": "This figure shows the performance comparison between the baseline method (Pref) and the proposed adaptive preference scaling method (Ada-Pref) on three robotic control tasks. The top row presents learning curves, illustrating the average return over time for each method across multiple trials. The bottom row displays percentile plots, showing the distribution of returns across different trials for each method and providing a more robust measure of performance.  Ada-Pref consistently outperforms Pref across all tasks and percentiles.", "section": "4.1 Robotic control"}, {"figure_path": "GnaFrZRHPf/figures/figures_18_1.jpg", "caption": "Figure 10: Hyperparameter sensitivity of \u03c1.", "description": "This figure shows the impact of the hyperparameter \u03c1 on the performance of the model in terms of rewards and win rate. The left plot shows the relationship between \u03c1 and the average return achieved by the model.  The right plot shows the relationship between \u03c1 and the win rate, reflecting the model's ability to correctly predict human preferences.  The plots demonstrate that there is an optimal range for \u03c1, outside which performance decreases.", "section": "C.3 Discussions on hyperparameter tuning"}]