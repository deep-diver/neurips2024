[{"type": "text", "text": "Learning to Reason via Program Generation, Emulation, and Search ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nathaniel Weir\u2217\u2020 Muhammad Khalifa\u2217\u2020 Linlu Qiu\u2217 Johns Hopkins University University of Michigan MIT nweir@jhu.edu khalifam@umich.edu linluqiu@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Orion Weller\u2217 Johns Hopkins University oweller@cs.jhu.edu ", "page_idx": 0}, {"type": "text", "text": "Peter Clark Allen Institute for AI peterc@allenai.org ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Program synthesis with language models (LMs) has unlocked a large set of reasoning abilities; code-tuned LMs have proven adept at generating programs that solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word concatenation). However, not all reasoning tasks are easily expressible as code, e.g. tasks involving commonsense reasoning, moral decision-making, and sarcasm understanding. Our goal is to extend an LM\u2019s program synthesis skills to such tasks and evaluate the results via pseudo-programs, namely Python programs where some leaf function calls are left undefined. To that end, we propose, Code Generation and Emulated EXecution (COGEX). COGEX works by (1) training LMs to generate pseudo-programs, (2) teaching them to emulate their generated program\u2019s execution, including those leaf functions, allowing the LM\u2019s knowledge to fill in the execution gaps; and (3) using them to search over many programs to find an optimal one. To adapt the COGEX model to a new task, we introduce a method for performing program search to find a single program whose pseudo-execution yields optimal performance when applied to all the instances of a given dataset. We show that our approach yields large improvements compared to standard in-context learning approaches on a battery of tasks, both algorithmic and soft reasoning. This result thus demonstrates that code synthesis can be applied to a much broader class of problems than previously considered.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently there have been rapid advances in training language models (LMs) to generate code rather than natural language (NL), following the intuition that code may be more effective than NL for certain tasks, such as those requiring complex calculations, iteration, or data structure manipulation(Chen et al., 2022; Gao et al., 2023). Although successful, these works have mostly studied tasks conducive to a programmatic paradigm, such as symbolic manipulation or algorithmic reasoning, i.e., tasks for which a clear compilable program can be devised. However, it is unclear how to apply this approach to \u201csofter\u201d reasoning tasks such as commonsense and social reasoning, where algorithmic solutions are less obvious (Zhang et al., 2023a). ", "page_idx": 0}, {"type": "image", "img_path": "te6VagJf6G/tmp/ece75161e8db12a55e1568b6537e652e974498b1da7ffb91a739f5107847d58f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Example from the COGEX dataset automatically converted from an Alpaca (Taori et al., 2023) instance via LLM prompting. We train the model to receive the instruction and input and generate the Python program and function call (as an intermediate), before outputting the final dictionary that contains the answer and any intermediate reasoning steps. ", "page_idx": 1}, {"type": "text", "text": "Our goal is to expand an LM\u2019s program synthesis skills to such softer reasoning tasks. Our approach builds on the insight that, beyond generating code, LMs can also emulate the execution of code. This includes handling function calls that are defined only by name and documentation, even if they lack a full implementation. We refer to these types of programs\u2014where only the function skeletons are provided without actual code implementations\u2014as pseudo-programs. Such pseudo-programs can encompass both well-defined reasoning steps, such as mathematical or algorithmic operations, as well as function calls representing less precise reasoning, such as commonsense logic. This work investigates whether generating and pseudo-executing such programs can effectively address soft reasoning tasks in addition to traditional algorithmic problems. ", "page_idx": 1}, {"type": "text", "text": "To achieve this, we propose a novel approach: training models to follow NL instructions by generating a program and then emulating that program\u2019s code execution. Our paradigm, called COGEX, changes the inference process to (1) generate a Python function given an arbitrary instruction and optional input, (2) generate a call to that function, and (3) produce the result of simulating its execution. Unlike other work (Zhang et al., 2023b; Li et al., 2023), we do not use a Python interpreter to execute any code; rather, the model is trained to emulate execution. This allows the generated code to deliberately include calls to underspecified functions (i.e., where only the function name and documentation are included), by allowing the LM emulator to fill in the functions implementations using its latent knowledge. We train the model to not only output the result of pseudo-executing the code but also the results of intermediate function calls in the code. COGEX thus suggests a way to leverage the flexible ad-hoc reasoning abilities of LMs while encouraging programmatic reasoning via the program structure. We train COGEX models by adapting the recent Alpaca instruction tuning dataset (Taori et al., 2023) into a set of analogous Pythonic examples by prompting GPT-4 to perform the conversion, and then use the resulting COGEX dataset to fine-tune smaller (7B and 13B) LMs to answer instructions via code. ", "page_idx": 1}, {"type": "text", "text": "The COGEX paradigm allows us to explore a new way to learn new tasks: identifying a general program that applies across a task, such that new task instances can be solved by emulating calls to that one program. Inspired by work on hard prompt tuning (Wen et al., 2024) and example selection for in-context learning (Gupta et al., 2023a), we introduce a search procedure that uses the frozen COGEX model to try out many programs on a set of training examples and identify which single program is optimal for the dataset. The procedure, termed COTACS: COGEX Task Adaptation via Code Search, performs no parameter updates and requires saving nothing more than a program string. ", "page_idx": 1}, {"type": "text", "text": "We evaluate over a diverse suite of reasoning tasks, including commonsense QA, text classification, and math datasets. These datasets cover symbolic tasks that could conceptually benefit from programmatic operations and standard natural language tasks whose solutions might not be easily described in code. We find that applying COTACS leads the COGEX models to substantially outperform the comparable NL-based LM using the same original checkpoint and the same set of training examples available for in-context learning, even in the few-shot regime. COTACS thus gives us one way to fit a model to a new dataset without having to perform any gradient descent or parameter updates, both for algorithmic and softer reasoning datasets. Our contributions are thus: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1. A novel reasoning paradigm, COGEX, that trains language models to generate and emulate the execution of pseudo-programs. COGEX is a general paradigm that allows LMs to leverage code for different types of reasoning. 2. A program search method, COTACS, enabling a task-general program suitable for a dataset (rather than a single instance) to be found using a COGEX model. 3. A dataset, derived from the Alpaca instruction tuning dataset, for training COGEX models. ", "page_idx": 2}, {"type": "text", "text": "Overall, this work provides a significant step in showing how code generation can be applied to a much broader class of problems than previously considered. ", "page_idx": 2}, {"type": "text", "text": "2 Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we start by formalizing our approach and describing our data construction process (\u00a72.1). We then describe our program search approach to tune a COGEX model on a given task through program search (\u00a72.2). ", "page_idx": 2}, {"type": "text", "text": "2.1 Method: COGEX ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formulation. Our goal is for the model to execute a given task by simulating code execution. That means our model will take as input the task description, generate a Python program, and simulate the expected output of executing that program. Formally, given a natural language (NL) task description $I$ , optional input argument $A$ , Python function $F$ , function call $C$ , and output dictionary $O$ designating the output from the program pseudo-execution, the LM will take $\\langle I,A\\rangle$ as input and generates $\\langle F,C,O\\rangle$ as output. Since the process is sequential, COGEX models can work as either a reasoner $f_{\\mathrm{reasoner}}(I,A)\\,\\stackrel{*}{\\to}(P,C)\\to O$ or as a call-instantiating and execution-emulating model $f_{\\mathrm{emulator}}(I,A,P)\\,\\to\\,C\\,\\to\\,O$ that takes a pre-specified program $P$ and applies it to the variable arguments $A$ . This latter formulation enables searching over the space of task-specific programs: searching for one $P_{\\mathrm{{task}}}$ to solve a class of problem (e.g., emotion classification) and then applying $f_{\\mathrm{emulator}}(\\bar{I},A_{i},P_{\\mathrm{task}})$ to emulate its execution on each instance $A_{i}$ of that problem. We expand on program search in $\\S2.2$ . ", "page_idx": 2}, {"type": "text", "text": "Training Data Construction. As we want a general-purpose dataset that spans tasks with diverse reasoning requirements, we choose the Alpaca instruction tuning dataset (Taori et al., 2023). Following Peng et al. (2023), we rely on GPT- $\\cdot4^{2^{\\cdot}}$ to convert the Alpaca dataset into their COGEX versions. Specifically, every NL instance in the Alpaca dataset is mapped into a corresponding COGEX version. We split the conversion process into three steps, each of which involves prompting GPT-4 with the output from the previous. This stepwise approach proved more effective than directly prompting GPT to convert each instance to code in one shot. ", "page_idx": 2}, {"type": "text", "text": "As depicted in Figure 1, the three steps are: (1) converting the outputs and (optional) inputs into Pythonic data structures like strings, lists, and integers whenever relevant as determined by GPT-4; (2) generating an instruction-specific plan, or a series of NL steps that should perform the task for any potential input; (3) instantiating the plan as a Python program whose inline comments are the plan steps and whose output is a dictionary containing all intermediate and final outputs that the LM believes would result from executing each step. Prompts for all steps can be found in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Importantly, we allow for GPT to include undefined functions, e.g., identify_ending() and find_pluralization_rule() in Figure 1. The goal is to leverage the LM knowledge to fill in the semantics of these undefined functions when emulating the execution of a given program. In addition, we include the program\u2019s intermediate results in the output dictionary before the final answer to encourage the model to stick to the NL reasoning plan delineated in the program comments. After defining the program, we cue GPT-4 to call the function on an argument, e.g. pluralize_word(\u2018corpus\u2019) which can reflect the optional Alpaca example input, or can reflect specific details from the instruction itself. Our prompts encourage GPT-4 to write a program that is as general purpose as possible and not tied to a specific input: e.g. pluralize_word(word) is preferable to pluralize_corpus(). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Fine-tuning any LM on the resulting COGEX dataset creates our desired model, which accepts any task description/input combination and responds by dynamically generating a Python program and then emulating its execution. ", "page_idx": 3}, {"type": "text", "text": "2.2 Program Search: COTACS ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A COGEX model can generate a new program for any new task instruction and instance; however, some programs might be more or less effective at performing the task. How can we find the optimal program for a specific task, especially when some training data is available? As COGEX relies on argument-accepting pseudo-programs, it naturally enables program optimization. Given multiple examples of a new task, we can search for one program that performs well, and then apply the same program to new test examples by invoking the program with different input arguments. ", "page_idx": 3}, {"type": "text", "text": "Our search process, COTACS: COGEX Task Adaptation via Code Search, finds a single program that optimizes a COGEX model for a particular task dataset, enabling adapting a COGEX model to a given task without learning any weight parameters. We learn a new dataset simply by using a finetuned COGEX model to generate and then evaluate many program candidates to find the one that best fits the given dataset. As described in algorithm 1, we split a dataset $D$ of argument and output pairs $(a_{i},o_{i})$ into a small training set $n=300$ in experiments) and a larger development set; we then generate a separate code candidate for every training item and retain the programs with decent performance on the training set.3 We then rank these programs according by their performance on the development set. For certain tasks, we find it beneficial to find multiple programs for a task and then at test time take a majority vote across the COGEX model\u2019s responses using each code. To accomplish this, we retain some top- $k$ performing codes over the development set. ", "page_idx": 3}, {"type": "table", "img_path": "te6VagJf6G/tmp/ec1d7ad5fcb4d891d251e2fed87ac448e7c09d02452c51f2f935865e083fb76a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "table", "img_path": "te6VagJf6G/tmp/e2c8006baabfa189ef56ae00b5caaaba811c1991a6bf5ee0632b4ca99b8fd977.jpg", "table_caption": ["Table 1: Benchmark results by COGEX models optimized for each dataset using the COTACS method, compared to the corresponding off-the-shelf Llama-2 checkpoint performing 2-shot reasoning using a BM25 retrieval index of 1000 exemplars. Results are also compared to a zero-shot Alpaca model fine-tuned from the same checkpoint. The top score per size is bolded. Colored cells indicate changes ( gains , losses , or the same ) relative to the best-performing non-COGEX baseline (Alpaca or 2-shot). Results show that COGEX with COTACS outperforms the baselines on nearly every task and often does so even with only 10 examples. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3 Experiments and Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Below we describe the training and evaluation of COGEX models. We first show overall performance across a wide array of tasks compared to off-the-shelf baselines $(\\S3.2)$ then ask a series of follow-up research questions investigating ablated scenarios (\u00a73.3). ", "page_idx": 4}, {"type": "text", "text": "3.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Model Training. We fine-tune the 7B and 13B variants of Llama-2 (Touvron et al., 2023). We use parameter-efficient training via Low-rank adaptation (LoRA) (Hu et al., 2021) with a rank of $r=16$ , dropout rate of 0.05, and LoRA weights added to the query, key, value, and output matrices in all layers. We train all models for five epochs using a batch size of 32 and a learning rate of 0.0003. As a validation set, we randomly sample 2K examples from the training set and keep the checkpoint with the lowest perplexity on the validation set for testing. Model training was done on a server with 128GB of RAM and 2 Nvidia A6000 48GB GPUs. On our dataset, training a single 7B model and 13B models took around 12 and 20 hours, respectively. To ensure a fair comparison, all the baselines are trained with the exact same hyperparameters. ", "page_idx": 4}, {"type": "text", "text": "Datasets. We measure COGEX model performance on a variety of benchmarks ranging from symbolic manipulation to commonsense and social reasoning. We choose these datasets as representatives of tasks that involve complex reasoning and tasks whose solutions cannot be easily described in code. As for symbolic and math reasoning, we use the Word Sorting task from BIG-bench hard (Srivastava et al., 2022; Suzgun et al., 2022), the math word problem dataset SVAMP (Patel et al., 2021), the coin flip tracking dataset from Wei et al. (2022), and the large number arithmetic task (referred to as Sum) from Zelikman et al. (2022). For the last, we use the 5-digit examples for training and 6-digit for testing. We measure string-normalized exact match accuracy for all tasks. Following Zhang et al. (2023b), we evaluate on a series of Text Classification datasets: CoLA (2 labels) (Warstadt et al., 2019), Emotion (6 labels) (Saravia et al., 2018), and SST2 (2 labels) (Socher et al., 2013). We also evaluate on the Commonsense Reasoning datasets CommonsenseQA (Talmor et al., 2019) and Social IQa (Sap et al., 2019), which are 4- and 5-way multiple-choice datasets. We hand-write the instruction $I_{\\mathrm{task}}$ for these datasets as they do not provide any. ", "page_idx": 4}, {"type": "text", "text": "Code Search. For all datasets, we use a maximum of 1000 training examples. We use $n=300$ training items to generate candidate codes and evaluate them on the remaining 700 items to identify the most generalizable ones. We experiment with retaining the top- $k\\in\\{1,3\\}$ performing programs for use at test time. For $k=3$ , we take a majority vote of answers, breaking ties randomly. We use ", "page_idx": 4}, {"type": "text", "text": "Table 2: Difference in performance by COTACS $k=3$ comparing Llama-2 vs Code Llama COGEX models ( $\"+\"$ implies Llama-2 better). We see that Code Llama is more effective for some tasks but worse on others, while the 13B version performs worse than the 13B Llama-2 on all but 2 tasks. ", "page_idx": 5}, {"type": "table", "img_path": "te6VagJf6G/tmp/77c30652986238e50c27d483aaf78552850633e0cb487c15575267a4b6369e1a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "sampling temperature $t=0.7$ when generating candidate codes and $t=0.05$ to generate answers given a program and argument. We report results on the released dev sets of all considered tasks. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We consider two baselines that represent standard practices for adapting an LM to a new task: (1) few-shot prompting using the off-the-shelf Llama-2 and CodeLlama models and (2) zero-shot prompting using the Llama-2 models instruction-tuned from the original Alpaca dataset.4 For the in-context learning baseline for (1) we use the same 1000 training data-points as COTACS and optimize the examples by retrieving the most similar few-shot examples using BM25. For zero-shot alpaca models, we use the standard Alpaca-7B and -13B models. As COTACS might not require many training examples to achieve strong performance, we compare the 1000-example COTACS run with one that only uses 10 total examples to generate and evaluate candidates. ", "page_idx": 5}, {"type": "text", "text": "3.2 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our main results depicting the difference in performance between COGEX models tuned via COTACS versus off-the-shelf few-shot baselines and Alpaca models are shown in Table 1. The COTACS method with 1000 training examples outperforms the baselines for a large majority of tasks and models (8/9 tasks for both Llama-2 7B and 13B). COGEX shows particularly strong gains over the baselines in the Sum and coin flip tracking tasks $(+10\u201320\\%)$ as expected due to its code-related nature. We observe that the COTACS method with $N_{\\mathrm{train}}=1000$ training examples performs best on average across the 9 tasks, and still performs better than the baselines with only $N_{\\mathrm{train}}=10$ examples. Retaining the top $k=3$ programs instead of 1 improves performance in most cases ${}\\!+\\!1\\%$ average). ", "page_idx": 5}, {"type": "text", "text": "Instruction Following. As our models are trained on instruction following in code, can they still perform instruction-related tasks as well as models trained on text-only Alpaca? We verify this by using alpaca-eval to compare Alpaca-7B against our COGEX-7B model trained from the same base Llama model. We find a similar win rate $50\\%$ within the 2 SD range) indicating similar instruction following ability. Thus we can see that training on code-based instructions does not hurt standard instruction-following abilities, while opening up many possibilities for program search. ", "page_idx": 5}, {"type": "text", "text": "Effect of Code Pre-training. As we are fine-tuning LMs on code data and then evaluating them on tasks that are more or less code-related, a natural question to ask is whether LMs pre-trained on code datasets yield stronger COGEX models. We investigate this by fine-tuning Code Llama (Roziere et al., 2023) on the COGEX dataset instead of Llama. Table 2 shows the resulting change in performance using COTACS $(k{=}3)$ program search. The Llama-2 models show improved performance on Social IQa $(+4\\%)$ but much worse on coin flip tracking $(-10-12\\%)$ ). These results do not provide conclusive evidence that Llama-2 models are better or worse than Code Llama on particular task categories. ", "page_idx": 5}, {"type": "text", "text": "3.3 Ablation Studies ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here we present a series of ablation studies to ask the following questions: ", "page_idx": 5}, {"type": "text", "text": "How many training examples are needed for search? In the above experiments, we chose 1000 training examples and 300 program candidates for the COTACS algorithm. This raises the question: how many examples are required to yield the strong performance provided by the search? We investigate this by simulating the algorithm and sampling 1000 trials with varying numbers of training examples and program candidates. Results are shown in Figure 2. In nearly all cases, performance with 50 or 200 training examples is within a couple of points of the full performance with the 300/1000 configuration. The performance when sampling 10 code candidates (green) lands within 2 points of sampling 300 candidates on 5 of the 7 datasets. Benefits do not appear on Word Sorting, as performance lands between 0.51 and 0.515 regardless of configuration. This suggests that the range of quality in generated programs for the Word Sorting dataset is much smaller than the others, so picking between just a few candidates is sufficient. Overall, we see that we can significantly reduce the search space and still see large gains on most tasks. ", "page_idx": 5}, {"type": "image", "img_path": "te6VagJf6G/tmp/cd2d6dbd652e9d5603e91864a73768bdbbcbd795f75c50472fc18b4d3943ce9d.jpg", "img_caption": ["Figure 2: Change in COTACS performance using Llama-2 13B as we increase the number of training items from 5 to 1000 and program candidates from 3 to 300. Results are averaged across 1000 trials. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Is it better to execute an NL plan instead of Python code? We have proposed a mechanism to generate Python programs whose steps are meant to reflect the reasoning process, stated in NL, that answers a given instruction. Is the code necessary, or can a model be trained to generate only the plan and achieve the same performance? We fine-tune a Llama model on a version of the COGEX 52k-item training set where each Python program has been replaced with just the NL steps (removing step 3 of Figure 1). This NL Plan model still returns the same output dictionary with intermediate results. To fit the plan-only model to a dataset, we run the COTACS algorithm but sample and retain the NL plans instead of programs. We see in Figure 4 (orange vs gold) that NL Plan COTACS can match the performance of regular program COTACS on some tasks (CoLA, SST, Word Sorting, Emotion), but performs much worse on others, particularly Coin Flip, SVAMP, and Sum. This follows the intuition that these tasks benefit from a programmatic reasoning paradigm. ", "page_idx": 6}, {"type": "text", "text": "Is it better to find one program or generate a new one for each instance? COTACS finds one or multiple programs that can be reapplied to all task instances in a dataset to achieve high performance. Is this better than letting the COGEX model generate a separate program for every instance? It might be the case that the latter allows for catering the program to the specifics of a particular task instance\u2013 e.g. in Figure 3, where the left (red) COGEX-generated program has steps specifically crafted to identify actions to be taken by a particular person. Finding a single program disallows this flexibility. We investigate this question by running the model end-to-end on each instance. The COTACS model performs the mapping $f(I_{\\mathrm{task}},A_{i},P_{\\mathrm{CoTACS}}^{\\bar{-}})\\rightarrow C_{i}\\rightarrow O_{i}$ for each task instance $A_{i}$ , while the end-to-end model performs $f(I_{\\mathrm{task}},A_{i})\\rightarrow(P_{i},C_{i})\\rightarrow O_{i}$ . We sample from the latter using temperature $t=0.05$ .5 Results are shown in Figure 4 (maroon vs gold); end-to-end performance is comparable to COTACS only on Word Sorting and Sum. In all other cases, it is substantially worse. ", "page_idx": 6}, {"type": "image", "img_path": "te6VagJf6G/tmp/f239731f35f83c7790be73f52c4049e8b8ea59aa2e0bd92a0fd5871d7aae0e17.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Example COGEX model-generated programs for Social IQa (Sap et al., 2019) questions. The left item fits well to a specific SocialIQa question pertaining to question-specific entities but does not generalize well to the dataset, while the right item applies more generally to cases such as the instance shown at the bottom, which does not pertain to character actions. Applying COTACS to identify a single program such as the right one shows to improve overall task accuracy. ", "page_idx": 7}, {"type": "image", "img_path": "te6VagJf6G/tmp/4cac583cb557097e4ab373f0723181f8108fdfed3ee58eedf8163920ecb3684e.jpg", "img_caption": ["Figure 4: Performance comparison between COTACS $\\scriptstyle{k=1}$ ) and various ablations: (1) COGEX End-to-End that generates separate programs for each instance, (2) chain-of-thought prompting, and (3) searching for an optimal NL plan instead of code program. COTACS consistently equals or outperforms all ablations on all tasks, while each ablation drops in performance on at least 2-3 tasks. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Is COTACS better than chain-of-thought? A common practice to elicit systematic reasoning from LMs is to prompt it for the reasoning via some version of \u201cexplain your answer step-by-step\u201d (Kojima et al., 2022). How does this compare to COGEX models on a given dataset? We compare COGEX to zero-shot CoT by prompting our Alpaca models with a task-specific instruction, while additionally appending the instruction to \u201cthink step-by-step\u201d before producing the answer. Figure 4 (blue vs gold) shows that CoT prompting performs similarly to the NL plan search method; it can approach COTACS performance on some NLP classification tasks but performs worse over SVAMP, Number Summing, and CoinFlip. ", "page_idx": 7}, {"type": "text", "text": "When is COTACS better than fine-tuning? Fine-tuning is a standard practice to adapt an LM to a new task. However, fine-tuning often requires a large amount of training data and storing a new model for each task. Here, we study the impacts of the number of examples on fine-tuning and COTACS. We find that when there are many examples available, fine-tuning achieves stronger performance. However, COTACS is generally better until there are a large number of examples available: it outperforms fine-tuning on 4/9 tasks with 500 examples. This suggests that COTACS can be a lightweight alternative in the low-to-medium shot setup. ", "page_idx": 7}, {"type": "image", "img_path": "te6VagJf6G/tmp/1d1e7a110962013257ad7ad46ac138ca6630f70717165acc5b3d00e230bec4e6.jpg", "img_caption": ["Figure 5: Performance tradeoff between COTACS, which requires saving just a program string, and fine-tuning, which requires saving an entire checkpoint, as we increase the number of training examples. Although fine-tuning typically performs better with more data, COTACS provides an alternative that is lighter-weight and stronger at low-to-medium numbers of instances. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "te6VagJf6G/tmp/95633bc4b92e1a982035e2ad0b2b93faef60f82cab40d0d51c553a8abb96bc57.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: Qualitative examples of LLama-2 13B the coin flip tracking task where the model fails to correctly simulate the program and is correct for the wrong reasons. ", "page_idx": 8}, {"type": "text", "text": "3.4 Qualitative Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Since we rely on the LM as a code emulator, there is no guarantee of correct execution. The generated intermediate outputs allow us to examine if the model can faithfully emulate the program. We observe failure cases where the LM incorrectly simulates the program execution even if the generated program is correct as shown in Figure 6. We also include positive qualitative examples in Appendix B. ", "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Reasoning via Code. Using code for reasoning is a burgeoning area that has shown improved results on many algorithmic tasks (Chen et al., 2022; Gao et al., 2023). Many approaches ask LLMs to express their reasoning as code and leverage code interpreters to execute them. Recently, and concurrent with our work, some studies investigate training LLMs as code compilers, where the LM is prompted to emulate code execution (Li et al., 2023; Chae et al., 2024; Mishra et al., 2023). These ", "page_idx": 8}, {"type": "text", "text": "LM-as-compiler approaches fall into a broader category of work that invokes LLMs as subroutines in programs (Kalyanpur et al., 2022; Weir et al., 2024). Different from ours, these works mainly rely on manually prompting very large models, while we focus on training open-source LMs to both generate and emulate programs. In addition, we aim to achieve task generalization by searching for an optimal program for a given task\u2014different from Chae et al. (2024) who rely on prompting LMs with specific code instructions. Ours is the first work on code-based reasoning that employs search over the program space with the goal of generalizing an optimal program to a task. ", "page_idx": 9}, {"type": "text", "text": "Prompt Optimization. Our search procedure, COTACS, has a similar spirit to in-context learning optimization approaches where the goal is to find an optimal set of exemplars (an optimal pseudoprogram, in our case) for a given task. Existing studies (Zhang et al., 2022; Rubin et al., 2022; Ye et al., 2023a; Gupta et al., 2023b; Khalifa et al., 2023a) explored various methods to select optimal in-context examples, leveraging similarity- or diversity-based heuristics\u2014to name a few. Searching for useful task instructions has also been explored (Honovich et al., 2022; Khalifa et al., 2023b; Chen et al., 2023). ", "page_idx": 9}, {"type": "text", "text": "Another related area of research is automated prompt engineering (Shin et al., 2020; Deng et al., 2022; Prasad et al., 2023) that bootstraps an effective prompt using some reward function. While LMs have been shown to be effective at producing their own prompts (Zhou et al., 2022; Yang et al., 2024; Pryzant et al., 2023; Ye et al., 2023b), our work shows that LMs can also reason by generating and executing their own generated programs. Our method differs from these studies as it uses the same input instruction and optimizes the intermediate representation, rather than modifying it via prompt optimization. Finding a single program string to solve a class of problems is also related to finding a high-level NL description of a task using one or multiple demonstrations (Weir et al., 2023). ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present COGEX, a methodology that trains language models to generate and execute their own Pythonic reasoning programs in response to task instructions. We convert the Alpaca instruction tuning data into COGEX instances that can be used to COGEX-tune any models. We design an optimization algorithm, COTACS, that applies COGEX models to a new dataset by generating and searching through possible programs that can be reapplied to new task items. Applying the COTACS search algorithm yields task performance that exceeds that of few-shot in-context-learning and typical NL instruction following. Our work demonstrates a way to apply LM-based programmatic reasoning to NLP benchmarks that require softer reasoning skills not easily stated in code syntax. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While our work represents a step towards utilizing code language models for non-algorithmic reasoning tasks, COGEX still suffers from the following limitations: ", "page_idx": 9}, {"type": "text", "text": "\u2022 COGEX is suitable for soft reasoning tasks for which step-by-step programs are difficult to describe. However, when solving algorithmic tasks where a precise step-by-step program is possible, passing the generated code directly to an interpreter may be preferable to emulating code execution via the LM.   \n\u2022 We have found that the LM can occasionally produce a result that is inconsistent with the code emulated, as noted in subsection 3.4. In which case, the code does not faithfully reflect the reasoning process followed by the model.   \n\u2022 There is an extra computational overhead when emulating code execution via an LM compared to using an interpreter as the LM needs to generate intermediate variables along with the final answer. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Li Zhang, Valentina Pyatkin, and Khyathi Chandu for feedback on ideas and earlier drafts. We also thank the organizers of the AI2 Summer 2023 Hackathon during which this project was initially conceived. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, Jiwan Chung, Youngjae Yu, et al. Language models as compilers: Simulating pseudocode execution improves algorithmic reasoning in language models. arXiv preprint arXiv:2404.02575, 2024.   \nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero: Efficient instruction optimization for black-box large language models. arXiv preprint arXiv:2306.03082, 2023.   \nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.   \nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3369\u20133391, 2022.   \nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: program-aided language models. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.   \nShivanshu Gupta, Matt Gardner, and Sameer Singh. Coverage-based example selection for in-context learning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 13924\u201313950, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.930. URL https://aclanthology.org/2023.findings-emnlp.930.   \nShivanshu Gupta, Matt Gardner, and Sameer Singh. Coverage-based example selection for incontext learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 13924\u201313950, 2023b.   \nOr Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022.   \nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nAditya Kalyanpur, Tom Breloff, and David A Ferrucci. Braid: Weaving symbolic and neural knowledge into coherent logical explanations. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pp. 10867\u201310874, 2022.   \nMuhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. Exploring demonstration ensembling for in-context learning. arXiv preprint arXiv:2308.08780, 2023a.   \nMuhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. Few-shot reranking for multi-hop QA via language model prompting. In Anna Rogers, Jordan L. BoydGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 15882\u201315897. Association for Computational Linguistics, 2023b. doi: 10.18653/V1/ 2023.ACL-LONG.885. URL https://doi.org/10.18653/v1/2023.acl-long.885.   \nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199\u201322213, 2022.   \nChengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with a language model-augmented code emulator. arXiv preprint arXiv:2312.04474, 2023. ", "page_idx": 10}, {"type": "text", "text": "Mayank Mishra, Prince Kumar, Riyaz Ahmad Bhat, Rudra Murthy, Danish Contractor, and Srikanth G Tamilselvam. Prompting with pseudo-code instructions. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. ", "page_idx": 11}, {"type": "text", "text": "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080\u20132094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.   \nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.   \nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction search for prompting large language models. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 3845\u20133864, 2023.   \nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with\" gradient descent\" and beam search. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.   \nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.   \nOhad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2655\u20132671, 2022.   \nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4463\u20134473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454.   \nElvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER: Contextualized affect representations for emotion recognition. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3687\u20133697, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1404. URL https: //aclanthology.org/D18-1404.   \nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.   \nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew $\\mathrm{Ng}$ , and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard (eds.), Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170.   \nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.   \nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.   \nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149\u20134158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421.   \nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ 1PL1NIMMrw.   \nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641, 2019. doi: 10.1162/ tacl_a_00290. URL https://aclanthology.org/Q19-1040.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \nNathaniel Weir, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Matthew Hausknecht, Romain Laroche, Ida Momennejad, Harm Van Seijen, and Benjamin Van Durme. One-shot learning from a demonstration with hierarchical latent language. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, pp. 2388\u20132390, 2023.   \nNathaniel Weir, Peter Clark, and Benjamin Van Durme. NELLIE: A neuro-symbolic inference engine for grounded, compositional, and explainable reasoning. IJCAI, 2024.   \nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Advances in Neural Information Processing Systems, 36, 2024.   \nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ Bb4VGOWELI.   \nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context learning. In International Conference on Machine Learning, pp. 39818\u201339833. PMLR, 2023a.   \nQinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. Prompt engineering a prompt engineer. arXiv preprint arXiv:2311.05661, 2023b.   \nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.   \nLi Zhang, Liam Dugan, Hainiu Xu, and Chris Callison-burch. Exploring the curious case of code prompts. In Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE), 2023a.   \nTianhua Zhang, Jiaxin Ge, Hongyin Luo, Yung-Sung Chuang, Mingye Gao, Yuan Gong, Xixin Wu, Yoon Kim, Helen Meng, and James Glass. Natural language embedded programs for hybrid language symbolic reasoning. arXiv preprint arXiv:2309.10814, 2023b.   \nYiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 9134\u20139148, 2022.   \nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.   \nClean up the following json items. Map the input and output fields in each item to a proper pythonic item (e.g. list, dictionary, or clean string). It shouldn $^>\\t$ have newlines if it\u2019s a string. DO NOT INCLUDE \"...\" in your outputs.   \nINPUT 1:   \n{\u2018instruction\u2019: \u2018Classify the following objects by color.\u2019, \u2018input\u2019: \u2018Ribbon, Tie, Pen\u2019, \u2018output\u2019: \u2018-Red: Ribbon\\n-Blue: Tie\\n-Black: Pen\u2019}   \nOUTPUT 1:   \n{\u2018instruction\u2019: \u2018Classify the following objects by color.\u2019, \u2018input\u2019: [\u2018Ribbon\u2019, \u2018Tie\u2019, \u2018Pen\u2019], \u2018output\u2019: {\u2018 Ribbon\u2019: \u2018Red\u2019, \u2018Tie\u2019: \u2018Blue\u2019, \u2018Pen\u2019: \u2018Black\u2019}   \nINPUT 2:   \n{\u2018instruction\u2019: \u2018Convert the following text into a list.\u2019, \u2018input\u2019: \u2018The four elements of design are line, color, shape, and texture.\u2019, \u2018output\u2019: \u2018- Line \\n- Color \\n- Shape\\n- Texture\u2019}   \nOUTPUT 2:   \n{\u2018instruction\u2019: \u2018Convert the following text into a list.\u2019, \u2018input\u2019: \u2018The four elements of design are line, color, shape, and texture.\u2019, \u2018output\u2019: [\u2018line\u2019, \u2018color\u2019, \u2018shape\u2019, \u2018texture\u2019]}   \nINPUT 3:   \n{\u2018instruction\u2019: \u2018Generate a list of five items a person might need for a camping trip\u2019, \u2018input\u2019: \u2018\u2019, \u2018 output\u2019: \u20181. Tent\\n2. Sleeping bags\\n3. Flashlight\\n4. Matches/lighter\\n5. Insect repellent}   \nOUTPUT 3:   \n{\u2018instruction\u2019: \u2018Generate a list of five items a person might need for a camping trip\u2019, \u2018input\u2019: \u2018\u2019, output\u2019: [\u2018tent\u2019, \u2018sleeping bags\u2019, \u2018flashlight\u2019, \u2018matches/lighter\u2019, \u2018insect repellent\u2019]}   \nINPUT 4: ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "{input} ", "page_idx": 14}, {"type": "text", "text": "OUTPUT 4: ", "page_idx": 14}, {"type": "text", "text": "Figure 7: Prompt used for converting inputs and outputs of Alpaca items into Pythonic data types. ", "page_idx": 14}, {"type": "text", "text": "Generate a high-level plan with at most 3 steps that a problem-solving artificial agent could use to complete the following problem. If the problem takes in inputs, your plan should be a high-level abstraction that is generally applicable to new inputs, not just the one shown here.   \nInstruction: {instruction}   \nInput: {possible_inputs}   \nYour output format should be a series of serialized jsons, 1 per line, for each step of the plan.   \nThey should have the format {\"number\": <step number>\", \"description\": <step description>} ", "page_idx": 14}, {"type": "text", "text": "Figure 8: Prompt used for generating stepwise NL plans for Alpaca items. ", "page_idx": 14}, {"type": "text", "text": "A Prompts for Converting Alpaca to COGEX ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 7, Figure 8, and Figure 9 display the prompts used to GPT-4 in sequence to convert Alpaca into the COGEX dataset. The prompts (1) convert all inputs and outputs into Pythonic types like strings, lists and dicts, (2) generate plans to answer a given instruction, and (3) instantiate each plan as a Python program with underspecified function calls. ", "page_idx": 14}, {"type": "text", "text": "B Further Qualitative Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 10 and Figure 11 show good qualitative examples generated by COTACS, along with outputs from the 2-shot prompting baseline for the text classification and math reasoning tasks, respectively. We find that COTACS encourages general-purpose code that is generalizable across multiple examples within the same task. It also enables better interpretability by generating outputs of the intermediate reasoning steps. ", "page_idx": 14}, {"type": "text", "text": "For the following questions with example inputs and outputs, generate a function that performs the   \nprovided high-level steps. The function should return a dictionary with the field \"answer\": <answer>   \nas well as the values for intermediate decisions. Don\u2019t hard code input-specific items whenever possible.   \nYou can make external calls to undefined functions as long as the function name describes its purpose.   \nInstruction: Generate three antonyms for the word \"wonderful\".   \nInput:   \nAnswer: [\u2018horrible\u2019, \u2018abysmal\u2019, \u2018appalling\u2019]   \nSteps:   \n1. Search for synonyms of the target word using a thesaurus.   \n2. Identify antonyms of the synonyms found in step 3.   \n3. Package the antonyms as the output in the required format.   \nCode:   \ndef generate_antonyms(num_words, word): \"\"\" Generate antonyms for a given word. Args: num_words (int): the number of antonyms to generate for the word word (str): a string representing the word for which we need to find the antonyms. Returns: A dictionary containing the antonyms of the given word, plus the result of the intermediate steps of the reasoning process \"\"\" # Step 1: Search for synonyms of the target word using a thesaurus. synonyms $=$ thesaurus_lookup(word) # Step 2: Identify antonyms of the synonyms found in step 1. antonyms_of_synonyms $=$ [lookup_antonyms(synonym) for synonym in synonyms] # Step 3: Package the antonyms as the output in the required format. all_antonyms $\\mathbf{\\mu}=\\mathbf{\\mu}\\left[\\boldsymbol{\\Lambda}\\right]$ for antonym_list in antonyms_of_synonyms: all_antonyms.extend(antonym_list) n_antonyms $=$ all_antonyms[:num_words] return { \u2018synonyms\u2019: synonyms, \u2018antonyms_of_synonyms\u2019: antonyms_of_synonyms, \u2018all_antonyms\u2019: all_antonyms, \u2018answer\u2019: n_antonyms }   \n>>> generate_antonyms(3, \u2018wonderful\u2019)   \nExample Output: output $=\\{$ \u2018synonyms\u2019: [\u2018amazing\u2019, \u2018fantastic\u2019, \u2018terrific\u2019], \u2018antonyms_of_synonyms\u2019: [ [\u2018horrible\u2019, \u2018abysmal\u2019, \u2018appalling\u2019], [\u2018dull\u2019, \u2018disappointing\u2019, \u2018unexceptional\u2019], [\u2018awful\u2019, \u2018terrible\u2019, \u2018dreadful\u2019] ], \u2018all_antonyms\u2019: [ \u2018horrible\u2019, \u2018abysmal\u2019, \u2018appalling\u2019, \u2018dull\u2019, \u2018disappointing\u2019, \u2018unexceptional\u2019, \u2018awful\u2019, \u2018terrible\u2019, \u2018dreadful\u2019 ], \u2018answer\u2019: [\u2018horrible\u2019, \u2018abysmal\u2019, \u2018appalling\u2019]   \n}   \n###   \nInstruction: Generate ideas for a travel blog for young tourists visiting India   \n<...>   \n###   \nInstruction: {instruction}   \nInput: {input}   \nAnswer: {output} ", "page_idx": 15}, {"type": "text", "text": "Steps: {steps} Code: ", "page_idx": 15}, {"type": "text", "text": "Figure 9: Prompt used for instantiating Python programs from NL plans. See repository for full-length prompt. ", "page_idx": 15}, {"type": "text", "text": "def determine_emotion(sentence): \"\"\" Determine the emotion expressed in a given sentence. Args: sentence (str): the sentence for which the emotion is to be determined. Returns: A dictionary containing (at least) the field \u2018answer\u2019, whose value is of type \u2018str\u2019 and contains the emotion expressed in the sentence. The dictionary also contains the result of the intermediate steps of the reasoning process. \"\"\" # Step 1: Extract all the words from the input sentence and analyze them to understand their context and meaning. words $=$ extract_words(sentence) word_context $=$ analyze_words_context(words) # Step 2: Identify the emotion-related words or phrases in the sentence. emotion_related_words $=$ identify_emotion_related_words(words) # Step 3: Return the emotion that best fits the context and the emotion-related words identified in the sentence. emotion $=$ determine_best_fit_emotion(word_context, emotion_related_words) return {\u2018sentence\u2019: sentence, \u2018words\u2019: words, \u2018word_context\u2019: word_context, \u2018emotion_related_words\u2019: emotion_related_words, \u2018answer\u2019: emotion} ", "page_idx": 16}, {"type": "text", "text": "Example Input: i was feeling festive yesterday ", "page_idx": 16}, {"type": "text", "text": "Example Output: { \u2018sentence\u2019: \u2018i was feeling festive yesterday\u2019, \u2018words\u2019: [\u2018i\u2019, \u2018was\u2019, \u2018feeling\u2019, \u2018festive\u2019, \u2018yesterday\u2019], \u2018word_context\u2019: { \u2018i\u2019: \u2018first person singular\u2019, \u2018was\u2019: \u2018past tense\u2019, \u2018feeling\u2019: \u2018verb\u2019, \u2018festive\u2019: \u2018adjective\u2019 }, \u2018emotion_related_words\u2019: [\u2018festive\u2019], \u2018answer\u2019: \u2018joy\u2019   \n} ", "page_idx": 16}, {"type": "text", "text": "2-shot Baseline Output: joy ", "page_idx": 16}, {"type": "text", "text": "Figure 10: Qualitative example of a LLama-2 13B COGEX-generated program for the text classification task on the Emotion benchmark. ", "page_idx": 16}, {"type": "text", "text": "def solve_math_word_problem(word_problem): ", "page_idx": 17}, {"type": "text", "text": "Solve a math word problem. ", "page_idx": 17}, {"type": "text", "text": "Args: word_problem (str): a string representing the math word problem.   \nReturns: A dictionary containing (at least) the field \u2018answer\u2018, whose value is of type \u2018int\u2018 and contains the solution to the math word problem. The dictionary also contains the result of the intermediate steps of the reasoning process.   \n\"\"\"   \n# Step 1: Identify the key numbers and variables from the problem statement.   \nkey_numbers $=$ identify_key_numbers(word_problem)   \n# Step 2: Understand the problem context.   \nproblem_context $=$ understand_problem_context(word_problem)   \n# Step 3: Perform the appropriate mathematical operations to solve the problem.   \nsolution $=$ perform_math_operations(key_numbers, problem_context)   \nreturn { \u2018key_numbers\u2019: key_numbers, \u2018problem_context\u2019: problem_context, \u2018answer\u2019: solution   \n} ", "page_idx": 17}, {"type": "text", "text": "Example Input: Paco had 36 cookies. He gave 14 cookies to his friend and ate 10 cookies. How many cookies did Paco have left? ", "page_idx": 17}, {"type": "text", "text": "Example Output: { \u2018key_numbers\u2019: {\u2018initial_quantity\u2019: 36, \u2018quantity_given_away\u2019: 14, \u2018quantity_eaten\u2019: 10}, \u2018problem_context\u2019: \u2018Paco had 36 cookies. He gave 14 cookies to his friend and ate 10 cookies.\u2019, \u2018answer\u2019: 12} ", "page_idx": 17}, {"type": "text", "text": "2-shot Baseline Output: 20 ", "page_idx": 17}, {"type": "text", "text": "Figure 11: Qualitative example of a LLama-2 13B COGEX-generated program for the math reasoning task on the SVAMP benchmark. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The claim of the paper is a novel reasoning paradigm, dataset for finetuning models on the paradigm, and a task-specific optimization algorithm. We show the empirical benefits of the paradigm and algorithm through our experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: In section 6. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: No theoretical proofs are given. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Hyperparameters and other details are included in section 2. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Public access to these will be released when not anonymized. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In section 2. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We only report single runs, following previous work. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In subsection 3.1 ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: It follows the code of ethics. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In section 6. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not provide any new risks and discusss risks in section 6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: They are properly credited via citation and licenses are in their respective Github pages. As they are open-source we do not explicitly discuss the license here and point interested readers to the original authors license. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, described in Section 2. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: No crowdsourcing was done. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: No crowdsourcing was done. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]