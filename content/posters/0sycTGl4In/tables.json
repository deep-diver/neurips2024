[{"figure_path": "0sycTGl4In/tables/tables_8_1.jpg", "caption": "Table 1: Quantitative comparisons on a challenging dataset, DyCheck. Our approach shows the best performance among 4D Gaussian Splatting-based methods. However, Gaussian Splatting is generally worse than MLP-based methods in more challenging settings with casually recorded videos using a monocular handheld camera.", "description": "This table presents a quantitative comparison of the proposed method (UA-4DGS) against existing methods for dynamic novel view synthesis on the DyCheck dataset.  The metrics used for evaluation include peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and learned perceptual image patch similarity (LPIPS), with masked versions (mPSNR, mSSIM, mLPIPS) also reported to focus on co-visible regions.  The table highlights that the proposed UA-4DGS significantly outperforms other Gaussian splatting based methods but still lags behind MLP-based methods, indicating the inherent challenges in handling casually recorded monocular videos. FPS indicates frames per second.", "section": "5.2 Experimental Results"}, {"figure_path": "0sycTGl4In/tables/tables_8_2.jpg", "caption": "Table 2: Few-shot novel view synthesis results with three views for static scenes, tested on the LLFF [33] dataset. Our method significantly outperforms existing methods across all metrics. FSGS\u2020 and UA-3DGS were tested over five runs, with (\u2020) indicating reproduced results. Results for other methods are taken from [72] and [7].", "description": "This table compares the performance of the proposed uncertainty-aware 3D Gaussian splatting (UA-3DGS) method against other state-of-the-art methods for few-shot novel view synthesis on the LLFF dataset.  The metrics used for comparison are Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  The results demonstrate that UA-3DGS significantly outperforms existing methods.", "section": "5.2 Experimental Results"}, {"figure_path": "0sycTGl4In/tables/tables_9_1.jpg", "caption": "Table 3: Ablation test results of our training schemes on the spin scene in the DyCheck [14] dataset. Dynamic Dens. refers to dynamic region densification.", "description": "This table presents the ablation study results focusing on the spin scene from the DyCheck dataset. It shows the impact of different components of the proposed uncertainty-aware 4D Gaussian splatting method on the model's performance.  The components evaluated include the data-driven loss (Ldata), dynamic region densification, the uncertainty-aware diffusion loss (LUA-diff), and the uncertainty-aware total variation loss (LUA-TV).  The metrics used to evaluate performance are mPSNR, mSSIM, and mLPIPS.", "section": "5.3 Analysis"}, {"figure_path": "0sycTGl4In/tables/tables_9_2.jpg", "caption": "Table 4: Quantitative comparison of regularization methods with and without uncertainty estimation on the static room scene in the LLFF dataset, where FSGS [72] are used as the baseline. Incorporating uncertainty into the regularization improves novel view synthesis by enhancing the balance between reconstruction quality on training images and performance on novel views.", "description": "This table compares different regularization methods used in novel view synthesis, focusing on the impact of incorporating uncertainty. It shows the performance of the FSGS model with and without various regularization techniques (with and without uncertainty), demonstrating improved performance for those with uncertainty.", "section": "5.3 Analysis"}, {"figure_path": "0sycTGl4In/tables/tables_14_1.jpg", "caption": "Table 1: Quantitative comparisons on a challenging dataset, DyCheck. Our approach shows the best performance among 4D Gaussian Splatting-based methods. However, Gaussian Splatting is generally worse than MLP-based methods in more challenging settings with casually recorded videos using a monocular handheld camera.", "description": "This table presents a quantitative comparison of the proposed method (UA-4DGS) against other existing methods for dynamic novel view synthesis on the DyCheck dataset.  The metrics used for comparison are Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  Masked versions of these metrics (mPSNR, mSSIM, mLPIPS) are also included, focusing on the co-visible regions. The table highlights that UA-4DGS outperforms other 4D Gaussian Splatting methods but that Gaussian Splatting methods in general perform worse than MLP-based methods when using casually recorded monocular videos.", "section": "5.2 Experimental Results"}, {"figure_path": "0sycTGl4In/tables/tables_14_2.jpg", "caption": "Table 1: Quantitative comparisons on a challenging dataset, DyCheck. Our approach shows the best performance among 4D Gaussian Splatting-based methods. However, Gaussian Splatting is generally worse than MLP-based methods in more challenging settings with casually recorded videos using a monocular handheld camera.", "description": "This table presents a quantitative comparison of the proposed method (UA-4DGS) against existing methods for dynamic novel view synthesis on the DyCheck dataset.  The metrics used for evaluation are peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and learned perceptual image patch similarity (LPIPS).  Masked versions of these metrics (mPSNR, mSSIM, mLPIPS) are also reported, focusing on co-visible regions. The table highlights the superior performance of the proposed method compared to other 4D Gaussian Splatting algorithms and MLP-based methods, especially in challenging scenarios with casually recorded monocular videos.", "section": "5.2 Experimental Results"}]