[{"type": "text", "text": "PARD: Permutation-invariant Autoregressive Diffusion for Graph Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lingxiao Zhao Xueying Ding Leman Akoglu Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University lingxiaozlx@gmail.com xding2@andrew.cmu.edu lakoglu@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to node ordering. Diffusion models, on the other hand, have garnered increasing attention as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, however they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant AutoRegressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without order sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block\u2019s probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules. PARD is open-sourced at https://github.com/LingxiaoShawn/Pard ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graphs provide a powerful abstraction for representing relational information in many domains, including social networks, biological and molecular structures, recommender systems, and networks of various infrastructures such as computers, roads, etc. Accordingly, generative models of graphs that learn the underlying graph distribution from data find applications in network science [7], drug discovery [26, 42], protein design [1, 43], and various use-cases for Internet of Things [10]. Importantly, they serve as a prerequisite for building a generative foundation model [6] for graphs. ", "page_idx": 0}, {"type": "text", "text": "Despite significant progress in generative models for images and language, graph generation is uniquely challenged by its inherent combinatorial nature. Specifically: 1) Graphs are naturally high-dimensional and discrete with varying sizes, contrasting with the continuous space and fixedsize advancements that cannot be directly applied here; 2) Being permutation-invariant objects, graphs require modeling an exchangeable probability distribution, where permutations of nodes and edges do not alter a graph\u2019s probability; and 3) The rich substructures in graphs necessitate an expressive model capable of capturing higher-order motifs and interactions. Several graph generative models have been proposed to address (part of) these challenges, based on various techniques like autoregression [48, 28], VAEs [37], GANs [11], flow-based methods [36], and denoising diffusion [32, 44]. Among these, autoregressive models and diffusion models stand out with superior performance, thus significant popularity. However, current autoregressive models, while efficient, are sensitive to node/edge order with non-exchangeable probabilities; whereas diffusion models, though promising, are less efficient, requiring thousands of denoising steps and extra node/edge/graph-level features (structural and/or domain-specific) to achieve high generation quality. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce PARD (leopard in Ancient Greek), the first Permutation-invariant AutoRegressive Diffusion model that combines the efficiency of autoregressive methods and the quality of diffusion models together, while retaining the property of exchangeable probability. Instead of generating an entire graph directly, we explore the direction of generating through block-wise graph enlargement. Graph enlargement offers a fine-grained control over graph generation, which can be particularly advantageous for real-world applications that require local revisions to generate graphs. Moreover, it essentially decomposes the joint distribution of the graph into a series of simpler conditional distributions, thereby leveraging the data efficiency characteristic of autoregressive modeling. We also argue that graphs, unlike sets, inherently exhibit a unique partial order among nodes, naturally facilitating the decomposition of the joint distribution. Thanks to this unique partial order, PARD\u2019s block-wise autoregressive sequence is permutation-invariant, unlike any prior graph autoregressive methods in the literature. ", "page_idx": 1}, {"type": "text", "text": "To model the conditional distribution of nodes and edges within a block, we have, for the first time, identified a fundamental challenge in equivariant models for generation: it is impossible for any equivariant model, no matter how powerful, to perform general graph transformations without symmetry breaking. However, through a diffusion process that injects noise, a permutation equivariant network can progressively denoise to realize targeted graph transformations. This approach is inspired by the annealing process where energy is initially heightened before achieving a stable state, akin to the process of tempering iron. Our analytical findings naturally lead to the design of our proposed PARD that combines autoregressive approach with local block-wise discrete denoising diffusion. Using a diffusion model with equivariant networks ensures that each block\u2019s conditional distribution is exchangeable. Coupled with the permutation-invariant block sequence, this renders the entire process permutation-invariant and the joint distribution exchangeable. What is more, this inevitable combination of autoregression and diffusion successfully combines the strength of both approaches while getting rid of their shortcomings: By being permutation-invariant, a key advantage of diffusion, it generalizes better than autoregressive models while also being much more data-efficient. By decomposing the challenging joint probability into simpler conditional distributions, an advantage of autoregression, it requires significantly fewer diffusion steps, outperforming pure diffusion methods by a large margin. Additionally, each inference step in the diffusion process incurs lower computational cost by processing only the generated part of the graph, rather than the entire graph. And it can further leverage caching mechanisms (to be explored in future) to avoid redundant computations. ", "page_idx": 1}, {"type": "text", "text": "Within PARD, we further propose several architectural improvements. First, to achieve 2-FWL expressivity with improved memory efficiency, we propose a higher-order graph transformer that integrates transformer with PPGN [30], while utilizing a significantly reduced representation size for edges. Second, to ensure training efficiency without substantial overhead compared to the original diffusion model, we design a GPT-like causal mechanism to support parallel training of all blocks. These extensions are generalizable and can lay the groundwork for a higher-order GPT. ", "page_idx": 1}, {"type": "text", "text": "PARD achieves new SOTA performance on many molecular and non-molecular datasets without any extra features, significantly outperforming DiGress [44]. Thanks to efficient architecture and parallel training, PARD scales to large datasets like MOSES [33] with 1.9M graphs. Finally, not only PARD can serve as a generative foundation model for graphs in the future, its autoregressive parallel mechanism can further be combined with language models for language-graph generative pretraining. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Autoregressive (AR) Models for Graph Generation. AR models create graphs step-by-step, adding nodes and edges sequentially. This method acknowledges graphs\u2019 discrete nature but faces a key challenge as there is no inherent order in graph generation. To address this, various strategies have been proposed to simplify orderings and approximate the marginalization over permutations; i.e. $\\begin{array}{r}{p(G)\\,\\,=\\,\\,\\dot{\\sum}_{\\pi\\in\\mathcal{P}(G)}\\,p(\\bar{G},\\bar{\\pi})}\\end{array}$ . Li et al. [27] propose using random or deterministic empirical orderings. GraphRNN [48] aligns permutations with breadth-first-search (BFS) ordering, with a many-to-one mapping. GRAN [28] offers marginalization over a family of canonical node orderings, including node degree descending, DFS/BFS tree rooted at the largest degree node, and k-core ordering. GraphGEN [16] uses a single canonical node ordering, but does not guarantee the same canonical ordering during generation. Chen et al. [9] avoid defining ad-hoc orderings by modeling the conditional probability of orderings, $p(\\pi|G)$ , with a trainable AR model, estimating marginalized probabilities during training to enhance both the generative model and the ordering probability model. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Diffusion Models for Graph Generation. EDP-GNN [32] is the first work that adapts score matching [39] to graph generation, by viewing graphs as matrices with continuous values. GDSS [24] generalizes EDP-GNN by adapting SDE-based diffusion [40] and considers node and edge features. Yan et al. [46] argues that learning exchangeable probability with equivariant networks is hard, hence proposes permutation-sensitive SwinGNN with continuous-state score matching. Previous works apply continuous-state diffusion to graph generation, ignoring the natural discreteness of graphs. DiGress [44] is the first to apply discrete-state diffusion [3, 20] to graph generation and achieves significant improvement. However, DiGress relies on many additional structural and domain-specific features. GraphArm [25] applies Autoregressive Diffusion Model (ADM) [21] to graph generation, where exactly one node and its adjacent edges decay to the absorbing states at each forward step based on a random node order. Similar to AR models, GraphArm is permutation sensitive. ", "page_idx": 2}, {"type": "text", "text": "We remark that although both are termed \u201cautoregressive diffusion\u201d, it is important to distinguish that PARD is not ADM. The term \u201cautoregressive diffusion\u201d in our context refers to the integration of autoregressive methods with diffusion models. In contrast, ADM represents a specific type of discrete denoising diffusion where exactly one dimension decays to an absorbing state at a time in the forward diffusion process. See Fan et al. [13] for a survey of recent diffusion models on graphs. ", "page_idx": 2}, {"type": "text", "text": "3 Permutation-invariant Autoregressive Denoising Diffusion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first introduce setting and notations. We focus on graphs with categorical features. Let $G=(\\boldsymbol{\\nu},\\boldsymbol{\\mathcal{E}})$ be a labeled graph with the number of distinct node and edge labels denoted $K_{v}$ and $K_{e}$ , respectively. Let $\\pmb{v}^{i}\\in\\{0,\\overline{{1}}\\}^{\\dot{K}_{v}},\\forall i\\in\\mathcal{V}$ be the one-hot encoding of node $i$ \u2019s label. Let $e^{i,j}\\in\\{0,1\\}^{K_{e}},\\forall i,j\\in\\mathcal{V}$ be the one-hot encoding of the label for the edge between node $i$ and $j$ . We also represent \u201cabsence of edge\u201d as a type of edge label, hence $|\\mathcal{E}|=|\\mathcal{V}|\\times|\\mathcal{V}|$ . Let $\\mathbf{V}\\in\\{0,1\\}^{|\\mathcal{V}|\\times K_{v}}$ and $\\dot{\\mathrm{E}}\\in\\{0,1\\}^{|\\mathcal{V}|\\times|\\mathcal{V}|\\times K_{e}}$ be the collection of one-hot encodings of all nodes and edges using the default node order, and let $\\mathbf{G}:=(\\mathbf{V},\\mathbf{E})$ . To describe probability, let $\\mathbf{x}$ be a random variable with its sampled value $\\textbf{\\em x}$ . Similarly, $\\mathbf{G}$ is a random graph with its sampled graph G. In diffusion process, noises are injected from $t{=}0$ to $t{=}T$ with $T$ being the maximum time step. Let $\\mathbf{x}_{0}\\sim p_{\\mathrm{data}}(\\mathbf{x}_{0})$ be the random variable of observed data with underlying distribution $p_{\\mathrm{data}}(\\mathbf{x}_{0})$ , $\\mathbf{x}_{t}\\sim q(\\mathbf{x}_{t})$ be the random variable at time $t$ , and let $\\mathbf{x}_{t\\mid s}\\sim q(\\mathbf{x}_{t}|\\mathbf{x}_{s})$ denote the conditional random variable. Also, we interchangeably use $q(\\mathbf{x}_{t}|\\mathbf{x}_{s})$ , $q(\\mathbf{x}_{t}{=}\\mathbf{x}_{t}|\\mathbf{x}_{s}{=}\\mathbf{x}_{s})$ , and $q_{t|s}(\\pmb{x}_{t}|\\pmb{x}_{s})$ when there is no ambiguity. We model the forward diffusion process independently for each node and edge, while the backward denoising process is modeled jointly for all nodes and edges. All vectors are column-wise vectors. Let $\\langle\\cdot,\\cdot\\rangle$ denote inner product. ", "page_idx": 2}, {"type": "text", "text": "3.1 Discrete Denoising Diffusion on Graphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Denoising Diffusion is first developed by Sohl-Dickstein et al. [38] and later improved by Ho et al. [19]. It is further generalized to discrete-state case by Hoogeboom et al. [20] and Austin et al. [3]. Taking a graph ${\\mathrm{G}}_{0}$ as example, diffusion model defines a forward diffusion process to gradually inject noise to all nodes and edges independently until all reach a non-informative state ${\\mathrm{G}}_{T}$ . Then, a denoising network is trained to reconstruct ${\\mathrm{G}}_{0}$ from the noisy sample $\\mathrm{G}_{t}$ at each time step, by optimizing a Variational Lower Bound (VLB) for $\\log p_{\\theta}(\\mathbf{G}_{0})$ . Specifically, the forward process is defined as a Markov chain with $q(\\mathbf G_{t}|\\mathbf G_{t-1}),\\forall t\\in[1,T]$ , and the backward denoising process is parameterized with another Markov chain $\\begin{array}{r}{\\dot{p}_{\\theta}\\big(\\mathbf G_{t-1}\\big|\\mathbf G_{t}\\big),\\:\\:\\forall t\\in[1,T]}\\end{array}$ . Note that while the forward process is independently applied to all elements, the backward process is coupled together with conditional independence assumption. Formally, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle\\mathbf{G}_{t}|\\mathbf{G}_{t-1}\\rangle=\\prod_{i\\in\\mathcal{V}}q(\\mathbf{v}_{t}^{i}|\\mathbf{v}_{t-1}^{i})\\prod_{i,j\\in\\mathcal{V}}q(\\mathbf{e}_{t}^{i,j}|\\mathbf{e}_{t-1}^{i,j}),\\quad p_{\\theta}(\\mathbf{G}_{t-1}|\\mathbf{G}_{t})=\\prod_{i\\in\\mathcal{V}}p_{\\theta}(\\mathbf{v}_{t-1}^{i}|\\mathbf{G}_{t})\\prod_{i,j\\in\\mathcal{V}}p_{\\theta}(\\mathbf{e}_{t-1}^{i,j}|\\mathbf{G}_{t})\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, the VLB of $\\log p_{\\theta}(\\mathbf{G}_{0})$ can be written (see Apdx. $\\S\\mathrm{A.l}$ ) as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{og}\\,p_{\\theta}(\\mathbf{G}_{0})\\geq\\underbrace{\\mathbb{E}_{q(\\mathbf{G}_{1}|\\mathbf{G}_{0})}\\left[\\log p_{\\theta}(\\mathbf{G}_{0}|\\mathbf{G}_{1})\\right]}_{-\\mathcal{L}_{1}(\\theta)}-\\underbrace{D_{\\mathrm{KL}}\\big(q(\\mathbf{G}_{T}|\\mathbf{G}_{0})\\big|\\,\\big|p_{\\theta}(\\mathbf{G}_{T})\\big)}_{\\mathcal{L}_{p\\mathrm{eac}}}-\\sum_{t=2}^{T}\\underbrace{\\mathbb{E}_{q(\\mathbf{G}_{t}|\\mathbf{G}_{0})}\\left[D_{\\mathrm{KL}}\\big(q(\\mathbf{G}_{t-1}|\\mathbf{G}_{t},\\mathbf{G}_{0})\\big|\\,\\big|p_{\\theta}(\\mathbf{G}_{t-1}|\\mathbf{G}_{t})\\big)\\right]}_{\\mathcal{L}_{t}(\\theta)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{prior}}\\approx0$ , since $p_{\\theta}(\\mathbf{G}_{T})\\approx q(\\mathbf{G}_{T}|\\mathbf{G}_{0})$ is designed as a fixed noise distribution that is easy to sample from. To compute Eq. (2), we need to formalize the distributions (i) $)\\ q(\\mathbf{G}_{t}|\\mathbf{G}_{0})$ and $(i i)$ ", "page_idx": 2}, {"type": "text", "text": "$q(\\mathbf{G}_{t-1}|\\mathbf{G}_{t},\\mathbf{G}_{0})$ , as well as $(i i i)$ the parameterization of $p_{\\theta}\\big(\\mathbf{G}_{t-1}\\big|\\mathbf{G}_{t}\\big)$ . DiGress [44] applies D3PM [3] to define these three terms. Different from DiGress, we closely follow the approach in Zhao et al. [52] to define these three terms, as their formulation is simplified with improved memory usage and loss computation. For brevity, we refer readers to Appx. $\\S\\mathrm{A}.2$ for the details. Notice that while a neural network can directly be used to parameterize (iii) $p_{\\theta}\\big(\\mathbf{G}_{t-1}\\big|\\mathbf{G}_{t}\\big)$ , we follow [52] to parameterize $p_{\\theta}(\\mathbf{G}_{0}|\\mathbf{G}_{t})$ instead, and compute $(i i i)$ from $p_{\\theta}(\\mathbf{G}_{0}|\\mathbf{G}_{t})$ . ", "page_idx": 3}, {"type": "text", "text": "With $(i\\sim i i i)$ ) known, one can compute the negative VLB loss in Eq. (2) exactly. In addition, at each time step $t$ , the cross entropy (CE) loss between (i) $q(\\mathbf{G}_{t}|\\mathbf{G}_{0})$ and $p_{\\theta}(\\mathbf{G}_{0}|\\mathbf{G}_{t})$ that quantifies reconstruction quality is often employed as an auxiliary loss, which is formulated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t}^{C E}(\\boldsymbol{\\theta})=-\\mathbb{E}_{q(\\mathbf{G}_{t}|\\mathbf{G}_{0})}\\Big[\\sum_{i\\in\\mathcal{V}}\\log p_{\\boldsymbol{\\theta}}(\\mathbf{v}_{0}^{i}|\\mathbf{G}_{t})+\\sum_{i,j\\in\\mathcal{V}}\\log p_{\\boldsymbol{\\theta}}(\\mathbf{e}_{0}^{i,j}|\\mathbf{G}_{t})\\Big]\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In fact, DiGress solely uses $\\underline{{\\mathcal{L}}}_{t}^{C E}(\\theta)$ to train their diffusion model. In this paper, we adopt a hybrid loss [3], that is $\\mathcal{L}_{t}(\\theta)\\dot{+}\\lambda\\mathcal{L}_{t}^{C E}\\dot{(\\theta)}$ with $\\lambda=0.1$ at each time $t$ , as we found it to help reduce overftiting. To generate a graph from $p_{\\theta}(\\mathbf{G}_{0})$ , a pure noise graph is first sampled from $p_{\\theta}(\\mathbf{G}_{T})$ and gradually denoised using the learned $p_{\\theta}(\\mathbf{G}_{t-1}|\\mathbf{G}_{t})$ from step $T$ to 0. ", "page_idx": 3}, {"type": "text", "text": "A significant advantage of diffusion models is their ability to achieve exchangeable probability in combination with permutation equivariant networks under certain conditions [45]. DiGress is the first work that applied discrete denoising diffusion to graph generation, achieving significant improvement over previous continuous-state based diffusion. However, given the inherently high-dimensional nature of graphs and their complex internal dependencies, modeling the joint distribution of all nodes and edges directly presents significant challenges. DiGress requires thousands of denoising steps to accurately capture the original dependencies. Moreover, DiGress relies on many extra supplementary node and graph-level features, such as cycle counts and eigenvectors, to effectively break symmetries among structural equivalences to achieve high performance. ", "page_idx": 3}, {"type": "text", "text": "3.2 Autoregressive Graph Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Order is important for AR models. Unlike diffusion models that aim to capture the joint distribution directly, AR models decompose the joint probability into a product of simpler conditional probabilities based on an order. This makes AR models inherently suitable for ordinal data, where a natural order exists, such as in natural languages and images. ", "page_idx": 3}, {"type": "text", "text": "Order Sensitivity. Early works of graph generation contain many AR models like GraphRNN [48] and GRAN [28] based on non-deterministic heuristic node orders like BFS/DFS and $\\mathbf{k}$ -core ordering. Despite being permutation sensitive, AR models achieve SOTA performance on small simulated structures like grid and lobster graphs. However, permutation invariance is necessary for estimating an accurate likelihood of a graph, and can benefit large-size datasets for better generalization. ", "page_idx": 3}, {"type": "text", "text": "Let $\\pi$ denote an ordering of nodes. To make AR order-insensitive, there are two directions: (1) Modeling the joint probability $p(\\mathbf{G},\\pi)$ and then marginalizing $\\pi$ , (2) Finding a unique canonical order $\\pi^{*}(\\mathbf{G})$ for any graph G such that $p(\\pi|\\mathbf{G})=1$ if $\\pi=\\pi^{*}(\\mathbf{G})$ and 0 otherwise. In direction (1), directly integrating out $\\pi$ is prohibitive as the number of permutations is factorial in the graph size. Several studies [27, 9, 28] have used subsets of either random or canonical orderings. This approach aims to simplify the process, but it results in approximated integrals with indeterminate errors. Moreover, it escalates computational expense due to the need for data augmentation involving these subsets of orderings. In direction (2), identifying a universal canonical order for all graphs is referred to as graph canonicalization. There exists no polynomial time solution for this task on general graphs, which is at least as challenging as the NP-intermediate Graph Isomorphism problem [2]. Goyal et al. [17] explored using minimum DFS code to construct canonical labels for a specific dataset with non-polynomial time complexity. However, the canonicalization is specific to each training dataset with the randomness derived from DFS. This results in a generalization issue, due to the canonical order being $\\pi(\\mathrm{G}|\\mathrm{TrainSet})$ instead of $\\pi(\\mathrm{G})$ . ", "page_idx": 3}, {"type": "text", "text": "The Existence of Partial Order. While finding a unique order for all nodes of a graph is NPintermediate, we argue that finding a unique partial order, where certain nodes and edges are with the same rank, is easily achievable. For example, a trivial partial order is simply all nodes and edges having the same rank. Nevertheless, a graph is not the same as a set (a set is just a graph with empty $\\mathcal{E}$ ), where all elements are essentially unordered with equivalent rank. That is because a non-empty graph contains edges between nodes, and these edges give different structural properties to nodes. Notice that some nodes or edges have the same structural property as they are structurally equivalent. ", "page_idx": 3}, {"type": "text", "text": "1: Input: Graph $G$ , maximum hops $K_{h}$ .   \n2: Init: $G_{0}=G$ , $i=0$ , $\\phi$ with $\\phi(\\pmb{v})=0\\,\\forall\\pmb{v}$ .   \n3: while $G_{i}$ is not $\\varnothing$ do   \n4: Compute $w_{K_{h}}(\\pmb{v}),\\forall\\pmb{v}\\in\\mathcal{V}(G_{i})$ , using Eq. (4).   \n5: Find all nodes $\\mathcal{L}$ with $\\begin{array}{r}{w_{K_{h}}=\\operatorname*{min}_{{\\pmb v}\\in\\mathcal{V}(G)}w_{K_{h}}({\\pmb v})}\\end{array}$   \n6: Let $\\phi(\\pmb{v})=i\\,\\forall\\pmb{v}\\in\\mathcal{L}$ .   \n7: $G_{i+1}\\gets G_{i}[\\mathcal{V}(G)\\setminus\\mathcal{L}]$ ; $i\\gets i+1$ .   \n8: end while   \n9: Output: $\\phi\\leftarrow i-\\phi$ ", "page_idx": 4}, {"type": "image", "img_path": "x4Kk4FxLs3/tmp/427aa0c405be06cdfee7c84b3a0d897324c4a582de73e4fc00ea731f5bb01317.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Example case where the equivariant graph transformation from $G[{B_{1:i}}]$ to $G[B_{1:i+1}]$ is impossible for any permutationequivariant network due to structural equivalence of nodes. See Proposition 3.3. ", "page_idx": 4}, {"type": "text", "text": "We can view each structural property as a color, and rank all unique colors within the graph to define the partial order over nodes, which we call a structural partial order. The structural partial order defines a sequence of blocks such that all nodes within a block have the same rank (i.e. color). ", "page_idx": 4}, {"type": "text", "text": "Let $\\phi:\\mathcal{V}\\rightarrow[1,...,K_{B}]$ be the function that assigns rank to nodes based on their structural properties, where $K_{B}$ denotes the maximum number of blocks. We use $G[S]$ to denote the induced subgraph on the subset ${\\mathcal{S}}\\subseteq{\\mathcal{V}}$ . There are many ways to assign rank to structural colors, however we would like the resulting partial order to satisfy certain constraints. Most importantly, we want ", "page_idx": 4}, {"type": "equation", "text": "$\\forall r\\in[1,...,K_{B}],\\;G[\\phi(\\mathcal{V})\\leq r]$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The connectivity requirement is to ensure a more accurate representation of real-world graph generation processes, where it is typical of real-world dynamic graphs to enlarge with newcoming nodes being connected at any time. Then, one can sequentially remove all nodes with the lowest degree to maintain this connectivity and establish a partial order. However, degree only reflects limited information of the first-hop neighbors, and many nodes share the same degree\u2014leading to only a few distinct blocks, not significantly different from a trivial, single-block approach. ", "page_idx": 4}, {"type": "text", "text": "To ensure connectivity while reducing rank collision, we consider larger hops to define a weighted degree. Consider a maximum of $K_{h}$ hops. For any node $\\pmb{v}\\in\\mathcal{V}$ , the number of neighbors at each hop of $\\pmb{v}$ can be easily obtained as $[d_{1}(\\pmb{v}),...,d_{K_{h}}(\\pmb{v})]$ . We then define the weighted degree as ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{K_{h}}(\\pmb{v})=\\sum_{k=1}^{K_{h}}d_{k}(\\pmb{v})\\times|\\mathcal{V}|^{K_{h}-k}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Equation Eq. (4) may appear as an ad-hoc design, but it fundamentally serves as a hash that maps the vector $[\\bar{d}_{1}(\\pmb{v}),d_{2}(\\pmb{v}),...,d_{K}(\\pmb{v})]$ to a unique scalar value. This mapping ensures a one-to-one correspondence between the vector and the scalar. Furthermore, it prioritizes lower-hop degrees over higher-hop degrees, akin to how e.g. the number $\"123\"$ is represented as $1\\times10^{2}+2\\times\\dot{1}0^{1}\\overset{\\circ}{+}3\\times10^{0}$ . Eq. (4) is efficient to compute and guarantees: 1) Nodes have the same rank if and only if they have the same number of neighbors up to $K_{h}$ hops. 2) Lower-hop degrees are weighted more heavily. With $w_{K_{h}}$ defined, we present our structural partial order in Algo. 1. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1. For any $G_{z}$ , its structural partial order $\\phi$ defined by Algo. $^{\\,l}$ is permutation equivariant, such that $\\phi(P\\star\\mathrm{G})=P\\star\\phi(\\mathrm{G})$ for any permutation operator $_{P}$ . ", "page_idx": 4}, {"type": "text", "text": "It is easy to prove Prop. 3.1. Algo. 1 shows that $\\phi(i)$ for $\\forall i\\in\\mathcal{V}$ is uniquely determined by node $i$ \u2019s structural higher-order degree. As nodes\u2019 higher-order degree is permutation equivariant, $\\phi$ is also permutation equivariant. Notice that $\\phi$ is unique and deterministic for any graph. ", "page_idx": 4}, {"type": "text", "text": "Autoregressive Blockwise Generation. $\\phi$ in Algo. 1 with output in range $\\left[1,K_{B}\\right]$ divides the nodes $\\mathcal{V}(G)$ into $K_{B}$ blocks $[B_{1},...,B_{K_{B}}]$ in order, where ${\\mathcal{B}}_{j}=\\{i\\in{\\dot{\\mathcal{V}}}(G)|\\phi({\\dot{i}})\\stackrel{.}{=}j\\}$ . Let $B_{1:i}:=\\cup_{j=1}^{i}B_{j}$ be the union of the first $i$ blocks. PARD decomposes the joint probability of a graph $G$ into ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\boldsymbol{\\theta}}(\\mathbf{G})=\\prod_{i=1}^{K_{B}}p_{\\boldsymbol{\\theta}}\\left(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\;\\Big|\\;\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{G}[{B_{1:0}}]$ is defined as the empty graph, and $\\mathbf{G}[B_{1:i}]\\setminus\\mathbf{G}[B_{1:i-1}]$ denotes the set of nodes and edges that are present in $\\mathrm{G}[{B_{1:i}}]$ but not in $\\mathrm{G}[{B_{1:i-1}}]$ . All of them are represented in natural order of $G$ . As each conditional probability only contains a subset of edges and nodes, and having access to all previous blocks, this conditional probability is significantly easier to model than the whole joint probability. Given the property Prop. 3.1 of $B_{i}$ , it is easy to verify that $p_{\\theta}(\\mathbf{G})$ is exchangeable with permutation-invariant probability for any $G$ if and only if all conditional probabilities are exchangeable. See Appx. $\\S\\mathrm{A}.6$ for details of permutation-invariant probability. Note that while GRAN [28] also generates graphs block-by-block, all nodes within GRAN have different generation ordering, even within the same block (it breaks symmetry for the problem identified later). Essentially, GRAN is an autoregressive method and, as such, suffers from all the disadvantages inherent to AR. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3 Impossibility of Equivariant Graph Transformation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Eq. (5), we need to parameterize the conditional probability $p_{\\theta}\\left(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\ \\Big|\\ \\mathbf{G}[\\mathcal{B}_{1:i-1}]\\right)$ to be permutation-invariant. This can be achieved by letting the conditional probability be ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\theta}\\left(\\left|\\mathcal{B}_{i}\\right|\\;\\middle|\\;\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\right)\\prod_{\\mathbf{x}\\in\\mathbf{G}[\\mathcal{B}_{1:i}]\\backslash\\mathbf{G}[\\mathcal{B}_{1:i-1}]}p_{\\theta}\\left(\\mathbf{x}\\right|\\;\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\emptyset[\\mathcal{B}_{1:i}]\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{x}$ is any node and edge in $\\mathbf{G}[B_{1:i}]\\setminus\\mathbf{G}[B_{1:i-1}]$ , $\\varnothing$ denotes an empty graph, hence $\\mathrm{G}[{B_{1:i-1}}]\\cup$ $\\emptyset[B_{1:i}]$ depicts augmenting $\\mathrm{G}[B_{1:i-1}]$ with empty (or virtual) nodes and edges to the same size as $\\mathrm{G}[{B_{1:i}}]$ . With the augmented graph, we can parameterize $p_{\\theta}\\big(\\mathbf{x}\\mid\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\emptyset[\\mathcal{B}_{1:i}]\\big)$ for any node and edge $\\mathbf{x}$ with a permutation equivariant network to achieve the required permutation invariance. For simplicity, let $\\begin{array}{r}{\\mathring{\\mathbf{G}}\\big[\\mathcal{B}_{1:i-1},|\\mathcal{B}_{i}|\\big]:=\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\varnothing[\\mathcal{B}_{1:i}]}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "The Flaw in Equivariant Modeling. Although the parameterization in Eq. (6) along with an equivariant network makes the conditional probability in Eq. (5) become permutation-invariant, we have found that the equivariant graph transformation $p_{\\theta}(\\mathbf{x}\\mid\\mathbf{G}\\big[B_{1:i-1},\\bar{|B_{i}|}\\big])$ cannot be achieved in general for any permutation equivariant network, no matter how powerful it is (!) For definition of graph transformation, see Appx.\u00a7A.7. The underlying cause is the symmetry of structural equivalence, which is also a problem in link prediction [41, 49]. Formally, let $A(G)$ be the adjacency matrix of $G$ (ignoring labels) based on $G$ \u2019s default node order, then an automorphism $\\sigma$ of $G$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\nA(G)=A(\\sigma\\star G)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\sigma\\star G$ is a reordering of nodes based on the mapping $\\sigma$ . Then the automorphism group is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{Aut}(G)=\\{\\sigma\\in\\mathbb{P}_{|\\nu|}\\mid A(G)=A(\\sigma\\star G)\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbb{P}_{n}$ denotes all permutation mappings for size $n$ . That is, $\\operatorname{Aut}(G)$ contains all automorphisms of $G$ . For a node $i$ of $G$ , the orbit that contains node $i$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\no(i)=\\{\\sigma(i)\\mid\\forall\\sigma\\in\\operatorname{Aut}(G)\\}~.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In words, the orbit $o(i)$ contains all nodes that are structurally equivalent to node $i$ in $G$ . Two edges $(i,j)$ and $(u,v)$ are structurally equivalent if $\\exists\\sigma\\in\\operatorname{Aut}(G)$ , such that $\\sigma(i)=u$ and $\\sigma(j)=v$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. Any structurally equivalent nodes and edges will have identical representations in any equivariant network, regardless of its power or expressiveness. ", "page_idx": 5}, {"type": "text", "text": "See proof in Apdx.\u00a7A.5. Theorem 3.2 indicates that no matter how powerful the equivariant network is, any structually equivalent elements have the same representation, which implies the following. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.3. General graph transformation is not achievable with any equivariant model. ", "page_idx": 5}, {"type": "text", "text": "Proof. To prove it, we only need to show there are many \u201cbottleneck\u201d cases where the transformation cannot be achieved. Fig. 1 shows a case where $G[B_{1:}]$ is a 4-cycle, and the next target block contains two additional nodes, each with a single edge connecting to one of the nodes of $\\overline{{G[B_{1:}]}}$ . It is easy to see that nodes 1\u20134 are all structurally equivalent, and so are nodes $5,6$ in the augmented case (middle). Hence, edges in $\\{(5,i)|\\forall i\\in[1,4]\\}$ are structurally equivalent (also $\\{(6,i)|\\forall i\\in[1,4]\\})$ . Similarly, $\\forall i\\in[1,4]$ , edge $(5,i)$ and $(6,i)$ are structurally equivalent. Combining all cases, edges in $\\{(j,i)|\\forall i\\in[\\bar{1},4],\\dot{j}\\in\\{5,6\\}\\}$ are structurally equivalent. Theorem 3.2 states that all these edges would have the same prediction, hence making the target $G[B_{1:i+1}]$ not achievable. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "3.4 PARD: Autoregressive Denoising Diffusion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Magic of Annealing/Randomness. In Fig. 1 we showed that a graph with many automorphisms cannot be transformed to a target graph with fewer automorphisms. We hypothesize that $a$ graph with lower \u201cenergy\u201d is hard to be transformed to a graph with higher \u201cenergy\u201d with equivariant networks. There exist some definitions and discussion of graph energy [18, 4] based on symmetry and eigen-information to measure graph complexity, where graphs with more symmetries have lower energy. The theoretical characterization of the conditions for successful graph transformation is a valuable direction, which we leave for future work to investigate. ", "page_idx": 5}, {"type": "text", "text": "Based on the above hypothesis, to achieve a successful transformation of a graph into a target graph, it is necessary to increase its energy. Since graphs with fewer symmetries exhibit higher energy levels, our approach involves adding random noise to nodes and edges. Our approach of elevating the energy level, followed by its reduction to attain desired target properties, mirrors the annealing process. ", "page_idx": 5}, {"type": "image", "img_path": "x4Kk4FxLs3/tmp/bf60c6afd59935333d70ef4e8dab0782d5b0ae225a970e389fd7eeb4a68ae600.jpg", "img_caption": ["Figure 2: PARD integrates the autoregressive method with diffusion modeling. (top) PARD decomposes the joint probability into a series of block-wise enlargements, where each block\u2019s conditional distribution is captured with a shared discrete diffusion (bottom). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Diffusion. This further motivates us to use denoising diffusion to model $p_{\\theta}(\\mathbf{x}\\mid\\mathbf{G}\\big[\\mathcal{B}_{1:i-1},|\\mathcal{B}_{i}|\\big])$ : it naturally injects noise in the forward process, and its backward denoising process is the same as annealing. As we show below, this yields $p_{\\theta}(G)$ in Eq. (5) to be permutation-invariant. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.4. PARD is permutation-invariant such that $p_{\\theta}(P\\star\\mathbf{G})=p_{\\theta}(\\mathbf{G})\\:\\forall\\:p$ ermutator $_{P}$ . ", "page_idx": 6}, {"type": "text", "text": "The proof is given in Appx. $\\S\\mathrm{A}.6$ . With all constituent parts presented, we summarize our proposed PARD, the first permutation-invariant autoregressive diffusion model that integrates AR with denoising diffusion. PARD relies on a unique, permutation equivariant structural partial order $\\phi$ (Algo. 1) to decompose the joint graph probability to the product of simpler conditional probabilities, based on Eq. (5). Each block\u2019s conditional probability is modeled with the product of a conditional block size probability and a conditional block enlargement probability as in Eq. (6), where the latter for every block is a shared discrete denoising diffusion model as described in $\\S3.1$ . Fig. 2 illustrates PARD\u2019s two parts $:$ (top) block-wise AR and (bottom) local denoising diffusion at each AR step. ", "page_idx": 6}, {"type": "text", "text": "Notice that there are two tasks in Eq. (6); one for predicting the next block\u2019s size, and the other for predicting the next block\u2019s nodes and edges with diffusion. These two tasks can be trained together with a single network, although for better performance we use two different networks. For each block\u2019s diffusion model, we set the maximum time steps to 40 without much tuning. ", "page_idx": 6}, {"type": "text", "text": "Training and Inference. We provide the training and inference algorithms for PARD in Apdx. $\\S\\mathrm{A}.8$ . Specifically, Algo. 2 is used to train next block\u2019s size prediction model; Algo. 3 is used to train the shared diffusion for block conditional probabilities; and Algo. 4 presents the generation steps. ", "page_idx": 6}, {"type": "text", "text": "4 Architecture Improvement ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "PARD is a general framework that can be combined with any equivariant network. Nevertheless, we would like an equivariant network with enough expressiveness to process symmetries inside the generated blocks for modeling the next block\u2019s conditional probability. While there are many expressive GNNs like subgraph GNNs [5, 50] and higher-order GNNs [51, 31], PPGN [30] is still a natural choice that models edge (2-tuple) representations directly with 3-WL expressivity and $O(n^{3})$ complexity in graph size. However, PPGN\u2019s memory cost is relatively high for many datasets. ", "page_idx": 6}, {"type": "text", "text": "4.1 Efficient and Expressive Higher-order Transformer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To enhance the memory efficiency of PPGN while maintaining the expressiveness equivalent to the 3-Weisfeiler-Lehman (3-WL) test, we introduce a hybrid approach that integrates Graph Transformers with PPGN. Graph Transformers operate on nodes as the fundamental units of representation, offering better scalability and reduced memory consumption $(O(n^{2}))$ compared to PPGN. PPGN utilizes edges as their primary representation units and therefore incurs significantly higher memory requirements $(O(n^{3}))$ . However, the expressiveness of Graph Transformers (without position encoding) is limited to the 1-WL test [8]. By combining these two models, we can drastically decrease the size of edge representations while allocating larger hidden sizes to nodes. This synergistic approach not only substantially lowers the memory footprint but also enhances overall performance, leveraging the strengths of both architectures to achieve a balance between expressivity and efficiency. We provide the detailed design in Appx. $\\S\\mathrm{A}.9$ . Note that we use GRIT [29] as the graph transformer block. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Parallel Training with Causal Transformer ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As shown in Eq. (5), for a graph $G$ , there are $K_{B}$ conditional probabilities being modeled by a shared diffusion model using a $\\theta^{}$ -parameterized network $f_{\\theta}$ . By default, these $K_{B}$ number of inputs $\\{\\mathbf{G}[B_{1:i-1}]\\}_{i=1}^{K_{B}}$ are viewed as separate graphs and the representations during network passing $f_{\\theta}(\\mathbf{G}[\\mathcal{B}_{1:i-1}])$ for different $i\\in[1,K_{B}]$ are not shared. This leads to a scalability issue; in effect enlarging the dataset by roughly $K_{B}$ times and resulting in $K_{B}$ times longer training. ", "page_idx": 7}, {"type": "text", "text": "To minimize computational overhead, it is crucial to enable parallel training of all the $K_{B}$ conditional probabilities, and allow these processes to share representations, through which we can pass the full graph $G$ to the network $f_{\\theta}$ only once and obtain all $K_{B}$ conditional probabilities. This is also a key advantage of transformers over RNNs. Transformers (GPTs) can train all next-token predictions simultaneously with representation sharing through causal masking, whereas RNNs must train sequentially. However, the default causal masking of GPTs is not applicable to our architecture, as PARD contains both Transformer and PPGN where the PPGN\u2019s causal masking is not designed. ", "page_idx": 7}, {"type": "text", "text": "To ensure representation sharing without risking information leakage, we first assign a \u201cblock ID\u201d to every node and edge within graph $G$ . Specifically, for every node and edge in $\\bar{G[B_{1:i}]}\\setminus G[B_{1:i-1}]$ , we assign the ID equal to $i$ . To prevent information leakage effectively, it is crucial that any node and edge labeled with ID $i$ are restricted to communicate only with other nodes and edges whose ID is $\\leq i$ . Let $\\pmb{A}$ , $\\mathbf{\\Delta},B\\in\\mathbb{R}^{n\\times n}$ , and $\\mathbf{x}\\in\\mathbb{R}^{n}$ . There are mainly two non-elementwise operations in Transformer and PPGN that have the risk of leakage: the attention-vector product operation $\\mathbf{{Ax}}$ of Transformer, and the matrix-matrix product operation $_{A B}$ of PPGN. (We ignore the feature dimension of $\\pmb{A}$ and $\\mathbf{x}$ as it does not affect the information leakage.) Let $M\\in\\{0,1\\}^{n\\times n}$ be a mask matrix, such that $M_{i,j}=1$ if block_ID(node $i)\\geq$ block_ID(node $j$ ) else 0. One can verify that ", "page_idx": 7}, {"type": "equation", "text": "$$\n(A\\odot M)B+A(B\\odot M^{\\top})-(A\\odot M)(B\\odot M^{\\top})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "generalize $\\mathbf{{Ax}}$ and $_{A B}$ respectively and safely bypass information leakage. We provide more details of parallel training and derivation of Eq. (10) in Appx. $\\S\\mathrm{A.l0}$ . We use these operations in our network and enable representation sharing, along with parallel training of all $K_{B}$ blocks for denoising diffusion as well as next block size prediction. In practice, these offer more than $10\\times$ speed-up, and the parallel training allows PARD to scale to large datasets like MOSES [33]. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate PARD on 8 diverse benchmark datasets with varying sizes and structural properties, including both molecular (\u00a75.1) and non-molecular/generic (\u00a75.2) graph generation. A summary of the datasets and details are in Appx. $\\S\\mathrm{A.11}$ . Ablations and runtime measures are in Appx. $\\S\\mathrm{A}.12$ . ", "page_idx": 7}, {"type": "text", "text": "5.1 Molecular Graph Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets. We experiment with three different molecular datasets used across the graph generation literature: (1) QM9 [34] (2) ZINC250K [23], and (3) MOSES [33] that contains more than 1.9 million graphs. We use a $80\\%$ - $.20\\%$ train and test split, and among the train data we split additional $20\\%$ as validation. For QM9 and ZINC250K, we generate 10,000 molecules for stand-alone evaluation, and on MOSES we generate 25,000 molecules. ", "page_idx": 7}, {"type": "text", "text": "Baselines. The literature has not been consistent in evaluating molecule generation on well-adopted benchmark datasets and metrics. Among baselines, DiGress [44] stands out as the most competitive. We also compare to many other baselines shown in tables, such as GDSS [24] and GraphARM [25]. ", "page_idx": 7}, {"type": "text", "text": "Metrics. The literature has adopted a number of different evaluation metrics that are not consistent across datasets. Most common ones include Validity (\u2191) , Uniqueness (\u2191) (frac. of valid molecules that are unique), and Novelty (\u2191) (frac. of valid molecules that are not included in the training set). For QM9, following earlier work [44], we report additional evaluations w.r.t. Atom Stability $(\\uparrow)$ and Molecule Stability (\u2191), as defined by [22], whereas Novelty is not reported as explained in [44]. On ZINC250K and MOSES, we also measure the Fr\u00e9chet ChemNet Distance (FCD) $\\left(\\downarrow\\right)$ between the generated and the training samples, which is based on the embedding learned by ChemNet [27]. For MOSES, there are three additional measures: Filter (\u2191) score is the fraction of molecules passing the same filters as the test set, SNN (\u2191) evaluates nearest neighbor similarity using Tanimoto Distance, and Scaffold similarity (\u2191) analyzes the occurrence of Bemis-Murcko scaffolds [33]. ", "page_idx": 7}, {"type": "table", "img_path": "x4Kk4FxLs3/tmp/569b9ed04b27ae398308563d01022e5070c4e38ec02c2859b50d7eba82d6797c.jpg", "table_caption": ["Table 1: Generation quality on QM9 with explict hydrogens. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "x4Kk4FxLs3/tmp/5c06f3168a8a1d4ad39bfc77e905447235594a6d049d23c1495bdbee7ae50462.jpg", "table_caption": ["Table 2: Generation quality on ZINC250K. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Results. Table 1 shows generation evaluation results on QM9, where the baseline results are sourced from [44]. PARD outperforms DiGress and variants that do not use any auxiliary features, with slightly lower Uniqueness. What is notable is that PARD, without using any extra features, achieves a similar performance gap against DiGress that uses specialized extra features. Table 2 shows PARD\u2019s performance on ZINC250K, with baseline results carried over from [25] and [46]. PARD achieves the best Uniqueness, stands out in FCD alongside SwinGNN [46], and is the runner-up w.r.t. Validity. ", "page_idx": 8}, {"type": "table", "img_path": "x4Kk4FxLs3/tmp/640ee24c0e3d8c71182e99cf717facbd05cb38497cfb2e7293149bc9024372fe.jpg", "table_caption": ["Table 3: Generation quality on MOSES. The top three methods use hard-coded rules (not highlight). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Finally, Table 3 shows generation quality on the largest dataset MOSES. We mainly compare with DiGress and its variants, which has been the only general-purpose generative model in the literature that is not based on molecular fragments or SMILES strings. Baselines are sourced from [44]. While other specialized models, excluding PARD and DiGress, have hard-coded rules to ensure high Validity, PARD outperforms those on several other metrics including FCD and SNN, and achives competitive performance on others. Again, it is notable here that PARD, without relying on any auxiliary features, achieves similarly competitive results as with DiGress which utilizes extra features. ", "page_idx": 8}, {"type": "text", "text": "5.2 Generic Graph Generation ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "x4Kk4FxLs3/tmp/cba26816b171c3af5ae07404c58e89d789fa7c609b7b40107bc7e590b51f311c.jpg", "table_caption": ["Table 4: Generation quality on generic graphs. All metrics are based on generated-to-test set MMD distances, the lower the better. Top performance is in bold, and Runner-up is underlined. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Datasets. We use five generic graph datasets with various structure and semantic: (1) COMMUNITYSMALL [48], (2) CAVEMAN [47], (3) CORA [35], (4) BREAST [15], and (5) GRID [48]. We split each dataset into $80\\%{-}20\\%$ train-test, and randomly sample $20\\%$ of training graphs for validation. We generate the same number of samples as the test set. Notice that GRID contains graphs with $100{\\sim}400$ nodes, which is relatively large for diffusion models. There are lots of symmetries inside, hence it is difficult to capture all dependencies with permutation-equivariant models. ", "page_idx": 8}, {"type": "text", "text": "Baselines. We mainly compare against the latest general-purpose GraphArm [25], which reported DiGress [44] and GDSS [24] as top two most competitive, along with several other baselines. ", "page_idx": 8}, {"type": "text", "text": "Metrics. We follow [48] to measure generation quality using the maximum mean discrepancy (MMD) as a distribution distance between the generated graphs and the test graphs $\\left(\\downarrow\\right)$ , as pertain to distributions of $(i)$ Degree, $(i i)$ Clustering coefficient, and $(i i i)$ occurrence count of all Orbits. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 4 provides the generation results of PARD against the baselines as sourced from [25]. PARD shows outstanding performance achieving SOTA or close runner-up results, while none of the baselines shows as consistent performance across datasets and metrics. ", "page_idx": 9}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Q1: do we really need AR in diffusion, given diffusion is already permutation-invariant? ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In DiGress [44], we observed that pure diffusion, while being permutation-invariant, requires (1) many sampling/denoising steps to break symmetries and (2) additional features like eigenvectors to further break symmetry. This shows that directly capturing the FULL joint distribution and solving the transformation difficulty (in Sec. 3.3) via diffusion is challenging. Additionally, AR methods still dominate LLMs, indicating their potential to benefit diffusion models. To quantitatively verify our analysis, we perform an ablation study on the maximum number of hops, $K_{h}$ , which controls the extent of autoregression (AR). When $K_{h}=0$ , all nodes have a fixed degree of 1, resulting in a single block per graph, equivalent to full diffusion without AR. As $K_{h}$ increases, more blocks are generated with smaller average block sizes, indicating a greater number of AR steps. ", "page_idx": 9}, {"type": "table", "img_path": "x4Kk4FxLs3/tmp/3ac4cf58332057e3717a51d5757aa2c696b94e04f33a0507859d8911a42d02eb.jpg", "table_caption": ["Table 5: Ablation study on QM9 with varying maximum hops while keeping the total diffusion steps fixed (first two parts). The last part examines the effect of increasing steps for the no AR case. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 5 shows the result of the controlled experiment with 140 total diffusion steps across all trials, using the same model architecture, diffusion algorithm, and training settings. The significant improvement from $K_{h}=0$ to $K_{h}=1$ confirms that our enhancement stems from breaking the full joint distribution into several conditional distributions. Furthermore, adding more diffusion steps to full diffusion approach does not close the gap to AR enhanced diffusion, indicates the necessary of combining AR and diffusion. ", "page_idx": 9}, {"type": "text", "text": "Q2: how does model architecture affect performance? ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Table 6: Results for QM9 dataset with different model architectures with $K_{h}=3$ and 140 total steps. ", "page_idx": 9}, {"type": "table", "img_path": "x4Kk4FxLs3/tmp/a6ca30604225c4b3cd74d2ffc9db5a7919d0a106854b9044e91e79fe8d5957b1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 6 this indicates that PPGN component is essential for diffusion with autoregression. The design of combining PPGN and transformer in Sec. 4 further addresses the efficiency of PPGN. ", "page_idx": 9}, {"type": "text", "text": "Other ablations: other ablations and runtime measures are in Appx. $\\S\\mathrm{A.12}$ ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We presented PARD, the first permutation-invariant autoregressive diffusion model for graph generation. PARD decomposes the joint probability of a graph autoregressively into the product of several block conditional probabilities, by relying on a unique and permutation equivariant structural partial order. All conditional probabilities are then modeled with a shared discrete diffusion. PARD can be trained in parallel on all blocks, and efficiently scales to millions of graphs. PARD achieves SOTA performance on molecular and non-molecular datasets without using any extra features. Notably, we expect PARD to serve as a cornerstone toward generative foundation modeling for graphs. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Namrata Anand and Possu Huang. Generative modeling for protein structures. Advances in neural information processing systems, 31, 2018.   \n[2] Vikraman Arvind, Bireswar Das, and Johannes K\u00f6bler. The space complexity of k-tree isomorphism. In Algorithms and Computation: 18th International Symposium, ISAAC 2007, Sendai, Japan, December 17-19, 2007. Proceedings 18, pages 822\u2013833. Springer, 2007.   \n[3] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021.   \n[4] R Balakrishnan. The energy of a graph. Linear Algebra and its Applications, 387:287\u2013295, 2004.   \n[5] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. In International Conference on Learning Representations, 2022.   \n[6] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[7] Angela Bonifati, Irena Holubov\u00e1, Arnau Prat-P\u00e9rez, and Sherif Sakr. Graph generators: State of the art and open challenges. ACM computing surveys (CSUR), 53(2):1\u201330, 2020.   \n[8] Chen Cai, Truong Son Hy, Rose Yu, and Yusu Wang. On the connection between mpnn and graph transformer. International Conference on Machine Learning, 2023.   \n[9] Xiaohui Chen, Xu Han, Jiajing Hu, Francisco Ruiz, and Liping Liu. Order matters: Probabilistic modeling of node sequence for graph generation. In International Conference on Machine Learning, pages 1630\u20131639. PMLR, 2021.   \n[10] Suparna De, Maria Bermudez-Edo, Honghui Xu, and Zhipeng Cai. Deep generative models in the industrial internet of things: a survey. IEEE Transactions on Industrial Informatics, 18(9): 5728\u20135737, 2022.   \n[11] Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. arXiv preprint arXiv:1805.11973, 2018.   \n[12] William A Falcon. Pytorch lightning. GitHub, 3, 2019.   \n[13] Wenqi Fan, Chengyi Liu, Yunqing Liu, Jiatong Li, Hang Li, Hui Liu, Jiliang Tang, and Qing Li. Generative diffusion models on graphs: Methods and applications. arXiv preprint arXiv:2302.02591, 2023.   \n[14] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019.   \n[15] Laura Gonzalez-Malerva, Jaehong Park, Lihua Zou, Yanhui Hu, Zahra Moradpour, Joseph Pearlberg, Jacqueline Sawyer, Hallam Stevens, Ed Harlow, and Joshua LaBaer. High-throughput ectopic expression screen for tamoxifen resistance identifies an atypical kinase that blocks autophagy. Proceedings of the National Academy of Sciences, 108(5):2058\u20132063, 2011.   \n[16] Nikhil Goyal, Harsh Vardhan Jain, and Sayan Ranu. Graphgen: A scalable approach to domain-agnostic labeled graph generation. In Proceedings of The Web Conference 2020, pages 1253\u20131263, 2020.   \n[17] Nikhil Goyal, Harsh Vardhan Jain, and Sayan Ranu. Graphgen: A scalable approach to domain-agnostic labeled graph generation. In Proceedings of The Web Conference 2020, pages 1253\u20131263, 2020.   \n[18] Ivan Gutman, Xueliang Li, and Jianbin Zhang. Graph energy. Analysis of Complex Networks: From Biology to Linguistics, pages 145\u2013174, 2009.   \n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[20] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454\u201312465, 2021.   \n[21] Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. In International Conference on Learning Representations, 2022.   \n[22] Emiel Hoogeboom, V\u00edctor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3D. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 8867\u20138887. PMLR, 17\u201323 Jul 2022.   \n[23] John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52 (7):1757\u20131768, 2012.   \n[24] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning, pages 10362\u201310383. PMLR, 2022.   \n[25] Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B Aditya Prakash, and Chao Zhang. Autoregressive diffusion model for graph generation. In International Conference on Machine Learning, pages 17391\u201317408. PMLR, 2023.   \n[26] Yibo Li, Liangren Zhang, and Zhenming Liu. Multi-objective de novo drug design with conditional graph generative model. Journal of Cheminformatics, 10:1\u201324, 2018.   \n[27] Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative models of graphs. arXiv preprint arXiv:1803.03324, 2018.   \n[28] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel Urtasun, and Richard Zemel. Efficient graph generation with graph recurrent attention networks. Advances in neural information processing systems, 32, 2019.   \n[29] Liheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, K. Dokania, Mark Coates, Philip H.S. Torr, and Ser-Nam Lim. Graph Inductive Biases in Transformers without Message Passing. In Proc. Int. Conf. Mach. Learn., 2023.   \n[30] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. Advances in neural information processing systems, 32, 2019.   \n[31] Christopher Morris, Gaurav Rattan, Sandra Kiefer, and Siamak Ravanbakhsh. Speqnets: Sparsity-aware permutation-equivariant graph networks. In International Conference on Machine Learning, pages 16017\u201316042. PMLR, 2022.   \n[32] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling. In International Conference on Artificial Intelligence and Statistics, pages 4474\u20134484. PMLR, 2020.   \n[33] Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Simon Johansson, Hongming Chen, Sergey Nikolenko, Alan Aspuru-Guzik, and Alex Zhavoronkov. Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models. Frontiers in Pharmacology, 2020.   \n[34] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1\u20137, 2014.   \n[35] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[36] Chence $S\\mathrm{hi^{*}}$ , Minkai $\\mathrm{\\DeltaXu^{*}}$ , Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. In International Conference on Learning Representations, 2020.   \n[37] Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using variational autoencoders. In International conference on artificial neural networks, pages 412\u2013422. Springer, 2018.   \n[38] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and sunrya Ganguli. Deep unsunpervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.   \n[39] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[40] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[41] Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional node embeddings and structural graph representations. In International Conference on Learning Representations, 2020.   \n[42] Xiaochu Tong, Xiaohong Liu, Xiaoqin Tan, Xutong Li, Jiaxin Jiang, Zhaoping Xiong, Tingyang Xu, Hualiang Jiang, Nan Qiao, and Mingyue Zheng. Generative models for de novo drug design. Journal of Medicinal Chemistry, 64(19):14011\u201314027, 2021.   \n[43] Jeanne Trinquier, Guido Uguzzoni, Andrea Pagnani, Francesco Zamponi, and Martin Weigt. Efficient generative modeling of protein sequences using simple autoregressive models. Nature communications, 12(1):5800, 2021.   \n[44] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. In The Eleventh International Conference on Learning Representations, 2023.   \n[45] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In International Conference on Learning Representations, 2022.   \n[46] Qi Yan, Zhengyang Liang, Yang Song, Renjie Liao, and Lele Wang. Swingnn: Rethinking permutation invariance in diffusion models for graph generation. arXiv preprint arXiv:2307.01646, 2023.   \n[47] Jiaxuan You. Caveman Dataset. https://github.com/JiaxuanYou/graph-generation/ blob/master/create_graphs.py, 2018. [Online; accessed 31-Jan-2024].   \n[48] Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Generating realistic graphs with deep auto-regressive models. In International conference on machine learning, pages 5708\u20135717. PMLR, 2018.   \n[49] Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling trick: A theory of using graph neural networks for multi-node representation learning. Advances in Neural Information Processing Systems, 34:9061\u20139073, 2021.   \n[50] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any GNN with local structure awareness. In International Conference on Learning Representations, 2022.   \n[51] Lingxiao Zhao, Neil Shah, and Leman Akoglu. A practical, progressively-expressive gnn. Advances in Neural Information Processing Systems, 35:34106\u201334120, 2022.   \n[52] Lingxiao Zhao, Xueying Ding, Lijun Yu, and Leman Akoglu. Improving and unifying discrete&continuous-time discrete denoising diffusion. arXiv preprint arXiv:2402.03701, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Variational Lower Bound Derivation ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon\\int_{\\mathcal{T}}q(G_{1,T}|\\dot{\\Omega}_{0},\\dot{\\mathcal{H}}_{0}|\\Omega_{T})\\,d\\Omega_{T}\\geq\\Xi_{\\mathbf{t}(0,T)}q_{0}|\\mathfrak{t}_{\\infty}[\\log(\\Omega_{\\sigma})-\\log(\\Omega_{\\sigma,T}|\\Omega_{0})]}\\\\ &{=\\underbrace{\\mathbb{E}_{\\{\\theta(t_{0},T)\\},\\delta}\\Big[\\log(|\\Omega_{0,T}|)+\\sum_{i=1}^{T}\\log\\frac{p_{0}(G_{i,T}|\\Omega_{0})}{p_{i}(G_{i,T}|)}}\\\\ &{=\\underbrace{\\mathbb{E}_{\\eta(t_{0},T)\\log(|\\Omega_{0,T}|)}}_{=\\sum_{i=1}^{T}\\log(|\\Omega_{0,T}|)}+\\underbrace{\\log\\frac{p_{0}(G_{i,T}|\\Omega_{0})}{p_{i}(G_{i,T}|)}}_{=\\sum_{i=1}^{T}\\log\\frac{p_{0}(G_{i,T}|\\Omega_{0})}{p_{i}(G_{i,T}|)}}}\\\\ &{=\\underbrace{p_{0}(\\Theta_{t},\\log(|\\Omega_{0,T}|))}_{=\\sum_{i=1}^{T}\\log(|\\Omega_{0,T}|)}+\\underbrace{p_{0}(\\Theta_{t}|\\Omega_{0,T}|)}_{=\\sum_{i=1}^{T}\\log\\frac{p_{0}(G_{i,T}|\\Omega_{0})}{p_{i}(G_{i,T}|)}}\\underbrace{q_{0}(G_{i,T}|\\Omega_{0})}_{=\\sum_{i=1}^{T}\\log\\frac{p_{0}(G_{i,T}|\\Omega_{0})}{p_{i}(G_{i,T}|)}}\\}}\\\\ &{=\\underbrace{p_{0}(\\Theta_{t},\\log(|\\Omega_{0,T}|))}_{=\\sum_{i=1}^{T}\\log(|\\Omega_{0,T}|)}\\log(|\\Omega_{0,T}|)+\\underbrace{p_{0}^{\\alpha}(\\Theta_{t}|\\Omega_{0,T}|)}_{=\\sum_{i=1}^{T}\\log\\frac{p_{0}(G_{i,T}|\\Omega_{0})}{p_{i}(G_{i,T}|)}+\\sum_{i=1}^{T}\\log\\frac{p_{0}(G_{i,T\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using Eq. (1), the first term can simplified as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q(\\mathbf{G}_{1}|\\mathbf{G}_{0})}[\\sum_{i}\\log p_{\\theta}(\\mathbf{v}_{0}^{i}|\\mathbf{G}_{1})+\\sum_{i,j}\\log p_{\\theta}(\\mathbf{e}_{0}^{i,j}|\\mathbf{G}_{1})]\\;,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and similarly, the $t$ -th step loss $\\textstyle{\\mathcal{L}}_{t}(\\theta)$ is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{q(\\mathbf{G}_{t}|\\mathbf{G}_{0})}\\big[\\sum_{i}K L\\big(q(\\mathbf{v}_{t-1}^{i}|\\mathbf{v}_{t}^{i},\\mathbf{v}_{0}^{i})\\mid|\\,p_{\\theta}(\\mathbf{v}_{t-1}^{i}|\\mathbf{G}_{t}\\big)+}\\\\ &{}&{\\displaystyle\\sum_{i,j}K L\\big(q(\\mathbf{e}_{t-1}^{i,j}|\\mathbf{e}_{t}^{i,j},\\mathbf{e}_{0}^{i,j})\\mid|\\,p_{\\theta}(\\mathbf{e}_{t-1}^{i,j}|\\mathbf{G}_{t})\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2 Details of Discrete Diffusion Used in Paper ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We closely follow the approach in Zhao et al. [52] to define forward and backward process in discrete diffusion, and get the formulation of three important distributions, (i) $q\\big(\\mathbf{G}_{t}|\\mathbf{G}_{0}\\big)$ (ii) $q(\\mathbf{G}_{t-1}|\\mathbf{G}_{t},\\mathbf{G}_{0})$ , and (iii) $p_{\\theta}\\big(\\mathbf{G}_{t-1}\\big|\\mathbf{G}_{t}\\big)$ , needed for computing negative VLB based loss. In here, we explain the detailed constructions. Notice that we only use the most basic discrete-time discrete diffusion formulation in Zhao et al. [52], mainly for improved memory efficiency over other old approaches like D3PM [3]. Other enhanced techniques in [52] like approximated loss and continuous-time formulation are not used, to keep the discrete diffusion part simple. ", "page_idx": 13}, {"type": "text", "text": "DiGress [44] applies D3PM [3] to define these three terms, our approach is similar. Since all elements in the forward process are independent as shown in Eq. (1), one can verify that the two terms $q(\\mathbf{G}_{t}|\\mathbf{G}_{0})$ and $q(\\mathbf{G}_{t-1}|\\mathbf{G}_{t},\\mathbf{G}_{0})$ are in the form of a product of independent distributions on each element. For simplicity, we introduce the formulation for a single element $\\mathbf{x}$ , with $\\mathbf{x}$ being $\\mathbf{v}^{i}$ or $\\mathbf{e}^{i,j}$ . We assume each discrete random variable $\\mathbf{x}_{t}$ has a categorical distribution, i.e. $\\mathbf{x}_{t}\\sim\\mathrm{Cat}(\\mathbf{x}_{t};\\pmb{p})$ with $\\pmb{p}\\in[0,1]^{K}$ and $\\mathbf{1}^{\\top}p=1$ . One can verify that $p(\\mathbf{x}_{t}=\\mathbf{x}_{t})=x_{t}^{\\top}p$ , or simply $p(\\mathbf{x}_{t})\\,=\\,\\pmb{x}_{t}^{\\top}\\pmb{p}$ . As shown in Hoogeboom et al. [20], Austin et al. [3], the forward process with discrete variables $q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})$ can be represented as a transition matrix $\\tilde{Q}_{t}\\,\\in\\,[0,1]^{K\\times\\^{\\prime}K}$ such that $[Q_{t}]_{i j}\\,=\\,q(\\mathbf{x}_{t}\\,=\\,e_{j}|\\mathbf{x}_{t-1}\\,=\\,e_{i})$ ). Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\nq({\\bf x}_{t}|{\\bf x}_{t-1})=\\mathrm{Cat}({\\bf x}_{t};Q_{t}^{\\top}{\\pmb x}_{t-1})~.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Given transition matrices $Q_{1},...,Q_{T}$ , the forward conditional marginal distribution is ", "page_idx": 14}, {"type": "text", "text": "and the $(t\\mathrm{-}1)$ -step posterior distribution can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(i i\\right)\\;q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0})=\\mathrm{Cat}(\\mathbf{x}_{t-1};\\frac{Q_{t}\\mathbf{x}_{t}\\odot\\overline{{Q}}_{t-1}^{\\top}\\mathbf{x}_{0}}{{x}_{t}^{\\top}\\overline{{Q}}_{t}^{\\top}\\mathbf{x}_{0}})\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "See Apdx. $\\S\\mathrm{A.3}$ for the derivation. We have the option to specify node- or edge-specific quantities, $Q_{t}^{v,i}$ and $Q_{t}^{e,i,j}$ , respectively, or allow all nodes and edges to share a common $Q_{t}^{v}$ and $Q_{t}^{e}$ . Leveraging Eq. (15) and Eq. (16), we can precisely determine $q(\\mathbf{v}_{t}^{\\bar{i}}|\\mathbf{v}_{0}^{i})$ and $q(\\mathbf{v}_{t-1}^{i}|\\mathbf{v}_{t}^{i},\\mathbf{v}_{0}^{i})$ for every node, and a similar approach can be applied for the edges. To ensure simplicity and a non-informative $q(\\mathbf{G}_{T}|\\mathbf{G}_{0})$ (see Apdx. $\\S\\mathrm{A}.4)$ , we choose ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ_{t}=\\alpha_{t}I+(1-\\alpha_{t})\\mathbf{1}m^{\\top}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all nodes and edges, where $\\alpha_{t}\\in[0,1]$ , and $\\mathbf{\\nabla}m$ is a uniform distribution $(\\mathbf{1}/K_{v}$ for nodes and ${\\mathbf{1}}/K_{e}$ for edges). Note that DiGress [44] chooses $\\mathbf{\\nabla}m$ as the marginal distribution of nodes and edges. As $\\begin{array}{r}{p(\\mathbf{x}_{t-1}|\\bar{\\mathbf{x}_{t}})=\\sum_{\\mathbf{x}_{0}}q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0})p(\\mathbf{x}_{0}|\\mathbf{x}_{t})}\\end{array}$ , the parameterization of $p_{\\theta}(\\mathbf{G}_{t-1}|\\mathbf{G}_{t})$ can use the relationship, with ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(i i i)}&{{}p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{G}_{t})=\\displaystyle\\sum_{\\mathbf{x}_{0}}q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0})p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{G}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where x can be any $\\mathbf{v}^{i}$ or $\\mathbf{e}^{i,j}$ . With Eq. (18), we can parameterize $p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{G})$ directly with a neural network, and compute the negative VLB loss in Eq. (2) exactly, using Eq.s (15), (16) and (18). ", "page_idx": 14}, {"type": "text", "text": "A.3 Derivation of $q(x_{t-1}|x_{t},x_{0})$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "First, define $\\overline{{Q}}_{t\\vert s}=Q_{s+1}...Q_{t}$ . Note that $\\overline{{Q}}_{t\\mid0}=\\overline{{Q}}_{t}$ and $\\overline{{Q}}_{t\\mid t-1}=Q_{t}$ . Accordingly, we can derive the following two equalities. ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathsf{C a t}(\\mathbf{x}_{t};\\overline{{Q}}_{t}^{\\top}\\mathbf{x}_{t-1})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cdot(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0})=\\frac{q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})q(\\mathbf{x}_{t-1}|\\mathbf{x}_{0})}{q(\\mathbf{x}_{t}|\\mathbf{x}_{0})}=\\frac{\\mathbf{Cat}(\\mathbf{x}_{t};Q_{t}^{\\top}\\mathbf{x}_{t-1})\\mathbf{Cat}(\\mathbf{x}_{t-1};\\overline{{Q}}_{t-1}^{\\top}\\mathbf{x}_{0})}{\\mathbf{Cat}(\\mathbf{x}_{t};\\overline{{Q}}_{t}^{\\top}\\mathbf{x}_{0})}}\\\\ &{\\qquad\\qquad=\\frac{\\mathbf{x}_{t-1}^{\\top}Q_{t}\\mathbf{x}_{t}\\cdot\\mathbf{x}_{t-1}^{\\top}\\overline{{Q}}_{t-1}^{\\top}\\mathbf{x}_{0}}{\\mathbf{x}_{t}^{\\top}\\overline{{Q}}_{t}^{\\top}\\mathbf{x}_{0}}=\\mathbf{x}_{t-1}^{\\top}\\frac{Q_{t}\\mathbf{x}_{t}\\odot\\overline{{Q}}_{t-1}^{\\top}\\mathbf{x}_{0}}{\\mathbf{x}_{t}^{\\top}\\overline{{Q}}_{t}^{\\top}\\mathbf{x}_{0}}=\\mathbf{Cat}(\\mathbf{x}_{t-1};\\frac{Q_{t}\\mathbf{x}_{t}\\odot\\overline{{Q}}_{t-1}^{\\top}\\mathbf{x}_{0}}{\\mathbf{x}_{t}^{\\top}\\overline{{Q}}_{t}^{\\top}\\mathbf{x}_{0}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.4 Simplification of Transition Matrix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For any transition matrices $Q_{1},...,Q_{T}$ , which however should be chosen such that every row of $\\overline{{Q}}_{t}=\\dot{\\overline{{Q}}}_{t|0}$ converge to the same known stationary distribution when $t$ becomes large (i.e. at $T$ ), let the known stationary distribution be $\\mathbf{m}_{0}\\sim\\mathrm{Cat}(\\mathbf{m}_{0};m)$ . Then, the constraint can be stated as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow T}\\overline{{Q}}_{t}=\\mathbf{1}m^{\\top}\\ .\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To achieve the desired convergence on nominal data (which is typical data type in edges and nodes of graphs), while keeping the flexibility of choosing any categorical stationary distribution $\\mathbf{m}_{0}\\sim\\mathrm{Cat}(\\mathbf{m}_{0};m)$ , we define $Q_{t}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ_{t}=\\alpha_{t}I+(1-\\alpha_{t})\\mathbf{1}m^{\\top}\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\alpha_{t}\\in[0,1]$ . This results in the accumulated transition matrix $\\overline{{Q}}_{t\\mid s}$ being equal to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\overline{{Q}}_{t|s}=\\overline{{\\alpha}}_{t|s}I+(1-\\overline{{\\alpha}}_{t|s})\\mathbf{1}m^{\\top}\\,\\forall t>s\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{\\alpha}}_{t\\mid s}=\\prod_{i=s+1}^{t}\\alpha_{i}}\\end{array}$ . Note that $\\overline{{\\alpha}}_{t}=\\overline{{\\alpha}}_{t|0}=\\overline{{\\alpha}}_{t|s}\\overline{{\\alpha}}_{s}$ . We achieve Eq. (21) by picking $\\alpha_{t}$ such that $\\operatorname*{lim}_{t\\to T}\\overline{{\\alpha}}_{t}=0$ . ", "page_idx": 14}, {"type": "text", "text": "A.5 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We prove this for node case. For structurally equivalent edges, the analysis is the same. Assume node $i$ and node $j$ are structually equivalent, then we can find an automorphism $\\sigma\\in\\operatorname{Aut}(G)$ such that $\\sigma(i)=j$ . For any permutation $P\\in\\mathbb{P}_{|G|}$ and an equivariant network $f$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(P\\star\\mathrm{G})=P\\star f(\\mathrm{G})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Replace $_{P}$ with $\\sigma$ , and using the fact that $\\sigma\\star\\mathrm{G}=\\mathrm{G}$ . We can get ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\mathbf{G})=f({\\boldsymbol{\\sigma}}\\star\\mathbf{G})={\\boldsymbol{\\sigma}}\\star f(\\mathbf{G})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, we get $f(\\mathbf{G})_{i}=f(\\mathbf{G})_{j}$ , that is two nodes $i$ and $j$ have the same representation. ", "page_idx": 15}, {"type": "text", "text": "A.6 Proof of Exchangeable Probability ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To prove that $p_{\\theta}(\\mathbf{G})$ is an exchangeable probability, we need to show that for any permutation operator $_{P}$ with $P\\star\\mathrm{G}=(P\\mathrm{V},P\\mathrm{E}P^{\\top})$ , the probability satisfies $p_{\\theta}(P\\star\\mathrm{G})=p_{\\theta}(\\mathrm{G})$ . While $_{P}$ is defined as a group acting on all nodes $(|V|)$ in the graph $G$ , this operator can also be applied directly to sub-components of $\\mathrm{G}$ , in such a manner that it permutes only the elements within these components. We use $_{P}$ to these sub-components directly in the following proof. ", "page_idx": 15}, {"type": "text", "text": "Recall that PARD define $p_{\\theta}(\\mathbf{G})$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{G})=\\prod_{i=1}^{K_{B}}p_{\\theta}\\Big(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\Bigm|\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\Big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Observe that for a graph $G,\\,B_{i}$ represents a subset containing certain nodes from $G$ . When $G$ is represented in its default order $\\mathrm{G}$ , the subset $B_{i}$ can be represented as a binary vector mask $\\in\\{0,1\\}^{|\\mathcal{V}|}$ , where a value of 1 at the $j$ -th position indicates that the $j$ -th node in $\\mathrm{G}$ is included in the subset. Then ${\\bf G}[B_{i}]$ can be view as indexing nodes and edges from $\\mathrm{G}$ using mask $B_{i}$ . As indexing operation is permutation equivariant, we have $P\\star(\\mathrm{G}[\\mathcal{B}_{1:i}])\\stackrel{\\_}{=}(P\\star\\mathrm{G})[P\\star\\mathcal{B}_{1:i}]$ . What is more, $B_{i}$ is actually a function of $\\mathrm{G}$ and can be represented as $\\mathcal{B}_{1:i}(\\mathbf{G})$ . This function is actually permutation equivariant, such that $\\mathcal{B}_{i}(P\\!\\star\\!\\mathbf{G})=P\\!\\star\\!\\bar{\\mathcal{B}_{i}}(\\mathbf{G})$ , as $\\mathcal{B}_{i}(\\mathbf{G})$ is determined by algorithm 1 solely based on structural degree information (up to $K$ hops) of each node, where the structural feature is permutation equivariant. ", "page_idx": 15}, {"type": "text", "text": "Then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{0}(P_{\\star}\\,\\mathsf{G})}\\\\ &{=\\displaystyle\\prod_{i=1}^{K}p_{i}\\left(\\!\\left(P_{\\star}\\!\\:\\mathsf{G}\\right)\\!\\left|{B_{1:i}(P_{\\star}\\!\\:\\mathsf{G})}\\right\\rangle\\!\\left\\langle{P_{\\star}\\!\\:\\mathsf{G}}\\right\\rangle\\!\\left|{B_{1:i-1}\\!\\left(P_{\\star}\\!\\:\\mathsf{G}\\right)}\\right|\\:\\left|\\:(P_{\\star}\\,\\mathsf{G})\\!\\right\\rangle\\!\\right)}\\\\ &{\\quad\\displaystyle\\times_{\\mathrm{H}}^{K}p_{0}\\left(\\!\\left(P_{\\star}\\!\\:\\mathsf{G})\\!\\left|P_{\\star}\\!\\:\\mathsf{B}_{1:i}\\right|\\:\\middle\\rangle\\left(P_{\\star}\\!\\:\\mathsf{G}\\right)\\!\\left|P_{\\star}\\!\\:\\mathsf{B}_{1:i-1}\\right|\\:\\left|\\:(P_{\\star}\\!\\:\\mathsf{G})\\!\\right|P_{\\star}\\!\\:\\mathsf{B}_{1:i-1}\\right|\\right)~~~(\\mathrm{Prop},~3.1)}\\\\ &{=\\displaystyle\\prod_{i=1}^{K}p_{0}\\left(\\!\\left(P_{\\star}\\!\\:\\mathsf{G})\\!\\left|P_{\\star}\\!\\:\\mathsf{B}_{1:i}\\right|\\:\\middle\\rangle\\left(P_{\\star}\\!\\:\\mathsf{G}\\right)\\!\\left|P_{\\star}\\!\\:\\mathsf{A}_{1:i-1}\\right|\\:\\left|\\:(P_{\\star}\\!\\:\\mathsf{G})\\!\\right|P_{\\star}\\!\\:\\mathsf{B}_{1:i-1}\\right|\\right)~~~(\\mathrm{Prop},~3.1)}\\\\ &{\\quad\\displaystyle\\times_{\\mathrm{H}}^{K}p_{0}\\left(P_{\\star}\\!\\:\\left(\\!\\left[{B}_{1:i}\\right]\\!\\right)\\!\\setminus\\!P_{\\star}\\!\\:\\mathsf{G}[{B}_{1:i-1}]\\:\\middle\\vert\\:P_{\\star}\\!\\:\\mathsf{(G}[B_{1:i-1}]\\right)\\right)~~~(\\mathrm{Indexing}\\mathrm{\\\"~sequivatatace})}\\\\ &{\\quad\\displaystyle\\:\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Notice that we are using a permutation invariant function to approximate $p_{\\theta}\\Big(|\\mathcal{B}_{i}|\\;\\Big|\\;P\\star\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\Big)$ , hence we have ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{\\theta}\\Big(|\\mathcal{B}_{i}|\\;\\Big|\\;P\\star\\mathrm{G}[\\mathcal{B}_{1:i-1}]\\Big)=p_{\\theta}\\Big(|\\mathcal{B}_{i}|\\;\\Big|\\;\\mathrm{G}[\\mathcal{B}_{1:i-1}]\\Big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the second part, notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta}\\left(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\right|\\,\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\emptyset[\\mathcal{B}_{1:i}]\\right)=}\\\\ &{\\,\\,\\,\\int p_{\\theta}\\left(\\mathbf{H}[\\mathcal{B}_{1:i-1}]\\cup\\left(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\right)\\Big|\\,\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\emptyset[\\mathcal{B}_{1:i}]\\right)d\\mathbf{H}[\\mathcal{B}_{1:i-1}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta}\\left(P\\star(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}])\\Big|\\ P\\star(\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\varnothing[\\mathcal{B}_{1:i}])\\right)}\\\\ &{=\\int p_{\\theta}\\left(\\mathbf{H}[\\mathcal{B}_{1:i-1}]\\cup P\\star\\left(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\right)\\Big|\\ P\\star(\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\varnothing[\\mathcal{B}_{1:i}])\\right)d\\mathbf{H}[\\mathcal{B}_{1:i-1}]}\\\\ &{=\\int p_{\\theta}\\left(P\\star\\mathbf{H}[\\mathcal{B}_{1:i-1}]\\cup P\\star\\left(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\right)\\Big|\\ P\\star\\left(\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\varnothing[\\mathcal{B}_{1:i}]\\right)\\right)d P\\star\\mathbf{H}[\\mathcal{B}_{1:i-1}]}\\\\ &{=\\int p_{\\theta}\\left(P\\star\\left(\\mathbf{H}[\\mathcal{B}_{1:i-1}]\\cup\\left(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\right)\\right)\\Big|\\ P\\star\\left(\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\varnothing[\\mathcal{B}_{1:i}]\\right)\\right)d\\mathbf{H}[\\mathcal{B}_{1:i-1}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now notice that the probability $p_{\\theta}\\left(P{\\star}\\bigl(\\mathrm{H}[\\mathcal{B}_{1:i-1}]\\cup\\bigl(\\mathrm{G}[\\mathcal{B}_{1:i}]\\backslash\\mathrm{G}[\\mathcal{B}_{1:i-1}]\\bigr)\\bigr)\\right)\\Big|\\,P{\\star}\\bigl(\\mathrm{G}[\\mathcal{B}_{1:i-1}]\\cup\\varnothing[\\mathcal{B}_{1:i}]\\bigr)\\right)$ is actually a conditional distribution from one graph to another graph (with same size), and we can simplify it as $p_{\\theta}(\\mathrm{H}|\\mathbf{G})$ . As this part is modeled by diffusion model, we prove that this function is permutation equivariant. That is, for any permutation $_{P}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathrm{H}|\\mathbf{G})=p_{\\theta}(P\\star\\mathrm{H}|P\\star\\mathrm{G})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Our diffusion process is using an equivariant model through a markov chain that from $\\mathrm{G}$ to $\\mathrm{H}_{T}$ , and then from $\\mathrm{H}_{T}$ to $\\mathrm{H}_{T-1},...,\\mathrm{H}_{t}$ to $\\mathrm{H}_{t-1}$ , until $\\mathrm{H_{1}}$ to $\\mathrm{H}_{0}=\\mathrm{H}$ . Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{H}|\\mathbf{G})=p_{\\theta}(\\mathbf{H}_{0}|\\mathbf{G})=\\int p_{\\theta}(\\mathbf{H}_{T}|\\mathbf{G})\\prod_{t=1}^{T}p_{\\theta}(\\mathbf{H}_{t}|\\mathbf{H}_{t-1})d\\mathbf{H}_{1:T}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Because we are using an shared permutation equivariant model to model all $p_{\\theta}(\\mathrm{H}_{t}|\\mathrm{H}_{t-1})$ , we have $p_{\\theta}(\\mathrm{H}_{t}|\\mathrm{H}_{t-1})=\\bar{p}_{\\theta}(P\\star\\mathrm{H}_{t}|\\bar{P}\\star\\mathrm{H}_{t-1})\\forall t,P.$ . Also, because we have chosen the distribution $p_{\\theta}(\\mathbf{H}_{T}|\\dot{\\mathbf{G}})$ , such that all conditioned part from $\\boldsymbol{B}_{1:i}$ are the same with input $\\mathrm{G}$ , and all other left elements are sampled from isotrophic noise, the distribution $p_{\\theta}(\\mathrm{H}_{T}|\\mathbf{G})$ is also equivariant with $p_{\\theta}(\\mathrm{H}_{T}|\\mathbf{G})=p_{\\theta}(\\bar{P}\\star\\mathrm{H}_{T}|P\\star\\mathrm{G})$ . Take these properties back we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{p_{\\theta}(\\boldsymbol{P}\\star\\mathbf{H}|\\boldsymbol{P}\\star\\mathbf{G})=\\int p_{\\theta}(\\mathbf{H}_{T}|\\boldsymbol{P}\\star\\mathbf{G})p_{\\theta}(\\mathbf{H}_{1}|\\boldsymbol{P}\\star\\mathbf{H}_{0})\\prod_{t=2}^{T}p_{\\theta}(\\mathbf{H}_{t}|\\mathbf{H}_{t-1})d\\mathbf{H}_{1:T}}}\\\\ &{}&{\\quad=\\int p_{\\theta}(\\boldsymbol{P}\\star\\mathbf{H}_{T}|\\boldsymbol{P}\\star\\mathbf{G})p_{\\theta}(\\boldsymbol{P}\\star\\mathbf{H}_{1}|\\boldsymbol{P}\\star\\mathbf{H}_{0})\\prod_{t=2}^{T}p_{\\theta}(\\boldsymbol{P}\\star\\mathbf{H}_{t}|\\boldsymbol{P}\\star\\mathbf{H}_{t-1})d\\mathbf{H}_{1:T}}\\\\ &{}&{\\quad=\\int p_{\\theta}(\\mathbf{H}_{T}|\\mathbf{G})\\prod_{t=1}^{T}p_{\\theta}(\\mathbf{H}_{t}|\\mathbf{H}_{t-1})d\\mathbf{H}_{1:T}}\\\\ &{}&{\\quad=p_{\\theta}(\\mathbf{H}|\\mathbf{G})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With the conclusion from Eq. (31), we can apply it to Eq. (30) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta}\\left(P\\star(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}])\\Big|\\ P\\star(\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\emptyset[\\mathcal{B}_{1:i}])\\right)}\\\\ &{=\\int p_{\\theta}\\left(P\\star\\left(\\mathbf{H}[\\mathcal{B}_{1:i-1}]\\cup\\left(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\right)\\right)\\Big|\\ P\\star(\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\emptyset[\\mathcal{B}_{1:i}])\\right)d\\mathbf{H}[\\mathcal{B}_{1:i-1}]}\\\\ &{=\\int p_{\\theta}\\left(\\mathbf{H}[\\mathcal{B}_{1:i-1}]\\cup\\left(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\right)\\Big|\\ \\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\emptyset[\\mathcal{B}_{1:i}]\\right)d\\mathbf{H}[\\mathcal{B}_{1:i-1}]}\\\\ &{=p_{\\theta}\\left(\\mathbf{G}[\\mathcal{B}_{1:i}]\\setminus\\mathbf{G}[\\mathcal{B}_{1:i-1}]\\Big|\\ \\mathbf{G}[\\mathcal{B}_{1:i-1}]\\cup\\emptyset[\\mathcal{B}_{1:i}]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Apply Eq. (28) and Eq. (34) to Eq. (27), we can finalize our proof that ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{_{i\\theta}(P\\star\\mathbf{G})=\\displaystyle{\\prod_{i=1}^{K_{B}}p_{\\theta}}\\left(|{\\mathcal{B}}_{i}|~{\\Big|}~P\\star\\mathbf{G}[{\\mathcal{B}}_{1:i-1}]\\right)p_{\\theta}\\left(P\\star(\\mathbf{G}[{\\mathcal{B}}_{1:i}]\\setminus\\mathbf{G}[{\\mathcal{B}}_{1:i-1}]){\\Big|}~P\\star(\\mathbf{G}[{\\mathcal{B}}_{1:i-1}]\\cup\\theta[{\\mathcal{B}}_{1:i}])\\right)}\\\\ &{\\qquad\\qquad=\\displaystyle{\\prod_{i=1}^{K_{B}}p_{\\theta}}\\left(|{\\mathcal{B}}_{i}|~{\\Big|}~\\mathbf{G}[{\\mathcal{B}}_{1:i-1}]\\right)p_{\\theta}\\left(\\mathbf{G}[{\\mathcal{B}}_{1:i}]\\setminus\\mathbf{G}[{\\mathcal{B}}_{1:i-1}]\\right)\\left[\\mathbf{G}[{\\mathcal{B}}_{1:i-1}]\\cup\\theta[{\\mathcal{B}}_{1:i}]\\right)}\\\\ &{\\qquad=\\displaystyle{\\prod_{i=1}^{K_{B}}p_{\\theta}}\\left(\\mathbf{G}[{\\mathcal{B}}_{1:i}]~\\backslash~\\mathbf{G}[{\\mathcal{B}}_{1:i-1}]~\\right|~\\mathbf{G}[{\\mathcal{B}}_{1:i-1}]\\right)=p_{\\theta}(\\mathbf{G})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, our method PARD is a permutation-invariant probability model. ", "page_idx": 17}, {"type": "text", "text": "A.7 Definition of Graph Transformation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Definition A.1 (Graph Transformation). Given a labeled graph $\\mathbf{G}=(\\mathbf{V}_{G},\\mathbf{E}_{G})$ and a labeled target graph $\\mathbf{H}\\;=\\;\\left(\\mathbf{V}_{H},\\mathbf{E}_{H}\\right)$ that have the same number of nodes, the graph transformation function $f:\\mathbf{G}\\rightarrow\\mathbf{H}$ is defined such that $\\mathrm{f}(\\mathrm{G})=\\mathrm{H}$ . ", "page_idx": 17}, {"type": "text", "text": "Notice that while the graph transformation function requires input and output graph have the same graph size, this function is general enough to accommodate any changing of size operation. Specifically, if G has more nodes than $\\mathrm{H}$ , the function must assign an \u2019empty\u2019 token to nodes and edges within G that do not correspond to those in H. Conversely, if G has fewer nodes than H, the function will augment $\\mathrm{G}$ with \u2019empty\u2019 nodes and edges until it matches the size of H. Subsequently, it transforms this augmented version of $\\mathrm{G}$ to match $_\\mathrm{H}$ . ", "page_idx": 17}, {"type": "text", "text": "Definition A.2 (Equivariant Graph Transformation). An equivariant graph transformation $f$ is a graph transformation that satisfies $f(P\\star\\mathrm{G})=f(P\\star\\mathrm{H})$ for any permutation operation $_{P}$ . ", "page_idx": 17}, {"type": "text", "text": "A.8 Algorithms of Training and Generation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present blocksize prediction algorithm in Algo. 2 and denoising diffusion algorithm in Algo. 3. Notice while the blocksize network $g_{\\boldsymbol{\\theta}}$ in Algo. 2 and denoising diffusion network $f_{\\theta}$ in Algo. 3 use the same subscript parameter $\\theta$ , the $\\theta$ essentially represents concatenation of $g$ and $f$ \u2019s parameters. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 2 Train blocksize distribution $\\overline{{p_{\\theta}\\big(|B_{i}|\\;\\big|\\;\\mathbf{G}[B_{1:i-1}]\\big)}}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1: Input: G, maximum hop $K_{h}$ , a network $g_{\\theta}$ that takes a graph as input and output graph wise   \nprediction.   \n2: Get structural partial order function $\\phi$ of $\\mathrm{G}$ from Algo.1.   \n3: Using $\\phi$ to get the sequence of node blocks $[B_{1},...,B_{K_{B}}]$ for G.   \n4: Minimize $\\sum_{i=1}^{K_{B}}$ CrossEntropy $(g_{\\theta}(\\mathbf{G}[B_{i}]),|B_{i+1}|)$ , with $|B_{K_{B}+1}|=0$   \n1: Input: blocksize model $g_{\\theta}$ , diffusion model $f_{\\theta}$ ; first blocksize distribution from TrainSet.   \n2: $\\mathbf{G}\\gets\\emptyset$ ; $i\\gets1$   \n3: Sample $n$ from the first block\u2019s size distribution.   \n4: while $n>0$ do   \n5: Add a new block $B_{i}$ with $n$ nodes into $\\mathrm{G}$   \n6: $M\\gets$ indice mask of $\\mathbf{G}[B_{1:i}]\\setminus\\mathbf{G}[B_{1:i-1}]$   \n7: $\\Tilde{\\mathbf{G}}\\gets$ For nodes and edges within $M$ , sample from noise ${\\mathbf{\\nabla}}m_{n}$ and ${\\mathbf{\\nabla}}m_{e}$ .   \n8: for $j=1:T$ do   \n9: p \u2190f\u03b8(G\u02dc)   \n10: $S\\gets$ Sample according to p   \n11: ${\\tilde{\\mathbf{G}}}\\gets M\\odot S+(1-M)\\odot{\\tilde{\\mathbf{G}}}$   \n12: end for   \n13: $\\mathbf{G}\\gets\\tilde{\\mathbf{G}}$   \n14: n \u2190Sample from $g_{\\theta}(\\mathbf{G})$   \n15: $i\\gets i+1$   \n16: end while   \n17: Return: G ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "A.9 Visualization of the PPGN-Transformer Block ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "x4Kk4FxLs3/tmp/148b3aeee9f16aac214a843022d8f0b84c80cf8eccf5978e4a7ab4a7c110d1b0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 3: The Architecture of the PPGN-Transformer Block. In (b) and (c) we provide illustrations of how edge and node features are processed through Transformer and PPGN blocks. ", "page_idx": 18}, {"type": "text", "text": "A.10 Details and Derivations of Causal Matrix-Matrix Product ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.10.1 Derivation of causal version matrix product ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For any matrix $\\mathbf{\\deltaX}$ , let $X_{i}$ denotes the $i^{\\th}$ -th row of $\\mathbf{\\deltaX}$ , and $X_{:,i}$ denotes $i$ -th column of $\\mathbf{\\deltaX}$ . Let $\\langle\\cdot,\\cdot\\rangle$ denotes vector inner product. For normal matrix product $A B$ , we know that ", "page_idx": 18}, {"type": "equation", "text": "$$\n(A B)_{i j}=\\langle A_{i},B_{:,j}\\rangle\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In our causal setting with block ID introduced in main context, the $(i,j)$ position will have its block ID being max(block_ $\\operatorname{ID}(i)$ , block_ ${\\mathrm{ID}}(j)$ ). Hence, the product in Eq. (35) should not use any information outside of block whose $\\mathrm{ID}$ is larger than $\\mathrm{max}(\\mathrm{block\\_ID}(i),\\mathrm{block\\_ID}(j))$ ). Let $O_{i j}$ be the needed safe output at position $(i,j)$ of the matrix-matrix product, one can verify that the following uses all information within useable blocks without any information leakage. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{O_{i j}=\\langle A_{i}\\odot(M_{i}\\mathrm{~or~}M_{j}),B_{:,j}\\rangle\\quad\\mathrm{(where~or~denotes~elementwise~or~op~}}\\\\ &{\\hphantom{=}=\\langle A_{i}\\odot(M_{i}+M_{j}-M_{i}\\odot M_{j}),B_{:,j}\\rangle}\\\\ &{\\hphantom{=}=\\langle A_{i}\\odot M_{i},B_{j}\\rangle+\\langle A_{i},B_{:,j}\\odot M_{j}\\rangle-\\langle A_{i}\\odot A_{i},B_{:,j}\\odot M_{j}\\rangle}\\\\ &{\\hphantom{=}=\\langle A_{i}\\odot M_{i},B_{j}\\rangle+\\langle A_{i},B_{:,j}\\odot M_{:,j}^{\\top}\\rangle-\\langle A_{i}\\odot A_{i},B_{:,j}\\odot M_{:,j}^{\\top}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Where mask matrix $_M$ satisfies $M_{i,j}=1$ if block_ID(node $i)\\geq$ block_ID(node $j$ ) else 0. ", "page_idx": 18}, {"type": "text", "text": "Based on Eq. (36), we can rewrite it to matrix operation such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nO=(A\\odot M)B+A(B\\odot M^{\\top})-(A\\odot M)(B\\odot M^{\\top})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Where the matrix $^o$ \u2019s $(i,j)$ position value is just $O_{i j}$ in Eq. (36). Hence we have derived the equation in Eq. (10). ", "page_idx": 19}, {"type": "text", "text": "A.10.2 Discussion of expressivity ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "While the causal matrix-matrix product makes the training more efficient with parallel support, we have to acknowledge that it is highly possible that its expressiveness in graph distinguishing ability is reduced. Hence the causal PPGN should have less expressiveness than the normal PPGN. In fact, we have found that for symmetry-rich datasets like GRID, sequential training of PARD\u2019s loss is easier to minimize than parallel training of PARD. ", "page_idx": 19}, {"type": "text", "text": "A.10.3 Additional details of parallel training ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The previous discussion mainly focuses on preventing information leakage, which is the most critical aspect of causal parallel training. Another essential component of parallel training is the ability to output all predictions for subsequent blocks simultaneously. For GPT, no modifications are necessary for predicting all next tokens, as the next token can simply use the position from the previous token. However, when predicting the next block given all previous blocks, it\u2019s not feasible to use any positions within previous blocks to hold the prediction for the next block due to differences in size and lack of alignment. Therefore, as shown in Eq. (6), a virtual block needs to be augmented to serve as a prediction placeholder for the next block. For parallel prediction, a virtual block will be augmented for each block in the graph, with each virtual block having the same block ID as the corresponding original block. To summarize, for a graph with $N$ nodes, it needs to be expanded to $2N$ nodes by adding $N$ virtual nodes for parallel training. ", "page_idx": 19}, {"type": "text", "text": "A.11 Experiment Details ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "x4Kk4FxLs3/tmp/7652ee49d3c0ecb856891b62543378929ad02da204b97a22007c3e0b9e75906f.jpg", "table_caption": ["Table 7: Dataset summary "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "We use a single RTX-A6000 GPU for all experiments. For discrete diffusion, we follow [52]\u2019s basic discrete diffusion implementation, where exponential moving average is not enabled. For model implementation, we use Pytorch Geometric [14], and we implement our combination of PPGN and Transformer by referencing the code in Maron et al. [30] and Ma et al. [29]. Additionally, we use Pytorch Lightning [12] for training and keeping the code clean. We use Adam optimizer with cosine decay learning rate scheduler to train. For diffusion and blocksize prediction, we also input the embedding of block id and node degree as additional feature, as we have block id computed in preprocessing. Additionally, it is important to observe that nodes within the same block all have the same degree. At diffusion stage, we conditional on the predicted degree for the next block to train diffusion model, where the degree is predicted along with the block size using the same network. We provide source code and all configuration files at https://github.com/LingxiaoShawn/Pard. ", "page_idx": 19}, {"type": "text", "text": "A.12 Ablations and Runtime Measure ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "How does the sampling steps for diffusion models in PARD impact the performance of graph generation? To answer it, we conducted a detailed ablation on number of diffusion steps in QM9, with results in Table. 8. To summarize, too small a number of steps hurt the performance, while too large doesn\u2019t help improve the performance further. See the table below. Within the table, we have emphasized a configuration that employs just 140 total diffusion steps. This amount is considerably less than the steps used by DiGress, yet it dramatically outperforms DiGress. ", "page_idx": 19}, {"type": "text", "text": "While graphs are trained with a single, fixed generation path based on $\\phi$ , is the same path being used in generation? We acknowledge that although each graph undergoes a single decomposition block sequence during training, it is possible for the generation path to differ from the training path. This discrepancy is attributed to a phenomenon known as exposure bias, a challenge encountered in autoregressive methods and imitation learning alike. Despite this, we have conducted empirical analysis to estimate the likelihood of divergence between the generation and training paths. By sampling 2,000 graphs generated by the model trained on the QM9 dataset, we tracked the generation path for each graph and compared it to the training path defined by the decomposition block sequence algorithm. Our findings indicate that $94.7\\%$ of the generated samples followed the exact same path as their corresponding training path. ", "page_idx": 19}, {"type": "table", "img_path": "x4Kk4FxLs3/tmp/eea6fe9ce4edf3b949bb95bed9cb352af4fa0b272659075383651ea2c24bbb35.jpg", "table_caption": ["Table 8: Comparison of different models on QM9 dataset "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Runtime comparison. To show that our method doesn\u2019t introduce much runtime overhead, we report the training time in Table 9 used to achieve the number reported on two datasets: QW9 and Moses. The training time for DiGress is mentioned in their github. Notice that the training time is also affected by the machine, we use GPU A6000 in all experiments. ", "page_idx": 20}, {"type": "table", "img_path": "x4Kk4FxLs3/tmp/ff661bb167a4fbc73a6b3bb2678da96734ee92a8f4a33a593c61cda5e1d2b1df.jpg", "table_caption": ["Table 9: Training time comparison on QM9 and Moses datasets "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.13 Visualization of Generation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A.13.1 QM9 ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "x4Kk4FxLs3/tmp/835549b45a0fa5e5d8b9e2f8bc5df1f19b53bc5eddcbe3c93f4d27b0c43f8b87.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 4: Non curated QM9 (with explicit hydrogens) graphs generated from the PARD trained with 20 steps per block. ", "page_idx": 20}, {"type": "table", "img_path": "x4Kk4FxLs3/tmp/98a064a08e15d53c3836717180ef9782cf99eb2a0e7c32dfd8ba70c628c85a1a.jpg", "table_caption": [], "table_footnote": ["Table 10: Performance comparison of diffusion methods on GRID. "], "page_idx": 21}, {"type": "text", "text": "A.13.2 Grid ", "page_idx": 21}, {"type": "image", "img_path": "x4Kk4FxLs3/tmp/3755b5ffcdf08470fca42d3853a597f10f0538905cadde43bd258dc9497da86d.jpg", "img_caption": ["Figure 5: Non curated grid graphs generated from the PARD trained with 50 steps per block. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "x4Kk4FxLs3/tmp/1c0df1f7679f8e39c6d97bce84b65b961afee2da0668b58cea46855208c2198b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 6: Non curated grid graphs generated from the PARD (with eigenvector) trained with 50 steps per block. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We propose a new graph diffusion method that integrates autoregressive graph generation with diffusion. We showcase our method in the main paper with thorough experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We briefly discuss the limitations of PARD with respect to long training and sampling times, but we have pointed out potential improvements with for parallel training in causal transformers. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All our theorems, formulas and proofs in the paper are numbered and referenced. The assumptions are clearly stated. The proofs are shown in main paper and appendix. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We introduce a new algorithm and architecture for graph generation tasks. The detailed explanations on the algorithm, and the experiment details are clearly shown in the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 23}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have publicized our training repository with an anonymous link referenced in the paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We use the same test bed as our benchmarking methods, so the dataset split and evaluation codes are publicly available. The hyperparameter configurations are discussed in the appendix as well as in our code library. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide experiment results and evaluations over 10,000-25,000 sampled graphs, so the numbers are of high confidence. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the memory, running time, sampling time discussions in the main paper and in appendix. All experiments are running on A6000 GPUs. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Justification: The authors have reviewed the code of ethics.The paper and the code base preserve anonymity. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer:[NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Justification: All our datasets and benchmarking methods are from publicly available libraries under proper licenses. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide anonymous code base. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]