[{"figure_path": "pqYceEa87j/tables/tables_5_1.jpg", "caption": "Table 1: TruthfulQA results. All models are built on LLaMA-2-Chat-7B. TTrain and TInf. are the overall training and average inference time (seconds) per sample. \u2020: SEA significantly increases MC1/2 on ICL by pair-wise t-test with p < 0.05. Part of results are from [45]. For ICL and SEA, we also report the performance in the Best-of-N distribution [36].", "description": "This table presents the results of several methods on the TruthfulQA benchmark, focusing on truthfulness evaluation.  It compares the performance of In-Context Learning (ICL), LoRA Fine-tuning (LoRA-FT), Inference-time Intervention (ITI), Decoding by contrasting layers (DoLA), Contrastive Decoding (CD), Induce-then-Contrast Decoding (ICD), and Spectral Editing of Activations (SEA).  The metrics include MC1, MC2, Info, Truth, Info*Truth, inference time (TInf.), and training time (TTrain).  Best-of-N results are also shown for ICL and SEA to demonstrate the impact of ensemble methods.", "section": "3.1 Truthfulness"}, {"figure_path": "pqYceEa87j/tables/tables_6_1.jpg", "caption": "Table 3: BBQ performance (in terms of accuracy, Acc.%\u2191, unknown-answer rate, Unk.%\u2193 and stereotypical response rate, SR%\u2193) and TruthfulQA performance (in terms of MC1\u2191/2\u2191) after applying ICL, Linear SEA and non-linear SEA (\u03a6-SEA) on six open-source LLMs. We highlight the improved and worsened metrics, respectively.", "description": "This table presents the results of applying ICL, linear SEA, and non-linear SEA to six different open-source LLMs on two datasets: BBQ and TruthfulQA.  For each LLM and method, it shows the percentage increase/decrease in accuracy, unknown answer rate, bias score, and stereotypical response rate (BBQ) as well as the percentage increase in MC1 and MC2 scores (TruthfulQA).  Improvements and worsens are highlighted to show the effectiveness of the proposed methods. The table aims to demonstrate the generalizability of the SEA approach across various LLMs.", "section": "3.2 Bias"}, {"figure_path": "pqYceEa87j/tables/tables_6_2.jpg", "caption": "Table 2: Ablation study on 7B LLaMA-2-Chat model's performance on TruthfulQA.", "description": "This table presents the results of an ablation study conducted to evaluate the contribution of each individual component of the SEA method on the TruthfulQA benchmark using the 7B LLaMA-2-Chat model.  It shows the MC1 and MC2 scores for the full SEA method and variations where only positive or negative editing is used, averaging is performed instead of merging, only the top or bottom three layers are edited, and when the editing projections are reversed. The results highlight the importance of using both positive and negative projections, merging the edited activations, using the top layers and correctly orientating the edits for optimal performance.", "section": "4.1 Truthfulness Evaluation"}, {"figure_path": "pqYceEa87j/tables/tables_7_1.jpg", "caption": "Table 3: BBQ performance (in terms of accuracy, Acc.%\u2191, unknown-answer rate, Unk.%\u2193 and stereotypical response rate, SR%\u2193) and TruthfulQA performance (in terms of MC1\u2191/2\u2191) after applying ICL, Linear SEA and non-linear SEA (\u03a6-SEA) on six open-source LLMs. We highlight the improved and worsened metrics, respectively.", "description": "This table presents the results of applying three different methods (ICL, Linear SEA, and non-linear SEA) to six different open-source LLMs.  The performance is evaluated on two benchmarks: BBQ (Bias Benchmark for QA) and TruthfulQA. For BBQ, the metrics are accuracy, unknown answer rate, and stereotypical response rate.  For TruthfulQA, the metrics are MC1 and MC2 scores.  The table highlights whether the performance of each method improves or worsens compared to the baseline (ICL).", "section": "3.2 Bias"}, {"figure_path": "pqYceEa87j/tables/tables_8_1.jpg", "caption": "Table 4: Performance of LLaMA-2-Chat-7B and its three SEA edited models for truthfulness and fairness on six control tasks covering multi-task ability, commonsense question answering, and mathematical ability. Details of evaluation are provided in Appendix H.4.", "description": "This table presents the results of evaluating the LLaMA-2-Chat-7B model and three variants modified using Spectral Editing of Activations (SEA) on six different tasks. These tasks assess various aspects of language model capabilities, including multi-task performance, commonsense reasoning, and mathematical abilities.  The table shows the performance of each model on each task, allowing for a comparison of the impact of SEA on these diverse capabilities. Note that the details of the evaluation methodology are described in Appendix H.4 of the paper.", "section": "4.5 Post-Editing Performance on Control Tasks"}, {"figure_path": "pqYceEa87j/tables/tables_8_2.jpg", "caption": "Table 5: Results of all SEA methods editing with BBQ's demonstrations on all bias categories of CrowS-Pairs. Abbreviations: Aut \u2013 Autre, Dis \u2013 Disability, Gen \u2013 Gender, Nat \u2013 Nationality, App \u2013 Appearance, Rel \u2013 Religion, R/C \u2013 Race/Color, SE \u2013 Socioeconomic, and SO \u2013 Sexual Orientation.", "description": "This table presents the results of applying three different SEA methods (linear SEA, squared exponential \u03a6-SEA, and tanh \u03a6-SEA) on the CrowS-Pairs dataset, using demonstrations from the BBQ dataset. It evaluates the generalization of SEA's bias-mitigating effects to a new dataset with similar bias categories. For each bias category and method, the table shows the percentage of stereotypical responses predicted by the model. Lower values indicate better performance, indicating less bias in the model's output.", "section": "4.6 Generalisation of Editing Effects to Similar Tasks"}, {"figure_path": "pqYceEa87j/tables/tables_16_1.jpg", "caption": "Table 6: Performance of the joint and specialised SEA's editing on TruthfulQA and BBC Datasets.", "description": "This table compares the performance of different spectral editing of activations (SEA) methods on two benchmark datasets: TruthfulQA (for truthfulness) and BBC (for bias). It contrasts the results of applying SEA in a specialized way (focused on either truthfulness or bias) versus a joint approach (targeting both simultaneously).  The metrics used are MC1 and MC2 for TruthfulQA, and Accuracy for BBC.  The table shows that specialized SEA outperforms the joint approach, suggesting that focusing on one attribute at a time yields better results.", "section": "E Joint Editing for the Truthfulness and Fairness"}, {"figure_path": "pqYceEa87j/tables/tables_16_2.jpg", "caption": "Table 7: Ablation study of Linear-SEA editing on the LLaMA-2-Chat-7B model using varied token activations on TruthfulQA.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different methods for extracting LLM activations on the performance of Linear-SEA on the TruthfulQA benchmark.  The study compared three methods: using activations from the last token, the mean of all tokens, and randomly selected tokens.  The MC1 and MC2 scores are reported for each method, demonstrating the impact of activation selection on the model's performance.", "section": "F Ablation on the Types of LLM's Activations"}, {"figure_path": "pqYceEa87j/tables/tables_18_1.jpg", "caption": "Table 1: TruthfulQA results. All models are built on LLaMA-2-Chat-7B. TTrain and TInf. are the overall training and average inference time (seconds) per sample. \u2020: SEA significantly increases MC1/2 on ICL by pair-wise t-test with p < 0.05. Part of results are from [45]. For ICL and SEA, we also report the performance in the Best-of-N distribution [36].", "description": "This table presents the results of different methods on the TruthfulQA benchmark, focusing on the multiple-choice question answering task.  It compares the performance of In-Context Learning (ICL), LoRA fine-tuning, Inference-time Intervention (ITI), Decoding with Layer Attention (DOLA), Contrastive Decoding (CD), Induce-then-Contrast Decoding (ICD), and the proposed Spectral Editing of Activations (SEA) method.  The metrics used are MC1, MC2, Info, Truth, and Info*Truth, along with inference and training times.  The table highlights SEA's superior performance and efficiency, particularly in conjunction with the Best-of-N evaluation strategy.", "section": "3.1 Truthfulness"}]