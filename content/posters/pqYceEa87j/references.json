{"references": [{"fullname_first_author": "Stephanie Lin", "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods", "publication_date": "2022", "reason": "This paper introduces TruthfulQA, a benchmark dataset used in the experiments for evaluating the truthfulness of language models, directly influencing the paper's experimental design and results."}, {"fullname_first_author": "Zhongzhi Chen", "paper_title": "Truth Forest: Toward multi-scale truthfulness in large language models through intervention without tuning", "publication_date": "2024", "reason": "This paper proposes a method for enhancing truthfulness in LLMs without retraining, offering a comparative approach to the proposed SEA method."}, {"fullname_first_author": "Yung-Sung Chuang", "paper_title": "DoLa: Decoding by contrasting layers improves factuality in large language models", "publication_date": "2023", "reason": "This paper presents another approach to improve LLM factuality, offering a basis for comparison with the proposed SEA method."}, {"fullname_first_author": "Kenneth Li", "paper_title": "Inference-time intervention: Eliciting truthful answers from a language model", "publication_date": "2024", "reason": "This paper explores inference-time editing of LLM activations, providing a relevant technique compared to the proposed method."}, {"fullname_first_author": "Alicia Parrish", "paper_title": "BBQ: A hand-built bias benchmark for question answering", "publication_date": "2022", "reason": "This paper introduces the BBQ benchmark dataset, crucial for evaluating the fairness of language models, which significantly impacts the experimental evaluation in the paper."}]}