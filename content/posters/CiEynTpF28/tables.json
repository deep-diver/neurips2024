[{"figure_path": "CiEynTpF28/tables/tables_5_1.jpg", "caption": "Table 1: Properties of different distribution divergences in typical distributional RL algorithms. d is the sample dimension and \u03ba = 2\u03b2d + ||c||\u221e, where the cost function c is B-Lipschitz [24]. Sample complexity is improved to O(1/n) using the kernel herding technique [10] in MMD.", "description": "This table compares different distributional reinforcement learning algorithms based on three criteria: the distribution divergence used, the representation of the return distribution (categorical, quantiles, or samples), the convergence rate of the Bellman operator, and the sample complexity of the divergence.  It highlights the differences in how various algorithms approach estimating the distribution of returns and the computational cost associated with each.", "section": "3 Related Work"}, {"figure_path": "CiEynTpF28/tables/tables_23_1.jpg", "caption": "Table 2: Evaluation of best Human Normalized Scores (HNS) across 55 Atari games. Results are averaged over 3 seeds. Our proposed SinkhornDRL achieves the best performance in terms of Mean and IQM(5%) HNS as well as the \u201c>DQN\u201d metric, and is on par with MMD-DQN in terms of Median of HNS.", "description": "This table presents a comparison of the performance of several distributional reinforcement learning algorithms across 55 Atari games.  The algorithms are DQN, C51, QR-DQN, MMD-DQN, and the proposed SinkhornDRL.  Performance is measured using three metrics: Mean, Interquartile Mean (IQM) at the 5% level, and Median of the Human Normalized Scores (HNS). The final column, '>DQN', indicates how many games each algorithm outperformed DQN.", "section": "Summary Table for Human Normalized Scores (HNS)"}, {"figure_path": "CiEynTpF28/tables/tables_25_1.jpg", "caption": "Table 1: Properties of different distribution divergences in typical distributional RL algorithms. d is the sample dimension and \u03ba = 2\u03b2d + ||c||\u221e, where the cost function c is B-Lipschitz [24]. Sample complexity is improved to O(1/n) using the kernel herding technique [10] in MMD.", "description": "This table summarizes the properties of various distribution divergences used in distributional reinforcement learning algorithms.  It compares algorithms (C51, QR-DQN, MMD-DQN, and SinkhornDRL) across distribution divergence used, distribution representation, convergence rate of the Bellman operator, and sample complexity of the divergence metric.  The table highlights the differences in how these algorithms approach the problem of estimating and utilizing return distributions in reinforcement learning.", "section": "Related Work"}, {"figure_path": "CiEynTpF28/tables/tables_26_1.jpg", "caption": "Table 1: Properties of different distribution divergences in typical distributional RL algorithms. d is the sample dimension and \u03ba = 2\u03b2d + ||c||\u221e, where the cost function c is \u03b2-Lipschitz [24]. Sample complexity is improved to O(1/n) using the kernel herding technique [10] in MMD.", "description": "This table compares several distributional reinforcement learning algorithms based on different divergence metrics (Cram\u00e9r, Wasserstein, MMD, and Sinkhorn).  It highlights key properties for each algorithm, such as the type of distribution representation used (categorical, quantiles, or samples), the convergence rate of the distributional Bellman operator, and the sample complexity of the divergence metric.", "section": "Related Work"}]