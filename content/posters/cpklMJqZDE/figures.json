[{"figure_path": "cpklMJqZDE/figures/figures_8_1.jpg", "caption": "Figure 1: LDNet for Compressed Sensing. On the left, we plot the NMSE (in dB) obtained by LDNet and Bayes AMP baselines on the Bernoulli-Gaussian prior. On the right, we plot NMSE (not in dB) achieved on the Z2 prior. LDNet (along with the guided denoisers) achieves virtually identical performance to the conjectured computationally optimal Bayes AMP.", "description": "This figure compares the Normalized Mean Squared Error (NMSE) achieved by three different methods for compressed sensing: LDNet (Learned Denoising Network), guided denoisers, and Bayes AMP (Approximate Message Passing).  Two different priors are used: Bernoulli-Gaussian and Z2.  The left panel shows the NMSE in dB for the Bernoulli-Gaussian prior, while the right panel shows NMSE (not in dB) for the Z2 prior.  The results demonstrate that LDNet and guided denoisers achieve nearly identical performance to the theoretically optimal Bayes AMP, showcasing the effectiveness of the unrolled network architecture in learning the optimal denoising functions.", "section": "4.1 Compressed sensing"}, {"figure_path": "cpklMJqZDE/figures/figures_8_2.jpg", "caption": "Figure 2: Learned Denoisers for Compressed Sensing. We plot layerwise denoising functions learned by LDNet on the Bernoulli-Gaussian and Z2 priors relative to their optimal denoisers over a range of inputs in (\u22122, 2). The state evolution input \u03c4e to each denoiser is set to be its empirical estimate.", "description": "This figure compares the denoising functions learned by the LDNet model with the theoretically optimal denoisers for both Bernoulli-Gaussian and Z2 priors in a compressed sensing task.  It visualizes how well the learned denoisers approximate the optimal denoisers at different layers (iterations) of the unrolled network. The x-axis represents the input to the denoiser, and the y-axis represents the denoiser's output.  Different colors represent different layers of the network and the optimal denoiser.", "section": "4.1 Compressed sensing"}, {"figure_path": "cpklMJqZDE/figures/figures_22_1.jpg", "caption": "Figure 1: LDNet for Compressed Sensing. On the left, we plot the NMSE (in dB) obtained by LDNet and Bayes AMP baselines on the Bernoulli-Gaussian prior. On the right, we plot NMSE (not in dB) achieved on the Z2 prior. LDNet (along with the guided denoisers) achieves virtually identical performance to the conjectured computationally optimal Bayes AMP.", "description": "This figure compares the performance of LDNet and Bayes AMP on two different priors (Bernoulli-Gaussian and Z2) for compressed sensing.  The left panel shows NMSE in dB for the Bernoulli-Gaussian prior, while the right panel shows NMSE (not in dB) for the Z2 prior.  The results demonstrate that LDNet achieves nearly identical performance to Bayes AMP, which is considered computationally optimal.  The results also include a comparison with guided denoisers.", "section": "4.1 Compressed sensing"}, {"figure_path": "cpklMJqZDE/figures/figures_22_2.jpg", "caption": "Figure 2: Learned Denoisers for Compressed Sensing. We plot layerwise denoising functions learned by LDNet on the Bernoulli-Gaussian and Z2 priors relative to their optimal denoisers over a range of inputs in (\u22122, 2). The state evolution input \u03c4e to each denoiser is set to be its empirical estimate.", "description": "This figure compares the learned denoising functions from the LDNet model with the optimal denoising functions (from Bayes AMP) for different layers (0,3,6,14) and two different prior distributions (Bernoulli-Gaussian and Z2).  The x-axis represents the input to the denoiser, and the y-axis represents the output.  The plot shows that the LDNet successfully learns denoising functions that closely approximate the optimal functions, especially as the number of layers increases. The empirical estimate of the state evolution parameter, \u03c4e, is used for each denoiser.", "section": "4.1 Compressed sensing"}, {"figure_path": "cpklMJqZDE/figures/figures_23_1.jpg", "caption": "Figure 5: Learned B with Decreasing Dimension. We hold \u03b4 = \u00bd fixed while scaling m from 200 down to 100. Plots show NMSE (dB) performance of unrolling denoisers and learning B vs. Bayes AMP for randomly drawn measurement matrices. There is an increasing gap in performance as m decreases.", "description": "This figure shows the results of experiments where the dimension m of the measurement matrix A is varied while keeping the ratio of the dimension of the measurement matrix to the signal dimension constant. The NMSE (dB) is plotted as a function of the number of layers in the network.  The figure shows that the performance gap between the proposed learned B method and Bayes AMP increases as the dimension m decreases. This suggests that the improvement offered by learning the auxiliary parameter B becomes more pronounced in lower-dimensional settings.", "section": "4.2 Beyond Bayes AMP performance"}, {"figure_path": "cpklMJqZDE/figures/figures_23_2.jpg", "caption": "Figure 6: Non-Gaussian Measurements. On the left, we plot LDNet with learnable B compared to several baselines for a random truncated orthogonal measurement matrix, and on the right, for a random truncated Gram matrix. LDNet outperforms the other baselines in NMSE as well as convergence.", "description": "This figure compares the performance of LDNet (with a learnable parameter B) against several baselines (Bayes AMP, ISTA, and CoSaMP) for compressed sensing with non-Gaussian measurement matrices.  The two subfigures show results for two different types of matrices: a truncated random orthogonal matrix and a truncated random Gram matrix.  The results indicate that LDNet achieves lower normalized mean squared error (NMSE) and converges faster than the other methods in both scenarios.", "section": "4.2 Beyond Bayes AMP performance"}, {"figure_path": "cpklMJqZDE/figures/figures_24_1.jpg", "caption": "Figure 5: Learned B with Decreasing Dimension. We hold \u03b4 = \u00bd fixed while scaling m from 200 down to 100. Plots show NMSE (dB) performance of unrolling denoisers and learning B vs. Bayes AMP for randomly drawn measurement matrices. There is an increasing gap in performance as m decreases.", "description": "This figure shows the performance of the proposed method (LDNet with learnable matrix B) compared to Bayes AMP in low-dimensional settings.  Three different dimensions (m = 100, 150, 200) are shown, with the y-axis representing NMSE (dB) and x-axis representing the number of layers. As the dimension decreases, the performance gap between LDNet and Bayes AMP increases, indicating that LDNet with a learnable matrix B is advantageous in low-dimensional regimes where Bayes AMP's asymptotic optimality does not apply.", "section": "4.2 Beyond Bayes AMP performance"}, {"figure_path": "cpklMJqZDE/figures/figures_25_1.jpg", "caption": "Figure 7: Multi-Dimensional LDNet for Rank-One Matrix Estimation. On the left, we plot the NMSE obtained by LDNet and Bayes AMP on Z2, while the right plots are on the mixture of Gaussians. LDNet outperforms Bayes AMP by significant margins.", "description": "This figure compares the performance of the proposed LDNet and Bayes AMP on rank-one matrix estimation for two different non-product priors: Z2 and a mixture of Gaussians.  The plot shows the NMSE (normalized mean squared error) versus the number of layers in the network.  The results demonstrate that the LDNet significantly outperforms Bayes AMP in both cases, achieving lower NMSE with fewer layers.", "section": "D.2 Non-product priors"}, {"figure_path": "cpklMJqZDE/figures/figures_25_2.jpg", "caption": "Figure 8: Transfer Experiments. Above we plot the NMSE (in dB) over 15 iterations for different choices of measurement matrices coupled with our learned MLP denoisers, including the training-time sensing matrix. We see that the denoising functions are roughly transferable to several random Gaussian measurement settings, suggesting the learning process is not coupled to the fixed sensing matrix seen during training.", "description": "This figure shows the robustness of the learned denoisers to different sensing matrices. The NMSE (dB) is plotted against the number of layers for four different scenarios: the original sensing matrix used during training, and three other randomly generated Gaussian sensing matrices. The results demonstrate that the learned denoisers generalize well to new sensing matrices, indicating that the learning process is not highly dependent on the specific sensing matrix used during training.", "section": "4.1 Compressed sensing"}]