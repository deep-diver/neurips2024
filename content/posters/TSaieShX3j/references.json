{"references": [{"fullname_first_author": "M. Andriushchenko", "paper_title": "Sgd with large step sizes learns sparse features", "publication_date": "2022-10-01", "reason": "This paper is foundational to the understanding of the relationship between SGD's step size and its resulting sparsity, a key concept explored in the main paper."}, {"fullname_first_author": "S. Arora", "paper_title": "On the optimization of deep networks: Implicit acceleration by overparameterization", "publication_date": "2018-01-01", "reason": "This work provides a crucial theoretical foundation on the optimization of deep networks, which is directly relevant to the main paper's analysis of gradient methods."}, {"fullname_first_author": "S. Arora", "paper_title": "A convergence analysis of gradient descent for deep linear networks", "publication_date": "2019-01-01", "reason": "This paper offers a detailed convergence analysis of gradient descent specifically for deep linear networks, a model closely related to the one studied in the main paper."}, {"fullname_first_author": "N. S. Keskar", "paper_title": "On large-batch training for deep learning: Generalization gap and sharp minima", "publication_date": "2017-01-01", "reason": "This paper investigates the impact of batch size on generalization in deep learning, a topic directly relevant to the main paper's exploration of the effects of stochasticity on model simplicity."}, {"fullname_first_author": "S. Saxe", "paper_title": "Exact solutions to the nonlinear dynamics of learning in deep linear networks", "publication_date": "2014-01-01", "reason": "This paper provides exact solutions for the dynamics of learning in deep linear networks, offering a crucial benchmark for comparison with the results obtained using stochastic gradient descent in the main paper."}]}