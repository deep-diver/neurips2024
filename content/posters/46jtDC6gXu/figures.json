[{"figure_path": "46jtDC6gXu/figures/figures_0_1.jpg", "caption": "Figure 1: We introduce a new distributed acceleration paradigm that attains a 2.8x speed-up on Stable Diffusion XL while maintaining pixel-level consistency, using four NVIDIA A5000 GPUs.", "description": "This figure shows five example images generated using Stable Diffusion XL.  The top row shows the images generated using the standard method. The bottom row shows the images generated using the AsyncDiff method which achieves a 2.8x speedup with the use of four NVIDIA A5000 GPUs while maintaining the visual quality.", "section": "Abstract"}, {"figure_path": "46jtDC6gXu/figures/figures_1_1.jpg", "caption": "Figure 2: By preparing each component's input beforehand, we enable parallel computation of the denoising model, which substantially reduces latency while minimally affecting quality.", "description": "This figure illustrates the core concept of AsyncDiff, which is a method for parallelizing the denoising process in diffusion models.  The top part shows the traditional sequential denoising process where each component of the model is processed sequentially on a single GPU. The bottom part shows the AsyncDiff approach where the model is divided into multiple components and each component is processed in parallel on a different GPU. Communication between the GPUs is minimized by preparing each component's input in advance. This parallel approach significantly reduces latency (the time it takes to generate an image) while maintaining good image quality.", "section": "1 Introduction"}, {"figure_path": "46jtDC6gXu/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of the asynchronous denoising process. The denoising model e\u03b8 is divided into four components {e\u1d62}\u1d62=1 for clarity. Following the warm-up stage, each component's input is prepared in advance, breaking the dependency chain and facilitating parallel processing.", "description": "This figure illustrates the asynchronous denoising process used in AsyncDiff. The denoising model is divided into components, each assigned to a different GPU.  After a brief warm-up phase of sequential processing, the components process inputs asynchronously, using the output of the previous component at a slightly earlier time step as its input, rather than waiting for the completion of the immediately preceding step.  This breaks the sequential dependency and allows parallel computation.", "section": "3.2 Asynchronous Diffusion Model"}, {"figure_path": "46jtDC6gXu/figures/figures_4_1.jpg", "caption": "Figure 4: Illustration of stride denoising. The model e\u0473 is divided into three components {n}=1, with a stride S of 2 for clarity. Components e and e are skipped at time step t. A single parallel batch results in the completion of denoising for two steps, producing xt\u22121 and xt\u22122.", "description": "This figure illustrates how the AsyncDiff method uses stride denoising to further enhance efficiency.  Instead of processing one denoising step at a time, stride denoising groups multiple steps together for parallel processing.  This diagram uses a stride of 2, meaning that for each parallel processing batch, only components e and e need to be processed, skipping components e and e. This reduces communication overhead and accelerates the completion of multiple denoising steps.", "section": "3.2 Asynchronous Diffusion Model"}, {"figure_path": "46jtDC6gXu/figures/figures_5_1.jpg", "caption": "Figure 5: Qualitative Results. (a) Our method significantly accelerates the denoising process with minimal impact on generative quality. (b) Increasing warm-up steps achieves pixel-level consistency with the original output while maintaining a high speed-up ratio.", "description": "This figure presents qualitative results to showcase the impact of AsyncDiff on image generation quality and speed.  (a) demonstrates the acceleration achieved with different configurations (number of devices and stride). (b) shows the effect of adjusting the warm-up steps on the consistency between AsyncDiff's output and the original Stable Diffusion output.  The results show that AsyncDiff significantly speeds up inference while maintaining high image quality. Increasing the warm-up steps further improves the consistency of the generated images.", "section": "4 Experiments"}, {"figure_path": "46jtDC6gXu/figures/figures_7_1.jpg", "caption": "Figure 6: Qualitative Comparison with Distrifusion on SD2.1. At the same acceleration ratio, AsyncDiff outperforms in generating higher quality and more consistent images with the original.", "description": "This figure compares the image generation quality of AsyncDiff and Distrifusion on the Stable Diffusion 2.1 model.  Both methods achieve similar acceleration ratios (1.6x, 2.3x, and 2.7x). However, AsyncDiff consistently produces higher-quality images that are visually closer to the original images compared to Distrifusion, especially at higher acceleration ratios.", "section": "4 Experiments"}, {"figure_path": "46jtDC6gXu/figures/figures_15_1.jpg", "caption": "Figure 3: Overview of the asynchronous denoising process. The denoising model e\u03b8 is divided into four components {e0}n=1 for clarity. Following the warm-up stage, each component's input is prepared in advance, breaking the dependency chain and facilitating parallel processing.", "description": "This figure illustrates the asynchronous denoising process used in AsyncDiff. The denoising model is split into four components (in this example), each assigned to a different GPU.  A warm-up phase uses sequential processing for the initial steps to establish a baseline. After the warm-up, the components process different time steps concurrently, breaking the sequential dependency and achieving parallel computation.  The dashed lines show communication between GPUs.", "section": "3 Methods"}, {"figure_path": "46jtDC6gXu/figures/figures_18_1.jpg", "caption": "Figure 8: Qualitative results on SD 2.1 and SDXL with different configurations. Our method maintains excellent generation quality even when achieving speedups of up to four times.", "description": "This figure demonstrates the qualitative results of the AsyncDiff method on two different Stable Diffusion models (SD 2.1 and SDXL) under various configurations.  It visually compares the image generation quality of the original models with the outputs of AsyncDiff using different numbers of GPUs (2, 3, and 4) and different stride values (S=1 and S=2). The images show that even with significant speedups (up to 4x), AsyncDiff preserves the quality of the generated images, maintaining pixel-level consistency in many cases.", "section": "4 Experiments"}, {"figure_path": "46jtDC6gXu/figures/figures_19_1.jpg", "caption": "Figure 9: Qualitative results on AnimateDiff (1)", "description": "This figure shows the qualitative results of applying AsyncDiff to the AnimateDiff model with a prompt of \"Brilliant fireworks on the town, Van Gogh style, digital artwork, illustrative, painterly, matte painting, highly detailed, cinematic\".  It compares the original generation (43.5 seconds) with the results using AsyncDiff on 2 devices (23.5 seconds) and 4 devices (11.5 seconds). The images showcase the visual quality and consistency across different acceleration levels.", "section": "4 Experiments"}, {"figure_path": "46jtDC6gXu/figures/figures_19_2.jpg", "caption": "Figure 10: Qualitative results on AnimateDiff (2)", "description": "This figure shows the qualitative results of applying AsyncDiff to the AnimateDiff model with the prompt \"panda playing a guitar, on a boat, in the blue ocean, high quality\".  It presents the original images generated by AnimateDiff, and compares them to results obtained using AsyncDiff with 2 and 4 devices. The comparison is to illustrate that the AsyncDiff method can significantly reduce inference latency (43.5s down to 23.5s with 2 devices and 11.5s with 4 devices) while preserving the image quality.", "section": "4 Experiments"}, {"figure_path": "46jtDC6gXu/figures/figures_19_3.jpg", "caption": "Figure 3: Overview of the asynchronous denoising process. The denoising model \u03b8 is divided into four components {\u03b8i}i=1 for clarity. Following the warm-up stage, each component's input is prepared in advance, breaking the dependency chain and facilitating parallel processing.", "description": "This figure illustrates the asynchronous denoising process of the AsyncDiff model. The denoising model is divided into multiple components, each assigned to a different GPU. The initial 'warm-up' steps are processed sequentially. Then, the dependencies between components are broken by utilizing the similarity between hidden states in consecutive diffusion steps, allowing components to compute in parallel. This figure clearly shows how the asynchronous denoising process enables parallel execution, resulting in a significant reduction of inference latency.", "section": "3 Methods"}, {"figure_path": "46jtDC6gXu/figures/figures_20_1.jpg", "caption": "Figure 12: Qualitative results on Stable Video Diffusion.", "description": "This figure shows a qualitative comparison of video generation results using Stable Video Diffusion with different numbers of GPUs. The original video took 184 seconds to generate. Using 2 GPUs, the generation time was reduced to 101 seconds; with 3 GPUs, it took 80 seconds; and with 4 GPUs, it took 64 seconds. The figure visually demonstrates the effectiveness of the proposed AsyncDiff method in accelerating video generation, showing that as the number of GPUs increases, the generation time decreases while the visual quality of the generated videos remains largely consistent.", "section": "4 Experiments"}]