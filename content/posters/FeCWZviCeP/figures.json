[{"figure_path": "FeCWZviCeP/figures/figures_2_1.jpg", "caption": "Figure 2: Hierarchical Programmatic Option Framework. (a): Retrieving programmatic options. After learning the program embedding space, we propose an advanced search scheme built upon the Cross-Entropy Method (CEM) to search programs pm1,..., Pmk, pmk+1 of different skills. While searching for the next program pmk+1, we consider its compatibility with predetermined programs pm1, \u2026, Pmk by randomly sampling a sequence of programs. We also consider the diversity among all programs using the diversity multiplier. (b): Learning High-Level Policy with Programmatic Options.", "description": "This figure illustrates the proposed HIPO framework, showing two stages: (a) the program search algorithm that retrieves a set of diverse and compatible programs as options and (b) the high-level policy learning process using the retrieved programs to solve long-horizon tasks by switching between these options.", "section": "4 Approach"}, {"figure_path": "FeCWZviCeP/figures/figures_3_1.jpg", "caption": "Figure 2: Hierarchical Programmatic Option Framework. (a): Retrieving programmatic options. After learning the program embedding space, we propose an advanced search scheme built upon the Cross-Entropy Method (CEM) to search programs pm1,..., Pmk, pmk+1 of different skills. While searching for the next program pmk+1, we consider its compatibility with predetermined programs pm1, \u2026, Pmk by randomly sampling a sequence of programs. We also consider the diversity among all programs using the diversity multiplier. (b): Learning the high-level policy. Given the current environment state s and the current programmatic option mi, the high-level policy outputs a probability distribution over all programmatic options, aiming to maximize the total accumulative reward from the environment.", "description": "This figure illustrates the proposed Hierarchical Programmatic Option framework (HIPO).  Panel (a) shows the process of retrieving diverse and compatible programmatic options using a search algorithm based on the Cross-Entropy Method (CEM). The algorithm considers both the effectiveness of individual programs and their compatibility with previously selected programs to ensure a diverse set of skills. The diversity multiplier further enhances this process by discouraging the selection of similar programs. Panel (b) demonstrates how a high-level policy is used to select and execute the appropriate option based on the current state and the previously selected option.  This allows the agent to reuse learned skills and efficiently solve long and repetitive tasks.", "section": "4 Approach"}, {"figure_path": "FeCWZviCeP/figures/figures_5_1.jpg", "caption": "Figure 3: KAREL-LONG problem set: This work introduces a new set of tasks in the Karel domain. These tasks necessitate learning diverse, repetitive, and task-specific skills. For example, in our designed INF-HARVESTER, the agent needs to traverse the whole map and pick nearly 400 markers to solve the tasks since the environment randomly generates markers; in contrast, the HARVESTER from the KAREL problem set [74] can be solved by picking merely 36 markers.", "description": "This figure shows five new complex tasks for the Karel domain, designed to test the capabilities of the proposed HIPO framework for handling long and repetitive tasks that require diverse and reusable skills.  Each task involves a longer time horizon than previously studied tasks and requires more sophisticated planning and learning strategies to solve. The complexities are emphasized in the caption, highlighting the significant differences between these new challenges and previous benchmarks.", "section": "5.1 Karel problem sets"}, {"figure_path": "FeCWZviCeP/figures/figures_7_1.jpg", "caption": "Figure 4: (a) Program sample efficiency. The training curves of HIPO and other programmatic RL approaches, where the x-axis is the total number of executed programs for interacting with the environment, and the y-axis is the maximum validation return. This demonstrates that our proposed framework has better program sample efficiency and converges to better performance. (b) Inductive generalization performance. We evaluate and report the performance drop in the testing environments with an extended horizon, where the x-axis is the extended horizon length compared to the horizon of the training environments, and the y-axis is the performance drop in percentage. Our proposed framework can inductively generalize to longer horizons without any fine-tuning.", "description": "This figure shows two sub-figures. (a) compares the program sample efficiency of HIPO against other approaches by plotting the maximum validation return against the total number of executed programs.  (b) evaluates the inductive generalization ability of HIPO by comparing its performance drop in testing environments with extended horizons against other methods.", "section": "5. Experiments"}, {"figure_path": "FeCWZviCeP/figures/figures_16_1.jpg", "caption": "Figure 2: Hierarchical Programmatic Option Framework. (a): Retrieving programmatic options. After learning the program embedding space, we propose an advanced search scheme built upon the Cross-Entropy Method (CEM) to search programs pm1,..., Pmk, pmk+1 of different skills. While searching for the next program pmk+1, we consider its compatibility with predetermined programs pm1, \u2026, Pmk by randomly sampling a sequence of programs. We also consider the diversity among all programs using the diversity multiplier. (b): Learning High-Level Policy with Programmatic Options", "description": "This figure illustrates the HIPO framework, which consists of two main stages: retrieving programmatic options and learning a high-level policy.  The first stage uses a program embedding space and an advanced search method (CEM) to identify a set of effective, diverse, and compatible programs (options) with various skills.  The compatibility of the selected programs is checked by evaluating random sequences of them.  The diversity among selected options is considered to allow high-level versatility. The second stage uses the selected programs as low-level policies, and a neural network trains a high-level policy to choose the best option based on the current environment state and the current option. The final goal is to maximize the total reward obtained from the environment.", "section": "4 Approach"}, {"figure_path": "FeCWZviCeP/figures/figures_17_1.jpg", "caption": "Figure 6: CEM+diversity searching trajectories. A demonstration of 3 searching trajectories of the CEM+diversity procedure in the latent space. The CEM-acquired program embeddings are reduced into 2-dimensional representation with PCA. Given the diversity factor, the 2nd CEM-search exploration is ushered in the opposite direction of the searching trajectory of the 1st CEM-search, and the 3rd CEM-search trajectory is perpendicular to the 1st and 2nd searching paths.", "description": "This figure shows three different searching trajectories obtained from the CEM+diversity algorithm in a 2D latent space (reduced from a higher dimensional space using PCA). The algorithm's diversity factor influences the search direction.  The first search follows a certain path.  Subsequent searches, due to the diversity factor, explore directions that are opposite or perpendicular to previous searches, thus covering a wider area of the search space to ensure diversity in retrieved programs.", "section": "A.2 CEM+diversity"}, {"figure_path": "FeCWZviCeP/figures/figures_19_1.jpg", "caption": "Figure 7: Program sample efficiency. Results of different programmatic RL approaches in FARMER, INF-DOORKEY, INF-HARVESTER.", "description": "This figure shows the program sample efficiency of different programmatic reinforcement learning approaches, namely HIPO, LEAPS, HPRL, and HC, across three KAREL-LONG tasks: FARMER, INF-DOORKEY, and INF-HARVESTER. Program sample efficiency measures the total number of program executions required to achieve a certain level of performance. The x-axis represents the number of executed programs, and the y-axis shows the maximum validation return. The figure illustrates that HIPO achieves better program sample efficiency compared to other methods, indicating that it converges to optimal performance with fewer program executions.", "section": "B Program sample efficiency"}, {"figure_path": "FeCWZviCeP/figures/figures_19_2.jpg", "caption": "Figure 4: (a) Program sample efficiency. The training curves of HIPO and other programmatic RL approaches, where the x-axis is the total number of executed programs for interacting with the environment, and the y-axis is the maximum validation return. This demonstrates that our proposed framework has better program sample efficiency and converges to better performance. (b) Inductive generalization performance. We evaluate and report the performance drop in the testing environments with an extended horizon, where the x-axis is the extended horizon length compared to the horizon of the training environments, and the y-axis is the performance drop in percentage. Our proposed framework can inductively generalize to longer horizons without any fine-tuning.", "description": "This figure shows two graphs. Graph (a) compares the program sample efficiency of different reinforcement learning algorithms and demonstrates that the proposed HIPO framework has better program sample efficiency and converges faster. Graph (b) shows the inductive generalization performance of the algorithms by testing them in environments with extended horizons. The results show that HIPO maintains good performance in these extended horizon environments, demonstrating its inductive generalization capability.", "section": "5. Experiments"}, {"figure_path": "FeCWZviCeP/figures/figures_20_1.jpg", "caption": "Figure 4: (a) Program sample efficiency. The training curves of HIPO and other programmatic RL approaches, where the x-axis is the total number of executed programs for interacting with the environment, and the y-axis is the maximum validation return. This demonstrates that our proposed framework has better program sample efficiency and converges to better performance. (b) Inductive generalization performance. We evaluate and report the performance drop in the testing environments with an extended horizon, where the x-axis is the extended horizon length compared to the horizon of the training environments, and the y-axis is the performance drop in percentage. Our proposed framework can inductively generalize to longer horizons without any fine-tuning.", "description": "This figure shows two sub-figures. Sub-figure (a) compares the program sample efficiency of HIPO against other programmatic RL methods by plotting the maximum validation return achieved against the total number of executed programs. It shows that HIPO achieves better sample efficiency and faster convergence. Sub-figure (b) evaluates the inductive generalization capabilities of HIPO by comparing its performance drop in extended horizon testing environments against baselines. It shows that HIPO maintains its performance better than other methods as the horizon extends.", "section": "5. Experiments"}, {"figure_path": "FeCWZviCeP/figures/figures_21_1.jpg", "caption": "Figure 2: Hierarchical Programmatic Option Framework. (a): Retrieving programmatic options. After learning the program embedding space, we propose an advanced search scheme built upon the Cross-Entropy Method (CEM) to search programs pm1,..., Pmk, pmk+1 of different skills. While searching for the next program pmk+1, we consider its compatibility with predetermined programs pm1, \u2026, Pmk by randomly sampling a sequence of programs. We also consider the diversity among all programs using the diversity multiplier. (b): Learning High-Level Policy with Programmatic Options", "description": "This figure illustrates the HIPO framework, which consists of two main stages: (a) retrieving diverse and compatible programmatic options and (b) learning a high-level policy using these options.  Stage (a) uses a modified CEM algorithm to find programs with diverse skills, considering compatibility with already selected programs and overall diversity. Stage (b) shows how the high-level policy determines which option to use based on the current state and the current option, to ultimately maximize rewards. ", "section": "4 Approach"}, {"figure_path": "FeCWZviCeP/figures/figures_22_1.jpg", "caption": "Figure 10: Example of extracted state machine on FARMER. 01 to 031 represent the unique quantized vectors encoded from observations. The corresponding programs of M1 to M5 are displayed in Figure 23.", "description": "This figure shows a state machine extracted from the high-level policy of the HIPO framework when applied to the FARMER task.  Each node represents a state, and each edge represents a transition between states, labeled with the quantized vector observed from the environment that triggered the transition.  The states are further connected to low-level programmatic options (M1-M5) which are human-readable programs that execute actions within the environment.  More detail about the specific programs is found in Figure 23.  The state machine visualization enhances interpretability, showcasing the high-level policy's decision-making process in the task.", "section": "D State machine extraction"}, {"figure_path": "FeCWZviCeP/figures/figures_22_2.jpg", "caption": "Figure 10: Example of extracted state machine on FARMER. 01 to 031 represent the unique quantized vectors encoded from observations. The corresponding programs of M1 to M5 are displayed in Figure 23.", "description": "This figure shows a state machine extracted from the high-level policy for the FARMER task. Each node represents a state, and each edge represents a transition between states, labeled with the programmatic option selected by the high-level policy. The figure illustrates the flow of the high-level policy's decisions. The corresponding programs for options M1-M5 are detailed in Figure 23.", "section": "D State machine extraction"}, {"figure_path": "FeCWZviCeP/figures/figures_23_1.jpg", "caption": "Figure 2: Hierarchical Programmatic Option Framework. (a): Retrieving programmatic options. After learning the program embedding space, we propose an advanced search scheme built upon the Cross-Entropy Method (CEM) to search programs pm1,..., Pmk, pmk+1 of different skills. While searching for the next program pmk+1, we consider its compatibility with predetermined programs pm1, \u2026, Pmk by randomly sampling a sequence of programs. We also consider the diversity among all programs using the diversity multiplier. (b): Learning High-Level Policy with Programmatic Options.", "description": "This figure illustrates the HIPO framework, which consists of two main stages: retrieving programmatic options and learning a high-level policy.  The first stage (a) uses a search algorithm based on the Cross-Entropy Method (CEM) to find a diverse set of programs (options) with different skills. The algorithm considers program compatibility and diversity. The second stage (b) trains a high-level neural network policy to select the appropriate option based on the current state and the last selected option, aiming to maximize the overall reward.", "section": "4 Approach"}, {"figure_path": "FeCWZviCeP/figures/figures_27_1.jpg", "caption": "Figure 14: Visualization of STAIRCLIMBER, FOURCORNER, TOPOFF, and MAZE in the KAREL problem set presented in Trivedi et al. [74]. For each task, a random initial state, a legitimate internal state, and the ideal end state are shown. In most tasks, the position of markers and the initial location of the Karel agent are randomized. More details of the KAREL problem set can be found in Section F.", "description": "This figure shows four example tasks from the KAREL problem set used in the paper.  Each task (STAIRCLIMBER, FOURCORNER, TOPOFF, MAZE) is illustrated with three snapshots: a random initial state, an intermediate state, and the goal state. The initial position of the Karel agent and marker locations are randomized for each instance. The details of the problem set are described in Section F of the paper.  The figure visually demonstrates the different types of tasks considered, each with varying complexity and goal configurations.", "section": "Details of KAREL problem set"}, {"figure_path": "FeCWZviCeP/figures/figures_28_1.jpg", "caption": "Figure 15: Visualization of CLEANHOUSE and HARVESTER in the KAREL problem set presented in Trivedi et al. [74]. For each task, a random initial state, a legitimate internal state, and the ideal end state are shown. More details of the KAREL problem set can be found in Section F.", "description": "This figure shows two examples from the KAREL problem set used as a benchmark in the paper.  CLEANHOUSE involves collecting scattered markers in a grid world.  HARVESTER involves collecting markers from a grid where all cells are initially populated with markers. The figure shows the initial state, an intermediate state, and the goal state for each of the tasks. These are randomly generated states, to avoid bias in the results.", "section": "Details of KAREL problem set"}, {"figure_path": "FeCWZviCeP/figures/figures_29_1.jpg", "caption": "Figure 16: Visualization of SEESAW in the KAREL-LONG problem set. This figure partially illustrates a typical trajectory of the Karel agent during the task SEESAW. (a): Once the Karel agent collects a marker in the left chamber, a new marker appears in the right chamber. (b): The agent must navigate through the central corridor to collect the marker in the right chamber. (c): Once the Karel agent collects a marker in the right chamber, a new marker further appears in the left chamber. (d): Once again, the agent is traversing through the corridor to the left chamber. (e): A new marker appears in the right chamber again after the agent picks up the marker in the left chamber. (f): The agent will move back and forth between the two chambers to collect the emerging markers continuously. Note that the locations of all the emerging markers are randomized. Also, note that we have set the number of emerging markers to 64 during the training phase (i.e., the agent has to pick up 64 markers to fully complete the task.) More details of the task SEESAW can be found in Section H.", "description": "This figure shows a partial trajectory of the Karel agent in the SEESAW task.  The agent moves between two chambers, collecting markers. Each time it collects a marker, a new one appears in the opposite chamber. This demonstrates the repetitive nature of the task, and how the agent's actions (movement and marker collection) repeat in a cycle.", "section": "H Details of KAREL-LONG problem set"}, {"figure_path": "FeCWZviCeP/figures/figures_33_1.jpg", "caption": "Figure 17: Visualization of UP-N-DOWN in the KAREL-LONG problem set. This figure partially illustrates a typical trajectory of the Karel agent during the task UP-N-DOWN. (a): The Karel agent is ascending the stairs to collect a load located above the stairs. Note that the agent can theoretically collect the load without directly climbing up the stairs, but it will receive some penalties for doing so. (b): Once the agent collects the load, a new load appears below the stairs. (c): The agent then descends the stairs to collect a load located below. Note that the agent can theoretically collect the load without directly climbing down the stairs, but it will receive some penalties for doing so. (d): Upon the agent collecting the load, a new load appears above the stairs. (e): The agent once again ascends the stairs to collect a load. (f): A new load appears below the stairs again after the agent collects the load located above. (g): The agent would continue to collect the emerging loads in descend-ascend cycles repeatedly on the stairs. Note that the locations of all the emerging loads are randomly initiated right next to the stairs. The load must appears below/above the stairs after the agent just finished ascending/descending. Also, we have fixed the number of emerging loads to 100 during the training phase (i.e., the agent shall collect 100 loads to complete the task). More details of the task UP-N-DOWN can be found in Section H.", "description": "This figure illustrates a typical trajectory of the Karel agent in the UP-N-DOWN task.  The agent repeatedly ascends and descends stairs to collect loads (markers).  A new load appears above/below the stairs after the agent collects one, continuing the cycle.  The agent is penalized for collecting loads without directly climbing the stairs.", "section": "H Details of KAREL-LONG problem set"}, {"figure_path": "FeCWZviCeP/figures/figures_34_1.jpg", "caption": "Figure 20: Visualization of INF-HARVESTER in the KAREL-LONG problem set. This figure partially illustrates a legitimate trajectory of the Karel agent during the task INF-HARVESTER. (a): The Karel agent picks up markers in the last row. Meanwhile, no new markers are popped out in the last row. (b): The agent turns left and picks up 6 markers in the 7th column while 3 markers appear in 3 previously empty grids in the last row. (c): The agent collects markers in the 8th row while 1 marker appears in a previously empty grid in the 7th column. (d): The agent picks up 6 markers in the 5th column while 2 markers appear in 2 previously empty grids in the 7th column. (e): The agent picks up 2 more markers in the last row while 2 markers appeared in 2 previously empty grids in the 5th column. (f): Since markers appear in previously empty grids based on the emerging probability, the agent will continuously and indefinitely collect markers until none remain and no new markers appear in the environment. The emerging probability has been fixed to 1 during the training phase. More details of the task INF-HARVESTER can be found in Section H.", "description": "This figure shows a sample trajectory of an agent executing the INF-HARVESTER task.  The agent repeatedly traverses the grid, picking markers. As it does, new markers randomly appear in empty locations. The cycle continues until no markers remain. The figure highlights the dynamic and repetitive nature of the task.", "section": "H Details of KAREL-LONG problem set"}, {"figure_path": "FeCWZviCeP/figures/figures_35_1.jpg", "caption": "Figure 20: Visualization of INF-HARVESTER in the KAREL-LONG problem set. This figure partially illustrates a legitimate trajectory of the Karel agent during the task INF-HARVESTER. (a): The Karel agent picks up markers in the last row. Meanwhile, no new markers are popped out in the last row. (b): The agent turns left and picks up 6 markers in the 7th column while 3 markers appear in 3 previously empty grids in the last row. (c): The agent collects markers in the 8th row while 1 marker appears in a previously empty grid in the 7th column. (d): The agent picks up 6 markers in the 5th column while 2 markers appear in 2 previously empty grids in the 7th column. (e): The agent picks up 2 more markers in the last row while 2 markers appeared in 2 previously empty grids in the 5th column. (f): Since markers appear in previously empty grids based on the emerging probability, the agent will continuously and indefinitely collect markers until none remain and no new markers appear in the environment. The emerging probability has been fixed to during the training phase. More details of the task INF-HARVESTER can be found in Section H.", "description": "This figure shows a partial trajectory of an agent performing the INF-HARVESTER task in the Karel-Long problem set.  The agent repeatedly collects markers, and with a certain probability, new markers appear in empty cells. The figure illustrates how the agent's actions and the environment's changes cause a progression through the task.", "section": "5.2 Cross-entropy method with diversity multiplier"}, {"figure_path": "FeCWZviCeP/figures/figures_36_1.jpg", "caption": "Figure 16: Visualization of SEESAW in the KAREL-LONG problem set. This figure partially illustrates a typical trajectory of the Karel agent during the task SEESAW. (a): Once the Karel agent collects a marker in the left chamber, a new marker appears in the right chamber. (b): The agent must navigate through the central corridor to collect the marker in the right chamber. (c): Once the Karel agent collects a marker in the right chamber, a new marker further appears in the left chamber. (d): Once again, the agent is traversing through the corridor to the left chamber. (e): A new marker appears in the right chamber again after the agent picks up the marker in the left chamber. (f): The agent will move back and forth between the two chambers to collect the emerging markers continuously. Note that the locations of all the emerging markers are randomized. Also, note that we have set the number of emerging markers to 64 during the training phase (i.e., the agent has to pick up 64 markers to fully complete the task.) More details of the task SEESAW can be found in Section H.", "description": "This figure shows a partial trajectory of the Karel agent in the SEESAW task.  It demonstrates the back-and-forth movement between two chambers, collecting markers.  A new marker appears in the opposite chamber after one is collected, creating a continuous cycle.  Marker locations are randomized, and the training phase involves collecting 64 markers.", "section": "H Details of KAREL-LONG problem set"}, {"figure_path": "FeCWZviCeP/figures/figures_37_1.jpg", "caption": "Figure 21: Visualization of each task in the KAREL-HARD problem set proposed by Liu et al. [46]. For each task, a random initial state, some legitimate internal state(s), and the ideal end state are shown. More details of the KAREL-HARD problem set can be found in Section G.", "description": "This figure shows the initial state, intermediate states, and goal state for each of the five tasks in the KAREL-HARD problem set.  Each task presents unique challenges and complexities in terms of navigation, marker placement, and goal achievement. The figure serves to visually illustrate the range of scenarios encompassed within this problem set.", "section": "G Details of KAREL-HARD problem set"}, {"figure_path": "FeCWZviCeP/figures/figures_40_1.jpg", "caption": "Figure 2: Hierarchical Programmatic Option Framework. (a): Retrieving programmatic options. After learning the program embedding space, we propose an advanced search scheme built upon the Cross-Entropy Method (CEM) to search programs pm1,..., Pmk, pmk+1 of different skills. While searching for the next program pmk+1, we consider its compatibility with predetermined programs pm1, \u2026, Pmk by randomly sampling a sequence of programs. We also consider the diversity among all programs using the diversity multiplier. (b): Learning High-Level Policy with Programmatic Options", "description": "This figure illustrates the proposed Hierarchical Programmatic Option framework (HIPO). The left panel (a) shows the process of retrieving a diverse set of effective and reusable programmatic options from a program embedding space using an advanced search algorithm based on the Cross-Entropy Method (CEM) that considers compatibility and diversity. The right panel (b) shows how a high-level policy, represented by neural networks, learns to select and execute these programmatic options to solve long-horizon RL tasks.", "section": "4 Approach"}]