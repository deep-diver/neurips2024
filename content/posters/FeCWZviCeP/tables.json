[{"figure_path": "FeCWZviCeP/tables/tables_6_1.jpg", "caption": "Table 1: Evaluation on KAREL and KAREL-HARD tasks. Mean return and standard deviation of all methods across the KAREL and KAREL-HARD problem set, evaluated over five random seeds. CEM+diversity outperforms CEM with significantly smaller standard deviations across 8 out of 10 tasks, highlighting the effectiveness and stability of CEM+diversity. In addition, HIPO outperforms LEAPS and HPRL on 8 out of 10 tasks.", "description": "This table presents the results of evaluating different reinforcement learning methods on the KAREL and KAREL-HARD tasks.  The metrics used are mean return and standard deviation, calculated across five random seeds for each task.  The comparison highlights the superior performance and stability of the CEM+diversity method compared to the standard CEM method.  Furthermore, it demonstrates the outperformance of the proposed HIPO framework compared to LEAPS and HPRL on most of the tasks. ", "section": "5.2 Cross-entropy method with diversity multiplier"}, {"figure_path": "FeCWZviCeP/tables/tables_7_1.jpg", "caption": "Table 2: KAREL-LONG performance. Mean return and standard deviation of all methods across the KAREL-LONG problem set, evaluated over five random seeds. Our proposed framework achieves the best mean reward across most of the tasks by learning a high-level policy with a set of effective, diverse, and compatible programs.", "description": "This table presents the performance results of various methods on the KAREL-LONG tasks. The mean return and standard deviation are calculated across five random seeds. It shows that HIPO, the proposed framework, outperforms other methods by learning a high-level policy that utilizes a set of effective, diverse, and compatible programs.", "section": "5.4 Comparing with deep RL and programmatic RL Methods"}, {"figure_path": "FeCWZviCeP/tables/tables_31_1.jpg", "caption": "Table 2: KAREL-LONG performance. Mean return and standard deviation of all methods across the KAREL-LONG problem set, evaluated over five random seeds. Our proposed framework achieves the best mean reward across most of the tasks by learning a high-level policy with a set of effective, diverse, and compatible programs.", "description": "This table presents the results of the KAREL-LONG experiments, comparing the performance of different methods on five different tasks that require long sequences of actions.  The table shows that the proposed HIPO framework outperforms other methods by learning a high-level policy that uses a set of diverse and compatible programs as options, enabling it to achieve better average returns and lower variance (more consistent performance) across the tasks.", "section": "5.4 Comparing with deep RL and programmatic RL Methods"}, {"figure_path": "FeCWZviCeP/tables/tables_38_1.jpg", "caption": "Table 1: Evaluation on KAREL and KAREL-HARD tasks. Mean return and standard deviation of all methods across the KAREL and KAREL-HARD problem set, evaluated over five random seeds. CEM+diversity outperforms CEM with significantly smaller standard deviations across 8 out of 10 tasks, highlighting the effectiveness and stability of CEM+diversity. In addition, HIPO outperforms LEAPS and HPRL on 8 out of 10 tasks.", "description": "This table presents the performance comparison of different methods (CEM, CEM+diversity, DRL, LEAPS, HPRL, and HIPO) on the KAREL and KAREL-HARD tasks.  The mean return and standard deviation are calculated over five random seeds for each task.  The results show that CEM+diversity outperforms the standard CEM algorithm, demonstrating improved effectiveness and stability.  Furthermore, the HIPO method outperforms LEAPS and HPRL on most tasks, indicating its superiority in solving these types of problems.", "section": "5.2 Cross-entropy method with diversity multiplier"}, {"figure_path": "FeCWZviCeP/tables/tables_39_1.jpg", "caption": "Table 1: Evaluation on KAREL and KAREL-HARD tasks. Mean return and standard deviation of all methods across the KAREL and KAREL-HARD problem set, evaluated over five random seeds. CEM+diversity outperforms CEM with significantly smaller standard deviations across 8 out of 10 tasks, highlighting the effectiveness and stability of CEM+diversity. In addition, HIPO outperforms LEAPS and HPRL on 8 out of 10 tasks.", "description": "This table presents the results of evaluating different methods on the KAREL and KAREL-HARD tasks.  The mean return and standard deviation are shown for each method across the ten tasks.  The results demonstrate that CEM+diversity (a variation of the Cross Entropy Method) significantly improves upon the standard CEM, showing both higher average performance and much lower variance.  Furthermore, the Hierarchical Programmatic Option (HIPO) method achieves the best performance across 8 out of 10 tasks compared to LEAPS and HPRL.", "section": "5.2 Cross-entropy method with diversity multiplier"}, {"figure_path": "FeCWZviCeP/tables/tables_41_1.jpg", "caption": "Table 1: Evaluation on KAREL and KAREL-HARD tasks. Mean return and standard deviation of all methods across the KAREL and KAREL-HARD problem set, evaluated over five random seeds. CEM+diversity outperforms CEM with significantly smaller standard deviations across 8 out of 10 tasks, highlighting the effectiveness and stability of CEM+diversity. In addition, HIPO outperforms LEAPS and HPRL on 8 out of 10 tasks.", "description": "This table presents the results of the KAREL and KAREL-HARD tasks, comparing the performance of different methods, including CEM, CEM+diversity, DRL, LEAPS, HPRL, and HIPO.  The table shows the mean return and standard deviation for each method across ten tasks, averaged over five random seeds. The results demonstrate that CEM+diversity outperforms the standard CEM approach, and HIPO outperforms LEAPS and HPRL in most tasks.  This highlights the effectiveness and stability of the CEM+diversity method and the superiority of the proposed HIPO framework.", "section": "5.2 Cross-entropy method with diversity multiplier"}]