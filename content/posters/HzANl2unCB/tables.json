[{"figure_path": "HzANl2unCB/tables/tables_6_1.jpg", "caption": "Table 1: State-of-the-art comparisons on the datasets of TNL2K, LaSOT and TrackingNet. The best two results are shown in red and blue color. Our approach performs favorably against the state-of-the-art methods on all datasets. * indicates vision-language trackers. All metrics of performance are in % in tables unless otherwise specified.", "description": "This table presents a comparison of the ChatTracker model's performance against other state-of-the-art visual and vision-language trackers on three benchmark datasets: TNL2K, LaSOT, and TrackingNet.  The results are shown for several metrics (AUC, precision, and normalized precision) and highlight that ChatTracker achieves competitive or superior performance to existing methods.", "section": "4 Experiment"}, {"figure_path": "HzANl2unCB/tables/tables_7_1.jpg", "caption": "Table 1: State-of-the-art comparisons on the datasets of TNL2K, LaSOT and TrackingNet. The best two results are shown in red and blue color. Our approach performs favorably against the state-of-the-art methods on all datasets. * indicates vision-language trackers. All metrics of performance are in % in tables unless otherwise specified.", "description": "This table presents a comparison of the ChatTracker model's performance against other state-of-the-art visual and vision-language trackers on three benchmark datasets: TNL2K, LaSOT, and TrackingNet.  The results are evaluated using AUC, Precision (P), and Precision normalized (PNorm).  The table highlights ChatTracker's competitive performance, particularly in achieving the best or second-best results on several metrics across datasets. The asterisk (*) indicates that the corresponding method is a vision-language tracker.", "section": "4 Experiment"}, {"figure_path": "HzANl2unCB/tables/tables_7_2.jpg", "caption": "Table 3: The comparison of results for Vision-Language trackers using ChatTracker-generated text (marked by *). We report the AUC value on the datasets.", "description": "This table compares the Area Under the Curve (AUC) values for several vision-language trackers, both with manually annotated descriptions and with descriptions generated by the ChatTracker model.  The purpose is to show how the ChatTracker-generated descriptions improve the performance of existing trackers.", "section": "4.2 Comparison with Existing Trackers"}, {"figure_path": "HzANl2unCB/tables/tables_7_3.jpg", "caption": "Table 4: Text-to-image alignment scores for manually annotated and ChatTracker-generated language descriptions. ViT and RN refer to the use of ViT-B/32 and RN-50 as CLIP [23] image encoders, respectively.", "description": "This table presents a comparison of text-to-image alignment scores obtained using two different sources of text descriptions: manually annotated descriptions and descriptions generated by the ChatTracker model.  Two different image encoders (ViT-B/32 and RN-50) from the CLIP model were used to compute the alignment scores for each text source on three datasets: LaSOT, TNL2K, and OTB-lang. The scores indicate the degree of alignment between the textual descriptions and the corresponding images, reflecting the quality of the descriptions in terms of accurately capturing the visual features of the target.", "section": "4.4 Analysis on Language Descriptions Generated by ChatTracker"}, {"figure_path": "HzANl2unCB/tables/tables_13_1.jpg", "caption": "Table 7: The performance comparison of Foreground Verification module,random selection module in our framework on OTB-lang Dataset", "description": "This table presents the performance comparison of three different methods on the OTB-lang dataset.  The methods being compared are the complete ChatTracker-L (with the Foreground Verification module), a version where a foreground proposal is randomly selected instead of using the verification module, and finally an upper bound representing perfect selection of the proposal with the highest IoU with ground truth.  The results show that the Foreground Verification module significantly improves performance compared to random selection, demonstrating its effectiveness.", "section": "4.5 Ablation Study"}, {"figure_path": "HzANl2unCB/tables/tables_14_1.jpg", "caption": "Table 8: Results on the OTB-Lang dataset", "description": "This table presents a comparison of the performance of four different visual trackers on the OTB-Lang dataset.  The trackers evaluated are ChatTracker-L, ChatTracker-B, JointNLT, and ARTrack-256.  The metrics used to evaluate performance are AUC (Area Under the Curve), Precision (P), and Precision with normalized overlap (PNorm).  The table shows that ChatTracker-L achieves the best performance in terms of all three metrics.", "section": "4.2 Comparison with Existing Trackers"}, {"figure_path": "HzANl2unCB/tables/tables_14_2.jpg", "caption": "Table 1: State-of-the-art comparisons on the datasets of TNL2K, LaSOT and TrackingNet. The best two results are shown in red and blue color. Our approach performs favorably against the state-of-the-art methods on all datasets. * indicates vision-language trackers. All metrics of performance are in % in tables unless otherwise specified.", "description": "This table presents a comparison of the ChatTracker model's performance against other state-of-the-art visual and vision-language trackers on three benchmark datasets: TNL2K, LaSOT, and TrackingNet.  The results are shown in terms of AUC, Precision (P), and Precision normalized (PNorm).  The best two performing models for each metric on each dataset are highlighted in red and blue. The table demonstrates that ChatTracker outperforms existing models, especially in the AUC metric.", "section": "4.2 Comparison with Existing Trackers"}]