[{"figure_path": "TutGINeJzZ/figures/figures_8_1.jpg", "caption": "Figure 1: Convergence of mean squared error with balanced users.", "description": "This figure shows the convergence of the mean squared error for the Huber loss minimization approach (HLM) and the Winsorized Mean Estimator (WME) methods under different sample sizes (m) and numbers of users (n), for balanced users (i.e., all users have the same number of samples). The results are presented for four different data distributions: uniform, Gaussian, Lomax, and IPUMS datasets (total income and salary).  Different dimensionalities (d=1 and d=3) are considered, demonstrating how HLM performs better for heavy-tailed distributions (Lomax and IPUMS).", "section": "7 Numerical Examples"}, {"figure_path": "TutGINeJzZ/figures/figures_9_1.jpg", "caption": "Figure 1: Convergence of mean squared error with balanced users.", "description": "This figure displays the convergence of mean squared error for different sample sizes (m) and numbers of users (n = 1000, 10000) across four distributions: Uniform, Gaussian, Lomax, and real-world IPUMS data (total income and salary).  It compares the Huber loss minimization approach (HLM) to the Winsorized Mean Estimator (WME) method. The results demonstrate how each method's performance changes with varying sample sizes and distributions. For uniform and Gaussian distributions, both methods show similar performance, while for heavy-tailed distributions (Lomax), the Huber loss method shows significantly faster convergence.", "section": "7 Numerical Examples"}]