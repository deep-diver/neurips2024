{"references": [{"fullname_first_author": "Jiang, N.", "paper_title": "Doubly robust off-policy value evaluation for reinforcement learning", "publication_date": "2016-00-00", "reason": "This paper is foundational for doubly robust methods which are a key part of the OPERA algorithm."}, {"fullname_first_author": "Thomas, P.", "paper_title": "Data-efficient off-policy policy evaluation for reinforcement learning", "publication_date": "2016-00-00", "reason": "This paper introduced the MAGIC estimator, which is closely related to OPERA's approach of combining multiple estimators."}, {"fullname_first_author": "Nachum, O.", "paper_title": "Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections", "publication_date": "2019-00-00", "reason": "DualDICE is a prominent minimax OPE estimator, and understanding its properties is important for the OPERA algorithm which combines various OPE estimators."}, {"fullname_first_author": "Efron, B.", "paper_title": "Bootstrap methods: another look at the jackknife", "publication_date": "1992-00-00", "reason": "Bootstrapping is a core component of OPERA, used for estimating the mean squared error of the combined estimators, and this paper is the seminal work on bootstrapping."}, {"fullname_first_author": "Fu, J.", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-00-00", "reason": "The D4RL benchmark is used for evaluating OPERA's performance, making it a crucial reference for understanding the context and evaluation of the algorithm."}]}