[{"type": "text", "text": "Universal Exact Compression of Differentially Private Mechanisms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yanxiao Liu The Chinese University of Hong Kong yanxiaoliu@link.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Wei-Ning Chen Stanford University wnchen@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Ayfer \u00d6zg\u00fcr Stanford University aozgur@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Cheuk Ting Li The Chinese University of Hong Kong ctli@ie.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "To reduce the communication cost of differential privacy mechanisms, we introduce a novel construction, called Poisson private representation (PPR), designed to compress and simulate any local randomizer while ensuring local differential privacy. Unlike previous simulation-based local differential privacy mechanisms, PPR exactly preserves the joint distribution of the data and the output of the original local randomizer. Hence, the PPR-compressed privacy mechanism retains all desirable statistical properties of the original privacy mechanism such as unbiasedness and Gaussianity. Moreover, PPR achieves a compression size within a logarithmic gap from the theoretical lower bound. Using the PPR, we give a new order-wise trade-off between communication, accuracy, central and local differential privacy for distributed mean estimation. Experiment results on distributed mean estimation show that PPR consistently gives a better trade-off between communication, accuracy and central differential privacy compared to the coordinate subsampled Gaussian mechanism, while also providing local differential privacy. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In modern data science, there is a growing dependence on large amounts of high-quality data, often generated by edge devices (e.g., photos and videos captured by smartphones, or messages hosted by social networks). However, this data inherently contains personal information, making it susceptible to privacy breaches during acquisition, collection, or utilization. For instance, despite the significant recent advancement in foundational models [9], studies have shown that these models can accidentally memorize their training data. This poses a risk where malicious users, even with just API access, can extract substantial portions of sensitive information [14, 15]. In recent years, differential privacy (DP) [28] has emerged as a powerful framework for safeguarding users\u2019 privacy by ensuring that local data is properly randomized before leaving users\u2019 devices. Apart from privacy concerns, communicating local data from edge devices to the central server often becomes a bottleneck in the system pipeline, especially with high-dimensional data common in many machine learning scenarios. This leads to the following fundamental question: how can we efficiently communicate privatized data? ", "page_idx": 0}, {"type": "text", "text": "Recent works have shown that a wide range of differential privacy mechanisms can be \u201csimulated\u201d and \u201ccompressed\u201d using shared randomness, resulting in a \u201ccompressed mechanism\u201d which has a smaller communication cost compared to the original mechanism, while retaining the (perhaps slightly weakened) privacy guarantee. This can be done via rejection sampling [31], importance sampling [71, 78], or dithered quantization [56, 72, 46, 49, 80] with each approach having its own advantages and disadvantages. For example, importance-sampling-based methods [71, 78] and the rejection-sampling-based method [31] can simulate a wide range of privacy mechanisms; however, the output distribution of the induced mechanism does not perfectly match the original mechanism. This is limiting in scenarios where the original mechanism is designed to satisfy some desired statistical properties, e.g. it is often desirable for the local randomizer to be unbiased or to be \u201csummable\u201d noise such as Gaussian or other infinitely divisible distributions. Since the induced mechanism is different from the original one, these statistical properties are not preserved. On the other hand, ditheredquantization-based approaches [48, 56, 72, 46, 49, 80] can ensure a correct simulated distribution, but they can only simulate additive noise mechanisms. More importantly, dithered quantization relies on shared randomness between the user and the server, and the server needs to know the dither for decoding. This annuls the local privacy guarantee on the user data, unless we are willing to assume a trusted aggregator [46], use an additional secure aggregation step [49], or restrict attention to specific privacy mechanisms (e.g., one-dimensional Laplace [72]). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a novel \u201cDP mechanism compressor\u201d called Poisson private representation $(P P R)$ , designed to compress and exactly simulate any local randomizer while ensuring local DP, through the use of shared randomness.1 We elaborate on three main advantages of PPR, namely universality, exactness and communication efficiency. ", "page_idx": 1}, {"type": "text", "text": "Universality. Unlike dithered-quantization-based approaches which can only simulate additive noise mechanisms, PPR can simulate any local or central DP mechanism with discrete or continuous input and output. Moreover, PPR is universal in the sense that the user and the server only need to agree on the output space and a proposal distribution, and the user can simulate any DP mechanism with the same output space. The user can choose a suitable DP mechanism and privacy budget according to their communication bandwidth and privacy requirement, without divulging their choice to the server. ", "page_idx": 1}, {"type": "text", "text": "Exactness. Unlike previous DP mechanism compressors such as Feldman and Talwar [31], Shah et al. [71], Triastcyn et al. [78], PPR enables exact simulation, ensuring that the reproduced distribution perfectly matches the original one. Exact distribution recovery offers several advantages. Firstly, the compressed sample maintains the same statistical properties as the uncompressed one. If the local randomizer is unbiased (a crucial requirement for many machine learning tasks like DP-SGD), the outcome of PPR remains unbiased. In contrast, reconstruction distributions in prior simulation-based compression methods [31, 71] are often biased unless specific debiasing steps are performed (only possible for certain DP mechanisms [71]). Secondly, when the goal is to compute the mean (e.g., for private mean or frequency estimation problems) and the local noise is \u201csummable\u201d (e.g., Gaussian noise or other infinitely divisible distributions [55, 42]), exact distribution recovery of the local noise enables precise privacy accounting for the final central DP guarantee, without relying on generic privacy amplification techniques like shuffilng [30, 32]. PPR can compress a central DP mechanism (e.g., the Gaussian mechanism [27]) and simultaneously achieve weaker local DP (i.e., with a larger $\\varepsilon_{\\mathsf{I o c a l}})$ ) and stronger central DP (i.e., with a smaller $\\varepsilon_{\\mathsf{c e n t r a l}})$ , while maintaining exactly the same privacy-utility trade-offs as the uncompressed Gaussian mechanism. ", "page_idx": 1}, {"type": "text", "text": "Communication efficiency. PPR compresses the output of any DP mechanism to a size close to the theoretical lower bound. For a mechanism on the data $X$ with output $Z$ , the compression size of PPR is $I(X;Z)+\\log(I(X;Z)+1)+O(1)$ , with only a logarithmic gap from the mutual information lower bound $I(X;Z)$ .2The \u201c $\\cdot O(1)^{\\circ}$ constant can be given explicitly in terms of a tunable parameter $\\alpha>1$ which controls the trade-off between compression size, computational time and privacy. ", "page_idx": 1}, {"type": "text", "text": "The main technical tool we utilize for PPR is the Poisson functional representation [61, 60], which provides precise control over the reconstructed joint distribution in channel simulation problems [6, 45, 61, 35, 41, 10, 7, 21]. Channel simulation aims to achieve the minimum communication for simulating a channel (i.e., a specific conditional distribution). Typically, these methods rely on shared randomness between the user and server, and privacy is only preserved when the shared randomness is hidden from the adversary. This setup conflicts with local DP, where the server (which requires access to shared randomness for decoding) is considered adversarial. To ensure local DP, we introduce a randomized encoder based on the Poisson functional representation, which stochastically maps a private local message to its representation. Hence, PPR achieves order-wise trade-offs between privacy, communication, and accuracy, while preserving the original distribution of local randomizers. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Notations. Entropy $H(X)$ , mutual information $I(X;Y)$ , KL divergence $D(P\\|Q)$ and logarithm are to the same base, e.g., they can be all in bits (base 2), or all in nats (base $e$ ). For $P,Q,\\mathrm{d}\\bar{P(\\cdot)}/\\mathrm{d}Q$ denotes the Radon-Nikodym derivative. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Generic compression of local DP mechanisms. In this work, we consider both central DP [28] and local DP [79, 53]. Recent research has explored methods for compressing local DP randomizers when shared randomness is involved. For instance, when $\\varepsilon\\leq1$ , Bassily and Smith [5] demonstrated that a single bit can simulate any local DP randomizer with a small degradation of utility, as long as the output can be computed using only a subset of the users\u2019 data. Bun et al. [12] proposed another generic compression technique based on rejection sampling, which compresses a $\\varepsilon$ -DP mechanism into a $10\\varepsilon$ -DP mechanism. Feldman and Talwar [31] proposed a distributed simulation approach using rejection sampling with shared randomness, while Shah et al. [71], Triastcyn et al. [78] utilized importance sampling (or more specifically, minimum random coding [20, 74, 47]). However, all these methods only approximate the original local DP mechanism, unlike our scheme, which achieves an exact distribution recovery. ", "page_idx": 2}, {"type": "text", "text": "Distributed mean estimation under DP. Mean estimation is the canonical problems in distributed learning and analytics. They have been widely studied under privacy [24, 8, 23, 4], communication [40, 11, 75], or both constraints [18, 31, 71, 43, 17, 19]. Among them, Asi et al. [4] has demonstrated that the optimal unbiased mean estimation scheme under local differential privacy is privUnit [8]. Subsequently, communication-efficient mechanisms introduced by Feldman and Talwar [31], Shah et al. [71], Isik et al. [51] aimed to construct communication-efficient versions of privUnit, either through distributed simulation or discretization. However, these approaches only approximate the privUnit distribution, while our proposed method ensures exact distribution recovery. ", "page_idx": 2}, {"type": "text", "text": "Distributed channel simulation. Our approach relies on the notion of channel simulation [6, 45, 61, 35, 41, 10, 7, 21]. One-shot channel simulation is a lossy compression task, which aims to find the minimum amount of communications over a noiseless channel that is in need to \u201csimulate\u201d some channel $P_{Z|X}$ (a specific conditional distribution). By Harsha et al. [45], Li and El Gamal [61], the average communication cost is $I(X;Z)+O(\\log(I(X;Z)))$ . In [45], algorithms based on rejection sampling are proposed, and it is further generalized in [39] by introducing the greedy rejection coding. Dithered quantization [81] has also been used to simulate an additive noise channel in [2] for neural compression. As also shown in [2], the time complexity of channel simulation protocols (e.g., in [61]) is usually high, and [76, 35, 41] try to improve the runtime under certain assumptions. Moreover, channel simulation tools have also been used in neural network compression [47], image compression via variational autoencoders [37], diffusion models with perfect realism [77] and differentially private federated learning [71]. ", "page_idx": 2}, {"type": "text", "text": "Poisson functional representation. The Poisson functional representation is a channel simulation scheme studied in [61]. Also refer to [65] for related constructions for Monte Carlo simulations. Based on the Poisson functional representation, the Poisson matching lemma has been used in proving one-shot achievability results for various network information theory problems [60, 64]. Also see applications on unequal message protection [54], hypothesis testing [44], information hiding [63], minimax learning [62] and secret key generation [50]. A variation called the importance matching lemma [69] has also used in distributed lossy compression. By [38], the Poisson functional representation can be viewed as a certain variant of the $\\mathrm{A^{*}}$ sampling [66, 65], and hence an optimized version with better runtime for one-dimensional unimodal distribution has been proposed in [38]. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin by reviewing the formal definitions of differential privacy (DP). We consider two models of DP data analysis. In the central model, introduced in Dwork et al. [28], the data of the individuals is stored in a database $X\\,\\in\\,{\\mathcal{X}}$ by the server. The server is then trusted to perform data analysis whose output $Z=A(X)\\in{\\mathcal{Z}}$ (where $\\boldsymbol{\\mathcal{A}}$ is a randomized algorithm), which is sent to an untrusted data analyst, does not reveal too much information about any particular individual\u2019s data. While this model requires a higher level of trust than the local model, it is possible to design significantly more accurate algorithms. We say that two databases $X,X^{\\prime}\\in\\mathcal{X}$ are neighboring if they differ in a single data point. More generally, we can consider a symmetric neighbor relation $N\\stackrel{*}{\\subseteq}\\mathcal X^{2}$ , and regard $X,X^{\\prime}$ as neighbors if $(X,{\\dot{X}}^{\\prime})\\in{\\mathcal{N}}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "On the other hand, in the local model, each individual (or client) randomizes their data before sending it to the server, meaning that individuals are not required to trust the server. A local DP mechanism [53] is a local randomizer $\\boldsymbol{\\mathcal{A}}$ that maps the local data $X\\in\\mathcal{X}$ to the output $Z=A(X)\\in{\\mathcal{Z}}$ . Note that here $X$ is the data at one user, unlike central-DP where $X$ is the database with the data of all users. We now review the notion of $(\\varepsilon,\\delta)$ -central and local DP. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Differential privacy [28, 53]). Given a mechanism $\\boldsymbol{\\mathcal{A}}$ which induces the conditional distribution $P_{Z|X}$ of $Z=A(X)$ , we say that it satisfies $(\\varepsilon,\\delta)$ -DP if for any neighboring $(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})\\in\\mathcal{N}$ and $S\\subseteq{\\mathcal{Z}}$ , it holds that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(Z\\in S\\,|\\,X=x)\\leq e^{\\varepsilon}\\operatorname*{Pr}(Z\\in S\\,|\\,X=x^{\\prime})+\\delta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, if $N=\\chi^{2}$ , we say that the mechanism satisfies $(\\varepsilon,\\delta)$ -local DP [53].3 ", "page_idx": 3}, {"type": "text", "text": "When a mechanism satisfies $(\\varepsilon,0)$ -central/local DP, we will refer to it simply as $\\varepsilon$ -central/local DP.   \n$\\varepsilon$ -DP can be generalized to metric privacy by considering a metric $d_{\\mathcal{X}}(x,\\Bar{x^{\\prime}})$ over $\\mathcal{X}$ [16, 3]. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 $\\varepsilon\\cdot d_{\\mathcal{X}}$ -privacy [16, 3]). Given a mechanism $\\boldsymbol{\\mathcal{A}}$ with conditional distribution $P_{Z|X}$ , and a metric $d_{\\mathcal{X}}$ over $\\mathcal{X}$ , we say that $\\boldsymbol{\\mathcal{A}}$ satisfies $\\varepsilon\\cdot d_{\\mathcal{X}}$ -privacy if for any $x,x^{\\prime}\\in\\mathcal{X},\\mathcal{S}\\subseteq\\mathcal{Z}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(Z\\in{\\mathcal{S}}\\,|\\,X=x)\\leq e^{\\varepsilon\\cdot d_{\\mathcal{X}}(x,x^{\\prime})}\\,\\operatorname*{Pr}(Z\\in{\\mathcal{S}}\\,|\\,X=x^{\\prime}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This recovers the original $\\varepsilon$ -central DP by considering $d_{\\mathcal{X}}$ to be the Hamming distance among databases, and recovers the original $\\varepsilon$ -local DP by considering $d_{\\mathcal{X}}$ to be the discrete metric [16]. ", "page_idx": 3}, {"type": "text", "text": "The reason we use $X$ to refer to both the database in central DP and the user\u2019s data in local DP is that our proposed method can compress both central and local DP mechanisms in exactly the same manner. In the following sections, the mechanism $\\boldsymbol{\\mathcal{A}}$ to be compressed (often written as a conditional distribution $P_{Z|X}{}~.$ ) can be either a central or local DP mechanism, and the neighbor relation $\\mathcal{N}$ can be any symmetric relation. The \u201cencoder\u201d refers to the server in central DP, or the user in local DP. The \u201cdecoder\u201d refers to the data analyst in central DP, or the server in local DP. ", "page_idx": 3}, {"type": "text", "text": "4 Poisson Private Representation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Definition 4.1 (Poisson functional representation [61, 60]). Let $(T_{i})_{i}$ be a Poisson process with rate 1 (i.e., $T_{1},T_{2}-T_{1},T_{3}-T_{2},\\ldots\\stackrel{i i d}{\\sim}\\mathrm{Exp}(1))$ , independent of $Z_{i}\\ {\\overset{i i d}{\\sim}}\\ Q$ for $i=1,2,\\dots$ Then $(Z_{i},T_{i})_{i}$ is a Poisson process with intensity measure $Q\\times\\lambda_{[0,\\infty)}$ [57], where $\\lambda_{[0,\\infty)}$ is the Lebesgue measure over $[0,\\infty)$ . Fix any distribution $P$ over $\\mathcal{Z}$ that is absolutely continuous with respect to $Q$ . Let ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{T}_{i}:=T_{i}\\cdot\\Big(\\frac{\\mathrm{d}P}{\\mathrm{d}Q}(Z_{i})\\Big)^{-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then $(Z_{i},\\tilde{T}_{i})$ is a Poisson process with intensity measure $P\\times\\lambda_{[0,\\infty)}$ , which is from the mapping theorem [57]. The Poisson functional representation (PFR) [61, 60] selects the point $Z=Z_{K}$ with the smallest associated $\\tilde{T}_{K}$ , i.e., let $K:=\\mathrm{argmin}_{i}\\tilde{T}_{i}$ and $Z:=Z_{K}$ . ", "page_idx": 3}, {"type": "text", "text": "The PFR selects a sample following the target distribution $P$ using another distribution $Q$ . It draws a random sequence $(Z_{i})_{i}$ from $Q$ and a sequence of times $(T_{i})_{i}$ according to a Poisson process. If we select the sample $Z_{i}$ with the smallest $T_{i}$ , then the selected sample follows $Q$ . To obtain a sample from $P$ instead, we multiply the time by the factor $\\textstyle\\big(\\frac{\\mathrm{d}P}{\\mathrm{d}Q}(Z_{i})\\big)^{-1}$ in (1) to give $\\tilde{T}_{i}$ , so the $Z_{i}$ with the smallest $\\tilde{T}_{i}$ will follow $P$ . ", "page_idx": 3}, {"type": "text", "text": "The Poisson functional representation guarantees that $Z\\sim P$ [61]. To simulate a DP mechanism with a conditional distribution $P_{Z|X}$ using the Poisson functional representation, we can use $(Z_{i})_{i}$ as the shared randomness between the encoder and the decoder. 5 Upon observing $X$ , the encoder generates the Poisson process $(T_{i})_{i}$ , computes $\\tilde{T}_{i}$ and $K$ using $P=P_{Z|X}$ , and transmits $K$ to the decoder. The decoder simply outputs $Z_{K}$ , which follows the conditional distribution $P_{Z|X}$ . The issue is that $K$ is a function of $X$ and the shared randomness $(Z_{i},T_{i})_{i}$ , and a change of $X$ may affect $K$ in a deterministic manner, and hence this method cannot be directly used to protect the privacy of $X$ . ", "page_idx": 4}, {"type": "text", "text": "Poisson private representation. To ensure privacy, we introduce randomness in the encoder by a generalization of the Poisson functional representation, which we call Poisson private representation $(P P R)$ with parameter $\\alpha\\in(1,\\infty]$ , proposal distribution $Q$ and the simulated mechanism $P_{Z|X}$ . Both $X$ and $Z$ can be discrete or continuous, though as a regularity condition, we require $P_{Z|X}{\\left(\\cdot|X\\right)}$ to be absolutely continuous with respect to $Q$ almost surely. The PPR-compressed mechanism is given as: ", "page_idx": 4}, {"type": "text", "text": "1. We use $(Z_{i})_{i=1,2,\\ldots},Z_{i}\\stackrel{i i d}{\\sim}Q$ as the shared randomness between the encoder and the decoder. Practically, the encoder and the decoder can share a random seed and generate $Z_{i}\\ {\\overset{i i d}{\\sim}}\\ Q$ from it using a pseudorandom number generator.6   \n2. The encoder knows $(Z_{i})_{i},X,P_{Z|X}$ and performs the following steps:   \n(a) Generates the Poisson process $(T_{i})_{i}$ with rate 1.   \n(b) Computes $\\begin{array}{r}{\\tilde{T}_{i}:=T_{i}\\cdot(\\frac{\\mathrm{d}P}{\\mathrm{d}Q}(Z_{i}))^{-1}}\\end{array}$ , where $P:=P_{Z|X}(\\cdot|X)$ . Take $\\tilde{T}_{i}=\\infty$ if $\\begin{array}{r}{\\frac{\\mathrm{d}P}{\\mathrm{d}Q}(Z_{i})=0}\\end{array}$ . (c) Generates $K\\in\\mathbb{Z}_{+}$ using local randomness with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(K=k)=\\frac{\\tilde{T}_{k}^{-\\alpha}}{\\sum_{i=1}^{\\infty}\\tilde{T}_{i}^{-\\alpha}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(d) Compress $K$ (e.g., using Elias delta coding [29]) and sends $K$ . ", "page_idx": 4}, {"type": "text", "text": "3. The decoder, which knows $(Z_{i})_{i},K$ , outputs $Z=Z_{K}$ . ", "page_idx": 4}, {"type": "text", "text": "Note that when $\\alpha=\\infty$ , we have $K=\\mathrm{argmin}_{i}\\tilde{T}_{i}$ , and PPR reduces to the original Poisson functional representation [61, 60]. PPR can simulate the privacy mechanism $P_{Z|X}$ precisely, as shown in the following proposition. The proof is in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.2. The output $Z$ of PPR follows the conditional distribution $P_{Z|X}$ exactly. ", "page_idx": 4}, {"type": "text", "text": "Due to the exactness of PPR, it guarantees unbiasedness for tasks such as DME. If the goal is only to design a stand-alone privacy mechanism, we can focus on the privacy and utility of the mechanism without studying the output distribution. However, if the output of the mechanism is used for downstream tasks (e.g., for DME, after receiving information from clients, the server sends information about the aggregated mean to data analysts, where central DP is crucial), having an exact characterization of the conditional distribution of the output given the input allows us to obtain precise (central) privacy and utility guarantees. ", "page_idx": 4}, {"type": "text", "text": "Notably, PPR is universal in the sense that only the encoder needs to know the simulated mechanism $P_{Z|X}$ . The decoder can decode the index $K$ as long as it has access to the shared randomness $(Z_{i})_{i}$ . This allows the encoder to choose an arbitrary mechanism $P_{Z|X}$ with the same $\\mathcal{Z}$ , and adapt the choice of $P_{Z|X}$ to the communication and privacy constraints without explicitly informing the decoder which mechanism is chosen. ", "page_idx": 4}, {"type": "text", "text": "Practically, the algorithm cannot compute the whole infinite sequence $(\\tilde{T}_{i})_{i}$ . We can truncate the method and only compute $\\tilde{T}_{i},\\dotsc,\\tilde{T}_{N}$ for a large $N$ and select $K\\,\\in\\,\\{1,\\ldots,N\\}$ , which incurs a small distortion in the distribution of $Z$ .7 While this method is practically acceptable, it might defeat the purpose of having an exact algorithm that ensures the correct conditional distribution $P_{Z|X}$ . In Appendix B, we will present an exact algorithm for PPR that terminates in a finite amount of time, using a reparametrization that allows the encoder to know when the optimal point $Z_{i}$ has already been encountered (see Algorithm 1 in Appendix B). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "By the lower bound for channel simulation [6, 61], we must have $H(K)\\;\\geq\\;I(X;Z)$ , i.e., the compression size is at least the mutual information between the data $X$ and the output $Z$ . The following result shows that the compression provided by PPR is \u201calmost optimal\u201d, i.e., close to the theoretical lower bound $I(X;Z)$ . The proof is given in Appendix F. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 (Compression size of PPR). For PPR with parameter $\\alpha>1$ , when the encoder is given the input $x$ , the message $K$ given by PPR satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\log K]\\leq D(P\\|Q)+(\\log(3.56))/\\operatorname*{min}\\{(\\alpha-1)/2,1\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $P:=P_{Z|X}(\\cdot|x)$ . As a result, when the input $X\\sim P_{X}$ is random, taking $Q=P_{Z}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\log K]\\leq I(X;Z)+(\\log(3.56))/\\operatorname*{min}\\{(\\alpha-1)/2,1\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note the running time complexity (which depends on the number of samples $Z_{i}$ the algorithm must examine before outputting the index $K$ ) can be quite high. Since $\\mathbb{E}[\\log K]\\approx I(X;Z),$ $K$ (and hence the running time) is at least exponential in $I(X;Z)$ . See more discussions in Section 8. ", "page_idx": 5}, {"type": "text", "text": "If a prefix-free encoding of $K$ is required, then the number of bits needed is slightly larger than $\\log_{2}K$ . For example, if Elias delta code [29] is used, the expected compression size is $\\leq\\mathbb{E}[\\log_{2}K]+$ $2\\log_{2}(\\mathbb{E}[\\log_{2}K]+1)+1$ bits. If the Shannon code [73] (an almost-optimal prefix-free code) for the Zipf distribution $p(k)\\propto k^{-\\lambda}$ with $\\lambda=1+1/\\mathbb{E}[\\log_{2}K]$ is used, the expected compression size is $\\leq\\mathbb{E}[\\log_{2}K]{+}\\!\\log_{2}(\\mathbb{E}[\\log_{2}K]{+}1){+}2$ bits (see [61]). Both codes yield an $I(X;Z){+}\\bar{O}(\\log I(X;Z))$ size, within a logarithmic gap from the lower bound $I(X;Z)$ . This is similar to some other channel simulation schemes such as [45, 10, 61], though these schemes do not provide privacy guarantees. ", "page_idx": 5}, {"type": "text", "text": "Note that if $P_{Z|X}$ is $\\varepsilon$ -DP, then by definition, for any $z\\in{\\mathcal{Z}}$ and $x,x_{0}\\in\\mathcal{X}$ , it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\nD\\left(P_{Z|X=x}\\big\\|P_{Z|X=x_{0}}\\right)=\\mathbb{E}_{Z\\sim P_{Z|X=x}}\\left[\\log\\left(\\frac{\\mathrm{d}P_{Z|X=x}}{\\mathrm{d}P_{Z|X=x_{0}}}(Z)\\right)\\right]\\leq\\varepsilon\\log e.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Setting the proposal distribution $Q=P_{Z|X=x_{0}}$ for an arbitrary $x_{0}\\in\\mathcal{X}$ gives the following bound. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.4 (Compression size under $\\varepsilon$ -LDP). Let $P_{Z|X}$ satisfy $\\varepsilon$ -differential privacy. Let $x_{0}\\in\\mathcal{X}$ and $Q=P_{Z|X=x_{0}}$ . Then for PPR with parameter $\\alpha>1$ , the expected compression size is at most $\\ell+\\log_{2}(\\ell+1)+2$ bits, where $\\ell:=\\varepsilon\\log_{2}e+(\\log_{2}(3.56))/\\operatorname*{min}\\big\\{(\\alpha-1)/2,1\\big\\}.$ . ", "page_idx": 5}, {"type": "text", "text": "Next, we analyze the privacy guarantee of PPR. The PPR method induces a conditional distribution $P_{\\left(Z_{i}\\right)_{i},K\\left|X\\right.}$ of the knowledge of the decoder $((Z_{i})_{i},K)$ , given the data $X$ . To analyze the privacy guarantee, we study whether the randomized mapping $P_{\\left(Z_{i}\\right){}_{i},K\\left|X\\right|}$ from $X$ to $((Z_{i})_{i},K)$ satisfies $\\varepsilon$ -DP or $(\\varepsilon,\\delta)$ -DP. 8 This is similar to the privacy condition in [71], and is referred as decoder privacy in [72], which is stronger than database privacy which concerns the privacy of the randomized mapping from $X$ to the final output $Z$ [72] (which is simply the privacy of the original mechanism $P_{Z|X}$ to be compressed since PPR simulates $P_{Z|X}$ precisely). Since the decoder knows $((Z_{i})_{i},K)$ , more than just the final output $Z$ , we expect that the PPR-compressed mechanism $P_{(Z_{i})_{i},K|X}$ to have a worse privacy guarantee than the original mechanism $P_{Z|X}$ , which is the price of having a smaller communication cost. The following result shows that, if the original mechanism $P_{Z|X}$ is $\\varepsilon$ -DP, then the PPR-compressed mechanism is guaranteed to be $2\\alpha\\varepsilon$ -DP. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.5 ( $\\varepsilon$ -DP of PPR). If the mechanism $P_{Z|X}$ is $\\varepsilon$ -differentially private, then PPR $P_{\\left(Z_{i}\\right){_i},K\\mid X}$ with parameter $\\alpha>1$ is $2\\alpha\\varepsilon$ -differentially private. ", "page_idx": 6}, {"type": "text", "text": "Similar results also apply to $(\\varepsilon,\\delta)$ -DP and metric DP. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6 $\\mathit{\\check{\\Psi}}(\\varepsilon,\\delta)$ -DP of PPR). If the mechanism $P_{Z|X}$ is $(\\varepsilon,\\delta)$ -differentially private, then PPR $P_{\\left(Z_{i}\\right)_{i},K\\left|X\\right.}$ with parameter $\\alpha>1$ is $(2\\alpha\\varepsilon,2\\delta)$ -differentially private. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.7 (Metric privacy of PPR). If the mechanism $P_{Z|X}$ satisfies $\\varepsilon\\cdot d_{\\mathcal{X}}$ -privacy, then PPR $P_{\\left(Z_{i}\\right)_{i},K\\left|X\\right.}$ with parameter $\\alpha>1$ satisfies $2\\alpha\\varepsilon\\cdot d_{\\mathcal{X}}$ -privacy. ", "page_idx": 6}, {"type": "text", "text": "Refer to Appendices $\\mathbf{C}$ and $\\mathrm{D}$ for the proofs. In Theorem 4.5 and Theorem 4.6, PPR imposes a multiplicative penalty $2\\alpha$ on the privacy parameter $\\varepsilon$ . This penalty can be made arbitrarily close to 2 by taking $\\alpha$ close to 1, which increases the communication cost (see Theorem 4.3). Compared to minimal random coding which has a factor 2 penalty in the DP guarantee [47, 71], the $2\\alpha$ factor in PPR is slightly larger, though PPR ensures exact simulation (unlike [47, 71] which are approximate). The method in [31] does not have a penalty on $\\varepsilon$ , but the utility and compression size depends on computational hardness assumptions on the pseudorandom number generator, and there is no guarantee that the compression size is close to the optimum. In comparison, the compression and privacy guarantees of PPR are unconditional and does not rely on computational assumptions. ", "page_idx": 6}, {"type": "text", "text": "In order to make the penalty of PPR close to 1, we have to consider $(\\varepsilon,\\delta)$ -differential privacy, and allow a small failure probability, i.e., a small increase in $\\delta$ . The following result shows that PPR can compress any $\\varepsilon$ -DP mechanism into a $(\\approx\\varepsilon,\\approx0)$ -DP mechanism as long as $\\alpha$ is close enough to 1 (i.e., almost no inflation). More generally, PPR can compress an $(\\varepsilon,\\delta)$ -DP mechanism into an $(\\approx\\varepsilon,\\approx2\\delta)$ )-DP mechanism for $\\alpha$ close to 1. The proof is in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.8 (Tighter $(\\varepsilon,\\delta)$ -DP of PPR). If the mechanism $P_{Z|X}$ is $(\\varepsilon,\\delta)$ -differentially private, then PPR $P_{\\left(Z_{i}\\right)_{i},K\\mid X}$ with parameter $\\alpha>1$ is $(\\alpha\\varepsilon+\\tilde{\\varepsilon},\\,2(\\delta+\\tilde{\\delta}))$ -differentially private, for every $\\tilde{\\varepsilon}\\in(0,1]$ and $\\tilde{\\delta}\\in(0,1/3]$ that satisfy $\\alpha\\leq e^{-4.2}\\widetilde{\\delta}\\widetilde{\\varepsilon}^{2}/(-\\ln\\widetilde{\\delta})+1$ . ", "page_idx": 6}, {"type": "text", "text": "5 Applications to Distributed Mean Estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We demonstrate the efficacy of PPR by applying it to distributed mean estimation (DME) [75]. Note that private DME is the core sub-routine in various private and federated optimization algorithms, such as DP-SGD [1] or DP-FedAvg [67]. ", "page_idx": 6}, {"type": "text", "text": "Consider the following general distributed setting: each of $n$ clients holds a local data point $X_{i}\\in\\mathcal{X}$ , and a central server aims to estimate a function of all local data $\\mu\\left(X^{n}\\right)$ , subject to privacy and local communication constraints. To this end, each client $i$ compresses $X_{i}$ into a message $Z_{i}\\in{\\mathcal{Z}}_{n}$ via a local encoder, and we require that each $Z_{i}$ can be encoded into a bit string with an expected length of at most $b$ bits. Upon receiving $Z^{n}:=(Z_{1},...,Z_{n})$ , the central server decodes it and outputs a DP estimate $\\hat{\\mu}$ . Two DP criteria can be considered: the $(\\varepsilon,\\delta)$ -central DP of the randomized mapping from $X^{n}$ to $\\hat{\\mu}$ , and the $(\\varepsilon,\\delta)$ -local DP of the randomized mapping from $X_{i}$ to $Z_{i}$ for each client $i$ . ", "page_idx": 6}, {"type": "text", "text": "In the distributed $L_{2}$ mean estimation problem, $\\mathcal{X}\\,=\\,\\mathcal{B}_{d}(C)\\,:=\\,\\left\\{v\\in\\mathbb{R}^{d}\\,|\\,\\|v\\|_{2}\\leq C\\right\\}$ , and the central server aims to estimate the sample mean $\\begin{array}{r}{\\mu(X^{n})\\,:=\\,\\frac{1}{n}\\sum_{i=1}^{n}X_{i}}\\end{array}$ by minimizing the mean squared error (MSE) $\\mathbb{E}[\\left\\Vert\\boldsymbol{\\mu}-\\hat{\\boldsymbol{\\mu}}\\right\\Vert_{2}^{2}]$ . It is recently proved that under $\\varepsilon$ -local DP, privUnit [8, 4] is the optimal mechanism. By simulating privUnit with PPR and applying Corollary 4.4 and Theorem 4.6, we immediately obtain the following corollary: ", "page_idx": 6}, {"type": "text", "text": "Corollary 5.1 (PPR simulating privUnit). Let $P$ be the density defined by $\\varepsilon$ -privUnit $\\cdot2$ Bhowmick et al. $I8,$ Algorithm $I J.$ Let $Q$ be the uniform density over the sphere $\\mathbb{S}^{d-1^{\\bullet}}(1/\\dot{m})$ where the radius $1/m$ is defined in Bhowmick et al. $[8,(l5)]_{\\cdot}$ . Let $r^{*}:=e^{\\varepsilon}$ . Then the outcome of PPR (see Algorithm 1) satisfies $(I)$ $2\\alpha\\varepsilon$ -local $D P$ ; and (2) $(\\alpha\\varepsilon+\\tilde{\\varepsilon},2\\tilde{\\delta})$ -DP for any $\\alpha\\leq e^{-4.2}\\tilde{\\delta}\\tilde{\\varepsilon}^{2}/\\log(1/\\tilde{\\delta})+1$ . In addition, the average compression size is at most $\\ell{+}\\log_{2}(\\ell{+}1){+}2$ bits where $\\ell:=\\varepsilon\\!+\\!(\\log_{2}{(3.56)})/\\operatorname*{min}\\{(\\alpha-$ $1)/2,1\\}$ . Moreover, PPR achieves the same MSE as $\\varepsilon$ -privUnit $^2$ , which is $O\\left(d/\\operatorname*{min}\\left(\\varepsilon,\\varepsilon^{2}\\right)\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "Note that PPR can simulate arbitrary local DP mechanisms. However, we present only the result of privUnit $\\dot{\\cdot}_{2}$ because it achieves the optimal privacy-accuracy trade-off. Besides simulating local DP mechanisms, PPR can also compress central DP mechanisms while still preserving some (albeit weaker) local guarantees. We give a corollary of Theorems 4.3 and 4.6. The proof is in Appendix $\\mathrm{H}$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 5.2 (PPR-compressed Gaussian mechanism). $\\mathrm{{}}^{\\gamma}e t\\,\\varepsilon,\\delta\\in(0,1)$ . Consider the Gaussian mechanism $\\begin{array}{r}{P_{Z|X}(\\cdot|x)=\\mathcal{N}(x,\\frac{\\sigma^{2}}{n}\\mathbb{I}_{d}),}\\end{array}$ , and the proposal distribution $\\begin{array}{r}{Q=\\mathcal{N}(0,(\\frac{C^{2}}{d}+\\frac{\\sigma^{2}}{n})\\mathbb{I}_{d})}\\end{array}$ , where $\\begin{array}{r}{\\sigma\\geq\\frac{C\\sqrt{2\\ln(1.25/\\delta)}}{\\varepsilon}}\\end{array}$ . For each client $i$ , let $Z_{i}$ be the output of PPR applied on $P_{Z|X}{\\left(\\cdot|X_{i}\\right)}$ . We have: ", "page_idx": 7}, {"type": "text", "text": "\u2022 $\\begin{array}{r}{\\hat{\\mu}(Z^{n}):=\\frac{1}{n}\\sum_{i}Z_{i}}\\end{array}$ yields an unbiased estimator of $\\begin{array}{r}{\\mu(X^{n})=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}}\\end{array}$ satisfying $(\\varepsilon,\\delta)$ central DP and has $\\big{M S E}\\mathbb{E}[\\|\\mu-\\hat{\\mu}\\|_{2}^{2}]=\\sigma^{2}d/n^{2}$ .   \n\u2022 As long as $\\varepsilon<1/\\sqrt{n}$ , PPR satisfies $(2\\alpha\\sqrt{n}\\varepsilon,2\\delta)$ -local $D P_{\\cdot}^{9}$   \n\u2022 The average per-client communication cost is at most $\\ell+\\log_{2}(\\ell+1)+2$ bits where where $\\begin{array}{r l}&{\\quad\\ell:=\\displaystyle\\frac{d}{2}\\log_{2}\\left(\\frac{C^{2}n}{d\\sigma^{2}}+1\\right)+\\eta_{\\alpha}\\,\\le\\,\\displaystyle\\frac{d}{2}\\log_{2}\\left(\\frac{n\\varepsilon^{2}}{2d\\ln(1.25/\\delta)}+1\\right)+\\eta_{\\alpha},}\\\\ &{\\quad_{\\alpha}:=(\\log_{2}(3.56))/\\operatorname*{min}\\{(\\alpha-1)/2,\\,1\\}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "A few remarks are in order. First, notice that when $\\alpha$ is fixed, for an $\\begin{array}{r}{O\\big(\\frac{C^{2}d}{n^{2}\\varepsilon^{2}}\\log(1/\\delta)\\big)}\\end{array}$ MSE, the per-client communication cost is ", "page_idx": 7}, {"type": "equation", "text": "$$\nO\\Big(d\\log\\Big(\\frac{n\\varepsilon^{2}}{d\\log(1/\\delta)}+1\\Big)+1\\Big),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which is at least as good as the $O(n\\varepsilon^{2}/\\log(1/\\delta)+1)$ bound in [75, 19], and can be better than $O(n\\varepsilon^{2}/\\log(1/\\delta)+\\bar{1})$ when $n\\gg d$ . Hence, the PPR-compressed Gaussian mechanism is order-wise optimal. Second, compared to other works that also compress the Gaussian mechanism, PPR is the only lossless compressor; schemes based on random sparsification, projection, or minimum random coding (e.g., Triastcyn et al. [78], Chen et al. [19]) are lossy, i.e., they introduce additional distortion on top of the DP noise. Finally, other DP mechanism compressors tailored to local randomizers [31, 71] do not provide the same level of central DP guarantees when applied to local Gaussian noise since the reconstructed noise is no longer Gaussian. Refer to Section 7 for experiments. ", "page_idx": 7}, {"type": "text", "text": "6 Applications to Metric Privacy ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Metric privacy [16, 3] (see Definition 3.2) allows users to send privatized version $Z\\in\\mathbb{R}^{d}$ of their data vectors $\\boldsymbol{X}\\in\\dot{\\mathbb{R}}^{d}$ to an untrusted server, so that the server can know $X$ approximately but not exactly. A popular mechanism is the Laplace mechanism [16, 3, 33, 34], where a $d_{\\cdot}$ -dimensional Laplace noise is added to $X$ . The conditional density function of $Z$ given $X$ is $f_{Z|X}(z|x)\\propto e^{-\\varepsilon d_{X}(x,z)}$ , where $\\varepsilon$ is the privacy parameter, and the metric $d_{\\mathcal{X}}(x,z)=\\|x-z\\|_{2}$ is the Euclidean distance. The Laplace mechanism achieves $\\varepsilon\\cdot d_{\\mathcal{X}}$ -privacy, and has been used, for example, in geo-indistinguishability to privatize the users\u2019 locations [3], and to privatize high-dimensional word embedding vectors [33, 34]. ", "page_idx": 7}, {"type": "text", "text": "A problem is that the real vector $Z$ cannot be encoded into finitely many bits. To this end, [3] studies a discrete Laplace mechanism where each coordinate of $Z$ is quantized to a finite number of levels, introducing additional distortion to $Z$ . PPR provides an alternative compression method that preserves the statistical behavior of $Z$ (e.g., unbiasedness) exactly. We give a corollary of Theorems 4.3 and 4.7. The proof is in Appendix I. Refer to Appendix J for an experiment on metric privacy. ", "page_idx": 7}, {"type": "text", "text": "Corollary 6.1 (PPR-compressed Laplace mechanism). Consider PPR applied to the Laplace mechanism $P_{Z|X}$ where $\\bar{X}\\ \\in\\ \\mathcal{B}_{d}(\\bar{C})\\ =\\ \\{x\\ \\in\\ \\mathbb{R}^{d}\\,|\\,\\|x\\|_{2}\\ \\leq\\ C\\},$ , with a proposal distribution $\\begin{array}{r}{Q=\\mathcal{N}(0,(\\frac{C^{2}}{d}+\\frac{d+1}{\\varepsilon^{2}})\\mathbb{I}_{d})}\\end{array}$ . It achieves an MSE $\\textstyle{\\frac{d(d+1)}{\\varepsilon^{2}}}$ , $a\\;2\\alpha\\epsilon\\cdot d_{\\mathcal{X}}$ -privacy, and a compression size at most $\\ell+\\bar{\\log_{2}}(\\ell+1)+2$ bits, where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\ell:=\\frac{d}{2}\\log_{2}\\left(\\frac{2}{e}\\left(\\frac{C^{2}\\varepsilon^{2}}{d}+d+1\\right)\\right)-\\log_{2}\\frac{\\Gamma(d+1)}{\\Gamma(\\frac{d}{2}+1)}+\\eta_{\\alpha},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\eta_{\\alpha}:=(\\log_{2}(3.56))/\\operatorname*{min}\\{(\\alpha-1)/2,\\,1\\}.$ ", "page_idx": 7}, {"type": "text", "text": "9The restricted range on $\\varepsilon<1/\\sqrt{n}$ is due to the simpler privacy accountant [25]. By using the R\u00e9nyi DP accountant instead, one can achieve a tighter result that applies to any $n$ . We present the R\u00e9nyi DP version of the corollary in Appendix G. Moreover, in the context of federated learning, $_n$ refers to the number of clients in each round, which is typically much smaller than the total number of clients. For example, as observed in [52], the per-round cohort size in Google\u2019s FL application typically ranges from $10^{3}$ to $10^{5}$ , significantly smaller than the number of trainable parameters $d\\in[\\dot{10^{6}},10^{9}]$ or the number of available users $N\\in[10^{6},10^{\\circ}]$ . ", "page_idx": 7}, {"type": "text", "text": "7 Empirical Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We empirically evaluate our scheme on the DME problem (which is formally introduced in Section 5), examine the privacy-accuracy-communication trade-off, and compare it with the Coordinate Subsampled Gaussian Mechanism (CSGM) [19, Algorithm 1], an order-optimal scheme for DME under central DP. In Chen et al. [19], each client only communicates partial information (via sampling a subset of the coordinates of the data vector) about its samples to amplify the privacy, and the compression is mainly from subsampling. Moreover, CSGM only guarantees central DP. ", "page_idx": 8}, {"type": "text", "text": "We use the same setup that has been used in [19]: consider $n=500$ clients, and the dimension of local vectors is $d=1000$ , each of which is generated according to $X_{i}(j)$ i.i\u223c.d. $(2\\cdot\\mathrm{Ber}(0.8)-1)$ , where $\\mathrm{Ber}(0.8)$ is a Bernoulli random variable with parameter $p=0.8$ . We require $(\\varepsilon,\\delta)$ -central DP with $\\delta=10^{-6}$ and $\\varepsilon\\in[0.05,6]$ and apply the PPR with $\\alpha=2$ to simulate the Gaussian mechanism, where the privacy budgets are accounted via R\u00e9nyi DP. ", "page_idx": 8}, {"type": "text", "text": "We compare the MSE of PPR $\\alpha=2$ , using Theorem 4.3) and CSGM under various compression sizes in Figure 1 (the $y$ -axis is in logarithmic scale).10 Note that the MSE of the (uncompressed) Gaussian mechanism coincides with the CSGM with 1000 bits, and the PPR with only 400 bits. We see that PPR consistently achieves a smaller MSE compared to CSGM for all $\\varepsilon$ \u2019s and compression sizes considered. For $\\epsilon=1$ and we compress $d=1000$ to 50 bits, CSGM has an MSE 0.1231 , while PPR has an MSE 0.08173, giving a $33.\\bar{6}1\\%$ reduction. For $\\epsilon=0.5$ and we compress $d=1000$ to 25 bits (the case of high compression and conservative privacy), CSGM has an MSE 0.3877, while PPR has an MSE 0.3011, giving a $22.33\\%$ reduction. These reductions are significant, since all considered mechanisms are asymptotically close to optimal and a large improvement compared to an (almost optimal) mechanism is unexpected. See Section L for more about MSE against the compression sizes. ", "page_idx": 8}, {"type": "text", "text": "We also emphasize that PPR provides both central and local DP guarantees according to Theorem 4.5, 4.6 and 4.8. In contrast, CSGM only provides central DP guarantees. Another advantage of PPR under conservative privacy (small $\\epsilon$ ) is that the trade-off between $\\epsilon$ and MSE of PPR exactly coincides with the trade-off of the Gaussian mechanism for small $\\epsilon$ (see Figure 1), and CSGM is only close to (but strictly worse than) the Gaussian mechanism. This means that for small $\\epsilon$ , PPR provides compression without any drawback in terms of $\\epsilon$ -MSE trade-off compared to the Gaussian mechanism (which requires an infinite size communication to exactly realize). ", "page_idx": 8}, {"type": "text", "text": "Moreover, although directly applying PPR on the $d_{\\cdot}$ -dimensional vectors is impractical for a large $d$ , one can ensure an efficient $O(d)$ running time (see Section 8 for details) by breaking the vector with $d=1000$ dimensions into small chunks of fixed lengths (we use $d_{\\mathrm{chunk}}=50$ dimensions for each chunk), and apply the PPR to each chunk. We call it the sliced PPR in Figure 1. Though the sliced PPR has a small penalty on the MSE (as shown in Figure 1), it still outperforms the CSGM (400 bits) for the range of $\\varepsilon$ in the plot. For the sliced PPR for one $d=1000$ vector, when $\\epsilon=0.05$ , the running time is 1.3348 seconds on average.11 For larger $\\epsilon$ \u2019s, we can choose smaller $d_{\\mathrm{chunk}}$ \u2019s to have reasonable running time: For $\\epsilon=6$ and $d_{\\mathrm{chunk}}=2$ we have an average running time 0.0127 seconds and with $d_{\\mathrm{chunk}}=4$ we have an average running time 0.6343 seconds; for $\\epsilon=10$ and $d_{\\mathrm{chunk}}=2$ we have an average running time 0.0128 seconds and with $d_{\\mathrm{chunk}}=4$ we have an average running time 0.7301 seconds. See Appendix K for more experiments on the running time of the sliced PPR. ", "page_idx": 8}, {"type": "text", "text": "8 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While PPR is communication-efficient, having only a logarithmic gap from the theoretical lower bound on the compression size as shown in Theorem 4.3, the running time complexity can be high. However, we note that an exponential complexity is also needed in sampling methods that do not ensure privacy, such as [65, 47]. It has been proved in [2] that no polynomial time general samplingbased method exists (even without privacy constraint), if $R P\\neq N P$ . All existing polynomial time exact channel simulation methods can only simulate specific noisy channels.12 Hence, a polynomial time algorithm for exactly compressing a general DP mechanism is likely nonexistent. ", "page_idx": 8}, {"type": "image", "img_path": "CgGjT8EG8A/tmp/9b7e7f52e06e0967ee5452ecf86d497a3b61bbe8e743278044129d0d4b2ea964.jpg", "img_caption": ["Figure 1: MSE of distributed mean estimation for PPR and CSGM [19] for different $\\varepsilon$ \u2019s. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Nevertheless, this is not an obstacle for simulating local DP mechanisms, since the mutual information $I(X;Z)$ for a reasonable local DP mechanism must be small, or else the leakage of the data $X$ in $Z$ would be too large. For an $\\varepsilon$ -local DP mechanism, we have $I(X;Z)\\leq\\operatorname*{min}\\{\\bar{\\varepsilon},\\varepsilon^{2}\\}$ (in nats) [22]. Hence, the PPR algorithm can terminate quickly even if has a running time exponential in $I(X;Z)$ . ", "page_idx": 9}, {"type": "text", "text": "Another way to ensure a polynomial running time is to divide the data into small chunks and apply the mechanism to each chunk separately. For example, to apply the Gaussian mechanism to a high-dimensional vector, we break it into several shorter vectors and apply the mechanism to each vector. Experiments in Section 7 show that this greatly reduces the running time while having only a small penalty on the compression size. See Appendix K for experiments on the running time of PPR. ", "page_idx": 9}, {"type": "text", "text": "9 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed a novel scheme for compressing DP mechanisms, called Poisson private representation (PPR). Unlike previous schemes which are either constrained on special classes of DP mechanisms or introducing additional distortions on the output, our scheme can compress and exactly simulate arbitrary mechanisms while protecting differential privacy, with a compression size that is close to the theoretic lower bound. A future direction is to reduce the running time of PPR under certain restrictions on $P_{Z|X}$ . For example, the techniques in [38, 35] may be useful when $P_{Z|X}$ is unimodal. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "YL was partially supported by the CUHK PhD International Mobility for Partnerships and Collaborations Award 2023-24. WC and AO were supported by the NSF grant CIF-2213223. CTL was partially supported by two grants from the Research Grants Council of the Hong Kong Special Administrative Region, China [Project No.s: CUHK 24205621 (ECS), CUHK 14209823 (GRF)]. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318, 2016.   \n[2] Eirikur Agustsson and Lucas Theis. Universally quantized neural compression. Advances in neural information processing systems, 33:12367\u201312376, 2020.   \n[3] Miguel E Andr\u00e9s, Nicol\u00e1s E Bordenabe, Konstantinos Chatzikokolakis, and Catuscia Palamidessi. Geo-indistinguishability: Differential privacy for location-based systems. In Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security, pages 901\u2013914, 2013.   \n[4] Hilal Asi, Vitaly Feldman, and Kunal Talwar. Optimal algorithms for mean estimation under local differential privacy. In International Conference on Machine Learning, pages 1046\u20131056. PMLR, 2022.   \n[5] Raef Bassily and Adam Smith. Local, private, efficient protocols for succinct histograms. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 127\u2013135, 2015.   \n[6] Charles H Bennett, Peter W Shor, John Smolin, and Ashish V Thapliyal. Entanglement-assisted capacity of a quantum channel and the reverse Shannon theorem. IEEE Trans. Inf. Theory, 48 (10):2637\u20132655, 2002.   \n[7] Charles H Bennett, Igor Devetak, Aram W Harrow, Peter W Shor, and Andreas Winter. The quantum reverse shannon theorem and resource tradeoffs for simulating quantum channels. IEEE Transactions on Information Theory, 60(5):2926\u20132959, 2014.   \n[8] Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan Rogers. Protection against reconstruction and its applications in private federated learning. arXiv preprint arXiv:1812.00984, 2018.   \n[9] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[10] Mark Braverman and Ankit Garg. Public vs private coin in bounded-round information. In International Colloquium on Automata, Languages, and Programming, pages 502\u2013513. Springer, 2014.   \n[11] Mark Braverman, Ankit Garg, Tengyu Ma, Huy L Nguyen, and David P Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1011\u20131020, 2016.   \n[12] Mark Bun, Jelani Nelson, and Uri Stemmer. Heavy hitters and the structure of local privacy. In Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, SIGMOD/PODS \u201918, page 435\u2013447, New York, NY, USA, 2018. Association for Computing Machinery.   \n[13] Cl\u00e9ment L Canonne, Gautam Kamath, and Thomas Steinke. The discrete Gaussian for differential privacy. Advances in Neural Information Processing Systems, 33:15676\u201315688, 2020.   \n[14] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650, 2021.   \n[15] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pages 5253\u20135270, 2023.   \n[16] Konstantinos Chatzikokolakis, Miguel E Andr\u00e9s, Nicol\u00e1s Emilio Bordenabe, and Catuscia Palamidessi. Broadening the scope of differential privacy using metrics. In Privacy Enhancing Technologies: 13th International Symposium, PETS 2013, Bloomington, IN, USA, July 10-12, 2013. Proceedings 13, pages 82\u2013102. Springer, 2013.   \n[17] Kamalika Chaudhuri, Chuan Guo, and Mike Rabbat. Privacy-aware compression for federated data analysis. In Uncertainty in Artificial Intelligence, pages 296\u2013306. PMLR, 2022.   \n[18] Wei-Ning Chen, Peter Kairouz, and Ayfer \u00d6zg\u00fcr. Breaking the communication-privacy-accuracy trilemma. Advances in Neural Information Processing Systems, 33:3312\u20133324, 2020.   \n[19] Wei-Ning Chen, Dan Song, Ayfer \u00d6zg\u00fcr, and Peter Kairouz. Privacy amplification via compression: Achieving the optimal privacy-accuracy-communication trade-off in distributed mean estimation. Advances in Neural Information Processing Systems, 36, 2024.   \n[20] Paul Cuff. Communication requirements for generating correlated random variables. In 2008 IEEE International Symposium on Information Theory, pages 1393\u20131397. IEEE, 2008.   \n[21] Paul Cuff. Distributed channel synthesis. IEEE Trans. Inf. Theory, 59(11):7071\u20137096, Nov 2013.   \n[22] Paul Cuff and Lanqing Yu. Differential privacy as a mutual information constraint. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 43\u201354, 2016.   \n[23] John Duchi and Ryan Rogers. Lower bounds for locally private estimation via communication complexity. In Conference on Learning Theory, pages 1161\u20131191. PMLR, 2019.   \n[24] John C Duchi, Michael I Jordan, and Martin J Wainwright. Local privacy and statistical minimax rates. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pages 429\u2013438. IEEE, 2013.   \n[25] Cynthia Dwork. Differential privacy. In International colloquium on automata, languages, and programming, pages 1\u201312. Springer, 2006.   \n[26] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014.   \n[27] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In Advances in CryptologyEUROCRYPT 2006: 24th Annual International Conference on the Theory and Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28-June 1, 2006. Proceedings 25, pages 486\u2013503. Springer, 2006.   \n[28] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265\u2013284. Springer, 2006.   \n[29] Peter Elias. Universal codeword sets and representations of the integers. IEEE transactions on information theory, 21(2):194\u2013203, 1975.   \n[30] \u00dalfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Abhradeep Thakurta. Amplification by shuffling: From local to central differential privacy via anonymity. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2468\u20132479. SIAM, 2019.   \n[31] Vitaly Feldman and Kunal Talwar. Lossless compression of efficient private local randomizers. In International Conference on Machine Learning, pages 3208\u20133219. PMLR, 2021.   \n[32] Vitaly Feldman, Audra McMillan, and Kunal Talwar. Hiding among the clones: A simple and nearly optimal analysis of privacy amplification by shuffling. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 954\u2013964. IEEE, 2022.   \n[33] Natasha Fernandes, Mark Dras, and Annabelle McIver. Generalised differential privacy for text document processing. In Principles of Security and Trust: 8th International Conference, POST 2019, pages 123\u2013148. Springer International Publishing, 2019.   \n[34] Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, and Tom Diethe. Privacy-and utilitypreserving textual analysis via calibrated multivariate perturbations. In Proceedings of the 13th international conference on web search and data mining, pages 178\u2013186, 2020.   \n[35] Gergely Flamich. Greedy poisson rejection sampling. Advances in Neural Information Processing Systems, 36, 2024.   \n[36] Gergely Flamich and Lucas Theis. Adaptive greedy rejection sampling. In 2023 IEEE International Symposium on Information Theory (ISIT), pages 454\u2013459. IEEE, 2023.   \n[37] Gergely Flamich, Marton Havasi, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Compressing images by encoding their latent representations with relative entropy coding. Advances in Neural Information Processing Systems, 33:16131\u201316141, 2020.   \n[38] Gergely Flamich, Stratis Markou, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Fast relative entropy coding with $\\mathbf{A}^{*}$ coding. In International Conference on Machine Learning, pages 6548\u20136577. PMLR, 2022.   \n[39] Gergely Flamich, Stratis Markou, and Jos\u00e9 Miguel Hern\u00e1ndez Lobato. Faster relative entropy coding with greedy rejection coding. arXiv preprint arXiv:2309.15746, 2023.   \n[40] Ankit Garg, Tengyu Ma, and Huy Nguyen. On communication cost of distributed statistical estimation and dimensionality. In Advances in Neural Information Processing Systems, pages 2726\u20132734, 2014.   \n[41] Daniel Goc and Gergely Flamich. On channel simulation with causal rejection samplers. arXiv preprint arXiv:2401.16579, 2024.   \n[42] Slawomir Goryczka and Li Xiong. A comprehensive comparison of multiparty secure additions with differential privacy. IEEE transactions on dependable and secure computing, 14(5): 463\u2013477, 2015.   \n[43] Chuan Guo, Kamalika Chaudhuri, Pierre Stock, and Michael Rabbat. Privacy-aware compression for federated learning through numerical mechanism design. In International Conference on Machine Learning, pages 11888\u201311904. PMLR, 2023.   \n[44] Yuanxin Guo, Sadaf Salehkalaibar, Stark C. Draper, and Wei Yu. One-shot achievability region for hypothesis testing with communication constraint. In accepted at the IEEE Information Theory Workshop. IEEE, 2024.   \n[45] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. IEEE Trans. Inf. Theory, 56(1):438\u2013449, Jan 2010.   \n[46] Burak Has\u0131rc\u0131og\u02d8lu and Deniz G\u00fcnd\u00fcz. Communication efficient private federated learning using dithering. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7575\u20137579. IEEE, 2024.   \n[47] Marton Havasi, Robert Peharz, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. In 7th International Conference on Learning Representations, ICLR 2019, 2019.   \n[48] Mahmoud Hegazy and Cheuk Ting Li. Randomized quantization with exact error distribution. In 2022 IEEE Information Theory Workshop (ITW), pages 350\u2013355. IEEE, 2022.   \n[49] Mahmoud Hegazy, R\u00e9mi Leluc, Cheuk Ting Li, and Aymeric Dieuleveut. Compression with exact error distribution for federated learning. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 613\u2013621. PMLR, 02\u201304 May 2024.   \n[50] Henri Hentil\u00e4, Yanina Y Shkel, and Visa Koivunen. Communication-constrained secret key generation: Second-order bounds. IEEE Transactions on Information Theory, 2024.   \n[51] Berivan Isik, Wei-Ning Chen, Ayfer \u00d6zg\u00fcr, Tsachy Weissman, and Albert No. Exact optimality of communication-privacy-utility tradeoffs in distributed mean estimation. Advances in Neural Information Processing Systems, 36, 2024.   \n[52] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and trends\u00ae in machine learning, 14(1\u20132):1\u2013210, 2021.   \n[53] Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793\u2013826, 2011.   \n[54] Ashish Khisti, Arash Behboodi, Gabriele Cesa, and Pratik Kumar. Unequal message protection: One-shot analysis via poisson matching lemma. In 2024 IEEE International Symposium on Information Theory (ISIT). IEEE, 2024.   \n[55] Samuel Kotz, Tomasz Kozubowski, and Krzystof Podgorski. The Laplace distribution and generalizations: a revisit with applications to communications, economics, engineering, and finance. Springer Science & Business Media, 2012.   \n[56] Natalie Lang, Elad Sofer, Tomer Shaked, and Nir Shlezinger. Joint privacy enhancement and quantization in federated learning. IEEE Transactions on Signal Processing, 71:295\u2013310, 2023.   \n[57] G\u00fcnter Last and Mathew Penrose. Lectures on the Poisson process, volume 7. Cambridge University Press, 2017.   \n[58] Cheuk Ting Li. Pointwise redundancy in one-shot lossy compression via Poisson functional representation. arXiv preprint, 2024.   \n[59] Cheuk Ting Li and Venkat Anantharam. Pairwise multi-marginal optimal transport and embedding for earth mover\u2019s distance. arXiv preprint arXiv:1908.01388, 2019.   \n[60] Cheuk Ting Li and Venkat Anantharam. A unified framework for one-shot achievability via the Poisson matching lemma. IEEE Transactions on Information Theory, 67(5):2624\u20132651, 2021.   \n[61] Cheuk Ting Li and Abbas El Gamal. Strong functional representation lemma and applications to coding theorems. IEEE Transactions on Information Theory, 64(11):6967\u20136978, Nov 2018.   \n[62] Cheuk Ting Li, Xiugang Wu, Ayfer \u00d6zg\u00fcr, and Abbas El Gamal. Minimax learning for remote prediction. In 2018 IEEE ISIT, pages 541\u2013545, June 2018. doi: 10.1109/ISIT.2018.8437318.   \n[63] Yanxiao Liu and Cheuk Ting Li. One-shot information hiding. In accepted at the IEEE Information Theory Workshop. IEEE, 2024.   \n[64] Yanxiao Liu and Cheuk Ting Li. One-shot coding over general noisy networks. arXiv preprint arXiv:2402.06021, 2024.   \n[65] Chris J Maddison. A Poisson process model for Monte Carlo. Perturbation, Optimization, and Statistics, pages 193\u2013232, 2016.   \n[66] Chris J Maddison, Daniel Tarlow, and Tom Minka. A\\* sampling. Advances in neural information processing systems, 27, 2014.   \n[67] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[68] Ilya Mironov. R\u00e9nyi differential privacy. In 2017 IEEE 30th computer security foundations symposium (CSF), pages 263\u2013275. IEEE, 2017.   \n[69] Buu Phan, Ashish Khisti, and Christos Louizos. Importance matching lemma for lossy compression with side information. In International Conference on Artificial Intelligence and Statistics, pages 1387\u20131395. PMLR, 2024.   \n[70] John K Salmon, Mark A Moraes, Ron O Dror, and David E Shaw. Parallel random numbers: as easy as 1, 2, 3. In Proceedings of 2011 international conference for high performance computing, networking, storage and analysis, pages 1\u201312, 2011.   \n[71] Abhin Shah, Wei-Ning Chen, Johannes Balle, Peter Kairouz, and Lucas Theis. Optimal compression of locally differentially private mechanisms. In International Conference on Artificial Intelligence and Statistics, pages 7680\u20137723. PMLR, 2022.   \n[72] Ali Moradi Shahmiri, Chih Wei Ling, and Cheuk Ting Li. Communication-efficient laplace mechanism for differential privacy via random quantization. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4550\u2013 4554. IEEE, 2024.   \n[73] Claude E Shannon. A mathematical theory of communication. Bell system technical journal, 27(3):379\u2013423, 1948.   \n[74] Eva C Song, Paul Cuff, and H Vincent Poor. The likelihood encoder for lossy compression. IEEE Trans. Inf. Theory, 62(4):1836\u20131849, 2016.   \n[75] Ananda Theertha Suresh, X Yu Felix, Sanjiv Kumar, and H Brendan McMahan. Distributed mean estimation with limited communication. In International conference on machine learning, pages 3329\u20133337. PMLR, 2017.   \n[76] Lucas Theis and Noureldin Y Ahmed. Algorithms for the communication of samples. In International Conference on Machine Learning, pages 21308\u201321328. PMLR, 2022.   \n[77] Lucas Theis, Tim Salimans, Matthew D Hoffman, and Fabian Mentzer. Lossy compression with Gaussian diffusion. arXiv preprint arXiv:2206.08889, 2022.   \n[78] Aleksei Triastcyn, Matthias Reisser, and Christos Louizos. DP-REC: Private & communicationefficient federated learning. arXiv preprint arXiv:2111.05454, 2021.   \n[79] Stanley L Warner. Randomized response: A survey technique for eliminating evasive answer bias. Journal of the American Statistical Association, 60(309):63\u201369, 1965.   \n[80] Guangfeng Yan, Tan Li, Tian Lan, Kui Wu, and Linqi Song. Layered randomized quantization for communication-efficient and privacy-preserving distributed learning. arXiv preprint arXiv:2312.07060, 2023.   \n[81] Jacob Ziv. On universal quantization. IEEE Transactions on Information Theory, 31(3):344\u2013347, 1985. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proof of Proposition 4.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Write $(X_{i})_{i}\\sim\\operatorname{PP}(\\mu)$ if the points $(X_{i})_{i}$ (as a multiset, ignoring the ordering) form a Poisson point process with intensity measure $\\mu$ . Similarly, for $f:[0,\\infty)^{n}\\to[0,\\infty)$ , we write $\\operatorname{PP}(f)$ for the Poisson point process with intensity function $f$ (i.e., the intensity measure has a Radon-Nikodym derivative $f$ against the Lebesgue measure). ", "page_idx": 15}, {"type": "text", "text": "Let $(T_{i})_{i}\\sim\\operatorname{PP}(1)$ be a Poisson process with rate 1, independent of $Z_{1},Z_{2},\\ldots\\stackrel{i i d}{\\sim}Q$ . By the marking theorem [57], $(\\dot{Z_{i}},\\dot{T_{i}})_{i}\\sim\\mathrm{PP}(\\bar{Q}\\times\\lambda_{[0,\\infty)})$ , where $Q\\times\\lambda_{[0,\\infty)}$ is the product measure between $Q$ and the Lebesgues measure over $[0,\\infty)$ . Let $P=P_{Z|X}(\\cdot|x)$ , and $\\begin{array}{r}{\\tilde{T}_{i}=T_{i}\\cdot(\\frac{\\mathrm{d}P}{\\mathrm{d}Q}(Z_{i}))^{-1}}\\end{array}$ . By the mapping theorem [57] (also see [61, 60]), $(Z_{i},\\tilde{T}_{i})_{i}\\sim\\mathrm{PP}(P\\times\\lambda_{[0,\\infty)})$ . Note that the points $(Z_{i},\\tilde{T}_{i})_{i}$ may not be sorted in ascending order of $\\tilde{T}_{i}$ . Therefore, we will sort them as follows. Let $j_{1}$ be the $j$ such that $\\tilde{T}_{j}$ is the smallest, $j_{2}$ be the $j$ other than $j_{1}$ such that $\\tilde{T}_{j}$ is the smallest, and so on. Break ties arbitrarily. Then $(\\Tilde{T}_{j_{i}})_{i}$ is an ascending sequence, and we still have $(Z_{j_{i}},\\tilde{T}_{j_{i}})_{i}\\sim\\mathrm{PP}(P\\times\\lambda_{[0,\\infty)})$ since we are merely rearranging the points. Comparing $(Z_{j_{i}},\\tilde{T}_{j_{i}})_{i}\\sim\\mathrm{PP}(P\\times\\lambda_{[0,\\infty)})$ with the definition of $(Z_{i},T_{i})_{i}\\sim\\mathrm{PP}(Q\\times\\lambda_{[0,\\infty)})$ , we can see that $(\\tilde{T}_{j_{i}})_{i}\\sim\\mathrm{PP}(1)$ is independent of $Z_{j_{1}},Z_{j_{2}},\\dots\\stackrel{i i d}{\\sim}P$ . Recall that in PPR, we generate $K\\in\\mathbb{Z}_{+}$ with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(K=k)=\\frac{\\tilde{T}_{k}^{-\\alpha}}{\\sum_{i=1}^{\\infty}\\tilde{T}_{i}^{-\\alpha}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the final output is $Z_{K}$ . Rearranging the points according to $(j_{i})_{i}$ , the distribution of the final output remains the same if we instead generate $K^{\\prime}\\in\\mathbb{Z}_{+}$ with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(K^{\\prime}=k)=\\frac{\\tilde{T}_{j_{k}}^{-\\alpha}}{\\sum_{i=1}^{\\infty}\\tilde{T}_{j_{i}}^{-\\alpha}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the final output is $Z_{j_{K^{\\prime}}}$ . Since $(\\tilde{T}_{j_{i}})_{i}\\sim\\mathrm{PP}(1)$ is independent of $Z_{j_{i}}\\stackrel{i i d}{\\sim}P$ , we know that $K^{\\prime}$ is independent of $(Z_{j_{i}})_{i}$ , and hence $Z_{j_{K^{\\prime}}}\\sim P$ follows the desired distribution. ", "page_idx": 15}, {"type": "text", "text": "B Reparametrization and Detailed Algorithm of PPR ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We now discuss the implementation of the Poisson private representation in Section 4. Practically, the algorithm cannot compute the whole infinite sequence $(\\Tilde{T}_{i})_{i}^{\\bar{}}$ . We now present an exact algorithm for PPR that terminates in a finite amount of time using a reparametrization. ", "page_idx": 15}, {"type": "text", "text": "In the proof of Theorem F.1, we showed that, letting $(T_{i})_{i}\\,\\sim\\,\\mathrm{PP(1)},\\,Z_{1},Z_{2},...\\,\\stackrel{i i d}{\\sim}\\,Q,\\,R_{i}\\,:=$ $(\\mathrm{d}P/\\mathrm{d}Q)(Z_{i}),V_{1},V_{2},\\ldots\\stackrel{i i d}{\\sim}\\mathrm{Exp}(1).$ , PPR can be equivalently expressed as ", "page_idx": 15}, {"type": "equation", "text": "$$\nK=\\underset{k}{\\mathrm{argmin}}T_{k}^{\\alpha}R_{k}^{-\\alpha}V_{k}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The problem of finding $K$ is that there is no stopping criteria for the argmin. For example, if we scan the points $(T_{i},R_{i},V_{i})_{i}$ in increasing order of $T_{i}$ , it is always possible that there is a future point with $V_{i}$ so small that it makes $T_{i}^{\\alpha}R_{i}^{-\\alpha}V_{i}$ smaller than the current minimum. If we scan the points in increasing order of $V_{i}$ instead, it is likewise possible that there is a future point with a very small $T_{i}$ . We can scan the points in increasing order of $U_{i}:=T_{i}^{\\alpha}V_{i}$ , but we would not know the indices of the points in the original process where $T_{1}\\leq T_{2}\\leq\\cdots$ is in increasing order, which is necessary to find out the $Z_{i}$ corresponding to each point (recall that in PPR, the point with the smallest $T_{i}$ corresponds to $Z_{1}$ , the second smallest $T_{i}$ corresponds to $Z_{2}$ , etc.). ", "page_idx": 15}, {"type": "text", "text": "Therefore, we will scan the points in increasing order of $B_{i}\\,:=\\,T_{i}^{\\alpha}\\operatorname*{min}\\{V_{i},1\\}$ instead. By the mapping theorem [57], $(T_{i}^{\\alpha})_{i}\\sim\\operatorname{PP}(\\alpha^{-1}t^{1/\\alpha-1})$ . By the marking theorem [57], ", "page_idx": 15}, {"type": "equation", "text": "$$\n(T_{i}^{\\alpha},V_{i})_{i}\\sim\\mathrm{PP}(\\alpha^{-1}t^{1/\\alpha-1}e^{-v}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the mapping theorem, ", "page_idx": 15}, {"type": "equation", "text": "$$\n(T_{i}^{\\alpha},T_{i}^{\\alpha}V_{i})_{i}\\sim\\mathrm{PP}(\\alpha^{-1}t^{1/\\alpha-2}e^{-v t^{-1}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $B_{i}=\\operatorname*{min}\\{T_{i}^{\\alpha},T_{i}^{\\alpha}V_{i}\\}$ , again by the mapping theorem, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(B_{i})_{i}\\sim\\mathrm{PP}\\left(\\int_{b}^{\\infty}\\alpha^{-1}b^{1/\\alpha-2}e^{-v b^{-1}}\\mathrm{d}v\\right.}}\\\\ &{\\qquad\\qquad\\left.+\\int_{b}^{\\infty}\\alpha^{-1}t^{1/\\alpha-2}e^{-b t^{-1}}\\mathrm{d}t\\right)}\\\\ &{=\\mathrm{PP}\\left(\\alpha^{-1}b^{1/\\alpha-1}e^{-1}+\\alpha^{-1}b^{1/\\alpha-1}\\gamma(1-\\alpha^{-1},1)\\right)}\\\\ &{=\\mathrm{PP}\\left(\\alpha^{-1}\\left(e^{-1}+\\gamma_{1}\\right)b^{1/\\alpha-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\gamma_{1}:=\\gamma(1-\\alpha^{-1},1)$ and $\\begin{array}{r}{\\gamma(\\beta,x)=\\int_{0}^{x}e^{-\\tau}\\tau^{\\beta-1}\\mathrm{d}\\tau}\\end{array}$ is the lower incomplete gamma function. Comparing the distribution of $(B_{i})_{i}$ and $(T_{i}^{\\bar{\\alpha}})_{i}$ , we can generate $(B_{i})_{i}$ by first generating $(U_{i})_{i}\\sim$ $\\mathrm{PP}(1)$ , and then taking $B_{i}\\,=\\,(U_{i}\\alpha/(e^{-1}+\\gamma_{1}))^{\\alpha}$ . The conditional distribution of $(T_{i},V_{i})$ given $B_{i}=b$ is described as follows: ", "page_idx": 16}, {"type": "text", "text": "\u2022 With probability $e^{-1}/(e^{-1}+\\gamma_{1})$ , we have $T_{i}^{\\alpha}=b$ and $T_{i}^{\\alpha}V_{i}\\sim b(\\mathrm{Exp}(1)+1)$ , and hence $T_{i}=b^{1/\\alpha}$ and $V_{i}\\sim\\mathrm{Exp}(1)+1$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 With probability $\\gamma_{1}/(e^{-1}+\\gamma_{1})$ , we have $T_{i}^{\\alpha}V_{i}=b$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{i}^{\\alpha}\\sim\\frac{\\alpha^{-1}t^{1/\\alpha-2}e^{-b t^{-1}}}{\\alpha^{-1}\\gamma(1-\\alpha^{-1},1)b^{1/\\alpha-1}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, for $0<\\tau\\leq1$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(V_{i}\\leq\\tau)=\\operatorname*{Pr}(T_{i}^{\\alpha}\\geq b/\\tau)=\\frac{\\gamma(1-\\alpha^{-1},\\tau)}{\\gamma(1-\\alpha^{-1},1)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $V_{i}$ follows the truncated gamma distribution with shape $1-\\alpha^{-1}$ and scale 1, truncated within the interval [0, 1]. We then have $T_{i}=(b/V_{i})^{1/\\alpha}$ . ", "page_idx": 16}, {"type": "text", "text": "The algorithm is given in Algorithm 1. The encoder and decoder require a shared random seed $s$ . One way to generate $s$ is to have the encoder and decoder maintain two synchronized pseudorandom number generators (PRNGs) that are always at the same state, and invoke the PRNGs to generate $s$ , guaranteeing that the $s$ at the encoder is the same as the $s$ at the decoder. The encoder maintains a collection of points $\\left(T_{i},V_{i},\\Theta_{i}\\right)$ , stored in a heap to allow fast query and removal of the point with the smallest $T_{i}$ . The value $\\Theta_{i}\\in\\{0,1\\}$ indicates whether it is possible that the point $(T_{i},V_{i})$ attains the minimum of $T_{k}^{\\alpha}R_{k}^{-\\alpha}V_{k}$ . The encoding algorithm repeats until there is no possible points left in the heap, and it is impossible for any future point to be better than the current minimum of $T_{k}^{\\alpha}R_{k}^{-\\alpha}V_{k}$ . The encoding time complexity is $O(\\operatorname*{sup}_{z}(\\mathrm{d}P/\\mathrm{d}Q)(z)\\log(\\operatorname*{sup}_{z}(\\mathrm{d}P/\\mathrm{d}Q)(z)))$ , which is close to other sampling-based channel simulation schemes [45, 36].13 The decoding algorithm simply outputs the $k$ -th sample generated using the random seed $s$ , which can be performed in $O(1)$ time.14 ", "page_idx": 16}, {"type": "text", "text": "The PPR is implemented by Algorithm 1. We write $x\\gets\\mathrm{Exp}_{\\mathcal{G}}(1)$ to mean that we generate an exponential random variate $x$ with rate 1 using the pseudorandom number generator $\\mathcal{G}$ . Write $x\\leftarrow\\mathrm{Exp}_{\\mathrm{local}}(1)$ to mean that $x$ is generated using a local pseudorandom number generator (not $\\mathcal{G}$ ). ", "page_idx": 16}, {"type": "text", "text": "C Proofs of Theorem 4.5 and Theorem 4.7 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "First prove Theorem 4.5. Consider a $\\varepsilon$ -DP mechanism $P_{Z|X}$ . Consider neighbors $x_{1},x_{2}$ , and let $\\begin{array}{r}{P_{j}:=P_{Z|X}(\\cdot|x_{j}),\\tilde{T}_{j,i}:=T_{i}/(\\frac{\\mathrm{d}P_{j}}{\\mathrm{d}Q}(Z_{i}))}\\end{array}$ , and $K_{j}$ be the output of PPR applied on $P_{j}$ , for $j=1,2$ . Since $P_{Z|X}$ is $\\varepsilon$ -DP, ", "page_idx": 16}, {"type": "equation", "text": "$$\ne^{-\\varepsilon}\\frac{\\mathrm{d}P_{2}}{\\mathrm{d}Q}(z)\\leq\\frac{\\mathrm{d}P_{1}}{\\mathrm{d}Q}(z)\\leq e^{\\varepsilon}\\frac{\\mathrm{d}P_{2}}{\\mathrm{d}Q}(z)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 Poisson private representation   \nProcedure P $\\mathrm{'PRENCODE}(\\alpha,Q,r,r^{\\ast},s)$ :   \nInput: parameter $\\alpha>1$ , distribution $Q$ , density $r(z):=(\\mathrm{d}P/\\mathrm{d}Q)(z)$ ,   \nbound $r^{*}\\geq\\operatorname*{sup}_{z}r(z)$ , random seed $s$   \nOutput: index $k\\in\\mathbb{Z}_{>0}$   \n1: Initialize PRNG $\\mathcal{G}$ using the seed $s$   \n2: $u\\gets0$ , $w^{*}\\gets\\infty$ , $k\\leftarrow0$ , $k^{*}\\gets0$ , $n\\leftarrow0$   \n3: $\\begin{array}{r}{\\gamma_{1}\\gets\\gamma(1-\\alpha^{-1},1)=\\int_{0}^{1}e^{-\\tau}\\tau^{-\\alpha^{-1}}\\mathrm{d}\\tau}\\end{array}$   \n4: $h\\leftarrow\\emptyset$ (empty heap)   \n5: while true do   \n6: $\\begin{array}{l}{u\\leftarrow u+\\mathrm{Exp}_{\\mathrm{local}}(1)}\\\\ {b\\leftarrow(u\\alpha/(e^{-1}+\\gamma_{1}))^{\\alpha}}\\end{array}$ \u25b7Generated using local randomness (not $\\mathcal{G}$ )   \n7:   \n8: if $n=0$ and $b(r^{*})^{-\\alpha}\\geq w^{*}$ then \u25b7No possible points left and future points impossible   \n9: return $k^{*}$   \n10: end if   \n11: if $\\mathrm{Unif_{local}}(0,1)<e^{-1}/(e^{-1}+\\gamma_{1})$ then \u25b7Run with prob. $e^{-1}/(e^{-1}+\\gamma_{1})$   \n12: $t\\leftarrow b^{1/\\alpha}$ , $v\\leftarrow\\mathrm{Exp}_{\\mathrm{local}}(1)+1$   \n13: else   \n14: repeat   \n15: $\\begin{array}{c}{\\dot{v}\\leftarrow\\operatorname{Gamma}_{\\operatorname{local}}(1-\\alpha^{-1},1)}\\\\ {\\mathbf{until}\\;v\\leq1}\\\\ {t\\leftarrow(b/v)^{1/\\alpha}}\\end{array}$ \u25b7Gamma distribution   \n16:   \n17:   \n18: end if   \n19: $\\theta\\gets\\mathbf{1}\\{(t/r^{*})^{\\alpha}v\\leq w^{*}\\}$ \u25b7Is it possible for this point to be optimal   \n20: Push $(t,v,\\theta)$ to $h$   \n21: $n\\gets n+\\theta$ \u25b7Number of possible points in heap   \n22: while $h\\neq\\emptyset$ and $\\operatorname*{min}_{(t^{\\prime},v^{\\prime},\\theta^{\\prime})\\in h}t^{\\prime}\\leq b^{1/\\alpha}$ do \u25b7Assign $Z_{i}$ \u2019s to points in heap with small $T_{i}$   \n23: $(t,v,\\theta)\\gets\\arg\\operatorname*{min}_{(t^{\\prime},v^{\\prime},\\theta^{\\prime})\\in h}t^{\\prime}$ , and pop $(t,v,\\theta)$ from $h$   \n24: n \u2190n \u2212\u03b8   \n25: k \u2190k + 1   \n26: Generate $z\\sim Q$ using $\\mathcal{G}$   \n27: $w\\leftarrow(t/r(z))^{\\alpha}v$   \n28: if $w<w^{*}$ then   \n29: $\\boldsymbol{w}^{*}\\leftarrow\\boldsymbol{w}$   \n30:   \n31: end if   \n32: end while   \n33: end while   \nProcedure PPRDECODE $(Q,k,s):$   \nInput: $Q$ , index $k\\in\\mathbb{Z}_{>0}$ , seed $s$   \nOutput: sample $z$   \n1: Initialize PRNG $\\mathcal{G}$ using the seed $s$   \n2: for $i=1,2,\\dots,k$ do   \n3: Generate $z\\sim Q$ using $\\mathcal{G}$ \u25b7See footnote 14   \n4: end for   \n5: return $z$ ", "page_idx": 17}, {"type": "text", "text": "for $Q$ -almost every $z,{^{15}}$ and hence $e^{-\\varepsilon}\\tilde{T}_{2,i}\\leq\\tilde{T}_{1,i}\\leq e^{\\varepsilon}\\tilde{T}_{2,i}$ . For $k\\in\\mathbb{Z}_{+}$ , we have, almost surely, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(K_{1}=k\\,|\\,(Z_{i},T_{i})_{i})=\\frac{\\tilde{T}_{1,k}^{-\\alpha}}{\\sum_{i=1}^{\\infty}\\tilde{T}_{1,i}^{-\\alpha}}}\\\\ &{\\qquad\\qquad\\qquad\\le\\frac{e^{\\alpha\\varepsilon}\\tilde{T}_{2,k}^{-\\alpha}}{\\sum_{i=1}^{\\infty}e^{-\\alpha\\varepsilon}\\tilde{T}_{2,i}^{-\\alpha}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=e^{2\\alpha\\varepsilon}\\operatorname*{Pr}(K_{2}=k\\,|\\,(Z_{i},T_{i})_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For any measurable $S\\subseteq\\mathcal{Z}^{\\infty}\\times\\mathbb{Z}_{>0}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(\\left(\\left(Z_{i}\\right)_{i},K_{1}\\right)\\in\\mathcal{S}\\right)}\\\\ &{=\\mathbb{E}\\left[\\operatorname*{Pr}\\left(\\left(\\left(Z_{i}\\right)_{i},K_{1}\\right)\\in\\mathcal{S}\\big|\\left(Z_{i},T_{i}\\right)_{i}\\right)\\right]}\\\\ &{=\\mathbb{E}\\left[\\underset{k\\geq0^{\\varepsilon}\\ldots}{\\sum}\\underset{k\\geq0^{\\varepsilon}\\ldots}{\\operatorname*{Pr}\\left(K_{1}=k\\,\\big|\\left(Z_{i},T_{i}\\right)_{i}\\right)}\\right]}\\\\ &{\\leq e^{2\\alpha\\varepsilon}\\cdot\\mathbb{E}\\left[\\underset{k\\geq0^{\\varepsilon}\\ldots}{\\sum}\\underset{k\\geq0^{\\varepsilon}\\ldots}{\\operatorname*{Pr}\\left(K_{2}=k\\,\\big|\\left(Z_{i},T_{i}\\right)_{i}\\right)}\\right]}\\\\ &{=e^{2\\alpha\\varepsilon}\\operatorname*{Pr}\\left(\\left(\\left(Z_{i}\\right)_{i},K_{2}\\right)\\in\\mathcal{S}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, $P_{\\left(Z_{i}\\right)_{i},K\\mid X}$ is $2\\alpha\\varepsilon$ -DP. ", "page_idx": 18}, {"type": "text", "text": "For Theorem 4.7, consider a $\\varepsilon\\cdot d_{\\mathcal{X}}$ -private mechanism $P_{Z|X}$ , and consider $x_{1},x_{2}\\in\\mathcal{X}$ . We have ", "page_idx": 18}, {"type": "equation", "text": "$$\ne^{-\\varepsilon\\cdot d_{\\mathcal{X}}(x_{1},x_{2})}\\frac{\\mathrm{d}P_{2}}{\\mathrm{d}Q}(z)\\leq\\frac{\\mathrm{d}P_{1}}{\\mathrm{d}Q}(z)\\leq e^{\\varepsilon\\cdot d_{\\mathcal{X}}(x_{1},x_{2})}\\frac{\\mathrm{d}P_{2}}{\\mathrm{d}Q}(z)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $Q$ -almost every $z$ . By exactly the same arguments as in the proof of Theorem 4.5, $\\operatorname*{Pr}\\left(((Z_{i})_{i},K_{1})\\in\\mathcal{S}\\right)\\,\\le\\,e^{2\\alpha\\varepsilon\\cdot d_{\\mathcal{X}}(x_{1},x_{2})}\\operatorname*{Pr}\\left(((Z_{i})_{i},K_{2})\\in\\mathcal{S}\\right)$ , and hence $P_{\\left(Z_{i}\\right){_i},K\\left|X\\right.}$ is $2\\alpha\\varepsilon\\cdot d_{\\mathcal{X}}.$ - private. ", "page_idx": 18}, {"type": "text", "text": "D Proof of Theorem 4.6 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Consider a $(\\varepsilon,\\delta)$ -DP mechanism $P_{Z|X}$ . Consider neighbors $x_{1},x_{2}$ , and let $P_{j}:=P_{Z|X}(\\cdot|x_{j})$ , and $K_{j}$ be the output of PPR applied on $P_{j}$ , for $j=1,2$ . By the definition of $(\\varepsilon,\\delta)$ -differential privacy, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\int\\operatorname*{max}\\left\\{\\rho_{1}(z)-e^{\\varepsilon}\\rho_{2}(z),\\,0\\right\\}Q(\\mathrm{d}z)\\leq\\delta,}\\\\ &{}&{\\int\\operatorname*{max}\\left\\{\\rho_{2}(z)-e^{\\varepsilon}\\rho_{1}(z),\\,0\\right\\}Q(\\mathrm{d}z)\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{\\rho}}(z):=\\operatorname*{min}\\left\\lbrace\\operatorname*{max}\\left\\lbrace\\rho_{1}(z),\\,e^{-\\varepsilon}\\rho_{2}(z)\\right\\rbrace,\\,e^{\\varepsilon}\\rho_{2}(z)\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that $e^{-\\varepsilon}\\rho_{2}(z)\\leq\\overline{{\\rho}}(z)\\leq e^{\\varepsilon}\\rho_{2}(z)$ . We then consider two cases: ", "page_idx": 18}, {"type": "text", "text": "Case 1: $\\begin{array}{r}{\\int\\overline{{\\rho}}(z)Q(\\mathrm{d}z)\\leq1.}\\end{array}$ . Let $\\rho_{3}(z)$ be such that $\\begin{array}{r}{\\int\\rho_{3}(z)Q(\\mathrm{d}z)=1}\\end{array}$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{{\\rho}}}(z)\\leq\\rho_{3}(z)\\leq e^{\\varepsilon}\\rho_{2}(z).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can always find such $\\rho_{3}$ by taking an appropriate convex combination of the lower bound above (which integrates to $\\leq1$ ) and the upper obund above (which integrates to $\\geq1$ ). We then have ", "page_idx": 18}, {"type": "equation", "text": "$$\ne^{-\\varepsilon}\\rho_{2}(z)\\leq\\rho_{3}(z)\\leq e^{\\varepsilon}\\rho_{2}(z).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $\\rho_{1}(z)\\!-\\!e^{\\varepsilon}\\rho_{2}(z)\\leq0$ , then $\\rho_{1}(z)\\!-\\!\\rho_{3}(z)\\leq\\rho_{1}(z)\\!-\\!\\overline{{\\rho}}(z)\\leq0.$ . If $\\rho_{1}(z)\\!-\\!e^{\\varepsilon}\\rho_{2}(z)>0$ , then $\\rho_{3}(z)=$ $\\overline{{\\rho}}(z)=e^{\\varepsilon}\\rho_{2}(z)$ . Either way, we have max $\\{\\rho_{1}(z)-\\rho_{3}(z),\\,0\\}=\\operatorname*{max}\\left\\{\\rho_{1}(z)-e^{\\varepsilon}\\rho_{2}(z),\\,0\\right\\}$ . By (5), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int\\operatorname*{max}\\left\\{\\rho_{1}(z)-\\rho_{3}(z),\\,0\\right\\}Q(\\mathrm{d}z)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $P_{3}\\,=\\,\\rho_{3}Q$ be the probability measure with $\\mathrm{d}P_{3}/\\mathrm{d}Q=\\rho_{3}$ . Then the total variation distance $d_{\\mathrm{TV}}(P_{1},P_{3})$ between $P_{1}$ and $P_{3}$ is at most $\\delta$ , and by (7), ", "page_idx": 19}, {"type": "equation", "text": "$$\ne^{-\\varepsilon}\\frac{\\mathrm{d}P_{2}}{\\mathrm{d}Q}(z)\\leq\\frac{\\mathrm{d}P_{3}}{\\mathrm{d}Q}(z)\\leq e^{\\varepsilon}\\frac{\\mathrm{d}P_{2}}{\\mathrm{d}Q}(z).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Case 2: $\\begin{array}{r}{\\int\\overline{{\\rho}}(z)Q(\\mathrm{d}z)>1.}\\end{array}$ . Let $\\rho_{3}(z)$ be such that $\\begin{array}{r}{\\int\\rho_{3}(z)Q(\\mathrm{d}z)=1}\\end{array}$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\ne^{-\\varepsilon}\\rho_{2}(z)\\leq\\rho_{3}(z)\\leq\\overline{{\\rho}}(z).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can always find such $\\rho_{3}$ by taking an appropriate convex combination of the lower bound above (which integrates to $\\leq1$ ) and the upper obund above (which integrates to $>1$ ). We again have $e^{-\\varepsilon}\\rho_{2}(z)\\,\\leq^{-}\\!\\rho_{3}(z)\\,\\leq\\,e^{\\varepsilon}\\rho_{2}(z)$ . If $\\bar{e^{-\\varepsilon}}\\rho_{2}(z)-\\rho_{1}(z)\\,\\leq\\,0$ , then $\\bar{\\rho_{3}}(z)\\,-\\,\\rho_{1}(z)\\,\\le\\,\\bar{\\rho}(\\bar{z})\\,-\\,$ $\\rho_{1}(z)\\,\\leq\\,0$ . If $e^{-\\varepsilon}\\rho_{2}(z)\\,-\\,\\rho_{1}(z)\\,>\\,0$ , then $\\rho_{3}(z)\\,=\\,\\overline{{{\\rho}}}(z)\\,=\\,e^{-\\varepsilon}\\rho_{2}(z)$ . Either way, we have max $\\{\\rho_{3}(z)-\\rho_{1}(z),\\,0\\}=\\operatorname*{max}\\left\\{e^{-\\varepsilon}\\rho_{2}(z)-\\rho_{1}(z),\\,0\\right\\}$ . By (6), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int\\operatorname*{max}\\left\\{\\rho_{3}(z)-\\rho_{1}(z),\\,0\\right\\}Q(\\mathrm{d}z)\\leq e^{-\\varepsilon}\\delta\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $P_{3}=\\rho_{3}Q$ be the probability measure with $\\mathrm{d}P_{3}/\\mathrm{d}Q=\\rho_{3}$ . Again, we have $d_{\\mathrm{TV}}(P_{1},P_{3})\\leq\\delta$ and (8). Therefore, regardless of whether Case 1 or Case 2 holds, we can construct $P_{3}$ satisfying $d_{\\mathrm{TV}}(P_{1},P_{3})\\leq\\delta$ and (8). Let $K_{3}$ be the output of PPR applied on $P_{3}$ . ", "page_idx": 19}, {"type": "text", "text": "In the proof of Theorem F.1, we see that PPR has the following equivalent formulation. Let $(T_{i})_{i}\\sim$ $\\mathrm{PP}(1)$ be a Poisson process with rate 1, independent of $Z_{1},Z_{2},\\ldots\\stackrel{i i d}{\\sim}Q$ . Let $R_{i}:=(\\mathrm{d}P/\\mathrm{d}Q)(Z_{i})$ , and let its probability measure be $P_{R}$ . Let $V_{1},V_{2},\\ldots\\stackrel{i i d}{\\sim}\\mathrm{Exp}(1)$ . PPR can be equivalently expressed as ", "page_idx": 19}, {"type": "equation", "text": "$$\nK=\\mathop{\\mathrm{argmin}}_{k}T_{k}^{\\alpha}R_{k}^{-\\alpha}V_{k}=\\mathop{\\mathrm{argmin}}_{k}\\frac{T_{k}V_{k}^{1/\\alpha}}{R_{k}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $\\begin{array}{r}{(T_{i}V_{i}^{1/\\alpha})_{i}\\sim\\operatorname{PP}(\\int_{0}^{\\infty}v^{-1/\\alpha}e^{-v}\\mathrm{d}v)=\\operatorname{PP}(\\Gamma(1-\\alpha^{-1}))}\\end{array}$ is a uniform Poisson process. Therefore PPR is the same as the Poisson functional representation [61, 60] applied on $(T_{i}V_{i}^{1/\\alpha})_{i}$ . By the grand coupling property of Poisson functional representation [60, 59] (see [59, Theorem 3]), if we apply the Poisson functional representation on $P_{1}$ and $P_{3}$ to get $K_{1}$ and $K_{3}$ respectively, then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(K_{1}\\neq K_{3})\\le2d_{\\mathrm{TV}}(P_{1},P_{3})\\le2\\delta.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, for any measurable $S\\subseteq\\mathcal{Z}^{\\infty}\\times\\mathbb{Z}_{>0}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(((Z_{i})_{i},K_{1})\\in\\mathcal{S}\\right)\\leq\\operatorname*{Pr}\\left(((Z_{i})_{i},K_{3})\\in\\mathcal{S}\\right)+2\\delta}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq e^{2\\alpha\\varepsilon}\\operatorname*{Pr}\\left(((Z_{i})_{i},K_{2})\\in\\mathcal{S}\\right)+2\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality is by applying (3) on $P_{3},P_{2}$ instead of $P_{1},P_{2}$ . Hence, $P_{\\left(Z_{i}\\right)_{i},K\\left|X\\right.}$ is $\\left(2\\alpha\\varepsilon,2\\delta\\right)$ -DP. ", "page_idx": 19}, {"type": "text", "text": "E Proof of Theorem 4.8 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present the proof of $(\\varepsilon,\\delta)$ -DP of PPR (i.e., Theorem 4.8). ", "page_idx": 19}, {"type": "text", "text": "Proof. We assume ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha-1\\leq\\frac{\\beta\\tilde{\\delta}\\tilde{\\varepsilon}^{2}}{-\\ln\\tilde{\\delta}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\beta:=e^{-4.2}$ . Using the Laplace functional of the Poisson process $(\\Tilde{T}_{i})_{i}$ [57, Theorem 3.9], for $w>0$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\left[\\exp\\left(-w\\sum_{i}\\tilde{T}_{i}^{-\\alpha}\\right)\\right]=\\exp\\left(-\\int_{0}^{\\infty}(1-\\exp(-w t^{-\\alpha}))\\mathrm{d}t\\right)}}\\\\ &{}&{=\\exp\\left(-w^{1/\\alpha}\\Gamma(1-\\alpha^{-1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We first bound the left tail of $\\sum_{i}\\tilde{T}_{i}^{-\\alpha}$ . By Chernoff bound, for $d\\geq0$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\tau}\\Bigg(\\sum_{i}\\widehat{\\tau}_{i}^{t_{1}}\\leq\\frac{\\epsilon}{2}\\Bigg)}\\\\ &{\\leq\\frac{1}{6\\epsilon}\\mathrm{e}^{-\\epsilon_{2}\\epsilon_{1}}\\Bigg[\\mathrm{e}^{-\\epsilon_{3}(\\tau-\\epsilon_{1}^{\\prime}-\\epsilon_{1}^{\\prime})}\\Bigg]}\\\\ &{=\\frac{1}{6\\epsilon}\\mathrm{e}^{\\epsilon_{2}}\\Bigg(\\mathrm{e}^{-\\epsilon_{1}(2\\tau-\\epsilon_{2}^{\\prime})}(1-\\alpha^{-\\epsilon_{1}})^{2}}\\\\ &{\\leq\\mathrm{e}^{\\epsilon_{3}}\\Bigg(\\left(\\frac{\\Gamma(1-\\alpha^{-1})}{a}\\right)^{\\epsilon_{1}}d-\\left(\\frac{\\Gamma(1-\\alpha^{-1})}{a}\\right)^{\\epsilon_{1}}\\Gamma(1-\\alpha^{-1})\\Bigg)}\\\\ &{=\\exp\\Big((\\Gamma(1-\\alpha^{-1}))^{-\\epsilon_{1}}d^{-\\epsilon_{2}}\\Big(\\alpha^{-\\epsilon_{2}}\\tau-\\alpha^{-\\epsilon_{2}}\\Big)\\Big)}\\\\ &{=\\exp\\Bigg(-\\left(\\frac{\\alpha d}{(\\Gamma(1-\\alpha^{-1}))^{2}}\\right)^{-\\epsilon_{2}}(1-\\alpha^{-1})\\Bigg)}\\\\ &{=\\exp\\Bigg(-\\left(\\frac{\\alpha d(1-\\alpha^{-1})^{\\epsilon_{1}}}{(\\Gamma(1-\\alpha^{-1}))^{\\epsilon_{1}}}\\right)^{-\\epsilon_{1}}(1-\\alpha^{-1})\\Bigg)}\\\\ &{=\\exp\\Bigg(-\\left(\\frac{\\alpha d(1-\\alpha^{-1})^{\\epsilon_{1}}}{(\\Gamma(2-\\alpha^{-1}))^{\\epsilon_{1}}}\\right)^{-\\epsilon_{1}}(1-\\alpha^{-1})\\Bigg)}\\\\ &{=\\exp\\Bigg(-\\left(\\frac{\\alpha(1-\\alpha^{-1})^{\\epsilon_{1}}}{(\\Gamma(1-\\alpha^{-1}))^{\\epsilon_{1}}}\\right)^{-\\epsilon_{1}}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, to guarantee $\\operatorname*{Pr}(\\sum_{i}\\tilde{T}_{i}^{-\\alpha}\\le d)\\le\\tilde{\\delta}/3$ , we require ", "page_idx": 20}, {"type": "equation", "text": "$$\nd\\leq\\frac{\\Gamma(2-\\alpha^{-1})^{\\alpha}\\left(-\\ln(\\tilde{\\delta}/3)\\right)^{-(\\alpha-1)}}{\\alpha-1},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma(2-\\alpha^{-1})^{\\alpha}\\left(-\\ln(\\tilde{\\delta}/3)\\right)^{-(\\alpha-1)}}\\\\ &{\\geq\\left(\\exp\\left(-\\gamma(\\alpha-1)\\right)\\right)^{\\alpha}\\left(-\\ln(\\tilde{\\delta}^{2})\\right)^{-(\\alpha-1)}}\\\\ &{\\geq\\exp\\left(-\\gamma\\alpha\\frac{\\beta\\tilde{\\delta}^{2}}{-1-\\tilde{\\delta}}\\right)\\left(-2\\ln\\tilde{\\delta}\\right)^{-\\frac{\\beta\\tilde{\\delta}^{2}}{-1-\\tilde{\\delta}^{2}}}}\\\\ &{\\geq\\exp\\left(-2\\gamma\\frac{\\beta\\tilde{\\delta}^{2}}{-1-\\tilde{\\delta}}-2e^{-1}\\beta\\tilde{\\delta}^{2}\\right)}\\\\ &{\\geq\\exp\\left(-\\left(\\frac{2\\gamma}{3\\ln2}+\\frac{2}{3c}\\right)\\beta\\tilde{\\delta}^{2}\\right)}\\\\ &{\\geq\\exp\\left(-0.81\\cdot\\beta^{2}\\right)}\\\\ &{>e^{-\\tilde{\\delta}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "since $1<\\alpha\\leq2,0<\\tilde{\\delta}\\leq1/3,\\beta=e^{-4.2}$ and $0<\\tilde{\\varepsilon}\\le1$ , where $\\gamma$ is the Euler-Mascheroni constant. Hence, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\sum_{i}\\tilde{T}_{i}^{-\\alpha}\\le\\frac{e^{-\\tilde{\\varepsilon}/2}}{\\alpha-1}\\right)\\le\\frac{\\tilde{\\delta}}{3}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We then bound the right tail of $\\sum_{i}\\tilde{T}_{i}^{-\\alpha}$ . Unfortunately, the Laplace functional (10) does not work since the integral diverges for  small $t$ . Therefore, we have to bound $t$ away from 0. Note that $\\operatorname*{min}_{i}\\tilde{T}_{i}\\sim\\mathrm{Exp}(1)$ , and hence ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(\\operatorname*{min}_{i}\\tilde{T}_{i}\\leq\\tilde{\\delta}/3)\\leq\\tilde{\\delta}/3.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Write $\\tau=\\tilde{\\delta}/3$ . Using the Laplace functional again, for $w>0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp\\left(w\\displaystyle\\sum_{i\\neq j>0}\\bar{T}_{i}^{-\\alpha}\\right)\\right]}\\\\ &{=\\exp\\left(-\\int_{\\tau}^{\\infty}(1-\\exp(w t^{-\\alpha}))\\mathrm{d}t\\right)}\\\\ &{=\\exp\\left(\\int_{\\tau}^{\\infty}(\\exp(w t^{-\\alpha})-1)\\mathrm{d}t\\right)}\\\\ &{\\leq\\exp\\left(\\int_{\\tau}^{\\infty}(\\exp(w\\tau^{-\\alpha})-1)\\frac{t^{-\\alpha}}{\\tau^{-\\alpha}}\\mathrm{d}t\\right)}\\\\ &{=\\exp\\left(\\frac{\\exp(w\\tau^{-\\alpha})-1}{\\tau^{-\\alpha}}\\cdot\\frac{\\tau^{-(\\alpha-1)}}{-1}\\right)}\\\\ &{=\\exp\\left(\\frac{\\tau(\\exp(w\\tau^{-\\alpha})-1)}{\\alpha-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, by Chernoff bound, for $d\\geq0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\Big(\\sum_{t=1}^{\\infty}\\widehat{T}_{t}^{-\\alpha}\\geq d\\Big)}\\\\ &{\\leq\\frac{1}{w^{2}}\\exp\\left(-w d+\\frac{\\tau\\left(\\exp\\left(w\\tau^{-\\alpha}\\right)-1\\right)}{\\alpha-1}\\right)}\\\\ &{\\leq\\exp\\left(-d^{\\alpha}\\ln\\left(d(\\alpha-1)\\gamma^{\\alpha-1}\\right)+\\frac{\\tau\\left(\\exp\\left(\\ln\\left(d(\\alpha-1)\\gamma^{\\alpha-1}\\right)\\right)-1\\right)}{\\alpha-1}\\right)}\\\\ &{=\\exp\\left(-d^{\\alpha}\\ln(d(\\alpha-1)\\gamma^{\\alpha-1})+\\frac{\\tau\\left(d(\\alpha-1)\\gamma^{\\alpha-1}-1\\right)}{\\alpha-1}\\right)}\\\\ &{=\\exp\\left(-\\frac{c\\tau}{\\alpha-1}\\ln{c}+\\frac{c\\tau-1}{\\alpha-1}\\right)}\\\\ &{=\\exp\\left(-\\frac{\\tau}{\\alpha-1}(c\\ln{c}-c+1)\\right)}\\\\ &{\\leq\\exp\\left(-\\frac{\\tau}{\\alpha-1}\\ln(c-1)(c-1)^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\nc:=d(\\alpha-1)\\tau^{\\alpha-1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the last inequality holds whenever $c\\in[1,2]$ since in this range, ", "page_idx": 21}, {"type": "equation", "text": "$$\nc\\ln c-c+1\\geq(2\\ln2-1)(c-1)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Substituting ", "page_idx": 21}, {"type": "equation", "text": "$$\nd=\\frac{e^{\\tilde{\\varepsilon}/2}}{\\alpha-1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we have $c=e^{\\tilde{\\varepsilon}/2}\\tau^{\\alpha-1}$ . By (13), to guarantee $\\begin{array}{r}{\\operatorname*{Pr}(\\sum_{i:\\,\\tilde{T}_{i}>\\tau}\\tilde{T}_{i}^{-\\alpha}\\geq d)\\leq\\tilde{\\delta}/3=\\tau}\\end{array}$ , we require ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\tau(2\\ln2-1)(e^{\\tilde{\\varepsilon}/2}\\tau^{\\alpha-1}-1)^{2}}{\\alpha-1}\\geq-\\ln\\tau,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\ne^{\\tilde{\\varepsilon}/2}\\tau^{\\alpha-1}\\geq\\sqrt{\\frac{(\\alpha-1)(-\\ln{\\tau})}{\\tau(2\\ln{2}-1)}}+1.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Substituting (9), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e^{\\tilde{\\varepsilon}/2}\\tau^{\\alpha-1}\\geq e^{\\tilde{\\varepsilon}/2}\\tau^{\\frac{\\beta\\tilde{\\delta}\\tilde{\\varepsilon}^{2}}{1-\\tilde{\\varepsilon}}}}\\\\ &{\\qquad\\qquad=\\exp\\left(\\frac{\\tilde{\\varepsilon}}{2}+\\left(\\ln\\frac{\\tilde{\\delta}}{3}\\right)\\frac{\\beta\\tilde{\\delta}\\tilde{\\varepsilon}^{2}}{-\\ln\\tilde{\\delta}}\\right)}\\\\ &{\\qquad\\qquad\\geq\\exp\\left(\\frac{\\tilde{\\varepsilon}}{2}+\\left(2\\ln\\tilde{\\delta}\\right)\\frac{\\beta\\tilde{\\varepsilon}}{-3\\ln\\tilde{\\delta}}\\right)}\\\\ &{\\qquad\\qquad=\\exp\\left(\\tilde{\\varepsilon}\\left(\\frac{1}{2}-\\frac{2\\beta}{3}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "since $0\\,<\\,\\tilde{\\delta}\\,\\leq\\,1/3$ . Note that this also guarantees $c=e^{\\tilde{\\varepsilon}/2}\\tau^{\\alpha-1}\\in[1,2]$ since $\\beta\\,=\\,e^{-4.2}$ and $0<\\tilde{\\varepsilon}\\le1$ . We also have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\left(\\alpha-1\\right)\\left(-\\ln{\\tau}\\right)}{\\tau\\left(2\\ln{2}-1\\right)}\\leq\\frac{\\frac{\\beta\\delta\\tilde{\\varepsilon}^{2}}{-\\ln{\\delta}}\\left(-\\ln{\\tau}\\right)}{\\tau\\left(2\\ln{2}-1\\right)}}\\\\ &{\\phantom{\\frac{\\left(\\alpha-1\\right)\\left(-\\ln{\\tau}\\right)}{\\tau\\left(2\\ln{2}-1\\right)}}\\leq\\frac{\\frac{\\beta\\delta\\tilde{\\varepsilon}^{2}}{-\\ln{\\delta}}\\left(-2\\ln{\\tilde{\\delta}}\\right)}{\\left(\\tilde{\\delta}/3\\right)\\left(2\\ln{2}-1\\right)}}\\\\ &{\\phantom{\\frac{\\left(\\alpha-1\\right)\\left(-\\ln{\\tau}\\right)}{\\tau\\left(2\\ln{2}-1\\right)}}=\\frac{6\\beta\\tilde{\\varepsilon}^{2}}{2\\ln{2}-1}}\\\\ &{\\phantom{\\frac{\\left(\\alpha-1\\right)\\left(-\\ln{\\tau}\\right)}{\\tau\\left(2\\ln{2}-1\\right)}}\\leq16\\beta\\tilde{\\varepsilon}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\frac{\\left(\\alpha-1\\right)\\left(-\\ln{\\tau}\\right)}{\\tau\\left(2\\ln{2}-1\\right)}}+1\\leq4\\tilde{\\varepsilon}\\sqrt{\\beta}+1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\exp\\left(4\\tilde{\\varepsilon}\\sqrt{\\beta}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\stackrel{(a)}{\\leq}\\exp\\left(\\tilde{\\varepsilon}\\left(\\frac{1}{2}-\\frac{2\\beta}{3}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq e^{\\tilde{\\varepsilon}/2}\\tau^{\\alpha-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (a) is by $\\beta=e^{-4.2}$ . Hence (14) is satisfied, and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\,\\Big(\\sum_{i:\\,\\tilde{T}_{i}>\\tau}\\tilde{T}_{i}^{-\\alpha}\\geq\\frac{e^{\\tilde{\\varepsilon}/2}}{\\alpha-1}\\Big)\\leq\\frac{\\tilde{\\delta}}{3}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining this with (11) and (12), ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\Big(\\displaystyle\\sum_{i}\\tilde{T}_{i}^{-\\alpha}\\notin\\Big[\\frac{e^{-\\xi/2}}{\\alpha-1},\\,\\frac{e^{\\xi/2}}{\\alpha-1}\\Big]\\Big)}\\\\ &{\\leq\\operatorname*{Pr}\\Big(\\displaystyle\\sum_{i}\\tilde{T}_{i}^{-\\alpha}\\leq\\frac{e^{-\\xi/2}}{\\alpha-1}\\Big)+\\operatorname*{Pr}(\\operatorname*{min}_{i}\\tilde{T}_{i}\\leq\\tilde{\\delta}/3)}\\\\ &{\\quad+\\operatorname*{Pr}\\bigg(\\displaystyle\\sum_{i:\\,\\tilde{T}_{i}>\\tilde{\\delta}/3}\\tilde{T}_{i}^{-\\alpha}\\geq\\frac{e^{\\xi/2}}{\\alpha-1}\\bigg)}\\\\ &{\\leq\\tilde{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consider an $(\\varepsilon,\\delta)$ -differentially private mechanism $P_{Z|X}$ . Consider neighbors $x_{1},x_{2}$ , and let $P_{j}:=$ $\\begin{array}{r}{P_{Z|X}(\\cdot|x_{j}),\\,\\tilde{T}_{j,i}:=T_{i}/(\\frac{\\mathrm{d}P_{j}}{\\mathrm{d}Q}(Z_{i}))}\\end{array}$ , and $K_{j}$ be the output of PPR applied on $P_{j}$ , for $j=1,2$ . We ", "page_idx": 22}, {"type": "text", "text": "first consider the case $\\delta\\,=\\,0$ , which gives $\\begin{array}{r}{\\frac{\\mathrm{d}P_{1}}{\\mathrm{d}Q}(z)\\,\\le\\,e^{\\varepsilon}\\frac{\\mathrm{d}P_{2}}{\\mathrm{d}Q}(z)}\\end{array}$ for every $z$ . For any measurable $S\\subseteq\\mathcal{Z}^{\\infty}\\times\\mathbb{Z}_{>0}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}\\left[\\operatorname*{lim}_{t\\leq0}\\frac{\\mathbb{E}_{t}(\\cdot,\\tau)}{\\sum_{k\\geq0}^{T}\\sum_{i=1}^{T}\\tau_{k-1}^{(i)}}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\bigg|\\sum_{i=1}^{T}\\sum_{j=i,j=1}^{T}\\frac{\\mathbb{E}_{t}^{(j)}}{|\\alpha_{j}|^{\\alpha_{i}}}\\frac{\\mathbb{E}_{t}^{(j)}}{\\sum_{k\\geq0}^{T}-1}\\right]\\sin\\bigg\\{\\sum_{i=1}^{T}\\sum_{j=1}^{T}\\frac{\\mathbb{E}_{t}^{(j)}}{|\\alpha_{j}|^{\\alpha_{i}}}\\frac{1}{\\sum_{k=1}^{T}}\\bigg\\}\\right]+\\beta}\\\\ &{\\leq\\mathbb{E}\\left[\\operatorname*{lim}_{t\\leq0}\\frac{\\sum_{k\\geq0}^{T}\\sqrt{T}(\\frac{\\eta_{k-1}}{\\sum_{k\\geq0}^{T}}-1)}{\\sum_{k\\geq0}^{T}\\sum_{i=1}^{T}\\sum_{p=1}^{T}\\tau_{k-1}^{(i)}}\\right]+\\mathbb{E}\\left[\\vphantom{\\frac{(\\eta_{k-1})^{2}}{\\sum_{k\\geq0}^{T}\\sum_{i=1}^{T}\\tau_{k-1}^{(i)}}}\\right]+\\mathbb{E}\\left[\\vphantom{\\frac{(\\eta_{k-1})^{2}}{\\sum_{k\\geq0}^{T}\\sum_{i=1}^{T}\\tau_{k\\geq0}^{(i)}}}\\right]}\\\\ &{=\\mathbb{E}\\left[\\operatorname*{lim}_{t\\leq0}\\frac{\\sum_{k\\geq0}^{T}(\\frac{\\eta_{k-1}}{\\sum_{k\\geq0}^{T}\\sum_{i=1}^{T}(\\frac{\\eta_{k-1}}{\\sum_{k\\geq0}^{T}}-1)})}{\\sum_{k\\geq0}^{T}\\bigg|\\alpha_{j}\\bigg|\\alpha_{k-1}^{(1)}}\\right]+\\mathbb{E}\\bigg[\\bigg|\\alpha_{1}^{(2)}}\\\\ &{\\leq\\mathbb{E}\\left[\\operatorname*{lim}_{t\\leq0}\\frac{\\sum_{k\\geq0}^{T}(\\frac{\\eta_{k-1}}{\\sum_{k\\geq0}^{T}\\sum_{i=1}^{T}\\tau_{k-1}^{(i)}})}{\\sum_\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence PPR is $(\\alpha\\varepsilon+\\tilde{\\varepsilon},\\,2\\tilde{\\delta})$ -differentially private. ", "page_idx": 23}, {"type": "text", "text": "For the case $\\delta>0$ , by the definition of $(\\varepsilon,\\delta)$ -differential privacy, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int\\operatorname*{max}\\left\\{\\frac{\\mathrm{d}P_{1}}{\\mathrm{d}Q}(z)-e^{\\varepsilon}\\frac{\\mathrm{d}P_{2}}{\\mathrm{d}Q}(z),\\,0\\right\\}Q(\\mathrm{d}z)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $P_{3}$ be a probability measure that satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{\\frac{\\mathrm{d}P_{1}}{\\mathrm{d}Q}(z),\\,e^{\\varepsilon}\\frac{\\mathrm{d}P_{2}}{\\mathrm{d}Q}(z)\\right\\}\\leq\\frac{\\mathrm{d}P_{3}}{\\mathrm{d}Q}(z)\\leq e^{\\varepsilon}\\frac{\\mathrm{d}P_{2}}{\\mathrm{d}Q}(z),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for every $z$ . Such $P_{3}$ can be constructed by taking an appropriate convex combination of the lower bound above (which integrates to $\\leq1$ ) and the upper bound above (which integrates to $\\geq1$ ) such that $P_{3}$ integrates to 1. We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int\\operatorname*{max}\\left\\{\\frac{\\mathrm{d}P_{1}}{\\mathrm{d}Q}(z)-\\frac{\\mathrm{d}P_{3}}{\\mathrm{d}Q}(z),\\,0\\right\\}Q(\\mathrm{d}z)\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and hence the total variation distance $d_{\\mathrm{TV}}(P_{1},P_{3})$ between $P_{1}$ and $P_{3}$ is at most $\\delta$ . Let $K_{3}$ be the output of PPR applied on $P_{3}$ . ", "page_idx": 23}, {"type": "text", "text": "In the proof of Theorem F.1, we see that PPR has the following equivalent formulation. Let $(T_{i})_{i}\\sim$ $\\mathrm{PP}(1)$ be a Poisson process with rate 1, independent of $Z_{1},Z_{2},\\ldots\\stackrel{i i d}{\\sim}Q$ . Let $R_{i}:=(\\mathrm{d}P/\\mathrm{d}Q)(Z_{i})$ , and let its probability measure be $P_{R}$ . Let $V_{1},V_{2},\\ldots\\stackrel{i i d}{\\sim}\\mathrm{Exp}(1)$ . PPR can be equivalently expressed as ", "page_idx": 24}, {"type": "equation", "text": "$$\nK=\\mathop{\\mathrm{argmin}}_{k}T_{k}^{\\alpha}R_{k}^{-\\alpha}V_{k}=\\mathop{\\mathrm{argmin}}_{k}\\frac{T_{k}V_{k}^{1/\\alpha}}{R_{k}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $\\begin{array}{r}{(T_{i}V_{i}^{1/\\alpha})_{i}\\sim\\operatorname{PP}(\\int_{0}^{\\infty}v^{-1/\\alpha}e^{-v}\\mathrm{d}v)=\\operatorname{PP}(\\Gamma(1-\\alpha^{-1}))}\\end{array}$ is a uniform Poisson process. Therefore PPR is the same as the Poisson functional representation [61, 60] applied on $(T_{i}V_{i}^{1/\\alpha})_{i}$ . By the grand coupling property of Poisson functional representation [60, 59] (see [59, Theorem 3]), if we apply the Poisson functional representation on $P_{1}$ and $P_{3}$ to get $K_{1}$ and $K_{3}$ respectively, then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(K_{1}\\neq K_{3})\\le2d_{\\mathrm{TV}}(P_{1},P_{3})\\le2\\delta.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, for any measurable $S\\subseteq\\mathcal{Z}^{\\infty}\\times\\mathbb{Z}_{>0}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(((Z_{i})_{i},K_{1})\\in\\mathcal{S}\\right)}\\\\ &{\\le\\operatorname*{Pr}\\left(((Z_{i})_{i},K_{3})\\in\\mathcal{S}\\right)+2\\delta}\\\\ &{\\le e^{\\alpha\\varepsilon+\\tilde{\\varepsilon}}\\operatorname*{Pr}\\left(((Z_{i})_{i},K_{2})\\in\\mathcal{S}\\right)+2\\tilde{\\delta}+2\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality is by applying (15) on $P_{3},P_{2}$ instead of $P_{1},P_{2}$ . This completes the proof. ", "page_idx": 24}, {"type": "text", "text": "F Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We now bound the size of the index output by the Poisson private representation. The following is a refined version of Theorem 4.3. ", "page_idx": 24}, {"type": "text", "text": "Theorem F.1. For PPR with parameter $\\alpha>1$ , when the encoder is given the input $x$ , the message $K$ given by PPR satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\log K]\\leq D(P\\|Q)}\\\\ &{\\qquad\\qquad+\\operatorname*{inf}_{\\eta\\in(0,1]\\cap(0,\\alpha-1)}\\frac{1}{\\eta}\\log\\left(\\frac{\\Gamma\\left(1-\\frac{\\eta+1}{\\alpha}\\right)\\Gamma\\left(\\eta+1\\right)}{(\\Gamma(1-\\frac{1}{\\alpha}))^{\\eta+1}}+1\\right)}\\\\ &{\\qquad\\qquad\\leq D(P\\|Q)+\\frac{\\log(3.56)}{\\operatorname*{min}\\left\\{(\\alpha-1)/2,1\\right\\}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $P:=P_{Z|X}(\\cdot|x)$ . ", "page_idx": 24}, {"type": "text", "text": "Note that for $\\alpha=\\infty$ , (16) with $\\eta=1$ gives $\\mathbb{E}[\\log K]\\leq D(P\\|Q)+\\log2$ , recovering the bound in [58] (which strengthened [61]). ", "page_idx": 24}, {"type": "text", "text": "Proof. Write $(X_{i})_{i}\\sim\\operatorname{PP}(\\mu)$ if the points $(X_{i})_{i}$ (as a multiset, ignoring the ordering) form a Poisson point process with intensity measure $\\mu$ . Similarly, for $f:[0,\\infty)^{n}\\rightarrow[0,\\infty)$ , we write $\\operatorname{PP}(f)$ for the Poisson point process with intensity function $f$ (i.e., the intensity measure has a Radon-Nikodym derivative $f$ against the Lebesgue measure). Let $(T_{i})_{i}\\sim\\operatorname{PP}(1)$ be a Poisson process with rate 1, independent of $Z_{1},Z_{2},\\ldots\\stackrel{i i d}{\\sim}Q$ . Let $R_{i}:=(\\mathrm{d}P/\\mathrm{d}Q)(Z_{i})$ , and let its probability measure be $P_{R}$ . We have $\\tilde{T}_{i}=T_{i}/R_{i}$ . Let $V_{1},V_{2},\\ldots\\stackrel{i i d}{\\sim}\\mathrm{Exp}(1)$ . By the property of exponential random variables, for any $p_{1},p_{2},\\dotsc\\geq0$ with $\\textstyle\\sum_{i}p_{i}<\\infty$ , we have $\\begin{array}{r}{\\mathrm{Pr}(\\mathrm{argmin}_{k}V_{k}/p_{k}=\\bar{k})=p_{k}/\\sum_{i}p_{i}}\\end{array}$ . Therefore, PPRF can be equivalently e xpressed as ", "page_idx": 24}, {"type": "equation", "text": "$$\nK=\\underset{k}{\\mathrm{argmin}}T_{k}^{\\alpha}R_{k}^{-\\alpha}V_{k}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the marking theorem [57], $(T_{i},R_{i},V_{i})_{i}$ is a Poisson process over $[0,\\infty)^{3}$ with intensity measure ", "page_idx": 24}, {"type": "equation", "text": "$$\n(T_{i},R_{i},V_{i})_{i}\\sim\\mathrm{PP}\\left(e^{-v}P_{R}(r)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the mapping theorem [57], letting $W_{i}:=T_{i}^{\\alpha}R_{i}^{-\\alpha}V_{i}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n(T_{i},R_{i},W_{i})_{i}\\sim\\mathrm{PP}\\left(r^{\\alpha}t^{-\\alpha}e^{-w r^{\\alpha}t^{-\\alpha}}P_{R}(r)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Again by the mapping theorem, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(W_{i})_{i}\\sim\\operatorname*{PP}\\left(\\mathbb{E}_{R\\sim P_{R}}\\left[\\int_{0}^{\\infty}R^{\\alpha}t^{-\\alpha}e^{-w R^{\\alpha}t^{-\\alpha}}\\mathrm{d}t\\right]\\right)}\\quad}&{}\\\\ &{=\\operatorname*{PP}\\left(\\mathbb{E}\\left[\\alpha^{-1}(w R^{\\alpha})^{1/\\alpha-1}\\Gamma(1-\\alpha^{-1})R^{\\alpha}\\right]\\right)}\\\\ &{=\\operatorname*{PP}\\left(\\mathbb{E}\\left[\\alpha^{-1}w^{1/\\alpha-1}\\Gamma(1-\\alpha^{-1})R\\right]\\right)}\\\\ &{=\\operatorname*{PP}\\left(\\alpha^{-1}w^{1/\\alpha-1}\\Gamma(1-\\alpha^{-1})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "since $\\begin{array}{r}{\\mathbb{E}[R]=\\int(\\mathrm{d}P/\\mathrm{d}Q)(z)Q(\\mathrm{d}z)=1}\\end{array}$ . Recall that $W_{K}=\\operatorname*{min}_{i}W_{i}$ by the definition of $K$ . We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(W_{K}>w)=\\exp\\left(-\\displaystyle\\int_{0}^{w}\\alpha^{-1}v^{1/\\alpha-1}\\Gamma(1-\\alpha^{-1})\\mathrm{d}v\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\exp\\left(-w^{1/\\alpha}\\Gamma(1-\\alpha^{-1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence the probability density function of $W_{K}$ is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;\\frac{\\mathrm{d}}{\\mathrm{d}w}\\exp\\left(-w^{1/\\alpha}\\Gamma(1-\\alpha^{-1})\\right)}\\\\ &{=\\alpha^{-1}w^{1/\\alpha-1}\\Gamma(1-\\alpha^{-1})\\exp\\left(-w^{1/\\alpha}\\Gamma(1-\\alpha^{-1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By (18), the Radon-Nikodym derivative between the conditional distribution of $R_{K}$ given $W_{K}=w$ and $P_{R}$ is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(R_{K}\\in[r,r+\\mathrm{d}r)\\,|\\,W_{K}=w)/P_{R}(\\mathrm{d}r)}\\\\ &{=\\frac{\\int_{0}^{\\infty}r^{\\alpha}t^{-\\alpha}e^{-w r^{\\alpha}t^{-\\alpha}}\\mathrm{d}t}{\\mathbb{E}_{R\\sim P_{R}}\\,\\left[\\int_{0}^{\\infty}R^{\\alpha}t^{-\\alpha}e^{-w R^{\\alpha}t^{-\\alpha}}\\mathrm{d}t\\right]}}\\\\ &{=\\frac{\\alpha^{-1}w^{1/\\alpha-1}\\Gamma(1-\\alpha^{-1})r}{\\alpha^{-1}w^{1/\\alpha-1}\\Gamma(1-\\alpha^{-1})}}\\\\ &{=r}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "does not depend on $w$ . Hence $R_{K}$ is independent of $W_{K}$ . By (18), for $0\\leq\\eta<\\alpha-1$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[T_{K}^{\\eta}\\,\\vert\\,R_{K}=r,\\,W_{K}=w]}\\\\ &{\\,\\,=\\frac{\\int_{0}^{\\infty}t^{\\eta}r^{\\alpha}t^{-\\alpha}e^{-w r^{\\alpha}t^{-\\alpha}}\\mathrm{d}t}{\\int_{0}^{\\infty}r^{\\alpha}t^{-\\alpha}e^{-w r^{\\alpha}t^{-\\alpha}}\\mathrm{d}t}}\\\\ &{\\,\\,=\\frac{\\alpha^{-1}w^{(\\eta+1)/\\alpha-1}\\Gamma(1-(\\eta+1)\\alpha^{-1})r^{\\eta+1}}{\\alpha^{-1}w^{1/\\alpha-1}\\Gamma(1-\\alpha^{-1})r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $R_{K}$ is independent of $W_{K}$ , using (20) and (19), for $\\eta\\in(0,1]\\cap(0,\\alpha-1)$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[T_{K}^{\\eta}\\mid R_{K}=r]}\\\\ &{\\;=\\int_{0}^{\\infty}\\alpha^{-1}w^{(\\eta+1)/\\alpha-1}\\Gamma(1-(\\eta+1)\\alpha^{-1})r^{\\eta}\\exp\\left(-w^{1/\\alpha}\\Gamma(1-\\alpha^{-1})\\right)\\mathrm{d}w}\\\\ &{\\;=r^{\\eta}\\Gamma(1-(\\eta+1)\\alpha^{-1})\\displaystyle\\int_{0}^{\\infty}\\alpha^{-1}w^{(\\eta+1)/\\alpha-1}\\exp\\left(-w^{1/\\alpha}\\Gamma(1-\\alpha^{-1})\\right)\\mathrm{d}w}\\\\ &{\\;=r^{\\eta}\\Gamma(1-(\\eta+1)\\alpha^{-1})(\\Gamma(1-\\alpha^{-1}))^{-(\\eta+1)}\\Gamma(\\eta+1)}\\\\ &{\\;=:c_{\\alpha,\\eta}r^{\\eta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{\\alpha,\\eta}:=\\Gamma(1-(\\eta+1)\\alpha^{-1})(\\Gamma(1-\\alpha^{-1}))^{-(\\eta+1)}\\Gamma(\\eta+1).\\,\\mathrm{I}\\,}\\\\ &{\\qquad\\qquad\\qquad\\quad\\mathbb{E}[\\log(T_{K}+1)\\,|\\,R_{K}=r]}\\\\ &{\\qquad\\qquad\\qquad\\quad\\leq\\mathbb{E}[\\log((T_{K}^{\\eta}+1)^{1/\\eta})\\,|\\,R_{K}=r]}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\mathbb{E}[\\eta^{-1}\\log(T_{K}^{\\eta}+1)\\,|\\,R_{K}=r]}\\\\ &{\\qquad\\qquad\\qquad\\quad\\leq\\eta^{-1}\\log(c_{\\alpha,\\eta}r^{\\eta}+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that ", "page_idx": 26}, {"type": "equation", "text": "$$\nK-1=|\\{i:\\,T_{i}<T_{K}\\}|\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and hence the expecation of $K\\!-\\!1$ given $T_{K}$ should be around $T_{K}$ . This is not exact since conditioning on $T_{K}$ changes the distribution of the process $(T_{i},R_{i},V_{i})_{i}$ . To resolve this problem, we define a new process $(T_{i}^{\\prime},R_{i}^{\\prime},V_{i}^{\\prime})_{i}$ which includes all points in $(T_{i},\\dot{R}_{i},V_{i})_{i}$ excluding the point $(T_{K},R_{K},V_{K})$ , together with newly generated points according to ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{PP}\\left(e^{-v}P_{R}(r)\\mathbf{1}\\{t^{\\alpha}r^{-\\alpha}v<T_{K}^{\\alpha}R_{K}^{-\\alpha}V_{K}\\}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Basically, $\\{t^{\\alpha}r^{-\\alpha}v\\,<\\,T_{K}^{\\alpha}R_{K}^{-\\alpha}V_{K}\\}$ is the \u201cimpossible region\u201d where the points in $(T_{i},R_{i},V_{i})_{i}$ cannot be located in, since $K$ attains the minimum of $T_{K}^{\\alpha}R_{K}^{-\\alpha}V_{K}$ . The new process $(T_{i}^{\\prime},R_{i}^{\\prime},V_{i}^{\\prime})_{i}$ removes the point $(T_{K},R_{K},V_{K})$ , and then fills in the impossible region. It is straightforward to check that $(T_{i}^{\\prime},R_{i}^{\\prime},V_{i}^{\\prime})_{i}\\sim\\mathrm{PP}(e^{-v}P_{R}(r))$ , independent of $(T_{K},R_{K},\\bar{V}_{K})$ . We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[K\\,|\\,T_{K}]}\\\\ &{\\,=\\mathbb{E}\\left[\\left\\vert\\left\\{i:\\:T_{i}<T_{K}\\right\\}\\right\\vert\\,\\Big\\vert\\:T_{K}\\right]+1}\\\\ &{\\,\\le\\mathbb{E}\\left[\\left\\vert\\left\\{i:\\:T_{i}^{\\prime}<T_{K}\\right\\}\\right\\vert\\,\\Big\\vert\\:T_{K}\\right]+1}\\\\ &{\\,=T_{K}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, by (22) and Jensen\u2019s inequality, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\log K\\}}\\\\ &{=\\mathbb{E}\\{\\left\\|\\log K\\right\\|T_{K}\\}}\\\\ &{\\leq\\mathbb{E}\\left[\\log(T_{K}+1)\\right]}\\\\ &{=\\mathbb{E}\\left[\\log(T_{K}+1)\\left|\\;K_{K}\\right|\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\eta^{-1}\\log(c_{o,\\eta}R_{K}^{\\prime}+1)\\right]}\\\\ &{=\\eta^{-1}\\mathbb{E}_{\\mathcal{Z}\\sim\\mathcal{P}}\\left[\\log\\left(c_{o,\\eta}\\left(\\frac{\\mathrm{d}P}{\\mathrm{d}Q}(Z)\\right)^{\\eta}+1\\right)\\right]}\\\\ &{=\\eta^{-1}\\mathbb{E}\\left[\\log\\left(\\frac{\\left(\\mathrm{d}P\\right)(Z)}{\\mathrm{d}Q}(Z)\\right)^{\\eta}\\right]+\\eta^{-1}\\mathbb{E}\\left[\\log\\left(c_{o,\\eta}+\\left(\\frac{\\mathrm{d}P}{\\mathrm{d}Q}(Z)\\right)^{-\\eta}\\right)\\right]}\\\\ &{\\leq D(P\\|Q)+\\eta^{-1}\\log\\left(c_{o,\\eta}+\\left(\\mathbb{E}\\left[\\left(\\frac{\\mathrm{d}P}{\\mathrm{d}Q}(Z)\\right)^{-1}\\right]\\right)^{\\eta}\\right)}\\\\ &{\\leq D(P\\|Q)+\\eta^{-1}\\log\\left(c_{o,\\eta}+1\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last line is due to $\\mathbb{E}[((\\mathrm{d}P/\\mathrm{d}Q)(Z))^{-1}]\\;=\\;\\int((\\mathrm{d}P/\\mathrm{d}Q)(Z))^{-1}P(\\mathrm{d}z)\\;\\le\\;1$ (this step appeared in [58]). The bound (16) follows from minimizing over $\\eta\\in(0,1]\\cap(0,\\alpha-1)$ . ", "page_idx": 26}, {"type": "text", "text": "To show (17), substituting $\\eta=\\operatorname*{min}\\{(\\alpha-1)/2,1\\}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{c_{\\alpha,\\eta}=\\frac{\\Gamma\\left(1-(\\eta+1)\\alpha^{-1}\\right)\\Gamma\\left(\\eta+1\\right)}{\\left(\\Gamma\\left(1-\\alpha^{-1}\\right)\\right)^{\\eta+1}}}}\\\\ &{\\stackrel{(a)}{\\leq}\\frac{\\left(1-\\alpha^{-1}\\right)^{\\eta+1}}{0.885^{\\eta+1}\\cdot(1-(\\eta+1)\\alpha^{-1})}}\\\\ &{}&{\\leq\\frac{(1-\\alpha^{-1})^{\\eta+1}}{0.885^{2}\\cdot(1-((\\alpha-1)/2+1)\\alpha^{-1})}}\\\\ &{}&{=\\frac{2}{0.885^{2}}(1-\\alpha^{-1})^{\\eta}}\\\\ &{\\leq2.56,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where (a) is because $0.885\\leq x\\Gamma(x)=\\Gamma(x+1)\\leq1$ for $0<x\\le1$ . Hence, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\log K]\\leq D(P\\|Q)+\\eta^{-1}\\log(c_{\\alpha,\\eta}+1),}\\\\ &{\\qquad\\qquad\\leq D(P\\|Q)+\\frac{\\log\\left(3.56\\right)}{\\operatorname*{min}\\left\\{(\\alpha-1)/2,1\\right\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "G Distributed Mean Estimation with R\u00e9nyi DP ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In many machine learning applications, privacy budgets are often accounted in the moment space, and one popular moment accountant is the R\u00e9nyi DP accountant. For completeness, we provide a R\u00e9nyi DP version of Corollary 5.2 in this section. We begin with the following definition of R\u00e9nyi DP: ", "page_idx": 27}, {"type": "text", "text": "Definition G.1 (R\u00e9nyi Differential privacy [1, 68]). Given a mechanism $\\boldsymbol{\\mathcal{A}}$ which induces the conditional distribution $P_{Z|X}$ of $Z\\,=\\,A(X)$ , we say that it satisfies $(\\gamma,\\varepsilon)$ - R\u00e9nyi DP if for any neighboring $(x,x^{\\prime})\\in\\mathcal{N}$ and $S\\subseteq{\\mathcal{Z}}$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\nD_{\\gamma}\\left(P_{Z|X=x}\\big|\\big|P_{Z|X=x^{\\prime}}\\right)\\le\\varepsilon,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\nD_{\\gamma}\\left(P\\|Q\\right):=\\frac{1}{\\gamma-1}\\log\\left(\\mathbb{E}_{Q}\\left[\\left(\\frac{P}{Q}\\right)^{\\gamma}\\right]\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "is the R\u00e9nyi divergence between $P$ and $Q$ . If $\\mathcal{N}\\,=\\,\\mathcal{X}^{2}$ , we say that the mechanism satisfies $(\\gamma,\\varepsilon)$ -local DP. ", "page_idx": 27}, {"type": "text", "text": "The following conversion lemma from [13] relates R\u00e9nyi DP to $(\\varepsilon_{\\mathsf{D P}}(\\delta),\\delta)$ -DP. ", "page_idx": 27}, {"type": "text", "text": "Lemma G.2. If $\\boldsymbol{\\mathcal{A}}$ satisfies $(\\gamma,\\varepsilon)$ -R\u00e9nyi $D P$ for some $\\gamma\\,\\geq\\,1$ , then, for any $\\delta\\:>\\:0_{!}$ , $\\boldsymbol{\\mathcal{A}}$ satisfies $(\\varepsilon_{\\mathsf{D P}}(\\delta),\\delta){\\cdot}D P,$ where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\varepsilon_{\\sf D P}(\\delta)=\\varepsilon+\\frac{\\log{(1/\\gamma\\delta)}}{\\gamma-1}+\\log(1-1/\\gamma).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The following theorem states that, when simulating the Gaussian mechanism, PPR satisfies the following both central and local DP guarantee: ", "page_idx": 27}, {"type": "text", "text": "Corollary G.3 (PPR-compressed Gaussian mechanism). Let $\\varepsilon\\;\\geq\\;0$ and $\\gamma\\,\\geq\\,1$ . Consider the Gaussian mechanism $\\begin{array}{r}{\\bar{P_{Z|X}}(\\cdot|x)=\\mathcal{N}(x,\\frac{\\sigma^{2}}{n}\\mathbb{I}_{d}),}\\end{array}$ , and the proposal distribution $\\begin{array}{r}{Q=\\mathcal{N}(0,(\\frac{C^{2}}{d}+}\\end{array}$ $\\textstyle{\\frac{\\sigma^{2}}{n}}\\bigr)\\mathbb{I}_{d})$ , where $\\begin{array}{r}{\\sigma\\geq\\sqrt{\\frac{C\\gamma}{2\\varepsilon}}}\\end{array}$ . For each client $i$ , let $Z_{i}$ be the output of PPR applied on $P_{Z|X}{\\left(\\cdot|X_{i}\\right)}$ . We have: ", "page_idx": 27}, {"type": "text", "text": "\u2022 $\\begin{array}{r}{\\hat{\\mu}(Z^{n}):=\\frac{1}{n}\\sum_{i}Z_{i}}\\end{array}$ yields an unbiased estimator of $\\begin{array}{r}{\\mu(X^{n})=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}}\\end{array}$ satisfying $(\\gamma,\\varepsilon)$ - (central) R\u00e9nyi DP and $(\\varepsilon_{\\mathsf{D P}}(\\delta),\\delta)$ -DP, where $\\varepsilon_{\\mathsf{D P}}(\\delta)$ is defined in (23). ", "page_idx": 27}, {"type": "text", "text": "\u2022 $P_{Z\\mid X_{i}}$ satisfies $(2\\alpha\\tilde{\\varepsilon}_{\\mathsf{D P}}(\\delta),2\\delta)$ -local $D P,$ where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\tilde{\\varepsilon}_{\\sf D P}(\\delta):=\\sqrt{n}\\varepsilon+\\frac{\\log{(1/\\gamma\\delta)}}{\\gamma-1}+\\log(1-1/\\gamma).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$\\hat{\\mu}(Z^{n})\\,h a s\\,M S E\\,\\mathbb{E}[\\left\\|\\mu-\\hat{\\mu}\\right\\|_{2}^{2}]=\\sigma^{2}d/n^{2}.$ ", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 The average per-client communication cost is at most $\\ell+\\log_{2}(\\ell+1)+2$ bits where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\ell:=\\frac{d}{2}\\log_{2}\\bigg(\\frac{C^{2}n}{d\\sigma^{2}}+1\\bigg)+\\eta_{\\alpha}\\ \\leq\\ \\frac{d}{2}\\log_{2}\\bigg(\\frac{n\\varepsilon^{2}}{2d\\ln(1.25/\\delta)}+1\\bigg)+\\eta_{\\alpha},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\eta_{\\alpha}:=(\\log_{2}(3.56))/\\operatorname*{min}\\{(\\alpha-1)/2,\\,1\\}.$ ", "page_idx": 27}, {"type": "text", "text": "Proof. The central DP guarantee follows from [68] and Lemma G.2. The local DP guarantee follows from Lemma G.2 and Theorem 4.8. Finally, the communication bound can be obtained from the same analysis as in Corollary 5.2. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "H Proof of Corollary 5.2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Consider the PPR applied on the Gaussian mechanism $\\begin{array}{r}{P_{Z|X}(\\cdot|x)=\\mathcal{N}(x,\\frac{\\sigma^{2}}{n}\\mathbb{I}_{d})}\\end{array}$ , with the proposal distribution $\\begin{array}{r}{Q=\\mathcal{N}(0,(\\frac{C^{2}}{d}+\\frac{\\sigma^{2}}{n})\\mathbb{I}_{d})}\\end{array}$ . PPR ensures that $Z_{i}$ follows the distribution $\\textstyle N(X_{i},{\\frac{\\sigma^{2}}{n}}\\mathbb{I}_{d})$ . Therefore the MSE is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\mu-\\hat{\\mu}\\right\\Vert_{2}^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-Z_{i})\\right\\Vert_{2}^{2}\\right]}\\\\ &{\\phantom{\\mathbb{E}\\left[\\left\\Vert\\mu-\\hat{\\mu}\\right\\Vert_{2}^{2}\\right]}=\\frac{1}{n}\\cdot d\\cdot\\frac{\\sigma^{2}}{n}}\\\\ &{\\phantom{\\mathbb{E}\\left[\\left\\Vert\\mu-\\hat{\\mu}\\right\\Vert_{2}^{2}\\right]}=\\frac{\\sigma^{2}d}{n^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For the compression size, for $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ with $\\|x\\|_{2}\\leq C$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D(P_{\\mathcal{Z}|X}(\\cdot\\vert x)\\vert Q)}\\\\ &{=\\mathbb{E}_{2\\sim P_{\\mathcal{Z}|X}(\\cdot\\vert x)}\\left[\\log\\frac{\\mathrm{d}P_{\\mathcal{Z}|X}(\\cdot\\vert x)}{\\mathrm{d}Q}(Z)\\right]}\\\\ &{=\\mathbb{E}_{2\\sim P_{\\mathcal{Z}|X}(\\cdot\\vert x)}\\left[\\log\\frac{\\left(2\\pi\\sigma^{2}/n\\right)^{d-2}\\exp(-\\frac{1}{2}\\vert\\vert Z-x\\vert\\vert_{2}^{2}/(\\sigma^{2}/n))}{(2\\pi(\\frac{C^{2}}{d}+\\frac{\\sigma^{2}}{n}))-d/2\\exp(-\\frac{1}{2}\\vert\\vert Z\\vert_{2}^{2}/(\\frac{C^{2}}{d}+\\frac{\\sigma^{2}}{n}))}\\right]}\\\\ &{=\\mathbb{E}_{2\\sim P_{\\mathcal{Z}|X}(\\cdot\\vert x)}\\left[\\frac{d}{2}\\log\\frac{C_{\\mathcal{I}}^{2}}{\\sigma^{2}/n}+\\frac{1}{2}\\left(\\frac{\\vert Z\\vert_{2}^{2}}{C_{\\mathcal{I}}^{2}}+\\frac{\\vert Z\\vert^{2}}{\\sigma^{2}/n}-\\frac{\\vert Z\\vert-x\\vert_{2}^{2}}{\\sigma^{2}/n}\\right)\\right]}\\\\ &{\\leq\\frac{d}{2}\\log\\frac{C_{\\mathcal{I}}^{2}}{\\sigma^{2}/n}+\\frac{1}{2}\\left(\\frac{C^{2}+\\sigma^{2}d/n}{\\frac{C^{2}}{d}+\\frac{\\sigma^{2}}{n}}-\\frac{\\sigma^{2}d/n}{\\sigma^{2}/n}\\right)}\\\\ &{=\\frac{d}{2}\\log\\left(\\frac{C^{2}n}{d\\sigma^{2}}+1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, by Theorem 4.3, the compression size is at most $\\ell+\\log_{2}(\\ell+1)+2$ bits, where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell:=\\displaystyle\\frac{d}{2}\\log_{2}\\left(\\frac{C^{2}n}{d\\sigma^{2}}+1\\right)+\\eta_{\\alpha}}\\\\ &{\\phantom{{=}}\\leq\\displaystyle\\frac{d}{2}\\log_{2}\\left(\\frac{n\\epsilon^{2}}{2d\\ln(1.25/\\delta)}+1\\right)+\\eta_{\\alpha}}\\\\ &{\\phantom{{=}}\\leq\\frac{n\\epsilon^{2}\\log_{2}(e)}{4\\ln(1.25/\\delta)}+\\eta_{\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\eta_{\\alpha}:=(\\log_{2}(3.56))/\\operatorname*{min}\\{(\\alpha-1)/2,\\,1\\}.$ . ", "page_idx": 28}, {"type": "text", "text": "The central-DP guarantee follows from $(\\varepsilon,\\delta)$ -DP of Gaussian mechanism [26, Appendix A] since the output distribution of PPR is exactly the same as the Gaussian mechanism, whereas the local-DP guarantee follows from Theorem 4.6 and [26, Appendix A]. ", "page_idx": 28}, {"type": "text", "text": "I Proof of Corollary 6.1 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Let $\\|X-Z\\|_{2}=R S$ where $R\\in[0,\\infty)$ is the magnitude of $X-Z$ , and $\\|S\\|_{2}=1$ . As shown in [33], $R$ follows the Gamma distribution with shape $d$ and scale $1/\\varepsilon$ . Hence the MSE is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|X-Z\\|_{2}^{2}\\right]=\\mathbb{E}\\left[R^{2}\\right]=\\left({\\frac{d}{\\varepsilon}}\\right)^{2}+{\\frac{d}{\\varepsilon^{2}}}={\\frac{d(d+1)}{\\varepsilon^{2}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The conditional differential entropy (in nats) of $Z$ given $X$ is ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(Z|X)=h(R)+h(S|R)}\\\\ &{\\qquad\\qquad=d+\\ln\\Gamma(d)-(d-1)\\psi(d)-\\ln\\varepsilon+\\mathbb{E}\\left[\\ln(n R^{d-1}\\mathrm{Vol}(B_{d}(1)))\\right]}\\\\ &{\\qquad\\qquad=d+\\ln\\Gamma(d)-(d-1)\\psi(d)-\\ln\\varepsilon+\\ln d+\\ln(\\mathrm{Vol}(B_{d}(1)))+(d-1)\\mathbb{E}\\left[\\ln R\\right]}\\\\ &{\\qquad\\quad=d+\\ln\\Gamma(d)-(d-1)\\psi(d)-\\ln\\varepsilon+\\ln d+\\frac{d}{2}\\ln\\pi-\\ln\\Gamma\\left(\\frac{d}{2}+1\\right)}\\\\ &{\\qquad\\qquad\\quad-(d-1)\\ln\\epsilon+(d-1)\\psi(d)}\\\\ &{\\qquad\\qquad\\quad=d\\ln\\frac{e\\sqrt{\\pi}}{\\varepsilon}+\\ln\\frac{d\\Gamma(d)}{\\Gamma(\\frac{d}{2}+1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\psi$ is the digamma function. Therefore, the KL divergence between $P_{Z|X}(\\cdot|x)$ and $Q$ (in nats) is ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D(P_{Z|X}(\\cdot|x)||Q)}\\\\ &{=-\\mathbb{E}_{Z\\sim P_{Z|X}(\\cdot|x)}\\left[\\ln\\left(\\left(2\\pi\\left(\\frac{C^{2}}{d}+\\frac{d+1}{\\varepsilon^{2}}\\right)\\right)^{-d/2}\\exp\\left(-\\frac{\\|Z\\|_{2}^{2}}{2\\left(\\frac{C^{2}}{d}+\\frac{d+1}{\\varepsilon^{2}}\\right)}\\right)\\right)\\right]-h(Z|X)}\\\\ &{=\\frac{d}{2}\\ln\\left(2\\pi\\left(\\frac{C^{2}}{d}+\\frac{d+1}{\\varepsilon^{2}}\\right)\\right)+\\frac{\\mathbb{E}_{Z\\sim P_{Z|X}(\\cdot|x)}\\left[\\|Z\\|_{2}^{2}\\right]}{2\\left(\\frac{d^{2}}{d}+\\frac{d+1}{\\varepsilon^{2}}\\right)}-d\\ln\\frac{e\\sqrt{\\pi}}{\\varepsilon}-\\ln\\frac{d\\Gamma(d)}{\\Gamma(\\frac{d}{2}+1)}}\\\\ &{\\leq\\frac{d}{2}\\ln\\left(2\\pi\\left(\\frac{C^{2}}{d}+\\frac{d+1}{\\varepsilon^{2}}\\right)\\right)+\\frac{C^{2}}{2\\left(\\frac{d^{2}+1}{d}+\\frac{d+1}{\\varepsilon^{2}}\\right)}-d\\ln\\frac{e\\sqrt{\\pi}}{\\varepsilon}-\\ln\\frac{d\\Gamma(d)}{\\Gamma(\\frac{d}{2}+1)}}\\\\ &{=\\frac{d}{2}\\ln\\left(\\frac{2}{e}\\left(\\frac{C^{2}\\varepsilon^{2}}{d}+d+1\\right)\\right)-\\ln\\frac{\\Gamma(d+1)}{\\Gamma(\\frac{d}{2}+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, by Theorem 4.3, the compression size is at most $\\ell+\\log_{2}(\\ell+1)+2$ bits. The metric privacy guarantee follows from Theorem 4.7. ", "page_idx": 29}, {"type": "text", "text": "J Experiments on Metric Privacy ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We use PPR to simulate the Laplace mechanism [3, 33, 34] $f_{Z|X}(z|x)\\,\\propto\\,e^{-\\varepsilon d_{X}(x,z)}$ discussed in Section 6. We consider $X\\ \\in\\ B_{d}(C)$ where $C\\;=\\;10000$ and $d\\,=\\,500$ . A large number of dimensions $d$ is common, for example, in privatizing word embedding vectors [33, 34]. We compare the performance of PPR-compressed Laplace mechanism (Corollary 6.1) with the discrete Laplace mechanism [3]. The discrete Laplace mechanism is described as follows (slightly modified from [3] to work for the $d$ -ball $B_{d}(C))$ : 1) generate a Laplace noise $Y$ with probability density function $f_{Y}(y)\\propto e^{-\\varepsilon\\|y\\|_{2}};2$ 2) compute ${\\hat{Z}}=X+Y;3$ ) truncate $\\hat{Z}$ to the closest point $Z$ in $B_{d}(C)$ ; and 4) quantize each coordinate of $Z$ by a quantizer with step size $u>0$ . The number of bits required by the discrete Laplace mechanism is $\\lceil\\bar{\\log}_{2}(\\mathrm{Vol}(\\mathcal{B}_{d}(C))/\\bar{u}^{d})\\rceil$ , where $\\mathrm{Vol}(B_{d}(C))/u^{d}$ is the number of quantization cells (hypercube of side length $u$ ) inside $B_{d}(C)$ . The parameter $u$ is selected to fit the number of bits allowed. ", "page_idx": 29}, {"type": "text", "text": "Figure 2 shows the mean squared error of PPR-compressed Laplace mechanism $\\stackrel{.}{\\alpha}=2$ ) and the discrete Laplace mechanism for different $\\varepsilon$ \u2019s, when the number of bits is limited to 500, 1000 and 1500.16 We can see that PPR performs better for larger $\\epsilon$ or smaller MSE, whereas the discrete Laplace mechanism performs better for smaller $\\epsilon$ or larger MSE. The performance of discrete Laplace mechanism for smaller $\\epsilon$ is due to the truncation step which limits $Z$ to $B_{d}(C)$ , which reduces the error at the expense of introducing distortion to the distribution of $Z$ , and making $Z$ a biased estimate of $X$ . In comparison, PPR preserves the Laplace conditional distribution $f_{Z\\mid X}$ exactly, and hence produces an unbiased $Z$ . ", "page_idx": 29}, {"type": "image", "img_path": "CgGjT8EG8A/tmp/146d9a156aa666e1632584bff84a304bfcf2da2b73c42cc72851e4ff594b06d7.jpg", "img_caption": ["Figure 2: MSE of PPR-compressed Laplace mechanism and discrete Laplace mechanism [3] for different $\\varepsilon$ \u2019s. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "K Running Time of PPR ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "As discussed in Section 7, we can ensure an $O(d)$ running time for the Gaussian mechanism by using the sliced PPR, where the $d$ -dimensional vector $X$ is divided into $\\lceil d/d_{\\mathrm{chunk}}\\rceil$ chunks, each with a fixed dimension $d_{\\mathrm{chunk}}$ (possibly except the last chunk if $d_{\\mathrm{chunk}}$ is not a factor of $d$ ). The average total running time is $\\lceil d/d_{\\mathrm{chunk}}\\rceil T_{\\mathrm{chunk}}$ , where $T_{\\mathrm{chunk}}$ is the average running time of PPR applied on one chunk.17 Therefore, to study the running time of the sliced PPR, we study how $T_{\\mathrm{chunk}}$ depend on $d_{\\mathrm{chunk}}$ . ", "page_idx": 30}, {"type": "text", "text": "In Figure 3 we show the running time $T_{\\mathrm{chunk}}$ of PPR applied on one chunk with dimension $d_{\\mathrm{chunk}}$ , where $d_{\\mathrm{chunk}}$ ranges from 40 to 110.18 With $d\\,=\\,1000$ , $n\\,=\\,500$ , $\\varepsilon\\,=\\,0.05$ and $\\delta\\,=\\,10^{-6}$ , we require a Gaussian mechanism with noise $\\mathcal{N}(0,n\\tilde{\\sigma}^{2}\\mathbb{I}_{d_{\\mathrm{chunk}}})$ where $\\tilde{\\sigma}=1.0917$ at each user in order to ensure $(\\varepsilon,\\delta)$ -central DP. We record the mean $T_{\\mathrm{chunk}}$ and the standard error of the mean19 of the running time of PPR applied to simulate this Gaussian mechanism (averaged over 20000 trials). ", "page_idx": 30}, {"type": "image", "img_path": "CgGjT8EG8A/tmp/ef5677f4a6e0886c3561aa21e2471b5de1eb3d757c83bdc1e02d04486e1aa846.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 3: Average running time of PPR applied to a chunk of dimension $d_{\\mathrm{chunk}}$ , with error bars indicating the interval $T_{\\mathrm{chunk}}\\pm2\\sigma_{\\mathrm{mean}}$ , where $T_{\\mathrm{chunk}}$ is the sample mean of the running time, and $\\sigma_{\\mathrm{mean}}$ is the standard error of the mean (see Footnote 19). ", "page_idx": 31}, {"type": "text", "text": "K.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We plot the average running time (over 20000 trials for each data point) against the values of $\\epsilon\\in[0.06,10]$ , with $d_{\\mathrm{chunk}}$ always chosen to be 4. The average running time is denoted as $T_{\\mathrm{chunk}}$ , and the standard error of the mean is given by $\\sigma_{\\mathrm{mean}}=\\sigma_{\\mathrm{time}}/\\sqrt{n_{\\mathrm{trials}}}$ , where $\\sigma_{\\mathrm{time}}$ is the standard deviation of the running time among the $\\sigma_{\\mathrm{time}}=20000$ trials. ", "page_idx": 31}, {"type": "image", "img_path": "CgGjT8EG8A/tmp/6691590508325da9b351e921da33db347d211a863d7caa936bac97711d840615.jpg", "img_caption": ["Figure 4: Average running time (over 20000 trials), $d_{\\mathrm{chunk}}=4$ and $\\varepsilon\\in[0.06,10]$ , with error bars indicating the interval $T_{\\mathrm{chunk}}\\pm2\\sigma_{\\mathrm{mean}}$ , where $T_{\\mathrm{chunk}}$ is the sample mean of the running time, and $\\sigma_{\\mathrm{mean}}$ is the standard error of the mean. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "L MSE against Compression Size ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We plot the MSE against the compression size (ranging from 25 to 1000 bits) for $\\epsilon\\ \\ \\in$ $\\{0.2\\bar{5},0.5,1.0,2.0\\}$ in the following figure. ", "page_idx": 31}, {"type": "image", "img_path": "CgGjT8EG8A/tmp/320ee1a5f2a28aca24c15d374a713e9c7ee0569acd3725e050785670e6f07a7f.jpg", "img_caption": ["Figure 5: The MSE of PPR and CSGM against the compression size in bits, where $\\varepsilon$ is chosen from $\\{0.25,0.5,1.0,2.0\\}$ and compression sizes vary from 25 to 1000 bits. Note that parts of the curves for PPR are flat, because a lower compression size is already sufficient for PPR to exactly simulate the best Gaussian mechanism for that value of $\\varepsilon$ , so a higher compression size than necessary will not affect the result. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have clearly stated our paper\u2019s contributions and scope in both the abstract and introduction. We try to answer the fundamental question: how can we efficiently communicate privatized data? The main contribution is our novel \u201cDP mechanism compressor\u201d, whose main advantages: universality, exactness and communication efficiency, have been elaborated in the introduction. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The main limitation of the proposed method is that it has a running time exponential with respect to the mutual information, though this is not an obstacle for simulating local DP mechanism (where the mutual information must be small). This is elaborated in Section 8. Note that there is no limitation on the type of DP mechanism that can be compressed by PPR (it can compress any DP mechanism to almost the theoretically smallest size). See Section 8 for details. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 33}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All theorems are precise mathematical statements, with all necessary assumptions included in either the theorem statement or the definition of PPR. Complete proofs are included in the appendices. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The detailed pseudocode of the PPR method is given in Algorithm 1 in the appendix, and the implementation together with the codes and data for Section 7 have been submitted in the supplementary material. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have provided detailed implementations of Algorithm 1 and data for Figure 1 (the main experimental result) in the supplemental material. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have specified all the experiment details in Section 7. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: For the PPR applied on the Gaussian mechanism in Section 7, the mean squared error is obtained by a closed-form formula, and the compression size is bounded using the expression in Theorem 4.3. Therefore, the plot about PPR in in Figure 1 is precise. For the running time of sliced PPR, we have reported the standard error of the mean of the running time via error bars in Figure 3. For the experiments on the Laplace mechanism in Appendix J, the standard error of the mean has been recorded for the estimation of the MSE of the discrete Laplace mechanism, and the largest coefficient of variation of the sample mean is reported, which is very small and would be unnoticeable even if the error bars were plotted. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have provided all the necessary information on the experiments in Section 7, including the computer and CPU information. Figure 1 plotted according to closed-form formulas and hence can be immediately derived. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We confirm that the research conducted in this paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: We believe research on privacy preservation generally has a positive societal impact, and research on reducing the communication cost of privacy mechanisms (such as this paper) is beneficial. Nevertheless, we have not addressed this in the paper, since we believe our theoretical results has no specific societal impact outside of the general impacts of privacy and reduced communication cost. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This piece of research is theoretical. We do not release any new dataset or pretrained model. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 37}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We have appropriately mentioned and cited the paper whenever we compare with it. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper is mainly for theoretical research. We have provided the codes for our algorithm (implemented by Python) in the supplementary material, which is well documented. We do not release any new dataset. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This piece of research does not involve crowdsourcing or human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This piece of research does not involve crowdsourcing or human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]