[{"type": "text", "text": "On the cohesion and separability of average-link for hierarchical agglomerative clustering ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Eduardo S. Laber Miguel Batista\u2217 Departmento de Inform\u00e1tica, PUC-RIO Departmento de Inform\u00e1tica, PUC-RIO laber@inf.puc-rio.br miguel260503@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Average-link is widely recognized as one of the most popular and effective methods for building hierarchical agglomerative clustering. The available theoretical analyses show that this method has a much better approximation than other popular heuristics, as single-linkage and complete-linkage, regarding variants of Dasgupta\u2019s cost function [STOC 2016]. However, these analyses do not separate average-link from a random hierarchy and they are not appealing for metric spaces since every hierarchical clustering has a $1/2$ approximation with regard to the variant of Dasgupta\u2019s function that is employed for dissimilarity measures [Moseley and Yang 2020]. In this paper, we present a comprehensive study of the performance of average-link in metric spaces, regarding several natural criteria that capture separability and cohesion, and are more interpretable than Dasgupta\u2019s cost function and its variants. We also present experimental results with real datasets that, together with our theoretical analyses, suggest that average-link is a better choice than other related methods when both cohesion and separability are important goals. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Clustering is the task of partitioning a set of objects/points so that similar ones are grouped together while dissimilar ones are put in different groups. Clustering methods are widely used for exploratory analysis and for reducing the computational resources required to handle large datasets. ", "page_idx": 0}, {"type": "text", "text": "Hierarchical clustering is an important class of clustering methods. Given a set of $\\mathcal{X}$ of $n$ points, a hierarchical clustering is a sequence of clusterings $(\\mathcal{C}^{n},\\mathcal{C}^{\\bar{n}-1},\\ldots,\\mathcal{C}^{1})$ , where $\\mathcal{C}^{n}$ is a clustering with $n$ unitary clusters, each of them corresponding to a point in $\\mathcal{X}$ , and the clustering $\\mathcal{C}^{i}$ , with $i<n$ , is obtained from $\\mathcal{C}^{i+1}$ by replacing two of its clusters with their union $A^{i}$ . A hierarchical clustering induces a strictly binary tree with $n$ leaves, where each leaf corresponds to a point in $\\mathcal{X}$ and the ith internal node, with $i<n$ , is associated with the cluster $A^{i}$ ; the points in $A^{i}$ correspond to the leaves of the subtree rooted in $A^{i}$ . Hierarchical clustering methods are often taught in data science/ML courses, are implemented in many machine learning libraries, such as scipy, and have applications in different fields as evolution studies via phylogenetic trees [Eisen et al., 1998], finance [TUM, 2010] and detection of closely related entities [Kobren et al., 2017, Monath et al., 2021]. ", "page_idx": 0}, {"type": "text", "text": "Average-link is widely considered one of the most effective hierarchical clustering algorithms. It belongs to the class of agglomerative methods, that is, methods that start with a set of $n$ clusters, corresponding to the $n$ input points, and iteratively use a linkage rule to merge two clusters. Due to its relevance, we can find some recent works dedicated to improving average-link\u2019 efficiency and scalability [Yu et al., 2021, Dhulipala et al., 2021, 2022, 2023] as well as recent theoretical work that try to understand its success in practice [Cohen-Addad et al., 2019, Charikar et al., 2019a, Moseley and Wang, 2023, Charikar et al., 2019b]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Most of the available theoretical works give approximation bounds for average-link regarding the cost function introduced by [Dasgupta, 2016] as well as for some variants of it. Let $\\mathcal{D}$ be the tree induced by a hierarchical clustering. Dasgupta\u2019s cost function and its variation for dissimilarities considered in [Cohen-Addad et al., 2019] are, respectively, given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathsf{D a s g}(\\mathcal D)=\\sum_{a,b\\in\\mathcal X}\\mathsf{s i m}(a,b)\\cdot|D(a,b)|\\;\\;\\mathsf{a n d}\\;\\;\\mathsf{C K M M}(\\mathcal D)=\\sum_{a,b\\in\\mathcal X}\\mathsf{d i s s}(a,b)\\cdot|D(a,b)|,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathtt{s i m}(a,b)\\;(\\mathtt{d i s s}(a,b))$ is the similarity (dissimilarity) of points $a$ and $b$ ; $D(a,b)$ is the subtree of $\\mathcal{D}$ rooted at the least common ancestor of the leaves corresponding to $a$ and $b$ , and $|D(a,b)|$ is the number of leaves in $D(a,b)$ . In general, the existing results show that average-link achieves constant approximation for variants of Dasgupta\u2019s function while other linkage methods do not. ", "page_idx": 1}, {"type": "text", "text": "However, there is significant room for further analysis due to the following reasons. First, Dasgupta\u2019s cost function, despite its nice properties, is less interpretable than traditional cost functions that measure compactness and separability. Second, although the analyses based on Dasg and its variants allow to separate average-link from other linkage methods as single-linkage and complete-linkage in terms of approximation, they do not separate average-link from a random hierarchy [Cohen-Addad et al., 2019, Moseley and Wang, 2023, Charikar et al., 2019b]. Moreover, for the case in which the points lie in a metric space every hierarchical clustering has $1/2$ approximation for the maximization of CKMM [Wang and Moseley, 2020], so this cost function is less appealing in this relevant setting. Finally, to the best of our knowledge, Dasg does not reveal how good are the clusters generated for a specific range of $k$ . As an example, small $k$ are important for exploratory analysis while large $k$ is important for de-duplication tasks [Kobren et al., 2017]. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Motivated by this scenario, we present a comprehensive study of the performance of average-link in metric spaces, with regards to several natural criteria that capture separability and cohesion of clustering. In a nutshell, these results, as explained below, show that average link has much better global properties than other popular heuristics when these two important goals are taken into account. ", "page_idx": 1}, {"type": "text", "text": "Let $({\\mathcal{X}},{\\mathsf{d i s t}})$ be a metric space, where $\\mathcal{X}$ is a set of $n$ points. The diameter ${\\tt d i a m}(S)$ of a set of points $S$ is given by $\\mathtt{d i a m}(S)=\\operatorname*{max}\\{\\mathtt{d i s t}(x,y)|x,y\\in S\\}$ . For a cluster $A$ and for two clusters $A$ and $B$ , let ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname{avg}(A)={\\frac{1}{{\\binom{|A|}{2}}}}\\sum_{x,y\\in A}\\operatorname{dist}(x,y)\\;{\\mathrm{~and~}}\\;\\operatorname{avg}(A,B)={\\frac{1}{|A|\\cdot|B|}}\\sum_{x\\in A}\\sum_{y\\in B}\\operatorname{dist}(x,y)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Let $\\mathcal{C}=(C_{1},\\ldots,C_{k})$ be a $k$ -clustering for $({\\mathcal{X}},{\\mathsf{d i s t}})$ . To study separability we consider the average $(\\mathtt{s e p a v})$ and the minimum $(\\mathtt{s e p}_{\\mathtt{m i n}})$ ) avg among clusters in $\\mathcal{C}$ , that is, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathsf{s e p}_{\\mathsf{a v}}(\\mathscr{C}):=\\frac{1}{\\binom{k}{2}}\\sum_{i\\neq j}\\mathsf{a v g}(C_{i},C_{j})\\ \\mathrm{~and~}\\ \\mathsf{s e p}_{\\mathsf{m i n}}(\\mathscr{C}):=\\operatorname*{min}_{i\\neq j}\\{\\mathsf{a v g}(C_{i},C_{j})\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "On the other hand, for studying cohesion, we consider the maximum diameter (max-diam) and the maximum average pairwise distance (max-avg) of the clusters in $\\mathcal{C}$ . In formulae, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{max-diam}({\\mathcal{C}}):=\\operatorname*{max}\\{\\operatorname{diam}(C_{i})|1\\leq i\\leq k\\}{\\mathrm{~and~}}\\operatorname*{max-avg}({\\mathcal{C}}):=\\operatorname*{max}\\{\\operatorname{avg}(C_{i})|1\\leq i\\leq k\\}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We also study natural optimization goals that capture both the separability and the cohesion of a clustering. We define the cs-ratio $\\cdot_{\\tt A V}$ and cs-rati ${\\tt O p m}$ of a clustering $\\mathcal{C}$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathtt{c s-r a t i o}_{\\mathtt{A V}}(\\mathcal C):=\\frac{\\mathtt{m a x-a v g}(\\mathcal C)}{\\mathtt{s e p}_{\\mathtt{m i n}}(\\mathcal C)}\\,\\,\\,\\mathrm{and}\\,\\,\\,\\mathtt{c s-r a t i o}_{\\mathtt{D M}}(\\mathcal C):=\\frac{\\mathtt{m a x-d i a m}(\\mathcal C)}{\\mathtt{s e p}_{\\mathtt{m i n}}(\\mathcal C)}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Let $\\mathcal{A}^{k}$ be a $k$ -clustering produced by average-link. We first prove through a simple inductive argument that cs-rat $\\dot{\\mathtt{i o}_{\\mathtt{A V}}}(\\mathcal{A}^{k})\\le1$ . This result does not assume that the points in $\\mathcal{X}$ lie in a metric space and it is tight in the sense that there are instances in which cs-rati $\\mathsf{o}_{\\mathtt{A V}}({\\mathcal{C}})\\,=\\,1$ for every $k$ -clustering $\\mathcal{C}$ . For the related cs-rati ${\\tt O p m}$ criterion, we present a more involved analysis which shows that cs-rati $\\mathsf{o}_{\\mathsf{D M}}(A^{k})$ as well as the approximation of average-link regarding OPT (the minimum possible cs-ratioDM) are $O(\\log n)$ ; these bounds are nearly tight since there exists an instance for which cs-rati $\\mathsf{o}_{\\mathsf{D M}}(\\mathcal{A}^{k})$ and cs-rati ${\\operatorname{o}}_{\\mathrm{DM}}({\\mathcal{A}}^{k})/{\\mathrm{OPT}}$ are ( log n ). Both cs-ratioAV and cs-rati ${\\tt O p m}$ allow an exponential separation between average-link and other linkage methods, as single-linkage and complete-linkage. Interestingly, in contrast to CKMM (Eq. 1), our criteria also allow a very clear separation between average-link and the clustering induced by a random hierarchy. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Next, we focus on separability criteria. Let $\\mathrm{OPT}_{\\mathrm{SEP}}(k)$ be the maximum possible $\\tt s e p_{a v}$ of a $k$ -clustering for $({\\mathcal{X}},{\\mathsf{d i s t}})$ . We show that $\\mathsf{s e p}_{\\mathsf{a v}}(A^{k})$ is at least OkP+T2S ElPn( kn) and that this result is nearly tight. Furthermore, we argue that any hierarchical clustering algorithm that has bounded approximation regarding max-diam or max-avg does not have approximation better than $1/k$ to $\\tt s e p_{a v}$ . Regarding single-linkage and complete-linkage, we present instances that show that their approximation with respect to $\\mathtt{s e p a v}$ are exponentially worse than that of average-link, for the relevant case that $k$ is small. ", "page_idx": 2}, {"type": "text", "text": "We also investigate the cohesion of average-link. For a $k$ -clustering $\\mathcal{C}$ , let avg-diam be the average diameter of the $k$ clusters in $\\mathcal{C}$ . Let $\\mathrm{OPT}_{\\mathrm{DM}}(k)$ and $\\mathrm{OPT}_{\\mathrm{AV}}(k)$ be, respectively, the minimum possible max-diam and avg-diam of a $k$ -clustering for $(\\mathcal{X},\\mathtt{d i s t})$ . We prove that for all $k$ , $\\mathrm{max-diam}({\\mathcal A}^{k})\\ \\leq\\ \\mathrm{min}\\{k,1+4\\ln n\\}k^{\\log_{2}3}\\mathrm{OPT}_{\\mathtt{A V}}(k)$ . This result together with the instance given by Theorem 3.4 of [Dasgupta and Laber, 2024] allow to separate average-link from single-linkage, in terms of approximation, when $k$ is $\\Omega(\\log^{2.41}n)$ . We also show that max-diam $(\\mathcal{A}^{k})$ is $\\Omega(k)\\mathrm{OPT}_{\\mathrm{DM}}(k)$ , which is, to the best of our knowledge, the first lower bound on the maximum diameter of average-link. ", "page_idx": 2}, {"type": "text", "text": "Finally, to complement our study, we present some experiments with 10 real datasets in which we evaluate, to some extent, if our theoretical results line up with what is observed in practice. These experiments conform with our theoretical results since they also suggest that average-link performs better than other related methods when both cohesion and separability are taken into account. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "There is a vast literature about hierarchical agglomerative clustering methods. Here, we focus on works that provide provable guarantees for average-link and some other well-known linkage methods. ", "page_idx": 2}, {"type": "text", "text": "Average-link. There are works that present bounds on the approximation of average-link regarding some criteria [Cohen-Addad et al., 2019, Charikar et al., 2019b,a, Moseley and Wang, 2023, Dasgupta and Laber, 2024]. All these works but [Dasgupta and Laber, 2024] analyse the approximation of average-link regarding variants of Dasgupta\u2019s cost function. [Moseley and Wang, 2023] assumes that the proximity between the points in $\\mathcal{X}$ is given by a similarity matrix. They show that average-link is a $1/3$ -approximation with respect to the \"dual\" of Dasgupta\u2019s cost function. [Cohen-Addad et al., 2019], as in our work, assumes that the proximity between points in $\\mathcal{X}$ is given by a dissimilarity measure and shows that average-link has $2/3$ approximation for the problem of maximizing CKMM (Eq. 1). [Charikar et al., 2019b] show that these approximation ratio for average-link are tight. These papers also show that a random hierarchy obtained by a divisive heuristic that randomly splits the set of points in each cluster matches the $1/3$ and $2/3$ bounds. ", "page_idx": 2}, {"type": "text", "text": "[Dasgupta and Laber, 2024] presents an interesting approach to derive upper bounds on cohesion criteria for a certain class of linkage methods that includes average-link. They show that ${\\mathsf{a v g}}(A)\\leq$ $k^{1.59}\\mathrm{OPT}_{\\mathtt{A V}}(k)$ for every cluster $A\\in{\\mathcal{A}}^{k}$ . Our bound on the maximum diameter of a cluster in $\\mathcal{A}^{k}$ incurs an extra factor of $\\operatorname*{min}\\{k,1+4\\ln n\\}$ to this bound and its proof combines their approach with some new ideas/analyses. ", "page_idx": 2}, {"type": "text", "text": "Other Linkage Methods. There are also works that give bounds on the diameter of the clustering built by complete-linkage and single-linkage on metric spaces [Dasgupta and Long, 2005, Ackermann et al., 2010, Gro\u00dfwendt and R\u00f6glin, 2015, Arutyunova et al., 2023, Dasgupta and Laber, 2024]. Let $\\mathcal{C}$ and $\\boldsymbol{S}$ be the $k$ -clustering built by these methods, respectively. [Arutyunova et al., 2023] shows that max-diam $(\\mathcal{C})$ is $\\Omega(k\\mathrm{OPT}_{\\mathrm{DM}}(k))$ while [Dasgupta and Laber, 2024] shows that max-diam $(\\mathcal{C})$ is ${\\cal O}(\\mathrm{min}\\{k^{1.30}0\\mathrm{PT}_{\\mathtt{D M}}(k),k^{1.59}\\mathrm{OPT}_{\\mathtt{A V}}(k)\\})$ . Regarding single-linkage, max-diam $(S)$ is $\\Theta(k\\mathrm{OPT}_{\\mathrm{DM}}(k))$ [Dasgupta and Long, 2005, Arutyunova et al., 2023] and $\\Omega(\\bar{k^{2}}\\mathrm{OPT}_{\\mathtt{A V}}(k))$ [Dasgupta and Laber, 2024]. [Ackermann et al., 2010, Gro\u00dfwendt and R\u00f6glin, 2015] give bounds for the case in which dist is the Euclidean metric. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In terms of separability criteria, it is well known that single-linkage maximizes the minimum spacing of a clustering [Kleinberg and Tardos, 2006][Chap 4.7]. Recently, [Laber and Murtinho, 2023] observed that it also maximizes the cost of the minimum spanning tree spacing, a stronger criterion. These criteria, in contrast to ours, just take into account the minimum distance between points in different clusters and then they can be significantly impacted by noise. ", "page_idx": 3}, {"type": "text", "text": "[Gro\u00dfwendt et al., 2019] shows that Ward\u2019s method gives a 2-approximation for $k$ -means when the optimal clusters are well-separated. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Algorithm 2 shows a pseudo-code for average-link. The function dist $A L\\left(A,B\\right)$ at line 3 that measures the distance between clusters $A$ and $B$ is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathtt{d i s t}_{A L}(A,B):=\\frac{1}{|A||B|}\\sum_{a\\in A}\\sum_{b\\in B}\\mathtt{d i s t}(a,b).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "single-linkage and complete-linkage are obtained by replacing ${\\tt d i s t}_{A L}$ , in Algorithm 2, with dis $:_{S L}(A,\\bar{B^{\\big}})~:=~\\operatorname*{min}\\{\\mathsf{d i s t}(a,b)|(a,b)~\\in~\\bar{A}~\\times~\\bar{B}\\}$ and dist $\\mathbf{\\Theta}_{C L}(A,B)\\;\\;:=\\;\\;$ $\\operatorname*{max}\\{\\mathsf{d i s t}(a,b)|(a,b)\\in A\\times B\\}$ , respectively. ", "page_idx": 3}, {"type": "table", "img_path": "2LuSHTFWzK/tmp/fa6153612fd667553bb65391e7abb3c2546cb9de0ed5709fb155cdf5f9a83290.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "A version of the triangle inequality for averages will be employed a number of times in our analyses.   \nIts proof can be found in Section A. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.1 (Triangle Inequality for averages). Let $A$ , $B$ and $C$ be three clusters. Then, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{a v g}(A,C)\\leq\\mathsf{a v g}(A,B)+\\mathsf{a v g}(B,C).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For two disjoint clusters $A$ and $B$ , the following identity holds ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\binom{(|A|+|B|)}{2}\\mathbf{avg}(A\\cup B)=\\binom{|A|}{2}\\mathbf{avg}(A)+|A||B|\\mathbf{avg}(A,B)+\\binom{|B|}{2}\\mathbf{avg}(B).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Dividing both sides by $\\binom{(|A|+|B|)}{2}$ , we conclude that $\\mathsf{a v g}(A\\cup B)$ is a convex combination of $\\mathsf{a v g}(A),\\mathsf{a v g}(B)$ and ${\\mathsf{a v g}}(A,B)$ , a fact will be used a couple of times in our analyses. ", "page_idx": 3}, {"type": "text", "text": "The following notation will be used throughout the text. We use $\\begin{array}{r}{H_{p}=\\sum_{i=1}^{p}\\frac{1}{i}}\\end{array}$ to denote the $p$ th harmonic number and $\\mathcal{A}^{k}$ to refer to the $k$ -clustering obtained by average-link for the instance under consideration, which will always be clear from the context. ", "page_idx": 3}, {"type": "text", "text": "3 Cohesion and separability ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we analyze the performance of average-link with respect to both cs-rati $.0_{\\tt A V}$ and cs-rati ${\\tt O p m}$ (Eq. 4), criteria that simultaneously take into account the separability and the cohesion of a clustering. Moreover, we contrast its performance with that achieved by other linkage methods. ", "page_idx": 3}, {"type": "text", "text": "We first show that cs-rati $\\mathsf{O}_{\\mathtt{A V}}(\\mathcal{A}^{k})\\leq1$ . The proof of this result can be found in Section B.1, it uses induction on the number of iterations of average-link together with a fairly simple case analysis. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Let $\\mathcal{A}^{k}$ be a $k$ -clustering built by average-link. Then, for every $k$ , cs-rati $\\mathsf{o}_{\\mathtt{A V}}(\\mathcal{A}^{k})\\leq1$ . ", "page_idx": 4}, {"type": "text", "text": "We note that the above result does not assume the triangle inequality and it is tight in the sense that for the instance $({\\mathcal{X}},{\\mathsf{d i s t}})$ , in which the $n$ points of $\\mathcal{X}$ have pairwise distance 1, every clustering has cs-rat $\\mathtt{i o}_{\\mathtt{A V}}$ equal to 1. ", "page_idx": 4}, {"type": "text", "text": "In Section B.2, we present instances which show that cs-rati $\\mathtt{O A V}$ can be $\\Omega(n),\\,\\Omega({\\sqrt{n}})$ and unbounded in terms of $n$ for single-linkage, complete-linkage and a random hierarchy, respectively. Interestingly, all the $k$ -clustering, with $2<k\\le n/2$ , induced by the hierarchical clustering obtained by these methods satisfy these bounds. Furthermore, since cs-rati $\\mathsf{o}_{\\mathtt{D M}}(\\mathcal C)\\geq\\mathsf{c s-r a t i o}_{\\mathtt{A V}}^{\\mathtt{C}}(\\mathcal C)$ for every clustering $\\mathcal{C}$ , these bounds also hold for the cs-rati ${\\tt O p m}$ criterion. ", "page_idx": 4}, {"type": "text", "text": "A natural question that arises is whether average-link has a \"good\" approximation with respect to cs-rat $.0_{\\tt A V}$ . Unfortunately, the answer is no. In fact, in Section B.3 we show an instance where the approximation is unbounded in terms of $n$ . However, as we show in the next section, average-link has a logarithmic approximation with respect to cs-ratioDM. ", "page_idx": 4}, {"type": "text", "text": "3.2 The cs-rati ${\\tt O p m}$ criterion", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We analyze the cs-rati ${\\tt O p m}$ of average-link. The results of this section will have an important role in the analysis of both the separability and cohesion of average-link presented further. ", "page_idx": 4}, {"type": "text", "text": "First, we show that for every cluster $X$ in $\\mathcal{A}^{k}$ , the average distance of a point $x\\in X$ to the other points in $X-x$ is at most a logarithmic factor of the average distance between any two clusters $Y$ and $Z$ . The proof can be found in Section B.5. Let $T_{i-1}$ be the cluster that contains $x$ before the ith merge involving $x$ and let $S_{i}$ be the cluster that is merged with $T_{i-1}$ . We prove by induction that $\\mathsf{a v g}(x,T_{i}-x)\\leq\\ln H_{|T_{i}|-1}\\mathsf{a v g}(Y,Z)$ , which implies on the desired result because $T_{t}=X$ for some $t$ . To establish the induction, we use the triangle inequality to write $\\mathsf{a v g}(x,T_{i}-x)$ as a function of both $\\mathsf{a v g}(x,T_{i-1}-x)$ and $\\mathsf{a v g}(T_{i-1},S_{i})$ , and also argue that $\\mathsf{a v g}(T_{i-1},S_{i})\\leq\\mathsf{a v g}(X,Y)$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2. Let $X$ , $Y$ and $Z$ , with $|X|\\ge2$ and $Y\\neq Z$ , be clusters of $\\mathcal{A}^{k}$ . Then, for every $x\\in X$ , we have that $\\operatorname{avg}(x,X)\\leq\\operatorname{avg}(x,X-x)\\leq H_{|X|-1}\\mathbf{avg}(Y,Z)$ . ", "page_idx": 4}, {"type": "text", "text": "The next result is a simple consequence of the previous one. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3. Let $k\\geq2$ and let $X$ , $Y$ and $Z$ , with $Y\\neq Z$ , be clusters of a $k$ -clustering built by average-link. Then, $\\mathtt{d i a m}(X)\\leq2H_{|X|-1}\\mathtt{a v g}(Y,Z)$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. If $|X|=1$ the result holds because ${\\tt d i a m}(X)=0$ . Thus, we assume that $|X|>1$ . Let $x$ and $x^{\\prime}$ be such that dist $(x,x^{\\prime})=\\mathsf{d i a m}(X)$ . We have that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathtt{d i s t}(x,x^{\\prime})\\leq\\mathtt{a v g}(x,X)+\\mathtt{a v g}(X,x^{\\prime})\\leq2H_{|X|-1}\\mathsf{a v g}(Y,Z)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the first inequality follows from the triangle inequality and the second one due to Lemma 3.2. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "The next theorem shows that cs-rati $\\mathsf{o}_{\\mathsf{D M}}(A^{k})\\leq2H_{n}$ and that average-link has a logarithmic approximation for the cs-rati ${\\tt O p m}$ criterion. The first upper bound is a simple consequence of Theorem 3.3. Let OPT be the minimum possible cs-ratioDM. To prove the bound on the approximation we consider two cases. If $\\mathrm{OPT}\\ge1/3$ the result holds because cs-rati $\\mathsf{o}_{\\mathtt{D M}}(\\mathcal{A}^{k})\\leq2\\hat{\\ln n}\\leq6\\mathrm{OPT}\\ln n$ If $\\mathrm{OPT}<1/3$ , we argue that the clusters in the optimal clustering are \"well separated\" and, hence, average-link builds the optimal clustering. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4. For all $k_{i}$ , the $k$ -clustering $\\boldsymbol{\\mathcal{A}}^{k}$ built by average-link satisfies cs-rati $\\mathsf{o}_{\\mathsf{D M}}(\\mathcal{A}^{k})\\leq$$2H_{n}$ . Furthermore, for all $k$ , cs-rati $\\mathsf{o}_{\\mathsf{D M}}(\\mathcal{A}^{k})$ is $O(\\log n)\\cdot O P T$ where OPT is cs-rati ${\\tt O p m}$ of the$k$ -clustering with minimum possible cs-ratioDM.", "page_idx": 4}, {"type": "text", "text": "Proof. The inequality cs-rati $\\mathsf{o}_{\\mathsf{D M}}(A^{k})\\leq2H_{n}$ is obtained by using Theorem 3.3, with $X$ being the cluster with the largest diameter in $\\mathcal{A}^{k}$ and $Y$ and $Z$ being the clusters in $\\mathcal{A}^{k}$ that satisfy $\\mathsf{a v g}(Y,Z)=\\mathsf{s e p}_{\\mathsf{m i n}}(A^{\\tilde{k}})$ .   \nNow we prove that $\\mathcal{A}^{k}$ has logarithmic approximation. If $\\mathrm{OPT}\\ge1/3$ , then cs-rat $.0_{\\tt D M}({\\mathcal A}^{k})\\leq$ $2H_{n}\\leq60\\mathrm{PT}H_{n}$ and, hence, the desired result holds.   \nThus, we assume OPT $<1/3$ , Let ${\\mathcal{C}}^{*}(k)$ be a $k$ -clustering that satisfies cs-rat $.\\circ_{\\mathsf{D M}}(\\mathcal{C}^{*}(k))=\\mathsf{O P T}$ . The following claim will be useful.   \nClaim 1. Let $C,C^{\\prime}$ be two clusters in $\\mathcal{C}^{*}(k)$ and let $a,b$ be two closest points in $C$ and $C^{\\prime}$ , that is, ", "page_idx": 5}, {"type": "text", "text": "$\\mathtt{d i s t}(a,b)=\\operatorname*{min}\\{\\mathtt{d i s t}(x,y)|(x,y)\\in C\\times C^{\\prime}\\}$ . Thus, $\\mathtt{d i s t}(a,b)>\\operatorname*{max}\\{\\mathtt{d i a m}(C),\\mathtt{d i a m}(C^{\\prime})\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof of the claim. We assume w.l.o.g. that $\\mathtt{d i a m}(C)\\,\\geq\\,\\mathtt{d i a m}(C^{\\prime})$ . For the sake of reaching a contradiction, assume that dist $(a,b)\\leq\\mathsf{d i a m}(C)$ . Then, it follows from the triangle inequality that the maximum distance between a point in $C$ and $C^{\\prime}$ is at most $3\\mathtt{d i a m}(C)$ . Thus, $\\mathsf{s e p}_{\\mathrm{min}}({\\mathcal C}^{*}(k))\\leq$ $\\mathsf{a v g}(C,C^{\\prime})\\leq\\mathsf{3d i a m}(C)$ and so cs-rati $.0_{\\tt D M}({\\mathscr C}^{*}(k))\\geq\\tt d i a m({C})/3d i a m(C)=1/3$ , which contradicts our assumption. \u25a1. ", "page_idx": 5}, {"type": "text", "text": "Now, we argue that average-link constructs the clustering ${\\mathcal{C}}^{*}(k)$ when cs-rat $\\mathsf{i o}_{\\mathsf{D M}}(\\mathcal{C}^{*}(k))<1/3,$ , so its approximation is 1 in this case. For the sake of reaching a contradiction, let us assume $A^{k}\\neq\\mathcal{C}^{*}(k)$ . Hence, at some iteration average-link merges two clusters, say $A$ and $B$ , that satisfy the following properties: $A\\subseteq C$ and $B\\subseteq C^{\\prime}$ , where $C$ and $C^{\\prime}$ are two different clusters in $\\mathcal{C}^{*}(k)$ . Let $t$ be the first iteration of average-link when it occurs. ", "page_idx": 5}, {"type": "text", "text": "Case 1) $A\\subset C$ or $B\\subset C^{\\prime}$ . Let us assume w.l.o.g. that $A\\subset C$ . In this case, there is a cluster $A^{\\prime}$ at the beginning of iteration $t$ such that $A^{\\prime}\\cup A\\subseteq C$ . We have that $\\mathtt{a v g}(A,A^{\\prime})\\leq\\mathtt{d i a m}(C)$ and by the above claim the minimum distance between $A$ and $B$ is larger than $\\operatorname*{max}\\{\\mathsf{d i a m}(C),\\mathsf{d i a m}(C^{\\prime})\\}$ . Thus, $\\operatorname{avg}(A,B)\\,>\\,\\operatorname*{max}\\{\\operatorname{diam}(C),\\operatorname{diam}(C^{\\prime})\\}\\,\\geq\\,\\operatorname{avg}(A,A^{\\prime}).$ , which contradicts the choice of average-link. ", "page_idx": 5}, {"type": "text", "text": "Case 2) $A=C$ and $B=C^{\\prime}$ . If $k=2$ we are done. Otherwise, there exists a cluster $C^{\\prime\\prime}\\in\\mathcal{C}^{*}(k)$ and two clusters $X$ and $Y$ at the beginning of iteration $t$ such that $X\\cup Y\\subseteq C^{\\prime\\prime}$ . Thus, it follows from the condition $\\mathrm{OPT}<1/3$ that $\\begin{array}{r}{\\bar{\\mathsf{a v g}}(\\bar{X^{\\prime}}Y)\\leq\\mathsf{d i a m}(C^{\\prime\\prime})<\\frac{1}{3}\\mathsf{s e p}_{\\mathsf{m i n}}(\\mathscr{C}^{*}(k))\\leq\\frac{1}{3}\\mathsf{a v g}(C,C^{\\prime})\\leq}\\end{array}$ $\\mathsf{a v g}(C,C^{\\prime})$ , which again contradicts the choice of average-link. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "It is noteworthy that, in contrast to Theorem 3.1, the assumption that the points lie in a metric space is necessary to prove Theorem 3.4. In Section B.4 we present an instance that supports this observation. ", "page_idx": 5}, {"type": "text", "text": "Now, we present an instance, denoted by $\\mathcal{I}^{C S}$ , that shows that the above results are nearly tight. This instance with small modifications will also be used to investigate the tightness of our results regarding the separability (Section 4) and the cohesion (Section 5) of average-link. We note that in most of the instances presented here, including $\\mathcal{I}^{C S}$ , will have more than one possible execution for the methods we analyze. In these cases, we will always consider the execution that is more suitable for our purposes. These multiple executions can be avoided at the price of more complicated descriptions that involve the addition of small values $\\epsilon$ to the distance or points to break ties. ", "page_idx": 5}, {"type": "text", "text": "Let $t$ be an integer that satisfies $t!=n$ ; note that (lolgo lgo gn n). Moreover, let A0 be a set containing a single point located at position $p_{0}$ in the real line and $A_{i}$ , for $0<i\\leq t-1$ , be a set of $(i+1)!-i!$ points that are located at position $p_{i}$ of the real line. We define $B_{0}=A_{0}$ and $B_{i}=B_{i-1}\\cup A_{i}$ , for $i\\geq1$ . Set $p_{0.}=0,p_{1}=1$ and, for $i>1$ , $p_{i}=p_{i-1}+\\mathsf{a v g}(A_{i-1},B_{i-2})$ . The set of points for our instance $\\mathcal{I}^{C S}$ is $B_{t-1}$ and the distance between a point in $A_{i}$ and a point in $A_{j}$ is $|p_{i}-p_{j}|$ . The following lemma gives properties of $\\mathcal{I}^{C S}$ and, in particular, how average-link behaves on it. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.5. For $i\\geq0$ , we have that $|B_{i}|=(i\\!+\\!1)!$ ! and for $i\\geq2$ , we have diam $(B_{i-2})=i(i\\!-\\!1)/2$ , $\\mathsf{a v g}(B_{i-2},A_{i-1})=i+1$ and $p_{i}=i(i+1)/2$ . Furthermore, for $k\\leq t$ , average-link obtains the $k$ -clustering $\\mathcal{A}^{k}=\\left(B_{t-k},\\,A_{t-k+1},\\ldots,A_{t-1}\\right)$ and, in particular, for $k=2$ it obtains the clustering $\\mathcal{A}^{2}=(B_{t-2},A_{t-1})$ . ", "page_idx": 5}, {"type": "text", "text": "From Lemma 3.5, we have that $\\mathsf{s e p}_{\\mathrm{min}}(A^{2})\\;=\\;\\mathsf{a v g}(B_{t-2},A_{t-1})\\;=\\;t+1$ and $\\mathsf{i i a m}(B_{t-2})\\;=$ $t(t-1)/2$ , so cs-rati $\\begin{array}{r}{\\ o_{\\tt D M}=\\frac{t(t-1)}{2(t+1)}}\\end{array}$ , which is $\\Omega({\\frac{\\log n}{\\log\\log n}})$ . ", "page_idx": 5}, {"type": "text", "text": "Furthermore, for the clustering $\\mathcal{A}^{\\prime}=\\left(A_{0},B_{t-1}-A_{0}\\right)$ we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{i e p}_{\\mathrm{min}}(\\mathcal A^{\\prime})=\\mathsf{a v g}(A_{0},B_{t-1}-A_{0})\\ge\\frac{|A_{t-1}|}{|B_{t-1}|}\\mathsf{a v g}(A_{0},A_{t-1})=\\left(\\frac{t!-(t-1)!}{t!}\\right)p_{t-1}=\\frac{(t-1)^{2}}{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and max-di $\\mathtt{a m}({\\mathcal A}^{\\prime})\\,\\leq\\,\\mathtt{d i a m}(B_{t-1})\\,=\\,(t+1)(t+2)/2$ . Thus, $\\mathtt{c s-r a t i o_{D M}(A^{\\prime})}=O(1)$ and the logarithmic approximation of average-link to cs-rati ${\\tt O p m}$ is also nearly tight. ", "page_idx": 6}, {"type": "text", "text": "4 Separability criteria ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we investigate the separability of average-link. Recall that $\\mathrm{OPT}_{\\mathtt{S E P}}(k)$ is the maximum possible $\\mathtt{s e p}_{\\mathtt{a v}}$ of a $k$ -clustering for ( $\\chi$ , dist). We show that for average-link $\\tt s e p_{a v}$ is at least OkP+T2S ElPn( kn) and that this bound is nearly tight. We also show that there are instances in which the $\\mathtt{s e p}_{\\mathtt{a v}}$ of single-linkage and complete-linkage are exponentially smaller than that of average-link. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2 gives an upper bound on $\\tt s e p_{a v}$ for average-link and its complete proof can be found in Section D.2. Here, we give an overview of the proof for the case $k\\,>\\,2$ , which is the most involved one. The proof uses the fact established by Proposition 4.1 that there exists a set of $k$ points $P\\subseteq\\mathcal{X}$ that satisfies ${\\mathsf{a v g}}(P)\\geq{\\mathsf{O P T}}_{\\mathtt{S E P}}(k)$ . This holds because a set of $k$ randomly selected points that intersect all clusters of a $k$ -clustering with maximum $\\tt s e p_{a v}$ satisfies the the desired property (in expectation). Having this result in hands, it is enough to show that ${\\mathsf{a v g}}(P)$ is $O((k+H_{n-1})\\mathsf{s e p}_{\\mathsf{a v}}(A^{k}))$ . ", "page_idx": 6}, {"type": "text", "text": "This bound on $\\mathsf{a v g}(P)$ is obtained by relating the distance of each pair of points $p,p^{\\prime}\\in P$ with the average distance between clusters in $\\mathcal{A}^{k}$ . Let $p,p^{\\prime}\\in P$ and let $A$ and $A^{\\prime}$ be clusters in $\\mathcal{A}^{k}$ such that $p\\in A$ and $p^{\\prime}\\in A^{\\prime}$ . Moreover, let $S$ be a cluster in $\\mathcal{A}^{k}$ , with $S\\notin\\{A,A^{\\prime}\\}$ . From the triangle inequality we have that $\\operatorname{dist}(p,p^{\\prime})=\\operatorname{avg}(p,p^{\\prime})\\leq\\operatorname{avg}(p,A)+\\operatorname{avg}(A,S)+\\operatorname{avg}(S,A^{\\prime})+\\operatorname{avg}(A^{\\prime},p^{\\prime})$ . Then, by bounding both ${\\mathsf{a v g}}(p,A)$ and $\\mathsf{a v g}(A^{\\prime},p^{\\prime})$ via Lemma 3.2, wit $_{\\textrm{h}Y}\\textrm{a r}$ d $Z$ satisfying ${\\mathsf{a v g}}(Y,Z)\\leq$ $\\mathsf{s e p}_{\\mathsf{a v}}(A^{k})$ , we conclude that $\\mathtt{d i s t}(p,p^{\\prime})\\leq2H_{n}\\mathtt{s e p}_{\\mathtt{a v}}({\\mathcal{A}}^{k})+\\mathtt{a v g}(A,S)+\\mathtt{a v g}(S,A^{\\prime})$ . In general lines, the result is then established by averaging this inequality for all ${\\cal S}\\,\\notin\\,\\{{\\cal A},{\\cal A}^{\\prime}\\}$ and for all $p,p^{\\prime}\\in P$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.1. There is a set of points $P\\subseteq\\ X$ with the following properties: $|P|\\,=\\,k$ and $\\mathsf{a v g}(P)\\geq O P T_{\\mathtt{S E P}}(k)$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2. For every $k$ , the $k$ -clustering $\\mathcal{A}^{k}$ obtained by average-link satisfies $\\mathtt{s e p}_{\\mathtt{a v}}(\\mathcal{A}^{k})\\geq$ $\\frac{O P T_{\\tt S E P}(k)}{k+2H_{n}}$ . ", "page_idx": 6}, {"type": "text", "text": "We present two instances that, together, show that the previous theorem is nearly tight. The first is the instance $\\mathcal{I}^{C S}$ presented right after Theorem 3.4. For $\\dot{\\mathcal{I}}^{C S}$ , the clustering $\\mathcal{A}^{2}=\\dot{(}A_{t-1}^{-},B_{t-2})$ built by average-link satisfies $\\mathsf{s e p}_{\\mathsf{a v}}(\\mathcal{A}^{2})=\\mathsf{a v g}(A_{t-1},B_{t-2})=t+1$ . On the other hand, Eq. (5) shows that sepav(A\u2032) = (t\u221221)2, for the clustering $\\mathcal{A}^{\\prime}=\\left(A_{0},B_{t-1}-A_{0}\\right)$ . Thus, for $\\mathcal{I}^{C S}$ , $\\mathtt{s e p a v}(A^{2})$ is $O(\\frac{\\mathrm{OPT}_{\\mathtt{S E P}}(k)\\log\\log n}{\\log n})$ ", "page_idx": 6}, {"type": "text", "text": "Now, we present our second instance, denoted by $\\mathcal{T}_{k}^{s e p}$ . Let $k$ be an odd number and let $D$ and $\\epsilon$ be positive numbers. The set of points of $\\mathcal{T}_{k}^{s e p}$ is given by $S_{1}\\cup S_{2}\\cup S_{3}$ , where $|S_{1}|=|S_{2}|=(k-1)/2$ and $S_{3}=\\{s_{i}|1\\leq i\\leq k-2\\}$ . We have dis $:(x,y)=\\epsilon\\quad$ for $x,y\\in S_{1}$ , dist $(x,y)=\\epsilon$ for $x,y\\in S_{2}$ , $\\mathtt{d i s t}(x,y)=1$ for $x,y\\in S_{3}$ and dist $(x,y)=D$ if $x$ and $y$ are not in the same set. ", "page_idx": 6}, {"type": "text", "text": "For $\\mathcal{T}_{k}^{s e p}$ , when $D$ is sufficiently large and $\\epsilon$ is sufficiently small, $\\mathcal{A}^{k}=\\left(S_{1},S_{2},s_{1},\\ldots,s_{k-2}\\right)$ and $\\mathtt{s e p a v}({\\cal A}^{k})\\,=\\,{\\cal O}(D/k)$ . On the other hand, the $\\tt s e p_{a v}$ of the $k-$ clustering that has the cluster $S_{3}$ and $k\\mathrm{~-~}1$ singletons corresponding to the points in $S_{1}\\cup S_{2}$ is $\\Omega(D)$ . Thus, $\\mathsf{s e p}_{\\mathsf{a v}}(A^{k})$ is $O(\\mathrm{OPT}_{\\mathrm{SEP}}(k)/k)$ . ", "page_idx": 6}, {"type": "text", "text": "We note that single-linkage and complete-linkage also obtain the $k$ -clustering $\\mathcal{A}^{k}$ for $\\mathcal{T}_{k}^{s e p}$ , so the upper bound $\\mathrm{OPT}_{\\mathrm{SEP}}(k)/k$ also holds for them. In Section D.3 we present instances that show that $\\tt s e p_{a v}$ is $O(\\frac{\\mathrm{OPT}_{\\mathtt{S E P}}(k)}{\\sqrt{n}})$ for both single-linkage and complete-linkage. ", "page_idx": 6}, {"type": "text", "text": "The instance $\\mathcal{T}_{k}^{s e p}$ is particularly interesting because it also shows that natural cohesion and separability criteria can be conflicting. The key reason is that any method $M$ with bounded approximation (in terms of $n$ ) regarding max-diam or to max-avg (Equation 3) has to build the $k$ -clustering $\\mathcal{A}^{k}$ for $\\mathcal{T}_{k}^{s e p}$ . Thus, by analysing $\\mathcal{T}_{k}^{s e p}$ we can conclude that the approximation factor of $M$ to $\\mathtt{s e p a v}$ is $O(1/k)$ and to $\\mathtt{s e p}_{\\mathtt{m i n}}$ is $O(1/\\ddot{D})$ . The details can be found in Section D.4. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 On the cohesion of average-link ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we prove that ma $\\mathfrak{c}_{\\mathsf{-d i a m}}(A^{k})\\ \\leq\\ \\operatorname*{min}\\{k,1+4\\ln n\\}k^{1.59}\\mathrm{OPT}_{\\mathsf{A V}}(k)$ and we also present an instance which shows that m $\\operatorname{\\sf{ax}-d i a m}(A^{k})\\geq k\\mathrm{OPT}_{\\mathrm{DM}}(k)$ . ", "page_idx": 7}, {"type": "text", "text": "Dasgupta and Laber [2024] presented an interesting approach to devise upper bounds on cohesion criteria for a class of linkage methods that includes average-link. Although this approach was used to show that the maximum pairwise average distance of a cluster in $\\mathcal{A}^{k}$ is at most ${\\dot{k}}^{\\mathbb{1}.59}\\mathrm{OPT}_{\\mathbb{A V}}(k)$ , it cannot be employed, at least directly, to bound the maximum diameter of a cluster in $\\mathcal{A}^{k}$ . Thus, to obtain our $(1\\dot{+}\\,\\dot{4}\\ln n)k^{1.59}\\mathrm{OPT}_{\\mathtt{A V}}(\\dot{k})$ bound we combine the results of [Dasgupta and Laber, 2024] with Theorem 3.4 while for the $k^{1+1.59}\\mathrm{OPT_{AV}}(k)$ bound we add some new ideas/analysis on top of those from [Dasgupta and Laber, 2024]. ", "page_idx": 7}, {"type": "text", "text": "The analysis in Dasgupta and Laber [2024] keeps a dynamic partition of the clusters produced by the linkage method under consideration. Each group in the partition is a set of clusters denoted by family. A point $p$ belongs to a family $F$ if it belongs to some cluster in $F$ . Thus, ${\\tt d i a m}(F)$ is given by the maximum distance among the points that belong to $F$ . The approach bounds the diameter of each family $F$ as (essentially) a function of the clusters that $F$ touches in a target $k$ -clustering $\\mathcal{T}=(T_{1},\\dots,\\bar{T}_{k})$ . The bound on ${\\tt d i a m}(F)$ is then used to upper bound the diameter of the clusters in $F$ . For a $k$ -clustering $\\mathcal{C}$ , let av $\\begin{array}{r}{\\mathsf{\\Sigma}^{\\mathsf{\\tilde{\\Sigma}}\\mathsf{d i a m}}(\\mathcal{C}):=\\frac{1}{k}\\sum_{i=i}^{k}\\mathsf{d i a m}(C_{i})}\\end{array}$ . As in Dasgupta and Laber [2024], we use as the target clustering the one with minimum avg-diam. ", "page_idx": 7}, {"type": "text", "text": "We explain how the families evolve along the execution of a linkage method, in particular average-link. Initially, we have $k$ families, $F_{1},\\ldots,F_{k}$ , where $F_{i}$ is a family that contains $|T_{i}|$ clusters, each one being a point from $T_{i}$ . Furthermore, the families are organized in a directed forest $D$ that initially consists of $k$ isolated nodes, where the ith node corresponds to family $F_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "We specify how the families and the forest $D$ are updated when the linkage method merges the clusters $g$ and $g^{\\prime}$ belonging to the families $F$ and $F^{\\prime}$ , respectively. Assume w.l.o.g. $|F|\\geq|F^{\\prime}|$ . We have the following cases: ", "page_idx": 7}, {"type": "text", "text": "case 1 $|\\boldsymbol{F}^{\\prime}|=1$ and $|\\boldsymbol F|>1$ . In this case two new families are created, $F^{n e w}:=F-\\{g\\}$ and $F^{n e w^{\\prime}}:=\\{g\\cup g^{\\prime}\\}$ . Moreover, $F^{n e w}$ and $F^{n e w^{\\prime}}$ become, respectively, parents of $F$ and $F^{\\prime}$ in $D$   \ncase 2 $|{F^{\\prime}}|>1$ or $|\\boldsymbol F|=1$ . In this case, only one family is created, $F^{n e w}:=(F\\cup F^{\\prime}\\cup\\{g\\cup$ $g^{\\prime}\\})-g-g^{\\prime}$ . Moreover, $F^{n e w}$ becomes parent of both $F$ and $F^{\\prime}$ in $D$ . ", "page_idx": 7}, {"type": "text", "text": "We say that a family $F$ is regular if $|{\\boldsymbol{F}}|>1$ . ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.1 (Proposition 3.1 of Dasgupta and Laber [2024]). At the beginning of each iteration of average-link at least one of the roots of the forest $D$ corresponds to a regular family. ", "page_idx": 7}, {"type": "text", "text": "Let $\\mathcal{M}$ be the class of linkage methods (Algorithm 2) whose function $f$ , employed to measure the distance between clusters $A$ and $B$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\{\\mathsf{d i s t}(a,b)|(a,b)\\in A\\times B\\}\\leq f(A,B)\\leq\\mathsf{d i a m}(A\\cup B)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proposition 5.2 (Proposition 5.1 of Dasgupta and Laber [2024] ). The diameter of every regular family $F$ produced along the execution of a linkage method in $\\mathcal{M}$ is at most $k^{\\log_{2}3}\\dot{O}P T_{\\mathtt{A V}}(k)$ . ", "page_idx": 7}, {"type": "text", "text": "Note that the function ${\\tt d i s t}_{A L}$ employed by average-link satisfies the condition given by (6) and, thus, the above proposition holds for average-link. ", "page_idx": 7}, {"type": "text", "text": "We are ready to establish the main result of this section. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.3. Every cluster $S$ in $\\mathcal{A}^{k}$ satisfies di $.\\mathsf{a m}(S)\\leq\\operatorname*{min}\\{k,4\\ln n+1\\}k^{\\log_{2}3}O P T_{\\mathsf{A V}}(k).$ ", "page_idx": 7}, {"type": "text", "text": "Proof. Let $V\\,=\\,\\{T\\,\\in\\,T|S\\cap T\\neq\\emptyset\\}$ be the set of clusters of the target clustering $\\tau$ that intersect $S$ . We build a graph $G$ whose nodes correspond to the clusters in $V$ . At the beginning of average-link\u2019s execution, $G$ contains the set of nodes $V$ and no edges. ", "page_idx": 7}, {"type": "text", "text": "At each iteration, there are two possibilities for the clusters $g$ and $g^{\\prime}$ that are merged by average-link: $(g\\cup g^{\\prime})\\cap S=\\emptyset$ or $(g\\cup g^{\\prime})\\subseteq S$ . We define how $G$ is updated in each case: ", "page_idx": 8}, {"type": "text", "text": "Case 1) $(g\\cup g^{\\prime})\\cap S=\\emptyset$ . In this case, $G$ is not updated. ", "page_idx": 8}, {"type": "text", "text": "Case 2) $(g\\cup g^{\\prime})\\subseteq S$ . Let $x$ and $y$ be points in $g$ and $g^{\\prime}$ such that dist $(x,y)$ is minimum and let $T^{x}$ and $T^{y}$ be the clusters in $\\tau$ that contain $x$ and $y$ , respectively. We add an edge of weight dist $(x,y)$ between $T^{x}$ and $T^{y}$ . We say, in this case, that $x$ and $y$ are associated with the edge that links $T^{x}$ to $T^{y}$ . ", "page_idx": 8}, {"type": "text", "text": "We need the following two claims: ", "page_idx": 8}, {"type": "text", "text": "Proof of the claim. Let $H$ be a regular family at the beginning of iteration $t$ Such family does exist due to Proposition 5.1. Moreover, let $h$ and $h^{\\prime}$ be two clusters in $H$ . We have that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{dist}(x,y)\\leq\\mathrm{dist}_{A L}(g,g^{\\prime})\\leq\\mathrm{dist}_{A L}(h,h^{\\prime})\\leq\\mathrm{diam}(h\\cup h^{\\prime})\\leq\\mathrm{diam}(H)\\leq k^{\\log_{2}3}\\mathrm{OPT}_{M}(k),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the second inequality holds by the choice of average-link and the last inequality holds due to the Proposition 5.2. \u25a1 ", "page_idx": 8}, {"type": "text", "text": "Claim 3. For a cluster $C$ , let $V_{C}\\;:=\\;\\{T\\,\\in\\,T|T\\cap C\\,\\neq\\,\\emptyset\\}$ . Let $S^{\\prime}$ be a cluster generated by average-link that is a subset of $S$ . Then, when $S^{\\prime}$ is created, the subgraph of $G$ induced by $V_{S^{\\prime}}$ is connected. ", "page_idx": 8}, {"type": "text", "text": "Proof of the claim If $|S^{\\prime}|=1$ the property holds. Let $S^{\\prime}$ be a cluster obtained by merging $S_{1}$ and $S_{2}$ . By induction, the property holds for $S_{1}$ and $S_{2}$ . Since an edge is added between nodes in $V_{S_{1}}$ and $V_{S_{2}}$ then the property also holds for $S$ . \u25a1 ", "page_idx": 8}, {"type": "text", "text": "Thus, at the end of the algorithm, $G$ is connected and each of its edges has weight at most $k^{\\mathrm{log_{2}\\,3}}\\mathrm{OPT}_{\\mathrm{AV}}(k)$ . Let $x$ and $y$ be points in $S$ such that $\\mathtt{d i s t}(x,y)\\ =\\ \\mathtt{\\bar{d i a m}}(S)$ and let $\\begin{array}{r l}{T^{x}}&{{}=}\\end{array}$ $v_{1}\\ldots v_{\\ell}=T^{y}$ be a path in $G$ from $T^{x}$ to $T^{y}$ . ", "page_idx": 8}, {"type": "text", "text": "Consider a sequence of points $x=p_{1}p_{1}^{\\prime}\\ldots p_{\\ell}p_{\\ell}^{\\prime}=y$ , where $p_{i}$ and $p_{i}^{\\prime}$ are the points in $v_{i}$ associated with the edge $v_{i-1}v_{i}$ and $v_{i}v_{i+1}$ , respectively. From the triangle inequality ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{tist}(x,y)\\leq\\sum_{i=1}^{\\ell-1}\\mathrm{dist}(p_{i}^{\\prime},p_{i+1})+\\sum_{i=1}^{\\ell}\\mathrm{dist}(p_{i},p_{i}^{\\prime})\\leq(k-1)k^{\\log_{2}3}\\mathrm{OPT}_{k\\ell}(k)+\\sum_{i=1}^{k}\\mathrm{diam}(T_{i})\\leq\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For the logarithmic bound, let $S_{1}$ and $S_{2}$ be the two clusters that are merged to form $S$ . At the beginning of the iteration in which $S_{1}$ and $S_{2}$ are merged, Proposition 5.1 assures that there exists a regular family, say $H$ . Let $h$ and $h^{\\prime}$ be two clusters in $H$ . By Proposition 5.2, $\\mathsf{a v g}(h,h^{\\prime})\\leq$ $\\mathsf{d i a m}(H)\\,\\leq\\,k^{\\log_{2}3}\\mathsf{O P T}_{\\mathsf{A V}}(k)$ . Thus, by Theorem 3.3, $\\mathtt{d i a m}(S_{1})\\,\\leq\\,2\\ln n\\cdot\\mathsf{a v g}(h,h^{\\prime})\\,\\leq\\,2\\ln n\\cdot$ $k^{\\mathrm{log_{2}\\,3}}\\mathrm{OPT}_{\\mathrm{AV}}(k)$ and $\\mathrm{diam}(S_{2})\\,\\leq\\,2\\ln n\\cdot k^{\\log_{2}3}\\mathrm{OPT}_{\\mathtt{A V}}(k)$ . Let $s_{1}\\in S_{1}$ and $s_{2}\\in S_{2}$ be such that di $\\mathfrak{t t}(s_{1},s_{2})\\,=\\,\\mathrm{min}\\{\\mathsf{d i s t}(p,q)|(p,q)\\,\\in\\,S_{1}\\,\\times\\,S_{2}\\}$ . Since $S_{1}$ and $S_{2}$ are merged we have that dist(s1, s2) \u2264 $\\langle\\ \\mathsf{a v g}(S_{1},S_{2})\\ \\leq\\ \\mathsf{a v g}(h,h^{\\prime})\\ \\leq\\ k^{\\log_{2}3}\\mathrm{OPT}_{\\mathsf{A V}}(k)$ . Thus, $\\mathtt{d i a m}(S)\\ \\leq\\ \\mathtt{d i a m}(S_{1})\\ +$ $\\mathtt{d i s t}(s_{1},s_{2})+\\mathtt{d i a m}(S_{1})\\leq(1+4\\ln n)k^{\\log_{2}3}\\mathrm{OPT}_{\\mathtt{A V}}(k)$ ). \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Theorem 3.4 of Dasgupta and Laber [2024] presents an instance with $n\\,=\\,2k\\,-\\,2$ points for which single-linkage builds a $k$ -clustering that has a cluster whose diameter is $\\Omega(k^{2}\\bar{\\mathrm{OPT}}_{\\mathtt{A V}}(k))$ . Thus, this result together with Theorem 5.3 show a separation between average-link and single-linkage when $k$ is $\\Omega(\\log^{2.41}n)$ . ", "page_idx": 8}, {"type": "text", "text": "Our last theoretical result is a lower bound on the maximum diameter of the clustering built by average-link. Its proof can be found in the Section $\\boldsymbol{\\mathrm E}$ and it employs an augmented version of instance $\\mathcal{I}^{C S}$ , presented right after Theorem 3.4. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.4. There is an instance for which the $k$ -clustering $\\mathcal{A}^{k}$ built by average-link satisfies max-dia $\\mathsf{\\Omega}_{1}(\\mathcal{A}^{k})\\in\\Omega(k O P T_{\\mathsf{D M}}(k))$ ", "page_idx": 8}, {"type": "text", "text": "Table 1: Average ratio between the result of a method and the best one for each criterion and each group of $k$ . The best results are bold-faced ", "page_idx": 9}, {"type": "table", "img_path": "2LuSHTFWzK/tmp/20bd62eeab6f33755414197c74efeae65bf1922b47e944f63ed0de1790cb89ae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this final section, we briefly present an experiment in which we evaluate whether average-link, in addition to having better theoretical bounds, it also has a better performance in practice for the studied criteria. We employed 10 datasets and used the Euclidean metric to measure distances. For each of them, we executed average-link, complete-linkage an\u221ad single-link\u221aage, for the following sets of values of $k$ : ${\\mathtt{S m a l l}}{=}\\{k|2\\leq k\\leq10\\}$ , Mediu $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ and $\\mathtt{L a r g e=}\\{k|k=n/i$ and $2\\leq i\\leq10\\}$ . More details, as well as the results of our experiment with other distances, can be found in Section F. ", "page_idx": 9}, {"type": "text", "text": "Table 6 shows the average ratio between the result of a method and that of the best one, grouped by criterion and set of $k$ . Each entry is the average of 90 ratios $\\mathrm{~\\textperthousand~}$ and 10 datasets) and each of these ratios for a method $M$ is a value between 0 and 1 that is obtained by dividing the minimum between the result of $M$ and that of the best method by the maximum between them. The letters A, C and S are the initials of the evaluated methods. ", "page_idx": 9}, {"type": "text", "text": "Concerning separability criteria, single-linkage and average-link have the best results for $\\tt s e p_{a v}$ . The latter has some advantage when $k$ is small, which is in line with its better worst-case bound for small $k$ (results from Section 4). For $\\mathtt{s e p}_{\\mathtt{m i n}}$ , average-link has a huge advantage, which is not surprising since its linkage rule tries to increase $\\mathtt{s e p}_{\\mathtt{m i n}}$ at each step by merging the the clusters $A$ and $B$ for which $\\operatorname{avg}(A,B)^{\\prime}=\\operatorname{sep\\!{\\big}}_{\\operatorname*{min}}({\\mathcal{C}})$ , where $\\mathcal{C}$ is the current clustering. ", "page_idx": 9}, {"type": "text", "text": "Regarding cohesion criteria, complete-linkage and average-link were the best methods. They had close results for max-avg while for max-diam the former had a strong dominance. These results align with ours and those from [Dasgupta and Laber, 2024], in the sense that they show that these linkage methods present better worst-case upper bounds than single-linkage when the comparison is made against $\\mathrm{OPT}_{\\mathrm{AV}}(k)$ . Moreover, the advantage of complete-linkage for max-diam is also expected since it is the \"natural\" greedy rule to minimize the maximum diameter (See Proposition 2.1 of Dasgupta and Laber [2024]). ", "page_idx": 9}, {"type": "text", "text": "For cs-rati ${\\tt O p m}$ , average-link and complete-linkage present the best results, with the former being slightly superior for the small $k$ and the latter being slightly superior when $k$ is not small. average-link has a huge dominance for the cs-rati $\\mathsf{O}_{\\mathtt{A V}}$ criterion, which lines up with the theoretical results from Section 3.1. ", "page_idx": 9}, {"type": "text", "text": "In summary, these experiments, together with our theoretical results, provide evidence that average-link is a better choice when both cohesion and separability are relevant. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements The work of the first author is partially supported by CNPq (grant 310741/2021- 1). This study was financed in part by the Coordena\u00e7\u00e3o de Aperfei\u00e7oamento de Pessoal de N\u00edvel Superior - Brasil (CAPES) - Finance Code 001 ", "page_idx": 9}, {"type": "text", "text": "Limitations. We have not identified a major limitation in our work. That said, the assumption that the points lie in a metric space used in our results (except Theorem 3.1) could be seen as a limitation. On the experimental side, having more than 10 datasets would give our conclusions more robustness. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Michael B. Eisen, Paul T. Spellman, Patrick O. Brown, and David Botstein. Cluster analysis and display of genome-wide expression patterns. Proceedings of the National Academy of Sciences ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "of the United States of America, 95(25):14863\u201314868, December 1998. ISSN 0027-8424. doi: 10.1073/pnas.95.25.14863.   \nCorrelation, hierarchies, and networks in financial markets. Journal of Economic Behavior & Organization, 75(1):40\u201358, 2010. ISSN 0167-2681. doi: https://doi.org/10.1016/j.jebo.2010.01. 004. Transdisciplinary Perspectives on Economic Complexity.   \nAri Kobren, Nicholas Monath, Akshay Krishnamurthy, and Andrew McCallum. A hierarchical algorithm for extreme clustering. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017, pages 255\u2013264. ACM, 2017. doi: 10.1145/3097983.3098079. URL https: //doi.org/10.1145/3097983.3098079.   \nNicholas Monath, Kumar Avinava Dubey, Guru Guruganesh, Manzil Zaheer, Amr Ahmed, Andrew McCallum, G\u00f6khan Mergen, Marc Najork, Mert Terzihan, Bryon Tjanaka, Yuan Wang, and Yuchen Wu. Scalable hierarchical agglomerative clustering. In Feida Zhu, Beng Chin Ooi, and Chunyan Miao, editors, KDD \u201921: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 1245\u20131255. ACM, 2021. doi: 10.1145/3447548.3467404. URL https://doi.org/10.1145/3447548.3467404.   \nShangdi Yu, Yiqiu Wang, Yan Gu, Laxman Dhulipala, and Julian Shun. Parchain: A framework for parallel hierarchical agglomerative clustering using nearest-neighbor chain. Proc. VLDB Endow., 15(2):285\u2013298, 2021. doi: 10.14778/3489496.3489509. URL http://www.vldb.org/pvldb/ vol15/p285-yu.pdf.   \nLaxman Dhulipala, David Eisenstat, Jakub Lacki, Vahab S. Mirrokni, and Jessica Shi. Hierarchical agglomerative graph clustering in nearly-linear time. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 2676\u20132686. PMLR, 2021. URL http://proceedings.mlr.press/v139/dhulipala21a.html.   \nLaxman Dhulipala, David Eisenstat, Jakub Lacki, Vahab Mirrokni, and Jessica Shi. Hierarchical agglomerative graph clustering in poly-logarithmic depth. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 909de96145d97514b143dfde03e6cd2b-Abstract-Conference.html.   \nLaxman Dhulipala, Jakub Lacki, Jason Lee, and Vahab Mirrokni. Terahac: Hierarchical agglomerative clustering of trillion-edge graphs. Proc. ACM Manag. Data, 1(3):221:1\u2013221:27, 2023. doi: 10.1145/3617341. URL https://doi.org/10.1145/3617341.   \nVincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn, and Claire Mathieu. Hierarchical clustering: Objective functions and algorithms. J. ACM, 66(4):26:1\u201326:42, 2019. doi: 10.1145/ 3321386. URL https://doi.org/10.1145/3321386.   \nMoses Charikar, Vaggos Chatziafratis, Rad Niazadeh, and Grigory Yaroslavtsev. Hierarchical clustering for euclidean data. In Kamalika Chaudhuri and Masashi Sugiyama, editors, The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, volume 89 of Proceedings of Machine Learning Research, pages 2721\u2013 2730. PMLR, 2019a. URL http://proceedings.mlr.press/v89/charikar19a.html.   \nBenjamin Moseley and Joshua R. Wang. Approximation bounds for hierarchical clustering: Average linkage, bisecting k-means, and local search. J. Mach. Learn. Res., 24:1:1\u20131:36, 2023. URL http://jmlr.org/papers/v24/18-080.html.   \nMoses Charikar, Vaggos Chatziafratis, and Rad Niazadeh. Hierarchical clustering better than average-linkage. In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages 2291\u20132304. SIAM, 2019b. doi: 10.1137/1.9781611975482.139. URL https: //doi.org/10.1137/1.9781611975482.139.   \nSanjoy Dasgupta. A cost function for similarity-based hierarchical clustering. In Daniel Wichs and Yishay Mansour, editors, Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 118\u2013127. ACM, 2016. doi: 10.1145/2897518.2897527. URL https://doi.org/10.1145/2897518.2897527.   \nYuyan Wang and Benjamin Moseley. An objective for hierarchical clustering in euclidean space and its connection to bisecting k-means. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 6307\u20136314. AAAI Press, 2020. doi: 10.1609/AAAI.V34I04.6099. URL https://doi.org/10.1609/aaai.v34i04.6099.   \nSanjoy Dasgupta and Eduardo Laber. New bounds on the cohesion of complete-link and other linkage methods for agglomerative clustering, 2024. URL https://arxiv.org/abs/2405.00937. To appear in ICML 2024.   \nSanjoy Dasgupta and Philip M. Long. Performance guarantees for hierarchical clustering. Journal of Computer and System Sciences, 70(4):555\u2013569, 2005. ISSN 0022-0000. doi: https://doi.org/10. 1016/j.jcss.2004.10.006. URL https://www.sciencedirect.com/science/article/pii/ S0022000004001321. Special Issue on COLT 2002.   \nMarcel R. Ackermann, Johannes Bl\u00f6mer, Daniel Kuntze, and Christian Sohler. Analysis of agglomerative clustering. CoRR, abs/1012.3697, 2010. URL http://arxiv.org/abs/1012.3697.   \nAnna Gro\u00dfwendt and Heiko R\u00f6glin. Improved analysis of complete-linkage clustering. In Nikhil Bansal and Irene Finocchi, editors, Algorithms - ESA 2015 - 23rd Annual European Symposium, Patras, Greece, September 14-16, 2015, Proceedings, volume 9294 of Lecture Notes in Computer Science, pages 656\u2013667. Springer, 2015. doi: 10.1007/978-3-662-48350-3\\_55. URL https: //doi.org/10.1007/978-3-662-48350-3_55.   \nAnna Arutyunova, Anna Gro\u00dfwendt, Heiko R\u00f6glin, Melanie Schmidt, and Julian Wargalla. Upper and lower bounds for complete linkage in general metric spaces. Machine Learning, pages 1\u201330, 2023.   \nJon M. Kleinberg and \u00c9va Tardos. Algorithm design. Addison-Wesley, 2006. ISBN 978-0-321- 37291-8.   \nEduardo Sany Laber and Lucas Murtinho. Optimization of inter-group criteria for clustering with minimum size constraints. In NeurIPS, 2023.   \nAnna Gro\u00dfwendt, Heiko R\u00f6glin, and Melanie Schmidt. Analysis of ward\u2019s method. In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages 2939\u20132957. SIAM, 2019. doi: 10.1137/1.9781611975482.182. URL https://doi.org/10.1137/1.9781611975482.182.   \nPope D. Brooks, Thomas and Michael Marcolini. Airfoil Self-Noise. UCI Machine Learning Repository, 2014. DOI: https://doi.org/10.24432/C5VW2C.   \nVolker Lohweg. Banknote Authentication. UCI Machine Learning Repository, 2013. DOI: https://doi.org/10.24432/C55P57.   \nI-Cheng Yeh. Concrete Compressive Strength. UCI Machine Learning Repository, 2007. DOI: https://doi.org/10.24432/C5PK67.   \nKaynak Cenk Alpaydin, Ethem. Cascading classifiers. Kybernetika, 34(4):[369]\u2013374, 1998. URL http://eudml.org/doc/33363.   \nFang Zhou. Geographical Origin of Music. UCI Machine Learning Repository, 2014. DOI: https://doi.org/10.24432/C5VK5D.   \nGardiner Katheleen Higuera, Clara and Krzysztof Cios. Mice Protein Expression. UCI Machine Learning Repository, 2015. DOI: https://doi.org/10.24432/C50S3Z. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Cassotti Matteo Consonni Viviana Ballabio, Davide and Roberto Todeschini. QSAR fish toxicity. UCI Machine Learning Repository, 2019. DOI: https://doi.org/10.24432/C5JG7B. ", "page_idx": 12}, {"type": "text", "text": "Shini Renjith. Travel Reviews. UCI Machine Learning Repository, 2018. DOI: https://doi.org/10.24432/C56K6W. ", "page_idx": 12}, {"type": "text", "text": "A Proof of proposition 2.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Let $a\\in A$ and $c\\in C$ . Then, $\\mathbf{dist}(a,c)\\leq\\mathbf{dist}(a,b)+\\mathbf{dist}(b,c)$ for every $b\\in B$ . Thus, ", "page_idx": 13}, {"type": "equation", "text": "$$\n|B|\\mathsf{d i s t}(a,c)\\leq\\sum_{b\\in B}(\\mathsf{d i s t}(a,b)+\\mathsf{d i s t}(b,c))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It follows that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|B|\\displaystyle\\sum_{a\\in A}\\displaystyle\\sum_{c\\in C}\\mathtt{d i s t}(a,c)\\leq\\displaystyle\\sum_{a\\in A}\\displaystyle\\sum_{c\\in C}(\\sum_{b\\in B}(\\mathtt{d i s t}(a,b)+\\mathtt{d i s t}(b,c)))=}\\\\ {|C|\\displaystyle\\sum_{a\\in A}\\sum_{b\\in B}\\mathtt{d i s t}(a,b)+|A|\\displaystyle\\sum_{b\\in B}\\sum_{c\\in C}\\mathtt{d i s t}(b,c)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Dividing both sides by $|A|\\cdot|B|\\cdot|C|$ we establish the inequality. ", "page_idx": 13}, {"type": "text", "text": "B Proofs of section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. When $k\\,=\\,n$ the result is valid because $\\mathtt{a v g}(A^{n})=0$ for every $A\\in A^{n}$ . We assume by induction that the result holds for $k+1$ and we prove that it also holds for $k$ . Let $A$ and $B$ be the clusters in $A^{k+1}$ that are merged to obtain $\\mathcal{A}^{k}$ , so ${\\dot{A}}^{k}=A^{k+1}\\cup(A\\cup B)-\\{A,B\\}$ . Let $S,T$ and $U$ be clusters in $\\mathcal{A}^{k}$ , with $T\\neq U$ . It is enough to prove that $\\mathsf{a v g}(S)\\leq\\mathsf{a v g}(T,U)$ . ", "page_idx": 13}, {"type": "text", "text": "Case 1) $A\\cup B\\not\\in\\{S,T,U\\}$ . In this case, $S,T,U\\,\\in\\,A^{k+1}$ and, then, by induction, $\\mathsf{a v g}(S)\\leq$ $\\mathsf{a v g}(T,U)$ . ", "page_idx": 13}, {"type": "text", "text": "Case 2) $A\\cup B=S$ and $S\\notin\\{T,U\\}$ . Since $A,B,T,U\\in A^{k+1}$ , the induction hypothesis assures that $\\mathsf{a v g}(A)\\,\\leq\\,\\mathsf{a v g}(T,U)$ and $\\mathsf{a v g}(B)\\,\\leq\\,\\mathsf{a v g}(T,U)$ and the average-link rule ensures that $\\mathsf{a v g}(A,B)\\leq\\mathsf{a v g}(T,U)$ . Since $\\mathsf{a v g}(S)$ is a convex combination of $\\mathsf{a v g}(\\bar{A}),\\mathsf{a v g}(B)$ and ${\\mathsf{a v g}}(A,B)$ , the above inequalities imply that $\\mathsf{a v g}(S)=\\mathsf{a v g}(A\\cup B)\\leq\\mathsf{a v g}(T,U)$ . ", "page_idx": 13}, {"type": "text", "text": "Case 3) $A\\cup B\\ =\\ S$ and $S~\\in~\\{T,U\\}$ . We assume w.l.o.g. that $\\textit{S}=\\textit{T}$ . The induction hypothesis and the average-link rule guarantee that $\\operatorname*{max}\\{\\mathsf{a v g}(A),\\mathsf{a v g}(B),\\mathsf{a v g}(A,B)\\}\\ \\leq$ $\\operatorname*{min}\\{\\mathbf{avg}(A,U),\\mathbf{avg}(B,U)\\}$ Since $\\mathsf{a v g}(S,U)$ is a convex combination of $\\mathsf{a v g}(A,U)$ and $\\mathsf{a v g}(B,U)$ and $\\mathsf{a v g}(S)$ is a convex combination of $\\mathsf{a v g}(A)$ , $\\mathsf{a v g}(B)$ and ${\\mathsf{a v g}}(A,B)$ , the above inequality implies that $\\mathsf{a v g}(S)=\\mathsf{a v g}(A\\cup B)\\leq\\mathsf{a v g}(T,U)$ . ", "page_idx": 13}, {"type": "text", "text": "Case 4) $S\\ \\ne\\ A\\cup B$ and $A\\cup B\\ \\in\\ \\{T,U\\}$ . We assume w.l.og. that $T\\ =\\ A\\cup B$ . Since $S,A,B,U\\in{\\mathcal{C}}^{k+1}$ , the induction hypothesis assures that $\\mathsf{a v g}(S)\\leq\\operatorname*{min}\\{\\mathsf{a v g}(A,U),\\mathsf{a v g}(B,U)\\}$ Since $\\mathsf{a v g}(T,U)$ is a convex combination of $\\mathsf{a v g}(A,U)$ and $\\mathsf{a v g}(B,U)$ , the above inequality assures that $\\mathsf{a v g}(S)\\leq\\mathsf{a v g}(T,U)$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "B.2 Lower bounds on cs-rat $\\mathtt{i o}_{\\mathtt{A V}}$ for other methods", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The following examples show that the cs-rati $\\mathtt{O A V}$ of complete-linkage, single-linkage and a random hierarchy can be much higher than that of average-link in metric spaces. ", "page_idx": 13}, {"type": "text", "text": "single-linkage. Consider the instance with $n$ points $x_{1},\\ldots,x_{n}$ in the real line, where $x_{i}=1$ , if $i=1$ , and $x_{i}=x_{i-1}+1-i\\epsilon$ , for $i>1$ . For $\\epsilon$ sufficiently small, single-linkage builds the $k$ -clustering ${\\mathcal{C}}=(x_{1},x_{2},\\ldots,x_{k-1}$ , $\\{x_{k},\\ldots,x_{n}\\})$ . We have that $\\mathsf{a v g}(\\{\\bar{x}_{k},\\ldots,x_{n}\\})$ is $\\Omega(n-k)$ while $\\mathtt{a v g}(x_{1},x_{2})=1-\\epsilon$ , so that cs-rati $\\circ_{\\mathtt{A V}}({\\mathcal{C}})$ is $\\Omega(n-k)$ . ", "page_idx": 13}, {"type": "text", "text": "complete-linkage. Let $t=2^{m}-1$ , where $m$ is a positive integer and let $p=2(t^{2}+t)$ . We build an instance whose set of points $\\mathcal{X}=A\\cup B\\cup C\\cup D\\cup E$ has $n=2p$ points, where $A,B,C,D$ and $E$ are sets of points in $\\mathbb{R}^{\\stackrel{\\bullet}{p}+1}$ that satisfy the following properties: ", "page_idx": 13}, {"type": "text", "text": "\u2022 the first coordinate of the points in $A\\cup B\\cup C\\cup D$ is the only one that has a value different than 0; \u2022 $A=\\{a_{1},\\ldots,a_{t}\\}$ and the first coordinate of $a_{i}$ is equal to $i+1/2$ ; ", "page_idx": 13}, {"type": "text", "text": "\u2022 $B=\\{b_{1},\\ldots,b_{t}\\}$ and the first coordinate of $b_{i}$ is equal to $-(i+1/2)$ ;   \n\u2022 $C$ has $t^{2}$ points and all have the first coordinate $1/2$ ;   \n\u2022 $D$ has $t^{2}$ points and all have the first coordinate $-1/2$ ;   \n\u2022 $E=\\{e_{1},\\ldots,e_{p}\\}$ , where the value of the first coordinate of $e_{i}$ is $t^{2}$ , the $(i+1)$ th coordinate has value $1.5t$ and all other coordinates have value equal to 0. ", "page_idx": 14}, {"type": "text", "text": "The distance between any two points in $\\mathcal{X}$ is given by the $\\ell_{1}$ metric. Hence, the distance between any two points in $E$ is $3t$ , the distance between points in $A\\cup B\\cup C\\cup D$ is at most $2t+1$ and the distance between a point in $A\\cup B\\cup C\\cup D$ and a point in $E$ is at least $t^{2}$ . For $i\\leq p$ , let $E_{i}=\\{e_{i},\\ldots,e_{p}\\}$ . ", "page_idx": 14}, {"type": "text", "text": "Thus, for $2\\,<\\,k\\,<\\,p\\,=\\,n/2$ , there is a way to break ties for which the $k$ -clustering obtained by complete-linkage is $\\mathcal{C}^{k}=(A\\cup C,B\\cup D,e_{1},e_{2},.~.~,e_{k-3},E_{k-2})$ . ", "page_idx": 14}, {"type": "text", "text": "We have that $\\operatorname*{max}\\{\\mathsf{d i s t}(a,d)\\,\\in\\,A\\times D\\}\\,\\leq\\,t+1$ , $\\operatorname*{max}\\{\\mathsf{d i s t}(b,c)\\,\\in\\,B\\times C\\}\\,\\leq\\,t+1$ and $\\operatorname*{max}\\{{\\mathsf{d i s t}}(a,b)\\in{\\dot{A}}\\times{\\dot{B}}\\}\\leq2t+1$ . Thus, we get that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{t^{2}+t)^{2}}\\left(\\sum_{x\\in A}\\sum_{y\\in B}\\mathrm{dist}(x,y)+\\sum_{x\\in A}\\sum_{y\\in D}\\mathrm{dist}(x,y)+\\sum_{x\\in C}\\sum_{y\\in B}\\mathrm{dist}(x,y)+\\sum_{x\\in C}\\sum_{y\\in D}\\mathrm{dist}(x,y)\\right)\\times\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since max $-\\mathtt{a v g}(\\mathcal{C})\\geq\\mathtt{a v g}(E_{k-2})=3t$ , we get that cs-rati ${\\mathsf{o}}_{\\mathtt{A V}}({\\mathcal{C}}^{k})$ is $\\Omega(t)$ and, hence, $\\Omega({\\sqrt{n}})$ . ", "page_idx": 14}, {"type": "text", "text": "random hierarchy. To analyze a random hierarchy, we first need to define how it is generated. We start with a random permutation of the points in $\\mathcal{X}$ and a clustering $\\mathcal{C}$ containing initially the cluster comprised by all points in $\\mathcal{X}$ . Let $x_{1},\\ldots,x_{n}$ be the points in $\\mathcal{X}$ according to the order given by the permutation. Then, we perform the following steps until we have $n$ clusters: ", "page_idx": 14}, {"type": "text", "text": "\u2022 $j\\leftarrow\\mathbf{a}$ randomly selected a number in the set $\\{1,2,\\ldots,n-1\\}$ .   \n\u2022 If the points $x_{j}$ and $x_{j+1}$ are in the same cluster $C\\in{\\mathcal{C}}$ \u2013 split $C$ into $C_{\\leq}=\\{x_{i}\\in C|i\\leq j\\}$ and the cluster $C_{>}=C-C_{\\leq}$ . \u2013 Update $\\mathcal{C}$ by replacing $C$ with $C_{\\le}$ and $C_{>}$ ", "page_idx": 14}, {"type": "text", "text": "After $t$ splits we have a clustering with $n-t$ clusters. ", "page_idx": 14}, {"type": "text", "text": "Now, we consider an instance with $n$ points and 3 groups $X,Y$ and $Z$ , that satisfy $\\left|X\\right|=\\left|Y\\right|=$ $(n-1)/2$ and $Z=\\{z\\}$ . The distance between any two points in $X$ is 1 and the same holds for $Y$ . Moreover, the distance between points in $X$ and $Y$ is 2. The distance of $z$ to any other point is $D>>n^{2}$ . Any $k$ -clustering, with $k\\geq3$ , has $\\mathtt{s e p\\\"m i n}\\le2$ because at least two clusters do not contain $z$ . Let $k\\leq n/2$ . The probability that $z$ is a singleton in the $k$ -clustering when $z\\not\\in\\{x_{1},x_{n}\\}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{\\binom{n-3}{k-3}}{\\binom{n-1}{k-1}}}={\\frac{(k-1)(k-2)}{(n-1)(n-2)}}<{\\frac{1}{4}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, with probability at least $3/4$ , there will be a cluster $C$ that contains $z$ and a point in $X\\cup Y$ , which implies that $\\dot{E}[\\mathsf{a v g}(C)]\\,\\geq\\,D/4n^{2}$ . Thus, with probability at least $3/4$ the $k$ -clustering induced by the random hierarchy has ${\\mathsf{s e p}}_{\\mathsf{a v}}\\;\\Omega(D/4n^{2})$ , when $z\\not\\in\\{x_{1},x_{n}\\}$ . Since the probability of $z\\not\\in\\{x_{1},x_{n}\\}$ is $(n-2)/n$ , the same bound holds when we drop this constraint. ", "page_idx": 14}, {"type": "text", "text": "B.3 On the approximation of average-link for cs-rati $\\mathsf{O}_{\\mathtt{A V}}$", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $n$ be an even number, $k=2$ and $\\epsilon$ a positive number very close to 0. Consider 4 set of points $S_{1},S_{2}$ , $S_{3}$ and $S_{4}$ , where $S_{1}=\\{s_{1}\\},S_{2}\\stackrel{=}{=}\\{s_{2}\\}$ and $S_{3}$ and $S_{4}$ have $n/2-1$ points each. We have d $\\mathtt{i s t}(x,y)=\\epsilon$ for $x,y\\in S_{3}$ , d $\\mathtt{i s t}(x,y)=\\epsilon$ for $x,y\\in S_{4}$ , di $\\mathbf{st}(s_{1},s_{2})=T$ and dist $:(x,y)=T$ for $(x,y)\\in S_{3}\\times S_{4}$ . In addition, we have dist $(s_{1},x)=2T$ for $x\\neq s_{2}$ and dist $(s_{2},y)=2T$ for $y\\ne s_{1}$ . ", "page_idx": 14}, {"type": "text", "text": "Clearly, the 4-clustering obtained by average-link is $(S_{1},S_{2},S_{3},S_{4})$ . Then, to obtain a 2- clustering, it merges the clusters $S_{1}$ and $S_{2}$ and, next, $S_{3}$ and $S_{4}$ , so that the final 2-clustering is $\\overset{\\vartriangle}{\\mathcal{A}^{2}}=\\phantom{\\frac{1}{2}}(S_{1}\\cup\\mathring{S_{2}},S_{3}\\cup S_{4})$ , which satisfies $\\mathtt{m a x-a v g}(A^{2})\\,=\\,T$ and $\\mathsf{s e p}_{\\mathrm{min}}(A)\\,=\\,2T$ . On the other hand, for the clustering $\\mathcal{S}=(S_{1}\\cup S_{3},S_{2}\\cup S_{4})$ , we have that max-avg $(S)$ is ${\\cal O}(T/n^{2})$ and $\\mathtt{s e p}_{\\mathtt{m i n}}(S)\\geq T$ . Thus, the approximation of average-link is $\\Omega(n^{2})$ ", "page_idx": 15}, {"type": "text", "text": "B.4 Triangle inequality is necessary for Theorem 3.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present an instance that shows that the assumption that points lie in a metric space is necessary to establish Theorem 3.4. ", "page_idx": 15}, {"type": "text", "text": "Let $A$ and $B$ be sets with $n/2\\!-\\!1$ and $n/2$ points, respectively. We have $\\mathtt{d i s t}(a,a^{\\prime})=1$ if $a,a^{\\prime}\\in A$ ; d $\\mathtt{i s t}(b,b^{\\prime})=1$ if $b,b^{\\prime}\\in\\dot{B}$ and dist $(a,b)=4$ if $(a,b)\\in\\dot{A}\\times B$ . Moreover, let $p$ be a point that is not in $A\\cup B$ . There is a point $a\\in A$ for which $\\mathtt{d i s t}(a,p)=n/2-2$ and for all other points $a^{\\prime}\\in A-\\{a\\}$ , $\\mathtt{d i s t}(a^{\\prime},p)\\overset{\\mathtt{\\Lambda}}{=}2$ . Moreover, d $\\mathtt{i s t}(p,b)=\\dot{4}$ for $b\\in B$ . ", "page_idx": 15}, {"type": "text", "text": "For this instance average-link builds the 2-clustering $\\mathcal{A}^{2}=(A\\cup\\{p\\},B)$ . We have that $\\mathtt{d i a m}(A U$ $p)=n/2-2$ and $\\mathsf{a v g}(A\\cup p,B)=4$ , Thus, cs-rat $\\bar{\\mathrm{i}}\\circ_{\\mathsf{D M}}(A^{2})$ is $\\Omega(n)$ . On the other hand, for the clustering $\\mathcal{A}^{\\prime}=(A,B\\cup p)$ , cs-rat $\\dot{\\mathsf{1}}\\mathsf{o}_{\\mathsf{D M}}(A^{\\prime})$ is $O(1)$ , so the approximation of average-link is $\\Omega(n)$ . ", "page_idx": 15}, {"type": "text", "text": "B.5 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "pPrroovoef. t hTeh see cfirosnt di noenqeu.ality holds because $\\begin{array}{r}{\\mathsf{a v g}(x,X)=\\frac{|X|-1}{|X|}\\mathsf{a v g}(x,X-x)}\\end{array}$ . Thus, we just need to ", "page_idx": 15}, {"type": "text", "text": "Let $S_{1}$ be the first cluster merged with $x$ by average-link and let $S_{i}$ , for $i>1$ , be the cluster merged with $S_{1}\\cup\\cdot\\cdot\\cdot\\cup S_{i-1}$ by average-link . Define $T_{0}:=\\{x\\}$ and, for $i\\geq1,T_{i}:=T_{i-1}\\cup S_{i}$ . Furthermore, define $e_{i}$ and $m_{i}$ as $e_{i}:={\\sf a v g}(T_{i-1},S_{i})$ and $m_{i}:=\\mathsf{a v g}(x,T_{i}-x)$ , respectively. Note that there is $t$ for which $T_{t}=X$ and, hence, $m_{t}={\\mathsf{a v g}}(x,X-x)$ . ", "page_idx": 15}, {"type": "text", "text": "We have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{i+1}=\\cfrac{\\vert T_{i}\\vert-1}{\\vert T_{i+1}\\vert-1}\\mathrm{avg}(x,T_{i}-x)+\\cfrac{\\vert S_{i+1}\\vert}{\\vert T_{i+1}\\vert-1}\\mathrm{avg}(x,S_{i+1})\\leq}\\\\ &{\\frac{\\vert T_{i}\\vert-1}{\\vert T_{i+1}\\vert-1}m_{i}+\\cfrac{\\vert S_{i+1}\\vert}{\\vert T_{i+1}\\vert-1}(m_{i}+e_{i+1})=m_{i}+\\cfrac{\\vert S_{i+1}\\vert}{\\vert T_{i+1}\\vert-1}e_{i+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inequality follows from the triangle inequality. ", "page_idx": 15}, {"type": "text", "text": "Let us consider the beginning of the iteration in which $T_{i-1}$ and $S_{i}$ are merged. At this point we have $\\ell\\geq1$ clusters $Y_{1},\\ldots,Y_{\\ell}$ such that $Y=Y_{1}\\cup\\cdot\\cdot\\cdot\\cup Y_{\\ell}$ and $\\ell^{\\prime}$ clusters $Z_{1},\\ldots,Z_{\\ell^{\\prime}}$ such that $Z=Z_{1}\\cup\\cdots\\cup Z_{\\ell}$ . Note that there exist $i$ and $j$ such that $\\mathsf{a v g}(Y_{i},Z_{j})\\leq\\mathsf{a v g}(Y,Z)$ . Thus, we must have $e_{i}\\leq\\mathsf{a v g}(Y,Z)$ , otherwise average-link would merge $Y_{i}$ and $Z_{j}$ rather than $T_{i-1}$ and $S_{i}$ . ", "page_idx": 15}, {"type": "text", "text": "To establish the result, we show by induction that $m_{i}\\le{\\mathsf{a v g}}(Y,Z)\\cdot H_{|T_{i}|-1}$ , for $i\\geq1$ . The lemma is then established by taking $i=t$ , where $t$ satisfies $T_{t}=X$ . ", "page_idx": 15}, {"type": "text", "text": "For $i=1$ , we have $m_{1}=e_{1}\\leq\\mathsf{a v g}(Y,Z)<\\mathsf{a v g}(Y,Z)\\cdot H_{|T_{1}|-1}$ . We assume by induction that $m_{i-1}\\leq\\mathsf{a v g}(Y,Z)\\cdot H_{|T_{i-1}|-1}$ . By inequality ( 7)-(8), ", "page_idx": 15}, {"type": "equation", "text": "$$\nn_{i}\\leq m_{i-1}+e_{i}{\\frac{|S_{i}|}{|T_{i}|-1}}\\leq\\arg(Y,Z)\\left(\\sum_{h=1}^{|T_{i-1}|-1}{\\frac{1}{h}}\\right)+\\arg(Y,Z)\\left(\\sum_{h=|T_{i-1}|}^{|T_{i}|-1}{\\frac{1}{h}}\\right)=\\arg(Y,Z)\\cdot H_{|T_{i}|-1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Proof of Lemma 3.5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. First, we note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|B_{i-1}|=\\sum_{h=0}^{i-1}|A_{i}|=\\sum_{h=0}^{i-1}(h+1)!-h!=i!,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $i\\geq1$ . ", "page_idx": 16}, {"type": "text", "text": "Moreover, for $i\\geq2$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{a v g}(A_{i},B_{i-1})=\\displaystyle\\frac{|A_{i-1}|}{|B_{i-1}|}\\mathsf{a v g}(A_{i},A_{i-1})+\\displaystyle\\frac{|B_{i-2}|}{|B_{i-1}|}\\mathsf{a v g}(A_{i},B_{i-2})=}\\\\ {\\displaystyle\\frac{|A_{i-1}|}{|B_{i-1}|}\\mathsf{a v g}(A_{i},A_{i-1})+\\frac{|B_{i-2}|}{|B_{i-1}|}(\\mathrm{avg}(A_{i},A_{i-1})+\\mathsf{a v g}(A_{i-1},B_{i-2}))=}\\\\ {\\mathsf{a v g}(A_{i},A_{i-1})+\\displaystyle\\frac{|B_{i-2}|}{|B_{i-1}|}\\mathsf{a v g}(A_{i-1},B_{i-2})=}\\\\ {\\displaystyle\\left(1+\\frac{|B_{i-2}|}{|B_{i-1}|}\\right)\\mathsf{a v g}(A_{i-1},B_{i-2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last identity follows because $\\operatorname{avg}(A_{i},A_{i-1})=p_{i}-p_{i-1}=\\operatorname{avg}(A_{i-1},B_{i-2}).$ ", "page_idx": 16}, {"type": "text", "text": "By applying the above equation successively, we conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{avg}(A_{i},B_{i-1})=(i+1)\\cdot\\operatorname{avg}(A_{1},B_{0})=(i+1)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and, hence, ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{i}=1+\\sum_{h=1}^{i-1}(h+1)=\\frac{i(i+1)}{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathtt{d i a m}(B_{i-1})=p_{i-1}-p_{0}=p_{i-1}=\\frac{i(i-1)}{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we show that at the beginning of the step $(n-t)+i$ average-link keeps a clustering that contains the cluster $B_{i-1}$ and the clusters $A_{j}$ , for $i\\leq j\\leq t-1$ . First, we observe that after $n-t$ steps average-link produces a $t$ -clustering $(A_{0},\\ldots,A_{t-1})$ since points in the same group $A_{i}$ are located at the same position. We analyze what happens in the remaining $t-1$ steps. ", "page_idx": 16}, {"type": "text", "text": "For $i=1$ the result holds because $B_{0}=A_{0}$ . We assume as an induction hypothesis that at beginning of the step $(n-t)+i$ , we have the clusters $B_{i-1}$ and $A_{j}$ , for $j\\geq i$ . By construction, for $i\\le r<s$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{avg}(A_{s},A_{r})=p_{s}-p_{r}>p_{i+1}-p_{i}=\\operatorname{avg}(A_{i},B_{i-1}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, ", "page_idx": 16}, {"type": "equation", "text": "$$\ni-1=\\mathsf{a v g}(A_{i},B_{i-1})<\\mathsf{a v g}(A_{j},B_{i-1}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $j~>i$ . Thus, average \u2212link prefers merging $A_{i}$ and $B_{i-1}$ rather than any other pair of clusters, which completes the inductive step. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "D Proofs from section 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Proof of Proposition 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\mathcal{C}^{*}=(C_{1}^{*},\\ldots,C_{k}^{*})$ be a $k$ -clustering that maximizes $\\mathtt{s e p}_{\\mathtt{a v}}$ . Let $\\mathcal{Q}$ be the family of sets of points $Q$ such that $|Q|=k$ and $Q$ intersects all clusters $C_{1}^{*},\\ldots,C_{k}^{*}$ . Let $P=\\{p_{1},\\ldots,p_{k}\\}$ be a set in $\\mathcal{Q}$ that satisfies $\\mathtt{a v g}(P)\\geq\\mathtt{a v g}(Q)$ , for every $Q\\in\\mathcal{Q}$ . Moreover, let $U=\\{u_{1},\\ldots,u_{k}\\}$ be a ", "page_idx": 16}, {"type": "text", "text": "set of $k$ points where $u_{i}$ is randomly selected from $C_{i}^{*}$ . It follows from the choice of $P$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{k(k-1)}{2}\\mathbf{avg}(P)\\geq\\frac{k(k-1)}{2}E[\\mathbf{avg}(U)]=}\\\\ {\\displaystyle E\\left[\\sum_{i=1}^{k-1}\\sum_{j=i+1}^{k}\\mathbf{dist}(u_{i},u_{j})\\right]=\\sum_{i=1}^{k-1}\\sum_{j=i+1}^{k}E\\left[\\mathbf{dist}(u_{i},u_{j})\\right]=\\sum_{i=1}^{k-1}\\sum_{j=i+1}^{k}\\mathbf{avg}(C_{i}^{*},C_{j}^{*})\\geq}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D.2 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Let $P=\\{p_{i}|1\\leq i\\leq k\\}$ be the $k$ points given by Proposition 4.1 and let $h$ be a function that maps each point $p\\in P$ into its cluster in $\\mathcal{A}^{k}$ . Moreover, let $Y$ and $Z$ be clusters in $\\mathcal{A}^{k}$ that satisfy $\\mathsf{a v g}(Y,Z)\\stackrel{\\cdot}{=}\\mathsf{s e p}_{\\mathsf{m i n}}(A^{k})$ . ", "page_idx": 17}, {"type": "text", "text": "Let $p$ and $p^{\\prime}$ be distinct points in $P$ . We consider two cases: ", "page_idx": 17}, {"type": "text", "text": "Case 1) $p$ and $p^{\\prime}$ belong to the same cluster $A$ in $\\mathcal{A}^{k}$ . From Theorem 3.3 we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathtt{d i s t}(p,p^{\\prime})\\leq\\mathtt{d i a m}(A)\\leq2H_{|A|}\\mathtt{a v g}(Y,Z)=2H_{|A|}\\mathtt{s e p}_{\\mathtt{m i n}}(A^{k})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{p,p^{\\prime}\\in P\\cap A}\\mathbf{dist}(p,p^{\\prime})\\leq\\sum_{p,p^{\\prime}\\in P\\cap A}2H_{|A|}\\mathbf{sep}_{\\mathtt{m i n}}(A^{k}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By considering all clusters $A\\in{\\mathcal{A}}^{k}$ we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{\\underset{h(p)=h(p^{\\prime})}{p,p^{\\prime}\\in P}}\\mathbf{dist}(p,p^{\\prime})\\leq\\ \\sum_{\\underset{h(p)=h(p^{\\prime})}{p,p^{\\prime}\\in P}}2H_{n}\\mathbf{sep}_{\\mathtt{m i n}}({\\mathcal{A}}^{k})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Case 2) $p$ and $p^{\\prime}$ belong, respectively, to different clusters $A$ and $A^{\\prime}$ in $\\mathcal{A}^{k}$ . We consider two subcases: subcase 2.1) $k\\ =\\ 2$ . In this case, from the triangle inequality, we have that di ${\\sf s t}(p,p^{\\prime})\\;=\\;$ $\\mathsf{a v g}(p,p^{\\prime})\\leq\\mathsf{a v g}(p,A)\\!+\\!\\mathsf{a v g}(A,A^{\\prime})\\!+\\!\\mathsf{a v g}(A^{\\prime},p^{\\prime}$ ). By using Lemma 3.2, we have that ${\\mathsf{a v g}}(p,A)\\leq$ $H_{n-1}\\mathsf{a v g}(A,A^{\\prime})=H_{n-1}\\mathsf{s e p}_{\\mathrm{min}}(A^{k})$ and avg(p\u2032, A\u2032) \u2264Hn\u22121avg(A, A\u2032) = Hn\u22121sepmin(Ak). Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{p,p^{\\prime}\\in P}{h(p)\\neq h(p^{\\prime})}}\\mathsf{d i s t}(p,p^{\\prime})=\\mathsf{d i s t}(p,p^{\\prime})\\leq2H_{n-1}\\mathsf{s e p}_{\\mathsf{m i n}}(A^{k})+\\mathsf{a v g}(A,A^{\\prime}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first identity holds because $P=\\{p,p^{\\prime}\\}$ . ", "page_idx": 17}, {"type": "text", "text": "subcase 2.2) $k>2$ . Let $S$ be a cluster in ${\\mathcal{A}}^{k}-\\{A,A^{\\prime}\\}$ . From the triangle inequality, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{dist}(p,p^{\\prime})=\\mathsf{a v g}(p,p^{\\prime})\\leq\\mathsf{a v g}(p,A)+\\mathsf{a v g}(A,S)+\\mathsf{a v g}(S,A^{\\prime})+\\mathsf{a v g}(A^{\\prime},p^{\\prime})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If $|{\\cal A}|=1$ , $\\mathsf{a v g}(p,A)=0\\leq H_{|A|}\\cdot\\mathsf{s e p}_{\\mathsf{m i n}}(A^{k})$ . Moreover, if $|{\\cal A}|\\ge2$ , it follows from Lemma 3.2 that $\\operatorname{avg}(p,A)\\leq H_{|A|}\\cdot\\operatorname{avg}(Y,Z)=H_{|A|}\\operatorname{\\mathbf{sep}\\!\\operatorname{min}}(A^{k}$ ). Analogously, we have ${\\mathsf{a v g}}(p^{\\prime},A^{\\prime})\\leq$ $H_{|A^{\\prime}|}\\mathbf{s}\\mathsf{e p}_{\\operatorname*{min}}(A^{k})$ . Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathtt{d i s t}(p,p^{\\prime})\\leq H_{|A|}\\mathtt{s e p}_{\\mathtt{m i n}}(\\mathcal{A}^{k})+\\mathtt{a v g}(A,S)+\\mathtt{a v g}(S,A^{\\prime})+H_{|A^{\\prime}|}\\mathtt{s e p}_{\\mathtt{m i n}}(\\mathcal{A}^{k}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By averaging over all possible $S\\in\\mathcal{A}^{k}-\\{A,A^{\\prime}\\}$ we get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{dist}(p,p^{\\prime})\\leq\\cdot2H_{n}\\mathbf{s}\\mathbf{ep}_{\\operatorname*{min}}(A^{k})+{\\frac{1}{k-2}}\\sum_{S\\notin\\{A,A^{\\prime}\\}}(\\operatorname{avg}(A,S)+\\operatorname{avg}(S,A^{\\prime}))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By adding over all points $p\\in P\\cap A$ and $p^{\\prime}\\in P\\cap A^{\\prime}$ we get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{\\substack{p\\in P\\cap A\\,\\,p^{\\prime}\\in P\\cap A^{\\prime}}}2H_{n}\\mathbf{s}\\mathbf{ep}_{\\mathrm{min}}(A^{k})+\\frac{|P\\cap A|\\cdot|P\\cap A^{\\prime}|}{k-2}\\sum_{\\substack{S\\notin\\{A,A^{\\prime}\\}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By adding the above inequalities for $p,p^{\\prime}\\in P$ , with $h(p)\\neq h(p^{\\prime})$ , we get that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{p,p^{\\prime}\\in P_{}\\atop h(p)\\neq h(p^{\\prime})}\\tt d i s t(p,p^{\\prime})\\leq\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{y,r\\,\\in\\,P}{(r)\\neq b(r^{\\prime})}}2H_{n}\\cdot\\mathsf{s e p}_{\\mathrm{nin}}(A^{k})+\\frac{1}{k-2}\\sum_{A,A^{\\prime}\\in A^{k}\\atop A\\neq A^{\\prime}}|P\\cap A|\\cdot|P\\cap A^{\\prime}|\\sum_{\\stackrel{S\\notin\\{A,A^{\\prime}\\}}{S\\notin\\{A,A^{\\prime}\\}}}(\\arg(A,S)+\\arg(S,A^{\\prime})=\\arg(A,S))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{y,r\\,\\in\\,P}{(r)\\neq k\\,(r^{\\prime})}}2H_{n}\\cdot\\operatorname{sep}_{\\operatorname{nin}}(A^{k})+\\frac{1}{k-2}\\sum_{\\stackrel{A,A^{\\prime}\\in\\,A^{k}}{A\\neq A^{\\prime}}}\\left(|P\\cap(A\\cup A^{\\prime})|\\right)\\cdot(k-|P\\cap(A\\cup A^{\\prime})|)\\cdot\\operatorname{avg}(A,A^{\\prime})\\leq k-2.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality holds because $(|P\\cap(A\\cup A^{\\prime})|)\\cdot(k-|P\\cap(A\\cup A^{\\prime})|)\\leq k^{2}/4.$ ", "page_idx": 18}, {"type": "text", "text": "If we compare inequalities (16)-(19) with inequality (15), we conclude that (16)-(19) also hold for the subscase $k=2$ . ", "page_idx": 18}, {"type": "text", "text": "Then, by adding inequality (14) with the inequalities (16)-(19) and also using the fact $\\mathsf{s e p}_{\\mathrm{min}}(A^{k})\\leq$ $\\mathsf{s e p}_{\\mathsf{a v}}(A^{k})$ , we get that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{x,y^{\\prime}\\in P}{x\\neq y^{\\prime}}}\\mathrm{dist}(p,p^{\\prime})\\leq2H_{n}\\frac{k(k-1)}{2}\\mathbf{sep}_{\\mathrm{nin}}(A^{k})+k\\sum_{\\stackrel{A,A^{\\prime}\\in A^{k}}{A\\neq x^{\\prime}}}\\mathbf{avg}(A,A^{\\prime})\\leq(2H_{n}+k)\\frac{k(k-1)\\mathbf{sep}_{\\mathrm{av}}(A,A^{\\prime})}{2}\\leq C_{1},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proposition 4.1 ensures that ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{k(k-1)}{2}}\\mathrm{OPT}_{\\mathtt{S E P}}(k)\\leq{\\frac{k(k-1)}{2}}\\mathsf{a v g}(P)=\\sum_{p,p^{\\prime}\\in P}\\mathsf{d i s t}(p,p^{\\prime})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, from the two previous inequalities, we conclude that ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathsf{s e p}}_{\\mathsf{a v}}(A^{k})\\geq{\\frac{{\\mathsf{O P T}}_{\\mathsf{S E P}}(k)}{2H_{n}+k}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D.3 The $\\tt s e p_{a v}$ criterion for other linkage methods ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The following instances show that the separability of both single-linkage and complete-linkage can be much lower than $\\frac{\\mathrm{OPT}_{\\mathtt{S E P}}(k)}{\\log n}$ . ", "page_idx": 18}, {"type": "text", "text": "For single-linkage, \u221aconsider the instance $\\mathcal{X}=A\\cup B\\cup\\{p\\}$ , where $A$ contains $n-1-{\\sqrt{n}}$ points and $B$ contains $\\sqrt{n}$ points $b_{1},\\ldots,b_{\\sqrt{n}}$ . Moreover, we have $\\mathtt{d i s t}(x,y)=\\epsilon$ , for $x,y\\in A$ , di $\\mathtt{s t}(b_{i},x)=i$ for every point $x\\in A$ and dis $\\boldsymbol{\\mathsf{\\bar{\\rho}}}(b_{i},b_{j})=|i-j|$ . Moreover, ${\\tt d i s t}(p,x)=1+\\epsilon$ , for every point $x\\in A$ . and ${\\mathsf{d i s t}}(p,b_{i})=1+\\epsilon+i$ In this case, single-linkage b\u221auilds the clustering $(A\\cup{\\bar{B}},\\{p\\})$ . We have that $\\mathsf{s e p a v}(A\\cup B,p)\\leq2$ , while $\\mathtt{s e p}_{\\mathtt{a v}}(A\\cup p,B)$ is $\\textstyle{\\overbrace{\\Omega(\\sqrt{n})}}$ . ", "page_idx": 18}, {"type": "text", "text": "Regarding complete-linkage, we consider the instance presented at Section B.2, but without the set $E$ , that is, the set of points is $\\mathcal{X}=A\\cup B\\cup C\\cup D$ . When $k=2$ , complete-linkage builds the clustering $(A\\cup C,B\\cup D)$ that has $\\mathtt{s e p}_{\\mathtt{a v}}\\,O(1)$ while the clustering $(A,C\\cup D\\cup B)$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{s e p}_{\\mathsf{a v}}(A,C\\cup D\\cup B)\\geq\\frac{\\frac{t^{2}}{2}(2t^{2}+t)}{t(2t^{2}+t))}=\\frac{t}{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$O(\\frac{\\mathrm{OPT}_{\\mathtt{S E P}}(k)}{\\sqrt{n}})$ $t=\\Theta({\\sqrt{n}})$ , we conclude that the separability of complete-linkage for this instance is ", "page_idx": 19}, {"type": "text", "text": "D.4 Separability and cohesion can be conflicting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall that for instance $\\mathcal{T}_{k}^{s e p}$ average-link builds the $k$ -clustering $\\mathcal{A}^{k}=(S_{1},S_{2},s_{1},s_{2},\\ldots,s_{k-2})$ Note that max-diam $(\\mathcal{A}^{k})^{\\stackrel{*}{}}=\\mathtt{m a x-a v g}(A^{k})=\\epsilon$ . Let $\\mathcal{A^{\\prime}}$ be a $k$ -clustering different from $\\mathcal{A}^{k}$ . We argue that max-diam $(\\mathcal{A}^{\\prime})\\geq1$ and $\\mathtt{m a x-a v g}(A^{\\prime})$ is $\\Omega(1/k^{2})$ . In fact, if $\\mathcal{A^{\\prime}}$ has a cluster $A$ that satisfies $|{\\cal A}|\\ge2$ and $|A\\cap S_{3}|\\geq1$ , then max-diam $\\operatorname{\\rho}(A^{\\prime})\\geq1$ and $\\mathtt{m a x-a v g}(A^{\\prime})$ is $\\Omega(1/k^{2})$ . Otherwise, if $\\mathcal{A^{\\prime}}$ does not have such a cluster, then all points in $S_{3}$ must be singletons in $\\mathcal{A}^{\\prime}$ . Since $A^{\\prime}\\neq A^{k}$ , there is a cluster in $\\mathcal{A^{\\prime}}$ that contains both a point in $S_{1}$ and a point in $S_{2}$ . Thus, max-diam $\\left(\\mathcal{A}^{\\prime}\\right)=D$ and $\\mathtt{m a x-a v g}(A^{\\prime})$ is $\\Omega(D/k^{2})$ . ", "page_idx": 19}, {"type": "text", "text": "Let $\\mathcal{M}$ be the class of methods with bounded approximation regarding max-diam or to max-avg. Then any method $M\\in\\mathcal{M}$ builds the clustering ${\\dot{A}}^{k}$ . Since $\\mathsf{s e p}_{\\mathsf{a v}}(A^{k})$ is $O(D/k)$ and there is a $k$ -clustering for $\\mathcal{T}_{k}^{s e p}$ whose $\\tt s e p_{a v}$ is $\\Omega(D)$ , we conclude that the approximation factor of any method $M\\in\\mathcal{M}$ regarding $\\mathtt{s e p a v}$ is $O(1/k)$ . ", "page_idx": 19}, {"type": "text", "text": "Now, we consider $\\mathtt{s e p}_{\\mathtt{m i n}}$ . We have that $\\mathsf{s e p}_{\\mathrm{min}}(\\mathcal A^{k})=1$ . Let $\\boldsymbol{B}=(B_{1},\\ldots,B_{k})$ be a $k$ -clustering with the following properties: (i) $\\left|B_{i}\\cap S_{3}\\right|\\geq1$ for each $i\\leq k-2$ ; (ii) each $B_{i}$ , with $i\\geq2$ , has exactly one point in $S_{1}\\cup S_{2}$ (iii) $B_{k-1}$ has a point in $S_{1}$ and $B_{k}$ has a point in $S_{2}$ . We have that $\\mathtt{s e p\\_m i n}(B)$ is $\\Omega(D)$ . Thus, any method $M\\in\\mathcal{M}$ has approximation $O(1/D)$ to $\\mathtt{s e p}_{\\mathtt{m i n}}$ , that is, the approximation is unbounded in terms of $n$ . ", "page_idx": 19}, {"type": "text", "text": "E Proof of Theorem 5.4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Let $\\mathcal{T}$ be the instance obtained by augmenting the instance $\\mathcal{I}^{C S}$ , presented right after Theorem 3.4, with the points $x_{0},\\ldots,x_{t-1}$ , where ${\\mathsf{d i s t}}(x_{i},A_{i})=t+1+\\epsilon$ and for $i\\neq j$ , dis $\\mathsf{t}(x_{i},x_{j})=$ $|p_{j}-p_{i}|+2({t+1+\\epsilon})$ and ${\\tt d i s t}(x_{i},A_{j})=|p_{j}-p_{i}|+t+1+\\epsilon$ . ", "page_idx": 19}, {"type": "text", "text": "Consider $t=k$ . We argue that the $(k+1)$ -clustering obtained by average-link for $\\mathcal{T}$ consists of the clusters $(B_{k-1},\\{x_{0}\\},\\dots,\\{x_{k-1}\\})$ . In fact, in its first steps average-link obtains the $2k$ - clustering $(A_{0},\\ldots,A_{k-1},x_{0},\\ldots,x_{k-1})$ since the distance between points in $A_{i}$ is 0. In the next $k-1$ steps, average-link does not make a merge involving a point $x_{i}$ because the average distance of $x_{i}$ to any other cluster is larger $k+1$ and, by Lemma 3.5, the average distance between $B_{i-2}$ and $A_{i}$ is $i+1\\le k+1$ . Thus, the execution of average-link for $\\mathcal{T}$ merges the same clusters that are merged in the instance $\\mathcal{I}^{C S}$ and, then, ends up with the $(k\\!+\\!1)$ -clustering $(B_{k-1},\\{x_{0}\\},\\dots,\\{x_{k-1}\\})$ . ", "page_idx": 19}, {"type": "text", "text": "Thus, for instance $\\mathcal{T}$ , the maximum diameter of a cluster in $\\mathcal{A}^{k}$ is at least ${\\tt d i a m}(B_{k-1})$ , which is $\\Omega(k^{2})$ , while the $k$ -clustering $(x_{0}\\cup A_{0},\\ldots,x_{k-1}\\cup A_{k-1})$ ) has diameter $k+\\epsilon$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "F Experiments: extra details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 2 presents our datasets with their main characteristics. ", "page_idx": 19}, {"type": "text", "text": "Figures (1)-(6) show the results obtained by single-linkage, complete-linkage and average-link, for all datasets and the different criteria considered in the paper. For a given dataset $D$ , method $M$ and criterion $\\alpha$ , the height of the bar is given by the average of $m_{k}$ for every $k$ considered in our experiments, where $m_{k}$ is the ratio between the value of criterion $\\alpha$ achieved by method $M$ on dataset $D$ divided by the best value for criterion $\\alpha$ , among those achieved by single-linkage, average-link and complete-linkageon dataset $D$ . ", "page_idx": 19}, {"type": "table", "img_path": "2LuSHTFWzK/tmp/abc5be7d89ca77ecf86ad069087f406b14d9f8b67868dab3bda4407652ffb956.jpg", "table_caption": ["Table 2: Datasets "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "2LuSHTFWzK/tmp/a8b546223c17d7ae10b2416d9459b3ff57bd33093219cb61e0a5f01b4629a7f2.jpg", "img_caption": ["Figure 1: Results for the max-diam for the different datasets. For interpreting the bars, the lower the better "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Regarding the cohesion criteria complete-linkage presents the best results for max-diam, followed by average-link. For max-avg, again complete-linkage and average-link are the best, with the latter having a slight advantage. ", "page_idx": 20}, {"type": "text", "text": "In terms of the separability criteria, average-linkis much better than the other methods for $\\mathtt{s e p}_{\\mathtt{m i n}}$ , while for $\\tt s e p_{a v}$ there is a balance between average-link and single-linkage. ", "page_idx": 20}, {"type": "text", "text": "For the criteria that combine cohesion and separability, average-linkis superior for cs-rati $\\mathsf{\\Pi}_{\\mathsf{A V}}$ , while there is a balance between average-link and complete-linkage for cs-ratioDM. ", "page_idx": 20}, {"type": "text", "text": "Table 3 and 4 show the results for the experiment described in Section 6, when the Euclidean distance is replaced with the $\\ell_{1}$ and $\\ell_{\\infty}$ norm, respectively. The observations made in Section 6 also hold when these metrics are used. ", "page_idx": 20}, {"type": "text", "text": "Finally, we note that the variance of the results for average-link is small. Indeed, an entry (average) close to 1 (e.g. 0.96) cannot have an underlying large variance because 1 is the maximum possible value for an entry. Since most entries for average-link are close to 1, one can conclude that the variance of its results is usually small. In the supplemental material, we have .csv files with our full results. ", "page_idx": 20}, {"type": "image", "img_path": "2LuSHTFWzK/tmp/6d2bba7ebe10068f1aae2001a10c8c46662e0853501740af2eb2c64c632ba816.jpg", "img_caption": ["Figure 2: Results for the max-avg for the different datasets. For interpreting the bars, the lower the better "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "2LuSHTFWzK/tmp/99717a8c1526d52083af085a8d03e810c52bffe007040c354f6f75fbd7ff6bf6.jpg", "img_caption": ["Figure 3: Results for the $\\mathtt{s e p}_{\\mathtt{m i n}}$ for the different datasets. For interpreting the bars, the higher the better "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 21}, {"type": "image", "img_path": "2LuSHTFWzK/tmp/0f0618c7f820cc90246e6ccbea38a93e0b6d7312c05c1102d8e787a005bb1069.jpg", "img_caption": ["Figure 4: Results for the $\\tt s e p_{a v}$ for the different datasets. For interpreting the bars, the higher the better "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "2LuSHTFWzK/tmp/1b2d2b1c3e238f11d95d475f64af96e9ec5e4366e247a7099d9fa04767ca8185.jpg", "img_caption": ["Figure 5: Results for the cs-rati $.O_{\\tt A V}$ for the different datasets and methods. For interpreting the bars, the lower the better "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. ", "page_idx": 22}, {"type": "image", "img_path": "2LuSHTFWzK/tmp/4933c93b60309726a4421192565405ffb4f5bcb715eee53b989887f4ef04851f.jpg", "img_caption": ["Figure 6: Results for the cs-rati ${\\tt O p m}$ for the different datasets and methods. For interpreting the bars, the lower the better "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 3: Average ratio between the result of a method and the best one for each criterion and each group of $k$ . The best results are bold-faced. Distances are computed using $\\ell_{1}$ norm ", "page_idx": 23}, {"type": "table", "img_path": "2LuSHTFWzK/tmp/f49b44815861d1d3d263f1cbc4722bb021cf6a6b77ee751ac78f36a438be27f9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We included a section at the end of the paper. ", "page_idx": 23}, {"type": "text", "text": "Table 4: Average ratio between the result of a method and the best one for each criterion and each group of $k$ . The best results are bold-faced. Distances are computed using $\\ell_{\\infty}$ norm ", "page_idx": 23}, {"type": "table", "img_path": "2LuSHTFWzK/tmp/1d5526a5b844ec792c21b681a0867a852f6dbb8a7928a117715a41b3342180f1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes]   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 25}, {"type": "text", "text": "\u2022 Providing as much information as possible in the supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The details are in the paper and also in the supplemental material. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have not included error bars because they do not help much in our case. However, from the tables and our analyses, the reader should have a clear idea of the variability of our results (see last paragraph of Section F). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: This information is irrelevant to reproducing our experiments or reaching our conclusions. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper is mostly about theoretical results. We provide several new analyses for algorithms that are widely known. We do not see a clear societal impact that deserves to be mentioned. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We cite the datasets we use in Appendix F ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Our supplementary material contains our codes. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]