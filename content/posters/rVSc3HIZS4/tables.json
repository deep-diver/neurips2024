[{"figure_path": "rVSc3HIZS4/tables/tables_8_1.jpg", "caption": "Table 1: Side-by-side evaluation for Education Dialogue using Flan-T5 XL as the prompted preference model. Each entry is the average preference of 1,600 conversations generated with row method y, over ones generated with column method y'. We evaluate each method using 3 different seeds, compute 3 \u00d7 3 comparisons matrix and report the mean (the standard deviation is reported in Appendix D).", "description": "This table presents a comparison of different methods for generating conversations in the Education Dialogue task.  The rows and columns represent different conversation generation methods. Each cell shows the average preference score (out of 1600 conversations) for the row method compared to the column method.  Higher scores indicate better performance.  The methods compared include supervised fine-tuning (SFT), single-turn and multi-turn reinforcement learning from human feedback (RLHF), and the proposed MTPO algorithms. The standard deviation is available in Appendix D for a more detailed understanding of the variations.", "section": "6 Experiments"}, {"figure_path": "rVSc3HIZS4/tables/tables_9_1.jpg", "caption": "Table 2: Side-by-side evaluation for Education Dialogue using Gemini Ultra as the prompted preference model. Each entry is the average preference of 1,000 conversations generated with row method y, over ones generated with column method y'.", "description": "This table presents a side-by-side comparison of different methods for generating conversations in the Education Dialogue environment using Gemini Ultra for preference evaluation.  It shows the average preference score for conversations generated by each method against those generated by every other method.  Higher scores indicate better performance.", "section": "6 Experiments"}, {"figure_path": "rVSc3HIZS4/tables/tables_9_2.jpg", "caption": "Table 3: Car Dealer experiments averaged across 5 seeds and reported with 95% confidence interval.", "description": "This table compares the performance of different reinforcement learning methods in the Car Dealer environment.  The \"Online oracle\" columns represent scenarios where reward is obtained directly from an oracle. The \"Model from preferences data\" columns show results when a model is trained using preference data to approximate rewards.  Both single-turn (RLHF) and multi-turn (MTPO) methods are compared. The reward is measured by the final sale price achieved. The results indicate that MTPO achieves comparable performance to RL, even when trained on preference data instead of explicit rewards.", "section": "6 Experiments"}, {"figure_path": "rVSc3HIZS4/tables/tables_26_1.jpg", "caption": "Table 4: Hyperparameters of all multi-turn algorithms.", "description": "This table lists the hyperparameters used for the three multi-turn algorithms evaluated in the paper: RLHF, MTPO, and MTPO-\u03c4.  The hyperparameters include settings related to the number of generations and updates per context, KL regularization, mixing coefficient (for MTPO-\u03c4 only), batch size, GAE coefficient, policy learning delay, optimizer, optimizer decay, policy learning rate, value learning rate, and value initialization.  The consistent use of AdaFactor as the optimizer across all algorithms is notable.", "section": "D Hyperparameters and additional experimental results"}, {"figure_path": "rVSc3HIZS4/tables/tables_26_2.jpg", "caption": "Table 1: Side-by-side evaluation for Education Dialogue using Flan-T5 XL as the prompted preference model. Each entry is the average preference of 1,600 conversations generated with row method y, over ones generated with column method y'. We evaluate each method using 3 different seeds, compute 3 \u00d7 3 comparisons matrix and report the mean (the standard deviation is reported in Appendix D).", "description": "This table presents a comparison of different methods for evaluating conversations in the Education Dialogue task.  It shows the average preference scores for each method compared to all other methods.  The scores are based on 1600 conversations per comparison, using three different random seeds for each evaluation to improve statistical robustness, and standard deviations are given in the appendix.  The methods compared include supervised fine-tuning (SFT), single-turn RLHF (with reward and value functions), and the multi-turn algorithms MTPO and MTPO-\u03c4. ", "section": "6 Experiments"}]