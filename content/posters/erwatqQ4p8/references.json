{"references": [{"fullname_first_author": "R. A. Jacobs", "paper_title": "Adaptive mixtures of local experts.", "publication_date": "1991-01-01", "reason": "This paper introduces the foundational concept of mixture of experts (MoE) models, which forms the theoretical basis for the current work's analysis of self-attention mechanisms."}, {"fullname_first_author": "M. I. Jordan", "paper_title": "Hierarchical mixtures of experts and the EM algorithm.", "publication_date": "1994-01-01", "reason": "This work extends the MoE framework to hierarchical structures, providing a deeper theoretical understanding of MoE's capabilities for complex data modeling."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale.", "publication_date": "2021-01-01", "reason": "This paper introduces the Vision Transformer (ViT) architecture, which is the foundation of the pre-trained models used in the proposed continual learning method."}, {"fullname_first_author": "X. Li", "paper_title": "Prefix-tuning: Optimizing continuous prompts for generation.", "publication_date": "2021-01-01", "reason": "This work introduces the prefix tuning method, a key component of the prompt-based continual learning approaches investigated in this research."}, {"fullname_first_author": "L. Wang", "paper_title": "Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality.", "publication_date": "2023-01-01", "reason": "This paper, closely related to the current work, delves into the theoretical limitations of prompt-based continual learning and proposes improvements; the current work builds upon these findings."}]}