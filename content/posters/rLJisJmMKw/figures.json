[{"figure_path": "rLJisJmMKw/figures/figures_0_1.jpg", "caption": "Figure 1: Teaser. Our model generates plausible novel views, conditioned on only a single input view, enabling to handle both in-domain images (top) and out-of-domain images (bottom).", "description": "This figure demonstrates the model's ability to generate novel views from a single input image.  The top row shows examples of in-domain images (images similar to those the model was trained on), where the model generates consistent and plausible views when moving left or right. The bottom row displays the model's performance on out-of-domain images (images different from the training data).  It successfully generates plausible views for these images as well, demonstrating the model's generalization capabilities.", "section": "Introduction"}, {"figure_path": "rLJisJmMKw/figures/figures_2_1.jpg", "caption": "Figure 2: Limitations of explicit warping-and-inpainting approach [35, 7, 31]. Results from challenging new camera viewpoints for warping-and-inpainting approach show artifacts. (a) The neon sign present in the input view is distorted after geometric warping due to the noisy depth. (b) The next room peeked in from the new camera viewpoint lacks the context given by the input view.", "description": "This figure demonstrates the limitations of the warping-and-inpainting approach for novel view synthesis. The top row shows an example where noisy depth estimation leads to distortions in the warped image, which the inpainting model struggles to correct. The bottom row shows an example of severe occlusion where the warped image loses significant semantic details, resulting in an incomplete and inaccurate novel view. GenWarp addresses these limitations by utilizing a generative warping approach, instead of relying on explicit warping followed by inpainting.", "section": "2 Related Work"}, {"figure_path": "rLJisJmMKw/figures/figures_4_1.jpg", "caption": "Figure 3: Method overview: (Left) Given an input view and a desired camera viewpoint, we obtain a pair of embeddings: a 2D coordinate embedding for the input view, and a warped coordinate embedding for the novel view from estimated depth through MDE. With these embeddings, a semantic preserver network produces a semantic feature of the input view, and a diffusion model conditioned on them learns to conduct geometric warping to generate novel views. (Right) We augment self-attention with cross-view attention, followed by aggregating the features with both attentions at once. It helps the model to consider where to generate and where to warp.", "description": "This figure illustrates the GenWarp framework's architecture.  The left side shows the process of generating a novel view image from a single input view, using MDE (Monocular Depth Estimation) to create warped coordinates and a semantic preserver network to extract semantic features. A pretrained T2I (text-to-image) diffusion model then combines these to generate the novel view. The right side details how cross-view attention and self-attention are combined to improve the model's ability to determine what to warp and what to generate.", "section": "3.2 Semantic-preserving generative warping"}, {"figure_path": "rLJisJmMKw/figures/figures_5_1.jpg", "caption": "Figure 4: Visualization of augmented self-attention map. In augmented self-attention map A, the original self-attention part Aself is more attentive to regions requiring generative priors, such as occluded or ill-warped areas (top), while the concatenated cross-view attention part Across focuses on regions that can be reliably warped from the input view (bottom). By aggregating both attentions at once, the model naturally determines which regions to generate and which to warp.", "description": "This figure visualizes the augmented self-attention map used in the GenWarp model.  It shows how the model combines self-attention (focusing on areas needing generation) and cross-view attention (focusing on areas that can be reliably warped from the input image) to determine which parts of a novel view to generate and which to warp from the input image. The top row shows a situation where the self-attention focuses on an occluded area, while the bottom row shows a situation where it focuses on an area that is poorly warped due to noise in the depth map. The cross-view attention helps the model maintain consistency and coherence between the input and generated views.", "section": "3.2 Semantic-preserving generative warping"}, {"figure_path": "rLJisJmMKw/figures/figures_6_1.jpg", "caption": "Figure 10: Extensive qualitative results on in-the-wild images. We present extensive qualitative results of our method and baseline methods [35, 7] on the in-the-wild images.", "description": "This figure shows a qualitative comparison of the proposed GenWarp method against the Stable Diffusion Inpainting method on several in-the-wild images.  It demonstrates the ability of GenWarp to generate more realistic and semantically consistent novel views compared to the baseline method, particularly in challenging scenarios with significant viewpoint changes or occlusions.", "section": "Additional qualitative results"}, {"figure_path": "rLJisJmMKw/figures/figures_6_2.jpg", "caption": "Figure 10: Extensive qualitative results on in-the-wild images. We present extensive qualitative results of our method and baseline methods [35, 7] on the in-the-wild images.", "description": "This figure showcases qualitative results from the GenWarp model and baselines (Stable Diffusion Inpainting [35] and a warping-and-inpainting approach [7]) on a diverse set of real-world images.  The results demonstrate the ability of GenWarp to generate high-quality and semantically consistent novel views from a single input image, even when dealing with complex and varied scenes. It particularly highlights the model's strengths over baseline methods in situations with challenging camera viewpoints or scenes with significant occlusions.", "section": "A Additional qualitative results"}, {"figure_path": "rLJisJmMKw/figures/figures_7_1.jpg", "caption": "Figure 7: Qualitative comparisons with baseline methods [35, 17, 36]. We present single-shot novel view generation results with large viewpoint changes on RealEstate10K [52] test set. Our GenWarp generates high-quality novel views consistent with the input views. We also provide qualitative results on ScanNet [8] in Fig. 9 of Appendix.", "description": "This figure compares the novel view generation results of the proposed GenWarp model with three baseline methods: Stable Diffusion Inpainting, Photometric-NVS, and GeoGPT.  The comparison uses images from the RealEstate10K dataset, showcasing the ability of each method to generate novel views with significant viewpoint changes. GenWarp demonstrates superior performance in generating high-quality, consistent views compared to the baselines.", "section": "4.2 Qualitative results"}, {"figure_path": "rLJisJmMKw/figures/figures_8_1.jpg", "caption": "Figure 8: Comparison on various viewpoint changes.", "description": "This figure shows the relationship between the difficulty of camera viewpoint changes and the degree of distortion in the generated novel views.  The x-axis represents the difficulty of the viewpoint change (measured using LPIPS between the ground truth source and target views). The y-axis represents the distortion of the generated view (measured using LPIPS between the generated and ground truth target views).  The graph shows that GenWarp outperforms other methods across various levels of viewpoint change difficulty.  In other words, GenWarp produces consistently lower distortion even as viewpoint change difficulty increases.", "section": "4.4 Ablation study"}, {"figure_path": "rLJisJmMKw/figures/figures_13_1.jpg", "caption": "Figure 9: Extensive qualitative comparisons in out-of-domain setting. We provide qualitative results of our model trained on RealEstate10K [52], on the external dataset, ScanNet [8].", "description": "This figure shows a qualitative comparison of novel view generation results on the ScanNet dataset using GenWarp, SD-Inpainting, and GeoGPT.  The comparison is done in an out-of-domain setting, where the models were trained on the RealEstate10K dataset, which is different from the test dataset.  The figure highlights the visual quality and consistency of novel views generated by each method, showcasing GenWarp's superior performance in generating plausible and semantically consistent novel views, especially in complex scenes.", "section": "A Additional qualitative results"}, {"figure_path": "rLJisJmMKw/figures/figures_14_1.jpg", "caption": "Figure 10: Extensive qualitative results on in-the-wild images. We present extensive qualitative results of our method and baseline methods [35, 7] on the in-the-wild images.", "description": "This figure shows qualitative comparisons of novel view synthesis results using GenWarp and baseline methods (Stable Diffusion Inpainting) on various real-world images. Each row represents a different scene, with the input view, warped image, inpainted image using Stable Diffusion, and the result of GenWarp shown side-by-side. The figure demonstrates the ability of GenWarp to generate more realistic and semantically consistent novel views compared to the baseline method.", "section": "Additional qualitative results"}, {"figure_path": "rLJisJmMKw/figures/figures_15_1.jpg", "caption": "Figure 11: Following LucidDreamer [7], we apply inverse warping and occlusion mask filtering to reproduce the existing warping-and-inpainting approach [7, 31] with Stable Diffusion Inpainting [35].", "description": "This figure shows a comparison of forward and inverse warping methods used in the warping-and-inpainting approach with Stable Diffusion inpainting. The figure demonstrates that inverse warping and applying occlusion masks based on depth map filtering are used in the warping-and-inpainting approach. It also shows how these methods create artifacts in the results.", "section": "3 Method"}, {"figure_path": "rLJisJmMKw/figures/figures_16_1.jpg", "caption": "Figure 12: Unstable training of an explicit feature warping model using pseudo depth data.", "description": "This figure shows the results of training a model that warps features explicitly using pseudo depth maps. The instability of the training process is highlighted by comparing generated views at different iteration numbers (3,000 and 5,000).  The instability is likely due to inaccuracies in the pseudo depth maps, demonstrating the challenges of relying on explicit warping without a more robust approach like the GenWarp method proposed in the paper.", "section": "4.4 Ablation study"}, {"figure_path": "rLJisJmMKw/figures/figures_16_2.jpg", "caption": "Figure 13: Comparison of LPIPS with other methods regarding ratio of invisible region. We measure LPIPS between generated views and GT target views, following GeoGPT [36]'s evaulation protocol.", "description": "This figure shows a comparison of the Learned Perceptual Image Patch Similarity (LPIPS) scores for different novel view synthesis methods as the ratio of invisible regions increases.  The LPIPS score measures the perceptual difference between the generated novel view and the ground truth view.  The x-axis represents the ratio of invisible pixels in the novel view, which increases as the viewpoint of the camera changes drastically. The y-axis represents the LPIPS score, indicating how different the generated view is from the ground truth. The lower the score, the better the generation. The figure helps to assess the performance and robustness of various methods under challenging scenarios with significant occlusion.", "section": "4.4 Ablation study"}, {"figure_path": "rLJisJmMKw/figures/figures_16_3.jpg", "caption": "Figure 15: Comparison with reconstruction-based methods on DTU dataset [18]. Note that our model used here is not trained on DTU dataset.", "description": "This figure compares the novel view synthesis results of GenWarp against three other reconstruction-based methods (NeRF, PixelNeRF, and Nerdi) on the DTU dataset.  The input view shows a scene with a pumpkin and other objects. GenWarp produces a result that is visually similar to the input image and preserves details and object appearances well. The other methods have significantly more artifacts and distortions. Notably, GenWarp was not trained on the DTU dataset, showcasing its generalization capabilities.", "section": "4.3 Quantitative results"}, {"figure_path": "rLJisJmMKw/figures/figures_17_1.jpg", "caption": "Figure 10: Extensive qualitative results on in-the-wild images. We present extensive qualitative results of our method and baseline methods [35, 7] on the in-the-wild images.", "description": "This figure showcases qualitative comparisons between the proposed GenWarp method and baseline methods (Stable Diffusion Inpainting) on real-world images. It visually demonstrates the ability of GenWarp to generate high-quality novel views from a single input image, even in challenging scenarios involving complex scenes and diverse image styles.", "section": "Additional qualitative results"}]