[{"figure_path": "TusuJSbRxm/figures/figures_4_1.jpg", "caption": "Figure 1: The features for both MDPs are $\n\\phi(s_1,\\cdot) = (1), \\phi(s_3,\\cdot) = (0.5), \\phi(\\cdot,\\cdot) = (0)\n\\text{otherwise. Left: A (0,1)-approximately } q^\\pi \\text{-realizable MDP. Right: Linear MDP,\nobtained by skipping low range (red) states in the left MDP. Source: Figure 1 from [Weisz et al., 2023].", "description": "This figure shows two Markov Decision Processes (MDPs). The MDP on the left is approximately linearly realizable but not linear, while the MDP on the right is linear.  The linear MDP is derived from the first by removing (or \"skipping\") certain states (shown in red). The key idea is that, by carefully skipping over low-range states, a linearly realizable MDP can be transformed into a linear MDP, making the problem of solving it computationally easier.  This transformation is central to the algorithm proposed in the paper.", "section": "4.1 Background Theory"}]