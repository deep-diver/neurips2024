[{"figure_path": "TusuJSbRxm/tables/tables_1_1.jpg", "caption": "Table 1: Notation is defined as: \u03c0-opt = policy optimization, \u03c0-eval = policy evaluation, Conc = Concentrability, q\u2122 = linear q\u2122-realizability, Traj = Trajectory, \u221a = poly(d, H, Cconc, 1/\u20ac) sample complexity, x = exponential lower bound in terms of one of d, H, Cconc.", "description": "This table compares the results of the current work with other related works in offline reinforcement learning.  It shows the task (policy optimization or evaluation), the type of data used (individual transitions or trajectories), the assumptions made (linear  q-realizability, concentrability, and others), and the sample complexity results.  A checkmark indicates a polynomial sample complexity, while an 'x' indicates an exponential lower bound.", "section": "2 Related Works"}]