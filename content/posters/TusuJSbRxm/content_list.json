[{"type": "text", "text": "Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear $q^{\\pi}$ -Realizability and Concentrability ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Volodymyr Tkachuk GellertWeisz University of Alberta, Edmonton, Canada Google DeepMind, London, UK ", "page_idx": 0}, {"type": "text", "text": "Csaba Szepesvari Google DeepMind, Edmonton, Canada University of Alberta, Edmonton, Canada ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider offline reinforcement learning (RL) in $H$ -horizon Markov decision processes (MDPs) under the linear $q^{\\pi}$ -realizability assumption, where the actionvalue function of every policy is linear with respect to a given $d$ -dimensional feature function. The hope in this setting is that learning a good policy will be possible without requiring a sample size that scales with the number of states in the MDP. Foster et al. [2021] have shown this to be impossible even under concentrability, a data coverage assumption where a coefficient $C_{\\mathrm{conc}}$ bounds the extent to which the state-action distribution of any policy can veer off the data distribution. However, the data in this previous work was in the form of a sequence of individual transitions. This leaves open the question of whether the negative result mentioned could be overcome if the data was composed of sequences of full trajectories. In this work we answer this question positively by proving that with trajectory data, a dataset of size poly $(d,H,C_{\\mathrm{conc}})/\\bar{\\epsilon}^{2}$ is sufficient for deriving an $\\epsilon_{}$ -optimal policy, regardless of the size of the state space. The main tool that makes this result possible is due to Weisz et al. [2023], who demonstrate that linear MDPs can be used to approximate linearly $q^{\\pi}$ -realizable MDPs. The connection to trajectory data is that the linear MDP approximation relies on \u201cskipping\" over certain states. The associated estimation problems are thus easy when working with trajectory data, while they remain nontrivial when working with individual transitions. The question of computational efficiency under our assumptions remains open. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the offline reinforcement learning (RL) setting, where the objective is to derive a nearoptimal policy for an $H$ -horizon Markov decision process (MDP) using offline data. This contrasts with the online RL paradigm, where learners interact directly with an MDP - or its simulator - to collect new data. Offline RL is especially relevant when acquiring new data guided by the learner is infeasible or ill-advised for safety reasons. ", "page_idx": 0}, {"type": "text", "text": "Deriving a near-optimal policy is only possible from offine data that covers the MDP well enough. One way to formalize this as an assumption, which we adopt for this work, is called concentrability. This assumption posits that the offline data sufficiently covers the distribution of state-action pairs that are accessible through running any policy. Challenges also arise in MDPs characterized by large or infinite state spaces. In such scenarios, an efficient learner's data requirements cannot scale with the state space size. An approach to remove state space dependence is to assume that the stateaction value function of any policy can be linearly represented using a $d_{\\cdot}$ dimensional feature map, an assumptionknown aslinear $q^{\\pi}$ -realizability. ", "page_idx": 0}, {"type": "table", "img_path": "TusuJSbRxm/tmp/aaf00e7ed67c4972b8137f6a8580b8e118605d9fcb5ecb0f0577d549fbdf99e6.jpg", "table_caption": [], "table_footnote": ["Table 1: Notation is defined as: $\\pi$ -opt $=$ policy optimization, $\\pi$ -eval $=$ policy evaluation, $\\mathrm{Conc=}$ Concentrability, $q^{\\pi}=$ linear $q^{\\pi}$ -realizability, Traj $=$ Trajectory, $\\surd=\\mathrm{poly}(d,H,C_{\\mathrm{conc}},1/\\epsilon)$ sample complexity, $\\Chi=$ exponential lower bound in terms of one of $d,H,C_{\\mathrm{conc}}$ \uff1a "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While linear $q^{\\pi}$ -realizability facilitates effcient online RL [Weisz et al., 2023], its applicability has been limited in offline contexts. For instance, Foster et al. [2021] proves that no learning algorithm can derive an $\\epsilon_{}$ -optimal policy under linear $q^{\\pi}$ -realizability and concentrability bounded by. $C_{\\mathrm{conc}}$ with a $\\mathrm{poly}(d,H,\\^{.}C_{\\mathrm{conc}},\\dot{\\epsilon}^{-1})$ number of samples. However,their result does not apply to trajectory data, where the offline data contains full sequences of state, action, and reward tuples obtained by following some policy from the initial state to the terminal state. The following problem is left open: ", "page_idx": 1}, {"type": "text", "text": "\"Does there exist an effcient learner that outputs an e-optimal policy, under the assumptions of linear $q^{\\pi}$ -realizability, concentrability, and trajectory data? ", "page_idx": 1}, {"type": "text", "text": "Our findings affirmatively answer this question in terms of sample complexity, highlighting a notable distinction in the requirements for trajectory data versus general offine data for effective learning. This underscores the practical value of accumulating trajectory data whenever feasible. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In Table 1 we provide a comparison of our result to the other works in offline RL discussed below. ", "page_idx": 1}, {"type": "text", "text": "Lower bounds: As we have already discussed in Section 1, the work of Foster et al. [2021] shows a lower bound that depends on the size of the state space (in the same setting as ours), except they do not assume access to trajectory data. The work by Jia et al. [2024] is perhaps the most relevent to ours. They show an exponential lower bound in the horizon for policy evaluation, under the assumptions of trajectory data, concentrability, and a restricted linear $q^{\\pi}$ -realizability where the value function of only the target policy is linear. While we anticipate that evaluating policies (their focus) is no more difficult than optimizing policies (our focus), our $q^{\\pi}$ realizability is for all memoryless policies (Assumption 1), while theirs is restricted to the target policy. Zanette [2021] shows an exponential lower bound in terms of the feature dimension $d$ , under linear $q^{\\pi}$ -realizability, and various other structural assumptions; however, their setting would result in a concentrability coefficient larger than the size of the state space. Wang et al. [2020], Amortila et al. [2020] show a lower bound that is exponential in the horizon, under linear $q^{\\pi}$ -realizability. However, they use a $\\lambda_{\\operatorname*{min}}$ lower bound condition, which requires a lower bound on the minimum eigenvalue $\\lambda_{\\operatorname*{min}}$ of the expected covariance matrix used for least-squares estimation. This is seen as a weaker condition than ours, as it only posits good coverage in terms of the feature space, not the (possibly much richer) state-action space. ", "page_idx": 1}, {"type": "text", "text": "Upper bounds: Chen and Jiang [2019], Munos and Szepesvari [2008] present efficient algorithms under concentrability and Bellman completeness, an assumption that the Bellman optimality operator outputs a linearly realizable function when its input is linearly realizable. As linear $q^{\\pi}$ realizability does not imply Bellman completeness [Zanette et al., 2020], these results do not transfer to our setting. Xie and Jiang [2021] show an upper bound under linear $q^{\\pi}$ -realizability, albeit, using a stronger notion of data coverage than concentrability, which we call strong concentrability. The work of Xie et al. [2021, 2022] give data-dependent sample complexity bounds that hold under both Bellman completeness and linear $q^{\\pi}$ -realizability even in the absence of explicit data coverage assumptions. Jin et al. [2021] assume a general function approximation setting and also provide data-dependent bounds, while Duan et al. [2020], Xiong et al. [2022] show upper bounds for linear $M D P s$ (a stronger assumption than linear $q^{\\pi}$ -realizability [Zanette et al., 2020]) with the $\\lambda_{\\operatorname*{min}}$ lower bound condition. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Throughout we fix the integer $d\\geq1$ . Let $\\vec{0}\\in\\mathbb{R}^{d}$ be the $d\\!.$ -dimensional, all zero vector. For $L>0$ let $\\mathcal{B}(\\bar{L})\\,=\\,\\{x\\,\\in\\,\\mathbb{R}^{d}:\\,\\|x\\|_{2}\\,\\le\\,L\\}$ denote the $d$ -dimensional Euclidean ball of radius $L$ centered at the origin, where $\\Vert\\cdot\\Vert_{2}$ denotes the Euclidean norm. The inner product $\\langle x,y\\rangle$ for $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{d}$ is defined as the dot product $x^{\\top}y$ . Let $\\mathbb{1}\\{B\\}$ be the indicator function of a boolean-valued (possibly random) variable $B$ , taking the value $1$ if $B$ is true and O if false. Let $\\mathcal{M}_{1}(X)$ denote the set of probability distributions over the set $X$ . Let $\\mathbb{E}_{B\\sim\\mathcal{P}}$ denote the expectation of random variable $B$ under distribution $\\mathcal{P}$ . For integers $i,j$ ,let $[i]\\;=\\;\\{1,2,\\ldots,i\\}$ and $[i~:j]~=~\\{i,\\dots,j\\}$ . For a symmetric matrix $M\\,\\in\\,\\mathbb{R}^{d\\times d}$ we write $\\lambda_{\\mathrm{min}}(M)$ and $\\lambda_{\\operatorname*{max}}(M)$ for its minimum and maximum eigenvalue. ", "page_idx": 2}, {"type": "text", "text": "The environment is modeled by a finite horizon Markov decision process (MDP). Fix the horizon to $H$ . This MDP is defined by a tuple $(S,{\\mathcal{A}},P,{\\mathcal{R}})$ . Here, the state space $\\boldsymbol{S}$ is finite', and organized by stages: $\\begin{array}{r}{\\textstyle S=\\bigcup_{h\\in[H+1]}S_{h}}\\end{array}$ staring msignatdinial sta $s_{1}$ $({\\cal S}_{1}=\\{s_{1}\\})^{2}$ , and culminating in a designated terminal state $s\\tau$ $(S_{H+1}=\\{s_{\\top}\\})^{3}$ . Without loss of generality, we assume $\\ensuremath{\\boldsymbol{S}}_{h}$ and ${\\cal S}_{h^{\\prime}}$ for $h\\neq h^{\\prime}$ are disjoint sets. Define the function stage $\\begin{array}{r}{S\\rightarrow[H+1]}\\end{array}$ , such that stage $\\mathbf{\\partial}:\\left(s\\right)=h$ $s\\in S_{h}$ . The action space $\\boldsymbol{\\mathcal{A}}$ is finite. The transition kernel is $\\bar{P^{\\prime}}\\colon(\\bigcup_{h\\in[H]}S_{h})\\times\\mathcal{A}\\rightarrow\\mathcal{M}_{1}(\\mathcal{S})$ with the property that transitions occur between successive stages. Specifically, for any $h\\,\\in\\,[H]$ state $s_{h}\\ \\in\\mathcal{S}_{h}$ , and action $a\\in{\\mathcal{A}}$ $P(s_{h},a)\\in\\mathcal{M}_{1}(S_{h+1})$ . The reward kernel is $\\mathcal{R}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow$ $\\mathcal{M}_{1}([0,1])$ . So that the terminal state $s\\tau$ has no influence on the learner we force the reward kernel to deterministically give zero reward for all actions $a\\in{\\mathcal{A}}$ in $s\\tau$ (i.e. $\\mathcal{R}(s_{\\top},a)(r)=\\mathbb{1}\\{0=r\\})$ . An agent interacts with this environment sequentially across an episode of $H+1$ stages, by selecting an action $a\\in A$ in the current state. The environment (except at stage $H+1]$ then transitions to a subsequent state according to $P$ and provides a reward in $[0,1]$ as specified by $\\mathcal{R}^{4}$ ", "page_idx": 2}, {"type": "text", "text": "We define an agent's interaction with the MDP through a policy $\\pi$ , which assigns a probability distribution over actions based on the history of interactions (including states, actions, and rewards). For this work, we restrict policies to be memoryless, that is, their action distribution depends solely on the most recent state in the history. The set of all memoryless policies is $\\Pi=\\{\\pi^{'}\\colon\\pi:S\\ \\dot{\\rightarrow}$ $\\mathcal{M}_{1}(\\mathcal{A})\\}$ . For $\\pi\\,\\in\\,\\Pi$ , we write $\\pi(a|s)$ to denote the probability $\\pi(s)$ assigns to action $a$ .For deterministic policies only (i.e., those that for each state place a unit probability mass on some action) we sometimes abuse notation by writing $\\pi(s)$ to denote arg $\\operatorname*{max}_{a\\in{\\cal A}}\\pi(a|s)$ . Starting from any state $s$ within the MDP and using a policy $\\pi$ induces a probability distribution over trajectories, denoted as $\\mathbb{P}_{\\pi,s}$ . For any $a\\in\\mathcal{A},\\mathbb{P}_{\\pi,s,a}$ is the distribution over the trajectories when first action $a$ is used in state $s$ , after which policy $\\pi$ is followed. Specifically, for some $h\\in[H+1]$ and $(s,a)\\in{\\mathcal{S}}_{h}\\times{\\mathcal{A}}$ we write Traj $\\sim\\mathbb{P}_{\\pi,s,a}$ to denote that ${\\mathrm{Traj}}=(S_{h},A_{h},R_{h},\\ldots,S_{H+1},A_{H+1},R_{H+1})$ for a random trajectory that follows the distribution specified by $\\mathbb{P}_{\\pi,s,a}$ , that is, $S_{h}=s$ $A_{h}=a$ $A_{i}\\sim\\pi(S_{i})$ for $i\\in[h+1:H+1]$ $S_{i+1}\\sim P(S_{i},A_{i})$ for $i\\in[h:H]$ , and $R_{i}\\sim\\mathcal{R}(S_{i},A_{i})$ for $i\\in[h:H+1]$ Writing Traj $\\sim\\mathbb{P}_{\\pi,s}$ has an analogous meaning, with the only difference being that $A_{h}$ is not fixed, and instead $A_{h}\\sim\\pi({S_{h}})$ .For $h\\in[H+1]$ , we write $\\mathbb{P}_{\\pi,s}^{h}$ (and $\\mathbb{P}_{\\pi,s,a}^{h})$ for the marginal distribution of $(S_{h},A_{h})$ (i.e., the state-action pair of stage $h$ ) under the joint distribution of $\\mathbb{P}_{\\pi,s}$ (and $\\mathbb{P}_{\\pi,s,a})$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "For $1\\leq t\\leq t^{\\prime}\\leq H{+}1$ , we use the notation $x_{t:t^{\\prime}}=(x_{u})_{u\\in[t:t^{\\prime}]}$ throughout, except when $(x_{u})_{u\\in[t:t^{\\prime}]}$ are a sequence of scalar rewards. In that case, for convenience, we write $\\textstyle r_{t:t^{\\prime}}\\;=\\;\\sum_{u=t}^{t^{\\prime}}r_{u}$ and $\\begin{array}{r}{R_{t:t^{\\prime}}=\\sum_{u=t}^{t^{\\prime}}R_{u}}\\end{array}$ The state-value and action-value functions $v^{\\pi}$ and $q^{\\pi}$ are defined as the expected total reward along the rest of the trajectory while $\\pi$ is used: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{\\pi}(s)=\\underset{\\mathrm{Traj\\sim\\mathbb{P}_{\\pi,s}}}{\\mathbb{E}}R_{\\mathrm{stage}(s):H}\\mathrm{~for~}s\\in\\mathcal{S}\\quad\\mathrm{~and~}\\quad q^{\\pi}(s,a)=\\underset{\\mathrm{Traj\\sim\\mathbb{P}_{\\pi,s,a}}}{\\mathbb{E}}R_{\\mathrm{stage}(s):H}\\mathrm{~for~}(s,a)\\in\\mathcal{S}\\times\\mathcal{A}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let $\\pi^{\\star}\\in\\Pi$ be an optimal policy, satisfying $\\begin{array}{r}{q^{\\pi^{\\star}}(s,a)=\\operatorname*{sup}_{\\pi\\in\\Pi}q^{\\pi}(s,a)}\\end{array}$ for all $(s,a)\\in S\\times A$ Let $\\boldsymbol{q}^{\\star}(s,a)=\\boldsymbol{q}^{\\pi^{\\star}}(s,a)$ and $v^{\\star}(s)=\\operatorname*{max}_{a\\in A}q^{\\star}(s,a)$ for all $(s,a)\\in S\\times A$ . By definition, we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{\\star}(s_{\\top})=v^{\\pi}(s_{\\top})=0\\quad\\mathrm{and}\\quad q^{\\star}(s_{\\top},a)=q^{\\pi}(s_{\\top},a)=0\\qquad\\mathrm{~for~all~}\\pi\\in\\Pi,a\\in\\mathcal{A}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.1   Assumptions and Problem Statement ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A feature map is defined as $\\phi:S\\times A\\to B(L_{1})$ forsome $L_{1}>0$ . The representative power of a feature map for an MDP is described by the following assumption: ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 $((\\eta,L_{2})$ -Approximately Linear $q^{\\pi}$ -Realizable MDP). For some $\\eta\\;\\geq\\;0,L_{2}\\;>\\;0,$ assume that the MDP (together with a feature map $\\phi$ )issuchthat ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\pi\\in\\Pi}\\operatorname*{min}_{\\theta_{h}\\in B(L_{2})}\\operatorname*{max}_{(s_{h},a_{h})\\in S_{h}\\times A}\\left|q^{\\pi}(s_{h},a_{h})-\\langle\\phi(s_{h},a_{h}),\\theta_{h}\\rangle\\right|\\leq\\eta\\qquad f o r\\;a l l\\;h\\in[H+1]\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For any $h\\in[H+1]$ let $\\psi_{h}:\\Pi\\to B(L_{2})$ be a mapping from policies to parameter values $\\theta_{h}$ that attain the min in the above display. For $h=H+1$ we restrict this mapping to $\\psi_{H+1}(\\cdot)=\\bar{0}$ which satisfies the above display by definition. We write $\\psi_{h:t}(\\pi)$ for $(\\psi_{h}(\\pi),\\dots,\\psi_{t}(\\pi))$ ", "page_idx": 3}, {"type": "text", "text": "We also make the following assumptions on the offline data (the relationship to non-trajectory data and the negative result by Foster et al. [2021] is discussed in Appendix B) : ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Full Length Trajectory Data). Assume the learner is given a dataset of full length trajectories? and corresponding features of size $n\\geq1$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(\\mathrm{traj}^{1},\\dots,\\mathrm{traj}^{n}\\right)}&{\\boldsymbol{a n d}}&{\\left(\\big(\\phi\\big(s_{h}^{1},\\boldsymbol{a}\\big)\\big)_{h\\in[H],a\\in A},\\dots,\\big(\\phi\\big(s_{h}^{n},\\boldsymbol{a}\\big)\\big)_{h\\in[H],a\\in A}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where for some \u201cdata collection policy'\" $\\pi^{0}\\in\\Pi$ unknown to the learner, $(\\operatorname{traj}^{j})_{j=1}^{n}$ are independent samples from $\\mathbb{P}_{\\pi^{0},s_{1}}$ where traj $\\mathbf{\\lambda}^{j}=(s_{t}^{j},a_{t}^{j},r_{t}^{j})_{t\\in[H+1]}$ . To simplify notation we write ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{h}^{j}=\\phi(s_{h}^{j},a_{h}^{j})\\ \\ \\,f o r\\,a l l\\,h\\in[H],j\\in[n]\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Admissible Distribution). $A$ sequenceof $H$ state-action distributions $\\nu=(\\nu_{h})_{h\\in[H]}\\in$ $(\\mathcal{M}_{1}(\\varS_{h}\\times\\varLambda))^{H}$ is admissible for an MDP if there exists a policy $\\pi\\in\\Pi$ suchthat ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nu_{h}(s_{h},a_{h})=\\mathbb{P}_{\\pi,s_{1}}^{h}(s_{h},a_{h})\\quad f o r\\,a l l\\left(s_{h},a_{h}\\right)\\in{\\mathcal{S}}_{h}\\times{\\mathcal{A}},\\,h\\in\\left[H\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Define the state-action occupancy measure of the data collction policy $\\pi^{0}$ as $\\mu=(\\mu_{h})_{h\\in[H]}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu_{h}(s_{h},a_{h})=\\mathbb{P}_{\\pi^{0},s_{1}}^{h}(s_{h},a_{h})\\quad\\mathrm{for~all~}(s_{h},a_{h})\\in S_{h}\\times\\mathcal{A},\\:h\\in[H]\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 3 (Concentrability). Assume there exists a constant $C_{\\mathrm{conc}}\\geq1$ such that for all admissible distributions v = (vh)he[H] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{h\\in[H]}\\operatorname*{max}_{(s_{h},a_{h})\\in S_{h}\\times A}\\left\\{\\frac{\\nu_{h}(s_{h},a_{h})}{\\mu_{h}(s_{h},a_{h})}\\right\\}\\leq C_{\\mathrm{conc}}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Problem 1. Let $\\epsilon>0$ .Under Assumptions 1 to $^3$ does there exist alearner,with access to only $n=\\mathrm{poly}(1/\\epsilon,H,d,C_{\\mathrm{conc}},\\log(1/\\delta),\\bar{\\log}(1/L_{1}),\\log(1/L_{2}))$ : full length trajectories (as defined in Assumption 2), that outputs a policy $\\pi$ such that, with probability at least $1-\\delta$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nv^{\\star}(s_{1})-v^{\\pi}(s_{1})\\leq\\epsilon\\,?\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4 Result ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We resolve Problem 1 in the positive by defining a learner (Algorithm 1) that: selects parameters optimistically from modified MDPs that \u201cskip over\u201d certain states while preserving tight $q$ value estimation guarantees (achieved by solving Optimization Problem 1); then, outputs a greedy policy $\\pi^{\\prime}$ (defined in line 3) over the selected parameters. This result is made formal in following theorem (proof in Section 5): ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $\\epsilon\\,\\in\\,(0,H]$ .Under Assumptions $^{\\,l}$ to 3, if the number of full length trajectories $n\\,=\\,\\tilde{\\Theta}(C_{\\mathrm{conc}}^{4}H^{7}d^{4}/\\epsilon^{2})$ and $\\eta\\,=\\,\\tilde{\\mathcal{O}}\\bigl(\\epsilon^{2}/(C_{\\mathrm{conc}}^{2}H^{5}d^{2})\\bigr)^{6}$ thn,wibil $1-\\delta$ the policy $\\pi^{\\prime}$ output by our learner (Algorithm $^{\\,l}$ ) satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\nv^{\\star}(s_{1})-v^{\\pi^{\\prime}}(s_{1})\\leq\\epsilon\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{\\Omega},\\tilde{O}$ and $\\Tilde{\\Theta}$ are the counterparts of $\\Omega,{\\mathcal{O}}$ and $\\Theta$ from the big-Oh notation that hide polylogarithmic factors of the problem parameters $(1/\\epsilon,1/\\delta,H,d,C_{\\mathrm{conc}},L_{1},L_{2})$ . The following subsections focus on introducing the theory needed to formally present our learner, giving intuition behind our learner, and presenting our learner. ", "page_idx": 4}, {"type": "text", "text": "4.1 Background Theory ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our learner relies on the observation due to Weisz et al. [2023] that linearly $q^{\\pi}$ -realizableMDPsare linear MDPs, as long as they contain no low-range states. The range of a state is the largest possible regret from that state, that is, the largest difference in action-value that the choice of action in that state can make (up to misspecification): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{range}(s)=\\operatorname*{sup}_{\\pi\\in\\Pi}\\operatorname*{max}_{a,a^{\\prime}\\in\\mathcal{A}}\\bigl\\langle\\phi(s,a,a^{\\prime}),\\psi_{\\mathrm{stage}(s)}(\\pi)\\bigr\\rangle\\quad\\mathrm{~for~all~}s\\in\\mathcal{S}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi(s,a,a^{\\prime})=\\phi(s,a)-\\phi(s,a^{\\prime})$ is a notation we use to denote feature differences. Intuitively, the choice of actions in low-range states are unimportant, as ", "page_idx": 4}, {"type": "equation", "text": "$$\n|v^{\\pi}(s)-q^{\\pi}(s,a)|\\leq\\mathrm{range}(s)+2\\eta\\qquad\\mathrm{~for~any~}\\pi\\in\\Pi\\mathrm{~and~all~}(s,a)\\in S\\times{\\mathcal A}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As a warm-up, consider the example MDPs shown in Fig. 1. We will transform the linearly $q^{\\pi}$ realizable MDP on the left into a linear MDP on the right by \u201cskipping over\u201d the red low-range states. Let the features for both MDPs be $\\phi(s_{1},\\cdot)=(1)\\bar{,}\\phi(s_{3},\\cdot)\\doteq\\bar{(}0.\\bar{5}),\\phi(\\cdot,\\cdot)=(0)$ otherwise. Then the left MDP is $(0,1)$ -approximately $q^{\\pi}$ -realizable, with realizability parameter $\\psi_{h}(\\pi)=(1)$ for all $h\\in[H+1],\\pi\\in\\Pi$ . However, it is not a linear MDP, since the rewards cannot be represented by a linear function of the features. To see this, notice that there exists no $\\theta$ such that $\\langle\\phi(s_{1},\\bar{a}_{1}),\\theta\\rangle=$ $r(s_{1},a_{1})=1$ and $\\langle\\phi(s_{1},a_{2}),\\theta\\rangle=r(s_{1},a_{2})=0.5$ , since $\\phi(s_{1},a_{1})=\\phi(s_{1},a_{2})=\\dot{(1)}$ Wemodify this MDP on the left to \u201cskip over\"\u2019 low-range red states, by automatically taking the first available action at such states, and summing up the rewards along skipped paths. This turns the MDP into the one on the right of Fig. 1, which is a linear MDP. ", "page_idx": 4}, {"type": "image", "img_path": "TusuJSbRxm/tmp/6c3a54fc7ee7db98f170b4f139647309236dc5209dad74bf70c5b6a6f5d26539.jpg", "img_caption": ["Figure 1: The features for both MDPs are $\\phi(s_{1},\\cdot)=(1),\\phi(s_{3},\\cdot)=(0.5),\\phi(\\cdot,\\cdot)=(0)$ otherwise. Left:A $(0,1)$ -approximately $q^{\\pi}$ -realizable MDP. Right: Linear MDP, obtained by skipping low range (red) states in the left MDP. Source: Figure 1 from [Weisz et al., 2023]. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The key fact about linear MDPs that we will use is that for any function $f:S\\rightarrow[0,H]$ (e.g., $v$ -value approximators), and any $h\\in[H]$ , there is some parameter $\\theta_{h}\\in\\mathbb{R}^{d}$ so that for any $(s,a)\\in\\ensuremath{\\mathcal{S}}_{h}\\times\\ensuremath{\\mathcal{A}}$ $\\langle\\bar{\\phi}(s,a),\\theta_{h}\\rangle$ gives the expectation of the reward plus $f$ 's value on the next state. In our modified ", "page_idx": 4}, {"type": "text", "text": "MDP this result transfers to the fact that the expected sum of rewards along a skipped path, plus $f$ value on the next state after the skipped path, is linearly realizable. Before making this result formal in Lemma 4.2, we clarify the skipping behavior. ", "page_idx": 5}, {"type": "text", "text": "First, we address the fact that we need an approximate, parametric bound on range $(\\cdot)$ with a parameter count that is independent of $|{\\cal S}|$ . For $\\bar{h}\\in[2:H]$ , let $\\Psi_{h}=\\{\\psi_{h}(\\pi)\\,:\\,\\pi\\,\\in\\,\\dot{\\Pi}\\}\\,\\subseteq\\,\\mathcal{B}(\\bar{L}_{2})$ be the (compact) set of parameter values corresponding to all policies. For all $h\\in[2:H]$ , fix a subset $\\bar{G}_{h}\\subset\\bar{\\Psi_{h}}$ of size $|\\bar{G}_{h}^{\\cdot}|=d_{0}:=\\lceil4d\\log\\log(d)+\\bar{16}\\rceil$ that is the basis of a near-optimal design for $\\Psi_{h}$ (more precisely, satisfying Definition 3). The existence of such a near-optimal design follows from [Todd, 2016, Part (i) of Lemma 3.9]. Let $\\bar{G}\\,=\\,\\bar{G}_{2:H}$ , which we call the true guess. Now notice that $\\bar{G}\\in\\mathbf{G}$ where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{G}=(\\boldsymbol{\\mathcal{B}}(L_{2}))^{[2:H]\\times[d_{0}]}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For $G\\in\\mathbf{G}$ we will use the notation that $G\\,=\\,G_{2:H}$ where $G_{h}\\,=\\,(\\vartheta_{h}^{i})_{i\\in[d_{0}]}\\,\\in\\,{\\cal B}(L_{2})^{d_{0}}$ .Any $G=G_{2:H}\\in\\mathbf{G}$ can be used to define an approximate, low parameter-count \u201c\\*version\u201d\" of range that is completely specified by $\\tilde{O}(H d^{2})$ parameters: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{range}^{G}(s)=\\operatorname*{max}_{\\vartheta\\in G_{h}}\\operatorname*{max}_{a,a^{\\prime}\\in A}\\langle\\phi(s,a,a^{\\prime}),\\vartheta\\rangle\\qquad\\mathrm{for~all~}h\\in[2:H],s\\in\\mathcal{S}_{h}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As shown in Proposition 4.5 of [Weisz et al., 2023],rangeG can be used to bound the true range: ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.1. For all $h\\in[2:H]$ and $s\\in S_{h},\\,\\mathrm{range}(s)\\leq\\sqrt{2d}\\cdot\\mathrm{range}^{\\bar{G}}(s).$ ", "page_idx": 5}, {"type": "text", "text": "Based on any $G\\in\\mathbf{G}$ we are interested in simulating a modified MDP that \u201cskips over\u201d states $s$ that have a low range $^G(s)$ , by taking an action according to $\\pi^{0}$ , and presenting as the reward the summed up rewards along paths consisting of skipped states. This \u201cmodified MDP\" only serves as intuition, and will not be formally defined or used in our formal arguments. Instead, we define the \"'skipping probability\u201d parameter at state $s\\in S$ ,with $\\alpha>0$ (set later in Eq. (35)), as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\omega_{G}(s)=\\left\\{\\!\\!\\begin{array}{l l}{1}&{\\mathrm{if~}s\\not\\in S_{1}\\cup S_{H+1}\\mathrm{~and~range}^{G}(s)\\leq\\alpha/\\sqrt{2d}}\\\\ {2-\\sqrt{2d}\\cdot\\mathrm{range}^{G}(s)/\\alpha}&{\\mathrm{if~}s\\not\\in S_{1}\\cup S_{H+1}\\mathrm{~and~}\\alpha/\\sqrt{2d}\\leq\\mathrm{range}^{G}(s)\\leq2\\alpha/\\sqrt{2d}}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The skipping behavior is probabilistic7: it never skips for stages 1 and $H+1$ (where rangeG is not defined); it always skips for ranges lower than some threshold, never skips for ranges higher than twice this threshold, and linearly interpolates between the two in between the thresholds. For $h\\in[H]$ , and $1\\leq l\\leq h$ , let traj $=(s_{t},a_{t},r_{t})_{l\\le t\\le H+1}$ be any fixed trajectory that starts from some stage $l$ Let $\\tau\\sim F_{G,\\mathrm{traj},h+1}\\in\\mathcal{M}_{1}([h+1:H+\\bar{1}])$ be the random stopping stage, when starting from state $s_{h}$ and skipping subsequent states with probability $\\omega_{G}(\\cdot)$ . Formally, for $t\\in[h+1:H+1]$ let $\\begin{array}{r}{F_{G,\\mathrm{traj},h+1}(\\tau\\,=\\,t)\\,=\\,(1\\,-\\,\\omega_{G}(s_{t}))\\prod_{u=h+1}^{t-1}\\omega_{G}(s_{u})}\\end{array}$ . We will ften write $F_{G,h+1}^{j}$ to denote $F_{G,\\mathrm{traj}^{j},h+1}$ where $\\mathrm{traj}^{j}=(s_{t}^{j},a_{t}^{j},r_{t}^{j})_{t\\in[H+1]},j\\in[n]$ ", "page_idx": 5}, {"type": "text", "text": "Next, we present a key tool derived from results of Weisz et al. [2023]: as long as the skips are informed by the true guess, the resulting MDP is approximately linear (proof in Appendix C): ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.2 (Approximate Linear MDP under the true guess). Let $\\eta\\,\\geq\\,0,L_{2}\\,>\\,0.$ Let $M$ be an $(\\eta,L_{2})$ -approximately linear $q^{\\pi}$ -realizable MDP (Assumption 1) with corresponding feature map $\\phi$ Let $\\tilde{L_{2}}=L_{2}(8H^{2}d_{0}/\\alpha+1)$ . Then, for each $f:S\\rightarrow[0,H]$ with $f(s_{\\intercal})=0,$ policy $\\pi\\in\\Pi$ and stage $h\\in[H].$ there exists a parameter $\\rho_{h}^{\\pi}(f)\\in B(\\tilde{L_{2}})$ such that for all $(s,a)\\in{\\cal S}_{h}\\times{\\cal A}_{}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi,s,a}}{\\mathbb{E}}\\frac{\\mathbb{E}}{\\tau\\sim F_{\\bar{G},\\mathrm{Traj},h+1}}[R_{h:\\tau-1}+f(S_{\\tau})]-\\langle\\phi(s,a),\\rho_{h}^{\\pi}(f)\\rangle\\right|\\le\\tilde{\\eta}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tilde{\\eta}=\\eta(10H^{2}d_{0}/\\alpha+1)$ ", "page_idx": 5}, {"type": "text", "text": "4.2  The Benefit of Trajectory Data ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our learner will heavily rely on the result presented in Lemma 4.2. We will need to learn good estimates of the parameters $\\bar{\\rho_{h}^{\\pi^{0}}}(f)$ ,for any $f:S\\rightarrow[0,H],h\\in[H]$ However,to estimate a $\\bar{\\rho_{h}^{\\bar{\\pi}^{0}}}(f)$ ", "page_idx": 5}, {"type": "text", "text": "parameter well we will require least-squares targets that have bounded noise and expectation equal to $\\langle\\phi(s,a),\\rho_{h}^{\\pi}(f)\\rangle$ for all $(s,a)\\in S_{h}\\stackrel{\\cdot}{\\times}A$ . Full trajectory data (Assumption 2) makes this possible. Each full length trajectory $\\mathrm{traj}^{j}\\,=\\,(s_{t}^{j},a_{t}^{j},r_{t}^{j})_{t\\in[H+1]}j\\,\\in\\,[n]$ can be used to create the following least-squares target (which has the desired properties): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underset{\\tau\\sim F_{\\bar{G},h+1}^{j}}{\\mathbb{E}}\\left[r_{h:\\tau-1}^{j}+f\\left(s_{\\tau}^{j}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Importantly, it is because we have full length trajectories that we can transform the data available to simulate arbitrary length skipping mechanisms. ", "page_idx": 6}, {"type": "text", "text": "4.3Intuition Behind our Learner ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we describe the high-level intuition and ideas behind our learner. Consider the \u201cmodified\" MDP where low-range states are skipped. As the learner has access to trajectory data (Assumption 2), it can transform this data accordingly to simulate trajectories from the modified MDP. Any near-optimal policy for the modified MDP is also near-optimal for the original MDP (due to Eq. (3)). Thus, our previous linear realizability property (Lemma 4.2) allows for an offline RL version of the algorithm ELEANOR [Zanette et al., 2020] to statistically efficiently derive a near-optimal policy for the modified MDP. Indeed, the optimization problem underlying ELEANOR serves as a starting point for Optimization Problem 1, which is at the heart of our learner. ", "page_idx": 6}, {"type": "text", "text": "The challenge is that the true guess $\\bar{G}$ that Lemma 4.2 relies upon is not known to the learner. This means that the learner is not given any explicit information of what states to \u201cskip over\". To overcome this, we design a learner to output the policy $\\pi^{\\prime}$ (defined in Algorithm 1) based on Optimization Problem 1, where the optimization problem considers all guesses for the possible values of $\\bar{G}$ For each $G\\in\\bf G$ , it considers the MDP that skips over low-range states when the range is calculated according to $G$ . It then calculates sets $\\Theta_{G,h}$ for each stage $h$ , that are guaranteed (with high probability) to include the parameter $\\psi_{h}(\\pi_{G}^{\\star})$ realizing $q^{\\pi_{G}^{\\star}}$ (where $\\pi_{G}^{\\star}$ , defined in Eq. (15), is the optimal policy in the MDP with skipping based on $G$ ). We achieve this by defining $\\Theta_{G,h}$ backwards for $h=H,H-1,\\ldots,1$ By induction, if $\\Theta_{G,h+1},\\hdots,\\Theta_{G,H}$ all contain the desired parameter for their stage, then some parameter sequence in the Cartesian product $\\Theta_{G,h+1}\\times\\cdots\\times\\Theta_{G,H}$ allows us to near-perfectly (up to some misspecification error) compute $q^{\\pi_{G}^{\\star}}$ -values of stages $>h$ . Therefore, the least-squares parameter based on this sequence will be near the true parameter for stage $h$ . Defining $\\hat{\\Theta}_{G,h}$ to be all least-squares predictors for sequences in the aforementioned Cartesian product, and $\\Theta_{G,h}$ to be unions of the confidence ellipsoids around these predictors ensures the true parameter $\\psi_{h}(\\pi_{G}^{\\star})$ realizing $q^{\\pi_{G}^{\\star}}$ for stage $h$ is included in $\\Theta_{G,h}$ . This argument is made precise in Lemma D.2. ", "page_idx": 6}, {"type": "text", "text": "There are two problems remaining. One is that some values of $G$ considered by Optimization Problem 1 lead to skipping over important large-value states, degrading the performance of the best policy $\\pi_{G}^{\\star}$ available under that skipping. The other problem is that at the expense of making sure the true parameters are included in the sets $\\Theta_{G,h}$ , these sets might become large, in the sense of containing parameters that lead to very different predictions. Avoiding the first problem would make $v^{\\pi_{G}^{\\star}}(s_{1})$ nearly as large as $v^{\\star}(s_{1})$ . Avoiding the second problem would lead to tight $q$ -value estimators, and therefore to $v^{\\pi_{G}^{\\prime}}(s_{1})$ being nearly as large as $v^{\\pi_{G}^{\\star}}(s_{1})$ , for a policy $\\pi_{G}^{\\prime}$ that is greedy with respect to our hypothetically tight $q$ -value estimator. A key idea is to reject from consideration any $G\\in\\mathbf{G}$ that leads to $q$ -estimations that are not sufficiently tight (Eq. (14)). The reason we can do this is because for $\\dot{G}=\\bar{G}$ we can show that this condition passes (with high probability), and therefore we do not reject $\\bar{G}$ (precise statement in Lemma D.3). We can show this since we have trajectory data (Assumption 2), allowing us to use least-squares targets of the form used in Eq. (12), which we know are linearly realizable when $G=\\bar{G}$ (discussed in Section 4.2). Finally, we resolve the first problem by selecting among these tight estimators the one that guarantees the highest policy value from $s_{1}$ , which can be no worse than the value guaranteed by the choice of $G=\\breve{\\bar{G}}$ , which itself can be seen to be close to $v^{\\star}(s_{1})$ ", "page_idx": 6}, {"type": "text", "text": "4.4  Learner ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we formally introduce our learner, at the heart of which lies Optimization Problem 1. We define various $q$ and $v$ value estimators that we use. For $x\\in\\mathbb R$ ,let $\\mathrm{clip}_{[0,H]}\\,x=\\operatorname*{max}\\{0,\\operatorname*{min}\\{H,x\\}\\}$ ", "page_idx": 6}, {"type": "text", "text": "Then, for $h\\in[H],s\\in\\cup_{t\\in[h:H+1]}S_{t},a\\in\\mathcal{A},\\theta\\in\\mathbb{R}^{d},\\theta_{h:H+1}=(\\theta_{h},\\ldots,\\theta_{H+1})\\in\\mathbb{R}^{d(H-h+2)}$ let ", "page_idx": 7}, {"type": "equation", "text": "$$\nq_{\\theta}(s,a)=\\langle\\phi(s,a),\\theta\\rangle,\\quad q_{\\theta_{h:H+1}}(s,a)=q_{\\theta_{\\mathrm{stage}(s)}}(s,a),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\nv_{\\theta}(s)=\\operatorname*{max}_{a\\in{\\cal{A}}}q_{\\theta}(s,a),\\quad v_{\\theta_{h:H+1}}(s)=\\operatorname*{max}_{a\\in\\cal{A}}q_{\\theta_{h}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{q}_{\\theta}(s,a)=\\mathrm{clip}_{[0,H]}\\,q_{\\theta}(s,a),\\quad\\bar{q}_{\\theta_{h:H+1}}(s,a)=\\bar{q}_{\\theta_{\\mathrm{stage}(s)}}(s,a),}\\\\ &{\\qquad\\bar{v}_{\\theta}(s)=\\mathrm{clip}_{[0,H]}\\,v_{\\theta}(s),\\quad\\bar{v}_{{\\theta_{h:H+1}}}(s)=\\bar{v}_{\\theta_{\\mathrm{stage}(s)}}(s)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Optimization Problem 1. ", "text_level": 1, "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\qquad\\mathrm{arg\\,max}_{G\\in\\mathbf{G},\\theta_{1,H+1}^{\\dagger}\\in\\Theta_{G,1}\\times\\cdots\\times\\Theta_{G,H+1}}\\,\\bar{v}_{\\theta_{1}^{\\dagger}}(s_{1})\\qquad\\mathrm{subject~to,~for~all~}h\\in[H]}\\\\ &{X_{h}=\\lambda I+\\sum_{j\\in[n]}\\phi_{h}^{j}(\\phi_{h}^{j})^{\\top}\\,,}\\\\ &{\\hat{\\Theta}_{G,h}=\\left\\{X_{h}^{-1}\\displaystyle\\sum_{j\\in[n]}\\phi_{h}^{j}\\displaystyle\\sum_{\\tau\\sim F_{G,h+1}^{\\dagger}}\\left[r_{h:\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}\\left(s_{\\tau}^{j}\\right)\\right]:\\theta_{h+1:H+1}\\in\\displaystyle\\sum_{u=h+1}^{H+1}\\Theta_{G,u}\\right\\},}\\\\ &{\\Theta_{G,h}=\\left\\{\\theta_{h}\\in B(\\tilde{L}_{2}):\\operatorname*{min}_{\\hat{\\theta}_{h}\\in\\hat{\\Theta}_{G,h}}\\lVert\\theta_{h}-\\hat{\\theta}_{h}\\rVert_{X_{h}}\\leq\\beta\\right\\},\\ \\Theta_{G,H+1}=\\left\\{\\bar{\\Theta}\\right\\}\\ \\beta\\,\\mathrm{\\,defn~Eq.\\,(32)}\\,,}\\\\ &{\\frac{1}{n}\\sum_{j\\in[n]}\\left(\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s_{h}^{j},a_{h}^{j})-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s_{h}^{j},a_{h}^{j})\\right)\\leq\\bar{\\epsilon},\\qquad\\qquad\\epsilon\\mathrm{\\,defn~Eq.\\,(29)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Let $(G^{\\prime},\\theta_{1:H+1}^{\\prime})$ denote the solution to Optimization Problem1.Notice that unlike ELEANOR,we optimize over all $G\\in\\mathbf{G}$ , which can be seen as an optimization over all possible \u201c\"modified MDPs\" with different skipping mechanisms. Another observation is that apart from $h=1$ , the choice of $\\theta_{h}^{\\prime}$ from $\\Theta_{G^{\\prime},h}$ made by the optimization is arbitrary. Indeed, unlike ELEANOR, which chooses globally optimistic least-squares predictors for each stage, our optimization does not need to care about (or optimize for) the specific choice of $q$ -value predictors from their respective confidence sets $\\Theta_{G,h}$ We can be agnostic to the choice of $q$ -value predictors because all choices lead to similar predictions due to Eq. (14), as will be shown formally, in Lemma 5.1. However, fixing an arbitrary concrete choice in the optimization allows us to define an output policy $\\pi^{\\prime}$ that is parametrized only by these vectors, making both the memory and computational requirements of representing and executing $\\pi^{\\prime}$ small. ", "page_idx": 7}, {"type": "text", "text": "Our learner (Algorithm 1) solves Optimization Problem 1 and uses $\\theta_{1:H+1}^{\\prime}$ to output a greedy policy. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 1: Learner ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1: input: accuracy $\\epsilon>0$ , failure probability $\\delta>0$ , concentrability coefficient $C_{\\mathrm{conc}}<\\infty$ \uff0c trajectories $(\\mathbf{traj}^{1},\\ldots,\\mathbf{traj}^{n})$ , features $\\left(\\left(\\phi(s_{h}^{1},a)\\right)_{h\\in[H],a\\in\\mathcal{A}},\\ldots,\\left(\\phi(s_{h}^{n},a)\\right)_{h\\in[H],a\\in\\mathcal{A}}\\right)$ , norm bounds $L_{1},L_{2}$   \n2: $G^{\\prime},\\theta_{1:H+1}^{\\prime}\\leftarrow$ solution to Optimization Problem 1   \n3 $\\begin{array}{r}{\\pi^{\\prime}(a|s)\\xleftarrow{}\\mathbb{1}\\biggl\\{a=\\arg\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}\\bar{q}_{\\theta_{\\mathrm{stage}(s)}^{\\prime}}(s,a^{\\prime})\\biggr\\}\\quad\\mathrm{for}\\;\\mathrm{all}\\;(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "4: return $\\pi^{\\prime}$ ", "page_idx": 7}, {"type": "text", "text": "5  Proof of Theorem 1 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Before giving the proof, we formally define the optimal policy in the modified MDP that skips according to rangeG', for any $G\\in\\mathbf{G}$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{G}^{\\star}(a|s)=\\pi^{0}(a|s)\\omega_{G}(s)+\\mathbb{1}\\big\\{a=\\arg\\operatorname*{max}_{a^{\\prime}\\in A}q^{\\pi_{G}^{\\star}}(s,a^{\\prime})\\big\\}(1-\\omega_{G}(s))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Notice that for states $s~\\in~S_{h}$ for some $\\textit{h}\\in[H]$ \uff0c $\\pi_{G}^{\\star}(\\cdot|s)$ in the above definition depends on the value of $q^{\\pi_{G}^{\\star}}(s^{\\prime},\\cdot)$ for some $s^{\\prime}\\in S_{h+1}$ . We can therefore interpret the above recursive definition as defining $\\pi_{G}^{\\star}(\\cdot|s)$ for $s\\in S_{h}$ , first for $h=H+1$ , then $h\\,=\\,H$ , etc., down to $h\\,=\\,1$ . Every time we define the policy for some stage $h$ in such a way, the policy and $q^{\\pi_{G}^{\\star}}(s^{\\prime},\\cdot)$ are already defined on later stages, making the definition valid, and resolving the recursive nature of Eq. (15). ", "page_idx": 7}, {"type": "text", "text": "Proof. $v^{\\star}(s_{1})-v^{\\pi^{\\prime}}(s_{1})$ can be decomposed into the following eror terms. ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{\\star}(s_{1})-v^{\\pi^{\\prime}}(s_{1})=\\underbrace{v^{\\star}(s_{1})-v^{\\pi_{G}^{\\star}}(s_{1})}_{(1)}+\\underbrace{v^{\\pi_{G}^{\\star}}(s_{1})-v_{\\theta_{1}^{\\prime}}(s_{1})}_{(\\mathrm{II})}+\\underbrace{v_{\\theta_{1}^{\\prime}}(s_{1})-v^{\\pi^{\\prime}}(s_{1})}_{(\\mathrm{III})}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The remainder of the proof focuses on bounding these error terms. Following the intuition described in Section 4, showing that terms (I) and $(\\mathrm{II})$ are small can be seen as addressing the first problem of potentially skipping over large-value states, while showing that term (Ill) is small can be seen as addressing the second problemof $\\pi^{\\prime}$ being greedy w.r.t. to a potentially inaccurate estimates $\\theta_{1:H+1}^{\\prime}$ ", "page_idx": 8}, {"type": "text", "text": "Bounding $\\mathrm{(I)}=v^{\\star}(s_{1})-v^{\\pi_{\\bar{G}}^{\\star}}(s_{1})$ : This term cannot be too large since the rangeG function is approximately correct (Lemma 4.1), and we only skip over states with low rangeG (Eq. (6)), implying the action we take doesn't affect the value function much (Eq. (3)). In Appendix D.1 we formalize this intuition, and show the following result. ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\mathrm{(I)}}=v^{\\star}(s_{1})-v^{\\pi_{\\bar{G}}^{\\star}}(s_{1})\\leq H(2\\alpha+2\\eta)\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Bounding $\\mathbf{\\Omega}(\\mathbf{II})\\ =\\ v^{\\pi}\\dot{\\bar{\\alpha}}\\big(s_{1}\\big)\\,-\\,\\bar{v}_{\\theta_{1}^{\\prime}}\\big(s_{1}\\big)$ : This term can be bounded by approximately zero due to Optimization Problem 1 being optimistic from the start state. First, note that $v^{\\pi_{\\bar{G}}^{\\star}}$ is approximately equal to $\\bar{v}_{\\psi_{1}(\\pi_{\\bar{G}}^{\\star})}$ (Assumption 1). Then, in Lemma D.2 we show that $\\psi_{1}(\\pi_{\\bar{G}}^{\\star})\\;\\in\\;\\bar{\\Theta}_{\\bar{G},1}$ , and in Lemma D.3 we show that $\\bar{G}$ is a feasible solution to Optimization Problem 1. Since $\\left(G^{\\prime},\\theta_{1:H+1}^{\\prime}\\right)$ is the solution to Optimization Problem 1, it holds that $\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})\\geq\\bar{v}_{\\theta}(s_{1})$ for any $\\theta\\in\\Theta_{G,1}$ where $G$ is a feasible solution to Optimization Problem 1. Thus $\\bar{v_{\\theta_{1}^{\\prime}}}(s_{1})\\geq v_{\\psi_{1}(\\pi_{\\bar{G}}^{\\star})}(s_{1})$ In Appendix D.2 we formalize this intuition, and show that with probability at least $1-\\delta$ \uff0c ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{(II)}=v^{\\pi}\\dot{\\bar{G}}\\left(s_{1}\\right)-\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})\\leq\\eta\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Bounding $\\quad\\mathrm{(III)}\\,=\\,{\\bar{v}}_{\\theta_{1}^{\\prime}}(s_{1})\\,-\\,v^{\\pi^{\\prime}}(s_{1})$ : To bound term (I) we will first show in Lemma 5.1 that value estimates in terms of $\\theta_{h}^{\\prime}$ and $\\psi_{h}(\\pi_{G^{\\prime}}^{\\star})$ are close with high probability for all $h\\in[H+1]$ . This lemma allows us torelate $\\bar{v}_{\\theta_{1}^{\\prime}}$ $\\bar{v}_{\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})}$ and then Assumption 1 relates $\\bar{v}_{\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})}$ 0 $v^{\\pi_{G^{\\prime}}^{\\star}}$ . Wwe are then left with relating $v^{\\pi_{G^{\\prime}}^{\\star}}$ to $v^{\\pi^{\\prime}}$ . To do this we claim that $\\pi^{\\prime}$ is an approximate policy improvement step w.t. $v^{\\pi_{G^{\\prime}}^{\\star}}$ whianebrllin $\\pi^{\\prime}$ is greedy w.t. $\\bar{q}_{\\theta_{1:H+1}^{\\prime}}$ and as we mentioned a couple sentences ago, $\\bar{v}_{\\theta_{h}^{\\prime}}$ and $\\bar{v}_{\\psi_{h}(\\pi_{G^{\\prime}}^{\\star})}$ are close for all $h\\in[H+1]$ ", "page_idx": 8}, {"type": "text", "text": "To formalize this intuition we begin by decomposing $\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})\\mathrm{~-~}v^{\\pi^{\\prime}}(s_{1})$ into the following error terms ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})-v^{\\pi^{\\prime}}(s_{1})=\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})-\\bar{q}_{\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})}(s_{1},\\pi^{\\prime}(s_{1}))+\\bar{q}_{\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})}(s_{1},\\pi^{\\prime}(s_{1}))-v^{\\pi^{\\prime}}(s_{1})\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To bound $\\begin{array}{r}{\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})-\\bar{q}_{\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})}(s_{1},\\pi^{\\prime}(s_{1}))}\\end{array}$ we introduce a useful lemma (proof in Appendix F). ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.1. There is an event $\\mathcal{E}_{2}$ ,that occurs with probability at least $1-\\delta/3$ suchthatunder event $\\mathcal{E}_{2}$ , for all $G\\,\\in\\,\\mathbf{G}$ that are feasible solutions to Optimization Problem $^{\\,l}$ , for all $\\textit{h}\\in[H]$ for all $(\\theta_{s,a})_{(s,a)\\in S_{h}\\times A}$ and $(\\check{\\theta}_{s,a})_{(s,a)\\in\\mathcal{S}_{h}\\times A}\\in\\Theta_{G,h}^{\\mathcal{S}_{h}\\times A}$ ,x, and for all admissible distributions v = $(\\nu_{t})_{t\\in[H]}$ , it holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\underset{(S,A)\\sim\\nu_{h}}{\\mathbb{E}}\\Big[\\bar{q}_{\\theta_{S,A}}(S,A)-\\bar{q}_{\\check{\\theta}_{S,A}}(S,A)\\Big]\\leq\\tilde{\\epsilon}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To use Lemma 5.1 we must show that $\\begin{array}{r}{\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})-\\bar{q}_{\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})}(s_{1},\\pi^{\\prime}(s_{1}))}\\end{array}$ satisfies its requirements. First, note that $\\bar{v}_{\\theta_{1}^{\\prime}}\\bigl(s_{1}\\bigr)\\;=\\;\\bar{q}_{\\theta_{1}^{\\prime}}\\bigl(s_{1},\\pi^{\\prime}\\bigl(s_{1}\\bigr)\\bigr)\\;\\;$ , by definition of $\\pi^{\\prime}$ (line 3). Second, $G^{\\prime}$ is the solution to Optimization Problem 1, thus, a feasible solution. Third, by Lemma D.2, there is an event ${\\mathcal{E}}_{1}$ , which occurs with probability at least $1\\!-\\!\\delta/3$ , such that under event ${\\mathcal{E}}_{1}$ $\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})\\in\\Theta_{G^{\\prime},1}$ , and by definition $\\theta_{1}^{\\prime}\\in\\Theta_{G^{\\prime},1}$ Let $\\nu_{h}(s,a)=\\mathbb{P}_{\\underline{{\\pi}}^{\\prime},s_{1}}^{h}(s,a)$ for all $h\\in[H],(s,a)\\in S_{h}\\times A$ Clearly $\\nu=(\\nu_{h})_{h\\in[H]}$ .s an admissible distribution by Definition 1. Thus, under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}$ , by Lemma 5.1, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})-\\bar{q}_{\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})}(s_{1},\\pi^{\\prime}(s_{1}))=\\underset{(S,A)\\sim\\nu_{1}}{\\mathbb{E}}\\Big[\\bar{q}_{\\theta_{1}^{\\prime}}(S,A)-\\bar{q}_{\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})}(S,A)\\Big]\\leq\\tilde{\\epsilon}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "It is left to bound $\\bar{q}_{\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})}(s_{1},\\pi^{\\prime}(s_{1}))-v^{\\pi^{\\prime}}(s_{1})$ in Eq. (18). To do this, first note that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\bar{q}_{\\psi_{1}(\\pi_{G^{\\prime}}^{\\star})}(s_{1},\\pi^{\\prime}(s_{1}))-v^{\\pi^{\\prime}}(s_{1})\\le q^{\\pi_{G^{\\prime}}^{\\star}}(s_{1},\\pi^{\\prime}(s_{1}))-v^{\\pi^{\\prime}}(s_{1})+\\eta\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where the inequality holds since we have approximate linear $q^{\\pi}$ -realizability (Assumption 1). To bound $q^{\\pi_{G^{\\prime}}^{\\star}}(s_{1},\\pi^{\\prime}(\\dot{s}_{1}))-v^{\\pi^{\\prime}}(s_{1})$ notice that $\\stackrel{\\_}{v}{}^{\\pi^{\\prime}}(s_{1})=q^{\\pi^{\\prime}}(s_{1},\\pi^{\\prime}(s_{1}))$ , which implies that 9 $\\pi_{G^{\\prime}}^{*}(s_{1},\\pi^{\\prime}(s_{1}))-v^{\\pi^{\\prime}}(s_{1})=q^{\\pi_{G^{\\prime}}^{*}}(s_{1},\\pi^{\\prime}(s_{1}))-q^{\\pi^{\\prime}}(s_{1},\\pi^{\\prime}(s_{1}))=\\underset{\\mathrm{Trij}\\sim\\mathbb{P}_{\\pi^{\\prime},s_{1}}}{\\mathbb{E}}\\left[v^{\\pi_{G^{\\prime}}^{*}}(S_{2})-v^{\\pi^{\\prime}}(S_{2})\\right].$ Next, we give a bound on $\\mathbb{E}_{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{\\prime},s_{1}}}\\left[v^{\\pi_{G^{\\prime}}^{\\star}}(S_{2})-v^{\\pi^{\\prime}}(S_{2})\\right]$ (proofin Appendix D.3): ", "page_idx": 9}, {"type": "text", "text": "Lemma 5.2. Under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}$ for any $h\\in[2:H+1]$ it holds that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{\\prime},s_{1}}}{\\mathbb{E}}\\Big[v^{\\pi_{G^{\\prime}}^{\\star}}(S_{h})-v^{\\pi^{\\prime}}(S_{h})\\Big]\\le2(H-h+2)(\\eta+\\tilde{\\epsilon})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Intuitively, the above lemma holds since $\\pi^{\\prime}$ can be thought of as an approximate policy improvement step w.rt. $v^{\\pi_{G^{\\prime}}^{\\star}}$ . To see this, eallthat $\\pi^{\\prime}$ is greedy w.t. $\\bar{q}_{\\theta_{1:H+1}^{\\prime}}$ (ine 3). Then, with Lemma 5.1, we can show $\\bar{v}_{\\theta_{h}^{\\prime}}$ and $\\bar{v}_{\\psi_{h}(\\pi_{G^{\\prime}}^{\\star})}$ (which is close to $v^{\\pi_{G^{\\prime}}^{\\star}}$ (Assumption 1)) are close for all $h\\in[H+1]$ The above bounds imply that under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}$ , which occurs with probability at least $1-2\\delta/3$ \uff0c ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathrm{(III)}=\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})-v^{\\pi^{\\prime}}(s_{1})\\le2H(\\eta+\\tilde{\\epsilon})+\\tilde{\\epsilon}+\\eta\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Combining the Bounds: To finish the proof we combine the bounds on all three terms (Eqs. (16), (17) and (19)), to get that under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ , which occurs with probability at least $1-\\delta$ ", "page_idx": 9}, {"type": "equation", "text": "$$\nv^{\\star}(s_{1})-v^{\\pi^{\\prime}}(s_{1})\\leq H(2\\alpha+2\\eta)+\\eta+2H(\\eta+\\tilde{\\epsilon})+\\tilde{\\epsilon}+\\eta\\leq4(H+1)(\\alpha+\\eta+\\tilde{\\epsilon})\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "To bound the above display by $\\epsilon$ we set $\\alpha=\\epsilon/(12(H+1))<1$ . If $n=\\tilde{\\Theta}\\big(C_{\\mathrm{conc}}^{4}H^{7}d^{4}/\\epsilon^{2}\\big)$ and $\\eta=\\tilde{\\mathcal{O}}\\mathopen{}\\mathclose\\bgroup\\left(\\alpha/\\sqrt{n H}\\aftergroup\\egroup\\right)$ (Eq. (26), we show that $\\tilde{\\epsilon}=\\tilde{\\mathcal{O}}\\big(C_{\\mathrm{conc}}^{2}H^{5/2}d^{2}/\\sqrt{n}\\big)$ (Eq. (44). This implies that ", "page_idx": 9}, {"type": "equation", "text": "$$\n4(H+1)(\\alpha+\\eta+\\tilde{\\epsilon})\\leq\\epsilon\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "6  Limitations and Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work we resolved an open problem in the positive, by presenting the first statistically efficient learner (Section 4.4) that outputs a near optimal policy in the offline RL setting with approximate linear $q^{\\pi}$ -realizability (Assumption 1), trajectory data (Assumption 2), and concentrability (Assumption 3). One limitation of this work is that we are not aware of any computationally efficient implementation of Optimization Problem 1, which is at the heart of our learner. As such, it is left as an open problem whether computationally effcient learning is possible in the setting we considered. Another limitation is that we are not sure if our statistical rate in Theorem 1 is optimal. Showing a matching lower bound or improving the rate is left for future work. ", "page_idx": 9}, {"type": "text", "text": "Another limitation of our work originates from our setting underpinning our result (Section 4), namely the three assumptions: approximate linear $q^{\\pi}$ -realizability, trajectory data, and concentrability. Approximate linear $q^{\\pi}$ -realizability requires the value function of all memoryless policies to be linear in a fixed and known $d_{\\cdot}$ -dimensional feature map. While strictly weaker than the linear MDP assumption [Zanette et al., 2020], this assumption is still strong. Trajectory data requires full sequences of interactions with an environment to be collected by a single policy. For long horizon problems this can be practically challenging. Concentrability requires the state and action spaces to be well-covered. This can be challenging to guarantee since often the state and action spaces are unknown at the time of data collection. Further, since we require the trajectory data to be collected by a single policy, it may be the case that no single policy exists that covers the state and action spaces well, and a mixture of policies must be considered, which our current result does not immediately hold for. Although the assumptions appear strong, a justification for them is that under many variations of weaker assumptions (for instance: general data, or linear $q^{\\pi}$ -realizability of only one policy, or only coverage of the feature space), polynomial statistical rates have been shown to be impossible to achieve by any learner (Table 1). ", "page_idx": 9}, {"type": "text", "text": "Since this work is focused on foundational theoretical research it is unlikely to have any direct and immediate societal impacts. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Y. Abbasi- Yadkori, D. Pal, and C. Szepesvari. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24, 2011.   \nP. Amortila, N. Jiang, and T. Xie. A variant of the wang-foster-kakade lower bound for the discounted setting. arXiv preprint arXiv:2011.01075, 2020.   \nQ. Cai, Z. Yang, C. Jin, and Z. Wang. Provably efficient exploration in policy optimization. In International Conference on Machine Learning, pages 1283-1294. PMLR, 2020.   \nJ. Chen and N. Jiang. Information-theoretic considerations in batch reinforcement learning. In International Conference on Machine Learning, pages 1042-1051. PMLR, 2019.   \nY. Duan, Z. Jia, and M. Wang. Minimax-optimal off-policy evaluation with linear function approximation. In International Conference on Machine Learning, pages 2701-2709. PMLR, 2020.   \nD. J. Foster, A. Krishnamurthy, D. Simchi-Levi, and Y. Xu. Offline reinforcement learning: Fundamental barriers for value function approximation. arXiv preprint arXiv:2111.10919, 2021.   \nW. Hoeffding. Probability inequalities for sums of bounded random variables. The collected works of Wassily Hoeffding, pages 409-426, 1994.   \nZ. Jia, A. Rakhlin, A. Sekhari, and C.-Y. Wei. Offine reinforcement learning: Role of state aggregation and trajectory data. arXiv preprint arXiv:2403.17091, 2024.   \nY. Jin, Z. Yang, and Z. Wang. Is pessimism provably efficient for offine rl? In International Conference on Machine Learning, pages 5084-5096. PMLR, 2021.   \nT. Lattimore and C. Szepesvari. Bandit algorithms. Cambridge University Press, 2020.   \nR. Munos and C. Szepesvari. Finite-time bounds for ftted value iteration. Journal of Machine Learning Research, 9(5), 2008.   \nM. J. Todd. Minimum-volume ellipsoids: Theory and algorithms. SIAM, 2016.   \nR. Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \nR. Wang, D. P. Foster, and S. M. Kakade. What are the statistical limits of offline rl with linear function approximation? arXiv preprint arXiv:2010.11895, 2020.   \nG. Weisz, A. Gyorgy, and C. Szepesvari. Online rl in linearly qpi-realizable mdps is as easy as in linear mdps if you learn what to ignore. arXiv preprint arXiv:2310.07811, 2023.   \nT. Xie and N. Jiang. Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404-11413. PMLR, 2021.   \nT. Xie, C.-A. Cheng, N. Jiang, P. Mineiro, and A. Agarwal. Bellman-consistent pessimism for offine reinforcement learning. Advances in neural information processing systems, 34:6683-6694, 2021.   \nT. Xie, M.Bhardwaj, N. Jiang, and C-A. Cheng. Armor: A model-based framework for improving arbitrary baseline policies with offline data. arXiv preprint arXiv:2211.04538, 2022.   \nW. Xiong, H. Zhong, C. Shi, C. Shen, L. Wang, and T. Zhang. Nearly minimax optimal offine reinforcement learning with linear function approximation: Single-agent mdp and markov game. arXiv preprint arXiv:2205.15512, 2022.   \nA.ZanetExpnntial webousforbathreforcmnt laing:Batch anbxtially harder than online rl. In International Conference on Machine Learning, pages 12287- 12297. PMLR, 2021.   \nA. Zanette, A. Lazaric, M. Kochenderfer, and E. Brunskill. Learning near optimal policies with low inherent bellman error. In International Conference on Machine Learning, pages 10978-10989. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A  Parameter Settings and Notation ", "text_level": 1, "page_idx": 11}, {"type": "equation", "text": "$$\nn=\\tilde{\\Theta}\\bigg(\\frac{C_{\\mathrm{conc}}^{4}H^{7}d^{4}}{\\epsilon^{2}}\\bigg)\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Set at the end of Section 5 ", "page_idx": 11}, {"type": "text", "text": "$\\begin{array}{r l}&{d_{0}=\\lceil4d\\log\\log(d)+16\\rceil}\\\\ &{L_{1}=\\operatorname{Upper\\,bound\\;on\\;2-norm\\;of\\;features}}\\end{array}$ $\\phi$ $L_{2}=$ Upper bound on 2-norm of true parameters $\\psi_{h},h\\in[H]$ $\\begin{array}{r l}&{\\begin{array}{r l}&{L_{22}^{\\prime}=L_{21}\\Delta{t}^{2}\\Delta{t}\\Delta{t}\\Delta{t}\\quad}\\\\ &{\\times\\Delta{t}^{2}\\Delta{t}^{2}L_{1}^{\\prime}}\\\\ &{\\times\\displaystyle{\\frac{t}{2}}\\Biggl(H^{2}\\Delta{t}\\Delta{t}\\Biggr)}\\\\ &{\\times\\displaystyle{\\frac{t}{2}}\\Biggl(\\prod_{i=1}^{3}H^{2}\\Delta{t}\\Biggl)-6\\left(\\frac{H^{2}\\Delta{t}}{\\sqrt{T_{i}\\Delta{t}}}\\right)-6\\left(\\frac{L_{1}^{\\prime}}{C_{22}\\sqrt{t}H^{2}}\\right)}\\\\ &{\\phantom{\\times}\\Biggl{\\{6(10\\,t^{2}\\Delta{t}\\cos{t}\\,+1)-\\phi\\left(H^{2}\\Delta{t}\\right)\\}}}\\\\ &{\\quad\\times=\\mathrm{0}(L_{2}^{\\prime}\\sin{t})}\\\\ &{\\quad\\times=\\mathrm{0}(L_{2}^{\\prime}\\sin^{2}{t})}\\\\ &{\\quad\\times=\\mathrm{\\bar{\\phi}}\\left(\\frac{\\displaystyle{\\frac{t}{2}}\\sum_{i=1}^{3}H^{2}\\Delta{t}^{2}}{\\sqrt{T_{i}\\Delta{t}^{2}}}\\right)}\\\\ &{\\quad\\times=\\mathrm{\\bar{\\phi}}\\left(\\frac{L_{2}^{\\prime}\\sin^{2}{t}{t}}{\\sqrt{T_{i}\\Delta{t}^{2}}}\\right)}\\\\ &{\\displaystyle{\\bar{\\beta}}=\\mathrm{\\bar{\\phi}}\\left(\\frac{\\Delta{t}^{2}\\sin^{2}{t}{t}}{\\sqrt{T_{i}\\Delta{t}^{2}}}\\right)}\\\\ &{\\quad\\times\\displaystyle{\\frac{\\partial{t}^{2}}{\\partial{t}}}\\Biggl(\\prod_{i=1}^{3}\\left(1-2L_{2}\\right)\\cos{\\phi}\\left(1\\right)\\cos{\\phi}\\left(1\\right)\\sin{\\phi}\\left(\\Delta{t}\\right)}\\\\ &{\\quad L_{1}^{\\prime}=\\mathrm{\\bar{\\phi}}\\left(1\\right)\\sin{\\phi}\\left(1-\\cos^{2}{t}\\left(\\Delta{t}\\Delta{t}\\right)L_{1}^{\\prime}\\sin^{2}{t}\\Delta{t}\\right)}\\\\ &{\\quad\\times=\\mathrm{\\bar{\\Psi}}\\left(\\frac{\\Delta{t}}{\\sqrt{T_{i}\\Delta{t}^{2}}}\\right)+\\mathrm{\\bar{\\Psi}}\\left(\\frac{\\Delta{$ ", "page_idx": 11}, {"type": "text", "text": "(20) Defined above Eq. (4) (21) Defined above Assumption 1 (22) Assumption 1 (23) Defined in Lemma 4.2 (24) Defined in Eq. (57) (25) Defined in Eq. (59) (26) Defined in Lemma 4.2 (27) Defined in Eq. (72) (28) Defined in Eq. (51) (29) Defined in Eq. (44) (30) Defined in Eq. (77) (31) Defined in Eq. (58) (32) Defined in Lemma I.4 (33) Defined in Eq. (88) (34) Defined at the end of Section 5 (35) ", "page_idx": 11}, {"type": "text", "text": "BTrajectory Data vs. Non-Trajectory Data ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To compare the two types of data, we first define \u201cnon-trajectory\u201d data, which consist of individual transitions without any guarantees of them coming from complete MDP trajectories. ", "page_idx": 12}, {"type": "text", "text": "Assumption 4 (Non-Trajectory Data). Assume that for each $h\\in[H]$ the learner is given a dataset of transition tuples and corresponding features of size $n\\geq1$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left(s_{h}^{j},a_{h}^{j},r_{h}^{j},\\bar{s}_{h+1}^{j}\\right)_{j\\in[n]}\\quad a n d\\quad((\\phi(s_{h}^{j},a))_{a\\in A},(\\phi(\\bar{s}_{h+1}^{j},a))_{a\\in A})_{j\\in[n]}\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $(s_{h}^{j},a_{h}^{j})\\sim\\mu_{h}^{\\prime},r_{h}^{j}\\sim\\mathcal{R}(s_{h}^{j},a_{h}^{j}),\\bar{s}_{h+1}^{j}\\sim P(s_{h}^{j},a_{h}^{j}),$ for all $j\\in[n]$ and $\\mu_{h}^{\\prime}\\in\\mathcal{M}_{1}(S_{h}\\times\\mathcal{A})$ is $a$ \u201cdata collection distribution\u201d'unknown to the learner. ", "page_idx": 12}, {"type": "text", "text": "There are two differences between the above non-trajectory data and trajectory data as defined in Assumption 2. In particular, if $\\mu_{h}^{\\prime}=\\mathbb{P}_{\\pi^{0},s_{1}}^{h}$ and sh+1 = sh+1, then we get back the trajectory data assumption. ", "page_idx": 12}, {"type": "text", "text": "Foster et al. [2021] showed a negative result under Assumption 4. Our method addresses the hard instance from [Foster et al., 2021] if the data is given as complete trajectories. Below we explain why the lower bound constructions from [Foster et al., 2021] break down if they need to use trajectory data, and why our algorithm breaks down if it doesnt have trajectory data. ", "page_idx": 12}, {"type": "text", "text": "The lower bound constructions in Theorems 1.1 and 1.2 of [Foster et al., 2021] were both made hard because the data collection distributions of individual transition tuples $(s,a,r,{\\bar{s}})$ were selected such that they reveal no (or almost no) information about the MDP instance. In both cases, receiving samples from the joint distribution of the entire trajectory $\\mathbb{P}_{\\pi^{0},s_{1}}$ makes the problem easy. In the case of Theorem 1.1, one would simply observe which states are reachable from the start state (the planted states). For Theorem 1.2, some information on whether any next-state $\\bar{s}$ is planted or not would be leaking in each trajectory, in the form of being able to observe the next-state transition from exactly $\\bar{s}$ ", "page_idx": 12}, {"type": "text", "text": "A simpler example showing the root of the problem with non-trajectory data is as follows. Consider the toy problem of learning the value of some policy $\\pi$ after taking action $a$ in state $s_{1}$ in a 2- stage MDP. The data is given as tuples of the form $\\left(s_{1},a,r_{1}^{1},\\bar{s}_{2}^{1},\\ldots,\\bar{s}_{1},a,r_{1}^{n},\\bar{s}_{2}^{n}\\right)$ for the first stage and $\\left(s_{2}^{1},a_{2}^{1},r_{2}^{1},\\bar{s}_{3}^{1},\\ldots,s_{2}^{n},a_{2}^{n},r_{2}^{n},\\bar{s}_{3}^{n}\\right)$ for the second stage. Notice there is no guarantee that $s_{2}^{j}\\sim$ $P(s_{1},a)$ with $j\\in[n]$ . We cannot infer what the rewards from the second-stage states distributed as $P(s_{1},a)$ might look like from the data, making this problem hopelessly hard. In the extreme, the MDP might have infinitely many second-stage states, with the probability of any $\\bar{s}_{2}^{j}=s_{2}^{k}$ (for any $j$ and $k$ ) being O, highlighting that one cannot just connect and importance weight matching nextstates $\\bar{s}_{2}^{j}$ of the first-stage transitions with matching start-states $s_{2}^{k}$ of the second-stage transitions. In contrast, if we assume the data is such that $\\bar{s}_{2}^{j}=s_{2}^{j}$ , this problem is immediately avoided as samples from $P(s_{1},a)$ , along with rewards from those states are directly handed to the learner. The learner can then simply use all of the rewards $r_{2}^{j}$ from tuples that contain the action $\\pi(s_{2}^{j})$ (which we have on average at least $1/C_{\\mathrm{conc}}$ of, due to concentrability) to estimate the value of policy $\\pi$ after taking $s_{1},a$ (solving the toy problem). ", "page_idx": 12}, {"type": "text", "text": "Now consider our algorithm if we do not have trajectory data. In this case we are no longer able to construct least squares targets of the form needed to make use of Lemma 4.2 (discussed in Section 4.2). This means that we would not be able to guarantee that our targets are linear, even under the true skipping mechanism $\\bar{G}$ , implying that $\\bar{G}$ might not be a feasible solution to our optimization problem. Then our optimism argument that the output of the optimization problem has a value estimate at least as large as the value estimate based on G would no longer hold, causing our whole proof strategy to break down. ", "page_idx": 12}, {"type": "text", "text": "C Proof of Lemma 4.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. We follow a proof technique introduced in [Weisz et al., 2023]. We start by quoting their definition of admissible functions and their admissible realizability lemma. ", "page_idx": 13}, {"type": "text", "text": "Definition 2 (Definition 4.6 in [Weisz et al., 2023]). For any $h\\in[H]$ $f:S_{h}\\rightarrow\\mathbb{R}$ is $\\alpha^{\\prime}$ -admissible for some $\\alpha^{\\prime}>0$ if for all $s\\in S_{h}$ \uff0c $|f(s)|\\le\\mathrm{range}(s)/\\alpha^{\\prime}$ ", "page_idx": 13}, {"type": "text", "text": "Lemma C.1 (Admissible-realizability (Lemma 4.7 in [Weisz et al., 2023]). If $f:S_{h}\\rightarrow\\mathbb{R}$ is $\\alpha^{\\prime}$ admissible then it is realizable, that is, for all $t\\in[h-1]$ and $\\pi\\in\\Pi$ thereexistssome $\\tilde{\\theta}\\in\\mathbb{R}^{d}$ with $\\left\\|\\widetilde{\\theta}\\right\\|_{2}\\leq4d_{0}L_{2}/\\alpha^{\\prime}$ such that for all $(s,a)\\in S_{t}\\times A_{t}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\underset{\\mathrm{Trij}\\sim\\mathbb{P}_{\\pi,s,a}}{\\mathbb{E}}f(S_{h})-\\left\\langle\\phi(s,a),\\tilde{\\theta}\\right\\rangle\\right|\\le\\eta_{0}\\qquad\\qquad w h e r e\\;\\eta_{0}=5d_{0}\\eta/\\alpha^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, we fix some $f:S\\rightarrow[0,H]$ with $f(s_{\\intercal})=0$ , and policy $\\pi\\in\\Pi$ . For $2\\leq h\\leq H+1$ , define $\\tilde{g}_{h}:S_{h}\\rightarrow[-H,H]$ as $\\tilde{g}_{H+1}(\\cdot)=0$ , and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{g}_{h}(s)=\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi,s}}{\\mathbb{E}}\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{Traj},h}}{\\mathbb{E}}[-R_{\\tau:H}+f(S_{\\tau})]}\\\\ &{\\qquad=\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi,s}}{\\mathbb{E}}\\underset{t=h}{\\overset{H}{\\sum}}[-R_{t:H}+f(S_{t})](1-\\omega_{\\bar{G}}(S_{t}))\\underset{u=h}{\\overset{t-1}{\\prod}}\\omega_{\\bar{G}}(S_{u})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Notice that for any $h\\in[H]$ $(s,a)\\in{\\cal S}_{h}\\times{\\cal A}$ , the function of $(s,a)$ we aim to linearly realize can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi,s,a}}{\\mathbb{E}}\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{Traj},h}}{\\mathbb{E}}[R_{h:\\tau-1}+f(S_{\\tau})]=\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi,s,a}}{\\mathbb{E}}\\tilde{g}_{h+1}(S_{h+1})+R_{h:H}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\underset{S_{h+1}\\sim P(s,a)}{\\mathbb{E}}\\tilde{g}_{h+1}(S_{h+1})+q^{\\pi}(s,a)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The second term of the sum, $\\boldsymbol{q}^{\\pi}(s,\\boldsymbol{a})$ is linearly realizable by Assumption 1 with parameter $\\psi_{h}(\\pi)$ The first term needs more work before Lemma C.1 can be applied. To this end, for $h\\in[2:H]$ we define $g_{h}:S_{h}\\to\\mathbb{R}$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\ng_{h}(s)=(1-\\omega_{\\bar{G}}(s))\\mathop{\\mathbb{E}}_{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi,s}}[-R_{h:H}+f(s)-\\tilde{g}_{h+1}(S_{h+1})]\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Notice that ${\\tilde{g}}_{h}$ can be decomposed into a sum of $g_{t}$ functions as for all $h\\in[2:H],s\\in S_{h}.$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{g}_{h}(s)=\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi,s}}{\\mathbb{E}}\\sum_{t=h}^{H}g_{t}(S_{t})\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The benefit of decomposing ${\\tilde{g}}_{h}$ into $g_{t}$ functions is that $g_{t}$ are $\\alpha^{\\prime}$ -admissible under Definition 2 for $\\alpha^{\\prime}\\,=\\,\\alpha/(2H)$ . To see this, note that for any trajectory and $s$ \uff0c $-R_{h:H}+f(s)-\\tilde{g}_{h+1}(S_{h+1})\\,\\in$ $[-2H,2H]$ $\\overset{\\cdot}{g_{t}}(s)$ multiplies this by $1-\\omega_{\\bar{G}}(s)$ which by Eq. (6) is between 0 and 1, and satisfies $1-\\omega_{\\bar{G}}(s)=0$ if range $\\bar{G}(s)\\leq\\alpha/\\sqrt{2d}$ . By Lemma 4.1, range( $'s)\\leq\\sqrt{2d}\\cdot\\mathrm{range}^{\\bar{G}}(s)$ , SO $g_{t}(s)=0$ for any $s$ with range $(s)\\leq\\alpha$ . On any other $s$ $|g_{t}(s)|\\,\\le\\,2H$ . Therefore $g_{t}$ is $\\alpha^{\\prime}$ -admissible. This allows us to use Lemma C.1 to get that for any $h\\in[H{-}1]$ , there exist $\\tilde{\\theta}_{h+1:H}\\in B(8H d_{0}L_{2}/\\alpha)^{H-h}$ such that for any stage $t\\in[h+1:H]$ , for all $(s,a)\\in S_{h}\\times A$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi,s,a}}{\\mathbb{E}}g_{t}(S_{t})-\\Big\\langle\\phi(s,a),\\tilde{\\theta}_{t}\\Big\\rangle\\right|\\le10H d_{0}\\eta/\\alpha\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By combining this with Eq (37), for all $\\textit{b}\\in\\textit{}[H]$ there exists $\\begin{array}{r c l}{\\tilde{\\theta}}&{=}&{\\sum_{t=h+1}^{H}{\\tilde{\\theta}}_{t}}\\end{array}$ with $\\tilde{\\theta}~\\in$ $B(8H^{2}d_{0}L_{2}/\\alpha)$ such that for all $(s,a)\\in{\\cal S}_{h}\\times{\\cal A}$ \uff0c ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|\\underset{S_{h+1}\\sim P(s,a)}{\\mathbb{E}}\\tilde{g}_{h+1}(S_{h+1})-\\left\\langle\\phi(s,a),\\tilde{\\theta}\\right\\rangle\\right|\\leq10H^{2}d_{0}\\eta/\\alpha\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combined with Eq. (36) and the parameter $\\psi_{h}(\\pi)$ from Assumption 1, there exists $\\theta=\\tilde{\\theta}+\\psi_{h}(\\pi)$ With $\\theta\\in B(L_{2}(8\\bar{H}^{2}d_{0}/\\alpha+1))$ such that for all $(s,a)\\in S_{h}\\times A$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|\\underset{\\mathrm{Tr}{\\underline{{\\mathrm{a}}}}\\sim\\mathbb{P}_{\\pi,s,a}}{\\mathbb{E}}\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{Tr}{\\underline{{\\mathrm{a}}}},h}}{\\mathbb{E}}[R_{h:\\tau-1}+f(S_{\\tau})]-\\langle\\phi(s,a),\\theta\\rangle\\right|\\leq\\eta(10H^{2}d_{0}/\\alpha+1)=\\tilde{\\eta}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To finish the proof, we define $\\rho_{h}^{\\pi}(f)=\\theta$ for the arbitrary $h\\in[H],\\pi$ , and $f$ picked above. ", "page_idx": 13}, {"type": "text", "text": "D Results used in Section 5 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Bounding term (I) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We begin by defning an alternative policy $\\pi_{\\bar{G}}^{\\dagger}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi_{\\bar{G}}^{\\dagger}(a|s)=\\pi^{0}(a|s)\\omega_{\\bar{G}}(s)+\\pi^{\\star}(a|s)(1-\\omega_{\\bar{G}}(s))\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This policy can only be worse in value than $\\pi_{\\bar{G}}^{\\star}$ ", "page_idx": 14}, {"type": "text", "text": "Lemma D.1. For all $s\\in S$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nv^{\\pi_{\\bar{G}}^{\\star}}(s)\\geq v^{\\pi_{\\bar{G}}^{\\dagger}}(s)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We prove by induction for $h=H+1,H,\\cdot\\cdot\\cdot,1$ that for all $s\\in S_{h}$ \uff0c $v^{\\pi_{\\tilde{G}}^{\\star}}(s)\\geq v^{\\pi_{\\tilde{G}}^{\\dagger}}(s)$ . The base case of $h=H+1$ is immediately true by definition as $v$ -values are O on $s\\tau$ , regardless of the policy. Assuming the inductive hypothesis holds for $h+1$ , we continue by proving it for $h$ . Let $(s,a)\\in\\ensuremath{\\mathcal{S}}_{h}\\times\\ensuremath{\\mathcal{A}}$ be arbitrary. Notice that ", "page_idx": 14}, {"type": "equation", "text": "$$\nq^{\\pi_{\\bar{G}}^{\\star}}(s,a)-q^{\\pi_{\\bar{G}}^{\\dagger}}(s,a)=\\underset{S^{\\prime}\\sim P(s,a)}{\\mathbb{E}}v^{\\pi_{\\bar{G}}^{\\star}}(S^{\\prime})-v^{\\pi_{\\bar{G}}^{\\dagger}}(S^{\\prime})\\geq0\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the inequality is due to the inductive hypothesis. Next, for any $s\\in S_{h}$ , by the above and the definition of the policies, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{\\pi_{\\overline{{G}}}^{\\star}}(s)=\\omega_{\\bar{G}}(s)\\underset{A\\sim\\pi^{0}(s)}{\\mathbb{E}}~q^{\\pi_{\\overline{{G}}}^{\\star}}(s,A)+(1-\\omega_{\\bar{G}}(s))\\underset{a\\in A}{\\operatorname*{max}}q^{\\pi_{\\overline{{G}}}^{\\star}}(s,a)}\\\\ &{\\qquad\\quad\\geq\\omega_{\\bar{G}}(s)\\underset{A\\sim\\pi^{0}(s)}{\\mathbb{E}}~q^{\\pi_{\\overline{{G}}}^{\\dagger}}(s,A)+(1-\\omega_{\\bar{G}}(s))\\underset{a\\in A}{\\operatorname*{max}}q^{\\pi_{\\overline{{G}}}^{\\dagger}}(s,a)}\\\\ &{\\qquad\\quad\\geq\\omega_{\\bar{G}}(s)\\underset{A\\sim\\pi^{0}(s)}{\\mathbb{E}}q^{\\pi_{\\overline{{G}}}^{\\dagger}}(s,A)+(1-\\omega_{\\bar{G}}(s))\\underset{A\\sim\\pi_{\\overline{{G}}}^{\\star}(s)}{\\mathbb{E}}q^{\\pi_{\\overline{{G}}}^{\\dagger}}(s,A)=v^{\\pi_{\\overline{{G}}}^{\\dagger}}(s)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "finishing the induction. ", "page_idx": 14}, {"type": "text", "text": "Due to Lemma D.1, $\\begin{array}{r}{v^{\\star}(s_{1})-v^{\\pi_{\\bar{G}}^{\\star}}(s_{1})\\leq v^{\\star}(s_{1})-v^{\\pi_{\\bar{G}}^{\\dagger}}(s_{1})}\\end{array}$ . We continue by bounding $v^{\\star}(s_{1})-$ $\\boldsymbol{v}^{\\pi_{\\bar{G}}^{\\dagger}}(s_{1})$ .We first decompose it using the performance diffrence lemma (Lemma J.3) to get that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\boldsymbol{v}^{\\star}(s_{1})-\\boldsymbol{v}^{\\pi_{\\bar{G}}^{\\dagger}}(s_{1})=\\sum_{h=2}^{H}\\underset{(S_{h},A_{h})\\sim\\mathbb{P}_{\\pi^{\\star},s_{1}}^{h}}{\\mathbb{E}}\\left(\\boldsymbol{q}^{\\pi_{\\bar{G}}^{\\dagger}}(S_{h},A_{h})-\\boldsymbol{v}^{\\pi_{\\bar{G}}^{\\dagger}}(S)\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we only have the sum from $h=2$ to $H$ , since for $h=1$ and $h=H+1$ , for any $s\\in S_{h}$ \uff0c $\\omega_{\\bar{G}}(s)\\,=\\,0$ and therefore $\\pi_{\\bar{G}}^{\\dagger}(s)\\,=\\,\\pi^{\\star}(s)$ . By the definition of $\\omega_{\\bar{G}}$ (Eq. (6)), we can see that for $s\\in\\mathcal{S},\\omega_{\\bar{G}}(s)\\neq0$ only when range $\\bar{G}}(s)\\leq2\\alpha/\\sqrt{2d}$ Thus, the policies $\\pi^{\\star}(s)$ and $\\pi_{\\bar{G}}^{\\dagger}(s)$ are equal for all $s\\in S$ that satisfy range $\\bar{G}}(s)\\geq2\\alpha/\\sqrt{2d}$ Making use of this result in Eq. (38), we get that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{h=2}^{H}\\underset{(S_{h},A_{h})\\sim\\mathbb{P}_{\\pi^{k},s_{1}}^{h}}{\\mathbb{E}}\\Big(q^{\\pi_{G}^{\\dagger}}(S_{h},A_{h})-v^{\\pi_{G}^{\\dagger}}(S)\\Big)}\\\\ &{\\displaystyle=\\sum_{h=2}^{H}\\underset{(S_{h},A_{h})\\sim\\mathbb{P}_{\\pi^{k},s_{1}}^{h}}{\\mathbb{E}}\\Big[1\\Big\\{\\mathrm{range}^{\\bar{G}}(S_{h})\\leq2\\alpha/\\sqrt{2d}\\Big\\}\\Big(q^{\\pi_{G}^{\\dagger}}(S_{h},A_{h})-v^{\\pi_{G}^{\\dagger}}(S_{h})\\Big)\\Big]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Eq. (3), we know that ", "page_idx": 14}, {"type": "equation", "text": "$$\nq^{\\pi_{\\bar{G}}^{\\dagger}}(s,a)-v^{\\pi_{\\bar{G}}^{\\dagger}}(s)\\leq\\mathrm{range}(s)+2\\eta\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we can use Lemma 4.1 to get that for all $h\\in[2:H]$ and $(s,a)\\in\\ensuremath{\\mathcal{S}}_{h}\\times\\ensuremath{\\mathcal{A}}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nq^{\\pi_{\\bar{G}}^{\\dagger}}(s,a)-v^{\\pi_{\\bar{G}}^{\\dagger}}(s)\\leq\\sqrt{2d}\\cdot\\mathrm{range}^{\\bar{G}}(s)+2\\eta\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Putting things together we get the following bound. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathrm{I})\\le v^{\\star}(s_{1})-v^{\\pi_{\\widetilde{G}}^{\\star}}(s_{1})}\\\\ &{\\quad\\le v^{\\star}(s_{1})-v^{\\pi_{\\widetilde{G}}^{\\dagger}}(s_{1})}\\\\ &{\\quad=\\displaystyle\\sum_{h=2}^{H}\\displaystyle({\\cal S}_{h})_{\\sim\\mathbb{P}_{\\pi^{\\star},s_{1}}^{h}}\\Big[1\\Big\\{\\mathrm{range}^{\\bar{G}}(S)\\le2\\alpha/\\sqrt{2d}\\Big\\}\\Big(q^{\\pi_{\\widetilde{G}}^{\\dagger}}(S,A)-v^{\\pi_{\\widetilde{G}}^{\\dagger}}(S)\\Big)\\Big]}\\\\ &{\\quad\\le H(2\\alpha+2\\eta)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D.2  Bounding term $(\\mathbf{II})$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To bound $\\boldsymbol{v}^{\\pi}\\bar{\\boldsymbol{G}}\\left(s_{1}\\right)-\\bar{\\boldsymbol{v}}_{\\boldsymbol{\\theta}_{1}^{\\prime}}\\left(s_{1}\\right)$ we decompose it into the following error terms ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{v^{\\pi_{\\widetilde{G}}^{\\star}}(s_{1})-\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})=v^{\\pi_{\\widetilde{G}}^{\\star}}(s_{1})-\\bar{v}_{\\psi_{1}(\\pi_{\\widetilde{G}}^{\\star})}(s_{1})+\\bar{v}_{\\psi_{1}(\\pi_{\\widetilde{G}}^{\\star})}(s_{1})-\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})}}\\\\ {{\\le\\bar{v}_{\\psi_{1}(\\pi_{\\widetilde{G}}^{\\star})}(s_{1})-\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})+\\eta\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inequality holds since we have approximate linear $q^{\\pi}$ -realizability (Assumption 1), which implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\nv^{\\pi_{\\widehat{G}}^{\\star}}(s_{1})-\\bar{v}_{\\psi_{1}(\\pi_{\\widehat{G}}^{\\star})}(s_{1})\\leq\\operatorname*{max}_{a\\in\\mathcal{A}}\\Bigl(q^{\\pi_{\\widehat{G}}^{\\star}}(s_{1},a)-\\bar{q}_{\\psi_{1}(\\pi_{\\widehat{G}}^{\\star})}(s_{1},a)\\Bigr)\\leq\\eta\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To help us bound $\\bar{v}_{\\psi_{1}(\\pi_{\\bar{G}}^{\\star})}(s_{1})-\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})$ in Eq. (39) we make use of two lemmas. The first is the following (proof in Appendix E). ", "page_idx": 15}, {"type": "text", "text": "Lemma D.2. There is an event ${\\mathcal{E}}_{1}$ which occurs with probability at least $1-\\delta/3$ such that under ${\\mathcal{E}}_{1}$ for all $G\\in\\mathbf G$ and $h\\in[H+1]$ it holds that $\\psi_{h}(\\pi_{G}^{\\star})\\in\\Theta_{G,h}$ ", "page_idx": 15}, {"type": "text", "text": "Lemma D.2 tells us that under event ${\\mathcal{E}}_{1}$ \uff0c $\\psi_{1}(\\pi_{\\bar{G}}^{\\star})\\in\\Theta_{\\bar{G},1}$ . The second lemma is the following (proof in Appendix G). ", "page_idx": 15}, {"type": "text", "text": "Lemma D.3 (Feasibility). There is an event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ _which occurs with probability at least $1-\\delta$ such that under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ the true guess $\\bar{G}$ is a feasible solution to Optimization Problem1. ", "page_idx": 15}, {"type": "text", "text": "Notice that since $\\left(G^{\\prime},\\theta_{1:H+1}^{\\prime}\\right)$ is the solution to Optimization Problem 1, it holds that $\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})\\geq$ $\\bar{v}_{\\theta}(s_{1})$ for any $\\theta\\in\\Theta_{G,1}$ where $G$ is a feasible solution to Optimization Problem 1. Thus, we get that under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ , since $\\psi_{1}(\\pi_{\\bar{G}}^{\\star})\\in\\Theta_{\\bar{G},1}$ (by Lemma D.3), and $\\bar{G}$ is a feasible solution to Optimization Problem 1 (by Lemma D.3), it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{v}_{\\psi_{1}(\\pi_{\\bar{G}}^{\\star})}(s_{1})-\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})\\leq\\eta\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which together with Eq. (39), implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{(II)}=v^{\\pi_{\\bar{G}}^{\\star}}(s_{1})-\\bar{v}_{\\theta_{1}^{\\prime}}(s_{1})\\leq\\eta\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D.3 Proof of Lemma 5.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Recall that $\\left(G^{\\prime},\\theta_{1:H+1}^{\\prime}\\right)$ is the solution to Optimization Problem 1. We aim to show that under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}$ ,for any ${\\dot{h}}\\in[2:H+1]$ , it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{\\prime},s_{1}}}{\\mathbb{E}}\\Big[v^{\\pi_{G^{\\prime}}^{\\star}}(S_{h})-v^{\\pi^{\\prime}}(S_{h})\\Big]\\le2(H-h+2)(\\eta+\\tilde{\\epsilon})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To prove Eq. (40) we will use induction. The base case is when $h=H+1$ , which trivially holds, since $v^{\\pi}(s_{\\top})=r_{H+1}(s,a)=0$ for all $\\pi\\in\\Pi,s_{\\intercal}\\in S_{H+1},a\\in\\mathcal{A}$ ", "page_idx": 15}, {"type": "text", "text": "Now, we show the inductive step. Let $h\\in[2:H]$ be arbitrary. Assume that Eq. (40) holds for any $t\\in[h+1:H+1]$ . We prove that Eq. (40) also holds for $h$ . For any $(s,a)\\in(S\\backslash S_{1})\\times A$ , let ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\pi}_{h}(a|s)=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\pi_{G^{\\prime}}^{\\star}(a|s)}&{\\mathrm{if~}\\operatorname{stage}(s)=h}\\\\ {\\pi^{\\prime}(a|s)}&{\\mathrm{if~}\\operatorname{stage}(s)\\neq h\\,.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathrm{Rp\\\"{p}\\in\\mathbb{R}_{h},s_{h}}}{\\underbrace{\\mathbb{E}}}\\left[v^{\\pi_{\\varepsilon^{\\prime}}^{*}}(S_{h})-v^{\\pi^{\\prime}}(S_{h})\\right]}\\\\ &{=\\underset{\\mathrm{Rp\\\"{p}\\in\\mathbb{R}_{h},s_{h}}}{\\underbrace{\\mathbb{E}}}q^{\\pi_{\\varepsilon^{\\prime}}^{*}}(S_{h},A_{h})-\\underset{\\mathrm{Tp\\\"{p}\\in\\mathbb{R}_{s},t_{1}}}{\\underbrace{\\mathbb{E}}}q^{\\pi^{\\prime}}(S_{h},A_{h})}\\\\ &{=\\underset{\\mathrm{Tp\\\"{p}\\in\\mathbb{R}_{h},s_{h}}}{\\underbrace{\\mathbb{E}}}\\left[q^{\\pi_{\\varepsilon^{\\prime}}^{*}}(S_{h},A_{h})-\\bar{q}_{\\theta_{h}^{\\prime}}(S_{h},A_{h})\\right]+\\underset{\\mathrm{Tq\\\"{p}\\in\\mathbb{R}_{h},s_{h}}}{\\underbrace{\\mathbb{E}}}\\bar{q}_{\\theta_{h}^{\\nu}}(S_{h},A_{h})-\\underset{\\mathrm{Tq\\\"{p}\\in\\mathbb{R}_{h},s_{h}}}{\\underbrace{\\mathbb{E}}}\\bar{q}_{\\theta_{h}^{\\nu}}(S_{h},A_{h})}\\\\ &{\\quad+\\underset{\\mathrm{Tq\\\"{p}\\in\\mathbb{R}_{s},t_{1}}}{\\underbrace{\\mathbb{E}}}\\left[\\bar{q}_{\\theta_{h}^{\\nu}}(S_{h},A_{h})-q^{\\pi_{\\varepsilon^{\\prime}}^{*}}(S_{h},A_{h})\\right]+\\underset{\\mathrm{Tq\\\"{p}\\in\\mathbb{R}_{s},t_{1}}}{\\underbrace{\\mathbb{E}}}\\left[q^{\\pi_{\\varepsilon^{\\prime}}^{*}}(S_{h},A_{h})-q^{\\pi^{\\prime}}(S_{h},A_{h})\\right]}\\\\ &{\\le\\underset{\\mathrm{Tp\\\"{p}\\in\\mathbb{R}_{s},s_{h}}}{\\underbrace{\\mathbb{E}}}\\left[\\bar{q}_{\\psi_{h}(\\pi_{\\varepsilon^{\\prime}}^{*})}(S_{h},A_{h})-\\bar{q}_{\\psi_{h}^{\\prime}}(S_{h},A_{h})\\right]+\\underset{\\mathrm{Tq\\\"{p}\\in\\mathbb\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the inequality holds since we have approximate linear $q^{\\pi}$ -realizability (Assumption 1). To bound the first and third error terms above notice that under event ${\\mathcal{E}}_{1}$ , by Lemma D.2, we know that $\\psi_{h}(\\pi_{G^{\\prime}}^{\\star})\\in\\Theta_{G^{\\prime},h}$ We also know that $\\theta_{h}^{\\prime}\\in\\Theta_{G^{\\prime},h}$ ,by definition Let $\\tilde{\\nu}_{u}(s_{u},a_{u})=\\mathbb{P}_{\\tilde{\\pi}_{h},s_{1}}^{u}(s_{u},a_{u})$ and $\\nu_{u}^{\\prime}(s_{u},a_{u})=\\mathbb{P}_{\\pi^{\\prime},s_{1}}^{u}(s_{u},a_{u})$ for all $u\\in[H]$ $(s_{u},a_{u})\\in S_{u}\\times A$ Clearly, $\\tilde{\\nu}=(\\tilde{\\nu}_{u})_{u\\in[H]}$ and $\\nu^{\\prime}=(\\nu_{u}^{\\prime})_{u\\in[H]}$ are admissble distributions, by Definition 1. Notice that we have satisfied all the conditions to make use of Lemma 5.1. Thus, under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}$ it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\underset{\\mathrm{raj}\\sim\\mathbb{P}_{\\bar{n}_{h},s_{1}}}{\\mathbb{E}}\\left[\\bar{q}_{\\psi_{h}(\\pi_{G^{\\prime}}^{\\star})}(S_{h},A_{h})-\\bar{q}_{\\theta_{h}^{\\prime}}(S_{h},A_{h})\\right]=\\underset{(S_{h},A_{h})\\sim\\bar{\\nu}_{h}}{\\mathbb{E}}\\left[\\bar{q}_{\\psi_{h}(\\pi_{G^{\\prime}}^{\\star})}(S_{h},A_{h})-\\bar{q}_{\\theta_{h}^{\\prime}}(S_{h},A_{h})\\right]\\le\\tilde{\\epsilon}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathrm{raj}\\sim\\mathbb{P}_{\\pi^{\\prime},s_{1}}}{\\mathbb{E}}\\Big[\\bar{q}_{\\theta_{h}^{\\prime}}(S_{h},A_{h})-\\bar{q}_{\\psi_{h}(\\pi_{G^{\\prime}}^{*})}(S_{h},A_{h})\\Big]=\\underset{(S_{h},A_{h})\\sim\\nu_{h}^{\\prime}}{\\mathbb{E}}\\Big[\\bar{q}_{\\theta_{h}^{\\prime}}(S_{h},A_{h})-\\bar{q}_{\\psi_{h}(\\pi_{G^{\\prime}}^{*})}(S_{h},A_{h})\\Big]\\leq\\tilde{\\epsilon}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The term $\\mathbb{E}_{\\mathrm{Traj}\\sim\\mathbb{P}_{\\tilde{\\pi}_{h},s_{1}}}\\,\\bar{q}_{\\theta_{h}^{\\prime}}(S_{h},A_{h})-\\mathbb{E}_{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{\\prime},s_{1}}}\\,\\bar{q}_{\\theta_{h}^{\\prime}}(S_{h},A_{h})$ inEq (41) canbe boundedbyrecalling the definition of $\\pi^{\\prime}(s)$ (line 3), to get that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\tilde{\\pi}_{h},s_{1}}}{\\mathbb{E}}\\bar{q}_{\\theta_{h}^{\\prime}}(S_{h},A_{h})\\le\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\,\\bar{q}_{\\theta_{h}^{\\prime}}(S_{h},a)=\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{\\prime},s_{1}}}{\\mathbb{E}}\\bar{q}_{\\theta_{h}^{\\prime}}(S_{h},A_{h})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}$ , after plugging the above bounds into Eq. (41), we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{\\prime},s_{1}}}{\\mathbb{E}}\\bigg[v^{\\pi_{G^{\\prime}}^{*}}(S_{h})-v^{\\pi^{\\prime}}(S_{h})\\bigg]\\leq\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{\\prime},s_{1}}}{\\mathbb{E}}\\Big[q^{\\pi_{G^{\\prime}}^{*}}(S_{h},A_{h})-q^{\\pi^{\\prime}}(S_{h},A_{h})\\Big]+2\\eta+2\\tilde{\\epsilon}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{\\prime},s_{1}}}{\\mathbb{E}}\\Big[v^{\\pi_{G^{\\prime}}^{*}}(S_{h+1})-v^{\\pi^{\\prime}}(S_{h+1})\\Big]+2\\eta+2\\tilde{\\epsilon}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le2(H-h+2)(\\eta+\\tilde{\\epsilon})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality holds by the inductive hypothesis for $h\\!+\\!1$ (Eq. (40)), completing the proof of the claim. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "E Proof of Lemma D.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. We will prove the claim using induction. The base case is when $h\\,=\\,H\\,+\\,1$ for which $\\psi_{H+1}(\\pi)\\;=\\;\\vec{0}$ for all $\\pi\\,\\in\\,\\Pi$ by definition. Thus, for all $G\\ \\in\\textbf{G}$ , it holds that $\\psi_{H+1}(\\pi_{G}^{\\star})\\;\\in$ $\\Theta_{G,H+1}=\\{\\vec{0}\\}$ ", "page_idx": 17}, {"type": "text", "text": "Now, we show the inductive step. Let $\\textit{h}\\in[H]$ be arbitrary. Assume Lemma D.2 holds for any $t\\in[h+1:H+1]$ . We prove that it also holds for $h$ .Define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\psi}_{h}(\\pi_{G}^{\\star})=X_{h}^{-1}\\sum_{j\\in[n]}\\phi_{h}^{j}\\,\\mathbb{E}_{\\tau\\sim F_{G,h+1}^{j}}\\Bigl[r_{h:\\tau-1}^{j}+\\bar{v}_{\\psi_{h+1:H+1}(\\pi_{G}^{\\star})}\\bigl(s_{\\tau}^{j}\\bigr)\\Bigr]\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the inductive hypothesis we know that for any $G\\in\\mathbf{G}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\psi_{h+1:H+1}(\\pi_{G}^{\\star})\\in\\Theta_{G,h+1}\\times\\cdots\\times\\Theta_{G,H+1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, $\\hat{\\psi}_{h}(\\pi_{G}^{\\star})\\in\\hat{\\Theta}_{G,h}$ It is lfto how that $\\left\\|\\hat{\\psi}_{h}(\\pi_{G}^{\\star})-\\psi_{h}(\\pi_{G}^{\\star})\\right\\|_{X_{h}}\\le\\beta$ which together with the fact that $\\psi_{h}(\\pi_{G}^{\\star})\\in B(L_{2})$ implies the desired result, $\\psi_{h}(\\pi_{G}^{\\star})\\in\\Theta_{G,h}$ ", "page_idx": 17}, {"type": "text", "text": "We would like to make use of Lemma H.1 to bound $\\left\\|\\hat{\\psi}_{h}(\\pi_{G}^{\\star})-\\psi_{h}(\\pi_{G}^{\\star})\\right\\|_{X_{h}}$ . To do so, we map the terms used in the Lemma H.1 to our terms as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota=n,\\lambda=\\lambda,\\theta_{\\star}=\\psi_{h}(\\pi_{G}^{*}),V=X_{h},\\hat{\\theta}=\\hat{\\psi}_{h}(\\pi_{G}^{*}),A=\\left(\\phi_{h}^{j}\\right)_{j\\in[n]},}\\\\ &{\\gamma=\\tilde{Y}+\\Delta=\\left(\\left\\langle\\phi_{h}^{j},\\psi_{h}(\\pi_{G}^{*})\\right\\rangle\\right)_{j\\in[n]}+\\gamma+\\Delta=\\left(\\frac{\\mathbb{E}}{\\tau\\kappa_{b,j+1}^{2}}\\Big[r_{h:t=1}^{j}+\\bar{v}_{\\psi_{h+1,(H+1)}(\\pi_{G}^{*})}\\big(s_{\\tau}^{j}\\big)\\Big]\\right)_{j\\in[n]},}\\\\ &{\\quad=\\left(\\frac{\\mathbb{E}}{\\tau\\kappa_{b,j+1}^{2}}\\Big[r_{h:t=1}^{j}+\\bar{v}_{\\psi_{h+1,H+1}(\\pi_{G}^{*})}\\big(s_{\\tau}^{j}\\big)\\Big]-\\frac{\\mathbb{E}}{\\tau\\kappa_{b,j}^{2}\\kappa_{e,s,\\iota_{h}^{*},e}}\\frac{\\mathbb{E}}{\\tau\\kappa_{e,m_{b+1}}^{2}\\tau_{e}\\sqrt{F_{E_{h},(H_{1},\\pi_{G}^{*})}}}\\Big[R_{h:t=1}+\\bar{v}_{\\psi_{h+1,H+1}(\\pi_{G}^{*})}\\big(s_{\\tau}^{j}\\big)\\Big]\\right)_{j\\in[n]}}\\\\ &{\\bar{\\lambda}=\\left(\\frac{\\mathbb{E}}{\\tau\\kappa_{b}\\kappa_{e,\\eta,e}^{*}}\\frac{\\mathbb{E}}{\\kappa_{e,\\eta,e}^{*}\\tau_{e}\\sqrt{F_{e,T,\\eta}}\\kappa_{b,\\iota_{b}+1}}\\Big[R_{h:\\tau-1}+\\bar{v}_{\\psi_{h+1,H+1}(\\pi_{G}^{*})}(S_{\\tau})\\Big]-\\left\\langle\\phi_{h}^{j},\\psi_{h}(\\pi_{G}^{*})\\right\\rangle\\right)_{j\\in[n]},}\\\\ &{\\quad=\\sum_{j\\in[n]}\\phi_{h}^{j}\\Bigg(\\frac{\\mathbb{E}}{\\tau\\kappa_{b}\\kappa_{e,\\eta,e}^{*}}\\Big[r_{h:t=1}^{j}+\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With the definitions as above, applying Lemma H.1 we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{\\psi}_{h}(\\pi_{G}^{\\star})-\\psi_{h}(\\pi_{G}^{\\star})\\right\\|_{X_{h}}\\le\\sqrt{\\lambda}\\|\\psi_{h}(\\pi_{G}^{\\star})\\|_{2}+\\|\\Delta\\|_{\\infty}\\sqrt{n}+\\|\\iota\\|_{X_{h}^{-1}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first term in Eq. (42) can be bounded by $H^{3/2}d$ by recalling that $\\sqrt{\\lambda}=H^{3/2}d/\\tilde{L_{2}}$ (Eq. (25)) and $\\|\\psi_{h}(\\pi_{G}^{\\star})\\|_{2}\\leq L_{2}\\leq\\tilde{L}_{2}$ ", "page_idx": 17}, {"type": "text", "text": "The $\\|\\Delta\\|_{\\infty}$ in the second term in Eq. (42) can be bounded by first decomposing the error, and using a triangle inequality as follows. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\|_{\\infty}\\leq\\left\\|\\left(\\underset{\\mathrm{Traje}\\,\\forall_{\\sigma^{0},s_{h}^{j}},a_{h}^{j}}{\\mathbb{E}}\\frac{\\mathbb{E}}{\\tau\\sim F_{G,\\mathrm{rnj},h+1}}\\bigg[\\bar{v}_{\\psi_{h+1:H+1}(\\pi_{G}^{*})}(S_{\\tau})-\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}q^{\\pi_{G}^{*}}(S_{\\tau},a^{\\prime})\\bigg]\\right)_{j\\in[n]}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad+\\left\\|\\left(\\underset{\\mathrm{Traje}\\,\\forall_{\\sigma^{0},s_{h}^{j}},a_{h}^{j}}{\\mathbb{E}}\\frac{\\mathbb{E}}{\\tau\\sim F_{G,\\mathrm{rnj},h+1}}\\bigg[R_{h:T-1}+\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}q^{\\pi_{G}^{*}}(S_{\\tau},a^{\\prime})\\bigg]-\\Big\\langle\\phi_{h}^{j},\\psi_{h}(\\pi_{G}^{*})\\Big\\rangle\\right)_{j\\in[n]}\\right\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the first term in Eq. (43), notice that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{\\gamma}_{\\psi_{h+1:H+1}(\\pi_{G}^{\\star})}(S_{\\tau})-\\operatorname*{max}_{a^{\\prime}\\in A}q^{\\pi_{G}^{\\star}}(S_{\\tau},a^{\\prime})\\le\\operatorname*{max}_{a^{\\prime}\\in A}\\Bigl(\\bigl\\langle\\phi(S_{\\tau},a^{\\prime}),\\psi_{\\mathrm{stage}(S_{\\tau})}(\\pi_{G}^{\\star})\\bigr\\rangle-q^{\\pi_{G}^{\\star}}(S_{\\tau},a^{\\prime})\\Bigr)\\le\\eta\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality holds since we have approximate linear $q^{\\pi}$ -realizability (Assumption 1). To bound the second term in Eq. (43) notice that by the definition of $\\pi_{G}^{\\star}$ (Eq. (15)), arg $\\operatorname*{max}_{a^{\\prime}\\in A}q^{\\pi_{G}^{\\star}}(S_{\\tau},a^{\\prime})$ is exactly the action $\\pi_{G}^{\\star}$ would take at the stopping stage $\\tau$ , and that the distribution of Traj under policy $\\pi^{0}$ until stopping stage $\\tau$ is same as the distribution of Traj under policy $\\pi_{G}^{\\star}$ until stopping stage $\\tau$ . This implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left(\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{0},s_{h}^{j},a_{h}^{j}}}{\\mathbb{E}}\\underset{\\tau\\sim F_{G,\\mathrm{raj},h+1}}{\\mathbb{E}}\\left[R_{h:\\tau-1}+\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}q^{\\pi_{G}^{*}}(S_{\\tau},a^{\\prime})\\right]-\\left<\\phi_{h}^{j},\\psi_{h}(\\pi_{G}^{\\star})\\right>\\right)_{j\\in[n]}\\right\\|_{\\infty}}\\\\ &{=\\left\\|\\left(q^{\\pi_{G}^{*}}(s_{h}^{j},a_{h}^{j})-\\left<\\phi_{h}^{j},\\psi_{h}(\\pi_{G}^{\\star})\\right>\\right)_{j\\in[n]}\\right\\|_{\\infty}\\le\\eta\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality holds since we have approximate linear $q^{\\pi}$ -realizability (Assumption 1). Plugging the above bounds into Eq. (43), we get that $\\|\\Delta\\|_{\\infty}\\leq2\\eta$ ", "page_idx": 18}, {"type": "text", "text": "To bound the third term in Eq. (42), let the event ${\\mathcal{E}}_{1}$ be as defined in the proof of Lemma H.2, which occurs with probability at least $1\\,-\\,\\delta/3$ . Then, under event ${\\mathcal{E}}_{1}$ , the third term in Eq. (42) can be bounded by $\\bar{\\beta}$ (Eq. (31), by applying Lemma H.2 since $\\psi_{h+1:H+1}(\\pi_{G}^{\\star})\\in\\mathcal{B}(L_{2})^{H-\\bar{h}+1}\\subset$ B(L2)H-h+1. ", "page_idx": 18}, {"type": "text", "text": "Plugging the three bounds above back into Eq. (42) we get that, under event ${\\mathcal{E}}_{1}$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{\\psi}_{h}(\\pi_{G}^{\\star})-\\psi_{h}(\\pi_{G}^{\\star})\\right\\|_{X_{h}}\\le H^{3/2}d+2\\eta\\sqrt{n}+\\bar{\\beta}\\le\\beta\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\beta$ is defined in Eq. (32), and the last inequality can be seen to hold by plugging in parameter values according to Appendix A. Thus, under event ${\\mathcal{E}}_{1}$ , which occurs with probability at least $1\\!-\\!\\delta/3$ for any $G\\in\\mathbf G$ and $h\\in[H]$ it holds that $\\psi_{h}(\\pi_{G}^{\\star})\\in\\Theta_{G,h}$ , which completes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "F Proof of Lemma 5.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Let $G\\in\\bf G$ be a feasible solution to Optimization Problem 1. By Lemma I.1, there is an event $\\mathcal{E}_{2}$ , that occurs with probability at least $1-\\delta/3$ , such that under event $\\mathcal{E}_{2}$ , for all $G\\in\\mathbf G$ , and for all $h\\in[H]$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{(S,A)\\sim\\mu_{h}}\\left[\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(S,A)-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(S,A)\\right]-\\displaystyle\\frac{1}{n}\\sum_{i\\in[n]}\\left(\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s_{h}^{i},a_{h}^{i})-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s_{h}^{i},a_{h}^{i})\\right)\\right]}\\\\ &{\\leq\\displaystyle\\frac{H}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|C_{\\xi}^{\\mathbf{G}}|}{\\delta}\\right)}+2L_{\\xi}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Wwhere $|C_{\\xi}^{\\mathbf{G}}|,L_{\\xi}$ , are defined in Eqs. (33) and (34). Let $h\\,\\in\\,[H]$ .Recall that since $G$ is a feasible solution to Optimization Problem 1 we know that Eq. (14) passed for $h$ .Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i\\in[n]}\\left(\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s_{h}^{i},a_{h}^{i})-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s_{h}^{i},a_{h}^{i})\\right)\\leq\\bar{\\epsilon}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the above two results, we have that under event $\\mathcal{E}_{2}$ ,for any $h\\in[H]$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}\\bigg[\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(S,A)-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(S,A)\\bigg]\\leq\\frac{H}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|C_{\\xi}^{\\mathbf{G}}|}{\\delta}\\right)}+2L_{\\xi}+\\bar{\\epsilon}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now we will relate the data collecting distribution $\\mu$ to any admissible distribution $\\nu$ . Notice that by the definition of max and min, for any $(s,a)\\in S\\times A$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s,a)-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s,a)\\geq0\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we can apply Lemma G.4 to get that under event $\\mathcal{E}_{2}$ , for all $h\\in[H]$ , and admissible distribution $\\nu=(\\nu_{t})_{t\\in[H]}$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\underset{(S,A)\\sim\\nu_{h}}{\\mathbb{E}}\\left[\\operatorname*{max}_{\\theta\\in\\Theta_{\\mathcal{L},h}}\\bar{q}_{\\theta}(S,A)-\\operatorname*{min}_{\\theta\\in\\Theta_{\\mathcal{L},h}}\\bar{q}_{\\theta}(S,A)\\right]\\leq C_{\\mathrm{conc}}\\left(\\frac{H}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|C_{\\xi}^{\\mathbf{G}}|}{\\delta}\\right)}+2L_{\\xi}+\\bar{\\epsilon}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To conclude, we have that under event $\\mathcal{E}_{2}$ , for any $G\\in\\mathbf{G}$ that is a feasible solution to Optimization Problem 1, for any $h\\in[H]$ , for any $\\left(\\theta_{s,a}\\right)_{(s,a)\\in S_{h}\\times\\mathcal{A}}$ and $(\\check{\\theta}_{s,a})_{(s,a)\\in S_{h}\\times A}$ with $\\theta_{s,a},\\bar{\\theta_{s,a}}\\in\\Theta_{G,h}$ for all $(s,a)\\in\\ensuremath{\\mathcal{S}}_{h}\\times\\ensuremath{\\mathcal{A}}$ and for any admissible distribution $\\nu=(\\nu_{t})_{t\\in[H]}$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{(S,A)\\sim w_{k}}{\\mathbb{E}}\\bigg[\\bar{\\wp}_{\\sigma,\\mathbf{x}}(S,A)-\\bar{\\wp}_{\\delta,\\mathbf{x},A}(S,A)\\bigg]}\\\\ &{\\leq\\underset{(S,A)\\sim w_{k}}{\\mathbb{E}}\\bigg[\\frac{\\mathrm{form}_{S,\\mathbf{x}}}{\\mathrm{for}\\big[\\log_{S,\\mathbf{x}}\\big(\\mathcal{S},A\\big)-\\sigma_{\\mathrm{ept},\\mathbf{x}}^{\\mathsf{m i n}}\\,\\bar{\\psi}(S,A)\\big]}}\\\\ &{\\leq C\\cos\\bigg(\\frac{H}{\\sqrt{n}}\\bigg\\langle\\log\\bigg(\\frac{6H|C|C|^{\\alpha}|}{\\delta}\\bigg)+2L_{\\ell}+\\epsilon\\bigg\\rangle}\\\\ &{\\leq C\\cos\\bigg(\\frac{H}{\\sqrt{n}}\\bigg\\langle\\log\\bigg(\\frac{6H(1+2L_{2}/\\xi)\\big)^{\\alpha\\beta}}{\\delta}\\bigg)+24\\sqrt{2}d H^{2}L_{1}\\xi\\alpha^{-1}\\bigg(2\\sqrt{n}L_{1}\\bar{L}_{2}/(H^{3/2}(d)^{\\alpha})\\bigg)^{\\mu}+\\bar{\\alpha}}\\\\ &{\\leq C\\cos\\bigg(\\frac{H}{\\sqrt{n}}\\bigg\\langle\\bar{H}H^{2}d_{0}\\log\\bigg(1+96\\sqrt{2}d H^{2}L_{1}L_{2}\\alpha^{-1}\\sqrt{n}L_{1}\\bar{L}_{2}/(H^{3/2}(d))+\\log\\bigg(\\frac{6H}{\\delta}\\bigg)+\\frac{1}{\\sqrt{n}}+}\\\\ &{=\\xi\\bigg)\\bigg(\\frac{C\\cos H^{2}d}{\\sqrt{n}}+\\frac{C\\cos}{\\sqrt{n}}+\\frac{C\\cos}{\\sqrt{n}}\\sqrt{n}L_{1}^{\\beta/2}d^{2}\\bigg)=\\bigg(\\frac{C\\cos\\beta}{\\sqrt{n}}\\sqrt{\\frac{\\delta}{\\pi}}L_{1}^{\\beta/2}d^{2}\\bigg).}\\\\ &{\\quad\\times\\quad\\mathrm{for~}\\quad(\\frac{L}{\\sqrt{n}})=-\\xi\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The third inequality holds by plugging in the values of $|C_{\\xi}^{\\mathbf{G}}|,L_{\\xi}$ , as defined in Eqs. (33) and (34). The last inequality holds by setting $\\xi^{-1}=24\\sqrt{2d}\\sqrt{n}H^{2}\\dot{L_{1}}\\alpha^{-1}\\Bigl(2\\sqrt{n}L_{1}\\tilde{L_{2}}/(H^{3/2}d)\\Bigr)^{H}$ . The last two equalities hold by plugging in parameter values according to Appendix A. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "G Proof of Lemma D.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. To show that G is a feasible solution we need to show that Eq. (14) is satisfied for all $h\\in[H]$ By Lemma I.1, there is an event $\\mathcal{E}_{2}$ , that occurs with probability at least $1-\\delta/3$ , such that under event $\\mathcal{E}_{2}$ , for all $h\\in[H]$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\frac{1}{n}\\displaystyle\\sum_{i\\in[n]}\\left(\\operatorname*{max}_{\\theta\\in\\Theta_{\\tilde{\\mathcal{L}},h}}\\bar{q}_{\\theta}(s_{h}^{i},a_{h}^{i})-\\operatorname*{min}_{\\theta\\in\\Theta_{\\tilde{\\mathcal{L}},h}}\\bar{q}_{\\theta}(s_{h}^{i},a_{h}^{i})\\right)-\\displaystyle\\mathbb{E}_{(S,A)\\sim\\mu_{h}}\\left[\\operatorname*{max}_{\\theta\\in\\Theta_{\\tilde{\\mathcal{L}},h}}\\bar{q}_{\\theta}(S,A)-\\operatorname*{min}_{\\theta\\in\\Theta_{\\tilde{\\mathcal{L}},h}}\\bar{q}_{\\theta}(S,A)\\right]}\\\\ &{\\leq\\displaystyle\\frac{H}{\\sqrt{n}}\\sqrt{\\log{\\left(\\frac{6H|C_{\\xi}^{\\mathbf{G}}|}{\\delta}\\right)}}+2L_{\\xi}\\cdot}&{(45)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $|C_{\\xi}^{\\mathbf{G}}|,L_{\\xi}$ , are defined in Eqs. (33) and (34). Let $h\\,\\in\\,[H]$ be arbitrary for the remainder of the prof. We focus on bounding $\\begin{array}{r}{\\mathbb{E}_{(S,A)\\sim\\mu_{h}}\\Bigl[\\operatorname*{max}_{\\theta\\in\\Theta_{\\bar{G},h}}\\bar{q}_{\\theta}(S,A)-\\operatorname*{min}_{\\theta\\in\\Theta_{\\bar{G},h}}\\bar{q}_{\\theta}(S,A)\\Bigr]}\\end{array}$ forthe remainder of the proof. The following lemma will be helpful (proof in Appendix G.1). ", "page_idx": 20}, {"type": "text", "text": "Lemma G.1. There is an event ${\\mathcal{E}}_{1}$ , that occurs with probability at least $1-\\delta/3$ ,such that under event ${\\mathcal{E}}_{1}$ for any $h\\in[H],(s,a)\\in S_{h}\\times\\mathcal{A},\\theta_{h}\\in\\Theta_{\\bar{G},h}$ ,it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\bar{q}_{\\theta_{h}}(s,a)-q^{\\pi_{\\bar{G}}^{*}}(s,a)\\right|\\leq2\\beta\\operatorname{\\mathbb{E}}_{\\mathrm{Taj}\\sim\\mathbb{P}_{\\pi,s,a}}\\sum_{t=h}^{H}\\operatorname*{min}\\Bigl\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\Bigr\\}+(H-h+1)\\tilde{\\eta}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where, for any $(s^{\\prime},a^{\\prime})\\in\\mathcal{S}\\times\\mathcal{A}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\pi}(a^{\\prime}|s^{\\prime})=\\pi^{0}(a^{\\prime}|s^{\\prime})\\omega_{\\bar{G}}(s^{\\prime})+\\mathbb{1}\\!\\left\\{\\underset{a^{\\prime\\prime}\\in\\mathcal{A}}{\\operatorname{arg\\,max}}g^{\\bar{\\pi}}(s^{\\prime},a^{\\prime\\prime})=a^{\\prime}\\right\\}\\!(1-\\omega_{\\bar{G}}(s^{\\prime}))\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $g^{\\bar{\\pi}}$ is a state-action value function of policy $\\bar{\\pi}$ (similar to $q^{\\bar{\\pi}}$ ), except in the alternative MDP that has the same state and action spaces, and transition distributions as the original MDP under consideration,but with $a$ reward function modified as follows.For all (s\",a) S \u00d7 A, thereward in thi aterative MDP is determinsticlly minI1, o(s, a)lx$\\Big(i.e.\\ \\mathcal{R}(s^{\\prime},a^{\\prime})=\\mathbb{1}\\Big\\{\\operatorname*{min}\\Bigl\\{1,\\|\\phi(s^{\\prime},a^{\\prime})\\|_{X_{h}^{-1}}\\Bigr\\}\\Big\\}\\Big)$ . In particular for any $h^{\\prime}\\in[H],(s^{\\prime},a^{\\prime})\\in S_{h^{\\prime}}\\times\\mathcal{A}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\ng^{\\bar{\\pi}}(s^{\\prime},a^{\\prime})=\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\bar{\\pi},s^{\\prime},a^{\\prime}}}{\\mathbb{E}}\\sum_{t=h^{\\prime}}^{H}\\operatorname*{min}\\Bigl\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\Bigr\\}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The recursive definition of $\\bar{\\pi}$ can be interpreted in the same way as described below Eq. (15). ", "page_idx": 20}, {"type": "text", "text": "Let $\\bar{\\pi}$ be as defined in Lemma G.1. Then, by Lemma G.1, under event ${\\mathcal{E}}_{1}$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{s_{h},A_{h})\\sim\\mu_{h}}{\\mathbb{E}}\\bigg[\\underset{\\theta\\in\\Theta_{\\hat{\\pi}_{h}}}{\\operatorname*{max}}\\bar{\\pi}_{\\theta}(S_{h},A_{h})-\\underset{\\theta\\in\\Theta_{\\hat{\\pi}_{h}}}{\\operatorname*{min}}\\bar{q}_{\\theta}(S_{h},A_{h})\\bigg]}\\\\ &{=\\underset{(S_{h},A_{h})\\sim\\mu_{h}}{\\mathbb{E}}\\bigg[\\underset{\\theta\\in\\Theta_{\\hat{\\pi}_{h}}}{\\operatorname*{max}}\\frac{\\bar{q}_{\\theta}(S_{h},A_{h})-v^{\\pi}\\hat{\\alpha}(S_{h})+v^{\\pi}\\hat{\\alpha}(S_{h})-\\underset{\\theta\\in\\Theta_{\\hat{\\pi}_{h}}}{\\operatorname*{min}}}\\bar{q}_{\\theta}(S_{h},A_{h})\\bigg]}\\\\ &{\\leq\\underset{(S_{h},A_{h})\\sim\\mu_{h}}{\\mathbb{E}}\\bigg[4\\beta\\underset{\\operatorname*{min}}{\\mathbb{E}}\\underset{\\eta\\in\\mathcal{S}_{\\pi},S_{h},A_{h}}{\\mathbb{E}}\\underset{t=h}{\\operatorname*{min}}\\Big\\{1,\\|\\phi(S_{t},A_{t})||_{X_{t}^{-1}}\\Big\\}+2(H-h+1)\\tilde{\\eta}\\bigg]}\\\\ &{=4\\beta\\underset{(S_{h},A_{h})\\sim\\mu_{h}}{\\mathbb{E}}\\underset{(S_{h},A_{h})\\sim\\mu_{h}}{\\operatorname*{min}}\\bigg\\{1,|\\phi(S_{h},A_{h})||_{X_{t}^{-1}}\\Big\\}+4\\beta\\underset{t=h+1}{\\overset{H}{\\prod}}\\underset{(S_{h},A_{h})\\sim\\mu_{h}}{\\mathbb{E}}\\underset{(S_{t},A_{t})\\sim\\mathbb{P}_{\\hat{\\pi}_{s}^{t},S_{h},A_{h}}}{\\mathbb{E}}\\quad\\underset{(49)}{\\operatorname*{min}}\\bigg\\{1,|\\phi(S_{t},\\cdot_{1}}\\\\ &{\\quad+(H-h+1)\\tilde{\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The inequality used Lemma G.1. As we will show shortly, the first term can be bounded by Lemma G.3, since its expectation is taken w.r.t. the data collecting distribution $\\mu$ \uff1aThus, the approach we take to bounding the second term is to relate its nested expectations to just be a single ", "page_idx": 20}, {"type": "text", "text": "expectation taken w.r.t. the distribution $\\mu$ (similar to the first term, which we claim we know how to bound). To this end, for any $(s^{\\prime},a^{\\prime})\\in\\dot{S}\\times\\mathcal{A}$ , define the policy $\\check{\\pi}_{h}$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\check{\\pi}_{h}(a^{\\prime}|s^{\\prime})=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\pi^{0}(a^{\\prime}|s^{\\prime})}&{\\mathrm{if~}\\operatorname{stage}(s^{\\prime})\\leq h\\smallskip}\\\\ {\\bar{\\pi}(a^{\\prime}|s^{\\prime})}&{\\mathrm{if~}\\operatorname{stage}(s^{\\prime})>h\\,.}\\end{array}\\ \\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice that for any t E [H],u E [t+ 1 : H +1], E(Sst,At)\\~\u03bc E(Su,A)\\~P,st,Ag =E(Su,Au)\\~P%t\uff0cs&\\* Let $\\check{\\nu}_{u}(s_{u},a_{u})=\\mathbb{P}_{\\check{\\pi}_{h},s_{1}}^{u}(s_{u},a_{u})$ for all $u\\in[H]$ $(s_{u},a_{u})\\in S_{u}\\times A$ Clearly, $\\check{\\nu}=(\\check{\\nu}_{u})_{u\\in[H]}$ is an admissible distribution by Definition 1. Thus, with the definition of $\\check{\\nu}$ and using Lemma G.4, we get that Eq. (49) is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle=4\\beta\\underset{(S_{h},A_{h})\\sim\\mu_{h}}{\\mathbb{E}}\\operatorname*{min}\\biggr\\{1,\\|\\phi(S_{h},A_{h})\\|_{X_{h}^{-1}}\\biggr\\}+4\\beta\\underset{t=h+1}{\\overset{H}{\\sum}}\\underset{(S_{t},A_{t})\\sim\\tilde{\\nu}_{h}}{\\mathbb{E}}\\operatorname*{min}\\biggr\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\biggr\\}+(H\\int_{0}^{1}\\mu_{h}^{\\top}\\mu_{h}^{\\top}\\mu_{h}^{\\top}\\mu_{h}^{\\top})}\\\\ {\\displaystyle\\leq4\\beta\\underset{(S_{h},A_{h})\\sim\\mu_{h}}{\\mathbb{E}}\\operatorname*{min}\\biggr\\{1,\\|\\phi(S_{h},A_{h})\\|_{X_{h}^{-1}}\\biggr\\}+4C_{\\mathrm{conc}}\\beta\\underset{t=h+1}{\\overset{H}{\\sum}}(\\underset{S_{t},A_{t})\\sim\\mu_{t}}{\\mathbb{E}}\\operatorname*{min}\\biggr\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\biggr\\}+}\\\\ {\\displaystyle\\leq4C_{\\mathrm{conc}}\\beta\\underset{t=h}{\\overset{H}{\\sum}}(s_{t,A_{t})\\sim\\mu_{t}}\\operatorname*{min}\\biggr\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\biggr\\}+(H-h+1)\\tilde{\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The last inequality used that $C_{\\mathrm{conc}}~~\\geq~~1$ .Finally, we can apply Lemma G.3 to bound $\\mathbb{E}_{(S_{t},A_{t})\\sim\\mu_{t}}\\operatorname*{min}\\Bigl\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\Bigr\\}$ In priculr, let $\\mathcal{E}_{3}$ be as defined in the proo o Lemma G.3. Then, by Lemma G.3, under event ${\\mathcal{E}}_{3}$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n4C_{\\mathrm{conc}}\\beta\\sum_{t=h}^{H}\\underset{(S_{t},A_{t})\\sim\\mu_{t}}{\\mathbb{E}}\\operatorname*{min}\\biggr\\{1,\\Vert\\phi(S_{t},A_{t})\\Vert_{X_{t}^{-1}}\\biggr\\}\\leq4C_{\\mathrm{conc}}\\beta\\sum_{t=h}^{H}\\tilde{\\epsilon}\\leq4H C_{\\mathrm{conc}}\\tilde{\\epsilon}\\beta\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Putting all of the bound after Eq. (49) together and plugging them into Eq. (49), we have that, under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{(S_{h},A_{h})\\sim\\mu_{h}}\\left[\\operatorname*{max}_{\\theta\\in\\Theta_{\\mathcal{Q},h}}\\bar{q}_{\\theta}(S_{h},A_{h})-\\operatorname*{min}_{\\theta\\in\\Theta_{\\mathcal{Q},h}}\\bar{q}_{\\theta}(S_{h},A_{h})\\right]\\le(H-h+1)\\tilde{\\eta}+4H C_{\\mathrm{conc}}\\tilde{\\epsilon}\\beta\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We are now ready to state the final result. Noting that $h\\in[H]$ was arbitrary, by combining Eqs. (45) and (50), we have that, under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ , for all $\\bar{h^{\\ast}}\\bar{\\in[}H]$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\displaystyle\\sum_{i\\in[n]}\\left(\\operatorname*{max}_{\\phi\\in\\Theta_{n,i}}\\bar{\\phi}(\\omega_{h}^{i},a_{h}^{i})-\\operatorname*{min}_{\\theta\\in\\Theta_{n,i}}\\bar{\\phi}(\\omega_{h}^{i},a_{h}^{i})\\right)}\\\\ &{\\leq\\displaystyle\\frac{H}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|C_{\\phi}^{\\top}|}{{\\delta}}\\right)+2L_{\\ell}+(H-h+1)\\bar{\\eta}+4H C_{\\mathrm{coon}}\\varepsilon\\beta}}\\\\ &{=\\displaystyle\\frac{H}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H(1+2L_{2}/\\xi))^{(H+6)}}{{\\delta}}}+24\\sqrt{2d}H^{2}L_{1}\\xi\\alpha^{-1}\\left(2\\sqrt{n}L_{1}\\bar{L}_{2}/(H^{3/2}\\bar{\\alpha})\\right)^{n}}\\\\ &{\\quad\\quad+(H-h+1)\\bar{\\eta}+4H C_{\\mathrm{coon}}\\varepsilon\\beta}\\\\ &{\\leq\\displaystyle\\frac{H}{\\sqrt{n}}\\sqrt{H^{2}d_{0}\\log\\left(1+96\\sqrt{n}\\sqrt{2}d\\bar{H}^{2}L_{1}L_{2}\\alpha^{-1}\\sqrt{n}L_{1}\\bar{L}_{2}/(H^{3/2}\\bar{\\alpha})\\right)+\\log\\left(\\frac{6H}{\\delta}\\right)}+\\frac{1}{\\sqrt{n}}}\\\\ &{\\quad\\quad+(H-h+1)\\bar{\\eta}+4H C_{\\mathrm{coo}}\\varepsilon\\beta}\\\\ &{\\quad\\quad+(H\\left(\\frac{H^{2}}{H^{2}}+\\frac{1}{\\sqrt{n}}+\\frac{H^{3/2}\\bar{\\alpha}^{2}}{\\sqrt{n}}+\\frac{C_{\\mathrm{con}}H^{3/2}\\bar{\\alpha}^{2}}{\\sqrt{n}}\\right)=\\mathcal{O}\\left(\\frac{C_{\\mathrm{coo}}H^{5/2}\\bar{\\alpha}^{2}}{\\sqrt{n}}\\right),\\qquad\\tau\\in\\mathcal{O}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first equality holds by plugging in the values of $|C_{\\xi}^{\\mathbf{G}}|,L_{\\xi}$ , as defined in Eqs. (33) and (34). The second equality holds by setting $\\xi^{-1}=24\\sqrt{n}\\sqrt{2d}H^{2}\\dot{L_{1}}\\alpha^{-1}\\Bigl(2\\sqrt{n}L_{1}\\tilde{L_{2}}/(H^{3/2}d)\\Bigr)^{H}$ .The last two equalities hold by plugging in parameter values according to Appendix A. ", "page_idx": 21}, {"type": "text", "text": "Noticing that this is exactly the condition (Eq. (14)) in Optimization Problem 1 that needs to be satisfied by any feasible solution, we conclude that, under event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ ,the true guess $\\bar{G}$ is a feasible solution to Optimization Problem 1. ", "page_idx": 22}, {"type": "text", "text": "G.1 Proof of Lemma G.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. To prove Eq. (46) we will use induction. The base case is when $h\\,=\\,H\\,+\\,1$ , for which $\\bar{v}_{\\theta_{H+1}}(s)=v^{\\pi}\\dot{\\bar{c}}\\left(s\\right)$ for all $s\\in S_{H+1}$ . This holds, since for all $(s,a)\\in S_{H+1}\\times\\mathcal{A},\\bar{q}_{\\theta_{H+1}}(s,a)=0$ for all $\\theta_{H+1}\\in\\Theta_{\\bar{G},H+1}$ by the definition of $\\Theta_{\\bar{G},H+1}$ (Eq. (13), and $v^{\\pi}\\bar{\\bar{G}}\\left(s\\right)=0$ , by Eq. (1), since $S_{H+1}=\\{s_{\\top}\\}$ ", "page_idx": 22}, {"type": "text", "text": "Now, we show the inductive step. Let $h\\,\\in\\,[H]$ be arbitrary. Assume that Eq. (46) holds for any $t\\in[h+1:H+1]$ . We prove that Eq. (46) holds for $h$ . Let $(s,a)\\in S_{h}\\times\\mathcal{A},\\theta_{h}\\in\\Theta_{\\bar{G},h}$ Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left|\\bar{q}_{\\theta_{h}}(s,a)-q^{\\pi}\\mathring{\\boldsymbol{G}}(s,a)\\right|\\leq\\left|\\operatorname*{min}\\Bigl\\{H,\\langle\\phi(s,a),\\theta_{h}\\rangle-q^{\\pi}\\mathring{\\boldsymbol{G}}(s,a)\\Bigr\\}\\right|}\\\\ &{}&{=\\operatorname*{min}\\Bigl\\{H,\\left|\\langle\\phi(s,a),\\theta_{h}\\rangle-q^{\\pi}\\mathring{\\boldsymbol{G}}(s,a)\\right|\\Bigr\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last equality holds since $\\left(\\langle\\phi(s,a),\\theta_{h}\\rangle-q^{\\pi_{\\bar{G}}^{\\star}}(s,a)\\right)\\in[-H,H]$ . We will focus on bounding $\\left|\\langle\\phi(s,a),\\theta_{h}\\rangle-q^{\\pi}\\mathring{\\varepsilon}\\big(s,a\\big)\\right|$ .By the definition of the set $\\Theta_{\\bar{G},h}$ (Eq. (13) we know that there exists a $\\hat{\\theta}_{h}\\in\\hat{\\Theta}_{\\bar{G},h}$ such that $\\left\\|\\theta_{h}-\\hat{\\theta}_{h}\\right\\|_{X_{h}}\\le\\beta$ Thus, by the Cauchy-Schwarz inequality, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\left\\langle\\phi(s,a),\\theta_{h}-\\hat{\\theta}_{h}\\right\\rangle\\right|\\leq\\|\\phi(s,a)\\|_{X_{h}^{-1}}\\Big\\|\\theta_{h}-\\hat{\\theta}_{h}\\Big\\|_{X_{h}}\\leq\\beta\\|\\phi(s,a)\\|_{X_{h}^{-1}}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\langle\\phi(s,a),\\theta_{h}\\rangle-q^{\\pi_{\\overline{{G}}}^{\\star}}(s,a)\\right|\\leq\\left|\\left\\langle\\phi(s,a),\\widehat{\\theta}_{h}\\right\\rangle-q^{\\pi_{\\overline{{G}}}^{\\star}}(s,a)\\right|+\\beta\\|\\phi(s,a)\\|_{X_{h}^{-1}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since we know $\\hat{\\theta}_{h}\\in\\hat{\\Theta}_{\\bar{G},h}$ , by the definition of $\\hat{\\Theta}_{\\bar{G},h}$ (Eq. (12),there exists a $\\theta_{h+1:H+1}\\in\\Theta_{\\bar{G},h+1}\\times$ $\\cdots\\times\\Theta_{\\bar{G},H+1}$ , such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{h}=X_{h}^{-1}\\sum_{j\\in[n]}\\phi_{h}^{j}\\underbrace{\\mathbb{E}}_{\\tau\\sim F_{\\bar{G},h+1}^{j}}\\left[r_{h:\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}\\left(s_{\\tau}^{j}\\right)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma 4.2, we know there exists a parameter $\\rho_{h}^{\\pi^{0}}(f)\\in B(\\tilde{L_{2}})$ , such that for all $(s,a)\\in S_{h}\\times A$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{0},s,a}}{\\mathbb{E}}\\underset{\\tau\\sim F_{G,\\mathrm{rnj},h+1}}{\\mathbb{E}}\\big[R_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(S_{\\tau})\\big]-\\Big\\langle\\phi(s,a),\\rho_{h}^{\\pi^{0}}\\big(\\bar{v}_{\\theta_{h+1:H+1}}\\big)\\Big\\rangle\\right|\\le\\tilde{\\eta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\rho_{h}^{\\pi^{0}}$ be as defined above. Next, we will show a bound on $\\left|\\left\\langle\\phi(s,a),\\hat{\\theta}_{h}-{\\rho_{h}^{\\pi^{0}}}\\left(\\bar{v}_{\\theta_{h+1:H+1}}\\right)\\right\\rangle\\right|$ $\\begin{array}{r l r}&{\\mathrm{and}}&{\\left\\vert\\left\\langle\\phi(s,a),\\rho_{h}^{\\pi^{0}}\\left(\\bar{v}_{\\theta_{h+1:H+1}}\\right)\\right\\rangle-q^{\\pi}\\dot{\\sigma}(s,a)\\right\\vert,\\quad\\mathrm{which~\\tau~together~\\tau~will~\\tau~give~\\tau~us~\\pm~a~\\bound}\\quad\\mathrm{or}}\\\\ &{\\left\\vert\\left\\langle\\phi(s,a),\\hat{\\theta}_{h}\\right\\rangle-q^{\\pi_{G}^{*}}(s,a)\\right\\vert,\\quad\\mathrm{as~\\desired.}\\quad\\mathrm{The}\\quad\\mathrm{following~\\tau~result~\\tau~gives~\\tau~us~\\pm~bound}\\quad\\mathrm{or}}\\\\ &{\\left\\lVert\\hat{\\theta}_{h}-\\rho_{h}^{\\pi^{0}}\\left(\\bar{v}_{\\theta_{h+1:H+1}}\\right)\\right\\rVert_{X_{h}}.}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Lemma G.2. There is an event ${\\mathcal{E}}_{1}$ which occurs with probability at least $1-\\delta/3$ such that under event ${\\mathcal{E}}_{1}$ for all $G\\in\\mathbf{G}$ for all $h\\in[H]$ and for all $\\hat{\\theta}_{h}\\in\\hat{\\Theta}_{G,h}$ , it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|{\\hat{\\theta}}_{h}-\\theta_{h}^{\\star}\\right\\|_{X_{h}}\\leq\\beta\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{\\mathbf{\\eta}}}_{h}=X_{h}^{-1}\\sum_{j\\in[n]}\\phi_{h}^{j}\\underset{\\tau\\sim F_{G,h+1}^{j}}{\\mathbb{E}}\\Big[\\boldsymbol{r}_{h:\\tau-1}^{j}+\\bar{v}_{\\boldsymbol{\\theta}_{h+1:H+1}}\\big(\\boldsymbol{s}_{\\tau}^{j}\\big)\\Big]\\quad f o r\\,s o m e\\,\\boldsymbol{\\theta}_{h+1:H+1}\\in\\Theta_{G,h+1}\\times\\cdot\\cdot\\times\\Theta_{G,h+1}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $\\theta_{h}^{\\star}\\in B(\\tilde{L_{2}})$ is such that, for all $(s,a)\\in S_{h}\\times A,$ it satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{0},s,a}}{\\mathbb{E}}\\frac{\\mathbb{E}}{\\pi_{G,\\mathrm{raj},h+1}}\\big[R_{h:\\tau-1}+\\bar{v}_{{\\theta}_{h+1:H+1}}(S_{\\tau})\\big]-\\langle\\phi(s,a),\\theta_{h}^{\\star}\\rangle\\right|\\leq\\tilde{\\eta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Fix $G\\in\\mathbf{G},h\\in[H]$ , and $\\hat{\\theta}_{h}\\in\\hat{\\Theta}_{G,h}$ , such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{\\mathbf{\\rho}}}_{h}=X_{h}^{-1}\\sum_{j\\in[n]}\\phi_{h}^{j}\\underset{\\tau\\sim F_{G,h+1}^{j}}{\\mathbb{E}}\\Big[\\boldsymbol{r}_{h:\\tau-1}^{j}+\\bar{v}_{\\boldsymbol{\\theta}_{h+1:H+1}}\\big(\\boldsymbol{s}_{\\tau}^{j}\\big)\\Big]\\quad\\mathrm{for~some~}\\boldsymbol{\\theta}_{h+1:H+1}\\in\\Theta_{G,h+1}\\times\\cdot\\cdot\\times\\Theta_{G,h+1}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Fix $\\theta_{h}^{\\star}\\in B(\\tilde{L_{2}})$ , such that, for all $(s,a)\\in{\\cal S}_{h}\\times{\\cal A}$ , it satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{0},s,a}}{\\mathbb{E}}\\frac{\\mathbb{E}}{\\pi_{G,\\mathrm{raj},h+1}}\\big[R_{h:\\tau-1}+\\bar{v}_{{\\theta}_{h+1:H+1}}(S_{\\tau})\\big]-\\langle\\phi(s,a),\\theta_{h}^{\\star}\\rangle\\right|\\leq\\tilde{\\eta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We would like to make use of Lemma H.1 to bound $\\left\\|{\\hat{\\theta}}_{h}-\\theta_{h}^{\\star}\\right\\|_{X_{h}}.$ To do so, we map the terms used in the Lemma H.1 to our terms as follows ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n=n,\\lambda=\\lambda,\\theta_{\\star}=\\theta_{h}^{\\star},V=X_{h},\\hat{\\theta}=\\hat{\\theta}_{h},A=\\left(\\phi_{h}^{i}\\right)_{j\\in[n]},}\\\\ &{Y=\\bar{Y}+\\Delta=\\left(\\left\\langle\\phi_{h}^{j},\\theta_{h}^{\\star}\\right\\rangle\\right)_{j\\in[n]}+\\gamma+\\Delta=\\left(\\frac{\\mathbb{E}}{\\tau-\\mathcal{F}_{g,h+1}^{j}}\\left[r_{h;\\tau=1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}\\left(s_{\\tau}^{j}\\right)\\right]\\right)_{j\\in[n]},}\\\\ &{\\gamma=\\left(\\underbrace{\\mathbb{E}}_{r\\sim\\mathcal{F}_{\\theta,h+1}^{j}}\\left[r_{h;\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}\\left(s_{\\tau}^{j}\\right)\\right]-\\underbrace{\\mathbb{E}}_{\\operatorname*{min}\\sim\\bar{v}_{\\theta_{h},a,j}}\\underbrace{\\mathbb{E}}_{r\\sim\\mathcal{F}_{\\theta,h+1}^{j}}\\left[R_{h;\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}\\left(S_{\\tau}\\right)\\right]\\right)}\\\\ &{\\Delta=\\left(\\underbrace{\\mathbb{E}}_{\\operatorname*{min}\\sim\\bar{v}_{\\theta_{\\star},a,j}}\\underbrace{\\mathbb{E}}_{r\\sim\\mathcal{F}_{\\theta;r,\\theta_{h+1}}}\\left[R_{h;\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}\\left(S_{\\tau}\\right)\\right]-\\left\\langle\\phi_{h}^{j},\\theta_{h}^{\\star}\\right\\rangle\\right)_{j\\in[n]},}\\\\ &{t=\\sum_{j\\in[n]}\\phi_{h}^{j}\\left(\\underbrace{\\mathbb{E}}_{r\\sim\\mathcal{F}_{\\theta,h+1}^{j}}\\left[r_{h;\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}\\left(s_{\\tau}^{j}\\right)\\right]-\\underbrace{\\mathbb{E}}_{\\operatorname*{min}\\sim\\bar{v}_{\\theta_{\\star},a,j}}\\underbrace{\\mathbb{E}}_{r\\sim\\mathcal{F}_{\\theta,h+1}^{j}}\\left[R_{h;\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}\\left(s_{\\tau}\\right)\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With the definitions as above, applying Lemma H.1 we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{\\theta}_{h}-\\theta_{h}^{\\star}\\right\\|_{X_{h}}\\leq\\sqrt{\\lambda}\\|\\theta_{h}^{\\star}\\|_{2}+\\|\\Delta\\|_{\\infty}\\sqrt{n}+\\|\\iota\\|_{X_{h}^{-1}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The frst term can be bounded by $H^{3/2}d$ by noting that $\\lVert\\theta_{h}^{\\star}\\rVert_{2}\\leq\\tilde{L_{2}}$ , and setting ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sqrt{\\lambda}=H^{3/2}d/\\tilde{L_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The second term can be bounded by $\\tilde{\\eta}\\sqrt{n}$ , by using Eq. (55) ", "page_idx": 23}, {"type": "text", "text": "To bound the third term let the event ${\\mathcal{E}}_{1}$ be as defined in the proof of Lemma H.2, which occurs with probability at least $1-\\delta/3$ . Then, under event ${\\mathcal{E}}_{1}$ , the third term in Eq. (56) can be bounded by. $\\bar{\\beta}$ (Eq. (31), by applying Lemma H.2, since Oh+1:H+1 E OG,h+1 X ..\u00b7XG,H+1 C B(L2)H-h+1. ", "page_idx": 23}, {"type": "text", "text": "Plugging the three bounds above back into Eq. (56) we get that under event ${\\mathcal{E}}_{1}$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Big\\|\\hat{\\theta}_{h}-\\theta_{h}^{\\star}\\Big\\|_{X_{h}}\\leq H^{3/2}d+\\tilde{\\eta}\\sqrt{n}+\\bar{\\beta}}}\\\\ &{=\\beta=\\tilde{\\mathcal{O}}\\Big(H^{3/2}d\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The last equality holds by setting ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta\\le\\frac{H^{3/2}d}{\\sqrt{n}(10H^{2}d_{0}/\\alpha+1)}\\implies\\tilde{\\eta}\\le H^{3/2}d/\\sqrt{n}\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the values of $n,{\\bar{\\beta}}$ are set according to Appendix A ", "page_idx": 23}, {"type": "text", "text": "We return back to the proof of Lemma G.1. Let ${\\mathcal{E}}_{1}$ be as defined in the proof of Lemma G.2. For the remainder of the proof, operate under event ${\\mathcal{E}}_{1}$ . By Lemma G.2 (with $\\begin{array}{r}{\\bar{\\rho}_{h}^{\\pi^{0}}\\big(\\bar{v}_{\\theta_{h+1:H+1}}\\big)=\\theta_{h}^{\\star})}\\end{array}$ ,we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{\\theta}_{h}-\\rho_{h}^{\\pi^{0}}\\left(\\bar{v}_{\\theta_{h+1:H+1}}\\right)\\right\\|_{X_{h}}\\le\\beta\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Returning to $\\left|\\left\\langle\\phi(s,a),\\hat{\\theta}_{h}-{\\rho_{h}^{\\pi}}^{0}\\left(\\bar{v}_{\\theta_{h+1:H+1}}\\right)\\right\\rangle\\right|$ we can now bound it as follows. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\left\\langle\\phi(s,a),\\hat{\\theta}_{h}-\\rho_{h}^{\\pi^{0}}\\left(\\bar{v}_{\\theta_{h+1:H+1}}\\right)\\right\\rangle\\right|\\leq\\|\\phi(s,a)\\|_{X_{h}^{-1}}\\Big\\|\\hat{\\theta}_{h}-\\rho_{h}^{\\pi^{0}}\\left(\\bar{v}_{\\theta_{h+1:H+1}}\\right)\\Big\\|_{X_{h}}\\leq\\beta\\|\\phi(s,a)\\|_{X_{h}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The first inequality used the Cauchy-Schwarz inequality. The second inequality used Eq. (60). Now, webound $\\left|\\left\\langle\\stackrel{\\cdot}{\\phi}(s,\\stackrel{\\cdot}{a}),\\rho_{h}^{\\pi^{0}}\\left(\\bar{v}_{\\theta_{h+1:H+1}}\\right)\\right\\rangle-q^{\\pi_{\\bar{G}}^{\\star}}(s,a)\\right|$ by making use of Eq. (54), to get that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\biggl|\\left\\langle\\phi(s,a),\\rho_{h}^{\\pi^{*}}\\widehat{\\left(v_{\\theta_{h+1,H+1}}\\right)}\\right\\rangle-q^{\\pi_{G}^{*}}(s,a)\\biggr|}\\\\ &{\\le\\biggl|\\frac{\\mathbb{E}}{\\prod_{\\pi\\mid\\sim\\widehat{\\Gamma}_{\\pi^{0}}(s,a)}\\tau\\sim\\mathbb{F}_{G,\\eta_{0},h+1}}\\bigl[R_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(S_{\\tau})\\bigr]-q^{\\pi_{G}^{*}}(s,a)\\biggr|+\\tilde{\\eta}}\\\\ &{=\\biggl|\\frac{\\mathbb{E}}{\\prod_{\\pi\\mid\\sim\\widehat{\\Gamma}_{\\pi^{0}}(s,a)}\\tau\\sim\\mathbb{F}_{G,\\eta_{0},h+1}}\\biggl[R_{h:\\tau-1}+\\operatorname*{max}\\bar{q}_{\\theta_{\\pi\\tan(s_{\\tau})}(S_{\\tau})}(S_{\\tau},a^{\\prime})-\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}q^{\\pi_{G}^{*}}(S_{\\tau},a^{\\prime})+\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}q^{\\pi_{G}^{*}}(S_{\\tau},\\eta_{0})\\biggr]}\\\\ &{\\le\\biggl|\\frac{\\mathbb{E}}{\\prod_{\\pi\\mid\\sim\\widehat{\\Gamma}_{\\pi^{0}}(s,a,\\tau\\sim\\mathbb{F}_{G,\\eta_{0},h+1}}\\left[R_{h:\\tau-1}+\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}\\tau_{\\sigma}^{*}(S_{\\tau},a^{\\prime})\\right]-q^{\\pi_{G}^{*}}(s,a)\\biggr|}}\\\\ &{\\qquad+\\underbrace{\\mathbb{E}}_{\\biggl|\\Gamma_{\\Pi\\sim\\widehat{\\Gamma}_{\\pi^{0}}(s,a)}\\tau\\sim\\mathbb{F}_{G,\\eta_{0},h+1}}\\underbrace{\\operatorname*{max}\\left(\\bar{q}_{\\theta_{\\pi\\tan(s_{\\tau})}(S_{\\tau})}(S_{\\tau},a^{\\prime})-q^{\\pi_{G}^{*}}(S_{\\tau},a^{\\prime})\\right)\\biggr|}_{\\alpha:\\epsilon,A}+\\tilde{\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The equality used the definition of $\\bar{v}$ (Eq. (10). To bound the first term in Eq. (62) notice that by the definition of $\\pi_{\\bar{G}}^{\\star}$ (Eq. (15)), $\\operatorname*{arg\\,max}_{a^{\\prime}\\in A}q^{\\pi}\\mathring{\\varepsilon}\\left(S_{\\tau},a^{\\prime}\\right)$ is exactly the action $\\pi_{\\bar{G}}^{\\star}$ would take at the stopping stage $\\tau$ and that the distribution of Traj under policy $\\pi^{0}$ until stopping stage $\\tau$ is same as the distribution of Traj under policy $\\pi_{\\bar{G}}^{\\star}$ until stopping stage $\\tau$ . This gives that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\underset{\\mathrm{Traj}\\sim\\mathrm{P}_{\\sigma^{0},s,a}}{\\mathbb{E}}\\underset{\\substack{\\tau\\sim F_{\\tilde{G},\\mathrm{Traj},h+1}}}{\\mathbb{E}}\\left[R_{h\\cdot\\tau-1}+\\operatorname*{max}_{a^{\\prime}\\in A}q^{\\pi_{a}^{*}}(S_{\\tau},a^{\\prime})\\right]-q^{\\pi_{G}^{*}}(s,a)\\Big|=\\left|q^{\\pi_{G}^{*}}(s,a)-q^{\\pi_{G}^{*}}(s,a)\\right|=0\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To bound the second term in Eq. (62) we can use the inductive hypothesis (Eq. (46). Defining notation that will be needed for the below display, for any $\\left\\langle\\bar{s}^{\\prime},a^{\\prime}\\right\\rangle\\;\\in\\;\\bar{\\mathcal{S}}\\,\\times\\,\\mathcal{A}$ we  will write $\\mathrm{Traj}^{\\prime}\\,\\sim\\,\\mathbb{P}_{\\check{\\pi},s,a}$ to have the usual definition $\\mathrm{Traj}^{\\prime}\\;=\\;\\left(s^{\\prime},a^{\\prime},R_{h}^{\\prime},\\ldots,S_{H+1}^{\\prime},A_{H+1}^{\\prime},R_{H+1}^{\\prime}\\right)$ \uff0c except with a superscript $(\\cdot)^{\\prime}$ added to all of the random elements.  Then, by letting $\\mathit{a}_{\\tau}=$ arg $\\operatorname*{max}_{\\substack{\\ldots\\,a^{\\prime}\\in\\mathcal{A}}}\\Bigl(\\bar{q}_{\\theta_{\\mathrm{stage}(S_{\\tau})}}(S_{\\tau},a^{\\prime})-q^{\\pi_{\\bar{G}}^{\\star}}(S_{\\tau},a^{\\prime})\\Bigr)$ , applying the inductive hypothesis, and a triangle inequality,wehave that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathrm{Traj}\\sim\\nabla_{\\pi^{0},s,a}}{\\mathbb{E}}\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{Traj},h+1}}{\\underbrace{\\mathbb{E}}}\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}\\Big(\\bar{q}_{\\theta_{\\mathrm{stage}(S_{\\tau})}}(S_{\\tau},a^{\\prime})-q^{\\pi}\\dot{\\bar{\\alpha}}(S_{\\tau},a^{\\prime})\\Big)\\Big|}\\\\ &{\\le\\left|\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{0},s,a}}{\\mathbb{E}}\\underset{\\tau\\sim F_{G,\\mathrm{Traj},h+1}}{\\mathbb{E}}\\left[2\\beta\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{0},S_{\\tau},a_{\\tau}}}{\\mathbb{E}}\\underset{t=\\tau}{\\overset{H}{\\prod}}\\mathrm{min}\\Big\\{1,\\|\\phi(S_{t}^{\\prime},A_{t}^{\\prime})\\|_{X_{t}^{-1}}\\Big\\}\\right|+\\bigg|\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{0},s,a}}{\\mathbb{E}}\\underset{\\tau\\sim F_{G,\\mathrm{Traj},h+1}}{\\mathbb{E}}\\bigg[2\\beta\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{0},S_{\\tau},a_{\\tau}}}{\\mathbb{E}}\\bigg[\\mathrm{E}\\bigg]\\bigg|_{\\tau\\sim\\mathbb{T}_{\\tau}}\\mathrm{E}\\bigg]\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We bound each of the terms in the expectation separately. For the first term, we first recall the definitionof $g^{\\bar{\\pi}}$ (Eq. (48)) and upper bound the term inside the expectation as follows, which will help us relate things to $\\bar{\\pi}$ as we shall see soon. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\underset{\\mathrm{Trij}^{\\prime}\\sim\\mathbb{P}_{\\bar{\\pi},S_{\\tau},a_{\\tau}}}{\\mathbb{E}}\\sum_{t=\\tau}^{H}\\operatorname*{min}\\Bigl\\{1,\\|\\phi(S_{t}^{\\prime},A_{t}^{\\prime})\\|_{X_{t}^{-1}}\\Bigr\\}=g^{\\bar{\\pi}}(S_{\\tau},a_{\\tau})\\le\\underset{a^{\\prime}\\in\\mathcal{A}}{\\operatorname*{max}}g^{\\bar{\\pi}}(S_{\\tau},a^{\\prime})\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Along with the above result, notice that by the definition of $\\bar{\\pi}$ (Eq. (47), $\\arg\\operatorname*{max}_{a^{\\prime}\\in A}g^{\\bar{\\pi}}(S_{\\tau},a^{\\prime})$ .s exactly the action $\\bar{\\pi}$ would take at the stopping stage $\\tau$ , and that the distribution of Traj under policy $\\pi^{0}$ until stopping stage $\\tau$ is same as the distribution of Traj under policy $\\bar{\\pi}$ until stopping stage $\\tau$ Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\beta\\biggl|\\underset{\\mathrm{Trij}\\sim\\mathbb{P}_{\\pi^{0},s,a}}{\\mathbb{E}}\\underset{\\tau\\sim F_{G,\\mathrm{mj},h+1}}{\\mathbb{E}}\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}g^{\\bar{\\pi}}(S_{\\tau},a^{\\prime})\\biggr|}\\\\ &{\\ \\leq2\\beta\\underset{\\mathrm{Trij}\\sim\\mathbb{P}_{\\pi^{0},s,a}}{\\mathbb{E}}\\underset{\\tau\\sim F_{G,\\mathrm{rrij},h+1}}{\\mathbb{E}}\\Biggl[\\underset{t=h+1}{\\overset{\\tau-1}{\\sum}}\\operatorname*{min}\\Bigl\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\Bigr\\}+\\underset{a^{\\prime}\\in A}{\\operatorname*{max}}g^{\\bar{\\pi}}(S_{\\tau},a^{\\prime})\\Biggr]}\\\\ &{\\ =2\\beta g^{\\bar{\\pi}}(s,a)=2\\beta\\underset{\\mathrm{Trij}\\sim\\mathbb{P}_{\\pi,s,a}}{\\mathbb{E}}\\underset{t=h+1}{\\overset{H}{\\sum}}\\operatorname*{min}\\Bigl\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\Bigr\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the second term, since $\\tau\\geq h+1$ where $\\tau\\sim F_{\\bar{G},\\mathrm{Traj},h+1}$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\underset{\\mathrm{Traj}\\sim\\mathbb{P}_{\\pi^{0},s,a}}{\\mathbb{E}}\\frac{\\mathbb{E}}{\\tau\\sim F_{\\bar{G},\\mathrm{Traj},h+1}}(H-\\tau+1)\\tilde{\\eta}\\right|\\leq(H-h)\\tilde{\\eta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Plugging the above two bounds into Eq. (62), we get that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\langle\\phi(s,a),\\rho_{h}^{\\pi^{0}}(\\bar{v}_{\\theta_{h+1:H+1}})\\right\\rangle-q^{\\pi_{G}^{*}}(s,a)\\Big|\\le2\\beta\\operatorname{\\mathbb{1}}_{\\mathrm{Traje}\\mathbb{P}_{\\pi,s,a}}\\sum_{t=h+1}^{H}\\operatorname*{min}\\left\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\right\\}+\\left(H-h\\right)\\operatorname{\\mathbb{1}}_{\\rho\\in\\mathbb{P}_{\\pi}}\\sum_{s=t+1}^{H}\\left\\{1,\\phi(S_{t})\\|_{X_{t}^{-1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining Eqs. (52), (53), (61) and (63), we get that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\bar{q}_{\\theta_{h}}\\big(s,a\\big)-q^{\\pi_{\\hat{\\alpha}}^{\\star}}(s,a)\\Big|}\\\\ &{\\le\\operatorname*{min}\\Biggl\\{H,2\\beta\\|\\phi(s,a)\\|_{X_{h}^{-1}}+2\\beta\\underbrace{\\mathbb{E}}_{\\mathrm{Trije}\\mathbb{P}\\cdot\\mathbb{P}_{\\pi,s,a}}\\underbrace{H}_{t=h+1}\\operatorname*{min}\\Bigl\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\Bigr\\}+\\bigl(H-h+1\\bigr)\\tilde{\\eta}\\Biggr\\}}\\\\ &{\\le2\\beta\\operatorname*{min}\\Bigl\\{1,\\|\\phi(s,a)\\|_{X_{h}^{-1}}\\Bigr\\}+2\\beta\\underbrace{\\mathbb{E}}_{\\mathrm{Trije}\\mathbb{P}\\cdot\\mathbb{P}_{\\pi,s,a}}\\underbrace{H}_{t=h+1}\\operatorname*{min}\\Bigl\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\Bigr\\}+\\bigl(H-h+1\\bigr)\\tilde{\\eta}}\\\\ &{\\le2\\beta\\underbrace{\\mathbb{E}}_{\\mathrm{Trije}\\mathbb{P}\\cdot\\mathbb{P}_{\\pi,s,a}}\\underbrace{H}_{t=h}\\operatorname*{min}\\Bigl\\{1,\\|\\phi(S_{t},A_{t})\\|_{X_{t}^{-1}}\\Bigr\\}+\\bigl(H-h+1\\bigr)\\tilde{\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The second inequality used that $\\beta\\geq H$ and that $\\operatorname*{min}(a,b+c)\\leq\\operatorname*{min}(a,b)+c$ for $a,b,c\\geq0$ \uff1a\u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma G.3. There is an event ${\\mathcal{E}}_{3}$ , that occurs with probability at least $1-\\delta/3$ such that under event ${\\mathcal{E}}_{3}$ for all $h\\in[H]$ ,it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}\\operatorname*{min}\\Bigl\\{1,\\|\\phi(S,A)\\|_{X_{h}^{-1}}\\Bigr\\}\\le\\check{\\epsilon}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where E is defined in Eq. (28). ", "page_idx": 25}, {"type": "text", "text": "Proof. First we will show two useful results (namely Eq. (64) and Eq. (66) that are needed in the proof.Let ", "page_idx": 25}, {"type": "text", "text": "Notcethatany $X\\in\\mathbb{X}$ can be writenas $\\textstyle X=\\sum_{i=1}^{d}x_{i}x_{i}^{\\top}$ Wwith $x_{i}\\in B(1/\\lambda)$ forall $i\\,\\in\\,[d]$ By Lemma J.2, we know there exists a set $C_{\\xi}\\,\\subset B(a),a,\\xi>0$ with $|C_{\\xi}|=(1+2a/\\xi)^{d}$ such that for any $x\\in B(a)$ there exists a $y\\in C_{\\xi}$ such that $\\left\\|x-y\\right\\|_{2}\\leq\\xi$ . Define the set ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{Y}=\\left\\{\\sum_{i=1}^{d}y_{i}y_{i}^{\\top}:y_{i}\\in C_{\\xi}\\;{\\mathrm{for}}\\;{\\mathrm{all}}\\;i\\in[d]\\right\\},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "With YI = (1+2/(\u5165>)a, Then,for any X = \u2265d $\\begin{array}{r}{X=\\sum_{i=1}^{d}x_{i}x_{i}^{\\top}\\in\\mathbb{X}}\\end{array}$ there exists a $\\begin{array}{r}{Y=\\sum_{i=1}^{d}y_{i}y_{i}^{\\top}\\in\\mathbb{Y},}\\end{array}$ such that $\\|{\\boldsymbol x}_{i}-{\\boldsymbol y}_{i}\\|_{2}\\leq\\xi$ for all $i\\in[d]$ .Let $X,Y$ be as we just defined them. Then, wrting $\\lVert\\cdot\\rVert_{\\mathrm{op}}$ ", "page_idx": 25}, {"type": "text", "text": "for the operator norm, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|X-Y\\|_{\\mathrm{qp}}=\\left\\|\\sum_{i=1}^{d}x_{i}x_{i}^{\\top}-y_{i}y_{i}^{\\top}\\right\\|_{\\mathrm{q}}}}\\\\ &{=\\left\\|\\displaystyle\\sum_{i=1}^{d}(x_{i}-y_{i})(x_{i}-y_{i})^{\\top}+y_{i}(x_{i}-y_{i})^{\\top}+(x_{i}-y_{i})y_{i}^{\\top}\\right\\|_{\\mathrm{qp}}}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{d}\\left\\|(x_{i}-y_{i})(x_{i}-y_{i})^{\\top}\\right\\|_{\\mathrm{qp}}+\\left\\|y_{i}(x_{i}-y_{i})^{\\top}\\right\\|_{\\mathrm{qp}}+\\left\\|(x_{i}-y_{i})y_{i}^{\\top}\\right\\|_{\\mathrm{qp}}}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{d}\\left\\|(x_{i}-y_{i})\\right\\|_{\\mathrm{qp}}\\|(x_{i}-y_{i})\\left\\|_{2}+\\left\\|y_{i}\\right\\|_{\\mathrm{qp}}\\right\\|(x_{i}-y_{i})\\left\\|_{2}+\\|(x_{i}-y_{i})\\|_{2}\\right\\|_{\\mathrm{pt}}}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{d}\\left\\|(x_{i}-y_{i})\\right\\|_{\\mathrm{qp}}\\|_{\\mathrm{qp}}}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{d}\\xi^{2}+\\displaystyle\\sum_{i=1}^{2d}d\\xi^{2}+\\displaystyle\\sum_{i=1}^{2d}\\xi\\left.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, for any $u\\in B(L_{1})$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Big|\\|u\\|_{X}^{2}-\\|u\\|_{Y}^{2}\\Big|=\\big|u^{\\top}(X-Y)u\\big|\\le\\|u\\|_{2}^{2}\\|X-Y\\|_{\\mathrm{op}}\\leq L_{1}^{2}\\Big(d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which implies that (since for non-negative $a,b,{\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}})$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|u\\|_{X}\\leq\\sqrt{\\|u\\|_{Y}^{2}+L_{1}^{2}\\bigg(d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}\\bigg)}\\leq\\|u\\|_{Y}+\\sqrt{L_{1}^{2}\\bigg(d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}\\bigg)},}\\\\ &{\\|u\\|_{Y}\\leq\\sqrt{\\|u\\|_{X}^{2}+L_{1}^{2}\\bigg(d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}\\bigg)}\\leq\\|u\\|_{X}+\\sqrt{L_{1}^{2}\\bigg(d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}\\bigg)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, for any $u\\in B(L_{1})$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|u\\|_{X}-\\|u\\|_{Y}|\\leq L_{1}\\sqrt{d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Eq. (64) is the first useful result that we alluded to at the beginning of the proof. ", "page_idx": 26}, {"type": "text", "text": "Now, we will show the second useful result. For any $Y\\in\\mathbb{Y}$ and $h\\in[H]$ , define the event ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{E}_{3}^{Y,h}=\\left\\{\\left|\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}\\operatorname*{min}\\{1,\\|\\phi(S,A)\\|_{Y}\\}-\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\Bigl\\{1,\\left\\|\\phi_{h}^{j}\\right\\|_{Y}\\Bigr\\}\\right|\\leq\\frac{1}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|\\mathbb{Y}|}{\\delta}\\right)}\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\operatorname*{min}\\{1,\\|u\\|_{Y}\\}~\\in~[0,1]$ for all $u\\;\\in\\;\\mathcal{B}(L_{1}),Y\\;\\in\\;\\mathbb{Y}$ we can use Hoeffding's inequality LemmaJt get that, for ay $Y\\ \\in\\ \\mathbb{Y},h\\ \\in\\ [H]$ , event $\\mathcal{E}_{3}^{Y,h}$ occurswith probability at least $1-\\delta/(3H|\\mathbb{Y}|)$ . Let ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{E}_{3}=\\bigcap_{Y\\in\\mathbb{Y},h\\in[H]}\\mathcal{E}_{3}^{Y,h}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, by applying a union bound over $Y,h$ we have that the event $\\mathcal{E}_{3}$ occurs with probability at least $1-\\delta/3$ , and under event $\\mathcal{E}_{3}$ , for all $Y\\in\\mathbb{Y},h\\in[H]$ , it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{(S,A)\\sim\\mu_{h}}\\operatorname*{min}\\{1,\\|\\phi(S,A)\\|_{Y}\\}\\leq\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\Bigl\\{1,\\left\\|\\phi_{h}^{j}\\right\\|_{Y}\\Bigr\\}+\\frac{1}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|\\mathbb{Y}|}{\\delta}\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Eq. (66) is the second useful result that we alluded to at the beginning of the proof. ", "page_idx": 26}, {"type": "text", "text": "Now, we turn to proving Lemma G.3. Let $h\\in[H]$ . Let $X\\in\\mathbb{X}$ and select $Y\\in\\mathbb{Y}$ such that, for any $u\\in B(L_{1})$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|u\\|_{X}-\\|u\\|_{Y}|\\le L_{1}\\sqrt{d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}}\\,,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which we know exists by Eq. (64). By using Eq. (67) we get that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{(S,A)\\sim\\mu_{h}}\\operatorname*{min}\\{1,\\|\\phi(S,A)\\|_{X}\\}\\leq\\operatorname{\\mathbb{E}}_{(S,A)\\sim\\mu_{h}}\\operatorname*{min}\\{1,\\|\\phi(S,A)\\|_{Y}\\}+L_{1}\\sqrt{d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To bound the first term on the RHS in Eq. (68) we can use Eq. (66), to get that under event ${\\mathcal{E}}_{3}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{(S,A)\\sim\\mu_{h}}\\operatorname*{min}\\{1,\\|\\phi(S,A)\\|_{Y}\\}\\leq\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\Bigl\\{1,\\left\\|\\phi_{h}^{j}\\right\\|_{Y}\\Bigr\\}+\\frac{1}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|\\mathbb{Y}|}{\\delta}\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We can bound the first term on the RHS of Eq. (69), by again using Eq. (67), to get that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\Bigl\\{1,\\left\\|\\phi_{h}^{j}\\right\\|_{Y}\\Bigr\\}\\le\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\Bigl\\{1,\\left\\|\\phi_{h}^{j}\\right\\|_{X}\\Bigr\\}+L_{1}\\sqrt{d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, by Jensen's inequality we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\Bigl\\{1,\\left\\Vert\\phi_{h}^{j}\\right\\Vert_{X}\\Bigr\\}=\\sqrt{\\left(\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\Bigl\\{1,\\left\\Vert\\phi_{h}^{j}\\right\\Vert_{X}\\Bigr\\}\\right)^{2}}\\leq\\sqrt{\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\Bigl\\{1,\\left\\Vert\\phi_{h}^{j}\\right\\Vert_{X}^{2}\\Bigr\\}}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Putting Eqs. (68) to (71) together and noting that $X,h$ were arbitrary, we get that, under event $\\mathcal{E}_{3}$ for any $X\\in\\mathbb{X},h\\in[H]$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{s,A\\sim\\mu_{h}}\\operatorname*{min}\\{1,\\|\\phi(S,A)\\|_{X}\\}\\leq\\sqrt{\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\biggl\\{1,\\left\\|\\phi_{h}^{j}\\right\\|_{X}^{2}\\biggr\\}}+2L_{1}\\sqrt{d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}}+\\frac{1}{\\sqrt{n}}\\sqrt{\\log\\biggl(\\frac{6H|\\phi|_{X}^{2}}{\\delta}\\biggr)}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We can now introduce $X_{h}$ and make use of the above result. Notice that for any $h\\in[H],X_{h}^{-1}=$ $\\begin{array}{r}{(\\lambda I+\\sum_{j\\in[n]}\\phi_{h}^{j}(\\phi_{h}^{j})^{\\top})^{-1}}\\end{array}$ is such that $\\lambda_{\\operatorname*{max}}(X_{h}^{-1})\\leq1/\\lambda$ since $\\lambda_{\\operatorname*{min}}(X_{h})\\geq\\lambda$ . Thus, $X_{h}^{-1}\\in\\mathbb{X}$ Forany $t\\in[n],h\\in[H]$ define $\\begin{array}{r}{X_{t,h}=\\lambda I+\\sum_{j\\in[t]}\\phi_{h}^{j}(\\phi_{h}^{j})^{\\top}}\\end{array}$ , and noice that $X_{n,h}^{-1}=X_{h}^{-1}$ and that $X_{t,h}^{-1}-X_{n,h}^{-1}$ is positive semidefnite. This implies that, for all $h\\in[H]$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\Biggl\\{1,\\left\\|\\phi_{h}^{j}\\right\\|_{X_{h}^{-1}}^{2}\\Biggr\\}\\leq\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\Biggl\\{1,\\left\\|\\phi_{h}^{j}\\right\\|_{X_{j-1,h}^{-1}}^{2}\\Biggr\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now, we can use the elliptical potential lemma (Lemma J.4), to conclude that, for all $h\\in[H]$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{j\\in[n]}\\operatorname*{min}\\biggr\\{1,\\left\\|\\phi_{h}^{j}\\right\\|_{X_{j-1,h}^{-1}}^{2}\\biggr\\}\\leq\\frac{2d}{n}\\log\\biggr(\\frac{d\\lambda+n L_{1}^{2}}{d\\lambda}\\biggr)\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Putting everything together, we get that, under event $\\mathcal{E}_{3}$ , for all $h\\in[H]$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}\\operatorname*{min}\\left\\{1,\\|\\phi(S,A)\\|_{X_{h}^{-1}}\\right\\}}\\\\ &{\\leq2L_{1}\\sqrt{d\\xi^{2}+\\frac{2d\\xi}{\\sqrt{\\lambda}}}+\\frac{1}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|\\mathbb{Y}|}{{\\delta}}\\right)}+\\sqrt{\\frac{2d}{n}\\log\\left(\\frac{d\\lambda+n L_{1}^{2}}{d\\lambda}\\right)}}\\\\ &{\\leq2L_{1}\\sqrt{d\\xi^{2}+\\frac{2\\xi\\tilde{L}_{2}}{H^{3/2}}}+\\frac{1}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{3H(1+2\\tilde{L}_{2}^{-2}/\\xi)^{d^{2}}}{{\\delta}}\\right)}+\\sqrt{\\frac{2d}{n}\\log\\left(\\frac{d\\lambda+n L_{1}^{2}}{d\\lambda}\\right)}}\\\\ &{=\\frac{\\sqrt{d}}{\\sqrt{n}}+\\frac{1}{\\sqrt{n}}\\sqrt{d^{2}\\log\\left(1+16n L_{1}^{2}\\tilde{L}_{2}^{-3}\\right)+\\log\\left(\\frac{3H}{{\\delta}}\\right)}+\\sqrt{\\frac{2d}{n}\\log\\left(\\frac{d\\lambda+n L_{1}^{2}}{d\\lambda}\\right)}}\\\\ &{=\\ell=\\tilde{\\mathcal{O}}(d/\\sqrt{n})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The second inequality used that $|\\mathbb{Y}|=(1+2/(\\lambda\\xi))^{d^{2}}$ . The first equality holds by setting $\\xi^{-1}=$ $8\\tilde{L_{2}}L_{1}^{2}n$ . The last equality holds by plugging in parameter values according to Appendix A. ", "page_idx": 28}, {"type": "text", "text": "Lemma G.4. If Assumption $^3$ holds, then for any non-negative function $f:S\\times A\\to[0,\\infty),$ for any admissible distribution $\\nu=(\\nu_{t})_{t\\in[H]}$ ,andfor any $h\\in[H]$ it holdsthat ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underset{(S,A)\\sim\\nu_{h}}{\\mathbb{E}}f(S,A)\\leq C_{\\mathrm{conc}}\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}f(S,A)\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Let $f:S\\times A\\to[0,\\infty)$ be any non-negative function. Let $h\\in[H]$ be any stage. Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(S,A)\\sim\\nu_{h}}{\\mathbb{E}}f(S,A)=\\int_{z\\in S_{h}\\times A}f(z)\\nu_{h}(z)d z}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\int_{z\\in S_{h}\\times A}f(z)\\frac{\\nu_{h}(z)}{\\mu_{h}(z)}\\mu_{h}(z)d z}\\\\ &{\\qquad\\qquad\\qquad\\leq\\int_{z\\in S_{h}\\times A}f(z)C_{\\mathrm{conc}}\\mu_{h}(z)d z}\\\\ &{\\qquad\\qquad\\qquad\\qquad=C_{\\mathrm{conc}}\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}f(S,A)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the inequality holds by applying Assumption 3, and noting that $f$ is non-negative.  This implies the desired result, since $f$ and $h$ were arbitrary. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "H   Lemmas Related to Least-squares ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Lemma H.1 (Least-squares Error Decomposition). Let $\\lambda\\;>\\;0,\\theta_{\\star}\\;\\in\\;\\mathbb{R}^{d}$ and $\\textit{n}\\in\\mathbb{N}^{+}$ ,For all $k\\in[n].$ .let ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k}\\in\\mathbb{R}^{d},\\ \\gamma_{k}\\in\\mathbb{R},\\ \\tilde{Y}_{k}=\\langle A_{k},\\theta_{\\star}\\rangle+\\gamma_{k},\\ Y_{k}=\\tilde{Y}_{k}+\\Delta_{k},}\\\\ &{V=\\lambda I+\\displaystyle\\sum_{t=1}^{n}A_{t}A_{t}^{\\top},\\ \\hat{\\theta}=V^{-1}\\displaystyle\\sum_{t=1}^{n}A_{t}Y_{t},\\ \\iota=\\displaystyle\\sum_{t=1}^{n}A_{t}\\gamma_{t},\\ \\Delta=(\\Delta_{t})_{t\\in[n]}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\hat{\\theta}-\\theta_{\\star}\\right\\|_{V}\\leq\\sqrt{\\lambda}\\|\\theta_{\\star}\\|_{2}+\\|\\Delta\\|_{\\infty}\\sqrt{n}+\\|\\iota\\|_{V^{-1}}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. We begin by decomposing the targets used in $\\hat{\\theta}$ asfollows ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\theta}=V^{-1}\\sum_{t=1}^{n}{A_{t}Y_{t}}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=V^{-1}\\sum_{t=1}^{n}{A_{t}(\\langle A_{t},\\theta_{\\star}\\rangle+\\gamma_{t}+\\Delta_{t})}}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\left(\\lambda I+\\sum_{t=1}^{n}{A_{t}A_{t}^{\\top}}\\right)^{-1}\\left(\\sum_{t=1}^{n}{A_{t}A_{t}^{\\top}\\theta_{\\star}}+\\lambda I\\theta_{\\star}-\\lambda I\\theta_{\\star}\\right)+V^{-1}\\sum_{t=1}^{n}{A_{t}(\\gamma_{t}+\\Delta_{t})}}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\theta_{\\star}+\\lambda V^{-1}\\theta_{\\star}+V^{-1}\\sum_{t=1}^{n}{A_{t}\\gamma_{t}+V^{-1}\\sum_{t=1}^{n}A_{t}\\Delta_{t}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, subtracting $\\theta_{\\star}$ from both sides and taking the matrix $V$ weighted norm of both sides gives us that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\hat{\\theta}-\\theta_{\\star}\\right\\|_{V}=\\left\\|\\lambda V^{-1}\\theta_{\\star}+V^{-1}\\displaystyle\\sum_{t=1}^{n}A_{t}\\gamma_{t}+V^{-1}\\displaystyle\\sum_{t=1}^{n}A_{t}\\Delta_{t}\\right\\|_{V}}}\\\\ &{\\leq\\lambda\\|\\theta_{\\star}\\|_{V^{-1}}+\\left\\|\\displaystyle\\sum_{t=1}^{n}A_{t}\\gamma_{t}\\right\\|_{V^{-1}}+\\left\\|\\displaystyle\\sum_{t=1}^{n}A_{t}\\Delta_{t}\\right\\|_{V^{-1}}}\\\\ &{\\leq\\frac{\\lambda}{\\lambda_{\\operatorname*{min}}(V)}\\|\\theta_{\\star}\\|_{2}+\\|\\varepsilon\\|_{V^{-1}}+\\|\\Delta\\|_{\\infty}\\sqrt{n}}\\\\ &{\\leq\\sqrt{\\lambda}\\|\\theta_{\\star}\\|_{2}+\\|\\iota\\|_{V^{-1}}+\\|\\Delta\\|_{\\infty}\\sqrt{n}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second inequality used that $\\|V^{-1}\\|\\le\\lambda_{\\operatorname*{max}}(V^{-1})=1/\\lambda_{\\operatorname*{min}}(V)$ and Lemma J.5 to bound $\\begin{array}{r}{\\|\\sum_{t=1}^{n}A_{t}\\Delta_{t}\\|_{V^{-1}}.}\\end{array}$ \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma H.2 (Least-squares Noise Bound). There is an event ${\\mathcal{E}}_{1}$ \uff0cwhichoccurs with probability at least $1\\,-\\,\\delta/3$ . such that under event ${\\mathcal{E}}_{1}$ , for all $\\textit{h}\\in[H]$ for all $G\\ \\in\\textbf{G}$ and $\\theta_{h+1:H+1}\\,\\in$ $B(\\tilde{L_{2}})^{H-h+1}$ it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\displaystyle\\sum_{j\\in[n]}\\phi_{h}^{j}\\Bigg(\\underset{\\tau\\sim F_{G,h+1}^{j}}{\\mathbb{E}}\\Big[r_{h:\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}\\big(s_{\\tau}^{j}\\big)\\Big]-\\underset{\\mathrm{Trij}\\sim\\bar{\\mathbb{P}}_{\\tau^{0},s_{h}^{j},a_{h}^{j}}}{\\mathbb{E}}\\underset{\\tau\\sim F_{G,w_{0},h+1}}{\\mathbb{E}}\\big[R_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(S_{\\tau})\\big]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\bar{\\beta}$ is defined in Eq. (31). ", "page_idx": 29}, {"type": "text", "text": "Proof. We begin the proof by showing two useful results (namely Eq. (74) and Eq. (75), which will be needed later in the proof. ", "page_idx": 29}, {"type": "text", "text": "By Lemma J.2, we know there exists a set $C_{\\xi}\\,\\,\\subset\\,B(a),a,\\xi\\,>\\,0$ with $|C_{\\xi}|\\,=\\,(1+2a/\\xi)^{d}$ such that for any $x\\ \\in\\ {\\mathcal{B}}(a)$ there exists a $y\\ \\in\\ C_{\\xi}$ such that $\\|x-y\\|_{2}\\,\\leq\\,\\xi$ Define the set $C_{\\xi}^{\\Theta}\\;=\\;$ ", "page_idx": 29}, {"type": "text", "text": "Xhe[2:H+1 Cg C B(L2) with |C| = (1+ 2L2/))dH. Then,for any 02:H+1 E B(L2)H, there exists a $\\tilde{\\theta}_{2:H+1}\\in C_{\\xi}^{\\Theta}$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|\\theta_{h}-\\tilde{\\theta}_{h}\\right\\|_{2}\\leq\\xi\\quad\\mathrm{for}\\,\\mathrm{all}\\,h\\in[2:H+1]\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Which implies that for any $h\\in[H],t\\in[h+1:H+1],s\\in S_{t}$ and $\\theta_{h+1:H+1},\\theta_{h+1:H+1}^{\\sim}$ as defined above ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\bar{v}_{{\\theta}_{t}}(s)-\\bar{v}_{\\theta_{\\widetilde{t}}^{\\sim}}(s)\\right|\\leq\\left|\\exp_{[0,H]}\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\langle\\phi(s,a),\\theta_{t}\\rangle-\\exp_{[0,H]}\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\langle\\phi(s,a),\\theta_{t}^{\\sim}\\rangle\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left|\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\langle\\phi(s,a),\\theta_{t}-\\theta_{t}^{\\sim}\\rangle\\right|}\\\\ &{\\qquad\\qquad\\leq\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\lVert\\phi(s,a)\\rVert_{2}\\lVert\\theta_{t}-\\theta_{t}^{\\sim}\\rVert_{2}}\\\\ &{\\qquad\\qquad\\leq L_{1}\\xi\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $\\xi>0$ . Combining Eq. 73) with Lemma I.4, we get that there exists a set $C_{\\xi}^{\\mathbf{G}}\\times C_{\\xi}^{\\Theta}\\subset\\mathbf{G}\\times$ $B(\\tilde{L_{2}})^{H}$ with $|C_{\\xi}^{\\mathbf{G}}\\!\\times\\!C_{\\xi}^{\\Theta}|\\leq(1\\!+\\!2L_{2}\\tilde{L_{2}}/\\xi))^{d H(d_{0}+1)}$ such that, for any $(G,\\theta_{2:H+1})\\in\\mathbf{G}\\times\\mathcal{B}(\\tilde{L_{2}})^{H}$ there exists a $(\\tilde{G},\\theta_{2:H+1}^{\\sim})\\in C_{\\xi}^{\\mathbf{G}}\\times C_{\\xi}^{\\ominus}$ such that, for any $h\\in[H]$ for any $u\\in[h]$ and trajectory $\\mathrm{traj}=(s_{t},a_{t},r_{t})_{t\\in[u,H+1]}$ it hoids that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\underset{\\tau\\sim F_{G,\\mathrm{tr}_{\\delta},h+1}}{\\mathbb{E}}\\!\\left[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau})\\right]-\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{tr}_{\\delta},h+1}}{\\mathbb{E}}\\!\\left[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau})\\right]\\right|}\\\\ &{\\leq(H-h+1)6\\sqrt{2d}H L_{1}\\xi/\\alpha+\\displaystyle\\sum_{t=h}^{H}L_{1}\\xi}\\\\ &{=(H-h+1)7\\sqrt{2d}H L_{1}\\xi/\\alpha\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Eq. (74) is the first useful result we alluded to at the beginning of the proof. ", "page_idx": 30}, {"type": "text", "text": "Now, we show the second useful result, which is a bound under a high probability event. For any $(\\tilde{G},\\theta_{2:H+1}^{\\sim})\\in C_{\\xi}^{\\mathbf{G}}\\times C_{\\xi}^{\\Theta}$ and $h\\in[H]$ definethevent ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\tilde{\\mathbf{\\Xi}}_{1},\\theta^{\\sim},h}{\\mathbf{\\Xi}_{1}^{\\cdot}}=\\left\\lbrace\\left\\|\\iota_{\\tilde{G},\\tilde{\\theta}^{\\sim},h}\\right\\|_{X_{h}^{-1}}\\leq\\sqrt{2H^{2}\\log\\left(\\frac{3H|C_{\\xi}^{\\mathbf{G}}\\times C_{\\xi}^{\\Theta}|}{\\delta}\\right)+\\log\\left(\\sqrt{\\frac{\\operatorname*{det}(X_{h})}{\\operatorname*{det}(\\lambda I)}}\\right)}\\right\\rbrace}\\\\ &{\\mathrm{\\quad~vhere~}\\iota_{\\tilde{G},\\theta^{\\sim},h}=\\displaystyle\\sum_{j\\in[n]}\\phi_{h}^{j}\\left(\\frac{\\mathbb{E}}{\\tau\\sim\\!\\!F_{\\tilde{G},h+1}^{j}}\\!\\left[r_{h:\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}^{\\sim}}(s_{\\tau}^{j})\\right]-\\!\\frac{\\mathbb{E}}{\\mathrm{Tr}\\!\\!\\!\\partial_{\\tau^{0},s_{h}^{j},a_{h}^{j}}\\tau\\sim\\!\\!F_{\\tilde{G},\\tau\\bmod{h}+1}}\\!\\left[R_{h:\\tau-1}+\\phi_{h:H+1}^{\\pi}(s_{\\tau}^{j})\\right]\\right)\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Notice that $\\iota_{\\tilde{G},\\theta^{\\sim},h}$ $H$ subgaussian. Thus, we can use Theorem 1 from [Abbasi-Yadkori et al. ", "page_idx": 30}, {"type": "text", "text": "2011] to get that the event $\\mathcal{E}_{1}^{\\tilde{G},\\theta^{\\sim},h}$ occurs with probability at least $1-\\delta/(3H|C_{\\xi}^{\\mathbf{G}}\\times C_{\\xi}^{\\boldsymbol{\\Theta}}|)$ . Define ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{E}_{1}=\\bigcap_{(\\tilde{G},\\theta^{\\sim})\\in C_{\\xi}^{\\mathbf{G}}\\times C_{\\xi}^{\\Theta},h\\in[H]}\\mathcal{E}_{1}^{\\tilde{G},\\theta^{\\sim},h}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, by applying a union bound over ${\\tilde{G}},\\theta^{\\sim},h$ we have that the event ${\\mathcal{E}}_{1}$ occurs with probability at least $1-\\delta/3$ and underevent ${\\mathcal{E}}_{1}$ foral $(\\tilde{G},\\theta^{\\sim})\\in C_{\\xi}^{\\mathbf{G}}\\times C_{\\xi}^{\\Theta},h\\in[H]$ it hodsthat ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Bigg|\\sum_{j\\in[n]}\\phi_{h}^{j}\\!\\left(\\!\\frac{\\mathbb{E}}{\\tau\\!\\times\\!F_{G,h+1}^{j}}\\!\\left[r_{h;\\tau-1}^{j}+\\bar{v}_{\\ell_{h+1:H+1}^{\\sim}}\\!\\left(s_{\\tau}^{j}\\right)\\right]-\\!\\frac{\\mathbb{E}}{\\mathrm{Troiss}^{\\!j}\\tau^{0}\\!\\cdot\\!s_{h}^{j}}\\!\\!\\!\\!\\!\\frac{\\mathbb{E}}{\\tau\\!\\times\\!F_{G,\\mathrm{rois},h+1}}\\!\\left[R_{h;\\tau-1}+\\bar{v}_{\\ell_{h+1:H+1}^{\\sim}}(S_{\\tau})\\right]\\!\\!\\!\\!\\right)}\\\\ &{\\leq\\sqrt{2H^{2}\\log\\left(\\frac{3H(1+2L_{2}\\tilde{L_{2}}/\\xi))^{d H(d_{0}+1)}}{\\delta}\\right)+\\log\\left(\\sqrt{\\frac{\\operatorname*{det}(X_{h})}{\\operatorname*{det}(\\lambda I)}}\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now with all of the above results in hand, we turn to finally proving Lemma H.2. Let $G\\in\\mathbf{G}$ as in the lemma statement. Then, by Eq. (74) we know that there exists a $(\\tilde{G},\\theta_{2:H+1}^{\\sim})\\in C_{\\xi}^{\\mathbf{G}}\\times C_{\\xi}^{\\Theta}$ such that, for any $h\\in[H]$ , for any $u\\in[h]$ and trajectory traj $=(s_{t},a_{t},r_{t})_{t\\in[u:H+1]}$ , it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\underset{\\tau\\sim F_{G,w_{i},h+1}}{\\mathbb{E}}\\Big[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau})\\Big]-\\underset{\\tau\\sim F_{G,w_{i},h+1}}{\\mathbb{E}}\\Big[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}^{\\sim}}(s_{\\tau})\\Big]\\Bigg|\\le7\\sqrt{2d}H^{2}L_{1}\\xi/\\alpha.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let $\\tilde{G},\\theta_{2:H+1}^{\\sim}$ be as defined above. Then, for any $h\\in[H]$ , by using the triangle inequalty we can write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{j\\in[n]}\\phi_{h}^{j}\\Bigg(\\frac{\\mathbb{E}}{r\\kappa_{h+1}^{j}}\\bigg[r_{h+7-1}^{j}+\\bar{v}_{\\theta_{h+1:H+}}(s_{\\tau}^{j})\\bigg]-\\frac{\\mathbb{E}}{\\operatorname*{min}\\sim\\bar{v}_{\\theta_{h+1},h}^{j}}\\frac{\\mathbb{E}}{r\\kappa_{h}^{j}}[r_{h-1}+\\bar{v}_{\\theta_{h+1:H+1}}(S_{\\tau})]}\\\\ &{\\leq\\left\\|\\sum_{j\\in[n]}\\phi_{h}^{j}\\Bigg(\\frac{\\mathbb{E}}{r\\kappa_{h}^{j}}\\bigg[r_{h+7-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau}^{j})\\bigg]-\\frac{\\mathbb{E}}{\\operatorname*{min}\\sim\\bar{v}_{\\theta_{h+1:H}}(s_{\\tau}^{j})}\\frac{\\mathbb{E}}{r\\kappa_{h}^{j}\\kappa_{\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(S_{\\tau})}\\left[R_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(S_{\\tau})\\right]}\\\\ &{\\quad+\\left\\|\\sum_{j\\in[n]}\\phi_{h}^{j}\\Bigg(\\frac{\\mathbb{E}}{r\\kappa_{h}^{j}\\kappa_{h+1}^{j}}\\bigg[r_{h+7-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau}^{j})\\bigg]-\\frac{\\mathbb{E}}{r\\kappa_{h}^{j}\\kappa_{h+1:H}^{j}}\\left[r_{h+7-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau}^{j})\\right]\\Bigg)\\right\\|_{X_{h}^{-1}}}\\\\ &{\\quad+\\left\\|\\sum_{j\\in[n]}\\phi_{h}^{j}\\Bigg(\\frac{\\mathbb{E}}{r\\kappa_{h}^{j}\\kappa_{h}^{j}\\sigma_{s,h+1}^{j}}\\left[r_{s\\in\\bar{F}_{\\theta_{h+1:H+1}}}^{j}\\left[R_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(S_{\\tau})\\right]-r_{\\tau\\times\\bar{F}_{\\theta_{h+1:H+1}}}^ \n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The first term can be bounded by Eq. (75), if we are under event ${\\mathcal{E}}_{1}$ . For the second and third term we make use of Lemma J.5, which ensures that for any sequence $(b_{j})_{j\\in[n]}$ such that $|b_{j}|\\leq c\\in\\mathbb{R}$ the following holds ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{j\\in[n]}\\phi_{h}^{j}b_{j}\\right\\|_{X_{h}^{-1}}\\leq c\\sqrt{n}\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For the second term and third term the respective $b_{j}$ terms can be bounded by using Eq. (76), giving us $c=7\\sqrt{2d}H^{2}L_{1}\\xi/\\alpha$ ", "page_idx": 31}, {"type": "text", "text": "Putting the above three bounds together we have that under event ${\\mathcal{E}}_{1}$ , which occurs with probability at least $1-\\delta/3$ , for all $h\\in[H]$ , for all $G\\in\\mathbf{G}$ , and $\\theta_{h+1:H+1}\\in\\mathcal{B}(\\tilde{L_{2}})^{H-h+1}$ , it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\|\\sum_{j\\in[n]}\\phi_{h}^{j}\\!\\left(\\!\\frac{\\mathbb{E}}{\\tau\\!\\times\\!F_{\\ell,h+1}^{j}}\\!\\left[r_{h:\\tau-1}^{j}\\!+\\!\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau}^{j})\\right]-\\!\\frac{\\mathbb{E}}{\\tau\\!\\times\\!\\tau_{s^{\\prime},h}^{j}\\!\\exp_{s,h}^{j}\\!\\exp_{s,h+1}}\\big[R_{h:\\tau-1}\\!+\\!\\bar{v}_{\\theta_{h+1:H+1}}(S\\!-\\!\\bar{v})\\big]\\!\\right)\\right\\|_{\\tau^{\\prime}}}\\\\ &{\\leq\\sqrt{2H^{2}\\log\\left(\\frac{3H(1+2L_{2}\\bar{L}_{2}/\\xi)d H(d_{0}+1)}{\\delta}\\right)+\\log\\!\\left(\\sqrt{\\frac{\\operatorname*{det}(X_{h})}{\\operatorname*{det}(\\lambda I)}}\\right)}+14\\sqrt{n}\\sqrt{2d}H^{2}L_{1}\\xi/\\alpha}\\\\ &{=H\\sqrt{2d H(d_{0}+1)\\log\\left(1+2L_{2}\\bar{L}_{2}/\\xi\\right)}+\\log(\\operatorname*{det}(X_{h}))-d\\log(\\lambda)+\\log\\!\\left(\\frac{3H}{\\delta}\\right)+14\\sqrt{n}\\sqrt{2d}}\\\\ &{\\leq H\\sqrt{2d H(d_{0}+1)\\log\\left(1+2L_{2}\\bar{L}_{2}/\\xi\\right)}+d\\log(\\lambda+n L_{1}^{2}/d)-d\\log(\\lambda)+\\log\\!\\left(\\frac{3H}{\\delta}\\right)+14\\sqrt{n}\\sqrt{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last inequality used the Determinant-Trace Inequality (see Lemma 10 in [Abbasi- Yadkori et al., 2011l). Setting $\\dot{\\xi^{-1}}=14\\sqrt{n}\\sqrt{2d}H^{2}L_{1}\\alpha^{-1}$ , we get that the above display is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\leq H\\sqrt{2d H(d_{0}+1)\\log\\Bigl(1+28\\sqrt{2d}H^{2}L_{2}\\tilde{L_{2}}L_{1}\\alpha^{-1}\\Bigr)+d\\log(\\lambda+n L_{1}^{2}/d)-d\\log(\\lambda)+\\log\\biggl(\\frac{3H}{\\delta}\\biggr)}}\\\\ &{\\leq\\bar{\\beta}=\\tilde{\\mathcal{O}}\\Bigl(H^{3/2}d\\Bigr)\\;.}&{(77)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The last equality holds by plugging in parameter values according to Appendix A. ", "page_idx": 31}, {"type": "text", "text": "1  Lemmas Related to Covering G ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Lemma I.1. There is an event $\\mathcal{E}_{2}$ \uff0cthat occurs with probability at least $1-\\delta/3$ \uff0csuch that under event $\\mathcal{E}_{2}$ for all $G\\in\\mathbf{G}$ ,and for all $h\\in[H]$ it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{(S,A)\\sim\\mu_{h}}\\left[\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(S,A)-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(S,A)\\right]-\\displaystyle\\frac{1}{n}\\sum_{i\\in[n]}\\left(\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s_{h}^{i},a_{h}^{i})-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s_{h}^{i},a_{h}^{i})\\right)\\right]}\\\\ &{\\leq\\displaystyle\\frac{H}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|C_{\\xi}^{\\mathbf{G}}|}{\\delta}\\right)}+2L_{\\xi}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $|C_{\\xi}^{\\mathbf{G}}|,L_{\\xi}$ , are defined in Eqs. (33) and (34). ", "page_idx": 32}, {"type": "text", "text": "Proof. Let $G\\in\\mathbf{G}$ be a feasible solution to Optimization Problem 1. By the first result in Lemma I.2, there exists a set $C_{\\xi}^{\\mathbf{G}}\\subset\\mathbf{G}$ such that, there exists a $\\tilde{G}\\in C_{\\xi}^{\\mathbf{G}}$ such that, for any $h\\,\\in\\,[H],(s,a)\\,\\in$ $S_{h}\\times A$ , it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\vert\\left(\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s,a)-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s,a)\\right)-\\left(\\operatorname*{max}_{\\theta\\sim\\in\\Theta_{\\bar{G},h}}\\bar{q}_{\\theta\\sim}(s,a)-\\operatorname*{min}_{\\theta\\in\\Theta_{\\bar{G},h}}\\bar{q}_{\\theta\\sim}(s,a)\\right)\\right\\vert\\le L_{\\xi}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Select $\\tilde{G}$ as defined above. Let $h\\in[H]$ . Using Eq. (78), we know that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}\\left[\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(S,A)-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(S,A)\\right]-\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}\\left[\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta\\sim}(S,A)-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta\\sim}(S,A)\\right]\\right]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To bound the second term in the absolute value of Eq. (79) to its empirical mean, we can use the second result in Lemma I.2, which gives us that under event $\\mathcal{E}_{2}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}\\!\\left[\\underset{\\theta\\sim\\in\\Theta_{\\tilde{\\mathcal{G}},h}}{\\operatorname*{max}}\\bar{\\bar{q}}_{\\theta\\sim}(S,A)-\\underset{\\theta\\sim\\in\\Theta_{\\tilde{\\mathcal{G}},h}}{\\operatorname*{min}}\\bar{\\bar{q}}_{\\theta\\sim}(S,A)\\right]-\\frac{1}{n}\\underset{i\\in[n]}{\\sum}\\left(\\underset{\\theta\\sim\\in\\Theta_{\\tilde{\\mathcal{G}},h}}{\\operatorname*{max}}\\bar{\\bar{q}}_{\\theta\\sim}(s_{h}^{i},a_{h}^{i})-\\underset{\\theta\\sim\\in\\Theta_{\\tilde{\\mathcal{G}},h}}{\\operatorname*{min}}\\bar{\\bar{q}}_{\\theta\\sim}(S,A)\\right)-\\underset{(S,A)\\sim\\mu_{h}}{\\operatorname*{min}}}\\\\ &{\\leq\\frac{H}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|C_{\\xi}^{\\mathbf{G}}|}{\\delta}\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We can relate the $\\tilde{G}$ in the second term of the absolute value in Eq. (80) back to $G$ , by once again using Eq. (78), to get that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i\\in[n]}\\left(\\operatorname*{max}_{\\theta\\sim\\in\\Theta_{\\hat{G},h}}\\bar{q}_{\\theta\\sim\\left(s_{h}^{i},a_{h}^{i}\\right)}-\\operatorname*{min}_{\\theta\\sim\\in\\Theta_{\\hat{G},h}}\\bar{q}_{\\theta\\sim\\left(s_{h}^{i},a_{h}^{i}\\right)}\\right)-\\frac{1}{n}\\sum_{i\\in[n]}\\left(\\operatorname*{max}_{\\theta\\in\\Theta_{\\hat{G},h}}\\bar{q}_{\\theta\\sim\\theta_{\\hat{G},h}}+\\operatorname*{min}_{\\theta\\in\\Theta_{\\hat{G},h}}\\bar{q}_{\\theta\\sim\\left(s_{h}^{i},a_{h}^{i}\\right)}\\right)\\right|\\ .\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Putting together Eqs. (79) to (81), and noting that $h$ was arbitrary, gives the desired result. ", "page_idx": 32}, {"type": "text", "text": "Lemma I.2. Let $\\xi\\,>\\,0$ There exists a set $C_{\\xi}^{\\mathbf{G}}\\,\\subset\\,\\mathbf{G}$ such that, for any $G\\,\\in\\,\\mathbf{G}$ , there exists $a$ $\\tilde{G}\\in C_{\\xi}^{\\mathbf{G}}$ such hat, for ary $h\\in[H],(s,a)\\in S_{h}\\times A,$ it holds that $\\left\\vert\\left(\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s,a)-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s,a)\\right)-\\left(\\operatorname*{max}_{\\theta\\sim\\in\\Theta_{\\bar{G},h}}\\bar{q}_{\\theta\\sim\\big(s,a\\big)}-\\operatorname*{min}_{\\theta\\sim\\in\\Theta_{\\bar{G},h}}\\bar{q}_{\\theta\\sim\\big(s,a\\big)}\\right)\\right\\vert\\le L_{\\xi}\\,,$ ", "page_idx": 32}, {"type": "text", "text": "where $|C_{\\xi}^{\\mathbf{G}}|,L_{\\xi}$ are defined in Eqs. (33) and (34). Furthermore, there is an event $\\mathcal{E}_{2}$ which occurs with probablity at least $1-\\delta/3$ such that under event $\\mathcal{E}_{2}$ for any $\\tilde{G}\\in C_{\\xi}^{\\mathbf{G}}$ and $h\\in[H],$ it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}\\!\\left[\\underset{\\theta\\sim\\in\\Theta_{\\tilde{\\mathcal{O}},h}}{\\operatorname*{max}}\\bar{\\bar{q}}_{\\theta\\sim}(S,A)-\\underset{\\theta\\sim\\in\\Theta_{\\tilde{\\mathcal{O}},h}}{\\operatorname*{min}}\\bar{\\bar{q}}_{\\theta\\sim}(S,A)\\right]-\\frac{1}{n}\\underset{i\\in[n]}{\\sum}\\left(\\underset{\\theta\\sim\\in\\Theta_{\\tilde{\\mathcal{O}},h}}{\\operatorname*{max}}\\bar{\\bar{q}}_{\\theta\\sim}(s_{h}^{i},a_{h}^{i})-\\underset{\\theta\\sim\\in\\Theta_{\\tilde{\\mathcal{O}},h}}{\\operatorname*{min}}\\bar{\\bar{q}}_{\\theta\\sim}(S,A)\\right)-\\underset{s}{\\operatorname*{min}}\\int_{\\mathbb{D}}\\left(\\underset{s}{\\operatorname*{max}}\\right)}\\\\ &{\\leq\\frac{H}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|C_{\\xi}^{\\mathbf{G}}|}{\\delta}\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Let $\\xi\\,>\\,0,\\kappa_{t}\\,\\geq\\,0,\\forall t\\,\\in\\,[2\\,:\\,H+1]$ . By Lemma I.4, there exists a set $C_{\\xi}^{\\mathbf{G}}\\,\\subset\\,\\mathbf{G}$ with $|C_{\\xi}^{\\mathbf{G}}|\\,\\le\\,(1+2L_{2}/\\xi))^{d H d_{0}}$ such tht, for any $G\\in\\mathbf{G}$ there exists a $\\tilde{G}\\,\\in\\,C_{\\xi}^{\\mathbf{G}}$ such tht for any $h\\in[H]$ , for any $\\theta_{h+1:H+1},\\theta_{h+1:H+1}^{\\sim}\\in\\mathcal{B}(\\tilde{L_{2}})^{H-h+1}$ , such that for all $t\\in[h+1:H+1],s\\in$ $S_{t},\\left|\\bar{v}_{\\theta_{t}}(s)-\\bar{v}_{\\theta_{t}^{\\sim}}(s)\\right|\\le\\kappa_{t}$ , and for any $j\\in[n]$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\underset{\\tau\\sim F_{G,h+1}^{j}}{\\mathbb{E}}\\!\\left[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau})\\right]-\\underset{\\tau\\sim F_{\\bar{G},h+1}^{j}}{\\mathbb{E}}\\!\\left[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}^{\\sim}}(s_{\\tau})\\right]\\right|}\\\\ &{\\leq(H-h+1)6\\sqrt{2d}H L_{1}\\xi/\\alpha+\\displaystyle\\sum_{t=h}^{H}\\kappa_{t+1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For the remainder of the proof let $G,{\\tilde{G}}$ be as described above. ", "page_idx": 33}, {"type": "text", "text": "We first show the following intermediate result. ", "page_idx": 33}, {"type": "text", "text": "Lemma I.3. For all $h\\in[H].$ it holds that: ", "page_idx": 33}, {"type": "text", "text": "1. For any $\\hat{\\theta}_{h}\\in\\hat{\\Theta}_{G,h},\\theta_{h}\\in\\Theta_{G,h}$ there exists $\\hat{\\theta}_{h}^{\\sim}\\in\\hat{\\Theta}_{\\tilde{G},h},\\theta_{h}^{\\sim}\\in\\Theta_{\\tilde{G},h}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{\\theta}_{h}-\\hat{\\theta}_{h}^{\\sim}\\right\\|_{X_{h}}\\le c_{h}^{\\xi},\\quad\\|\\theta_{h}-\\theta_{h}^{\\sim}\\|_{X_{h}}\\le c_{h}^{\\xi}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "2. For any $\\hat{\\theta}_{h}^{\\sim}\\in\\hat{\\Theta}_{\\tilde{G},h},\\theta_{h}^{\\sim}\\in\\Theta_{\\tilde{G},h},$ there exists $\\hat{\\theta}_{h}\\in\\hat{\\Theta}_{G,h},\\theta_{h}\\in\\Theta_{G,h}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{\\theta}_{h}-\\hat{\\theta}_{h}^{\\sim}\\right\\|_{X_{h}}\\le c_{h}^{\\xi},\\quad\\|\\theta_{h}-\\theta_{h}^{\\sim}\\|_{X_{h}}\\le c_{h}^{\\xi}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where, ", "page_idx": 33}, {"type": "equation", "text": "$$\nc_{h}^{\\xi}=6\\sqrt{n}\\sqrt{2d}H^{2}L_{1}\\xi\\alpha^{-1}\\Bigl(1+\\sqrt{n}L_{1}\\tilde{L_{2}}/(H^{3/2}d)\\Bigr)^{H-h}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Proof of result 1.: To show Eq. (83) we will use induction. The base case is when $h=H$ forwhich ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\hat{\\Theta}_{G,H}=\\hat{\\Theta}_{\\tilde{G},H}=\\left\\{X_{H}^{-1}\\sum_{j\\in[n]}\\phi_{H}^{j}r_{H}^{j}\\right\\},\\implies\\Theta_{G,H}=\\Theta_{\\tilde{G},H}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, for any $\\widehat{\\theta}_{H}\\in\\widehat{\\Theta}_{G,H},\\theta_{H}\\in\\Theta_{G,H}$ , select $\\widehat{\\theta}_{H}^{\\sim}=\\widehat{\\theta}_{H}\\in\\widehat{\\Theta}_{\\tilde{G},H},\\theta_{H}^{\\sim}=\\theta_{H}\\in\\Theta_{\\tilde{G},H}.$ Then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\hat{{\\boldsymbol\\theta}}_{H}-\\hat{{\\boldsymbol\\theta}}_{H}^{\\sim}\\right\\rVert_{{\\boldsymbol X}_{H}}\\leq0,\\quad\\left\\lVert{\\boldsymbol\\theta}_{H}-{\\boldsymbol\\theta}_{H}^{\\sim}\\right\\rVert_{{\\boldsymbol X}_{H}}\\leq0\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now, we show the inductive step. Let $h\\,\\in\\,[H\\mathrm{~-~}1]$ be arbitrary. Assume Eq. (83) holds for any $t\\in[h+1:H]$ . We prove that Eq. (83) also holds for $h$ . Let $\\hat{\\theta}_{h}\\in\\hat{\\Theta}_{G,h}$ be arbitrary. Notice that $\\hat{\\theta}_{h}$ must have the following form. ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\stackrel{1}{r}_{h}=X_{h}^{-1}\\sum_{j\\in[n]}\\phi_{h}^{j}\\underset{\\tau\\sim F_{G,h+1}^{j}}{\\mathbb{E}}\\left[r_{h:\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}\\left(s_{\\tau}^{j}\\right)\\right]\\in\\hat{\\Theta}_{G,h},\\mathrm{~for~some~}\\theta_{h+1:H+1}\\in\\Theta_{G,h+1}\\times\\cdot\\cdot.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Select $\\theta_{h+1:H+1}^{\\sim}\\in\\Theta_{\\tilde{G},h+1}\\times\\cdots\\times\\Theta_{\\tilde{G},H+1}$ such that for all $t\\in[h+1:H+1]$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\lVert{\\boldsymbol{\\theta}}_{t}-{\\boldsymbol{\\theta}}_{t}^{\\sim}\\rVert_{X_{t}}\\leq c_{t}^{\\xi}\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which exists by the inductive hypothesis (Eq. (83)) and since $\\Theta_{G,H+1}=\\Theta_{\\tilde{G},H+1}=\\{\\vec{0}\\}$ .Define ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{h}^{\\sim}=X_{h}^{-1}\\sum_{j\\in[n]}\\phi_{h}^{j}\\underset{\\tau\\sim F_{\\tilde{G},h+1}^{j}}{\\mathbb{E}}\\Big[r_{h:\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}^{\\sim}}\\big(s_{\\tau}^{j}\\big)\\Big]\\in\\hat{\\Theta}_{\\tilde{G},h}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Recall that we aim tobound $\\left\\|\\hat{\\theta}_{h}-\\hat{\\theta}_{h}^{\\sim}\\right\\|_{X_{h}}$ Plugging in theexpressions for $\\hat{\\theta}_{h},\\hat{\\theta}_{h}^{\\sim}$ , as defined above, we get that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{-\\hat{\\theta}_{h}^{\\sim}\\bigg\\|_{X_{h}}=\\left\\|X_{h}^{-1}\\sum_{j\\in[n]}\\phi_{h}^{j}\\left(\\underbrace{\\mathbb{E}}_{\\tau\\sim F_{G,h+1}^{j}}\\left[r_{h:\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}\\left(s_{\\tau}^{j}\\right)\\right]-\\underbrace{\\mathbb{E}}_{\\tau\\sim F_{G,h+1}^{j}}\\left[r_{h:\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}^{\\sim}}\\left(s_{\\tau}^{j}\\right)\\right]\\right)\\right\\|_{\\tau}}}\\\\ &{}&{=\\left\\|X_{h}^{-1}\\sum_{j\\in[n]}\\phi_{h}^{j}b_{j}\\right\\|_{X_{h}}=\\left\\|\\sum_{j\\in[n]}\\phi_{h}^{j}b_{j}\\right\\|_{X_{h}^{-1}},}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where in the second equality we let $\\begin{array}{r l r}{b_{j}}&{{}=}&{\\mathbb{E}_{\\tau\\sim F_{G,h+1}^{j}}\\Big[r_{h:\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}}\\big(s_{\\tau}^{j}\\big)\\Big]\\begin{array}{r l}{-}\\end{array}$ $\\mathbb{E}_{\\tau\\sim F_{\\tilde{G},h+1}^{j}}\\Big[r_{h:\\tau-1}^{j}+\\bar{v}_{\\theta_{h+1:H+1}^{\\sim}}\\big(s_{\\tau}^{j}\\big)\\Big]$ . To bound the above term we can make use of Lemma J.5, which ensures that for any sequence $(b_{j})_{j\\in[n]}$ such that $|b_{j}|\\leq a\\in\\mathbb{R}$ the following holds ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{j\\in[n]}\\phi_{h}^{j}b_{j}\\right\\|_{X_{h}^{-1}}\\leq a\\sqrt{n}\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, we are left to bound $|b_{j}|$ . To do so, we can make use of Eq. (82), which requires us to bound $\\left|\\bar{v}_{\\theta_{t}}(s)-\\bar{v}_{\\theta_{t}^{\\sim}}(s)\\right|$ for all $t\\in[h+1:H],s\\in S_{t}$ , which can be done as follows. ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\bar{v}_{\\theta_{t}}(s)-\\bar{v}_{\\theta_{t}^{\\sim}}(s)\\right|=\\left|\\exp_{[0,H]}\\operatorname*{max}_{a\\in\\mathcal A}\\langle\\phi(s,a),\\theta_{t}\\rangle-\\exp_{[0,H]}\\operatorname*{max}_{a\\in\\mathcal A}\\langle\\phi(s,a),\\theta_{t}^{\\sim}\\rangle\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\left|\\operatorname*{max}_{a\\in\\mathcal A}\\langle\\phi(s,a),\\theta_{t}-\\theta_{t}^{\\sim}\\rangle\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\operatorname*{max}_{a\\in\\mathcal A}\\lVert\\phi(s,a)\\rVert_{X_{t}^{-1}}\\lVert\\theta_{t}-\\theta_{t}^{\\sim}\\rVert_{X_{t}}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac{L_{1}}{\\sqrt{\\lambda}}c_{t}^{\\xi}=L_{1}\\tilde{L}_{2}c_{t}^{\\xi}/(H^{3/2}d)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The second inequality uses the Cauchy-Schwarz inequality. The third inequality used Eq. (22), that $\\lambda_{\\operatorname*{max}}(X_{h}^{-1})\\ \\stackrel{\\cdot}{\\leq}\\ 1/\\lambda$ (by definition of $X_{h}$ Eq. (11), and Eq. (86). The last equality used that $\\sqrt{\\lambda}=H^{3/2}d/\\tilde{L_{2}}$ (Eq (25) Plugging Eq. (87) into Eq. (82) we get that for any $j\\in[n]$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|b_{j}|=\\Biggl|\\underset{\\tau\\sim F_{G,h+1}^{j}}{\\mathbb{E}}\\Bigl[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}\\bigl(s_{\\tau}\\bigr)\\Bigr]-\\underset{\\tau\\sim F_{G,h+1}^{j}}{\\mathbb{E}}\\Bigl[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}^{\\sim}}\\bigl(s_{\\tau}\\bigr)\\Bigr]\\Biggr|}\\\\ &{\\quad\\le(H-h+1)6\\sqrt{2d}H L_{1}\\xi/\\alpha+\\displaystyle\\sum_{t=h}^{H}L_{1}\\tilde{L}_{2}c_{t+1}^{\\xi}/(H^{3/2}d)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\hat{\\theta}_{h}-\\hat{\\theta}_{h}^{\\sim}\\right\\|_{X_{h}}=\\left\\|\\displaystyle\\sum_{j\\in[n]}\\phi_{h}^{j}b_{j}\\right\\|_{X_{h}^{-1}}\\leq\\left(6\\sqrt{2d}H^{2}L_{1}\\xi\\alpha^{-1}+\\displaystyle\\frac{L_{1}\\tilde{L}_{2}}{H^{3/2}d}\\displaystyle\\sum_{t=h}^{H}c_{t+1}^{\\xi}\\right)\\sqrt{n}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=6\\sqrt{n}\\sqrt{2d}H^{2}L_{1}\\xi\\alpha^{-1}+\\sqrt{n}\\displaystyle\\frac{L_{1}\\tilde{L}_{2}}{H^{3/2}d}\\displaystyle\\sum_{t=h}^{H}c_{t+1}^{\\xi}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=6\\sqrt{n}\\sqrt{2d}H^{2}L_{1}\\xi\\alpha^{-1}\\Big(1+\\sqrt{n}L_{1}\\tilde{L}_{2}/(H^{3/2}d)\\Big)^{H-h}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=c_{h}^{\\xi}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To see why the second last equality is true, let $\\begin{array}{r l r}{x}&{{}=}&{6\\sqrt{n}\\sqrt{2d}H^{2}L_{1}\\xi\\alpha^{-1}}\\end{array}$ and $y=$ $\\sqrt{n}L_{1}\\tilde{L_{2}}/(H^{3/2}d)$ Then, for any $t\\in[h:H],c_{t}^{\\xi}=x(1+y)^{H-t}$ (by Eq. (85)) and ", "page_idx": 34}, {"type": "equation", "text": "$$\n6\\sqrt{n}\\sqrt{2d}H^{2}L_{1}\\xi\\alpha^{-1}+\\sqrt{n}\\frac{L_{1}\\tilde{L_{2}}}{H^{3/2}d}\\sum_{t=h}^{H}c_{t+1}^{\\xi}=x+y\\sum_{t=h+1}^{H}x(1+y)^{H-t}\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Notice that the sum can be rewritten as a finite geometric series. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{t=h+1}^{H}x(1+y)^{H-t}=x\\sum_{k=0}^{H-h-1}(1+y)^{k}=x\\frac{(1+y)^{H-h}-1}{y}\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, ", "page_idx": 35}, {"type": "equation", "text": "$$\nx+y\\sum_{t=h+1}^{H}x(1+y)^{H-t}=x+y\\cdot x\\frac{(1+y)^{H-h}-1}{y}=x(1+y)^{H-h}=c_{h}^{\\xi}\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This proves the first result in Eq. (83) ", "page_idx": 35}, {"type": "text", "text": "Next, we show the second result in Eq. (83). Let $\\theta_{h}\\in\\Theta_{G,h}$ . By the definition of the set $\\Theta_{G,h}$ , there exists a $\\hat{\\theta}_{h}\\in\\hat{\\Theta}_{G,h}$ such that $\\left\\|\\theta_{h}-\\hat{\\theta}_{h}\\right\\|_{X_{h}}\\le\\beta$ . Then, by the first result in Eq. (83) (which we have shown holds for $h$ above), there exists a $\\widehat{\\theta}_{h}^{\\sim}\\in\\widehat{\\Theta}_{\\tilde{G},h}$ , such that $\\left\\|\\widehat{\\theta}_{h}-\\widehat{\\theta}_{h}^{\\sim}\\right\\|_{X_{h}}\\leq c_{h}^{\\xi}$ . Let $\\hat{\\theta}_{h},\\hat{\\theta}_{h}^{\\sim}$ be as defined above, and select $\\theta_{h}^{\\sim}=\\theta_{h}-\\hat{\\theta}_{h}+\\hat{\\theta}_{h}^{\\sim}$ , which is an element of $\\Theta_{\\tilde{G},h}$ since ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|\\theta_{h}^{\\sim}-\\hat{\\theta}_{h}^{\\sim}\\right\\|_{X_{h}}=\\left\\|\\theta_{h}-\\hat{\\theta}_{h}+\\hat{\\theta}_{h}^{\\sim}-\\hat{\\theta}_{h}^{\\sim}\\right\\|_{X_{h}}=\\left\\|\\theta_{h}-\\hat{\\theta}_{h}\\right\\|_{X_{h}}\\le\\beta\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\theta_{h}-\\theta_{h}^{\\sim}\\|_{{\\boldsymbol X}_{h}}=\\left\\|\\theta_{h}-\\theta_{h}-\\hat{\\theta}_{h}+\\hat{\\theta}_{h}^{\\sim}\\right\\|_{{\\boldsymbol X}_{h}}=\\left\\|\\hat{\\theta}_{h}^{\\sim}-\\hat{\\theta}_{h}\\right\\|_{{\\boldsymbol X}_{h}}\\le c_{h}^{\\xi}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This completes the proof of the second result in Eq. (83) ", "page_idx": 35}, {"type": "text", "text": "Proof of result 2.: The proof is identical to that of the proof of result 1., except swapping the roles of $\\hat{\\theta}_{h},\\theta_{h},G$ and $\\bar{\\theta}_{h}^{\\sim},\\theta_{h}^{\\sim},\\tilde{G}$ \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Now, with the results of Eqs. (83) and (84) in hand, we return to proving Lemma I.2. For any $h\\in[H]$ let $\\theta_{h}=\\arg\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\langle\\phi(s,a),\\theta\\rangle$ then, by Eq. (83), there exists a $\\theta_{h}^{\\sim}\\in\\Theta_{\\tilde{G},h}$ such that $\\lVert{\\boldsymbol{\\theta}}_{h}-{\\boldsymbol{\\theta}}_{h}^{\\sim}\\rVert_{X_{t}}\\leq c_{h}^{\\xi}$ . This gives that, for all $h\\in[H],(s,a)\\in S_{h}\\times A$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi(s,a),\\theta\\rangle-\\underset{\\theta\\sim\\in\\Theta_{\\hat{G},h}}{\\operatorname*{max}}\\langle\\phi(s,a),\\theta^{\\sim}\\rangle=\\langle\\phi(s,a),\\theta_{h}\\rangle-\\langle\\phi(s,a),\\theta_{h}^{\\sim}\\rangle+\\langle\\phi(s,a),\\theta_{h}^{\\sim}\\rangle-\\underset{\\theta\\sim\\in\\Theta_{\\hat{G},h}}{\\operatorname*{max}}\\langle\\phi(s,a),\\theta^{\\sim}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\Vert\\phi(s,a)\\Vert_{X_{h}^{-1}}\\Vert\\theta_{h}-\\theta_{h}^{\\sim}\\Vert_{X_{h}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{L_{1}}{\\sqrt{\\lambda}}c_{h}^{\\xi}=L_{1}\\tilde{L}_{2}c_{h}^{\\xi}/(H^{3/2}d)\\,.}\\end{array}\n$$max ),0\\~) DEOG,h ", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The second inequality used Eq. (22), that $\\lambda_{\\operatorname*{max}}(X_{h}^{-1})\\,\\leq\\,1/\\lambda$ (by definition of $X_{h}$ Eq. (11), and Eq. (86). The last equality used that $\\sqrt{\\lambda}\\,=\\,H^{3/2}d/\\tilde{L_{2}}$ (Eq. (25). Now, for the other direction, using simila steps as above, for any $\\textit{h}\\in[H]$ . let $\\begin{array}{r}{\\bar{\\theta}_{h}^{\\sim}\\,=\\,\\arg\\operatorname*{max}_{\\theta^{\\sim}\\in\\Theta_{\\tilde{G},h}}\\langle\\phi(s,a),\\theta^{\\sim}\\rangle}\\end{array}$ then, by Eq. (84), there exists a $\\theta_{h}\\ \\in\\ \\Theta_{G,h}$ such that $\\|\\theta_{h}-\\theta_{h}^{\\sim}\\|_{X_{h}}\\,\\le\\,c_{h}^{\\xi}$ . This gives that, for all $h\\ \\in$ $[H],(s,a)\\in S_{h}\\times A$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{\\sim\\in\\Theta_{\\vec{G},h}}\\langle\\phi(s,a),\\theta^{\\sim}\\rangle-\\operatorname*{max}_{\\theta\\in\\Theta_{\\vec{G},h}}\\langle\\phi(s,a),\\theta\\rangle=\\langle\\phi(s,a),\\theta_{h}^{\\sim}\\rangle-\\langle\\phi(s,a),\\theta_{h}\\rangle+\\langle\\phi(s,a),\\theta_{h}\\rangle-\\operatorname*{max}_{\\theta\\in\\Theta_{\\vec{G},h}}\\langle\\phi(s,a),\\theta_{h}\\rangle}&{{}=\\operatorname*{max}_{\\theta\\in\\Theta_{\\vec{G},h}}\\langle\\phi(s,a),\\theta_{h}\\rangle}\\\\ {\\displaystyle}&{\\leq\\|\\phi(s,a)\\|_{X_{h}^{-1}}\\|\\theta_{h}^{\\sim}-\\theta_{h}\\|_{X_{h}}}\\\\ {\\displaystyle}&{\\leq\\displaystyle\\frac{L_{1}}{\\sqrt{\\lambda}}c_{h}^{\\xi}=L_{1}\\tilde{L}_{2}c_{h}^{\\xi}/(H^{3/2}d)\\,.}\\end{array}\n$$b(s,a), 0) ", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The above two results together imply that, for all $h\\in[H],(s,a)\\in S_{h}\\times A$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\langle\\phi(s,a),\\theta\\rangle-\\operatorname*{max}_{\\theta\\sim\\in\\Theta_{\\tilde{G},h}}\\langle\\phi(s,a),\\theta^{\\sim}\\rangle\\right|\\leq L_{1}\\tilde{L}_{2}c_{h}^{\\xi}/(H^{3/2}d)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Following the same steps as above for min we can get that, for all $h\\in[H],(s,a)\\in S_{h}\\times A$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\langle\\phi(s,a),\\theta\\rangle-\\operatorname*{min}_{\\theta\\sim\\in\\Theta_{\\tilde{G},h}}\\langle\\phi(s,a),\\theta^{\\sim}\\rangle\\right|\\leq L_{1}\\tilde{L}_{2}c_{h}^{\\xi}/(H^{3/2}d)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The above two results together imply that, for all $h\\in[H],(s,a)\\in S_{h}\\times A$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\vert\\left(\\operatorname*{max}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s,a)-\\operatorname*{min}_{\\theta\\in\\Theta_{G,h}}\\bar{q}_{\\theta}(s,a)\\right)-\\left(\\operatorname*{max}_{\\theta\\sim\\in\\Theta_{\\bar{G},h}}\\bar{q}_{\\theta\\sim}(s,a)-\\operatorname*{min}_{\\theta\\sim\\in\\Theta_{\\bar{G},h}}\\bar{q}_{\\theta\\sim}(s,a)\\right)\\right\\vert=L_{\\xi}\\,,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where (by recalling Eq. (85)), ", "page_idx": 36}, {"type": "equation", "text": "$$\nL_{\\xi}=12\\sqrt{2d}H^{2}L_{1}\\xi\\alpha^{-1}\\Big(2\\sqrt{n}L_{1}\\tilde{L_{2}}\\big/(H^{3/2}d)\\Big)^{H}\\geq2L_{1}\\tilde{L_{2}}c_{h}^{\\xi}\\big/(H^{3/2}d)\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This concludes the proof of the first result in Lemma I.2. ", "page_idx": 36}, {"type": "text", "text": "Now we prove the second result in Lemma I.2. For any $\\tilde{G}\\in C_{\\xi}^{\\mathbf{G}},h\\in[H]$ , define the event ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{E}_{2}^{\\tilde{G},h}=\\left\\{\\left|\\underset{(S,A)\\sim\\mu_{h}}{\\mathbb{E}}\\left[\\underset{\\theta\\sim\\in\\Theta_{\\tilde{G},h}}{\\operatorname*{max}}\\bar{q}_{\\theta\\sim}(S,A)-\\underset{\\theta\\sim\\in\\Theta_{\\tilde{G},h}}{\\operatorname*{min}}\\bar{q}_{\\theta\\sim}(S,A)\\right]\\right.\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n-\\left.\\frac{1}{n}\\sum_{i\\in[n]}\\left(\\operatorname*{max}_{\\theta\\sim\\in\\Theta_{\\bar{G},h}}\\bar{q}_{\\theta\\sim\\big(s_{h}^{i},a_{h}^{i}\\big)}-\\operatorname*{min}_{\\theta\\sim\\in\\Theta_{\\bar{G},h}}\\bar{q}_{\\theta\\sim\\big(s_{h}^{i},a_{h}^{i}\\big)}\\right)\\right|\\le\\frac{H}{\\sqrt{n}}\\sqrt{\\log\\left(\\frac{6H|C_{\\xi}^{\\mathbf{G}}|}{\\delta}\\right)}\\right\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, since $\\begin{array}{r}{\\operatorname*{max}_{\\theta\\sim\\in\\Theta_{\\tilde{G},h}}\\bar{q}_{\\theta\\sim}(s,a)-\\operatorname*{min}_{\\theta\\sim\\in\\Theta_{\\tilde{G},h}}\\bar{q}_{\\theta\\sim}(s,a)\\,\\in\\,[0,H]}\\end{array}$ for all $(s,a)\\in\\ensuremath{\\mathcal{S}}_{h}\\times\\ensuremath{\\mathcal{A}}$ by Hoeffdins inequality Lma1),we have that, fr ay $\\tilde{G}\\,\\in\\,C_{\\xi}^{\\mathbf{G}},h\\,\\in\\,[H]$ event $\\mathcal{E}_{2}^{\\tilde{G},h}$ occurs with probability at least $1-\\delta/(3H|C_{\\xi}^{\\bf G}|)$ . Let ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathscr{E}_{2}=\\bigcap_{\\tilde{G}\\in C_{\\xi}^{\\mathbf{G}},h\\in[H]}\\mathscr{E}_{2}^{\\tilde{G},h}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, by applying a union bound over $\\tilde{G},h$ we have that the event $\\mathcal{E}_{2}$ occurs with probability at least $1-\\delta/3$ \u53e3 ", "page_idx": 36}, {"type": "text", "text": "Lemma I.4. Let $\\xi>0,\\kappa_{t}\\geq0,\\forall t\\in[2:H\\!+\\!1]$ Then, there exists a set $C_{\\xi}^{\\mathbf{G}}\\subset\\mathbf{G}\\,w i t h\\,|C_{\\xi}^{\\mathbf{G}}|\\leq(1+$ $2L_{2}/\\xi))^{d H d_{0}}$ such that, for any $G\\in\\mathbf{G}$ there exists a $\\tilde{G}\\in C_{\\xi}^{\\mathbf{G}}$ such that, for any $h\\in[H]$ for any $u\\in[h]$ and trajectory raj $\\mathbf{\\Phi}=(s_{t},a_{t},r_{t})_{t\\in[u,H+1]},$ and for any $\\theta_{h+1:H+1},\\theta_{h+1:H+1}^{\\sim}\\in\\mathcal{B}(\\tilde{L_{2}})^{H-h+1}$ that areclose in predictions, that is,such that forall $t\\in[h+1:H+1],s\\in{\\mathcal S}_{t},\\left|\\bar{v}_{\\theta_{t}}(s)-\\bar{v}_{\\theta_{t}^{\\sim}}(s)\\right|\\leq$ $\\kappa_{t}$ ,itholdsthat ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\underset{\\tau\\sim F_{G,\\mathrm{mj},h+1}}{\\mathbb{E}}\\!\\left[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau})\\right]-\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{mj},h+1}}{\\mathbb{E}}\\!\\left[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau})\\right]\\right|}\\\\ &{\\leq(H-h+1)6\\sqrt{2d}H L_{1}\\xi/\\alpha+\\displaystyle\\sum_{t=h}^{H}\\kappa_{t+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbf{G}=\\left\\{(G_{h})_{h\\in[2:H]}=(\\vartheta_{h}^{i})_{h\\in[2:H],i\\in[d_{0}]}:\\,\\mathrm{for~all~}h\\in[2:H],i\\in[d_{0}],\\vartheta_{h}^{i}\\in B(L_{2})\\right\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By Lemma J.2, we know there exists a set $C_{\\xi}\\,\\,\\subset\\,B(a),a,\\xi\\,>\\,0$ with $|C_{\\xi}|\\,=\\,(1+2a/\\xi)^{d}$ such that for any $x\\ \\in\\ {\\cal B}(a)$ therexstsa $y\\ \\in\\ C_{\\xi}$ such that $\\|x-y\\|_{2}\\,\\leq\\,\\xi$ Definethe set $C_{\\xi}^{\\mathbf{G}}\\ =$ $\\begin{array}{r}{\\ X_{h\\in[2:H],i\\in[d_{0}]}\\,C_{\\xi}\\,\\subset\\,\\mathbf{G}}\\end{array}$ with $|C_{\\xi}^{\\mathbf{G}}|\\leq(1+2L_{2}/\\xi))^{d H d_{0}}$ . Then, for any $G=(\\vartheta_{h}^{i})_{h\\in[2:H],i\\in[d_{0}]}\\in$ $\\mathbf{G}$ , there exists a $\\tilde{G}=(\\tilde{\\vartheta}_{h}^{i})_{h\\in[2:H],i\\in[d_{0}]}\\in C_{\\xi}^{\\mathbf{G}}$ such that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\|\\vartheta_{h}^{i}-\\tilde{\\vartheta}_{h}^{i}\\right\\|_{2}\\leq\\xi\\quad\\mathrm{for~all~}h\\in[2:H],i\\in[d_{0}]\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let $G,{\\tilde{G}}$ be as defined above. Then, for all $s\\in\\mathcal S\\setminus S_{1}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{range}^{G}(s)-\\mathrm{range}^{\\tilde{G}}(s)\\vert=\\biggl\\vert\\displaystyle\\operatorname*{max}_{k\\in[d_{0}]}\\displaystyle\\operatorname*{max}_{a,\\,a^{\\prime}\\in A}\\left\\langle\\phi(s,a,a^{\\prime}),\\vartheta_{\\mathrm{stage}(s)}^{k}\\right\\rangle-\\displaystyle\\operatorname*{max}_{k\\in[d_{0}]}\\displaystyle\\operatorname*{max}_{a,\\,a^{\\prime}\\in A}\\left\\langle\\phi(s,a,a^{\\prime}),\\tilde{\\vartheta}_{\\mathrm{stage}(s)}^{k}\\right\\rangle}\\\\ &{\\leq\\displaystyle\\bigg\\vert\\operatorname*{max}_{k\\in[d_{0}]}\\displaystyle\\operatorname*{max}_{a,\\,a^{\\prime}\\in A}\\left\\langle\\phi(s,a,a^{\\prime}),\\vartheta_{\\mathrm{stage}(s)}^{k}-\\tilde{\\vartheta}_{\\mathrm{stage}(s)}^{k}\\right\\rangle\\bigg\\vert}\\\\ &{\\leq\\displaystyle\\operatorname*{max}_{k\\in[d_{0}]}\\displaystyle\\operatorname*{max}_{a,a^{\\prime}\\in A}\\left\\Vert\\phi(s,a,a^{\\prime})\\right\\Vert_{2}\\left\\Vert\\vartheta_{\\mathrm{stage}(s)}^{k}-\\tilde{\\vartheta}_{\\mathrm{stage}(s)}^{k}\\right\\Vert_{2}}\\\\ &{\\leq2L_{1}\\xi,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and, since we have by definition (Eq. (6)) that $\\omega_{G}$ is a smooth function in terms of rangeG, we get that ", "page_idx": 37}, {"type": "equation", "text": "$$\n|\\omega_{G}(s)-\\omega_{\\tilde{G}}(s)|\\leq\\left|2-\\frac{\\sqrt{2d}\\cdot\\mathrm{range}^{G}(s)}{\\alpha}-\\left(2-\\frac{\\sqrt{2d}\\cdot\\mathrm{range}^{\\tilde{G}}(s)}{\\alpha}\\right)\\right|\\leq\\frac{2\\sqrt{2d}L_{1}\\xi}{\\alpha}\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For all $\\textit{b}\\in\\:[H]$ let $\\theta_{h+1:H+1},\\theta_{h+1:H+1}^{\\sim}~\\in~\\mathcal{B}(\\tilde{L_{2}})^{H-h+1}$ , such that for all $t\\ \\in\\ [h\\,+\\,1\\ :$ $H\\,+\\,1],s\\;\\;\\in\\;\\;{\\cal S}_{t},\\left|\\bar{v}_{\\theta_{t}}(s)-\\bar{v}_{\\theta_{t}^{\\sim}}(s)\\right|\\;\\le\\;\\kappa_{t}$ .Then, for any $h~\\in~[H],u~\\in~[h]$ ,and trajectory $\\mathrm{traj}=(s_{t},a_{t},r_{t})_{t\\in[u:H+1]}$ , it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\tau\\sim F_{G,\\mathrm{up},h+1}}{\\mathbb{E}}\\big[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau})\\big]}\\\\ &{=r_{h}+(1-\\omega_{G}(s_{h+1}))\\bar{v}_{{\\theta_{h+1}}}(s_{h+1})+\\omega_{G}(s_{h+1})\\big(r_{h+1}+(1-\\omega_{G}(s_{h+2}))v_{\\theta_{h+2}}(s_{h+2})\\big)+...\\ }\\\\ &{=r_{h}+(1-\\omega_{G}(s_{h+1}))\\bar{v}_{{\\theta_{h+1}}}(s_{h+1})+\\omega_{G}(s_{h+1})\\underset{\\tau\\sim F_{G,\\mathrm{up},h+2}}{\\mathbb{E}}\\big[r_{h+1:\\tau-1}+\\bar{v}_{\\theta_{h+2:H+1}}(s_{\\tau})\\big]\\ .\\ \\ (9)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using similar steps to above it can be shown that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{trij},h+1}}{\\mathbb{E}}\\Big[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}^{\\sim}}\\big(s_{\\tau}\\big)\\Big]}\\\\ &{=r_{h}+(1-\\omega_{\\tilde{G}}(s_{h+1}))\\bar{v}_{{\\theta}_{h+1}}(s_{h+1})+\\omega_{\\tilde{G}}\\big(s_{h+1}\\big)\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{trij},h+2}}{\\mathbb{E}}\\Big[r_{h+1:\\tau-1}+\\bar{v}_{\\theta_{h+2:H+1}^{\\sim}}\\big(s_{\\tau}\\big)\\Big]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We claim that for any $\\u\\in[H],u\\in[h],\\mathop{\\mathrm{and~trajectory~traj}}=(s_{t},a_{t},r_{t})_{t\\in[u:H+1]}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\underset{\\tau\\sim F_{G,\\mathrm{mj},h+1}}{\\mathbb{E}}\\!\\left[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}(s_{\\tau})\\right]-\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{mj},h+1}}{\\mathbb{E}}\\!\\left[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}^{\\sim}}(s_{\\tau})\\right]\\right|}\\\\ &{\\leq(H-h+1)6\\sqrt{2d}H L_{1}\\xi/\\alpha+\\displaystyle\\sum_{t=h}^{H}\\kappa_{t+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "To show Eq. (93) we will use induction on $h$ . The base case is when $h=H$ , for which ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\underset{\\tau\\sim F_{G,\\mathrm{traj},H+1}}{\\mathbb{E}}\\!\\left[r_{H:\\tau-1}+\\bar{v}_{\\theta_{H+1:H+1}}(s_{\\tau})\\right]-\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{traj},H+1}}{\\mathbb{E}}\\!\\left[r_{H:\\tau-1}+\\bar{v}_{\\theta_{H+1:H+1}}(s_{\\tau})\\right]\\right|}\\\\ &{=\\left|\\underset{\\tau\\sim F_{G,\\mathrm{traj},H+1}}{\\mathbb{E}}\\!\\left[r_{H:\\tau-1}\\right]-\\underset{\\tau\\sim F_{\\tilde{G},\\mathrm{traj},H+1}}{\\mathbb{E}}\\!\\left[r_{H:\\tau-1}\\right]\\right|=0\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the first equality holds since $\\Theta_{G,H+1}=\\Theta_{\\tilde{G},H+1}=\\{\\vec{0}\\}$ (defined in Eq. (13). The second equality holds since for any $u\\in[H+1]$ , and trajectory traj $\\mathbf{\\lambda}=(s_{t},a_{t},r_{t})_{t\\in[u:H+1]}$ \uff0c $F_{G,\\mathrm{traj},H+1}(\\tau=$ $H+1)=F_{\\tilde{G},\\mathrm{traj},H+1}(\\tau=H+1)=1$ ", "page_idx": 37}, {"type": "text", "text": "Now, we show the inductive step. Let $h\\,\\in\\,[H\\mathrm{~-~}1]$ be arbitrary. Assume Eq. (93) holds for any $t\\in[h+1,H]$ . We prove that Eq. (93) also holds for $h$ . Fix any $u\\in[H+1]$ , and trajectory traj $=$ $(s_{t},a_{t},r_{t})_{t\\in[u:H+1]}$ To shorten notation let $E_{G,\\theta,h}~=~\\mathbb{E}_{\\tau\\sim F_{G,\\mathrm{traj},h+1}}\\Big[r_{h:\\tau-1}+\\bar{v}_{\\theta_{h+1:H+1}}\\big(s_{\\tau}\\big)\\Big]$ (and similar for $E_{\\tilde{G},\\theta^{\\sim},h})$ . Then using the assumption that for all $u\\;\\in\\;[2\\;:\\;H\\,+\\,1],s\\;\\;\\in$ $S_{u},\\left|\\bar{v}_{\\theta_{u}}(s)-\\bar{v}_{\\theta_{u}^{\\sim}}(s)\\right|\\ \\ \\leq\\ \\ \\kappa_{u}$ along with Eqs. (90) to (92) and noting that for any $h\\quad\\in$ $[H],E_{G,\\theta,h},E_{\\tilde{G},\\theta\\sim,h}\\in[0,2H],\\bar{v}_{\\theta_{h+1:H+1}},\\bar{v}_{\\theta_{h+1:H+1}^{\\sim}}\\in[0,H],\\omega_{G},\\omega_{\\tilde{G}}\\in[0,1],$ we have that ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{r>r\\_{d,n+1}}{\\underbrace{\\mathbb{E}}}\\Big[r_{h_{r}-1}+\\bar{\\nu}_{\\theta_{h+1,n+1}}(s_{r})\\Big]-\\underset{r>r_{d,n+1}}{\\underbrace{\\mathbb{E}}}\\Big[r_{h_{r}-1}+\\bar{\\nu}_{\\theta_{h+1,n+1}}(s_{r})\\Big]\\Big|}\\\\ &{=\\Big|(1-\\omega(s_{h+1}))\\bar{v}_{\\theta_{h+1}}(s_{h+1})-(1-\\omega\\bar{\\epsilon}_{h+1}))\\bar{v}_{\\bar{\\theta}_{h+1}}(s_{h+1})+\\omega_{G}(s_{h+1})E_{G,\\theta,h+1}-\\omega_{\\bar{G}}(s_{h+1})}\\\\ &{\\le|(1-\\omega(s_{h+1})-(1-\\omega_{\\bar{G}}(s_{h+1}))|\\bar{v}_{\\bar{\\theta}_{h+1}}(s_{h+1})|+|1-\\omega_{\\bar{G}}(s_{h+1})||\\bar{v}_{\\theta_{h+1}}(s_{h+1})-\\bar{v}_{\\bar{\\theta}_{h+1}}(s_{h})}\\\\ &{\\quad\\quad+|\\omega(s_{h+1})-\\omega_{\\bar{G}}(s_{h+1})||E_{G,\\theta,h}|+|\\omega_{G}(s_{h+1})|\\Big|E_{G,\\theta,h+1}-E_{\\bar{G},\\theta^{-1}}{\\underbrace{\\mathbb{A}}}_{|h_{\\bar{G}}|}}\\\\ &{\\le2\\sqrt{2d}H L\\xi/\\alpha+\\kappa_{h+1}+4\\sqrt{2d}H L\\xi/\\alpha+\\Big|E_{G,\\theta,h+1}-E_{\\bar{G},\\theta^{-1}}{\\underbrace{1}}_{|h_{\\bar{G}}|}}\\\\ &{\\le6\\sqrt{2d}H L\\xi/\\alpha+\\kappa_{h+1}+(H-h)6\\sqrt{2d}H L\\xi/\\alpha+\\displaystyle\\sum_{t=h+1}^{H+1}}\\\\ &{\\le(H-h+1)6\\sqrt{2d}H L\\xi/\\alpha+\\displaystyle\\sum_{t=h+1}^{H}\\kappa_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the second last inequality holds by the inductive hypothesis. ", "page_idx": 38}, {"type": "text", "text": "J   Other Useful Results and Definitions ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Definition 3. A finite set $G\\subset\\mathbb{R}^{d}$ is the basis of a near-optimal design for a set $\\Theta\\subseteq\\mathbb{R}^{d}$ if there exists a probability distribution $\\rho$ over elements of $G$ such that for any $\\theta\\in\\Theta$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle v,\\theta\\rangle=0\\ \\ \\,f o r\\,a l l\\,v\\in\\mathrm{Ker}(V(G,\\rho)),\\ a n d}\\\\ &{\\|\\theta\\|_{V(G,\\rho)^{\\dag}}^{2}\\leq2d,}\\\\ &{w h e r e\\,V(G,\\rho)=\\displaystyle\\sum_{x\\in G}\\rho(x)x x^{\\top}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where for some matrix $X$ $X^{\\dagger}$ denotes the Moore-Penrose inverse of some, and $\\operatorname{Ker}(X)$ itskernel (or null space). ", "page_idx": 39}, {"type": "text", "text": "Lemma J.1 (Hoeffding's Inequality (Theorem 2 in [Hoeffding, 1994]). Let $(X_{i})_{i\\in\\mathbb{N}}$ be independent randomvariables such that $X_{i}\\,\\in\\,[a,b]$ for some $a,b\\in\\mathbb{R},$ and let $\\begin{array}{r}{S_{n}\\,=\\,\\frac{1}{n}\\sum_{i=1}^{n}X_{i}}\\end{array}$ . Then, with probability at least $1-\\zeta$ it holds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n|\\mathbb{E}\\,S_{n}-S_{n}|\\leq{\\frac{(b-a)}{\\sqrt{n}}}{\\sqrt{\\log\\left(\\frac{2}{\\zeta}\\right)}}\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma J.2 (Covering number of the Euclidean ball). Let $a>0,\\epsilon>0,d\\geq1,$ and $\\mathcal{B}_{d}(a)=\\{x\\in$ $\\mathbb{R}^{d}:\\|\\boldsymbol{x}\\|_{2}\\leq\\boldsymbol{a}\\}$ denote the $d$ -dimensional Euclidean ball of radius a centered at the origin. The covering number of $\\boldsymbol{B}_{d}(\\boldsymbol{a})$ is upper bounded by $\\begin{array}{r}{\\left(1+\\frac{2a}{\\epsilon}\\right)^{d}}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Proof. Same as the proof of Corollary 4.2.13 in [Vershynin, 2018] with $\\,\\!B(1)$ replaced with $\\boldsymbol{B}(\\boldsymbol{a})$ ", "page_idx": 39}, {"type": "text", "text": "Lemma J.3 (Performance Difference Lemma (Lemma 3.2 in [Cai et al., 2020]). For any policies $\\pi,\\bar{\\pi}$ itholdsthat ", "page_idx": 39}, {"type": "equation", "text": "$$\nv^{\\pi}(s_{1})-v^{\\bar{\\pi}}(s_{1})=\\sum_{h=1}^{H}\\underset{(S_{h},A_{h})\\sim\\mathbb{P}_{\\pi,s_{1}}^{h}}{\\mathbb{E}}\\big(q^{\\bar{\\pi}}(S_{h},A_{h})-v^{\\bar{\\pi}}(S)\\big)\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma J.4 (Elliptical Potential Lemma (Lemma 19.4 in [Lattimore and Szepesvari, 2020])). Let $V_{0}\\in\\mathbb{R}^{d\\times d}$ bepositivedefiniteand $a_{1},\\ldots,a_{n}\\in\\mathbb{R}^{d}$ beasequenceof vectorswith $\\|a_{t}\\|_{2}\\leq L\\leq\\infty$ for all $\\begin{array}{r}{t\\in[n],V_{t}=V_{0}+\\sum_{s\\leq t}a_{s}a_{s}^{\\top}}\\end{array}$ .then, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{n}\\operatorname*{min}\\Bigl\\{1,\\|a_{t}\\|_{V_{t-1}^{-1}}^{2}\\Bigr\\}\\le2\\log\\biggl(\\frac{\\operatorname*{det}V_{n}}{\\operatorname*{det}V_{0}}\\biggr)\\le2d\\log\\biggl(\\frac{\\mathrm{Tr}\\,V_{0}+n L^{2}}{d\\operatorname*{det}(V_{0})^{1/d}}\\biggr)\\le2d\\log\\biggl(\\frac{d\\lambda+n L^{2}}{d\\lambda}\\biggr)\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma J.5 (Projection Bound (Lemma 8 in [Zanette et al., 2020]). Let $(a_{i})_{i\\in[n]}$ be any sequence of vectors in $\\mathbb{R}^{d}$ and $(b_{i})_{i\\ i n[n]}$ be any sequence of scalars such that $\\left|b_{i}\\right|\\leq c$ For any $\\lambda\\geq0$ and $k\\in\\mathbb{N}$ we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{i=1}^{n}a_{i}b_{i}\\right\\|_{(\\sum_{i=1}^{n}a_{i}a_{i}^{\\top}+\\lambda I)^{-1}}^{2}\\leq n c^{2}\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All claims explained and shown in Section 4. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: Discussed in Section 6. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The result is Theorem 1, which lists all the assumptions it uses (which are presented in Section 3.1), and its proof is presented in Section 5. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: To the best of our knowledge our research does not violate anything in the Code of Ethics upon our careful review. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Discussed at the end of Section 6. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper poses no such risks since we do not use any data or models. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 44}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]