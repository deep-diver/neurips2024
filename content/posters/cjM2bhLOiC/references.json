{"references": [{"fullname_first_author": "Sanjeev Arora", "paper_title": "Understanding gradient descent on the edge of stability in deep learning", "publication_date": "2022-07-01", "reason": "This paper provides a theoretical analysis of the implicit bias of gradient descent, which is crucial for understanding the generalization capabilities of deep learning models."}, {"fullname_first_author": "Pierre Foret", "paper_title": "Sharpness-aware minimization for efficiently improving generalization", "publication_date": "2021-01-01", "reason": "This paper introduces Sharpness-Aware Minimization (SAM), a novel optimization technique that enhances generalization performance by explicitly minimizing the sharpness of the loss landscape, which is a key concept in the target paper."}, {"fullname_first_author": "Guy Blanc", "paper_title": "Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process", "publication_date": "2020-01-01", "reason": "This paper provides insights into implicit regularization in deep learning, which is the basis of the IRE framework introduced in the target paper, by analyzing the dynamics of SGD near global minima."}, {"fullname_first_author": "Lei Wu", "paper_title": "How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective", "publication_date": "2018-01-01", "reason": "This paper offers a dynamical systems perspective on understanding the implicit regularization of SGD, providing theoretical foundations for the IRE framework and its analysis."}, {"fullname_first_author": "Kaiyue Wen", "paper_title": "How sharpness-aware minimization minimizes sharpness?", "publication_date": "2023-01-01", "reason": "This paper provides further theoretical analysis of the sharpness-aware minimization, which is the core of IRE, by modeling its effective dynamics and showing its implicit regularization mechanism."}]}