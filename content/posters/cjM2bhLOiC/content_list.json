[{"type": "text", "text": "Improving Generalization and Convergence by Enhancing Implicit Regularization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mingze Wang1,3,\u2020 Jinbo Wang1,3 Haotian $\\mathbf{H}\\mathbf{e}^{1,3}$ Zilin Wang1 Guanhua Huang5,6 Feiyu Xiong3 Zhiyu Li3 Weinan ${\\bf E}^{1,2,3,4}$ Lei Wu1,2,\u2020 ", "page_idx": 0}, {"type": "text", "text": "1School of Mathematical Sciences, Peking University 2Center for Machine Learning Research, Peking University 3Institute for Advanced Algorithms Research (Shanghai) $^{4}\\mathrm{AI}$ for Science Institute 5School of Data Science, University of Science and Technology of China 6ByteDance Research {mingzewang, wangjinbo, haotianhe, wangzilin}@stu.pku.edu.cn guanhuahuang@mail.ustc.edu.cn {xiongfy, lizy}@iaar.ac.cn {weinan, leiwu}@math.pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we propose an Implicit Regularization Enhancement (IRE) framework to accelerate the discovery of flat solutions in deep learning, thereby improving generalization and convergence. Specifically, IRE decouples the dynamics of flat and sharp directions, which boosts the sharpness reduction along flat directions while maintaining the training stability in sharp directions. We show that IRE can be practically incorporated with generic base optimizers without introducing significant computational overload. Experiments show that IRE consistently improves the generalization performance for image classification tasks across a variety of benchmark datasets (CIFAR-10/100, ImageNet) and models (ResNets and ViTs). Surprisingly, IRE also achieves a $2\\times$ speed-up compared to AdamW in the pre-training of Llama models (of sizes ranging from 60M to 229M) on datasets including Wikitext-103, Minipile, and Openwebtext. Moreover, we provide theoretical guarantees, showing that IRE can substantially accelerate the convergence towards flat minima in sharpness-aware minimization (SAM). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning has achieved remarkable success across a variety of fields, including computer vision, scientific computing, and artificial intelligence. The core challenge in deep learning lies in how to train deep neural networks (DNNs) efficiently to achieve superior performance. Understanding and improving the generalization and convergence of commonly-used optimizers, such stochastic gradient descent (SGD) (Robbins and Monro, 1951; Rumelhart et al., 1986), in deep learning is crucial for both theoretical research and practical applications. ", "page_idx": 0}, {"type": "text", "text": "Notably, optimizers often exhibit a preference for certain solutions in training DNNs. For instance, SGD and its variants consistently converge to solutions that generalize well, even when DNNs are highly over-parameterized and there are many solutions that generalize poorly. This phenomenon is referred to as implicit regularization in the literature (Neyshabur et al., 2014; Zhang et al., 2017). ", "page_idx": 0}, {"type": "text", "text": "The most popular explanation for implicit regularization is that SGD and its variants tend to converge to flat minima (Keskar et al., 2016; Wu et al., 2017), and flat minima generalize better (Hochreiter and ", "page_idx": 0}, {"type": "text", "text": "Schmidhuber, 1997; Jiang et al., 2019). However, the process of this implicit sharpness regularization occurs at a very slow pace, as demonstrated in works such as Blanc et al. (2020), Li et al. (2022), and Ma et al. (2022). Consequently, practitioners often use a large learning rate (LR) and extend the training time even when the loss no longer decreases, ensuring the convergence to flatter minima (He et al., 2016; Goyal et al., 2017; Hoffer et al., 2017). Nevertheless, the largest allowable LR is constrained by the need to maintain training stability. In addition, Foret et al. (2021) proposed SAM, which aims to explicitly regularize sharpness during training and has achieved superior performance across a variety of tasks. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We propose an Implicit Regularization Enhancement (IRE) framework to speed up the convergence towards flatter minima. As suggested by works like Blanc et al. (2020), Li et al. (2022) and Ma et al. (2022), the implicit sharpness reduction often occurs at a very slow pace, along flat directions. Inspired by this picture, IRE particularly accelerates the dynamics along flat directions, while keeping sharp directions\u2019 dynamics unchanged. As such, IRE can boost the implicit sharpness reduction substantially without hurting training stability. For a detailed illustration of this mechanism, we refer to Section 2. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We then provide a practical IRE framework, which can be efficiently incorporated with generic base optimizers. We evaluate the performance of this practical IRE in both vision and language tasks. For vision tasks, IRE consistently improves the generalization performance of popular optimizers like SGD, Adam, and SAM in classifying the CIFAR-10/100 and ImageNet datasets with ResNets (He et al., 2016) and vision transformers (ViTs) (Dosovitskiy et al., 2020). For language modelling, we consider the pre-training of Llama models (Touvron et al., 2023) of various sizes, finding that IRE surprisingly can accelerate the pre-training convergence. Specifically, we observe a remarkable $2.0\\times$ speedup compared to AdamW in the scenarios we examined, despite IRE being primarily motivated to speed up the convergence to flat solutions. \u2022 Lastly, we provide theoretical guarantees showing that IRE can achieves a $\\Theta(1/\\rho)$ -time acceleration over the base SAM algorithm in minimizing the trace of Hessian, where $\\rho\\in(0,1)$ is a small hyperparameter in SAM. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Implicit sharpness regularization. There have been extensive attempts to explain the mystery of implicit regularization in deep learning (see the survey by Vardi (2023) and references therein). Here, we focus on works related to implicit sharpness regularization. Wu et al. (2018; 2022) and Ma and Ying (2021) provided an explanation of implicit sharpness regularization from a dynamical stability perspective. Moreover, in-depth analysis of SGD dynamics near global minima shows that the SGD noise (Blanc et al., 2020; Li et al., 2022; Ma et al., 2022; Damian et al., 2021) and the edge of stability (EoS)-driven (Wu et al., 2018; Cohen et al., 2021) oscillations (Even et al., 2024) can drive SGD/GD towards flatter minima. Additional studies explored how training components, including learning rate and batch size (Jastrze\u02dbbski et al., 2017), normalization (Lyu et al., 2022), cyclic LR (Wang and Wu, 2023), influence this sharpness regularization. Furthermore, some works have provided theoretical evidence explaining the superior generalization of flat minima for neural networks (Ma and Ying, 2021; Mulayoff et al., 2021; Wu and Su, 2023; Gatmiry et al., 2023; Wen et al., 2023b). Our work is inspired by this line of research, aiming to boost implicit sharpness regularization by decoupling the dynamics along flat and sharp directions. ", "page_idx": 1}, {"type": "text", "text": "Sharpness-aware minimization. IRE shares the same motivation as SAM in enhancing sharpness regularization, although their specific approaches differ significantly. It is worth noting that the per-step computational cost of SAM is twice that of base optimizers. Consequently, there have been numerous attempts to reduce the computational cost of SAM (Kwon et al., 2021; Liu et al., 2022; Du et al., 2021; Mi et al., 2022; Mueller et al., 2024). In contrast, the per-step computational cost of IRE is only approximately 1.1 times that of base optimizers (see Table 5). Moreover, we provide both theoretical and experimental evidence demonstrating that the mechanism of IRE in boosting sharpness regularization is nearly orthogonal to that of SAM. ", "page_idx": 1}, {"type": "text", "text": "Optimizers for large language model (LLM) pre-training. (Momentum) SGD (Sutskever et al., 2013; Nesterov, 1983) and its adaptive variants like Adagrad (Duchi et al., 2011), RMSProp (Tieleman, 2012), and Adam (Kingma and Ba, 2014) have been widely used in DNN training. Despite the efforts in designing better adaptive gradient methods (Liu et al., 2019a; Luo et al., 2019; Heo et al., 2020; Zhuang et al., 2020; Xie et al., 2022b;a), AdamW(Adam+decoupled weight decay) (Loshchilov and Hutter, 2017) has become the default optimizer in LLM pre-training. Recently, Chen et al. (2024) discovered Lion by searching the space of adaptive first-order optimizers; Liu et al. (2024) introduced Sophia, a scalable second-order optimizer. In this paper, we instead empirically demonstrate that IRE can accelerate the convergence of AdamW in the pre-training of Llama models. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.2 Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Throughout this paper, let $\\mathcal{L}:\\mathbb{R}^{p}\\mapsto\\mathbb{R}_{\\geqslant0}$ be the function of total loss, where $p$ denotes the number of model parameters. For a $\\mathcal{C}^{2}$ -submanifold $\\mathcal{M}$ in $\\mathbb{R}^{p}$ , we denote the tangent space of $\\mathcal{M}$ at $\\pmb\\theta\\in\\mathcal M$ as $\\mathcal{T}_{\\theta}\\mathcal{M}$ , which is a linear subspace in $\\mathbb{R}^{p}$ . For $f\\in\\mathcal{C}^{1}(\\mathcal{M})$ and $\\pmb\\theta\\in\\mathcal M$ , let $\\nabla_{\\mathcal{M}}f(\\pmb{\\theta}):=\\mathcal{Q}_{T_{\\theta},M}\\nabla f(\\pmb{\\theta})$ denote the Riemannian gradient, where $\\mathcal{Q}_{\\mathcal{T}_{\\theta}\\mathcal{M}}:\\mathbb{R}^{p}\\mapsto\\mathbb{R}^{p}$ denotes the orthogonal projection to $\\mathcal{T}_{\\theta}\\mathcal{M}$ . For a symmetric matrix $A\\in\\mathbb{R}^{p\\times p}$ , its eigen pairs are denoted as $\\{(\\lambda_{i},{\\pmb u}_{i})\\}_{i\\in[p]}$ with the order \u03bb1 \u2a7e \u00b7 \u00b7 \u00b7 \u2a7e \u03bbp. We use Pi:j(A) =  jk=i ukuk\u22a4 to denote the projection operator onto $\\operatorname{span}\\{\\pmb{u}_{i},\\dotsc,\\pmb{u}_{j}\\}$ . Denote $\\mathcal{N}(\\pmb{\\mu},\\dot{\\Sigma})$ as the Gaussian distribution with mean $\\pmb{\\mu}$ and covariance matrix $\\Sigma$ , and $\\mathbb{U}(S)$ as the uniform distribution over a set $\\boldsymbol{S}$ . Given a vector $\\boldsymbol{h}=(h_{1},\\dots,h_{p})$ , let $|\\pmb{h}|=(|h_{1}|,\\ldots,|h_{p}|)$ . We denote by 1 the all-ones vector. We will use standard big-O notations like $O(\\cdot),\\Omega(\\cdot)$ , and $\\Theta(\\bar{\\cdot})$ to hide constants. ", "page_idx": 2}, {"type": "text", "text": "2 An Illustrative Example Motivating IRE ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we provide an illustration of how the dynamics along flat directions can reduce the sharpness (curvatures along sharp directions) and how IRE can accelerate this sharpness reduction. To this end, we consider the following phenomenological problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{p}}\\mathcal{L}(\\pmb{\\theta}):=\\pmb{v}^{\\top}H(\\pmb{u})\\pmb{v}/2,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\textbf{\\textit{v}}\\in\\mathbb{R}^{m}$ , $\\pmb{u}\\ \\in\\ \\mathbb{R}^{p-m}$ , and $\\pmb{\\theta}\\;=\\;\\mathrm{vec}(\\pmb{u},\\pmb{v})\\;\\in\\;\\mathbb{R}^{p}$ . We assume $H(\\cdot)\\;\\in\\;\\mathcal{C}^{2}(\\mathbb{R}^{p-m})$ and $\\operatorname*{inf}_{\\boldsymbol{u}}\\lambda_{\\operatorname*{min}}(H(\\boldsymbol{u}))>0$ . Then, the minimizers of $\\mathcal{L}(\\cdot)$ form a $m$ -dim manifold $\\mathcal{M}=\\left\\{(\\boldsymbol{u},\\boldsymbol{v}):\\boldsymbol{v}=\\mathbf{0}\\right\\}$ and the Hessian at any $\\pmb\\theta\\in\\mathcal M$ is given by $\\nabla^{2}\\mathcal{L}(\\pmb{\\theta})=\\left(\\begin{array}{c c}{\\mathbf{0}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{H(\\pmb{u})}\\end{array}\\right)$ . For clarity, we shall call $\\textbf{\\em u}$ and $\\pmb{v}$ the flat and sharp directions, respectively. ", "page_idx": 2}, {"type": "text", "text": "Example 2.1. The loss landscape of fitting zero labels with two-layer neural networks (2LNNs) exhibits exactly the form (1). Let $f(\\pmb{x};\\pmb{\\theta})\\,=\\,\\pmb{a}^{\\top}\\phi(\\pmb{x};W)$ be a 2LNN with $\\pmb{\\theta}=\\mathrm{vec}(W,\\pmb{a})$ . Then $\\mathcal{L}(\\pmb{\\theta})=\\mathbb{E}_{(\\pmb{x},\\pmb{y})}[(f(\\pmb{x};\\pmb{\\theta})-\\pmb{y})^{2}]/2=\\pmb{a}^{\\top}\\mathbb{E}_{\\pmb{x}}[\\phi(\\pmb{x};W)\\phi(\\pmb{x};W)^{\\top}]\\pmb{a}/2=:\\pmb{a}^{\\top}H(W)\\pmb{a}/2.$ . ", "page_idx": 2}, {"type": "text", "text": "For breviety, we further assume $H(\\pmb{u})=\\mathrm{diag}(\\pmb{\\lambda}(\\pmb{u}))$ with $\\lambda({\\boldsymbol u})=(\\lambda_{1}({\\boldsymbol u}),\\cdots,\\lambda_{m}({\\boldsymbol u}))$ . In this case, the GD dynamics can be naturally decomposed into the flat and sharp directions as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\pmb u}_{t+1}={\\pmb u}_{t}-\\frac{\\eta}{2}\\sum_{i=1}^{m}v_{t,i}^{2}\\nabla\\lambda_{i}({\\pmb u}_{t})},}\\\\ {{\\displaystyle v_{t+1}=({\\bf1}-\\eta{\\pmb\\lambda}({\\pmb u}_{t}))\\odot{\\pmb v}_{t}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\odot$ denotes the element-wise multiplication of two vectors. ", "page_idx": 2}, {"type": "text", "text": "The implicit sharpness regularization. From Eq. (2), we can see that 1) the flat direction $\\pmb{u}_{t}$ \u2019s dynamics monotonically reduces the sharpness $\\lambda(u)$ as long as $\\pmb{v}_{t}$ is nonzero; 2) the sharp direction $\\pmb{v}_{t}$ \u2019s dynamics determines the speed of sharpness reduction. The larger $\\left|v_{t}\\right|$ is, the faster the curvature $\\lambda(u)$ decreases. Particularly, when near convergence, we have $|{\\pmb v}_{t}|\\,=\\,\\dot{o}(1)$ and thus the implicit sharpness reduction is very slow during the late phase of GD. Figure 1a provides a visualization of this slow implicit sharpness reduction. ", "page_idx": 2}, {"type": "text", "text": "Accelerating the sharpness reduction. Inspired by the above analysis, we can accelerate the sharpness reduction by speeding up the flat directions\u2019 dynamics. To this end, there are two approaches: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Naively increasing the global learning rate $\\eta$ (fail). Increasing $\\eta$ accelerates the dynamics of $\\pmb{u}_{t}$ , but the largest allowed $\\eta$ is constrained by curvatures of sharpest directions. In GD (2), to maintain training stability, $\\eta$ must be smaller than $2/\\operatorname*{max}_{i}\\lambda_{i}({\\textbf u}_{t})$ . Otherwise, $\\pmb{v}_{t}$ \u2019s dynamics will blow up. As illustrated in Figure 1b, setting $\\eta=2$ leads to divergence, whereas $\\eta=1$ ensures convergence. ", "page_idx": 2}, {"type": "image", "img_path": "cjM2bhLOiC/tmp/93da16fa06dbaceb4be89c4fc3bebb3ff4f54da96eb76c4b6bed7a0dd87d7f2a.jpg", "img_caption": ["Figure 1: A 2-d example of (1): $\\mathcal{L}(u,v)=(1+u^{2})v^{2}/2$ . The gray arrows denote to the minima manifold $\\bar{\\mathcal{M}}=\\{(u,v):v=0\\}$ , where the smaller the $\\lvert u\\rvert$ , the flatter the minimizer. The red marker highlights the flattest minimizer $(0,0)$ . (a) The dynamics of GD $\\left.\\eta=1\\right.$ ), which moves slowly towards flatter minima as it converges. (b) The dynamics of GD $(\\eta=2)$ ), which diverges due to the excessively large $\\eta$ . (c) The behavior of our IRE approach with varying $\\kappa$ \u2019s v.s. GD $\\eta=1$ ). Is is shown that IRE can significantly accelerate the $u_{t}$ \u2019s dynamics, almost reaching the flattest minimum $(0,0)$ when taking a very large $\\kappa$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "\u2022 Increasing only the flat directions\u2019 learning rate (our approach, IRE). Specifically, for GD (2), the GD-IRE dynamics is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{u}_{t+1}=\\pmb{u}_{t}-(1+\\kappa)\\frac{\\eta}{2}\\sum_{i=1}^{m}v_{t,i}^{2}\\nabla\\lambda_{i}(\\pmb{u}_{t}),}\\\\ &{\\pmb{v}_{t+1}=(\\pmb{1}-\\eta\\pmb{\\lambda}(\\pmb{u}_{t}))\\odot\\pmb{v}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\kappa>0$ controls the enhancement strength. In GD-IRE (3), $\\pmb{u}_{t}$ \u2019s dynamics is $(1+\\kappa)$ faster than that of GD (2). Notably, the sharp directions\u2019 dynamics $\\left(\\pmb{v}_{t}\\right)$ are unchanged. The choice of $\\kappa$ only needs to maintain the stability of flat directions\u2019 dynamics, for which, we can always take a significantly large $\\kappa$ to enhance the sharpness regularization. As demonstrated in Figure 1c, IRE with larger $\\kappa$ always find flatter minima. ", "page_idx": 3}, {"type": "text", "text": "Remark 2.2 (The generality). It is worth noting that similar implicit sharpness regularization also holds for SGD (Ma et al., 2022; Li et al., 2022) and SAM (Wen et al., 2023a). In this section, we focus on the above toy model and GD mainly for illustration. In Appendix B, we provide an analogous illustrative analysis of how IRE accelerates the sharpness reduction of SGD. In Section 5, we further provide theoretical evidence to show that IRE can boost the implicit sharpness regularization of SAM. ", "page_idx": 3}, {"type": "text", "text": "3 A Practical Framework of Implementing IRE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Although the preceding illustration of IRE is for GD, in practice, we can incorporate IRE with any base optimizers. Specifically, for a generic update: $\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\eta\\pmb{g}_{t}$ , the corresponding IRE modification is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\eta(\\pmb{g}_{t}+\\kappa\\mathcal{P}_{t}\\pmb{g}_{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\kappa$ denotes the enhancement strength and $\\mathcal{P}_{t}:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}^{p}$ projects $\\scriptstyle g_{t}$ into the flat directions of the landscape. The flat directions and corresponding projection operator $\\mathcal{P}_{t}$ can be estimated using the Hessian information. ", "page_idx": 3}, {"type": "text", "text": "However, estimating the full Hessian matrix $\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{t})\\in\\mathbb{R}^{p\\times p}$ is computationally infeasible. Consequently, we propose to use only the diagonal Hessian $\\mathrm{diag}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{t}^{\\cdot}))\\in\\mathbb{R}^{p}$ to estimate $\\mathcal{P}_{t}$ . Let $\\pmb{h}_{t}\\in\\mathbb{R}^{p}$ be an estimate of the diagonal Hessian. Then, we perform the projection as follows ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{P}_{t}g_{t}=n_{t}\\odot g_{t},\\ \\mathrm{with}\\left(n_{t}\\right)_{i}=\\left\\{1\\begin{array}{l l}{\\mathrm{\\quad~if\\}(|h_{t}|)_{i}\\leqslant\\mathrm{Top}_{\\mathrm{small}}(|h_{t}|,\\gamma)}\\\\ {\\mathrm{\\quad~otherwise}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\gamma\\in(0,1)$ and $\\mathrm{Top}_{\\mathrm{small}}(|h_{t}|,\\gamma)$ returns the $\\lfloor p\\!\\cdot\\!\\gamma\\rfloor$ -th smallest value in $\\left|h_{t}\\right|$ . Note that $\\pmb{n}_{t}\\in\\mathbb{R}^{p}$ denotes a mask vector and the above approximate projection essentially masks the top- $(1-\\gamma)$ sharp coordinates out. As such, the projection (5) will retain the top- $\\gamma$ flat coordinates. Noticing that in DNNs, there are much more flat directions than sharp directions (Yao et al., 2020), we thus often use $\\gamma>0.5$ in practice. ", "page_idx": 3}, {"type": "image", "img_path": "cjM2bhLOiC/tmp/d34624f20fa55241d6a7167927fc6387a2f587f26c3061b486812cd80eccb225.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "A light-weight estimator of the diagonal Hessian. Let $\\ell(\\cdot,\\cdot)$ be the cross-entropy loss. Given an input data $\\textbf{\\em x}\\in\\mathbb{R}^{d_{x}}$ and label $\\textbf{\\em y}\\in\\mathbb{R}^{d_{y}}$ , let the model\u2019s prediction be $f(\\pmb{x};\\pmb{\\theta})\\;\\in\\;\\mathbb{R}^{d_{y}}$ . The Fisher (Gauss-Newton) matrix $F(\\pmb\\theta)$ is widely acknowledged to be a good approximation of the Hessian, particularly near minima. Thus, we can estimate the diagonal Hessian by $\\mathbf{\\bar{}}h_{t}=\\mathrm{diag}(F(\\pmb{\\theta}_{t}))$ , which has been widely used in deep learning optimization (Martens and Grosse, 2015; Grosse and Martens, 2016; George et al., 2018; Mi et al., 2022; Liu et al., 2024). Given an input batch $\\{(\\pmb{x}_{b},\\pmb{y}_{b})\\}_{b=1}^{B}$ , the empirical diagonal Fisher is given by $\\begin{array}{r}{\\mathrm{diag}(\\hat{F}(\\pmb{\\theta}))=\\frac{1}{B}\\sum_{b=1}^{B}\\nabla\\ell(f(\\pmb{x}_{b};\\pmb{\\theta});\\hat{\\pmb{y}}_{b})\\odot}\\end{array}$ $\\nabla\\ell(f(\\mathbf{\\boldsymbol{x}}_{b};\\mathbf{\\theta});\\hat{\\mathbf{\\boldsymbol{y}}}_{b})$ , where $\\hat{\\pmb y}_{b}\\sim\\mathrm{softmax}(f(\\pmb\\theta;\\pmb x_{b}))$ . However, as noted by Liu et al. (2024), implementing this estimator is computationally expensive due to the need to calculate $B$ single-batch gradients. Liu et al. (2024) proposed a more convenient estimator $\\mathrm{diag}(\\hat{F}_{\\mathrm{eff}}(\\pmb{\\theta}))$ , only requires computing the mini-batch gradient $\\begin{array}{r}{\\nabla\\hat{\\mathcal{L}}_{B}(\\pmb{\\theta})=\\frac{1}{B}\\sum_{b=1}^{B}\\nabla\\ell(f(\\pmb{x}_{b};\\pmb{\\theta});\\hat{\\pmb{y}}_{b})}\\end{array}$ with $\\hat{\\pmb y}_{b}\\sim\\mathrm{softmax}(f(\\pmb x_{b};\\pmb\\theta))$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{}h_{t}=\\mathrm{diag}(\\hat{F}_{\\mathrm{eff}}(\\pmb{\\theta}))=\\boldsymbol{B}\\cdot\\nabla\\hat{\\mathcal{L}}_{B}(\\pmb{\\theta})\\odot\\nabla\\hat{\\mathcal{L}}_{B}(\\pmb{\\theta}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "According to Liu et al. (2024), this estimator is an unbiased estimate of the empirical diagonal Fisher, i.e., $\\mathbb{E}_{\\hat{\\pmb{y}}}[\\mathrm{diag}(\\hat{F}_{\\mathrm{eff}}(\\pmb{\\theta}))]=\\mathbb{E}_{\\hat{\\pmb{y}}}[\\mathrm{diag}(\\hat{F}(\\pmb{\\theta}))]$ . For more discussions on the efficiency of this estimator, please refer to (Liu et al., 2024, Section 2). Additionally, for squared loss, one can simply use Fisher as the estimator (Liu et al., 2024). ", "page_idx": 4}, {"type": "text", "text": "The practical IRE and computational efficiency. The practical IRE is summarized in Algorithm 1, which is notably lightweight. The estimation of $h_{t}$ using (6) requires computational resources roughly equivalent to one back-propagation. Consequently, by setting $K=10$ in Algorithm 1 (estimating the projection every 10 steps), the average per-step computational load of IRE is only 1.1 times that of the base optimizer. This claim can be empirically validated as shown in Table 5. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we evaluate how IRE performs when incorporating with various base optimizers. Specifically, we examine the incorporation of IRE with SGD (SGD-IRE), SAM (SAM-IRE), and AdamW (AdmIRE) across vision and language tasks. ", "page_idx": 4}, {"type": "text", "text": "4.1 Image classification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1.1 Validating our motivation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To show that IRE can accelerate the sharpness reduction, we train WideResNet-16-8 (Zagoruyko and Komodakis, 2016) on CIFAR-10 dataset (Krizhevsky and Hinton, 2009) by SAM-IRE (with $K=10$ , varying $\\kappa$ and $\\gamma$ ). Here, we incoporate IRE into SAM starting from the 30-th epochs. We vary $\\gamma\\in\\{0.8,0.9,0.95\\}$ and $\\kappa\\in\\{0,2,5,10\\}$ . Regarding the learning rate (LR), both constant LR and decayed LR are considered. The sharpness is measured by $\\operatorname{Tr}(\\nabla^{\\tilde{2}}\\mathcal{L}(\\pmb{\\theta}))$ . Further experimental details can be found in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "As depicted in Fig. 2(a), SAM-IRE (with constant LR) consistently finds flatter solutions compared to SAM and higher $\\kappa$ always leads to flatter minima. Additionally, SAM-IRE also shows robustness to variations of $\\gamma$ . For SAM-IRE with decayed LR (Fig. 2(b)), SAM-IRE still consistently finds flatter solutions than SAM. Notably, flatter solutions correlate positively with lower training loss and higher test accuracy (Fig. 2(c,d)). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "cjM2bhLOiC/tmp/90db4c9540f1ce52a8074d7e2e301c34785ec97b32cabb44671de58db108b70a.jpg", "img_caption": ["(a) sharpness, constant LR.(b) sharpness, decayed LR.(c) train loss, decayed LR. (d) test acc, decayed LR. Figure 2: Training WRN-16-8 on CIFAR-10 by SAM-IRE with varying $\\gamma,\\kappa$ . Particularly, the case of $\\kappa=0$ correspond to the standard SAM. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1.2 IRE can consistently improve generalization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Convolutional Neural Networks (CNNs). In this experiment, we first consider the classification of CIFAR-{10,100} with WideResNet-28-10 (Zagoruyko and Komodakis, 2016) and ResNet-56 (He et al., 2016). Both SGD and SAM optimizers are adopted. All the experiments use base data augmentation and label smoothing. For SGD-IRE/SAM-IRE, we fix $K=10$ , and tune hyperparameters $\\gamma$ and $\\kappa$ via a grid search over $\\gamma\\in\\{0.99,0.9,0.8\\}$ and $\\kappa\\in\\{1,2\\}$ . The total epochs are set to 100 for CIFAR-10 and 200 for CIFAR-100, and we switch from SGD/SAM to SGD-IRE/SAM-IRE when the training loss approaches 0.1. The other experimental details are deferred to Appendix C and the results are shown in Table 1. ", "page_idx": 5}, {"type": "text", "text": "Secondly, we evaluate IRE for training ResNet-50 on ImageNet (Deng et al., 2009). The experimental details are deferred to Appendix C and the results are shown in Table 2. ", "page_idx": 5}, {"type": "text", "text": "Vision Transformers (ViTs). We also examine the impact of IRE on generalization of ViT-T and ViT-S (Dosovitskiy et al., 2020) on CIFAR100. The default optimizers used are AdamW and SAM (Mueller et al., 2024). Strong data augmentations (basic $^+$ AutoAugment) are utilized. The total epochs are set to 200 and we switch from AdamW/SAM to AdmIRE/SAM-IRE when the training loss approaches 0.5. For AdmIRE/SAM-IRE, we fix $K=10$ , and tune hyperparameters $\\gamma$ and $\\kappa$ via a grid search over $\\gamma\\in\\{0.99,0.9,0.8\\}$ and $\\kappa\\in\\{20,50\\}$ . Other experimental details are deferred to Appendix C. The results are shown in Table 3. ", "page_idx": 5}, {"type": "text", "text": "Additionally, we evaluate IRE for training ViT-S on ImageNet. The experimental details are deferred to Appendix C and the results are shown in Table 4. ", "page_idx": 5}, {"type": "text", "text": "As demonstrated in Table 1, 2, 3, and 4, IRE consistently improves generalization of SGD, AdamW and SAM across all settings examined. ", "page_idx": 5}, {"type": "table", "img_path": "cjM2bhLOiC/tmp/b9d6b5043f01ff52a75d5bec8081939eb608def800ea04484c02640f711ffdc2.jpg", "table_caption": ["Table 1: WRN-28-10/ResNet-56 on CIFAR-10/100. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "cjM2bhLOiC/tmp/e67e03c3b05adf55fd53968511bd874e692178a120423c6c80fb72691f2a24e0.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "cjM2bhLOiC/tmp/9f20fc0e8486cd35d952554dea2337f1e2143322515dafa9a697162f6ed9a578.jpg", "table_caption": ["Table 3: ViT-T/S on CIFAR-100. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "cjM2bhLOiC/tmp/e912040db539687deede8b3c6eac522746604fb4013cb34a96f8322b2fe7b4b7.jpg", "table_caption": ["Table 4: ViT-S on ImageNet. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Large language model pre-training ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now evaluate IRE in the pre-training of decoder-only large language models (LLMs). Following the training protocol of Llama models, we employ the AdamW optimizer with hyperparameters $\\beta_{1}=0.9,\\beta_{2}=0.95$ and weight decay $\\lambda=0.1$ (Touvron et al., 2023). The learning rate strategy includes a warm-up phase followed by a cosine decay scheduler, capped at lr_max. In each experiment, we tune $\\exists\\,\\mathbf{r}$ _max only for AdamW and use it also for AdmIRE, for which the IRE is activated at the end of warm-up phase. ", "page_idx": 6}, {"type": "text", "text": "4.2.1 Computational efficiency and hyperparameter robustness ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The first experiment is conducted to verify both the computational efficiency and the robustness of hyperparameters $(\\gamma,\\kappa)$ in IRE for pre-training tasks. Specifically, we train a 2-layer decoder-only Transformer (8M) on the Wikitext-2 dataset (4.3M) (Merity et al., 2016) by AdamW and AdmIRE (with $K\\,=\\,10$ and varying $\\gamma,\\kappa)$ . The total training duration is $100\\mathbf{k}$ steps, including a $3\\mathbf{k}.$ -step warm-up phase. ", "page_idx": 6}, {"type": "text", "text": "First, we tune lr_max in AdamW, identifying the optimal $\\mathtt{l r\\_m a x=6e-4}$ . Subsequently, we train both AdamW and AdmIRE using this lr_max. ", "page_idx": 6}, {"type": "text", "text": "Computational efficiency. As shown in Table 5, AdmIRE with $K=10$ (estimating the projection mask every 10 steps) is computationally efficient: the average time per step of AdmIRE is only 1.12 times that of AdamW, corresponding to the theoretical estimation (1.1 times). ", "page_idx": 6}, {"type": "table", "img_path": "cjM2bhLOiC/tmp/c3f634519df41fd5659e244e2b2bc576422837bf8c679208eea564b546f8663c.jpg", "table_caption": ["Table 5: Wall-clock time on 1 A800. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Robustness to hyperparameters. Figure 3 shows that AdmIRE, with varying $\\gamma$ and $\\kappa$ , consistently speeds up the pretraining. Remarkably, with the best configuration, AdmIRE can achieves $5.4\\times$ speedup than well-tuned AdamW. ", "page_idx": 6}, {"type": "image", "img_path": "cjM2bhLOiC/tmp/2cb7cf9d00343b820085504252f638094ff87919b8d99cda657f3ca18854bdd6.jpg", "img_caption": ["Figure 3: Transformer on wikitext-2. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "More experimental details and results are deferred to Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4.2.2 Experiments on Llama models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Llama (Touvron et al., 2023), a popular open LLM, exhibits remarkable capabilities across general domains. In this section, we examine the performance of AdmIRE in training Llama models of various sizes across various datasets: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Llama (60M) on wikitext-103 (0.5G). Wikitext-103 (Merity et al., 2016) serves as a standard language modeling benchmark for pre-training, which contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article.   \n\u2022 Llama (119M) on minipile (6G). Minipile (Kaddour, 2023), a 6GB subset of the deduplicated Pile (825GB) (Gao et al., 2020) presents a highly diverse text corpus. Given its diversity, training on minipile poses more challenges and potential instabilities for optimizers compared to Wikitext-103.   \n\u2022 Llama (229M) on openwebtext (38G). Openwebtext (Gokaslan and Cohen, 2019), an opensource recreation of the WebText corpus, is extensively utilized for LLM pre-training such as RoBERTa (Liu et al., 2019b) and GPT-2 (Radford et al., 2019). ", "page_idx": 6}, {"type": "text", "text": "Additionally, gradient clipping is adopted to maintain the training stability (Pascanu et al., 2012). First, we tune lr_max in AdamW for each of the three experiments, separately. The optimal lr_max identified for these three experiments is all 6e-4. Then, both AdamW and AdmIRE are trained using this optimal $\\exists\\,\\mathbf{r}$ _max. For more details, please refer to Appendix C. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "AdmIRE is $2\\times$ faster than AdamW. The results are reported in Figure 4. We can see that AdmIRE consistently achieves a $2.1\\times$ speedup compared with well-tuned AdamW for all three cases. ", "page_idx": 7}, {"type": "image", "img_path": "cjM2bhLOiC/tmp/32c306af8a7ced976d61410668d58e585dbeb1bbfbdfc4d98baf318b6d5ff532.jpg", "img_caption": ["Figure 4: AdmIRE outperforms AdamW in the pre-training of Llama models. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Notice that the primary motivation behind IRE is to speed up the sharpness reduction, which only requires to increase learning rate along completely flat (zero-curvature) directions. However, practical implementation may also increase the learning rate along directions with small but non-zero curvatures, which can further speed up loss convergence. A thorough explanation for the significant acceleration provided by this approach is left for future research. ", "page_idx": 7}, {"type": "text", "text": "We further assess the sharpness reduction capability of IRE for LLM pre-training. Specifically, we compare the sharpness of solutions, $\\operatorname{\\bar{Tr}}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}))$ , found by AdamW/AdmIRE during pre-training of Llama (60M) on wiki-103 dataset (corresponding to Figure 4 (left)). The results shown in Table 6 demonstrate that AdamIRE not only achieves the same loss in only half the iterations required by AdamW, but also the solutions found by AdmIRE are significantly flatter than that found by AdamW. ", "page_idx": 7}, {"type": "table", "img_path": "cjM2bhLOiC/tmp/08c3a7930ba0594b552c22c6fe4565bdf6021c825f9e9d8edb249ce66be48dbe.jpg", "table_caption": ["Table 6: Comparison of the sharpness of the solutions found by AdamW/AdmIRE. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Recently, Liu et al. (2023) revealed a strong correlation between the sharpness and downstream task performance, suggesting that for models with the same pre-training loss, flatter solutions yield better performance on downstream tasks. Based on this, we hypothesize that the solutions found by IRE may also have better performance in downstream tasks, which we leave to future work. ", "page_idx": 7}, {"type": "text", "text": "5 Theoretical Guarantees for IRE on SAMs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Both empirical (Foret et al., 2021) and theoretical (Wen et al., 2023a) studies have validated that SAM algorithms exhibit superior sharpness regularization compared to (S)GD. In this section, we provide a theoretical analysis demonstrating that IRE can further enhance the sharpness regularization of SAM algorithms substantially. ", "page_idx": 7}, {"type": "text", "text": "5.1 Theoretical setups ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\mathcal{L}(\\pmb{\\theta}):=\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}_{i}(\\pmb{\\theta})}\\end{array}$ denote the total loss, where $\\mathcal{L}_{i}(\\pmb{\\theta})$ is the loss on the $i$ -th data. Without loss of generality, we assume min\u03b8 $\\mathcal{L}(\\pmb{\\theta})=0$ . We further make the following assumption: ", "page_idx": 7}, {"type": "text", "text": "Assumption 5.1 (Manifold of minimizers). Assume that $\\mathcal{L}\\in\\mathcal{C}^{4}(\\mathbb{R}^{p})$ , ${\\mathcal{M}}:=\\arg\\operatorname*{min}_{\\theta}{\\mathcal{L}}(\\pmb{\\theta})$ is a $(p-m)$ -dim $\\mathcal{C}^{2}$ -submanifold in $\\mathbb{R}^{p}$ for some $m\\in[p]$ , and $\\operatorname{\\cdotank}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}))=m$ for any $\\pmb\\theta\\in\\mathcal M$ . ", "page_idx": 7}, {"type": "text", "text": "The above connectivity assumption on the manifold of minimizers $\\mathcal{M}$ has been empirically verified in works such as Draxler et al. (2018) and Garipov et al. (2018), and theoretically supported in Cooper (2018). This assumption is also widely used in the theoretical analysis of implicit regularization (Fehrman et al., 2020; Li et al., 2022; Arora et al., 2022; Wen et al., 2023a). ", "page_idx": 7}, {"type": "text", "text": "Besides, we introduce the following definitions to characterize the dynamics of gradient flow (GF) near the minima manifold $\\mathcal{M}$ , which is also used in the related works above. ", "page_idx": 7}, {"type": "text", "text": "Definition 5.2 (Limiting map of GF). Consider the GF: $\\begin{array}{r}{\\frac{\\mathrm{d}\\pmb{\\theta}(t)}{\\mathrm{d}t}=-\\nabla\\mathcal{L}(\\pmb{\\theta}(t))}\\end{array}$ starting from $\\pmb\\theta(0)=\\pmb\\theta$ .   \nDenote by $\\begin{array}{r}{\\Phi(\\pmb\\theta):=\\operatorname*{lim}_{t\\rightarrow\\infty}\\pmb\\theta(t)}\\end{array}$ the limiting map of this GF. ", "page_idx": 8}, {"type": "text", "text": "Definition 5.3 (Attraction set of $\\mathcal{M}$ ). Let $U$ be the attraction set of $\\mathcal{M}$ under GF, i.e., GF starting in $U$ converges to some point in $\\mathcal{M}$ . Formally, $U:=\\{\\pmb\\theta\\in\\mathbb{R}^{p}:\\Phi(\\pmb\\theta)\\in\\mathcal{M}\\}$ . ", "page_idx": 8}, {"type": "text", "text": "As proven in (Arora et al., 2022, Lemma B.15), Assumption 5.1 ensures that $U$ (in Definition 5.3) is open and $\\Phi(\\cdot)$ (in Definition 5.2) is $\\mathcal{C}^{2}$ on $U$ . ", "page_idx": 8}, {"type": "text", "text": "5.2 Theoretical results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The stochastic SAM (Foret et al., 2021) is given by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\eta\\nabla\\mathcal{L}_{i_{t}}\\left(\\theta_{t}+\\rho\\frac{\\nabla\\mathcal{L}_{i_{t}}(\\theta_{t})}{\\left\\|\\nabla\\mathcal{L}_{i_{t}}(\\theta_{t})\\right\\|}\\right),\\;\\mathrm{where}\\;i_{t}\\sim\\mathbb{U}([n]).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The generalization capability of standard SAM can be bounded by the average sharpness, $\\mathcal{L}^{\\mathrm{avg}}(\\theta):=$ $\\mathbb{E}_{\\xi\\sim\\mathcal{N}(\\mathbf{0},I)}\\mathcal{L}\\left(\\pmb{\\theta}+\\rho\\pmb{\\xi}/\\|\\pmb{\\xi}\\|\\right)$ (Foret et al., 2021). This leads researchers to also explore average SAM (Wen et al., 2023a; Zhu et al., 2023; Ujv\u00e1ry et al., 2022), which minimizes $\\mathcal{L}^{\\mathrm{avg}}$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\eta\\nabla\\mathcal{L}\\left(\\pmb{\\theta}_{t}+\\rho\\pmb{\\xi}_{t}/\\|\\pmb{\\xi}_{t}\\|\\right),\\;\\mathrm{where}\\;\\pmb{\\xi}_{t}\\sim\\mathcal{N}(\\mathbf{0},I).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Two-phase algorithms. Our theoretical focus is on how IRE accelerates the sharpness reduction of SAM (7) and (8) near the minima manifold $\\mathcal{M}$ . Thus, we analyze the two-phase algorithms. Specifically, let the initialization $\\pmb\\theta_{0}\\in U$ . In Phase I $(t\\leqslant T_{\\mathrm{I}})$ , we employ GF $\\begin{array}{r}{\\frac{\\mathrm{d}\\bar{\\theta_{t}}}{\\mathrm{d}t}=-\\bar{\\nabla}\\mathcal{L}(\\pmb{\\theta_{t}})}\\end{array}$ to ensure that the loss decreases sufficiently; then in Phase $\\mathbf{II}$ $T_{\\mathrm{I}}<t\\leqslant T_{\\mathrm{I}}+T_{\\mathrm{II}}:=\\stackrel{\\mathrm{w}}{T}$ ), we incorporate IRE into the standard / average SAM. ", "page_idx": 8}, {"type": "text", "text": "Effective dynamics: sharpness regularization. The implicit regularization of SAMs can be modeled using effective dynamics. In Phase II, $\\theta_{t}$ are close the manifold of minimizers $\\mathcal{M}$ and let $z_{t}\\;:=\\;\\Phi(\\pmb\\theta_{t})\\;\\in\\;\\mathcal{M}$ . Then, the effective dynamics is given by $\\{z_{t}\\}_{t=T_{\\mathrm{I}}+1}^{T}$ ,P arr etiv\u2208ceuallMairnlgy,  hWoewn  Set AalM. s( 2e0x2p3lao)r seh tohwe edm tahnaitf othlde  eofff ecmtiinvie$\\bar{\\mathcal{M}}$ dynamics of standard/average SAM are both ", "page_idx": 8}, {"type": "image", "img_path": "cjM2bhLOiC/tmp/9c309013b535a101e134ca2ff57faf909ead8a4620cac6464f1f640efb4fc52b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[z_{t+1}\\right]=z_{t}-\\eta_{\\mathrm{eff}}\\nabla_{\\mathcal{M}}\\mathrm{Tr}\\left[\\nabla^{2}\\mathcal{L}(z_{t})/2\\right]+o(\\eta_{\\mathrm{eff}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which minimizes the trace of Hessian on $\\mathcal{M}$ . The difference between the standard SAM (7) and average SAM (8) lies in the effective learning rate (LR) $\\eta_{\\mathrm{eff}}$ \u2019s. A visual illustration of some quantities in (9) is provided in the figure above. ", "page_idx": 8}, {"type": "text", "text": "Summary of our theoretical results. In this section, we show that incorporating IRE into SAMs can significantly increase the effective LR $\\eta_{\\mathrm{eff}}$ in (9) while maintaining the same training stability as SAMs. In Table 7, we present the effective LR for SAMs and the SAM-IREs. We see clearly that IRE can accelerate the sharpness reduction by a non-trivial factor for both standard and average SAM. ", "page_idx": 8}, {"type": "table", "img_path": "cjM2bhLOiC/tmp/091d02baa940b26f352d50c59eee6741b8c2b924cfbf4f182760d79b2eca7d20.jpg", "table_caption": ["Table 7: Comparison of the implicit regularization strength of SAMs w/o IRE. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Remark 5.4 (The mechanism of IRE\u2019s success). The success of SAM-IRE follows the same mechanism illustrated in Section 2. The key fact that IRE only increases the LR along flat directions has two implications: 1) It does not change the trend of implicit regularization in Eq. (9) but accelerates SAMs\u2019 effective dynamics by a factor of $(1+\\kappa)$ ; 2) Since the LR is only increased along flat directions, $\\kappa$ can be set substantially large without hurting the training stability, because the dynamics in sharp directions remain unchanged. Specifically, we theoretically justify in SAM-IRE, $\\kappa$ can be selected as large as $1/\\rho$ . ", "page_idx": 8}, {"type": "text", "text": "5.2.1 IRE on average SAM: An $\\Omega(1/\\eta^{0.5})$ acceleration ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We first consider IRE on average SAM. Let $T_{\\mathrm{I}}$ be the hitting time: $T_{\\mathrm{I}}:=\\operatorname*{inf}\\{t\\geqslant0:\\|\\pmb\\theta_{t}-\\Phi(\\pmb\\theta_{t})\\|=$ ${\\mathcal{O}}({\\sqrt{\\eta}}\\rho)\\}$ . When running GF in Phase I, Definition 5.3 guarantees $T_{\\mathrm{I}}<\\infty$ . Thus, at the starting of Phase II, $\\lVert\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\rVert=\\mathcal{O}(\\sqrt{\\eta}\\rho)$ . Furthermore, the following result holds for Phase II. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.5 (IRE on average SAM). Suppose Assumption 5.1 holds. If $\\eta=\\mathcal{O}(1)$ and $\\rho=\\mathcal{O}(\\sqrt{\\eta})$ in SAM (8), $\\kappa\\leqslant1/\\rho$ , and $\\mathscr{P}_{t}={P}_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{t}))$ in $I R E$ (4), then with high probability at least $1-T_{\\mathrm{II}}^{2}\\exp\\left(-\\Omega\\left(1/\\left(\\eta+p^{-1}\\right)\\right)\\right)$ , $\\lVert\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\rVert=\\mathcal{O}(\\sqrt{\\eta}\\rho)$ holds for all $T_{\\mathrm{I}}\\leqslant t\\leqslant T$ . Furthermore, the effective dynamics of $\\boldsymbol{z}_{t}:=\\Phi(\\boldsymbol{\\theta}_{t})\\in\\mathcal{M}$ satisfies: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\xi_{t}}[z_{t+1}]=z_{t}-\\frac{(1+\\kappa)\\eta\\rho^{2}}{p}\\nabla_{M}\\operatorname{Tr}\\left[\\nabla^{2}\\mathcal{L}(z_{t})/2\\right]+\\mathcal{O}(\\eta^{3/2}\\rho^{2}).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Note that $\\rho=\\mathcal{O}(\\sqrt{\\eta})$ and $\\kappa$ can be as large as $1/\\rho$ . Consequently, the effective LR of minimizing the trace of Hessian can be selected as large as $\\eta_{\\mathrm{eff}}=(\\kappa+1)\\eta\\rho^{2}/p=\\mathcal{O}(\\eta^{1.5}/p)$ . In contrast, that of average SAM is at most ${\\mathcal{O}}(\\eta^{2}/p)$ . The proof of Theorem 5.5 can be found in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "5.2.2 IRE on standard SAM: An $\\Omega(1/\\rho)$ acceleration ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This subsection delves into IRE on standard SAM (7), which is more widely used and often yields better performance than average SAM (8). However, since standard SAM (7) requires stochastic gradients $\\nabla{\\mathcal{L}}_{i}(\\pmb{\\theta})\\;(i\\in[n])$ , we need an additional assumption regarding the features on the manifold (see Setting E.1), which is commonly used in the literature (Du et al., 2018; 2019; Li et al., 2022; Arora et al., 2022; Wen et al., 2023a). We defer it to Appendix E due to space constraints. Under this Setting, Assumption 5.1 holds naturally with $m=n$ . ", "page_idx": 9}, {"type": "text", "text": "During Phase I of GF, Definition 5.3 ensures that there exists $t<\\infty$ such that $\\lVert{\\pmb\\theta}_{t}-\\Phi({\\pmb\\theta}_{t})\\rVert=$ $\\mathcal{O}(\\eta^{1-\\alpha}\\rho)$ for any $\\alpha\\in[0,1)$ . We define $T_{\\mathrm{I}}$ as the hitting time: $T_{\\mathrm{I}}:=\\operatorname*{inf}\\{t\\geqslant0:\\|\\pmb\\theta_{t}-\\Phi(\\pmb\\theta_{t})\\|=$ ${\\mathcal{O}}(\\eta^{1-\\alpha}\\rho)\\}$ . Then the following result holds for Phase II, whose proof can be founded in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.6 (IRE on standard SAM). Under Setting E.1, if $\\eta,\\rho=\\mathcal{O}(1)$ in SAM (7), $\\kappa\\leqslant1/\\rho$ , and $\\mathcal{P}_{t}=P_{n+1:p}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{t}))$ in IRE (4), then with high probability at least $1-T_{\\mathrm{II}}^{2}\\exp\\left(-\\Omega(1/\\eta^{\\alpha})\\right)$ , $\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|\\,=\\,\\mathcal{O}(\\eta^{1-\\alpha}\\rho)$ holds for all $T_{\\mathrm{I}}\\,\\leqslant\\,t\\,\\leqslant\\,T$ . Moreover, the effective dynamics $z_{t}~=$ $\\Phi(\\pmb{\\theta}_{t})\\in\\mathcal{M}$ satisfies: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathbb{E}_{i_{t}}[z_{t+1}]=z_{t}-(1+\\kappa)\\eta\\rho^{2}\\nabla_{M}\\,\\mathrm{Tr}\\left[\\nabla^{2}\\mathcal{L}(z_{t})/2\\right]+\\mathcal{O}\\left((\\kappa+1)\\eta\\rho^{2}(\\rho+\\eta^{1-\\alpha})\\right).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Taking $\\kappa=0$ and $\\alpha=0$ recovers the result established in Wen et al. (2023a). However, $\\kappa$ can be as large as $1/\\rho$ , where IRE provides a $\\Theta(1/\\rho)$ -time acceleration over the standard SAM. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a novel IRE framework to enhance the implicit sharpness regularization of base optimizers. Experiments demonstrate that IRE not only consistently improves generalization but also accelerates loss convergence in the pre-training of Llama models of various sizes. The code is available at https://github.com/wmz9/IRE-algorithm-framework. ", "page_idx": 9}, {"type": "text", "text": "For future work, there are two urgent directions: 1) understanding why IRE can accelerate convergence, which may require studying the interplay between IRE and the Edge of Stability (EoS) (Wu et al., 2018; Jastrz\u02dbebski et al., 2017; Cohen et al., 2021); and 2) conducting a larger-scale investigation into the acceleration of AdmIRE compared to AdamW in LLM pre-training, as well as the downstream performance of the LLMs pre-trained by AdmIRE. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Lei Wu is supported by the National Key R&D Program of China (No. 2022YFA1008200) and National Natural Science Foundation of China (No. 12288101). Mingze Wang is supported in part by the National Key Basic Research Program of China (No. 2015CB856000). We thank Dr. Hongkang Yang, Liu Ziyin, Liming Liu, Zehao Lin, Hao Wu, and Kai Chen for helpful discussions and anonymous reviewers for their valuable suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on the edge of stability in deep learning. In International Conference on Machine Learning, pages 948\u20131024. PMLR, 2022. 8, 9, 10, 21, 22, 23   \nGuy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process. In Conference on learning theory, pages 483\u2013513. PMLR, 2020. 2   \nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. Advances in Neural Information Processing Systems, 36, 2024. 3, 20   \nLenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on Learning Theory, pages 1305\u20131338. PMLR, 2020. 17   \nJeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. International Conference on Learning Representations, 2021. 2, 10   \nYaim Cooper. The loss landscape of overparameterized neural networks. arXiv preprint arXiv:1804.10200, 2018. 8   \nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. 21   \nAlex Damian, Tengyu Ma, and Jason D Lee. Label noise SGD provably prefers flat global minimizers. Advances in Neural Information Processing Systems, 34:27449\u201327461, 2021. 2, 18   \nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009. 6   \nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2, 6   \nFelix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In International conference on machine learning, pages 1309\u20131318. PMLR, 2018. 8   \nJiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent YF Tan. Efficient sharpness-aware minimization for improved training of neural networks. arXiv preprint arXiv:2110.03141, 2021. 2   \nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International Conference on Machine Learning, pages 1675\u20131685. PMLR, 2019. 10   \nSimon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018. 10   \nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011. 2   \nMathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (S)GD over diagonal linear networks: Implicit regularisation, large stepsizes and edge of stability. arXiv preprint arXiv:2302.08982, 2023. 18   \nMathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (S) GD over diagonal linear networks: Implicit bias, large stepsizes and edge of stability. Advances in Neural Information Processing Systems, 36, 2024. 2   \nBenjamin Fehrman, Benjamin Gess, and Arnulf Jentzen. Convergence rates for the stochastic gradient descent method for non-convex objective functions. Journal of Machine Learning Research, 21, 2020. 8   \nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. International Conference on Learning Representations, 2021. 2, 8, 9, 19, 20   \nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. 7   \nTimur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. Advances in neural information processing systems, 31, 2018. 8   \nKhashayar Gatmiry, Zhiyuan Li, Ching-Yao Chuang, Sashank Reddi, Tengyu Ma, and Stefanie Jegelka. The inductive bias of flatness regularization for deep matrix factorization. arXiv preprint arXiv:2306.13239, 2023. 2   \nThomas George, C\u00e9sar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast approximate natural gradient descent in a Kronecker-factored eigenbasis. Advances in Neural Information Processing Systems, 31, 2018. 5   \nAaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. 7   \nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 2   \nRoger Grosse and James Martens. A Kronecker-factored approximate Fisher matrix for convolution layers. In International Conference on Machine Learning, pages 573\u2013582. PMLR, 2016. 5   \nSuriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. Advances in neural information processing systems, 31, 2018. 17   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. 2, 6   \nByeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, and Jung-Woo Ha. Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights. arXiv preprint arXiv:2006.08217, 2020. 3   \nSepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural computation, 9(1):1\u201342, 1997. 1   \nElad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. arXiv preprint arXiv:1705.08741, 2017. 2   \nStanis\u0142aw Jastrze\u02dbbski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in SGD. arXiv preprint arXiv:1711.04623, 2017. 2, 10   \nZiwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. International Conference on Learning Representations, 2019a. 17   \nZiwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. Conference on Learning Theory, 2019b. 17   \nZiwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. Advances in Neural Information Processing Systems, 33:17176\u201317186, 2020. 17   \nZiwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis. In Algorithmic Learning Theory, pages 772\u2013804. PMLR, 2021. 17   \nZiwei Ji, Miroslav Dud\u00edk, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the regularization path for general losses. In Conference on Learning Theory, pages 2109\u20132136. PMLR, 2020. 17   \nZiwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization via dual acceleration. In International Conference on Machine Learning, pages 4860\u20134869. PMLR, 2021. 17   \nYiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning Representations, 2019. 2   \nJean Kaddour. The MiniPile challenge for data-efficient language models. arXiv preprint arXiv:2304.08442, 2023. 7   \nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2016. 1   \nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 2   \nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images, 2009. URL https://www.cs.toronto.edu/\\~kriz/cifar.html. 5   \nDaniel Kunin, Atsushi Yamamura, Chao Ma, and Surya Ganguli. The asymmetric maximum margin bias of quasi-homogeneous neural networks. International Conference on Learning Representations, 2023. 17   \nJungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: Adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning, pages 5905\u20135914. PMLR, 2021. 2, 20   \nQianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic gradient algorithms. In International Conference on Machine Learning, pages 2101\u20132110. PMLR, 2017. 18   \nQianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and dynamics of stochastic gradient algorithms I: Mathematical foundations. The Journal of Machine Learning Research, 20 (1):1474\u20131520, 2019. 18   \nZhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after SGD reaches zero loss?\u2013a mathematical framework. International Conference on Learning Representations, 2022. 2, 4, 8, 10, 18   \nHong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better downstream: Implicit bias matters for language models. In International Conference on Machine Learning, pages 22188\u201322214. PMLR, 2023. 8   \nHong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training. International Conference on Learning Representations, 2024. 3, 5, 21   \nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019a. 3   \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019b. 7   \nYong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable sharpness-aware minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12360\u201312370, 2022. 2   \nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 3   \nLiangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843, 2019. 3   \nKaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. arXiv preprint arXiv:1906.05890, 2019. 17   \nKaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer nets: Margin maximization and simplicity bias. Advances in Neural Information Processing Systems, 34, 2021. 17   \nKaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of normalization layers: Sharpness reduction. Advances in Neural Information Processing Systems, 35: 34689\u201334708, 2022. 2   \nChao Ma and Lexing Ying. On linear stability of SGD and input-smoothness of neural networks. Advances in Neural Information Processing Systems, 34:16805\u201316817, 2021. 2   \nChao Ma, Daniel Kunin, Lei Wu, and Lexing Ying. Beyond the quadratic approximation: The multiscale structure of neural network loss landscapes. Journal of Machine Learning, 1(3): 247\u2013267, 2022. 2, 4, 18   \nJames Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In International conference on machine learning, pages 2408\u20132417. PMLR, 2015. 5   \nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. 7   \nPeng Mi, Li Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and Dacheng Tao. Make sharpness-aware minimization stronger: A sparsified perturbation approach. Advances in Neural Information Processing Systems, 35:30950\u201330962, 2022. 2, 5   \nMaximilian Mueller, Tiffany Vlaar, David Rolnick, and Matthias Hein. Normalization layers are all that sharpness-aware minimization needs. Advances in Neural Information Processing Systems, 36, 2024. 2, 6, 20   \nRotem Mulayoff, Tomer Michaeli, and Daniel Soudry. The implicit bias of minima stability: A view from function space. Advances in Neural Information Processing Systems, 34:17749\u201317761, 2021. 2   \nMor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In International Conference on Machine Learning, pages 4683\u20134692. PMLR, 2019a. 17   \nMor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3420\u20133428. PMLR, 2019b. 17   \nMor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3051\u20133059. PMLR, 2019c. 17   \nMor Shpigel Nacson, Kavya Ravichandran, Nathan Srebro, and Daniel Soudry. Implicit bias of the step size in linear diagonal neural networks. In International Conference on Machine Learning, pages 16270\u201316295. PMLR, 2022. 18   \nYurii Nesterov. A method of solving a convex programming problem with convergence rate $o(1/k^{2})$ . Doklady Akademii Nauk SSSR, 269(3):543, 1983. 2 Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014. 1 Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem. CoRR, abs/1211.5063, 2(417):1, 2012. 7 Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks. Advances in Neural Information Processing Systems, 2023. 18 Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of SGD for diagonal linear networks: a provable benefit of stochasticity. Advances in Neural Information Processing Systems, 34:29218\u201329230, 2021. 18 Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 7 Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951. 1 David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Nature, 323(6088):533\u2013536, 1986. 1 Noam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 21 Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):   \n2822\u20132878, 2018. 17 Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 21 Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139\u2013   \n1147. PMLR, 2013. 2 Tijmen Tieleman. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26, 2012. 2 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2, 7, 20, 21 Szilvia Ujv\u00e1ry, Zsigmond Telek, Anna Kerekes, Anna M\u00e9sz\u00e1ros, and Ferenc Husz\u00e1r. Rethinking sharpness-aware minimization as variational inference. arXiv preprint arXiv:2210.10452, 2022. 9 Gal Vardi. On the implicit bias in deep-learning algorithms. Communications of the ACM, 66(6):   \n86\u201393, 2023. 2, 17 Gal Vardi, Ohad Shamir, and Nati Srebro. On margin maximization in linear and ReLU networks. Advances in Neural Information Processing Systems, 35:37024\u201337036, 2022. 17 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 20 Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018. 38 Guanghui Wang, Rafael Hanashiro, Etash Guha, and Jacob Abernethy. On accelerated perceptrons and beyond. arXiv preprint arXiv:2210.09371, 2022. 17 Mingze Wang and Chao Ma. Understanding multi-phase optimization dynamics and rich nonlinear behaviors of ReLU networks. Advances in Neural Information Processing Systems, 2023. 17   \nMingze Wang and Lei Wu. The noise geometry of stochastic gradient descent: A quantitative and analytical characterization. arXiv preprint arXiv:2310.00692, 2023. 2   \nMingze Wang, Zeping Min, and Lei Wu. Achieving margin maximization exponentially fast via progressive norm rescaling. International Conference on Machine Learning, 2024. 17   \nKaiyue Wen, Tengyu Ma, and Zhiyuan Li. How sharpness-aware minimization minimizes sharpness? In The Eleventh International Conference on Learning Representations, 2023a. 4, 8, 9, 10, 32, 33, 34   \nKaiyue Wen, Tengyu Ma, and Zhiyuan Li. Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. Advances in Neural Information Processing Systems, 2023b. 2   \nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on Learning Theory, pages 3635\u20133673. PMLR, 2020. 18   \nLei Wu and Weijie J Su. The implicit regularization of dynamical stability in stochastic gradient descent. In The 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 37656\u201337684. PMLR, 2023. 2   \nLei Wu, Zhanxing Zhu, and Weinan E. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017. 1   \nLei Wu, Chao Ma, and Weinan E. How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective. Advances in Neural Information Processing Systems, 31:8279\u20138288, 2018. 2, 10   \nLei Wu, Mingze Wang, and Weijie J Su. The alignment property of SGD noise and how it helps select flat minima: A stability analysis. Advances in Neural Information Processing Systems, 35: 4680\u20134693, 2022. 2   \nXingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan. Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models. arXiv preprint arXiv:2208.06677, 2022a. 3   \nZeke Xie, Xinrui Wang, Huishuai Zhang, Issei Sato, and Masashi Sugiyama. Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum. In International conference on machine learning, pages 24430\u201324459. PMLR, 2022b. 3   \nZhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks through the lens of the Hessian. In 2020 IEEE international conference on big data (Big data), pages 581\u2013590. IEEE, 2020. 4   \nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. 5, 6   \nBiao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. 21   \nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017. 1   \nTongtian Zhu, Fengxiang He, Kaixuan Chen, Mingli Song, and Dacheng Tao. Decentralized SGD and average-direction SAM are asymptotically equivalent. In International Conference on Machine Learning, pages 43005\u201343036. PMLR, 2023. 9   \nJuntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. Advances in neural information processing systems, 33:18795\u201318806, 2020. 3 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Other Related Works 17 ", "page_idx": 16}, {"type": "text", "text": "B Proofs for SGD in Section 2 18 ", "page_idx": 16}, {"type": "text", "text": "C Experimental Details 19   \nC.1 Experimental details in Section 4.1 19   \nC.2 Experimental details in Section 4.2 20   \nD Proofs in Section 5.2.1 21   \nD.1 Preliminary Lemmas 21   \nD.2 Proof of Theorem 5.5 24   \nE.1 Preliminary Lemmas 32   \nE.2 Proof of Theorem 5.6 34 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "37 ", "page_idx": 16}, {"type": "text", "text": "A Other Related Works ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Other Implicit Biases. Beyond the implicit sharpness regularization, numerous other attempts have explored implicit biases in deep learning algorithms Vardi (2023). Among these, a popular research line is the max-margin bias, which suggests that optimizers favor the solutions with large margin, which generalizes well. Soudry et al. (2018) showed that GD converges to max-margin solutions under exponentially-tailed loss on linearly separable data, albeit with an extremely slow rate $\\mathcal{O}(1/\\log t)$ . Furthermore, Nacson et al. (2019c) studied this bias for SGD, Ji and Telgarsky (2019b) investigated linearly non-separable data, and Ji et al. (2020) analyzed the effects of the tail behavior of loss functions. ", "page_idx": 16}, {"type": "text", "text": "To achieve faster margin maximization, Nacson et al. (2019b); Ji and Telgarsky (2021) demonstrated that GD with aggressively loss-scaled step sizes can achieve a faster polynomial rate of $\\mathcal{O}(1/t)$ . Building on this, Ji et al. (2021); Wang et al. (2022) proposed momentum-based gradient methods which achieve a rate of ${\\mathcal{O}}(1/t^{2})$ . Recently, Wang et al. (2024) established that the polynomial rates for most previous algorithms are tight, and proposed a progressive rescaling gradient descent method that achieves margin maximization exponentially fast $\\bar{O}\\bar{(}e^{-\\Omega(t)})$ . ", "page_idx": 16}, {"type": "text", "text": "The margin-maximization bias has also been studied for nonlinear models. Ji and Telgarsky (2019a); Gunasekar et al. (2018) examined deep linear networks, while Chizat and Bach (2020) focused on wide two-layer ReLU networks. Notably, Nacson et al. (2019a); Lyu and Li (2019); Ji and Telgarsky (2020) demonstrated that for general homogeneous networks, Gradient Flow (GF) and GD converge to solutions corresponding the KKT point of the max-margin problem. Recently, Kunin et al. (2023) extended this analysis to quasi-homogeneous networks. For two-layer (leaky-)ReLU neural networks, Lyu et al. (2021); Vardi et al. (2022); Wang and Ma (2023) examined whether the convergent KKT point of GF correspond to global optima of the max-margin problem. ", "page_idx": 16}, {"type": "text", "text": "Future work could investigate whether the IRE framework can also enhance the margin maximization bias, although it is primarily designed for enhancing the implicit sharpness regularization. ", "page_idx": 17}, {"type": "text", "text": "Additionally, Woodworth et al. (2020); Pesme et al. (2021); Nacson et al. (2022); Pesme and Flammarion (2023); Even et al. (2023) conducted fine-grained analyses of training dynamics, examining how initialization and step size impact (S)GD\u2019s minima selection in linear diagonal networks. ", "page_idx": 17}, {"type": "text", "text": "B Proofs for SGD in Section 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For the example in Section 2, we have studied the implicit sharpness regularization of GD dynamics and how IRE enhances the implicit regularization of GD. In this Section, we illustrate that, for this example, similar results hold for SGD dynamics. ", "page_idx": 17}, {"type": "text", "text": "SDE Modelling of SGD. We consider SGD approximated by SDE (Li et al., 2017; 2019; 2022) with noise covariance $\\Sigma$ : $\\mathrm{d}\\pmb{\\theta}_{t}\\,=\\,-\\nabla\\mathcal{L}(\\pmb{\\theta}_{t})\\mathrm{d}t+\\sqrt{\\eta\\Sigma(\\pmb{\\theta}_{t})}\\mathrm{d}W_{t}$ . We consider that the noise covariance aligns with the Hessian near minima, i.e., $\\Sigma(\\pmb{\\theta})=\\left(\\begin{array}{c c}{\\mathbf{0}_{d}}&{0}\\\\ {0}&{\\sigma^{2}h(\\pmb{u})}\\end{array}\\right)$ (where $\\sigma>0$ is a scalar), such as the label noise (Damian et al., 2021; Li et al., 2022). Then, the SDE above can be rewritten as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\left(\\!\\!\\begin{array}{c}{u_{t}}\\\\ {v_{t}}\\end{array}\\!\\!\\right)=-\\left(\\!\\!\\begin{array}{c}{v_{t}^{2}\\nabla h(u_{t})/2}\\\\ {v_{t}h(u_{t})}\\end{array}\\!\\!\\right)\\mathrm{d}t+\\left(\\!\\!\\begin{array}{c}{0}\\\\ {\\sqrt{\\eta\\sigma^{2}h(u_{t})}\\mathrm{d}W_{t}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Implicit Sharpness Regularization. Intuitively, when $v_{t}$ is close to 0, the speed of $\\pmb{u}_{t}$ is much slower than $v_{t}$ due to $v_{t}^{2}\\ll v_{t}$ . Following Ma et al. (2022), when this speed separation is large, $v_{t}$ is always at equilibrium given $\\pmb{u}_{t}$ . Solving the Ornstein\u2013Uhlenbeck process about $v_{t}$ , we know the equilibrium distribution of $v_{t}$ is $v_{\\infty}\\sim\\mathcal{N}(\\bar{0},\\eta\\sigma^{2})$ , and hence the dynamics $\\pmb{u}_{t}$ (along the manifold) is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\boldsymbol{u}_{t}/\\mathrm{d}t=-\\mathbb{E}_{\\boldsymbol{v}_{\\infty}}\\left[v_{\\infty}^{2}\\nabla h(\\boldsymbol{u}_{t})/2\\right]=-\\nabla h(\\boldsymbol{u}_{t})/2.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This derivation clearly shows the slow \u201ceffective dynamics\u201d $\\pmb{u}_{t}$ along the manifold is a gradient flow minimizing the sharpness $h(\\cdot)$ . When SGD minimizes the loss, it also minimzes the sharpness implicitly, that is to say, SGD has the following implicit sharpness regularization: min\u03b8 $\\operatorname{Tr}(\\nabla^{2}\\bar{\\mathcal{L}}(\\pmb{\\theta}))$ s.t. $\\mathcal{L}(\\pmb{\\theta})\\approx0$ . ", "page_idx": 17}, {"type": "text", "text": "Generalization and Optimization benefits of the sharpness regularization. In terms of generalization, as discussed in related works, a common view is that flat minima generalize well, which has been proved in a large number of previous works. In addition, in terms of optimization, after SGD reaches the equilibrium $v_{\\infty}\\sim\\mathcal{N}(0,\\dot{\\eta}\\sigma^{2})$ , the loss near the flat minimum is smaller because $\\mathbb{E}_{v_{\\infty}}[\\mathcal{L}(\\pmb{\\theta})]=$ $\\mathbb{E}_{v_{\\infty}}[\\bar{h}(\\pmb{u})\\ensuremath{v_{\\infty}}^{2}/2]=\\eta\\sigma^{2}h(\\pmb{u})/2\\stackrel{.}{\\propto}\\mathrm{Tr}(\\nabla^{2}\\mathcal{L}(\\pmb{u}))$ . ", "page_idx": 17}, {"type": "text", "text": "Q. How can we enhance the implicit sharpness regularization of SGD? A. Accelerating SGD\u2019s slow \u201ceffective dynamics\u201d $\\pmb{u}_{t}$ along the minima manifold. ", "page_idx": 17}, {"type": "text", "text": "Implicit Regularization Enhancement (IRE) by accelerating the effective dynamics along minima manifold. First, it is worth noting that naively increasing the learning rate $\\eta$ cannot achieve our aim, because increasing $\\eta$ will influence the dynamic stability of $v_{t}$ and the equilibrium $v_{\\infty}$ . Therefore, we need to accelerate the effective dynamics $\\pmb{u}_{t}$ without affecting the dynamics of $v_{t}$ . Another main point is that SGD\u2019s effective dynamics $\\pmb{u}_{t}$ can naturally minimize the sharpness implicitly, so we only need to enhance this property. To achieve this, we only need to correct the update direction in (10) from $-\\nabla{\\mathcal{L}}(\\pmb{\\theta}_{t})$ to $-(\\bar{\\nabla}\\bar{\\mathcal{L}}(\\pmb{\\theta}_{t})+\\kappa P_{\\mathcal{M}}\\nabla\\mathcal{L}(\\pmb{\\theta}_{t}))$ , where $P_{\\mathcal{M}}$ is the projection matrix to the manifold $\\mathcal{M}$ and $\\kappa$ is a scalar. Using this new algorithm, the SDE dynamics corresponds to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\left(\\!\\!\\begin{array}{c}{u_{t}}\\\\ {v_{t}}\\end{array}\\!\\!\\right)=-\\left(\\!\\!\\begin{array}{c}{(1+\\kappa)v_{t}^{2}\\nabla h(u_{t})/2}\\\\ {v_{t}h(u_{t})}\\end{array}\\!\\!\\right)\\mathrm{d}t+\\left(\\!\\!\\begin{array}{c}{0}\\\\ {\\sqrt{\\eta\\sigma^{2}h(u_{t})}\\mathrm{d}W_{t}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Comparing (10) and (12), the dynamics of $v_{t}$ are the same, so they attain the same equilibrium distribution $v_{\\infty}\\sim\\mathcal{N}(0,\\eta\\sigma^{2})$ . As for the effective dynamics along manifold, (10) corresponds to the form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\boldsymbol{u}_{t}/\\mathrm{d}t=-\\mathbb{E}_{\\boldsymbol{v}_{\\infty}}\\left[(1+\\kappa)\\boldsymbol{v}_{\\infty}^{2}\\nabla h(\\boldsymbol{u}_{t})/2\\right]=-(1+\\kappa)\\nabla h(\\boldsymbol{u}_{t})/2.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$(1+\\kappa)$ times Enhancement. Comparing (13) and (11), it is clear that our new algorithm can enhance implicit sharpness regularization $(1+\\kappa)$ times faster than the original SGD. ", "page_idx": 18}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section describes the experimental details in Section 4. ", "page_idx": 18}, {"type": "text", "text": "C.1 Experimental details in Section 4.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1.1 Experimental details in Section 4.1.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We train WideResNet-16-8 on CIFAR-10 dataset by SAM-IRE (with $K=10$ , varying $\\kappa$ and $\\gamma$ ). The experiments employ basic data augmentations and 0.1 label smoothing, as outlined by Foret et al. (2021). The mini-batch size is set to 128, the weight decay is set to 5e-4, and the $\\rho$ in SAM is to 0.05, as in Foret et al. (2021). To evaluate the implicit flatness regularization of SAM itself, the momentum is set to 0.0. Regarding the learning rate (lr), we evaluate for both constant lr (within our theoretical framework) and decayed lr (common in practice though not covered by our theory). In the experiment in Fig 2 (a), a fixed $1\\mathrm{r}\\,0.1$ is used. In the experiment in Fig 2 (b)(c)(d), a step-decayed lr schedule is employed, starting at 0.1 and reducing lr by a factor of 5 at epoch 20, 50, 80. We transit from SAM to SAM-IRE at the 30th epoch out of 100 total epochs. We test $\\gamma$ in $0.8,0.9,0.95$ , and $\\kappa$ in 0 (original SAM), 2, 5, 10. ", "page_idx": 18}, {"type": "text", "text": "The flatness measure, $\\operatorname{Tr}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}))$ , is approximated by the trace of Fisher $\\operatorname{Tr}(F(\\pmb\\theta))$ . Specifically, we use $\\mathrm{diag}(\\hat{F}_{\\mathrm{eff}}(\\pmb{\\theta}))$ in (6) for the estimate because $\\mathbb{E}_{\\hat{\\pmb{y}}}[\\mathrm{diag}(\\hat{F}_{\\mathrm{eff}}(\\pmb{\\theta}))]=\\mathbb{E}_{\\hat{\\pmb{y}}}[\\mathrm{diag}(\\hat{F}(\\pmb{\\theta}))]$ and thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Tr}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}))\\approx\\mathbb{E}_{\\hat{y}}[\\mathrm{Tr}(\\hat{F}(\\pmb{\\theta}))]=\\mathbb{E}_{\\hat{y}}[\\mathrm{Tr}(\\mathrm{diag}(\\hat{F}(\\pmb{\\theta})))]=\\mathbb{E}_{\\hat{y}}[\\mathrm{diag}(\\hat{F}_{\\mathrm{eff}}(\\pmb{\\theta}))].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, the first $\\mathrel{\\sim}\\mathrel{\\tilde{}}$ above takes $\\vDash\\v{\\omega}^{\\vartriangle}$ when $\\mathcal{L}(\\pmb{\\theta})=0$ . ", "page_idx": 18}, {"type": "text", "text": "In this section, all experiments were conducted using a single A800 GPU. ", "page_idx": 18}, {"type": "text", "text": "C.1.2 Experimental details in Section 4.1.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Experiments for CNNs on CIFAR-10/CIFAR-100. We first evaluate the impact of IRE on generalization of baseline models (WideResNet-28-10 and ResNet-56) and default optimizers (SGD and SAM) on CIFAR-{10,100}. For the base optimizers, SGD and SAM, cosine learning rate decay is adopted with an initial lr 0.1. For other training components, we follow the settings in Foret et al. (2021): basic data augmentations and 0.1 label smoothing; for both SGD and SAM, the momentum is set to 0.9, the batch size is set to 128, and the weight decay is set to 5e-4; for SAM, $\\rho$ is set to 0.05 for CIFAR-10 and 0.1 for CIFAR-100. The total epochs is set to 100 for CIFAR-10 and 200 for CIFAR-100, and we switch from SGD/SAM to SGD-IRE/SAM-IRE when the training loss approaches 0.1. For SGD-IRE/SAM-IRE, we fix $K=10$ , and tune hyperparameters $\\gamma$ and $\\kappa$ via a grid search over $\\gamma\\in\\{0.99,0.9,0.8\\}$ and $\\kappa\\in\\{1,2\\}$ . The results are reported in Table 1. ", "page_idx": 18}, {"type": "text", "text": "Experiments without finely tuned hyperparameters. A high sensitivity to the choice of hyperparameters would make a method less practical. To demonstrate that our IRE performs even when $\\kappa,\\gamma$ are not finely tuned, we conduct experiments using fixed $\\gamma=0.99$ , $\\kappa=1$ , under the same settings as described above.. The results (averaged over 3 random seeds) are reported in Table 8. ", "page_idx": 18}, {"type": "table", "img_path": "cjM2bhLOiC/tmp/3149f818e98ad46e001a909a00c6c409aab660bf61a25e64abb839d91711ae94.jpg", "table_caption": ["Table 8: Results for SGD-IRE and SAM-IRE on {WideResNet-28-10, ResNet-56} on CIFAR-{10, 100}, using fixed $\\gamma=0.99,\\kappa=1$ in IRE. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Experiments for CNNs on ImageNet. We also examine the impact of IRE on generalization of ResNet-50 and default optimizers (SGD and SAM) on on ImageNet. Following Foret et al. (2021) and Kwon et al. (2021), we use basic data augmentations and 0.1 label smoothing. For the base optimizers, SGD and SAM, we also follow the settings in Kwon et al. (2021): the momentum is set to 0.9; cosine learning rate decay is adopted with an initial lr 0.2; the batch size is set to 1024; the weight decay is set to 1e-4; for SAM, $\\rho$ is set to 0.05. The total epochs is set to 200, and we switch from SGD/SAM to SGD-IRE/SAM-IRE when the training loss approaches 1.5. For SGD-IRE/SAM-IRE, we fix $K=10$ , and tune hyperparameters $\\gamma$ and $\\kappa$ via a grid search over $\\gamma\\in\\{0.8,0.6\\}$ and $\\kappa\\in\\{2,4\\}$ . The results are reported in Table 2. ", "page_idx": 19}, {"type": "text", "text": "Experiments for ViTs on CIFAR-100. We examine the impact of IRE on generalization of ViT-T and ViT-S on CIFAR-100. We follow the settings in Mueller et al. (2024): the default optimizers used are AdamW and SAM, with cosine lr decay to 0 starting from an initial lr 1e-4; the weight decay is 5e-4; batch size is 64; strong data augmentations (basic $^+$ AutoAugment) are utilized; $\\rho=0.1$ for SAM. The total epochs are set to 200, and we switch from AdamW/SAM to AdmIRE/SAM-IRE when the training loss approaches 0.5. For AdmIRE/SAM-IRE, we fix $K=10$ , and tune hyperparameters $\\gamma$ and $\\kappa$ via a grid search over $\\gamma\\in\\{0.99,0.9,0.8\\}$ and $\\kappa\\in\\{20,50\\}$ . The results are reported in Table 3. ", "page_idx": 19}, {"type": "text", "text": "Experiments for ViTs on ImageNet. We also examine the impact of IRE on generalization of ViT-S/16 on ImageNet. We follow the settings in Chen et al. (2024): RandAugment and Mixup with $\\alpha=0.5$ are utilized; the default optimizer used is AdamW with hyperparameters $\\beta_{1}=0.9,\\beta_{2}=$ 0.999 and weight decay $\\lambda=1.0$ ; the learning rate strategy integrates a warm-up phase followed by a cosine decay scheduler with $\\mathtt{l r\\_m a x}{=}3\\mathrm{e}{-}3$ ; batch size is 4096; the total training duration is 300 epochs, including 30 warm-up epochs; For AdmIRE, we switch from AdamW to AdmIRE at epoch 100 and examine different $\\gamma,\\kappa$ . The results are reported in Table 4. ", "page_idx": 19}, {"type": "text", "text": "In this section, the experiments on CIFAR-10/CIFAR-100 were conducted using a single A800 GPU, and the experiments on ImageNet were conducted using 4 A800 GPUs. ", "page_idx": 19}, {"type": "text", "text": "C.2 Experimental details in Section 4.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.2.1 Experimental details in Section 4.2.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We train a 2-layer decoder-only Transformer (8M parameters) using absolute positional encodings (Vaswani et al., 2017), with 8 heads in each layer and a hidden size of 128, on the wikitext-2 dataset (4.3M) by AdamW and AdmIRE (with $K=10$ and varying $\\gamma,\\kappa)$ . The (max) sequence length is set to 512, and the batch size is set to 32. The experiments in this section are conducted on 1 A800. ", "page_idx": 19}, {"type": "text", "text": "For the optimizer AdamW, we use the hyperparameters $\\beta_{1}\\,=\\,0.9,\\beta_{2}\\,=\\,0.95$ and weight decay $\\lambda=0.1$ , as suggested in Touvron et al. (2023). The total training duration is 100,000 steps, including 3,000 warm-up steps followed by a cosine decay scheduler with lr_max and $\\mathtt{l r\\_m i n=l r\\_m a x/20}$ . ", "page_idx": 19}, {"type": "text", "text": "First, we tune lr_max in AdamW from $\\left\\{1\\,.5\\mathrm{e}{-}4,3\\mathrm{e}{-}4,6\\mathrm{e}{-}4,1\\,.2\\mathrm{e}{-}3,1\\,.8\\mathrm{e}{-}3,3\\mathrm{e}{-}3\\right\\}$ . The results, shown in Figure 5 (left), identify the optimal $\\mathtt{l r\\_m a x=6e-4}$ . We also use the optimal lr_max of 6e-4 for AdmIRE, for which the IRE is enable at the end of warm-up phase. ", "page_idx": 19}, {"type": "text", "text": "The results are reported in Figure 3. ", "page_idx": 19}, {"type": "image", "img_path": "cjM2bhLOiC/tmp/ffd59992d6d698a0d9ceeb25584ced96d5dee19343c81d794e0452abaf91d652.jpg", "img_caption": ["Figure 5: The results for tuning lr_max in AdamW. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.2.2 Experimental details in Section 4.2.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Llama (Touvron et al., 2023) is a decode-only Transformer architecture using Rotary Positional Encoding (RoPE) (Su et al., 2024), Swish-Gated Linear Unit (SwiGLU) (Shazeer, 2020), and Root mean square layer normalization (RMSNorm) (Zhang and Sennrich, 2019). The experiments in this section examine the performance of AdmIRE in training Llama models with different sizes. For implementation, we utilize the Llama code available on huggingface. The experiments are conducted on 4 H800. ", "page_idx": 20}, {"type": "text", "text": "For the optimizer AdamW, we use the well-tuned hyperparameters $\\beta_{1}=0.9,\\beta_{2}=0.95$ and weight decay $\\lambda=0.1$ for LLama (Touvron et al., 2023). The learning rate strategy integrates a warm-up phase followed by a cosine decay scheduler with lr_max and $\\mathtt{l r\\_m i n=l r\\_m a x}/20$ . Additionally, it is used with gradient clipping 1.0 to maintain the training stability. ", "page_idx": 20}, {"type": "text", "text": "In each experiment, we tune the optimal lr_max for AdamW and then use it also for AdmIRE, for which the IRE is enable at the end of warm-up phase. ", "page_idx": 20}, {"type": "text", "text": "Llama (60M) on wikitext-103 (0.5G). We train a 16-layer Llama model, with 10 heads in each layer and a hidden size of 410, on the wikitext-103 dataset. The (max) sequence length is set to 150, and the batch size is set to 240, following Dai et al. (2019). The total training duration is 50,000 or 100,000 steps, including 500 warm-up steps. First, we tune lr_max in AdamW from $\\{3{\\sf e}-4,6{\\sf e}-4$ , $1.2{\\ e}{-3},1.8{\\ e}{-3}\\big\\}$ , identifying the optimal lr_max 6e-4. The experimental results are very similar to Figure 5 (right), so we will not show them again. Then, both AdamW and AdmIRE are trained using the optimal lr_max. ", "page_idx": 20}, {"type": "text", "text": "Llama (119M) on minipile (6G). We train a 6-layer Llama model, with 12 heads in each layer and a hidden size of 768, on the minipile dataset. The (max) sequence length is set to 512, and the batch size is set to 300. The total training duration is 30,000 or 60,000 steps, including 300 warm-up steps. First, we tune lr_max in AdamW from $\\left\\{3{\\bf e}{-}4,\\,6{\\bf e}{-}4,\\,1{\\bf\\cdot}2{\\bf e}{-}3,\\,1{\\bf\\cdot}8{\\bf e}{-}3\\right\\}$ , identifying the optimal lr_max 6e-4. (The results are very similar to Figure 5 (right), so we do not show them repeatly.) Then, both AdamW and AdmIRE are trained using the optimal lr_max. ", "page_idx": 20}, {"type": "text", "text": "Llama (229M) on openwebtext (38G). We train a 16-layer Llama model, with 12 heads in each layer and a hidden size of 768, on the openwebtext dataset. The (max) sequence length is set to 1024, and the batch size is set to 480, following nanoGPT and Liu et al. (2024). The total training duration is 50,000 or 100,000 steps, including 1,000 warm-up steps. First, we tune lr_max in AdamW from $\\left\\{3{\\bf e}{-}4,6{\\bf e}{-}4,1{\\bf\\cdot}2{\\bf e}{-}3,1{\\bf\\cdot}8{\\bf e}{-}3\\right\\}$ , identifying the optimal lr_max 6e-4. (The results are very similar to Figure 5 (right), so we do not show them repearly.) Then, both AdamW and AdmIRE are trained using the optimal lr_max. ", "page_idx": 20}, {"type": "text", "text": "D Proofs in Section 5.2.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Additional Notations. For the proofs in Section 5, some additional notations are used. For any set $K\\subset\\mathbb{R}^{p}$ and a constant $R>0$ , we denote $\\mathbb{B}(K;R):=\\{\\pmb\\theta\\in\\mathbb{R}^{p}:\\mathrm{dist}(\\pmb\\theta;K)\\leqslant R\\}$ . $\\langle\\cdot,\\cdot\\rangle$ represents the standard Euclidean inner product between two vectors. $\\lVert\\cdot\\rVert$ denotes the $\\ell_{2}$ norm of a vector or the spectral norm of a matrix, whereas $\\Vert\\cdot\\Vert_{\\mathrm{F}}$ denotes the Frobenius norm of a matrix. ", "page_idx": 20}, {"type": "text", "text": "D.1 Preliminary Lemmas ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma D.1 (Arora et al. (2022), Lemma B.2). Under Assumption 5.1, for any compact set $K\\subset\\Gamma$ , there exist absolute constants $R_{1},\\mu>0$ such that ", "page_idx": 20}, {"type": "text", "text": "$\\bullet\\,\\,(i)\\,\\mathbb{B}(K;R_{1})\\subset U;$ \u2022 (ii) $\\mathcal{L}(\\cdot)$ is $\\mu{-}P L$ (defined in Def F.1) on $\\mathbb{B}(K;R_{1})$ ; $\\bullet\\,\\left(i i i\\right)\\operatorname*{inf}_{\\pmb{\\theta}\\in\\mathbb{B}(K;R_{1})}\\lambda_{m}\\left(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta})\\right)\\geqslant\\mu.$ ", "page_idx": 20}, {"type": "text", "text": "We further define the following absolute constants on $\\mathbb{B}(K;R_{1})$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{2}:=\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{sup}}\\left\\|\\nabla^{2}\\mathcal{L}(\\theta)\\right\\|;\\quad\\beta_{3}:=\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{sup}}\\left\\|\\nabla^{3}\\mathcal{L}(\\theta)\\right\\|;\\quad\\beta_{4}:=\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{sup}}\\left\\|\\nabla^{4}\\mathcal{L}(\\theta)\\right\\|;}\\\\ &{\\qquad\\qquad\\nu:=\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{inf}}\\,\\lambda_{m}\\left(\\nabla^{2}\\mathcal{L}(\\theta)\\right);\\quad\\zeta_{\\Phi}:=\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{sup}}\\left\\|\\nabla^{2}\\Phi(\\theta)\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma D.2 (Key properties of $\\Phi(\\cdot)$ (Arora et al., 2022)). Under Assumption 5.1, ", "page_idx": 21}, {"type": "text", "text": "\u2022 For any $\\pmb\\theta\\in U$ , $\\partial\\Phi(\\pmb\\theta)\\nabla{\\mathcal L}(\\pmb\\theta)=\\mathbf{0}$ .   \n\u2022 For any $\\theta\\in\\Gamma$ $,\\partial\\Phi(\\pmb\\theta)=P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\pmb\\theta))$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma D.3 (Continuity of $P_{m+1:p})$ . Under Assumption 5.1, there exists absolute constants $R_{2},\\zeta_{P}>$ 0 such that for any $\\pmb{\\theta}\\in\\mathbb{B}(K;R_{2})$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}))-P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\Phi(\\pmb{\\theta})))\\right\\|\\leqslant\\zeta_{P}\\left\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma $D.3$ . ", "page_idx": 21}, {"type": "text", "text": "Let the orthogonal decomposition of $\\nabla^{2}{\\mathcal{L}}(\\pmb{\\theta})$ and $\\nabla^{2}{\\mathcal{L}}(\\Phi(\\pmb{\\theta}))$ be $\\begin{array}{r}{\\nabla^{2}\\mathcal{L}(\\pmb{\\theta})=\\sum_{k=1}^{p}\\lambda_{k}\\pmb{u}_{k}\\pmb{u}_{k}^{\\top}\\left(\\lambda_{1}\\geqslant\\right.}\\end{array}$ $\\cdots\\geqslant\\lambda_{p},$ and $\\begin{array}{r}{\\nabla^{2}\\mathcal{L}(\\Phi(\\pmb{\\theta}))=\\sum_{k=1}^{p}{\\mu_{k}\\pmb{v}_{k}\\pmb{v}_{k}^{\\top}}}\\end{array}$ $(\\mu_{1}\\geqslant\\cdot\\cdot\\geqslant\\mu_{p})$ , respectively. ", "page_idx": 21}, {"type": "text", "text": "By Lemma D.1, for any $\\pmb{\\theta}\\in\\mathbb{B}(K;R_{1})$ , it holds that $\\left\\|\\nabla^{2}\\mathcal{L}(\\pmb{\\theta})-\\nabla^{2}\\mathcal{L}(\\Phi(\\pmb{\\theta}))\\right\\|\\leqslant\\beta_{3}\\left\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\right\\|$ We choose $\\begin{array}{r}{R_{2}:=R_{1}\\wedge\\frac{\\mu}{4\\beta_{3}}}\\end{array}$ . Then for any $\\pmb{\\theta}\\in\\mathbb{B}(K;\\ddot{R}_{2})$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\nabla^{2}{\\mathcal{L}}(\\pmb{\\theta})-\\nabla^{2}{\\mathcal{L}}(\\Phi(\\pmb{\\theta}))\\right\\|\\leqslant\\beta_{3}\\left\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\right\\|\\leqslant{\\frac{\\mu}{4}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consequently, by Lemma F.2, we can bound the gap of eigenvalues: for any $k\\in[p]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\lambda_{k}-\\mu_{k}|\\leqslant\\left\\|\\nabla^{2}\\mathcal{L}(\\pmb{\\theta})-\\nabla^{2}\\mathcal{L}(\\Phi(\\pmb{\\theta}))\\right\\|\\leqslant\\frac{\\mu}{4}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Noticing $\\Phi(\\pmb\\theta)\\in\\Gamma$ , by Lemma D.1, it holds that $\\mu_{1}\\geqslant\\mu_{m}\\geqslant\\mu$ and $\\mu_{m+1}=\\cdot\\cdot=\\mu_{p}=0$ . Thus, we can obtain the bounds of $\\{\\lambda_{k}\\}_{k}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\mathrm{for\\;all}\\;k\\leqslant m,\\quad\\lambda_{k}\\geqslant\\lambda_{m}\\geqslant\\mu_{m}-\\vert\\lambda_{m}-\\mu_{m}\\vert\\geqslant\\displaystyle\\frac{3\\mu}{4};}\\\\ &{\\qquad\\mathrm{for\\;all}\\;k\\geqslant m+1,\\quad\\lambda_{k}\\leqslant\\lambda_{m+1}\\leqslant\\mu_{m+1}+\\vert\\lambda_{m+1}-\\mu_{m+1}\\vert\\leqslant\\displaystyle\\frac{\\mu}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For simplicity, we denote $\\begin{array}{r c l c l c l}{{U_{\\mathrm{top}}}}&{{:=}}&{{\\left(u_{1},\\cdot\\cdot\\cdot,\\,,u_{m}\\right),\\ U_{\\mathrm{bottom}}}}&{{:=}}&{{\\left(u_{m+1},\\cdot\\cdot\\cdot\\,,u_{p}\\right),\\ V_{\\mathrm{top}}}}&{{:=}}&{{\\left(u_{m+1},\\cdot\\cdot\\cdot\\cdot\\,,u_{p}\\right).}}\\end{array}$ = $(v_{1},\\cdot\\cdot\\cdot,v_{m}),V_{\\mathrm{bottom}}:=(v_{m+1},\\cdot\\cdot\\cdot,v_{p})$ . ", "page_idx": 21}, {"type": "text", "text": "By Lemma F.3, we can bound the gap between the subspaces: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|U_{\\mathrm{bottom}}^{\\top}V_{\\mathrm{top}}\\right\\|_{\\mathrm{F}}\\leqslant\\frac{\\left\\|U_{\\mathrm{bottom}}^{\\top}\\left(\\nabla^{2}\\mathcal{L}(\\theta)-\\nabla^{2}\\mathcal{L}(\\Phi(\\theta))\\right)V_{\\mathrm{top}}\\right\\|_{\\mathrm{F}}}{\\frac{3\\mu}{4}-\\frac{\\mu}{4}}}\\quad}&{}\\\\ {\\stackrel{\\mathrm{Lemma~F.6}}{\\leqslant}\\left\\{\\frac{2}{\\mu}\\left\\|U_{\\mathrm{bottom}}^{\\top}\\right\\|\\left\\|\\nabla^{2}\\mathcal{L}(\\theta)-\\nabla^{2}\\mathcal{L}(\\Phi(\\theta))\\right\\|\\left\\|V_{\\mathrm{top}}\\right\\|_{\\mathrm{F}}}\\\\ &{\\qquad\\leqslant\\frac{2}{\\mu}\\left\\|U_{\\mathrm{bottom}}^{\\top}\\right\\|_{\\mathrm{F}}\\left\\|\\nabla^{2}\\mathcal{L}(\\theta)-\\nabla^{2}\\mathcal{L}(\\Phi(\\theta))\\right\\|\\left\\|V_{\\mathrm{top}}\\right\\|}\\\\ &{\\qquad\\qquad=\\frac{2\\,\\left(m\\wedge(p-m)\\right)}{\\mu}\\left\\|\\nabla^{2}\\mathcal{L}(\\theta)-\\nabla^{2}\\mathcal{L}(\\Phi(\\theta))\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "According to the definition of $P_{m+1:p}(\\cdot)$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}))=\\displaystyle\\sum_{k=m+1}^{p}\\pmb{u}_{k}\\pmb{u}_{k}^{\\top}=U_{\\mathrm{bottom}}U_{\\mathrm{bottom}}^{\\top},}\\\\ &{P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\Phi(\\pmb{\\theta})))=\\displaystyle\\sum_{k=m+1}^{p}v_{k}\\pmb{v}_{k}^{\\top}=V_{\\mathrm{bottom}}V_{\\mathrm{bottom}}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Noticing the relationship ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\Vert P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))-P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta)))\\right\\Vert^{2}\\leqslant\\left\\Vert P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))-P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta)))\\right\\Vert_{\\mathrm{L}}^{2}}\\\\ &{=\\left\\Vert U_{\\mathrm{bottom}}U_{\\mathrm{bottom}}^{\\top}-V_{\\mathrm{bottom}}V_{\\mathrm{bottom}}^{\\top}\\right\\Vert_{\\mathrm{F}}^{2}=2(p-m)-2\\operatorname{Tr}\\left(U_{\\mathrm{bottom}}U_{\\mathrm{bottom}}^{\\top}V_{\\mathrm{bottom}}V_{\\mathrm{bottom}}^{\\top}\\right)}\\\\ &{=2\\operatorname{Tr}\\left(U_{\\mathrm{bottom}}U_{\\mathrm{bottom}}^{\\top}\\left(I-V_{\\mathrm{botom}}V_{\\mathrm{bottom}}^{\\top}\\right)\\right)=2\\operatorname{Tr}\\left(U_{\\mathrm{botom}}U_{\\mathrm{bottom}}^{\\top}V_{\\mathrm{top}}V_{\\mathrm{top}}^{\\top}\\right)}\\\\ &{=2\\left\\Vert U_{\\mathrm{botom}}^{\\top}V_{\\mathrm{top}}\\right\\Vert_{\\mathrm{F}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we obtain the bound: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))-P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta)))\\right\\|\\leqslant\\sqrt{2}\\left\\|U_{\\mathrm{botom}}^{\\top}V_{\\mathrm{top}}\\right\\|_{\\mathrm{F}}}\\\\ &{\\leqslant\\!\\frac{2\\sqrt{2}\\,\\left(m\\wedge(p-m)\\right)}{\\mu}\\left\\|\\nabla^{2}\\mathcal{L}(\\theta)-\\nabla^{2}\\mathcal{L}(\\Phi(\\theta))\\right\\|\\leqslant\\frac{2\\sqrt{2}\\,\\left(m\\wedge(p-m)\\right)\\,\\beta_{3}}{\\mu}\\left\\|\\theta-\\Phi(\\theta)\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To summarize, we only need to choose the constants R2 = R1 \u22274\u00b5\u03b23 and \u03b6P = 2 2(m\u2227(\u00b5p\u2212m))\u03b23to \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Proof Notations. Now we introduce some additional useful notations in the proof in this section. ", "page_idx": 22}, {"type": "text", "text": "First, we choose $R:=(R_{1}\\land R_{2})/2$ , where $R_{1}$ is defined in Lemma D.1 and $R_{2}$ is defined in Lemma D.3. Let $\\mu$ be the PL constant on $\\mathbb{B}(K;R)$ . Moreover, we use the following notations: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{2}:=\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\left\\|\\nabla^{2}\\mathcal{L}(\\theta)\\right\\|;\\quad\\beta_{3}:=\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\left\\|\\nabla^{3}\\mathcal{L}(\\theta)\\right\\|;\\quad\\beta_{4}:=\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\left\\|\\nabla^{4}\\mathcal{L}(\\theta)\\right\\|;}\\\\ &{\\qquad\\qquad\\nu:=\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{inf}}\\,\\lambda_{m}\\left(\\nabla^{2}\\mathcal{L}(\\theta)\\right);\\quad\\zeta_{\\Phi}:=\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\left\\|\\nabla^{2}\\Phi(\\theta)\\right\\|;}\\\\ &{\\qquad\\qquad\\zeta_{P}:=\\underset{\\theta\\in\\mathbb{B}(K;R)-\\Gamma}{\\operatorname*{sup}}\\frac{\\left\\|P_{m+1:p}\\left(\\nabla^{2}\\mathcal{L}(\\theta)\\right)-P_{m+1:p}\\left(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta))\\right)\\right\\|}{\\left\\|\\theta-\\Phi(\\theta)\\right\\|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Ensured by Lemma D.1 and D.3, these quantities are all absolute constants in $(0,+\\infty)$ . Moreover, without loss of generality, we can assume that $\\beta_{1},\\beta_{2},\\beta_{3},\\zeta_{\\Phi},\\zeta_{P}>1$ and $\\mu\\leqslant\\nu<1$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma D.4 (Connections between para norm, grad norm, and loss). For any $\\pmb{\\theta}\\in\\mathbb{B}(K;R)>0,$ , it holds that: ", "page_idx": 22}, {"type": "text", "text": "\u2022 (para norm v.s. grad norm) $\\mu\\,\\|\\nabla{\\mathcal{L}}(\\pmb{\\theta})\\|\\leqslant\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\|\\leqslant\\beta_{2}\\,\\|\\nabla{\\mathcal{L}}(\\pmb{\\theta})\\|;$ \u2022 (grad norm v.s. loss) $\\begin{array}{r}{2\\mu\\mathcal{L}(\\pmb{\\theta})\\leqslant\\|\\nabla\\mathcal{L}(\\pmb{\\theta})\\|^{2}\\leqslant\\frac{2\\beta_{2}^{2}}{\\mu}\\mathcal{L}(\\pmb{\\theta}),}\\end{array}$ \u2022 (loss v.s. para norm) $\\begin{array}{r}{\\frac{\\mu}{2}\\left\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\right\\|^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta})\\leqslant\\frac{\\beta_{2}^{2}}{2\\mu}\\left\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\right\\|^{2}\\!.}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma D.4. This lemma is a corollary of local PL and smoothness (Lemma D.1). For the three lower bounds, please refer to the proof of Lemma B.6 in Arora et al. (2022). Then utilizing these lower bounds and the smoothness $\\beta_{2}$ , the upper bounds hold naturally. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma D.5. For all $\\pmb{\\theta}\\in\\mathbb{B}(K;R)$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}))\\nabla^{2}\\mathcal{L}(\\pmb{\\theta})\\right\\|\\leqslant\\mathcal{O}\\left(\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\|\\right);}\\\\ &{\\left\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}))\\nabla\\mathcal{L}(\\pmb{\\theta})\\right\\|\\leqslant\\mathcal{O}\\left(\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\|^{2}\\right);}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\left\\|\\nabla\\mathcal{L}(\\theta+\\rho v)\\right\\|\\leqslant\\left\\|\\nabla\\mathcal{L}(\\theta)\\right\\|+\\rho\\beta_{2};}\\\\ {\\left\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla\\mathcal{L}(\\theta+\\rho v)\\right\\|\\leqslant\\mathcal{O}\\left(\\left\\|\\theta-\\Phi(\\theta)\\right\\|^{2}\\right)+\\mathcal{O}\\left(\\rho\\left\\|\\theta-\\Phi(\\theta)\\right\\|\\right)+\\frac{\\rho^{2}\\beta_{3}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma D.5. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla^{2}\\mathcal{L}(\\theta)\\right|\\leqslant\\left\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta)))\\nabla^{2}\\mathcal{L}(\\Phi(\\theta))\\right\\|+\\zeta_{P}\\beta_{2}\\left\\|\\theta-\\Phi(\\theta)\\right\\|+\\beta_{3}\\left\\|\\theta-\\Phi(\\theta)\\right\\|,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=0+\\mathcal{O}\\left(\\left\\|\\theta-\\Phi(\\theta)\\right\\|\\right);}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla\\mathcal{L}(\\theta)\\|\\leqslant\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla^{2}\\mathcal{L}(\\Phi(\\theta))(\\Phi(\\theta)-\\theta)\\|+\\mathcal{O}\\left(\\|\\theta-\\Phi(\\theta)\\|^{2}\\right)}\\\\ &{\\leqslant\\big\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta)))\\nabla^{2}\\mathcal{L}(\\Phi(\\theta))(\\Phi(\\theta)-\\theta)\\big\\|+\\mathcal{O}\\left(\\|\\theta-\\Phi(\\theta)\\|^{2}\\right)=\\mathcal{O}\\left(\\|\\theta-\\Phi(\\theta)\\|^{2}\\right);}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\|\\nabla\\mathcal{L}(\\theta+\\rho v)\\|\\leqslant\\|\\nabla\\mathcal{L}(\\theta)\\|+\\rho\\beta_{2};}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla\\mathcal{L}(\\theta+\\rho v)\\|}\\\\ &{\\qquad\\qquad\\leqslant\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla\\mathcal{L}(\\theta)\\|+\\rho\\|P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla^{2}\\mathcal{L}(\\theta)v\\|+\\frac{\\rho^{2}\\beta_{3}}{2}}\\\\ &{\\qquad\\qquad\\leqslant\\mathcal{O}\\left(\\|\\theta-\\Phi(\\theta)\\|^{2}\\right)+\\mathcal{O}\\left(\\rho\\|\\theta-\\Phi(\\theta)\\|\\right)+\\frac{\\rho^{2}\\beta_{3}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.2 Proof of Theorem 5.5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.2.1 Proof of Keeping Moving Near Minimizers for SAM ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We first give the proof for SAM about \u201ckeeping moving near minimizers\u201d, which provides important insights into the proof for SAM-IRE. ", "page_idx": 23}, {"type": "text", "text": "Recalling (8), the update rule of average SAM is: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\eta\\nabla\\mathcal{L}\\left(\\pmb{\\theta}_{t}+\\rho\\frac{\\pmb{\\xi}_{t}}{\\lVert\\pmb{\\xi}_{t}\\rVert}\\right),\\;\\mathrm{where}\\;\\pmb{\\xi}_{t}\\sim\\mathcal{N}(\\mathbf{0},I).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let the $\\rho$ in SAM satisfy: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\rho={\\mathcal{O}}({\\sqrt{\\eta}}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For simplicity, we denote ", "page_idx": 23}, {"type": "equation", "text": "$$\nv_{t}:=\\frac{\\xi_{t}}{\\|\\xi_{t}\\|},\\quad C_{1}:=\\frac{4\\beta_{2}^{3}}{\\mu},\\quad C_{2}:=\\beta_{2}^{3}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Notice $\\mathcal{L}(\\pmb{\\theta}_{T_{\\mathrm{I}}})<C_{1}\\eta\\rho^{2}$ , we have the following upper bound for the probability ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}\\left(\\exists\\,t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}+T_{\\mathrm{II}}],\\mathcal{L}(\\pmb{\\theta}_{t})\\geqslant2C_{1}\\eta\\rho^{2}\\right)}\\\\ &{\\leqslant\\displaystyle\\sum_{t=T_{\\mathrm{I}}}^{T_{\\mathrm{I}}+T_{\\mathrm{II}}-1}\\mathbb{P}\\left(\\mathcal{L}(\\pmb{\\theta}_{t+1})\\geqslant2C_{1}\\eta\\rho^{2};\\,\\forall\\,s\\in[T_{\\mathrm{I}},t],\\mathcal{L}(\\pmb{\\theta}_{s})<2C_{1}\\eta\\rho^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For each term $t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}+T_{\\mathrm{II}}-1]$ , it can be bounded by: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}\\left(\\mathcal{L}(\\theta_{t+1})\\geqslant2C_{1}\\eta\\rho^{2};\\,\\forall\\,s\\in[T_{\\Gamma},t],\\mathcal{L}(\\theta_{s})<2C_{1}\\eta\\rho^{2}\\right)}\\\\ &{\\leqslant\\mathbb{P}\\left(\\mathcal{L}(\\theta_{t+1})\\geqslant2C_{1}\\eta\\rho^{2};\\mathcal{L}(\\theta_{t})<C_{1}\\eta\\rho^{2}\\right)}\\\\ &{\\quad\\,+\\,\\displaystyle\\sum_{s=T_{1}}^{t-1}\\mathbb{P}\\left(\\mathcal{L}(\\theta_{t+1})\\geqslant2C_{1}\\eta\\rho^{2};\\mathcal{L}(\\theta_{s})<C_{1}\\eta\\rho^{2};\\,\\forall\\,\\tau\\in[s+1,t],C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\theta_{\\tau})<2C_{1}\\eta\\rho^{2}\\right)}\\\\ &{\\leqslant\\mathbb{P}\\left(\\mathcal{L}(\\theta_{t+1})\\geqslant2C_{1}\\eta\\rho^{2}\\Big|\\mathcal{L}(\\theta_{t})<C_{1}\\eta\\rho^{2}\\right)}\\\\ &{\\quad\\,+\\,\\displaystyle\\sum_{s=T_{1}}^{t-1}\\mathbb{P}\\left(\\mathcal{L}(\\theta_{t+1})\\geqslant2C_{1}\\eta\\rho^{2};\\,\\forall\\,\\tau\\in[s+1,t],C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\theta_{\\tau})<2C_{1}\\eta\\rho^{2}\\Big|\\mathcal{L}(\\theta_{s})<C_{1}\\eta\\rho^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For simplicity, we denote ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbb{P}_{t+1,t}:=\\mathbb{P}\\left(\\mathcal{L}(\\theta_{t+1})\\geqslant2C_{1}\\eta\\rho^{2}\\Big|\\mathcal{L}(\\theta_{t})<C_{1}\\eta\\rho^{2}\\right),}\\\\ {\\mathfrak{d}_{t+1,s}:=\\mathbb{P}\\left(\\mathcal{L}(\\theta_{t+1})\\geqslant2C_{1}\\eta\\rho^{2};\\,\\forall\\,\\tau\\in[s+1,t],C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\theta_{\\tau})<2C_{1}\\eta\\rho^{2}\\Big|\\mathcal{L}(\\theta_{s})<C_{1}\\eta\\rho^{2}\\right),\\;s}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "\u2022 Step I. Bounding $\\mathbb{P}_{t+1,t}$ ", "page_idx": 24}, {"type": "text", "text": "From $\\mathcal{L}(\\pmb{\\theta}_{t})\\leqslant C_{1}\\eta\\rho^{2}$ , we have $\\begin{array}{r}{\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|\\leqslant\\sqrt{\\frac{2}{\\mu}\\mathcal{L}(\\pmb{\\theta}_{t})}=\\mathcal{O}(\\frac{\\sqrt{\\eta}\\rho}{\\mu})}\\end{array}$ , thus ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|\\leqslant\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|+\\rho=\\mathcal{O}(\\sqrt{\\eta}\\rho)+\\mathcal{O}(\\rho)<R,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which means $\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}\\in\\mathbb{B}(K;R)$ . Furthermore, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\pmb{\\theta}_{t+1}-\\pmb{\\Phi}(\\pmb{\\theta}_{t})\\|\\leqslant\\|\\pmb{\\theta}_{t}-\\pmb{\\Phi}(\\pmb{\\theta}_{t})\\|+\\eta\\,\\|\\nabla\\mathcal{L}\\left(\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}\\right)\\|}\\\\ &{\\leqslant\\!\\mathcal{O}(\\sqrt{\\eta}\\rho)+\\eta\\,\\|\\nabla\\mathcal{L}\\left(\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}\\right)\\|\\leqslant\\mathcal{O}(\\sqrt{\\eta}\\rho)+\\eta\\,\\|\\nabla\\mathcal{L}\\left(\\pmb{\\theta}_{t}\\right)\\|+\\beta_{2}\\eta\\rho}\\\\ &{\\leqslant\\!\\mathcal{O}(\\sqrt{\\eta}\\rho)+\\eta\\sqrt{\\frac{2\\beta_{2}^{2}}{\\mu}\\mathcal{L}(\\pmb{\\theta}_{t})}+\\beta_{2}\\eta\\rho\\leqslant\\mathcal{O}(\\sqrt{\\eta}\\rho)+\\mathcal{O}(\\eta^{3/2}\\rho)+\\mathcal{O}(\\eta\\rho)\\leqslant R,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which implies $\\theta_{t+1}\\in\\mathbb{B}(K;R)$ . Consequently, we have the following quadratic upper bound: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(\\theta_{t+1})=\\mathcal{L}\\left(\\theta_{t}-\\eta\\nabla\\mathcal{L}\\left(\\theta_{t}+\\rho\\mathbf{v}_{t}\\right)\\right)}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{t})-\\eta\\langle\\nabla\\mathcal{L}(\\theta_{t}),\\nabla\\mathcal{L}(\\theta_{t}+\\rho\\mathbf{v}_{t})\\rangle+\\frac{\\eta^{2}\\beta_{2}}{2}\\|\\nabla\\mathcal{L}\\left(\\theta_{t}+\\rho\\mathbf{v}_{t}\\right)\\|^{2}}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{t})-\\eta\\|\\nabla\\mathcal{L}(\\theta_{t})\\|^{2}-\\eta\\langle\\nabla\\mathcal{L}(\\theta_{t}),\\nabla^{2}\\mathcal{L}(\\theta_{t})\\mathbf{v}_{t}\\rangle+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\|\\nabla\\mathcal{L}(\\theta)\\|+\\frac{\\eta^{2}\\beta_{2}}{2}(\\|\\nabla\\mathcal{L}(\\theta_{t})\\|+\\rho\\beta}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{t})-\\eta\\|\\nabla\\mathcal{L}(\\theta_{t})\\|^{2}-\\eta\\rho\\langle\\nabla\\mathcal{L}(\\theta_{t}),\\nabla^{2}\\mathcal{L}(\\theta_{t})\\mathbf{v}_{t}\\rangle+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\|\\nabla\\mathcal{L}(\\theta)\\|+\\eta^{2}\\beta_{2}\\left(\\|\\nabla\\mathcal{L}(\\theta_{t})\\|^{2}+\\rho^{2}\\right.}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{t})+\\eta\\rho\\left\\|\\nabla^{2}\\mathcal{L}(\\theta_{t})\\nabla\\mathcal{L}(\\theta_{t})\\|+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\|\\nabla\\mathcal{L}(\\theta_{t})\\|+\\eta^{2}\\beta_{2}\\rho^{2}\\beta_{2}^{2}}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{t})+\\left(\\eta\\beta_{2}+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\right)\\|\\nabla\\mathcal{L}(\\theta_{t})\\|+\\mathcal{O}(\\eta^{2}\\rho^{2})}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{t})+\\left(\\eta\\beta_{2}+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\right)\\sqrt{\\frac{2\\beta_{2}}{\\mu}}\\sqrt{\\mathcal{L}(\\theta_{t})}+\\mathcal{O}(\\eta^\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t+1,t}=\\mathbb{P}\\left(\\mathcal{L}(\\pmb{\\theta}_{t+1})\\geqslant2C_{1}\\eta\\rho^{2}\\middle|\\mathcal{L}(\\pmb{\\theta}_{t})<C_{1}\\eta\\rho^{2}\\right)=0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "\u2022 Step II. Bounding $\\mathbb{P}_{t+1,s}$ for $s\\in[T_{\\mathrm{I}},t-1]$ . ", "page_idx": 24}, {"type": "text", "text": "We prove this step under the condition $\\mathcal{L}(\\pmb{\\theta}_{s})<C_{1}\\eta\\rho^{2}$ . Define a process $\\{X_{\\tau}\\}_{\\tau=s}^{t+1};X_{s+1}=$ $\\mathcal{L}(\\bar{\\pmb{\\theta}_{s+1}})$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nX_{\\tau+1}=\\left\\{\\!\\!\\begin{array}{l}{{\\!\\displaystyle\\mathcal{L}(\\pmb{\\theta}_{\\tau+1}),\\quad\\mathrm{if}\\;C_{1}\\eta\\rho^{2}\\leqslant X_{\\tau}=\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}\\quad}}\\\\ {{\\!\\displaystyle X_{\\tau}-C_{2}\\eta^{2}\\rho^{2},\\quad\\mathrm{else}}}\\end{array}\\!\\!\\right..\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It is clear that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t+1,s}\\leqslant\\mathbb{P}\\left(X_{t+1}\\geqslant2C_{1}\\eta\\rho^{2}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then our key step is to prove the following two claims about the process $\\{X_{\\tau}\\}$ . ", "page_idx": 24}, {"type": "text", "text": "\u2013 Claim I. $X_{\\tau}-C_{2}\\tau\\eta\\rho^{2}$ is a super-martingale. From the definition of $X_{\\tau}$ , we only need to prove that if $\\begin{array}{r}{C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}}\\end{array}$ , then $\\mathbb{E}\\left[\\mathcal{L}(\\pmb{\\theta}_{\\tau+1})\\right]\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})-C_{2}\\eta^{2}\\rho^{2}$ . If $C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}$ , similar to Step I, it is clear that $\\pmb{\\theta}_{\\tau+1}\\in\\mathbb{B}(K;R)$ . Applying the quadratic upper bound, it holds that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta}_{\\tau+1})=\\mathcal{L}\\left(\\pmb{\\theta}_{\\tau}-\\eta\\nabla\\mathcal{L}\\left(\\pmb{\\theta}_{\\tau}+\\rho\\pmb{v}_{\\tau}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\llangle(\\pmb\\theta_{\\tau})-\\eta\\left\\langle\\nabla\\mathcal L(\\pmb\\theta_{\\tau}),\\nabla\\mathcal L(\\pmb\\theta_{\\tau}+\\rho\\pmb v_{\\tau})\\right\\rangle+\\frac{\\eta^{2}\\beta_{2}}{2}\\left\\|\\nabla\\mathcal L\\left(\\pmb\\theta_{\\tau}+\\rho\\pmb v_{\\tau}\\right)\\right\\|^{2}}\\\\ {\\displaystyle\\leqslant\\mathcal L(\\pmb\\theta_{\\tau})-\\eta\\left\\|\\nabla\\mathcal L(\\pmb\\theta_{\\tau})\\right\\|^{2}-\\eta\\rho\\left\\langle\\nabla\\mathcal L(\\pmb\\theta_{\\tau}),\\nabla^{2}\\mathcal L(\\pmb\\theta_{\\tau})\\pmb v_{t}\\right\\rangle}\\\\ {\\displaystyle\\qquad+\\,\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\left\\|\\nabla\\mathcal L(\\pmb\\theta_{\\tau})\\right\\|+\\eta^{2}\\beta_{2}\\left(\\left\\|\\nabla\\mathcal L(\\pmb\\theta_{\\tau})\\right\\|^{2}+\\rho^{2}\\beta_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Taking the expectation, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathcal{L}(\\theta_{\\tau+1})\\right]\\leqslant\\!\\mathcal{L}(\\theta_{\\tau})-\\eta\\left\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\|^{2}+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\left\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\|+\\eta^{2}\\beta_{2}\\left(\\left\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\|^{2}+\\rho^{2}\\beta_{2}^{2}\\right)}\\\\ &{\\qquad\\qquad\\leqslant\\!\\mathcal{L}(\\theta_{\\tau})-\\frac{3}{4}\\eta\\left\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\|^{2}+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\left\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\|+\\eta^{2}\\rho^{2}\\beta_{2}^{3}}\\\\ &{\\qquad\\qquad\\leqslant\\!\\mathcal{L}(\\theta_{\\tau})-\\eta\\left\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\|\\left(\\frac{3}{4}\\left\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\|-\\frac{\\rho^{2}\\beta_{3}}{2}\\right)+\\eta^{2}\\rho^{2}\\beta_{2}^{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From $\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\geq C_{1}\\eta\\rho^{2}$ and $\\rho=\\mathcal{O}(\\sqrt{\\eta})$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\nabla\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\|\\geqslant\\sqrt{2\\mu\\mathcal{L}(\\pmb{\\theta}_{t})}\\geqslant\\sqrt{2C_{1}\\mu}\\sqrt{\\eta}\\rho\\geqslant4\\beta_{3}\\rho^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathcal{L}(\\pmb{\\theta}_{\\tau+1})\\right]\\leqslant\\!\\mathcal{L}(\\pmb{\\theta}_{\\tau})-\\frac{5}{8}\\eta\\,\\|\\nabla\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\|^{2}+\\eta^{2}\\rho^{2}\\beta_{2}^{3}}\\\\ &{\\quad\\quad\\quad\\leqslant\\!\\mathcal{L}(\\pmb{\\theta}_{\\tau})-\\frac{10}{8}\\eta\\mu\\mathcal{L}(\\pmb{\\theta}_{\\tau})+\\eta^{2}\\rho^{2}\\beta_{2}^{3}\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})-\\frac{10}{8}C_{1}\\mu\\eta^{2}\\rho^{2}+\\eta^{2}\\rho^{2}\\beta_{2}^{3}}\\\\ &{\\quad\\quad\\quad\\leqslant\\!\\mathcal{L}(\\pmb{\\theta}_{\\tau})-\\beta_{2}^{3}\\eta^{2}\\rho^{2}=\\mathcal{L}(\\pmb{\\theta}_{\\tau})-C_{2}\\eta^{2}\\rho^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "\u2013 Claim II. $X_{\\tau+1}-X_{\\tau}+C_{2}\\eta^{2}\\rho^{2}$ is $\\mathcal{O}(\\eta^{2}\\rho^{2}+\\eta^{3/2}\\rho^{2}/p^{1/2})$ -sub-Gaussian. From the definition of $X_{\\tau}$ , we only need to prove for the case $\\overline{{C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}}}$ . If $C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}$ , then $X_{\\tau}=\\mathcal{L}(\\pmb{\\theta}_{\\tau})$ and $X_{\\tau+1}=\\mathcal{L}(\\pmb{\\theta}_{\\tau+1})$ . Similar to Step I, it is clear that $\\pmb{\\theta}_{\\tau+1}\\in\\mathbb{B}(K;R)$ . Applying the smoothness, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(\\theta_{\\tau+1})=\\mathcal{L}\\left(\\theta_{\\tau}-\\eta\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)\\right)}\\\\ &{=\\!\\mathcal{L}(\\theta_{\\tau})-\\eta\\,\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)\\rangle+\\mathcal{O}\\left(\\eta^{2}\\left\\Vert\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)\\right\\Vert^{2}\\right)}\\\\ &{=\\!\\mathcal{L}(\\theta_{\\tau})-\\eta\\left\\Vert\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\Vert^{2}-\\eta\\rho\\left\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})v_{t}\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\mathcal{O}\\left(\\eta\\rho^{2}\\left\\Vert\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\Vert\\right)+\\mathcal{O}\\left(\\eta^{2}\\left\\Vert\\nabla\\mathcal{L}\\left(\\theta_{\\tau}\\right)\\right\\Vert^{2}+\\eta^{2}\\rho^{2}\\right)}\\\\ &{=\\!\\mathcal{L}(\\theta_{\\tau})+\\mathcal{O}\\left(\\eta^{2}\\rho^{2}\\right)-\\eta\\rho\\left\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})v_{t}\\right\\rangle+\\mathcal{O}\\left(\\eta\\rho^{2}\\sqrt{\\eta}\\rho\\right)+\\mathcal{O}\\left(\\eta^{3}\\rho^{2}+\\eta^{2}\\rho^{2}\\right)}\\\\ &{=\\!\\mathcal{L}(\\theta_{\\tau})-\\eta\\rho\\left\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})v_{t}\\right\\rangle+\\mathcal{O}(\\eta^{2}\\rho^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which implies: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|X_{\\tau+1}-X_{\\tau}-C_{2}\\eta^{2}\\rho^{2}\\right\\|_{\\psi_{2}}\\leqslant\\|{\\mathcal{L}}(\\theta_{\\tau+1})-{\\mathcal{L}}(\\theta_{\\tau})\\|_{\\psi_{2}}+\\left\\|C_{2}\\eta^{2}\\rho^{2}\\right\\|_{\\psi_{2}}}\\\\ &{\\leqslant\\left\\|\\eta\\rho\\left\\langle\\nabla{\\mathcal{L}}(\\theta_{\\tau}),\\nabla^{2}{\\mathcal{L}}(\\theta_{\\tau})\\upsilon_{t}\\right\\rangle\\right\\|_{\\psi_{2}}+{\\mathcal{O}}(\\eta^{2}\\rho^{2})}\\\\ &{\\leqslant\\eta\\rho\\left\\|\\nabla^{2}{\\mathcal{L}}(\\theta_{\\tau})\\nabla{\\mathcal{L}}(\\theta_{\\tau})\\right\\|\\left\\|\\left\\langle\\frac{\\nabla^{2}{\\mathcal{L}}(\\theta_{\\tau})\\nabla{\\mathcal{L}}(\\theta_{\\tau})}{\\left\\|\\nabla^{2}{\\mathcal{L}}(\\theta_{\\tau})\\nabla{\\mathcal{L}}(\\theta_{\\tau})\\right\\|},\\upsilon_{t}\\right\\rangle\\right\\|_{\\psi_{2}}+{\\mathcal{O}}(\\eta^{2}\\rho^{2})}\\\\ &{\\leqslant{\\mathcal{O}}\\left(\\eta^{3/2}\\rho^{2}\\left\\|\\left\\langle\\frac{\\nabla^{2}{\\mathcal{L}}(\\theta_{\\tau})\\nabla{\\mathcal{L}}(\\theta_{\\tau})}{\\left\\|\\nabla^{2}{\\mathcal{L}}(\\theta_{\\tau})\\nabla{\\mathcal{L}}(\\theta_{\\tau})\\right\\|},\\upsilon_{t}\\right\\rangle\\right\\|_{\\psi_{2}}\\right)+{\\mathcal{O}}(\\eta^{2}\\rho^{2})}\\\\ {\\overset{\\mathrm{Lemma~FS}}{\\leqslant}{\\mathcal{O}}\\left(\\eta^{3/2}\\rho^{2}/\\sqrt{p}\\right)+{\\mathcal{O}}(\\eta^{2}\\rho^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "With the preparation of Claim I and Claim $\\mathrm{II}$ , we can use the Azuma-Hoeffding inequality (Lemma F.4 (ii)): for any $Q>0$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(X_{t+1}-X_{s+1}+(t-s)C_{2}\\eta^{2}\\rho^{2}>Q\\right)\\leqslant2\\exp\\left(-\\frac{Q^{2}}{2(t-s)\\left(\\mathcal{O}(\\eta^{3/2}\\rho^{2}/p^{1/2}+\\eta^{2}\\rho^{2})\\right)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As proved in Claim I, $\\begin{array}{r}{X_{s+1}\\,=\\,\\mathcal{L}(\\pmb{\\theta}_{s+1})\\,\\leqslant\\,\\frac{3}{2}C_{1}\\eta\\rho^{2}}\\end{array}$ due to $\\mathcal{L}(\\pmb{\\theta}_{s})\\,\\leqslant\\,C_{1}\\eta\\rho^{2}$ . Therefore, by choosing $\\begin{array}{r}{Q=(t-s)C_{2}\\eta^{2}\\rho^{2}-\\frac{3}{2}C_{1}\\eta\\rho^{2}+2C_{1}\\eta\\rho^{2}=(t-s)C_{2}\\eta^{2}\\rho^{2}+\\frac{1}{2}C_{1}\\eta\\rho^{2}}\\end{array}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leqslant\\mathbb{P}\\left(X_{t+1}-X_{s+1}+(t-s)C_{2}\\eta^{2}\\rho^{2}>(t-s)C_{2}\\eta^{2}\\rho^{2}+\\cfrac{1}{2}C_{1}\\eta\\rho^{2}\\right)}\\\\ &{\\leqslant2\\exp\\left(-\\cfrac{\\big((t-s)C_{2}\\eta^{2}\\rho^{2}+\\frac{1}{2}C_{1}\\eta\\rho^{2}\\big)^{2}}{2\\left(t-s\\right)\\left(\\mathcal{O}(\\eta^{3/2}\\rho^{2}/p^{1/2}+\\eta^{2}\\rho^{2})\\right)^{2}}\\right)}\\\\ &{\\leqslant2\\exp\\left(-\\cfrac{4(t-s)C_{2}\\eta^{2}\\rho^{2}\\cdot\\frac{1}{2}C_{1}\\eta\\rho^{2}}{4(t-s)\\left(\\mathcal{O}(\\eta^{3}\\rho^{4}/p+\\eta^{4}\\rho^{4})\\right)}\\right)\\leqslant2\\exp\\left(-\\Omega\\left(\\frac{1}{\\eta+p^{-1}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we obtain the union bound: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\exists\\,t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}+T_{\\mathrm{II}}],\\mathcal{L}(\\theta_{t})\\geqslant2C_{1}\\eta\\rho^{2}\\right)\\leqslant\\displaystyle\\sum_{t=T_{I}}^{T_{\\mathrm{I}}+T_{\\mathrm{II}}-1}\\left(\\mathbb{P}_{t+1,t}+\\displaystyle\\sum_{s=T_{\\mathrm{I}}}^{t-1}\\mathbb{P}_{t+1,s}\\right)}\\\\ &{\\leqslant\\displaystyle\\sum_{t=T_{I}}^{T_{\\mathrm{I}}+T_{\\mathrm{II}}-1}\\displaystyle\\sum_{s=T_{\\mathrm{I}}}^{t-1}\\mathbb{P}_{t+1,s}\\leqslant T_{\\mathrm{II}}^{2}\\exp\\left(-\\Omega\\left(\\displaystyle\\frac{1}{\\eta+p^{-1}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, with probability at least $\\begin{array}{r}{1-T_{\\mathrm{II}}^{2}\\exp\\left(-\\Omega\\left(\\frac{1}{\\eta+p^{-1}}\\right)\\right)}\\end{array}$ , for any $t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}+T_{\\mathrm{II}}].$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|\\leqslant\\sqrt{\\frac{2}{\\mu}\\mathcal{L}(\\pmb{\\theta}_{t})}\\leqslant2\\sqrt{\\frac{C_{1}}{\\mu}}\\sqrt{\\eta}\\rho=\\frac{4\\beta_{2}^{3/2}}{\\mu}\\sqrt{\\eta}\\rho=\\mathcal{O}(\\sqrt{\\eta}\\rho).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D.2.2 Proof of Moving Near Minimizers for SAM-IRE ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We prove \u201ckeeping moving near minimizers\u201d for SAM-IRE. The proof outline for SAM-IRE is the same as SAM. However, the key non-trivial difference is that the IRE term will hardly cause loss instability since IRE only perturbs the parameters in the flat directions. ", "page_idx": 26}, {"type": "text", "text": "Under the conditions in Theorem 5.5, the update rule of IRE on average SAM is: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\eta\\nabla\\mathcal{L}\\left(\\pmb{\\theta}_{t}+\\rho\\frac{\\pmb{\\xi}_{t}}{\\lVert\\pmb{\\xi}_{t}\\rVert}\\right)-\\eta\\kappa P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{t}))\\nabla\\mathcal{L}\\left(\\pmb{\\theta}_{t}+\\rho\\frac{\\pmb{\\xi}_{t}}{\\lVert\\pmb{\\xi}_{t}\\rVert}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let the $\\rho$ in SAM and the $\\kappa$ in IRE satisfy: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\rho=\\mathcal{O}(\\sqrt{\\eta}),\\quad\\kappa\\leqslant\\frac{1}{\\rho}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For simplicity, we denote ", "page_idx": 26}, {"type": "equation", "text": "$$\nv_{t}:=\\frac{\\xi_{t}}{\\|\\xi_{t}\\|},\\quad P(\\pmb\\theta_{t}):=P_{m+1:p}(\\nabla^{2}\\mathcal{L}(\\pmb\\theta_{t})),\\quad C_{1}=\\frac{4\\beta_{2}^{3}}{\\mu}\\vee\\frac{4\\beta_{2}\\beta_{3}^{2}}{\\mu},\\quad C_{2}=\\beta_{2}^{3}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Following the proof for SAM, we denote ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t+1,t}:=\\mathbb{P}\\left(\\mathcal{L}(\\pmb{\\theta}_{t+1})\\geqslant2C_{1}\\eta\\rho^{2}\\Big|\\mathcal{L}(\\pmb{\\theta}_{t})<C_{1}\\eta\\rho^{2}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{t+1,s}:=\\mathbb{P}\\Big(\\mathcal{L}(\\theta_{t+1})\\geqslant2C_{1}\\eta\\rho^{2};}\\\\ &{\\qquad\\qquad\\quad\\forall\\,\\tau\\in[s+1,t],C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\theta_{\\tau})<2C_{1}\\eta\\rho^{2}\\Big|\\mathcal{L}(\\theta_{s})<C_{1}\\eta\\rho^{2}\\Big),\\;s\\in[T_{1},t-1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists\\,t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}+T_{\\mathrm{II}}],\\mathcal{L}(\\theta_{t})\\geqslant2C_{1}\\eta\\rho^{2}\\right)\\leqslant\\sum_{t=T_{\\mathrm{I}}}^{T_{\\mathrm{I}}+T_{\\mathrm{II}}-1}\\left(\\mathbb{P}_{t+1,t}+\\sum_{s=T_{\\mathrm{I}}}^{t-1}\\mathbb{P}_{t+1,s}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "\u2022 Step I. Bounding $\\mathbb{P}_{t+1,t}$ . ", "page_idx": 27}, {"type": "text", "text": "From $\\mathcal{L}(\\pmb{\\theta}_{t})\\leqslant C_{1}\\eta\\rho^{2}$ , we have $\\begin{array}{r}{\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|\\leqslant\\sqrt{\\frac{2}{\\mu}\\mathcal{L}(\\pmb{\\theta}_{t})}=\\mathcal{O}(\\sqrt{\\eta}\\rho)}\\end{array}$ , thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|\\leqslant\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|+\\rho=\\mathcal{O}(\\sqrt{\\eta}\\rho)+\\mathcal{O}(\\rho)<R,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which means $\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}\\in\\mathbb{B}(K;R)$ . Furthermore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{t+1}-\\Phi(\\theta_{t})\\|}\\\\ &{\\leqslant\\|\\theta_{t}-\\Phi(\\theta_{t})\\|+\\eta\\,\\|\\nabla{\\mathcal L}(\\theta_{t}+\\rho v_{t})\\|+\\eta\\kappa\\,\\|P(\\theta_{t})\\nabla{\\mathcal L}(\\theta_{t}+\\rho v_{t})\\|}\\\\ &{\\tt{L e m m a}\\,D.5}\\\\ &{\\leqslant\\quad{\\mathcal O}(\\sqrt{\\eta}\\rho)+\\eta\\,\\|\\nabla{\\mathcal L}(\\theta_{t})\\|+\\beta_{2}\\eta\\rho+{\\mathcal O}\\left(\\eta\\kappa\\rho^{2}\\right)}\\\\ &{\\leqslant{\\mathcal O}(\\sqrt{\\eta}\\rho)+{\\mathcal O}(\\eta^{3/2}\\rho)+{\\mathcal O}\\left(\\eta\\rho\\right)+{\\mathcal O}\\left(\\eta\\rho\\right)}\\\\ &{\\leqslant{\\mathcal O}(\\sqrt{\\eta}\\rho)+{\\mathcal O}(\\eta^{3/2}\\rho)+{\\mathcal O}\\left(\\eta\\rho\\right)\\leqslant R,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies $\\theta_{t+1}\\in\\mathbb{B}(K;R)$ . Consequently, we have the following quadratic upper bound: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\zeta(\\theta_{4})+\\mathcal{E}\\left(\\theta_{3}-\\theta_{2}\\right)\\psi(\\theta_{2}+\\mu_{3})-\\psi^{2}\\psi(\\theta_{4})\\nabla^{2}(\\theta_{4}+\\mu_{3})}\\\\ &{\\quad=\\psi\\left(\\nabla L(\\theta_{3}),\\nabla L(\\theta_{4}+\\mu_{3})\\right)+\\psi\\left(\\theta_{4}\\right)\\nabla L(\\theta_{5}+\\mu_{3})}\\\\ &{\\quad\\quad+\\frac{\\psi^{2}\\psi^{2}}{2}\\left(\\nabla L(\\theta_{4})+\\rho_{1}+\\rho_{0}\\right)\\psi\\left(\\theta_{3}\\right)\\nabla L(\\theta_{4}+\\mu_{3})\\psi\\left(\\theta_{2}+\\mu_{3})}\\\\ &{\\quad\\quad\\null+\\frac{\\psi^{2}\\psi}{2}\\left(\\nabla L(\\theta_{2})+\\rho_{1}+\\rho_{0}\\right)\\psi\\left(\\theta_{3}\\right)\\psi\\left(\\theta_{4},\\mu_{3}\\right)\\nabla L(\\theta_{4}+\\mu_{3})}\\\\ &{\\quad\\quad\\null+\\frac{\\psi^{2}\\psi}{2}\\left(\\nabla L(\\theta_{3})+\\rho_{1}\\right)^{2}+\\psi^{2}\\left(\\theta_{4}\\right)\\nabla L(\\theta_{2}+\\mu_{3})\\psi\\left(\\theta_{4}+\\mu_{3})\\psi^{2}\\left(\\theta_{4}+\\mu_{3}\\right)}\\\\ &{\\quad\\quad\\null+\\frac{\\psi}{2}\\left(\\nabla L(\\theta_{2})+\\psi\\left(\\theta_{3}\\right)\\right)\\psi^{2}+\\psi^{2}\\left(\\theta_{1}\\right)\\nabla L(\\theta_{3})\\nabla L(\\theta_{4})\\psi}\\\\ &{\\quad\\quad\\null+\\mathcal{E}\\left(\\theta_{3})-\\psi(\\theta_{4})\\nabla L(\\theta_{4})\\psi^{2}(\\theta_{4})\\psi_{1}+\\rho_{0}\\frac{\\psi^{2}\\psi^{2}}{2}\\left(\\left|\\nabla L(\\theta_{3})+\\rho_{1}\\right|\\right)}\\\\ &{\\quad\\quad\\null-\\psi(\\theta_{4})\\nabla L(\\theta_{2})\\psi(\\theta_{3})\\nabla L(\\theta_{4})\\psi+\\frac{\\psi^{2}\\psi^{2}}{2}\\left(\\left|\\nabla L(\\theta_{4})+\\rho_{3}\\right|\\right)\\psi\\left(\\theta_{4}\\right)\\nabla L(\\theta_{2}) \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t+1,t}=\\mathbb{P}\\left(\\mathcal{L}(\\pmb{\\theta}_{t+1})\\geqslant2C_{1}\\eta\\rho^{2}\\middle|\\mathcal{L}(\\pmb{\\theta}_{t})<C_{1}\\eta\\rho^{2}\\right)=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 Step II. Bounding $\\mathbb{P}_{t+1,s}$ for $s\\in[T_{\\mathrm{I}},t-1]$ . ", "page_idx": 27}, {"type": "text", "text": "We prove this step under the condition $\\mathcal{L}(\\pmb{\\theta}_{s})<C_{1}\\eta\\rho^{2}$ . Define a process $\\{X_{\\tau}\\}_{\\tau=s}^{t+1};X_{s+1}=$ $\\mathcal{L}(\\bar{\\pmb{\\theta}_{s+1}})$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\nX_{\\tau+1}=\\left\\{\\!\\!\\begin{array}{l}{{\\!\\displaystyle\\mathcal{L}(\\pmb{\\theta}_{\\tau+1}),\\quad\\mathrm{if}\\;C_{1}\\eta\\rho^{2}\\leqslant X_{\\tau}=\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}\\quad}}\\\\ {{\\!\\displaystyle X_{\\tau}-C_{2}\\eta^{2}\\rho^{2},\\quad\\mathrm{else}}}\\end{array}\\!\\!\\right..\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "It is clear that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t+1,s}\\leqslant\\mathbb{P}\\left(X_{t+1}\\geqslant2C_{1}\\eta\\rho^{2}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then our key step is to prove the following two claims about the process $\\{X_{\\tau}\\}$ . ", "page_idx": 27}, {"type": "text", "text": "\u2013 Claim I. $X_{\\tau}-C_{2}\\tau\\eta\\rho^{2}$ is a super-martingale. From the definition of $X_{\\tau}$ , we only need to prove that if $\\overline{{C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}}}$ , then $\\mathbb{E}[\\mathscr{L}(\\pmb{\\theta}_{\\tau+1})]\\leqslant\\mathscr{L}(\\pmb{\\theta}_{\\tau})-C_{2}\\eta^{2}\\rho^{2}$ . ", "page_idx": 27}, {"type": "text", "text": "If $C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}$ , similar to Step I, it is clear that $\\pmb{\\theta}_{\\tau+1}\\in\\mathbb{B}(K;R)$ . Applying the quadratic upper bound, it holds that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(\\theta_{\\tau+1})=\\mathcal{L}(\\theta_{\\tau}-\\eta\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho v_{\\tau})-\\eta\\kappa P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho v_{\\tau}))}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{\\tau})-\\eta\\sqrt{\\nabla\\mathcal{L}(\\theta_{\\tau})},\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho v_{\\tau})+\\kappa P(\\theta_{\\theta})\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho v_{\\tau})\\big)}\\\\ &{\\quad\\quad+\\frac{\\eta^{2}\\beta_{2}}{2}\\|\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho v_{\\tau})+\\kappa P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho v_{\\tau})\\|^{2}}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{\\tau})-\\eta\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho v_{\\tau})\\rangle-\\kappa\\eta\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho v_{\\tau})\\rangle}\\\\ &{\\quad\\quad\\quad+\\eta^{2}\\beta_{2}\\left(\\|\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho v_{\\tau})\\|^{2}+\\kappa^{2}\\left\\|P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho v_{\\tau})\\right\\|^{2}\\right)}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{\\tau})-\\eta\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|^{2}-\\eta\\rho\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),\\nabla^{2}\\mathcal{L}(\\theta_{t})v_{\\tau}\\rangle+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\left\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\|}\\\\ &{\\quad\\quad-\\kappa\\eta\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau})\\rangle-\\kappa\\eta\\rho\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),P(\\theta_{\\tau})\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})v_{\\tau}\\rangle+\\kappa\\eta\\frac{\\beta_{3}\\rho^{2}}{2}\\left\\|P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\|}\\\\ &{\\quad\\quad\\quad+\\eta^{\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Taking the expectation, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[\\langle G(\\tau_{+})\\rangle]}\\\\ &{\\leqslant\\mathcal{L}(\\mu_{\\tau})\\sim\\eta\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|^{2}+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|-\\kappa\\eta\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau})\\rangle}\\\\ &{\\quad+\\kappa\\eta\\frac{\\beta_{3}\\partial^{2}\\rho^{2}}{2}\\|P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau})\\|+\\eta^{2}\\beta_{2}\\left((\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|+\\rho\\beta_{2})^{2}+\\kappa^{2}\\|P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho\\nu_{\\tau})\\|^{2}\\right)}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{\\tau})-\\eta\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|^{2}+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|+\\kappa\\eta\\frac{\\beta_{3}\\partial^{2}}{2}\\|P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau})\\|}\\\\ &{\\quad+2\\eta^{2}\\beta_{2}\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|^{2}+2\\beta_{3}^{2}\\eta^{2}\\rho^{2}+\\beta_{2}\\eta^{2}\\kappa^{2}\\|P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau}+\\rho\\nu_{\\tau})\\|^{2}}\\\\ &{\\stackrel{\\mathrm{conn}}{\\leqslant}\\mathcal{L}(\\theta_{\\tau})-\\frac{3\\eta}{4}\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|^{2}+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|+\\mathcal{O}\\left(\\kappa\\eta\\rho^{2}\\cdot\\eta\\rho^{2}\\right)}\\\\ &{\\quad+2\\beta_{3}^{2}\\eta^{2}\\rho^{2}+\\beta_{2}\\eta^{2}\\kappa^{2}\\left(\\mathcal{O}(\\sqrt{\\eta\\rho})+\\frac{\\beta_{3}\\partial^{2}}{2}\\rho^{2}\\right)^{2}}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{\\tau})-\\frac \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "From $C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}$ and $\\rho=\\mathcal{O}(\\sqrt{\\eta})$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\nabla\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\|\\geqslant\\sqrt{2\\mu\\mathcal{L}(\\pmb{\\theta}_{\\tau})}\\geqslant\\sqrt{2C_{1}\\mu}\\sqrt{\\eta}\\rho\\geqslant4\\beta_{3}\\rho^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, recall $\\kappa\\leqslant1/\\rho$ . Therefore, we have the upper bound: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[\\mathcal{L}(\\theta_{\\tau+1})]}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{\\tau})-\\frac{3\\eta}{4}\\,\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|^{2}+\\frac{\\eta\\rho^{2}\\beta_{3}}{2}\\,\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|+2\\beta_{2}^{3}\\eta^{2}\\rho^{2}+\\beta_{2}\\beta_{3}^{2}\\eta^{2}\\kappa^{2}\\rho^{4}+o(\\eta^{2}\\rho^{2})}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{\\tau})-\\frac{5\\eta}{8}\\,\\|\\nabla\\mathcal{L}(\\theta_{\\tau})\\|^{2}+2\\beta_{2}^{3}\\eta^{2}\\rho^{2}+\\beta_{2}\\beta_{3}^{2}\\eta^{2}\\kappa^{2}\\rho^{4}+o(\\eta^{2}\\rho^{2})}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{\\tau})-\\frac{10}{8}\\eta\\mu\\mathcal{L}(\\theta_{\\tau})+2\\beta_{2}^{3}\\eta^{2}\\rho^{2}+\\beta_{2}\\beta_{3}^{2}\\eta^{2}\\rho^{2}+o(\\eta^{2}\\rho^{2}).}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{\\tau})-\\frac{10}{2}\\left(\\frac{\\beta_{2}^{3}}{\\mu}\\vee\\frac{\\beta_{2}\\beta_{3}^{2}}{\\mu}\\right)\\eta^{2}\\rho^{2}+2\\beta_{2}^{3}\\eta^{2}\\rho^{2}+\\beta_{2}\\beta_{3}^{2}\\eta^{2}\\rho^{2}+o(\\eta^{2}\\rho^{2})}\\\\ &{\\leqslant\\mathcal{L}(\\theta_{\\tau})-\\beta_{2}^{3}\\eta^{2}\\rho^{2}=\\mathcal{L}(\\theta_{\\tau})-C_{2}\\eta^{2}\\rho^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u2013 Claim II. $X_{\\tau+1}-X_{\\tau}+C_{2}\\eta^{2}\\rho^{2}$ is $\\mathcal{O}(\\eta^{2}\\rho^{2}+\\eta^{3/2}\\rho^{2}/p^{1/2})$ -sub-Gaussian. From the definition of $X_{\\tau}$ , we only need to prove for the case $\\overline{{C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}}}$ . If $C_{1}\\eta\\rho^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\leqslant2C_{1}\\eta\\rho^{2}$ , then $X_{\\tau}=\\mathcal{L}(\\pmb{\\theta}_{\\tau})$ and $X_{\\tau+1}=\\mathcal{L}(\\pmb{\\theta}_{\\tau+1})$ . Similar to Step I, it is clear that $\\pmb{\\theta}_{\\tau+1}\\in\\mathbb{B}(K;R)$ . ", "page_idx": 28}, {"type": "text", "text": "Applying the smoothness, we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(\\theta_{\\tau+1})=\\mathcal{L}\\left(\\theta_{\\tau}-\\eta\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)-\\eta\\kappa P(\\theta_{\\tau})\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)\\right)}\\\\ &{=\\!\\mathcal{L}(\\theta_{\\tau})-\\eta\\left\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)+\\kappa P(\\theta_{t})\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)\\right\\rangle}\\\\ &{\\qquad+\\left(\\eta^{2}\\left\\Vert\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)+\\kappa P(\\theta_{\\tau})\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)\\right\\Vert^{2}\\right)}\\\\ &{=\\!\\mathcal{L}(\\theta_{\\tau})-\\eta\\left\\Vert\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\Vert^{2}-\\eta\\rho\\left\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})v_{\\tau}\\right\\rangle+\\mathcal{O}\\left(\\eta\\rho^{2}\\left\\Vert\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\Vert\\right)}\\\\ &{\\qquad-\\eta\\kappa\\left\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\rangle-\\eta\\kappa\\rho\\left\\langle\\nabla\\mathcal{L}(\\theta_{\\tau}),P(\\theta_{\\tau})\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})v_{t}\\right\\rangle+\\mathcal{O}\\left(\\eta\\kappa\\rho^{2}\\left\\Vert P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\Vert^{2}\\right)}\\\\ &{\\qquad+\\left(\\eta^{2}\\left\\Vert\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)+\\kappa P(\\theta_{\\tau})\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)\\right\\Vert^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In the same way as the proof of Claim I, it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\left\\lVert\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\rVert^{2}=\\mathcal{O}(\\eta^{2}\\rho^{2}),\\quad\\eta\\rho^{2}\\left\\lVert\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\rVert=\\mathcal{O}(\\eta^{2}\\rho^{2}),\\quad\\eta\\kappa\\rho^{2}\\left\\lVert P(\\theta_{\\tau})\\nabla\\mathcal{L}(\\theta_{\\tau})\\right\\rVert=\\mathcal{O}\\left(\\eta^{2}\\rho^{3}\\right),}\\\\ &{\\qquad\\qquad\\quad\\eta^{2}\\left\\lVert\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)+\\kappa P(\\theta_{\\tau})\\nabla\\mathcal{L}\\left(\\theta_{\\tau}+\\rho v_{\\tau}\\right)\\right\\rVert^{2}=\\mathcal{O}\\left(\\eta^{2}\\rho^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(\\pmb{\\theta}_{\\tau+1})-\\mathcal{L}(\\pmb{\\theta}_{\\tau})}\\\\ &{=-\\eta\\rho\\left\\langle\\nabla\\mathcal{L}(\\pmb{\\theta}_{\\tau}),\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\pmb{v}_{t}\\right\\rangle-\\eta\\kappa\\rho\\left\\langle\\nabla\\mathcal{L}(\\pmb{\\theta}_{\\tau}),P(\\pmb{\\theta}_{\\tau})\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{\\tau})\\pmb{v}_{t}\\right\\rangle+\\mathcal{O}(\\eta^{2}\\rho^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which implies: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|X_{\\tau+1}-X_{\\tau}-C_{2}\\eta^{2}\\rho^{\\tau}\\|_{\\infty}^{2}\\leq\\|Z(\\theta_{\\tau})-{\\cal C}(\\theta_{\\tau})\\|_{\\{\\infty,2\\}}+\\|C_{2}\\eta^{2}\\rho^{\\tau}\\|_{\\infty}}\\\\ &{\\leq\\eta\\rho\\left\\|\\left\\langle\\nabla C(\\theta_{\\tau}),\\nabla^{2}\\zeta(\\theta_{\\tau})\\right\\rangle\\right\\|_{\\mathrm{e}_{\\theta}}+\\eta\\kappa\\rho\\left\\|\\nabla C(\\theta_{\\tau}),P(\\theta_{\\tau})\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})\\nabla^{2}\\right\\rangle_{\\|\\phi_{\\tau}}}\\\\ &{\\leq\\eta\\rho\\left\\|\\left\\|\\nabla^{2}\\mathcal{L}(\\theta_{\\tau}),\\nabla^{2}\\mathcal{L}(\\theta)\\right\\|\\left\\|\\left\\langle\\frac{\\nabla^{2}\\zeta(\\theta_{\\tau})}{\\|\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})\\nabla^{2}\\zeta(\\theta_{\\tau})\\|},\\psi_{\\tau}\\right\\rangle\\right\\|_{\\mathrm{e}_{\\theta}}}\\\\ &{\\qquad\\quad+\\eta\\kappa\\rho\\left\\|\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})P(\\theta_{\\tau})\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})\\right\\|\\left\\|\\left\\langle\\frac{\\nabla^{2}\\zeta(\\theta_{\\tau})P(\\theta_{\\tau})}{\\|\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})P(\\theta_{\\tau})\\nabla^{2}\\widehat{C}(\\theta_{\\tau})\\|},\\psi_{\\tau}\\right\\rangle\\right\\|_{\\mathrm{e}_{\\theta}}+\\mathcal{O}(\\eta^{2}\\rho^{2})}\\\\ &{\\underset{\\leq\\theta}{\\operatorname{Lemay}}{\\operatorname{Lemmax}}\\:\\sigma\\left(\\eta^{3/2}\\rho^{2}\\right)\\left\\|\\left\\langle\\frac{\\nabla^{2}\\zeta(\\theta_{\\tau})\\nabla C(\\theta_{\\tau})}{\\|\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})\\nabla\\zeta(\\theta_{\\tau})\\|},\\psi_{\\tau}\\right\\rangle\\right\\|_{\\mathrm{e}_{\\theta}}}\\\\ &{\\qquad\\quad+\\mathcal{O}\\left(\\eta^{3/2}\\left(\\eta^{2}\\right)^{2}\\left\\|\\left\\langle\\frac{\\nabla^{2}\\zeta(\\theta_{\\tau})}{\\|\\nabla^{2}\\mathcal{L}(\\theta_{\\tau})\\nabla^{2} \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "With the preparation of Claim I and Claim $\\mathrm{II}$ , we can use the Azuma-Hoeffding inequality (Lemma F.4 (ii)): for any $Q>0$ , it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(X_{t+1}-X_{s+1}+(t-s)C_{2}\\eta^{2}\\rho^{2}>Q\\right)\\leqslant2\\exp\\left(-\\frac{Q^{2}}{2(t-s)\\left(\\mathcal{O}(\\eta^{3/2}\\rho^{2}/p^{1/2}+\\eta^{2}\\rho^{2})\\right)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As proved in Claim I, $\\begin{array}{r}{X_{s+1}\\,=\\,\\mathcal{L}(\\pmb{\\theta}_{s+1})\\,\\leqslant\\,\\frac{3}{2}C_{1}\\eta\\rho^{2}}\\end{array}$ due to $\\mathcal{L}(\\pmb{\\theta}_{s})\\,\\leqslant\\,C_{1}\\eta\\rho^{2}$ . Therefore, by choosing $\\begin{array}{r}{Q=(t-s)C_{2}\\eta^{2}\\rho^{2}-\\frac{3}{2}C_{1}\\eta\\rho^{2}+2C_{1}\\eta\\rho^{2}=(t-s)C_{2}\\eta^{2}\\rho^{2}+\\frac{1}{2}C_{1}\\eta\\rho^{2}}\\end{array}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{t+1,s}\\leqslant\\mathbb{P}\\left(X_{t+1}\\geqslant2C_{1}\\eta\\rho^{2}\\right)}\\\\ &{\\leqslant\\mathbb{P}\\left(X_{t+1}-X_{s+1}+(t-s)C_{2}\\eta^{2}\\rho^{2}>(t-s)C_{2}\\eta^{2}\\rho^{2}+\\frac{1}{2}C_{1}\\eta\\rho^{2}\\right)}\\\\ &{\\leqslant2\\exp\\left(-\\frac{\\big((t-s)C_{2}\\eta^{2}\\rho^{2}+\\frac{1}{2}C_{1}\\eta\\rho^{2}\\big)^{2}}{2(t-s)\\left(\\mathcal{O}(\\eta^{3/2}\\rho^{2}/p^{1/2}+\\eta^{2}\\rho^{2})\\right)^{2}}\\right)}\\\\ &{\\leqslant2\\exp\\left(-\\frac{4(t-s)C_{2}\\eta^{2}\\rho^{2}\\cdot\\frac{1}{2}C_{1}\\eta\\rho^{2}}{4(t-s)\\left(\\mathcal{O}(\\eta^{3}\\rho^{4}/p+\\eta^{4}\\rho^{4})\\right)}\\right)\\leqslant2\\exp\\left(-\\Omega\\left(\\frac{1}{\\eta+p^{-1}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, we obtain the union bound: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\exists\\,t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}+T_{\\mathrm{II}}],\\mathcal{L}(\\theta_{t})\\geqslant2C_{1}\\eta\\rho^{2}\\right)\\leqslant\\displaystyle\\sum_{t=T_{I}}^{T_{\\mathrm{I}}+T_{\\mathrm{II}}-1}\\left(\\mathbb{P}_{t+1,t}+\\displaystyle\\sum_{s=T_{\\mathrm{I}}}^{t-1}\\mathbb{P}_{t+1,s}\\right)}\\\\ &{\\leqslant\\displaystyle\\sum_{t=T_{I}}^{T_{\\mathrm{I}}+T_{\\mathrm{II}}-1}\\displaystyle\\sum_{s=T_{\\mathrm{I}}}^{t-1}\\mathbb{P}_{t+1,s}\\leqslant T_{\\mathrm{II}}^{2}\\exp\\left(-\\Omega\\left(\\displaystyle\\frac{1}{\\eta+p^{-1}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, with probability at least $\\begin{array}{r}{1-T_{\\mathrm{II}}^{2}\\exp\\left(-\\Omega\\left(\\frac{1}{\\eta+p^{-1}}\\right)\\right)}\\end{array}$ , for any $t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}+T_{\\mathrm{II}}]$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|\\leqslant\\sqrt{\\frac{2}{\\mu}\\mathcal{L}(\\pmb{\\theta}_{t})}\\leqslant2\\sqrt{\\frac{C_{1}}{\\mu}}\\sqrt{\\eta}\\rho=\\frac{4\\beta_{2}^{3/2}}{\\mu}\\sqrt{\\eta}\\rho=\\mathcal{O}(\\sqrt{\\eta}\\rho).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "D.2.3 Proof of the Effective Dynamics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We have proved that with high probability at least $\\begin{array}{r}{1-T_{\\mathrm{II}}^{2}\\exp\\left(-\\Omega\\left(\\frac{1}{\\eta+p^{-1}}\\right)\\right)}\\end{array}$ , for any $t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}\\pm$ $T_{\\mathrm{II}}]$ , $\\lVert\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\rVert=\\mathcal{O}(\\sqrt{\\eta}\\rho)$ . Then we prove this theorem when the above event occurs. For any $t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}+T_{\\mathrm{II}}]$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\theta_{t+1}-\\theta_{t}\\|}\\\\ &{\\quad\\leqslant\\eta\\,\\|\\nabla{\\mathcal{L}}(\\theta_{t}+\\rho v_{t})\\|+\\eta\\kappa\\,\\|P(\\theta_{t})\\nabla{\\mathcal{L}}(\\theta_{t}+\\rho v_{t})\\|}\\\\ &{\\quad\\leqslant\\eta\\,\\|\\nabla{\\mathcal{L}}(\\theta_{t})\\|+{\\mathcal{O}}(\\eta\\rho)+\\eta\\kappa\\,\\|P(\\theta_{t})\\nabla{\\mathcal{L}}(\\theta_{t})\\|+\\eta\\kappa\\rho\\,\\|P(\\theta_{t})\\nabla^{2}{\\mathcal{L}}(\\theta_{t})\\|+{\\mathcal{O}}(\\eta\\kappa\\rho^{2})}\\\\ &{\\quad\\underset{\\leqslant\\quad{\\mathcal{O}}(\\eta^{3/2}\\rho)}{\\mathrm{Lemma~D.S}}{\\mathrm{Lemma~D.}}{\\mathcal{O}}(\\eta^{3/2}\\rho)+{\\mathcal{O}}(\\eta\\rho)+{\\mathcal{O}}(\\eta^{3/2}\\rho)+{\\mathcal{O}}(\\eta^{3/2}\\rho^{2})+{\\mathcal{O}}(\\eta\\rho^{2})+{\\mathcal{O}}(\\eta\\rho)={\\mathcal{O}}(\\eta\\rho).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then by Taylor\u2019s expansion, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Phi(\\theta_{t+1})-\\Phi(\\theta_{t})=\\partial\\Phi(\\theta_{t})\\left(\\theta_{t+1}-\\theta_{t}\\right)+\\mathcal{O}\\left(\\left\\Vert\\theta_{t+1}-\\theta_{t}\\right\\Vert^{2}\\right)}\\\\ &{=-\\eta\\partial\\Phi(\\theta_{t})\\nabla\\mathcal{L}(\\theta_{t}+\\rho v_{t})-\\eta\\kappa\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\mathcal{L}(\\theta_{t}+\\rho v_{t})+\\mathcal{O}(\\eta^{2}\\rho^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the term $\\partial\\Phi(\\pmb{\\theta}_{t})\\nabla\\mathcal{L}(\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t})$ and $\\partial\\Phi(\\pmb{\\theta}_{t})P(\\pmb{\\theta}_{t})\\nabla\\mathcal{L}(\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t})$ , using Taylor\u2019s expansion, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\partial\\Phi(\\theta_{t})\\nabla\\mathcal L(\\theta_{t}+\\rho v_{t})}\\\\ &{=\\!\\partial\\Phi(\\theta_{t})\\nabla\\mathcal L(\\theta_{t})+\\rho\\partial\\Phi(\\theta_{t})\\nabla^{2}\\mathcal L(\\theta_{t})v_{t}+\\frac{\\rho^{2}}{2}\\partial\\Phi(\\theta_{t})\\nabla\\operatorname{Tr}\\big(v_{t}\\nabla^{2}\\mathcal L(\\theta_{t})v_{t}^{\\top}\\big)+\\mathcal O(\\rho^{3})}\\\\ &{=\\!\\partial\\Phi(\\theta_{t})\\nabla\\mathcal L(\\theta_{t})+\\rho\\partial\\Phi(\\theta_{t})\\nabla^{2}\\mathcal L(\\theta_{t})v_{t}+\\frac{\\rho^{2}}{2}\\partial\\Phi(\\theta_{t})\\nabla\\left(v_{t}^{\\top}\\nabla^{2}\\mathcal L(\\theta_{t})v_{t}\\right)+\\mathcal O(\\rho^{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\mathcal{L}(\\theta_{t}+\\rho v_{t})}\\\\ &{=\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\mathcal{L}(\\theta_{t})+\\rho\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla^{2}\\mathcal{L}(\\theta_{t})v_{t}+\\frac{\\rho^{2}}{2}\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\mathrm{Tr}\\left(v_{t}\\nabla^{2}\\mathcal{L}(\\theta_{t})v_{t}^{\\top}\\right)+\\mathcal{O}(\\rho^{3})}\\\\ &{=\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\mathcal{L}(\\theta_{t})+\\rho\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla^{2}\\mathcal{L}(\\theta_{t})v_{t}+\\frac{\\rho^{2}}{2}\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\left(v_{t}^{\\top}\\nabla^{2}\\mathcal{L}(\\theta_{t})v_{t}\\right)+\\mathcal{O}(\\rho^{3})}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "\u03c13) . Taking the expectation (about $\\pmb{v}_{t}$ ), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\pmb{v}_{t}]=0,\\quad\\mathbb{E}\\left[\\pmb{v}_{t}^{\\top}\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{t})\\pmb{v}_{t}\\right]=\\frac{\\operatorname{Tr}\\left(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{t})\\right)}{p}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Additionally, using Lemma D.2 and Taylor\u2019s expansion, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\partial\\Phi(\\theta_{t})\\nabla\\mathcal{L}(\\theta_{t})=\\mathbf{0};}\\\\ {\\partial\\Phi(\\theta_{t})=\\partial\\Phi(\\Phi(\\theta_{t}))+\\|\\theta_{t}-\\Phi(\\theta_{t})\\|=\\partial\\Phi(\\Phi(\\theta_{t}))+\\mathcal{O}(\\sqrt{\\eta}\\rho);}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\partial\\Phi(\\theta_{t})P(\\theta_{t})=\\partial\\Phi(\\Phi(\\theta_{t}))P(\\Phi(\\theta_{t}))+\\|\\theta_{t}-\\Phi(\\theta_{t})\\|=\\partial\\Phi(\\Phi(\\theta_{t}))+\\mathcal{O}(\\sqrt{\\eta}\\rho);\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\nabla\\,\\mathrm{Tr}\\left(\\nabla^{2}\\mathcal{L}(\\theta_{t})\\right)=\\nabla\\,\\mathrm{Tr}\\left(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta_{t}))\\right)+\\|\\theta_{t}-\\Phi(\\theta_{t})\\|=\\nabla\\,\\mathrm{Tr}\\left(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta_{t}))\\right)+\\mathcal{O}(\\sqrt{\\eta}\\rho);\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\mathcal{L}(\\theta_{t})=\\!\\partial\\Phi(\\Phi(\\theta_{t}))P(\\Phi(\\theta_{t}))\\nabla\\mathcal{L}(\\theta_{t})+\\mathcal{O}\\left(\\|\\theta_{t}-\\Phi(\\theta_{t})\\|\\,\\|\\nabla\\mathcal{L}(\\theta_{t})\\|\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\!\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla\\mathcal{L}(\\theta_{t})+\\mathcal{O}\\left(\\eta\\rho^{2}\\right)={\\bf0}+\\mathcal{O}(\\eta\\rho^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining the results above, we obtain: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\Phi(\\theta_{t+1})]}\\\\ &{=\\!\\Phi(\\theta_{t})-\\eta\\partial\\Phi(\\theta_{t})\\nabla{\\mathcal{L}}(\\theta_{t})-\\eta\\kappa\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla{\\mathcal{L}}(\\theta_{t})}\\\\ &{\\quad-\\frac{\\eta\\rho^{2}}{2p}\\partial\\Phi(\\theta_{t})\\nabla\\mathrm{Tr}\\left(\\nabla^{2}{\\mathcal{L}}(\\theta_{t})\\right)-\\frac{\\kappa\\eta\\rho^{2}}{2p}\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\mathrm{Tr}\\left(\\nabla^{2}{\\mathcal{L}}(\\theta_{t})\\right)+{\\mathcal O}(\\eta\\rho^{3})}\\\\ &{=\\!\\Phi(\\theta_{t})+{\\mathcal O}\\left(\\eta^{2}\\kappa\\rho^{2}\\right)-\\frac{(\\kappa+1)\\eta\\rho^{2}}{2p}\\partial\\Phi(\\theta_{t})\\nabla\\mathrm{Tr}\\left(\\nabla^{2}{\\mathcal{L}}(\\theta_{t})\\right)+{\\mathcal O}(\\eta^{3/2}\\rho^{3})+{\\mathcal O}(\\kappa\\eta^{3/2}\\rho^{3})+{\\mathcal O}(\\eta}\\\\ &{=\\!\\Phi(\\theta_{t})+{\\mathcal O}\\left(\\eta^{2}\\kappa\\rho^{2}\\right)-\\frac{(\\kappa+1)\\eta\\rho^{2}}{2p}\\partial\\Phi(\\theta_{t})\\nabla\\mathrm{Tr}\\left(\\nabla^{2}{\\mathcal{L}}(\\Phi(\\theta_{t}))\\right)+{\\mathcal O}(\\eta^{3/2}\\rho^{3})+{\\mathcal O}(\\kappa\\eta^{3/2}\\rho^{3})+{\\mathcal O}(\\eta^{2}\\kappa\\rho^{4})\\nabla\\mathrm{Tr}\\left(\\nabla^{2}{\\mathcal{L}}(\\Phi(\\theta_{t}))\\right)}\\\\ &{=\\!\\Phi(\\theta_{t})-(\\kappa+1)\\eta\\rho^{2}\\partial\\Phi(\\theta_{t})\\nabla\\mathrm{Tr}\\left(\\nabla^{2}{\\mathcal{L}}(\\Phi(\\theta_{t}))/2p\\right)+{\\mathcal O}(\\eta^{3/2}\\rho^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "E Proofs in Section 5.2.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Setting E.1. Consider the empirical risk minimization $\\begin{array}{r}{\\operatorname*{min}:\\mathcal{L}(\\pmb{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}_{i}(\\pmb{\\theta})}\\end{array}$ , where ${\\mathcal{L}}_{i}(\\pmb{\\theta})=$ $\\ell(f_{i}(\\pmb\\theta),y_{i})$ is the loss on the $i$ -th data $({\\pmb x}_{i},y_{i})$ . Let $f_{i}(\\cdot)$ and $\\ell(\\cdot,\\cdot)$ be $\\mathcal{C}^{4}$ . Suppose all global minimizers interpolate the training dataset, i.e., $\\begin{array}{r}{\\mathcal{L}(\\theta^{\\star})\\,=\\,\\operatorname*{min}_{\\theta}\\mathcal{L}(\\theta)}\\end{array}$ implies $f_{i}(\\bar{\\theta^{\\star}})\\,=\\,y_{i}$ for all $i\\in[n]$ . We denote the minima manifold by $\\mathcal{M}=\\left\\{\\pmb{\\theta}:f_{i}(\\pmb{\\theta})=y_{i},\\forall i\\in[n]\\right\\}$ . Moreover, we assume that $\\frac{\\partial^{2}\\ell(\\hat{y},y)}{\\partial\\hat{y}^{2}}|_{\\hat{y}=y}>0$ and the feature matrix $(\\nabla f_{i}(\\pmb{\\theta}),\\cdot\\cdot\\cdot\\,,\\nabla f_{n}(\\pmb{\\theta}))\\in\\mathbb{R}^{p\\times n}$ is full-rank at $\\pmb\\theta\\in\\mathcal M$ . ", "page_idx": 31}, {"type": "text", "text": "Lemma E.2 (Theorem 5.2 in Wen et al. (2023a)). Under Setting $E.l$ , Assumption 5.1 holds with $m=n$ . ", "page_idx": 31}, {"type": "text", "text": "E.1 Preliminary Lemmas ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Similar to the proofs for Section 5.2.1, we need the following similar preliminary lemmas. Lemma E.3. Under Setting E.1, for any compact set $K\\in\\Gamma$ , there exist absolute constants $R_{1},\\mu>0$ such that ", "page_idx": 31}, {"type": "text", "text": "\u2022 $(i)\\,{\\overline{{\\mathbb{B}(K;R_{1})}}}\\subset U;$   \n\u2022 $(i i)\\,{\\mathcal{L}}_{i}(\\cdot)\\,(i\\in[n])$ and $\\mathcal{L}(\\cdot)$ are $\\mu$ -PL on $\\overline{{\\mathbb{B}(K;R_{1})}}$ ;   \n$\\bullet\\;(i i i)\\operatorname*{inf}_{\\theta\\in\\mathbb{B}(K;R_{1})}\\lambda_{n}\\left(\\nabla^{2}\\mathcal{L}(\\theta)\\right)\\geqslant\\mu;\\operatorname*{inf}_{\\theta\\in\\mathbb{B}(K;R_{1})}\\lambda_{1}\\left(\\nabla^{2}\\mathcal{L}_{i}(\\theta)\\right)\\geqslant\\mu,\\forall i\\in[n].$ ", "page_idx": 31}, {"type": "text", "text": "We further define the following absolute constants on $\\overline{{\\mathbb{B}(K;R)}}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{2}:=\\left(\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{sup}}\\left\\Vert\\nabla^{2}\\mathcal{L}(\\theta)\\right\\Vert\\right)\\vee\\left(\\underset{i\\in[n]}{\\operatorname*{max}}\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{sup}}\\left\\Vert\\nabla^{2}\\mathcal{L}_{i}(\\theta)\\right\\Vert\\right);}\\\\ &{\\beta_{3}:=\\left(\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{sup}}\\left\\Vert\\nabla^{3}\\mathcal{L}(\\theta)\\right\\Vert\\right)\\vee\\left(\\underset{i\\in[n]}{\\operatorname*{max}}\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{sup}}\\left\\Vert\\nabla^{3}\\mathcal{L}_{i}(\\theta)\\right\\Vert\\right);}\\\\ &{\\nu:=\\left(\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{inf}}\\,\\lambda_{m}\\left(\\nabla^{2}\\mathcal{L}(\\theta)\\right)\\right)\\wedge\\left(\\underset{i\\in[n]}{\\operatorname*{min}}\\,\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{inf}}\\,\\lambda_{1}\\left(\\nabla^{2}\\mathcal{L}_{i}(\\theta)\\right)\\right);}\\\\ &{\\quad\\zeta_{1}^{\\Phi}:=\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{sup}}\\left\\Vert\\nabla\\Phi(\\theta)\\right\\Vert,\\quad\\zeta_{2}^{\\Phi}:=\\underset{\\theta\\in\\mathbb{B}(K;R_{1})}{\\operatorname*{sup}}\\left\\Vert\\nabla^{2}\\Phi(\\theta)\\right\\Vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma E.4 (Wen et al. (2023a)). Under Assumption 5.1, ", "page_idx": 32}, {"type": "text", "text": "\u2022 For any $\\pmb\\theta\\in U$ , $\\partial\\Phi(\\pmb\\theta)\\nabla{\\mathcal L}(\\pmb\\theta)=\\mathbf{0}$ . \u2022 For any $\\theta\\in\\Gamma$ , $\\partial\\Phi(\\pmb\\theta)=P_{n+1:p}(\\nabla^{2}\\mathcal L(\\pmb\\theta))\\,a n d\\,\\partial\\Phi(\\pmb\\theta)\\nabla^{2}\\mathcal L(\\pmb\\theta)=0.$ \u2022 For any $\\theta\\in\\Gamma$ , $\\partial\\Phi(\\pmb\\theta)\\nabla^{2}\\mathcal{L}_{i}(\\pmb\\theta)=0$ , i [n]. ", "page_idx": 32}, {"type": "text", "text": "Lemma E.5. Under Setting E.1, there exists absolute constants $R_{2},\\zeta_{P}\\;>\\;0$ such that for any $\\pmb{\\theta}\\in\\mathbb{B}(K;R_{2})$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|P_{n+1:p}(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}))-P_{n+1:p}(\\nabla^{2}\\mathcal{L}(\\Phi(\\pmb{\\theta})))\\right\\|\\leqslant\\zeta_{P}\\left\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\right\\|.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof Notations. Now we introduce some additional useful notations in the proof in this section. First, we choose $R:=(R_{1}\\land R_{2})/2$ , where $R_{1}$ is defined in Lemma E.3 and $R_{2}$ is defined in Lemma E.5. Let $\\mu$ be the PL constant on $\\mathbb{B}(K;R)$ . Moreover, we use the following notations: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{2}:=\\left(\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\,\\Vert\\nabla^{2}\\mathcal{L}(\\theta)\\Vert\\right)\\vee\\left(\\underset{\\psi\\in[n]}{\\operatorname*{max}}\\,\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\,\\Vert\\nabla^{2}\\mathcal{L}_{4}(\\theta)\\Vert\\right);}\\\\ &{\\beta_{3}:=\\left(\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\,\\Vert\\nabla^{3}\\mathcal{L}(\\theta)\\Vert\\right)\\vee\\left(\\underset{\\psi\\in[n]}{\\operatorname*{max}}\\,\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\,\\Vert\\nabla^{3}\\mathcal{L}_{4}(\\theta)\\Vert\\right);}\\\\ &{\\beta_{4}:=\\left(\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\,\\Vert\\nabla^{3}\\mathcal{L}(\\theta)\\Vert\\right)\\vee\\left(\\underset{\\psi\\in[n]}{\\operatorname*{max}}\\,\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\,\\Vert\\nabla^{4}\\mathcal{L}_{4}(\\theta)\\Vert\\right);}\\\\ &{\\nu:=\\left(\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\,\\lambda_{m}\\left(\\nabla^{2}\\mathcal{L}(\\theta)\\right)\\right)\\wedge\\left(\\underset{\\psi\\in[n]}{\\operatorname*{min}}\\,\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{inf}}\\,\\lambda_{1}\\left(\\nabla^{2}\\mathcal{L}_{4}(\\theta)\\right)\\right);}\\\\ &{\\zeta_{6}:=\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\,\\Vert\\nabla^{2}\\Phi(\\theta)\\Vert;\\quad\\zeta_{P}:=\\underset{\\theta\\in\\mathbb{B}(K;R)}{\\operatorname*{sup}}\\,\\frac{\\Vert\\boldsymbol{P}_{n+1:p}\\left(\\nabla^{2}\\mathcal{L}(\\theta)\\right)-\\zeta_{n+1:p}\\left(\\nabla^{2}\\mathcal{L}(\\phi)\\right)\\Vert}{\\Vert\\theta-\\Phi(\\theta)\\Vert}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Ensured by Lemma D.1 and D.3, these quantities are all absolute constants in $(0,+\\infty)$ . Moreover, without loss of generality, we can assume that $\\beta_{1},\\beta_{2},\\beta_{3},\\zeta_{\\Phi},\\zeta_{P}>1$ and $\\mu\\leqslant\\nu<1$ . ", "page_idx": 32}, {"type": "text", "text": "Then we have the following two lemmas, similar to Lemma D.4 and D.5. ", "page_idx": 32}, {"type": "text", "text": "Lemma E.6. For any $\\pmb{\\theta}\\in\\mathbb{B}(K;R)$ , it holds that: ", "page_idx": 32}, {"type": "text", "text": "\u2022 (para norm v.s. grad norm) $\\begin{array}{r}{\\mu\\left\\|\\nabla{\\mathcal L}(\\pmb\\theta)\\right\\|\\;\\leqslant\\;\\|\\pmb\\theta-\\Phi(\\pmb\\theta)\\|\\;\\leqslant\\;\\beta_{2}\\left\\|\\nabla{\\mathcal L}(\\pmb\\theta)\\right\\|;\\;\\mu\\left\\|\\nabla{\\mathcal L}_{i}(\\pmb\\theta)\\right\\|\\;\\leqslant\\;}\\end{array}$ $\\|\\pmb\\theta-\\Phi(\\pmb\\theta)\\|\\leqslant\\beta_{2}\\,\\|\\nabla\\mathcal L_{i}(\\pmb\\theta)\\|$ , $\\forall i\\in[n]$ .   \n\u2022 (grad norm v.s. loss) $\\begin{array}{r}{\\mathrm{)}\\,2\\mu\\mathcal{L}(\\pmb{\\theta})\\leqslant\\|\\nabla\\mathcal{L}(\\pmb{\\theta})\\|^{2}\\leqslant\\frac{2\\beta_{2}^{2}}{\\mu}\\mathcal{L}(\\pmb{\\theta});\\,2\\mu\\mathcal{L}_{i}(\\pmb{\\theta})\\leqslant\\|\\nabla\\mathcal{L}_{i}(\\pmb{\\theta})\\|^{2}\\leqslant\\frac{2\\beta_{2}^{2}}{\\mu}\\mathcal{L}_{i}(\\pmb{\\theta}),}\\end{array}$ $\\forall i\\in[n]$ .   \n\u2022 (loss v.s. para norm) $\\begin{array}{r}{\\frac{\\mu}{2}\\left\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\right\\|^{2}\\leqslant\\mathcal{L}(\\pmb{\\theta})\\leqslant\\frac{\\beta_{2}^{2}}{2\\mu}\\left\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\right\\|^{2};\\,\\frac{\\mu}{2}\\left\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\right\\|^{2}\\leqslant\\mathcal{L}_{i}(\\pmb{\\theta})\\leqslant\\mathcal{L}_{i}(\\pmb{\\theta})\\leqslant\\mathcal{L}_{i}(\\pmb{\\theta})}\\end{array}$ $\\begin{array}{r}{\\frac{\\beta_{2}^{2}}{2\\mu}\\left\\|\\pmb{\\theta}-\\Phi(\\pmb{\\theta})\\right\\|^{2},\\forall i\\in[n].}\\end{array}$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma E.7. For all $\\pmb{\\theta}\\in\\mathbb{B}(K;R),\\,\\forall i\\in[n],$ , ", "page_idx": 32}, {"type": "text", "text": "$\\begin{array}{r}{\\left\\|P_{n+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla^{2}\\mathcal{L}(\\theta)\\right\\|,\\left\\|P_{n+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla^{2}\\mathcal{L}_{i}(\\theta)\\right\\|,\\left\\|\\partial\\Phi(\\theta)\\nabla^{2}\\mathcal{L}_{i}(\\theta)\\right\\|\\leqslant\\mathcal{O}\\left(\\|\\theta-\\Phi(\\theta)\\|\\right)}\\end{array}$ ; $\\begin{array}{r}{\\left\\|P_{n+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla\\mathcal{L}(\\theta)\\right\\|,\\left\\|P_{n+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla\\mathcal{L}_{i}(\\theta)\\right\\|,\\left\\|\\partial\\Phi(\\theta)\\nabla\\mathcal{L}_{i}(\\theta)\\right\\|\\leqslant\\mathcal{O}\\left(\\left\\|\\theta-\\Phi(\\theta)\\right\\|^{2}\\right);}\\end{array}$ \u2022 Let $\\rho>0$ and $\\pmb{v}\\in\\mathbb{S}^{p-1}$ . If $\\pmb{\\theta}+\\rho\\pmb{v}\\in\\mathbb{B}(K;R)$ , then $\\begin{array}{c}{\\displaystyle\\|\\nabla\\mathcal{L}_{i}(\\theta+\\rho v)\\|\\leqslant\\|\\nabla\\mathcal{L}_{i}(\\theta)\\|+\\rho\\beta_{2},\\forall i\\in[n];}\\\\ {\\displaystyle\\left\\|P_{n+1:p}(\\nabla^{2}\\mathcal{L}(\\theta))\\nabla\\mathcal{L}_{i}(\\theta+\\rho v)\\right\\|\\leqslant\\mathcal{O}\\left(\\|\\theta-\\Phi(\\theta)\\|^{2}\\right)+\\mathcal{O}\\left(\\rho\\left\\|\\theta-\\Phi(\\theta)\\right\\|\\right)+\\frac{\\rho^{2}\\beta_{3}}{2},\\forall i\\in[n];}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Lemma E.8 (Lemma H.9 in Wen et al. (2023a)). For any absolute constant $C\\,>\\,0,$ , there exist absolute constant $C_{1},C_{2}>0$ such that: $i f\\pmb{\\theta}_{t}\\in\\mathbb{B}(K;R)$ and $C_{1}\\eta\\rho\\leqslant\\|\\pmb\\theta_{t}-\\Phi(\\pmb\\theta_{t})\\|\\leqslant C\\rho_{*}$ , then it holds that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{i_{t}}\\left[\\left\\|\\pmb{\\theta}_{t+1/2}-\\Phi(\\pmb{\\theta}_{t+1/2})\\right\\|\\right]\\leqslant\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|-C_{2}\\eta\\rho.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma E.9 (Lemma H.10 in Wen et al. (2023a)). For any absolute constant $C>0,$ , there exists absoulte constant $C_{3}$ such that: if $\\mathbf{\\theta}^{*}\\mathbf{\\theta}^{*}\\in\\mathbb{B}(K;R)$ and $\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|\\leqslant C\\rho$ , then we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|\\left\\|\\pmb{\\theta}_{t+1/2}-\\Phi(\\pmb{\\theta}_{t+1/2})\\right\\|-\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|\\right|\\leqslant C_{3}\\eta\\rho.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now we fix the positive number $C=1$ and use the absolute constants $C_{2},C_{3}$ , defined in Lemma E.8 and Lemma E.9. ", "page_idx": 33}, {"type": "text", "text": "E.2 Proof of Theorem 5.6 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "E.2.1 Proof of Moving Near Minimizers for SAM-IRE ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "This proof is similar to the proof for Section 5.2.1. ", "page_idx": 33}, {"type": "text", "text": "Under the conditions in Theorem 5.6, the update rule of IRE on standard SAM is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t}-\\eta\\nabla\\mathcal{L}_{i_{t}}\\left(\\pmb{\\theta}_{t}+\\rho\\frac{\\nabla\\mathcal{L}_{i_{t}}\\left(\\pmb{\\theta}_{t}\\right)}{\\left\\|\\nabla\\mathcal{L}_{i_{t}}\\left(\\pmb{\\theta}_{t}\\right)\\right\\|}\\right)}\\\\ &{\\qquad\\quad-\\left.\\eta\\kappa P_{n+1:p}\\left(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{t})\\right)\\nabla\\mathcal{L}_{i_{t}}\\left(\\pmb{\\theta}_{t}+\\rho\\frac{\\nabla\\mathcal{L}_{i_{t}}\\left(\\pmb{\\theta}_{t}\\right)}{\\left\\|\\nabla\\mathcal{L}_{i_{t}}\\left(\\pmb{\\theta}_{t}\\right)\\right\\|}\\right),\\mathrm{~where~}i_{t}\\sim\\mathbb{U}([n]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let the $\\kappa$ in IRE satisfy ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\kappa\\leqslant1/\\rho.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Additionally, we fix a constant $\\alpha\\in(0,1)$ in the proof. ", "page_idx": 33}, {"type": "text", "text": "For simplicity, we denote ", "page_idx": 33}, {"type": "equation", "text": "$$\nv_{t}:=\\frac{\\nabla\\mathcal{L}_{i_{t}}(\\pmb{\\theta}_{t})}{\\|\\nabla\\mathcal{L}_{i_{t}}(\\pmb{\\theta}_{t})\\|},\\quad P(\\pmb{\\theta}_{t}):=P_{n+1:p}\\left(\\nabla^{2}\\mathcal{L}(\\pmb{\\theta}_{t})\\right);\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\theta}_{t+1/2}=\\pmb{\\theta}_{t}-\\nabla\\mathcal{L}_{i_{t}}\\left(\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}\\right);}\\\\ &{\\quad\\pmb{\\theta}_{t+1}=\\pmb{\\theta}_{t+1/2}-\\kappa\\eta P(\\pmb{\\theta}_{t})\\nabla\\mathcal{L}_{i_{t}}\\left(\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Additionally, we denote ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t+1,t}:=\\mathbb{P}\\left(\\|\\pmb{\\theta}_{t+1}-\\Phi(\\pmb{\\theta}_{t+1})\\|\\geqslant2C\\eta^{1-\\alpha}\\rho\\Big|\\,\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|<C\\eta^{1-\\alpha}\\rho\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{`}_{t+1,s}:=\\!\\mathbb{P}\\Big(\\|\\theta_{t+1}-\\Phi(\\theta_{t+1})\\|\\geqslant2C\\eta^{1-\\alpha}\\rho;}\\\\ &{\\qquad\\qquad\\qquad\\forall\\tau\\in[s+1,t],C\\eta^{1-\\alpha}\\rho\\leqslant\\|\\theta_{\\tau}-\\Phi(\\theta_{\\tau})\\|<2C\\eta^{1-\\alpha}\\rho\\Big|\\,\\|\\theta_{s}-\\Phi(\\theta_{s})\\|<C\\sqrt{\\eta}\\rho\\Big),\\;s\\in\\mathbb{R}^{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then the following bound holds naturally: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists\\,t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}+T_{\\mathrm{II}}],\\|\\theta_{t}-\\Phi(\\theta_{t})\\|\\geqslant2C\\eta^{1-\\alpha}\\rho\\right)\\leqslant\\sum_{t=T_{\\mathrm{I}}}^{T_{\\mathrm{I}}+T_{\\mathrm{I}}-1}\\left(\\mathbb{P}_{t+1,t}+\\sum_{s=T_{\\mathrm{I}}}^{t-1}\\mathbb{P}_{t+1,s}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "\u2022 Step I. Bounding $\\mathbb{P}_{t+1,t}$ ", "page_idx": 33}, {"type": "text", "text": "From $\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|=\\mathcal{O}(\\eta^{1-\\alpha}\\rho)$ , thus ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|\\leqslant\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|+\\rho=\\mathcal{O}(\\eta^{1-\\alpha}\\rho)+\\mathcal{O}(\\rho)<R,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which means $\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}\\in\\mathbb{B}(K;R)$ . Using Lemma E.7 and $\\kappa\\leqslant1/\\rho$ , we can estimate: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\pmb{\\theta}_{t+1}-\\pmb{\\theta}_{t+1/2}\\right\\|=\\eta\\kappa\\left\\|\\nabla P(\\pmb{\\theta}_{t})\\nabla\\mathcal{L}_{i_{t}}\\left(\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t}\\right)\\right\\|}\\\\ &{\\leqslant\\!\\eta\\kappa\\left(\\left\\|\\nabla P(\\pmb{\\theta}_{t})\\nabla\\mathcal{L}_{i_{t}}(\\pmb{\\theta}_{t})\\right\\|+\\rho\\left\\|\\nabla P(\\pmb{\\theta}_{t})\\nabla^{2}\\mathcal{L}_{i_{t}}(\\pmb{\\theta}_{t})\\right\\|+\\frac{\\beta_{3}}{2}\\rho^{2}\\right)}\\\\ &{\\leqslant\\!\\eta\\kappa\\left(\\eta\\rho^{2}+\\eta^{1-\\alpha}\\rho^{2}+\\frac{\\beta_{3}}{2}\\rho^{2}\\right)\\leqslant\\beta_{3}\\eta\\kappa\\rho^{2}\\leqslant\\frac{C_{2}}{2\\left(1+\\zeta_{1}^{\\Phi}\\right)}\\eta\\rho,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\pmb{\\theta}_{t+1}-\\Phi(\\pmb{\\theta}_{t+1})-(\\pmb{\\theta}_{t+1/2}-\\Phi(\\pmb{\\theta}_{t+1/2}))\\|}\\\\ &{\\leqslant(1+\\zeta_{1}^{\\Phi})\\,\\|\\pmb{\\theta}_{t+1}-\\pmb{\\theta}_{t+1/2}\\|\\leqslant\\displaystyle\\frac{C_{2}}{2}\\eta\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then we have the following bound: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{\\i}|\\theta_{t+1}-\\Phi(\\theta_{t+1})||}\\\\ &{\\leqslant\\|\\theta_{t}-\\Phi(\\theta_{t})\\|+\\left\\|\\theta_{t+1/2}-\\Phi(\\theta_{t+1/2})-(\\theta_{t}-\\Phi(\\theta_{t}))\\right\\|}\\\\ &{\\qquad\\qquad+\\left\\|\\theta_{t+1}-\\Phi(\\theta_{t+1})-(\\theta_{t+1/2}-\\Phi(\\theta_{t+1/2}))\\right\\|}\\\\ &{\\underset{\\mathrm{\\i}\\in\\mathrm{~\\Theta~}\\in\\mathrm{~\\Theta~}}{\\mathrm{Lemma~E.}}\\mathcal{C}_{1}\\eta^{1-\\alpha}\\rho+\\mathcal{O}(\\eta\\rho)+\\left\\|\\theta_{t+1}-\\Phi(\\theta_{t+1})-(\\theta_{t+1/2}-\\Phi(\\theta_{t+1/2}))\\right\\|}\\\\ &{\\qquad\\leqslant\\!\\!C_{1}\\eta^{1-\\alpha}\\rho+\\mathcal{O}(\\eta\\rho)+\\mathcal{O}(\\eta\\rho)<\\frac{3C_{1}}{2}\\eta^{1-\\alpha}\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t+1,t}=\\mathbb{P}\\left(\\|\\pmb{\\theta}_{\\tau+1}-\\Phi(\\pmb{\\theta}_{\\tau+1})\\|\\geqslant2C\\eta^{1-\\alpha}\\rho\\right|\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|<C\\eta^{1-\\alpha}\\rho\\right)=0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "\u2022 Step II. Bounding $\\mathbb{P}_{t+1,s}$ for $s\\in[T_{\\mathrm{I}},t-1]$ . ", "page_idx": 34}, {"type": "text", "text": "We prove this step under the condition $\\|\\pmb{\\theta}_{s}-\\Phi(\\pmb{\\theta}_{s})\\|<C\\eta^{1-\\alpha}\\rho$ . Define a process $\\{X_{\\tau}\\}_{\\tau=s}^{t+1}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\nX_{\\tau+1}=\\left\\{\\!\\!\\begin{array}{l l}{\\lVert\\theta_{\\tau+1}-\\Phi(\\theta_{\\tau+1})\\rVert\\,,\\quad\\mathrm{if}\\;C\\eta^{1-\\alpha}\\rho\\leqslant X_{\\tau}=\\lVert\\theta_{\\tau}-\\Phi(\\theta_{\\tau})\\rVert\\leqslant2C\\eta^{1-\\alpha}\\rho}\\\\ {X_{\\tau}-C_{2}\\eta\\rho/2,\\quad\\mathrm{else}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "It is clear that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t+1,s}\\leqslant\\mathbb{P}\\left(X_{t+1}\\geqslant2C\\eta^{1-\\alpha}\\rho\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then our key step is to prove the following two claims about the process $\\{X_{\\tau}\\}$ . ", "page_idx": 34}, {"type": "text", "text": "\u2013 Claim I. $X_{\\tau}-C_{2}\\tau\\eta\\rho/2$ is a super-martingale. From the definition of $X_{\\tau}$ , we only need to prove that if $\\overline{{C\\eta^{1-\\alpha}\\rho\\leqslant X_{\\tau}=\\|\\pmb\\theta_{\\tau}-\\Phi(\\pmb\\theta_{\\tau})\\|}}\\leqslant2C\\eta^{1-\\alpha}\\rho$ , then $\\mathbb{E}\\left\\|\\pmb{\\theta}_{\\tau+1}-\\Phi(\\pmb{\\theta}_{\\tau+1})\\right\\|\\leqslant$ $\\lVert\\pmb{\\theta}_{\\tau}-\\Phi(\\pmb{\\theta}_{\\tau})\\rVert-C_{2}\\eta\\rho/2$ . If C $\\begin{array}{r}{{\\gamma}\\eta^{1-\\alpha}\\rho\\leqslant X_{\\tau}=\\|\\pmb{\\theta}_{\\tau}-\\Phi(\\pmb{\\theta}_{\\tau})\\|\\leqslant2C\\eta^{1-\\alpha}\\rho}\\end{array}$ , similar to Step I, it holds that $\\theta_{\\tau+1}\\in$ $\\mathbb{B}(K;R)$ and $\\begin{array}{r}{\\left\\|\\pmb{\\theta}_{t+1}-\\Phi(\\pmb{\\theta}_{\\tau+1})-(\\pmb{\\theta}_{\\tau+1/2}-\\Phi(\\pmb{\\theta}_{\\tau+1/2}))\\right\\|\\leqslant\\frac{C_{2}}{2}\\eta\\rho}\\end{array}$ . Moreover, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\pmb{\\theta}_{\\tau+1}-\\Phi(\\pmb{\\theta}_{\\tau+1})\\|}\\\\ &{\\leqslant\\|\\pmb{\\theta}_{\\tau+1/2}-\\Phi(\\pmb{\\theta}_{\\tau+1/2})\\|+\\|\\pmb{\\theta}_{\\tau+1}-\\Phi(\\pmb{\\theta}_{\\tau+1})-(\\pmb{\\theta}_{\\tau+1/2}-\\Phi(\\pmb{\\theta}_{\\tau+1/2}))\\|}\\\\ &{\\leqslant\\|\\pmb{\\theta}_{\\tau+1/2}-\\Phi(\\pmb{\\theta}_{\\tau+1/2})\\|+\\frac{C_{2}}{2}\\eta\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Taking the expectation and using Lemma E.8, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left\\|\\pmb{\\theta}_{\\tau+1}-\\Phi(\\pmb{\\theta}_{\\tau+1})\\right\\|\\leqslant\\mathbb{E}\\left\\|\\pmb{\\theta}_{\\tau+1/2}-\\Phi(\\pmb{\\theta}_{\\tau+1/2})\\right\\|+\\displaystyle\\frac{C_{2}}{2}\\eta\\rho}\\\\ {\\leqslant\\|\\pmb{\\theta}_{\\tau}-\\Phi(\\pmb{\\theta}_{\\tau})\\|-C_{2}\\eta\\rho+\\displaystyle\\frac{C_{2}}{2}\\eta\\rho=\\|\\pmb{\\theta}_{\\tau}-\\Phi(\\pmb{\\theta}_{\\tau})\\|-\\displaystyle\\frac{C_{2}}{2}\\eta\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Claim II. $X_{\\tau+1}-X_{\\tau}+C_{2}\\eta\\rho/2$ is ${\\mathcal{O}}(\\eta\\rho)$ -bounded. From the definition of $X_{\\tau}$ , we only need to prove for the case $C\\eta^{1-\\alpha}\\rho\\leqslant X_{\\tau}=\\|\\pmb{\\theta}_{\\tau}-\\pmb{\\Phi}(\\pmb{\\theta}_{\\tau})\\|\\leqslant2C\\eta^{1-\\alpha}\\rho$ . If $C\\eta^{1-\\alpha}\\rho~\\leqslant~X_{\\tau}~=~\\|\\pmb{\\theta}_{\\tau}-\\Phi(\\pmb{\\theta}_{\\tau})\\|~\\leqslant~2C\\eta^{1-\\alpha}\\rho,$ we have $\\theta_{\\tau+1}~\\in~\\mathbb{B}(K;R)$ and $\\begin{array}{r}{\\left\\lVert\\theta_{t\\pm1_{-}}-\\Phi(\\theta_{t+1})-(\\theta_{t+1/2}-\\Phi(\\theta_{t+1/2}))\\right\\rVert\\ \\leqslant\\ \\frac{C_{2}}{2}\\eta\\rho}\\end{array}$ . Combining this result and Lemma E.9, we have $\\begin{array}{r l}&{\\quad|\\|\\theta_{\\tau+1}-\\Phi(\\theta_{\\tau+1})\\|-\\|\\theta_{\\tau}-\\Phi(\\theta_{\\tau})\\||}\\\\ &{\\leqslant\\left|\\|\\theta_{\\tau+1}-\\Phi(\\theta_{\\tau+1})\\|-\\left\\|\\theta_{\\tau+1/2}-\\Phi(\\theta_{\\tau+1/2})\\right\\|\\right|+\\left|\\|\\theta_{\\tau+1/2}-\\Phi(\\theta_{\\tau+1/2})\\right\\|-\\|\\theta_{\\tau}-\\Phi(\\theta_{\\tau})\\|\\right|}\\\\ &{\\leqslant\\left\\|(\\theta_{\\tau+1}-\\Phi(\\theta_{\\tau+1}))-(\\theta_{\\tau+1/2}-\\Phi(\\theta_{\\tau+1/2}))\\right\\|+\\left|\\|\\theta_{\\tau+1/2}-\\Phi(\\theta_{\\tau+1/2})\\|-\\|\\theta_{\\tau}-\\Phi(\\theta_{\\tau})\\|\\right|}\\\\ &{\\leqslant\\!\\frac{C_{2}}{2}\\eta\\rho+C_{3}\\eta\\rho=\\mathcal{O}(\\eta\\rho).}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "With the preparation of Claim I and Claim II, we can use the Azuma-Hoeffeding inequality: for any $Q>0$ , it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(X_{t+1}-X_{s+1}+(t-s)C_{2}\\eta\\rho/2>Q\\right)\\leqslant2\\exp\\left(-\\frac{Q^{2}}{2(t-s)\\mathcal{O}(\\eta^{2}\\rho^{2})}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As proved in Claim I, $\\begin{array}{r}{X_{s+1}\\;=\\;\\|\\pmb{\\theta}_{s+1}-\\Phi(\\pmb{\\theta}_{s+1})\\|\\;\\leqslant\\;\\frac{3}{2}C\\eta^{1-\\alpha}\\rho}\\end{array}$ due to $\\lVert{\\pmb{\\theta}}_{s}-{\\Phi}({\\pmb{\\theta}}_{s})\\rVert\\,\\leqslant$ $C\\eta^{1-\\alpha}\\rho$ . Therefore, by choosing $Q\\;=\\;(t-s)C_{2}\\eta\\rho/2\\,-\\,\\textstyle{\\frac{3}{2}}C\\eta^{1-\\alpha}\\rho+\\,2C\\eta^{1-\\alpha}\\rho\\,=\\,(t-$ $s)C_{2}\\eta\\rho/2+C\\eta^{1-\\alpha}\\rho/2$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}_{t+1,s}\\leqslant\\mathbb{P}\\left(X_{t+1}\\geqslant2C\\eta^{1-\\alpha}\\rho\\right)}\\\\ &{\\leqslant\\mathbb{P}\\left(X_{t+1}-X_{s+1}+(t-s)C_{2}\\eta\\rho/2>(t-s)\\frac{C_{2}}{2}\\eta\\rho+\\frac{C}{2}\\eta^{1-\\alpha}\\rho\\right)}\\\\ &{\\leqslant2\\exp\\left(-\\frac{\\left((t-s)C_{2}\\eta\\rho/2+C\\eta^{1-\\alpha}\\rho/2\\right)^{2}}{2(t-s)\\mathcal{O}(\\eta^{2}\\rho^{2})}\\right)}\\\\ &{\\leqslant2\\exp\\left(-\\frac{\\left(t-s\\right)C_{2}\\eta\\rho\\cdot C\\eta^{1-\\alpha}\\rho}{4(t-s)\\mathcal{O}(\\eta^{2}\\rho^{2})}\\right)\\leqslant2\\exp\\left(-\\Omega\\left(\\frac{1}{\\eta^{\\alpha}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, we obtain the union bound: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\exists\\,t\\in[T_{1},T_{1}+T_{\\Pi}],\\|\\theta_{t}-\\Phi(\\theta_{t})\\|\\geqslant2C\\eta^{1-\\alpha}\\rho\\right)\\leqslant\\displaystyle\\sum_{t=T_{I}}^{T_{1}+T_{\\Pi}-1}\\left(\\mathbb{P}_{t+1,t}+\\displaystyle\\sum_{s=T_{1}}^{t-1}\\mathbb{P}_{t+1,s}\\right)}\\\\ &{\\leqslant\\displaystyle\\sum_{t=T_{I}}^{T_{1}+T_{\\Pi}-1}\\sum_{s=T_{1}}^{t-1}\\mathbb{P}_{t+1,s}\\leqslant T_{\\Pi}^{2}\\exp\\left(-\\Omega\\left(1/\\eta^{\\alpha}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "E.2.2 Proof of the Effective Dynamics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "By our proof above, with probability at least $1-T_{\\mathrm{II}}^{2}\\exp\\left(-\\Omega\\left(1/\\eta^{\\alpha}\\right)\\right)$ , for any $t\\in[T_{\\mathrm{I}},T_{\\mathrm{I}}+T_{\\mathrm{II}}]$ , $\\|\\dot{\\pmb{\\theta}}_{t}-\\bar{\\Phi(\\pmb{\\theta}_{t})}\\|=\\mathcal{O}(\\eta^{1-\\alpha}\\bar{\\rho)}$ . Then we prove this theorem when the above event occurs. ", "page_idx": 35}, {"type": "text", "text": "Due to $\\|\\pmb{\\theta}_{t}-\\Phi(\\pmb{\\theta}_{t})\\|=\\mathcal{O}(\\eta^{1-\\alpha}\\rho)$ , we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Vert\\theta_{t+1}-\\theta_{t}\\Vert}}\\\\ &{}&{\\leqslant\\eta\\,\\Vert\\nabla{\\mathcal{L}}_{i_{t}}(\\theta_{t}+\\rho v_{t})\\Vert+\\eta\\kappa\\,\\Vert P(\\theta_{t})\\nabla{\\mathcal{L}}_{i_{t}}(\\theta_{t}+\\rho v_{t})\\Vert}\\\\ &{}&{\\stackrel{\\mathrm{Lemma~E.7}}{\\leqslant}\\eta\\,\\Vert\\nabla{\\mathcal{L}}_{i_{t}}(\\theta_{t})\\Vert+{\\mathcal{O}}(\\eta\\rho)+{\\mathcal{O}}(\\eta\\kappa\\rho^{2})\\leqslant{\\mathcal{O}}(\\eta\\rho).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then by Taylor\u2019s expansion, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Phi(\\theta_{t+1})-\\Phi(\\theta_{t})=\\partial\\Phi(\\theta_{t})(\\theta_{t+1}-\\theta_{t})+\\mathcal{O}\\left(\\left\\Vert\\theta_{t+1}-\\theta_{t}\\right\\Vert^{2}\\right)}\\\\ &{=-\\eta\\partial\\Phi(\\theta_{t})\\nabla\\mathcal{L}_{i_{t}}(\\theta_{t}+\\rho v_{t})-\\eta\\kappa\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\mathcal{L}_{i_{t}}(\\theta_{t}+\\rho v_{t})+\\mathcal{O}(\\eta^{2}\\rho^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For the term $\\partial\\Phi(\\pmb{\\theta}_{t})\\nabla\\mathcal{L}_{i_{t}}(\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t})$ and $\\partial\\Phi(\\pmb{\\theta}_{t})P(\\pmb{\\theta}_{t})\\nabla\\mathcal{L}_{i_{t}}(\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t})$ , using Taylor\u2019s expansion and Lemma E.7, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\partial\\Phi(\\pmb{\\theta}_{t})\\nabla\\mathcal{L}_{i_{t}}(\\pmb{\\theta}_{t}+\\rho\\pmb{v}_{t})\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!=\\!\\partial\\Phi(\\theta_{t})\\nabla\\mathcal{L}_{i_{t}}(\\theta_{t})+\\rho\\partial\\Phi(\\theta_{t})\\nabla^{2}\\mathcal{L}_{i_{t}}(\\theta_{t})v_{t}+\\frac{\\rho^{2}}{2}\\partial\\Phi(\\theta_{t})\\nabla\\mathrm{Tr}\\left(v_{t}\\nabla^{2}\\mathcal{L}_{i_{t}}(\\theta_{t})v_{t}^{\\top}\\right)+\\mathcal{O}(\\rho^{3})}\\\\ &{\\!=\\!\\partial\\Phi(\\theta_{t})\\nabla\\mathcal{L}_{i_{t}}(\\theta_{t})+\\rho\\partial\\Phi(\\theta_{t})\\nabla^{2}\\mathcal{L}_{i_{t}}(\\theta_{t})v_{t}+\\frac{\\rho^{2}}{2}\\partial\\Phi(\\theta_{t})\\nabla\\left(v_{t}^{\\top}\\nabla^{2}\\mathcal{L}_{i_{t}}(\\theta_{t})v_{t}\\right)+\\mathcal{O}(\\rho^{3})}\\\\ &{\\!=\\!\\mathcal{O}(\\|\\theta_{t}-\\Phi(\\theta_{t})\\|^{2})+\\rho\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla^{2}\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))\\frac{\\nabla\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))}{\\left\\|\\nabla\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))\\right\\|}+\\mathcal{O}(\\rho\\|\\theta_{t}-\\Phi(\\theta_{t})\\|)}\\\\ &{~~~\\!+\\frac{\\rho^{2}}{2}\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla\\left(\\frac{\\nabla\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))}{\\left\\|\\nabla\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))\\right\\|}^{\\top}\\nabla^{2}\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))\\frac{\\nabla\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))}{\\left\\|\\nabla\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))\\right\\|}\\right)}\\\\ &{~~~\\!+\\mathcal{O}(\\rho^{2}\\|\\theta_{t}-\\Phi(\\theta_{t})\\|)+\\mathcal{O}(\\rho^{3})}\\\\ &{\\!=\\!\\frac{\\rho^{2}}{2}\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla\\lambda_{1}\\left(\\nabla^{2}\\mathcal{L}_\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla C_{i_{t}}(\\theta_{t}+\\rho v_{t})}\\\\ &{=\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla C_{i_{t}}(\\theta_{t})+\\rho\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla^{2}\\mathcal{L}_{i_{t}}(\\theta_{t})v_{t}}\\\\ &{\\quad+\\frac{\\rho^{2}}{2}\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\mathrm{Tr}\\left(v_{t}\\nabla^{2}\\mathcal{L}_{i_{t}}(\\theta_{t})v_{t}^{\\top}\\right)+O(\\rho^{3})}\\\\ &{=\\!O(\\|\\theta_{t}-\\Phi(\\theta_{t})\\|^{2})+O(\\rho\\|\\theta_{t}-\\Phi(\\theta_{t})\\|)+\\frac{\\rho^{2}}{2}\\partial\\Phi(\\theta_{t})P(\\theta_{t})\\nabla\\left(v_{t}^{\\top}\\nabla^{2}\\mathcal{L}_{i_{t}}(\\theta_{t})v_{t}\\right)+O(\\rho^{3})}\\\\ &{=\\!O(\\eta^{1-\\alpha}\\rho^{2})+\\frac{\\rho^{2}}{2}\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla\\left(\\frac{\\nabla\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))}{\\|\\nabla\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))\\|}^{\\top}\\nabla^{2}\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))\\frac{\\nabla C_{i_{t}}(\\Phi(\\theta_{t}))}{\\|\\nabla Z_{i_{t}}(\\Phi(\\theta_{t}))\\|}\\right)}\\\\ &{\\quad+\\ O(\\rho^{2}\\|\\theta_{t}-\\Phi(\\theta_{t})\\|)+O(\\rho^{3})}\\\\ &{=\\frac{\\rho^{2}}{2}\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla\\lambda_{1}\\left(\\nabla^{2}\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))\\right)+O(\\eta^{1-\\alpha}\\rho^{2}+\\rho^{3}),.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining the results above, we obtain: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Phi(\\theta_{t+1})=\\Phi(\\theta_{t})-(1+\\kappa)\\frac{\\eta\\rho^{2}}{2}\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla\\lambda_{1}\\left(\\nabla^{2}\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))\\right)+\\mathcal{O}(\\kappa\\eta^{2-\\alpha}\\rho^{2}+\\kappa\\eta\\rho^{3}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Additionally, taking the expectation, we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}_{i_{t}}\\left[\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla\\lambda_{1}\\left(\\nabla^{2}\\mathcal{L}_{i_{t}}(\\Phi(\\theta_{t}))\\right)\\right]=\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla\\operatorname{Tr}\\left(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta_{t}))\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{i_{t}}\\left[\\Phi(\\theta_{t+1})\\right]=\\Phi(\\theta_{t})-(1+\\kappa)\\frac{\\eta\\rho^{2}}{2}\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla\\,\\mathrm{Tr}\\left(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta_{t}))\\right)+\\mathcal{O}(\\kappa\\eta^{2-\\alpha}\\rho^{2}+\\kappa\\eta\\rho^{3})}\\\\ &{\\qquad\\qquad\\qquad=\\Phi(\\theta_{t})-(1+\\kappa)\\frac{\\eta\\rho^{2}}{2}\\partial\\Phi(\\Phi(\\theta_{t}))\\nabla\\,\\mathrm{Tr}\\left(\\nabla^{2}\\mathcal{L}(\\Phi(\\theta_{t}))\\right)+h.o.t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "F Useful Inequalities ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Definition F.1 ( $\\mu$ -PL). Let $\\mu>0$ be a constant. A function $\\mathcal{L}$ is $\\mu$ -PL in a set $U$ iff $\\left\\|\\nabla{\\mathcal{L}}(\\pmb{\\theta})\\right\\|^{2}\\geqslant$ $2\\mu({\\mathcal{L}}(\\pmb{\\theta})-\\operatorname*{inf}_{\\pmb{\\theta}\\in U}{\\mathcal{L}}(\\pmb{\\theta})),\\forall\\pmb{\\theta}\\in U$ . ", "page_idx": 36}, {"type": "text", "text": "Lemma F.2 (Weyl Theorem). Let $\\pmb{A},\\pmb{B}\\in\\mathbb{R}^{p\\times p}$ be symmetric with eigenvalues $\\lambda_{1}\\geqslant\\cdot\\cdot\\geqslant\\lambda_{p}$ and $\\mu_{1}\\geqslant\\dots\\geqslant\\mu_{p}$ respectively, then for any $k\\in[p]$ , it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|\\lambda_{k}-\\mu_{k}\\right|\\leqslant\\left\\|A-B\\right\\|.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma F.3 (Davis-Kahan $\\mathrm{sin}(\\Theta)$ theorem). Let $A,B\\in\\mathbb{R}^{p\\times p}$ be symmetric matrices. Denote their orthogonal decomposition as $\\pmb{A}=E_{1}\\pmb{\\Lambda}_{1}\\pmb{E}_{1}^{\\top}+E_{2}\\pmb{\\Lambda}_{2}\\pmb{E}_{2}^{\\top}$ and $\\dot{B}\\,=\\,F_{1}\\Gamma_{1}F_{1}^{\\top}+F_{2}\\Gamma_{2}F_{2}^{\\top}$ with $(E_{1},E_{2})$ and $(D_{1},D_{2})$ orthogonal. If the eigenvalues in $\\pmb{\\Lambda}_{1}$ are contained in an interval $(a,b)$ , and the eigenvalues of $\\mathbf{\\Gamma}_{2}$ are excluded from the interval $(a-\\delta,b+\\delta)$ for some $\\delta>0$ , then for any unitarily invariant norm $\\left\\|\\cdot\\right\\|_{\\star}$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\|F_{2}^{\\top}E_{1}\\right\\|_{\\star}\\leqslant\\frac{\\left\\|F_{2}^{\\top}(A-B)E_{1}\\right\\|_{\\star}}{\\delta}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma F.4 (Azuma-Hoeffding Inequality). Suppose $\\{X_{n}\\}_{n\\in\\mathbb{N}}$ is a super-martingale. ", "page_idx": 37}, {"type": "text", "text": "\u2022 (i) (Bounded martingale difference). $I f-\\alpha\\leqslant X_{i+1}-X_{i}\\leqslant\\beta$ , then for any $n,t>0,$ , we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(X_{n}-X_{0}\\geqslant t\\right)\\leqslant2\\exp\\left(-\\frac{t^{2}}{2n(\\alpha+\\beta)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "\u2022 (ii) (Sub-Gaussian martingale difference). If $X_{i+1}-X_{i}$ is $\\sigma_{i}^{2}$ -sub-Gaussian, then for any $n,t>0$ , we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(X_{n}-X_{0}\\geqslant t\\right)\\leqslant2\\exp\\left(-\\frac{t^{2}}{2\\sum_{i=1}^{n}\\sigma_{i}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma F.5. Let $\\pmb{v}\\in\\mathbb{R}^{p}$ . Let $\\pmb{g}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ . Then there exists an absolute constant $c>0$ such that for any $t>0$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\left\\langle{\\frac{v}{\\|v\\|}},{\\frac{g}{\\|g\\|}}\\right\\rangle\\right|\\geqslant t\\right)\\leqslant4e^{-c p t^{2}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma F.5. From P54 in Vershynin (2018), there exists an absolute constant $c>0$ such that for any $t>0$ , $\\begin{array}{r}{\\mathbb{P}\\left(\\frac{|\\langle e_{1},\\pmb{g}\\rangle|}{\\|\\pmb{g}\\|}\\geqslant t\\right)\\leqslant4e^{-c p t^{2}}}\\end{array}$ . Without loss of generality, we can assume $\\pmb{v}\\neq\\mathbf{0}$ . Then we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\left\\langle\\frac{v}{\\|v\\|},\\frac{g}{\\|g\\|}\\right\\rangle\\right|\\geqslant\\frac{t}{\\sqrt{p}}\\right)=\\mathbb{P}\\left(\\left|\\frac{\\left\\langle e_{1},g\\right\\rangle}{\\|g\\|}\\right|\\geqslant t\\right)\\leqslant4e^{-c p t^{2}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma F.6. $\\left\\|A B\\right\\|_{\\mathrm{F}}\\leqslant\\left\\|A\\right\\|\\left\\|B\\right\\|_{\\mathrm{F}}$ . ", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We believe that the abstract and introduction reflect the contributions and scope of the paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: In Section 6. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: In Section 2 and 5; Appendix B, D, E, and F. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We believe that all of the experimental results are reproducible in our work. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: In https://github.com/wmz9/IRE-algorithm-framework. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: In Section 4 and Appendix C. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: In Appendix C. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: In Appendix C. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We have confirmed that the research is conducted with the NeurIPS Code of Ethics. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 42}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]