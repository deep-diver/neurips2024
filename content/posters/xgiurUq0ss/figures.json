[{"figure_path": "xgiurUq0ss/figures/figures_1_1.jpg", "caption": "Figure 1: The perplexity scores of different methods across different domains for different methods (See Section 4 for more details.). Note that \"Chinese CC\" denotes \"Chinese CommonCrawl\".", "description": "This figure shows the perplexity (PPL) scores for different LLMs across various domains.  The LLMs tested include a baseline student model, and then that same model after three different types of training:  Continual Pretraining (CPT), Knowledge Distillation (KD), and the authors' proposed method, Distilling Domain Knowledge (DDK).  The teacher model's PPL is shown for comparison. The graph visually demonstrates how DDK improves the student model's performance, especially in domains where it initially lagged behind the teacher model. The domains are categorized into general-purpose text data (Common Crawl, C4, The Stack, Wikipedia, Books, ArXiv, StackExchange) and Chinese-language data (Chinese Books, Chinese Common Crawl).", "section": "4 Experiments"}, {"figure_path": "xgiurUq0ss/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the distillation process of DDK. First, the training dataset is divided into distinct domains based on predefined criteria. Then, DDK dynamically modulates the distribution of domain-specific data, augmenting the amount allocated to domains where the student model struggles the most. The proportions attributed to each domain are recalculated at distillation intervals by employing a factor smooth updating approach.", "description": "This figure illustrates the DDK framework's distillation process. It begins by dividing the training dataset into distinct domains.  DDK then dynamically adjusts the distribution of domain-specific data, focusing more on domains where the student model underperforms.  The proportions allocated to each domain are recalculated periodically using a factor smooth updating approach, ensuring a stable and robust distillation process.", "section": "3 Methodology"}, {"figure_path": "xgiurUq0ss/figures/figures_6_1.jpg", "caption": "Figure 3: (a). Effect of distillation interval. (b). Effect of the number of training tokens.", "description": "This figure shows the ablation study results on the effects of two hyperparameters in the DDK framework: distillation interval and the number of training tokens.  The left subfigure (a) illustrates how varying the distillation interval affects the accuracy on three benchmark datasets (MMLU, Arc-C, and RACE).  The right subfigure (b) displays how changing the number of training tokens impacts the same three datasets. The results demonstrate the optimal ranges for both hyperparameters, which contribute to the overall effectiveness of the DDK method in improving the performance of student LLMs.", "section": "4.3 Ablation Study"}, {"figure_path": "xgiurUq0ss/figures/figures_6_2.jpg", "caption": "Figure 4: Effect of data sampling strategies.", "description": "This figure compares the performance of three different data sampling strategies used in the DDK framework on three benchmark datasets: MMLU, RACE, and Arc-C.  The strategies compared are the full DDK method, DDK without factor smoothing (w/o FS), and DDK with equal sampling (ES). The chart shows that the standard DDK approach achieves superior accuracy across all three datasets, indicating the value of the dynamic data sampling and factor smoothing in improving the effectiveness of the knowledge distillation.", "section": "4.3 Ablation Study"}, {"figure_path": "xgiurUq0ss/figures/figures_8_1.jpg", "caption": "Figure 3: (a). Effect of distillation interval. (b). Effect of the number of training tokens.", "description": "This figure shows the effects of two hyperparameters on the performance of the DDK model.  The left graph (a) illustrates how changing the distillation interval (how often the domain discrepancy factor is recalculated) impacts the model's performance across three domains. The right graph (b) shows how the number of training tokens affects model performance. These ablation studies help to demonstrate the robustness and optimal parameter settings for the DDK model.", "section": "4.3 Ablation Study"}]