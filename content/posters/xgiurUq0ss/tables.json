[{"figure_path": "xgiurUq0ss/tables/tables_4_1.jpg", "caption": "Table 1: Results of different methods on the Qwen-1.5 models. Note that we use Qwen-1.5 14B and Qwen-1.5 1.8B as teacher and student models, respectively. \u201cW.G.\u201d, \u201cC.QA\u201d and \u201cH.E.\u201d denote Winogrande, CommonsenseQA and Humeneval datasets, respectively.", "description": "This table presents a comparison of different knowledge distillation methods on the Qwen-1.5 large language model.  It uses Qwen-1.5 14B as the teacher model and Qwen-1.5 1.8B as the student model.  The table shows the performance of each method across several benchmark datasets, including evaluation metrics such as perplexity and accuracy.  The methods compared include the baseline student model, models using continuous pre-training (CPT), traditional knowledge distillation (KD), task-aware distillation (TED), MiniLLM, and the proposed DDK method.", "section": "4 Experiments"}, {"figure_path": "xgiurUq0ss/tables/tables_5_1.jpg", "caption": "Table 1: Results of different methods on the Qwen-1.5 models. Note that we use Qwen-1.5 14B and Qwen-1.5 1.8B as teacher and student models, respectively. \u201cW.G.\u201d, \u201cC.QA\u201d and \u201cH.E.\u201d denote Winogrande, CommonsenseQA and Humeneval datasets, respectively.", "description": "This table presents the performance comparison of different methods on the Qwen-1.5 large language models.  It shows the results obtained using various techniques, including the baseline student model (Qwen-1.5 1.8B),  continuous pre-training (+CPT), standard knowledge distillation (+KD), task-aware distillation (+TED), MiniLLM, and the proposed DDK method. The evaluation is performed across multiple benchmark datasets, encompassing evaluation metrics for various tasks like common sense reasoning,  multiple-choice questions, and code generation.", "section": "4 Experiments"}, {"figure_path": "xgiurUq0ss/tables/tables_5_2.jpg", "caption": "Table 2: Results of different methods on the LLaMA models. Note that we use LLAMA2 13B and TinyLLaMA 1.1B as teacher and student models, respectively.", "description": "This table presents the results of different knowledge distillation methods applied to LLaMA models.  It compares the performance of a smaller student model (TinyLLaMA 1.1B) against a larger teacher model (LLaMA2 13B) across various evaluation metrics (CEval, MMLU, RACE, C3, W.G., GSM8K, COSE-QA, Arc-E, Arc-C, H.E., MBPP) after different knowledge distillation techniques were applied. The methods compared include:  Continuously pre-trained baseline (CPT), standard Knowledge Distillation (KD), Task-aware filter-based knowledge distillation (TED), MiniLLM, and the proposed Distill Domain Knowledge (DDK) method.  The average performance across all metrics is also included for each method.", "section": "4 Experiments"}, {"figure_path": "xgiurUq0ss/tables/tables_7_1.jpg", "caption": "Table 3: Results of different methods on the Qwen-1.5 models. Note that we use Qwen-1.5 14B and Qwen-1.5 4B as teacher and student models, respectively. ", "description": "This table presents the results of different knowledge distillation methods on the Qwen-1.5 language models.  It compares the performance of a smaller student model (Qwen-1.5 4B) trained using different techniques against a larger teacher model (Qwen-1.5 14B).  The methods compared include: Continuous Pretraining (CPT), Knowledge Distillation (KD), Task-aware Enhanced Distillation (TED), MiniLLM, and the proposed method, Distilling Domain Knowledge (DDK). The evaluation metrics used are several benchmark datasets (CEval, MMLU, RACE, C3, Winogrande, GSM8K, CommonsenseQA, Arc-E, Arc-C, HumanEval, and MBPP), providing a comprehensive assessment of performance across various tasks and domains.", "section": "4.4 Further Analysis"}, {"figure_path": "xgiurUq0ss/tables/tables_7_2.jpg", "caption": "Table 1: Results of different methods on the Qwen-1.5 models. Note that we use Qwen-1.5 14B and Qwen-1.5 1.8B as teacher and student models, respectively. \u201cW.G.\u201d, \u201cC.QA\u201d and \u201cH.E.\u201d denote Winogrande, CommonsenseQA and Humeneval datasets, respectively.", "description": "This table presents the results of different knowledge distillation methods on the Qwen-1.5 language models.  It compares the performance of a larger teacher model (Qwen-1.5 14B) to a smaller student model (Qwen-1.5 1.8B) across various downstream tasks. The tasks include several benchmark evaluations measuring common sense reasoning, knowledge, and language understanding. Different methods like Continuous Pre-training (CPT), standard Knowledge Distillation (KD), Task-aware Embedding Distillation (TED), MiniLLM, and the proposed Distilling Domain Knowledge (DDK) are compared. The table displays the performance of each method on each task, providing a quantitative assessment of their effectiveness in improving the student model's performance.", "section": "4 Experiments"}, {"figure_path": "xgiurUq0ss/tables/tables_8_1.jpg", "caption": "Table 5: Results of different methods on the StarCoder models. Note that we use StarCoder 15.5B and StarCoder 3B as teacher and student models, respectively.", "description": "This table presents the results of different knowledge distillation methods on the StarCoder large language models.  It compares the performance of a smaller student model (StarCoder 3B) after distillation, using a larger teacher model (StarCoder 15.5B) as a source of knowledge. The methods compared include: using the teacher model directly, continued pre-training (CPT), standard knowledge distillation (KD), and the proposed DDK method.  Performance is measured on the EM (exact match) and ES (exact set) metrics across four programming languages (Python, Java, TypeScript, and C#), with an average score calculated across all languages.  DDK aims to improve upon the performance of previous techniques by dynamically focusing on domain areas where the student model is underperforming.", "section": "4 Experiments"}, {"figure_path": "xgiurUq0ss/tables/tables_8_2.jpg", "caption": "Table 6: Few-shot (5-shot) performance results of different methods on the Qwen-1.5 models. Note that we use Qwen-1.5 14B and Qwen-1.5 1.8B as teacher and student models, respectively.", "description": "This table presents the few-shot (5-shot) performance results on several benchmark datasets using different knowledge distillation methods.  It compares the performance of a small Qwen-1.5 1.8B model (student) against the performance of a larger Qwen-1.5 14B model (teacher) across five datasets (CEval, MMLU, GSM8K, Arc-E, and Arc-C). The methods compared include the baseline student model, continuous pre-training (CPT), traditional knowledge distillation (KD), and the proposed DDK method.  The average performance across all datasets is also reported.  This data helps to illustrate the effectiveness of DDK in improving the few-shot learning capabilities of a smaller language model through knowledge distillation.", "section": "4.2 Main Results"}, {"figure_path": "xgiurUq0ss/tables/tables_14_1.jpg", "caption": "Table 7: Training TFLOPs on all data of different methods for Qwen-1.5. For KD and DDK, we use the Qwen-1.5 14B to distill the Qwen-1.5 1.8B.", "description": "This table shows the training computational costs (measured in TFLOPs) for three different methods: CPT (Continued Pre-training), KD (Knowledge Distillation), and DDK (Distilling Domain Knowledge).  It compares the costs when using the Qwen-1.5 14B model as a teacher model to distill to the Qwen-1.5 1.8B student model for KD and DDK.  CPT represents continuing the pre-training of the student model without knowledge distillation.", "section": "B.2 Details on the training costs"}]