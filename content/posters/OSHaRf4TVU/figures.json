[{"figure_path": "OSHaRf4TVU/figures/figures_1_1.jpg", "caption": "Figure 1: Task Spaces in RL Generalization. Meta-RL agents adapt to dense variations of a core task. Multi-Task RL overcomes optimization challenges of learning from isolated tasks. Scalable ideas from both areas allow us to extend adaptive agents towards increasingly general behavior.", "description": "This figure illustrates the progression from Meta-RL, which focuses on adapting to variations of a single task, to Multi-Task RL, which tackles the optimization difficulties of handling multiple tasks simultaneously.  Finally, it shows how combining ideas from both Meta-RL and Multi-Task RL leads to the creation of a generalist agent capable of adapting to and mastering an increasingly diverse set of tasks.", "section": "1 Introduction"}, {"figure_path": "OSHaRf4TVU/figures/figures_3_1.jpg", "caption": "Figure 2: Transformer-based Actor-Critic Architecture", "description": "This figure illustrates the architecture of the Transformer-based actor-critic model used in the paper.  The bottom shows a timestep encoder that takes in observations, actions, rewards, resets, and timestep information to produce a timestep representation. These representations are then processed by a transformer to produce a latent representation (hj).  This latent representation is then used by both the actor and the critic. The actor uses the latent representation to output an action distribution, while the critic uses it to output a value distribution.  The figure highlights the flow of information through the model and shows the relationship between the different components.", "section": "Multi-Task Adaptation Without Task Labels"}, {"figure_path": "OSHaRf4TVU/figures/figures_4_1.jpg", "caption": "Figure 3: Scale-Resistant Value Regression. We plot the value of the standard critic loss (Eq. 1) as a function of the relative prediction error of the TD target (y) across four orders of magnitude (left). Y-axes are self-normalized according to the largest displayed value. Two-Hot classification (Eq. 4) maps the same relative error to similar loss values across the different absolute return scales of each task (right).", "description": "This figure compares the standard critic loss (mean squared error) with a proposed scale-resistant critic loss (symlog two-hot classification). The left plot shows that the standard critic loss is highly sensitive to the scale of the target values (y), while the right plot shows that the proposed loss is much less sensitive to the scale of y. This is because the proposed loss converts the scalar Q-values to discrete labels, which decouples the optimization from the scale of rewards. The figure demonstrates the effectiveness of the proposed approach in handling tasks with widely varying return scales.", "section": "Scale-Resistant Critics"}, {"figure_path": "OSHaRf4TVU/figures/figures_6_1.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure displays the training results of four different learning update combinations on the Meta-World ML45 benchmark.  The left panel shows the cumulative number of tasks where the agent achieved a success rate of at least 2/3. The center panel presents the average success rate across all tasks and variants over three episodes.  The right panel illustrates the average total return obtained over three episodes. The results are compared against the performance of MuZero and RL2-PPO from previous studies [17, 73]. Error bars in each panel represent the range observed across three independent trials.", "section": "Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_6_2.jpg", "caption": "Figure 5: Multi-Task POPGym Results. Returns are normalized by single-task experts trained for 15M timesteps. Error bars indicate the maximum and minimum returns across three trials.", "description": "This figure shows the results of the multi-task POPGym experiment.  The performance of different training methods (Ind. Actor, Ind. Critic; Dep. Actor, Dep. Critic; and PPO-GRU) are compared. The y-axis represents the average return, normalized by the best single-task performance, showing the relative improvement of the multi-task approaches.  The x-axis shows the total training timesteps across all 27 tasks. Error bars represent the variability across three independent trials of each method.", "section": "Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_7_1.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure presents the training results of four different learning update strategies on the Meta-World ML45 benchmark.  The left panel shows the coverage of the 45 tasks, indicating the number of tasks successfully completed (success rate \u2265 2/3). The center panel displays the average success rate across all tasks and variants, over three episodes. The right panel illustrates the average total return obtained across all tasks and their variants over three episodes.  Results from MuZero and RL2-PPO (baseline methods) are included for comparison. Error bars represent the range between the best and worst performance across three independent trials.", "section": "Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_7_2.jpg", "caption": "Figure 7: Learning from Rescaled Returns. (Left) Procgen test level returns normalized across games according to the standard benchmark scale [28]. (Center) Frequencies of the critic classification label index with the highest probability throughout training. We trim the y-axis to the portion of symlog [31] space that is relevant to Procgen. (Right) The value of LActor-Ind binary advantage weights create an estimate of the percentage of the replay buffer currently being imitated.", "description": "This figure shows the results of experiments on the Procgen environment with rescaled rewards. The left panel displays the normalized returns on unseen test levels, comparing the performance of different learning updates. The center panel illustrates the distribution of critic label indices (representing value estimates) during training, highlighting the effect of symlog transformation on return scales. The right panel presents the percentage of replay buffer actions approved for imitation learning by the scale-resistant actor update (LActor-Ind).", "section": "Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_8_1.jpg", "caption": "Figure 8: Multi-Task Procgen in Memory-Hard Mode. We measure policy performance across the two episodes of its adaptation window. Results are averaged over 30M frames in unseen test levels.", "description": "This figure displays the results of a multi-task experiment on the Procgen environment in \"Memory-Hard Mode\".  The experiment evaluates the performance of a policy trained to adapt to multiple tasks over two episodes in unseen levels.  The results are averaged over 30 million frames. The bars represent the normalized score (relative to the \"Hard Mode\" performance), separated for the first and second episodes.  The percentage change from Hard Mode is indicated above each bar.  The vertical dashed line separates \"Memory Mode\" games from \"Hard Mode\" games.", "section": "3 Multi-Task Adaptation Without Task Labels"}, {"figure_path": "OSHaRf4TVU/figures/figures_8_2.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure displays the training results for the Meta-World ML45 benchmark.  It shows the performance of four different training methods (combinations of dependent and independent actor and critic loss functions) across three key metrics:\n\n1.  **Coverage of Skills:** The percentage of the 45 manipulation tasks where the agent achieved a success rate of at least 2/3 within three episodes.\n2.  **Average Success Rate:** The average success rate across all tasks and their variations over three episodes.\n3.  **Average Total Return:** The total accumulated reward over three episodes averaged across all tasks and variations.\n\nThe results are shown as learning curves, plotting the metrics against total timesteps.  Reference scores from MuZero and RL2-PPO (previous state-of-the-art methods) are included for comparison.", "section": "Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_9_1.jpg", "caption": "Figure 10: Multli-Task BabyAI. Results are the average over four training seeds and plotted according to the median, mean, and interquartile range over the task set.", "description": "This figure shows the results of the multi-task BabyAI experiment. The x-axis represents the total training timesteps, and the y-axis represents the average total return per 2-episode rollout.  The figure presents data for both 50 train tasks with test variants and 18 test tasks, comparing two different learning updates: one that is dependent on return scales, and one that is independent of return scales.  Error bars show the interquartile range.  The results demonstrate that the learning update independent of return scales performs better on both training and test tasks.", "section": "Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_9_2.jpg", "caption": "Figure 11: In-Context BabyAI. We measure the average return of \u201cInd. / Ind.\u201d agents by attempt in unseen tasks and/or layouts (variants). Results are arbitrarily sorted in order of increasing second-episode return and are the average of four training seeds and 10M evaluation timesteps.", "description": "This figure shows the performance of the \"Ind. / Ind.\" agents (using scale-invariant actor and critic updates) in the BabyAI environment.  It specifically focuses on the in-context learning performance across two episodes. The x-axis displays different tasks, categorized as training tasks (with test variants) and test tasks.  The y-axis represents the episodic return.  Grey points show the return in the first episode, while purple points show the return in the second episode. The figure illustrates how the agents improve their performance over multiple episodes (in-context learning) on both training and held-out test tasks, showcasing the efficacy of the scale-invariant learning approach. The vertical dashed line separates training tasks from test tasks.", "section": "Multi-Task Adaptation Without Task Labels"}, {"figure_path": "OSHaRf4TVU/figures/figures_18_1.jpg", "caption": "Figure 3: Scale-Resistant Value Regression. We plot the value of the standard critic loss (Eq. 1) as a function of the relative prediction error of the TD target (y) across four orders of magnitude (left). Y-axes are self-normalized according to the largest displayed value. Two-Hot classification (Eq. 4) maps the same relative error to similar loss values across the different absolute return scales of each task (right).", "description": "This figure compares the performance of standard mean squared error (MSE) loss versus a two-hot classification loss in the context of multi-task reinforcement learning. The left panel shows how MSE loss is sensitive to the scale of the target values (TD target, y).  Tasks with higher absolute return values will disproportionately influence the loss function. In contrast, the right panel shows how a two-hot classification loss maps similar relative prediction errors to similar loss values regardless of the absolute scale of the targets. This makes the two-hot classification approach more robust to the problem of imbalanced training losses caused by uneven return scales across tasks.", "section": "Scale-Resistant Critics"}, {"figure_path": "OSHaRf4TVU/figures/figures_19_1.jpg", "caption": "Figure 13: Bin Counts in POPGym. We compare two label space sizes (B) with the same Rlow and Rhigh limits on two single-task POPGym environments [27].", "description": "This figure compares the performance of two different bin counts (B=32 and B=256) in the two-hot classification method used for scale-resistant critics. The figure shows that using a smaller number of bins (B=32) can be more sample efficient than using a larger number of bins (B=256), especially for simpler tasks like Battleship Easy.  The x-axis represents training timesteps in millions, and the y-axis represents average return. This illustrates that the choice of bin count can influence the learning process and that finding an optimal value for B requires task-specific tuning.", "section": "Scale-Resistant Critics"}, {"figure_path": "OSHaRf4TVU/figures/figures_19_2.jpg", "caption": "Figure 14: Imbalanced Procgen Datasets. We measure the inflow of experience to the replay buffer by game. Climber accounts for much of our early training data but the buffer is rebalanced as policies improve.", "description": "This figure shows the distribution of experience added to the replay buffer across different Procgen games over the course of training.  Early in training, the \"Climber\" game dominates the replay buffer, indicating that the agent spends significantly more time interacting with it. However, as training progresses and the agent's skills improve, the distribution becomes more balanced, showing that the agent is exploring and mastering a wider range of games.", "section": "4 Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_21_1.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure presents the training results for the Meta-World ML45 benchmark.  It shows the performance of four different training methods (combinations of dependent and independent actor/critic updates), comparing their ability to learn and generalize across 45 robotic manipulation tasks.  The left panel displays the number of tasks successfully completed, indicating skill coverage. The center panel shows the average success rate. The right panel shows the total reward achieved across three episodes.  The results demonstrate how scale-resistant updates contribute to improved performance.", "section": "Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_22_1.jpg", "caption": "Figure 10: Multli-Task BabyAI. Results are the average over four training seeds and plotted according to the median, mean, and interquartile range over the task set.", "description": "This figure shows the results of multi-task training on the BabyAI environment.  Four different training seeds were used, and the results are averaged across the entire task set. The figure displays the median, mean, and interquartile range of the total return achieved per two-episode rollout. This illustrates the performance of the model when adapting to new tasks in this environment.", "section": "Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_23_1.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure shows the training results on the Meta-World ML45 benchmark.  Three metrics are displayed: the coverage of manipulation skills (left), average success rate (center), and average total return (right).  Each metric is plotted against the total training timesteps across all 45 tasks, with separate lines for different combinations of actor and critic loss functions (dependent or independent of reward scale).  Reference scores from MuZero and RL2-PPO are provided for comparison. Error bars represent the variability across three random trials.", "section": "Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_24_1.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure shows the training results on the Meta-World ML45 benchmark. The left panel displays the number of tasks where the agent achieved a success rate of at least 2/3, demonstrating the policy's ability to generalize to unseen tasks. The center panel presents the average success rate across all tasks and their variants, illustrating the overall performance.  The right panel shows the average total return accumulated over three episodes, highlighting the reward obtained by the agent.  Each panel shows results from four different training strategies, using scale-dependent and scale-independent loss functions.  Reference scores for MuZero and RL2-PPO are included for comparison.", "section": "3 Multi-Task Adaptation Without Task Labels"}, {"figure_path": "OSHaRf4TVU/figures/figures_25_1.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure shows the training results for the Meta-World ML45 benchmark, comparing four different combinations of learning updates: dependent actor and critic, dependent actor and independent critic, independent actor and dependent critic, and independent actor and independent critic.  The three subplots display the coverage of the 45 tasks (left), the average success rate (center), and the average total return (right) across three episodes.  The results demonstrate that using scale-invariant learning updates leads to improved skill coverage, success rate, and total return, surpassing other methods like MuZero and RL2-PPO.  Error bars represent the maximum and minimum values across three separate trials for each experimental condition.", "section": "Experiments"}, {"figure_path": "OSHaRf4TVU/figures/figures_26_1.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure shows the training results for the Meta-World ML45 benchmark.  The left panel displays the number of tasks where the agent achieved at least a 2/3 success rate.  The center panel illustrates the average success rate across all tasks and rollouts. The right panel presents the total return during three-episode rollouts. The results compare different training approaches (using different actor and critic loss functions) and include comparison to the MuZero and RL2-PPO baselines from existing works.", "section": "Multi-Task Adaptation Without Task Labels"}, {"figure_path": "OSHaRf4TVU/figures/figures_27_1.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure displays the training results of four different learning update combinations on the Meta-World ML45 benchmark. The left panel shows the number of tasks where the agent achieved a success rate of at least 2/3. The center panel shows the average success rate across all tasks.  The right panel shows the average total return accumulated across three episodes.  The results demonstrate that using scale-resistant actor-critic objectives improves performance in challenging generalization problems.", "section": "3 Multi-Task Adaptation Without Task Labels"}, {"figure_path": "OSHaRf4TVU/figures/figures_28_1.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure presents the training results for the Meta-World ML45 benchmark. It shows the performance of four different training methods (varying combinations of dependent/independent actor-critic updates) in terms of skill coverage, success rate, and total return over three episodes. The left panel shows skill coverage, the middle shows average success rate, and the right shows total return.  MuZero and RL2-PPO results are shown for comparison.", "section": "3 Multi-Task Adaptation Without Task Labels"}, {"figure_path": "OSHaRf4TVU/figures/figures_29_1.jpg", "caption": "Figure 4: Meta-World ML45 Train Task Results. (Left) Coverage of the 45 manipulation skills measured by an adaptation horizon success rate \u2265 2/3. (Center) Average success rate over tasks, variants, and 3-episode rollouts. Reference scores for MuZero and RL2-PPO are gathered from results in [17, 73]. (Right) Total return over a 3-episode adaptation horizon, averaged across tasks and variants. All error bars indicate the maximum and minimum metric across three random trials.", "description": "This figure displays the training results on the Meta-World ML45 benchmark, comparing four different combinations of learning updates.  The left panel shows the coverage of the 45 tasks, measured by the proportion of tasks achieving a success rate of at least 2/3. The center panel shows the average success rate across all tasks and variants, while the right panel shows the average total return over three episodes.  The figure highlights the performance gains achieved by using scale-resistant actor-critic updates.  Reference scores for MuZero and RL2-PPO are provided for comparison.", "section": "3 Multi-Task Adaptation Without Task Labels"}]