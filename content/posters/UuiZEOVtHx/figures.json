[{"figure_path": "UuiZEOVtHx/figures/figures_1_1.jpg", "caption": "Figure 1: Performance of our algorithm on FrozenLake with completely random data.", "description": "The figure shows a comparison of the agent's performance on the FrozenLake environment when trained with completely random data using the proposed algorithm (left panel) and when trained with an ideal policy (right panel). The left panel demonstrates that even with random data, the algorithm finds a path to the goal, although it may not be optimal, whereas the right panel shows the optimal path to the goal.", "section": "1 Introduction"}, {"figure_path": "UuiZEOVtHx/figures/figures_8_1.jpg", "caption": "Figure 2: Reading order: (a) target demonstrations in yellow, wall in white; (b) result for log-density without considering the safety constraint; (c) target demonstrations that all states in top-right corners have cost; (d) result for log-density with safety constraint.", "description": "This figure shows the results of safe imitation learning experiments in a maze environment.  Panel (a) displays the expert demonstrations used for training.  Panel (b) illustrates the learned policy's log-density when safety constraints are ignored, and panel (c) shows the target demonstrations incorporating safety constraints (costs assigned to states in the top-right corners).  Finally, panel (d) presents the learned policy log-density when safety constraints are considered.  The figure demonstrates the ability of the proposed algorithm to successfully learn a safe policy even with partial data.", "section": "5.1 Safe Imitation Learning"}, {"figure_path": "UuiZEOVtHx/figures/figures_9_1.jpg", "caption": "Figure 3: Performance on FrozenLake with general function approximation. Reading order: (a) and (b) show the training result with four different behavior policies of COptiDICE and ours. (c) and (d) demonstrate the variations in rewards and costs as the dataset increases. Each point is the average result of 10 independent runs.", "description": "The figure presents the results of the proposed algorithm (POCC) and the baseline algorithm (COptiDICE) on the FrozenLake environment.  Subfigures (a) and (b) show the learning curves for reward and cost respectively, comparing the performance of both algorithms under four different behavior policies (p = 0.75, 0.5, 0.25, 0.0, representing the percentage of optimal policies in the dataset).  Subfigures (c) and (d) illustrate how the reward and cost vary with different dataset sizes for a fixed behavior policy (p=0.5). Error bars represent the average over 10 independent runs.", "section": "5.2 Offline CMDP"}]