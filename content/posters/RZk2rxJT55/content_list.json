[{"type": "text", "text": "Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky ResNets ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We study Leaky ResNets, which interpolate between ResNets $\\tilde{L}=0$ ) and Fully  \n2 Connected nets $\\begin{array}{r}{\\tilde{L}\\to\\infty)}\\end{array}$ depending on an \u2019effective depth\u2019 hyper-parameter L\u02dc.   \n3 In the infinite depth limit, we study \u2019representation geodesics\u2019 $A_{p}$ : continuous   \n4 paths in representation space (similar to NeuralODEs) from input $p=0$ to output   \n5 $p=1$ that minimize the parameter norm of the network. We give a Lagrangian   \n6 and Hamiltonian reformulation, which highlight the importance of two terms: a   \n7 kinetic energy which favors small layer derivatives $\\partial_{p}A_{p}$ and a potential energy   \n8 that favors low-dimensional representations, as measured by the \u2019Cost of Identity\u2019.   \n9 The balance between these two forces offers an intuitive understanding of feature   \n10 learning in ResNets. We leverage this intuition to explain the emergence of a   \n11 bottleneck structure, as observed in previous work: for large $\\tilde{L}$ the potential energy   \n12 dominates and leads to a separation of timescales, where the representation jumps   \n13 rapidly from the high dimensional inputs to a low-dimensional representation,   \n14 move slowly inside the space of low-dimensional representations, before jumping   \n15 back to the potentially high-dimensional outputs. Inspired by this phenomenon, we   \n16 train with an adaptive layer step-size to adapt to the separation of timescales. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Feature learning is generally considered to be at the center of the recent successes of deep neural   \n19 networks (DNNs), but it also remains one of the least understood aspects of DNN training.   \n20 There is a rich history of empirical analysis of the features learned by DNNs, for example the   \n21 appearance of local edge detections in CNNs with a striking similarity to the biological visual cortex   \n22 [19], feature arithmetic properties of word embeddings [22], similarities between representations   \n23 at different layers [18, 20], or properties such as Neural Collapse [24] to name a few. While some   \n24 of these phenomenon have been studied theoretically [3, 8, 27], a more general theory of feature   \n25 learning in DNNs is still lacking.   \n26 For shallow networks, there is now strong evidence that the first weight matrix is able to recognize a   \n27 low-dimensional projection of the inputs that determines the output (assuming this structure is present)   \n28 [4, 2, 1]. A similar phenomenon appears in linear networks, where the network is biased towards   \nlearning low-rank functions and low-dimensional representations in its hidden layers [13, 21, 29].   \n30 But in both cases the learned features are restricted to depend linearly on the inputs, and the feature   \n31 learning happens in the very first weight matrix, whereas it has been observed that features increase   \n32 in complexity throughout the layers [31].   \n33 The linear feature learning ability of shallow networks has inspired a line of work that postulates that   \n34 the weight matrices learn to align themselves with the backward gradients and that by optimizing for   \n35 this alignment directly, one can achieve similar feature learning abilities even in deep nets [5, 25].   \n36 For deep nonlinear networks, a theory that has garnered a lot of interest is the Information Bottleneck   \n37 [28], which observed amongst other things that the inner representations appear to maximize their   \n38 mutual information with the outputs, while minimizing the mutual information with the inputs. A   \n39 limitation of this theory is its reliance on the notion of mutual information which has no obvious   \n40 definition for empirical distributions, which lead to some criticism [26].   \n41 A recent theory that is similar to the Information Bottleneck but with a focus on the   \n42 dimensionality/rank of the representations and weight matrices rather than the mutual information is   \n43 the Bottleneck rank/Bottleneck structure [16, 15, 30]: which describes how, for large depths, most of   \n44 the representations will have approximately the same low dimension, which equals the Bottleneck   \n45 rank of the task (the minimal dimension that the inputs can be projected to while still allowing   \n46 for fitting the outputs). The intuitive explanation for this bias is that a smaller parameter norm is   \n47 required to (approximately) represent the identity on low-dimensional representations rather than   \n48 high dimensional ones. Some other types of low-rank bias have been observed in recent work [9, 14].   \n49 In this paper we will focus on describing the Bottleneck structure in ResNets, and formalize the   \n50 notion of \u2018cost of identity\u2019 as a driving force for the bias towards low dimensional representation.   \n51 The ResNet setup allows us to consider the continuous paths in representation space from input to   \n52 output, similar to the NeuralODE [6], and by adding weight decay, we can analyze representation   \n53 geodesics, which are paths that minimize parameter norm, as already studied in [23]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "54 1.1 Leaky ResNets ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "55 Our goal is to study a variant of the NeuralODE [6, 23] approximation of ResNet with leaky skip   \n56 connections and with $L_{2}$ -regularization. The classical NeuralODE describes the continuous evolution   \n57 of the activations $\\alpha_{p}(x)\\in\\mathbb{R}^{w}$ starting from $\\alpha_{0}(x)=x$ at the input layer $p=0$ and then follows ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\partial_{p}\\alpha_{p}(x)=W_{p}\\sigma(\\alpha_{p}(x))\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "58 for the $w\\times(w+1)$ matrices $W_{p}$ and the nonlinearity $\\sigma:\\mathbb{R}^{w}\\rightarrow\\mathbb{R}^{w+1}$ which maps a vector $z$ to   \n59 $\\sigma(z)\\,=\\,(\\,\\,\\left[z_{1}\\right]_{+}$ . . . $\\bigl[z_{w}\\bigr]_{+}\\ \\ \\ \\dot{1}\\ \\ \\bigr)$ , applying the ReLU nonlinearity entrywise and appending a   \n60 new entry with value 1. Thanks to the appended 1 we do not need any explicit bias, since the last   \n61 column $W_{p,\\cdot w+1}$ of the weights replaces the bias.   \n62 This can be thought of as a continuous version of the traditional ResNet with activations $\\alpha_{\\ell}(x)$ for   \n63 $\\ell=1,\\ldots,L;\\,\\alpha_{\\ell+1}(x)=\\alpha_{\\ell}(x)+W_{\\ell}\\sigma(\\alpha_{\\ell}(x))$ .   \n64 We will focus on Leaky ResNets, a variant of ResNets that interpolate between ResNets and FCNNs,   \n65 by tuning the strength of the skip connections leading to the following ODE with parameter $\\tilde{L}$ : ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\partial_{p}\\alpha_{p}(x)=-\\tilde{L}\\alpha_{p}(x)+W_{p}\\sigma(\\alpha_{p}(x)).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "66 This can be thought of as the continuous version of $\\alpha_{\\ell+1}(x)=(1-\\tilde{L})\\alpha_{\\ell}(x)+W_{\\ell}\\sigma(\\alpha_{\\ell}(x))$ . As we   \n67 will see, the parameter $\\tilde{L}$ plays a similar role as the depth in a FCNN. ", "page_idx": 1}, {"type": "text", "text": "68 Finally we will be interested describing the paths that minimize a cost with $L_{2}$ -regularization ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W_{p}}\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|f^{*}(x_{i})-\\alpha_{1}(x_{i})\\right\\|^{2}+\\frac{\\lambda}{2\\tilde{L}}\\int_{0}^{1}\\left\\|W_{p}\\right\\|_{F}^{2}d p.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "69 The scaling of $\\frac{\\lambda}{\\tilde{L}}$ for the regularization term will be motivated in Section 1.2. ", "page_idx": 1}, {"type": "text", "text": "70 This type of optimization has been studied in [23] without leaky connections, but we will describe in   \n71 this paper large $\\tilde{L}$ behavior which leads to a so-called Bottleneck structure [16, 15] as a result of a   \n72 separation of time scales in $p$ . ", "page_idx": 1}, {"type": "text", "text": "73 1.2 A Few Symmetries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "74 Changing the leakage parameter $\\tilde{L}$ is equivalent (up to constants) to changing the integration range   \n75 $[0,1]$ or to scaling the outputs.   \n76 Integration range: Consider the weights $W_{p}$ on the range $[0,1]$ and leakage parameter $\\tilde{L}$ , leading   \n77 to activations $\\alpha_{p}$ . Then stretching the weights to a new range $[0,c]$ , by defining $W_{q}^{\\prime}=\\textstyle{\\frac{1}{c}}W_{q/c}$ for ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "78 $q\\in[0,c]$ , and dividing the leakage parameter by $c$ , stretches the activations $\\alpha_{q}^{\\prime}=\\alpha_{p/c}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\partial_{q}\\alpha_{q}^{\\prime}(x)=-\\frac{\\tilde{L}}{c}\\alpha_{q}^{\\prime}(x)+\\frac{1}{c}W_{q/c}\\sigma(\\alpha_{q}^{\\prime}(x))=\\frac{1}{c}\\partial_{p}\\alpha_{q/2}(x),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "79 and the parameter norm is simply divided by $\\begin{array}{r}{c\\colon\\!\\!\\int_{0}^{c}\\left\\|W_{q}^{\\prime}\\right\\|^{2}d q=\\frac{1}{c}\\int_{0}^{1}\\left\\|W_{p}\\right\\|^{2}d p.}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "80 This implies that a path on the range $[0,c]$ with leakage parameter $\\tilde{L}=1$ is equivalent to a path on   \n81 the range $[0,1]$ with leakage parameter $\\tilde{L}=c$ up to a factor of $c$ in front of the parameter weights.   \n82 For this reason, instead of modeling different depths as changing the integration range, we will keep   \n83 the integration range to $[0,1]$ for convenience but change the leakage parameter $\\tilde{L}$ instead. To get rid   \n84 of the factor in front of the integral, we choose a regularization term of the form $\\frac{\\lambda}{\\tilde{L}}$ . From now on, we   \n85 call $\\tilde{L}$ the (effective) depth of the network.   \n86 Note that this also suggests that in the absence of leakage $\\tilde{L}=0,$ ), changing the range of integration   \n87 has no effect on the effective depth, since $2\\tilde{L}=0$ too. Instead, in the absence of leakage, the effective   \n88 depth can be increased by scaling the outputs as we now show.   \n89 Output scaling: Given a path $W_{p}$ on the $[0,1]$ (for simplicity, we assume that there are no bias, i.e.   \n90 $W_{p,\\cdot w+1}=0)$ , then increasing the leakage by a constant $\\tilde{L}\\rightarrow\\tilde{L}+c$ leads to a scaled down path   \n91 $\\alpha_{p}^{\\prime}=e^{-c p}\\alpha_{p}$ . Indeed we have $\\alpha_{0}^{\\prime}(x)=\\alpha_{0}(x)$ and ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\partial_{p}\\alpha_{p}^{\\prime}(x)=-(\\tilde{L}+c)\\alpha_{p}^{\\prime}(x)+W_{p}\\sigma(\\alpha_{p}^{\\prime}(x))=e^{-c p}\\left(\\partial_{p}\\alpha_{p}(x)-c\\alpha_{p}(x)\\right)=\\partial_{p}(e^{-c p}\\alpha_{p}(x)).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "92 Thus a nonleaky ResNet $\\tilde{L}=0$ with very large outputs $\\alpha_{1}(x)$ is equivalent to a leaky ResNet $\\tilde{L}>0$   \n93 with scaled down outputs $e^{-\\tilde{L}}\\alpha_{1}(x)$ . Such large outputs are common when training on cross-entropy   \n94 loss, and other similar losses that are only minimized at infinitely large outputs. When trained on   \n95 such losses, it has been shown that the outputs of neural nets will keep on growing during training   \n96 [12, 7], suggesting that when training ResNets on such a loss, the effective depth increases during   \n97 training (though quite slowly). ", "page_idx": 2}, {"type": "text", "text": "98 1.3 Lagrangian Reformulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "99 The optimization of Leaky ResNets can be reformulated, leading to a Lagrangian form. ", "page_idx": 2}, {"type": "text", "text": "100 First observe that the weights $W_{p}$ at any minimizer can be expressed in terms of the matrix of   \n101 activations $A_{p}=\\alpha_{p}(X)\\in\\mathbb{R}^{w\\times N}$ over the whole training set $X\\in\\mathbb{R}^{w\\times N}$ (similar to [17]): ", "page_idx": 2}, {"type": "equation", "text": "$$\nW_{p}=(\\tilde{L}A_{p}+\\partial_{p}A_{p})\\sigma(A_{p})^{+}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "102 where $(\\cdot)^{+}$ is the pseudo-inverse. ", "page_idx": 2}, {"type": "text", "text": "103 We therefore consider the equivalent optimization over the activations $A_{p}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A_{p}:A_{0}=X}\\frac{1}{N}\\left\\|f^{*}(X)-A_{1}\\right\\|^{2}+\\frac{\\lambda}{2\\tilde{L}}\\int_{0}^{1}\\left\\|\\tilde{L}A_{p}+\\partial_{p}A_{p}\\right\\|_{K_{p}}^{2}d p.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "104 This is our first encounter with the norm $\\|M\\|_{K_{p}}\\,=\\,\\|M\\sigma(A_{p})^{+}\\|_{F}$ corresponding to the scalar   \n105 product $\\langle A,B\\rangle_{K_{p}}\\,=\\,\\mathrm{Tr}\\left[A K_{p}^{+}B\\right]$ for $K_{p}\\,=\\,\\sigma(A_{p})^{T}\\sigma(A_{p})$ that will play a central role in our   \n106 upcoming analysis. By convention, we say that $\\|M\\|_{K_{p}}=\\infty$ if $M$ does not lie in the image of $K_{p}$ ,   \n107 i.e. ${\\mathrm{Im}}M^{T}\\not\\subseteq{\\mathrm{Im}}K_{p}$ . ", "page_idx": 2}, {"type": "text", "text": "108 It can be helpful to decompose this loss along the different neurons ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A_{p}:A_{0}=X}\\sum_{i=1}^{w}\\frac{1}{N}\\left\\|f_{i}^{*}(X)-A_{1,i}\\right\\|^{2}+\\frac{\\lambda}{2\\tilde{L}}\\int_{0}^{1}\\left\\|\\tilde{L}A_{p,i\\cdot}+\\partial_{p}A_{p,i\\cdot}\\right\\|_{K_{p}}^{2}d p,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "109 Leading to a particle flow behavior, where the neurons $A_{p,i\\cdot}\\in\\mathbb{R}^{N}$ are the particles. At first glance, it   \n110 appears that there is no interaction between the particles, but remember that the norm $\\left\\|\\cdot\\right\\|_{K_{p}}$ depends   \n111 on the covariance $\\begin{array}{r}{K_{p}=\\sum_{i=1}^{w}\\sigma(A_{i\\cdot})\\sigma(A_{i\\cdot})^{T}}\\end{array}$ , leading to a global interaction between the neurons. ", "page_idx": 2}, {"type": "text", "text": "112 If we assume that $\\mathrm{Im}A_{p}^{T}\\subset\\mathrm{Im}\\sigma(A_{p})^{T}$ , we can decompose the inside of the integral as three terms: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{1}{2\\tilde{L}}\\left\\|\\tilde{L}A_{p}+\\partial_{p}A_{p}\\right\\|_{K_{p}^{+}}^{2}=\\frac{\\tilde{L}}{2}\\left\\|A_{p}\\right\\|_{K_{p}}^{2}+\\tilde{L}\\left\\langle\\partial_{p}A_{p},A_{p}\\right\\rangle_{K_{p}^{+}}+\\frac{1}{2\\tilde{L}}\\left\\|\\partial_{p}A_{p}\\right\\|_{K_{p}}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "113 The middle term $\\langle\\partial_{p}A_{p},A_{p}\\rangle_{K_{p}^{+}}$ plays a relatively minor role in our analysis1, so we focus more on   \n114 the two other terms:   \n115 Cost of identity $\\big\\|A_{p}\\big\\|_{K_{p}}^{2}$ / potential energy $-\\frac{\\tilde{L}}{2}\\left\\|A_{p}\\right\\|_{K_{p}}^{2}\\colon$ : This term can be interpreted as a form of   \n116 potential energy, since it only depends on the representation $A_{p}$ and not its derivative $\\partial_{p}A_{p}$ . We call   \n117 it the cost of identity (COI), since it is the Frobenius norm of the smallest weight matrix $W_{p}$ such that   \n118 $W_{p}\\sigma(A_{p})=A_{p}$ . The COI can be interpreted as measuring the dimensionality of the representation,   \n119 inspired by the fact if the representations $A_{p}$ is non-negative (and there is no bias $\\beta\\,=\\,0$ ), then   \n120 $A_{p}=\\sigma(A_{p})$ and the COI simply equals the rank $\\left\\|A_{p}\\right\\|_{K_{p}}^{2}=\\mathrm{Rank}A_{p}$ (this interpretation is further   \n121 justified in Section 1.4). We follow the convention of defining the potential energy as the negative of   \n122 the term that appears in the Lagrangian, so that the Hamiltonian equals the sum of these two energies.   \n123 Kinetic energy $\\begin{array}{r}{\\frac{1}{2\\tilde{L}}\\left\\|\\partial_{p}A_{p}\\right\\|_{K_{p}}^{2}}\\end{array}$ : This term measures the size of the representation derivative $\\partial_{p}A_{p}$   \n124 w.r.t. the $K_{p}$ norm. It favors paths $p\\mapsto A_{p}$ that do not move too fast, especially along directions   \n125 where $\\sigma({A_{p}})$ is small.   \n126 This suggests that the local optimal paths must balance two objectives that are sometimes opposed:   \n127 the kinetic energy favors going from input representation to output representation in a \u2018straight line\u2019   \n128 that minimizes the path length, the COI on the other hand favors paths that spends most of the path in   \n129 low-dimensional representations that have a low COI. The balance between these two goals shifts   \n130 as the depth $\\tilde{L}$ grows, and for large depths it becomes optimal for the network to rapidly move to a   \n131 representation of smallest possible dimension (not too small that it becomes impossible to map back   \n132 to the outputs), remain for most of the layers inside the space of low-dimensional representations,   \n133 and finally move rapidly to the output representation; even if this means doing a large \u2018detour\u2019 and   \n134 having a large kinetic energy. The main goal of this paper is to describe this general behavior.   \n135 Note that one could imagine that as $\\tilde{L}\\to\\infty$ it would always be optimal to first go to the minimal   \n136 COI representation which is the zero representation $A_{p}=0$ , but once the network reaches a zero   \n137 representation, it can only learn constant representations afterwards (the matrix $K_{p}=\\mathbf{1}\\mathbf{1}^{T}$ is then   \n138 rank 1 and its image is the space of constant vectors). So the network must find a representation that   \n139 minimizes the COI under the condition that there is a path from this representation to the outputs.   \n140 Remark. While this interpretation and decomposition is a pleasant and helpful intuition, it is rather   \n141 difficult to leverage for theoretical proofs directly. The problem is that we will focus on regimes   \n142 where the representations $A_{p}$ and $\\sigma(A_{p})$ are approximately low-dimensional (since those are the   \n143 representations that locally minimize the COI), leading to an unbounded pseudo-inverse $\\sigma(A_{p})^{+}$ .   \n144 This is balanced by the fact that $(\\tilde{L}A_{p}+\\partial_{p}A_{p})$ is small along the directions where $\\sigma(A_{p})^{+}$ explodes,   \n145 ensuring a finite weight matrix norm   L\u02dcAp + \u2202pAp   2Kp+. But the suppression of $(\\tilde{L}A_{p}+\\partial_{p}A_{p})$   \n146 along these bad directions usually comes from cancellations, i.e. $\\partial_{p}A_{p}\\approx-\\tilde{L}A_{p}$ . In such cases, the   \n147 decomposition in three terms of the Lagrangian is ill adapted since all three terms are infinite and   \n148 cancel each other to yield a finite sum $\\left\\|\\tilde{L}\\tilde{A_{p}}+\\partial_{p}{A_{p}}\\right\\|_{K_{p}}^{2}$ . One of our goal is to save this intuition   \n149 and prove a similar decomposition with stable equivalent to the cost of identity and kinetic energy   \n150 where $K_{p}^{+}$ is replaced by the bounded $(K_{p}+\\gamma I)^{+}$ for the right choice of $\\gamma$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "151 1.4 Cost of Identity as a Measure of Dimensionality ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "152 The cost of identity can be thought of as a measure of dimensionality of the representation. It is   \n153 obvious for non-negative representations because $\\lVert A_{p}\\rVert_{K_{p}^{+}}^{2}=\\lVert A_{p}A_{p}^{\\phantom{+}}\\rVert_{F}^{2}=\\mathrm{Rank}A_{p}$ , but in general,   \n154 it can be shown to upper bound a notion of \u2018stable rank\u2019: ", "page_idx": 4}, {"type": "text", "text": "155 Proposition 1. $\\begin{array}{r}{\\left\\lVert A\\sigma(A)^{+}\\right\\rVert_{F}^{2}\\geq\\frac{\\lVert A\\rVert_{*}^{2}}{\\lVert A\\rVert_{F}^{2}}}\\end{array}$ for the nuclear norm $\\begin{array}{r}{\\|A\\|_{*}=\\sum_{i=1}^{\\mathrm{Rank}A}s_{i}(A)}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "156 Proof. We know that $\\|\\sigma(A)\\|_{F}\\leq\\|A\\|_{F}$ , therefore $\\begin{array}{r}{\\left\\|A\\sigma(A)^{+}\\right\\|_{F}^{2}\\geq\\operatorname*{min}_{\\left\\|B\\right\\|_{F}\\leq\\left\\|A\\right\\|_{F}}\\left\\|A B^{+}\\right\\|_{F}^{2}}\\end{array}$ which   \n157 is minimized when $B=\\frac{\\|A\\|_{F}}{\\sqrt{\\|A\\|_{*}}}\\sqrt{A}$ , yielding the result. \u53e3   \n158 The stable rank $\\frac{||A||_{*}^{2}}{||A||_{F}^{2}}$ is upper bounded by Rank $A$ , with equality if all non-zero singular values   \n159 of $A$ are equal, and it is lower bound by the more common notion of stable rank $\\frac{\\|A\\|_{F}^{2}}{\\|A\\|_{o p}^{2}}$ , because   \n160 $\\sum s_{i}\\operatorname*{max}s_{i}\\geq\\sum s_{i}^{2}$ for the singular values $s_{i}$ .   \n161 Note that in contrast to the COI which is a very unstable quantity because of the pseudo-inverse, the   \n162 ratio $\\frac{||A||_{*}^{2}}{||A||_{F}^{2}}$ is continuous except at $A=0$ . This also makes it much easier to compute empirically   \n163 than the COI itself.   \n164 We know that the COI matches the dimension or rank for positive representations, but it turns out that   \n165 the local minima of the COI that are stable under the addition of a new neuron are all positive:   \n166 Proposition 2. A local minimum of $A\\mapsto\\left\\lVert A\\sigma(A)^{+}\\right\\rVert_{F}^{2}$ is said to be stable if it remains a local   \n167 minimum after concatenating a zero vector $A^{\\prime}={\\left(\\begin{array}{l}{A}\\\\ {0}\\end{array}\\right)}\\,\\in\\,\\mathbb{R}^{(w+1)\\times N}$ . All stable minima are   \n168 non-negative, and satisfy $\\left\\|A\\sigma(A)^{+}\\right\\|_{F}^{2}=\\mathrm{Rank}A$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "169 Proof. The COI of the nearby point $\\left(\\begin{array}{c}{{A}}\\\\ {{\\epsilon z}}\\end{array}\\right)$ for $z\\in\\mathrm{Im}\\sigma(A)^{T}$ equals ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tr}\\left[(A^{T}A+\\epsilon^{2}z z^{T})\\left(\\left(\\sigma(A)^{T}\\sigma(A)+\\epsilon^{2}\\sigma(z)\\sigma(z)^{T}\\right)^{+}\\right]}\\\\ &{\\mathrm{~}=\\left\\|A\\sigma(A)^{+}\\right\\|^{2}+\\epsilon^{2}\\left\\|z^{T}\\sigma(A)^{+}\\right\\|^{2}-\\epsilon^{2}\\left\\|\\sigma(z)^{T}\\sigma(A)^{+}\\sigma(A)^{+T}A^{T}\\right\\|^{2}+O(\\epsilon^{4}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "170 Assume by contradiction that there is a $i\\;=\\;1,\\ldots,N$ such that $\\sigma(A._{i})\\ \\neq\\ A._{i}$ , then choosing   \n171 $z=\\sigma(A)^{\\tilde{T}}\\sigma(A._{i})$ we have $\\sigma(z)=z$ and the two $\\epsilon^{2}$ terms are negative: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon^{2}\\left\\|\\sigma(A_{i})\\right\\|^{2}-\\epsilon^{2}\\left\\|A_{i}\\right\\|^{2}<0,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "172 which implies that $A^{\\prime}$ it is not a local minimum. ", "page_idx": 4}, {"type": "text", "text": "173 These stable minima will play a significant role in the rest of our analysis, as we will see that for large   \n174 $\\tilde{L}$ the representations $A_{p}$ of most layers will be close to one such local minimum. Now we are not   \n175 able to rule out the existence of non-stable local minima (nor guarantee that they are avoided with   \n176 high probability), but one can show that all strict local minima of wide enough networks are stable.   \n177 Actually we can show something stronger, starting from any non-stable local minimum there is a   \n178 constant loss path that connects it to a saddle:   \n179 Proposition 3. If $w>N(N+1)$ then if $\\hat{A}\\in\\mathbb{R}^{w\\times N}$ is local minimum of $A\\mapsto\\left\\lVert A\\sigma(A)^{+}\\right\\rVert_{F}^{2}$ that is   \n180 not non-negative, then there is a continuous path $A_{t}$ of constant COI such that $A_{0}={\\hat{A}}$ and $A_{1}$ is $a$   \n181 saddle.   \n182 This could explain why a noisy GD would avoid such negative/non-stable minima, since there is   \n183 no \u2018barrier\u2019 between the minima and a lower one, one could diffuse along the path described in   \n184 Proposition 3 until reaching a saddle and going towards a lower COI minima. But there seems to be   \n185 something else that pushes away from such non-negative minima, as in our experiments with full   \n186 population GD we have only observed stable/non-negative local minimas. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "RZk2rxJT55/tmp/0c6696eb253b25d514af0d8b48839d01583d972e55c9703dc759686554a7e69d.jpg", "img_caption": ["(a) Hamiltonian measures across $\\tilde{L}$ "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "RZk2rxJT55/tmp/1d2dea94cfc1108b25cf36b64463694a92db02ed9d145ca5bfcd97dfe4107294.jpg", "img_caption": ["(b) Bottleneck structure "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "RZk2rxJT55/tmp/fbf2853e890ab55df29ec90f14feb13624591920304a119066624ff89e8cc0c8.jpg", "img_caption": ["(c) Hamiltonian dynamics "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 1: Leaky ResNet structures: We train equidistant networks with a fixed $L=20$ over a range of effective depths $\\tilde{L}$ . The true function $f^{*}:\\mathbb{R}^{30}\\rightarrow\\mathbb{R}^{30}$ is the composition of two random FCNNs $g_{1},g_{2}$ mapping from dim. 30 to 3 to 30. (a) Estimates of the Hamiltonian constants for networks trained with different $\\tilde{L}$ . The Hamiltonian refers to $-\\frac{2}{\\tilde{L}}\\mathcal{H}$ which estimates the true rank $k^{*}$ . The COI refers to $\\operatorname*{min}_{p}||A_{p}||$ . The trend line follows the median estimate for $-\\frac{2}{\\tilde{L}}\\mathcal{H}$ across each network\u2019s layers, whereas the error bars signify its minimum and maximum over $\\bar{p}\\in[0,1]$ . The \"stable\" Hamiltonians utilize the relaxation from Theorem 4. (b) Spectra of the representations $A_{p}$ and weights $W_{p}$ respectively for $\\tilde{L}=7$ . (c) Hamiltonian dynamics of the network in (b). ", "page_idx": 5}, {"type": "text", "text": "187 1.5 Hamiltonian Reformulation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "188 We can further reformulate the evolution of the optimal representations $A_{p}$ in terms of a Hamiltonian,   \n189 similar to Pontryagin\u2019s maximum principle.   \n190 Let us define the backward pass variables $\\begin{array}{r}{B_{p}=-\\frac{1}{\\lambda}\\partial_{A_{p}}C(A_{1})}\\end{array}$ for the cost $\\begin{array}{r}{C(A)=\\frac{1}{2}\\|f^{*}(X)\\!-\\!A\\|_{F}^{2}}\\end{array}$ ,   \n191 which play the role of the \u2018momenta\u2019 of $A_{p}$ in this Hamiltonian interpretation, which follows the   \n192 backward differential equation ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle B_{1}=-\\frac{1}{\\lambda}\\partial_{A_{1}}C(A_{1})=\\frac{2}{\\lambda N}(f^{*}(X)-A_{1})}\\\\ {\\displaystyle-\\partial_{p}B_{p}=\\dot{\\sigma}(A_{p})\\odot\\left[W_{p}^{T}B_{p}\\right]-\\tilde{L}B_{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "193 Now at any critical point, we have that $\\begin{array}{r l r}{\\partial_{W_{p}}C(A_{1})\\;+\\;\\frac{\\lambda}{\\tilde{L}}W_{p}}&{{}=}&{0}\\end{array}$ and thus $\\begin{array}{r l}{W_{p}}&{{}=}\\end{array}$   \n194 $\\begin{array}{r}{-\\frac{\\tilde{L}}{\\lambda}\\partial_{A_{p}}C(A_{1})\\sigma(A_{p})^{T}=\\tilde{L}B_{p}\\sigma(A_{p})^{T}}\\end{array}$ , leading to joint dynamics for $A_{p}$ and $B_{p}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\partial_{p}A_{p}=\\tilde{L}(B_{p}\\sigma(A_{p})^{T}\\sigma(A_{p})-A_{p})}\\\\ &{-\\partial_{p}B_{p}=\\tilde{L}\\left(\\dot{\\sigma}(A_{p})\\odot\\left[\\sigma(A_{p})B_{p}^{T}B_{p}\\right]-B_{p}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "195 These are Hamiltonian dynamics $\\partial_{p}A_{p}=\\partial_{B_{p}}\\mathcal{H}$ and $-\\partial_{p}B_{p}=\\partial_{A_{p}}\\mathcal{H}$ w.r.t. the Hamiltonian ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}(A_{p},B_{p})=\\frac{\\tilde{L}}{2}\\left\\|B_{p}\\sigma(A_{p})^{T}\\right\\|^{2}-\\tilde{L}\\mathrm{Tr}\\left[B_{p}A_{p}^{T}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "196 The Hamiltonian is a conserved quantity, i.e. it is constant in $p$ . It will play a significant role in   \n197 describing a separation of timescales that appears for large depths $\\tilde{L}$ . Another significant advantage   \n198 of the Hamiltonian reformulation over the Lagrangian approach is the absence of the unstable   \n199 pseudo-inverses $\\sigma(A_{p})^{+}$ .   \n200 Remark. Note that the Lagrangian and Hamiltonian reformulations have already appeared in previous   \n201 work [23] for non-leaky ResNets. Our main contributions are the description in the next section of the   \n202 Hamiltonian as the network becomes leakier $\\tilde{L}\\to\\infty$ , the connection to the cost of identity, and the   \n203 appearance of a separation of timescales. These structures are harder to observe in non-leaky ResNets   \n204 (though they could in theory still appear since increasing the scale of the outputs is equivalent to   \n205 increasing the effective depth $\\tilde{L}$ as shown in Section 1.2).   \n206 The Lagrangian and Hamiltonian are also very similar to the ones in [10, 11], and the separation of   \n207 timescales and rapid jumps that we will describe also bear a strong similarity. Though a difference   \n208 with our work is that the norm $\\|\\cdot\\|_{K_{p}}$ depends on $A_{p}$ and can be degenerate. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "209 2 Bottleneck Structure in Representation Geodesics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "210 A recent line of work [16, 15] studies the appearance of a so-called Bottleneck structure in large   \n211 depth fully-connected networks, where the weight matrices and representations of \u2018almost all\u2019 layers   \n212 of the layers are approximately low-rank/low-dimensional as the depth grows. This dimension $k$ is   \n213 consistent across layers, and can be interpreted as being equal to the so-called Bottleneck rank of the   \n214 learned function. This structure has been shown to extend to CNNs in [30], and we will observe a   \n215 similar structure in our leaky ResNets, further showcasing its generality.   \n216 More generally, our goal is to describe the \u2018representation geodesics\u2019 of DNNs: the paths in   \n217 representation space from input to output representation. The advantage of ResNets (leaky or   \n218 not) over FCNNs is that these geodesics can be approximated by continuous paths and are described   \n219 by differential equations (as described by the Hamiltonian reformulation).   \n220 This section provides an approximation of the Hamiltonian that illustrates the separation of timescales   \n221 that appears for large depths, with slow layers with low COI/dimension, and fast layers with high   \n222 COI/dimension. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "223 2.1 Separation of Timescales ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "224 If $\\mathrm{Im}A_{p}^{T}\\subset\\mathrm{Im}\\sigma(A_{p})^{T}$ , then the Hamiltonian equals the sum of the kinetic and potential energies: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{H}=\\frac{1}{2\\tilde{L}}\\left\\|\\partial_{p}A_{p}\\right\\|_{K_{p}}^{2}-\\frac{\\tilde{L}}{2}\\left\\|A_{p}\\right\\|_{K_{p}}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "225 This implies that $\\begin{array}{r}{\\|\\partial_{p}A_{p}\\|_{K_{p}}=\\tilde{L}\\sqrt{\\|A_{p}\\|_{K_{p}}^{2}+\\frac{2}{\\tilde{L}}\\mathcal{H}}}\\end{array}$ which implies that for large $\\tilde{L}$ , the derivative   \n226 $\\partial_{p}A_{p}$ is only finite at $p\\mathbf{s}$ where the COI $\\|A_{p}\\|_{K_{p}}^{2}$ is close to $-\\frac{2}{\\tilde{L}}\\mathcal{H}$ . On the other hand, $\\partial_{p}A_{p}$ will   \n227 blow up for all $p$ with a finite gap $\\sqrt{\\left\\|A_{p}\\right\\|_{K_{p}}^{2}+\\frac{2}{{\\tilde{L}}}{\\mathcal H}}>0$ between the COI and the Hamiltonian. This   \n228 suggests a separation of timescales as $\\tilde{L}\\to\\infty$ , with slow dynamics in layers whose COI/dimension   \n229 is close to $-\\frac{2}{\\tilde{L}}\\mathcal{H}$ and fast dynamics in the high COI/dimension layers.   \n230 But the assumption $\\mathrm{Im}A_{p}^{T}\\subset\\mathrm{Im}\\sigma(A_{p})^{T}$ seems to rarely be true in practice, and both kinetic and   \n231 COI appear to be often infinite in practice. But up to a few approximations, the same argument can   \n232 be made for stable versions of the kinetic energy/COI: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "233 Theorem 4. For sequence $A_{p}^{\\tilde{L}}$ of geodesics with $\\left\\|B_{p}^{\\tilde{L}}\\right\\|^{2}\\leq c<\\infty$ , and any $\\gamma>0$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n-\\left(\\frac{1}{\\tilde{L}}\\ell_{\\gamma,\\tilde{L}}+\\sqrt{\\gamma}c\\right)^{2}\\leq-\\frac{2}{\\tilde{L}}\\mathcal{H}-\\operatorname*{min}_{p}\\left\\|A_{p}^{\\tilde{L}}\\right\\|_{(K_{p}+\\gamma I)}^{2}\\leq\\gamma c,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "234 for the path length \u2113\u03b3,L\u02dc = 01   \u2202pApL\u02dc   (Kp+\u03b3I) d p. Finally ", "page_idx": 6}, {"type": "equation", "text": "$$\n-\\tilde{L}\\sqrt{\\gamma c}\\leq\\left\\|\\partial_{p}A_{p}\\right\\|_{(K_{p}+\\gamma i)}-\\tilde{L}\\sqrt{\\left\\|A_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2}+\\frac{2}{\\tilde{L}}\\mathcal{H}}\\leq2\\tilde{L}\\sqrt{\\gamma c}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "235 Note that the size of $\\|B_{p}^{\\tilde{L}}\\|^{2}$ can vary a lot throughout the layers, we therefore suggest choosing   \n236 a $p$ -dependent $\\gamma$ : $\\gamma_{p}=\\dot{\\gamma_{0}}\\|\\sigma(A_{p}^{\\tilde{L}})\\|_{o p}^{2}=\\gamma_{0}\\|K_{p}\\|_{o p}^{2}$ . There are two motivations for this: first it is   \n237 natural to have $\\gamma$ scale with $K_{p}$ , ; and second, since $\\dot{\\boldsymbol{W_{p}}}=\\tilde{L}\\boldsymbol{B_{p}}\\sigma(\\boldsymbol{A_{p}})^{T}$ is of approximately constant   \n238 size (thanks to balancedness, see Appendix A.3), we typically have that the size of $B_{p}$ is inversely   \n239 proportional to that of $\\sigma(A_{p})$ , so that $\\gamma_{p}\\lvert|B_{p}\\rvert|^{2}$ should keep roughly the same size for all $p$ .   \n240 Theorem 4 shows that for large $\\tilde{L}$ (and choosing e.g. $\\gamma=\\tilde{L}^{-1}.$ ), the Hamiltonian is close to the   \n241 minimal COI along the path. Second, the norm of the derivative $\\|\\partial_{p}A_{p}\\|_{(K_{p}+\\gamma i)}$ is close to $\\tilde{L}$ times   \n242 the \u2018extra-COI\u2019 $\\begin{array}{r}{\\sqrt{\\left\\|A_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2}+\\frac{2}{\\tilde{L}}\\mathcal{H}}\\approx\\sqrt{\\left\\|A_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2}-\\operatorname*{min}_{q}\\left\\|A_{q}\\right\\|_{(K_{q}+\\gamma I)}^{2}}}\\end{array}$ , which describes   \n243 the separation of timescales, with slow $(\\sim1)$ dynamics at layers $p$ where the COI is almost optimal   \n244 and fast $(\\sim\\tilde{L})$ dynamics everywhere the COI is far from optimal. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "RZk2rxJT55/tmp/ef440e37e78d3669427bdddd49d5764d831d0fd9a55f5891f73f0231c7b8b3d7.jpg", "img_caption": ["(a) Test performance versus depth "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "RZk2rxJT55/tmp/3a1558f04ebadf3847879074e2c47ce0dc9e8f439ea77469de1ac54fdd95f78f.jpg", "img_caption": ["(b) Bottleneck structure and adaptivity. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "RZk2rxJT55/tmp/75420dc014a9c28a7c9c5d2220b5061df601dd83431b446bd5c5e2e691c5464b.jpg", "img_caption": ["(c) Paths "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Discretization: We train networks with a fixed $\\tilde{L}{\\mathrm{~=~}}3$ over a range of depths $L$ and definitions of $\\rho_{\\ell}\\mathbf{s}$ . The true function $f^{*}:\\mathbb{R}^{30}\\rightarrow\\mathbb{R}^{30}$ is the composition of three random ResNets $g_{1},g_{2},g_{3}$ mapping from dim. 30 to 6 to 3 to 30. (a) Test error as a function of $L$ for different discretization schemes. (b) Weight spectra across layers for adaptive $\\rho_{\\ell}$ $\\langle L=18\\rangle$ , grey vertical lines represents the steps $p_{\\ell}$ (c) 2D projection of the representation paths $A_{p}$ for $L=18$ . Observe how adaptive $\\rho_{\\ell}\\mathbf{s}$ appears to better spread out the steps. ", "page_idx": 7}, {"type": "text", "text": "245 Assuming a finite length $\\ell_{\\gamma,\\tilde{L}}<\\infty$ , the norm of the derivative must be finite at almost all layers,   \n246 meaning that the COI/dimensionality is optimal in almost all layers, with only a countable number   \n247 of short high COI/dimension jumps. These jumps typically appear at the beginning and end of the   \n248 network, because the input and output dimensionality and COI are (mostly) fixed, so it will typically   \n249 be non-optimal, and so there will often be fast regions close to the beginning and end of the network.   \n250 We have actually never observed any jump in the middle of the network, though we are not able to   \n251 rule them out theoretically.   \n252 If we assume that the paths $A_{p}^{\\tilde{L}}$ are stable under adding a neuron, then we can additionally guarantee   \n253 that the representations in the slow layers (\u2018inside the Bottleneck\u2019) will be non-negative:   \n254 Proposition 5. Let $A_{p}^{\\tilde{L}}$ be a uniformly bounded sequence of local minima for increasing $\\tilde{L}$ , at   \n255 any $p_{0}\\,\\in\\,(0,1)$ such that $\\|\\partial_{p}A_{p}\\|$ is uniformly bounded in a neighborhood of $p_{0}$ for all ${\\tilde{L}},$ then   \n256 $A_{p_{0}}^{\\infty}=\\operatorname*{lim}_{\\tilde{L}}A_{p_{0}}^{\\tilde{L}}$ is non-negative.   \n257 We therefore know that the optimal $\\operatorname{COI\\,min}_{q}\\,\\|A_{q}\\|_{(K_{q}+\\gamma I)}^{2}$ is close to the dimension of the limiting   \n258 representations $A_{p_{0}}^{\\infty}$ , i.e. it must be an integer $k^{*}$ which we call the Bottleneck rank of the sequence   \n259 of minima since it is closely related to the Bottleneck rank introduced in [16]. The Hamiltonian $\\mathcal{H}$ is   \n260 then close to $-\\textstyle{\\frac{\\tilde{L}}{2}}k^{*}$ .   \n261 Figure 1 illustrates these phenomena: the Hamiltonian (and the stable Hamiltonians $\\mathcal{H}_{\\gamma}~=$   \n262 $\\begin{array}{r}{\\frac{1}{2\\tilde{L}}\\left\\|\\partial_{p}A_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2}-\\frac{\\tilde{L}}{2}\\left\\|A_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2})}\\end{array}$ approach the rank $k^{*}=3$ from below, while the minimal   \n263 COI approaches it from above; The kinetic energy is proportional to the extra COI, and they are both   \n264 large towards the beginning and end of the network where the weights $W_{p}$ are higher dimensional.   \n265 We see in Figure 1c that the (stable) Hamiltonian are not exactly constant, but it still varies much less   \n266 than its components, the kinetic and potential energies.   \n267 Because of the non-convexity of the loss we are considering, one can imagine that there could exist   \n268 distinct sequences of local minima as $\\tilde{L}\\to\\infty$ , which could have different rank, depending on what   \n269 low-dimension they reach inside their bottleneck. Indeed in our experiments we have seen that the   \n270 number of dimensions that are kept inside the bottleneck can vary by 1 or 2, and in FCNN distinct   \n271 sequences of depth increasing minima with different ranks have been observed in [15]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "272 3 Discretization Scheme ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "273 To use such Leaky ResNets in practice, we need to discretize over the range $[0,1]$ . For this we   \n274 choose a set of layer-steps $\\rho_{1},\\ldots,\\rho_{L}$ with $\\textstyle\\sum\\rho_{\\ell}=1$ , and define the activations at the locations ", "page_idx": 7}, {"type": "text", "text": "275 $p_{\\ell}=\\rho_{1}+\\cdot\\cdot\\cdot+\\rho_{\\ell}\\in[0,1]$ recursively as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{p_{0}}(x)=x}\\\\ &{\\alpha_{p_{\\ell}}(x)=(1-\\rho_{\\ell}\\tilde{L})\\alpha_{p_{\\ell-1}}(x)+\\rho_{\\ell}W_{p_{\\ell}}\\sigma\\left(\\alpha_{p_{\\ell-1}}(x)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "276 and the regularized cost $\\begin{array}{r}{\\boldsymbol{\\mathcal{L}}(\\theta)\\ =\\ C(\\alpha_{1}(\\boldsymbol{X}))\\,+\\,\\frac{\\lambda}{2\\tilde{L}}\\sum_{\\ell=1}^{L}\\rho_{\\ell}\\,\\|\\boldsymbol{W}_{p_{\\ell}}\\|^{2}\\,}\\end{array}$ , for the parameters $\\theta\\:=$   \n277 $(W_{p_{1}},\\hdots,W_{p_{L}})$ . Note that it is best to ensure that $\\rho_{\\ell}\\tilde{L}$ remains smaller than 1 so that the prefactor   \n278 $(1-\\rho_{\\ell}\\tilde{L})$ does not become negative, though we will also discuss certain setups where it might be   \n279 okay to take larger layer-steps. ", "page_idx": 8}, {"type": "text", "text": "280 Now comes the question of how to choose the $\\rho_{\\ell}\\mathbf{s}$ . We consider three options: ", "page_idx": 8}, {"type": "text", "text": "281 Equidistant: The simplest choice is to choose equidistant points $\\begin{array}{r}{\\rho_{\\ell}=\\frac{1}{L}}\\end{array}$ . Note that the condition   \n282 $\\rho\\ell L<1$ then becomes $L>\\tilde{L}$ . But this choice might be ill adapted in the presence of a Bottleneck   \n283 structure due to the separation of timescales.   \n284 Irregular: Since we typically observe that the fast layers appear close to the inputs and outputs with   \n285 a slow bottleneck in the middle, one could simply choose the $\\rho_{\\ell}$ to be go from small to large and back   \n286 to small as $\\ell$ ranges from 1 to $L$ . This way there are many discretized layers in the fast regions close   \n287 to the input and output and not too many layers inside the Bottleneck where the representations are   \n288 changing less. More concretely one can choose $\\begin{array}{r}{\\rho_{\\ell}=\\frac{1}{L}+\\frac{a}{L}(\\frac{1}{4}-\\left|\\frac{\\ell}{L}-\\frac{1}{2}\\right|)}\\end{array}$ for $a\\in[0,1)$ , the choice   \n289 $a=0$ leads to an equidistant mesh, but increasing $a$ will lead to more points close to the inputs and   \n290 outputs. To guarantee $\\rho_{\\ell}\\tilde{L}<1$ , we need $L>(1+a\\frac{1}{4})\\tilde{L}$ .   \n291 Adaptive: But this can be further improved by choosing the $\\rho_{\\ell}$ to guarantee that the distances   \n292 $\\|A_{\\ell}-A_{\\ell-1}\\|\\,/\\|A_{p}\\|$ are approximately the same for all $\\ell$ (we divide by the size of $A_{p}$ since   \n293 it can vary a lot throughout the layers). Since the rate of change of $A_{p}$ is proportional to $\\rho_{\\ell}$   \n294 $(\\|A_{\\ell}-A_{\\ell-1}\\|/\\|A_{p}\\|=\\rho_{\\ell}c_{\\ell})$ , it is optimal to choose $\\begin{array}{r}{\\rho_{\\ell}=\\frac{c_{\\ell}^{-1}}{\\sum c_{\\ell}^{-1}}}\\end{array}$ for $c_{\\ell}=\\|A_{\\ell}\\!-\\!A_{\\ell-1}\\|\\/\\rho_{\\ell}\\|A_{p}\\|$ . The   \n295 update $\\rho_{\\ell}\\gets\\frac{c_{i}^{-1}}{\\sum c_{i}^{-1}}$ can be done at every training step or every few training steps.   \n296 Note that the condition $\\rho_{\\ell}\\tilde{L}<1$ might not be necessary inside the bottleneck since we have the   \n297 approximation $W_{p}\\sigma(A_{p_{\\ell-1}})\\approx\\tilde{L}A_{p_{\\ell-1}}$ , canceling out the negative direction. In particular with the   \n298 adaptive layer-steps that we propose, a large $\\rho_{\\ell}$ is only possible for layers where $c_{\\ell}$ is small, which is   \n299 only possible when $W_{p}\\sigma(A_{p_{\\ell-1}}^{\\bar{}})\\approx\\tilde{L}A_{p_{\\ell-1}}^{\\bar{}}$ .   \n300 Figure 2 illustrates the effect of the choice of $\\rho_{\\ell}$ for different depths $L$ , we see a small but consistent   \n301 advantage in the test error when using adaptive or irregular $\\rho_{\\ell}\\mathbf{s}$ . Looking at the resulting Bottleneck   \n302 structure, we see that the adaptive $\\rho_{\\ell}\\mathbf{s}$ result in more steps especially in the beginning of the network,   \n303 but also at the end. This because the \u2018true function\u2019 $f^{\\ast}:\\mathbb{R}^{30^{\\bullet}}\\to\\mathbb{R}^{30}$ we are fitting in these   \n304 experiments is of the form $f^{*}=g_{3}\\circ g_{2}\\circ g_{1}$ where the first inner dimension is 6 and the second is 3,   \n305 thus resulting in a rank of $k^{*}=3$ . But before reaching this minimal dimension, the network needs to   \n306 represent $g_{2}\\circ g_{1}$ , which requires more layers, and one can almost see that the weight matrices are   \n307 roughly 6-dimensional around $p=0.3$ . The adaptivity to this structure could explain the advantage   \n308 in the test error. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "309 4 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "310 We have given a description of the representation geodesics $A_{p}$ of Leaky ResNets. We have identified   \n311 an invariant, the Hamiltonian, which is the sum of the kinetic and potential energy, where the kinetic   \n312 energy measures the size of the derivative $\\partial_{p}A_{p}$ , while the potential energy is inversely proportional   \n313 to the cost of identity, which is a measure of dimensionality of the representations. As the effective   \n314 depth of the network grows, the potential energy dominates and we observe a separation of timescales.   \n315 At layers with minimal dimensionality over the path, the kinetic energy (and thus the derivative $\\partial_{p}A_{p}{}^{\\prime}$ )   \n316 is finite. Conversely, at layers where the representation is higher-dimensional, the kinetic energy must   \n317 scale with $\\tilde{L}$ . This leads to a Bottleneck structure, with a short, high-dimensional jump from the input   \n318 representation to a low dimensional representation, followed by slow dynamics inside the space of   \n319 low-dimensional representations followed by a final high-dimensional jump to the high dimensional   \n320 outputs. ", "page_idx": 8}, {"type": "text", "text": "321 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "322 [1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property:   \n323 a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer   \n324 neural networks. In Conference on Learning Theory, pages 4782\u20134887. PMLR, 2022.   \n325 [2] Emmanuel Abbe, Enric Boix-Adser\u00e0, Matthew Stewart Brennan, Guy Bresler, and   \n326 Dheeraj Mysore Nagaraj. The staircase property: How hierarchical structure can guide deep   \n327 learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances   \n328 in Neural Information Processing Systems, 2021.   \n329 [3] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent   \n330 variable model approach to pmi-based word embeddings. Transactions of the Association   \n331 for Computational Linguistics, 4:385\u2013399, 2016.   \n332 [4] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal   \n333 of Machine Learning Research, 18(1):629\u2013681, 2017.   \n334 [5] Daniel Beaglehole, Adityanarayanan Radhakrishnan, Parthe Pandit, and Mikhail Belkin.   \n335 Mechanism of feature learning in convolutional neural networks. arXiv preprint   \n336 arXiv:2309.00570, 2023.   \n337 [6] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary   \n338 differential equations. Advances in neural information processing systems, 31, 2018.   \n339 [7] L\u00e9na\u00efc Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural   \n340 networks trained with the logistic loss. In Jacob Abernethy and Shivani Agarwal, editors,   \n341 Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of   \n342 Machine Learning Research, pages 1305\u20131338. PMLR, 09\u201312 Jul 2020.   \n343 [8] Kawin Ethayarajh, David Duvenaud, and Graeme Hirst. Towards understanding linear word   \n344 analogies. arXiv preprint arXiv:1810.04882, 2018.   \n345 [9] Tomer Galanti, Zachary S Siegel, Aparna Gupte, and Tomaso Poggio. Sgd and weight decay   \n346 provably induce a low-rank bias in neural networks. arXiv preprint arXiv:2206.05794, 2022.   \n347 [10] Tobias Grafke, Rainer Grauer, T Sch\u00e4fer, and Eric Vanden-Eijnden. Arclength parametrized   \n348 hamilton\u2019s equations for the calculation of instantons. Multiscale Modeling & Simulation,   \n349 12(2):566\u2013580, 2014.   \n350 [11] Tobias Grafke and Eric Vanden-Eijnden. Numerical computation of rare events via large   \n351 deviation theory. Chaos: An Interdisciplinary Journal of Nonlinear Science, 29(6):063118, 06   \n352 2019.   \n353 [12] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias   \n354 in terms of optimization geometry. In Jennifer Dy and Andreas Krause, editors, Proceedings of   \n355 the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine   \n356 Learning Research, pages 1832\u20131841. PMLR, 10\u201315 Jul 2018.   \n357 [13] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient   \n358 descent on linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,   \n359 N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,   \n360 volume 31. Curran Associates, Inc., 2018.   \n361 [14] Florentin Guth, Brice M\u00e9nard, Gaspar Rochette, and St\u00e9phane Mallat. A rainbow in deep   \n362 network black boxes. arXiv preprint arXiv:2305.18512, 2023.   \n363 [15] Arthur Jacot. Bottleneck structure in learned features: Low-dimension vs regularity tradeoff. In   \n364 A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in   \n365 Neural Information Processing Systems, volume 36, pages 23607\u201323629. Curran Associates,   \n366 Inc., 2023.   \n367 [16] Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. In   \n368 The Eleventh International Conference on Learning Representations, 2023.   \n369 [17] Arthur Jacot, Eugene Golikov, Cl\u00e9ment Hongler, and Franck Gabriel. Feature learning in   \n370 $l_{2}$ -regularized dnns: Attraction/repulsion and sparsity. In Advances in Neural Information   \n371 Processing Systems, volume 36, 2022.   \n372 [18] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural   \n373 network representations revisited. In International Conference on Machine Learning, pages   \n374 3519\u20133529. PMLR, 2019.   \n375 [19] A. Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep   \n376 convolutional neural networks. Communications of the ACM, 60:84 \u2013 90, 2012.   \n377 [20] Jianing Li and Vardan Papyan. Residual alignment: Uncovering the mechanisms of residual   \n378 networks. Advances in Neural Information Processing Systems, 36, 2024.   \n379 [21] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient   \n380 descent for matrix factorization: Greedy low-rank learning. In International Conference on   \n381 Learning Representations, 2020.   \n382 [22] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word   \n383 representations in vector space. arXiv preprint arXiv:1301.3781, 2013.   \n384 [23] Houman Owhadi. Do ideas have shape? plato\u2019s theory of forms as the continuous limit of   \n385 artificial neural networks. arXiv preprint arXiv:2008.03920, 2020.   \n386 [24] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the   \n387 terminal phase of deep learning training. Proceedings of the National Academy of Sciences,   \n388 117(40):24652\u201324663, 2020.   \n389 [25] Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin.   \n390 Mechanism for feature learning in neural networks and backpropagation-free machine learning   \n391 models. Science, 383(6690):1461\u20131467, 2024.   \n392 [26] Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky,   \n393 Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep   \n394 learning. In International Conference on Learning Representations, 2018.   \n395 [27] Peter S\u00faken\u00edk, Marco Mondelli, and Christoph H Lampert. Deep neural collapse is provably   \n396 optimal for the deep unconstrained features model. Advances in Neural Information Processing   \n397 Systems, 36, 2024.   \n398 [28] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In   \n399 2015 ieee information theory workshop (itw), pages 1\u20135. IEEE, 2015.   \n400 [29] Zihan Wang and Arthur Jacot. Implicit bias of SGD in $l_{2}$ -regularized linear DNNs: One  \n401 way jumps from high to low rank. In The Twelfth International Conference on Learning   \n402 Representations, 2024.   \n403 [30] Yuxiao Wen and Arthur Jacot. Which frequencies do cnns need? emergent bottleneck structure   \n404 in feature learning. to appear at ICML, 2024.   \n405 [31] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In   \n406 Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September   \n407 6-12, 2014, Proceedings, Part I 13, pages 818\u2013833. Springer, 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "408 A Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "409 A.1 Cost of Identity ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "410 Proposition 6 (Proposition 3 in the main.). If $w>N(N+1)$ then if $\\hat{A}\\in\\mathbb{R}^{w\\times N}$ is local minimum   \n411 of $A\\mapsto\\left\\lVert A\\sigma(A)^{+}\\right\\rVert_{F}^{2}$ that is not non-negative, then there is a continuous path $A_{t}$ of constant COI   \n412 such that $A_{0}={\\hat{A}}$ and $A_{1}$ is a saddle.   \n413 Proof. The local minimum $\\hat{A}$ leads to a pair of $N\\;\\times\\;N$ covariance matrices $\\begin{array}{r l}{\\hat{K}}&{{}=}\\end{array}$   \n414 $\\hat{A}^{T}\\bar{A}$ and $\\begin{array}{r l r}{\\hat{K}^{\\sigma}}&{{}=}&{\\sigma(\\hat{A})^{T}\\sigma(\\hat{A})}\\end{array}$ . The pair $(\\hat{K},\\hat{K}^{\\sigma})$ belongs to the conical hull   \n415 Cone $\\left\\{(\\hat{A}_{i\\cdot}\\hat{A}_{i\\cdot}^{T},\\sigma(\\hat{A}_{i\\cdot})\\sigma(\\hat{A}_{i\\cdot})^{T}):i=1,\\ldots,w\\right\\}$ . Since this cones lies in a $N(N+1)$ -dimensional   \n416 space (the space of pairs of symmetric $\\boldsymbol{N}^{'}\\times\\boldsymbol{N}$ matrices), we know by Caratheodory\u2019s   \n417 theorem (for convex cones) that there is a conical combination $(\\hat{K},\\hat{K}^{\\sigma}\\,\\stackrel{\\cdot}{-}\\,\\beta^{2}{\\bf1}_{N\\times N})\\stackrel{\\cdot}{\\ =}$   \n418 $\\begin{array}{r l}{\\sum_{i=1}^{w}a_{i}(\\hat{A}_{i}.\\hat{A}_{i\\cdot}^{T},\\sigma(\\hat{A}_{i\\cdot})\\sigma(\\hat{A}_{i\\cdot})^{T})}\\end{array}$ such that no more than $N(N+1)$ of the coefficients are non  \n419 zero. We now define $A_{t}$ to have lines $A_{t,i\\cdot}=\\sqrt{(1-t)+t a_{i}}\\hat{A}_{i}$ \u00b7, so that $A_{t=0}=\\hat{A}$ and at $t=1$ at   \n420 least one line of $A_{t=1}$ is zero (since at least one of the $a_{i}\\mathrm{s}$ is zero). First note that the covariance pairs   \n421 remain constant over the path: $\\begin{array}{r}{K_{t}=A_{t}^{T}A_{t}=\\sum_{i=1}^{w}((1-t)+t a_{i})\\hat{A}_{i}.\\hat{A}_{i}^{T}=(1-t)\\hat{K}+t\\hat{K}=\\hat{K}}\\end{array}$   \n422 and similarly $K_{t}^{\\sigma}\\,=\\,\\hat{K}^{\\sigma}$ , which implies that the cost $\\left\\|A_{t}\\sigma(A_{t})^{+}\\right\\|_{F}^{2}=\\operatorname{Tr}\\left[K_{t}K_{t}^{\\sigma+}\\right]$ is constant   \n423 too. Second, since a representation $A$ is non-negative iff the covariances satisfy $\\bar{K^{\\big}}=K^{\\sigma}$ , the   \n424 representation path $A_{t}$ cannot be non-negative either since it has the same kernel pairs $(\\hat{K},\\hat{K}^{\\sigma})$ with   \n425 $\\hat{K}\\neq\\hat{K}^{\\sigma}$ .   \n426 Now (the converse of) Proposition 2 tells us that if $A_{t=1}$ is not non-negative and has a zero line, then   \n427 it is not a local minimum, which implies that it is a saddle. \u53e3 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "428 A.2 Bottleneck ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "429 Theorem 7. For any uniformly bounded sequence $A_{p}^{\\tilde{L}}$ of geodesics, i.e. $\\left\\|A_{p}^{\\tilde{L}}\\right\\|^{2},\\left\\|B_{p}^{\\tilde{L}}\\right\\|^{2}\\leq c<\\infty$   \n430 and any $\\gamma>0$ , we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n-\\left(\\frac{1}{\\tilde{L}}\\ell_{\\gamma,\\tilde{L}}+\\sqrt{\\gamma}c\\right)^{2}\\leq-\\frac{2}{\\tilde{L}}\\mathcal{H}-\\operatorname*{min}_{p}\\left\\|A_{p}^{\\tilde{L}}\\right\\|_{(K_{p}+\\gamma I)}^{2}\\leq\\gamma c,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "431 for the path length \u2113\u03b3,L\u02dc = 01   \u2202pApL\u02dc   (Kp+\u03b3I) d p. Finally ", "page_idx": 11}, {"type": "equation", "text": "$$\n-\\tilde{L}\\sqrt{\\gamma c}\\leq\\left\\|\\partial_{p}A_{p}\\right\\|_{(K_{p}+\\gamma i)}-\\tilde{L}\\sqrt{\\left\\|A_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2}+\\frac{2}{\\tilde{L}}\\mathcal{H}}\\leq2\\tilde{L}\\sqrt{\\gamma c}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "432 Proof. First observe that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\frac{1}{\\tilde{L}}\\partial_{p}A_{p}+\\gamma B_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2}=\\left\\|B_{p}(K_{p}+\\gamma)-A_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2}}}\\\\ &{=\\left\\|B_{p}\\sigma(A_{p})^{T}\\right\\|^{2}+\\gamma\\left\\|B_{p}\\right\\|^{2}-2\\mathrm{Tr}\\left[B_{p}A_{p}^{T}\\right]+\\left\\|A_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2}}\\\\ &{=\\frac{2}{\\tilde{L}}\\mathcal{H}+\\gamma\\left\\|B_{p}\\right\\|^{2}+\\left\\|A_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "433 and thus we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n-\\frac{2}{\\tilde{L}}\\varkappa=\\|A_{p}\\|_{(K_{p}+\\gamma I)}^{2}-\\left\\|\\frac{1}{\\tilde{L}}\\partial_{p}A_{p}+\\gamma B_{p}\\right\\|_{(K_{p}+\\gamma I)}^{2}+\\gamma\\left\\|B_{p}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "434 (1) The upper bound $\\begin{array}{r}{-\\frac{2}{\\tilde{L}}\\mathcal{H}-\\operatorname*{min}_{p}\\left\\|A_{p}^{\\tilde{L}}\\right\\|_{(K_{p}+\\gamma I)}^{2}\\leq\\gamma c}\\end{array}$ then follows from the fact that $\\|B_{p}\\|^{2}\\leq c$ .   \n435 For the lower bound, first observe that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\tilde{L}}\\left\\lVert\\partial_{p}A_{p}\\right\\rVert_{(K_{p}+\\gamma I)}\\geq\\left\\lVert\\frac{1}{\\tilde{L}}\\partial_{p}A_{p}+\\gamma B_{p}\\right\\rVert_{(K_{p}+\\gamma I)}-\\left\\lVert\\gamma B_{p}\\right\\rVert_{(K_{p}+\\gamma I)}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\sqrt{\\left\\lVert A_{p}\\right\\rVert_{(K_{p}+\\gamma I)}^{2}+\\frac{2}{\\tilde{L}}\\mathcal{H}+\\gamma\\left\\lVert B_{p}\\right\\rVert^{2}}-\\sqrt{\\gamma c}}\\\\ &{\\qquad\\qquad\\geq\\sqrt{\\left\\lVert A_{p}\\right\\rVert_{(K_{p}+\\gamma I)}^{2}+\\frac{2}{\\tilde{L}}\\mathcal{H}}-\\sqrt{\\gamma c},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "436 and therefore ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac1{\\tilde{L}}\\ell_{\\gamma,\\tilde{L}}=\\frac1{\\tilde{L}}\\int_{0}^{1}\\left\\lVert\\partial_{p}A_{p}\\right\\rVert_{(K_{p}+\\gamma I)}d p}\\\\ {\\displaystyle\\ge\\int_{0}^{1}\\sqrt{\\|A_{p}\\|_{(K_{p}+\\gamma I)}^{2}+\\frac2{\\tilde{L}}\\mathcal H}-\\sqrt{\\gamma c}d p}\\\\ {\\displaystyle\\ge\\sqrt{\\operatorname*{min}_{p}\\left\\lVert A_{p}\\right\\rVert_{(K_{p}+\\gamma I)}^{2}+\\frac2{\\tilde{L}}\\mathcal H}-\\sqrt{\\gamma c}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "437 which implies the lower bound. ", "page_idx": 12}, {"type": "text", "text": "438 (2) The lower bound follows from equation 1. The upper bound follows from ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{\\tilde{L}}\\left\\|\\partial_{p}A_{p}\\right\\|_{(K_{p}+\\gamma I)}\\leq\\left\\|\\displaystyle\\frac1{\\tilde{L}}\\partial_{p}A_{p}+\\gamma B_{p}\\right\\|_{(K_{p}+\\gamma I)}+\\left\\|\\gamma B_{p}\\right\\|_{(K_{p}+\\gamma I)}}\\\\ {\\displaystyle\\leq\\sqrt{\\|A_{p}\\|_{(K_{p}+\\gamma I)}^{2}+\\frac{2}{\\tilde{L}}\\mathcal M+\\gamma\\left\\|B_{p}\\right\\|^{2}}+\\sqrt{\\gamma c}}\\\\ {\\displaystyle\\leq\\sqrt{\\|A_{p}\\|_{(K_{p}+\\gamma I)}^{2}+\\frac{2}{\\tilde{L}}\\mathcal M}+\\sqrt\\gamma\\left\\|B_{p}\\right\\|+\\sqrt{\\gamma c}}\\\\ {\\displaystyle\\leq\\sqrt{\\|A_{p}\\|_{(K_{p}+\\gamma I)}^{2}+\\frac{2}{\\tilde{L}}\\mathcal M}+2\\sqrt\\gamma c.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "439 ", "page_idx": 12}, {"type": "text", "text": "440 Proposition 8 (Proposition 5 in the main.). Let $A_{p}^{\\tilde{L}}$ be a uniformly bounded sequence of local minima   \n441 for increasing $\\tilde{L}$ , at any $p_{0}\\in(0,1)$ such that $\\|\\partial_{p}A_{p}\\|$ is uniformly bounded in a neighborhood of $p_{0}$   \n442 for all $\\tilde{L}$ , then $A_{p_{0}}^{\\infty}=\\operatorname*{lim}_{\\tilde{L}}A_{p_{0}}^{\\tilde{L}}$ is non-negative.   \n443 Proof. Given a path $A_{p}$ with corresponding weight matrices $W_{p}$ corresponding to a width $w$ , then   \n444 $\\left(\\begin{array}{c}{{A}}\\\\ {{0}}\\end{array}\\right)$ is a path with weight matrix $\\left(\\begin{array}{c c}{{W_{p}}}&{{0}}\\\\ {{0}}&{{0}}\\end{array}\\right)$ . Our goal is to show that for sufficiently large   \n445 depths, one can under certain assumptions slightly change the weights to obtain a new path with the   \n446 same endpoints but a slightly lower loss, thus ensuring that if certain assumptions are not satisfied   \n447 then the path cannot be locally optimal.   \n448 Let us assume that $\\|\\partial_{p}A_{p}\\|\\leq c_{1}$ in a neighborhood of a $p_{0}\\in(0,1)$ , and assume by contradiction   \n449 that there is an input index $i=1,\\ldots,N$ such that $A_{p_{0},\\cdot\\cdot i}$ has at least one negative entry, and therefore   \n450 $\\left\\|A_{p_{0},\\cdot i}\\right\\|^{2}-\\left\\|\\sigma(A_{p_{0},\\cdot i})\\right\\|^{2}=c_{0}>0$ for all $\\tilde{L}$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "451 We now consider the new weights ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c c}{W_{p}-\\tilde{L}\\epsilon^{2}t(p)A_{p,\\cdot i}\\sigma(A_{p,\\cdot i})^{T}}&{\\epsilon\\tilde{L}t(p)A_{p,\\cdot i}}\\\\ {\\epsilon\\tilde{L}t(p)\\sigma(A_{p,\\cdot i})}&{0}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "452 for $\\begin{array}{r}{t(p)=\\operatorname*{max}\\{0,1-\\frac{|p-p_{0}|}{r}\\}}\\end{array}$ a triangular function centered in $p_{0}$ and for an $\\epsilon>0$ . ", "page_idx": 12}, {"type": "text", "text": "453 For $\\epsilon$ and $r$ small enough, the parameter norm will decrease: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{1}\\left\\|\\begin{array}{l}{W_{p}-\\tilde{L}\\epsilon^{2}t(p)A_{p,\\cdot\\,i}\\sigma(A_{p,\\cdot\\,i})^{T}\\quad\\epsilon\\tilde{L}t(p)A_{p,\\cdot\\,i}}\\\\ {\\qquad\\qquad\\epsilon\\tilde{L}t(p)\\sigma(A_{p,\\cdot\\,i})\\qquad\\qquad0\\qquad}\\end{array}\\right\\|^{2}d p}\\\\ &{=\\int_{0}^{1}\\|W_{p}\\|^{2}+\\tilde{L}^{2}\\epsilon^{2}t(p)^{2}\\left(-\\frac{2}{\\tilde{L}}A_{p,\\cdot\\,i}^{T}W_{p}\\sigma(A_{p,\\cdot\\,i})+\\|A_{p,\\cdot\\,i}\\|^{2}+\\|\\sigma(A_{p,\\cdot\\,i})\\|^{2}\\right)d p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "454 Now since $W_{p}\\sigma(A_{p,\\cdot i})=\\partial_{p}A_{p,\\cdot i}+\\tilde{L}A_{p,\\cdot i}$ , this simplifies to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{0}^{1}\\|W_{p}\\|^{2}+\\tilde{L}^{2}\\epsilon^{2}t(p)^{2}\\left(-\\left\\|A_{p,\\cdot\\,i}\\right\\|^{2}+\\left\\|\\sigma(A_{p,\\cdot\\,i})\\right\\|^{2}-\\frac{1}{\\tilde{L}}A_{p,\\cdot\\,i}^{T}\\partial_{p}A_{p,\\cdot\\,i}\\right)d p+O(\\epsilon^{4}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "455 By taking $r$ small enough, we can guarantee that $\\begin{array}{r}{-\\left\\|A_{p,\\cdot i}\\right\\|^{2}+\\left\\|\\sigma(A_{p,\\cdot i})\\right\\|^{2}<-\\frac{c_{0}}{2}}\\end{array}$ for all $p$ such that   \n456 $t(p)>0$ , and for $\\tilde{L}$ large enough we can guarantee that $\\begin{array}{r}{\\left|{\\frac{1}{\\tilde{L}}}A_{p,\\cdot i}^{T}\\partial_{p}A_{p,\\cdot i}\\right|}\\end{array}$ is smaller then $\\frac{c_{0}}{4}$ , so that   \n457 we can guarantee that the parameter norm will be strictly smaller for $\\epsilon$ small enough. ", "page_idx": 13}, {"type": "text", "text": "458 We will now show that with these new weights the path becomes approximately $\\left(\\begin{array}{c}{{A_{p}}}\\\\ {{\\epsilon a_{p}}}\\end{array}\\right)$ where ", "page_idx": 13}, {"type": "equation", "text": "$$\na_{p}=\\tilde{L}\\int_{0}^{p}t(q)K_{p,i\\cdot}e^{\\tilde{L}(q-p)}d q.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "459 Note that $a_{p}$ is positive for all $p$ since $K_{p}$ has only positive entries. Also note that as $\\tilde{L}\\to\\infty$ ,   \n460 $a_{p}\\to t(p)\\dot{K}_{p,i}$ \u00b7 and so that $a_{0}\\to0$ and $a_{1}\\to1$ . ", "page_idx": 13}, {"type": "text", "text": "461 On one hand, we have the time derivative ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\partial_{p}\\left(\\begin{array}{c}{{A_{p}}}\\\\ {{\\epsilon a_{p}}}\\end{array}\\right)=\\left(\\begin{array}{c}{{W_{p}\\sigma(A_{p})-\\tilde{L}A_{p}}}\\\\ {{\\epsilon\\tilde{L}\\left(t(p)K_{p,i\\cdot}-a_{p}\\right)}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "462 On the other hand the actual derivative as determined by the new weights: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\begin{array}{c c}{W_{p}-\\tilde{L}\\epsilon^{2}t(p)A_{p,:}\\sigma(A_{p,:})^{T}}&{\\epsilon\\tilde{L}t(p)A_{p,:}i}\\\\ {\\epsilon\\tilde{L}t(p)\\sigma(A_{p,:}i)}&{0}\\end{array}\\right)\\left(\\begin{array}{c}{\\sigma(A_{p})}\\\\ {\\epsilon\\sigma(a_{p})}\\end{array}\\right)-\\tilde{L}\\left(\\begin{array}{c}{A_{p}}\\\\ {\\epsilon a_{p}}\\end{array}\\right)}\\\\ &{=\\left(\\begin{array}{c c}{W_{p}\\sigma(A_{p})-\\tilde{L}A_{p}-\\tilde{L}\\epsilon^{2}t(p)^{2}A_{p,:}K_{p,i}.+\\tilde{L}\\epsilon^{2}t(p)A_{p,:i}a_{p}}\\\\ {\\epsilon\\tilde{L}t(p)K_{p,i}.-\\epsilon\\tilde{L}a(p)}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "463 The only difference is the two terms ", "page_idx": 13}, {"type": "equation", "text": "$$\n-\\tilde{L}\\epsilon^{2}t(p)^{2}A_{p,\\cdot i}K_{i\\cdot}+\\tilde{L}\\epsilon^{2}t(p)A_{p,\\cdot i}a_{p}=\\tilde{L}\\epsilon^{2}t(p)A_{p,\\cdot i}\\left(t(p)K_{i\\cdot}-a_{p}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "464 One can guarantee with a Gr\u00f6nwall type of argument that the representation path resulting from the   \n465 new weights must be very close to the path $\\bar{\\left(\\begin{array}{c}{A_{p}}\\\\ {\\epsilon a_{p}}\\end{array}\\right)}$ \u53e3 ", "page_idx": 13}, {"type": "text", "text": "466 A.3 Balancedness ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "467 This paper will heavily focus on the Hamiltonian $\\mathcal{H}_{p}$ that is constant throughout the layers $p\\in[0,1]$ ,   \n468 and how it can be interpreted. Note that the Hamiltonian we introduce is distinct from an already   \n469 known invariant, which arises as the result of so-called balancedness, which we introduce now.   \n470 Though this balancedness also appears in ResNets, it is easiest to understand in fullyconnected   \n471 networks. First observe that for any neuron $i\\in1,\\dotsc,w$ at a layer $\\ell$ one can multiply the incoming   \n472 weights $(W_{\\ell,i\\cdot},b_{\\ell,i})$ by a scalar $\\alpha$ and divide the outcoming weights $W_{\\ell+1,\\cdot\\,i}$ by the same scalar   \n473 $\\alpha$ without changing the subsequent layers. One can easily see that the scaling that minimize the   \n474 contribution to the parameter norm is such that the norm of incoming weights equals the norm   \n475 of the outcoming weights $\\left\\|\\boldsymbol{W}_{\\ell,i\\cdot}\\right\\|^{2}+\\left\\|\\boldsymbol{b}_{\\ell,i}\\right\\|^{2}\\,=\\,\\left\\|\\boldsymbol{W}_{\\ell+1,\\cdot i}\\right\\|^{2}$ . Summing over the is we obtain   \n476 $\\left\\|W_{\\ell}\\right\\|_{F}^{2}+\\left\\|b_{\\ell}\\right\\|^{2}=\\left\\|W_{\\ell+1}\\right\\|_{F}^{2}$ and thus $\\begin{array}{r}{\\left\\|W_{\\ell}\\right\\|_{F}^{2}=\\left\\|W_{1}\\right\\|_{F}^{2}+\\sum_{k=1}^{\\ell-1}\\left\\|b_{k}\\right\\|_{F}^{2}}\\end{array}$ , which means that the   \n477 norm of the weights is increasing throughout the layers, and in t he absence of bias, it is even constant.   \n478 Leaky ResNet exhibit the same symmetry: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "479 Proposition 9. At any critical $W_{p}$ , we have $\\begin{array}{r}{\\|W_{p}\\|^{2}=\\|W_{0}\\|^{2}+\\tilde{L}\\int_{0}^{p}\\|W_{p,\\cdot w+1}\\|^{2}\\,d q.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "480 Proof. This proofs handles the bias $W_{p,\\cdot(w+1)}$ differently to the rest of the weights $W_{p,\\cdot(1:w)}$ , to   \n481 simplify notations, we write $V_{p}=W_{p,\\cdot(1:w)}$ and $b_{p}=W_{p,\\cdot(w+1)}$ for the bias.   \n482 First let us show that choosing the weight matrices $\\tilde{V}_{q}=r^{\\prime}(q)V_{r(q)}$ and bias $\\tilde{b}_{q}=r^{\\prime}(q)e^{\\tilde{L}(r(q)-q)}b_{r(q)}$   \n483 leads to the path $\\tilde{A}_{q}=e^{\\tilde{L}(r(q)-q)}A_{r(q)}$ . Indeed the path $\\tilde{A}_{q}=e^{\\tilde{L}(r(q)-q)}A_{r(q)}$ has the right value   \n484 when $p=0$ and it then satisfies the right differential equation: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{q}\\tilde{A}_{q}=\\tilde{L}(r^{\\prime}(q)-1)\\tilde{A}_{q}+e^{\\tilde{L}(r(q)-q)}r^{\\prime}(q)\\partial_{p}A_{r(q)}}\\\\ &{\\qquad=\\tilde{L}(r^{\\prime}(q)-1)\\tilde{A}_{q}+e^{\\tilde{L}(r(q)-q)}r^{\\prime}(q)\\left(-\\tilde{L}A_{r(q)}+V_{r(q)}\\sigma(A_{r(q)})+b_{r(q)}\\right)}\\\\ &{\\qquad=-\\tilde{L}\\tilde{Z}_{q}+r^{\\prime}(q)A_{r(q)}\\sigma\\left(\\tilde{Z}_{q}\\right)+e^{\\tilde{L}(r(q)-q)}r^{\\prime}(q)b_{r(q)}}\\\\ &{\\qquad=\\tilde{V}_{q}\\sigma\\left(\\tilde{A}_{q}\\right)+\\tilde{b}_{q}-\\tilde{L}\\tilde{A}_{q}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "485 The optimal reparametrization $r(q)$ is therefore the one that minimizes ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{0}^{1}\\left\\|\\tilde{W}_{q}\\right\\|^{2}+\\left\\|\\tilde{b}_{q}\\right\\|^{2}d q=\\int_{0}^{1}r^{\\prime}(q)^{2}\\left(\\left\\|W_{r(q)}\\right\\|^{2}+e^{2\\tilde{L}(r(q)-q)}\\left\\|b_{r(q)}\\right\\|^{2}\\right)d q\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "486 For the identity reparametrization $r(q)=q$ to be optimal, we need ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{0}^{1}2d r^{\\prime}(p)\\left(\\left\\|W_{p}\\right\\|^{2}+\\left\\|b_{p}\\right\\|^{2}\\right)+2\\tilde{L}d r(p)\\left\\|b_{p}\\right\\|^{2}d p=0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "487 for all $d r(q)$ with $d r(0)=d r(1)=0$ . Since ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{0}^{1}d r^{\\prime}(p)\\left(\\left\\|W_{p}\\right\\|^{2}+\\left\\|b_{p}\\right\\|^{2}\\right)d p=-\\int_{0}^{1}d r(p)\\partial_{p}\\left(\\left\\|W_{p}\\right\\|^{2}+\\left\\|b_{p}\\right\\|^{2}\\right)d q,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "488 we need ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{0}^{1}d r(p)\\left[-\\partial_{p}\\left(\\left\\|W_{p}\\right\\|^{2}+\\left\\|b_{p}\\right\\|^{2}\\right)+\\tilde{L}\\left\\|b_{p}\\right\\|^{2}\\right]d p=0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "489 and thus for all $p$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial_{p}\\left(\\left\\|W_{p}\\right\\|^{2}+\\left\\|b_{p}\\right\\|^{2}\\right)=\\tilde{L}\\left\\|b_{p}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "490 Integrating, we obtain as needed ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|W_{p}\\|^{2}+\\|b_{p}\\|^{2}=\\|W_{0}\\|^{2}+\\|b_{0}\\|^{2}+\\tilde{L}\\int_{0}^{p}\\|b_{q}\\|^{2}\\,d q.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "491 ", "page_idx": 14}, {"type": "text", "text": "492 B Experimental Setup ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "493 Our experiments make use of synthetic data to train leaky ResNets so that the Bottleneck rank $k^{*}$ is   \n494 known for our experiments. The synthetic data is generated by teacher networks for a given true rank   \n495 $k^{*}$ . To construct a bottleneck, the teacher network is a composition of networks for which the the   \n496 inner-dimension is $k^{*}$ . Our experiments used an input and output dimension of 30, and a bottleneck   \n497 of $k^{*}=3$ . For data, we sampled a thousand data points for training, and another thousand for testing   \n498 which are collectively augmented by demeaning and normalization.   \n499 To train the leaky ResNets, it is important for them to be wide, usually wider than the input or output   \n500 dimension, we opted for a width of 100. However, the width of the representation must be constant   \n501 to implement leaky residual connections, so we introduce a single linear mapping at the start, and   \n502 another at the end, of the forward pass to project the representations into a higher dimension for the   \n503 paths. These linear mappings can be either learned or fixed.   \n504 To achieve a tight convergence in training, we train primarily using Adam using Mean Squared Error   \n505 as a loss function, and our custom weight decay function. After training on Adam (we found 5000   \n506 epochs to work well), we then train briefly (usually 1000 epochs) using SGD with a smaller learning   \n507 rate to tighten the convergence.   \n508 The bottleneck structure of a trained network, as seen in Figure 1b and 2b, can be observed in the   \n509 spectra of both the representations $A_{p}$ and the weight matrices $W_{p}$ at each layer. As long as the   \n551110 tnruaimnibnegr  iosf  nloatr goev evra-lrueegsu laasr itzheed  r( $\\lambda$ tt odoe claary.g eI) nt hoeunr  tehxep sepriemcteran trse, $\\begin{array}{r}{\\lambda=\\frac{0.001}{\\tilde{L}}}\\end{array}$ rt os egpeatr agtoioond  breetswuletse.n $k^{*}$   \n512 facilitate the formation of the bottleneck structure, $L$ should be large, for our experiments we usually   \n513 use $L=20$ . Figure 2a shows how larger $L$ , which have better separation between large and small   \n514 singular values, lead to improved test performance.   \n515 As first noted in section 1.3, solving for the Cost Of Identity, the kinetic energy, and the Hamiltonian   \n516 $\\mathcal{H}$ is difficult due to the instability of the pseudo-inverse. Although the relaxation $(K_{p}+\\gamma I)$ improves   \n517 the stability, we also utilize the solve function to avoid computing a pseudo-inverse altogether. The   \n518 stability of these computations rely on the boundedness of some additional properties: the path length   \n519 $\\displaystyle\\int||\\partial_{p}\\bar{A}_{p}||~d p$ , as well as the magnitudes of $B_{p}$ , and $B_{p}\\sigma(A_{p})^{T}$ from the Hamiltonian reformulation.   \n520 Figure 3 shows how their respective magnitudes remains relatively constant as the effective depth $\\tilde{L}$   \n521 grows.   \n522 For compute resources, these small networks are not particularly resource intensive. Even on a CPU,   \n523 it only takes a couple minutes to fully train a leaky ResNet. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "RZk2rxJT55/tmp/78e1913b6025c64ac5b4e63f441bf048d96985e6bcdc43e499c83c217ed9d60a.jpg", "img_caption": ["Figure 3: Various properties of the Hamiltonian dynamics of Leaky ResNets which remain bounded "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "524 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "528 Answer: [Yes]   \n529 Justification: The contribution section accurately describes our contributions, and all   \n530 theorems/propositions are proven in the main or the appendix.   \n531 Guidelines:   \n532 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n533 made in the paper.   \n534 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n535 contributions made in the paper and important assumptions and limitations. A No or   \n536 NA answer to this question will not be perceived well by the reviewers.   \n537 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n538 much the results can be expected to generalize to other settings.   \n539 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n540 are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "541 2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We discuss limitations of our results and approach after we state them. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "5   \n6 \u2022 The answer NA means that the paper has no limitation while the answer No means that 7 the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n9 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n0 violations of these assumptions (e.g., independence assumptions, noiseless settings, 1 model well-specification, asymptotic approximations only holding locally). The authors   \n2 should reflect on how these assumptions might be violated in practice and what the 3 implications would be. 4 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was 5 only tested on a few datasets or with a few runs. In general, empirical results often 6 depend on implicit assumptions, which should be articulated. 7 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n8 For example, a facial recognition algorithm may perform poorly when image resolution 9 is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n62 \u2022 The authors should discuss the computational efficiency of the proposed algorithms 3 and how they scale with dataset size.   \n4 \u2022 If applicable, the authors should discuss possible limitations of their approach to 5 address problems of privacy and fairness.   \n6 \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers   \n8 discover limitations that aren\u2019t acknowledged in the paper. The authors should use 9 their best judgment and recognize that individual actions in favor of transparency play 0 an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "572 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "573 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n574 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "76 Justification: All assumptions are stated in the Theorem statements.   \n77 Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "588 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the 90 main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The experimental setup is described in the Appendix. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "630 Answer: [No]   \n631 Justification: We use synthetic data, with a description of how to build this synthetic data.   \n632 The code is not the main contribution of the paper, so there is little reason to publish it.   \n633 Guidelines:   \n634 \u2022 The answer NA means that paper does not include experiments requiring code.   \n635 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n636 public/guides/CodeSubmissionPolicy) for more details.   \n637 \u2022 While we encourage the release of code and data, we understand that this might not be   \n638 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n639 including code, unless this is central to the contribution (e.g., for a new open-source   \n640 benchmark).   \n641 \u2022 The instructions should contain the exact command and environment needed to run to   \n642 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n643 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n644 \u2022 The authors should provide instructions on data access and preparation, including how   \n645 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n646 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n647 proposed method and baselines. If only a subset of experiments are reproducible, they   \n648 should state which ones are omitted from the script and why.   \n649 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n650 versions (if applicable).   \n651 \u2022 Providing as much information as possible in supplemental material (appended to the   \n652 paper) is recommended, but including URLs to data and code is permitted.   \n653 6. Experimental Setting/Details   \n654 Question: Does the paper specify all the training and test details (e.g., data splits,   \n655 hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand   \n656 the results?   \n657 Answer: [Yes]   \n658 Justification: Most details are given in the experimental setup section in the Appendix.   \n659 Guidelines:   \n660 \u2022 The answer NA means that the paper does not include experiments.   \n661 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n662 that is necessary to appreciate the results and make sense of them.   \n663 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n664 material.   \n665 7. Experiment Statistical Significance   \n666 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n667 information about the statistical significance of the experiments?   \n668 Answer: [No]   \n669 Justification: The numerical experiments are mostly there as a visualization of the theoretical   \n670 results, our main goal is therefore clarity, which would be hurt by putting error bars   \n671 everywhere.   \n672 Guidelines:   \n673 \u2022 The answer NA means that the paper does not include experiments.   \n674 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars,   \n675 confidence intervals, or statistical significance tests, at least for the experiments that   \n676 support the main claims of the paper.   \n677 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n678 example, train/test split, initialization, random drawing of some parameter, or overall   \n679 run with given experimental conditions).   \n680 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n681 call to a library function, bootstrap, etc.)   \n682 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n683 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n684 of the mean.   \n685 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n686 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n687 of Normality of errors is not verified.   \n688 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n689 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n690 error rates).   \n691 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n692 they were calculated and reference the corresponding figures or tables in the text.   \n693 8. Experiments Compute Resources   \n694 Question: For each experiment, does the paper provide sufficient information on the   \n695 computer resources (type of compute workers, memory, time of execution) needed to   \n696 reproduce the experiments?   \n697 Answer: [Yes]   \n698 Justification: In the experimental setup section of the Appendix.   \n699 Guidelines:   \n700 \u2022 The answer NA means that the paper does not include experiments.   \n701 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n702 or cloud provider, including relevant memory and storage.   \n703 \u2022 The paper should provide the amount of compute required for each of the individual   \n704 experimental runs as well as estimate the total compute.   \n705 \u2022 The paper should disclose whether the full research project required more compute   \n706 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n707 didn\u2019t make it into the paper).   \n708 9. Code Of Ethics   \n709 Question: Does the research conducted in the paper conform, in every respect, with the   \n710 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n711 Answer: [Yes]   \n712 Justification: We have read the Code of Ethics and see no issue.   \n713 Guidelines:   \n714 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n715 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n716 deviation from the Code of Ethics.   \n717 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special   \n718 consideration due to laws or regulations in their jurisdiction).   \n719 10. Broader Impacts   \n720 Question: Does the paper discuss both potential positive societal impacts and negative   \n721 societal impacts of the work performed?   \n722 Answer: [NA]   \n723 Justification: The paper is theoretical in nature, so it has no direct societal impact that can   \n724 be meaningfully discussed.   \n725 Guidelines:   \n726 \u2022 The answer NA means that there is no societal impact of the work performed.   \n727 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n728 impact or why the paper does not address societal impact.   \n729 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n730 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n731 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n732 groups), privacy considerations, and security considerations.   \n733 \u2022 The conference expects that many papers will be foundational research and not tied   \n734 to particular applications, let alone deployments. However, if there is a direct path to   \n735 any negative applications, the authors should point it out. For example, it is legitimate   \n736 to point out that an improvement in the quality of generative models could be used to   \n737 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n738 that a generic algorithm for optimizing neural networks could enable people to train   \n739 models that generate Deepfakes faster.   \n740 \u2022 The authors should consider possible harms that could arise when the technology is   \n741 being used as intended and functioning correctly, harms that could arise when the   \n742 technology is being used as intended but gives incorrect results, and harms following   \n743 from (intentional or unintentional) misuse of the technology.   \n744 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n745 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n746 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n747 feedback over time, improving the efficiency and accessibility of ML).   \n748 11. Safeguards   \n749 Question: Does the paper describe safeguards that have been put in place for responsible   \n750 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n751 image generators, or scraped datasets)?   \n752 Answer: [NA]   \n753 Justification: Not relevant to our paper.   \n754 Guidelines:   \n755 \u2022 The answer NA means that the paper poses no such risks.   \n756 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n757 necessary safeguards to allow for controlled use of the model, for example by requiring   \n758 that users adhere to usage guidelines or restrictions to access the model or implementing   \n759 safety filters.   \n760 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n761 should describe how they avoided releasing unsafe images.   \n762 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n763 not require this, but we encourage authors to take this into account and make a best   \n764 faith effort.   \n765 12. Licenses for existing assets   \n766 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n767 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n768 properly respected?   \n769 Answer: [NA]   \n770 Justification: We only use our own synthetic data.   \n771 Guidelines:   \n772 \u2022 The answer NA means that the paper does not use existing assets.   \n773 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n774 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n775 URL.   \n776 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n777 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n778 service of that source should be provided.   \n779 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n780 package should be provided. For popular datasets, paperswithcode.com/datasets   \n781 has curated licenses for some datasets. Their licensing guide can help determine the   \n782 license of a dataset.   \n783 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n784 the derived asset (if it has changed) should be provided.   \n785 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n786 the asset\u2019s creators.   \n787 13. New Assets   \n788 Question: Are new assets introduced in the paper well documented and is the documentation   \n789 provided alongside the assets?   \n790 Answer: [NA]   \n791 Justification: We do not release any new assets.   \n792 Guidelines:   \n793 \u2022 The answer NA means that the paper does not release new assets.   \n794 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n795 submissions via structured templates. This includes details about training, license,   \n796 limitations, etc.   \n797 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n798 asset is used.   \n799 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n800 create an anonymized URL or include an anonymized zip file.   \n801 14. Crowdsourcing and Research with Human Subjects   \n802 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n803 include the full text of instructions given to participants and screenshots, if applicable, as   \n804 well as details about compensation (if any)?   \n805 Answer: [NA]   \n806 Justification: Not relevant to this paper.   \n807 Guidelines:   \n808 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n809 human subjects.   \n810 \u2022 Including this information in the supplemental material is fine, but if the main   \n811 contribution of the paper involves human subjects, then as much detail as possible   \n812 should be included in the main paper.   \n813 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n814 or other labor should be paid at least the minimum wage in the country of the data   \n815 collector.   \n16 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n17 Subjects   \n18 Question: Does the paper describe potential risks incurred by study participants, whether   \n19 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n820 approvals (or an equivalent approval/review based on the requirements of your country or   \n21 institution) were obtained?   \n22 Answer: [NA]   \n23 Justification: Not relevant to this paper.   \n24 Guidelines:   \n25 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n826 human subjects.   \n827 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n828 may be required for any human subjects research. If you obtained IRB approval, you   \n29 should clearly state this in the paper.   \n830 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n831 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n832 guidelines for their institution.   \n833 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n834 applicable), such as the institution conducting the review. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}]