[{"figure_path": "RZk2rxJT55/figures/figures_5_1.jpg", "caption": "Figure 1: Leaky ResNet structures: We train equidistant networks with a fixed L = 20 over a range of effective depths L. The true function f* : R30 \u2192 R30 is the composition of two random FCNNS g1, g2 mapping from dim. 30 to 3 to 30. (a) Estimates of the Hamiltonian constants for networks trained with different L. The Hamiltonian refers to \u2212H which estimates the true rank k*. The COI refers to minp ||Ap||. The trend line follows the median estimate for \u2212H across each network's layers, whereas the error bars signify its minimum and maximum over p \u2208 [0,1]. The \"stable\" Hamiltonians utilize the relaxation from Theorem 4. (b) Spectra of the representations Ap and weights Wp respectively for L = 7. (c) Hamiltonian dynamics of the network in (b).", "description": "This figure shows the results of training leaky ResNets with different effective depths (L).  Panel (a) compares different estimates of the Hamiltonian, including the cost of identity (COI), for varying L, illustrating the relationship between these quantities and the true rank (k*).  Panel (b) displays the spectral properties of the representations (Ap) and weights (Wp) for a specific depth (L=7), highlighting the low dimensionality of the representations.  Panel (c) presents the Hamiltonian dynamics of the network from (b), illustrating the separation of timescales. ", "section": "2 Bottleneck Structure in Representation Geodesics"}, {"figure_path": "RZk2rxJT55/figures/figures_5_2.jpg", "caption": "Figure 1: Leaky ResNet structures: We train equidistant networks with a fixed L = 20 over a range of effective depths L. The true function f* : R30 \u2192 R30 is the composition of two random FCNNS 91, 92 mapping from dim. 30 to 3 to 30. (a) Estimates of the Hamiltonian constants for networks trained with different L. The Hamiltonian refers to \u2212H which estimates the true rank k*. The COI refers to minp ||Ap||. The trend line follows the median estimate for - across each network's layers, whereas the error bars signify its minimum and maximum over p \u2208 [0,1]. The \"stable\" Hamiltonians utilize the relaxation from Theorem 4. (b) Spectra of the representations Ap and weights Wp respectively for L = 7. (c) Hamiltonian dynamics of the network in (b).", "description": "This figure shows results from training Leaky ResNets with varying effective depth (L).  Panel (a) shows how Hamiltonian constants (estimating the true rank, k*) and the Cost of Identity (COI) change with L, demonstrating the separation of timescales. Panel (b) displays the spectra of representations and weights, illustrating the bottleneck structure. Panel (c) visualizes the Hamiltonian dynamics of a network.", "section": "1.5 Hamiltonian Reformulation"}, {"figure_path": "RZk2rxJT55/figures/figures_5_3.jpg", "caption": "Figure 1: Leaky ResNet structures: We train equidistant networks with a fixed L = 20 over a range of effective depths L. The true function f* : R30 \u2192 R30 is the composition of two random FCNNS g1, g2 mapping from dim. 30 to 3 to 30. (a) Estimates of the Hamiltonian constants for networks trained with different L. The Hamiltonian refers to \u2212H which estimates the true rank k*. The COI refers to minp ||Ap||. The trend line follows the median estimate for \u2212H across each network's layers, whereas the error bars signify its minimum and maximum over p \u2208 [0,1]. The \"stable\" Hamiltonians utilize the relaxation from Theorem 4. (b) Spectra of the representations Ap and weights Wp respectively for L = 7. (c) Hamiltonian dynamics of the network in (b).", "description": "This figure consists of three subfigures showing different aspects of Leaky ResNets. Subfigure (a) shows Hamiltonian measures across different effective depths, subfigure (b) illustrates the bottleneck structure of the network through spectral analysis of representations and weights, and subfigure (c) visualizes the Hamiltonian dynamics of a specific network.", "section": "1.5 Hamiltonian Reformulation"}, {"figure_path": "RZk2rxJT55/figures/figures_7_1.jpg", "caption": "Figure 2: Discretization: We train networks with a fixed L = 3 over a range of depths L and definitions of pes. The true function f* : R30 \u2192 R30 is the composition of three random ResNets 91, 92, 93 mapping from dim. 30 to 6 to 3 to 30. (a) Test error as a function of L for different discretization schemes. (b) Weight spectra across layers for adaptive pe (L = 18), grey vertical lines represents the steps pe (c) 2D projection of the representation paths Ap for L = 18. Observe how adaptive pes appears to better spread out the steps.", "description": "This figure shows the results of experiments to study the effect of different discretization schemes on the performance of Leaky ResNets.  Panel (a) compares the test error for equidistant, irregular, and adaptive discretization schemes across various effective depths (L). Panel (b) displays weight spectra for adaptive discretization with L=18, highlighting the distribution of weights across layers. Finally, Panel (c) visualizes the representation paths for L=18 under adaptive discretization, demonstrating how these steps are better distributed compared to other methods.", "section": "Discretization Scheme"}, {"figure_path": "RZk2rxJT55/figures/figures_7_2.jpg", "caption": "Figure 1: Leaky ResNet structures: We train equidistant networks with a fixed \\(\\hat{L} = 20\\) over a range of effective depths \\(L\\). The true function \\(f^* : \\mathbb{R}^{30} \\to \\mathbb{R}^{30}\\) is the composition of two random FCNNS \\(g_1, g_2\\) mapping from dim. 30 to 3 to 30. (a) Estimates of the Hamiltonian constants for networks trained with different \\(L\\). The Hamiltonian refers to \\(-H\\) which estimates the true rank \\(k^*\\). The COI refers to \\(\\min_p ||A_p||_{K_p^+}\\). The trend line follows the median estimate for \\(-H\\) across each network's layers, whereas the error bars signify its minimum and maximum over \\(p \\in [0, 1]\\). The \\\"stable\\\" Hamiltonians utilize the relaxation from Theorem 4. (b) Spectra of the representations \\(A_p\\) and weights \\(W_p\\) respectively for \\(L = 7\\). (c) Hamiltonian dynamics of the network in (b).", "description": "This figure shows the results of training Leaky ResNets with different effective depths (L).  Subfigure (a) compares Hamiltonian constants (estimating the true rank k*), Cost of Identity (COI), and kinetic energy across various L. Subfigure (b) presents the spectra of the representations and weights for L=7, illustrating the bottleneck structure. Subfigure (c) displays the Hamiltonian dynamics of the network.", "section": "Bottleneck Structure in Representation Geodesics"}, {"figure_path": "RZk2rxJT55/figures/figures_7_3.jpg", "caption": "Figure 2: Discretization: We train networks with a fixed L = 3 over a range of depths L and definitions of pes. The true function f* : R30 \u2192 R30 is the composition of three random ResNets g1, g2, g3 mapping from dim. 30 to 6 to 3 to 30. (a) Test error as a function of L for different discretization schemes. (b) Weight spectra across layers for adaptive pe (L = 18), grey vertical lines represents the steps pe (c) 2D projection of the representation paths Ap for L = 18. Observe how adaptive pes appears to better spread out the steps.", "description": "This figure demonstrates the impact of different discretization schemes (equidistant, irregular, adaptive) on the training of leaky ResNets.  Subfigure (a) shows test error versus depth (L) for each scheme. Subfigure (b) displays weight spectra across layers for the adaptive scheme (L=18), highlighting the layer steps. Finally, subfigure (c) provides a 2D projection of the representation paths (Ap) for L=18, illustrating how the adaptive scheme effectively distributes the steps.", "section": "Discretization Scheme"}, {"figure_path": "RZk2rxJT55/figures/figures_15_1.jpg", "caption": "Figure 3: Various properties of the Hamiltonian dynamics of Leaky ResNets which remain bounded", "description": "This figure shows various properties of the Hamiltonian dynamics of Leaky ResNets, demonstrating that key quantities remain bounded as the effective depth L increases.  Specifically, it plots the path length (for different values of \u03b3), the maximum norm of B, the maximum norm of \u03c3(Ap), and the maximum product of the norms of \u03c3(Ap) and B, all against the effective depth L. The bounded nature of these quantities suggests a stability in the system's dynamics as the network deepens.", "section": "2.1 Separation of Timescales"}]