[{"type": "text", "text": "Scalable Ensemble Diversification for OOD Generalization and Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Training a diverse ensemble of models has several practical application scenarios,   \n2 such as model selection for out-of-distribution (OOD) generalization and the   \n3 detection of OOD samples via Bayesian principles. Previous approaches to diverse   \n4 ensemble training have relied on the framework of letting the models make the   \n5 correct predictions for the given in-distribution (ID) data while letting them come up   \n6 with different hypotheses for the OOD data. As such, they require well-separated   \n7 ID and OOD datasets to ensure a performant and diverse ensemble and have   \n8 only been verified in smaller-scale lab environments where such a separation is   \n9 readily available. In this work, we propose a framework, Scalable Ensemble   \n10 Diversification (SED), for scaling up existing diversification methods to large-scale   \n11 datasets and tasks (e.g. ImageNet), where the ID-OOD separation may not be   \n12 available. SED automatically identifies OOD samples within the large-scale ID   \n13 dataset on the fly and encourages the ensemble to make diverse hypotheses on   \n14 them. To make SED more suitable for large-scale applications, we propose an   \n15 algorithm to speed up the expensive pairwise disagreement computation. We verify   \n16 the resulting diversification of the ensemble on ImageNet and demonstrate the   \n17 benefit of diversification on the OOD generalization and OOD detection tasks.   \n18 In particular, for OOD detection, we propose a novel uncertainty score estimator   \n19 based on the diversity of ensemble hypotheses, which lets SED surpass all the   \n20 considered baselines in OOD detection task. Code will be available soon. ", "page_idx": 0}, {"type": "text", "text": "21 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "22 Training a diverse ensemble of models is useful in multiple applications. Diverse ensembles are used   \n23 to enhance out-of-distribution (OOD) generalization, where strong spurious features learned from   \n24 the in-distribution (ID) training data hinder generalization [30, 31, 28, 23]. By learning multiple   \n25 hypotheses, the ensemble is given a chance to learn causal features that are otherwise overshadowed   \n26 by the prominent spurious features [39, 4]. In Bayesian machine learning, diversification of the   \n27 posterior samples has been studied as a means to improve the precision and efficiency of sample   \n28 uncertainty estimates [5, 37].   \n29 A common strategy to train a diverse ensemble is to introduce two objectives: one for the main   \n30 task and one for diversification [29, 5, 28, 23]. The main task loss, such as the cross-entropy loss   \n31 for classification, encourages the hypotheses to solve the task on the labeled ID training set. The   \n32 diversification loss encourages the hypotheses to diversify the responses on an unlabelled OOD   \n33 dataset [28, 23] (Figure 1). The datasets for the objectives are separated to avoid contradictory   \n34 objectives: prediction diversification on the ID set will encourage wrong answers if there is only one   \n35 correct label.   \n36 This strategy, however, requires a separate OOD dataset where the hypotheses may make diverse   \n37 predictions without harming the main task performance on the ID training samples. Previous work   \n38 has thus been tested on hypothetical lab settings where the spurious and causal features can easily be   \n39 controlled to secure separate ID and OOD datasets for diverse ensemble training. It is not clear yet   \n40 how one could diversify an ensemble of models for realistic, uncontrolled, and large-scale applications   \n41 (e.g. ImageNet scale) where collecting a separate OOD dataset can be very costly, if not impossible.   \n42 To address the scalability challenge,   \n43 we propose a novel diversification   \n44 framework, Scalable Ensemble Diver  \n45 sification (SED, Figure 1). We intro  \n46 duce three ingredients. (1) OOD sam  \n47 ples are dynamically selected from the   \n48 ID training samples, on which the mod  \n49 els are trained to make different predic  \n50 tions. (2) At each iteration, a subset of   \n51 model pairs are stochastically selected   \n52 to construct the disagreement objec  \n53 tive, rather than the full list of model   \n54 pairs. (3) Deep networks are trained   \n55 to diversify only a few layers at the   \n56 end, rather than the full networks. This   \n57 framework allows scaling up existing   \n58 ensemble diversification methods. In   \n59 this work, we focus on scaling up the   \n60 Agree to Disagree (A2D) method [28].   \n61 We verify that SEDdiversifies a model   \n62 ensemble trained on ImageNet. We   \n63 demonstrate the benefit of diversifica  \n64 tion on OOD generalization and OOD   \n65 detection tasks. For the former, we showcase the usage of SED-diversified ensemble in three variants:   \n66 (a) vanilla ensemble of prediction probabilities [22], (b) an average of the model weights through   \n67 model soup [38], and (c) the oracle selection of the individual models for each OOD test set [23, 30].   \n68 In all three cases, SEDachieves a superior generalization to OOD datasets like ImageNet-A/R/C,   \n69 OpenImages, and iNaturalist.   \n70 For OOD detection, we seek multiple ways to use the SED-diversified ensemble: (a) treating them as   \n71 samples of the Bayesian posterior and (b) using our novel OODness estimate of Predictive Diversity   \n72 Score (PDS) that measures the diversity of predictions from an ensemble. We show that PDS provides   \n73 a superior detection of OOD samples like ImageNet-A/R/C, OpenImages, and iNaturalist. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "BQEOJZ0aSD/tmp/af5912be5fbfac8aceeda0ab696005d64b78025551911d49255fad09cd49d560.jpg", "img_caption": ["Figure 1: Existing diversification work vs SED. Unlike previous diversification approaches that require a separate OOD dataset on which the models are trained to diverge, our Scalable Ensemble Diversification (SED) operates on a single ID dataset where OOD samples are dynamically identified and are used to let the ensemble members diverge. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "74 Our contributions are ", "page_idx": 1}, {"type": "text", "text": "75 1. Scalable Ensemble Diversification (SED) framework that scales up existing ensemble   \n76 methods;   \n77 2. Predictive Diversity Score (PDS) that computes the OODness score for samples based on   \n78 ensemble prediction diversity;   \n79 3. First demonstration of the ensemble diversification and its application to OOD generalization   \n80 and detection at ImageNet level. ", "page_idx": 1}, {"type": "text", "text": "81 The code will be released with the next versions of the manuscript. ", "page_idx": 1}, {"type": "text", "text": "82 2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "83 In this section, we give a short overview of ensembling methods. At first, we speak about ensembles   \n84 in general and the role of diversity in them $(\\S\\,2.1)$ , then we focus on ensembling methods for neural   \n85 networks and separate them into two big groups. The first group includes algorithms that use loss   \n86 regularizers $(\\S\\,2.2)$ and the second group covers works that do not modify the training loss $(\\S\\,2.3)$ .   \n88 Ensembling is a powerful technique of aggregating the outputs of multiple models to make more   \n89 accurate predictions and it has been around for decades [12, 21, 18, 2, 3]. It is well known that   \n90 diversity in ensemble members\u2019 outputs leads to better performance of the ensemble compared to the   \n91 performance of a single model [21] because ensemble members make independent errors [12, 11].   \n92 Therefore, one way to reduce DNNs\u2019 reliance on spurious correlations is to train multiple models   \n93 on the same task and make them diverse in terms of errors they make so that their ensemble is less   \n94 dependent on such correlations. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "95 2.2 Neural network ensembles that promote diversity through loss regularizers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "96 Diversity in models can be induced by supplying training loss with a suitable regularizer.   \n97 Such regularizers can diversify models\u2019 weights [5, 7, 34, 6], features [39, 4], input gradients   \n98 [29, 30, 31, 33] and outputs [25, 5, 28, 23].   \n99 Notably, in [5] authors showed that regularizer of a certain structure that repulses ensemble members\u2019   \n100 weights or outputs leads to ensembles that provide a better approximation of Bayesian Model   \n101 Averaging. This idea was later extended by works that repulse ensemble members\u2019 features [39] and   \n102 input gradients [33].   \nSince the ensemble performs better due to the diversity of errors that ensemble members make   \n104 [21] we want those members to give pairwise different outputs for the same inputs. Unfortunately,   \n105 diversity in weights space, input gradient space, or features space does not guarantee such property   \n106 without additional assumptions due to functional symmetry which means that models can be different   \n107 in terms of their weights or feature maps and input gradients they produce but still give the same   \n108 outputs for a given input. That is why we are focused on methods that diversify models\u2019 outputs,   \n109 specifically [28, 23] which are state-of-the-art according to [1] and use regularizer of repulsive nature   \n110 conceptually similar to [5]. ", "page_idx": 2}, {"type": "text", "text": "111 2.3 Neural network ensembles that promote diversity without modifying loss ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "112 In addition to loss regularizers, there were an uncountable number of different ways to induce diversity   \n113 in ensembles of neural networks that did not modify the training loss. The most straightforward   \n114 approach of independently training multiple models of the same architecture by changing only random   \n115 seeds is called Deep Ensemble [22] which was extended from the Bayesian perspective in [37].   \n116 Another solution is to construct an ensemble from models trained with different hyperparameters [36],   \n117 augmentations [24], or architectures [40]. More computationally efficient direction allows training   \n118 only one base model inducing diversity by ensembling either checkpoints saved in different local   \n119 minima along the training trajectory of this base model [19] or models produced by the base model   \n120 after applying dropout [10] or masking [9] to it. The mixture of experts paradigm can also be viewed   \n121 as an ensemble diversification technique [41] where diversification happens due to assigning different   \n122 training samples to different ensemble members.   \n123 Despite their conceptual simplicity Deep Ensembles [22] and ensembles of models trained with   \n124 different hyperparameters [36] are strong baselines for OOD detection [27] and OOD generalization   \n125 tasks, especially when combined with model souping techniques [38]. That is why we selected them   \n126 as baselines for our experiments. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "127 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "128 We present our main technical contributions, Scalable Ensemble Diversification (SED, $\\S3.2)$ and the   \n129 Predictive Diversity Score (PDS, $\\S3.3)$ . ", "page_idx": 2}, {"type": "text", "text": "130 3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "131 We cover background materials before introducing our main technical contributions. We work with   \n132 a training set $\\bar{D}:=\\{x_{n},y_{n}\\}_{n=1}^{N}$ , which we refer to as the in-distribution (ID) dataset. For prior   \n133 diversification methods, we also assume the existence of a separate, unlabeled out-of-distribution ", "page_idx": 2}, {"type": "text", "text": "134 (OOD) dataset Dood :C=  {xono d}nN=oo1d. We write $f(\\cdot,\\theta)$ for a deep neural network classifier parametrized indicates the logit outputs for $C$ classes for input $x$ . We write $p(x)\\;:=\\;$ 136 $\\mathrm{Softmax}(f(x))\\in[0,1]^{C}$ for the probability outputs. We consider an ensemble $\\{f^{1},\\cdot\\cdot\\cdot,f^{\\bar{M}}\\}$ of $M$ 137 models. ", "page_idx": 3}, {"type": "text", "text": "138 3.1.1 Existing ensemble diversification approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "139 We introduce an existing approach for diversifying an ensemble of models [28, 23]. Two objectives   \n140 are imposed upon the ensemble of models: the main task loss and the diversification regularization.   \n141 For the main task, the community has focused on the classification task. The cross-entropy loss   \n142 $-\\log p_{y}(x;\\theta)$ is used to train the model ensemble $\\{f^{1},\\cdot\\cdot\\cdot,f^{M}\\}$ on the ID dataset $\\mathcal{D}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{main}}=\\frac{1}{M N}\\sum_{n}\\sum_{m}-\\log p_{y_{n}}^{m}(x_{n};\\theta).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "143 This encourages each member of the ensemble to behave similarly on the ID dataset. ", "page_idx": 3}, {"type": "text", "text": "144 Different diversification schemes use different diversification regularization loss $\\mathscr{L}_{\\mathrm{div}}$ applied on pairs   \n145 $(f^{m},f^{l})$ of ensemble members. The diversification objective is commonly optimized on the OOD   \n146 dataset $\\mathcal{D}^{\\mathrm{{ood}}}$ to encourage the training of multiple hypotheses on the OOD samples while avoiding   \n147 clashes with the main task objective. In this work, we focus on the Agree to Disagree [28] method.   \n148 The diversification loss for a pair $(p^{m},p^{l})$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{A2D}(p^{m}(x),p^{l}(x))=-\\log\\left[p_{\\hat{y}}^{m}(x)\\cdot(1-p_{\\hat{y}}^{l}(x))+(1-p_{\\hat{y}}^{m}(x))\\cdot p_{\\hat{y}}^{l}(x)\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "149 where $\\hat{y}:=\\arg\\operatorname*{max}_{c}p_{c}^{m}(x)$ is the predicted class for the first model $p^{m}$ . One may symmetrically   \n150 define $\\hat{y}$ to be the prediction for the second model $p^{l}$ ; in practice, it does not make a difference [28].   \n151 Note that the diversification loss favors $p^{l}$ to predict a lower likelihood for the prediction by $p^{m}$ ,   \n152 $p_{\\hat{y}}^{l}(x)$ , and vice versa. For $M$ models in an ensemble, A2D is applied on the OOD dataset $\\mathcal{D}^{00\\mathrm{d}}$ for   \n153 every pair of models $(p^{m},p^{l})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{div}}=\\frac{1}{N^{\\mathrm{ood}}\\cdot M(M-1)}\\sum_{n}\\sum_{m<l}\\mathrm{A2D}(p^{m}(x_{n}^{\\mathrm{ood}}),p^{l}(x_{n}^{\\mathrm{ood}})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "154 3.2 Scalable Ensemble Diversification (SED) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "155 We present Scalable Ensemble Diversification (SED) that addresses the limitation of the existing   \n156 ensemble diversification framework that requires a separate OOD dataset. We introduce two main   \n157 components of SED: dynamic selection of OOD samples within the ID dataset $(\\S3.2.1)$ and the   \n158 stochastic selection of pairs to diverge in the optimization iterations $(\\S3.2.2)$ . ", "page_idx": 3}, {"type": "text", "text": "159 3.2.1 Dynamic selection of OOD samples ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "160 If only the ID training dataset is present, it is difficult to induce diversity in ensemble members,   \n161 as they are uniformly incentivized to solve the main task objective: given $x$ , predict $y$ . Hence,   \n162 previous approaches have introduced a qualitatively disjoint unlabeled set, which we refer to as   \n163 the OOD dataset, where the ensemble members are encouraged to disagree with each other. The   \n164 clear separation of ID and OOD datasets for the two objectives matters for ensuring a good balance   \n165 between the main task performance and the diversity of hypotheses.   \n166 Previous works like Pagliardini et al. [28], Lee et al. [23] have performed experiments on small-scale   \n167 datasets where factors are well-controlled and clean versions of OOD datasets are readily available.   \n168 Examples include Waterbirds, Camelyon17, CelebA, MultiNLI, C-MNIST, and the Office-Home   \n169 datasets. For example, for Waterbirds, the ID dataset is set as the cases where the bird\u2019s habitat   \n170 matches with the visual background and the OOD dataset corresponds to the complementary case.   \n171 While conceptually desirable, collecting a separate OOD dataset can be highly cumbersome and   \n172 expensive. For a large-scale dataset like ImageNet, it is highly non-obvious how one could build a   \n173 corresponding OOD dataset where the underlying feature-label correlations are different from the ID   \n174 training dataset.   \n175 To address this challenge, we consider dynamically identifying an OOD subset of the ID dataset and   \n176 letting the ensemble diverge on this subset. The desiderata for the identification of OOD samples   \n177 within the ID dataset are twofold: (a) we wish to discriminate samples where the ensemble members   \n178 make mistakes and (b) we only trust the ensemble prediction for the OOD sample identification when   \n179 the ensemble is sufficiently trained. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "180 We define the sample-wise weight $\\alpha_{n}$ on each ID sample $(x_{n},y_{n})\\in\\mathcal{D}$ that satisfy the two conditions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha_{n}:=\\frac{\\mathrm{CE}(f^{1},\\cdot\\cdot\\cdot\\cdot,f^{M};x_{n},y_{n})}{\\left(\\frac{1}{|B|}\\sum_{b\\in B}\\mathrm{CE}(f^{1},\\cdot\\cdot\\cdot,f^{M};x_{b},y_{b})\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "181 where $\\begin{array}{r}{\\mathrm{CE}(f^{1},\\cdot\\cdot\\cdot\\ ,f^{M};x_{n},y_{n}):=\\mathrm{CE}(\\frac{1}{M}\\sum_{m}f^{m}(x_{n}),y_{n})}\\end{array}$ is the loss on the logit-averaged pre  \n182 diction and $B$ is a minibatch that contains the sample $\\left(x_{n},y_{n}\\right)$ . is a weight proportional to the   \n183 ensemble loss on the sample; we thus meet the condition (a). The normalization is designed to handle   \n184 the condition (b). To see this, consider the batch-wise weight ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha_{B}:=\\frac{1}{|{\\cal B}|}\\sum_{b\\in B}\\alpha_{b}=\\frac{1}{\\frac{1}{|{\\cal B}|}\\sum_{b}\\mathbf{C}\\mathbf{E}(f^{1},\\cdot\\cdot\\cdot,f^{M};x_{b},y_{b})}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "185 Note that $\\alpha_{B}$ is now inversely proportional to the average cross-entropy loss of the ensemble on   \n186 the batch $B$ . Thus, the overall level of $\\alpha_{n}$ for $n\\in B$ is lower for earlier iterations of the ensemble   \n187 training, where the predictions from the models are not trustworthy yet.   \n188 With this definition of sample-wise weight $\\alpha_{n}$ for the diversification objective, we define the SED   \n189 objective with the A2D loss for the diversification kernel: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{SED}}:=\\mathcal{L}_{\\mathrm{main}}+\\frac{\\lambda}{N M(M-1)}\\sum_{n}\\sum_{m<l}\\mathrm{stopgrad}(\\alpha_{n})\\cdot\\mathrm{A2D}(p^{m}(x_{n}),p^{l}(x_{n})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "190 where $\\lambda>0$ controls the overall weight of the diversification term. Note that, compared to Equation   \n191 3, this formulation does not rely on the OOD dataset $\\mathcal{D}^{\\mathrm{{ood}}}$ . Instead, all ID samples are treated as   \n192 potential OOD samples, where their OODness is softly determined via $\\alpha_{n}$ . This enables a seamless   \n193 adaptation of existing ensemble diversification methods to a relaxed setting where a separate OOD   \n194 dataset is unavailable. ", "page_idx": 4}, {"type": "text", "text": "195 3.2.2 Further tricks for scalability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "196 Ensemble diversification algorithms are often based on pairwise   \n197 similarities of the members. Pairwise similarity computation scales   \n198 quadratically with the size of the ensemble $M$ . The second term of   \n199 Equation 6 is an example of this. This is potentially a hurdle when   \n200 ensemble diversification is to be applied to $M\\geq10$ , and the data   \n201 and parameter sizes are in the order of millions (e.g. ImageNet).   \n202 We address this computational challenge by computing the summa  \n203 tion of pairwise distances as a stochastic sum. For every minibatch $B$   \n204 of SGD iterations, we uniformly-iid sample a subset $\\mathcal{T}$ of $\\{1,\\cdot\\cdot\\cdot,M\\}$   \n205 to compute the diversification term in Equation 6. The procedure is   \n206 illustrated in the figure on the right.   \n207 To further speed up the SED training, we consider diversifying only   \n208 a subset of layers, while freezing the other layers. In our experiments,   \n209 ensemble members share the same frozen feature extractor of Deit3b   \n210 [32] pretrained on ImageNet-21k [8] and we diversify only the last   \n211 two layers of the models. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "BQEOJZ0aSD/tmp/ab781d242c949d5510dc609eec661d1413a4a87893bb80dc9a9f55aa21860f75.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "212 3.3 Predictive Diversity Score (PDS) for OOD Detection ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "213 We demonstrate several beneftis of the diversified ensembles in $\\S4$ . One of them is the possibility of   \n214 using them for detecting OOD samples through the notion of epistemic uncertainty [13]. Given an   \n215 ensemble of models, a simple baseline for OOD detection is to compute the predictive uncertainty of   \n216 the Bayesian Model Averaging (BMA) by treating the ensemble members as samples of the posterior   \n217 $p(\\theta|\\mathcal{D})$ [22, 37]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{\\mathrm{BMA}}:=\\operatorname*{max}_{c}\\frac{1}{M}\\sum_{m}p_{c}^{m}(x).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "218 This notion of epistemic uncertainty does not directly exploit the potential diversity in individual   \n219 models of the ensemble because it averages out the predictions along the model index $m$ .   \n220 We propose a novel measure for epistemic uncertainty, Predictive Diversity Score (PDS), that directly   \n221 measures the prediction diversity of the individual members. The formulation is given below: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta_{\\mathrm{PDS}}:=\\frac{1}{C}\\sum_{c}\\operatorname*{max}_{m}p_{c}^{m}(x).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "222 PDS is a continuous relaxation of the number of unique argmax predictions within an ensemble   \n223 of models. To see this, consider the special case where $\\bar{p^{m}}\\in\\{\\bar{0,}1\\}$ are one-hot vectors. Then,   \n224 $\\operatorname*{max}_{m}p_{c}^{m}(x)$ is 1 if any of $m$ predicts $c$ and 0 otherwise. Thus, $\\begin{array}{r}{\\sum_{c}\\operatorname*{max}_{m}p_{c}^{m}(x)}\\end{array}$ computes the   \n225 number of classes that at least one of the ensemble members predict s. We show that, with our diverse   \n226 ensembles, PDS outperforms the DE baseline for the OOD detection task (\u00a74.4). ", "page_idx": 5}, {"type": "text", "text": "227 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "228 We verify our contributions, Scalable Ensemble Diversification (SED, $\\S3.2)$ and Predictive Diversity   \n229 Score (PDS, $\\S3.3)$ , on ImageNet-scale tasks and datasets. We first verify that SED diversifies the   \n230 ensemble (\u00a74.2). Then, we demonstrate the application of diversified ensemble to OOD generalization   \n231 (\u00a74.3) and OOD detection (\u00a74.4) tasks. ", "page_idx": 5}, {"type": "text", "text": "232 4.1 Experimental setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "233 We task the ensemble with the OOD generalization and OOD detection tasks. ", "page_idx": 5}, {"type": "text", "text": "234 Training settings. For both tasks, we train an ensemble of models with the SED framework with   \n235 the A2D [28] diversity regularization using AdamW optimizer [26]. We use the default settings of a   \n236 batch size of 16, learning rate $10^{-3}$ , weight decay 0.01, and the number of epochs 10. The overall   \n237 diversity weight $\\lambda$ is set to 0.1 and the stochastic pairing is done for $|{\\mathcal{T}}|=2$ models for each SGD   \n238 batch. We use Deit3b [32] network pretrained on ImageNet21k [8] for all the experiments. Following   \n239 the speed-up trick in $\\S3.2.2$ , we use only the last 2 layers of the network. For the in-distribution   \n240 (ID) dataset where the ensemble is trained to diversify, we use the training split of ImageNet with   \n241 $|\\mathcal \u1e0a D \u1e0c |=1$ , 281, 167. All experiments were ran on RTX2080Ti GPUs with 12GB vRAM and 40GB   \n242 RAM, each experiment took from 2 to 12 hours depending on the complexity of the training.   \n243 Baselines. For naive ensemble training, we consider the deep ensemble [22] where each ensemble   \n244 member independently with different random seeds that control the weight initialization and SGD   \n245 batch shuffling. To match the resource usage of our SED, where we diversify only the last 2 layers   \n246 of the network, we consider the shallow ensemble variant, which is the deep ensemble where only   \n247 the last 2 layers are trained. We further consider a viable diversification scheme that performs deep   \n248 ensemble with varying hyperparameters [36]. In addition to that, we reimplement A2D [28] and   \n249 DivDis [23] algorithms and apply them without stochastic model sampling to do classification on   \n250 labeled samples from ImageNet-Train and disagreement on unlabeled samples from ImageNet-R.   \n251 For A2D we use frozen feature extractor and a parallel variant of their method which means that all   \n252 ensemble members are trained simultaneously and not sequentially. The computational complexity   \n253 of both these approaches scales quadratically with ensemble size which is why they are called Naive   \n254 A2D and Naive DivDis respectively.   \n255 Evaluation benchmarks. The generalization performances of the ImageNet-trained ensembles are   \n256 measured on multiple test datasets, ranging from the in-distribution validation split of ImageNet with   \n257 50,000 samples to OOD datasets like ImageNet-A (A [17], $7.5\\mathrm{k}$ images & 200 classes), ImageNet-R   \n258 (A [16], 30k images, 200 classes), ImageNet-C (C-i for corruption strength $i$ [14], 50k images, 1k   \n259 classes). OpenImages-O (OI [35], 17k images, unlabeled), and iNaturalist (iNat [20], 10k images,   \n260 unlabeled). For OOD detection, we task the ensemble with the detection of the above OOD datasets   \n261 against the ImageNet validation split.   \n262 Evaluation metrics. For OOD generalization, we use the accuracy. For OOD detection, we use the   \n263 area under the ROC curve, following [15]. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "BQEOJZ0aSD/tmp/60c873484af130cebc93c99d03cb255d8d24be184d5e0d360c6f01a4dfcf1d6d.jpg", "img_caption": ["Figure 2: ImageNet-R examples leading to the greatest and least disagreement. We show the 5 most divergent and 5 least divergent samples according to the SED ensemble. We measure the prediction diversity with the Prediction Diversity Score (PDS) in $\\S3.3$ . GT refers to the ground truth category. Ensemble predictions are shown in bold, in cases when ensemble members predict classes different from the ensemble prediction we provide them on the next line with standard font. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "264 4.2 Diversification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "265 We start with the question of whether Scalable Ensemble Diversification (SED) truly diversify the   \n266 ensemble at the ImageNet scale. To measure the diversity of the ensemble, we compute the number   \n267 of unique predictions for each sample for the committee of models ( $\\#$ unique).   \n268 Table 1 shows the #unique values for the IN-Val   \n269 as well as multiple OOD datasets. We observe   \n270 that the deep ensemble baseline does not increase   \n271 the diversity dramatically (e.g. 1.09 for C-1) be  \n272 yond no-diversity values (1.0). Diversification   \n273 tricks like hyperparameter diversification (1.11   \n274 for C-1) or Naive A2D (1.04 for C-1) and DivDis   \n275 (1.04 for C-1) do not improve the prediction di  \n276 versity dramatically. On the other hand, our SED   \n277 increases the prediction diversity across the board   \n278 (e.g. 5.00 for C-1). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "BQEOJZ0aSD/tmp/93e1fc6a9f8d1259f4ee64bb26463116603b1e6c4e39480937ba0715fe8953de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: #unique for ensembles. We report the #unique on OOD datasets (see $\\S4.1$ for the datasets). The ensemble size $M$ is 5 for all methods; it is the max possible #unique value. ", "page_idx": 6}, {"type": "text", "text": "279 Qualitative results on ImageNet-R further verify the ability of SED to diversify the ensemble (Fig  \n280 ure 2). As a measure for diversity, we use the Predictive Diversity Score (PDS) in $\\S3.3$ . We observe   \n281 that the samples inducing the highest diversity (high PDS scores) are indeed ambiguous: for the   \n282 first image, where the \u201ccowboy hat\u201d is the ground truth category, we observe that \u201ccomic book\u201d is   \n283 also a valid label for the image style. On the other hand, samples with low PDS exhibit clearer   \n284 image-to-category relationship. ", "page_idx": 6}, {"type": "text", "text": "285 4.3 OOD Generalization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "286 We examine the first application of diversified ensembles: OOD generalization. We hypothesize that   \n287 the superior diversification ability verified in $\\S4.2$ leads to greater OOD generalization due to the   \n288 consideration of more robust hypotheses that do not rely on obvious spurious correlations.   \n289 Ensemble aggregation for OOD generalization. As a means to exploit such robust hypothe  \n290 ses, we consider 3 aggregation strategies. (1) Oracle selection: the best-performing individ  \n291 ual model is chosen from an ensemble [28, 30]. Final prediction is given by $f(x;\\theta^{m^{\\star}})$ where   \n292 $m^{\\star}:=\\arg\\operatorname*{max}_{m}$ $\\operatorname{Acc}(f^{m},{\\mathcal{D}}^{\\mathrm{ood}})$ . (2) Prediction ensemble is a vanilla prediction ensemble where   \n293 the logit values are averaged: $\\textstyle{\\frac{1}{M}}\\sum_{m}f^{m}(x)$ [38]. (3) Uniform soup [38] averages the weights   \n294 themselves. Final prediction is given by $\\begin{array}{r}{f(x;\\frac{1}{M}\\sum_{m}\\theta^{m})}\\end{array}$ .   \n295 SED improves OOD generalization for ensembles. We show the OOD generalization performances   \n296 of ensembles in Table 2, for the three ensemble prediction aggregation strategies described above. We   \n297 observe that our SED framework (SED-A2D) results in superior OOD generalization performances   \n298 for all three strategies. SED-A2D is particularly strong in prediction ensemble (e.g. $48.1\\%$ for $M=5$   \n299 and $54.4\\%$ for $M\\,=\\,50$ on ImageNet-R) and uniform soup (e.g. $46.1\\%$ for $M=5$ and $46.5\\%$   \n300 for $M=50$ on ImageNet-R). We contend that the increased ensemble diversity contributes to the   \n301 improvements in OOD generalization. We also remark that the SED framework (SED-A2D) envelops   \n302 the performance of Naive A2D in this ImageNet-scale experiment. Together with the superiority of   \n303 computational efficiency (as discussed at the end of $\\S\\ 4.4)$ of SED-A2D over the Naive A2D, this   \n304 demonstrates that SED fulfills its purpose of scaling up ensemble diversification methods like A2D.   \n305 Deep ensemble is a strong baseline. We also note that deep ensemble, particularly with diverse   \n306 hyperparameters, provides a strong baseline, outperforming dedicated diversification methodologies   \n307 under the oracle selection strategy when $M\\,=\\,5$ . It also provides a good balance between ID   \n308 (ImageNet validation split) and OOD generalization. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "BQEOJZ0aSD/tmp/b6503ba94f02349045d14a293463bfc927ab742ee3b842035aa987f4b4f699d5.jpg", "table_caption": ["Table 2: OOD generalization of ensembles. Models are trained on the ImageNet training split. $M$ is the ensemble size. For Naive DivDis and A2D, we use the ImageNet-R as the OOD datasets where the respective diversification objectives are applied. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "309 4.4 OOD Detection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "310 We study the impact of ensemble diversifi  \n311 cation on OOD detection capabilities of an   \n312 ensemble. Once an ensemble is trained, we   \n313 compute the epistemic uncertainty, or like  \n314 lihood of the sample being OOD, following   \n315 two schemes, $\\eta_{\\mathrm{BMA}}$ and $\\eta_{\\mathrm{PDS}}$ introduced in   \n316 $\\S3.3$ .   \n317 SED and PDS together lead to superior   \n318 OOD detection performances. We show   \n319 the OOD detection results in Table 3. For   \n320 the BMA scores, deep ensemble remains a   \n321 strong baseline. In particular, when the hy  \n322 perparameters are varied (\u201c+Diverse HPs\u201d),   \n323 the detection AUROC reaches the maximal   \n324 performances among the ensembles using   \n325 the BMA scores. The quality of PDS is   \n326 more sensitive to the ensemble diversity, as   \n327 seen in the jump from the deep ensemble   \n328 (e.g. 0.589 for OI) to the diverse-HP vari  \n329 ant (0.889). However, when the ensemble ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "BQEOJZ0aSD/tmp/3ee7a1b3cbbc83a543bb5c558090f0906300471698f2f022cd02a8767a251bb5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: OOD detection via ensembles. For each OOD dataset (C-1, C-5, iNat, and OI), the ensembles are tasked to detect the respective OOD samples among ID samples (ImageNet validation split). We show the AUROC scores for the OOD detection task. Ensemble size is fixed at $M=5$ . $\\eta$ refers to the epistemic uncertainty computation framework discussed in $\\S3.{3}$ . ", "page_idx": 7}, {"type": "text", "text": "330 is sufficiently diverse, such as when trained   \n331 with SED-A2D, the PDS leads to high-quality OODness scores. SED-A2D with PDS achieves the   \n332 best AUROC across the board, including the BMA variants.   \n333 Impact of diversification parameter $\\lambda$ . We further study the impact of ensemble diversification   \n334 on the OOD detection with the PDS estimator. In Figure 3, we observe that strengthening the   \n335 diversification objective (higher $\\lambda$ ) indeed leads to greater diversity (higher PDS), with a jump at   \n336 around $\\lambda\\in[10^{-\\bar{1}},10^{1}]$ . This range corresponds to the jump in the OOD detection performance   \n337 (higher AUROC).   \n338 Influence of ensemble size. How ensemble size   \n339 influences performance of our method? We can   \n340 see that increasing ensemble size helps to im  \n341 prove AUROC for OOD detection on C-1 (Fig  \n342 ure 4). Increasing ensemble size marginally   \n343 helps, but using 5 models provides already a   \n344 significant improvement over the smallest pos  \n345 sible ensemble of size 2. It is also important to   \n346 mention, that SED framework is computationally   \n347 more efficient w.r.t. ensemble size $M$ than Naive   \n348 A2D and Naive DivDis: since we train ensembles for the fixed number of epochs, training complexity   \n349 for SED is $O(1)$ thanks to stochastic model pairs selection, while for Naive A2D and Naive DivDis it   \n350 is $O(M^{2})$ . ", "page_idx": 8}, {"type": "image", "img_path": "BQEOJZ0aSD/tmp/97de0d810fa4eeb5af72c166107e7170f96d8d9dda325daf789221d04a79f059.jpg", "img_caption": ["Figure 3: Impact of diversity regulariser on OOD detection. We show the model answer diversity, measured by PDS, and the OOD detection performance, measured by AUROC, against $\\lambda$ values, the loss weight for the disagreement regularizer term. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "BQEOJZ0aSD/tmp/e41098386e21982b0fa34afdcbec61938d72635618d40b52d3da5a2fe1f1030f.jpg", "img_caption": ["Figure 4: Impact of ensemble size on OOD detection. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "351 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "352 Ensemble diversification has many implications for treating one of the ultimate goals of machine learn  \n353 ing, handling out-of-distribution (OOD) samples. By training a large number of plausible hypotheses   \n354 on an in-distribution (ID) dataset, an OOD-generalizable hypothesis may appear. Moreover, the   \n355 diversity of hypotheses lets us distinguish ID samples from OOD samples by measuring the degree of   \n356 divergence in ensemble members\u2019 predictions. Despite conceptual beneftis, diverse-ensemble training   \n357 has previously remained a lab-bound concept for several reasons. First, previous approaches required   \n358 a separate OOD dataset that may nurture diverse hypotheses. Second, computational complexities of   \n359 previous pairwise diversification objectives increase quadratically with the ensemble size.   \n360 We have addressed the challenges through the novel Scalable Ensemble Diversification (SED)   \n361 framework. SED identifies the OOD-like samples from a single dataset, bypassing the need to   \n362 prepare a separate OOD dataset. SED also employs a stochastic pair selection algorithm which   \n363 reduces the quadratic complexity of previous approaches to a constant cost per SGD iteration. We   \n364 have demonstrated good performances by SED on the OOD generalization and detection tasks, both   \n365 at the ImageNet scale, a largely underexplored regime in the ensemble diversification community.   \n366 In particular, for OOD detection, our novel diversity measure of Predictive Diversity Score (PDS)   \n367 amplifies the beneftis of diverse ensembles for OOD detection. The code to reproduce the results of   \n368 our experiments will provided with the next revision of the manuscript. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "369 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "370 We do not provide theoretical justification for the method. Our experiments were conducted on   \n371 models with a frozen feature extractor. ", "page_idx": 8}, {"type": "text", "text": "372 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "373 [1] H. L. Benoit, L. Jiang, A. Atanov, O. F. Kar, M. Rigotti, and A. Zamir. Unraveling the key compo  \n374 nents of OOD generalization via diversification. In The Twelfth International Conference on Learning   \n375 Representations, 2024. URL https://openreview.net/forum?id=Lvf7GnaLru.   \n376 [2] L. Breiman. Bagging predictors. Machine Learning, 24(2):123\u2013140, Aug 1996. ISSN 1573-0565. doi:   \n377 10.1007/BF00058655. URL https://doi.org/10.1007/BF00058655.   \n378 [3] L. Breiman. Random forests. Machine Learning, 45(1):5\u201332, Oct 2001. ISSN 1573-0565. doi: 10.1023/A:   \n379 1010933404324. URL https://doi.org/10.1023/A:1010933404324.   \n380 [4] A. S. Chen, Y. Lee, A. Setlur, S. Levine, and C. Finn. Project and probe: Sample-efficient domain   \n381 adaptation by interpolating orthogonal features. arXiv preprint arXiv:2302.05441, 2023.   \n382 [5] F. D\u2019Angelo and V. Fortuin. Repulsive deep ensembles are bayesian. Advances in Neural Information   \n383 Processing Systems, 34:3451\u20133465, 2021.   \n384 [6] A. de Mathelin, F. Deheeger, M. Mougeot, and N. Vayatis. Maximum weight entropy. arXiv preprint   \n385 arXiv:2309.15704, 2023.   \n386 [7] A. de Mathelin, F. Deheeger, M. Mougeot, and N. Vayatis. Deep anti-regularized ensembles provide   \n387 reliable out-of-distribution uncertainty quantification, 2023.   \n388 [8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image   \n389 database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009.   \n390 doi: 10.1109/CVPR.2009.5206848.   \n391 [9] N. Durasov, T. Bagautdinov, P. Baque, and P. Fua. Masksembles for uncertainty estimation. In Proceedings   \n392 of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13539\u201313548, 2021.   \n393 [10] Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep   \n394 learning. In international conference on machine learning, pages 1050\u20131059. PMLR, 2016.   \n395 [11] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. http://www.   \n396 deeplearningbook.org.   \n397 [12] L. Hansen and P. Salamon. Neural network ensembles. IEEE Transactions on Pattern Analysis and   \n398 Machine Intelligence, 12(10):993\u20131001, 1990. doi: 10.1109/34.58871.   \n399 [13] J. C. Helton, J. D. Johnson, and W. L. Oberkampf. An exploration of alternative approaches to the   \n400 representation of uncertainty in model predictions. Reliability Engineering & System Safety, 85(1-3):   \n401 39\u201371, 2004.   \n402 [14] D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corruptions   \n403 and perturbations. In International Conference on Learning Representations, 2019. URL https://   \n404 openreview.net/forum?id=HJz6tiCqYm.   \n405 [15] D. Hendrycks and K. Gimpel. A baseline for detecting misclassified and out-of-distribution examples   \n406 in neural networks. In International Conference on Learning Representations, 2017. URL https:   \n407 //openreview.net/forum?id=Hkg4TI9xl.   \n408 [16] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo,   \n409 et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings   \n410 of the IEEE/CVF international conference on computer vision, pages 8340\u20138349, 2021.   \n411 [17] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In Proceedings   \n412 of the IEEE/CVF conference on computer vision and pattern recognition, pages 15262\u201315271, 2021.   \n413 [18] T. K. Ho. Random decision forests. In Proceedings of 3rd International Conference on Document Analysis   \n414 and Recognition, volume 1, pages 278\u2013282 vol.1, 1995. doi: 10.1109/ICDAR.1995.598994.   \n415 [19] G. Huang, Y. Li, G. Pleiss, Z. Liu, J. E. Hopcroft, and K. Q. Weinberger. Snapshot ensembles: Train 1, get   \n416 m for free. arXiv preprint arXiv:1704.00109, 2017.   \n417 [20] R. Huang and Y. Li. Mos: Towards scaling out-of-distribution detection for large semantic space. In   \n418 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8710\u20138719,   \n419 2021.   \n420 [21] A. Krogh and J. Vedelsby. Neural network ensembles, cross validation, and active learning. In   \n421 G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural Information Processing Systems,   \n422 volume 7. MIT Press, 1994. URL https://proceedings.neurips.cc/paper_files/paper/1994/   \n423 file/b8c37e33defde51cf91e1e03e51657da-Paper.pdf.   \n424 [22] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation   \n425 using deep ensembles. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,   \n426 and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran As  \n427 sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/   \n428 9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf.   \n429 [23] Y. Lee, H. Yao, and C. Finn. Diversify and disambiguate: Out-of-distribution robustness via dis  \n430 agreement. In The Eleventh International Conference on Learning Representations, 2023. URL   \n431 https://openreview.net/forum?id=RVTOp3MwT3n.   \n432 [24] Z. Li, I. Evtimov, A. Gordo, C. Hazirbas, T. Hassner, C. C. Ferrer, C. Xu, and M. Ibrahim. A whac-a-mole   \n433 dilemma: Shortcuts come in multiples where mitigating one amplifies others. In Proceedings of the   \n434 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20071\u201320082, 2023.   \n435 [25] Y. Liu and X. Yao. Simultaneous training of negatively correlated neural networks in an ensemble. IEEE   \n436 Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(6):716\u2013725, 1999.   \n437 [26] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on   \n438 Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.   \n439 [27] Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. Dillon, B. Lakshminarayanan, and   \n440 J. Snoek. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift.   \n441 Advances in neural information processing systems, 32, 2019.   \n442 [28] M. Pagliardini, M. Jaggi, F. Fleuret, and S. P. Karimireddy. Agree to disagree: Diversity through disagree  \n443 ment for better transferability. In The Eleventh International Conference on Learning Representations,   \n444 2023. URL https://openreview.net/forum?id=K7CbYQbyYhY.   \n445 [29] A. Ross, W. Pan, L. Celi, and F. Doshi-Velez. Ensembles of locally independent prediction models. In   \n446 Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5527\u20135536, 2020.   \n447 [30] D. Teney, E. Abbasnejad, S. Lucey, and A. van den Hengel. Evading the simplicity bias: Training a diverse   \n448 set of models discovers solutions with superior ood generalization. In Proceedings of the IEEE/CVF   \n449 Conference on Computer Vision and Pattern Recognition (CVPR), pages 16761\u201316772, June 2022.   \n450 [31] D. Teney, M. Peyrard, and E. Abbasnejad. Predicting is not understanding: Recognizing and addressing   \n451 underspecification in machine learning. In S. Avidan, G. Brostow, M. Ciss\u00e9, G. M. Farinella, and T. Hassner,   \n452 editors, Computer Vision \u2013 ECCV 2022, pages 458\u2013476, Cham, 2022. Springer Nature Switzerland. ISBN   \n453 978-3-031-20050-2.   \n454 [32] H. Touvron, M. Cord, and H. J\u00e9gou. Deit iii: Revenge of the vit. In European conference on computer   \n455 vision, pages 516\u2013533. Springer, 2022.   \n456 [33] T. Trinh, M. Heinonen, L. Acerbi, and S. Kaski. Input-gradient space particle inference for neural network   \n457 ensembles. In International Conference on Learning Representations, 2024.   \n458 [34] H. Wang and Q. Ji. Diversity-enhanced probabilistic ensemble for uncertainty estimation. In R. J. Evans   \n459 and I. Shpitser, editors, Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence,   \n460 volume 216 of Proceedings of Machine Learning Research, pages 2214\u20132225. PMLR, 31 Jul\u201304 Aug   \n461 2023. URL https://proceedings.mlr.press/v216/wang23c.html.   \n462 [35] H. Wang, Z. Li, L. Feng, and W. Zhang. Vim: Out-of-distribution with virtual-logit matching. In   \n463 Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4921\u20134930,   \n464 2022.   \n465 [36] F. Wenzel, J. Snoek, D. Tran, and R. Jenatton. Hyperparameter ensembles for robustness and uncertainty   \n466 quantification. Advances in Neural Information Processing Systems, 33:6514\u20136527, 2020.   \n467 [37] A. G. Wilson and P. Izmailov. Bayesian deep learning and a probabilistic perspective of generalization.   \n468 Advances in neural information processing systems, 33:4697\u20134708, 2020.   \n469 [38] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong,   \n470 A. Farhadi, Y. Carmon, S. Kornblith, and L. Schmidt. Model soups: averaging weights of multiple   \n471 fine-tuned models improves accuracy without increasing inference time. In K. Chaudhuri, S. Jegelka,   \n472 L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference   \n473 on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 23965\u201323998.   \n474 PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/wortsman22a.html.   \n475 [39] S. Yashima, T. Suzuki, K. Ishikawa, I. Sato, and R. Kawakami. Feature space particle inference for neural   \n476 network ensembles. In International Conference on Machine Learning, pages 25452\u201325468. PMLR, 2022.   \n477 [40] S. Zaidi, A. Zela, T. Elsken, C. C. Holmes, F. Hutter, and Y. Teh. Neural ensemble search for uncertainty   \n478 estimation and dataset shift. Advances in Neural Information Processing Systems, 34:7898\u20137911, 2021.   \n479 [41] T. Zhou, S. Wang, and J. A. Bilmes. Diverse ensemble evolution: Curriculum data-model mar  \n480 riage. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar  \n481 nett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Asso  \n482 ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/   \n483 3070e6addcd702cb58de5d7897bfdae1-Paper.pdf. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "484 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "485 1. Claims   \n486 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n487 paper\u2019s contributions and scope?   \n488 Answer: [Yes]   \n489 Justification: Please refer to $\\S\\,4$   \n490 Guidelines:   \n491 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n492 made in the paper.   \n493 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n494 contributions made in the paper and important assumptions and limitations. A No or   \n495 NA answer to this question will not be perceived well by the reviewers.   \n496 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n497 much the results can be expected to generalize to other settings.   \n498 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n499 are not attained by the paper.   \n500 2. Limitations   \n501 Question: Does the paper discuss the limitations of the work performed by the authors?   \n502 Answer: [Yes]   \n503 Justification: Please refer to $\\S\\ S$   \n504 Guidelines:   \n505 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n506 the paper has limitations, but those are not discussed in the paper.   \n507 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n508 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n509 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n510 model well-specification, asymptotic approximations only holding locally). The authors   \n511 should reflect on how these assumptions might be violated in practice and what the   \n512 implications would be.   \n513 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n514 only tested on a few datasets or with a few runs. In general, empirical results often   \n515 depend on implicit assumptions, which should be articulated.   \n516 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n517 For example, a facial recognition algorithm may perform poorly when image resolution   \n518 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n519 used reliably to provide closed captions for online lectures because it fails to handle   \n520 technical jargon.   \n521 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n522 and how they scale with dataset size.   \n523 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n524 address problems of privacy and fairness.   \n525 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n526 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n527 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n528 judgment and recognize that individual actions in favor of transparency play an impor  \n529 tant role in developing norms that preserve the integrity of the community. Reviewers   \n530 will be specifically instructed to not penalize honesty concerning limitations.   \n531 3. Theory Assumptions and Proofs   \n532 Question: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 11}, {"type": "text", "text": "33 a complete (and correct) proof? ", "page_idx": 11}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 11}, {"type": "text", "text": "35 Justification: The paper contains no theoretical results.   \n36 Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 12}, {"type": "text", "text": "547 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "548 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n549 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n550 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "551   \n552 Justification: Please refer to $\\S\\,4$   \n553 Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 12}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "86 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n87 tions to faithfully reproduce the main experimental results, as described in supplemental   \n8 material?   \n589 Answer: [Yes]   \n590 Justification: Code will be available soon, please refer to $\\S\\,4.1$ .   \n591 Guidelines:   \n592 \u2022 The answer NA means that paper does not include experiments requiring code.   \n593 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n594 public/guides/CodeSubmissionPolicy) for more details.   \n595 \u2022 While we encourage the release of code and data, we understand that this might not be   \n596 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n597 including code, unless this is central to the contribution (e.g., for a new open-source   \n598 benchmark).   \n599 \u2022 The instructions should contain the exact command and environment needed to run to   \n600 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n601 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n602 \u2022 The authors should provide instructions on data access and preparation, including how   \n603 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n604 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n605 proposed method and baselines. If only a subset of experiments are reproducible, they   \n606 should state which ones are omitted from the script and why.   \n607 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n608 versions (if applicable).   \n609 \u2022 Providing as much information as possible in supplemental material (appended to the   \n610 paper) is recommended, but including URLs to data and code is permitted.   \n611 6. Experimental Setting/Details   \n612 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n613 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n614 results?   \n615 Answer: [Yes]   \n616 Justification: please refer to $\\S\\,4.1$ .   \n617 Guidelines:   \n618 \u2022 The answer NA means that the paper does not include experiments.   \n619 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n620 that is necessary to appreciate the results and make sense of them.   \n621 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n622 material.   \n623 7. Experiment Statistical Significance   \n624 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n625 information about the statistical significance of the experiments?   \n626 Answer: [No]   \n627 Justification: Error bars are not reported because their magnitude was below the rounding   \n628 error or roughly around it for the majority of experiments.   \n629 Guidelines:   \n630 \u2022 The answer NA means that the paper does not include experiments.   \n631 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n632 dence intervals, or statistical significance tests, at least for the experiments that support   \n633 the main claims of the paper.   \n634 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n635 example, train/test split, initialization, random drawing of some parameter, or overall   \n636 run with given experimental conditions).   \n637 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n638 call to a library function, bootstrap, etc.)   \n639 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n640 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n641 of the mean.   \n642 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n643 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n644 of Normality of errors is not verified.   \n645 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n646 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n647 error rates).   \n648 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n649 they were calculated and reference the corresponding figures or tables in the text.   \n650 8. Experiments Compute Resources   \n651 Question: For each experiment, does the paper provide sufficient information on the com  \n652 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n653 the experiments?   \n654 Answer: [Yes]   \n655 Justification: please refer to $\\S\\,4.1$ .   \n656 Guidelines:   \n657 \u2022 The answer NA means that the paper does not include experiments.   \n658 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n659 or cloud provider, including relevant memory and storage.   \n660 \u2022 The paper should provide the amount of compute required for each of the individual   \n661 experimental runs as well as estimate the total compute.   \n662 \u2022 The paper should disclose whether the full research project required more compute   \n663 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n664 didn\u2019t make it into the paper).   \n665 9. Code Of Ethics   \n666 Question: Does the research conducted in the paper conform, in every respect, with the   \n667 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n668 Answer: [Yes]   \n669 Justification: we followed the Code to the best of our knowledge.   \n670 Guidelines:   \n671 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n672 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n673 deviation from the Code of Ethics.   \n674 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n675 eration due to laws or regulations in their jurisdiction).   \n676 10. Broader Impacts   \n677 Question: Does the paper discuss both potential positive societal impacts and negative   \n678 societal impacts of the work performed?   \n679 Answer: [NA]   \n680 Justification: We believe that this work has no societal impact.   \n681 Guidelines:   \n682 \u2022 The answer NA means that there is no societal impact of the work performed.   \n683 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n684 impact or why the paper does not address societal impact.   \n685 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n686 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n687 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n688 groups), privacy considerations, and security considerations.   \n689 \u2022 The conference expects that many papers will be foundational research and not tied   \n690 to particular applications, let alone deployments. However, if there is a direct path to   \n691 any negative applications, the authors should point it out. For example, it is legitimate   \n692 to point out that an improvement in the quality of generative models could be used to   \n693 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n694 that a generic algorithm for optimizing neural networks could enable people to train   \n695 models that generate Deepfakes faster.   \n696 \u2022 The authors should consider possible harms that could arise when the technology is   \n697 being used as intended and functioning correctly, harms that could arise when the   \n698 technology is being used as intended but gives incorrect results, and harms following   \n699 from (intentional or unintentional) misuse of the technology.   \n700 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n701 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n702 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n703 feedback over time, improving the efficiency and accessibility of ML).   \n704 11. Safeguards   \n705 Question: Does the paper describe safeguards that have been put in place for responsible   \n706 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n707 image generators, or scraped datasets)?   \n708 Answer: [NA]   \n709 Justification: We believe that our paper does not pose such risks as we train models for   \n710 ImageNet classification.   \n711 Guidelines:   \n712 \u2022 The answer NA means that the paper poses no such risks.   \n713 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n714 necessary safeguards to allow for controlled use of the model, for example by requiring   \n715 that users adhere to usage guidelines or restrictions to access the model or implementing   \n716 safety filters.   \n717 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n718 should describe how they avoided releasing unsafe images.   \n719 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n720 not require this, but we encourage authors to take this into account and make a best   \n721 faith effort.   \n722 12. Licenses for existing assets   \n723 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n724 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n725 properly respected?   \n726 Answer: [No]   \n727 Justification: we were unable to find the license for the dataset we used.   \n728 Guidelines:   \n729 \u2022 The answer NA means that the paper does not use existing assets.   \n730 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n731 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n732 URL.   \n733 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n734 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n735 service of that source should be provided.   \n736 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n737 package should be provided. For popular datasets, paperswithcode.com/datasets   \n738 has curated licenses for some datasets. Their licensing guide can help determine the   \n739 license of a dataset.   \n740 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n741 the derived asset (if it has changed) should be provided.   \n742 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n743 the asset\u2019s creators.   \n744 13. New Assets   \n745 Question: Are new assets introduced in the paper well documented and is the documentation   \n746 provided alongside the assets?   \n747 Answer: [NA]   \n748 Justification: the paper does not release new assets.   \n749 Guidelines:   \n750 \u2022 The answer NA means that the paper does not release new assets.   \n751 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n752 submissions via structured templates. This includes details about training, license,   \n753 limitations, etc.   \n754 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n755 asset is used.   \n756 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n757 create an anonymized URL or include an anonymized zip file.   \n758 14. Crowdsourcing and Research with Human Subjects   \n759 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n760 include the full text of instructions given to participants and screenshots, if applicable, as   \n761 well as details about compensation (if any)?   \n762 Answer: [NA]   \n763 Justification: the paper does not involve crowdsourcing nor research with human subjects.   \n764 Guidelines:   \n765 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n766 human subjects.   \n767 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n768 tion of the paper involves human subjects, then as much detail as possible should be   \n769 included in the main paper.   \n770 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n771 or other labor should be paid at least the minimum wage in the country of the data   \n772 collector.   \n773 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n774 Subjects   \n775 Question: Does the paper describe potential risks incurred by study participants, whether   \n776 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n777 approvals (or an equivalent approval/review based on the requirements of your country or   \n778 institution) were obtained?   \n779 Answer: [NA]   \n780 Justification: the paper does not involve crowdsourcing nor research with human subjects.   \n781 Guidelines:   \n782 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n783 human subjects.   \n784 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n785 may be required for any human subjects research. If you obtained IRB approval, you   \n786 should clearly state this in the paper.   \n787 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n788 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n789 guidelines for their institution.   \n790 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n791 applicable), such as the institution conducting the review. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}]