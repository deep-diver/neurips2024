[{"figure_path": "3apt5AJ5QN/figures/figures_6_1.jpg", "caption": "Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for N = 4. Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.", "description": "This figure compares the performance of different restless multi-armed bandit (RMAB) policies on synthetic data with four different reward functions (Linear, Probability, Max, Subset).  It shows that the proposed adaptive policies (Iterative and MCTS Shapley-Whittle) consistently outperform baseline methods and are close to optimal for smaller problem sizes (N=4).", "section": "6 Results with Synthetic Data"}, {"figure_path": "3apt5AJ5QN/figures/figures_7_1.jpg", "caption": "Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for N = 4. Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.", "description": "This figure compares the performance of different policies (baselines and proposed policies) on four different reward functions with 4 and 10 arms. The results show that all the proposed policies outperform the baselines, and among them, Iterative and MCTS Shapley-Whittle policies perform the best and are close to optimal performance for the case of 4 arms.", "section": "6.1 Results with Synthetic Data"}, {"figure_path": "3apt5AJ5QN/figures/figures_7_2.jpg", "caption": "Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for N = 4. Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.", "description": "The figure compares the performance of six proposed policies (Linear-Whittle, Shapley-Whittle, Iterative Linear-Whittle, Iterative Shapley-Whittle, MCTS Linear-Whittle, and MCTS Shapley-Whittle) against several baseline methods (Random, Vanilla Whittle, Greedy, MCTS, DQN, and DQN Greedy) across four different reward functions (Linear, Probability, Max, and Subset) with 4 and 10 arms. The results show that all proposed policies outperform baselines, with Iterative and MCTS Shapley-Whittle consistently achieving the best performance, and coming within 3% of optimal for N=4.", "section": "6.1 Results with Synthetic Data"}, {"figure_path": "3apt5AJ5QN/figures/figures_13_1.jpg", "caption": "Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for N = 4. Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.", "description": "This figure compares the performance of different restless multi-armed bandit (RMAB) policies on four different reward functions.  The policies include baselines (Random, Vanilla Whittle, Greedy, MCTS, DQN, DQN Greedy), index-based policies (Linear-Whittle, Shapley-Whittle), and adaptive policies (Iterative Linear-Whittle, Iterative Shapley-Whittle, MCTS Linear-Whittle, MCTS Shapley-Whittle). The results show that all the proposed policies outperform the baselines, and the Iterative and MCTS Shapley-Whittle consistently achieve the best performance.  The performance is within 3% of optimal for problems with 4 arms.", "section": "6.1 Results with Synthetic Data"}, {"figure_path": "3apt5AJ5QN/figures/figures_15_1.jpg", "caption": "Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for N = 4. Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.", "description": "The figure compares the performance of several restless bandit algorithms across four reward functions.  The algorithms include baselines (Random, Vanilla Whittle, Greedy, MCTS, DQN, DQN Greedy), index-based policies (Linear-Whittle, Shapley-Whittle), and adaptive policies (Iterative Linear, Iterative Shapley, MCTS Linear, MCTS Shapley). The results show that all proposed policies outperform the baselines.  For smaller problem sizes (N=4), the best performing policy is within 3% of optimal.  The Iterative and MCTS Shapley-Whittle policies consistently show the best performance across reward functions.", "section": "6 Experiments"}, {"figure_path": "3apt5AJ5QN/figures/figures_16_1.jpg", "caption": "Figure 6: We compare the impact of additional training epochs on the performance of a DQN baseline. We find that, regardless of the number of training episodes, Linear-Whittle outperforms the DQN baseline. Additional training episodes do not result in a significantly better performance (compared with Linear-Whittle), showing that additional training episodes do not lead to a much smaller gap between Linear-Whittle and the DQN baseline.", "description": "The figure shows the comparison of the performance of Linear-Whittle and DQN algorithms with varying training epochs (50, 100, 200, and 400). The results indicate that Linear-Whittle consistently outperforms DQN across all training epochs. Increasing training epochs does not significantly improve DQN's performance, suggesting that additional training does not close the performance gap between Linear-Whittle and DQN.", "section": "F Deep Q Networks"}, {"figure_path": "3apt5AJ5QN/figures/figures_16_2.jpg", "caption": "Figure 7: We assess the impact of changing q, which parametrizes transition probabilities. Smaller q makes it less likely that arms transition to the 1 state. We find that, regardless of the transition probability chosen, MCTS Shapley-Whittle policies improve upon all other policies.", "description": "This figure shows the performance of different restless multi-armed bandit (RMAB) policies under varying transition probabilities (parameterized by q).  Lower values of q indicate a lower probability of arms transitioning to state 1.  The results demonstrate the consistent superior performance of the MCTS Shapley-Whittle policy across all transition probabilities.", "section": "6.1 Results with Synthetic Data"}, {"figure_path": "3apt5AJ5QN/figures/figures_17_1.jpg", "caption": "Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for N = 4. Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.", "description": "This figure compares the performance of several policies (baselines and the proposed ones) on restless multi-armed bandits with global rewards for different reward functions.  The results show that the proposed policies consistently outperform the baselines across various reward functions, with Iterative and MCTS Shapley-Whittle showing the best performance.  For smaller problem sizes (N=4 arms), the best-performing policy achieves near-optimal results.", "section": "6 Results with Synthetic Data"}, {"figure_path": "3apt5AJ5QN/figures/figures_17_2.jpg", "caption": "Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for N = 4. Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.", "description": "The figure compares the performance of six proposed policies (Linear-Whittle, Shapley-Whittle, Iterative Linear, Iterative Shapley, MCTS Linear, and MCTS Shapley) against six baselines (Random, Vanilla Whittle, Greedy, MCTS, DQN, and DQN Greedy) across four reward functions (Linear, Probability, Max, and Subset) for restless multi-armed bandits with global rewards. The results show that all proposed policies outperform the baselines, and the Iterative and MCTS Shapley-Whittle policies consistently achieve the best performance, often within 3% of the optimal policy for smaller problem sizes (N=4).", "section": "6 Results with Synthetic Data"}, {"figure_path": "3apt5AJ5QN/figures/figures_18_1.jpg", "caption": "Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for N = 4. Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.", "description": "The figure compares the performance of several policies (Linear-Whittle, Shapley-Whittle, Iterative Linear, Iterative Shapley, MCTS Linear, MCTS Shapley) against baselines (Random, Vanilla Whittle, Greedy, DQN, DQN Greedy, Optimal) across four different reward functions (Linear, Probability, Max, Subset).  The results show that all proposed policies outperform the baselines, with Iterative and MCTS Shapley-Whittle generally achieving the best performance.  For smaller problem sizes (N=4), the best performing policy is within 3% of the optimal solution.", "section": "6.1 Results with Synthetic Data"}, {"figure_path": "3apt5AJ5QN/figures/figures_19_1.jpg", "caption": "Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for N = 4. Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.", "description": "This figure compares the performance of several restless bandit algorithms across four different reward functions.  The algorithms include baselines (Random, Vanilla Whittle, Greedy, MCTS, DQN, and DQN Greedy) and the authors' proposed algorithms (Linear-Whittle, Shapley-Whittle, Iterative Linear, Iterative Shapley, MCTS Linear, and MCTS Shapley).  The results show that all of the authors' algorithms outperform the baselines, and the Iterative and MCTS Shapley-Whittle algorithms perform particularly well, achieving near-optimal results in most cases.", "section": "6.1 Results with Synthetic Data"}, {"figure_path": "3apt5AJ5QN/figures/figures_19_2.jpg", "caption": "Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for N = 4. Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.", "description": "This figure compares the performance of various restless multi-armed bandit (RMAB) policies, including baselines and the authors' proposed adaptive policies, across four different reward functions.  The results show that all the authors' proposed policies consistently outperform the baselines, and that the iterative and Monte Carlo Tree Search (MCTS) Shapley-Whittle policies generally achieve the best performance, coming within 3% of the optimal policy for a problem size of N=4.", "section": "6. Results with Synthetic Data"}]