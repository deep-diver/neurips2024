[{"figure_path": "uatPOPWzzU/figures/figures_2_1.jpg", "caption": "Figure 1: An illustration of TFE-GNN.", "description": "This figure illustrates the architecture of the Triple Filter Ensemble Graph Neural Network (TFE-GNN). It starts with a graph (G) as input. The graph's initial features are then filtered by a triple filter ensemble mechanism (TFE). This mechanism first constructs two ensembles, one for low-pass filters (extracting homophily) and one for high-pass filters (extracting heterophily).  These two ensembles are combined in a third ensemble with learnable coefficients (v1, v2), producing a graph convolution. This filtered signal is then passed through a fully connected neural network (NN) and a softmax layer to generate the final prediction.", "section": "3 Methodology"}, {"figure_path": "uatPOPWzzU/figures/figures_9_1.jpg", "caption": "Figure 1: An illustration of TFE-GNN.", "description": "This figure illustrates the architecture of the Triple Filter Ensemble Graph Neural Network (TFE-GNN). It shows how three ensembles of filters (low-pass, high-pass, and a combined ensemble) are used to extract homophily and heterophily from graph signals. The filtered signals are then passed through a fully connected neural network and a softmax layer for prediction.  The diagram visually represents the process of combining low and high-pass filters to adaptively extract features from graphs, representing both homophilic and heterophilic relationships.", "section": "3 Methodology"}, {"figure_path": "uatPOPWzzU/figures/figures_9_2.jpg", "caption": "Figure 3: Verification loss at different learning rates, keeping the rest of the parameters constant. There are two validation loss curves on each subfigure, and each loss is the average of five experiments.", "description": "This figure shows the effect of the learning rate on the stability of the training process. Two different learning rates were tested: 0.1 (left) and 0.001 (right). The plots show the validation loss for both learning rates. The plot on the left, with a learning rate of 0.1, shows a less stable training process with more oscillations in the validation loss.  The plot on the right, with a learning rate of 0.001, shows a more stable training process with fewer oscillations, indicating that the lower learning rate leads to better convergence.", "section": "4.4 Loss Oscillation Analysis"}, {"figure_path": "uatPOPWzzU/figures/figures_17_1.jpg", "caption": "Figure 1: An illustration of TFE-GNN.", "description": "This figure illustrates the architecture of the Triple Filter Ensemble Graph Neural Network (TFE-GNN). It shows how low-pass and high-pass filters are combined through ensembles to create a graph convolution (TFE-Conv), which is then fed into a fully connected neural network for prediction. The figure highlights the adaptive extraction of homophily and heterophily from graphs with varying homophily levels.", "section": "3 Methodology"}, {"figure_path": "uatPOPWzzU/figures/figures_18_1.jpg", "caption": "Figure 1: An illustration of TFE-GNN.", "description": "This figure illustrates the architecture of the Triple Filter Ensemble Graph Neural Network (TFE-GNN). It shows how the network combines low-pass and high-pass filters to extract both homophily and heterophily information from the graph, ultimately leading to a more robust and accurate representation. The process begins with the input features of the graph, which are then passed through separate low-pass and high-pass filter ensembles.  These ensembles are combined using learnable coefficients, producing a final graph convolution, which is then fed into a fully connected neural network for prediction. The TFE-GNN is designed to adaptively extract homophily and heterophily information from graphs with different levels of homophily, and it utilizes the initial features to improve accuracy.", "section": "3 Methodology"}, {"figure_path": "uatPOPWzzU/figures/figures_18_2.jpg", "caption": "Figure 1: An illustration of TFE-GNN.", "description": "This figure illustrates the architecture of the Triple Filter Ensemble Graph Neural Network (TFE-GNN). It shows how the base low-pass and high-pass filters are combined through ensemble methods (EM1, EM2) to form the first and second ensembles. These ensembles are further combined using two learnable coefficients (v1, v2) and another ensemble method (EM3) to generate the final graph convolution (TFE-Conv). The output of TFE-Conv is then fed into a fully connected neural network (NN), which produces the final prediction after a softmax layer. The figure clearly demonstrates the adaptive extraction of homophily and heterophily from graphs using this novel triple filter ensemble mechanism.", "section": "3 Methodology"}, {"figure_path": "uatPOPWzzU/figures/figures_19_1.jpg", "caption": "Figure 1: An illustration of TFE-GNN.", "description": "This figure illustrates the architecture of the Triple Filter Ensemble Graph Neural Network (TFE-GNN). It shows how the model combines low-pass and high-pass filters to adaptively extract homophily and heterophily from graphs. The process starts with the input features, then uses three filter ensembles: the first ensemble combines low-pass filters, the second combines high-pass filters, and the third combines the outputs of the first two ensembles with learnable coefficients. Finally, a fully connected neural network and a softmax layer provide the output prediction.", "section": "3 Methodology"}]