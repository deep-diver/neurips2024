[{"figure_path": "uatPOPWzzU/tables/tables_6_1.jpg", "caption": "Table 1: Dataset statistics.", "description": "This table presents the statistics of eleven real-world datasets used in the paper for evaluating the performance of the proposed TFE-GNN model.  For each dataset, it shows the number of nodes, edges, features, classes, and the edge homophily ratio (ehr). The ehr metric indicates the level of homophily (similarity of neighboring nodes) in the graph, ranging from 0.06 (strong heterophily) to 0.93 (strong homophily).", "section": "4 Experiments"}, {"figure_path": "uatPOPWzzU/tables/tables_7_1.jpg", "caption": "Table 2: Mean accuracy of different models on datasets for full-supervised node classification.", "description": "This table presents the mean accuracy and standard deviation of various graph neural network (GNN) models on eleven real-world datasets for the task of full-supervised node classification.  The results are averaged across ten random dataset splits.  The table allows for comparison of the performance of TFE-GNN against existing state-of-the-art GNN models across datasets with varying levels of homophily (ehr).", "section": "4 Experiments"}, {"figure_path": "uatPOPWzzU/tables/tables_8_1.jpg", "caption": "Table 2: Mean accuracy of different models on datasets for full-supervised node classification.", "description": "This table presents the mean accuracy of various graph neural network (GNN) models on eleven datasets for a full-supervised node classification task.  The results are averaged over ten random splits, and error bars (standard deviations) are included to indicate variability.  The table allows for a comparison of the performance of TFE-GNN against other state-of-the-art (SOTA) GNN models.  Datasets are grouped by their edge homophily ratio (ehr), offering insight into the models' performance across different homophily levels.", "section": "4 Experiments"}, {"figure_path": "uatPOPWzzU/tables/tables_15_1.jpg", "caption": "Table 4: Mean classification accuracy of models at different Hip and Hhp.", "description": "This table shows the mean classification accuracy of different models with varying low-pass (Hip) and high-pass (Hhp) filter settings.  The results are presented for two datasets: Citeseer and Wisconsin.  Different combinations of Hip and Hhp values are tested, allowing for the analysis of how these parameters affect model performance on datasets with varying levels of homophily.", "section": "4 Experiments"}, {"figure_path": "uatPOPWzzU/tables/tables_16_1.jpg", "caption": "Table 2: Mean accuracy of different models on datasets for full-supervised node classification.", "description": "This table presents the mean accuracy achieved by various graph neural network (GNN) models on eleven real-world datasets for the task of full-supervised node classification.  The results are reported as mean accuracy \u00b1 standard deviation, calculated over ten random dataset splits.  The table allows a comparison of the performance of TFE-GNN against state-of-the-art (SOTA) methods, highlighting TFE-GNN's superior performance on most datasets.", "section": "4 Experiments"}, {"figure_path": "uatPOPWzzU/tables/tables_17_1.jpg", "caption": "Table 6: Additional dataset statistics.", "description": "This table presents the key statistics for four additional datasets used in the experiments beyond the initial eleven.  These statistics include the number of nodes, edges, features (node attributes), classes (number of categories for node labels), and the edge homophily ratio (ehr). The ehr metric quantifies the level of homophily (similarity of connected nodes) within each graph, ranging from very low (0.05) to moderately high (0.62).", "section": "4 Experiments"}, {"figure_path": "uatPOPWzzU/tables/tables_17_2.jpg", "caption": "Table 7: The hyper-parameters of TFE-GNN for additional datasets.", "description": "This table shows the hyperparameters used for TFE-GNN on four additional datasets: roman-empire, amazon-rating, fb100-Penn94, and genius.  The hyperparameters include the optimizer, the order of the low-pass and high-pass graph filters (Klp and Khp), dropout rates for input and intermediate features, learning rates and weight decay for various learnable coefficients, and learning rates and weight decay for the MLP.  These settings were used for the experiments evaluating the model's performance.", "section": "4 Experiments"}, {"figure_path": "uatPOPWzzU/tables/tables_19_1.jpg", "caption": "Table 8: Time overheads (s) on Cora.", "description": "This table presents the training time (in seconds) for different graph neural networks (GNNs) on the Cora dataset.  The time is measured for various values of K, representing the order of the Chebyshev polynomials used in the GNNs.  The table compares the training time of ChebNet, BernNet, ChebNetII, and three variants of TFE-GNN (with different configurations of Klp and Khp).  This allows for an analysis of the impact of model complexity and hyperparameter settings on training efficiency.", "section": "4 Experiments"}, {"figure_path": "uatPOPWzzU/tables/tables_20_1.jpg", "caption": "Table 2: Mean accuracy of different models on datasets for full-supervised node classification.", "description": "This table presents the mean accuracy and standard deviation of different graph neural network models on eleven benchmark datasets for the task of full-supervised node classification.  The results are averaged over ten random splits.  The datasets vary in size, number of features, and importantly, their level of homophily (as measured by the edge homophily ratio, 'ehr'). This allows for a comparison of the models' performance across different graph characteristics.", "section": "4 Experiments"}, {"figure_path": "uatPOPWzzU/tables/tables_21_1.jpg", "caption": "Table 10: The hyper-parameters of TFE-GNN for semi-supervised node classifications.", "description": "This table shows the hyperparameter settings used for the TFE-GNN model in the semi-supervised node classification experiments.  It lists the optimizer used (optim), the orders of low-pass (Klp) and high-pass (Khp) filters, dropout rates for input and intermediate features (droppro and droplin), learning rates for different sets of coefficients (\u03b7, lrada, lradae, lrlin), and weight decay values (wdada, wdadae, wdlin).  The values vary depending on the dataset used.", "section": "4.2 Ablation Study"}]