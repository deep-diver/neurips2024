[{"type": "text", "text": "Unifying Homophily and Heterophily for Spectral Graph Neural Networks via Triple Filter Ensembles ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rui Duan1, Mingjian Guang2, Junli Wang3, Chungang $\\mathbf{Yan}^{3}$ , Hongda $\\mathbf{Qi^{4}}$ , ", "page_idx": 0}, {"type": "text", "text": "Wenkang $\\mathrm{\\mathbf{Su^{1}}}$ ,\u2217 Can Tian1,\u2217 Haoran Yang ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Cyber Engineering, Guangzhou University, China, 2Donghua University, China, 3Tongji University, China 4Shanghai Normal University, China 1{duan, swk1004, tiancan}@gzhu.edu.cn; 2guangmingjian@dhu.edu.cn; 3{junliwang, yanchungang, 2010498}@tongji.edu.cn; 4hongda_qi@shnu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Polynomial-based learnable spectral graph neural networks (GNNs) utilize polynomial to approximate graph convolutions and have achieved impressive performance on graphs. Nevertheless, there are three progressive problems to be solved. Some models use polynomials with better approximation for approximating filters, yet perform worse on real-world graphs. Carefully crafted graph learning methods, sophisticated polynomial approximations, and refined coefficient constraints leaded to overftiting, which diminishes the generalization of the models. How to design a model that retains the ability of polynomial-based spectral GNNs to approximate fliters while it possesses higher generalization and performance? In this paper, we propose a spectral GNN with triple filter ensemble (TFE-GNN), which extracts homophily and heterophily from graphs with different levels of homophily adaptively while utilizing the initial features. Specifically, the first and second ensembles are combinations of a set of base low-pass and high-pass filters, respectively, after which the third ensemble combines them with two learnable coefficients and yield a graph convolution (TFE-Conv). Theoretical analysis shows that the approximation ability of TFE-GNN is consistent with that of ChebNet under certain conditions, namely it can learn arbitrary fliters. TFE-GNN can be viewed as a reasonable combination of two unfolded and integrated excellent spectral GNNs, which motivates it to perform well. Experiments show that TFE-GNN achieves high generalization and new state-of-the-art performance on various real-world datasets. The source code of GEN is publicly available at https://github.com/graphNN/TFEGNN ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph neural networks (GNNs) are competitive in graph-related tasks (Scarselli et al., 2008; Yang et al., 2023; Shirzad et al., 2023; Duan et al., 2024) and can be divided into two main categories: spatial-based (Xu et al., 2019) and spectral-based (Shirzad et al., 2023; Tao et al., 2023; He et al., 2022; Bo et al., 2023b; Guo et al., 2023) GNNs. Spectral graph convolutions in the spectral domain of the graph Laplace matrix, i.e., the spectral graph fliters, are the core component of spectral-based GNNs. We further classify spectral-based GNNs into two categories based on whether their graph convolutions can be learned or not. ", "page_idx": 0}, {"type": "text", "text": "The first class of spectral-based GNNs\u2019 graph convolutions are predetermined, i.e., they filter the graph signals (features) in a fixed way. Graph convolutional networks (GCNs) (Kipf & Welling, 2016) ", "page_idx": 0}, {"type": "text", "text": "and their variants (Rong et al., 2020; Chen et al., 2020; Yang et al., 2021) utilize only the first two Chebyshev polynomials to simplify ChebNet (Defferrard et al., 2016), and their graph convolutions are the fixed low-pass filters. ", "page_idx": 1}, {"type": "text", "text": "The second class of spectral-based GNNs\u2019 graph convolutions are learnable, i.e., their filters are variable and they fliter the graph signals in learnable way. ChebNet (Defferrard et al., 2016) utilizes Chebyshev polynomials to approximate the graph convolutions and it can learn arbitrary filters in theory (Balcilar et al., 2021; He et al., 2022). CayleyNet (Levie et al., 2018) utilizes Cayley polynomials to learn the graph convolutions, and ARMA (Bianchi et al., 2021a) learns the rational graph convolutions by using the Auto-Regressive Moving Average filters family (Narang et al., 2013). GPR-GNN (Chien et al., 2021) and BernNet (He et al., 2021a) use Monomial and Bernstein polynomials to approximate the graph convolutions. ChebNetII (He et al., 2022) revisits ChebNet and makes learned coefficients more legal through Chebyshev interpolation. EvenNet (Lei et al., 2022) ignores odd-hop neighbors and improves the robustness of GNNs by using the even-polynomial graph fliter. PCNet (Li et al., 2023) uses the Possion-Charlier polynomials to approximate the graph fliter and constrain the coefficients. Just like EvenNet, the heterophilic graph heat kernel provided by PCNet pushes odd-hop neighbors away and aggregates even-hop neighbors. FavardGNN (Guo & Wei, 2023) learns a polynomial basis from the space of possible orthonormal bases and OptBasisGNN (Guo & Wei, 2023) computes the optimal basis for a given graph structure and signal. Specformer (Bo et al., 2023a) encodes the set of eigenvalues and performs self-attention in the spectral domain, which leads to a learnable set-to-set spectral filter. ", "page_idx": 1}, {"type": "text", "text": "Despite polynomial-based learnable spectral GNNs have achieved impressive performance on graphs, there are three progressive problems to be solved. First, some polynomial-based models use polynomials with better approximation than some other models when approximating fliters, but the former\u2019s performance is lagging behind that of the latter on real-world graphs. For example, GPR-GNN and BernNet outperform ChebNet, even though they use polynomials that are weaker than Chebyshev polynomials in approximation theory (He et al., 2022). ChebNetII, an enhanced version of ChebNet, whose performance still lags behind that of PCNet using Possion-Charlier polynomials. The important factors influencing the real-world performance of such GNNs are graph learning methods, polynomial approximation and coefficient constraints, but of course there are others as well, in any case not only the polynomial approximation ability. The following two facts exist: ChebNetII outperforms GPR-GNN and BernNet through refined coefficient constraints; and PCNet outperforms ChebNetII through the carefully crafted graph learning method, i.e., pushing odd-hop neighbors away to match the structural properties of heterophilic graph. ", "page_idx": 1}, {"type": "text", "text": "The second problem was raised based on the answer to the first problem. Carefully crafted graph learning methods, sophisticated polynomial approximations, and refined coefficient constraints leaded to overfitting while improving models\u2019 performance, which diminishes the generalization of the models. FFKSF (Zeng et al., 2023) attributes the degradation of polynomial fliters\u2019 performance to the overfitting of polynomial coefficients. ChebNetII (He et al., 2022) further constrains the coefficients to enable them easier to be optimized. ARMA (Bianchi et al., 2021b) suggests that the filter will overfit the training data when aggregating high-order neighbor information. Whereas the order of polynomial-based spectral GNNs is usually large to increase the approximation of the polynomials, which directs them to obtain high-order neighborhood information, and then leads to overfitting. Therefore, it is reasonable to assume that carefully crafted graph learning methods, sophisticated polynomial approximations, and refined coefficient constraints lead to overfitting of the models, which diminishes generalization of the models. Desired learnable spectral GNNs can learn various graph convolutions, which motivates them to extract homophily from heterophilic graph and vice versa. Instead of ignoring odd-hop or even-hop neighbors, which makes the model miss important neighbor information. ", "page_idx": 1}, {"type": "text", "text": "Finally, the third problem was raised. How to design a model that retains the ability of polynomialbased spectral GNNs to approximate fliters while it possesses higher generalization and performance? In this paper, inspired by ensemble learning (Schapire, 1990; Hansen & Salamon, 1990; Zhou, 2012) (see in section 3.1), we design triple filter ensemble (TFE) mechanism to adaptively extract homophily and heterophily from graphs with different levels of homophily while utilizing the initial features, where the first and second ensembles combine a set of base low-pass and high-pass graph filter, respectively, and the third ensemble combines them by two learnable coefficients. The third ensemble of TFE will yield a graph convolution (TFE-Conv) used to fliter the graph signal. The flitered signal is fed into a fully connected linear neural network (NN), whose output is then passed through a softmax layer to obtain the prediction. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "TFE-GNN does not impose refined constraints on the coefficients and does not design very complex learning methods, which possesses higher generalization. The key difference between TFE-GNN and prior models is that TFE-GNN retains the ability of polynomial-based spectral GNNs while getting rid of polynomial computations, coefficient constraints, and specific scenarios. We describe the differences between TFE-GNN and several other recent methods (Li et al., 2024; Huang et al., 2024b,a) in Appendix B.7. TFE-GNN also offers the following three advantages. We theoretically demonstrate that the approximation ability of TFE-GNN agrees with that of ChebNet under certain conditions, as outlined in Theorem 1, i.e., can learn arbitrary fliters. Theorem 2 shows that TFE-GNN is a reasonable combination of two excellent polynomial-based spectral GNNs, which motivates it to perform well. TFE extract the initial information, homophily and heterophily from graphs adaptively, which allows TFE-GNN to be applied to various homophily level cases. Experiments show that TFE-GNN achieves new state-of-the-art performance on datasets, and the homophily levels measured by the edge homophily ratio (Zhu et al., 2020) for these datasets is 0.06, 0.21, 0.23, 0.30, 0.57, 0.74, 0.80, 0.81, and 0.93. ", "page_idx": 2}, {"type": "image", "img_path": "uatPOPWzzU/tmp/5b04ea59a9f1ee6de28d688e2085dbf041e2319281569aae29440be29faae9fe.jpg", "img_caption": ["Figure 1: An illustration of TFE-GNN. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. Given a graph $G=(E,V)$ with node set $V$ and edge set $E\\subseteq V\\times V$ . Let $n=|V|$ denote the size of the node set, i.e., the number of nodes. This paper uses $x\\in\\mathbb{R}^{n}$ to denote the graph signals, and $x(i)$ to denote the signal at node i. $.\\,Y\\in\\{0,1\\}^{n\\times\\dot{C}}$ denotes label matrix of $G$ , and $Y_{i}$ is the label vector of node $i$ , where $C$ is the number of classes. We denote $X\\,\\in\\,\\mathbb{R}^{n\\times d_{0}}$ as the initial feature matrix of $G$ , denote the adjacency matrix of $G$ as $A\\in\\{0,1\\}^{n\\times n}$ , and denotes the degree matrix as $D$ , where $\\begin{array}{r}{D_{i i}=\\sum_{j=0}^{n}\\bar{A}_{i j}}\\end{array}$ , and $d_{0}$ is the initial feature dimension. The (combinatorial) graph Laplacian is defined as $L=D-A$ , and its eigendecomposition is ${\\cal L}=U\\Lambda U^{T}$ . The columns $u_{i}$ of $\\bar{U}\\,\\bar{\\in}\\,\\mathbb{R}^{n\\times n}$ are orthonormal eigenvectors, namely the graph Fourier basis, and $\\boldsymbol{\\Lambda}=d i a g([\\lambda_{1},...,\\lambda_{n}])$ is the diagonal matrix of eigenvalues. We also call these eigenvalues frequencies. ", "page_idx": 2}, {"type": "text", "text": "2.1 Metrics of Homophily ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The homophily metrics are used to define the homophily level of a graph by considering the different relationships between node labels/features and graph structures, including edge homophily (Zhu et al., 2020), node homophily (Pei et al., 2020), class homophily (Lim et al., 2021b), etc. In this paper, we use the edge homophily ratio ehr to measure the ratio of intra-class edges contained in a graph as the homophily level: ", "page_idx": 2}, {"type": "equation", "text": "$$\ne h r=\\frac{|(u,v):(u,v)\\in E\\land Y_{u}=Y_{v}|}{|E|},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $u,v$ are nodes, $Y_{u}$ is the label vector of $u$ , and $Y_{v}$ is the label vector of $v$ . The value of ehr close to 1 corresponds to strong homophily, while the value of ehr close to 0 indicates strong heterophily. ", "page_idx": 3}, {"type": "text", "text": "2.2 Graph Filter ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The graph filters are core components of spectral GNNs. We classify the graph filters into three categories based on the type of signal they filter: low-pass, high-pass and full-pass filters. The high-pass filter $H_{h p}$ are more suitable for extracting high-frequency signals (Bo et al., 2023a; Yang et al., 2022). Empirically, the commonly used high-pass filters are the symmetric normalized Laplacian $L_{s y m}=D^{-1/2}L D^{-1/2}=I{-}D^{-1/2}A D^{-1/2}$ and the random walk normalized Laplacian $L_{r w}=D^{-1}L=I-D^{-1}A$ . The low-pass fliter $H_{l p}$ are more suitable for extracting low-frequency signals and it is the affinity (transition) matrix of $\\bar{H}_{h p}$ , i.e., $H_{l p}$ corresponding to $H_{h p}$ above are $L_{s y m}^{a}=I-L_{s y m}=D^{-1/2}A D^{-1/2}$ and $L_{r w}^{a}=I-L_{r w}=D^{-1}A$ . The full-pass fliter $H_{f p}$ is the identity matrix $I$ and retains all graph initial signals. ", "page_idx": 3}, {"type": "text", "text": "2.3 Learnable Spectral GNNs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Spectral-based GNNs create the spectral graph convolutions (filters) in the domain of Laplacian spectrum and many methods use the polynomial spectral filters to achieve graph convolutions, such as ChebNet Defferrard et al. (2016), GPR-GNN Chien et al. (2021), BernNet He et al. (2021a), ChebNetII He et al. (2022), PCNet (Li et al., 2023), FavardGNN (Guo & Wei, 2023), etc. We begin by describing how the signal $x$ is filtered by $h$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y=h(L)x=h(U\\Lambda U^{T})x=U h(\\Lambda)U^{T}x=U d i a g([h(\\lambda_{1}),...,h(\\lambda_{n})])U^{T}x,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $y$ denotes the flitering results of $x$ , and $h$ denotes the spectral fliter, which is a function on the eigenvalues of the graph Laplacian matrix $L$ . Existing studies have replaced nonparametric filters with polynomial filters: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh(\\Lambda)=\\sum_{k=0}^{K-1}\\theta_{k}\\Lambda^{k},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\boldsymbol{\\theta}_{k}\\in\\mathbb{R}^{K}$ is a vector of polynomial coefficients. We bring Equation 3 into Equation 2: ", "page_idx": 3}, {"type": "equation", "text": "$$\ny=U\\sum_{k=0}^{K-1}\\theta_{k}\\Lambda^{k}U^{T}x=\\sum_{k=0}^{K-1}\\theta_{k}U\\Lambda^{k}U^{T}x=\\sum_{k=0}^{K-1}\\theta_{k}L^{k}x,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We describe polynomial-based spectral GNNs in terms of ChebNet (Defferrard et al., 2016) and its enhanced version ChebNetII (He et al., 2022), including how they approximate graph convolution with polynomials and how constraining the coefficients. Other related methods are similar, such as EvenNet (Lei et al., 2022) and PCNet (Li et al., 2023). ChebNet uses Chebyshev polynomial to approximate the filtering operation, which is a remarkable attempt: ", "page_idx": 3}, {"type": "equation", "text": "$$\ny=\\sum_{k=0}^{K-1}\\theta_{k}T_{k}(\\tilde{L})x,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{L}\\,=\\,2L_{s y m}/\\lambda_{m a x}\\,-\\,I$ denotes the scaled graph Laplacian matrix, $\\lambda_{m a x}$ is the largest eigenvalue of $L$ and $\\theta$ is a vector of Chebyshev coefficients. Chebyshev polynomial $T_{k}({\\pmb x})$ of order $k$ can be recursively defined as $T_{k}({\\pmb x})=2{\\pmb x}T_{k-1}({\\pmb x})-T_{k-2}({\\pmb x})$ with $\\bar{T}_{0}(\\mathbf{\\dot{\\sigma}})=1$ and $T_{1}({\\pmb x})={\\pmb x}$ . ChebNet\u2019s structure is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{Y}=\\sum_{k=0}^{K-1}T_{k}(\\tilde{L})X W_{k},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W_{k}$ are the trainable weights, which contain the Chebyshev coefficients $\\theta_{k}$ . ChebNetII (He et al., 2022) proposes ChebBase to determine who is more competitive, Chebyshev basis or other bases. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{Y}=\\sum_{k=0}^{K}\\theta_{k}T_{k}(\\tilde{L})f(X),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f(X)$ is a Multi-Layer Perceptron (MLP). ChebNetII believes that ChebNet learns the illegal coefficient by analyzing a series of polynomial fliters. Therefore, it then proposes ChebBase $\\mathit{\\Delta}/k$ , which is an improvement on ChebNet and constrains the coefficients with $\\theta_{k}/k$ (Equation 7). Finally, ChebNetII is proposed. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{Y}=\\frac{2}{K+1}\\sum_{k=0}^{K}\\sum_{j=0}^{K}\\gamma_{j}T_{k}(x_{j})T_{k}(\\Tilde{L})f(X),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\gamma_{j}$ is the learnable coefficient, and it links Equation 7 and Equation 8 with $\\theta_{k}\\quad=$ $\\textstyle{\\frac{2}{K+1}}\\sum_{j=0}^{K}\\gamma_{j}T_{k}(x_{j})$ , i.e., the learnable coefficients constrain. ", "page_idx": 4}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose a spectral GNN with triple filter ensemble (TFE-GNN) to solve three progressive problems in polynomial-based learnable spectral GNNs. In this section, we describe the methodology of TFE-GNN in detail and theoretically prove its approximation capabilities, including motivations, TFE-Conv, TFE-GNN, time complexity and scalability and theoretical analysis. ", "page_idx": 4}, {"type": "text", "text": "3.1 Motivations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Polynomial-based learnable spectral GNNs (He et al., 2022; Li et al., 2023; Guo & Wei, 2023) performs well on homophilic and heterophilic graphs, because their graph convolutions are flexible and variable. However, carefully crafted graph learning methods, sophisticated polynomial approximations, and refined coefficient constraints leaded to overfitting, which diminishes GNNs\u2019 generalization. ", "page_idx": 4}, {"type": "text", "text": "Inspired by the following properties of ensemble learning (Schapire, 1990; Hansen & Salamon, 1990; Zhou, 2012): the strong classifier determined by the base classifiers can be more accurate than any of them if the base classifiers are accurate and diverse; and this strong classifier retains the characteristics of the base classifier to some extent. First, we combine a set of weak base low-pass fliter to determine a strong low-pass fliter that can extract homophily. Then, we use the same method to extract heterophily. Finally, TFE-Conv is generated by combining the above two strong filters with two learnable coefficients, which retains the characteristics of both two strong fliters, i.e., it can extract homophily and heterophily from graphs adaptively. TFE-Conv and TFE-GNN are shown in Figure 1. ", "page_idx": 4}, {"type": "text", "text": "3.2 TFE-Conv ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We design triple filter ensemble (TFE) mechanism for combining low-pass and high-pass filters to yield a graph convolution (TFE-Conv), which can match various graph structures adaptively without carefully crafted graph learning methods, i.e., can extract homophily and heterophily adaptively from graphs with different homophily level. The first ensemble of TFE is formalized as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nT F E_{1}=E M_{1}\\{\\omega_{0}I,\\omega_{1}H_{l p},\\omega_{2}(H_{l p})^{2},\\cdot\\cdot\\cdot\\,,\\omega_{K_{l p}}(H_{l p})^{K_{l p}}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $T F E_{1}$ denotes the result of combining a set of base low-pass filters, which is a graph convolution that can extract homophily while utilizing the initial signals, $E M_{1}$ denotes the ensemble method of the first ensemble, $E\\bar{M}_{1}\\{\\dot{h}_{0},h_{1},\\cdot\\cdot\\cdot,h_{K}\\}$ denotes the combination of elements $h_{0},h_{1}$ , $\\cdots,h_{K}$ with $E M_{1}$ , $\\omega$ are the learnable coefficients, and $K_{l p}$ is the order of the first ensemble. The second ensemble is similar to the first ensemble and is formalized as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nT F E_{2}=E M_{2}\\{\\omega_{0}^{\\prime}I,\\omega_{1}^{\\prime}H_{h p},\\omega_{2}^{\\prime}(H_{h p})^{2},\\cdot\\cdot\\cdot\\,,\\omega_{K_{h p}}^{\\prime}(H_{h p})^{K_{h p}}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $T F E_{2}$ denotes the result of the second ensemble, which can extract heterophily while utilizing the initial signals, $E M_{2}$ denotes the ensemble method of the second ensemble, $\\omega^{\\prime}$ are the learnable coefficients, and $K_{l p}$ is the order of the second ensemble. The third ensemble combines $T F E_{1}$ and $T F E_{2}$ with two learnable coefficients $\\vartheta_{1}$ and $\\vartheta_{2}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nT F E_{3}=E M_{3}\\{\\vartheta_{1}T F E_{1},\\vartheta_{2}T F E_{2}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $T F E_{3}$ denotes the result of the third ensemble, i.e., TFE-Conv, and $E M_{3}$ denotes the ensemble method of the third ensemble. The learnable coefficients $\\vartheta_{1}$ and $\\vartheta_{2}$ used to combine the two strong graph convolutions $T F E_{1}$ and $T F E_{2}$ guarantee $T F E_{3}$ \u2019s adaptivity. ", "page_idx": 4}, {"type": "text", "text": "3.3 TFE-GNN ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "TFE-Conv matches various graph structures adaptively, which facilitates the filtering of the graph signals (features) $X$ . The filtered signal is fed into an MLP, whose output is then passed through a softmax layer for obtaining the prediction. This is the forward propagation of TFE-GNN and is coupled with the cross-entropy loss and the backpropagation mechanism to form the complete TFE-GNN. The formalization of $X$ being filtered by TFE-Conv is: ", "page_idx": 5}, {"type": "equation", "text": "$$\nZ=T F E_{3}\\cdot X=E M_{3}\\{\\vartheta_{1}T F E_{1},\\vartheta_{2}T F E_{2}\\}\\cdot X=E M_{3}\\{\\vartheta_{1}T F E_{1}\\cdot X,\\vartheta_{2}T F E_{2}\\cdot X\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $T F E_{1}$ and $T F E_{2}$ are the graph convolutions obtained from the first and second ensembles, respectively, which are similar to $T F E_{3}$ in that they filter the graphical signal as follows: $Z_{l p}=$ $T F E_{1}\\cdot X$ and $Z_{h p}=T F E_{2}\\cdot X$ . Thus, Equation 12 can be rewritten in the following form: ", "page_idx": 5}, {"type": "equation", "text": "$$\nZ=E M_{3}\\{\\vartheta_{1}Z_{l p},\\vartheta_{2}Z_{h p}\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We decouple transformation/prediction and feature propagation (Rong et al., 2020; He et al., 2022) for TFE-GNN and formalize TFE-GNN used for node classification: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Tilde{Z}=f_{m l p}(Z)}\\\\ &{Z^{*}=s o f t m a x(\\Tilde{Z})}\\\\ &{\\mathcal{L}=-\\displaystyle\\sum_{r\\in\\mathbb{Y}_{\\mathbb{Z}}}Y_{r}^{\\top}l o g(Z_{r}^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}$ denotes the cross-entropy loss, $\\mathbb{Y}_{\\mathbb{L}}$ denotes the training set with labels, and $\\intercal$ denotes the vector transpose. ", "page_idx": 5}, {"type": "text", "text": "3.4 Time Complexity and Scalability ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Similar to some spectral GNNs (He et al., 2022; Li et al., 2023), the triple fliter ensemble mechanism does not influence the time complexity magnitude of the TFE-GNN, i.e., the time complexity of TFE-GNN is linear to $K_{l p}\\!+\\!K_{h p}$ when $E M_{1},E M_{2}$ , and $E M_{3}$ take summation. Specifically, the time complexity of message propagation is $O((K_{l p}+K_{h p})|E|C)$ , the time complexity of the combination of $H_{g f}$ with respectively $\\omega$ and $\\omega^{\\prime}$ (Equations 9 and 10) is $O((K_{l p}+\\bar{K}_{h p}\\bar{)}n C)$ , and the time complexity of the coefficient calculation is not greater than $O(K_{l p}+^{\\stackrel{.}{}}K_{h p})$ . We report the training time overhead of the different spectral GNNs in Appendix B.6. ", "page_idx": 5}, {"type": "text", "text": "We scale TFE-GNN by exchanging the order of message propagation and feature dimensionality reduction: ${\\widetilde{Z}}=E M_{3}\\{\\vartheta_{1}Z_{l p},\\vartheta_{2}Z_{h p}\\}=E M_{3}\\{\\vartheta_{1}T F E_{1}f_{m l p}(X),\\vartheta_{2}T F E_{2}f_{m l p}(X)\\}$ . We use a sparse form of the adjacency matrix of large graphs, which greatly reduces the space required for TFE-GNN. Therefore, TFE-GNN scales well to large graphs and high-dimensional feature spaces. ", "page_idx": 5}, {"type": "text", "text": "3.5 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "ChebNet has been shown to learn arbitrary fliters in theory (Balcilar et al., 2021; He et al., 2022). We find a connection between TFE-GNN and ChebNet and then analyze the conditions under which they transform into each other. We prove that TFE-GNN is equal to ChebNet under certain conditions, i.e., they have the same approximation ability, so TFE-GNN can also learn arbitrary filters. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. TFE-GNN and ChebNet can be transformed into each other under the following conditions, $(I)$ learning the proper coefficients $\\omega,\\omega^{\\prime}$ , $\\vartheta$ and ChebNet\u2019 coefficient $\\theta$ , (2) the ensemble methods $E M_{1}$ , $E M_{2}$ and $E M_{3}$ take summation, the base high-pass filter $H_{h p}$ takes the symmetric normalized Laplacian $L_{s y m}$ and the base low-pass fliter $H_{l p}$ takes the affinity (transition) matrix of $L_{s y m}.$ , and (3) $K_{h p}=K_{l p}^{\\bar{)}}=K-1$ . Thus, TFE-GNN can also learn arbitrary filters. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (proof in Appendix A.1) shows that TFE-GNN matches various graph structures adaptively while learning arbitrary fliters under certain conditions. The proof of Theorem 1 shows that ChebNet can be unfolded into a combination of high-pass or low-pass filters, or a combination of high-pass and low-pass. TFE-GNN, on the other hand, is a combination of two different unfoldings of ChebNet. Condition (1) of Theorem 1 is shown in its proof, the $E M_{1}$ and $E M_{2}$ in condition (2) are taken to summation to correspond to the expansion of the Chebyshev polynomials, while $E M_{3}$ is an ensemble method capable of preserving the properties of the model, such as summation and concatenation, and condition (3) motivates the agreement between the orders of TFE-GNN and ChebNet. ", "page_idx": 5}, {"type": "text", "text": "Polynomial-based spectral graph GNNs can all be unfolded into combinations of filters, which is similar to the idea of filter ensemble in TFE-GNN. TFE-GNN is a combination of two polynomialbased GNNs under certain conditions by displacing its base filters and setting $K_{l p}$ and $K_{h p}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. TFE-GNN can be rewritten in the following form, with certain conditions to be satisfied, which is a combination of two polynomial-based learnable spectral GNNs: $Z^{*}=$ $\\begin{array}{r l}&{s o f t m a x(f_{m l p}(\\vartheta_{1}\\sum_{k=0}^{K^{\\prime}}\\bar{\\theta}_{k}^{1}P_{k}^{1}(\\bar{H}_{g f}^{1})^{k}X\\bigoplus\\vartheta_{2}\\sum_{k=0}^{K^{\\prime\\prime}}\\bar{\\theta}_{k}^{2}P_{k}^{2}(\\bar{H}_{g f}^{2})^{k}X))}\\end{array}$ , where $P_{k}$ denote polynomials used for approximation, $\\bar{\\theta}$ are the learnable coefficients, $\\bar{H}_{g f}$ denote graph filters, and $\\oplus$ denotes $E M_{3}$ . Conditions are $(I)$ learning the proper coefficients $\\omega$ $\\bar{\\mathcal{O}},\\,\\omega^{\\prime},\\,\\vartheta,\\,\\bar{\\theta}^{1}$ , and $\\bar{\\theta}^{2}$ , (2) the ensemble methods $E M_{1}$ , $E M_{2}$ take summation and $E M_{3}$ takes ensemble method capable of preserving the properties of the model, such as summation and concatenation, the base high-pass filter $H_{h p}$ takes $\\bar{H}_{g f}^{2}$ and the base low-pass filter $H_{l p}$ takes $\\bar{H}_{g f}^{1}$ , and (3) $K_{l p}\\,=\\,K^{\\prime}$ and $K_{h p}=K^{\\prime\\prime}$ . Thus, TFE-GNN can match various graph structures adaptively. ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 2 is reported in Appendix A.1. Theorems 1 and 2 show that polynomial-based learnable spectral GNNs are able to learn different fliters, and thus perform well on both homophilic and heterophilic graphs. In contrast, fixed-filter GNNs can only filter graph signals based on their fliter forms. TFE-GNN combines different fliters directly, preserving the ability of the different fliters while reducing overfitting problem. Theorem 2 also shows that TFE-GNN will perform well on real-world datasets: it is a reasonable combination of two excellent polynomial-based spectral GNNs. We discuss the limitations of TFE-GNN in Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct experiments to evaluate the proposed TFE-GNN against the state-of-theart (SOTA) GNNs on real-world datasets and conduct ablation study, generalization, visualization (Appendix B.3), hyper-parameters (Appendix B.5), and time efficiency (Appendix B.6) analysis to verify TFE-GNN\u2019s excellent. ", "page_idx": 6}, {"type": "text", "text": "Datasets and experimental setup. We evaluate TFE-GNN on several real-world datasets for supervised node classification and chose 11 graphs with various levels of homophily, including 4 citation graphs (Kipf & Welling, 2016) Cora, Citeseer, Pubmed, and Cora-Full (Bojchevski & G\u00fcnnemann, 2017), 2 Co-authorship graphs (Shchur et al., 2018) Coauthor CS and Coauthor Physics, 2 Wikipedia graphs (Rozemberczki et al., 2021) Chameleon and Squirrel, and 3 WebKB graphs (Pei et al., 2020) Texas, Cornell, and Wisconsin. The dataset statistics are summarized in Table 1. In addition, we choose four additional datasets, i.e. roman-empire (Platonov et al., 2023), amazon-rating (Platonov et al., 2023), fb100-Penn94 (Lim et al., 2021a) and genius (Lim et al., 2021a), to further validate the classification performance, generalization and scalability of TFEGNN. Detailed dataset statistics and experimental results are reported in the Appendix B.1. All experiments are carried out on the machine with Linux system, two NVIDIA Tesla V100 and twelve Intel(R) Xeon(R) Gold 5220 CPU $\\scriptstyle\\ @2.20\\mathrm{GHz}$ . ", "page_idx": 6}, {"type": "table", "img_path": "uatPOPWzzU/tmp/aee94fc3087a94f07be2145acb4892d11c0e3cddfaf1e07d6f63d696dead22bc.jpg", "table_caption": ["Table 1: Dataset statistics. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "TFE-GNN Settings. There are many options available for $H_{l p}$ , $H_{h p}$ , $E M_{1}$ , $E M_{2}$ , and $E M_{3}$ in TFE-GNN, and we choose common and frequently used options for them to make a broad and fair comparison between TFE-GNN and other SOTA GNNs. $H_{h p}$ and $H_{l p}$ take the symmetric normalized Laplacian $L_{s y m}$ and $L_{s y m}^{a}$ respectively. We add self-loops to the graph in practice, so $L_{s y m}=I-\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$ and $L_{s y m}^{a}=\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$ , where $\\tilde{A}=A+I$ , and $\\begin{array}{r}{\\tilde{D}_{i i}=\\sum_{j=0}^{n}\\tilde{A}_{i j}}\\end{array}$ . We use generalized normalization about $L_{s y m}$ to alleviate the overcorrelation issue in spectral GNNs (Yang et al., 2022; Li et al., 2023) and keep $L_{s y m}^{a}$ unchanged, namely $L_{s y m}\\,=\\,I-\\tilde{D}^{-\\eta}\\tilde{A}\\tilde{D}^{-\\eta}$ . $E M_{1}$ and $E M_{2}$ take summation, and $E M_{3}$ takes summation or concatenation. Thus, TFE-Conv ", "page_idx": 6}, {"type": "table", "img_path": "uatPOPWzzU/tmp/cfe113fa187ebf72c9349f745f698f576e3d947b16fd3744203e0b6b9a4992a1.jpg", "table_caption": ["Table 2: Mean accuracy of different models on datasets for full-supervised node classification. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "filters the graph signal $X$ in the following way: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\zeta=\\left\\{\\begin{array}{l l}{\\displaystyle\\vartheta_{1}Z_{l p}+\\vartheta_{2}Z_{h p}=\\big(\\vartheta_{1}T F E_{1}+\\vartheta_{2}T F E_{2}\\big)\\cdot X=\\big(\\vartheta_{1}\\sum_{i=0}^{K_{l p}}\\omega_{i}(H_{l p})^{i}+\\vartheta_{2}\\sum_{j=0}^{K_{h p}}\\omega_{j}^{\\prime}(H_{h p})^{j}\\big)\\cdot X}\\\\ {\\displaystyle\\vartheta_{1}Z_{l p}\\|\\vartheta_{2}Z_{h p}=\\big(\\vartheta_{1}T F E_{1}\\|\\vartheta_{2}T F E_{2}\\big)\\cdot X=\\big(\\vartheta_{1}\\sum_{i=0}^{K_{l p}}\\omega_{i}(H_{l p})^{i}\\|\\vartheta_{2}\\sum_{j=0}^{K_{h p}}\\omega_{j}^{\\prime}(H_{h p})^{j}\\big)\\cdot X,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\parallel$ denotes concatenation and has lower arithmetic priority than addition, subtraction, multiplication and division. ", "page_idx": 7}, {"type": "text", "text": "4.1 Supervised Node Classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setting and baselines. We compare TFE-GNN to a series of SOTA models for full-supervised node classification on datasets with random splits, including 11 polynomial approximation filter methods GCNs (Kipf & Welling, 2016), ARMA (Bianchi et al., 2021a), APPNP (Klicpera et al., 2018), ChebNet (Defferrard et al., 2016), GPR-GNN (Chien et al., 2021), BernNet (He et al., 2021a), ChebNetII (He et al., 2022), SPECFORMER (Bo et al., 2023a), EvenNet (Lei et al., 2022), FavardGNN/OptBasisGNN (Guo & Wei, 2023), and PCNet (Li et al., 2023). We also add 5 competitive SOTA models Half-Hop (Azabou et al., 2023), GCNII (Chen et al., 2020), TWIRLS (Yang et al., 2021), PDE-GCN (Eliasof et al., 2021), and EGNN (Zhou et al., 2021). We randomly split each class of nodes into $60\\%$ , $20\\%$ , and $20\\%$ as training, validation, and testing sets for full-supervised node classification and all models share the same ten random splits for a fair comparison. ", "page_idx": 7}, {"type": "text", "text": "For TFE-GNN, we set the hidden units to be 64 or 512 (Squirrel, Chaneleon, roman-empire, and amazon-rating), the number of early stoppings is 200 and the number of epochs is 1000 for all datasets. We employ the ReLu as an activation function for $f_{m l p}$ . We use the officially released code for GCNII, GPR-GNN, BernNet, etc and use the Deep Graph Library implementations for other models, such as GCNs, APPNP, ChebNet, etc. More experimental details of hyper-parameters and code URLs are listed in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Results. Tables 2 and 3 report the results of the different models on all datasets and gives mean classification accuracy and standard deviation over ten random splits, where the bolded numbers indicate the best results, \u201cFavard\u201d denotes FavardGNN/OptBasisGNN, TFE- ${\\mathrm{GNN}}_{s u m}$ denote that ", "page_idx": 7}, {"type": "table", "img_path": "uatPOPWzzU/tmp/9172f3e70e7dc045e37ecf092db2ef4770b71b6e34d7e0e705ee0cf79c698c55.jpg", "table_caption": ["Table 3: Mean accuracy of different models on datasets for full-supervised node classification. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "$E M_{3}$ of TFE-GNN is summation, and TFE- $\\mathrm{GNN}_{c o n}$ denote that $E M_{3}$ of TFE-GNN is concatenation. The experimental results are taken from ChebNetII, Half-Hop, SPECFORMER, EvenNet, and avardGNN/OptBasisGNN, when they report relevant results, and the remaining results are reproduced by us. Tables 2 and 3 illustrate that ChebNet starts to outperform GCNs when there is more training data available, which suggests the validity of the Chebyshev approximation. TFE-GNN achieves new state-of-the-art results on all datasets. TFE-GNN outperforms some SOTA models with similar results on Cora and Citeseer, including GPR-GNN (Chien et al., 2021), BernNet (He et al., 2021a), ChebNetII (He et al., 2022) and Half-Hop (Azabou et al., 2023). Notably, TFE-GNN outperforms ChebNetII on Chameleon and Squirrel by $5.79\\%$ and $14.85\\%$ , respectively, which amounts to performance improvements of $8\\%$ and $26\\%$ . TFE-GNN achieves exciting results on Physics (strong homophily) and Wisconsin (strong heterophily) for full-supervised node classification, already close to $100\\%$ accuracy. We put the relevant settings and results of TFE-GNN for semi-supervised node classification in Appendix B.2, due to space limitations. ", "page_idx": 8}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct experiments to investigate the (joint) contributions of TFE-GNN\u2019s components. The last three rows of Tables 2 and 3 report the results of the ablation experiments. The symbol \u201cTFE$\\mathrm{GNN}\\backslash T F E_{1}^{\\,,*}$ means $\\vartheta_{1}\\:=\\:0$ or $K_{l p}~=~0$ , \u201cTFE $.\\mathbf{GNN}\\backslash T F E_{2}^{\\bullet})$ means $\\vartheta_{2}\\;=\\;0$ or $K_{h p}\\;=\\;0$ , and \u201cTFE- $\\mathrm{{GNN}}_{r w+s u m}{}^{,,}$ indicates that TFE- $\\mathrm{GNN}_{s u m}$ uses the random walk normalized Laplacian $L_{r w}=D^{-1}L=I\\dot{-}D^{-1}A$ as the high-pass graph fliter $H_{h p}$ . Partial ablation experiments yielded the same results due to the choice of hyperparameters $K_{l p}$ and $K_{h p}$ . We observe that TFE- $\\mathrm{GNN}\\backslash T F E_{1}$ performs worse under strong homophily, while TFE- $\\mathbf{GNN}\\backslash T F E_{2}$ performs worse under strong heterophily. TFE-GNN with all graph filters achieves the best results, which suggests that it can match various graph structure. ", "page_idx": 8}, {"type": "text", "text": "4.3 Generalization Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We verify the generalization of TFE-GNN by analyzing its cross-entropy loss in the training and validation sets on Cora, and a smaller gap between the two losses indicates a better generalization of the model (Feng et al., 2020). Figure 2 shows the significant gap between the training and validation losses for ChebNet, ChebNetII, and PCNet, which indicates a possible overfitting and diminishes their generalization. The validation loss of TFE-GNN is much closer to its training loss and the early stopping mechanism allows the model to carry less stable losses. More generalization analyses are reported in Appendix B.4. ", "page_idx": 8}, {"type": "image", "img_path": "uatPOPWzzU/tmp/da71b84b7a6fcd7e6e0512431f056db2609ed91a40908020a44f72325f3c7fc4.jpg", "img_caption": ["Figure 2: Generalization on Cora. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.4 Loss Oscillation Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We try to explain the reason for the oscillation in TFE-GNN\u2019s losses in Figure 2. The learning rate controls the step size which in turn affects the loss optimization. The large learning rate $(=0.1)$ is responsible for the oscillations of the validation loss in Figure 2. Figure 3 shows that the loss is stable when the learning rate is 0.001. The early stopping mechanism allows TFE-GNN to carry less stable losses and losses do not fall into unacceptable local minimum. ", "page_idx": 9}, {"type": "image", "img_path": "uatPOPWzzU/tmp/2d964cc6a7506042427c319441abe6a7ddd6bea6494afd59827e4b61791a636d.jpg", "img_caption": ["Figure 3: Verification loss at different learning rates, keeping the rest of the parameters constant. There are two validation loss curves on each subfigure, and each loss is the average of five experiments. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose TFE-GNN with triple filter ensembles (TFE) to solve three progressive problems. TFEGNN extracts homophily and heterophily from graphs with different homophily levels adaptively while utilizing the initial features, which motivates it to match various graph structure. We theoretically prove that TFE-GNN can learn arbitrary filters and is a combination of two polynomial-based spectral GNNs. Experiments show that TFE-GNN achieves new state-of-the-art performance on various real-world datasets. In the future, we will dig deeper into ensemble methods of triple filter ensembles and expect to further improve the performance of TFE-GNN. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank the School of Computer Science of Guangzhou University and the School of Telecommunication of Tongji University for their help, including the experimental environment, office location, and writing guidance. This work was supported in part by the Education Bureau of Guangzhou Municipality under Grant 2024312243 and in part by the National Natural Science Foundation of China under Grant 62202507. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Azabou, M., Ganesh, V., Thakoor, S., Lin, C.-H., Sathidevi, L., Liu, R., Valko, M., Veli\u02c7ckovi\u00b4c, P., and Dyer, E. L. Half-hop: A graph upsampling approach for slowing down message passing. International Conference on Machine Learning, pp. 1341\u20131360, 2023.   \nBalcilar, M., Guillaume, R., H\u00e9roux, P., Ga\u00fcz\u00e8re, B., Adam, S., and Honeine, P. Analyzing the expressive power of graph neural networks in a spectral perspective. Proceedings of the International Conference on Learning Representations, 2021.   \nBianchi, F. M., Grattarola, D., Livi, L., and Alippi, C. Graph neural networks with convolutional arma fliters. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3496\u20133507, 2021a.   \nBianchi, F. M., Grattarola, D., Livi, L., and Alippi, C. Graph neural networks with convolutional arma filters. IEEE transactions on pattern analysis and machine intelligence, 44(7):3496\u20133507, 2021b.   \nBo, D., Wang, X., Shi, C., and Shen, H. Beyond low-frequency information in graph convolutional networks. Proceedings of the AAAI Conference on Artificial Intelligence, 35(5):3950\u20133957, 2021.   \nBo, D., Shi, C., Wang, L., and Liao, R. Specformer: Spectral graph neural networks meet transformers. In International Conference on Learning Representations, 2023a.   \nBo, D., Wang, X., Liu, Y., Fang, Y., Li, Y., and Shi, C. A survey on spectral graph neural networks. arXiv preprint arXiv:2302.05631, 2023b.   \nBojchevski, A. and G\u00fcnnemann, S. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. International Conference on Learning Representations, 2017.   \nChen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y. Simple and deep graph convolutional networks. International Conference on Machine Learning, pp. 1725\u20131735, 2020.   \nCheng, D., Yang, F., Xiang, S., and Liu, J. Financial time series forecasting with multi-modality graph neural network. Pattern Recognition, 121:108218, 2022.   \nChien, E., Peng, J., Li, P., and Milenkovic, O. Adaptive universal generalized pagerank graph neural network. arXiv preprint arXiv:2006.07988, 2020.   \nChien, E., Peng, J., Li, P., and Milenkovic, O. Adaptive universal generalized pagerank graph neural network. International Conference on Learning Representations, 2021.   \nDefferrard, M., Bresson, X., and Vandergheynst, P. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in Neural Information Processing Systems, 29, 2016.   \nDuan, R., Yan, C., Wang, J., and Jiang, C. Path-aware multi-hop graph towards improving graph learning. Neurocomputing, 494:13\u201322, 2022.   \nDuan, R., Yan, C., Wang, J., and Jiang, C. Class-homophilic-based data augmentation for improving graph neural networks. Knowledge-Based Systems, 269:110518, 2023.   \nDuan, R., Yan, C., Wang, J., and Jiang, C. Graph ensemble neural network. Information Fusion, 110: 102461, 2024. ISSN 1566-2535.   \nEliasof, M., Haber, E., and Treister, E. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. Advances in Neural Information Processing Systems, 34:3836\u20133849, 2021.   \nFeng, W., Zhang, J., Dong, Y., Han, Y., Luan, H., Xu, Q., Yang, Q., Kharlamov, E., and Tang, J. Graph random neural network for semi-supervised learning on graphs. NeurIPS, 2020.   \nGuang, M., Yan, C., Xu, Y., Wang, J., and Jiang, C. A multichannel convolutional decoding network for graph classification. IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u201311, 2023. doi: 10.1109/TNNLS.2023.3266243.   \nGuang, M., Yan, C., Xu, Y., Wang, J., and Jiang, C. Graph convolutional networks with adaptive neighborhood awareness. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1\u201313, 2024. doi: 10.1109/TPAMI.2024.3391356.   \nGuo, J., Huang, K., Yi, X., and Zhang, R. Graph neural networks with diverse spectral filtering. pp. 306\u2013316, 2023.   \nGuo, Y. T. and Wei, Z. Graph neural networks with learnable and optimal polynomial bases. International Conference on Machine Learning, 2023.   \nHansen, L. K. and Salamon, P. Neural network ensembles. IEEE transactions on pattern analysis and machine intelligence, 12(10):993\u20131001, 1990.   \nHe, M., Wei, Z., Xu, H., et al. Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. Advances in Neural Information Processing Systems, 34:14239\u201314251, 2021a.   \nHe, M., Wei, Z., Xu, H., et al. Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. Advances in Neural Information Processing Systems, 34:14239\u201314251, 2021b.   \nHe, M., Wei, Z., and Wen, J.-R. Convolutional neural networks on graphs with chebyshev approximation, revisited. Advances in Neural Information Processing Systems, 35:7264\u20137276, 2022.   \nHuang, C., Wang, Y., Jiang, Y., Li, M., Huang, X., Wang, S., Pan, S., and Zhou, C. Flow2gnn: Flexible two-way flow message passing for enhancing gnns beyond homophily. IEEE Transactions on Cybernetics, 2024a.   \nHuang, K., Wang, Y. G., Li, M., et al. How universal polynomial bases enhance spectral graph neural networks: Heterophily, over-smoothing, and over-squashing. ICML, 2024b.   \nKipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations, 2016.   \nKlicpera, J., Bojchevski, A., and G\u00fcnnemann, S. Predict then propagate: Graph neural networks meet personalized pagerank. International Conference on Machine Learning, 2018.   \nLei, R., Wang, Z., Li, Y., Ding, B., and Wei, Z. Evennet: Ignoring odd-hop neighbors improves robustness of graph neural networks. In Advances in Neural Information Processing Systems, volume 35, pp. 4694\u20134706, 2022.   \nLevie, R., Monti, F., Bresson, X., and Bronstein, M. M. Cayleynets: Graph convolutional neural networks with complex rational spectral filters. IEEE Transactions on Signal Processing, 67(1): 97\u2013109, 2018.   \nLi, B., Pan, E., and Kang, Z. Pc-conv: Unifying homophily and heterophily with two-fold filtering. arXiv preprint arXiv:2312.14438, 2023.   \nLi, J., Zheng, R., Feng, H., Li, M., and Zhuang, X. Permutation equivariant graph framelets for heterophilous graph learning. IEEE Transactions on Neural Networks and Learning Systems, 2024.   \nLim, D., Hohne, F., Li, X., Huang, S. L., Gupta, V., Bhalerao, O., and Lim, S. N. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances in Neural Information Processing Systems, 34:20887\u201320902, 2021a.   \nLim, D., Li, X., Hohne, F., and Lim, S.-N. New benchmarks for learning on non-homophilous graphs. arXiv preprint arXiv:2104.01404, 2021b.   \nLuan, S., Zhao, M., Hua, C., Chang, X.-W., and Precup, D. Complete the missing half: Augmenting aggregation filtering with diversification for graph convolutional neural networks. arXiv preprint arXiv:2212.10822, 2022.   \nNarang, S. K., Gadde, A., and Ortega, A. Signal processing techniques for interpolation in graph structured data. 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 5445\u20135449, 2013.   \nPei, H., Wei, B., Chang, K. C.-C., Lei, Y., and Yang, B. Geom-gcn: Geometric graph convolutional networks. International Conference on Machine Learning, 2020.   \nPlatonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evaluation of gnns under heterophily: Are we really making progress? ICLR, 2023.   \nRong, Y., Huang, W., Xu, T., and Huang, J. Dropedge: Towards deep graph convolutional networks on node classification. International Conference on Learning Representations, 2020.   \nRozemberczki, B., Allen, C., and Sarkar, R. Multi-scale attributed node embedding. Journal of Complex Networks, 9(2):cnab014, 2021.   \nSaltelli, A., Annoni, P., Azzini, I., Campolongo, F., Ratto, M., and Tarantola, S. Variance based sensitivity analysis of model output. design and estimator for the total sensitivity index. Computer Physics Communications, 181(2):259\u2013270, 2010.   \nScarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE Transactions on Neural Networks and Learning Systems, 20(1):61\u201380, 2008.   \nSchapire, R. E. The strength of weak learnability. Machine learning, 5(2):197\u2013227, 1990.   \nShchur, O., Mumme, M., Bojchevski, A., and G\u00fcnnemann, S. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.   \nShirzad, H., Velingker, A., Venkatachalam, B., Sutherland, D. J., and Sinop, A. K. Exphormer: Sparse transformers for graphs. International Conference on Machine Learning, 2023.   \nTang, J., Sun, J., Wang, C., and Yang, Z. Social influence analysis in large-scale networks. Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 807\u2013816, 2009.   \nTao, Q., Wang, Z., Yu, W., Li, Y., and Wei, Z. Lon-gnn: Spectral gnns with learnable orthonormal basis. ArXiv, 2023.   \nTraud, A. L., Mucha, P. J., and Porter, M. A. Social structure of facebook networks. Physica A: Statistical Mechanics and its Applications, 391(16):4165\u20134180, 2012.   \nWang, M. Y. Deep graph library: Towards efficient and scalable deep learning on graphs. International Conference on Learning Representations Workshop on representation learning on graphs and manifolds, 2019.   \nWeston, J., Ratle, F., Mobahi, H., and Collobert, R. Deep learning via semi-supervised embedding. pp. 639\u2013655. Springer, 2012.   \nWu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Weinberger, K. Simplifying graph convolutional networks. International Conference on Machine Learning, pp. 6861\u20136871, 2019.   \nXu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? International Conference on Learning Representations, 2019.   \nXu, Y., Wang, J., Guang, M., Yan, C., and Jiang, C. Multistructure graph classification method with attention-based pooling. IEEE Transactions on Computational Social Systems, 10(2):602\u2013613, 2022.   \nXu, Y., Wang, J., Guang, M., Yan, C., and Jiang, C. Graph contrastive learning with min-max mutual information. Information Sciences, pp. 120378, 2024.   \nYang, M., Shen, Y., Li, R., Qi, H., Zhang, Q., and Yin, B. A new perspective on the effects of spectrum in graph neural networks. In International Conference on Machine Learning, pp. 25261\u201325279. PMLR, 2022.   \nYang, M., Feng, W., Shen, Y., and Hooi, B. Towards better graph representation learning with parameterized decomposition $\\&$ filtering. International Conference on Machine Learning, 202: 39234\u201339251, 23\u201329 Jul 2023.   \nYang, Y., Liu, T., Wang, Y., Zhou, J., Gan, Q., Wei, Z., Zhang, Z., Huang, Z., and Wipf, D. Graph neural networks inspired by classical iterative algorithms. International Conference on Learning Representations, 139:11773\u201311783, 18\u201324 Jul 2021.   \nZeng, Z., Peng, Q., Mou, X., Wang, Y., and Li, R. Graph neural networks with high-order polynomial spectral filters. IEEE Transactions on Neural Networks and Learning Systems, 2023.   \nZhou, K., Huang, X., Zha, D., Chen, R., Li, L., Choi, S.-H., and Hu, X. Dirichlet energy constrained learning for deep graph neural networks. Advances in Neural Information Processing Systems, 34: 21834\u201321846, 2021.   \nZhou, Z.-H. Ensemble methods: foundations and algorithms. 2012.   \nZhu, H. and Koniusz, P. Simple spectral graph convolution. International Conference on Learning Representations, 2020.   \nZhu, J., Yan, Y., Zhao, L., Heimann, M., Akoglu, L., and Koutra, D. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proofs and Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proofs of Theorems 1 and 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 1. TFE-GNN and ChebNet can be transformed into each other under the following conditions, $(I)$ learning the proper coefficients $\\omega,\\omega^{\\prime}$ , $\\vartheta$ and ChebNet\u2019 coefficient $\\theta$ , (2) the ensemble methods $E M_{1}$ , $E M_{2}$ and $E M_{3}$ take summation, the base high-pass filter $H_{h p}$ takes the symmetric normalized Laplacian $L_{s y m}$ and the base low-pass fliter $H_{l p}$ takes the affinity (transition) matrix of $L_{s y m}$ , and (3) $K_{h p}=K_{l p}^{\\bar{)}}=K-1$ . Thus, TFE-GNN can also learn arbitrary filters. ", "page_idx": 14}, {"type": "text", "text": "proof: We first prove that ChebNet can be transformed into TFE-GNN under conditions (1), (2), and (3). Instead of ChebNet, we use its excellent version Chebyshev basis (Equation 7). We change Equation 7 to the following form that does not affect approximation ability: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{Y}=s o f t m a x(f_{m l p}(\\sum_{k=0}^{K}\\theta_{k}T_{k}(\\tilde{L})X)).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Equation 16 can be rewritten in the form of equal approximation ability using the learnable parameters (constants) $\\vartheta_{1}$ and $\\vartheta_{2}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{Y}=s o f t m a x(f_{m l p}(\\vartheta_{1}\\sum_{k=0}^{K}\\theta_{k}T_{k}(\\tilde{L})X+\\vartheta_{2}\\sum_{k=0}^{K}\\theta_{k}T_{k}(\\tilde{L})X))=s o f t m a x(f_{m l p}(\\vartheta_{1}\\tilde{Y}+\\vartheta_{2}\\tilde{Y})).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We parse and reconstruct the key parts of Equation 17 by unfolding and integrating technologies, i.e., analyzing $\\begin{array}{r}{\\sum_{k=0}^{K}\\theta_{k}T_{k}(\\tilde{L})X=\\tilde{Y}}\\end{array}$ . We get the follow equation by unfolding the summation and Chebyshev polynomial $T(k)(\\tilde{L})$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{Y}=(\\theta_{0}T_{0}(\\tilde{L})+\\theta_{1}T_{1}(\\tilde{L})+\\cdots+\\theta_{K-1}T_{K-1}(\\tilde{L}))X}\\\\ &{\\quad=(\\theta_{0}I+\\theta_{1}\\tilde{L}+\\theta_{2}(2\\tilde{L}-I)+\\cdots+\\theta_{K-1}(2\\tilde{L}(T_{K-2}(\\tilde{L}))-T_{K-3}(\\tilde{L})))X,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $T_{0}(\\tilde{L})\\,=\\,I,\\,T_{1}(\\tilde{L})\\,=\\,\\tilde{L},\\,T_{2}(\\tilde{L})\\,=\\,2\\tilde{L}^{2}\\,-\\,I,$ , and so on. Equation 18 can be viewed as a $(K-1)$ -th order polynomial of $\\tilde{Y}$ with respect to $\\tilde{L}$ . We then integrate the coefficients of different orders $\\tilde{L}$ in Equation 18: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{Y}=((\\theta_{0}-\\theta_{2}+\\theta_{4}-\\theta_{6}+\\cdot\\cdot\\cdot)I+\\cdot\\cdot\\cdot+\\theta_{K-1}2^{K-2}\\tilde{L}^{K-1})X}\\\\ &{\\quad=(\\theta_{0}^{\\prime}I+\\theta_{1}^{\\prime}\\tilde{L}+\\cdot\\cdot\\cdot+\\theta_{K-1}^{\\prime}\\tilde{L}^{K-1})X,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first coefficient of the polynomial $\\theta_{0}^{\\prime}\\equiv(\\theta_{0}-\\theta_{2}+\\theta_{4}-\\theta_{6}+\\cdot\\cdot\\cdot)$ in Equation 19, the coefficient of the highest order $\\bar{\\theta_{K-1}^{\\bar{\\prime}}}=\\theta_{K-1}\\breve{2}^{K-2}$ , and so on. We can calculate the relationship between coefficients $\\theta_{k}^{\\prime}$ and $\\theta_{k}(0<k<K-1)$ based on the generalized form of the Chebyshev polynomials. ", "page_idx": 14}, {"type": "equation", "text": "$$\nT_{K}(\\pmb{\\mathscr{x}})=\\sum_{k=0}^{\\lfloor n/2\\rfloor}[(-1)^{k}\\sum_{j=k}^{\\lfloor n/2\\rfloor}\\binom{n}{2j}\\binom{j}{k}]\\pmb{\\mathscr{x}}^{K-2k},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\lfloor n/2\\rfloor$ denotes downward rounding, and $\\scriptstyle{\\binom{n}{2j}}$ denotes combinatorial numbers. We get the connection between $\\hat{Y}_{1}$ and $Z_{h p}$ according to the given conditions (1), (2), and (3): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{Y}=(\\omega_{0}^{\\prime}I+\\omega_{1}^{\\prime}(H_{h p})^{1}+\\cdot\\cdot\\cdot+\\omega_{K_{h p}}^{\\prime}(H_{h p})^{K_{h p}})X=T F E_{2}\\cdot X=Z_{h p},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the conversion of the coefficients $\\theta^{\\prime}$ to $\\omega^{\\prime}$ are realized through equation $\\tilde{L}=2L_{s y m}/\\lambda_{m a x}-I$ We continue to unfold Equation 19: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\tilde{Y}=(\\theta_{0}^{\\prime}I+\\theta_{1}^{\\prime}(\\frac{2L}{\\lambda_{m a x}}-I)+\\cdots+\\theta_{K-1}^{\\prime}(\\frac{2L}{\\lambda_{m a x}}-I)^{K-1})X}}\\\\ {{=(\\theta_{0}^{\\prime}I+\\theta_{1}^{\\prime}(\\frac{2-\\lambda_{m a x}}{\\lambda_{m a x}}I-\\frac{2}{\\lambda_{m a x}}(D^{-\\frac{1}{2}}A D^{-\\frac{1}{2}}))+\\cdots+}}\\\\ {{\\theta_{K-1}^{\\prime}(\\frac{2-\\lambda_{m a x}}{\\lambda_{m a x}}I-\\frac{2}{\\lambda_{m a x}}(D^{-\\frac{1}{2}}A D^{-\\frac{1}{2}}))^{K-1})X}}\\\\ {{=(\\theta_{0}^{\\prime\\prime}I+\\theta_{1}^{\\prime\\prime}(D^{-\\frac{1}{2}}A D^{-\\frac{1}{2}})+\\cdots+\\theta_{K-1}^{\\prime\\prime}(D^{-\\frac{1}{2}}A D^{-\\frac{1}{2}})^{K-1})X,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\theta_{0}^{\\prime\\prime}=\\theta_{0}^{\\prime}+\\frac{2-\\lambda_{m a x}}{\\lambda_{m a x}}\\theta_{1}^{\\prime}+\\cdot\\cdot\\cdot+(\\frac{2-\\lambda_{m a x}}{\\lambda_{m a x}})^{K-1}\\theta_{K-1}^{\\prime}}\\end{array}$ , and $\\begin{array}{r}{\\theta_{K-1}^{\\prime\\prime}=(\\frac{2}{\\lambda_{m a x}})^{K-1}\\theta_{K-1}^{\\prime}}\\end{array}$ . We get the connection between $\\tilde{Y}$ and $Z_{l p}$ according to the given conditions (1), (2), and (3): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{Y}=(\\theta_{0}^{\\prime\\prime}I+\\theta_{1}^{\\prime\\prime}(H_{l p})+\\cdot\\cdot+\\theta_{K-1}^{\\prime\\prime}(H_{l p})X}\\\\ &{\\quad=(\\omega_{0}I+\\omega_{1}(H_{l p})^{1}+\\cdot\\cdot\\cdot+\\omega_{K_{l p}}(H_{h p})^{K_{h p}})X=T F E_{1}\\cdot X=Z_{l p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\omega=\\theta^{\\prime\\prime}$ . We get a new conclusion about $\\hat{Y}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{Y}=s o f t m a x(f_{m l p}(\\vartheta_{1}Z_{l p}+\\vartheta_{2}Z_{h p}))=s o f t m a x(f_{m l p}(Z))=s o f t m a x(\\tilde{Z})=Z^{\\ast}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "ChebNet and its excellent version Chebyshev basis are essentially the same (Defferrard et al., 2016), except that Chebyshev basis implements the decoupling (He et al., 2022). Therefore, we proved that ChebNet can be transformed into TFE-GNN under conditions (1), (2), and (3), and vice versa. ", "page_idx": 15}, {"type": "text", "text": "Theorem 2. TFE-GNN can be rewritten in the following form, with certain conditions to be satisfied, which is a combination of two polynomial-based learnable spectral GNNs: $Z^{*}=$ sof $\\begin{array}{r}{t m a x(f_{m l p}(\\vartheta_{1}\\sum_{k=0}^{K^{\\prime}}\\bar{\\theta}_{k}^{1}P_{k}^{1}\\big(\\bar{H}_{g f}^{1}\\big)^{k}X\\bigoplus\\vartheta_{2}\\sum_{k=0}^{K^{\\prime\\prime}}\\bar{\\theta}_{k}^{2}P_{k}^{2}(\\bar{H}_{g f}^{2})^{k}X))}\\end{array}$ , where $P_{k}$ denote polynomials used for approximation, $\\bar{\\theta}$ are the learnable coefficients, $\\bar{H}_{g f}$ denote graph filters, and $\\oplus$ denotes $E M_{3}$ . Conditions are $(I)$ learning the proper coefficients $\\omega$ $\\omega,\\,\\omega^{\\prime},\\,\\vartheta,\\,\\bar{\\theta}^{1}$ , and $\\bar{\\theta}^{2}$ , (2) the ensemble methods $E M_{1}$ , $E M_{2}$ take summation and $E M_{3}$ takes ensemble method capable of preserving the properties of the model, such as summation and concatenation, the base high-pass filter $H_{h p}$ takes $\\bar{H}_{g f}^{2}$ and the base low-pass filter $H_{l p}$ takes $\\bar{H}_{g f}^{1}$ , and (3) $K_{l p}\\,=\\,K^{\\prime}$ and $K_{h p}=K^{\\prime\\prime}$ . Thus, TFE-GNN can match various graph structures adaptively. ", "page_idx": 15}, {"type": "text", "text": "proof: The proof of Theorem 2 is similar to that of Theorem 1. First, we unfold the polynomials $\\bar{P}_{k}^{1}$ and $P_{k}^{2}$ in $Z^{*}$ to get a combination of $K^{\\prime}$ -th order polynomial with respect to $\\bar{H}_{g f}^{1^{\\prime}}$ and $K^{\\prime\\prime}$ -th order polynomial with respect to $\\bar{H}_{g f}^{2}$ , which is similar to Equation 18. $P_{k}$ can be the Chebyshev, Bernstein, even, Possion-Charlier polynomials in ChebNet (Defferrard et al., 2016), BernNet (He et al., 2021a), EvenNet (Lei et al., 2022), PCNet (Li et al., 2023), and so on. We then sum their coefficients using $(\\bar{H}_{g f}^{1})^{k}(k\\in[0,K^{\\prime}])$ and $(\\bar{H}_{g f}^{2})^{k}(k\\in[0,K^{\\prime\\prime}])$ as variables respectively, which is similar to Equations 19 and 22. Finally, we replace $\\bar{\\theta}^{1}$ and $\\bar{\\theta}^{2}$ with $\\omega$ and $\\omega^{\\prime}$ , which is similar to Equations 21 and 23. Therefore, TFE-GNN is a combination of two polynomial-based learnable spectral GNNs. Theorems 1 and 2 state that TFE-GNN can learn any fliter and matches various graph structure and extracts homophily and heterophily from graphs adaptively. ", "page_idx": 15}, {"type": "table", "img_path": "uatPOPWzzU/tmp/d1692b9d692b4dccd7bfb21cf713509743774470c6dd4552958bc9de5013135d.jpg", "table_caption": ["Table 4: Mean classification accuracy of models at different $H_{l p}$ and $H_{h p}$ . "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We design triple fliter ensembles to generate a TFE-Conv, which can match various graph structures and extract the initial information, homophily and heterophily adaptively. The learnable coefficients $\\vartheta_{1}$ and $\\vartheta_{2}$ and customized hyperparameters $K_{l p}$ and $K_{h p}$ enhance the performance of TFE-GNN while creating some limitations. In particular, $K_{l p}$ and $K_{h p}$ rely on one\u2019s experience, and it is solved by selecting the appropriate $K_{l p}$ and $K_{h p}$ according to the homophily level of datasets. For example, we choose a larger $K_{l p}$ for datasets with strong homophily while choosing a larger $K_{h p}$ for datasets with strong heterophily. However, the real-world dataset cannot determine its homophily level due to the presence of unlabeled nodes. We are able to determine its approximate homophily level based on training set labels or dataset knowledge, which may be in error. ", "page_idx": 15}, {"type": "text", "text": "It is conceivable that there are other limitations of the TFE-GNN, such as the choice of the base filter, which affect the performance of the model. In fact, none of $K_{l p}$ , $K_{h p}$ , $H_{l p}$ and $H_{h p}$ is fatally weakening the performance of TFE-GNN, as long as we make a suitable choice based on the general phenomenon of polynomial-based spectral GNNs. For example, $H_{h p}$ taking the symmetric normalized Laplacian $L_{s y m}$ or the random walk normalized Laplacian $L_{r w}$ , $\\bar{H}_{h p}$ taking their affinity (transition) matrix $L_{s y m}^{a}$ or $L_{r w}^{a}$ , $K_{l p}$ and $K_{h p}$ taking the interval [1,10]. Table 4 show mean classification accuracy of TFE-GNN ( $\\mathbf{\\bar{\\boldsymbol{H}}}_{h p}$ taking $\\cal{L}_{s y m},$ ) at different $H_{l p}$ and $H_{h p}$ , where $n_{1}$ and $n_{2}$ in $(n_{1},n_{2})$ are the values of $H_{l p}$ and $H_{h p}$ , respectively. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "uatPOPWzzU/tmp/937d57dae69a7528073a568f5e8bb9e8daff8b89013a6e3ae00792e9dc118034.jpg", "table_caption": ["Table 5: Mean classification accuracy of different models for semi-supervised node classification. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B More Experimental Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Additional experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Datasets and Setting. We illustrate the generalization and scalability of TFE-GNN with more intuitive experiments. We select four new datasets (Platonov et al., 2023; Lim et al., 2021a) and conduct relevant experiments to further explain why TFE-GNN has better generalization, including roman-empire, amazon-rating, and two large graphs (Lim et al., 2021a) fb100-Penn94 and genius, whose edge homophily are 0.05, 0.38, 0.47, and 0.62, respectively. Experimental results show that TFE-GNN achieves competitive performance on these datasets with results of $75.87\\%$ , $52.21\\%$ , $84.76\\%$ and $89.32\\%$ , respectively. TFE-GNN achieves competitive rankings on all datasets, with the best performance on fb100-Penn94, outperforming most spectral GNNs on roman-empire, and topping the rest of the datasets. These additional experimental results further validate the generalization ability of TFE-GNN on both homophilic and heterophilic datasets: TFE-GNN can generalize well on graphs with different edge homophily levels. Dataset statistics are reported in Table 6 and the hyperparameters are reported in Table 7. Note that we use the same dataset splits as in the article (Platonov et al., 2023; Lim et al., 2021a). ", "page_idx": 16}, {"type": "text", "text": "B.2 Semi-supervised node classification ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Setting and baselines. We compare TFE-GNN to a series of SOTA models for semi-supervised node classification on real-world datasets, including GCNs (Kipf & Welling, 2016), ARMA (Bianchi et al., 2021a), APPNP (Klicpera et al., 2018), ChebNet (Defferrard et al., 2016), GPR-GNN (Chien et al., 2021), BernNet (He et al., 2021a), ChebNetII (He et al., 2022), EvenNet (Lei et al., 2022), and FavardGNN/OptBasisGNN (Guo & Wei, 2023). We employ the standard and popular training/validation/testing split method (Kipf & Welling, 2016; Klicpera et al., 2018) on three citation networks for semi-supervised node classification, i.e., Cora, Citeseer, and Pubemd, with 20 nodes per class for training, 500 and 1,000 nodes for validation and testing. We split each class of nodes into $2.5\\%$ , $2.5\\%$ , and $95\\%$ as training, validation, and testing sets for other datasets and all models share the same ten random splits for a fair comparison. The seed used for dataset splitting is 42 or 1941488137 (He et al., 2022). ", "page_idx": 16}, {"type": "text", "text": "Results. Table 5 reports the results of the different models on all datasets for semi-supervised node classification and gives mean classification accuracy and standard deviation over ten foxed splits. Table 5 shows that TFE-GNN achieves state-of-the-art performance on 6 datasets, where TFE- $\\mathrm{GNN}_{c o n}$ and TFE- $\\operatorname{GNN}_{r w+s u m}$ also show strong competitiveness, which is similar to the experimental results of full-supervised node classification. TFE-GNN outperforms FavardGNN/OptBasisGNN (Guo & Wei, 2023) on Wisconsin, Texas, and Corenell (strong heterophily) by $8.99\\%$ , $12.01\\%$ , and $6.78\\%$ , respectively, which amounts to performance improvements of $17\\%$ , $22\\%$ , and $13\\%$ . ", "page_idx": 16}, {"type": "table", "img_path": "uatPOPWzzU/tmp/33b2b3770d33018763384e7acf624f39ce71384290b2b6b487940e17c4827f2a.jpg", "table_caption": ["Table 6: Additional dataset statistics. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "uatPOPWzzU/tmp/3de400b24b20b0b37deb3126b95d5a354926742e833e881199d067e1262d644c.jpg", "table_caption": ["Table 7: The hyper-parameters of TFE-GNN for additional datasets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 Visualization Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The t-SNE visualization in Figure 4 demonstrates that the graph convolution $T F E_{1}({\\sf b})$ can extract meaningful patterns on Citeseer (strong homophily), which the graph convolution $T F E_{2}(\\mathsf{c})$ is not able to capture. The t-SNE visualization in Figure 5 demonstrates that $T F E_{2}$ is better than $T F E_{1}$ at extracting the information used for categorization on Squirrel (strong heterophily). The output of TFE-GNN(d) shows clearer boundaries among classes (colors) than that of $T F E_{1}$ and $T F E_{2}$ . These visualization findings validate the competitiveness of TFE-GNN and echo its leading performance on node classification. ", "page_idx": 17}, {"type": "image", "img_path": "uatPOPWzzU/tmp/15528702eb0c32eec8b883aff796ce8dbe2c529d7e390e19d421bfc9c25de409.jpg", "img_caption": ["Figure 4: Visualization of different models on Citeseer, which uses a dimensionality reduction method t-SNE with 1000 iterations. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.4 Complementary Generalization Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We add some experiments to analyze TFE-GNN\u2019s generalization. These newly designed generalization experiments use the early stopping mechanism, i.e., no longer train for 1000 epochs as fixed as in Figure 2. Figure 6 reports the results for ChebNet(a), GCN(b), BernNet(c), ChebNetII(d), PCNet(e) and TFE-GNN(f), and a smaller gap between the two losses indicates a better generalization of the model. As in Figure 2, the gap between the training and validation losses for GCN(b) is less than ChebNet(a) and ChebNetII(c) and larger than TFE-GNN(f). We can observe that the validation loss of TFE-GNN(f) is much closer to its training loss in the six subplots of Figure 6, although its losses are more volatile. This observation demonstrates that TFE-GNN\u2019s generalization is best when it achieves state-of-the-art classification performance because the early stopping mechanism allows TFE-GNN to carry less stable losses. ", "page_idx": 17}, {"type": "image", "img_path": "uatPOPWzzU/tmp/866bab3d6dbfab623e25218857ed6396e57bd673c5277308d962443379bd1128.jpg", "img_caption": ["Figure 5: Visualization of different models on Squirrel, which uses a dimensionality reduction method t-SNE with 1000 iterations. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "uatPOPWzzU/tmp/106d9d8da4caef64c1e5249c89c52fdcad016af367a5ac31201705a5fac25541.jpg", "img_caption": ["Figure 6: Generalization of models on Cora. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.5 Hyper-parameters Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "TFE-GNN yields two hyper-parameters $K_{l p}$ and $K_{h p}$ in addition to those associated with the neural network, since we parameterize $\\omega,\\omega^{\\prime}$ , and $\\vartheta$ as learnable coefficients. We conduct 200 experiments to obtain the interactions between $K_{l p}$ and $K_{h p}$ and the objective values corresponding to model accuracy. We draw their contour plots on Cora and Chameleon. Figure 7 shows parametric contour, where the horizontal coordinate \u201chop1\u201d denotes the hyper-parameter $K_{l p}$ and the vertical coordinate \u201chop2\u201d denotes the hyper-parameter $K_{h p}$ . The parametric contour in Figure 7 demonstrates that there is a difference in the distributions of the objective values (model accuracy) on Cora(a) and Chameleon(b), i.e., there is a large difference in the influence of \u201chop1\u201d and \u201chop2\u201d on homophily and heterophily graphs. ", "page_idx": 18}, {"type": "text", "text": "B.6 Time Efficiency Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conduct time efficiency experiments to count training times for ChebNet (Defferrard et al., 2016), BernNet (He et al., 2021a), ChebNetII (He et al., 2022) and TFE-GNN. All experiments are carried out on the machine with Linux system, two NVIDIA Tesla V100 and twelve Intel(R) Xeon(R) Gold ", "page_idx": 18}, {"type": "image", "img_path": "uatPOPWzzU/tmp/3088d202be414b1d94f2d5a1624650c269cbeddc41cb0de2f583f0a80fd2481c.jpg", "img_caption": ["Figure 7: Parametric contours of Cora and Chameleon. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "5220 CPU $\\omega_{2.20\\mathrm{GHz}}$ . Table 8 shows the results of time efficient experiments on Cora, where TFE-GNN_zero indicates that $K_{l p}$ or $K_{h p}$ has and only a value of 0, TFE-GNN_ten indicates that hyper-parameter $K_{l p}$ or $K_{h p}$ has and only a value of 10, the other hyper-parameter has the value $K$ and TFE-GNN_all indicates that $K_{l p}$ and $K_{h p}$ have the same value $K$ . We take the average of the times of ten training (100 epochs each) as time overheads (unit: second). ", "page_idx": 19}, {"type": "text", "text": "The columns in Table 8 imply the change in model-invariant training time as $K$ increases, and the rows imply the change in $K$ -invariant training time as the model varies. Table 8 shows the variation in training time and the comparison between the different models. We observe that the training time of TFE-GNN is significantly smaller than that of ChebNetII when the value of $K$ is relatively large. Although BernNet and ChebNetII start with a low training time, they quickly catch up and surpass the other models as $K$ increases. Therefore, TFE-GNN has a clear advantage when a larger receptive field $(K)$ is required. ", "page_idx": 19}, {"type": "table", "img_path": "uatPOPWzzU/tmp/42119cc2ba5d17dc55f1a786f7abae37e456af772b8d8956d3f1352efedf2a45.jpg", "table_caption": ["Table 8: Time overheads (s) on Cora. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.7 TFE-GNN Versus Other GNNs with Similar Paradigms ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "TFE-GNN is more suitable for full-supervised node classification than semi-supervised node classification. The key technical differences: TFE-GNN is free from polynomial computation, coefficient constraints, and specific scenarios, compared to polynomial-based spectral GNNs and heterophilyspecific Models (Platonov et al., 2023), such as ChebNetII (He et al., 2022) and PCNet (Li et al., 2023). We also describe the differences between TFE-GNN and several other methods (Li et al., 2024; Huang et al., 2024b,a). ", "page_idx": 19}, {"type": "text", "text": "PEGFAN (Li et al., 2024) sets up three optional feature matrices for the homophilic and heterophilic graphs, respectively, and performs a concatenation operation for the selected feature matrices. PEGFAN performs row normalization on the feature matrices that message passing and dimensionalityreducing outputs, and adds a new linear layer before softmax after the activation function $R e L U$ . These components increase network complexity of PEGFAN, whereas TFE-GNN is more concise and requires less space. ", "page_idx": 19}, {"type": "text", "text": "UniFilter (Huang et al., 2024b) establishes a link between the estimated graph homophily rate $\\hat{h}$ and the propagation matrix $P$ by $\\theta=\\pi/2(1-\\hat{h})$ , and forms heterophily bases $u_{0},u_{1},\\dots,u_{K}$ and the rotation matrix $P_{\\theta}$ , in which learn coefficient $w$ discloses the significance of each frequency component in graph. The performance of UniFilter is affected by the $\\hat{h}$ estimated from the training data labels, which seems to require that the training data label distribution is similar to the graph true global label distribution. The difference between TFE-GNN and UniFilter is that TFE-GNN rarely relies on such priori knowledge and conditions, which ensures its better adaptability and classification performance. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Flow2GNN (Huang et al., 2024a) is an interesting and novel attempt, which decomposes the original adjacency matrix into two matrices via a random binary matrix $\\bar{G}^{(l)}$ with elements that follow the Bernoulli distribution. Information flows within the nodes in two respective disentangled graphs with reduced heterophily, and then adaptively aggregates them with the strength estimation vector $p_{i}^{(l)}$ of information flow. TFE-GNN combines the well-performing and well-tested graph filters, while Flow2GNN is highly dependent on $G^{(l)}$ and $p_{i}^{(l)}$ . ", "page_idx": 20}, {"type": "text", "text": "C More Experimental Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Hyper-parameters. We provide more experimental details for reproducing the experiments. Table 9 shows the hyper-parameters of TFE-GNN on datasets for full-supervised node classification. Table 10 shows the hyper-parameters of TFE-GNN on datasets for semi-supervised node classification. The symbol optim denotes the optimizer, $K_{l p}$ denotes the order of the low-pass graph fliter, $K_{h p}$ denotes the order of the high-pass graph filter, $d r o p_{p r o}$ denotes the dropout rate of input features, droplin denotes the dropout rate of intermediate features, $l r_{a d a}$ denotes the learning rate of the learnable coefficients $\\omega$ and $\\omega^{\\prime}$ , $w d_{a d a}$ denotes the weight decay of $\\omega$ and $\\omega^{\\prime}$ , $l r_{a d a e}$ denotes the learning rate of $\\vartheta$ , $w d_{a d a e}$ denotes the weight decay of $\\vartheta$ , $l r_{l i n}$ denotes the learning rate of MLP $f_{m l p}$ , and $w d_{l i n}$ denotes the weight decay of $f_{m l p}$ . ", "page_idx": 20}, {"type": "text", "text": "Note that different DGL and PyTorch versions can affect model performance, and that fine-tuning of parameters, including but not limited to hyperparameters and seeds, may be required in order to reproduce the effect in this paper. For datasets roman-empire, amazon-rating, fb100-Penn94 and genius, this paper uses DGL 0.5.2 and PyTorch 1.5.1, and for other datasets, this paper uses DGL 0.9.0 and PyTorch 1.12.1. ", "page_idx": 20}, {"type": "text", "text": "Baseline implementations. We use the officially released code for GCNII, TWIRLS, GPR-GNN, BernNet, ChebNetII, $\\mathrm{H_{2}G C N}$ , Haf-Hop, ARMA, EGNN, PDE-GCN, SPECFORMER, EvenNet, FavardGNN/OptBasisGNN,and PCNet. And we use the Deep Graph Library implementations for other models, such as GCNs, APPNP, ChebNet, etc. We did not spend a lot of time tuning parameters for these models. The code URLs are as follows. ", "page_idx": 20}, {"type": "text", "text": "Table 9: The hyper-parameters of TFE-GNN for full-supervised node classification. ", "page_idx": 20}, {"type": "table", "img_path": "uatPOPWzzU/tmp/2b47508df12356b6c81af22f8e06b65485d26e4122c27d1f3d695c28e5d5e9ac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Deep Graph Library: https://docs.dgl.ai/en/0.6.x/guide/index.html ", "page_idx": 20}, {"type": "text", "text": "GCNII: https://github.com/chennnM/GCNII ", "page_idx": 20}, {"type": "text", "text": "GPR-GNN: https://github.com/jianhao2016/GPRGNN ", "page_idx": 20}, {"type": "text", "text": "BernNet: https://github.com/ivam-he/BernNet ", "page_idx": 20}, {"type": "text", "text": "ChebNetII: https://github.com/ivam-he/ChebNetII $\\mathbf{H}_{\\mathrm{2}}\\mathbf{GCN}$ : https://github.com/GemsLab/H2GCN ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "uatPOPWzzU/tmp/b486776b9f20e70d87e5958610047b0123bc159db31ad778c9a7b08c9153ceec.jpg", "table_caption": ["Table 10: The hyper-parameters of TFE-GNN for semi-supervised node classifications. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "TWIRLS: https://github.com/FFTYYY/TWIRLS ", "page_idx": 21}, {"type": "text", "text": "Haf-Hop: https://github.com/nerdslab/halfhop   \nARMA: https://github.com/xnuohz/ARMA-dgl   \nEGNN: https://github.com/Kaixiong-Zhou/EGNN   \nPDE-GCN: https://openreview.net/forum?id $=$ wWtk6GxJB2x   \nEvenNet: https://github.com/Leirunlin/EvenNet   \nSPECFORMER: https://github.com/bdy9527/Specformer   \nPCNet: https://github.com/uestclbh/PC-Conv   \nFavardGNN/OptBasisGNN: https://github.com/yuziGuo/FarOptBasis ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: abstract and introduction. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: we discuss the limitations of TFE-GNN in Appendix A.2. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: the proofs of Theorem 1 and 2 are reported in Appendix A.1. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: all information is in section 4 and Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: we provide detailed code in the supplemental material, which can be run directly after installing the required packages. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: all details are in section 4 and Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: all information is in section 4 and Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: all information is in section 4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: full paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: our articles are fundamental research. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: our paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: we cite the papers covered in the public datasets in the experimental section 4. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: we provide our code in the supplementary material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: this paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: this paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]