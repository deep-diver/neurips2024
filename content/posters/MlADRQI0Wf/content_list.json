[{"type": "text", "text": "Implicit Regularization of Decentralized Gradient Descent for Decentralized Sparse Regression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tongle Wu Ying Sun The Pennsylvania State University The Pennsylvania State University tfw5381@psu.edu ybs5190@psu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider learning a sparse model from linear measurements taken by a network of agents. Different from existing decentralized methods designed based on the LASSO regression with explicit $\\ell_{1}$ norm regularization, we exploit the implicit regularization of decentralized optimization method applied to an over-parameterized nonconvex least squares formulation without penalization. Our first result shows that despite nonconvexity, if the network connectivity is good, the well-known decentralized gradient descent algorithm (DGD) with small initialization and early stopping can compute the statistically optimal solution. Sufficient conditions on the initialization scale, choice of step size, network connectivity, and stopping time are further provided to achieve convergence. Our result recovers the convergence rate of gradient descent in the centralized setting, showing its tightness. Based on the analysis of DGD, we further propose a communication-efficient version, termed T-DGD, by truncating the iterates before transmission. In the high signal-to-noise ratio (SNR) regime, we show that T-DGD achieves comparable statistical accuracy to DGD, while the communication cost is logarithmic in the number of parameters. Numerical results are provided to validate the effectiveness of DGD and T-DGD for sparse learning through implicit regularization. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Modern deep learning is generally in the over-parameterized regime where the models have significantly more parameters than available training examples [5, 37]. Although deep learning models exhibit remarkable performance in multiple domains, the theoretical understanding of optimization and generalization for deep learning is still limited. Recent studies show that despite being overparameterized, gradient-based methods applied to minimize the emperical loss exhibit the implicit regularization phenomenon. For example, a line of works [3, 4, 10, 14] shows that with certain initialization, networks trained with gradient descent (GD) land in the \u201ckernel regime\u201d and share similar behaviors to the kernel method. However, the literature [2, 8, 22] suggests that kernel regime analyses fall short in explaining the success of deep learning because neural networks analyzed in the kernel regime are almost linearized, thus hindering feature learning from data. Further, many works [32, 35, 36] start investigating the \u201crich regime\", showing that GD with small initialization induces structures on the solution, such as sparsity and low-rankness, that better explains the generalization capability of NNs. However, all aforementioned results are limited to the centralized setting, where data are stored on a single machine. Practical constraints such as limited computing and storage resources, data privacy and security, and regulation rules make the centralized learning framework increasingly inadequate for contemporary applications. Although a variety of decentralized learning algorithms can be applied to NN training, the questions of which solution they can converge, along with its generalization performance, are largely unclear. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we study the sparse learning problem [31] in the overparameterized regime, which shares many key characteristics with deep learning models but is more tractable to analyze, as a prototype for understanding the computation and statistical guarantees of decentralized learning algorithms. Specifically, we consider learning a sparse model $w^{\\star}$ from its noisy linear measurements over $m$ agents. These agents communicate over an undirected connected mesh network without a central coordinator, and each agent can only communicate with its one-hop neighbors. The $i$ -th agent has its own $n$ samples $\\{(x_{i,j},\\bar{y_{i,j}})\\}_{j=1}^{n}$ Each $j$ th data pair $(x_{i,j},y_{i,j})$ is generated according to the noisy linear model ", "page_idx": 1}, {"type": "equation", "text": "$$\ny_{i,j}=\\pmb{x}_{i,j}^{T}\\pmb{w}^{\\star}+\\xi_{i,j},\\quad\\forall i\\in[m]\\ \\mathrm{and}\\ \\forall j\\in[n],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\pmb{x}_{i,j}\\in\\mathbb{R}^{d}$ and $y_{i,j}\\in\\mathbb{R}$ denotes respectively the $j$ -th feature and its corresponding response, $\\xi_{i,j}$ is the observation noise, and $\\pmb{w}^{\\star}\\in\\mathbb{R}^{d}$ is the sparse model parameter to be learned, common to all agents, and has only $s$ $[s\\ll d)$ non-zero elements. We are interested in the high-dimensional setting where the ambient dimension $d$ is substantially larger than the total sample size $N:=m n$ i.e., $d\\gg N$ . By re-parameterizing $\\pmb{w}=\\pmb{u}\\odot\\pmb{u}-\\pmb{v}\\odot\\pmb{v}.$ the loss function can be formulated as minimizing the following regularization-free nonlinear least square problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\nF\\left(u,v\\right):=\\frac{1}{m}\\sum_{i=1}^{m}f_{i}\\left(u,v\\right),\\quad\\mathrm{with}\\quad f_{i}\\left(u,v\\right):=\\frac{1}{n}\\left\\Vert X_{i}\\left(u\\odot u-v\\odot v\\right)-y_{i}\\right\\Vert^{2};\\:\\forall i\\in[m],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $f_{i}(\\boldsymbol{u},\\boldsymbol{v})$ corresponds to the loss function of $i$ -th agent. Problem (2) can also regarded as the supervised learning problem on the diagonal linear network of degree-2 [35]. ", "page_idx": 1}, {"type": "text", "text": "Problem (2) is highly non-convex with $\\textbf{\\em u}$ and $\\pmb{v}$ , however, recent works [11, 33, 38] demonstrate that if the design matrix satisfies the restricted isometric property (RIP) condition, the centralized GD without any regularization can yield the statistically optimal estimator with properly chosen initialization scale, step size, and early stopping time. This intriguing phenomenon is derived from the implicit regularization of GD. Roughly speaking, with small initialization, the gap $|u_{i}^{2}-v_{i}^{2}|$ would increase with iteration for coordinate $i$ such that $w_{i}^{\\star}\\neq0$ , where the remaining ones stay small enough before early stopping. As a result, GD identifies the support of $w^{\\star}$ as the algorithm processes. In the decentralized setting, general results from the pure optimization perspective can only certify convergence to the stationary points, implying neither global optimality nor generalizability. Given the encouraging result of GD achieved in the centralized setting, it is natural to ask if statistically optimal solutions are also computable by decentralized gradient-type algorithms, which algorithm can achieve the goal, and what are the regularity conditions. ", "page_idx": 1}, {"type": "text", "text": "This paper aims to analyse the renowned decentralized gradient algorithm (DGD) for minimizing (2) over the undirected mesh networks. The main contributions of this paper are detailed as follows. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Statistical guarantee. It is well established that even for convex objectives, DGD cannot compute an exact minimizer. It only converges to the neighborhood of the solution whose radius depends on the step size. However, we show that under specific conditions\u2014namely, if the global design matrix satisfies the RIP condition, the initialization scale is sufficiently small, and the network is sufficiently connected\u2014-the solution computed by DGD with early stopping is statistically optimal. \u00b7 Computational complexity. Our convergence analysis reveals that the early stopping time increases logarithmically with the ambient dimension $d$ While network connectivity does not affect statistical error when it satisfies mild conditions, it does influence the stopping time of DGD to find the optimal estimator. Networks with poor connectivity will delay the early stopping time, and thus increase the iteration complexity. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Technical analysis. Compared to the techniques used for analyzing the centralized GD [33, 38], proving the convergence of DGD faces the following challenge. Because the consensus error terms induced from the mesh network result in a perturbed version of the multiplicative update. Compared with the exact multiplicative updates, the challenge is that the additional error term outside of multiplication prevents applying the centralized analysis directly. In addition, the error terms within the multiplication have more complicated consensus error terms than that of the centralized setting, which requires bounding the consensus error terms carefully to control these error terms that can achieve the same order statistical error. We separately control the consensus errors on support $_s$ and non-support $\\pmb{S}^{c}$ by the magnitudes of parameters on support $_s$ and non-support $\\pmb{S}^{c}$ ,respectively. Our fine-grained analysis for consensus errors is distinct from existing decentralized optimization analyses that bound the consensus errors uniformly. The additional error term also complicates the transfer of proof from the simplified non-negative $\\boldsymbol{w^{\\star}}$ case to the general $w^{\\star}$ setting as the centralized setting, we conduct a comprehensive induction process to both $\\textbf{\\em u}$ and $\\pmb{v}$ simultaneously for general $w^{\\star}$ ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u00b7 Truncated DGD. We propose a communication-efficient truncated DGD (T-DGD) method that at each iteration, vectors being transmitted are truncated, keeping only $s$ elements with the largest magnitudes nonzero. We prove that if each agent has sufficient samples and the signal-to-noise ratio is high enough, T-DGD can perform as well as the vanilla DGD while reducing communication complexity to logarithmic dependence on ambient dimension $d$ ", "page_idx": 2}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We categorize the existing works most relevant to our study into three main groups. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Implicit regularizations for sparse regression. The recent study in [13] reparameterized the model parameter through overparameterized Hadamard product and discovered encouraged empirical performance by the first-order optimization algorithms. The statistical and convergence guarantees for this phenomenon are established in [33, 38] under mild conditions. Woodworth et al. [35] studied the impact of initialization scale on solutions. Scott et al.[25] demonstrated the benefit of stochasticity of SGD in sparse regression and explored the impact of momentum in [23]. The more recent process in understanding the linear diagonal networks can be found in references [6, 9, 11, 17, 21, 24, 39]. To the best of our knowledge, existing works have only discussed implicit regularizations induced by centralized optimization methods in linear diagonal networks. However, the question of whether decentralized algorithms induce implicit regularizations, and what type of implicit regularization they may induce has not been studied so far. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Decentralized sparse regression with explicit regularization. For estimating ground truth $\\pmb{w}^{\\star}$ in high-dimensional sparse linear regression under the decentralized setting, Ji et al. [16] proposed DGD-CTA algorithm for tackling LASSO objective with consensus penalty and proved linear convergence rate to the neighbor of the statistical optimal estimator, but the convergence rate has polynomial dependence on ambient dimension $d$ . Further Sun et al. [29] proposed the NetLASSO based on the gradient tracking method and obtained $d$ -independent convergence rate and optimal statistical accuracy. To complement work [16], Ji et al. proposed DGD-ATC by mixing the local gradients along iterations and achieved logarithmic dependence on $d$ in [15]. Maros et al. [19] proposed $\\mathrm{DGD^{2}}$ method based on a double mixing for solving decentralized LASSO and obtained similar theoretical guarantees in [15, 29]. To improve the computation efficiency, Maros et al. [18] integrated accelerated proximal gradient descent with gradient tracking to solve decentralized LASSO. Despite these developments, it remains unclear whether leveraging the unregularized overparameterization and implicit regularization of decentralized optimization methods can achieve the optimal statistical guarantee over mesh networks. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Implicit regularizations of decentralized optimization. Implicit bias or regularizations of centralized optimization methods for overparameterized models have been extensively studied [12, 28], but only a few works have investigated the implicit regularization of decentralized optimization methods. Richards et al. [27] studied the implicit regularization for decentralized stochastic gradient descent for solving general unregularized convex problems. Zhu et al. [40] demonstrated that decentralized stochastic gradient descent implicitly executes the sharpness-aware minimization algorithm for general non-convex problems. Taheri et al. [30] studied the implicit regularization of DGD in overparameterized classification for separable data. Recent work [20] demonstrated the implicit regularization of the $\\mathrm{{DGD^{2}}}$ in solving the overparameterized matrix sensing problem. Different from these works, we establish the statistical and computational results for specific non-convex sparse regression problem. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we will introduce the basic notations used in this paper, and then formulate the DGD for solving the problem (2). Finally, we provide the necessary assumptions and definitions for the decentralized sparse regression problem. ", "page_idx": 2}, {"type": "text", "text": "3.1 Notations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Throughout this paper, we use $[m]$ to denote set $\\{1,\\cdot\\cdot\\cdot,m\\}$ for given positive integer $m$ $\\mathbf{1}_{d}$ denotes $d$ -dimensional vector that all element are one and $\\boldsymbol{I}_{d}$ denotes $d$ -dimensional identity matrix. For ground truth parameter $w^{\\star}$ , the relevant notations are support set $S\\;:=\\;\\{j|w_{j}^{\\star}\\;\\neq\\;0\\}$ , positive support set $S^{+}:=\\{j|w_{j}^{\\star}>0\\}$ , negative support set $S^{-}:=\\{j|w_{j}^{\\star}\\,<\\,0\\}$ and non-support set $S^{c}:=\\{j|w_{j}^{\\star}=0\\}$ \uff0c $w_{\\mathrm{max}}^{\\star}:=\\operatorname*{max}_{j\\in\\pmb{\\mathscr{S}}}|w_{j}^{\\star}|$ and $\\begin{array}{r}{w_{\\mathrm{min}}^{\\star}:=\\operatorname*{min}_{j\\in\\pmb{\\mathscr{S}}}|w_{j}^{\\star}|}\\end{array}$ $\\forall\\pmb{x}\\in\\mathbb{R}^{d},\\pmb{x}_{S}:=\\mathbf{1}_{S}\\odot\\pmb{x}$ where $\\mathbf{1}_{s}$ denotes a vector equal to one for all coordinates $j\\in S$ and equal to zero everywhere else. Symbol\u201c $\\because\\odot$ ' denotes Hadamard product that $(\\pmb{a}\\odot\\pmb{b})_{j}=\\dot{a}_{j}b_{j},\\forall\\pmb{a},\\pmb{b}\\in\\mathbb{R}^{d}$ The averaged signal is defneda w and simiartatiosanbextended t, ", "page_idx": 3}, {"type": "text", "text": "$\\pmb{X}:=[\\pmb{X}_{1};\\cdot\\cdot\\cdot;\\pmb{X}_{m}]$ denotes the concatenated sample matrix, where each row of $X_{i}$ represents one feature vector in agent $i$ $\\left\\Vert\\cdot\\right\\Vert$ denotes the Frobenius norm for vector and the spectral norm (maximum singular value) for matrix. $\\left\\|\\mathbfcal{A}\\right\\|_{\\infty}:=\\operatorname*{max}_{i,j}\\left|A_{i j}\\right|$ denotes infinity norm. We use $a=\\mathcal{O}(b)$ to denote that inequality $a\\leq C b$ holds with some absolute constants $C$ that do not depend on any parameters of the problem. The notation $a\\lesssim b$ shares the same meaning as $a=\\mathcal{O}(b)$ . Finally, we use $a\\gtrsim b$ if there exists a universal constant $c$ such that $a\\ge c b$ ", "page_idx": 3}, {"type": "text", "text": "3.2  Method and Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We focus on DGD solving problem (2) in mesh networks, modeled as an undirected graph $\\mathcal{G}=\\{\\mathcal{V},\\mathcal{E}\\}$ where nodes $\\nu\\,=\\,\\{1,\\cdot\\cdot\\cdot,m\\}$ represent the set of agents and edges $\\mathcal{E}\\,\\subset\\,\\mathcal{V}\\,\\times\\,\\mathcal{V}$ represent the communication links. An unordered pair $\\{i,j\\}$ is included in $\\mathcal{E}$ if and only if there is a bidirectional communication link between agent $i$ and $j$ . The set of one-hop neighbors for agent $i$ is denoted by $\\mathcal{N}_{i}:=\\{j\\in\\mathcal{V}|(i,j)\\in\\mathcal{E}\\}\\bigcup\\{i\\}$ ", "page_idx": 3}, {"type": "text", "text": "DGD allows each agent to independently update its parameters based on local gradient descent and then synchronize with neighboring agents by weighted averaging these updates. The recursive iteration of DGD for each agent is described as follows. ", "page_idx": 3}, {"type": "equation", "text": "$$\nu^{t+1,i}=\\sum_{j=1}^{m}W_{i j}\\left(u^{t,j}-\\eta\\frac{4}{n}u^{t,j}\\odot\\left(X_{j}^{T}\\left(X_{j}\\left(u^{t,j}\\odot u^{t,j}-v^{t,j}\\odot v^{t,j}\\right)-y_{j}\\right)\\right)\\right),\\,\\forall i\\in[m]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{v}^{t+1,i}=\\sum_{j=1}^{m}W_{i j}\\left(\\boldsymbol{v}^{t,j}+\\eta\\frac{4}{n}\\boldsymbol{v}^{t,j}\\odot\\left(\\boldsymbol{X}_{j}^{T}\\left(\\boldsymbol{X}_{j}\\left(\\boldsymbol{u}^{t,j}\\odot\\boldsymbol{u}^{t,j}-\\boldsymbol{v}^{t,j}\\odot\\boldsymbol{v}^{t,j}\\right)-\\boldsymbol{y}_{j}\\right)\\right)\\right),\\,\\forall i\\in[m],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the $\\pmb{w}^{t,i}:=\\pmb{u}^{t,i}\\odot\\pmb{u}^{t,i}-\\pmb{v}^{t,i}\\odot\\pmb{v}^{t,i}$ denotes the local estimator in agent $i$ at $t^{t h}$ iteration, the initialization is ${\\pmb u}^{0,i}={\\pmb v}^{0,i}=\\alpha{\\bf1}_{d},\\forall i\\in[m]$ and $\\eta$ is constant step size. $W$ is the nonnegative weight mixing matrix for the undirected mesh network, where $W_{i j}>0$ if there is a link between agents $i$ and $j$ , and $W_{i j}=0$ otherwise. The mixing matrix $W$ related to the undirected graph satisfies the following assumption. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. The communication network $\\mathcal{G}$ is connected. The weight matrix $\\pmb{W}=[w_{i j}]_{i,j=1}^{m}\\,f_{c}$ this graph has the following properties: (i) $w_{i j}=0$ for all pairs $(i,j)$ that are not in $\\mathcal{E}$ (ii) it is double stochastic that $\\mathbf{1}_{m}^{T}W=\\mathbf{1}_{m}^{T}$ and $W\\mathbf{1}_{m}=\\mathbf{1}_{m}$ \uff0c $(i i i)$ the spectral gap $\\begin{array}{r}{\\rho:=\\left\\|\\pmb{W}-\\frac1m\\mathbf{1}_{m}\\mathbf{1}_{m}^{T}\\right\\|\\leq1}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "This assumption is common in decentralized optimization literature. We need the following RIP condition which is a key condition to obtain the optimal estimator for sparse regression. ", "page_idx": 3}, {"type": "text", "text": "Definition 1. The global design matrix ${\\pmb X}/\\sqrt{N}\\in\\mathbb R^{N\\times d}$ satisfies the $(\\delta,s)$ -Restricted Isometry Property $\\left(R I P\\right)$ if for any $s$ -sparse vector $\\pmb{w}\\ \\in\\mathbb{R}^{d}$ , there is $\\left(1\\,-\\,\\delta\\right)\\left\\|w\\right\\|^{2}\\,\\leq\\,\\left\\|X w/\\sqrt{N}\\right\\|^{2}\\,\\leq$ $\\left(1+\\delta\\right)\\left\\|w\\right\\|^{2}$ ", "page_idx": 3}, {"type": "text", "text": "The RIP condition was first introduced in the compressed sensing literature in [7] which is a little more restrictive condition to achieve optimal statistical rate than the restricted eigenvalue condition in [1]. We inherit this assumption in the centralized setting [33, 38] to achieve optimal estimator error under the condition that parameter $\\delta$ is upper bounded. Besides the global RIP condition, we have the following local RIP condition for local design matrices { X }\"=1- ", "page_idx": 3}, {"type": "text", "text": "Definition 2. The local design matrices $X_{1}/{\\sqrt{n}},\\cdot\\cdot\\cdot,X_{m}/{\\sqrt{n}}\\in\\mathbb{R}^{n\\times d}$ satisfy the local $(\\delta_{\\operatorname*{max}},s)$ $(R I P)$ condition, if for any $s$ -sparse vector $\\pmb{w}\\in\\mathbb{R}^{d}$ and any local design matrix $X_{i}/\\sqrt{n}$ there is $\\left(1-\\delta_{\\operatorname*{max}}\\right)\\left\\Vert w\\right\\Vert^{2}\\leq\\left\\Vert X_{i}w/\\sqrt{n}\\right\\Vert^{2}\\leq\\left(1+\\delta_{\\operatorname*{max}}\\right)\\left\\Vert w\\right\\Vert^{2}\\!.$ ", "page_idx": 4}, {"type": "text", "text": "The definition of the local RIP condition is just for ease of proof presentation, as we do not necessitate any upper bound on the local RIP parameter $\\delta_{\\mathrm{max}}$ ", "page_idx": 4}, {"type": "text", "text": "4 Main Result ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Based on the above method and assumptions, we now give theoretical guarantees of DGD in solving problem (2) for sparse regression problem (1) as follows. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Considering the sequence generated by (3) and (4) based on DGD for solving problem (2) and $\\forall\\epsilon\\:>\\:0,$ if the global design matrix $X/\\sqrt{N}$ satisfies $(\\delta,s+1)$ -RIP condition with bounded RIP parameter $\\begin{array}{l l l}{\\delta}\\end{array}\\lesssim}&{{}\\frac{1}{\\sqrt{s}}$ , the local design matrices $\\left\\{X_{i}/{\\sqrt{n}}\\right\\}_{i=1}^{m}$ satisfy local $\\left(\\delta_{\\mathrm{max}},s+1\\right)$ -RIP condition,and the mesh network satisfies assumption $^{\\,l}$ , the initialization sat$\\begin{array}{r}{\\alpha\\lesssim\\operatorname*{min}\\left\\{1,\\frac{\\epsilon^{2}}{(12d+1)^{2}},\\frac{\\epsilon}{w_{\\mathrm{max}}^{\\star}},\\frac{\\zeta}{6(w_{\\mathrm{max}}^{\\star})^{2}},\\frac{w_{\\mathrm{min}}^{\\star}}{4}\\right\\}}\\end{array}$ ,the constant stepsize $\\eta$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta\\lesssim\\operatorname*{min}\\left\\{\\frac{1-\\sqrt{\\rho}}{64\\sqrt{\\rho}w_{\\mathrm{max}}^{\\star}},\\frac{\\log\\frac{1}{\\alpha^{4}}\\left(1-\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}\\right)}{4w_{\\mathrm{max}}^{\\star}},\\frac{1-\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)^{\\frac{1}{4}}}{w_{\\mathrm{max}}^{\\star}}\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the spectral gap $\\rho$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho^{\\frac{1}{4}}\\lesssim\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{s}\\delta_{\\operatorname*{max}}+1},\\frac{\\delta}{8\\delta_{\\operatorname*{max}}},\\frac{\\left\\|\\frac{X^{T}\\xi}{N}\\right\\|_{\\infty}}{8\\operatorname*{max}_{i}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}}\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then afer running $\\begin{array}{r}{t=\\mathcal{O}\\left(\\frac{1}{\\eta\\zeta}\\log\\frac{1}{\\alpha}\\right)}\\end{array}$ iterations. There would be ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\overline{{w}}_{j}^{t}-{w}_{j}^{\\star}\\right|\\leq\\left\\{\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\left|\\left(\\frac{S^{T}\\xi}{N}\\right)_{j}\\right|,\\delta\\sqrt{s}\\left\\|\\frac{X^{T}\\xi}{N}\\right\\|_{\\infty},\\epsilon\\right\\}\\right)\\right.}&{i f\\ j\\in S\\ a n d\\ {w}_{\\operatorname*{min}}^{\\star}\\geq\\mathcal{O}\\left(\\varsigma\\right)}\\\\ &{\\left.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "\u00b7 Mechanism to promote sparsity. The consensus errors induced from decentralized network complicate the multiplicative updates, which becomes inexact multiplicative updates as $\\overline{{\\pmb{u}}}^{t+1}\\ =$ $\\overline{{\\pmb{u}^{t}}}\\odot\\left(1-4\\eta\\left(\\overline{{\\pmb{u}^{t}}}\\odot\\overline{{\\pmb{u}^{t}-\\pmb{w}^{\\star}}}+\\hat{\\pmb{p}^{t}}+\\hat{\\pmb{b}^{t}}\\right)\\right)+\\pmb{e}^{t}$ . Compared with the exact multiplicative updates of GD in [33], the challenge is that the extra error term $e^{t}$ outside of the multiplication prevents applying the centralized analysis trivially. In addition, the perturbation error terms $\\hat{\\boldsymbol{p}}^{t},\\hat{\\boldsymbol{b}}^{t}$ within the multiplication are much more complicated than that of the centralized setting due to additional multiple consensus errors. This requires bounding the consensus error terms carefully, which can control the complicate perturbation errors $\\hat{\\boldsymbol{p}}^{t},\\hat{\\boldsymbol{b}}^{t},e^{t}$ not to be large. Thus, we can use network connectivity to control the consensus errors to bound these three perturbation errors small enough to make the distance between two trajectories obtained by inexact and exact multiplicative updates within statistical accuracy, which can promote sparsity in the decentralized setting. The detailed theoretical mechanism of promoting sparsity has been demystified in Proposition 3. ", "page_idx": 4}, {"type": "text", "text": "\u00b7 Statistical Guarantee. Based on the result in (7) and conditions in (5) and (6), we can observe that if the initialization $\\alpha$ is small enough and network connectivity is sufficiently well, the DGD with early stopping can obtain the desired estimator for sparse ground truth parameter $\\pmb{w}^{\\star}$ that achieves the same order of statistical error as the centralized setting in [33]. The formula in (7) not only illustrates that we establish the network-independent estimator error bound but also inherits the benefit of implicit regularization, which indicates that if the signal-to-noise is high enough, the statistical error is independent of ambient dimension $d$ . In contrast, existing results in decentralized LASSO methods [15, 16, 19, 29], have consistent dependence on $d$ in any case. ", "page_idx": 4}, {"type": "text", "text": "\u00b7 Computational Complexity. The iteration complexity of early stopping is network-dependent that is becausethe $\\begin{array}{r}{t=\\mathcal{O}\\left(\\frac{1}{\\eta\\zeta}\\log\\frac{1}{\\alpha}\\right)}\\end{array}$ has tee izewh ld satsfy ton in (5). This suggests that poorer network connectivity leads to higher computational complexity. Although the initialization has no dependence on network connectivity, $\\frac{1}{\\alpha}$ has polynomial dependence on $d$ , and the dependence of complexity on $d$ is just logarithmic, which is similar to DGD in solving LASSO in [15, 19] and improves the polynomial dependence on $d$ in [16]. ", "page_idx": 5}, {"type": "text", "text": "\u00b7 Dependence on network connectivity. For accurate estimation, it is essential that the network should be well-connected, as specified in condition (6). When this condition is not satisfied, we can run multiple rounds of communication per iteration. It is observable that the smaller ratio between the global RIP parameter $\\delta$ and the local RIP parameter $\\delta_{\\mathrm{max}}$ , and smaller ratio between the local noise and the global noise magnitude, necessitate a higher degree of network connectivity. This can be understood from the perspective of heterogeneity, where smaller ratios indicate a significant disparity between local and global design matrices. Consequently, condition (6) is reasonable as it suggests that higher levels of heterogeneity necessitate improved network connectivity. In numerical experiments, we can observe that if $\\rho$ does not satisfy the condition as (6), obtaining optimal statistical error is not achievable, which indicates the optimal statistical error undergoes a phase transition with the network connectivity. ", "page_idx": 5}, {"type": "text", "text": "Our results demonstrate the benefit of overparameterization for DGD. Theorem 1 shows that standard DGD is sufficient to provide statistically optimal estimator with efficient computation without gradient correction techniques. This finding challenges the widely held belief in decentralized optimization literature that extra techniques like gradient tracking and other gradient-correction-based methods are necessary for heterogeneous scenarios. The following corollary considers the specific instance where the design matrix and noise are generated from sub-Gaussian distribution, which indicates that DGD with early stopping can achieve the minimax optimal statistical rate under the $\\ell_{2}$ metric. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. Suppose that entries of global design matrix $\\mathbf{\\deltaX}$ generated from i.i.d $^{\\,l}$ -sub-Gaussian distributndtllei $\\begin{array}{r}{\\check{N}\\gtrsim s\\left(s\\log\\frac{\\check{e}d}{s}+\\log\\frac{d\\check{N}}{m}\\right)}\\end{array}$ . The noise vector $\\xi$ is generated from independent $\\sigma^{2}$ -sub-Gaussian entries,and the initialization is set as Theorem $^{\\,I}$ with $\\begin{array}{r}{\\epsilon=\\mathcal{O}\\left(\\sigma\\sqrt{\\frac{\\log d}{N}}\\right)}\\end{array}$ If the spectral gap satisfes $\\rho\\lesssim{\\frac{1}{m^{4}}}$ and stepsize is set as $\\begin{array}{r}{\\eta=\\mathcal{O}\\left(\\frac{1-\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)^{\\frac{1}{4}}}{w_{\\operatorname*{max}}^{\\star}}\\right),}\\end{array}$ then after running $\\begin{array}{r}{t=\\mathcal{O}\\left(\\frac{w_{\\operatorname*{max}}^{\\star}\\sqrt{N}}{\\sigma\\sqrt{\\log d}\\left(1-\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)^{\\frac{1}{4}}\\right)}\\log\\frac{1}{\\alpha}\\right)}\\end{array}$ iterations, the sequence generated by (3) and 4) based on DGD for solving problem (2) would obtain estimator that $\\begin{array}{r}{\\|\\overline{{\\boldsymbol{w}}}^{t}-\\boldsymbol{w}^{\\star}\\|\\lesssim\\sigma\\sqrt{\\frac{s\\log d}{N}}}\\end{array}$ with probability at least $\\begin{array}{r}{1-\\frac{3}{8d^{3}}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Corollary 1 indicates that in the sub-Gaussian setting, network-independent statistical error obtained by DGD matches optimal rate $\\mathcal{O}\\left(\\sigma\\sqrt{\\frac{s\\log d}{N}}\\right)$ under $\\ell_{2}$ metric in the centralized setting [26]. In this context, the condition for network connectivity implies that the smaller $\\rho$ is required as the number of agents $m$ increases. This is reasonable because when the total sample size $N$ is fixed, an increase in the number of agents results in fewer samples assigned to each agent. Consequently, better network connectivity is necessary to achieve optimal estimation. ", "page_idx": 5}, {"type": "text", "text": "5  Communication Efficient DGD via Truncation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "It is apparent that iterations in (3) and (4) of DGD, each agent has to transmit two $d$ dimensional vectors $\\pmb{u}^{t,i}$ and $\\pmb{v}^{t,i}$ to its neighboring agents per iteration. Because we are considering the high$\\mathcal{O}\\left(d\\cdot\\frac{1}{\\eta\\zeta}\\log\\frac{1}{\\alpha}\\right)$ high communication complexity (in terms of the bits transmitted) for DGD. The primary idea is whether it is possible to transmit fewer partial elements instead of the entire $d$ -dimensionalvectors for $\\pmb{u}^{t,i},\\pmb{v}^{t,i}$ in all rounds of communication. Since all elements of $\\pmb{u}^{t,i}$ and $\\pmb{v}^{t,i}$ equal to $\\alpha$ at initialization, we can utilize the one step of local gradient descent step in each agent to distinguish the support set and non-support based on changes of magnitudes for each element. The intuition is that the elements on the support would grow more rapidly than those on the non-support. Thus, we propose the Truncated Decentralized Gradient Descent (T-DGD) as ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t^{t+1,i}=\\displaystyle\\sum_{j=1}^{m}W_{i j}\\cdot\\mathrm{Trun}_{s}\\left(\\left(u^{t,j}-\\eta\\frac{4}{n}u^{t,j}\\odot\\left(X_{j}^{T}\\left(X_{j}\\left(u^{t,j}\\odot u^{t,j}-v^{t,j}\\odot v^{t,j}\\right)\\right)\\right)\\right);\\mathrm{~f~o~r~}\\tan{s}\\right)}\\\\ &{\\,\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\,\\!\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!\\,\\!\\,\\,\\,\\!\\!^{t+1,j}\\odot\\left(X_{j}^{T}\\left(X_{j}\\left(u^{t,j}\\odot u^{t,j}-v^{t,j}\\odot v^{t,j}\\odot v^{t,j}\\right)\\right)\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for $\\forall i\\in[m]$ ,where $\\operatorname{Trun}_{s}(x)$ is the operator that preserves only the $s$ largest magnitude elements of thevector $\\textbf{\\em x}$ while setting all other elements to zero. The following proposition shows the benefit of T-DGD in sparse regression under proper conditions. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. With the same setup in Corollary 1, if the ground truth $\\pmb{w}^{\\star}$ satisfies $\\frac{w_{\\operatorname*{min}}^{\\star}}{2}\\stackrel{}{\\sim}$   \n$\\begin{array}{r}{\\sqrt{s}\\delta_{\\operatorname*{max}}w_{\\operatorname*{max}}^{\\star}\\,+\\,\\sigma\\sqrt{\\frac{\\log d}{n}}}\\end{array}$ then the sequence generated by T-DGD as (8) for solving prob$\\begin{array}{r l r}{\\left\\|\\overline{{\\boldsymbol{w}}}^{t}-\\boldsymbol{w}^{\\star}\\right\\|}&{\\lesssim}&{\\sigma\\sqrt{\\frac{s\\log d}{N}}}\\end{array}$ $1\\,-$   \n$\\frac{3}{8d^{3}}$   \n$\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(s\\cdot\\left(w_{\\operatorname*{max}}^{\\star}\\sqrt{N}\\right)\\Big/\\left(\\sigma\\sqrt{\\log d}\\left(1-\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)^{\\frac{1}{4}}\\right)\\right)\\log\\frac{1}{\\alpha}\\right).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "To ensure condition $\\begin{array}{r}{\\frac{w_{\\operatorname*{min}}^{\\star}}{2}\\gtrsim\\sqrt{s}\\delta_{\\operatorname*{max}}w_{\\operatorname*{max}}^{\\star}+\\sigma\\sqrt{\\frac{\\log d}{n}}}\\end{array}$ satisfied, it is necessary to require that each agent has suffcient samples and SNR is high enough such that $\\begin{array}{r}{\\delta_{\\mathrm{max}}\\,\\lesssim\\,\\frac{w_{\\mathrm{min}}^{\\star}}{\\sqrt{s}w_{\\mathrm{max}}^{\\star}}}\\end{array}$ and o\u221a $\\sigma\\sqrt{\\frac{\\log d}{n}}\\,\\lesssim$ $w_{\\mathrm{min}}^{\\star}$ . This proposition enables each agent to transmit only $s$ elements of $d$ dimensional vector per communication round, which can achieve optimal statistical rate and eliminate the $d$ -linear increasing communication complexity. The result in Proposition 1 validates the usefulness of the Hadamard product over-parameterization in decentralized gradient-based optimization. ", "page_idx": 6}, {"type": "text", "text": "6 Numerical Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section conducts the experimental studies to evaluate the theoretical findings of DGD and T-DGD for solving problem (2) in Subsection 6.1, Subsection 6.2, respectively. In Subsection 6.3, we compare the effectiveness of implicit regularization of DGD with explicit regularization based decentralized methods. The communication networks $\\mathcal{G}$ are generated from Erd6s R\u00e9nyi (ER) graphs with link activation under given probabilities. By default, unless stated otherwise, all the design matrices $\\mathbf{\\deltaX}$ have i.i.d. standard Gaussian elements, noise $\\xi$ follows i.i.d. $\\mathcal{N}(0,0.5^{2})$ distribution, and the magnitudes of elements on support $\\boldsymbol{S}$ are 1. All experiments are conducted on 12th Gen Intel(R) Core(TM) i7-12700@ 2.10GHz processor and 16.0GB RAM under Windows 11 system. ", "page_idx": 6}, {"type": "text", "text": "6.1 Simulations on DGD ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We organize the experiments as follows: 1) We visualize the dynamics of averaged variables and consensus errors that allow us to evaluate the implicit regularization of DGD and the soundness of our technical analysis. 2) We check whether DGD can achieve optimal statistical error, the impacts of ambient dimension $d$ and initialization scale $\\alpha$ on statistical and computational properties. 3) We evaluate the condition of (6) that reveals the relationship between network connectivity and network scale for achieving the statistical accuracy of centralized setting. ", "page_idx": 6}, {"type": "text", "text": "\u00b7 Dynamics of $\\overline{{\\mathbf{w}}}^{t},\\overline{{\\mathbf{u}}}^{t},\\overline{{\\mathbf{v}}}^{t}$ and $\\pmb{u}^{t,i}-\\overline{{\\pmb{u}}}^{t},\\pmb{v}^{t,i}-\\overline{{\\pmb{v}}}^{t}$ . In this case, we set $d=2000,s=10,m=$ $10,N=400,\\rho=0.1778,\\alpha=10^{-6}$ . Fig. 1(a) demonstrates the convergence of averaged $\\overline{{\\pmb{w}}}^{t}$ in DGD, showing successful convergence of elements on support $\\boldsymbol{S}$ and maintenance of small magnitudes for elements on non-support $S^{c}$ . Fig. 1(b) and Fig. 1(c) further illustrate how DGD utilizes $\\overline{{\\pmb{u}}}^{t}$ and ${\\overline{{\\pmb{v}}}}^{t}$ to fit parameters on positive and negative support, respectively. Additionally, the magnitudes of $\\overline{{\\pmb{u}}}^{t}$ and ${\\overline{{\\pmb{v}}}}^{t}$ on non-positive and non-negative support remain small enough as the initialization. Consensus errors ${\\pmb u}^{t,i^{\\star}}-\\overline{{{\\pmb u}}}^{t}$ and $\\pmb{v}^{t,i}-\\overline{{\\pmb{v}}}^{\\overline{{t}}}$ are depicted in Fig. 1(d) and Fig. 1(e), respectively. The trends in these curves correspond to the magnitudes of the model parameter, affirming the validity of our analysis. ", "page_idx": 6}, {"type": "image", "img_path": "MlADRQI0Wf/tmp/bae35dc0b0946b57667fb0d20f0450de2692d66fee379efb5bc269690e4daa5d.jpg", "img_caption": ["Figure 1: Dynamics of avergaed variables and consensus errors. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u00b7 Impact of $d$ and $\\alpha$ on optimal estimation. We vary the dimension of $d$ $\\mathrm{~}(4\\times10^{2},4\\times10^{3},4\\times10^{4})$ to access effect of $d$ on both statistical and computational properties. With $s=\\lceil\\log d\\rceil$ and $N$ chosen to satisfy $s\\log d/N\\approx0.25.$ . we aim to maintain the same order optimal statistical error $\\mathcal{O}\\left(\\sigma\\sqrt{\\frac{s\\log d}{N}}\\right)$ . Testing is conducted on two networks with $m=20$ but different $\\rho$ . For each $d$ we select the maximum initialization $\\alpha$ that achieves optimal statistical error, resulting in $\\alpha=10^{-8}$ for $d=4\\times10^{2}$ $\\alpha=10^{-8.5}$ for $d=4\\times10^{3}$ and $\\bar{\\alpha}=10^{-9}$ for $d=4\\times10^{4}$ . The results for $\\rho=0.1778$ and $\\rho=0.7519$ are displayed in Fig. 2(a) and Fig. 2(b), respectively. It is observable that DGD obtains estimators with statistical error matching that of the centralized setting, with computational complexity remaining largely unaffected by ambient dimension $d$ across different network conditions. To assess the influence of $\\alpha$ ,we set $d=2000$ \uff0c $s=10$ \uff0c $m=20$ $N=400$ and different values for $\\alpha$ on network with $m=20$ \uff0c $\\rho=0.1778$ . The results in Fig. 2(c) illustrate that it is necessary to use small enough initialization to obtain optimal estimator. ", "page_idx": 7}, {"type": "image", "img_path": "MlADRQI0Wf/tmp/11798443658402988b8f133e49a73a3ba7b69f27d902962a93fcaeee3ac6de85.jpg", "img_caption": ["Figure 2: Impact of ambient dimension $d$ and initialization $\\alpha$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u00b7 Dependence on $\\rho$ and $m$ . We set $d=2000,s=10,N=200,\\alpha=10^{-6}$ and test on networks with different numbers of agents. The results are shown in Fig. 3 where Fig. 3(a) and Fig. 3(b) display the performance with varied numbers of agents under fixed $\\rho=0.9400$ and fixed $\\rho=0.1778$ , respectively. Fig. 3(a) indicates that DGD would not obtain the optimal estimator as the centralized setting when the number of agents is large which violates the condition in (6). When network connectivity is sufficiently connected as $\\rho=0.1778$ , Fig. 3(b) conveys that this can allow a larger scale of agents to attain optimal statistical error. In Fig. 3(c), we fix $m=10$ and observe the phenomenon under varied $\\rho$ by choosing proper stepsizes to achieve the best statistical error. ", "page_idx": 7}, {"type": "image", "img_path": "MlADRQI0Wf/tmp/751ca4b40a4e713932d020498f82fb7b4a3a5fdab485886f801ee11794fd563b.jpg", "img_caption": ["Figure 3: (a) $\\rho=0.9400$ ; (b) $\\rho=0.1778$ (c) $m=10$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Fig. 3(c) illustrates that $\\rho$ would influence the stopping time when DGD can obtain the optimal estimator. The worse the network, the more iterations it takes to find the optimal estimator. ", "page_idx": 8}, {"type": "text", "text": "6.2  Simulations on T-DGD ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate the effectiveness of T-DGD. Initially, we vary the values of $N$ ,keeping the other parameters consistent with the simulations in Fig. 1. Fig. 4(a) illustrates that when each agent has inadequate samples $N=100$ $n=10)$ ), T-DGD would fail in achieving optimal estimation. However, with increasing local samples ( $\\langle N=400,n=10)$ , T-DGD matches DGD in both statistical accuracy and convergence performance. Subsequently, we set the magnitudes of the ground truth on support as 100 and $N=300$ . The performance is depicted as dashed lines in Fig. 4(c), indicating failure of T-DGD under higher noise level $(\\sigma=0.5)$ ). We further reduce the noise magnitude to $\\sigma=0.1$ , and solid lines in Fig. 4(c) demonstrate the usefulness of T-DGD in sparse regression. These observations validate the statement in Proposition 1. ", "page_idx": 8}, {"type": "image", "img_path": "MlADRQI0Wf/tmp/439a05ee675b2c9c4fe4be19eeb4b2e6dc2f90841142720d573a7ebf797b19a7.jpg", "img_caption": ["Figure 4: (a) $N=100$ ; (b) $N=400$ ; (c) Different noise intensities. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.3  Comparison with explicit regularization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have compared our proposed method with three existing decentralized methods, namely: CTADGD (LASSO) [16], ATC-DGD (LASSO) [15], and DGT (NetLASSO) [29]. These methods are all derived based on the LASSO formulation with explicit regularization. The numerical results presented in Fig. 5 compare all four methods under three different network connectivity settings. For each method, we tuned the step size to achieve the best performance. Our proposed method demonstrated the best recovery performance in all network settings with minimal iterations. ", "page_idx": 8}, {"type": "text", "text": "We further compared T-DGD with existing methods with truncated versions of existing methods: Trun-CTA-DGD (LASSO), Trun-ATC-DGD (LASSO), and Trun-DGT (NetLASSO) which use the sameTop- $s$ truncation operator. As shown in Fig. 6, our proposed method is the only one to achieve successful recovery, while all other truncated decentralized methods failed. The numerical evidence demonstrates that naively combining sparsification with decentralized algorithms is not granted to converge. This is precisely one of the motivations of this work: to provide communication-efficient algorithms with both provably statistical and computational guarantees. This result also demonstrates the unique benefit of overparameterization and implicit regularization for decentralized learning setting, which has not been explored in the literature of learning theory. ", "page_idx": 8}, {"type": "image", "img_path": "MlADRQI0Wf/tmp/5c0bee5a1007e030d35f740f5bbd578adf4ad6390f245f0ad9d88f1c40374973.jpg", "img_caption": ["Figure 5: Comparison with decentralized sparse solvers under varying communication network. The setting is $d=1000,k=5,m=50,$ $N=280$ $\\sigma=0.5$ and magnitude of sparse signal is 10. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "MlADRQI0Wf/tmp/58b2d92ce3d118be27bc874b98accec27813ecee0c8af621ebd47018997147ef.jpg", "img_caption": ["Figure 6: Truncated version: comparison with truncated decentralized sparse solvers. The setting is $d=1000,s=5,m=50,N=550,\\sigma=0.1,\\rho=0.2458$ and magnitude of sparse signal is 10. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we study the implicit regularization of decentralized gradient descent for decentralized sparse regression in the unpenalized and overparameterized regimes. We establish both statistical and computational guarantees for the decentralized estimator under mild conditions of network connectivity, underscoring the utility of DGD in addressing overparameterized models. Furthermore, the proposed truncated DGD (T-DGD) offers a promising idea to reduce communication complexity while maintaining performance. In future work, exploring the possibility of relaxing the RIP condition in our assumption and leveraging the restricted eigenvalue condition to achieve optimal estimator in the decentralized setting is an interesting topic. Additionally, investigating alternative forms of implicit regularizations in decentralized optimization algorithms for more complicated overparameterized models is another intriguing direction. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. Advances in Neural Information ProcessingSystems,23,2010. ", "page_idx": 9}, {"type": "text", "text": "[2]  Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. In The Eleventh International Conference on Learning Representations, 2022.   \n[3]  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In International conference on machine learning, pages 242-252. PMLR, 2019.   \n[4]  Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pages 322-332. PMLR, 2019.   \n[5]  Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machinelearning practice and the classical bias-variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849-15854, 2019.   \n[6]  Raphael Berthier. Incremental learning in diagonal linear networks. Journal of Machine Learning Research, 24(171):1-26, 2023.   \n[7] Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on information theory, 51(12):4203-4215, 2005.   \n[8]  Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional neural networks. Advances in neural information processing systems, 35:25237- 25250, 2022.   \n[9] Hung-Hsu Chou, Johannes Maly, and Holger Rauhut. More is less: inducing sparsity via overparameterization. Information and Inference: A Journal of the IMA, 12(3):1437-1460, 2023.   \n[10] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pages 1675-1685. PMLR, 2019.   \n[11] Jianqing Fan, Zhuoran Yang, and Mengxin Yu. Understanding implicit regularization in over-parameterized single index model. Journal of the American Statistical Association, 118(544):2315-2328, 2023.   \n[12] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias interms of optimization geometry. In International Conference on Machine Learning, pages 1832-1841. PMLR, 2018.   \n[13]  Peter D Hoff. Lasso, fractional norm and structured sparse estimation using a hadamard product parametrization. Computational Statistics & Data Analysis, 115:186-198, 2017.   \n[14]  Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \n[15]  Yao Ji, Gesualdo Scutari, Ying Sun, and Harsha Honnappa. Distributed (atc) gradient descent for high dimension sparse regression. IEEE Transactions on Information Theory, 2023.   \n[16]  Yao Ji, Gesualdo Scutari, Ying Sun, and Harsha Honnappa. Distributed sparse regression via penalization. Journal of Machine Learning Research, 24(272):1-62, 2023.   \n[17] Jiangyuan Li, Thanh Nguyen, Chinmay Hegde, and Ka Wai Wong. Implicit sparse regularization: The impact of depth and early stopping. Advances in Neural Information Processing Systems, 34:28298-28309, 2021.   \n[18]  Marie Maros and Gesualdo Scutari. Acceleration in distributed sparse regression. Advances in Neural Information Processing Systems, 35:36832-36844, 2022.   \n[19] Marie Maros and Gesualdo Scutari. Dgd\u2019 2: A linearly convergent distributed algorithm for high-dimensional statistical recovery. Advances in Neural Information Processing Systems, 35:3475-3487, 2022.   \n[20] Marie Maros and Gesualdo Scutari. Decentralized matrix sensing: Statistical guarantes and fast convergence. Advances in Neural Information Processing Systems, 36, 2024.   \n[21] Mor Shpigel Nacson, Kavya Ravichandran, Nathan Srebro, and Daniel Soudry. Implicit bias of the step size in linear diagonal neural networks. In International Conference on Machine Learning, pages 16270-16295. PMLR, 2022.   \n[22] Eshaan Nichani, Yu Bai, and Jason D Lee. Identifying good directions to escape the ntk regime and effciently learn low-degree plus sparse polynomials. Advances in Neural Information Processing Systems, 35:14568-14581, 2022.   \n[23]  Hristo Georgiev Papazov, Scott Pesme, and Nicolas Flammarion. Leveraging continuous time to understand momentum when training diagonal linear networks. In Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AIS-TATS) 2024, 2024.   \n[24] Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[25]  Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: aprovable benefit of stochasticity.Advances in Neural Iforation Processing Systems, 34:29218-29230, 2021.   \n[26] Garvesh Raskuti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation for highdimensional linear regression over $\\ell\\_q$ balls. IEEE transactions on information theory, 57(10):6976-6994, 2011.   \n[27]  Dominic Richards and Patrick Rebeschini. Graph-dependent implicit regularisation for distributed stochastic subgradient descent. Journal of Machine Learning Research, 21(34):1-44, 2020.   \n[28] Ohad Shamir. The implicit bias of benign overitting. In Conference on Learning Theory, pages 448-478. PMLR, 2022.   \n[29] Ying Sun, Marie Maros, Gesualdo Scutari, and Guang Cheng. High-dimensional inference over networks: Linear convergence and statistical guarantees. arXiv preprint arXiv:2201.08507, 2022.   \n[30] Hossein Taheri and Christos Thrampoulidis. On generalization of decentralized learning with separable data. In International Conference on Artificial Intelligence and Statistics, pages 4917-4945. PMLR, 2023.   \n[31]  Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267-288, 1996.   \n[32] Gal Vardi. On the implicit bias in deep-learning algorithms. Communications of the ACM, 66(6):86-93, 2023.   \n[33]  Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse recovery. Advances in Neural Information Processing Systems, 32, 2019.   \n[34] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \n[35] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on Learning Theory, pages 3635-3673. PMLR, 2020.   \n[36] Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training linear neural networks. In International Conference on Learning Representations, 2020.   \n[37]  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2016.   \n[38]  Peng Zhao, Yun Yang, and Qiao-Chu He. Implicit regularization via hadamard product overparametrization in high-dimensional linear regression. arXiv preprint arXiv:1903.09367, 2(4):8, 2019.   \n[39]  Mo Zhou and Rong Ge. Implicit regularization leads to benign overfitting for sparse linear regression. In International Conference on Machine Learning, pages 42543-42573. PMLR, 2023.   \n[40]  Tongtian Zhu, Fengxiang He, Kaixuan Chen, Mingli Song, and Dacheng Tao. Decentralized sgd and average-direction sam are asymptotically equivalent. In International Conference on Machine Learning, pages 43005-43036. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Section A.1, A.2 give the additional notations and useful basic lemmas, respectively. Section A.3 provides the properties for the simplified setting where all the elements of ground truth parameter $\\pmb{w}^{\\star}$ are non-negative. Section A.4 gives the key proposition for the general case where $\\pmb{w}^{\\star}$ containsboth positive and negative elements on support $\\boldsymbol{S}$ . Final Section A.5 concludes the proofs for the Theorem 1, Corollary 1, and Proposition 1 in the main paper based on Proposition 3 in Section A.4. ", "page_idx": 13}, {"type": "text", "text": "A.1 Full Notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In addition to introduced notations from Section 3.1 in the main paper, we need additional notations to present the prof. The consensus eroris denoted as tu, := ut: - u'. \u25b3t's ts+=t1s+ and notations\u25b3tui's- $\\Delta_{u,S^{-}}^{t,i},\\Delta_{u,S^{c}}^{t,i}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U^{t}:=\\left[u^{t,i},\\cdots,u^{t,m}\\right],\\quad\\Delta_{u}^{t}:=\\left[\\Delta_{u}^{t,1},\\cdots,\\Delta_{u}^{t,m}\\right],\\quad\\overline{{U}}^{t}:=\\overline{{u}}^{t}{\\bf1}_{m}^{T};}\\\\ &{\\nabla_{u}f\\left(\\overline{{U}}^{t},\\overline{{V}}^{t}\\right):=\\left[\\nabla_{u}f_{1}(\\overline{{u}}^{t},\\overline{{v}}^{t}),\\cdots,\\nabla_{u}f_{m}(\\overline{{u}}^{t},\\overline{{v}}^{t})\\right],\\quad\\nabla_{u}F(\\overline{{U}}^{t},\\overline{{V}}^{t}):=\\nabla_{u}F(\\overline{{u}}^{t},\\overline{{v}}^{t}){\\bf1}_{m}}\\\\ &{\\nabla_{u}f\\left(U^{t},V^{t}\\right):=\\left[\\nabla_{u}f_{1}(u^{t,1},v^{t,1}),\\cdots,\\nabla_{u}f_{m}(u^{t,m},v^{t,m})\\right]}\\\\ &{\\nabla_{u}f\\left(\\overline{{U}}^{t},V^{t}\\right):=\\left[\\nabla_{u}f_{1}(\\overline{{u}}^{t},v^{t,1}),\\cdots,\\nabla_{u}f_{m}(\\overline{{u}}^{t},v^{t,m})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Above definitions can be also extended to variable $\\pmb{v}$ similarly. ", "page_idx": 13}, {"type": "text", "text": "A.2 Premilary Lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 1. (Theorem 6.5 in [34]) Consider the random feature vector $\\pmb{x}\\in\\mathbb{R}^{d}$ that all entries obey i.i.d. 1-sub-Gaussian distribution, if the sample size satisfies $n\\gtrsim\\delta^{-2}$ $s\\log\\frac{e d}{s}+\\log\\frac{2}{\\epsilon})$ ,then the sample covariance matrix $\\begin{array}{r}{\\hat{\\pmb X}=\\frac{1}{n}\\sum_{i=1}^{n}{\\pmb x}_{i}\\pmb x_{i}^{T}}\\end{array}$ satisfies $(\\delta,s)$ -RIP condition with probability $1-\\epsilon$ Lemma 2. (Lemma A.3 in [33]) Suppose that $\\frac{\\pmb{X}}{\\sqrt{n}}\\in\\mathbb{R}^{n\\times d}$ satisfies the $(\\delta,s+1)$ -RIP f $\\pmb{w}\\in\\mathbb{R}^{d}$ . a s-sparse vecto, hen $\\begin{array}{r}{\\left\\|\\left(\\frac{{\\boldsymbol X}^{T}{\\boldsymbol X}}{n}-{\\boldsymbol I}\\right){\\boldsymbol w}\\right\\|_{\\infty}\\leq\\bar{\\sqrt{s}}\\delta\\|{\\boldsymbol w}\\|_{\\infty}.}\\end{array}$   \nLemma3. (Lemma A.4in [33] Suppose that ${\\frac{1}{\\sqrt{n}}}X\\in\\mathbb{R}^{n\\times d}$ that satisjes $(\\delta,1)$ -RIP with $0\\leq\\delta\\leq1$ \uff0c then we have $\\begin{array}{r}{\\left\\|\\frac{\\mathbf{X}^{T}\\mathbf{X}}{n}\\pmb{w}\\right\\|_{\\infty}\\leq2d\\|\\pmb{w}\\|_{\\infty},\\forall\\pmb{w}\\in\\overset{\\cdot}{\\mathbb{R}}^{d}.}\\end{array}$   \nLemma 4. (Lemma $B.5$ in $[38]$ )Let $\\xi\\in\\mathcal{R}^{n}$ is a vector of independent $\\sigma$ -sub-Gaussianrandom variables and all $\\ell_{2}$ norm of column vectors of $\\pmb{X}\\in\\mathbb{R}^{n\\times d}$ are bounded, then with high probability $\\begin{array}{r}{1-\\frac{1}{8d^{3}}}\\end{array}$ 8ds such that $\\left\\|{\\frac{\\mathbf{X}^{T}\\pmb{\\xi}}{n}}\\right\\|_{\\infty}\\lesssim\\sigma{\\sqrt{\\frac{\\log d}{n}}}$ ", "page_idx": 13}, {"type": "text", "text": "A.3  Non-negative Case ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We consider the simplified setting where all the elements on support are positive for ground truth $\\pmb{w}^{\\star}$ The following lemma shows the recursion of average variable $\\mathbf{\\bar{\\mu}}^{-}\\mathbf{\\bar{u}}^{t}$ on support $\\boldsymbol{S}$ and non-support $S^{c}$ ", "page_idx": 13}, {"type": "text", "text": "Lemma 5. Consider the sequence $\\{\\pmb{u}^{t,i}\\}$ generated according to (3) and (4) by DGD for solving loss function in (2), the average signal $\\overline{{\\pmb{u}}}^{t}$ onsupport $\\boldsymbol{S}$ andnon-support $S^{c}$ areupdated according to thefollowingformulas ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\boldsymbol{u}}}_{S}^{t+1}=\\overline{{\\boldsymbol{u}}}_{S}^{t}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(\\overline{{\\boldsymbol{u}}}_{S}^{t}\\odot\\overline{{\\boldsymbol{u}}}_{S}^{t}-\\boldsymbol{w}^{\\star}\\right)-4\\eta\\frac{X^{T}X}{N}\\left(\\overline{{\\boldsymbol{u}}}_{S^{c}}^{t}\\odot\\overline{{\\boldsymbol{u}}}_{S^{c}}^{t}\\right)+4\\eta\\frac{X^{T}\\xi}{N}\\right.}\\\\ &{\\qquad\\quad-\\left.4\\eta\\left(\\frac{X X^{T}}{N}-I\\right)\\left(\\overline{{\\boldsymbol{u}}}_{S}^{t}\\odot\\overline{{\\boldsymbol{u}}}_{S}^{t}-\\boldsymbol{w}^{\\star}\\right)-4\\eta p^{t}\\right)-4\\eta q^{t};}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\overline{{u}}_{S c}^{t+1}=\\overline{{u}}_{S c}^{t}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(\\frac{X^{T}X}{N}\\left(\\overline{{u}}_{S c}^{t}\\odot\\overline{{u}}_{S c}^{t}\\right)-\\frac{X^{T}\\xi}{N}+\\left(\\frac{X X^{T}}{N}-I\\right)\\left(\\overline{{u}}_{S}^{t}\\odot\\overline{{u}}_{S}^{t}-w^{\\star}\\right)\\right)\\right)\\mathrm{~k~H~}^{s},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n-4\\eta\\pmb{g}^{t})-4\\eta\\pmb{f}^{t},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the perturbed error terms $\\pmb{p}^{t},\\pmb{q}^{t},\\pmb{g}^{t},\\pmb{f}^{t}$ induced from decentralized network are defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle p^{t}=\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{X_{i}^{t}X_{i}}{n}-I\\right)\\left(2\\overline{{u_{\\delta}}}\\odot\\Delta_{\\delta}^{t,i}+\\Delta_{\\delta}^{t,i}\\odot\\Delta_{\\delta}^{t,i}\\right)+3\\Delta_{\\delta}^{t,i}\\odot\\Delta_{\\delta}^{t,i}}\\\\ {\\displaystyle}&{\\qquad+\\frac{X_{i}^{t}X_{i}}{n}\\left(2\\overline{{u_{\\delta}}}^{t}\\odot\\Delta_{\\delta}^{t,i}+\\Delta_{\\delta}^{t,i}\\odot\\Delta_{\\delta}^{t,i}\\right);}\\\\ {\\displaystyle}&{\\displaystyle q^{t}=\\frac{1}{m}\\sum_{i=1}^{m}\\Delta_{\\delta}^{t,i}\\odot\\left(\\left(\\frac{X_{i}^{t}X_{i}}{n}-I\\right)\\left(\\overline{{u}}_{\\delta}^{t}\\odot\\overline{{u}}_{\\delta}^{t}-w^{*}+2\\overline{{u}}_{\\delta}^{t}\\odot\\Delta_{\\delta}^{t,i}+\\Delta_{\\delta}^{t,i}\\odot\\Delta_{\\delta}^{t,i}\\right)\\right.}\\\\ &{\\displaystyle\\qquad\\left.+\\Delta_{\\delta}^{t,i}\\odot\\Delta_{\\delta}^{t,i}-\\frac{X_{i}^{t}\\xi_{i}}{n}+\\frac{X_{i}^{t}X_{i}}{n}\\left(\\overline{{u}}_{\\delta}^{t}\\circ\\Delta_{\\delta}^{t,i}+\\Delta_{\\delta}^{t,i}\\right)^{2}\\right);}\\\\ {\\displaystyle g^{t}=\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{X_{i}^{t}X_{i}}{n}-I\\right)\\left(2\\overline{{u}}_{\\delta}^{t}\\odot\\Delta_{\\delta}^{t,i}+\\Delta_{\\delta}^{t,i}\\odot\\Delta_{\\delta}^{t,i}\\right)+\\frac{X_{i}^{T}X_{i}}{n}\\left(2\\overline{{u}}_{\\delta}^{t}\\odot\\Delta_{\\delta}^{t,i}+\\Delta_{\\delta}^{t,i}\\odot\\Delta_{\\delta}^{t,i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal f}^{t}=\\frac{1}{m}\\sum_{i=1}^{m}\\Delta_{S^{c}}^{t,i}\\odot\\left(\\left(\\frac{X_{i}^{T}X_{i}}{n}-I\\right)\\left(\\overline{{{u}}}_{S}^{t}\\odot\\overline{{{u}}}_{S}^{t}-{w}^{\\star}+2\\overline{{{u}}}_{S}^{t}\\odot\\Delta_{S}^{t,i}+\\Delta_{S}^{t,i}\\odot\\Delta_{S}^{t,i}\\right)\\right.}}\\\\ {{\\displaystyle\\qquad\\left.+\\frac{X_{i}^{T}X_{i}}{n}\\left(\\overline{{{u}}}_{S^{c}}^{t}+\\Delta_{S^{c}}^{t,i}\\right)^{2}-\\frac{X_{i}^{T}\\xi_{i}}{n}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Based on the updating of DGD, one-step iteration of the averaged parameter is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\overline{{\\pmb{u}}}^{t+1}=\\overline{{\\pmb{u}}}^{t}-\\eta\\nabla F(\\overline{{\\pmb{u}}}^{t})+\\frac{\\eta}{m}\\sum_{i=1}^{m}\\left(\\nabla f_{i}(\\overline{{\\pmb{u}}}^{t})-\\nabla f_{i}(\\pmb{u}^{t,i})\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The gradient difference has the formula as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{7f_{i}(\\overline{{u}}^{t})-\\nabla f_{i}(u^{t,i})=\\overline{{u}}^{t}\\odot\\left(\\frac{4}{n}X_{i}^{T}X_{i}\\left(\\overline{{u}}^{t}\\odot\\overline{{u}}^{t}-w^{*}\\right)-\\frac{4}{n}X_{i}^{T}\\xi_{i}\\right)}\\\\ &{\\phantom{7p c}-\\!u^{t,i}\\odot\\left(\\frac{4}{n}X_{i}^{T}X_{i}\\left(u^{t,i}\\odot u^{t,i}-w^{*}\\right)-\\frac{4}{n}X_{i}^{T}\\xi_{i}\\right)}\\\\ &{\\phantom{7p c}=\\left(\\overline{{u}}^{t}-u^{t,i}\\right)\\odot\\left(\\frac{4}{n}X_{i}^{T}X_{i}\\left(u^{t,i}\\odot u^{t,i}-w^{*}\\right)-\\frac{4}{n}X_{i}^{T}\\xi_{i}\\right)}\\\\ &{\\phantom{7p c}+\\overline{{u}}^{t}\\odot\\left(\\frac{4}{n}X_{i}^{T}X_{i}\\left(\\overline{{u}}^{t}\\odot\\overline{{u}}^{t}-u^{t,i}\\odot u^{t,i}\\right)\\right)}\\\\ &{\\phantom{7p c}=-\\Delta^{t,i}\\odot\\left(\\frac{4}{n}X_{i}^{T}X_{i}\\left(\\overline{{u}}^{t}\\odot\\overline{{u}}^{t}-w^{*}+2\\overline{{u}}^{t}\\odot\\Delta^{t,i}+\\Delta^{t,i}\\odot\\Delta^{t,i}\\right)-\\frac{4}{n}X_{i}^{T}\\right.}\\\\ &{\\phantom{7p c}\\left.-\\overline{{u}}^{t}\\odot\\left(\\frac{4}{n}X_{i}^{T}X_{i}\\left(2\\overline{{u}}^{t}\\odot\\Delta^{t,i}+\\Delta^{t,i}\\odot\\Delta^{t,i}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality is due to the definition of $\\Delta^{t,i}$ . Substituting the above equality into (16) would have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\overline{{{u}}}^{t+1}=\\overline{{{u}}}^{t}-4\\eta\\overline{{{u}}}^{t}\\odot\\left(\\overline{{{u}}}_{S}^{t}\\odot\\overline{{{u}}}_{S}^{t}-w^{\\star}+\\frac{X^{T}X}{N}\\left(\\overline{{{u}}}_{S^{c}}^{t}\\odot\\overline{{{u}}}_{S^{c}}^{t}\\right)-\\frac{X^{T}\\xi}{N}\\right.}}}\\\\ {{\\displaystyle{\\qquad+\\left.\\left(\\frac{X X^{T}}{N}-I\\right)\\left(\\overline{{{u}}}_{S}^{t}\\odot\\overline{{{u}}}_{S}^{t}-w^{\\star}\\right)\\right)}}}\\\\ {{\\displaystyle{\\qquad-\\,4\\eta\\overline{{{u}}}^{t}\\odot\\frac{1}{m}\\sum_{i=1}^{m}\\left(2\\left(\\frac{X_{i}^{T}X_{i}}{n}-I\\right)\\left(\\overline{{{u}}}_{S}^{t}\\odot\\Delta_{S}^{t,i}\\right)+2\\overline{{{u}}}_{S}^{t}\\odot\\Delta_{S}^{t,i}\\right.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+2\\frac{X^{\\prime}X}{n}\\left(u_{s}^{t}\\circ\\Delta_{s}^{t}\\right)+\\left(\\frac{X^{\\prime}X}{n}-I\\right)\\left(\\Delta_{s}^{t+1}\\circ\\Delta_{s}^{t}\\right)+\\Delta_{s}^{t+1}\\otimes\\Delta_{s}^{t}}\\\\ &{\\quad+\\frac{X^{\\prime}X}{n}\\left(\\Delta_{s}^{t+1}\\otimes\\Delta_{s}^{t+1}\\right)}\\\\ &{\\quad-4\\eta\\frac{Y}{n}\\Delta_{s}^{t}\\Delta^{t}\\otimes\\left(u_{s}^{t}\\circ\\Delta_{s}^{t}-w^{*}+\\left(\\frac{X^{\\prime}X}{n}-I\\right)\\left(u_{s}^{t}\\circ\\Delta_{s}^{t}-w^{*}\\right)\\right.}\\\\ &{\\quad\\left.+\\frac{X^{\\prime}X}{n}\\left(u_{s}^{t},\\circ\\Delta_{s}^{t}\\right)-\\frac{X^{\\prime}X^{\\prime}}{n}\\right)}\\\\ &{\\quad-4\\eta\\frac{Y}{m}\\frac{\\sqrt{n}}{m}\\Delta^{t+1}\\otimes\\left(2\\left(\\frac{X^{\\prime}X}{n}-I\\right)\\left(u_{s}^{t}\\circ\\Delta_{s}^{t}\\right)+2u_{s}^{t}\\circ\\Delta_{s}^{t}\\right.}\\\\ &{\\quad\\left.+2\\frac{X^{\\prime}X}{n}\\right)\\left(u_{s}^{t}\\circ\\Delta_{s}^{t}\\right)+\\left(\\frac{X^{\\prime}X}{n}-I\\right)\\left(\\Delta_{s}^{t+1}\\circ\\Delta_{s}^{t}\\right)+\\Delta_{s}^{t+1}\\otimes\\Delta_{s}^{t}}\\\\ &{\\quad\\left.+\\frac{X^{\\prime}X^{\\prime}}{n}\\left(\\Delta_{s}^{t}\\circ\\Delta_{s}^{t}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Because heare $\\begin{array}{r}{\\frac{1}{m}\\sum_{i=1}^{m}\\overline{{\\mathbf{u}}}_{S}^{t}\\odot\\pmb{\\Delta}_{S}^{t,i}=\\mathbf{0}}\\end{array}$ and $\\begin{array}{r}{\\frac{1}{m}\\sum_{i=1}^{m}\\Delta_{S}^{t,i}\\odot\\left(\\overline{{\\pmb{u}}}_{S}^{t}\\odot\\overline{{\\pmb{u}}}_{S}^{t}-\\pmb{w}^{\\star}\\right)=\\bf{0}}\\end{array}$ the above formula can be simplified as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{u^{x_{1}}}&{=-\\alpha_{1}u^{x_{2}}\\Big(u_{1}^{y_{1}}\\otimes u_{2}^{x_{2}}\\cdots u^{x_{N}}+\\frac{\\boldsymbol{X}^{x_{1}}}{\\boldsymbol{N}}\\left(u_{2}^{y_{2}}\\otimes u_{3}^{y_{1}}\\right)-\\frac{\\boldsymbol{X}^{y_{1}}}{\\boldsymbol{N}}\\boldsymbol{\\ell}}\\\\ &{\\quad+\\left(\\frac{\\boldsymbol{X}^{x_{2}}}{\\boldsymbol{N}}-\\frac{\\boldsymbol{X}^{y_{1}}}{\\boldsymbol{N}}\\right)\\left(u_{3}^{y_{2}}\\otimes u_{4}^{y_{2}}\\cdots\\cdots\\right)\\Big)}\\\\ &{\\quad-i q^{x_{N}}\\Big(u_{1}^{y_{1}}\\cdots u_{N}^{y_{1}}\\Big)}\\\\ &{\\quad+\\alpha_{1}\\overline{{\\alpha}}\\left(\\frac{\\boldsymbol{X}^{y_{2}}}{\\boldsymbol{N}}\\right)\\left(-\\left(\\frac{\\boldsymbol{X}^{x_{1}}}{\\boldsymbol{N}}-\\frac{\\boldsymbol{X}^{y_{2}}}{\\boldsymbol{N}}\\right)\\left(u_{3}^{y_{2}}\\otimes\\boldsymbol{X}^{y_{2}}\\right)+\\alpha_{2}^{y_{2}}\\otimes\\boldsymbol{X}^{y_{2}}\\right)}\\\\ &{\\quad+\\frac{\\boldsymbol{X}^{x_{1}}}{\\boldsymbol{N}}\\left(u_{1}^{y_{1}}\\otimes u_{2}^{y_{2}}\\right)+\\left(\\frac{\\boldsymbol{X}^{y_{2}}\\boldsymbol{X}}{\\boldsymbol{N}}_{1}-\\frac{\\boldsymbol{Y}}{\\boldsymbol{Y}}\\right)\\left(u_{3}^{y_{2}}\\otimes u_{3}^{y_{1}}\\right)+\\Delta_{3}^{y_{1}}\\alpha_{1}^{y_{2}}\\otimes\\boldsymbol{X}_{3}^{y_{2}}}\\\\ &{\\quad+\\frac{\\boldsymbol{X}^{x_{2}}\\boldsymbol{X}^{x_{1}}}{\\boldsymbol{N}}\\left(\\Delta_{3}^{y_{2}}\\otimes u_{4}^{y_{2}}\\right)}\\\\ &{\\quad-i q\\frac{\\boldsymbol{Y}}{\\boldsymbol{Y}}\\frac{1}{\\alpha_{1}}\\overline{{\\alpha}}\\left(\\frac{\\boldsymbol{Y}^{x_{1}}}{\\boldsymbol{Y}^{y_{2}}}-\\frac{\\boldsymbol{Y}^{y_{2} \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, this would obtain the recursion (10) for support averaged signal. ", "page_idx": 15}, {"type": "text", "text": "The recursion of optimization error on non-support $S^{c}$ becomes ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{u_{p}^{(k)}=u_{p}^{(k)}-i q_{\\phi}^{(k)}u_{p}^{(k)}}&{=\\left(\\!\\!\\frac{X^{\\prime}X^{\\prime}}{N}\\left(u_{p}^{(k)}\\otimes u_{p}^{(k)}-\\right)\\!-\\!\\frac{X^{\\prime}X^{\\prime}}{N}\\left(\\frac{X X^{\\prime}Y^{\\prime}}{N}-I\\right)(u_{\\phi}^{(k)}\\otimes u_{p}^{(k)}-u^{(k)})\\!\\right)}\\\\ &{\\quad-i q_{\\phi}^{(k)}\\frac{1}{N}\\frac{1}{N}\\left(\\frac{X Y^{\\prime}X^{\\prime}}{16}-I\\right)\\left(u_{\\phi}^{(k)}\\otimes A_{\\xi}^{(k)}\\right)}\\\\ &{\\quad+2\\frac{X^{\\prime}X^{\\prime}}{N}\\left(u_{p}^{(k)}\\otimes u_{p}^{(k)}\\right)+\\left(\\!\\frac{X^{\\prime}X^{\\prime}X^{\\prime}}{16}-I\\right)\\left(u_{\\phi}^{(k)}\\otimes A_{\\xi}^{(k)}\\right)}\\\\ &{\\quad+\\frac{X^{\\prime}X^{\\prime}}{N}\\left(\\Delta_{p}^{(k)}\\otimes A_{\\xi}^{(k)}\\right)}\\\\ &{\\quad-i q_{\\phi}^{(k)}\\frac{1}{N}\\Delta_{p}^{(k)}\\left(u_{p}^{(k)}\\otimes\\left(\\left(\\frac{X^{\\prime}X^{\\prime}}{16}-I\\right)(u_{\\phi}^{(k)}\\otimes u_{p}^{(k)}-u^{(k)})\\!\\right)\\!}\\\\ &{\\quad+\\frac{X^{\\prime}X^{\\prime}}{N}\\left(u_{p}^{(k)}\\otimes u_{p}^{(k)}\\right)-\\frac{X^{\\prime}Y^{\\prime}}{N}\\frac{1}{N}\\left(u_{p}^{(k)}\\otimes\\left(u_{p}^{(k)}-u^{(k)}\\right)\\!\\right)}\\\\ &{\\quad-i q_{\\phi}^{(k)}\\frac{1}{N}\\Delta_{p}^{(k)}u_{p}^{(k)}\\left(2\\left(\\frac{X^{\\prime}X^{\\prime}}{16}-I\\right)(u_{\\phi}^{(k)}\\otimes\\Delta_{\\xi}^{(k)}\\right)\\!\\right.}\\\\ &{\\quad\\left.+2\\frac{X^{\\prime}X^{\\prime}}{N}\\left(u_{p}^{(k)}\\otimes\\Delta\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Rearranging the above equality would obtain the (11). ", "page_idx": 16}, {"type": "text", "text": "The following lemma shows the recursion of consensus error on support $\\boldsymbol{S}$ and non-support $S^{c}$ Lemma 6. Consider the sequence $\\{\\pmb{u}^{t,i}\\}$ generated according to (3) and (4) by DGD for solving loss function in (2), the consensus error $\\Delta^{t}$ on support $\\boldsymbol{S}$ and non-support $S^{c}$ have following recursion ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|{\\Delta}_{S}^{t+1}\\right\\|_{\\infty}\\leq\\rho\\left\\|{\\Delta}_{S}^{t}\\right\\|_{\\infty}\\Bigg(1+4\\eta\\Bigg(\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+1\\right)\\left\\|{\\overline{{u}}}_{S}^{t}\\odot{\\overline{{u}}}_{S}^{t}-{\\boldsymbol w}^{*}\\right\\|_{\\infty}+2d\\left({\\overline{{u}}}_{S}^{t}\\epsilon+{\\Delta}_{S\\sigma}^{t}\\right)^{2}}\\\\ {+\\left.2\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+1\\right)\\left(\\left\\|{\\overline{{u}}}_{S}^{t}\\right\\|_{\\infty}+\\left\\|{\\Delta}_{S}^{t}\\right\\|_{\\infty}\\right)^{2}+\\underset{i}{\\operatorname*{max}}\\left\\|{\\underline{{X}}}_{i}^{T}\\xi_{i}\\right\\|_{\\infty}\\Bigg)\\Bigg)}\\\\ {+\\left.4\\rho\\eta\\left\\|{\\overline{{u}}}_{S}^{t}\\right\\|_{\\infty}\\cdot\\Bigg(\\sqrt{s}\\left(\\delta_{\\operatorname*{max}}+\\delta\\right)\\left\\|{\\overline{{u}}}_{S}^{t}\\odot{\\overline{{u}}}_{S}^{t}-{\\boldsymbol w}^{*}\\right\\|_{\\infty}+\\underset{i}{\\operatorname*{max}}\\left\\|{\\frac{X_{i}^{T}\\xi_{i}}{n}}\\right\\|_{\\infty}}\\\\ {+\\left\\|{\\frac{X^{T}\\xi}{n}}\\right\\|_{\\infty}+2d\\left({\\overline{{u}}}_{S}^{t}+{\\Delta}_{S\\sigma}^{t}\\right)^{2}\\Bigg);}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\Delta_{s c}^{t+1}\\right|\\right|_{\\infty}\\leq\\rho\\left\\|\\Delta_{s c}^{t}\\right\\|_{\\infty}\\Bigg(1+4\\eta\\Bigg(\\sqrt{s}\\delta_{\\operatorname*{max}}\\left\\|\\overline{{u}}_{s}^{t}\\odot\\overline{{u}}_{s}^{t}-w^{\\star}\\right\\|_{\\infty}+\\sqrt{s}\\delta_{\\operatorname*{max}}\\left(\\left\\|\\overline{{u}}_{s}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{s}^{t}\\right\\|_{\\infty}\\right)^{2}}\\\\ &{\\qquad\\qquad+6d\\left(\\left\\|\\overline{{u}}_{s c}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{s c}^{t}\\right\\|_{\\infty}\\right)^{2}+\\operatorname*{max}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}\\Bigg)\\Bigg)}\\\\ &{\\qquad\\qquad+4\\rho\\eta\\left\\|\\overline{{u}}_{s c}^{t}\\right\\|_{\\infty}\\cdot\\Big(\\sqrt{s}\\left(\\delta_{\\operatorname*{max}}+\\delta\\right)\\left\\|\\overline{{u}}_{s}^{t}\\odot\\overline{{u}}_{s}^{t}-w^{\\star}\\right\\|_{\\infty}+\\sqrt{s}\\delta_{\\operatorname*{max}}\\left\\|\\Delta_{s}^{t}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\cdot\\left(\\left\\|\\overline{{u}}_{s}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{s}^{t}\\right\\|_{\\infty}\\right)+4d\\left\\|\\overline{{u}}_{s c}^{t}\\right\\|_{\\infty}^{2}+\\operatorname*{max}_{i}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}+\\left\\|\\frac{X^{T}\\xi}{N}\\right\\|_{\\infty}\\Big).\\qquad(22)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Based on the iteration of DGD, there is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U^{t+1}\\left(I_{m}-\\displaystyle\\frac{1}{m}\\mathbf{1}_{m}\\mathbf{1}_{m}^{T}\\right)=\\left(U^{t}-\\eta\\nabla f(U^{t})\\right)W\\left(I_{m}-\\displaystyle\\frac{1}{m}\\mathbf{1}_{m}\\mathbf{1}_{m}^{T}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\left(U^{t}-\\eta\\nabla f(U^{t})\\right)\\left(W-\\displaystyle\\frac{1}{m}\\mathbf{1}_{m}\\mathbf{1}_{m}^{T}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\left(U^{t}-\\overline{{U}}^{t}-\\eta\\nabla f(U^{t})+\\eta\\nabla F(\\overline{{U}}^{t})\\right)\\left(W-\\displaystyle\\frac{1}{m}\\mathbf{1}_{m}\\mathbf{1}_{m}^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, the consensus error on support $\\boldsymbol{S}$ has recursion as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\operatorname{f}\\left(m\\neq0\\right)\\mathbb{E}\\left(\\left|S_{j}\\cap\\overline{{\\Omega}}\\right|-\\mathbb{F}\\left(M_{j}\\right)\\right)\\mathbb{E}_{x}\\times\\mathbb{F}\\left(\\mathbb{K}_{j}\\right)}\\\\ &{\\underset{=}^{3}\\int\\mathbb{E}\\left|\\mathcal{X}_{j}\\right|\\int_{x_{j-1}}^{1}+\\alpha\\left|\\alpha\\right|\\mathbb{E}_{x}\\right|\\left(-\\left|\\beta\\right|_{0}^{1}\\left|\\mathcal{X}_{j}-\\alpha\\right|^{2}\\right)_{\\infty}}\\\\ &{\\quad+2\\left|\\alpha\\left|\\frac{1}{\\alpha}\\right|_{x}^{2}+\\alpha+\\frac{{\\alpha}}{2}\\mathbb{E}\\left|\\frac{1}{x}\\right|_{x_{0}}^{2}\\bigg[\\mathrm{~s}\\right]+2\\left\\langle\\overline{{\\alpha}}\\overline{{\\gamma}}\\overline{{\\alpha}}_{0}+1\\right|M_{j}\\right|\\mathrm{E}_{x}\\right|}\\\\ &{\\quad+4\\alpha\\left|\\alpha_{0}^{\\prime}\\right|_{x}^{2}\\left|\\overline{{\\alpha}}_{0}\\right\\rangle\\mathbb{E}_{x}\\times\\left(\\sqrt{\\alpha}_{0}\\right)\\left|\\alpha\\right|_{x}^{2}+2\\alpha^{\\prime}\\left|\\alpha\\right|_{x}^{2}\\left|\\frac{1}{\\alpha}\\right|_{x}^{2}}\\\\ &{\\quad+4\\alpha^{\\prime}\\left|\\alpha\\right|_{x}^{2}\\bigg\\rfloor_{x}\\times\\bigg(2\\left\\langle\\overline{{\\alpha}}\\overline{{\\gamma}}_{0}+1\\right\\rangle\\left|\\alpha\\right|_{x}^{2}\\left|\\Delta_{j}\\right|+M_{j}^{\\alpha}\\bigg)\\mathbb{E}_{x}\\bigg[\\bigg|\\Delta_{j}\\bigg|\\mathrm{E}_{x}\\bigg]}\\\\ &{\\quad+\\left(\\gamma\\alpha_{0}^{\\prime}\\right)\\left(\\gamma\\right)\\mathbb{E}_{x}\\bigg[\\bigg|\\Delta_{j}^{1}\\right|_{x_{0}}^{2}+2\\alpha\\left|\\overline{{\\alpha}}_{0}\\right|\\Delta_{j}^{2}+\\left(\\gamma(\\alpha_{0}+\\frac{\\alpha}{2})+2\\left|\\alpha\\right|_{x}^{2}\\left|\\beta\\right|_{0}^{1}\\sigma_{0}-\\sigma\\right|_{0}^{2}\\bigg]_{\\infty}}\\\\ &{\\quad+2\\alpha\\left|\\alpha\\right|_{x}^{2}\\left|\\overline{{\\alpha}}_{0}+\\gamma\\right|\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(i)$ is due to the defined spectral gap of network in Assumption 1 and $(i i)$ uses the gradient difference formula in (17) and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f_{i}(\\overline{{\\boldsymbol u}}^{t})-\\nabla F(\\overline{{\\boldsymbol u}}^{t})=\\overline{{\\boldsymbol u}}^{t}\\odot\\left(\\frac{4X_{i}^{T}X_{i}}{n}\\left(\\overline{{\\boldsymbol u}}^{t}\\odot\\overline{{\\boldsymbol u}}^{t}-\\boldsymbol w^{\\star}\\right)-\\frac{4X_{i}^{T}\\xi_{i}}{n}\\right)}\\\\ &{\\phantom{\\nabla f_{i}(\\overline{{\\boldsymbol u}}^{t})}-\\overline{{\\boldsymbol u}}^{t}\\odot\\left(\\frac{4X^{T}X}{N}\\left(\\overline{{\\boldsymbol u}}^{t}\\odot\\overline{{\\boldsymbol u}}^{t}-\\boldsymbol w^{\\star}\\right)-\\frac{4X^{T}\\xi}{N}\\right)}\\\\ &{\\phantom{\\nabla f_{i}(\\overline{{\\boldsymbol u}}^{t})}=\\overline{{\\boldsymbol u}}^{t}\\odot\\left(4\\left(\\left(\\frac{X_{i}^{T}X_{i}}{n}-I\\right)-\\left(\\frac{X^{T}X}{N}-I\\right)\\right)\\left(\\overline{{\\boldsymbol u}}^{t}\\odot\\overline{{\\boldsymbol u}}^{t}-\\boldsymbol w^{\\star}\\right)\\right.}\\\\ &{\\phantom{\\nabla f_{i}(\\overline{{\\boldsymbol u}}^{t})}+\\left(\\frac{4X^{T}\\xi}{N}-\\frac{4X_{i}^{T}\\xi_{i}}{n}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "andlocal ad global RIPconditions. The $(i i i)$ is summing up terms involved $\\left\\|\\Delta_{S}^{t+1}\\right\\|_{\\infty}$ and $\\left\\|\\overline{{\\boldsymbol{u}}}_{S}^{t}\\right\\|_{\\infty}$ separately. ", "page_idx": 18}, {"type": "text", "text": "The recursion of consensus error on non-support $S^{c}$ part is as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\rho(\\mathbf{r},\\mathbf{u})\\rho_{1}\\leq\\sigma(\\mathbf{r},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f})=\\sigma(\\mathbf{r},\\mathbf{u})\\rho_{1}\\leq\\sigma(\\mathbf{r},\\mathbf{u})\\rho_{1},}\\\\ &{\\leq\\rho\\left\\{\\mathbf{u}\\right\\}\\rho_{1}\\leq\\sigma\\left\\{\\mathbf{u}\\right\\}\\rho_{1},\\quad\\forall\\,(\\mathbf{r},\\mathbf{u})\\leq\\sigma(\\mathbf{r},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f})=\\sigma\\left\\},\\mathbf{f},\\mathbf{f},\\mathbf{f}\\right\\},,}\\\\ &{\\quad+2\\sigma\\left\\{\\mathbf{u}\\right\\}\\rho_{2}\\leq\\sigma\\left\\{\\mathbf{u}\\right\\}\\sum_{\\mathbf{r},\\mathbf{u}}\\rho_{2}\\leq\\sigma\\left\\}\\\\ &{\\quad+4\\sigma\\left\\{\\mathbf{u}\\right\\}\\rho_{1}\\geq\\sigma\\left\\{\\mathbf{u},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f}\\right\\}}\\\\ &{\\quad+4\\sigma\\left\\{\\mathbf{u}\\right\\}\\rho_{2}\\leq\\sigma\\left\\{\\mathbf{u},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f}\\right\\}}\\\\ &{\\quad+\\sigma\\left\\{\\mathbf{u}\\right\\}\\rho_{1}\\leq\\sigma\\left\\{\\mathbf{u},\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f}\\right\\}}\\\\ &{\\quad+\\left\\{\\mathbf{u}\\right\\}\\rho_{2}\\leq\\sigma\\left\\{\\mathbf{f},\\mathbf{f},\\mathbf{f},\\mathbf{f}\\right\\}}\\\\ &{\\quad+\\sqrt{\\alpha}\\left\\{\\mathbf{u},\\mathbf{f},\\mathbf{f},\\mathbf{f}\\right\\}}\\\\ &{\\quad+\\sqrt{\\alpha}\\left\\{\\mathbf{u},\\mathbf{f},\\mathbf{f}\\right\\}}\\\\ &{\\quad+\\sqrt{\\alpha}\\frac{1}{2}\\sigma\\left\\{\\mathbf{b},\\mathbf{f},\\mathbf{f}\\right\\}}\\\\ &{\\leq\\rho\\left\\{\\mathbf{u}^{2}\\rho_{1}\\leq\\sigma\\left\\}\\sum_{\\mathbf{r}^{\\prime}}\\left\\{\\mathbf{f},\\mathbf{f}\\right\\}\\leq\\frac{\\mathbf{u}}{\\rho\\left\\}\\sum_{\\mathbf{r}^{\\prime}}\\alpha+\\sqrt{\\mathbf{f},\\mathbf{f}\\right\\}\\rho_{2}\\leq\\sigma\\left\\},\\mathbf{f},}\\\\ &{\\quad+\\sigma\\left\\{\\mathbf{b}^{2}\\rho_{1}\\leq\\sigma\\left\\{\\mathbf{b},\\mathbf{f\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The following proposition shows the dynamics of average variable $\\overline{{\\pmb{u}}}^{t}$ and consensus error $\\Delta^{t,i}$ in the form of an inductive hypothesis. Before showing the proposition, we define the following quantities. We define $\\begin{array}{r}{T:=\\frac{1}{\\eta w_{\\mathrm{max}}^{\\star}}\\log\\frac{1}{\\alpha^{4}}}\\end{array}$ and for any iteger $k\\,\\geq\\,-1,T_{k}\\,:=\\,2^{k}T$ and $\\textstyle{\\overline{{T}}}_{k}:=\\sum_{i=0}^{k}T_{i}$ with $\\overline{{T}}_{-1}\\,=\\,0$ where $T_{k}$ denotes the number of iterations between $(k-1)$ -th and $k$ -th induction step. Defining K :=l1og2 max as the number o induction steps, $\\begin{array}{r}{B_{k}:=\\frac{w_{\\mathrm{max}}^{\\star}}{40\\times2^{k}}}\\end{array}$ denotes he upper bound of perturbed error in (k - 1)-th induction step and constant scale parameter \u03b2 := 1/p", "page_idx": 18}, {"type": "text", "text": "Proposition 2. With the same setting as Theorem $^{\\,I}$ ,the following claims hold $k=0,1,\\cdots\\,,K-1$ steps. ", "page_idx": 18}, {"type": "text", "text": "\u00b7(a) For $\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}$ that $\\forall k\\in[K]$ there is $\\begin{array}{r}{\\left\\|\\overline{{\\mathbf{u}}}_{S}^{t}\\odot\\overline{{\\mathbf{u}}}_{S}^{t}-\\mathbf{w}^{\\star}\\right\\|_{\\infty}\\leq\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k}}}\\end{array}$   \n$\\bullet\\mathit{\\Pi}(b)\\mathit{F o r}\\;\\forall k\\in[K],$ there is $\\begin{array}{r}{\\left\\|\\overline{{\\boldsymbol{u}}}_{S}^{\\overline{{T}}_{k-1}}\\odot\\overline{{\\boldsymbol{u}}}_{S}^{\\overline{{T}}_{k-1}}-\\boldsymbol{w}^{\\star}\\right\\|_{\\infty}\\leq\\frac{\\boldsymbol{w}_{\\operatorname*{max}}^{\\star}}{2^{k}}.}\\end{array}$   \n\u00b7 (c) For $\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}$ that $\\forall k\\in[K]$ it has $\\left\\|\\Delta_{S}^{t}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{\\mathbf{u}}}_{S}^{t}\\right\\|_{\\infty}B_{k}$ . In addition, the   \nrefinedelement-wisebound is $\\left\\|\\Delta_{j}^{t}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta|\\overline{{u}}_{j}^{t}|B_{k},\\forall j\\in\\mathcal{S}$ ", "page_idx": 18}, {"type": "text", "text": "\u00b7 (d) For $\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}$ that $\\forall k\\in[K]$ , ithas $\\left\\|\\Delta_{S^{c}}^{t}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{\\pmb{u}}}_{S^{c}}^{t}\\right\\|_{\\infty}B_{k}$ . In addition, the refined element-wise bound is $\\left\\|\\Delta_{j}^{t}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta|\\overline{{u}}_{j}^{t}|B_{k},\\forall j\\in\\mathcal{S}^{c}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bullet\\,\\,(e)\\,F o r\\,\\forall k\\in[K]\\,a n d\\,\\forall j\\in\\mathcal{S},\\,\\alpha^{3}\\leq\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}\\leq w_{j}^{\\star}+4B_{k}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Proof idea: Inductions (a), (b), (e) indicate that if the connectivity of the network is sufficiently well ( $\\mathrm{\\Delta}_{\\rho}$ is small enough), the trajectory of the averaged signal $\\dot{\\overline{{\\mathbf{u}}}}^{t}$ would mimic that of the centralized case [33]. Different from the centralized setting, these three claims are based on inductions (c) and (d), which guarantee that the consensus error along both support $\\boldsymbol{S}$ and non-support $S^{c}$ can be controlled based on the magnitude of the respective signals. We utilize this property to reparameterize consensus error by Hadamard product based on the averaged signal. Thus, the perturbed error terms induced by the decentralized network in the recursion of the averaged signal can be quantitatively through the reparameterized consensus error. Then conditions on network connectivity $\\rho$ and step size $\\eta$ can guarantee that the averaged signal in decentralized would have properties in inductions (a), (b), (e) based on inductions (c), (d). ", "page_idx": 19}, {"type": "text", "text": "Base case: As the initialization ${\\pmb u}^{0,i}\\;=\\;\\alpha{\\bf1}_{d},\\forall i\\;\\in\\;[m]$ .Due to the condition on $\\alpha$ , the base case is true. ", "page_idx": 19}, {"type": "text", "text": "Induction Step: If the above (a)-(e) induction hypotheses hold all until some $0\\leq k\\leq K-1$ we should prove they still hold at $k+1$ -th induction step. ", "page_idx": 19}, {"type": "text", "text": "(a) The magnitude of $\\pmb{p}^{t}$ in (12) under this induction step can be bounded based on inductions (c), (d). $\\forall\\;\\overline{{T}}_{k-1}\\leq t<t+1<\\overline{{T}}_{k}$ if $\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}$ keep same order as initialization, then there is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{p}_{S}^{t}\\right\\|_{\\infty}\\leq8\\sqrt{s}\\delta_{\\operatorname*{max}}\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}\\left\\|\\overline{{u}}_{S}^{t}\\right\\|_{\\infty}^{2}+\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+3\\right)\\left(4\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}\\left\\|\\overline{{u}}_{S}^{t}\\right\\|_{\\infty}\\right)^{2}}\\\\ &{\\qquad\\qquad+\\,2d\\left(8\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}^{2}B_{k}+\\left(4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}B_{k}\\right)^{2}\\right)}\\\\ &{\\qquad\\qquad\\leq\\frac{3\\sqrt{\\rho}B_{k}}{16},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first inequality is based on Lemma 2 and Lemma 3, the last inequality is due to step size,value of $\\beta$ $\\left\\|\\bar{\\boldsymbol{u}}_{S}^{t}\\right\\|_{\\infty}^{2}\\leq2w_{\\mathrm{max}}^{\\star}$ , network connectivity condition and global RIP condition that $\\rho^{\\frac{1}{4}}\\sqrt{s}\\delta_{\\operatorname*{max}}\\leq\\sqrt{s}\\delta\\leq1$ ", "page_idx": 19}, {"type": "text", "text": "For the perturbation $\\pmb q^{t}$ in (13), which is an error term outside the multiplicative updates in (10), based on induction (a), fine-grained upper in (c), there is $\\forall j\\in S$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|q_{j}^{t}|\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}|\\overline{{u}}_{j}|\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}\\left\\|\\overline{{u}}_{S}^{t}\\odot\\overline{{u}}_{S}^{t}-{\\boldsymbol w}^{\\star}\\right\\|_{\\infty}+\\displaystyle\\frac{3\\sqrt{\\rho}B_{k}}{16}+\\operatorname*{max}_{i}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}\\right)}\\\\ {\\displaystyle\\qquad\\leq\\frac{\\sqrt{\\rho}B_{k}|\\overline{{u}}_{j}|}{32},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first inequality is due to (28) by comparing formula of $\\pmb q^{t}$ with $\\pmb{p}^{t}$ and last inequality is due to step size condition and such that $\\begin{array}{r}{\\rho^{\\frac14}\\mathrm{max}_{i}\\left\\|\\frac{X_{i}^{T}\\bar{\\xi}_{i}}{n}\\right\\|_{\\infty}\\leq\\left\\|\\frac{X^{T}\\bar{\\xi}}{N}\\right\\|_{\\infty}<\\bar{w}_{\\mathrm{max}}^{\\star}}\\end{array}$ Then $\\pmb q^{t}$ could be reparameterized as $\\pmb{q}^{t}=\\pmb{r}_{q}^{t}\\odot\\overline{{\\pmb{u}}}_{S}^{t}$ where $\\begin{array}{r}{\\left|\\left|\\boldsymbol{r}_{q}^{t}\\right|\\right|_{\\infty}\\le\\frac{\\sqrt{\\rho}B_{k}}{16}}\\end{array}$ $\\forall t$ that $\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}$ the prubed optimization recursion on support over decentralized network in (10) becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\overline{{\\boldsymbol u}}_{S}^{t+1}\\right)^{2}=\\left(\\overline{{\\boldsymbol u}}_{S}^{t}\\right)^{2}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(\\overline{{\\boldsymbol u}}_{S}^{t}\\odot\\overline{{\\boldsymbol u}}_{S}^{t}-\\boldsymbol w^{\\star}+E_{2}^{t}+E_{3}^{t}+p^{t}+r_{q}^{t}\\right)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the perturbation errors ${\\cal E}_{2}^{t}$ and ${\\cal{E}}_{3}^{t}$ are defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{{\\cal E}}_{2}^{t}=\\left(\\frac{\\pmb{X}^{T}\\pmb{X}}{N}-\\pmb{I}\\right)\\left(\\overline{{{\\pmb u}}}_{S}^{t}\\odot\\overline{{{\\pmb u}}}_{S}^{t}-\\pmb{w}^{\\star}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\pmb{{\\cal E}}}_{3}^{t}=\\frac{{\\pmb{X}}^{T}{\\pmb{X}}}{N}(\\mpb{\\overline{{u}}}_{S^{c}}^{t}\\odot\\pmb{\\overline{{u}}}_{S^{c}}^{t})-\\frac{{\\pmb{X}}^{T}{\\pmb{\\xi}}}{N}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Because there is $\\left\\|\\pmb{{\\cal E}}_{2}^{t}\\right\\|_{\\infty}+\\left\\|\\pmb{{\\cal E}}_{3}^{t}\\right\\|_{\\infty}+\\left\\|\\pmb{{\\cal p}}^{t}\\right\\|_{\\infty}+\\left\\|\\pmb{{\\cal r}}_{q}^{t}\\right\\|_{\\infty}\\leq B_{k}$ which is based on the upper bound in (70). Then the proof is divided into the following two cases based on the magnitude of the element in w's+\u00b7 ", "page_idx": 20}, {"type": "text", "text": "(1) $\\forall j$ that $w_{j}^{\\star}~\\ge~20B_{k}$ , based on induction hypothesis (e) that $\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}\\ \\leq\\ w_{j}^{\\star}\\ +\\ 4B_{k}$ there is $\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}\\ \\leq\\ \\frac{6}{5}w_{j}^{\\star}$ , which illustrates that it satisfies the conditions in Lemma B.10 in [33]. Then because induction hypothesis (a) and (b) are_ true until $t$ -th iteration, then if $\\begin{array}{r}{B_{k}\\ <\\ \\Big\\|\\big(\\overline{{u}}_{j}^{t}\\big)^{2}-w_{j}^{\\star}\\Big\\|_{\\infty}\\ \\leq\\ \\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k}}}\\end{array}$ $\\begin{array}{r}{\\left\\|\\left(\\overline{{\\boldsymbol u}}_{j}^{t+1}\\right)^{2}-\\boldsymbol w_{j}^{\\star}\\right\\|_{\\infty}\\,\\leq\\,\\left\\|\\left(\\overline{{\\boldsymbol u}}_{j}^{t}\\right)^{2}-\\boldsymbol w_{j}^{\\star}\\right\\|_{\\infty}\\,\\leq\\,\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k}}}\\end{array}$ $\\left\\|\\left(\\overline{{u}}_{j}^{t}\\right)^{2}-w_{j}^{\\star}\\right\\|_{\\infty}\\leq B_{k}$ ,then $\\left\\|\\left(\\overline{{u}}_{j}^{t+1}\\right)^{2}-w_{j}^{\\star}\\right\\|_{\\infty}\\leq B_{k}$ Combined withtwo cases,wecanconclude that (a) also holds for $(t+1)$ -th iteration for $j$ that $w_{j}^{\\star}\\geq20B_{k}$ ", "page_idx": 20}, {"type": "text", "text": "(2) For arbitrary $j$ -th elements whose magnitude is not sufficiently larger than the perturbation that $w_{j}^{\\star}\\leq20B_{k}$ , based on the upper bound in induction (e), perturbation bound and monotonic property in Lemma B.6 in [33], we can guarantee that $\\left(\\overline{{u}}_{j}^{t}\\right)^{2}$ would keep staying in $(0,w_{j}^{\\star}\\!+\\!4B_{k}]$ . With condition $w_{j}^{\\star}\\leq20B_{k}$ , we can conclude that $\\begin{array}{r}{\\left\\|\\left(\\overline{{u}}_{j}^{t+1}\\right)^{2}-w_{j}^{\\star}\\right\\|_{\\infty}\\leq\\operatorname*{max}\\{w_{j}^{\\star},4B_{k}\\}\\leq20B_{k}\\leq\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k}}.}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Combined these two cases would finish proof of (a). ", "page_idx": 20}, {"type": "text", "text": "(b) To prove this statement, we should guarantee that there are sufficient iterative steps in the $(k\\!-\\!1)$ -th induction that can make $\\left\\|\\left(\\overline{{\\boldsymbol{u}}}_{S}^{\\overline{{T}}_{k-1}}\\right)^{2}-\\boldsymbol{w}^{\\star}\\right\\|_{\\infty}$ decrease at least by half from the beginning iteration of current induction stage to that of next induction stage. The proof is also divided into two cases. (1) The one case is that $\\forall j$ that it already has $\\begin{array}{r}{\\Big|\\left(\\overline{{\\boldsymbol u}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}-\\boldsymbol w_{j}^{\\star}\\Big|<\\frac{\\boldsymbol w_{\\operatorname*{max}}^{\\star}}{2^{k+1}}}\\end{array}$ <x, then with similar proof in (a), we can guarantee that $\\forall t\\geq\\overline{{T}}_{k-1}$ \uff0c $\\begin{array}{r}{\\left|\\left(\\overline{{u}}_{j}^{t}\\right)^{2}-w_{j}^{\\star}\\right|<\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k+1}}}\\end{array}$ . Thus, for these supports, we prove the $(k+1)$ -th induction also holds. ", "page_idx": 20}, {"type": "text", "text": "(2) The second case is $\\forall j$ that tere is $\\begin{array}{r}{\\Big|\\left(\\overline{{\\boldsymbol u}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}-\\boldsymbol w_{j}^{\\star}\\Big|\\geq\\frac{\\boldsymbol w_{\\operatorname*{max}}^{\\star}}{2^{k+1}}=20B_{k}}\\end{array}$ Based o the upper $\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}\\leq w_{j}^{\\star}+4B_{k}$ $\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}},w_{j}^{\\star}$ $0\\le\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}\\le$ $w_{j}^{\\star}-20B_{k},w_{j}^{\\star}\\geq20B_{k}$ respectively. This means UT- is frawayfrom uy at least 2oBkdistance. According to Lemma B.12 in [33], to achieve $\\left|\\left(\\overline{{\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}}}\\right)^{2}-w_{j}^{\\star}\\right|\\leq20B_{k}$ , the sufficient condition for the number of iteraions in crret induction tage is \u2265 2ires $\\begin{array}{r}{t\\geq\\frac{15}{32\\eta w_{j}^{\\star}}\\log\\frac{(w_{j}^{\\star})^{2}}{19\\Big(\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}\\Big)^{2}B_{k}}}\\end{array}$ . Now we verify the setting of $T_{k}$ as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{k}=\\frac{2^{k}}{\\eta w_{\\mathrm{max}}^{\\star}}\\log\\frac{1}{\\alpha^{4}}}\\\\ &{\\phantom{\\frac{1}{\\eta}}\\geq\\frac{1}{40\\eta B_{k}}\\log\\left(\\frac{3\\left(w_{\\mathrm{max}}^{\\star}\\right)^{2}}{\\zeta}\\cdot\\frac{1}{\\alpha^{3}}\\right)}\\\\ &{\\phantom{\\frac{1}{\\eta}}\\geq\\frac{1}{2\\eta w_{j}^{\\star}}\\log\\left(\\frac{3\\left(w_{j}^{\\star}\\right)^{2}}{\\zeta}\\cdot\\frac{1}{\\left(\\frac{\\overline{{T}}_{k-1}}{u_{j}^{\\star}}\\right)^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\geq\\frac{1}{2\\eta w_{j}^{\\star}}\\log\\frac{(w_{j}^{\\star})^{2}}{16\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}B_{k}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality is due to the definition of $B_{k}$ and small initialization condition that $\\begin{array}{r}{\\alpha\\ \\leq\\ \\frac{\\zeta}{3(w_{\\mathrm{max}}^{\\star})^{2}}}\\end{array}$ and the second inequality is due to $w_{j}^{\\star}\\,\\geq\\,20B_{k}$ and lower bound in induction (e) that $\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}\\ \\geq\\ \\alpha^{3}$ .The last inequality is because $\\begin{array}{r l r}{\\frac{\\zeta}{3}\\!}&{{}\\le}&{\\!\\frac{16}{40}\\frac{w_{\\mathrm{max}}^{\\star}}{2^{K-1}}\\;\\le\\;\\frac{16}{40}\\frac{w_{\\mathrm{max}}^{\\star}}{2^{k}}\\;=\\;16B_{k}}\\end{array}$ Thus, combining the above two cases and similar proof, we can conclude that $\\forall t\\geq\\overline{{T}}_{k}$ , there is $\\begin{array}{r}{\\left\\|\\left(\\overline{{\\pmb{u}}}_{S}^{t}\\right)^{2}-\\pmb{w}^{\\star}\\right\\|_{\\infty}^{-}\\leq\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k+1}}}\\end{array}$ . This completes the proof of induction (b). ", "page_idx": 21}, {"type": "text", "text": "(c) To make the consensus error satisfy the above induction, $\\forall t$ that $\\overline{{T}}_{k-1}\\leq t<t+1<\\overline{{T}}_{k}$ based on one step iteration in (24) of Lemma 6, induction (a), (c) and step size condition $\\begin{array}{r}{\\eta\\le\\frac{1-\\sqrt{\\rho}}{64\\rho^{\\frac{1}{2}}w_{\\mathrm{max}}^{\\star}}}\\end{array}$ has ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\rho}\\Bigg(1+4\\eta\\Bigg(\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+1\\right)\\left\\|\\overline{{\\boldsymbol u}}_{S}^{t}\\odot\\overline{{\\boldsymbol u}}_{S}^{t}-\\boldsymbol w^{\\star}\\right\\|_{\\infty}+2d\\left(\\overline{{\\boldsymbol u}}_{S^{c}}^{t}+\\Delta_{S^{c}}^{t}\\right)^{2}+2\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+1\\right)\\left(\\frac{\\eta_{S}^{t}}{t}\\right)^{2}}\\\\ &{\\qquad\\cdot\\left(\\left\\|\\overline{{\\boldsymbol u}}_{S}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{S}^{t}\\right\\|_{\\infty}\\right)^{2}+\\operatorname*{max}_{i}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}\\Bigg)\\Bigg)\\leq1+\\frac{1-\\sqrt{\\rho}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then the recursion (24) in Lemma 6 becomes ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta_{S}^{t+1}\\right\\|_{\\infty}\\leq4\\left(\\displaystyle\\frac{1+\\sqrt{\\rho}}{2}\\right)\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{\\mathbf{u}}}_{S}^{t}\\right\\|_{\\infty}B_{k}+4\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{\\mathbf{u}}}_{S}^{t}\\right\\|_{\\infty}B_{k}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{4}{\\left(1-c_{10}\\left(1-\\sqrt{\\rho}\\right)\\right)^{2}}\\left(\\left(\\displaystyle\\frac{1+\\sqrt{\\rho}}{2}\\right)\\beta+1\\right)\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{\\mathbf{u}}}_{S}^{t+1}\\right\\|_{\\infty}B_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality is based on induction such as if $\\left\\|\\Delta_{S}^{t}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{\\mathbf{u}}}_{S}^{t}\\right\\|_{\\infty}B_{k}$ , then based (30) and step size condition $\\begin{array}{r}{\\eta\\leq\\frac{c_{10}\\left(1-\\sqrt{\\rho}\\right)}{w_{\\mathrm{max}}^{\\star}}}\\end{array}$ , there is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\overline{{u}}_{S}^{t+1}\\right\\|_{\\infty}\\geq\\left\\|\\overline{{u}}_{S}^{t}\\right\\|_{\\infty}\\left(1-c_{10}\\left(1-\\sqrt{\\rho}\\right)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To guarantee that (34) holds induction (c), the sufficient condition is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)\\beta+1}{\\left(1-c_{10}\\left(1-\\sqrt{\\rho}\\right)\\right)^{2}}\\leq\\beta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The $c_{10}$ should be chosen that $\\begin{array}{r}{\\frac{1+\\sqrt{\\rho}}{2\\left(1-c_{10}\\left(1-\\sqrt{\\rho}\\right)\\right)^{2}}\\,<\\,1}\\end{array}$ . which means that $c_{10}$ should saisfy $c_{10}~\\leq$ $\\scriptstyle{\\frac{1-{\\sqrt{\\frac{1+{\\sqrt{\\rho}}}{2}}}}{1-{\\sqrt{\\rho}}}}$ Then we can set $\\begin{array}{r}{c_{10}=\\frac{1-\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)^{\\frac{1}{4}}}{1-\\sqrt{\\rho}}}\\end{array}$ $\\begin{array}{r}{\\frac{1}{\\left(1-c_{10}\\left(1-\\sqrt{\\rho}\\right)\\right)^{2}}=\\sqrt{\\frac{2}{1+\\sqrt{\\rho}}}}\\end{array}$ 1+v\u221a. Based on (36), the lower bound for $\\beta$ is $\\begin{array}{r}{\\beta\\ge\\frac{2\\left(\\sqrt{2\\left(1+\\sqrt{\\rho}\\right)}+1+\\sqrt{\\rho}\\right)}{1-\\rho}.}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Unfolding the recursion in (24) from the beginning of $(k-1)$ -th induction to the beginning of $k$ -th induction based on (34) and combining induction (a) would have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta_{s}^{\\overline{{T}}_{k}}\\right\\|_{\\infty}\\leq\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)^{2^{k}T}\\left\\|\\Delta_{s}^{\\overline{{T}}_{k-1}}\\right\\|_{\\infty}+4\\rho^{\\frac{3}{4}}\\eta B_{k}\\displaystyle\\sum_{i=0}^{2^{k}T-1}\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)^{2^{k}T-1-i}\\left\\|\\overline{{\\boldsymbol u}}_{s}^{\\overline{{T}}_{k-1+i}}\\right\\|_{\\infty}}\\\\ &{\\qquad\\leq\\frac{4\\sqrt{2}\\rho^{\\frac{3}{4}}\\eta B_{k}}{\\sqrt{1+\\sqrt{\\rho}}}\\displaystyle\\sum_{i=0}^{2^{k}T-1}\\left(\\frac{1+\\sqrt{\\rho}}{2}\\cdot\\sqrt{\\frac{2}{1+\\sqrt{\\rho}}}\\right)^{2^{k}T-1-i}\\left\\|\\overline{{\\boldsymbol u}}_{s}^{\\overline{{T}}_{k}}\\right\\|_{\\infty}}\\\\ &{\\qquad+4\\beta\\rho^{\\frac{3}{4}}\\eta\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)^{2^{k}T}\\left\\|\\overline{{\\boldsymbol u}}_{s}^{\\overline{{T}}_{k-1}}\\right\\|_{\\infty}B_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\frac{4\\sqrt{2}\\rho^{\\frac{3}{4}}\\eta B_{k}}{\\sqrt{1+\\sqrt{\\rho}}}\\left(\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}\\right)^{2^{k_{T}}-1-i}\\left\\|\\overline{{\\boldsymbol u}}_{\\mathcal{S}}^{\\overline{{T}}_{k}}\\right\\|_{\\infty}}\\\\ &{\\quad+\\,4\\beta\\rho^{\\frac{3}{4}}\\eta\\left(\\frac{1+\\sqrt{\\rho}}{2}\\cdot\\sqrt{\\frac{2}{1+\\sqrt{\\rho}}}\\right)^{2^{k_{T}}}\\left\\|\\overline{{\\boldsymbol u}}_{\\mathcal{S}}^{\\overline{{T}}_{k}}\\right\\|_{\\infty}B_{k}}\\\\ &{\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta\\left(\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}\\right)^{2^{k_{T}}}\\left\\|\\overline{{\\boldsymbol u}}_{\\mathcal{S}}^{\\overline{{T}}_{k}}\\right\\|_{\\infty}B_{k}+\\frac{2\\sqrt{2}}{\\left(1-\\sqrt{\\rho}\\right)\\sqrt{1+\\sqrt{\\rho}}}\\cdot4\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{\\boldsymbol u}}_{\\mathcal{S}}^{\\overline{{T}}_{k}}\\right\\|_{\\infty}B_{k}}\\\\ &{\\leq4\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{\\boldsymbol u}}_{\\mathcal{S}}^{\\overline{{T}}_{k}}\\right\\|_{\\infty}\\left(\\beta\\left(\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}\\right)^{2^{k_{T}}}+\\frac{8}{1-\\sqrt{\\rho}}\\right)B_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where both the second and third inequalities use lower bound in (35). To guarantee that the last inequality satisfies the induction (c), the $\\beta$ and $\\rho$ should satisfy the following condition ", "page_idx": 22}, {"type": "equation", "text": "$$\nB_{k}\\left(\\beta\\left(\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}\\right)^{2^{k}T}+\\frac{8}{1-\\sqrt{\\rho}}\\right)\\leq B_{k+1}\\beta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "One sficien conionfo chievin above inquaityis $\\begin{array}{r}{\\frac{8}{1-\\sqrt{\\rho}}\\leq\\frac{\\beta}{4}}\\end{array}$ Combine above lower bound of \u03b2, we can verify that \u03b2 = 1/ satisfies this condition and following inequality is attained ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}\\right)^{\\frac{1}{\\eta w_{\\mathrm{max}}^{\\star}}\\log\\frac{1}{\\alpha^{4}}}\\leq\\frac{1}{4},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies that the step size $\\eta$ should satisfy ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{\\log\\frac{1}{\\alpha^{4}}\\ln\\left(1+\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}-1\\right)}{-2\\ln2w_{\\operatorname*{max}}^{\\star}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "One suficient condition for achieving inequality is $\\begin{array}{r}{\\eta\\le\\frac{\\log\\frac{1}{\\alpha^{4}}\\left(1-\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}\\right)}{4w_{\\operatorname*{max}}^{\\star}}}\\end{array}$ based on inequality that $\\begin{array}{r}{\\ln\\left(1+\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}-1\\right)\\,\\le\\,\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}-1}\\end{array}$ . Combining all the above conditions on $\\eta$ we obtain the upper bound as shown in condition (5). ", "page_idx": 22}, {"type": "text", "text": "(d) $\\forall t$ that $\\overline{{T}}_{k-1}\\leq t<t+1<\\overline{{T}}_{k}$ , based on one step iteration in (27) of Lemma 6 and inductions (a), (c), there is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta_{S^{c}}^{t+1}\\right\\|_{\\infty}\\leq4\\left(\\displaystyle\\frac{1+\\sqrt{\\rho}}{2}\\right)\\gamma\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}B_{k}+4\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\left(B_{k}+4\\sqrt{s}\\delta\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S}^{t}\\right\\|_{\\infty}B_{k}\\left(4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S}^{t}\\right\\|_{\\infty}B_{k}+\\left\\|\\overline{{u}}_{S}^{t}\\right\\|_{\\infty}\\right)\\right)}\\\\ &{\\qquad\\qquad\\leq4\\left(\\displaystyle\\frac{1+\\sqrt{\\rho}}{2}\\right)\\gamma\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}B_{k}+4\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}\\left(B_{k}+4\\sqrt{s}\\delta\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}\\left\\|\\overline{{u}}_{S}^{t}\\right\\|_{\\infty}^{2}\\right)}\\\\ &{\\qquad\\leq4\\left(\\displaystyle\\frac{1+\\sqrt{\\rho}}{2}\\right)\\gamma\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}B_{j}+4\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}B_{k}}\\\\ &{\\qquad\\qquad\\leq\\frac{4}{\\left(1-c_{10}\\left(1-\\sqrt{\\rho}\\right)\\right)^{2}}\\left(\\left(\\displaystyle\\frac{1+\\sqrt{\\rho}}{2}\\right)\\gamma+1\\right)\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{c}}^{t+1}\\right\\|_{\\infty}B_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where tesndiqultyisde4 25 B \u2264 1 and third inequality is due to global RIP condition on $\\delta$ that is order of $\\textstyle{\\frac{1}{\\sqrt{s}}}$ and $\\beta\\eta\\left\\|\\overline{{\\boldsymbol{u}}}_{S}^{t}\\right\\|_{\\infty}^{2}\\leq1$ , the last inequality is ", "page_idx": 22}, {"type": "text", "text": "based on induction such that if $\\left\\|\\Delta_{S^{c}}^{t}\\right\\|_{\\infty}\\leq4\\gamma\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{\\mathbf{u}}}_{S^{c}}^{t}\\right\\|_{\\infty}B_{k}$ , then based (45),if $\\begin{array}{r}{\\eta\\le\\frac{c_{10}(1-\\sqrt{\\rho})}{w_{\\mathrm{max}}^{\\star}}}\\end{array}$ wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\overline{{u}}_{S^{c}}^{t+1}\\right\\|_{\\infty}\\geq\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}\\left(1-c_{10}\\left(1-\\sqrt{\\rho}\\right)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With the same derivation in proving induction (c), the lower bound for $\\gamma$ is the same as that of $\\beta$ Then also unrolling the recursion in (27) from beginning of $(k-1)$ -th induction to beginning of $k$ -th induction and combining induction (a) would have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\Delta_{S^{c}}^{\\overline{{T}}_{k}}\\right\\|_{\\infty}\\leq\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)^{2^{k_{T}}}\\left\\|\\Delta_{S^{c}}^{\\overline{{T}}_{k-1}}\\right\\|_{\\infty}+4\\rho^{\\frac{3}{4}}\\eta B_{k}\\sum_{i=0}^{2^{k_{T}-1}}\\left(\\frac{1+\\sqrt{\\rho}}{2}\\right)^{2^{k_{T}-1-i}}\\left\\|\\overline{{u}}_{S^{c}}^{\\overline{{T}}_{k-1}+i}\\right\\|_{\\infty}}}\\\\ &{}&{\\leq4\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{c}}^{\\overline{{T}}_{k}}\\right\\|_{\\infty}B_{k}\\left(\\gamma\\left(\\sqrt{\\frac{1+\\sqrt{\\rho}}{2}}\\right)^{2^{k_{T}}}+\\frac{8}{1-\\sqrt{\\rho}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The derivation of above inequality is similar with (37). To guarantee the above two inequality satisfy induction d),the and p can have the same alue as in (38) that  =\u03b2 = /p ", "page_idx": 23}, {"type": "text", "text": "(e) For the upper bound, the proof is divided into three cases. ", "page_idx": 23}, {"type": "text", "text": "(1) One of case is $\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}\\leq w_{j}^{\\star}+B_{k}$ , based on the Lemma B.6 in [33], we can conclude that $\\forall t\\geq\\overline{{T}}_{k-1},\\,\\left(\\overline{{u}}_{j}^{t}\\right)^{2}$ would keep below $w_{j}^{\\star}+B_{k}$ ", "page_idx": 23}, {"type": "text", "text": "(2) Another case is $w_{j}^{\\star}+B_{k}\\,\\le\\,\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}\\,\\le\\,w_{j}^{\\star}+2B_{k}$ , in this case $\\forall t\\geq\\overline{{T}}_{k-1}$ , Either $0\\,\\leq$ $\\left(\\overline{{u}}_{j}^{t}\\right)^{2}\\leq w_{j}^{\\star}+B_{k}$ , which can use result in case (1) that $\\forall t^{\\prime}\\geq t$ $\\mathrm{,~0~}\\leq\\mathrm{~}\\left(\\overline{{\\boldsymbol{u}}}_{j}^{t^{\\prime}}\\right)^{2}\\leq\\boldsymbol{w}_{j}^{\\star}+\\boldsymbol{B}_{k}$ or that $w_{j}^{\\star}+B_{k}\\le\\left(\\overline{{u}}_{j}^{t}\\right)^{2}\\le w_{j}^{\\star}+2B_{k}$ which would guarantee that $\\left(\\overline{{u}}_{j}^{t+1}\\right)^{2}\\leq\\left(\\overline{{u}}_{j}^{t}\\right)^{2}$ ", "page_idx": 23}, {"type": "text", "text": "(3) The last case is $w_{j}^{\\star}+2B_{k}\\,<\\,\\left(\\overline{{u}}_{j}^{t}\\right)^{2}\\,\\leq w_{j}^{\\star}+4B_{k}$ , then based on Lemma B.14 [33], after the suficient number of iterations that t \u2265 1oB $\\begin{array}{r}{\\forall t\\geq\\frac{1}{10\\eta B_{k}}=\\frac{4\\times2^{k}}{\\eta w_{\\mathrm{max}}^{\\star}}}\\end{array}$ and we can check that $T_{k}$ is large enough that satisfies this condition, it can keep that $\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{k-1}+t}\\right)^{2}\\leq w_{j}^{\\star}+2B_{k}.$ ", "page_idx": 23}, {"type": "text", "text": "Following the above three cases, we can guarantee that there exists $\\begin{array}{r}{\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{k}}\\right)^{2}\\leq w_{j}^{\\star}+\\frac{1}{2}\\times4B_{k}=}\\end{array}$ $w_{j}^{\\star}+4B_{k+1}$ ", "page_idx": 23}, {"type": "text", "text": "For the lower bound, we can also have bound $\\begin{array}{r}{\\|\\pmb{g}^{t}\\|_{\\infty}\\leq\\frac{3\\sqrt{\\rho}B_{k}}{8}}\\end{array}$ 3Bk by comparing formulas between pt and $\\pmb{g}^{t},\\forall t$ that $\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}$ . For perturbation $\\pmb{f}^{t}$ , based on induction (a), fine-grained bound in (d), $\\forall j\\in S^{c}$ it has ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{|f_{j}^{t}|\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta|\\overline{{u}}_{j}^{t}|B_{k}\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}\\left\\|\\overline{{u}}_{S}^{t}\\odot\\overline{{u}}_{S}^{t}-{\\pmb w}^{\\star}\\right\\|_{\\infty}+\\frac{3\\sqrt{\\rho}B_{k}}{16}+\\operatorname*{max}_{i}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}\\right)}\\\\ {\\quad\\leq\\frac{\\sqrt{\\rho}B_{k}|\\overline{{u}}_{j}^{t}|}{16},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality is due to bound for $\\|\\pmb{g}^{t}\\|_{\\infty}$ and the last inequality is the same reason as (29). $\\pmb{f}^{t}$ can also be reparameterizedas ${\\pmb f}^{t}={\\pmb r}_{f}^{t}\\odot\\overline{{\\pmb u}}_{S^{c}}^{t}$ where $\\left\\|r_{f}^{t}\\right\\|_{\\infty}\\leq\\frac{\\sqrt{\\rho}B_{k}}{16}$ for $\\forall t$ that $\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}$ the perturbed optimization recursion on non-support $S^{c}$ over decentralized network in (11) becomes ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(\\overline{{\\pmb u}}_{S^{c}}^{t+1}\\right)^{2}=\\left(\\overline{{\\pmb u}}_{S^{c}}^{t}\\right)^{2}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(\\pmb E_{2}^{t}+\\pmb E_{3}^{t}+\\pmb g^{t}+\\pmb r_{f}^{t}\\right)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Based on the similar lower bound (79) and upper bound in (80), we can conclude that $\\forall t\\,\\leq$ $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{1}{\\eta\\zeta}\\log\\frac{1}{\\alpha^{4}}\\right)}\\end{array}$ ,there is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\prod_{i=0}^{t}\\left(1+8\\eta\\left\\|E_{2}^{t}\\right\\|_{\\infty}+\\left\\|E_{3}^{t}\\right\\|_{\\infty}+\\left\\|g^{t}\\right\\|_{\\infty}+\\left\\|r_{f}^{t}\\right\\|_{\\infty}\\right)^{2}\\leq\\frac{1}{\\alpha},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where this inequality guarantees that the averaged signal on non-support remains $\\left\\|\\overline{{\\boldsymbol{u}}}_{S^{c}}^{t}\\right\\|_{\\infty}\\leq\\sqrt{\\alpha}$ until early stopping. $\\forall j\\in S$ , we use $j_{k}$ to denote the largest index that $w_{j}^{\\star}\\,\\le\\,B_{j_{k}}\\,+\\,\\alpha^{3}$ .As $B_{0}=w_{\\mathrm{max}}^{\\star}$ , the existence of $j_{k}$ is guaranteed. Then for $t=0,\\cdot\\cdot\\cdot,\\overline{{T}}_{j_{k}}-1$ and based on (30), the $\\left(\\overline{{u}}_{j}^{t}\\right)^{2}$ would shrinkage from initialization $\\alpha^{2}$ , to obtain the lower bound, we should consider the maximum shrinkage as follows ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\overline{{\\overline{{u}}_{j}^{T}{_{j_{k}}}}}\\right)^{2}\\geq\\alpha^{2}\\prod_{i=0}^{\\overline{{T}}_{j_{k}}-1}\\left(1-4\\eta\\left(\\left(\\overline{{u}}_{j}^{i}\\right)^{2}+\\left\\Vert E_{2}^{i}\\right\\Vert_{\\infty}+\\left\\Vert E_{3}^{i}\\right\\Vert_{\\infty}+\\left\\Vert\\pmb{g}^{i}\\right\\Vert_{\\infty}+\\left\\Vert r_{f}^{i}\\right\\Vert_{\\infty}\\right)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality is due to (46), $\\left(\\overline{{u}}_{j}^{i}\\right)^{2}\\leq\\alpha^{2},\\forall\\,i=1,\\cdots\\,,\\overline{{T}}_{j_{k}}-1$ and step size condition that $\\begin{array}{r}{\\eta\\left(\\left(\\overline{{u}}_{j}^{i}\\right)^{2}+\\left\\|{E}_{2}^{i}\\right\\|_{\\infty}+\\left\\|{E}_{3}^{i}\\right\\|_{\\infty}+\\left\\|{g}^{i}\\right\\|_{\\infty}+\\left\\|{r}_{f}^{i}\\right\\|_{\\infty}\\right)\\leq\\frac{3}{40}}\\end{array}$ and $\\begin{array}{r}{(1-5x)(1+8x)\\ge1,\\forall x\\in[0,\\frac{3}{40}]}\\end{array}$ Thus, we have $\\left(\\overline{{u}}_{j}^{t}\\right)^{2}\\geq\\alpha^{3}$ for $\\forall t=0,\\cdots\\,,\\overline{{T}}_{j_{k}}$ ", "page_idx": 24}, {"type": "text", "text": "For $t~=~\\overline{{T}}_{j_{k}}~+~1,\\cdot\\cdot\\cdot~,\\overline{{T}}_{j_{k}+1}~-~1$ and $\\forall\\,j\\ \\in\\ s$ , let consider the auxiliary iterations that $\\begin{array}{r c l}{\\left(\\hat{u}_{j}^{t}\\right)^{2}}&{=}&{\\left(\\hat{u}_{j}^{t-1}\\right)^{2}\\;\\odot\\;\\left(1-4\\eta\\left(\\left(\\hat{u}_{j}^{t-1}\\right)^{2}-\\left(w_{j}^{\\star}-B_{j_{k}+1}\\right)\\right)\\right)^{2}}\\end{array}$ where $\\hat{u}_{j}^{\\overline{{T}}_{j_{k}}}\\ \\ =\\ \\ \\overline{{u}}_{j}^{\\overline{{T}}_{j_{k}}}\\ \\ \\ge\\ \\ \\alpha^{3}$ as above proved. Based on definition of $j_{k}$ , there is' $w_{j}^{\\star}\\ -\\ B_{j_{k}+1}\\ \\ \\geq\\ \\ \\alpha^{3}$ According to monotonic property in Lemma B.6 in [33], we can guarantee that $\\left(\\hat{u}_{j}^{\\overline{{T}}_{j_{k}+1}}\\right)^{2}\\;\\;\\in$ min T,w-Bi+1} ,max {\uff0c-Bi+1} that ensures that $\\left(\\overline{{u}}_{j}^{\\overline{{T}}_{j_{k}+1}}\\right)^{2}\\quad\\ge$ $\\left(\\hat{u}_{j}^{\\overline{{T}}_{j_{k}+1}}\\right)^{2}\\geq\\alpha^{3}$ where the first inequality is due to the squeezing property in Lemma B.9 in [33]. Then we can follow the same analysis to prove that left $i$ -th induction step that $i=j_{k}+2,\\cdot\\cdot\\cdot,m-1$ based on the monotonic property of $B_{k}$ ", "page_idx": 24}, {"type": "text", "text": "A.4 General Case ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This section considers the general setting where the ground truth $\\pmb{w}^{\\star}$ includes both positive and negative elements in its support. The analysis here is more complex than in the previous section due to the presence of $\\pmb{v}^{t,i}$ and its consensus error terms. The following formulates the recursion of perturbed average variable $\\overline{{\\pmb{u}}}^{t}$ and similar derivation could be applied to ${\\overline{{\\pmb{v}}}}^{t}$ ", "page_idx": 24}, {"type": "text", "text": "Lemma 7. Consider the sequence $\\{\\boldsymbol{u}_{i}^{t},\\boldsymbol{v}_{i}^{t}\\},i\\in[m]$ generated according to (3) and (4) by DGD for solving problem (2), the average signal $\\{\\overline{{\\pmb{u}}}^{t}\\}$ on positive support $S^{+}$ , negative support $S^{-}$ and non-support $S^{c}$ are updated according to the following formulas ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\iota}_{s+}^{t+1}=\\overline{{\\boldsymbol{u}}}_{s+}^{t}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(\\left(\\overline{{\\boldsymbol{u}}}_{s+}^{t}\\right)^{2}-\\boldsymbol{w}_{S+}^{\\star}\\right)-4\\eta\\frac{X^{T}X}{N}\\left(\\left(\\overline{{\\boldsymbol{u}}}_{S^{c}}^{t}\\right)^{2}-\\left(\\overline{{\\boldsymbol{v}}}_{S^{c}}^{t}\\right)^{2}+\\left(\\overline{{\\boldsymbol{u}}}_{S-}^{t}\\right)^{2}-\\left(\\overline{{\\boldsymbol{v}}}_{S+}^{t}\\right)^{2}\\right)\\right.}\\\\ {\\left.+4\\eta\\frac{X^{T}\\xi}{N}-4\\eta\\left(\\frac{X^{T}X}{N}-I\\right)\\left(\\left(\\overline{{\\boldsymbol{u}}}_{S^{c}}^{t}\\right)^{2}-\\boldsymbol{w}_{S+}^{\\star}-\\left(\\overline{{\\boldsymbol{v}}}_{S^{c}}^{t}\\right)^{2}-\\boldsymbol{w}_{S-}^{\\star}\\right)-4\\eta p_{u}^{t}\\right)-4\\eta q_{u}^{t};}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\overline{{\\iota}}_{S-}^{t+1}=\\overline{{u}}_{S-}^{t}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(-\\left(\\overline{{v}}_{S-}^{t}\\right)^{2}-w_{S-}^{\\star}+\\frac{X^{T}X}{N}\\left(\\left(\\overline{{u}}_{S^{c}}^{t}\\right)^{2}-\\left(\\overline{{v}}_{S c}^{t}\\right)^{2}+\\left(\\overline{{u}}_{S-}^{t}\\right)^{2}-\\left(\\overline{{v}}_{S+}^{t}\\right)^{2}\\right)\\right)\\right)\\mathrm{~t~o~r~}\\ .\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n-\\frac{X^{T}\\xi}{N}+\\left(\\frac{X^{T}X}{N}-I\\right)\\left(\\left(\\overline{{u}}_{S^{+}}^{t}\\right)^{2}-w_{S^{+}}^{\\star}-\\left(\\overline{{v}}_{S^{-}}^{t}\\right)^{2}-w_{S^{-}}^{\\star}\\right)\\right)-4\\eta y_{u}^{t}\\right)-4\\eta z_{u}^{t};\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{{u}}}_{S c}^{t+1}=\\overline{{{u}}}_{S c}^{t}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(\\frac{{\\mathbf{X}}^{T}X}{N}\\left(\\left(\\overline{{{u}}}_{S c}^{t}\\right)^{2}-\\left(\\overline{{{v}}}_{S c}^{t}\\right)^{2}+\\left(\\overline{{{u}}}_{S-}^{t}\\right)^{2}-\\left(\\overline{{{v}}}_{S+}^{t}\\right)^{2}\\right)-\\frac{X^{T}\\xi}{N}\\right.\\right.}\\\\ {+\\left.\\left.\\left(\\frac{X^{T}X}{N}-I\\right)\\left(\\left(\\overline{{{u}}}_{S+}^{t}\\right)^{2}-{w}_{S+}^{\\star}-\\left(\\overline{{{v}}}_{S-}^{t}\\right)^{2}-{w}_{S-}^{\\star}\\right)\\right)-4\\eta g_{u}^{t}\\right)-4\\eta f_{u}^{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the perturbed error terms $p_{u}^{t},q_{u}^{t},y_{u}^{t},z_{u}^{t},g_{u}^{t},f_{u}^{t}$ induced from decentralized network are definedas ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta_{\\varepsilon}=\\frac{1}{m}\\frac{1}{m!}\\left(\\frac{X^{2}X}{m!}-\\int\\Biggl(2\\pi_{\\varepsilon}^{2}\\circ\\delta\\Delta_{x,x}^{2}+\\left(\\Delta_{x,x}^{2}\\right)^{2}-2\\pi_{\\varepsilon}^{2}\\circ\\delta\\Delta_{x,x}^{2}-\\left(\\Delta_{x,x}^{2}\\right)^{2}\\right)\\right.}\\\\ &{\\quad+\\left.3\\left(\\Delta_{x,x}^{2}\\right)^{2}+\\frac{X^{2}X^{2}}{m!}\\left(2\\pi_{\\varepsilon}^{2}\\circ\\delta\\Delta_{x,x}^{2}+\\left(\\Delta_{x,x}^{2}\\right)^{2}+2\\pi_{\\varepsilon}^{2}\\circ\\delta\\Delta_{x,x}^{2}+\\left(\\Delta_{x,x}^{2}\\right)^{2}\\right)}\\\\ &{\\quad-2\\pi_{\\varepsilon}^{2}\\circ\\delta\\Delta_{x,x}^{2}-\\left(\\Delta_{x,x}^{2}\\right)^{2}-2\\pi_{\\varepsilon}^{2}\\circ\\delta\\Delta_{x,x}^{2}-\\left(\\Delta_{x,x}^{2}\\right)^{2}\\right);}\\\\ &{\\zeta_{\\varepsilon}=\\frac{1}{m}\\frac{1}{m}\\sum_{k=0}^{\\infty}\\mathrm{e}\\Biggl(\\Biggl(\\frac{X^{2}X}{m!}-\\int\\Biggl(\\alpha_{\\varepsilon}^{2}\\circ\\delta\\alpha_{x,x}^{2}-(\\alpha_{\\varepsilon,x}^{2})^{2}-w_{\\varepsilon,x}^{2}}\\\\ &{\\quad+\\frac{2\\pi_{\\varepsilon}^{2}}{m!})\\cos(\\alpha_{\\varepsilon,x}^{2}+(\\alpha_{\\varepsilon,x}^{2})^{2}-2\\pi_{\\varepsilon}^{2}\\circ\\delta\\alpha_{x,x}^{2}-(\\alpha_{\\varepsilon,x}^{2})^{2})}\\\\ &{\\quad+\\frac{X^{2}X^{2}}{m!}\\left(\\left(\\alpha_{\\varepsilon}^{2}+\\alpha_{x,x}^{2}\\right)^{2}-\\left(\\alpha_{\\varepsilon,x}^{2}+\\alpha_{x,x}^{2}\\right)^{2}+\\left(\\alpha_{x,x}^{2}+\\alpha_{x,x}^{2}\\right)^{2}\\right)}\\\\ &{\\quad+\\frac{X^{2}X^{2}}{m!}\\left(\\left(\\alpha_{\\varepsilon,x}^{2}+\\alpha_{x,x}^{2}\\right\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\eta_{u}^{t}:=g_{u}^{t}-\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\Delta_{v,v}^{t,i}\\right)^{2};}\\\\ {\\displaystyle\\xi_{u}^{t}:=\\frac{1}{m}\\sum_{i=1}^{m}\\Delta_{u,v}^{t,i}-\\frac{\\langle0\\rangle_{v}^{t}}{\\Delta_{v}^{t}S}\\ G_{v}^{-i}-\\left(\\Delta_{v,v-}^{t,i}\\right)^{2}+\\left(\\frac{X_{i}^{T}X_{i}}{n}-I\\right)\\left(\\left(\\overline{{u}}_{s+}^{t}\\right)^{2}-w_{s+}^{*}-\\left(\\overline{{u}}_{s+}^{t}\\right)^{2}\\right)}\\\\ {\\displaystyle-w_{s}^{*}-2\\overline{{u}}_{s+}^{t}\\odot\\Delta_{u,v+}^{t,i}+\\left(\\Delta_{u,v+}^{t,i}\\right)^{2}-2\\overline{{v}}_{s-}^{t}\\odot\\Delta_{v,v-}^{t,i}-\\left(\\Delta_{v,v-}^{t,i}\\right)^{2}\\right)}\\\\ {\\displaystyle+\\frac{X_{i}^{T}X_{i}}{n}\\left(\\left(\\overline{{u}}_{s-}^{t}+\\Delta_{u,v+}^{t,i}\\right)^{2}-\\left(\\overline{{v}}_{s-}^{t}+\\Delta_{v,v-}^{t,i}\\right)^{2}+\\left(\\overline{{u}}_{s-}^{t}+\\Delta_{u,v-}^{t,i}\\right)^{2}-\\left(\\overline{{v}}_{s+}^{t}+\\Delta_{v,v+}^{t,i}\\right)^{2}\\right)}\\\\ {\\displaystyle+\\left(\\Delta_{u,v+}^{t,i}\\right)^{2}-\\frac{X_{i}^{T}\\xi_{i}}{n}\\Big);}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The average of optimization for $\\pmb{u}^{t,i},i\\in[m]$ under DGD are as follows ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{u}}^{t+1}=\\overline{{u}}^{t}-\\displaystyle\\frac{\\eta}{m}\\sum_{i=1}^{m}\\nabla_{u}f_{i}\\left(u^{t,i},v^{t,i}\\right)}\\\\ &{\\qquad=\\overline{{u}}^{t}-\\displaystyle\\frac{\\eta}{m}\\sum_{i=1}^{m}\\nabla_{u}f_{i}\\left(\\overline{{u}}^{t},\\overline{{v}}^{t}\\right)+\\displaystyle\\frac{\\eta}{m}\\sum_{i=1}^{m}\\left(\\underbrace{\\nabla_{u}f_{i}\\left(\\overline{{u}}^{t},\\overline{{v}}^{t}\\right)-\\nabla_{u}f_{i}\\left(\\overline{{u}}^{t},v^{t,i}\\right)}_{\\Pi_{1,i}}\\right)}\\\\ &{\\qquad+\\displaystyle\\frac{\\eta}{m}\\sum_{i=1}^{m}\\left(\\nabla_{u}f_{i}\\left(\\overline{{u}}^{t},v^{t,i}\\right)-\\nabla_{u}f_{i}\\left(u^{t,i},v^{t,i}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Based on error decomposition on positive support $S^{+}$ , negative support $S^{-}$ and non-support $S^{c}$ there has ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\gamma_{u}F\\left(\\overline{{u}}^{t},\\overline{{v}}^{t}\\right)=4\\overline{{u}}^{t}\\odot\\left(\\left(\\overline{{u}}_{S^{+}}^{t}\\right)^{2}-\\left(\\overline{{v}}_{S^{-}}^{t}\\right)^{2}-w^{\\star}+\\left(\\displaystyle\\frac{X^{T}X}{N}-I\\right)\\left(\\left(\\overline{{u}}_{S^{+}}^{t}\\right)^{2}-\\left(\\overline{{v}}_{S^{-}}^{t}\\right)^{2}-w^{\\star}\\right)\\right.}\\\\ &{}&{\\left.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With same decomposition on $S^{+},S^{-},S^{c}$ for $\\Pi_{1,i}$ and $\\Pi_{2,i}$ , there are ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{I}_{\\lambda_{1}=0}=\\operatorname*{suf}\\left(2p_{x}^{a}-\\delta\\operatorname{a}_{x,x^{\\prime},\\varepsilon}^{\\lambda_{1}}+\\left(\\Delta_{x,x^{\\prime}}^{\\lambda_{2}}\\right)^{2}+\\left(\\frac{X^{\\prime}X}{n}-\\varepsilon\\right)\\left(2\\pi_{x^{\\prime}}^{a}-\\delta\\operatorname{a}_{x,x^{\\prime},\\varepsilon}^{\\lambda_{2}}+\\left(\\Delta_{x,x^{\\prime}}^{\\lambda_{1}}\\right)^{2}\\right)\\right.}\\\\ &{\\quad\\quad\\times\\left.\\frac{X^{\\prime}X}{n}\\sum_{i=0}^{n}\\left(2\\pi_{x^{\\prime}}^{a}-\\delta\\operatorname{a}_{x,x^{\\prime},\\varepsilon}^{\\lambda_{2}}+\\left(\\Delta_{x,x^{\\prime}}^{\\lambda_{2}}\\right)^{2}+2\\pi_{x^{\\prime}}^{a}\\otimes\\Delta_{x,x^{\\prime}}^{\\lambda_{1}}+\\left(\\Delta_{x,x^{\\prime}}^{\\lambda_{2}}\\right)^{2}\\right)\\right);}\\\\ &{\\mathbb{I}_{\\lambda_{1}=0}+\\operatorname*{suf}\\left(-2\\pi_{x^{\\prime}}^{a}+\\delta\\operatorname{a}_{x,x^{\\prime},\\varepsilon}^{\\lambda_{2}}-\\left(\\Delta_{x,x^{\\prime}}^{\\lambda_{2}}\\right)^{2}+\\left(\\frac{X^{\\prime}X}{n}-\\varepsilon\\right)\\left(-2\\pi_{x^{\\prime}}^{a}\\otimes\\Delta_{x,x^{\\prime}}^{\\lambda_{1}}-\\left(\\Delta_{x,x^{\\prime}}^{\\lambda_{1}}\\right)^{2}\\right.\\right.}\\\\ &{\\quad\\quad\\left.\\left.+\\frac{X^{\\prime}X}{n}\\right)\\left(-2\\pi_{x^{\\prime}}^{a}\\otimes\\Delta_{x,x^{\\prime},\\varepsilon}^{\\lambda_{2}}-\\left(\\Delta_{x,x^{\\prime}}^{\\lambda_{2}}\\right)^{2}-2\\pi_{x^{\\prime}}^{a}\\otimes\\Delta_{x,x^{\\prime},\\varepsilon}^{\\lambda_{2}}-\\left(\\Delta_{x,x^{\\prime}}^{\\lambda_{2}}\\right)^{2}\\right)\\right)}\\\\ &{\\quad\\quad-4\\Delta_{x,x^{\\prime}}^{\\lambda_{1}}\\otimes\\left(\\left(\\delta_{x,x^{\\prime}}^{\\lambda_{2 \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Substituting (59) into (57) would obtain the perturbed recursion of averaged $\\overline{{u}}_{S^{+}}^{t},\\overline{{u}}_{S^{-}}^{t},\\overline{{u}}_{S^{c}}^{t}$ Lemma 7. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "The following lemma separately bounds the consensus errors on $S^{+},\\,S^{-}$ ,and $S^{c}$ bythecorresponding magnitudes of parameters, which is different from the current analysis in the decentralized optimizationliterature. ", "page_idx": 26}, {"type": "text", "text": "Lemma 8. Consider the sequence $\\{\\pmb{u}^{t,i},\\pmb{v}^{t,i}\\},i\\;\\in\\;[m]$ generated according to (3) and (4) by DGD for solving (2),the consensus error of $\\Delta_{u}^{t}$ on positive support $S^{+}$ , negative support $S^{-}$ and non-support $S^{c}$ have the following recursions ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\Delta_{u,S^{*}}^{t+1}\\right|_{\\infty}\\leq\\rho\\left\\|\\Delta_{u,S^{*}}^{t}\\right\\|_{\\infty}\\cdot\\left(1+4\\eta\\left(\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+1\\right)\\left(\\left\\|(\\overline{{\\mathbf{u}}}_{s+1}^{t})^{2}-w_{S^{*}}^{*}\\right\\|_{\\infty}+\\left\\|(\\overline{{\\mathbf{v}}}_{s-1}^{t})^{2}-w_{S^{*}}^{*}\\right\\|_{\\infty}\\right)\\right.\\right.}\\\\ {+\\left.\\left.2d\\left(\\left(\\left\\|\\overline{{u}}_{s^{*}}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{u,S^{*}}^{t}\\right\\|_{\\infty}\\right)^{2}+\\left(\\left\\|\\overline{{\\mathbf{v}}}_{s^{*}}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{v,S^{*}}^{t}\\right\\|_{\\infty}\\right)^{2}\\right)}\\\\ {+\\left.\\left.\\left(\\left\\|\\overline{{u}}_{s-1}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{u,S^{*}}^{t}\\right\\|_{\\infty}\\right)^{2}+\\left(\\left\\|\\overline{{\\mathbf{v}}}_{s+1}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{v,S^{*}}^{t}\\right\\|_{\\infty}\\right)^{2}\\right)}\\\\ {+2\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+1\\right)\\left(\\left\\|\\overline{{u}}_{s+1}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{u,S^{*}}^{t}\\right\\|_{\\infty}^{2}+\\operatorname*{max}\\left\\|\\underline{{X_{i}^{T}\\xi_{i}}}\\right\\|_{\\infty}\\right)\\right)}\\\\ {+4\\rho\\eta\\left\\|\\overline{{u}}_{s+1}^{t}\\right\\|_{\\infty}\\cdot\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}\\left(2\\left\\|\\overline{{\\mathbf{v}}}_{s-1}^{t}\\right\\|_{\\infty}\\left\\|\\Delta_{v,S^{*}}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{v,S^{*}}^{t}\\right\\|_{\\infty}^{2}\\right)+C_{e r}^{t}\\right);}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{n^{3}}\\bigg|\\int_{\\mathbb{R}^{n}}\\int_{\\mathbb{Z}}|\\alpha|\\leq\\lambda+\\delta\\alpha\\bigg(\\nu(\\nabla\\xi_{n+1})+\\bigg(\\prod_{i=0}^{I}\\bigg(\\alpha_{i}\\bigg)^{2}-\\nu\\frac{\\alpha_{i}}{n}\\bigg)\\bigg|_{\\infty}+\\bigg\\mathcal{O}_{\\alpha}\\bigg)\\times\\bigg|_{\\infty}}&{=0,}\\\\ {+2\\frac{n}{\\mu}\\bigg|\\bigg|\\alpha_{i}\\bigg|_{\\infty}\\int_{\\mathbb{Z}}|\\alpha_{i}|\\geq\\nu\\bigg|_{\\infty}+\\frac{1}{n}\\bigg|\\bigg(\\alpha_{i}\\bigg)\\times\\bigg|_{\\infty}+2\\frac{\\alpha_{i}}{n}\\bigg|\\bigg|_{\\infty}+\\int_{\\mathbb{Z}}|\\alpha_{i}|\\leq\\nu\\bigg|_{\\infty}+\\underbrace{\\alpha_{i}}|\\bigg|_{\\infty}}\\\\ &{\\quad+\\delta\\alpha\\bigg(\\bigg(|\\alpha_{i}|_{\\infty}\\bigg)\\times\\bigg|\\alpha_{i}|\\leq\\lambda_{i}\\bigg)\\bigg|_{\\infty}+\\bigg(\\log_{1}-\\mathrm{i}\\left|\\alpha_{i}\\right|_{\\infty}\\bigg)\\times}\\\\ &{\\quad+\\bigg(\\log_{1}-\\mathrm{i}\\left|\\alpha_{i}\\bigg|_{\\infty}\\bigg)^{2}+\\bigg(\\log_{1}-\\mathrm{i}\\left|\\alpha_{i}\\right|_{\\infty}\\bigg)\\bigg)^{2}+\\bigg|\\alpha_{i}\\bigg|_{\\infty}\\bigg|\\bigg|_{\\infty}^{2}\\bigg|\\right)\\Biggr|}\\\\ {+\\delta\\alpha\\bigg|\\bigg|\\alpha_{i}\\bigg|_{\\infty}\\int_{\\mathbb{Z}}\\bigg(\\bigg(\\alpha\\sin\\alpha_{i}\\bigg)+\\bigg(\\ln_{1}\\bigg)\\alpha_{i}\\bigg)\\times\\bigg|_{\\infty}+\\bigg|\\alpha_{i}\\bigg|\\leq\\lambda_{i}\\bigg|_{\\infty}^{2}\\bigg|}\\\\ &{\\quad+2\\frac{n}{\\mu}\\bigg|\\bigg|\\alpha_{i}\\bigg|_{\\infty}\\bigg|\\int_{\\mathbb{Z}}|\\alpha_{i}|\\leq\\bigg|\\alpha_{i}\\bigg|_{\\infty}+\\bigg|\\alpha_{i}\\bigg|_{\\infty}^{2}\\bigg)+c_{\\alpha}\\bigg|\\bigg|}\\\\ {\\sum_{i}\\bigg|\\int_{\\mathbb{Z}}|\\alpha_{i}|\\leq\\lambda_{i}\\bigg|_{\\infty}^{2}\\bigg|-\\bigg(\\lambda_{i}\\bigg)\\Big(\\zeta\\alpha_{i}\\bigg \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the $C_{e r r}^{t}$ is defined as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{e r r}^{t}:=\\sqrt{s}(\\delta_{\\operatorname*{max}}+\\delta)\\cdot\\left(\\left\\|\\left(\\overline{{u}}_{s+}^{t}\\right)^{2}-w_{S^{+}}^{\\star}\\right\\|_{\\infty}+\\left\\|\\left(\\overline{{v}}_{s-}^{t}\\right)^{2}-w_{S^{-}}^{\\star}\\right\\|_{\\infty}\\right)}\\\\ &{\\qquad+\\,2d\\left(\\left(\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{u,S^{c}}^{t}\\right\\|_{\\infty}\\right)^{2}+\\left(\\left\\|\\overline{{v}}_{S^{c}}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{v,S^{c}}^{t}\\right\\|_{\\infty}\\right)^{2}\\right.}\\\\ &{\\qquad+\\left.\\left(\\left\\|\\overline{{u}}_{S^{-}}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{u,S^{-}}^{t}\\right\\|_{\\infty}\\right)^{2}+\\left(\\left\\|\\overline{{v}}_{S^{+}}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{v,S^{+}}^{t}\\right\\|_{\\infty}\\right)^{2}\\right)}\\\\ &{\\qquad+\\left.\\operatorname*{max}_{i}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}+\\left\\|\\frac{X^{T}\\xi}{N}\\right\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Based on the updating of DGD, there is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{U^{t+1}\\left(I_{m}-\\displaystyle\\frac{1}{m}\\mathbf{1}_{m}\\mathbf{1}_{m}^{T}\\right)=\\left(U^{t}-\\eta\\nabla_{u}f(U^{t},V^{t})\\right)W\\left(I_{m}-\\displaystyle\\frac{1}{m}\\mathbf{1}_{m}\\mathbf{1}_{m}^{T}\\right)}&{{}}\\\\ {=\\left(U^{t}-\\nabla_{u}f\\left(U^{t},V^{t}\\right)+\\nabla_{u}F\\left(\\overline{{U}}^{t},\\overline{{V}}^{t}\\right)\\right)\\left(W-\\displaystyle\\frac{1}{m}\\mathbf{1}_{m}\\mathbf{1}_{m}^{T}\\right)}&{{}}\\\\ {=\\left(U^{t}-\\nabla_{u}f\\left(U^{t},V^{t}\\right)+\\nabla_{u}f\\left(\\overline{{U}}^{t},V^{t}\\right)\\right.}&{{}}\\\\ {\\left.\\quad+\\nabla_{u}f\\left(\\overline{{U}}^{t},\\overline{{V}}^{t}\\right)-\\nabla_{u}f\\left(\\overline{{U}}^{t},V^{t}\\right)\\right.}&{{}}\\\\ {\\left.\\quad+\\nabla_{u}F\\left(\\overline{{U}}^{t},\\overline{{V}}^{t}\\right)-\\nabla_{u}f\\left(\\overline{{U}}^{t},\\overline{{V}}^{t}\\right)\\right)\\left(W-\\displaystyle\\frac{1}{m}\\mathbf{1}_{m}\\mathbf{1}_{m}^{T}\\right).}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, there is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big\\|_{\\infty}\\leq p\\Big\\|\\Delta_{\\xi}\\Big\\rho_{s}\\Big\\|_{\\infty}+p\\eta\\pi_{\\operatorname*{max}}\\Big\\|_{\\infty}\\leq\\big(\\nabla_{\\xi}\\rho_{s}\\big)\\Big(\\Pi_{\\theta}\\Big\\rho_{s}^{(\\theta)}\\big\\|_{\\infty}^{2}-\\nabla_{\\xi}\\rho_{\\theta}^{(\\theta)}\\big\\|_{\\infty}^{2}\\Big)\\leq r\\leq\\frac{1}{r}\\Big(\\Delta_{\\xi}\\Big\\rho_{\\theta}^{(\\theta)}\\Big)\\Big\\}\\Big\\|_{\\infty}}&{}\\\\ {\\leq p\\left\\|\\Delta_{\\xi}\\Big\\rho_{s}\\Big\\|_{\\infty}^{2}+p\\pi_{\\operatorname*{max}}\\Big\\|_{\\infty}+\\eta\\Big(\\Pi_{\\theta}\\Big\\rho_{s}^{(\\theta)}\\Big\\|_{\\infty}+p\\eta\\Big(\\Delta_{\\xi}\\Big\\|_{\\infty}^{2}\\Big\\|_{\\infty}+\\eta\\Big(\\Delta_{\\xi}\\Big\\rho_{\\theta}^{(\\theta)}\\Big\\|_{\\infty}}\\\\ {+\\left.\\rho\\partial_{\\theta}\\right)\\Big\\|_{\\infty}^{2}+\\nabla_{\\phi}\\Big(\\Pi_{\\phi}\\Big\\rho_{s}^{(\\theta)}\\Big\\|_{\\infty}^{2}-\\nabla_{\\phi}\\Big(\\Pi_{\\theta}\\Big\\rho_{s}^{(\\theta)}\\Big\\|_{\\infty}}\\\\ {\\leq\\rho\\Big\\|_{\\infty}^{2}\\Big\\|_{\\infty}+4\\eta\\Big\\|_{\\infty}^{2}+\\frac{1}{r}\\Big(\\Delta_{\\phi}\\Big\\|_{\\infty}^{2}\\Big\\|_{\\infty}+\\sqrt_{\\phi}\\Big\\|_{\\infty}^{2}\\Big\\|_{\\infty}^{2}\\Big\\|_{\\infty}^{2}+1\\Big\\|\\Delta_{\\xi}\\Big\\rho_{\\infty}\\Big\\|_{\\infty}^{2}}\\\\ {+\\frac{1}{r}\\Big(\\Delta_{\\phi}\\Big\\|_{\\infty}^{2}\\Big\\|_{\\infty}^{2}\\Big\\|_{\\infty}^{2}\\Big\\|_{\\infty}^{2}\\Big\\|_{\\infty}^{2}+1\\Big\\|\\Delta_{\\phi}\\Big\\rho_{\\infty}^{(\\theta)}\\Big\\|_{\\infty}^{2}+2\\Big\\|_{\\infty}^{2}\\Big\\|_{\\infty}^{2}+1\\Big\\|\\Delta_{\\phi}\\Big\\rho_{\\infty}\\Big\\|_{\\infty}^{2}}\\\\ {+\\frac{1}{r}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the second inequality is due to (64) and the last inequality is substituting into the formula $\\Pi_{1,i},\\Pi_{2,i}$ in (59) and gradient difference at averaged pair. Merging terms involved $\\left\\|\\overline{{\\boldsymbol{u}}}_{S^{+}}^{t}\\right\\|_{\\infty}$ and $\\left\\|\\Delta_{\\mathbf{u},S^{+}}^{t}\\right\\|_{\\infty}$ separately would obtain the result in (60). Performing analogous proof would yield results on negative support $S^{-}$ in (49) and non-support $S^{c}$ in (50). \u53e3 ", "page_idx": 28}, {"type": "text", "text": "The following proposition describes the dynamics of $\\pmb{u}^{t,i},\\pmb{v}^{t,i}$ by conducting inductive hypothesis for both $\\pmb{u}^{t,i}$ and $\\bar{\\mathbf{\\psi}}_{v}^{t,i}$ , which is different with centralized setting [33]. Because the complicated consensus errors prevent transferring the proof of non-negative case to general case trivially. ", "page_idx": 28}, {"type": "text", "text": "Proposition 3. This proposition inherits Proposition 2 for the general case with considering dynamicof $\\pmb{v}_{i}^{t}$ for learningnegativepart signal $w_{S^{-}}^{\\star}$ Recall thedefinitionsof $T,T_{k},\\overline{{T}}_{k},B_{k},K,^{'}\\!\\beta$ in Proposition 2, the following statements hold in each induction step. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ (a)\\forall t\\quad t h a t\\quad\\overline{{T}}_{k-1}\\leq\\qquad t\\qquad<\\qquad\\overline{{T}}_{k}\\quad w i t h\\quad\\forall k\\qquad\\in\\qquad[K],}\\\\ &{\\quad\\operatorname*{max}\\left\\{\\left\\|\\left(\\overline{{u}}_{S^{+}}^{t}\\right)^{2}-w_{S^{+}}^{\\star}\\right\\|_{\\infty},\\left\\|\\left(\\overline{{v}}_{S^{-}}^{t}\\right)^{2}-w_{S^{-}}^{\\star}\\right\\|_{\\infty}\\right\\}\\leq\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u00b7 $\\begin{array}{r l}&{(b)\\;F o r\\;\\forall k\\in[K],\\,t h e r e\\;i s\\operatorname*{max}\\left\\{\\left\\|\\left(\\overline{{u}}_{\\delta^{+}}^{T_{k-1}}\\right)^{2}-w_{\\delta^{+}}^{*}\\right\\|_{\\infty},\\left\\|\\left(\\overline{{\\sigma}}_{S^{-}}^{T_{k-1}}\\right)^{2}-w_{\\delta^{-}}^{*}\\right\\|_{\\infty}\\right\\}\\leq\\frac{w_{\\delta^{+}}^{*}}{2^{\\frac{3}{2}\\kappa}}.}\\\\ &{(c)\\;\\forall t\\,t h a t\\,\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}\\;t w i t h\\;\\forall k\\in[K],\\,t h e r e\\;a r e\\operatorname*{max}_{i}\\left\\|\\Delta_{u,S^{+}}^{t_{i}}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}\\eta}\\left\\|\\overline{{u}}_{\\delta^{+}}^{t_{i}}\\right\\|_{\\infty}B_{k}}\\\\ &{a n d\\operatorname*{max}_{i}\\left\\|\\Delta_{v,S^{-}}^{t_{i},i}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}\\eta}\\left\\|\\overline{{v}}_{\\delta^{-}}^{t_{i}}\\right\\|_{\\infty}B_{k}.}\\\\ &{(d)\\;F o r\\;a n y\\;\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}\\;t h a t\\,\\forall k\\in[K],\\,t h e r e\\;a r e\\operatorname*{max}_{i}\\left\\|\\Delta_{u,S^{+}}^{t_{i},i}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}\\eta}\\left\\|\\overline{{u}}_{\\delta^{+}}^{t_{i}}\\right\\|_{\\infty}B_{k}}\\\\ &{a n d\\operatorname*{max}_{i}\\left\\|\\Delta_{v,S^{+}}^{t_{i},i}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}\\eta}\\left\\|\\overline{{v}}_{\\delta^{+}}^{t_{i}}\\right\\|_{\\infty}B_{k}.}\\\\ &{(e)\\;F o r\\;a n y\\;\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}\\;t h a t\\,\\forall k\\in[K],\\,t h e r e\\;a r e\\operatorname*{max}_{i}\\left\\|\\Delta_{u,S^{-}}^{t_{i},i}\\right\\|_{\\infty}\\leq $ $(f)$ Vt that $\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}$ with $k\\,\\in\\,[K]$ , there are refined element-wise bounds for consensus errors as $\\left\\|\\Delta_{u,j}^{t}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta|\\overline{{u}}_{j}|B_{k}$ and $\\begin{array}{r}{\\left\\|\\Delta_{v,j}^{t}\\right\\|_{\\infty}\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta|\\overline{{v}}_{j}|B_{k},\\forall j\\in\\mathcal{S}}\\end{array}$   \n$(g)\\,F o r\\,\\forall j\\in S^{+}$ and $k\\in[K]$ there is $\\alpha^{3}\\leq\\left(\\overline{{\\pmb{u}}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}\\leq w_{j}^{\\star}+4B_{k}$ For $\\forall j\\in S^{-}$ and $k\\in[K]$ there is $\\alpha^{3}\\leq\\left(\\overline{{v}}_{j}^{\\overline{{T}}_{k-1}}\\right)^{2}\\leq w_{j}^{\\star}+4B_{k}$   \n$\\begin{array}{r}{\\mathbf{\\eta}\\cdot\\left(h\\right)\\forall t\\leq\\mathcal{O}\\left(\\frac{1}{\\eta\\zeta}\\log\\frac{1}{\\alpha}\\right)\\!,\\operatorname*{max}\\left\\{\\left\\Vert\\overline{{u}}_{S^{c}}^{t}\\right\\Vert_{\\infty},\\left\\Vert\\overline{{v}}_{S^{c}}^{t}\\right\\Vert_{\\infty}\\right\\}\\leq\\sqrt{\\alpha}.}\\end{array}$   \n$\\begin{array}{r}{(i)\\,\\forall t\\leq\\mathcal{O}\\left(\\frac{1}{\\eta\\zeta}\\log\\frac{1}{\\alpha}\\right)}\\end{array}$ and $\\forall j\\in S$ there is $\\overline{{{\\pmb{u}}}}_{j}^{t}\\overline{{{\\pmb{v}}}}_{j}^{t}\\leq\\alpha^{\\frac{3}{2}}$   \n$\\begin{array}{r}{(j)\\,\\forall t\\leq\\mathcal{O}\\left(\\frac{1}{\\eta\\zeta}\\log\\frac{1}{\\alpha}\\right),\\,i t\\,h a s\\operatorname*{max}\\left\\{\\left\\lvert\\left\\lvert\\overline{{u}}_{S^{-}}^{t}\\right\\rvert\\right\\rvert_{\\infty},\\left\\lVert\\overline{{v}}_{S^{+}}^{t}\\right\\rVert_{\\infty}\\right\\}\\leq\\sqrt{\\alpha}.}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Proof. Proof idea: The key difference in the proof between centralized and decentralized settings lies in the fact that we cannot directly transfer the proof from the simpler non-negative $\\pmb{w}^{\\star}$ to the general $w^{\\star}$ . This is because the error terms induced in (48), (49),(50) by the decentralized network in Lemma 7 prevent obtaining the results in inductions (i), G) without induction steps as Lemma B.16 in [33], which allows to apply proof of non-negative case to the general case directly in the centralized case. Therefore, we have to conduct the comprehensive induction process for the general $w^{\\star}$ , which can ensure that the magnitudes of $\\overline{{\\pmb{u}}}^{t}$ on the negative support $S^{-}$ and ${\\overline{{\\pmb{v}}}}^{t}$ on the positive support $S^{+}$ remain small up to the early stopping iteration steps. The inductions (a)-(h) show that the averaged signal $\\overline{{\\pmb{u}}}^{t}$ on the positive support $S^{+}$ and the averaged signal ${\\overline{{\\pmb{v}}}}^{t}$ on the negative support exhibit dynamics similar to those in non-negative case, as outlined in Proposition 2. ", "page_idx": 29}, {"type": "text", "text": "Base case: As the initialization ${\\pmb u}^{0,i}={\\pmb v}^{0,i}=\\alpha{\\bf1}_{d},\\forall i\\in[m]$ . Due to the condition on $\\alpha$ the base case is true. ", "page_idx": 29}, {"type": "text", "text": "Induction Step: Under the assumption that all above induction hypotheses hold until some $0\\leq k\\leq K-1$ , we should prove they still should hold at $k+1$ -th induction. ", "page_idx": 29}, {"type": "text", "text": "(a) We should prove that $\\forall t\\in\\{\\overline{{T}}_{k-1},\\cdot\\cdot\\cdot,\\overline{{T}}_{k}-1\\}$ the condition $\\begin{array}{r}{\\left\\|\\left(\\overline{{\\pmb{u}}}_{S^{+}}^{t}\\right)^{2}-\\pmb{w}_{S^{+}}^{\\star}\\right\\|_{\\infty}\\leq\\,\\frac{\\pmb{w}_{\\operatorname*{max}}^{\\star}}{2^{k}}}\\end{array}$ and $\\begin{array}{r}{\\left\\|\\left(\\overline{{\\pmb{v}}}_{S^{-}}^{t}\\right)^{2}-\\pmb{w}_{S^{-}}^{\\star}\\right\\|_{\\infty}\\leq\\frac{\\pmb{w}_{\\operatorname*{max}}^{\\star}}{2^{k}}}\\end{array}$ still holds If the condition is true for $t$ -th ieration, then based on claims (c),(d), (e), (h) and (j) the $\\|\\pmb{p}_{u}^{t}\\|_{\\infty}$ in (51) under current induction step could be bounded as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|{p}_{u}^{t}\\right|\\!\\right|_{\\infty}\\leq8\\sqrt{s}\\delta_{\\operatorname*{max}}\\beta\\rho^{\\frac{3}{4}}B_{k}\\eta\\left(\\left\\Vert\\overline{{u}}_{S^{+}}^{t}\\right\\Vert_{\\infty}^{2}+\\left\\Vert\\overline{{v}}_{S^{-}}^{t}\\right\\Vert_{\\infty}^{2}\\right)+\\sqrt{s}\\delta_{\\operatorname*{max}}\\left(4\\beta\\rho^{\\frac{3}{4}}B_{k}\\eta\\left\\Vert\\overline{{v}}_{S^{-}}^{t}\\right\\Vert_{\\infty}\\right)^{2}}\\\\ &{\\qquad\\qquad+\\left(\\sqrt{s}\\delta_{m}+3\\right)\\left(4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\Vert\\overline{{u}}_{S^{+}}^{t}\\right\\Vert_{\\infty}B_{k}\\right)^{2}+2d\\left(8\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}+\\left(4\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}\\right)^{2}\\right)}\\\\ &{\\qquad\\qquad\\cdot\\left(\\left\\Vert\\overline{{u}}_{S^{+}}^{t}\\right\\Vert_{\\infty}^{2}+\\left\\Vert\\overline{{v}}_{S^{+}}^{t}\\right\\Vert_{\\infty}^{2}+\\left\\Vert\\overline{{u}}_{S^{-}}^{t}\\right\\Vert_{\\infty}^{2}+\\left\\Vert\\overline{{v}}_{S^{+}}^{t}\\right\\Vert_{\\infty}^{2}\\right)}\\\\ &{\\qquad\\qquad\\leq32\\sqrt{s}\\delta_{\\operatorname*{max}}\\beta\\rho^{\\frac{3}{4}}B_{k}\\eta w_{\\operatorname*{max}}^{\\star}+32\\sqrt{s}\\delta_{\\operatorname*{max}}\\beta^{2}\\rho^{\\frac{3}{2}}B_{k}^{2}\\eta^{2}w_{\\operatorname*{max}}^{\\star}+32\\left(\\sqrt{s}\\delta_{m}+3\\right)\\beta^{2}\\rho^{\\frac{3}{2}}\\eta^{2}B_{k}^{2}w_{\\operatorname*{max}}^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\left.2d\\left(8\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}+16\\beta^{2}\\rho^{\\frac{3}{2}}\\eta^{2}B_{k}^{2}\\right)\\left(\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}^{2}+\\left\\|\\overline{{v}}_{S^{c}}^{t}\\right\\|_{\\infty}^{2}+\\left\\|\\overline{{u}}_{S^{-}}^{t}\\right\\|_{\\infty}^{2}+\\left\\|\\overline{{v}}_{S^{+}}^{t}\\right\\|_{\\infty}^{2}\\right)}\\\\ &{\\leq\\frac{3\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+3\\right)\\rho^{\\frac{3}{4}}B_{k}}{16}+\\frac{d\\alpha\\rho^{\\frac{3}{4}}}{32},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the first inequality is due to definition of local RIP condition and Lemma 2 and Lemma 3, secondiqualityistohythesis ald $t$ th iterationthat $\\left\\|\\overline{{\\pmb{u}}}_{S^{+}}^{t}\\right\\|_{\\infty}^{2}\\leq2w_{\\operatorname*{max}}^{\\star},\\left\\|\\overline{{\\pmb{v}}}_{S^{-}}^{t}\\right\\|_{\\infty}^{2}\\leq$ 2wmaxx , the last inequality is due to definitions of $\\beta,B_{k}$ , step size condition and hypothesis that $\\left|\\left|\\overline{{u}}_{S^{c}}^{t}\\right|\\right|_{\\infty},\\left|\\left|\\overline{{v}}_{S^{c}}^{t}\\right|\\right|_{\\infty},\\left|\\left|\\overline{{u}}_{S^{-}}^{t}\\right|\\right|_{\\infty},\\left|\\left|\\overline{{v}}_{S^{+}}^{t}\\right|\\right|_{\\infty}$ would keep below $\\sqrt{\\alpha}$ up to early stopping under hypothesis (h) and (j). ", "page_idx": 30}, {"type": "text", "text": "The element-wise bound for $\\pmb q_{u}^{t}$ in (52) under current induction step can be bounded as follows that $\\forall j\\in S^{+}$ ,there is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|q_{u,j}^{t}|\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta|\\overline{{u}}_{j}^{t}|B_{k}\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}\\left(\\left\\|\\left(\\overline{{u}}_{s^{+}}^{t}\\right)^{2}-w_{s^{+}}^{*}\\right\\|_{\\infty}+\\left\\|\\left(\\overline{{v}}_{s^{-}}^{t}\\right)^{2}-w_{s^{-}}^{*}\\right\\|_{\\infty}\\right)\\right.}\\\\ &{\\qquad\\qquad+\\frac{3\\left.\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+3\\right)\\rho^{\\frac{3}{4}}B_{k}}{16}+8d\\left(\\left\\|\\overline{{u}}_{s^{+}}^{t}\\right\\|_{\\infty}^{2}+\\left\\|\\overline{{v}}_{s^{-}}^{t}\\right\\|_{\\infty}^{2}+\\left\\|\\overline{{u}}_{s^{-}}^{t}\\right\\|_{\\infty}^{2}+\\left\\|\\overline{{v}}_{s^{+}}^{t}\\right\\|_{\\infty}^{2}\\right)+\\operatorname*{max}_{i}\\left\\|}\\\\ &{\\qquad\\leq4\\beta\\rho^{\\frac{1}{2}}\\eta|\\overline{{u}}_{j}^{t}|B_{k}\\left(\\rho^{\\frac{1}{4}}\\sqrt{s}\\delta_{\\operatorname*{max}}\\left(\\left\\|\\left(\\overline{{u}}_{s^{+}}^{t}\\right)^{2}-w_{s^{+}}^{*}\\right\\|_{\\infty}+\\left\\|\\left(\\overline{{v}}_{s^{-}}^{t}\\right)^{2}-w_{s^{-}}^{*}\\right\\|_{\\infty}\\right)\\right.}\\\\ &{\\qquad\\left.+\\frac{3\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+3\\right)\\rho B_{k}}{16}+32d\\rho^{\\frac{1}{4}}\\alpha+\\rho^{\\frac{1}{4}}\\operatorname*{max}_{i}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}\\right)}\\\\ &{\\qquad\\leq\\frac{\\rho^{\\frac{1}{2}}B_{k}|\\overline{{u}}_{j}^{t}|}{32},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the first inequality is based on refined bound in hypothesis (c), result in (66) and inequality $\\left\\|\\pmb{x}+\\pmb{y}\\right\\|_{\\infty}^{2}\\leq2\\left\\|\\pmb{x}\\right\\|_{\\infty}^{2}+2\\left\\|\\pmb{y}\\right\\|_{\\infty}^{2}$ and step size condition that $4\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}<1$ The second inequality uses hypothesis (h) and (Gj). The last inequality is because of hypothesis (a), the step size condition, network connectivity condition, small initialization $\\alpha$ anddefinition of $B_{k}$ ", "page_idx": 30}, {"type": "text", "text": "Then $\\mathbf{\\boldsymbol{q}}_{u}^{t}$ couldbe earameined as ${\\pmb q}_{u}^{t}={\\pmb r}_{q_{u}}^{t}\\odot\\overline{{{\\pmb u}}}_{S^{+}}^{t}$ where $\\begin{array}{r}{\\left\\|r_{q_{u}}^{t}\\right\\|_{\\infty}\\leq\\frac{\\sqrt{\\rho}B_{k}}{32},\\forall t,\\overline{{T}}_{k-1}\\leq t<\\overline{{T}}_{k}.}\\end{array}$ Then the perturbed recursion for $\\overline{{\\pmb{u}}}^{t}$ on positive support $S^{+}$ over decentralized network in (48) becomes ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\overline{{\\mathbf{u}}}_{S+}^{t+1}\\right)^{2}=\\left(\\overline{{\\mathbf{u}}}_{S+}^{t}\\right)^{2}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(\\left(\\overline{{\\mathbf{u}}}_{S+}^{t}\\right)^{2}-\\mathbf{w}_{S+}^{\\star}+\\mathbf{E}_{2s}^{t}+\\mathbf{E}_{3s}^{t}+p_{u}^{t}+r_{q_{u}}^{t}\\right)\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the new perturbed error tems ${\\cal E}_{2s}^{t},{\\cal E}_{2s}^{t}$ are defined as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle E_{2s}^{t}:=\\left(\\frac{X^{T}X}{N}-I\\right)\\left(\\left(\\overline{{{u}}}_{S^{+}}^{t}\\right)^{2}-{w}_{S^{+}}^{\\star}-\\left(\\overline{{{v}}}_{S^{-}}^{t}\\right)^{2}-{w}_{S^{-}}^{\\star}\\right)}}\\\\ {{\\displaystyle E_{3s}^{t}:=\\frac{X^{T}X}{N}\\left(\\left(\\overline{{{u}}}_{S^{\\star}}^{t}\\right)^{2}-\\left(\\overline{{{v}}}_{S^{+}}\\right)^{2}+\\left(\\overline{{{u}}}_{S^{-}}^{t}\\right)^{2}-\\left(\\overline{{{v}}}_{S^{\\star}}^{t}\\right)^{2}\\right)-\\frac{X^{T}\\xi}{N}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then the total error terms in (68) can be bounded as follows ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|E_{2s}^{t}\\right\\|_{\\infty}+\\left\\|E_{3s}^{t}\\right\\|_{\\infty}+\\left\\|p_{u}^{t}\\right\\|_{\\infty}+\\left\\|r_{q_{u}}^{t}\\right\\|_{\\infty}\\stackrel{(i)}{\\leq}2\\sqrt{s}\\delta}\\frac{w_{\\operatorname*{max}}^{\\star}}{2}+8d\\alpha+\\left\\|\\frac{X^{T}\\xi}{N}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.3\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+3\\right)\\rho^{\\frac{3}{4}}B_{k}+\\frac{d\\alpha\\rho^{\\frac{3}{4}}}{32}+\\frac{\\rho^{\\frac{1}{2}}B_{k}}{32}}\\\\ &{\\stackrel{(i i)}{\\leq}C_{\\gamma}\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k}}+C_{b}\\cdot\\frac{2}{C_{b}}\\operatorname*{max}\\left\\{\\left\\|\\frac{X^{T}\\xi}{N}\\right\\|_{\\infty},\\epsilon\\right\\}+\\frac{B_{k}}{2}}\\\\ &{\\stackrel{(i i i)}{\\leq}C_{\\gamma}\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k}}+C_{b}\\zeta+\\frac{B_{k}}{2}}\\\\ &{\\stackrel{(i v)}{\\leq}(C_{\\gamma}+2C_{b})\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k}}+\\frac{B_{k}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\stackrel{(v)}{\\le}\\frac{1}{2}\\cdot\\frac{1}{40}\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k}}+\\frac{B_{k}}{2}}\\\\ &{\\stackrel{(v i)}{\\le}B_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $(i)$ is because of Lemma 2 and Lemma 3 under the global RIP condition and induction hypothesis (a) and substituting the results in (66) and (67). $(i i)$ is due to condition on global RIP parameter that $\\begin{array}{r}{\\delta\\leq\\frac{C_{\\gamma}}{2\\sqrt{s}\\left(\\log\\lceil\\frac{w_{\\operatorname*{max}}^{\\star}}{\\zeta}\\rceil+1\\right)}}\\end{array}$ condition $\\begin{array}{r}{\\alpha\\leq\\frac{\\zeta}{(3d+1)^{2}}}\\end{array}$ and network connectivity condition $\\rho~\\leq~\\frac{1}{36\\big(\\sqrt{s}\\delta_{\\mathrm{max}}+3\\big)^{\\frac{4}{3}}}$ $(i i i)$ is due to definition of $\\zeta$ and $(i v)$ is based on definition of $K$ that $\\begin{array}{r}{\\zeta\\leq\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{K-1}}\\leq\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k-1}}}\\end{array}$ $(v)$ is because definition of $B_{k}$ and $C_{\\gamma},C_{b}$ are determined later that are small enough that C\u2190 + 2Cb \u2264 80\u00b7 ", "page_idx": 31}, {"type": "text", "text": "Badida $\\left\\|\\left(\\overline{{\\pmb{u}}}_{S^{+}}^{t+1}\\right)^{2}-\\pmb{w}_{S^{+}}^{\\star}\\right\\|_{\\infty}\\leq$ $\\frac{{w_{\\mathrm{max}}^{\\star}}}{2^{k}}$ $\\boldsymbol{S}$ $S^{+}$ We can obtain similar results for $\\overline{{\\pmb{v}}}_{S^{-}}^{t}$ . Combined two cases and induction on $t$ we have conclusion that $\\forall t\\geq\\overline{{T}}_{k-1}$ , there is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\left\\|\\left(\\overline{{\\boldsymbol{u}}}_{S^{+}}^{t}\\right)^{2}-\\boldsymbol{w}_{S^{+}}^{\\star}\\right\\|_{\\infty},\\left\\|\\left(\\overline{{\\boldsymbol{v}}}_{S^{-}}^{t}\\right)^{2}-\\boldsymbol{w}_{S^{-}}^{\\star}\\right\\|_{\\infty}\\right\\}\\leq\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{k}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, we finish the proof of induction (a). ", "page_idx": 31}, {"type": "text", "text": "(b) Comparing the (68), (70) with (30), the proof of this hypothesis can follow the proof of hypothesis (b) in Proposition 2. ", "page_idx": 31}, {"type": "text", "text": "(cC) $\\forall t$ that $\\overline{{T}}_{k-1}\\leq t<t+1<\\overline{{T}}_{k}$ , based on one step iteration in (60) and induction (a), there is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\left\\{\\begin{array}{l l}{\\alpha_{1}^{*}\\displaystyle\\sum_{i=1}^{j-1}\\left(H_{\\infty,i}^{-1}\\right)_{i}\\right\\}+\\delta\\left(\\sqrt{\\alpha_{33}}\\omega_{i}+1\\right)\\left\\{H_{\\infty,i}^{-1}\\right\\}_{1}^{\\infty}+\\operatorname*{max}\\left\\{\\displaystyle\\sum_{i=1}^{j-1}\\left(H_{\\infty,i}^{-1}\\right)_{i}^{\\infty}\\right\\}}\\\\ &{\\quad+\\delta\\left(\\nu_{1}^{3}-1\\right)\\omega_{i}\\left(1+\\omega_{33}^{\\prime}\\omega_{1}\\omega_{1}\\right)\\left\\{H_{\\infty,i}^{-1}\\right\\}_{2}^{\\infty}\\left(1+\\omega_{33}^{\\prime}\\omega_{1}\\omega_{1}\\right)\\left\\{H_{\\infty,i}^{-1}\\right\\}_{2}^{\\infty}\\left[\\nu_{2}^{*}\\right]_{-1}^{2},}\\\\ &{\\quad+\\delta\\left(\\nu_{3}^{2}\\omega_{1}-2\\nu_{33}^{\\prime}+8\\Big(\\left\\{H_{\\infty,i}^{-1}\\right\\}_{2}^{-1}+\\left\\{H_{\\infty,i}^{-1}\\right\\}_{2}^{-1}+\\left\\{H_{\\infty,i}^{-1}\\right\\}_{2}^{\\infty}\\right)\\right.}\\\\ &{\\quad\\left.+\\operatorname*{max}\\left\\{\\displaystyle\\sum_{i=1}^{j-1}\\left(H_{\\infty,i}^{-1}\\right)_{i}\\right\\}+\\frac{\\delta\\left(\\nu_{3}^{2}\\omega_{1}^{\\prime}\\right)_{i}^{\\infty}}{\\nu_{1}}\\right\\}}\\\\ &{\\stackrel{(a)}{\\underset{(b)}{\\longrightarrow}}\\displaystyle\\sum_{2}^{i}\\left\\{1\\right\\}\\omega_{i,\\nu-1}^{\\prime}\\left\\{1+\\delta\\left(\\nu_{3}^{2}\\nu_{1}^{*}\\right)_{i}^{\\infty}\\left(\\nu_{4}-1\\right)+3\\nu_{2}^{3}\\omega_{1}\\omega_{1}\\right\\}}\\\\ &{\\qquad+\\delta\\left(\\nu_{3}^{2}\\omega_{1}\\omega_{1}^{\\prime}\\right)\\displaystyle\\sum_{i=1}^{j-1}\\left(\\nu_{4}^{3}\\nu_{1}^{*}\\right)\\left\\{\\left(\\nu_{3}^{*}\\right)_{i-1}\\left(\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $(i)$ is due to the claim (a) and step size condition that $4\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}~\\leq~1$ and $\\delta~\\leq~\\delta_{\\mathrm{max}}$ $(i i)$ is due to $4\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}\\,\\leq\\,1$ , condition that all $\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty},\\left\\|\\overline{{v}}_{S^{c}}^{t}\\right\\|_{\\infty},\\left\\|\\overline{{u}}_{S^{-}}^{t}\\right\\|_{\\infty},\\left\\|\\overline{{v}}_{S^{+}}^{t}\\right\\|_{\\infty},\\left\\|\\overline{{v}}_{S^{+}}^{t}\\right\\|_{\\infty}$ are not larger than $\\sqrt{\\alpha}$ before early stopping in hypothesis (h), (j) and $\\begin{array}{r}{\\left\\|\\frac{{\\pmb X}^{T}{\\pmb\\xi}}{N}\\right\\|_{\\infty}\\ \\leq\\ \\operatorname*{max}_{i}\\left\\|\\frac{{\\pmb X}_{i}^{T}{\\pmb\\xi}_{i}}{n}\\right\\|_{\\infty}}\\end{array}$ The $(i i i)$ is because $\\begin{array}{r l r}{\\rho^{\\frac{1}{4}}}&{\\leq}&{\\operatorname*{min}\\Bigg\\{\\frac{\\delta}{\\sqrt{s}\\delta_{\\operatorname*{max}}+1},\\frac{\\delta}{8\\delta_{\\operatorname*{max}}},\\frac{\\left\\|\\frac{{\\mathbf{x}}^{T}\\xi}{N}\\right\\|_{\\infty}^{\\infty}}{8\\operatorname*{max}_{i}\\left\\|\\frac{{\\mathbf{x}}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}}\\Bigg\\},~\\alpha~~\\leq~~\\frac{\\epsilon}{(12d+1)^{2}}}\\end{array}$ and $\\operatorname*{max}\\left\\{\\left\\|\\frac{\\pmb{X}^{T}\\pmb{\\xi}}{N}\\right\\|_{\\infty},\\epsilon\\right\\}~\\leq~w_{\\operatorname*{max}}^{\\star}$ .The $(i v)$ is because in the current induction step hypothesis (c) holds in $t$ -th iteration and step size condition that satisfies $\\begin{array}{r l r}{\\eta}&{{}\\le}&{\\frac{1-\\sqrt{\\rho}}{160\\rho^{\\frac{1}{4}}w_{\\mathrm{max}}^{\\star}}}\\end{array}$ such that $\\begin{array}{r}{\\rho^{\\frac{1}{2}}\\left(\\rho^{\\frac{1}{4}}+80\\rho^{\\frac{1}{4}}\\eta w_{\\mathrm{max}}^{\\star}\\right)\\le(1-(1-\\rho^{\\frac{1}{2}}))(1+\\frac{1-\\sqrt{\\rho}}{2})\\le\\frac{1+\\sqrt{\\rho}}{2}}\\end{array}$ and $\\begin{array}{r}{16\\sqrt{s}\\delta_{\\operatorname*{max}}\\beta\\rho\\eta\\left\\|\\overline{{v}}_{S^{-}}^{t}\\right\\|_{\\infty}^{2}\\leq\\frac{B_{k}}{4}}\\end{array}$ where $\\left\\|\\overline{{v}}_{S^{-}}^{t}\\right\\|_{\\infty}^{2}\\,\\leq\\,2w_{\\mathrm{max}}^{\\star}$ is according to claim (a). The last inequality $(v)$ is based on the (68) and step size condition $\\begin{array}{r}{\\eta\\,\\le\\,\\frac{c_{10}\\left(1-\\sqrt{\\rho}\\right)}{w_{\\mathrm{max}}^{\\star}}}\\end{array}$ such that $4\\eta\\left(\\left\\|\\left(\\overline{{\\pmb{u}}}_{S^{+}}^{t}\\right)^{2}-\\pmb{w}_{S^{+}}^{\\star}\\right\\|_{\\infty}+B_{k}\\right)\\leq1$ based on hypothesis (a). Comparing the formula of (72) with (34), we can follow the poof of hypothesis (c) in Proposition 2 to finish the proof. ", "page_idx": 32}, {"type": "text", "text": "(d) The proof is similar to that of hypothesis (c). $\\forall t$ that $\\overline{{T}}_{k-1}\\leq t<t+1\\leq\\overline{{T}}_{k}$ , according to one step iteration in (49) and hypothesis (a), we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Lambda_{\\mu,\\nu}^{(k)}\\Big|_{\\infty}^{\\infty}}&{\\leq\\mathcal{P}\\left|\\Lambda_{\\mu,\\nu}^{(k)}\\right|_{\\infty}^{\\infty}\\cdot\\left(1+\\mathcal{Q}\\left(\\sqrt{\\Lambda_{\\mu,\\nu}}\\left(\\sum_{s=0}^{\\nu-1}+\\mathcal{P}\\right)\\mathbb{I}\\left|\\mathcal{R}_{\\mu}^{s}\\right|\\right)_{\\infty}+\\mathcal{Q}\\left(\\left|\\mathcal{R}_{\\mu}^{s}\\right|\\right)_{\\infty}^{\\infty}\\right|_{\\infty}}\\\\ &{\\quad+\\mathcal{P}\\left|\\Lambda_{\\mu}^{(k)}\\right|_{\\infty}^{\\infty}\\cdot\\left(\\Lambda_{\\mu}^{s}\\right)+\\mathcal{Q}^{\\dagger}\\left|\\mathcal{R}_{\\mu}^{s}\\right|_{\\infty}^{\\infty}\\right|_{\\infty}\\cdot\\left|\\Lambda_{\\mu}\\left(\\mathcal{R}_{\\nu}^{s}\\right|-\\Lambda_{\\nu}+\\mathcal{Q}\\Lambda_{\\mu}^{s}\\right)\\left|\\mathcal{R}_{\\nu}^{s}\\right|}\\\\ &{\\quad+\\mathcal{Q}\\left(\\Lambda_{\\mu}^{s}\\right)+\\mathcal{Q}^{\\dagger}\\left|\\Lambda_{\\nu}^{(k)}\\right|_{\\infty}^{\\infty}\\cdot\\left(\\sum_{s=0}^{\\nu-1}\\mathcal{Q}\\left|\\Lambda_{\\mu}^{s}\\right|_{\\infty}^{\\infty}\\right)\\cdot\\left(\\mathcal{P}_{\\nu}^{\\dagger}\\Lambda_{\\nu}\\left(\\left|\\mathcal{R}_{\\nu}^{s}\\right|-\\Lambda_{\\nu}\\right)+\\mathcal{Q}^{\\dagger}\\left|\\Lambda_{\\nu}^{(k)}\\right|_{\\infty}^{\\infty}\\right)}\\\\ &{\\quad-\\left(\\left|\\Lambda_{\\mu}^{(k)}\\right|_{\\infty}^{\\infty}+\\Lambda_{\\nu}^{(k)}\\right)\\left|\\mathbb{I}\\left|\\mathcal{R}_{\\mu}^{s}\\right|-\\left|\\Lambda_{\\nu}\\right|_{\\infty}^{\\infty}+\\mathcal{Q}^{\\dagger}\\left|\\Lambda_{\\nu}^{(k)}\\right|_{\\infty}^{\\infty}\\cdot\\left|\\Lambda_{\\nu}-\\mathcal{Q}^{\\dagger}\\right|_{\\infty}}\\\\ &{\\quad\\cdot\\left(\\mathcal{Q}^{\\dagger}\\Lambda_{\\mu}^{(k)}\\right)\\left|\\mathcal{P}_{\\mu}^{\\star}\\right|-\\Lambda_{\\mu}\\left|\\Lambda_{\\nu}\\right|_{\\infty}^{\\infty}\\right)+4\\mathcal{P}\\Lambda_{\\mu}^{(k)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the first two inequalities have the same reasons in (72). The third inequality is due to $16\\beta\\rho^{\\frac{3}{4}}B_{k}\\left(\\left\\lvert\\left\\lvert\\overline{{\\pmb{u}}}_{S^{+}}^{t}\\right\\rvert\\right\\rvert_{\\infty}^{2}+\\left\\lvert\\left\\lvert\\overline{{\\pmb{v}}}_{S^{-}}^{t}\\right\\rvert\\right\\rvert_{\\infty}^{2}\\right)\\leq\\frac{8}{5}w_{\\mathrm{max}}^{\\star}$ and $\\begin{array}{r}{\\rho^{\\frac{1}{4}}\\sqrt{s}\\delta_{\\operatorname*{max}}\\leq\\frac{\\sqrt{s}\\delta}{8}\\leq\\frac{1}{640}}\\end{array}$ due touper bound of global RIP condition in Proposition 1 with $\\begin{array}{r}{C_{\\gamma}\\leq\\frac{1}{80}}\\end{array}$ . The last inequality is due to (79). Then we can use analogized proof for hypothesis (c) to finish the proof. ", "page_idx": 32}, {"type": "text", "text": "(e) The proof is similar to hypothesis (d). $\\forall t$ that $\\overline{{T}}_{k-1}\\leq t<t+1\\leq\\overline{{T}}_{k}$ , based on one step iteration in (50) and hypothesis (a), there is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\displaystyle\\Delta_{u,S^{-}}^{t+1}\\right|_{\\infty}\\leq\\rho\\left\\|\\displaystyle\\Delta_{u,S^{-}}^{t}\\right\\|_{\\infty}\\cdot\\left(1+4\\rho\\eta\\left(\\left(\\sqrt{s}\\displaystyle{\\delta}_{\\operatorname*{max}}+1\\right)\\left(2w_{\\operatorname*{max}}^{*}+4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{+}}^{t}\\right\\|_{\\infty}B_{k}\\left(2\\left\\|\\overline{{u}}_{S^{+}}^{t}\\right\\|_{\\infty}\\right)\\right.\\right.\\right.}\\\\ &{\\left.\\left.\\left.\\left.+4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{+}}^{t}\\right\\|_{\\infty}B_{k}\\right)+4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{v}}_{S^{-}}^{t}\\right\\|_{\\infty}B_{k}\\left(2\\left\\|\\overline{{v}}_{S^{-}}^{t}\\right\\|_{\\infty}+4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{v}}_{S^{-}}^{t}\\right\\|_{\\infty}B_{k}\\right)\\right\\rangle}\\\\ &{+64d\\alpha+\\operatorname*{max}_{i=1}^{3}\\left\\|\\displaystyle\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}\\right\\|_{\\infty}\\right\\rangle\\Big)+4\\rho\\eta\\left\\|\\displaystyle\\left\\|\\displaystyle\\overline{{u}}_{S^{-}}^{t}\\right\\|_{\\infty}\\cdot\\left(\\left(\\sqrt{s}\\displaystyle{\\delta}_{\\operatorname*{max}}+1\\right)\\left(4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{+}}^{t}\\right\\|_{\\infty}\\right.\\right.}\\\\ &{\\left.\\left.\\left.\\cdot\\left(2\\left\\|\\displaystyle\\overline{{u}}_{S^{+}}^{t}\\right\\|_{\\infty}+4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{u}}_{S^{+}}^{t}\\right\\|_{\\infty}B_{k}\\right)+4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{v}}_{S^{-}}^{t}\\right\\|_{\\infty}B_{k}\\left(2\\left\\|\\overline{{v}}_{S^{-}}^{t}\\right\\|_{\\infty}\\right.\\right.}\\\\ &{\\left.\\left.\\left.+4\\beta\\rho^{\\frac{3}{4}}\\eta\\\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality and the left proof have analogized derivations in proof of hypothesis (d). ", "page_idx": 33}, {"type": "text", "text": "(f) To absorb the perturbed error $\\mathbf{\\boldsymbol{q}}_{u}^{t}$ in (48), we need to prove the fine-grained bound for consensus error in hypotheses (c), (d), (d). The idea is that we focus on the $\\forall j\\in\\bar{S}^{+}$ , then the upper bound on the consensus error on $j$ -thentry is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\Delta_{u,j}^{t+1}\\right|\\big|_{\\infty}\\leq\\rho\\left\\|\\Delta_{u,j}^{t}\\right\\|_{\\infty}\\cdot\\left(1+4\\eta\\left(\\sqrt{s}\\delta_{u,x}\\left(\\left\\|(\\overline{{u}}_{s}^{t})\\right.^{2}-w_{s}^{*})\\right\\|_{\\infty}+\\left\\|-(\\overline{{v}}_{s}^{t})^{2}-w_{s}^{*}\\right\\|_{\\infty}\\right)\\right.}\\\\ &{\\left.\\qquad+2\\left\\|\\overline{{u}}_{s}^{t}\\right\\|_{\\infty}\\left\\|\\Delta_{u,s}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{u,s}^{t}\\right\\|_{s}^{2}+2\\left\\|\\overline{{v}}_{s}^{t}\\right\\|_{\\infty}\\left\\|\\Delta_{v,x}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{v,x}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{u,s}^{t}\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\qquad+|(\\overline{{u}}_{s}^{t})^{2}-w_{s}^{*}|^{2}+2(\\overline{{u}}_{s}^{t})^{2}+3|\\overline{{u}}_{s}^{t}|\\left\\|\\Delta_{u,j}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{u,j}^{t}\\right\\|_{\\infty}^{2}+24d\\alpha+\\operatorname*{max}_{i}\\left\\|\\overline{{\\Delta}}_{u}^{T}\\boldsymbol{\\xi}_{i}\\right\\|_{\\infty}\\right)}\\\\ &{\\qquad+4\\rho|\\overline{{u}}_{s}^{t}|\\cdot\\left(\\sqrt{s}\\delta_{u,x}\\left(2\\left\\|\\overline{{u}}_{s}^{t}\\right\\|_{\\infty}\\left\\|\\Delta_{u,j}^{t}\\right\\|_{\\infty}+\\left\\|\\Delta_{u,s}^{t}\\right\\|_{\\infty}+\\left\\|\\overline{{\\Delta}}_{u,s}^{t}\\right\\|_{\\infty}^{2}+2\\left\\|\\overline{{v}}_{s}^{t}\\right\\|_{\\infty}\\left\\|\\Delta_{v,x}^{t}\\right\\|_{\\infty}\\right)}\\\\ &{\\qquad+\\left\\|\\Delta_{v,x}^{t}\\right\\|_{\\infty}^{2}+\\sqrt{s}\\left(\\delta_{u,x}+\\delta_{v}\\right)\\left(\\left\\|(\\overline{{u}}_{s}^{t})^{2}-w_{s}^{*}\\right\\|_{\\infty} \n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality is due to substituting the crude bound and analogous derivation in (72). Following the proof of crude bound would finish the proof. The proof for $\\bar{\\nabla}j^{-}\\in\\bar{S}^{-}$ and $\\forall j\\in S^{c}$ can combine this proof with proofs of hypotheses (e) and (d), respectively. The fine-grained bound for $v^{t}$ can be proved by using analogous proofs for $\\pmb{u}^{t}$ ", "page_idx": 33}, {"type": "text", "text": "(g) The proof can follow the proof of hypothesis (e) in Proposition 2 by replacing the $\\boldsymbol{S}$ With $S^{+}$ The proof of $\\overline{{\\pmb{v}}}_{S^{-}}^{t}$ is analogous to $\\overline{{\\boldsymbol{u}}}_{S^{+}}^{t}$ ", "page_idx": 33}, {"type": "text", "text": "(h) $\\forall t$ that $\\overline{{T}}_{k-1}\\leq t<t+1<\\overline{{T}}_{k}$ . First, we bound the perturbation error $\\|\\pmb{g}_{u}^{t}\\|_{\\infty}\\,,\\|\\pmb{f}_{u}^{t}\\|_{\\infty}$ in (50) induced from decentralized network ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|{\\pmb{g}_{u}^{t}}\\right\\|_{\\infty}\\leq\\left\\|{\\pmb{p}_{u}^{t}}\\right\\|_{\\infty}+3\\sqrt{s}\\delta_{\\operatorname*{max}}\\left(4\\beta\\rho^{\\frac{3}{4}}\\eta\\left\\|\\overline{{\\pmb{u}}}_{S^{+}}^{t}\\right\\|_{\\infty}B_{k}\\right)^{2}}\\\\ {\\qquad\\leq\\frac{3\\,\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+3\\right)\\rho^{\\frac{3}{4}}B_{k}}{16}+\\frac{d\\alpha\\rho^{\\frac{3}{4}}}{32}+96\\sqrt{s}\\delta_{\\operatorname*{max}}\\rho^{\\frac{3}{2}}\\beta^{2}\\eta^{2}w_{\\operatorname*{max}}^{\\star}B_{k}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\leq\\frac{\\rho^{\\frac{3}{4}}B_{k}\\left(150\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+3\\right)+3\\sqrt{\\rho}\\right)}{800}+\\frac{d\\alpha\\rho^{\\frac{3}{4}}}{32},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the second inequality is substituting the upper bound in (66) and $\\left\\|\\overline{{u}}_{S^{+}}^{t}\\right\\|_{\\infty}\\leq2w_{\\operatorname*{max}}^{\\star}$ dueto hypothesis (a). $\\forall j\\in S^{c}$ , there is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f_{j}^{t}|\\leq4\\beta\\rho^{\\frac{3}{4}}\\eta|\\overline{{u}}_{j}^{t}|B_{k}\\cdot\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}\\cdot\\left(80B_{k}+16\\beta\\rho^{\\frac{3}{4}}\\eta B_{k}\\left(\\left\\|\\overline{{u}}_{S^{+}}^{t}\\right\\|_{\\infty}^{2}+\\left\\|\\overline{{v}}_{S^{-}}^{t}\\right\\|_{\\infty}^{2}\\right)\\right)\\right.}\\\\ &{\\qquad+\\left.32d\\alpha+\\operatorname*{max}_{i}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}\\right)}\\\\ &{\\leq\\frac{\\sqrt{\\rho}B_{k}|\\overline{{u}}_{j}^{t}|}{32},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the first inequality is due to hypotheses (a), (c)-(f), (h), (g) and step size condition. The second inequality is due to the definition of $B_{k}$ ,the condition on $\\rho$ , and small initialization for $\\alpha$ ", "page_idx": 34}, {"type": "text", "text": "Then $\\pmb{f}_{u}^{t}$ $\\pmb{f}_{u}^{t}=\\pmb{r}_{f_{u}}^{t}\\odot\\pmb{\\overline{{u}}}_{S^{c}}^{t}$ where $\\left\\Vert r_{f_{u}}^{t}\\right\\Vert_{\\infty}\\leq\\frac{\\sqrt{\\rho}B_{k}}{32},$ Vthat $\\overline{{T}}_{k-1}\\leq$ $t<\\overline{{T}}_{k}$ . Then the perturbed recursion for $\\overline{{\\pmb{u}}}^{t}$ on non-support $S^{c}$ over decentralized network in (50) becomes ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left(\\overline{{\\boldsymbol u}}_{S^{c}}^{t+1}\\right)^{2}=\\left(\\overline{{\\boldsymbol u}}_{S^{c}}^{t}\\right)^{2}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(E_{2s}^{t}+E_{3s}^{t}+g_{u}^{t}+r_{f_{u}}^{t}\\right)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, we can conduct similar calculation in (70) and obtain the upper bound $\\left\\|\\boldsymbol{E}_{2s}^{t}\\right\\|_{\\infty}+\\left\\|\\boldsymbol{E}_{3s}^{t}\\right\\|_{\\infty}+$ $\\|\\pmb{{g}}_{u}^{t}\\|_{\\infty}+\\left\\|\\pmb{{r}}_{f_{u}}^{t}\\right\\|_{\\infty}\\leq B_{k}$ Based on the step size condition that $\\begin{array}{r}{\\eta\\le\\frac{c_{10}\\left(1-\\sqrt{\\rho}\\right)}{w_{\\mathrm{max}}^{\\star}}}\\end{array}$ cio(1-@,therefore there is a similar result in (35) as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|\\overline{{u}}_{S^{c}}^{t+1}\\right\\|_{\\infty}\\geq\\left\\|\\overline{{u}}_{S^{c}}^{t}\\right\\|_{\\infty}\\left(1-c_{10}\\left(1-\\sqrt{\\rho}\\right)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Based on the recursion in (78), $\\forall k$ -th induction stage, we have the following upper bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{n(\\cdot|z|)}{\\prod_{s=1}^{n-1}\\left(1+s\\eta\\Big(\\Big(k_{\\perp}^{0}\\Big)_{s=1}+\\Big\\vert k_{\\perp}^{0}\\Big\\vert_{z=1}+\\Big\\vert k_{\\perp}^{1}\\Big\\vert_{z=1}^{s}\\Big\\vert\\Big)\\right)^{2}}}\\\\ &{\\frac{(\\cdot|z|)}{\\prod_{s=1}^{n-1}\\left(1+s\\eta\\Big(k_{\\perp}^{0}\\Big)_{s=1}+\\Big\\vert k_{\\perp}^{0}\\Big\\vert_{z=1}^{s}-\\eta_{\\perp}^{0}\\Big)_{s=1}+\\Big\\vert\\left(k_{\\perp}^{0}\\right)^{2}-s_{\\perp}^{\\star}\\Big\\vert_{z=1}^{s}\\right)_{s=1}\\right)^{2}}\\\\ &{\\quad-\\frac{4}{\\eta(1+s)}\\left(\\frac{\\sqrt{1}k_{\\perp}(\\cdot|z|)\\big(\\eta(\\eta(\\eta(\\eta(\\eta(\\eta))))\\big)^{2}\\big)^{\\!-1}\\left(1+s\\eta\\Big)_{s=1}^{\\!-s}\\right)+\\frac{4}{\\eta(1+s)}\\sqrt{s_{\\perp}^{0}\\Big\\vert_{z=1}^{s}\\big\\vert_{z=1}^{s}}\\right)^{2}}\\\\ &{\\quad-\\frac{4}{\\eta(1+s)}\\left(\\frac{\\sqrt{1}k_{\\perp}(\\cdot|z|)\\big(\\eta(\\eta(\\eta)))^{2}\\big)^{\\!-1}\\left(1+s\\eta\\Big(\\frac{\\sqrt{1}k_{\\perp}\\eta(\\eta(\\eta(\\eta)))}{\\sqrt{\\eta(\\eta)}}+\\frac{\\sqrt{1}k_{\\perp}\\eta(\\eta(\\eta(\\eta)))}{\\sqrt{\\eta(\\eta)}}\\right)^{\\!-1}\\right)+\\Big(1+s\\eta\\Big)_{s=1}\\right)\\frac{n^{s}}{\\eta(1+s)}}\\\\ &{\\frac{(\\cdot|z|)}{\\eta(1+s)}\\left(1+s\\eta\\Big(\\frac{\\sqrt{1}k_{\\perp}\\eta(\\eta(\\eta(\\eta)))}{\\sqrt{\\eta(\\eta)}}+\\Big\\vert\\frac{\\sqrt{1}k_{\\perp}\\eta(\\eta(\\eta(\\eta(\\eta))))}{\\sqrt{\\eta(\\eta)}}+\\frac{\\sqrt{1}k_{\\perp}\\eta(\\eta(\\eta(\\eta)))}{\\sqrt{\\eta(\\eta)}}\\Big\\vert\\right)-\\frac{4\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $(i)$ is due to $\\forall x_{1},x_{2},\\cdot\\cdot\\cdot,x_{n}\\;\\geq\\;0$ , there is $\\textstyle1+\\sum_{i=1}^{n}x_{i}\\;\\leq\\;\\prod_{i=1}^{n}\\left(1+x_{i}\\right)$ , substituting the upper bound in (76) and definition of global RIP parameter. For $(i i)$ , we use $\\overline{{T}}_{k}\\ \\leq\\ 2T_{k}$ based on definition of $\\overline{{T}}_{k},T_{k}$ ,induction (a) with Lemma B.13 in [33], step size condition, small initialization for $\\alpha$ , definition of $B_{k}$ and refined upper bound for r r a fall \u2264 $\\begin{array}{r}{\\rho^{\\frac{3}{4}}\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}B_{k}+32d\\alpha+\\operatorname*{max}_{i}\\left\\|\\frac{\\mathbf{X}_{i}^{T}\\boldsymbol{\\xi}_{i}}{n}\\right\\|_{\\infty}\\right)\\left\\|\\overline{{\\boldsymbol{u}}}_{S^{c}}^{t}\\right\\|_{\\infty}}\\end{array}$ $(i i i)$ is because network connectivity condition that $\\begin{array}{r}{\\rho\\leq\\operatorname*{min}\\bigg\\{\\left(\\frac{\\sqrt{s}\\delta}{\\sqrt{s}\\delta_{\\operatorname*{max}}+4}\\right)^{\\frac{4}{3}},\\frac{\\left\\|\\mathbf{X}^{T}\\boldsymbol{\\xi}\\right\\|_{\\infty}}{8m\\operatorname*{max}_{i}\\left\\|\\mathbf{X}_{i}^{T}\\boldsymbol{\\xi}_{i}\\right\\|_{\\infty}}\\bigg\\}.}\\end{array}$ $(i v),(v)$ are due to condition $i\\leq K-1$ andieqgality $\\begin{array}{r}{1+\\sum_{i=1}^{K}x_{i}\\le\\prod_{i=1}^{K}\\left(1+x_{i}\\right)}\\end{array}$ that $\\begin{array}{r}{x_{1}=,\\cdot\\cdot\\cdot\\,,x_{K}=\\,\\frac{32\\eta C_{b}\\zeta}{K}}\\end{array}$ 32nCuc. (vi) is based on the definition of $K$ that $\\begin{array}{r}{\\zeta\\le\\frac{w_{\\mathrm{max}}^{\\star}}{2^{K-1}}}\\end{array}$ and condition that $4C_{b}\\leq C_{\\gamma}$ . The last inequality results from the Lemma A.2 in [33] with upper bound is set as ZKx2rex and initialization co = \u03b12,then for K,k 1og , thereis ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left(1+4\\eta\\cdot\\frac{2C_{\\gamma}w_{\\operatorname*{max}}^{\\star}}{K\\times2^{K-1}}\\right)^{t}\\leq\\frac{1}{\\alpha}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, we selec universal constant $C_{\\gamma}\\leq\\frac{1}{1280}$ such that kk- 1og \u2265 20K x TK-1, which would lead to the last inequality. ", "page_idx": 35}, {"type": "text", "text": "(i) Concentrate on the iteration by DGD algorithm on support $\\boldsymbol{S}$ , based on (57), we have the recursive formula as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathbf{u}}}_{S}^{t+1}=\\overline{{\\mathbf{u}}}_{S}^{t}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(\\left(\\left(\\overline{{\\mathbf{u}}}_{S}^{t}\\right)^{2}-\\left(\\overline{{\\mathbf{v}}}_{S}^{t}\\right)^{2}-\\mathbf{w}^{\\star}\\right)+E_{d1}^{t}+E_{d2}^{t}+E_{d3}^{t}\\right)\\right)}\\\\ &{\\qquad\\quad-\\displaystyle\\frac{4\\eta}{m}\\sum_{i=1}^{m}\\Delta_{u,S}^{t,i}\\odot E_{d4}^{t,i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the $E_{d1}^{t},E_{d2}^{t},E_{d3}^{t}$ and $E_{d4}^{t,i}$ are defned as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{d\\,1}^{t}:=\\left(\\frac{X^{T}X}{N}-I\\right)\\left(\\left(\\overline{{u_{\\delta}}}\\right)^{2}-\\left(\\overline{{v_{\\delta}}}\\right)^{2}-w^{*}\\right);}\\\\ &{E_{d\\,2}^{t}:=\\frac{X^{T}X}{N}\\left(\\left(\\overline{{u_{\\delta}}}\\right)^{2}-\\left(\\overline{{v_{\\delta}}}\\right)^{2}\\right)-\\frac{X^{T}\\xi}{N};}\\\\ &{E_{d\\,3}^{t}:=-\\frac{\\displaystyle\\sum_{i=1}^{m}\\frac{X^{T}X_{i}}{N}\\left(\\left(\\left(\\overline{{u_{\\delta}}}\\right)^{2}-\\left(u_{\\delta}^{t}\\right)^{2}+\\left(v_{\\delta}^{t}\\right)^{2}-\\left(\\overline{{v_{\\delta}}}\\right)^{2}\\right)}{\\displaystyle\\sum_{i=1}^{m}\\left(\\left(\\overline{{u_{\\delta}}}\\right)^{2}-\\left(u_{\\delta}^{t}\\right)^{2}+\\left(v_{\\delta}^{t}\\right)^{2}-\\left(\\overline{{v_{\\delta}}}\\right)^{2}\\right)}\\right.}\\\\ &{\\qquad+\\left(\\left(\\overline{{u_{\\delta}}}\\right)^{2}-\\left(\\overline{{u_{\\delta}}}\\right)^{2}+\\left(\\overline{{v_{\\delta}}}\\right)^{2}-\\left(\\overline{{v_{\\delta}}}\\right)^{2}\\right)\\right);}\\\\ &{E_{d\\,4}^{t}:=\\left(u_{\\delta}^{t}\\right)^{2}-\\left(v_{\\delta}^{t}\\right)^{2}+\\left(\\frac{X_{i}^{T}X_{i}}{n}-I\\right)\\left(\\left(u_{\\delta}^{t}\\right)^{2}-\\left(v_{\\delta}^{t}\\right)^{2}-w^{*}\\right)}\\\\ &{\\qquad+\\frac{X_{i}^{T}X_{i}}{n}\\left(\\left(u_{\\delta}^{t}\\right)^{2}-\\left(v_{\\delta}^{t}\\right)^{2}\\right)-\\frac{X_{i}^{T}\\xi}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For the negative part ${\\overline{{\\pmb{v}}}}^{t}$ , with similar decomposition in (57), there is ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\overline{{v}}}^{t+1}={\\overline{{v}}}^{t}-\\frac{\\eta}{m}\\sum_{i=1}^{m}\\nabla_{v}f_{i}\\left({\\overline{{u}}}^{t},{\\overline{{v}}}^{t}\\right)+\\frac{\\eta}{m}\\sum_{i=1}^{m}\\left(\\nabla_{v}f_{i}\\left({\\overline{{u}}}^{t},{\\overline{{v}}}^{t}\\right)-\\nabla_{v}f_{i}\\left({u}^{t,i},{\\overline{{v}}}^{t}\\right)\\right)}\\ ~}\\\\ {{\\displaystyle~~~~~~~+\\frac{\\eta}{m}\\sum_{i=1}^{m}\\left(\\nabla_{v}f_{i}\\left({u}^{t,i},{\\overline{{v}}}^{t}\\right)-\\nabla_{v}f_{i}\\left({u}^{t,i},{v}^{t,i}\\right)\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "After tedious calculation, its formula is as follows ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\overline{{v}}_{S}^{t+1}=\\overline{{v}}_{S}^{t}\\odot\\left(\\mathbf{1}_{d}+4\\eta\\left(\\left(\\left(\\overline{{u}}_{S}^{t}\\right)^{2}-\\left(\\overline{{v}}_{S}^{t}\\right)^{2}-w^{\\star}\\right)+E_{d1}^{t}+E_{d2}^{t}+E_{d3}^{t}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n+\\;\\frac{4\\eta}{m}\\sum_{i=1}^{m}\\Delta_{v,S}^{t,i}\\odot E_{d4}^{t,i}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Basedndnddcaeara $\\Delta_{u,S}^{t,i}$ and $\\pmb{\\Delta}_{\\pmb{v},S}^{t,i}$ $\\pmb{\\Delta}_{\\pmb{u},S}^{t,i}=\\pmb{r}_{\\pmb{u},S}^{t,i}\\odot\\pmb{\\overline{{u}}}_{S}^{t}$ u,s = ru,s O u's, where max us}\u22644\u03b2pnBk forTk-1 \u2264t<Tk,Thus, the (82) and (85) can be reformulated as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\overline{{{u}}}_{S}^{t+1}\\Big)^{2}=\\big(\\overline{{{u}}}_{S}^{t}\\big)^{2}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(\\left(\\left(\\overline{{{u}}}_{S}^{t}\\right)^{2}-\\left(\\overline{{{v}}}_{S}^{t}\\right)^{2}-w^{\\star}\\right)+E_{d1}^{t}+E_{d2}^{t}+E_{d3}^{t}\\right)-4\\eta\\frac{\\sum_{i=1}^{m}r_{u,S}^{t,i}\\odot_{\\overline{{{\\mathbf{k}}}}}}{m}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\langle\\overline{{v}}_{S}^{t+1}\\right\\rangle^{2}=\\left(\\overline{{v}}_{S}^{t}\\right)^{2}\\odot\\left(\\mathbf{1}_{d}+4\\eta\\left(\\left(\\left(\\overline{{u}}_{S}^{t}\\right)^{2}-\\left(\\overline{{v}}_{S}^{t}\\right)^{2}-w^{\\star}\\right)+E_{d1}^{t}+E_{d2}^{t}+E_{d3}^{t}\\right)+4\\eta\\frac{\\sum_{i=1}^{m}r_{v,S}^{t,i}\\odot_{i,j}^{t}}{m}\\right)\\times\\left(\\overline{{v}}_{S}^{t}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Based on hypotheses (a), (c)-(f),(j) and step size condition, we have the following bound for additional perturbed error terms ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\left\\{\\left\\|r_{u,s}^{t,i}\\odot E_{d_{4}}^{t,i}\\right\\|_{\\infty},\\left\\|r_{v,S}^{t,i}\\odot E_{d_{4}}^{t,i}\\right\\|_{\\infty}\\right\\}\\le4\\beta\\rho^{3}\\eta B_{k}\\left(4\\left(\\left\\|\\overline{{u}}_{S}^{t}\\right\\|_{\\infty}^{2}+\\left\\|\\overline{{v}}_{S}^{t}\\right\\|_{\\infty}^{2}\\right)\\right.}\\\\ &{\\quad\\left.+2\\sqrt{s}\\delta_{\\operatorname*{max}}\\left(w_{\\operatorname*{max}}^{*}+\\left\\|\\overline{{u}}_{S}^{t}\\right\\|_{\\infty}^{2}+\\left\\|\\overline{{v}}_{S}^{t}\\right\\|_{\\infty}^{2}\\right)+16d\\alpha+\\operatorname*{max}_{i}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}\\right)}\\\\ &{\\quad\\le4\\beta\\sqrt{\\rho}\\eta B_{k}\\left(\\rho^{\\frac{1}{4}}\\left(16+10\\sqrt{s}\\delta_{\\operatorname*{max}}\\right)w_{\\operatorname*{max}}^{*}+16d\\alpha+\\rho^{\\frac{1}{4}}\\operatorname*{max}_{i}\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}\\right)}\\\\ &{\\quad\\le40\\beta\\sqrt{\\rho}\\eta\\cdot\\frac{w_{\\operatorname*{max}}^{*}}{40\\times2^{k}}\\cdot\\rho^{\\frac{1}{4}}(2+\\sqrt{s}\\delta_{\\operatorname*{max}})w_{\\operatorname*{max}}^{*}}\\\\ &{\\quad\\le\\frac{C_{\\gamma}w_{\\operatorname*{max}}^{*}}{K\\times2^{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last inequality is due to network connectivity condition that $\\rho\\quad\\leq$ $\\begin{array}{r}{\\operatorname*{min}\\Bigg\\{\\left(\\frac{\\sqrt{s}\\delta}{\\sqrt{s}\\delta_{\\operatorname*{max}}+4}\\right)^{\\frac{4}{3}},\\frac{\\^{\\cdot}\\|\\dot{\\mathbf{X}}^{T}\\boldsymbol{\\xi}\\|_{\\infty}}{8m\\operatorname*{max}_{i}\\left\\|\\mathbf{X}_{i}^{T}\\boldsymbol{\\xi}_{i}\\right\\|_{\\infty}}\\Bigg\\}.}\\end{array}$ ", "page_idx": 36}, {"type": "text", "text": "Then multiplying (86) and (87), we can obtain that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left(\\overline{{u}}_{S}^{t+1}\\odot\\overline{{v}}_{S}^{t+1}\\right)^{2}\\le\\left(\\overline{{u}}_{S}^{t}\\odot\\overline{{v}}_{S}^{t}\\right)^{2}\\odot\\left(1-\\left(4\\eta\\left(\\left(\\overline{{u}}_{S}^{t}\\right)^{2}-\\left(\\overline{{v}}_{S}^{t}\\right)^{2}-w^{*}+E_{d1}^{t}+E_{d2}^{t}+E_{d3}^{t}\\right)\\right)^{2}\\right.}\\\\ &{}&{\\left.+8\\eta\\operatorname*{max}\\left\\{\\left\\|r_{u,S}^{t}\\odot E_{d4}^{t}\\right\\|_{\\infty},\\left\\|r_{\\mathbf{v},S}^{t,i}\\odot E_{d4}^{t,i}\\right\\|_{\\infty}\\right\\}}\\\\ &{}&{+\\left(4\\eta\\operatorname*{max}\\left\\{\\left\\|r_{u,S}^{t,i}\\odot E_{d4}^{t,i}\\right\\|_{\\infty},\\left\\|r_{v,S}^{t,i}\\odot E_{d4}^{t,i}\\right\\|_{\\infty}\\right\\}\\right)^{2}\\right)^{2}}\\\\ &{}&{\\stackrel{(i)}{\\le}\\left(\\overline{{u}}_{S}^{t}\\odot\\overline{{v}}_{S}^{t}\\right)^{2}\\odot\\left(1+4\\eta\\cdot\\frac{C_{\\gamma}w_{\\operatorname*{max}}^{*}}{K\\times2^{k}}\\right)^{4}}\\\\ &{}&{\\stackrel{(i i)}{\\le}\\alpha^{4}\\odot\\left(1+4\\eta\\cdot\\frac{C_{\\gamma}}{K}2^{-K+1}w_{\\operatorname*{max}}^{*}\\right)^{8K T\\kappa-1}}\\\\ &{}&{\\stackrel{(i i i)}{\\le}\\alpha^{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "wherethe $(i)$ is due to (88) and $4\\eta\\left\\|\\left(\\overline{{\\mathbf{u}}}_{S}^{t}\\right)^{2}-\\left(\\overline{{\\mathbf{v}}}_{S}^{t}\\right)^{2}-\\mathbf{w}^{\\star}+\\mathbf{{\\calE}}_{d1}^{t}+\\mathbf{{\\calE}}_{d2}^{t}+\\mathbf{{\\calE}}_{d3}^{t}\\right\\|_{\\infty}\\leq1$ The $(i i)$ is due to induction (a) and Lemma B.13 in [33]. The reason for $(i i i)$ is the same as the that of last inequality in (80) because of the setting of step size. ", "page_idx": 36}, {"type": "text", "text": "(j) Recall the recursive formula of $\\overline{{\\pmb{u}}}_{S^{-}}^{t}$ in (49) and compare the definition of the perturbation $\\pmb{y}_{u}^{t}$ with perturbations $\\pmb{p}_{u}^{t},\\pmb{g}_{u}^{t}$ we can obtain $\\begin{array}{r}{\\|y_{u}^{t}\\|_{\\infty}\\leq\\frac{150\\rho^{\\frac{3}{4}}B_{k}\\left(\\sqrt{s}\\delta_{\\operatorname*{max}}+4\\right)}{600}+\\frac{\\rho^{\\frac{3}{4}}d\\alpha}{32}.}\\end{array}$ Compare the outer perturbation $z_{u}^{t}$ Wwith outerpertrbations $\\pmb{f}_{u}^{t},\\pmb{q}_{u}^{t}$ , we can obtain $\\|z_{u}^{t}\\|_{\\infty}\\,\\leq\\,\\frac{\\sqrt{\\rho}B_{k}\\left\\|\\overline{{\\mathbf{u}}}_{s-}^{t}\\right\\|_{\\infty}}{16}$ With similar reparameterization as $\\pmb{z}_{u}^{t}=\\pmb{r}_{z_{u}}^{t}\\odot\\overline{{\\pmb{u}}}_{S-}^{t}$ , the (49) would become ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left(\\overline{{u}}_{S^{-}}^{t+1}\\right)^{2}=\\left(\\overline{{u}}_{S^{-}}^{t}\\right)^{2}\\odot\\left(\\mathbf{1}_{d}-4\\eta\\left(-w_{S^{-}}^{\\star}-\\left(\\overline{{v}}_{S^{-}}^{t}\\right)^{2}+E_{2s}^{t}+E_{3s}^{t}+y_{u}^{t}+r_{z_{u}}^{t}\\right)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "$\\forall j\\in S^{-}$ and $\\begin{array}{r}{\\forall t\\leq\\mathcal{O}\\left(\\frac{1}{\\eta\\zeta}\\log\\frac{1}{\\alpha}\\right)}\\end{array}$ Let $0\\leq\\tau\\leq t$ be thelargest $\\tau$ such hat $\\left(\\overline{{v}}_{j}^{\\tau}\\right)^{2}>-w_{j}^{\\star}$ If there is no such $\\tau$ exists or $\\tau=t$ , then ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\overline{{u}}_{j}^{t}\\right)^{2}\\leq\\left(\\overline{{u}}_{j}^{t-1}\\right)^{2}\\left(1+4\\eta\\left(\\left\\Vert E_{2s}^{t-1}\\right\\Vert_{\\infty}+\\left\\Vert E_{3s}^{t-1}\\right\\Vert+\\left\\Vert y_{s}^{t-1}\\right\\Vert_{\\infty}+\\left\\Vert r_{z_{s}}^{t-1}\\right\\Vert_{\\infty}\\right)\\right)^{2}}\\\\ &{\\qquad\\leq\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last inequality is due to the similar bound in (80). ", "page_idx": 37}, {"type": "text", "text": "If $\\tau<t$ , then unrolling (90) to $\\tau$ -th iteration would have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\overline{{u}}_{j}^{t}\\right)^{2}=\\left(\\overline{{u}}_{j}^{t}\\right)^{t-1}\\Bigg[1-4\\eta\\left(-w_{j}^{\\star}-\\left(\\overline{{v}}_{j}^{t}\\right)^{2}+E_{2s,j}^{i}+E_{3s,j}^{i}+y_{s,j}^{t}+r_{z,j}^{i}\\right)\\Bigg)^{2}}\\\\ &{\\overset{(i)}{\\leq}\\frac{\\alpha^{2}}{4}\\left(1-4\\eta\\left(-w_{j}^{\\star}-\\left(\\overline{{v}}_{j}^{t}\\right)^{2}+E_{2s,j}^{\\top}+E_{3s,j}^{\\top}+y_{s,j}^{\\top}+r_{z,j}^{\\top}\\right)\\right)^{2}}\\\\ &{\\quad\\quad\\cdot\\displaystyle\\prod_{i=\\tau+1}^{t-1}\\left(1-4\\eta\\left(-w_{j}^{\\star}-\\left(\\overline{{v}}_{j}^{t}\\right)^{2}+E_{2s,j}^{i}+E_{3s,j}^{i}+y_{s,j}^{t}+r_{z,j}^{i}\\right)\\right)^{2}}\\\\ &{\\overset{(i i)}{\\leq}\\alpha^{2}\\displaystyle\\prod_{i=\\tau+1}^{t-1}\\left(1+4\\eta\\left(\\left\\|E_{2s}^{i}\\right\\|_{\\infty}+\\left\\|E_{3s}^{i}\\right\\|+\\left\\|y_{s}^{i}\\right\\|_{\\infty}+\\left\\|r_{z,s}^{i}\\right\\|_{\\infty}\\right)\\right)^{2}}\\\\ &{\\overset{(i i i)}{\\leq}\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $(i)$ is based on condition for initialization that $\\alpha\\ \\ \\leq\\ \\ {\\frac{w_{\\operatorname*{min}}^{\\star}}{4}}$ and induction (h) that $\\begin{array}{r l r}{\\overline{{u}}_{j}^{\\tau}}&{\\le}&{\\frac{\\alpha^{\\frac{3}{2}}}{\\overline{{v}}_{j}^{\\tau}}\\;\\;\\le\\;\\;\\frac{\\alpha^{\\frac{3}{2}}}{\\sqrt{-w_{j}^{\\star}}}\\;\\;\\le\\;\\;\\frac{\\alpha}{2}}\\end{array}$ $(i i)$ is based on the induction (a), step size condition that $\\left(1-4\\eta\\left(-w_{j}^{\\star}-\\left(\\overline{{v}}_{j}^{\\tau}\\right)^{2}+E_{2s,j}^{\\tau}+E_{3s,j}^{\\tau}+y_{s,j}^{\\tau}+r_{z_{s},j}^{\\tau}\\right)\\right)^{2}\\,\\leq\\,4$ and $\\forall i>\\tau$ that $\\left(\\overline{{v}}_{j}^{i}\\right)^{2}\\;<\\;-w_{j}^{\\star}$ which is based on definition of $\\tau$ . The last inequality has the same reason as (91). ", "page_idx": 37}, {"type": "text", "text": "A.5 Proof of Main Results ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "A.5.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Based on the proof of hypothesis (a) in Proposition 3, we can conclude that $\\forall t\\:\\geq\\:\\overline{{T}}_{K}$ , there would be max $\\left\\{\\left\\|\\left(\\overline{{\\pmb{u}}}_{S^{+}}^{\\overline{{T}}_{k-1}}\\right)^{2}-\\pmb{w}_{S^{+}}^{\\star}$ $\\begin{array}{r}{\\left\\|\\left(\\dot{\\overline{{v}}}_{S^{-}}^{\\overline{{T}}_{k-1}}\\right)^{2}-w_{S^{-}}^{\\star}\\right\\|_{\\infty}\\right\\}\\,\\leq\\,\\frac{w_{\\operatorname*{max}}^{\\star}}{2^{K}}\\,\\leq\\,\\zeta}\\end{array}$ \"\\* \u2264 , where the last inequality is due to the definition of $K$ The total computational complexity is the value of $\\overline{{T}}_{K-1}=$ $\\begin{array}{r}{\\frac{2^{K}-1}{\\eta w_{\\mathrm{max}}^{\\star}}\\log\\frac{1}{\\alpha^{4}}=\\mathcal{O}\\left(\\frac{1}{\\eta\\zeta}\\log\\frac{1}{\\alpha}\\right)\\!.}\\end{array}$ ", "page_idx": 37}, {"type": "text", "text": "Due to the definition of $\\zeta$ in the Theorem 1, if $\\zeta\\geq w_{\\mathrm{max}}^{\\star}$ , then our result holds at $t=0$ based on the small initialization condition. When $\\zeta\\leq w_{\\mathrm{max}}^{\\star}$ we consider the two cases where the first case is the magnitudesof paramter isstrong nughtha $\\begin{array}{r}{\\zeta=\\frac{1}{5}w_{\\mathrm{min}}^{\\star}\\geq960\\varsigma.}\\end{array}$ where the value 960 is due to under the condition $\\begin{array}{r}{2C_{b}+C_{\\gamma}\\leq\\frac{1}{80}}\\end{array}$ \uff0c $4C_{b}\\leq C_{\\gamma}$ , we set $\\begin{array}{r}{\\bar{C}_{b}=\\frac{1}{480},C_{\\gamma}=\\frac{1}{120}}\\end{array}$ we can use run DGD $T_{1}^{\\prime}$ erations to obtain the estimator max $\\left\\{\\left\\|\\left(\\overline{{\\pmb{u}}}_{S^{+}}^{\\overline{{T}}_{k-1}}\\right)^{2}-\\pmb{w}_{S^{+}}^{\\star}\\right.\\right.$ $\\begin{array}{r}{\\left\\|\\left(\\overline{{\\boldsymbol{v}}}_{S^{-}}^{\\overline{{T}}_{k-1}}\\right)^{2}-\\boldsymbol{w}_{S^{-}}^{\\star}\\right\\|_{\\infty}\\right\\}\\leq\\frac{\\boldsymbol{w}_{\\operatorname*{max}}^{\\star}}{2^{K}}\\leq}\\end{array}$ $\\begin{array}{r}{\\frac{1}{5}w_{\\mathrm{min}}^{\\star}}\\end{array}$ $\\frac{45}{32\\eta w_{\\mathrm{min}}^{\\star}}\\log\\frac{w_{\\mathrm{min}}^{\\star}}{\\epsilon}$ wmin iterations to obtain the dimension-independent bound $|\\overline{{w}}_{j}^{t}-w_{j}^{\\star}|\\leq\\operatorname*{max}\\left\\{\\sqrt{s}\\delta\\operatorname*{max}_{i\\in\\mathcal{S}}\\overbrace{B_{i}}^{\\cdots\\cdots},B_{j},\\epsilon\\right\\},\\forall j\\in$ $\\boldsymbol{S}$ where the $\\begin{array}{r l r}{B_{j}}&{:=}&{\\left\\|\\frac{{\\bf X}^{T}{\\pmb\\xi}}{N}\\odot{\\bf1}_{j}\\right\\|_{\\infty}}\\end{array}$ The total iterations are $\\begin{array}{r l}{\\overline{{T}}_{K-1}\\:+\\:\\frac{45}{32\\eta w_{\\operatorname*{min}}^{\\star}}\\log\\frac{w_{\\operatorname*{min}}^{\\star}}{\\epsilon}}&{\\leq}\\end{array}$ $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{1}{\\eta w_{\\mathrm{min}}^{\\star}}\\log\\frac{1}{\\alpha}\\right)=\\mathcal{O}\\left(\\frac{1}{\\eta\\zeta}\\log\\frac{1}{\\alpha}\\right)}\\end{array}$ where te fist inequalityis due to the condition of $\\alpha$ ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "The second case is $\\zeta=960\\zeta\\geq\\frac{1}{5}w_{\\mathrm{min}}^{\\star}$ , then running of DGD with $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{1}{\\eta\\zeta}\\right)}\\end{array}$ iterations would obtain the result in Theorem 1. ", "page_idx": 38}, {"type": "text", "text": "A.5.2 Proof of Corollary 1 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Because the entries of global design matrix generated from i.i.d. 1-Sub-Gaussian distribution, we have the upper bound of global RIP parameter as $\\begin{array}{r}{\\delta\\lesssim\\frac{1}{\\sqrt{s}}}\\end{array}$ probability at least $\\textstyle1\\,-\\,{\\frac{1}{8d^{3}}}$ gd, which is based on the Lemma 1 and sample size lower bound in Corollary 1 matches the condition of sample complexity in Lemma 1. For the local design matrix $\\left\\{X_{i}/{\\dot{\\sqrt{n}}}\\right\\}_{i=1}^{m}$ , local sample size satisfies $\\begin{array}{r}{n=\\frac{N}{m}\\gtrsim\\left(\\sqrt{\\frac{m}{\\delta}}\\right)^{-2}\\left(s\\ln\\frac{e d}{s}+\\ln(d n)\\right)}\\end{array}$ , we can bound the local RIP parameter as $\\delta_{\\mathrm{max}}\\leq\\sqrt{\\frac{m}{\\delta}}$ with probability as least $\\textstyle1-{\\frac{1}{8d^{3}}}$ based on the union bound and Lemma 1. Based on condition in (6), the $\\rho\\lesssim{\\frac{1}{m^{4}}}$ holds with probabilityat least $1-\\textstyle{\\frac{1}{4d^{3}}}$ . Based on the Lemma 4, we have the upper bound for the noise level as $\\left\\|{\\frac{X^{T}\\pmb{\\xi}}{N}}\\right\\|_{\\infty}\\lesssim\\sigma{\\sqrt{\\frac{\\log d}{N}}}$ with probability t east $\\begin{array}{r}{1-\\frac{1}{8d^{3}}}\\end{array}$ Thus, with probability at least $\\begin{array}{r}{1-\\frac{3}{8d^{3}}}\\end{array}$ ,we have $\\begin{array}{r}{\\left\\|\\overline{{\\boldsymbol{w}}}^{t}-\\boldsymbol{w}^{\\star}\\right\\|_{2}^{2}\\lesssim s\\epsilon^{2}+\\frac{d\\epsilon^{2}}{(2d+1)^{2}}\\lesssim\\frac{s\\sigma^{2}\\log d}{N}}\\end{array}$ \u2264 s\u00b2log d where in the last inequality we select $\\begin{array}{r}{\\epsilon=4\\sigma\\sqrt{\\frac{2\\log(2d)}{N}}}\\end{array}$ 2log(2d) in Theorem 1. ", "page_idx": 38}, {"type": "text", "text": "A.5.3 Proof of Proposition 1 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Because each agent has same initialization ${\\pmb u}^{0,i}={\\pmb v}^{0,i}=\\alpha{\\bf1}_{d},\\forall i\\in[m]$ , after one step of local gradient descent from (3) and (4), there are ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{u}^{1,i}=\\alpha\\odot\\left(1+4\\eta\\left(\\pmb{w}^{\\star}+\\left(\\frac{\\mathbf{X}_{i}^{T}\\mathbf{X}_{i}}{n}-I\\right)\\pmb{w}^{\\star}+\\frac{\\mathbf{X}_{i}^{T}\\pmb{\\xi}_{i}}{n}\\right)\\right)}\\\\ &{\\pmb{v}^{1,i}=\\alpha\\odot\\left(1-4\\eta\\left(\\pmb{w}^{\\star}+\\left(\\frac{\\mathbf{X}_{i}^{T}\\mathbf{X}_{i}}{n}-I\\right)\\pmb{w}^{\\star}+\\frac{\\mathbf{X}_{i}^{T}\\pmb{\\xi}_{i}}{n}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Because the perturbed error bound ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\frac{X_{i}^{T}X_{i}}{n}-I\\right)\\mathbf{w}^{\\star}\\right\\|_{\\infty}+\\left\\|\\frac{X_{i}^{T}\\xi_{i}}{n}\\right\\|_{\\infty}\\lesssim\\phi:=\\sqrt{s}\\delta_{\\operatorname*{max}}w_{\\operatorname*{max}}+\\delta\\sqrt{\\frac{\\log d}{n}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "holds with probability at least $\\textstyle1\\,-\\,{\\frac{3}{8d^{3}}}$ &a based on Lemma 2 and Lemma 4, we denote vi = $\\begin{array}{r}{\\left(\\frac{{\\bf{X}}_{i}^{T}{\\bf{X}}_{i}}{n}-{\\bf{I}}\\right){\\bf{w}}^{\\star}+\\frac{{\\bf{X}}_{i}^{T}{\\pmb{\\xi}}_{i}}{n}}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "Consider $\\pmb{u}^{1,i}$ , for $\\forall p\\in S^{+},\\forall q\\in S^{c}$ , there is $w_{p}^{\\star}-|\\nu_{p}^{i}|>\\phi>|\\nu_{q}^{i}|>0$ based on the condition in Proposition 1. In addition, $\\forall j\\in S^{-}$ , there is $-|w_{j}^{\\star}|+|\\nu_{j}|<0$ based on the the condition on Proposition 1. Thus, the growth of elements on positive support $S^{+}$ would be larger than these of $S^{-},S^{c}$ , and the ${\\mathrm{Trun}}_{k}$ operator would identify the $S^{+}$ . The analogous analysis could also applied to ${\\pmb v}^{1,i}$ that the ${\\mathrm{Trun}}_{k}$ operator wouldidentify the $S^{-}$ . Because each agent can identify the $S^{+}$ and $S^{-}$ and based on results in Proposition 3, the ${\\mathrm{Trun}}_{k}$ would also obtain the optimal statistical error with probability at least $\\begin{array}{r}{1-\\frac{3}{8d^{3}}}\\end{array}$ \u53e3 8d3 . ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that infuence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental materiai, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: ", "page_idx": 40}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that paper does not include experiments requiring code. ", "page_idx": 40}, {"type": "text", "text": "\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips. cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g.,if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generateDeepfakesfaster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n: If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 'The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset'screators. ", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets. ", "page_idx": 43}, {"type": "text", "text": "\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer:[NA] Justification: ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}]