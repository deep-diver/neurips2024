[{"figure_path": "MlADRQI0Wf/figures/figures_7_1.jpg", "caption": "Figure 1: Dynamics of averaged variables and consensus errors.", "description": "This figure visualizes the dynamics of averaged variables (w, u, v) and consensus errors (ut,i - ut, vt,i - vt) in DGD.  Panel (a) shows the convergence of the averaged w, demonstrating successful convergence of elements on support S and maintenance of small magnitudes for elements on non-support Sc. Panels (b) and (c) illustrate how DGD utilizes u and v to fit parameters on positive and negative support, respectively.  Panels (d) and (e) show the consensus errors, which correspond to the magnitudes of the model parameters, affirming the validity of the analysis. ", "section": "6.1 Simulations on DGD"}, {"figure_path": "MlADRQI0Wf/figures/figures_7_2.jpg", "caption": "Figure 2: Impact of ambient dimension d and initialization \u03b1.", "description": "This figure shows the effects of ambient dimension (d) and initialization scale (\u03b1) on the performance of the Decentralized Gradient Descent (DGD) algorithm.  Subfigure (a) and (b) illustrate how DGD achieves optimal statistical error across varying dimensions (d = 400, 4000, 40000) and network connectivity (\u03c1 = 0.1778 and \u03c1 = 0.7519). The plots display the log of the error (||w<sub>t</sub> - w*||<sub>2</sub>) versus the number of iterations. The dashed lines represent the optimal statistical error obtained in a centralized setting. Subfigure (c) demonstrates the influence of different initialization scales (\u03b1 = 10<sup>-1</sup> to 10<sup>-5</sup>) on the performance, showing that a small initialization is crucial for achieving optimal results.", "section": "6.1 Simulations on DGD"}, {"figure_path": "MlADRQI0Wf/figures/figures_8_1.jpg", "caption": "Figure 3: (a) \u03c1 = 0.9400; (b) \u03c1 = 0.1778; (c) m = 10.", "description": "This figure shows the impact of network connectivity (\u03c1) and the number of agents (m) on the performance of the decentralized gradient descent (DGD) algorithm.  Subfigure (a) shows the performance for a poorly connected network (\u03c1 = 0.9400) with varying numbers of agents (m = 10, 50, 100) compared to the centralized setting. Subfigure (b) shows the performance for a well-connected network (\u03c1 = 0.1778) with varying numbers of agents. Subfigure (c) shows the performance for a fixed number of agents (m = 10) with varying network connectivity. The y-axis represents the log of the error between the decentralized solution and the true solution. The x-axis represents the number of iterations.", "section": "6.1 Simulations on DGD"}, {"figure_path": "MlADRQI0Wf/figures/figures_8_2.jpg", "caption": "Figure 1: Dynamics of averaged variables and consensus errors.", "description": "This figure visualizes the dynamics of averaged variables and consensus errors in the decentralized gradient descent (DGD) algorithm. Subfigures (a) to (c) show the convergence of averaged variables (w, u, v), demonstrating successful convergence for elements on the support set S and maintenance of small magnitudes for those on the non-support set Sc. Subfigures (d) and (e) illustrate the consensus errors (ut,i \u2212 \u016bt, vt,i \u2212 \u016bt), showing trends corresponding to model parameter magnitudes, validating the analysis.  The plots demonstrate how DGD effectively identifies the support of the sparse model parameter while controlling consensus errors.", "section": "6.1 Simulations on DGD"}, {"figure_path": "MlADRQI0Wf/figures/figures_9_1.jpg", "caption": "Figure 5: Comparison with decentralized sparse solvers under varying communication network. The setting is d = 1000, k = 5, m = 50, N = 280, \u03c3 = 0.5 and magnitude of sparse signal is 10.", "description": "This figure compares the performance of four different decentralized sparse regression solvers under varying network connectivity conditions.  The solvers are CTA-DGD (LASSO), ATC-DGD (LASSO), DGT (NetLASSO), and the proposed method. The x-axis represents the number of iterations, and the y-axis represents the log of the l2 error between the estimated sparse vector and the ground truth sparse vector.  The figure shows that the proposed method outperforms the others, especially in scenarios with poor network connectivity.", "section": "6.3 Comparison with explicit regularization"}, {"figure_path": "MlADRQI0Wf/figures/figures_9_2.jpg", "caption": "Figure 6: Truncated version: comparison with truncated decentralized sparse solvers. The setting is d = 1000, s = 5, m = 50, N = 550, \u03c3 = 0.1, p = 0.2458 and magnitude of sparse signal is 10.", "description": "This figure compares the performance of the proposed Truncated Decentralized Gradient Descent (T-DGD) method with three other truncated decentralized methods for solving the decentralized sparse regression problem.  The setting involves a high-dimensional problem (d=1000) with a relatively small number of non-zero elements (s=5), a moderate number of agents (m=50), a sample size (N=550), low noise (\u03c3=0.1), moderate network connectivity (p=0.2458), and a signal-to-noise ratio high enough to ensure successful recovery by T-DGD.  The results demonstrate the superiority of the proposed T-DGD method over the other methods in achieving successful recovery.", "section": "6.3 Comparison with explicit regularization"}]