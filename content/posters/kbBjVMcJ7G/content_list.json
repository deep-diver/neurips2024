[{"type": "text", "text": "Operator World Models for Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pietro Novelli Istituto Italiano di Tecnologia pietro.novelli@iit.it ", "page_idx": 0}, {"type": "text", "text": "Marco Prattic\u00f2 Istituto Italiano di Tecnologia marco.prattico@iit.it ", "page_idx": 0}, {"type": "text", "text": "Massimiliano Pontil ", "page_idx": 0}, {"type": "text", "text": "Carlo Ciliberto AI Centre, University College London c.ciliberto@ucl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Istituto Italiano di Tecnologia AI Centre, University College London massimiliano.pontil@iit.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Policy Mirror Descent (PMD) is a powerful and theoretically sound methodology for sequential decision-making. However, it is not directly applicable to Reinforcement Learning (RL) due to the inaccessibility of explicit action-value functions. We address this challenge by introducing a novel approach based on learning a world model of the environment using conditional mean embeddings. Leveraging tools from operator theory we derive a closed-form expression of the action-value function in terms of the world model via simple matrix operations. Combining these estimators with PMD leads to POWR, a new RL algorithm for which we prove convergence rates to the global optimum. Preliminary experiments in finite and infinite state settings support the effectiveness of our method1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, Reinforcement Learning (RL) [1] has seen significant progress, with methods capable of tackling challenging applications such as robotic manipulation [2], playing Go [3] or Atari games [4] and resource management [5] to name but a few. The central challenge in RL settings is to balance the trade-off between exploration and exploitation, namely to improve upon previous policies while gathering sufficient information about the environment dynamics. Several strategies have been proposed to tackle this issue, such as Q-learning-based methods [4], policy optimization [6, 7] or actor-critics [8] to name a few. In contrast, when full information about the environment is available, sequential decision-making methods need only to focus on exploitation. Here, strategies such as policy improvement or policy iteration [9] have been thoroughly studied from both the algorithmic and theoretical standpoints. Within this context, the understanding of Policy Mirror Descent (PMD) methods has recently enjoyed a significant step forward, with results guaranteeing convergence to a global optimum with associated rates [10, 11, 12]. ", "page_idx": 0}, {"type": "text", "text": "In their original formulation, PMD methods require explicit knowledge of the action-value functions for all policies generated during the optimization process. This is clearly inaccessible in RL applications. Recently, [12] showed how PMD convergence rates can be extended to settings in which inexact estimators of the action-value function are used (see [13] for a similar result from a regret-based perspective). The resulting convergence rates, however, depend on uniform norm bounds on the approximation error, usually guaranteed only under unrealistic and inefficient assumptions such as the availability of a (perfect) simulator to be queried on arbitrary state-action pairs. Moreover, these strategies require repeating this sampling/learning process for any policy generated by the PMD algorithm, which is computationally expensive and demands numerous interactions with the environment. A natural question, therefore, is whether PMD approaches can be efficiently deployed in RL settings while enjoying the same strong theoretical guarantees. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we address these issues by proposing a novel approach to estimating the action-value function. Unlike previous methods that directly approximate the action-value function from samples, we first learn the transition operator and reward function associated with the Markov decision process (MDP). To model the transition operator, we adopt the Conditional Mean Embedding (CME) framework [14, 15]. We then leverage an operatorial characterization of the action-value function to express it in terms of these estimated quantities. This strategy draws a peculiar connection with world model methods and can be interpreted as world model learning via CMEs. The notion of world models for RL has been popularized by Ha and Schmidhuber in [16] distilling ideas from the early nineties [17, 18]. Traditional world model methods such as [16, 19] emphasize learning an implicit model of the environment in the form of a simulator. The simulator can be sampled directly in the latent representation space, which is usually of moderate dimension, resulting in a compressed and high-throughput model of the environment. This approach, however, requires extensive sampling for application to PMD and incurs into two sources of error in estimating the action-value function: model and sampling error. In contrast, CMEs can be used to estimate expectations without sampling and incur only in model error, for which learning bounds are available [20, 21]. One of our key results shows that by modeling the transition operator as a CME between suitable Sobolev spaces, we can compute estimates of the action-value function of any sufficiently smooth policy in closed form via efficient matrix operations. ", "page_idx": 1}, {"type": "text", "text": "Combining our estimates of the action-value function with the PMD framework we obtain a novel RL algorithm that we dub Policy mirror descent with Operator World-models for Reinforcement learning (POWR). A byproduct of adopting CMEs to model the transition operator is that we can naturally extend PMD to infinite state space settings. We leverage recent advancements in characterizing the sample complexity of CME estimators to prove convergence rates for the proposed algorithm to the global maximum of the RL Problem. Our approach is similar in spirit to [22], which proposed a value iteration strategy based on CMEs. We extend these ideas to PMD strategies and refine previous results on convergence rates. Learning the transition operator with a least-squares based estimator was also recently considered in [23], which proposed an optimistic strategy to prove near-optimal regret bounds in linear mixture MDP settings [24]. In contrast, in this work, we cast our problem within a linear MDP setting with possibly infinite latent dimension. We validate our approach on simple environments from the Gym library [25] both in finite and infinite state settings, reporting promising evidence in support of our theoretical analysis. ", "page_idx": 1}, {"type": "text", "text": "Contributions. The main contributions of this paper are: $i$ ) a CME-based world model framework, which enables us to generate estimators for the action-value function of a policy in closed form via matrix operations. ii) An (inexact) PMD algorithm combining the learned CMEs world models with mirror descent update steps to generate improved policies. iii) Showing that the algorithm is well-defined when learning the world model as an operator between a suitable family of Sobolev spaces. iv) Showing convergence rates of the proposed approach to the global maximum of the RL problem, under regularity assumptions on the MDP. $v$ ) Empirically testing the proposed approach in practice, comparing it with well-established baselines. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Formulation and Policy Mirror Descent ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a Markov Decision Process (MDP) over a state space $\\mathcal{X}$ and action space $\\boldsymbol{\\mathcal{A}}$ , with transition kernel $\\tau$ . We assume $\\mathcal{X}$ and $\\boldsymbol{\\mathcal{A}}$ to be Polish, $\\tau:\\Omega\\rightarrow\\mathcal{P}(\\mathcal{X})$ to be a Borel measurable function from the joint space $\\Omega={\\mathcal{X}}\\times{\\mathcal{A}}$ to the space $\\mathcal{P}(\\mathcal{X})$ of Borel probability measures on $\\mathcal{X}$ We define a policy to be a Borel measurable function $\\pi:{\\dot{\\mathcal{X}}}\\to{\\mathcal{P}}({\\mathcal{A}})$ . When $\\boldsymbol{\\mathcal{A}}$ (respectively $\\mathcal{X}$ ) is a finite set, the space $\\mathcal{P}(A)=\\Delta(A)\\subseteq\\mathbb{R}^{|A|}$ (respectively $\\mathcal{P}(\\boldsymbol{\\mathcal{X}})=\\Delta(\\boldsymbol{\\mathcal{X}})\\subseteq\\mathbb{R}^{|\\boldsymbol{X}|})$ corresponds to the probability simplex. Given a discount factor $\\gamma>0$ , an initial state distribution $\\nu\\in\\mathcal{P}(\\mathcal{X})$ and a Borel measurable bounded and non-negative rewar $l^{2}$ function $r:\\Omega\\to\\mathbb{R}$ we denote by ", "page_idx": 1}, {"type": "equation", "text": "$$\nJ(\\pi)=\\mathbb{E}_{\\nu,\\pi,\\tau}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(X_{t},A_{t})\\right]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "the (discounted) expected return of the policy $\\pi$ applied to the MDP, yielding the Markov process $(X_{t},A_{t})_{t\\in\\mathbb{N}}$ , where $X_{0}$ is distributed according to $\\nu$ and for each $t\\in\\mathbb N$ the action $A_{t}$ is distributed according to $\\pi(\\cdot|X_{t})$ and $X_{t+1}$ according to $\\tau(\\cdot|X_{t},A_{t})$ . ", "page_idx": 2}, {"type": "text", "text": "In sequential decision settings, the goal is to find the optimal policy $\\pi_{*}$ maximizing (1) over the space of all measurable policies. In reinforcement learning, one typically assumes that knowledge of the transition $\\tau$ , the reward $r$ , and (possibly) the starting distribution $\\nu$ is not available. It is only possible to gather information about these quantities by interacting with the MDP to sample state-action pairs $(x_{t},a_{t})$ and corresponding rewards $r(x_{t},a_{t})$ and transitions $x_{t+1}$ . ", "page_idx": 2}, {"type": "text", "text": "Policy Mirror Descent (PMD). In so-called tabular settings \u2013 in which both $\\mathcal{X}$ and $\\boldsymbol{\\mathcal{A}}$ are finite sets \u2013 the policy optimization problem amounts to maximizing (1) over the space $\\Pi=\\Delta(A)\\otimes\\mathbb{R}^{|\\mathcal{X}|}$ of column substochastic matrices, namely matrices $M\\in\\mathbb{R}^{|A|\\times|\\mathcal{X}|}$ with non-negative entries and whose columns sum up to one, namely $M^{*}\\mathbf{1}_{{\\mathcal{A}}}=\\mathbf{1}_{{\\mathcal{X}}}$ , with 1 denoting the vector with all entries equal to one on the appropriate space. Borrowing from the convex optimization literature \u2013 where mirror descent algorithms offer a powerful approach to minimize a convex functional over a convex constraint set [26, 27] \u2013 recent work proposed to adopt mirror descent also for policy optimization, a strategy known as policy mirror descent (PMD) [10]. Even though the objective in (1) is not convex (or concave, since we are maximizing it), it turns out that mirror ascent can nevertheless enjoy global convergence to the maximum, with sublinear [11] or even linear rates [12], at the cost of dimension-dependent constants. ", "page_idx": 2}, {"type": "text", "text": "Starting from an initial policy $\\pi_{0}$ , PMD generates a sequence $(\\pi_{t})_{t\\in\\mathbb{N}}$ according to the update step ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{t+1}(\\cdot\\,|\\,x)=\\operatorname*{argmin}_{p\\in\\Delta(A)}\\quad-\\eta\\,\\langle q_{\\pi_{t}}(\\cdot,x),p\\rangle+D(p,\\pi_{t}(\\cdot|x)),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for any $x\\in\\mathscr{X}$ , with $\\eta>0$ a step size, $D$ a suitable Bregman divergence [27] and $q_{\\pi}:\\Omega\\to\\mathbb{R}$ the so-called action-value function of a policy $\\pi$ , see also [12, Sec. 4]. The action-value function ", "page_idx": 2}, {"type": "equation", "text": "$$\nq_{\\pi}(x,a)=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(X_{t},A_{t})\\bigg|X_{0}=x,A_{0}=a\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "is the discounted return obtained by taking action $a\\in{\\mathcal{A}}$ in state $x\\in\\mathscr{X}$ and then following the policy $\\pi$ . The solution to (2) crucially depends on the choice of $D$ . For example, in [10] the authors observed that if $D$ is the Kullback-Leibler divergence, PMD corresponds to the Natural Policy Gradient originally proposed in [28] while [12] showed that if $D$ is the squared euclidean distance, PMD recovers the Projected Policy Gradient method from [11]. ", "page_idx": 2}, {"type": "text", "text": "PMD in Reinforcement Learning. A clear limitation to adopting PMD in RL settings is that (3) needs exact knowledge of the action-value functions $q_{\\pi_{t}}$ associated to each iterate $\\pi_{t}$ of the algorithm. This requires evaluating the expectation in (3), which is not possible in RL where we do not know the reward $r$ and MDP transition distribution $\\tau$ in advance. While sampling strategies can be adopted to estimate $q_{\\pi_{t}}$ , a key question is how the approximation error affects PMD. ", "page_idx": 2}, {"type": "text", "text": "The work in [12] provides an answer to this question, extending the analysis of PMD to the case where estimates $\\hat{q}_{\\pi_{t}}$ are used in place of the true action-value function in (2). We recall here an informal version of the result for the case of sublinear convergence rates for PMD. We postpone a more rigorous statement of the theorem and its assumptions to Sec. 5, where we extend it to infinite state spaces $\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 (Inexact PMD (Sec. 5 in [12]) \u2013 Informal). In the tabular setting, let $(\\pi_{t})_{t\\in\\mathbb{N}}$ be a sequence of policies obtained by applying the PMD update in (2) with functions $\\hat{q}_{\\pi_{t}}:\\Omega\\rightarrow\\mathbb{R}\\:i.$ n place of $q_{\\pi_{t}}$ and $D$ a suitable Bregman divergence. For any $T\\in\\mathbb N$ and $\\varepsilon>0$ , $i f\\left|\\right|\\hat{q}_{\\pi_{t}}-q_{\\pi_{t}}\\right||_{\\infty}\\le\\varepsilon$ for all $t=1,\\dots,T$ , then ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}~J(\\pi)-J(\\pi_{T})\\leq O(\\varepsilon+1/T).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Thm. 1 implies that inexact PMD retains the convergence rates of its exact counterpart, provided that the approximation error for each action-value function is of order $1/T$ in uniform norm $\\|\\cdot\\|_{\\infty}$ . While this result supports estimating the action-value function in RL, implementing this strategy in practice poses two main challenges, even in tabular settings. First, approximating the expectation in (3) in ${\\bar{\\|}}\\cdot{\\|}_{\\infty}$ norm via sampling requires \u201cstarting\u201d the MDP from each state $x\\in\\mathscr{X}$ , multiple times. This is often not possible in RL, where we do not have control over the starting distribution $\\nu$ . Second, repeating this sampling process to learn a $\\hat{q}_{\\pi_{t}}$ for each policy $\\pi_{t}$ can become extremely expensive in terms of both the number of computations and interactions with the environment. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In this work, we propose a new strategy to tackle the problems above. Instead of re-sampling the MDP to estimate $\\hat{q}_{\\pi_{t}}$ at each iteration $t$ , we learn estimators $\\hat{r}$ and $\\hat{\\tau}$ for the reward and transition distribution, respectively. For any policy $\\pi$ , we then leverage the relation between these quantities in (3) to generate an estimator $\\hat{q}$ for $q_{\\pi}$ . This approach tackles the above challenges since 1) it enables us to control the approximation error on any action-value function in terms of the approximation error of $\\hat{r}$ and $\\hat{\\tau};2_{}$ ) it does not require sampling the MDP to learn a new $\\hat{q}_{\\pi_{t}}$ for each $\\pi_{t}$ generated by PMD. ", "page_idx": 3}, {"type": "text", "text": "3 Operator World Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present an operator-based formulation of the problem introduced in Sec. 2 (see also [11]). This will be instrumental in extending the PMD theory to arbitrary state spaces $\\mathcal{X}$ , to quantify the approximation error of the action-value function in terms of the approximation error of the reward and transition distribution, and to motivate conditional mean embeddings as the tool to learn these latter quantities. ", "page_idx": 3}, {"type": "text", "text": "Conditional Expectation Operators. We start by defining the transfer operator $\\top$ associated with the MDP transition distribution $\\tau$ . Let $B_{b}(\\mathcal{X})$ denote the space of bounded Borel measurable functions on a space $\\mathcal{X}$ . Formally, $\\mathsf{T}:B_{b}(\\mathcal{X})\\overset{}{\\to}B_{b}(\\Omega)$ is the linear operator such that, for any $f\\in B_{b}(\\mathcal{X})$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\mathsf{T}f)(x,a)=\\int_{\\mathcal{X}}f(x^{\\prime})\\;\\tau(d x^{\\prime}|x,a)=\\mathbb{E}\\left[f(X^{\\prime})\\mid x,a\\right]\\qquad\\mathrm{for~all~}(x,a)\\in\\Omega,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $X^{\\prime}$ is sampled according to $\\tau(\\cdot|x,a)$ . Note that $\\top$ is the Markov operator [29, Ch. 19] encoding the dynamics of the MDP and its conjugate $\\mathsf{T}^{*}:\\mathcal{M}(\\Omega)\\to\\mathcal{M}(\\mathcal{X})$ is the operator mapping signed Borel measures $\\mu\\in\\mathcal{M}(\\Omega)$ to their transition via $\\tau$ as $\\begin{array}{r}{(\\mathsf{T}^{*}\\mu)(\\dot{\\mathcal{B}})\\stackrel{}{=}\\int_{\\mathcal{B}\\times\\Omega}\\bar{\\tau}(d x^{\\prime}|x,a)\\bar{\\mu}\\bar{(}d x,d\\bar{a})}\\end{array}$ for any measurable $B\\subseteq\\mathcal{X}$ . For any policy $\\pi$ we define the operator $\\mathsf{P}_{\\pi}:B_{b}(\\Omega)\\to B_{b}(\\mathcal{X})$ such that for all $g\\in B_{b}(\\Omega)$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\mathsf{P}_{\\pi}g)(x)=\\int_{A}g(x,a)\\;\\pi(d a|x)=\\mathbb{E}\\left[g(X,A)\\mid X=x\\right]\\;\\mathrm{~for~all~}x\\in\\mathcal{X},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the expectation is taken over the action $A$ sampled according to $\\pi(\\cdot|x)$ . Also $\\mathsf{P}_{\\pi}$ is a Markov operator and its conjugate $\\mathsf{P}_{\\pi}^{\\ast}:\\mathcal{M}(\\mathcal{X})\\to\\mathcal{M}(\\Omega)$ is the operator mapping any $\\nu\\in\\mathcal{M}(\\mathcal{X})$ to its joint measure with $\\pi$ , namely $\\begin{array}{r}{\\bigl(\\mathsf{P}_{\\pi}^{*}\\nu\\bigr)(\\mathcal{C})=\\int_{\\mathcal{C}}\\pi(d a|x)\\nu(d x)}\\end{array}$ for any measurable ${\\mathcal{C}}\\subseteq{\\Omega}$ . ", "page_idx": 3}, {"type": "text", "text": "Operator Formulation of RL. With these two operators in place, we can characterize the expected reward after a single interaction between a policy $\\pi$ and the MDP as $(\\mathsf{T P}_{\\pi}r)(x,a)=\\mathbb{E}[r(X^{\\prime},\\bar{A}^{\\prime})|X_{0}=$ $x,A_{0}=a]$ . This observation can be applied recursively, yielding the operatorial characterization of the action-value function from (3) ", "page_idx": 3}, {"type": "equation", "text": "$$\nq_{\\pi}(x,a)=\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{E}[r(X_{t},A_{t})|X_{0}=x,A_{0}=a]=\\sum_{t=0}^{\\infty}(\\gamma\\mathsf{T P}_{\\pi})^{t}r=(\\mathsf{I d}-\\gamma\\mathsf{T P}_{\\pi})^{-1}r,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the last equality follows from $\\top$ and $\\mathsf{P}_{\\pi}$ being Markov operators [29, Ch. 19] $(\\|\\mathsf{T}\\|=\\|\\mathsf{P}_{\\pi}\\|=$ 1), making the Neumann series convergent. Analogously, we can reformulate the RL objective introduced in (1) as the pairing ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ(\\pi)=\\left\\langle\\mathsf{P}_{\\pi}(\\mathsf{I d}-\\gamma\\mathsf{T}\\mathsf{P}_{\\pi})^{-1}r,\\nu\\right\\rangle=\\left\\langle\\mathsf{P}_{\\pi}q_{\\pi},\\nu\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for $\\nu\\in\\mathcal{P}(\\mathcal{X})$ a starting distribution. In both (7) and (8) the operatorial formulation encodes the cumulative reward collected through the (possibly infinitely many) interactions of the policy with the MDP in closed form, as the inversion $(|{\\mathsf{d}}-\\gamma{\\mathsf{T}}{\\mathsf{P}}_{\\pi}^{\\prime})^{-1}r$ . This characterization motivates us to learn $\\top$ and $r$ from data and then express any action-value function as the interaction of these two terms with the policy $\\pi$ as in (7), rather than learning each $q_{\\pi}$ independently for any $\\pi$ . ", "page_idx": 3}, {"type": "text", "text": "Learning the World Model via Conditional Mean Embeddings. Conditional Mean Embeddings (CME) offer an effective tool to model and learn conditional expectation operators from data [15]. They cast the problem of learning $\\top$ by studying the restriction of its action on a suitable family of functions. Let $\\varphi:\\mathcal{X}\\rightarrow\\mathcal{F}$ and $\\psi\\,:\\,\\Omega\\,\\rightarrow\\,{\\mathcal{G}}$ two feature maps with values into the Hilbert spaces $\\mathcal{F}$ and $\\mathcal{G}$ . With some abuse of notation (which is justified by them being Hilbert spaces), we interpret $\\mathcal{F}$ and $\\mathcal{G}$ as subspaces of functions in $B_{b}(\\mathcal{X})$ and $B_{b}(\\Omega)$ of the form $\\bar{f}(x)=\\langle f,\\bar{\\varphi}(x)\\rangle$ and $g(x,a)=\\langle g,\\psi(x,a)\\rangle$ for any $f\\in\\mathcal F$ and $g\\in{\\mathcal{G}}$ and any $(x,a)\\in\\Omega$ . We say that the linear $M D P$ assumption holds with respect to $(\\varphi,\\psi)$ if ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (Linear MDP \u2013 Well-specified CME). The restriction of $\\top$ to $\\mathcal{F}$ is a Hilbert-Schmidt operator $\\mathsf{T}|_{\\mathcal{F}}\\in\\mathsf{H S}(\\mathcal{F},\\mathcal{G})$ . ", "page_idx": 4}, {"type": "text", "text": "In CME settings, the assumption above is known as requiring the CME of $\\tau$ to be well-specified. The following result, proved in Appendix A.2, clarifies this aspect and establishes the relation of Asm. 1 with the standard definition of linear MDP. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2 (Well-specified CME). Under Asm. 1, $(\\mathsf{T}|_{\\mathcal{F}})^{*}=(\\mathsf{T}^{*})|_{\\mathcal{G}}$ and, for any $(x,a)\\in\\Omega$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\mathsf{T}|_{\\mathcal{F}})^{*}\\psi(x,a)=\\int_{\\mathcal{X}}\\varphi(x^{\\prime})\\,\\tau(x^{\\prime}|x,a)=\\mathbb{E}[\\varphi(X^{\\prime})|X=x,A=a].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proposition 2 shows that (9) is equivalent to the standard linear MDP assumption [30, Ch. 8] when $\\mathcal{X}$ is a finite set (taking $\\varphi$ the one-hot encoding) while being weaker in infinite settings. From the CME perspective, the proposition characterizes the action of $(\\mathsf{T}|_{\\mathcal{F}})^{*}$ as sending evaluation vectors in $\\mathcal{G}$ to the conditional expectation of evaluation vectors in $\\mathcal{F}$ with respect to $\\tau$ , the definition of conditional mean embedding of $\\tau$ [31, 15]. This characterization also suggests a learning strategy: (9) characterizes the action of $\\top$ as evaluating the conditional expectation of a vector $\\varphi(X^{\\prime})$ given $(x,a)$ . Given a set of points $(x_{i},a_{i})_{i=1}^{n}$ and corresponding $x^{\\prime}$ sampled from $\\tau(\\cdot|x_{i},a_{i})$ , this can be learned by minimizing the squared loss, yielding the estimator (see [15, Sec 4.2]) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{T}_{n}=\\operatorname*{argmin}_{\\mathsf{T}\\in\\mathsf{H S}(\\mathcal{F},\\mathcal{G})}\\,\\,\\frac{1}{n}\\sum_{i=1}^{n}\\|\\varphi(x_{i}^{\\prime})-\\mathsf{T}^{*}\\psi(x_{i},a_{i})\\|_{\\mathcal{F}}^{2}+\\lambda\\,\\|\\mathsf{T}\\|_{\\mathsf{H S}}^{2}=S_{n}^{*}K_{\\lambda}^{-1}Z_{n}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When $\\mathcal{F}$ and $\\mathcal{G}$ are finite dimensional, $S_{n}$ and $Z_{n}$ are matrices with $n$ rows, each corresponding respectively to the vectors $\\psi(x_{i},a_{i})$ and $\\varphi(x_{i}^{\\prime})$ for $i=1,\\hdots,n$ . In the infinite setting, they generalize to operators $S_{n}:{\\mathcal{G}}\\rightarrow\\mathbb{R}^{n}$ and $Z_{n}:\\mathcal{F}\\rightarrow\\mathbb{R}^{n}$ . The matrix $K_{\\lambda}=S_{n}S_{n}^{*}+n\\lambda\\mathsf{I}\\bar{\\mathsf{d}}_{n}\\in\\bar{\\mathbb{R}}^{\\bar{n}\\times n}$ is the regularized Gram (or kernel) matrix with $(i,j)$ -th entry corresponding to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left(K_{\\lambda}\\right)_{i j}=\\langle\\psi(x_{i},a_{i}),\\psi(x_{j},a_{j})\\rangle+n\\lambda\\delta_{i j}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We conclude our discussion on learning world models via CMEs by noting that in most RL settings, the reward function is unknown, too. Analogously to what we have described for ${\\mathsf{T}}_{n}$ and following the standard practice in supervised settings, we can learn an estimator for $r$ solving a problem akin to (10). This yields a function of the form $\\begin{array}{r}{\\bar{r}_{n}=S_{n}^{*}b=\\sum_{i=1}^{n}b_{i}\\,\\psi(x_{i},a_{i})}\\end{array}$ as the linear combination of the embedded training points with the entries of the vector $b=K_{\\lambda}^{-1}y$ where $y\\in\\mathbb{R}^{n}$ is the vector with entries $y_{i}=r(x_{i},a_{i})$ . ", "page_idx": 4}, {"type": "text", "text": "Estimating the Action-value Function $q_{\\pi}$ . We now propose our strategy to generate an estimator for the action-value function $q_{\\pi}$ of a given policy $\\pi$ in terms of an estimator for the reward $r$ and a world model for $\\top$ learned in terms of the restriction to $\\mathcal{G}$ and $\\mathcal{F}$ . To this end, we need to introduce the notion of compatibility between a policy $\\pi$ and the pair $(\\mathcal{G},\\mathcal{F})$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 1 $\\left[\\mathcal{G},\\mathcal{F}\\right)$ -compatibility). $A$ policy $\\pi:{\\mathcal{X}}\\to{\\mathcal{P}}(A)$ is compatible with two subspaces ${\\mathcal{F}}\\subseteq B_{b}({\\mathcal{X}})$ and $\\mathcal{G}\\subseteq B_{b}(\\Omega)$ if the restriction $\\mathsf{P}_{\\pi}$ to $\\mathcal{G}$ is a bounded linear operator with range $\\subseteq{\\mathcal{F}}_{:}$ , that is $(\\mathsf{P}_{\\pi})|\\boldsymbol{\\mathcal{G}}:\\mathcal{G}\\to\\mathcal{F}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 1 is analogous to the linear MDP Asm. 1 in that it requires the restriction of $\\mathsf{P}_{\\pi}$ to $\\mathcal{G}$ to take values in the associated space $\\mathcal{F}$ . However, it is slightly weaker since it requires this restriction to be bounded (and linear) rather than being an HS operator. We will discuss in Sec. 4 how this difference will allow us to show that a wide range of policies (in particular those generated by our POWR method) is $(\\mathcal{G},\\mathcal{F})$ -compatible for our choice of function spaces. Definition 1 is the key condition that enables us to generate an estimator for $q_{\\pi}$ , as characterized by the following result. ", "page_idx": 4}, {"type": "text", "text": "Input: Dataset $(x_{i},a_{i},x_{i}^{\\prime},r_{i})_{i=1}^{n}$ , discount factor $\\gamma\\,\\in\\,(0,1)$ , step size $\\eta\\,>\\,0$ , kernel function $k(x,x^{\\prime})=\\langle\\phi(x),\\phi(x^{\\prime})\\rangle$ with $\\phi:\\mathcal{X}\\to\\mathcal{H}$ as in Proposition 4, initial weights $C_{0}=0\\in\\mathbb{R}^{n\\times|\\mathcal{A}|}$ . ", "page_idx": 5}, {"type": "text", "text": "$/*$ World Model Learning \\*/ let $E\\in\\mathbb{R}^{n\\times|A|}$ with rows $E_{i}=\\mathrm{ONEHOT}_{|A|}(a_{i})$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "$/*$ Policy Mirror Descent $\\ast/$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\pi_{t+1}=\\mathrm{sorrMax}(\\eta H C_{t})\\in\\mathbb R^{n\\times|A|}}&{\\quad\\mathrm{~\\beta>PMD~Step~(1)}}\\\\ &{M_{\\pi_{t+1}}=H\\odot(\\pi_{t+1}E^{\\top})\\in\\mathbb R^{n\\times n}}&{\\quad\\mathrm{~\\beta>Proposition~3,Eq.~(1)}}\\\\ &{C_{t+1}=C_{t}+\\mathrm{diag}(c)E\\mathrm{~with~}c=(\\mathsf{l d}-\\gamma K_{\\lambda}^{-1}M_{\\pi_{t+1}})^{-1}b}&{\\quad\\mathrm{~\\beta>Proposition~3,Eq.~(1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "return $\\pi_{T}:{\\mathcal{X}}\\to\\Delta(A)$ such that $\\pi_{T}(x)=\\mathrm{{soFTMAX}}(\\eta\\,H_{x}C_{T})$ with $H_{x}=(k(x,x_{i}))_{i=1}^{n}\\in\\mathbb{R}^{n}$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 3. Let ${\\sf T}_{n}=S_{n}^{*}B Z_{n}\\in{\\sf H S}(\\mathcal{F},\\mathcal{G})$ and $r_{n}=S_{n}^{*}b\\in\\mathcal{G}$ for respectively a $B\\in\\mathbb{R}^{n\\times n}$ and $b\\in\\mathbb{R}^{n}$ . Let $\\pi$ be $(\\mathcal{G},\\mathcal{F})$ -compatible. Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{q}_{\\pi}=(\\mathsf{I d}-\\gamma\\mathsf{T}_{n}\\mathsf{P}_{\\pi})^{-1}r_{n}=S_{n}^{*}(\\mathsf{I d}-\\gamma B M_{\\pi})^{-1}b}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $M_{\\pi}=Z_{n}\\mathsf{P}_{\\pi}S_{n}^{*}\\in\\mathbb{R}^{n\\times n}$ is the matrix with entries ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left(M_{\\pi}\\right)_{i j}=\\langle\\varphi(x_{i}^{\\prime}),\\mathsf{P}_{\\pi}\\psi(x_{j},a_{j})\\rangle=\\int_{A}\\left\\langle\\psi(x_{i}^{\\prime},a),\\psi(x_{j},a_{j})\\right\\rangle\\;\\pi(d a|x_{i}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 3 leverages a kernel trick argument to express the estimator for the action-value function of $\\pi$ as the linear combination $\\begin{array}{r}{\\tilde{q}_{\\pi}=\\sum_{i=1}^{\\tilde{n}}c_{i}\\,\\psi(x_{i},\\dot{a}_{i})}\\end{array}$ of the (embedded) training points $\\psi(x_{i},a_{i})$ and the entries $c_{i}$ of the vector $c=(\\mathsf{I d}-\\gamma B M_{\\pi})^{-1}b\\in\\mathbb{R}^{n}$ . We prove the result in Appendix A.4. We note that in (12) both $B$ and $M_{\\pi}$ are $n\\times n$ matrices and therefore the characterization of ${\\hat{q}}_{\\pi}$ amounts to solving a $n\\times n$ linear system. For settings where $n$ is large, one can adopt random projection methods such as Nystr\u00f6m approximation to learn ${\\mathsf{T}}_{n}$ and $r_{n}$ [32]. These strategies have been recently shown to significantly reduce the computational load of learning while retaining the same empirical and theoretical performance as their non-approximated counterparts [33, 34]. ", "page_idx": 5}, {"type": "text", "text": "We conclude this section noting how (13) implies that we only need to be able to evaluate $\\pi$ , but we do not need explicit knowledge of $\\mathsf{P}_{\\pi}$ as operator. As we shall see, this property will be instrumental to prove generalization bounds for our proposed PMD algorithm in Sec. 4. ", "page_idx": 5}, {"type": "text", "text": "4 Proposed Algorithm: POWR ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We are ready to describe our algorithm for world model-based PMD. In the following, we restrict to the case where $|{\\mathcal{A}}|<\\infty$ is a finite set. As introduced in Sec. 2, policy mirror descent methods are mainly characterized by the choice of Bregman divergence $D$ used for the mirror descent update and, in the case of inexact methods, the estimator $\\hat{q}_{\\pi_{t}}$ of the action-value function $q_{\\pi_{t}}$ for the intermediate policies generated by the algorithm. ", "page_idx": 5}, {"type": "text", "text": "In POWR, we combine the CME world model presented in Sec. 3 with mirror descent steps using the Kullback-Leibler divergence $\\begin{array}{r}{D_{\\mathrm{KL}}(p;p^{\\prime})\\,=\\,\\sum_{a\\in\\mathcal{A}}p_{a}\\log(p_{a}/p_{a}^{\\prime})}\\end{array}$ in the update of (2). It was shown in [10] that in this case PMD corresponds  to Natural Policy Gradient [28]. As showed in [35, Example 9.10], the solution to (2) can be written in closed form for any $x\\in\\mathscr{X}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{t+1}(\\cdot|x)=\\frac{\\pi_{t}(\\cdot|x)e^{\\eta\\hat{q}_{\\pi_{t}}(x,\\cdot)}}{\\sum_{a\\in\\mathcal{A}}\\pi_{t}(a|x)e^{\\eta\\hat{q}_{\\pi_{t}}(x,a)}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we used the estimated action-value function ${\\hat{q}}_{\\pi}$ from Proposition 3. Additionally, the formula above can be applied recursively, expressing $\\pi_{t+1}$ as the softmax operator applied to the discounted ", "page_idx": 5}, {"type": "text", "text": "sum of the action-value functions up to the current iteration ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{t+1}(\\cdot|x)=\\mathrm{soFTMAX}\\left(\\log(\\pi_{0}(\\cdot|x))+\\eta\\sum_{s=0}^{t}\\hat{q}_{\\pi_{s}}(x,\\cdot)\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Choice of the Feature Maps. A key question to address before adopting the action-value estimators introduced in Sec. 3 is choosing the two spaces $\\mathcal{F}$ and $\\mathcal{G}$ to perform world model learning. Specifically, to apply Proposition 3 and obtain proper estimators $\\hat{q}_{\\pi_{t}}$ , we need to guarantee that all policies generated by the PMD update (14) are $(\\mathcal{G},\\mathcal{F})$ -compatible (Definition 1). The following result describes a suitable family of such spaces. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4 (Separable Spaces). Let $\\phi:\\,\\mathcal{X}\\,\\rightarrow\\,\\mathcal{H}$ be a feature map into a Hilbert space $\\mathcal{H}$ . Let $\\mathcal{F}=\\mathcal{H}\\otimes\\mathcal{H}$ and $\\mathcal{G}\\,=\\,\\bar{\\mathbb{R}}^{|\\mathcal{A}|}\\otimes\\mathcal{H}$ with feature maps respectively $\\varphi(x)\\,=\\,\\phi(x)\\otimes\\phi(x)$ and $\\psi(x,a)=\\phi(x)\\otimes e_{a},$ , with $\\boldsymbol{e}_{a}\\in\\mathbb{R}^{|\\boldsymbol{A}|}$ the one-hot encoding of action $a\\in A.$ . Let $\\pi:{\\mathcal{X}}\\to\\Delta(A)$ be a policy such that $\\pi(a|\\cdot)=\\langle p_{a},\\phi(\\cdot)\\rangle$ with $p_{a}\\in\\mathcal{H}$ for any $a\\in{\\mathcal{A}}$ . Then, $\\pi$ is $(\\mathcal{G},\\mathcal{F})$ -compatible. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4 (proof in Appendix A.5) states that for the specific choice of function spaces $\\mathcal{F}=\\mathcal{H}\\otimes\\mathcal{H}$ and $\\mathcal{G}=\\mathbb{R}^{|\\bar{A}|}\\bar{\\otimes}\\mathcal{H}$ , we can guarantee $(\\mathcal{G},\\mathcal{F})$ -compatibility, provided that $\\mathcal{H}$ is rich enough to \u201ccontain\u201d all $\\pi(a|\\cdot)$ for $a\\in A$ . We postpone the discussion on identifying a suitable spaces $\\mathcal{H}$ for PMD to Sec. 5, since $(\\mathcal{G},\\mathcal{F})$ -compatibility is not needed to mechanically apply Proposition 3 and obtain an estimator ${\\hat{q}}_{\\pi}$ . This is because (12) exploits a kernel-trick to bypass the need to know $\\mathsf{P}_{\\pi}$ explicitly and rather requires only to be able to evaluate $\\pi$ on the training data. The latter is possible for $\\pi_{t+1}$ , thanks to the explicit form of the PMD update in (14). We can therefore present our algorithm. ", "page_idx": 6}, {"type": "text", "text": "POWR. Alg. 1 describes Policy mirror descent with Operator World-models for Reinforcement learning (POWR). Following the intuition of Proposition 4, the algorithm assumes to work with separable spaces. During an initial phase, we learn the world model $\\bar{\\mathsf{T}}_{n}=S_{n}^{*}K_{\\lambda}^{-1}Z_{n}$ and the reward $r_{n}=S_{n}^{*}b$ fitting the conditional mean embedding described in (10) on a dataset $(x_{i},a_{i},x_{i}^{\\prime},r_{i})_{i=1}^{n}$ (e.g. obtained via experience replay [36]). Once the world model has been learned, we optimize the policy and perform PMD iterations via (15). In this second phase, we first evaluate the past (cumulative) action-value estimators $\\sum_{s=0}^{t}\\hat{q}_{\\pi_{s}}$ to obtain the policy $\\pi_{t+1}(\\cdot|x_{i})$ by (inexact) PMD via the softmax operator in (15). We  use the newly obtained policy to compute the matrix $M_{\\pi_{t+1}}$ defined in (13), which is a key component to obtain the estimator $\\hat{q}_{\\pi_{t+1}}$ for $q_{\\pi_{t+1}}$ . We note that in the case of the separable spaces of Proposition 4, this matrix reduces to the $n\\times n$ matrix with entries $k(x_{i}^{\\prime},x_{j})\\pi_{t+1}\\bar{(a_{j}|x_{i}^{\\prime})}$ , where $k(x_{i}^{\\prime},\\stackrel{\\_}{x}_{j})=\\langle\\phi(x_{i}^{\\prime}),\\phi(x_{j})\\rangle$ is the kernel matrix between initial and evolved states. Finally, we obtain $c=(\\mathsf{I d}-\\gamma K_{\\lambda}^{-1}M)^{-1}b$ and model $\\hat{q}_{\\pi_{t+1}}=S_{n}^{*}c$ according to Proposition 3. ", "page_idx": 6}, {"type": "text", "text": "Clearly, the world model learning and PMD phases can be alternated in POWR, essentially finding a trade-off between exploration and exploitation. This could possibly lead to a refinement of the world model as more observations are integrated into the estimator. While in this work we do not investigate the effects of such alternating strategy, Thm. 7 offers relevant insights in this sense. The result characterizes the behavior of the PMD algorithm when combined with a varying (possibly increasing) accuracy in the estimation of the action-valued function (see Sec. 5 for more details). ", "page_idx": 6}, {"type": "text", "text": "5 Theoretical Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now show that POWR converges to the global maximizer of the RL problem in (1). To this end, we first identify a family of function spaces guaranteed to be compatible with the policies generated by Alg. 1. Then, we provide an extension of the result in [12] for inexact PMD to infinite state spaces $\\mathcal{X}$ , showing the impact of the action-value approximation error on the convergence rates. For the estimator introduced in Proposition 3, we relate this error to the approximation errors of ${\\mathsf{T}}_{n}$ and $r_{n}$ leveraging the Simulation Lemma A.6. Finally, we use recent advancements in the characterization of CMEs\u2019 fast learning rates to bound the sample complexity of these latter quantities, yielding error bounds for POWR. ", "page_idx": 6}, {"type": "text", "text": "POWR is Well-defined. To properly apply Proposition 3 to estimate the action-value function of any PMD iterate $\\pi_{t}$ , we need to guarantee that every iterate belongs to the space $\\mathcal{H}$ according to Proposition 4. The following result provides such a family of spaces. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5. Let $\\mathcal{X}\\subset\\mathbb{R}^{d}$ be a compact set and let $\\mathcal{H}=W^{2,s}(\\mathcal{X})$ be the Sobolev space of smoothness $s>0$ (see e.g. $I37J)$ . Let $\\pi_{t}(a|\\cdot)$ and $\\hat{q}_{\\pi_{t}}(\\cdot,a)$ belong to $\\mathcal{H}$ for any $a\\in A$ and $\\pi_{t}(a|x)>0$ for any $x\\in\\mathscr{X}$ . Then the policy $\\pi_{t+1}$ solution to the PMD update in (14) belongs to $\\mathcal{H}$ . ", "page_idx": 7}, {"type": "text", "text": "According to Thm. 5, Sobolev spaces offer a viable choice for compatibility with PMD-generated policies. This observation is further supported by the fact that Sobolev spaces of smoothness $s>d/2$ are so-called reproducing kernel Hilbert spaces $(r k h s)$ (see e.g. [38, Ch. 10]). We recall that rkhs are always naturally equipped with a $\\phi:\\mathcal{X}\\to\\mathcal{H}$ such that the inner product $\\langle\\phi(x),\\phi(x^{\\prime})\\rangle=k(x,x^{\\prime})$ defines a so-called reproducing kernel, namely a positive definite function that is (usually) efficient to evaluate, even if $\\phi(x)$ is high or infinite dimensional. For example, $\\mathcal{H}=W^{2,s}(\\mathcal{X})$ with $\\textstyle s=\\left\\lceil{\\frac{d}{2}}\\right\\rceil$ has associated kernel $k(x,x^{\\prime})=e^{-\\|x-x^{\\prime}\\|/\\sigma}$ with bandwidth $\\sigma>0$ [38]. By applying Thm. 5 to the iterates generated by Alg. 1 we have the following result. ", "page_idx": 7}, {"type": "text", "text": "Corollary 6. With the hypothesis of Proposition $^{4}$ let $\\mathcal{H}=W^{2,s}(\\mathcal{X})$ with $s\\,>\\,d/2$ . Let ${\\sf T}_{n}\\in$ $\\mathsf{H S}(\\mathcal{F},\\mathcal{G})$ and $r_{n}\\in{\\mathcal{G}}$ characterized as in Proposition 3. Let $\\pi_{0}(a|\\cdot)\\propto e^{\\eta q_{0}(\\cdot,a)}$ for $q_{0}$ such that $q_{0}(\\cdot,a)\\in\\mathcal{H}$ any $a\\in A$ . Then, for any $t\\in\\mathbb N$ the PMD iterates $\\pi_{t}$ generated by Alg. 1 are such that $\\pi_{t}(a|\\cdot)\\in\\mathcal{H}$ and hence are $(\\mathcal{G},\\mathcal{F})$ -compatible. ", "page_idx": 7}, {"type": "text", "text": "The above corollary guarantees us that if we are able to learn our estimates for the action-value function in a suitably regular Sobolev space $\\mathcal{H}$ , then POWR is well-defined. This is a necessary condition to then being able to study its theoretical behavior in our main result. We report the proofs of Thm. 5 and Cor. 6 in Appendix C.1. ", "page_idx": 7}, {"type": "text", "text": "Inexact PMD Converges. We now present a more rigorous version of the characterization of the convergence rates of the inexact PMD algorithm discussed informally in Thm. 1. ", "page_idx": 7}, {"type": "text", "text": "Theorem 7 (Convergenge of Inexact PMD). Let $(\\pi_{t})_{t\\in\\mathbb{N}}$ be a sequence of policies generated by Alg. 1 that are all $(\\mathcal{G},\\mathcal{F})$ -compatible. If the action-value functions $\\hat{q}_{\\pi_{t}}$ are estimated with an error $\\|q_{\\pi_{t}}-\\hat{q}_{\\pi_{t}}\\|_{\\infty}\\leq\\varepsilon_{t},$ , the iterates of $\\boldsymbol{A l g}$ . 1 converge to the optimal policy as ", "page_idx": 7}, {"type": "equation", "text": "$$\nJ(\\pi_{*})-J(\\pi_{T})\\leq\\varepsilon_{T}+O\\left(\\frac{1}{T}+\\frac{1}{T}\\sum_{t=0}^{T-1}\\varepsilon_{t}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\pi_{*}:\\mathcal{X}\\to\\Delta(\\mathcal{A})$ is a measurable maximizer of (8). ", "page_idx": 7}, {"type": "text", "text": "Thm. 7 shows that inexact PMD can behave comparably to its exact version provided that 1) the action value functions $\\hat{q}_{\\pi_{t}}$ are estimated with increasing accuracy, and 2) that the sequence of policies is $(\\mathcal{G},\\mathcal{F})$ -compatible, for example in the Sobolev-based setting of Cor. 6. Specifically, if $\\|q_{\\pi_{t}}-\\hat{q}_{\\pi_{t}}\\|_{\\infty}\\leq O(1/t)$ for any $t\\in{\\mathbb N}$ , the convergence rate of inexact PMD is of order $O(\\log T/T)$ , only a logarithmic factor slower than exact PMD. This means that we do not necessarily need a good approximation of the world model from the beginning but rather a strategy to improve upon such approximation as we perform more PMD iterations. This suggests adopting an alternating strategy between exploration (world model learning) and exploitation (PMD steps), as suggested in Sec. 4. We do not investigate this question in this work. ", "page_idx": 7}, {"type": "text", "text": "The demonstration technique used to prove Thm. 7 follows closely [12, Thm. 8 and 13]. We provide a proof in Appendix B.1 since the original result did not allow for a decreasing approximation error but rather assumed a constant one. Moreover, extending it to the case of infinite $\\mathcal{X}$ requires taking care of additional details related to potential measurability issues. ", "page_idx": 7}, {"type": "text", "text": "Action-value approximation error in terms of World Model estimates. Thm. 7 highlights the importance of studying the error of the estimator for the action-value functions produced by Alg. 1. These objects are obtained via the formula described in Proposition 3 in terms of ${\\mathsf{T}}_{n}$ , $r_{n}$ and $\\mathsf{P}_{\\pi}$ . The exact $q_{\\pi}$ has an analogous closed-form characterization in terms of $\\top$ , $r$ and $\\mathsf{P}_{\\pi}$ as expressed in (7), and motivating our operator-based approach. The following result compares these quantities in terms of the approximation errors of the world model and the reward function. ", "page_idx": 7}, {"type": "text", "text": "Lemma 8 (Implications of the Simulation Lemma). Let ${\\mathsf{T}}_{n}$ and $r_{n}$ the empirical estimators of the transfer operator $\\top$ and reward function $r$ as defined in Proposition $^3$ , respectively. If T satisfies Asm. 1, $r\\in\\mathcal G$ , and $\\gamma\\|\\mathsf T_{n}\\|<\\dot{\\gamma}^{\\prime}<1$ , then, for every $(\\mathcal{G},\\mathcal{F})$ -compatible policy $\\pi$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|\\hat{q}_{\\pi}-q_{\\pi}\\right\\|_{\\infty}\\leq\\frac{1}{1-\\gamma^{\\prime}}\\left[c o n s t_{\\psi}\\left\\|r_{n}-r\\right\\|_{\\mathcal{G}}+\\frac{\\gamma\\left\\|r\\right\\|_{\\infty}}{1-\\gamma}\\left\\|\\mathsf{T}|_{\\mathcal{F}}-\\mathsf{T}_{n}\\right\\|_{\\mathsf{H S}}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In the result above, when applied to a function in $\\mathcal{G}$ , such as $r_{n}$ , the uniform norm is to be interpreted as the uniform norm of the evaluation of such function, namely $\\begin{array}{r}{\\|r_{n}\\|_{\\infty}=\\operatorname*{sup}_{(x,a)\\in\\Omega}|\\left\\langle r_{n},\\psi(\\bar{x_{,}}a)\\right\\rangle|,}\\end{array}$ , and analogously for ${\\mathsf{T}}_{n}$ . The proof, reported in Appendix C.2, follows by first decomposing the difference $q_{\\pi}-{\\hat{q}}_{\\pi}$ with the simulation lemma [30, Lemma 2.2] and then applying the triangular inequality for the uniform norm. ", "page_idx": 8}, {"type": "text", "text": "POWR converges. We are ready to state the convergence result for Alg. 1. We consider the setting where the dataset used to learn ${\\mathsf{T}}_{n}$ (and $r_{n}$ ) is made of i.i.d. triplets $(x_{i},\\bar{a}_{i},x_{i}^{\\prime})$ with $(x_{i},a_{i})$ sampled from a distribution $\\rho\\,\\in\\,\\mathscr{P}(\\Omega)$ supported on all $\\Omega$ (such as the state occupancy measure (see e.g. [11] or Appendix A.3) of the uniform policy $\\pi(\\cdot|x)=1/|A|)$ and $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ sampled from $\\tau(\\cdot|x_{i},a_{i})$ . To guarantee bounds in uniform norm, the result makes a further regularity assumption, of the transfer operator (and the reward function) ", "page_idx": 8}, {"type": "text", "text": "Assumption 2 (Strong Source Condition). Let $\\rho\\ \\in\\ \\mathscr{P}(\\Omega)$ and $C_{\\rho}$ the covariance operator $\\textstyle\\sum_{a\\in{\\cal A}}\\bar{\\int}_{\\chi}\\,\\rho(d x,a)\\psi(x,a)\\otimes\\psi(x,a)$ . The transition operator $\\top$ and the reward function $r$ are such that $\\mathsf{T}|_{\\mathcal{F}}\\in\\mathsf{H S}(\\mathcal{F},\\mathcal{G})$ and $r\\,\\in\\,{\\mathcal{G}}$ . Further, $\\left\\|(T|_{\\mathcal{F}})^{*}C_{\\rho}^{-\\beta}\\right\\|_{\\mathsf{H S}}<\\infty$ and $\\left\\|C_{\\rho}^{-\\beta}r\\right\\|_{\\mathcal{G}}<\\infty$ for some $\\beta>0$ . ", "page_idx": 8}, {"type": "text", "text": "Asm. 2 imposes a strong requirement to the so-called source condition, a quantity that describes how well the target objective of the learning process (here $\\top$ and $r$ ) \u201cinteract\u201d with the sampling distribution. The assumption is always satisfied when the hypothesis space is finite dimensional (e.g. in the tabular RL setting) and imposes additional smoothness on $\\top$ and $r$ when belonging to a Sobolev space. Equipped with this assumption, we can now state the convergence theorem for Alg. 1. ", "page_idx": 8}, {"type": "text", "text": "Theorem 9. Let $(\\pi_{t})_{t\\in\\mathbb{N}}$ be a sequence of policies generated by Alg. 1 in the same setting of Cor. 6. If the action-value functions $\\hat{q}_{\\pi_{t}}$ are estimated from a dataset $(\\bar{x_{i}},a_{i};x_{i}^{\\prime})_{i=1}^{n}$ with $(x_{i},a_{i})\\sim\\rho\\in\\mathscr{P}(\\Omega)$ such that Asm. 2 holds with parameter $\\beta$ , the iterates of Alg. 1 converge to the optimal policy as ", "page_idx": 8}, {"type": "equation", "text": "$$\nJ(\\pi_{*})-J(\\pi_{T})\\le O\\left(\\frac{1}{T}+\\delta^{2}n^{-\\alpha}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with probability not less than $1-4e^{-\\delta}$ . Here, $\\begin{array}{r}{\\alpha\\,\\in\\,\\left(\\frac{\\beta}{2+2\\beta},\\frac{\\beta}{1+2\\beta}\\right)}\\end{array}$ and $\\pi_{*}\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\Delta(\\mathcal{A})$ is $a$ measurable maximizer of (8). ", "page_idx": 8}, {"type": "text", "text": "The proof of Thm. 9 is reported in Appendix C and combines the results discussed in this section with fast convergence rates for the least-squares [39] and CME [21] estimators. In particular we first use Thm. 5 to guarantee that the policies produced by Alg. 1 are all $(\\mathcal{G},\\mathcal{F})$ -compatible and therefore that applying Proposition 3 to obtain an estimator for the action-value function is well-defined. Then, we use Lemma 8 to study the approximation error of these estimators in terms of our estimates for the world model and the reward function. Bounds on these quantities are then used in the result for inexact PMD convergence in Thm. 7. We note here that since the latter results require convergence in uniform norm, we cannot leverage standard results for least-squares and CME convergence, which characterize convergence in $L_{2}(\\bar{\\Omega},\\mu)$ and would only require Asm. 1 (Linear MDP) to hold. Rather, we need to impose Asm. 2 to guarantee faster rates in uniform norm. ", "page_idx": 8}, {"type": "text", "text": "6 Experimental results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We empirically evaluated POWR on classical Gym environments [25], ranging from discrete (FrozenLake-v1,Taxi-v3) to continuous state spaces (MountainCar-v0). To ensure balancing between exploration and exploitation of our method, we alternated between running the environment with the current policy to collect samples for world model learning and running Alg. 1 for a number of steps to generate a new policy. Appendix D provides implementation details regarding this process as well as additional results. ", "page_idx": 8}, {"type": "text", "text": "Fig. 1 compares our approach with the performance of well-established baselines including A2C [40], DQN [4], TRPO [7], and PPO [6]. The figure reports the average cumulative reward obtained by the models on test environments with respect to the number of interactions with the MDP (timesteps in log scale in the figure) across 7 different training runs. In all plots, the horizontal dashed line represents the \u201csuccess\u201d threshold for the corresponding environment, according to official guidelines. We observe that our method outperforms all competitors by a significant margin in terms of sample complexity, that is, the reward achieved after a given number of timesteps. In the case of the Taxi-v3 environment, it avoids converging to a local optimum, in contrast every other method with the exception of DQN. On the downside, we note that our method exhibits less stability than other approaches, particularly during the initial stages of the training process. This is arguably due to a sub-optimal interplay between exploration and exploitation, which will be the subject of future work. ", "page_idx": 8}, {"type": "image", "img_path": "kbBjVMcJ7G/tmp/77e037eceb56d40abe374f8d49958d171b5c1677f483e76a6dcabf5d6d25bc99.jpg", "img_caption": ["Figure 1: The plots show the average cumulative reward in different environments with respect to the timesteps (i.e. number of interactions with MDP). The dark lines represent the mean of the cumulative reward and the shaded area is the minimum and maximum values reached across 7 independent runs. The horizontal dashed lines represent the reward threshold proposed by the Gym library [25]. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusions and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Motivated by recent advancements in policy mirror descent (PMD), this work introduced a novel reinforcement learning (RL) algorithm leveraging these results. Our approach operates in two, possibly alternating, phases: learning a world model and planning via PMD. During exploration, we utilize conditional mean embeddings (CMEs) to learn a world model operator, showing that this procedure is well-posed when performed over suitable Sobolev spaces. The planning phase involves PMD steps for which we guarantee convergence to a global optimum at a polynomial rate under specific MDP regularities. ", "page_idx": 9}, {"type": "text", "text": "Our analysis opens avenues for further exploration. Firstly, extending PMD to infinite action spaces remains a challenge. While we introduced the operatorial perspective on RL for infinite state space settings, the PMD update with KL divergence requires approximation methods (e.g., Monte Carlo) whose impact on convergence requires investigation. Secondly, scalability to large environments requires adopting approximated yet efficient CME estimators like Nystrom [34] or reduced-rank regressors [41, 42]. Thirdly, a question we touched upon only empirically, is whether alternating world model learning with inexact PMD updates benefits the exploration-exploitation trade-off. Studying this strategy\u2019s impact on convergence is a promising future direction. Finally, a crucial question is generalizing our policy compatibility results beyond Sobolev spaces. Ideally, a representation learning process would identify suitable feature maps that guarantee compatibility with the PMD-generated policies while allowing for added flexibility in learning the world model. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We acknowledge financial support from NextGenerationEU and MUR PNRR project PE0000013 CUP J53C22003010006 \u201cFuture Artificial Intelligence Research (FAIR)\u201d, EU grant ELISE (GA no 951847) and EU Project ELIAS (GA no 101120237). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. ", "page_idx": 9}, {"type": "text", "text": "[2] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3\u201320, 2020. ", "page_idx": 9}, {"type": "text", "text": "[3] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.   \n[4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv1312.5602, 2013.   \n[5] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. Resource management with deep reinforcement learning. In Proceedings of the 15th ACM workshop on hot topics in networks, pages 50\u201356, 2016.   \n[6] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv1707.06347, 2017.   \n[7] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization, 2017.   \n[8] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, 2018.   \n[9] Dimitri Bertsekas. Dynamic Programming and Optimal Control: Volume I, volume 4. Athena scientific, 2012.   \n[10] Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5668\u20135675, 2020.   \n[11] Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1\u201376, 2021.   \n[12] Lin Xiao. On the convergence rates of policy gradient methods. Journal of Machine Learning Research, 23(282):1\u201336, 2022.   \n[13] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. In International Conference on Machine Learning, pages 2160\u20132169. PMLR, 2019.   \n[14] Kenji Fukumizu, Francis R. Bach, and Michael I. Jordan. Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces. Journal of Machine Learning Research, 5:73\u201399, 2004.   \n[15] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Sch\u00f6lkopf, et al. Kernel mean embedding of distributions: A review and beyond. Foundations and Trends\u00ae in Machine Learning, 10(1-2):1\u2013141, 2017.   \n[16] David Ha and J\u00fcrgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018.   \n[17] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163, 1991.   \n[18] J\u00fcrgen Schmidhuber. Making the world differentiable: on using self supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments, volume 126. Inst. f\u00fcr Informatik, 1990.   \n[19] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.   \n[20] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A general framework for consistent structured prediction with implicit loss embeddings. Journal of Machine Learning Research, 21(98):1\u201367, 2020.   \n[21] Zhu Li, Dimitri Meunier, Mattes Mollenhauer, and Arthur Gretton. Optimal rates for regularized conditional mean embedding learning. Advances in Neural Information Processing Systems, 35:4433\u20134445, 2022.   \n[22] Steffen Gr\u00fcnew\u00e4lder, Guy Lever, Luca Baldassarre, Massimiliano Pontil, and Arthur Gretton. Modelling transition dynamics in mdps with rkhs embeddings. In Proceedings of the 29th International Conference on International Conference on Machine Learning, pages 1603\u2014- 1610, 2012.   \n[23] Antoine Moulin and Gergely Neu. Optimistic planning by regularized dynamic programming. In International Conference on Machine Learning, pages 25337\u201325357. PMLR, 2023.   \n[24] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In International Conference on Machine Learning, pages 463\u2013474. PMLR, 2020.   \n[25] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arxiv. arXiv preprint arXiv:1606.01540, 10, 2016.   \n[26] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167\u2013175, 2003.   \n[27] S\u00e9bastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends\u00ae in Machine Learning, 8(3-4):231\u2013357, 2015.   \n[28] Sham M. Kakade. A natural policy gradient. Advances in Neural Information Processing Systems, 14, 2001.   \n[29] C.D. Aliprantis and K.C. Border. Infinite Dimensional Analysis: A Hitchhiker\u2019s Guide. Studies in Economic Theory. Springer, 1999.   \n[30] Alekh Agarwal, Nan Jiang, Sham M. Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. 2021. URL https://rltheorybook.github.io, 2022.   \n[31] Steffen Gr\u00fcnew\u00e4lder, Guy Lever, Luca Baldassarre, Sam Patterson, Arthur Gretton, and Massimilano Pontil. Conditional mean embeddings as regressors. In Proceedings of the 29th International Conference on International Conference on Machine Learning, pages 1803\u20131810, 2012.   \n[32] Christopher Williams and Matthias Seeger. Using the Nystr\u00f6m method to speed up kernel machines. Advances in Neural Information Processing Systems, 13, 2000.   \n[33] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nystr\u00f6m computational regularization. Advances in Neural Information Processing Systems, 28, 2015.   \n[34] Giacomo Meanti, Antoine Chatalic, Vladimir R. Kostic, Pietro Novelli, Massimiliano Pontil, and Lorenzo Rosasco. Estimating Koopman operators with sketching to provably learn large scale dynamical systems. Advances in Neural Information Processing Systems, 36, 2023.   \n[35] Amir Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics, 2017.   \n[36] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.   \n[37] Robert A. Adams and John J. F. Fournier. Sobolev Spaces. Elsevier, 2003.   \n[38] Holger Wendland. Scattered data approximation, volume 17. Cambridge University Press, 2004.   \n[39] Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algorithms. Journal of Machine Learning Research, 21(205):1\u201338, 2020.   \n[40] Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. arXiv1602.01783, 2016.   \n[41] Vladimir R. Kostic, Pietro Novelli, Andreas Maurer, Carlo Ciliberto, Lorenzo Rosasco, and Massimiliano Pontil. Learning dynamical systems via Koopman operator regression in reproducing kernel Hilbert spaces. Advances in Neural Information Processing Systems, 35:4017\u20134031, 2022.   \n[42] Giacomo Turri, Vladimir Kostic, Pietro Novelli, and Massimiliano Pontil. A randomized algorithm to solve reduced rank operator regression. arXiv preprint arXiv:2312.17348, 2023.   \n[43] Gene H. Golub and Charles F. Van Loan. Matrix Computations. Johns Hopkins University Press, 2013.   \n[44] O. Kallenberg. Foundations of Modern Probability. Probability and Its Applications. Springer New York, 2002.   \n[45] Giulia Luise, Saverio Salzo, Massimiliano Pontil, and Carlo Ciliberto. Sinkhorn barycenters with free support via frank-wolfe algorithm. Advances in Neural Information Processing Systems, 32, 2019.   \n[46] Vladimir R. Kostic, Karim Lounici, Pietro Novelli, and Massimiliano Pontil. Sharp spectral rates for koopman operator learning. Advances in Neural Information Processing Systems, 36, 2023.   \n[47] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1\u20138, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendices are organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Appendix A discuss the operatorial formulation of RL and show how to derive the operatorbased results in this work.   \n\u2022 Appendix B focuses on policy mirror descent (PMD) and its convergence rate in the inexact setting.   \n\u2022 Appendix C proves the main result of this work, namely the theoretical analysis of POWR.   \n\u2022 Appendix D provide details on the experiments reported in this work. ", "page_idx": 13}, {"type": "text", "text": "A Operatorial Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Auxiliary Lemma ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We recall here a corollary of the Sherman-Woodbury identity [43]. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. Let $A$ and $B$ two conformable linear operators such that $(I+A B)^{-1}$ is invertible. Then $(I+A B)^{-1}A=A(I+B A)^{-1}$ ", "page_idx": 13}, {"type": "text", "text": "Proof. The result is obvious if $A$ is invertible. More generally, we consider the following two applications of the Sherman-Woodbury [43] formula ", "page_idx": 13}, {"type": "equation", "text": "$$\n(I+A B)^{-1}=I-A(I+B A)^{-1}B\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n(I+B A)^{-1}=I-(I+B A)^{-1}B A.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Multiplying the two equations by $A$ respectively to the right and to the left, we obtain the desired result. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "A.2 Markov operators and their properties ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We recall here the notion of Markov operators, which is central for a number of results in the following. We refer to [29, Chapter 19] for more details on the topic. ", "page_idx": 13}, {"type": "text", "text": "Definition A.1 (Markov operators). Let $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ be Polish spaces. A bounded linear operator $\\mathcal{L}(B_{b}(\\mathcal{X}),B_{b}(\\mathcal{Y}))$ is a Markov operator if is positive and maps the unit function to itself, that is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{a}.\\ f\\geq0\\in B_{b}(\\mathcal{X})\\implies\\mathsf{P}f\\geq0\\in B_{b}(\\mathcal{Y}),}\\\\ &{\\pmb{b}.\\ \\mathsf{P}\\mathbf{1}_{\\mathcal{X}}=\\mathbf{1}_{\\mathcal{Y}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{1}_{\\mathcal{X}}:\\mathcal{X}\\rightarrow\\mathbb{R}$ (respectively ${\\mathbf{1}}_{\\mathcal{{V}}}$ ) denotes the function taking constant value equal to $1$ on $\\mathcal{X}$ (respectively $\\boldsymbol{{y}}_{.}$ ). ", "page_idx": 13}, {"type": "text", "text": "We recall that Markov operators are a convex subset of $\\mathcal{L}(B_{b}(\\mathcal{X}),B_{b}(\\mathcal{Y}))$ . Here we denote this space as $\\mathcal{L}_{\\mathrm{M}}(B_{b}(\\mathcal{X}),B_{b}(\\mathcal{Y}))$ . Direct inspection of (5) and (6) shows that the transfer operator $\\top$ associated to an MDP and the policy operator $\\mathsf{P}_{\\pi}$ associated to a policy $\\pi$ are both Markov operators. ", "page_idx": 13}, {"type": "text", "text": "Markov Operators and Policy Operators. In (6) we defined the policy operator $\\mathsf{P}_{\\pi}$ associated to a policy $\\pi$ . It turns out that the converse is also true, namely that any such Markov operator is a policy operator. ", "page_idx": 13}, {"type": "text", "text": "Proposition A.2. Let $\\mathsf{P}\\in\\mathcal{L}_{\\mathrm{M}}(B_{b}(\\mathcal{X}),B_{b}(\\Omega))$ be a Markov operator. Then there exists $\\pi_{\\mathsf{P}}$ , such that the associated policy operator corresponds to $\\mathsf{P}$ , namely $\\mathsf{P}_{\\pi_{\\mathsf{P}}}=\\mathsf{P}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Define the map $\\pi_{\\mathsf{P}}:{\\mathcal{X}}\\to{\\mathcal{M}}(A)$ taking value in the space of bounded Borel measures over $\\boldsymbol{\\mathcal{A}}$ such that, for any $x\\in\\mathscr{X}$ and any $B\\subseteq A$ Borel measurable subset ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pi_{\\mathsf{P}}(\\beta|x)=(\\mathsf{P}1_{X\\times B})(x).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We need to guarantee that for every $x\\in\\mathscr{X}$ the function $\\pi_{\\mathsf{P}}(\\cdot|x)$ is a signed measure. To show this, first note that the operation defined by $\\pi_{\\mathsf{P}}$ is well-defined, since for any measurable set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ the function ${\\bf1}_{\\mathcal{X}\\times B}$ is also measurable, making $\\pi_{\\mathsf{P}}(\\boldsymbol{B}|\\boldsymbol{x})$ well defined as well. Moreover, since $\\mathbf{1}_{\\varnothing}(a)=0$ for any $a\\in A$ , it implies that ${\\mathbf{1}}_{{\\mathcal{X}}\\times\\emptyset}=0$ and therefore $\\pi_{\\mathsf{P}}(\\emptyset|x)=0$ for any $x\\in\\mathscr{X}$ . Finally, $\\sigma$ -additivity follows from the definition of indicator functions, namely 1 i\u221e=1 Bi = $\\begin{array}{r}{{\\bf1}_{\\bigcup_{i=1}^{\\infty}B_{i}}=\\sum_{i=1}^{\\infty}{\\bf1}_{B_{i}}}\\end{array}$ i=1 1Bi for any family of pair-wise disjoint sets $(B_{i})_{i=1}^{n}$ , which implies $\\begin{array}{r}{\\pi_{\\mathsf{P}}\\left(\\bigcup_{i=1}^{\\infty}\\mathcal{B}_{i}|x\\right)=\\sum_{i=1}^{\\infty}\\pi_{\\mathsf{P}}(\\mathcal{B}_{i}|x)}\\end{array}$ for any $x\\in\\mathscr{X}$ . ", "page_idx": 14}, {"type": "text", "text": "We now apply the two properties of Markov operators to show that $\\pi_{\\mathsf{P}}$ takes values in $\\mathcal{P}(\\mathcal{A})$ , namely it is a non-negative measure that sums to 1. Since Markov operators map non-negative functions in non-negative functions and since ${\\mathbf{1}}_{{\\mathcal{X}}\\times{\\mathcal{B}}}\\geq0$ for any $B\\subseteq\\mathcal{X}$ , we have $\\bar{\\pi}(\\cdot|x)\\bar{\\geq0}$ as well for any $x\\in\\mathscr{X}$ . Moreover, since $\\Omega=\\mathcal{X}\\times\\mathcal{A}$ and $\\mathsf{P1}_{\\Omega}=\\mathbf{1}_{\\mathcal{X}}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi(\\mathcal{A}|x)=(\\mathsf{P\\,}\\mathbf{1}_{\\Omega})(x)=\\mathbf{1}_{\\mathcal{X}}(x)=1,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any $x\\in\\mathscr{X}$ . Therefore $\\pi_{\\mathsf{P}}(\\cdot|x)$ is a probability measure for any $x\\in\\mathscr{X}$ . Direct application of (6) shows that the associated policy operator corresponds to $\\mathsf{P}$ , namely $\\mathsf{P}_{\\pi_{\\mathsf{P}}}=\\mathsf{P}$ as desired. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Given the correspondence between policies and their Markov operator according to (6) and Proposition A.2, in the following we will denote the policy operator associated to a policy $\\pi$ only $\\mathsf{P}$ where clear from context. ", "page_idx": 14}, {"type": "text", "text": "With the definition of the Markov operator in place, we can now prove the following result introduced in the main paper. ", "page_idx": 14}, {"type": "text", "text": "Proposition 2 (Well-specified CME). Under Assumption $^{\\,l}$ , $(\\mathsf{T}|_{\\mathcal{F}})^{*}=(\\mathsf{T}^{*})|_{\\mathcal{G}}$ and, for any $(x,a)\\in\\Omega$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\mathsf{T}|_{\\mathcal{F}})^{*}\\psi(x,a)=\\int_{\\mathcal{X}}\\varphi(x^{\\prime})\\,\\tau(x^{\\prime}|x,a)=\\mathbb{E}[\\varphi(X^{\\prime})|X=x,A=a].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Recall that since they are Hilbert spaces $\\mathcal{F}\\cong\\mathcal{F}^{*}$ and $\\mathcal{G}\\cong\\mathcal{G}^{*}$ are isometric to their dual and therefore we can interpret any $f\\in\\mathcal F$ as the function $f(\\cdot)=\\langle f,\\varphi(\\cdot)\\rangle$ with some abuse of notation, where clear from context. By Assumption 1 we have that $\\mathsf{T}|_{\\mathcal{F}}$ takes values in $\\mathcal{G}$ . This means that $(\\mathsf T|_{\\mathcal F}f)\\in\\mathcal G$ or, in other words ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathsf T|_{\\mathcal F}f,\\psi(x,a)\\rangle=(\\mathsf T|_{\\mathcal F}f)(x,a)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\int_{\\mathcal F}f(x^{\\prime})\\,\\tau(d x^{\\prime}|x,a)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\int_{\\mathcal F}\\langle f,\\varphi(x^{\\prime})\\rangle\\ \\tau(d x^{\\prime}|x,a)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\left\\langle f,\\displaystyle\\int\\varphi(x^{\\prime})\\,\\tau(d x^{\\prime}|x,a)\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "from which we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\langle f,(\\mathsf{T}|_{\\mathcal{F}})^{*}\\psi(x,a)\\right\\rangle=\\left\\langle f,\\int\\varphi(x^{\\prime})\\;\\tau(d x^{\\prime}|x,a)\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since the above equality holds for any $f\\in{\\mathcal{F}}\\left(9\\right)$ holds, as desired. ", "page_idx": 14}, {"type": "text", "text": "We note that the result can be extended to the setting where $\\mathsf{T}|_{\\mathcal{F}}({\\mathcal{F}})\\subseteq{\\mathcal{G}}$ , namely the image of $\\mathsf{T}|_{\\mathcal{F}}$ is contained in $\\mathcal{G}$ , namely a sort of $(\\mathcal{F},\\mathcal{G})$ -compatibility for the transition operator (see Definition 1). ", "page_idx": 14}, {"type": "text", "text": "A.3 The operatorial formulation of RL ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "According to the operatorial characterization in (7), the action value function of a policy $\\pi$ is directly related to the action of the associated policy operator $\\mathsf{P}$ . To highlight this relation, we will adopt the following notation: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Action-value (or Q-)function. ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathsf{P})=\\left(\\mathsf{I d}-\\gamma\\mathsf{T}\\mathsf{P}\\right)^{-1}r.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 Value function. ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\nv({\\mathsf{P}})={\\mathsf{P}}q({\\mathsf{P}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 Cumulative reward. The RL objective functional ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ(\\mathsf{P})=\\left\\langle\\mathsf{P}\\left(\\mathsf{I d}-\\gamma\\mathsf{T}\\mathsf{P}\\right)^{-1}r,\\nu\\right\\rangle=\\left\\langle\\mathsf{P}q(\\mathsf{P}),\\nu\\right\\rangle=\\left\\langle v(\\mathsf{P}),\\nu\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 State visitation (or State occupancy) measure. By the characterization of the adjoints of $\\mathsf{P}$ and $\\top$ (see discussion in Section 3 we can represent the evolution of a state distribution $\\nu_{t}$ at time $t$ to the next state distribution as $\\nu_{t+1}=\\mathsf{T}^{*}\\mathsf{P}^{*}\\nu_{t}$ . Applying this relation recursively, we recover the state visitation probability associated to the starting state distribution $\\nu_{0}=$ $\\nu\\in\\mathcal{P}(\\mathcal{X})$ , the MDP with transition $\\top$ and the policy $\\mathsf{P}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\nd_{\\nu}(\\mathsf{P})=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}(\\mathsf{T}^{*}\\mathsf{P}^{*})^{t}\\nu=(1-\\gamma)\\left(\\mathsf{I d}-\\gamma\\mathsf{P}\\mathsf{T}\\right)^{-*}\\nu,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the $(1-\\gamma)\\gamma^{t}$ is a normalizing factor to guarantee that the series corresponds to a convex combination of the probability distributions $\\nu_{t}$ , hence guaranteeing $d_{\\nu}({\\mathsf{P}})$ to be well-defined (namely it belongs to $\\mathcal{P}(\\mathcal{X}))$ . ", "page_idx": 15}, {"type": "text", "text": "Previous well-known RL results in operator form. Under the operatorial formulation of RL, we can recover several well-known results from the reinforcement literature with concise proofs. We recall here a few of these results that will be useful in the following. ", "page_idx": 15}, {"type": "text", "text": "Remark A.1. Algebraic manipulation of the cumulative expected reward $J(\\mathsf{P})$ implies ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ(\\mathsf{P})=\\left\\langle\\mathsf{P}\\left(\\mathsf{I d}-\\gamma\\mathsf{T P}\\right)^{-1}r,\\nu\\right\\rangle=\\left\\langle\\mathsf{P}r,(\\mathsf{I d}-\\gamma\\mathsf{P}\\mathsf{T})^{-\\ast}\\,\\nu\\right\\rangle=\\frac{1}{1-\\gamma}\\left\\langle\\mathsf{P}r,d_{\\nu}(\\mathsf{P})\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used Lemma A.1 and $d_{\\nu}({\\mathsf{P}})$ is the state visitation distribution starting from $\\nu$ and following the policy P. ", "page_idx": 15}, {"type": "text", "text": "The following result, known as Performance Difference Lemma [see e.g. 11, Lemma 1.16], will be instrumental to prove the convergence rates for PMD in Theorem 7. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3 (Performance difference). Let $\\mathsf{P}_{1}$ , $\\mathsf{P}_{2}$ two policy operators. The following equality holds ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ(\\mathsf{P}_{1})-J(\\mathsf{P}_{2})=\\frac{1}{1-\\gamma}\\left\\langle(\\mathsf{P}_{1}-\\mathsf{P}_{2})q(\\mathsf{P}_{2}),d_{\\nu}(\\mathsf{P}_{1})\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Using the definition of $J(\\mathsf{P}_{1})$ and Lemma A.1 one gets ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\mathbf{P}_{1})-J(\\mathbf{P}_{2})=\\left\\langle\\mathbf{P}_{1}\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{T}}\\mathbf{P}_{1}\\right)^{-1}r,\\nu\\right\\rangle-\\left\\langle\\mathbf{P}_{2}\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{T}}\\mathbf{P}_{2}\\right)^{-1}r,\\nu\\right\\rangle}\\\\ &{\\hphantom{=}\\left\\langle\\left(\\mathbf{d}-\\gamma\\mathbf{P}_{1}\\boldsymbol{\\Gamma}\\right)^{-1}\\mathbf{P}_{1}r,\\nu\\right\\rangle-\\left\\langle\\mathbf{\\mathcal{P}}_{2}\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{T}}\\mathbf{\\mathcal{P}}_{2}\\right)^{-1}r,\\nu\\right\\rangle}\\\\ &{\\hphantom{=}\\left\\langle\\left(\\mathbf{d}-\\gamma\\mathbf{P}_{1}\\boldsymbol{\\Gamma}\\right)^{-1}\\mathbf{P}_{1}\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{T}}\\mathbf{\\mathcal{P}}_{2}\\right)\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{T}}\\mathbf{\\mathcal{P}}_{2}\\right)^{-1}r,\\nu\\right\\rangle}\\\\ &{\\hphantom{=}\\;-\\left\\langle\\left(\\mathbf{(d}-\\gamma\\mathbf{P}_{1}\\boldsymbol{\\Gamma}\\right)^{-1}\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{P}}_{1}\\boldsymbol{\\Gamma}\\right)\\mathbf{\\mathcal{P}}_{2}\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{T}}\\mathbf{\\mathcal{P}}_{2}\\right)^{-1}r,\\nu\\right\\rangle}\\\\ &{\\hphantom{=}\\;-\\left\\langle\\left(\\mathbf{d}-\\gamma\\mathbf{P}_{1}\\boldsymbol{\\Gamma}\\right)^{-1}\\left(\\mathbf{P}_{1}\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{T}}\\mathbf{\\mathcal{P}}_{2}\\right)-(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{P}}_{1}\\boldsymbol{\\Gamma})\\mathbf{\\mathcal{P}}_{2}\\right)\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{T}}\\mathbf{\\mathcal{P}}_{2}\\right)^{-1}r,\\nu\\right\\rangle}\\\\ &{=\\left\\langle\\left(\\mathbf{d}-\\gamma\\mathbf{P}_{1}\\boldsymbol{\\Gamma}\\right)^{-1}\\left[\\mathbf{P}_{1}\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{T}}\\mathbf{\\mathcal{P}}_{2}\\right)\\left(|\\mathbf{d}-\\gamma\\mathbf{\\mathcal{P}}_{2}\\right)^{-1}r,\\nu\\right\\rangle\\right.}\\\\ &{\\hphantom{=}\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A direct consequence of the operator formulation of the performance difference lemma is the following operator-based characterization of the differential behavior of the RL objective. The result can be found in [11] for the case of finite state and action spaces, however here the operatorial formulation allows for a much more concise proof. ", "page_idx": 15}, {"type": "text", "text": "Corollary A.4 (Directional derivatives). For any two Markov $\\mathsf{P}_{1},\\mathsf{P}_{2}:B_{b}({\\mathcal{X}})\\to B_{b}(\\Omega)$ , we have that the directional derivative in $\\mathsf{P}_{1}$ towards $\\mathsf{P}_{2}$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{h\\to0}\\frac{J(\\mathsf{P}_{1}+h(\\mathsf{P}_{2}-\\mathsf{P}_{1}))-J(\\mathsf{P}_{1})}{h}=\\frac{1}{1-\\gamma}\\left<(\\mathsf{P}_{2}-\\mathsf{P}_{1})q(\\mathsf{P}_{1}),d_{\\nu}(\\mathsf{P}_{1})\\right>.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. The result follows by recalling that the space of Markov operators is convex, namely for any ${\\dot{h}}\\in[0,1]$ the term $\\mathsf{P}_{h}=\\mathsf{P}_{1}+h(\\bar{\\mathsf{P}}_{2}-\\mathsf{P}_{1})$ is still a Markov operator. Therefore, we can apply Lemma A.3 to obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\mathsf{P}_{h})-J(\\mathsf{P}_{1})=\\langle(\\mathsf{P}_{h}-\\mathsf{P}_{1})q(\\mathsf{P}_{1}),d_{\\nu}(\\mathsf{P}_{h})\\rangle}\\\\ &{\\qquad\\qquad\\qquad=h\\left\\langle(\\mathsf{P}_{2}-\\mathsf{P}_{1})q(\\mathsf{P}_{1}),d_{\\nu}(\\mathsf{P}_{h})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can therefore divide the above quantity by $h$ and send $h\\to0$ . The result follows by observing that $d_{\\nu}(\\mathsf{P}_{h})=(\\mathsf{I d}-\\gamma\\mathsf{T}\\mathsf{P}_{h})^{-*}\\nu\\to(\\mathsf{I d}-\\dot{\\gamma}\\mathsf{T}\\dot{\\mathsf{P}}_{1})^{-*}\\nu=d_{\\nu}(\\mathsf{P}_{1})$ for $h\\to0$ , since $\\|\\mathsf{P}_{h}\\|=1$ for any $h\\in[0,1]$ and the function $\\mathsf{M}\\mapsto(I-\\gamma\\mathsf{T}\\mathsf{M})^{-1}$ is continuous on the open ball of radius $1/\\gamma>1$ in $\\mathcal{L}(\\bar{B_{b}}(\\Omega),\\bar{B_{b}}(\\mathcal{X}))$ with respect to the operator norm. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Properties of $(\\mathsf{I d}-\\gamma\\mathsf{P T})^{-1}$ . The quantity $(\\mathsf{I d}-\\gamma\\mathsf{P T})^{-1}$ (note, not $(|{\\mathsf{d}}-\\gamma{\\mathsf{T}}{\\mathsf{P}})^{-1})$ plays a central role in the study of POWR. We prove here a few properties that will be useful in the following. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.5 (Properties of $\\mathsf{I d}-\\gamma\\mathsf{P T}$ ). The following facts are true: ", "page_idx": 16}, {"type": "text", "text": "1. For any $f\\geq0\\in B_{b}(\\mathcal{X})$ it holds $(\\mathsf{I d}-\\gamma\\mathsf{P T})^{-1}f\\geq f$ .   \n2. The operator $(1-\\gamma)(|{\\bf d}-\\gamma{\\sf P T})^{-1}$ is a Markov operator.   \n3. For any positive measure $\\nu\\in\\mathcal{M}(B_{b}(\\mathcal{X}))$ it holds $\\left(1-\\gamma\\right)\\Vert\\left(\\left|\\mathsf{d}-\\gamma\\mathsf{P T}\\right)^{-*}\\nu\\right|\\Vert_{\\mathrm{TV}}=\\Vert\\nu\\Vert_{\\mathrm{TV}}.$   \n4. For any positive measure $\\nu\\in\\mathcal{M}(B_{b}(\\mathcal{X}))$ it holds $\\|\\mathsf{P}^{*}\\nu\\|_{\\mathrm{TV}}=\\|\\nu\\|_{\\mathrm{TV}}$ .   \n5. For any bounded linear operator $\\mathsf{X},$ policy operator $\\mathsf{P}$ and discount factor $\\gamma<\\|\\sf X\\|$ , it holds   \n$\\begin{array}{r}{\\left\\|(|\\mathsf{d}-\\gamma\\mathsf{X}\\mathsf{P})^{-1}\\right\\|_{\\infty}\\le1/(1-\\gamma\\left\\|\\mathsf{X}\\right\\|)}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Since both $\\top$ and $\\mathsf{P}$ are Markov operators by construction, it immediately follows that their composition is a Markov operator as well. Using the Neumann series representation of $(|\\mathsf{d}-\\gamma\\mathsf{P}\\mathsf{T})^{-1}$ it follows that for all $f\\geq\\bar{0}\\in B_{b}(\\mathcal{X})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\mathsf{I d}-\\gamma\\mathsf{P T})^{-1}f=\\sum_{t=0}^{\\infty}\\gamma^{t}(\\mathsf{P T})^{t}f=f+\\sum_{t=1}^{\\infty}\\gamma^{t}(\\mathsf{P T})^{t}f\\ge f\\ge0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "proving (1). Further, ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\mathsf{I d}-\\gamma\\mathsf{P T})^{-1}\\mathbf{1}_{\\mathcal{X}}=\\sum_{t=0}^{\\infty}\\gamma^{t}(\\mathsf{P T})^{t}\\mathbf{1}_{\\mathcal{X}}=\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbf{1}_{\\mathcal{X}}=\\frac{\\mathbf{1}_{\\mathcal{X}}}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "showing that $(1-\\gamma)(\\mathsf{I d}-\\gamma\\mathsf{P T})^{-1}\\mathbf{1}_{\\mathcal{X}}=\\mathbf{1}_{\\mathcal{X}}$ and proving (2). Finally, since $(1-\\gamma)(|{\\mathsf{d}}-\\gamma{\\mathsf{P T}})^{-1}$ and $\\mathsf{P}$ are Markov operators, (3) and (4) follow from the direct application of [29, Theorem 19.2]. For the last point (5), let $f\\in B_{b}(\\Omega)$ . As $\\mathsf{P}$ is a conditional expectation operator, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\mathsf{X P}f\\right\\|_{\\infty}\\leq\\left\\|\\mathsf{X}\\right\\|\\mathsf{P}(\\mathbf{1}_{\\Omega}\\left\\|f\\right\\|_{\\infty})=\\left\\|\\mathsf{X}\\right\\|\\left\\|f\\right\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Where the inequality is just the conditional version of Jensen\u2019s inequality [44, Chapter 5] applied on the (convex) $\\|\\cdot\\|_{\\infty}$ function, while the equality comes from the fact that TP is a Markov operator. Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{sup}_{1\\in\\mathbb{I}|\\|_{\\infty}=1}\\!\\left\\|(\\mathsf{I d}-\\gamma\\mathsf{X P})^{-1}f\\right\\|_{\\infty}=\\displaystyle\\operatorname*{sup}_{\\parallel f\\|_{\\infty}=1}\\!\\!\\!\\!\\prod_{t=0}^{\\infty}\\!(\\gamma\\mathsf{X P})^{t}f\\big\\|_{\\infty}}&{}\\\\ {\\displaystyle\\leq\\operatorname*{sup}_{\\parallel f\\|_{\\infty}=1}\\!\\!\\!\\!\\sum_{t=0}^{\\infty}\\!\\gamma^{t}\\big\\|(\\mathsf{X P})^{t}f\\big\\|_{\\infty}}&{}\\\\ {\\displaystyle\\leq\\displaystyle\\operatorname*{sup}_{\\parallel f\\|_{\\infty}=1}\\!\\sum_{t=0}^{\\infty}\\gamma^{t}\\big\\|\\mathsf{X}\\big\\|^{t}\\big\\|f\\big\\|_{\\infty}}&{}\\\\ {\\displaystyle(\\|f\\|_{\\infty}=1)=\\frac{1}{1-\\gamma\\|\\mathsf{X}\\|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Simulation Lemma. We report here the Simulation lemma, since it will be key to bridging the gap between Policy Mirror Descent and Conditional Mean Embeddings in Theorem 9 through Lemma 8. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.6 (Simulation Lemma [30]-Lemma 2.2). Let $\\gamma>0$ and let $\\mathsf{T}_{1}$ , $\\mathsf{T}_{2}$ two linear operators with operator norm strictly less than $\\gamma$ . Let $\\mathsf{P}$ be a policy operator. Denote by $q(\\mathsf{P},\\mathsf{T})\\,=\\,(\\mathsf{I d}\\mathrm{~-~}$ $\\gamma\\mathsf{T}\\mathsf{P})^{\\bar{-}1}r$ the (generalized) action-value function associated to these terms and $v(\\mathsf{P},\\mathsf{T})=\\mathsf{P}q(\\mathsf{P},\\mathsf{T})$ the corresponding value function. Then the following equality holds ", "page_idx": 17}, {"type": "equation", "text": "$$\nq(\\mathsf{P},\\mathsf{T}_{1})-q(\\mathsf{P},\\mathsf{T}_{2})=\\gamma\\left(\\mathsf{I d}-\\gamma\\mathsf{T}_{1}\\mathsf{P}\\right)^{-1}\\left(\\mathsf{T}_{2}-\\mathsf{T}_{1}\\right)v(\\mathsf{P},\\mathsf{T}_{2})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Using the same technique of the proof of Lemma A.3 one has ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathsf{P},\\mathsf{T}_{1})-q(\\mathsf{P},\\mathsf{T}_{2})=(\\mathsf{I d}-\\gamma\\mathsf{T}_{1}\\mathsf{P})^{-1}\\,r-(\\mathsf{I d}-\\gamma\\mathsf{T}_{2}\\mathsf{P})^{-1}\\,r}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\gamma\\left(\\mathsf{I d}-\\gamma\\mathsf{T}_{1}\\mathsf{P}\\right)^{-1}\\left(\\mathsf{T}_{2}-\\mathsf{T}_{1}\\right)\\mathsf{P}\\left(\\mathsf{I d}-\\gamma\\mathsf{T}_{2}A\\right)^{-1}r}\\\\ &{\\qquad\\qquad\\qquad=\\gamma\\left(\\mathsf{I d}-\\gamma\\mathsf{T}_{1}\\mathsf{P}\\right)^{-1}\\left(\\mathsf{T}_{2}-\\mathsf{T}_{1}\\right)v(\\mathsf{P},\\mathsf{T}_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we have used fact that for any two invertible operators $\\mathsf{M}$ and $\\mathsf{P}$ it holds $\\mathsf{M}^{-1}\\mathrm{~-~}\\mathsf{P}^{-1}\\mathrm{~=~}$ ${\\mathsf{M}}^{-1}({\\mathsf{P}}-{\\mathsf{M}}){\\mathsf{P}}^{-1}$ for the second equation and applied the operatorial characterization of the value function to conclude the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "We then have the following result, which hinges on a generalization of the standard Simulation lemma in [30, Lemma 2.2] where we account also for the reward function to vary. ", "page_idx": 17}, {"type": "text", "text": "Corollary A.7. Let $\\gamma>0$ and let ${\\sf T}_{1},\\,{\\sf T}_{2}$ two linear operators with operator norm strictly less than $\\gamma$ . Let $r_{1}$ and $r_{2}$ be two reward functions and $\\mathsf{P}$ a policy operator. Denote by $q(\\mathsf{P},\\mathsf{T},r)=(\\mathsf{I d}\\!-\\!\\gamma\\mathsf{T}\\mathsf{P})^{-1}r$ the (generalized) action-value function associated to these terms and $v(\\mathsf{P},\\mathsf{T},r)=\\mathsf{P}q(\\mathsf{P},\\mathsf{T},r)$ the corresponding value function. Then the following equality holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\l(\\mathsf{P},\\mathsf{T}_{1},r_{1})-q(\\mathsf{P},\\mathsf{T}_{2},r_{2})=(\\mathsf{I d}-\\gamma\\mathsf{T}_{1}\\mathsf{P})^{-1}(r_{1}-r_{2})+\\gamma(\\mathsf{I d}-\\gamma\\mathsf{T}_{1}\\mathsf{P})^{-1}(\\mathsf{T}_{2}-\\mathsf{T}_{1})v(\\mathsf{P},\\mathsf{T}_{2},r_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The difference between action-value functions can be written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathsf{P},\\mathsf{T}_{1},r_{1})-q(\\mathsf{P},\\mathsf{T}_{2},r_{2})=(\\mathsf{I d}-\\gamma\\mathsf{T}_{1}\\mathsf{P})^{-1}r_{1}-(\\mathsf{I d}-\\gamma\\mathsf{T}_{2}\\mathsf{P})^{-1}r_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(\\mathsf{I d}-\\gamma\\mathsf{T}_{1}\\mathsf{P})^{-1}(r_{1}-r_{2})+\\left[(\\mathsf{I d}-\\gamma\\mathsf{T}_{1}\\mathsf{P})^{-1}-(\\mathsf{I d}-\\gamma\\mathsf{T}_{2}\\mathsf{P})^{-1}\\right]r_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we added and removed a term $(|\\mathsf{d}-\\gamma\\mathsf{T}_{1}\\mathsf{P})^{-1}r_{2}$ . The result follows by plugging in the Simulation Lemma A.6 for the second term of the right hand side. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "The corollary above will be useful in Appendix C to control the approximation error of the estimates $\\hat{q}_{\\pi_{t}}$ appearing in the convergence rates for inexact PMD in Theorem 7. ", "page_idx": 17}, {"type": "text", "text": "A.4 Action-value Estimator for $(\\mathcal{G},\\mathcal{F})$ -compatible Policies ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We can leverage the notation introduced in this section to prove the following form for the world model-based estimator of the action-value function. ", "page_idx": 17}, {"type": "text", "text": "Proposition 3. Let ${\\sf T}_{n}=S_{n}^{*}B Z_{n}\\in{\\sf H S}(\\mathcal{F},\\mathcal{G})$ and $r_{n}=S_{n}^{*}b\\in\\mathcal{G}$ for respectively a $B\\in\\mathbb{R}^{n\\times n}$ and $b\\in\\mathbb{R}^{n}$ . Let $\\pi$ be $(\\mathcal{G},\\mathcal{F})$ -compatible. Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{q}_{\\pi}=(\\mathsf{I d}-\\gamma\\mathsf{T}_{n}\\mathsf{P}_{\\pi})^{-1}r_{n}=S_{n}^{\\ast}(\\mathsf{I d}-\\gamma B M_{\\pi})^{-1}b}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $M_{\\pi}=Z_{n}\\mathsf{P}_{\\pi}S_{n}^{*}\\in\\mathbb{R}^{n\\times n}$ is the matrix with entries ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(M_{\\pi}\\right)_{i j}=\\langle\\varphi(x_{i}^{\\prime}),\\mathsf{P}_{\\pi}\\psi(x_{j},a_{j})\\rangle=\\int_{A}\\left\\langle\\psi(x_{i}^{\\prime},a),\\psi(x_{j},a_{j})\\right\\rangle\\;\\pi(d a|x_{i}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. By hypothesis ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{q}_{\\pi}=(\\mathsf{I d}-\\gamma\\mathsf{T}_{n}\\mathsf{P}_{\\pi})^{-1}r_{n}=(\\mathsf{I d}-\\gamma S_{n}^{\\ast}B Z_{n}\\mathsf{P}_{\\pi})^{-1}S_{n}^{\\ast}b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Eq. (12) follows by applying Lemma A.1. Eq. (13) can be verified by direct calculation. Denote by $(e_{i})_{i=1}^{m}$ the vectors of the canonical basis in $\\mathbb{R}^{n}$ . Then, for any $i,j=1,\\dots,n$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n(M_{\\pi})_{i j}=\\langle e_{i},M_{\\pi}e_{j}\\rangle=\\langle e_{i},Z_{n}\\mathsf{P}_{\\pi}S_{n}^{*}e_{j}\\rangle=\\langle Z_{n}^{*}e_{i},\\mathsf{P}_{\\pi}S_{n}^{*}e_{j}\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, we recall that the two operators $S_{n}:{\\mathcal{G}}\\rightarrow\\mathbb{R}^{n}$ and $Z_{n}:{\\mathcal{F}}\\rightarrow\\mathbb{R}^{n}$ are the evaluation operators for respectively the points $(x_{i},a_{i})_{i=1}^{n}$ and $(x_{i}^{\\prime})_{i=1}^{n}$ . Namely, for any vector $\\boldsymbol{v}\\in\\mathbb{R}^{n}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{n}^{*}v=\\sum_{i=1}^{n}v_{i}\\psi(x_{i},a_{i})\\qquad\\mathrm{and}\\qquad Z_{n}^{*}v=\\sum_{i=1}^{n}v_{i}\\varphi(x_{i}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n(M_{\\pi})_{i j}=\\langle Z_{n}^{*}e_{i},\\mathsf{P}_{\\pi}S_{n}^{*}e_{j}\\rangle=\\langle\\varphi(x_{i}^{\\prime}),\\mathsf{P}_{\\pi}\\psi(x_{j},a_{j})\\rangle\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\mathsf{P}_{\\pi}$ is $(\\mathcal{G},\\mathcal{F})$ -compatible by hypothesis, we can leverage the same reasoning used in Proposition 2 to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\mathsf{P}_{\\pi}|_{\\mathcal{G}})^{*}\\varphi(x^{\\prime})=\\int_{A}\\psi(x^{\\prime},a)\\;\\pi(d a|x^{\\prime})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any $x^{\\prime}\\in\\mathcal{X}$ . By plugging this equation in the previous characterization for $(M_{\\pi})_{i j}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\varphi(x_{i}^{\\prime}),\\mathsf{P}_{\\pi}\\psi(x_{j},a_{j})\\rangle=\\langle\\mathsf{P}_{\\pi}^{*}\\varphi(x_{i}^{\\prime}),\\psi(x_{j},a_{j})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\left\\langle\\int_{A}\\psi(x_{i}^{\\prime},a)\\;\\pi(d a|x_{i}^{\\prime}),\\psi(x_{j},a_{j})\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\int_{A}\\langle\\psi(x_{i}^{\\prime},a),\\psi(x_{j},a_{j})\\rangle\\;\\;\\pi(d a|x_{i}^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as required. ", "page_idx": 18}, {"type": "text", "text": "A.5 Separable Spaces ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We show here the sufficient condition for $(\\mathcal{G},\\mathcal{F})$ -compatibility of a policy in the case of the separable spaces introduced in Section 4. ", "page_idx": 18}, {"type": "text", "text": "Proposition 4 (Separable Spaces). Let $\\phi:\\,\\mathcal{X}\\,\\rightarrow\\,\\mathcal{H}$ be a feature map into a Hilbert space $\\mathcal{H}$ . Let $\\mathcal{F}=\\mathcal{H}\\otimes\\mathcal{H}$ and $\\mathcal{G}\\,=\\,\\mathbb{R}^{|\\mathcal{A}|}\\otimes\\mathcal{H}$ with feature maps respectively $\\varphi(x)\\,=\\,\\phi(x)\\otimes\\phi(x)$ and $\\psi(x,a)=\\phi(x)\\otimes e_{a},$ , with $\\boldsymbol{e}_{a}\\in\\mathbb{R}^{|\\boldsymbol{A}|}$ the one-hot encoding of action $a\\in A.$ . Let $\\pi:{\\mathcal{X}}\\to\\Delta(A)$ be a policy such that $\\pi(a|\\cdot)=\\langle p_{a},\\phi(\\cdot)\\rangle$ with $p_{a}\\in\\mathcal{H}$ for any $a\\in{\\mathcal{A}}$ . Then, $\\pi$ is $(\\mathcal{G},\\mathcal{F})$ -compatible. ", "page_idx": 18}, {"type": "text", "text": "Proof. The proposition follows from observing that for any $\\boldsymbol{v}\\,\\in\\,\\mathbb{R}^{|\\boldsymbol{A}|}$ and $h\\in\\mathcal H$ , applying $\\mathsf{P}_{\\pi}$ according to (6) to the function $g(x,a)=\\left\\langle h,\\phi(x)\\right\\rangle\\left\\langle v,D e_{a}\\right\\rangle$ yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathsf{P}_{\\pi}g)(x)=\\displaystyle\\sum_{a\\in A}g(x,a)\\pi(a|x)=\\left<h,\\phi(x)\\right>\\displaystyle\\sum_{a\\in A}\\left<v,D e_{a}\\right>\\pi(a|x)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left<h,\\phi(x)\\right>\\displaystyle\\sum_{a\\in A}\\left<v,D e_{a}\\right>\\left<p_{a},\\phi(x)\\right>}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left<h\\otimes\\displaystyle\\sum_{a\\in A}\\left<v,D e_{a}\\right>p_{a},\\phi(x)\\otimes\\phi(x)\\right>}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence $(\\mathsf{P}_{\\pi}g)(x)=\\langle f,\\varphi(x)\\rangle$ with $f=h\\otimes h^{\\prime}\\in\\mathcal{H}\\otimes\\mathcal{H}=\\mathcal{F}$ and $\\begin{array}{r}{h^{\\prime}=\\sum_{a\\in\\mathcal{A}}\\left\\langle v,D e_{a}\\right\\rangle p_{a}\\in\\mathcal{H}}\\end{array}$ . Therefore, the restriction of $\\mathsf{P}_{\\pi}$ to $\\mathcal{G}$ takes value in $\\mathcal{F}$ as desired. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B Policy Mirror Descent ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section we briefly review the tools needed to formulate the PMD method and discuss the convergence rates for inexact PMD. Most of the discussion follows the presentation in [12] formulated within the notation used in this work. ", "page_idx": 18}, {"type": "text", "text": "Let $D:\\Delta(A)\\times\\mathrm{rint}\\Delta(A)\\to\\mathbb{R}$ a Bregman divergence [35, Definition 9.2] over the probability simplex, where $\\mathrm{rint}\\Delta(A)$ denotes the relative interior of $\\Delta(A)$ . In the following, for any $t\\in{\\mathbb N}$ we will denote by $\\pi_{t}$ the policy produced at iteration $t$ by a PMD algorithm according to the update (2) (with either the exact action-value function or an estimator, as discussed in Section 3) with divergence $D$ and step-size $\\eta>0$ . We denote $\\mathsf{P}_{t}=\\mathsf{P}_{\\pi_{t}}$ the associated operator. We recall here the PMD update from (2), highlighting the dependency on the policy operator $\\mathsf{P}_{t}$ via the action-value function $\\bar{q}(\\mathsf{P}_{t})$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi_{t+1}(\\cdot|x)\\in\\operatorname{argmin}_{p\\in\\Delta(A)}\\left\\{-\\eta\\sum_{a\\in A}q(\\mathsf{P}_{t})(x,a)p_{a}+D(p;\\pi_{t}(\\cdot|x))\\right\\}\\quad{\\mathrm{for~all~}}x\\in{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "While this point-wise characterization is sufficient to define the updated policy $\\pi_{t+1}:\\mathcal{X}\\to\\Delta(\\mathcal{A})$ from the previous $\\pi_{t}$ and its action-value function $q(\\mathsf{P}_{t})$ , we need to guarantee that $\\pi_{t+1}$ is measurable. If that were not the case, we would not be able to guarantee the existence of a $\\mathsf{P}_{t+1}$ associated with it, possibly affecting the well-definiteness of iteratively applying the mirror descent update (B.1). The following result addresses this issue. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.1 (Measurability of the Mirror Descent updates). Let $D:\\Delta(A)\\times\\operatorname{rint}\\Delta(A)\\to\\mathbb{R}$ be $a$ Bregman divergence continuous in its first argument. There exists a measurable policy $\\pi_{t+1}:\\mathcal{X}\\to$ $\\Delta(A)$ that satisfies (B.1) for all $x\\in\\mathscr{X}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. The proof follows from the Measurable Maximum Theorem [29, Theorem 18.19]. Let us denote $f_{t}:\\bar{\\mathcal{X}}\\times\\Delta(\\mathcal{A})\\rightarrow\\mathbb{R}$ the function ", "page_idx": 19}, {"type": "equation", "text": "$$\nf_{t}(x,p):=-\\eta\\sum_{a\\in\\mathcal{A}}q(\\mathsf{P}_{t})(x,a)p_{a}+D(p;\\pi_{t}(x)).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let also $\\kappa:{\\mathcal{X}}\\twoheadrightarrow\\Delta(A)$ be the constant correspondance $x\\mapsto\\Delta(A)$ for all $x\\in\\mathscr{X}$ . $\\kappa$ clearly has nonempty compact values, and it is also weakly measurable since for any open set $G\\subset\\Delta(\\dot{A})$ , its lower inverse $\\kappa^{\\ell}(G):=\\{x\\in\\mathcal{X}:\\kappa(x)\\cap G\\neq\\varnothing\\}=\\mathcal{X}$ belongs to the Borel sigma-algebra of $\\mathcal{X}$ . Finally, since $q(\\mathsf{P}_{t})\\in B_{b}(\\Omega)$ , and by assumption $D$ is continuous in its first argument, then we have that $f_{t}$ is a Carath\u00e9odory function. Then, by [29, Theorem 18.19] we have that the correspondence of minimizers $\\mu:\\mathcal{X}\\rightarrow\\Delta(\\mathcal{A})$ defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mu(x):=\\left\\{p_{*}\\in\\kappa(x):f_{t}(x,p_{*})=\\operatorname*{min}_{p\\in\\kappa(x)}f_{t}(x,p)\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "admits a measurable selector, which we denote $\\pi_{t+1}:\\mathcal{X}\\to\\Delta(A)$ , proving the statement of the Lemma. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "The previous Lemma is the key technical step enabling us to extend the convergence rates of Mirror Descent proved in [12] to non-tabular settings. We now state and prove fa ew Lemmas instrumental to prove Theorem 7. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.2 (Three-points lemma). Let $\\pi_{t+1}:\\mathcal{X}\\to\\Delta(A)$ a measurable minimizer of (B.2) and $\\mathsf{P}_{t+1}$ its associated operator. For every measurable policy $\\pi:{\\mathcal{X}}\\rightarrow\\Delta(A)$ (alongside its associated operator $\\mathsf{P}$ ) it holds ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta\\left[(\\mathsf{P}_{t+1}-\\mathsf{P})q(\\mathsf{P}_{t})\\right](x)\\geq D(\\pi(x);\\pi_{t+1}(x))-D(\\pi(x);\\pi_{t}(x))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The function $f_{t}(x,p)$ in (B.2) is convex and differentiable in $p$ as it is a sum of a linear function and a (strictly convex) Bregman divergence. By the first-order optimality condition [35, Corollay 3.68], a minimizer $p_{*}$ of $f_{t}(x,\\cdot)$ satisfies, for all $p\\in\\Delta(A)$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\langle\\nabla f_{t}(x,p_{*}),\\pi(x)-p_{*}\\rangle\\geq0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\pi_{t+1}(x)$ is a minimizer of $f_{t}(x,\\cdot)$ by assumption, letting $p_{*}~=~\\pi_{t+1}(x)$ , the first order optimality condition (B.4) becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\left[(\\mathsf{P}_{t+1}-\\mathsf{P})q(\\mathsf{P}_{t})\\right](x)-\\langle\\nabla\\psi(\\pi_{t}(x))-\\nabla\\psi(\\pi_{t+1}(x)),\\pi(x)-\\pi_{t+1}(x)\\rangle\\ge0\\quad\\Longrightarrow}\\\\ &{\\eta\\left[(\\mathsf{P}_{t+1}-\\mathsf{P})q(\\mathsf{P}_{t})\\right](x)\\ge D(\\pi(x);\\pi_{t+1}(x))-D(\\pi(x);\\pi_{t}(x))+D(\\pi_{t+1}(x);\\pi_{t}(x))\\quad\\Longrightarrow}\\\\ &{\\eta\\left[(\\mathsf{P}_{t+1}-\\mathsf{P})q(\\mathsf{P}_{t})\\right](x)\\ge D(\\pi(x);\\pi_{t+1}(x))-D(\\pi(x);\\pi_{t}(x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where in the first line we used the definition of Bregman divergence [35, Definition 9.2] $D(p;q):=$ $\\psi(p)-\\psi(q)-\\langle\\nabla\\psi(q),p-q\\rangle$ for a suitable Legendre function $\\psi:\\Delta(A)\\rightarrow\\mathbb{R}.$ , the first implication follows from the three-points property of Bregman divergences [35, Lemma 9.11], and the last implication from the positivity of $D(\\pi_{t+1}(x);\\pi_{t}(x))$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Corollary B.3 (MD Iterations are monotonically increasing). This Corollary is essentially a restatement of [12, Lemma $7J.$ Let $(\\mathsf{P}_{t})_{t\\in\\mathbb{N}}$ be the sequence of policy operators associated to the measurable minimizers of (B.1) for all $t\\in{\\mathbb N}$ . For all $x\\in\\mathscr{X}$ it holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left[(\\mathsf{P}_{t+1}-\\mathsf{P}_{t})q(\\mathsf{P}_{t})\\right](x)\\geq0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\nJ(\\mathsf{P}_{t+1})-J(\\mathsf{P}_{t})\\geq0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "i.e. the objective function is always increased by a mirror descent iteration. Further, i ${}^{\\varepsilon}\\tilde{q}(\\mathsf{P}_{t})\\in B_{b}(\\Omega)$ is such that $\\left\\|q(\\mathsf{P}_{t})-\\tilde{q}(\\mathsf{P}_{t})\\right\\|_{\\infty}\\le\\varepsilon_{t}$ , then (B.5) holds inexactly on $\\tilde{q}(\\mathsf{P}_{t})$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left[\\left(\\mathsf{P}_{t+1}-\\mathsf{P}_{t}\\right)\\!\\tilde{q}\\!\\left(\\mathsf{P}_{t}\\right)\\right](x)\\geq-2\\varepsilon_{t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By setting $\\pi(x)=\\pi_{t}(x)$ in (B.3), and recalling that $D(p;q)\\geq0$ with equality if and only if $p=q$ , it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta\\left[(\\mathsf{P}_{t+1}-\\mathsf{P}_{t})q(\\mathsf{P}_{t})\\right](x)\\geq D(\\pi_{t}(x);\\pi_{t+1}(x))\\geq0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "giving (B.5). Integrating (B.5) over $(\\mathsf{I d}-\\gamma\\mathsf{P}_{t+1}\\mathsf{T})^{-*}\\,\\nu$ and using the Performance Difference Lemma A.3 one gets (B.6). Finally, we get (B.7) from ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\left(\\mathsf{P}_{t+1}-\\mathsf{P}_{t}\\right)\\widetilde{q}(\\mathsf{P}_{t})\\right](x)=\\left[(\\mathsf{P}_{t+1}-\\mathsf{P}_{t})q(\\mathsf{P}_{t})\\right](x)+\\left[\\left(\\mathsf{P}_{t+1}-\\mathsf{P}_{t}\\right)(\\widetilde{q}(\\mathsf{P}_{t})-q(\\mathsf{P}_{t}))\\right](x)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\left[\\left(\\mathsf{P}_{t+1}-\\mathsf{P}_{t}\\right)(\\widetilde{q}(\\mathsf{P}_{t})-q(\\mathsf{P}_{t}))\\right](x)}\\\\ &{\\qquad\\qquad\\qquad\\geq-\\|(\\mathsf{P}_{t+1}-\\mathsf{P}_{t})(\\widetilde{q}(\\mathsf{P}_{t})-q(\\mathsf{P}_{t}))\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\geq-\\|\\mathsf{P}_{t+1}-\\mathsf{P}_{t}\\|\\|\\widetilde{q}(\\mathsf{P}_{t})-q(\\mathsf{P}_{t})\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\geq-2\\varepsilon_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Where the first inequality follows from (B.5), and the latter from the fact that policy operators are Markov operators and have norm 1, and $\\left\\|\\mathsf{P}_{t+1}-\\mathsf{P}_{t}\\right\\|\\leq\\left\\|\\mathsf{P}_{t+1}\\right\\|+\\left\\|\\mathsf{P}_{t}\\right\\|=2$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "B.1 Convergence rates of PMD ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We are finally ready to prove the convergence rates for the Policy Mirror Descent algorithm (B.1). The proof technique is loosely based on [12, Theorem 8, Lemma 12], and extends them to the case of general state spaces through the key Lemma B.1 and using a fully operatorial formalism. ", "page_idx": 20}, {"type": "text", "text": "Theorem 7 (Convergenge of Inexact PMD). Let $(\\pi_{t})_{t\\in\\mathbb{N}}$ be a sequence of policies generated by Algorithm 1 that are all $(\\mathcal{G},\\mathcal{F})$ -compatible. If the action-value functions $\\hat{q}_{\\pi_{t}}$ are estimated with an error $\\left\\|q_{\\pi_{t}}-\\hat{q}_{\\pi_{t}}\\right\\|_{\\infty}\\leq\\varepsilon_{t}$ , the iterates of Algorithm $^{\\,l}$ converge to the optimal policy as ", "page_idx": 20}, {"type": "equation", "text": "$$\nJ(\\pi_{*})-J(\\pi_{T})\\leq\\varepsilon_{T}+O\\left(\\frac{1}{T}+\\frac{1}{T}\\sum_{t=0}^{T-1}\\varepsilon_{t}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\pi_{*}:\\mathcal{X}\\to\\Delta(\\mathcal{A})$ is a measurable maximizer of (8). ", "page_idx": 20}, {"type": "text", "text": "Proof. As usual, in this proof we denote the estimated and exact action-value functions as $q_{n}(\\mathsf{P}_{t}):=$ ${\\hat{q}}_{\\pi_{t}}\\,=\\,(|{\\mathsf{d}}-\\gamma\\mathsf{T}_{n}\\mathsf{P}_{t})^{-1}{\\bar{r}}_{n}$ and $q(\\mathsf{P}_{t})\\,:=\\,q_{\\pi_{t}}\\,=\\,(\\mathsf{I d}-\\gamma\\mathsf{T}\\mathsf{P}_{t})^{-1}r.$ , respectively. From hypothesis, Algorithm 1 is well-defined since all policies it generates are $(\\mathcal{G},\\mathcal{F})$ -compatible. The resulting sequence of policies $(\\pi_{t})_{t\\in\\mathbb{N}}$ are generated via the update rule (14) on the inexact action-value functions $q_{n}(\\mathsf{P}_{t})$ , as defined in (12). As the update rule (14) is a (measurable) minimizer of (B.1) when $D$ equals the Kullback-Leibler divergence, the three-points Lemma B.2 with $\\pi(x)=\\pi_{*}(x)$ yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left[(\\mathsf{P}_{*}-\\mathsf{P}_{t+1})q_{n}(\\mathsf{P}_{t})\\right](x)\\leq\\frac{1}{\\eta}D(\\pi_{*}(x);\\pi_{t}(x))-\\frac{1}{\\eta}D(\\pi_{*}(x);\\pi_{t+1}(x)).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Adding and subtracting the term $\\left[\\left(\\mathsf{P}_{*}-\\mathsf{P}_{t+1}\\right)\\!q(\\mathsf{P}_{t})\\right](x)$ , and bounding the remaining difference as $[(\\mathsf{P}_{*}-\\mathsf{P}_{t+1})(q(\\mathsf{P}_{t})-\\bar{q}_{n}(\\mathsf{P}_{t}))]\\,(\\ddot{x})\\leq2\\varepsilon_{t}-\\mathrm{se}$ e the derivation of (B.8) \u2013 one gets ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left[\\left(\\mathsf{P}_{*}-\\mathsf{P}_{t+1}\\right)q(\\mathsf{P}_{t})\\right](x)\\le2\\varepsilon_{t}+\\frac{1}{\\eta}D(\\pi_{*}(x);\\pi_{t}(x))-\\frac{1}{\\eta}D(\\pi_{*}(x);\\pi_{t+1}(x)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Adding and subtracting $\\mathsf{P}_{t}q(\\mathsf{P}_{t})$ on the left side gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mathsf{P}_{*}-\\mathsf{P}_{t})q(\\mathsf{P}_{t})]\\left(x\\right)\\leq\\left[(\\mathsf{P}_{t+1}-\\mathsf{P}_{t})q(\\mathsf{P}_{t})\\right](x)+2\\varepsilon_{t}+\\frac{1}{\\eta}D(\\pi_{*}(x);\\pi_{t}(x))-\\frac{1}{\\eta}D(\\pi_{*}(x);\\pi_{t+1}(x)),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and integrating with respect to the positive measure $(\\mathsf{I d}-\\gamma\\mathsf{P}_{*}\\mathsf{T})^{-*}\\nu$ and using the performance difference Lemma A.3 on the left hand side one has ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{J(\\mathsf{P}_{*})-J(\\mathsf{P}_{t})\\leq\\displaystyle\\frac{1}{1-\\gamma}\\left\\langle(\\mathsf{P}_{t+1}-\\mathsf{P}_{t})q(\\mathsf{P}_{t})+2\\varepsilon_{t},d_{\\nu}(\\mathsf{P}_{*})\\right\\rangle\\qquad}}\\\\ {{\\qquad\\qquad\\qquad\\displaystyle\\frac{1}{\\eta(1-\\gamma)}\\left\\langle D(\\pi_{*};\\pi_{t})-D(\\pi_{*};\\pi_{t+1}),d_{\\nu}(\\mathsf{P}_{*})\\right\\rangle,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used (A.8) on the right-hand-side terms. Since $(\\mathsf{P}_{t+1}-\\mathsf{P}_{t})q(\\mathsf{P}_{t})+2\\varepsilon_{t}\\,\\geq\\,0$ because of (B.7), we can use fact (1) from Lemma A.5 with $(\\mathsf{I d}-\\gamma\\mathsf{P}_{t+1}\\mathsf{T})^{-1}$ and the performance difference Lemma A.3 to get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle(\\mathsf{P}_{t+1}-\\mathsf{P}_{t})q(\\mathsf{P}_{t})+2\\varepsilon_{t},d_{\\nu}(\\mathsf{P}_{*})\\rangle\\leq\\langle(\\mathsf{I d}-\\gamma\\mathsf{P}_{t+1}\\mathsf{T})^{-1}\\left[(\\mathsf{P}_{t+1}-\\mathsf{P}_{t})q(\\mathsf{P}_{t})+2\\varepsilon_{t}\\right],d_{\\nu}(\\mathsf{P}_{*})\\rangle}\\\\ {=\\langle\\mathsf{P}_{t+1}q(\\mathsf{P}_{t+1}),d_{\\nu}(\\mathsf{P}_{*})\\rangle-\\langle\\mathsf{P}_{t}q(\\mathsf{P}_{t}),d_{\\nu}(\\mathsf{P}_{*})\\rangle+\\frac{2\\varepsilon_{t}}{1-\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Substituting this bound in (B.9) and summing from $t=0\\dots T-1$ one gets to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\sum_{t=0}^{T-1}J({\\mathsf{P}}_{*})-J({\\mathsf{P}}_{t})\\leq\\frac{1}{1-\\gamma}\\left(\\langle{\\mathsf{P}}_{T}q({\\mathsf{P}}_{T}),d_{\\nu}({\\mathsf{P}}_{*})\\rangle-\\langle{\\mathsf{P}}_{0}q({\\mathsf{P}}_{0}),d_{\\nu}({\\mathsf{P}}_{*})\\rangle\\right)+\\frac{2}{(1-\\gamma)^{2}}\\displaystyle\\sum_{t=0}^{T-1}\\varepsilon_{t}}}\\\\ {{\\displaystyle{\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{1}{\\eta(1-\\gamma)}\\left\\langle{\\cal D}(\\pi_{*};\\pi_{0})-{\\cal D}(\\pi_{*};\\pi_{T}),d_{\\nu}({\\mathsf{P}}_{*})\\right\\rangle}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using facts (3) and (4) from Lemma A.5 we have that the terms $\\langle\\mathsf{P}_{q}(\\mathsf{P}),d_{\\nu}(\\mathsf{P}_{*})\\rangle$ on the right hand side can be bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathsf{P}q(\\mathsf{P}),d_{\\nu}(\\mathsf{P}_{*})\\rangle=\\left\\langle\\mathsf{P}(\\mathsf{I d}-\\gamma\\mathsf{T}\\mathsf{P})^{-1}r,d_{\\nu}(\\mathsf{P}_{*})\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\left\\langle(\\mathsf{I d}-\\gamma\\mathsf{P}\\mathsf{T})^{-1}\\mathsf{P}r,d_{\\nu}(\\mathsf{P}_{*})\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad=\\left\\langle r,\\mathsf{P}^{*}(\\mathsf{I d}-\\gamma\\mathsf{P}\\mathsf{T})^{-*}d_{\\nu}(\\mathsf{P}_{*})\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad(\\mathsf{D u a l i t y})\\leq\\left\\|r\\right\\|_{\\infty}\\left\\|\\mathsf{P}^{*}(\\mathsf{I d}-\\gamma\\mathsf{P}\\mathsf{T})^{-*}d_{\\nu}(\\mathsf{P}_{*})\\right\\|_{\\mathrm{TV}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "while $-\\,\\langle D(\\pi_{*};\\pi_{T}),d_{\\nu}(\\mathsf{P}_{*})\\rangle$ can be dropped due to the positivity of Bregman divergences yielding ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}J(\\mathsf{P}_{*})-J(\\mathsf{P}_{t})\\leq\\!\\frac{2}{(1-\\gamma)^{2}}\\left(\\left\\|r\\right\\|_{\\infty}+\\sum_{t=0}^{T-1}\\varepsilon_{t}\\right)+\\frac{1}{\\eta(1-\\gamma)}\\left\\langle D(\\pi_{*};\\pi_{0}),d_{\\nu}(\\mathsf{P}_{*})\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now notice that for all $t<T$ it holds ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\mathsf{P}_{t})=\\langle\\mathsf{P}_{t}q(\\mathsf{P}_{t}),\\nu\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\langle\\mathsf{P}_{t}q_{n}(\\mathsf{P}_{t}),\\nu\\rangle+\\langle\\mathsf{P}_{t}(q(\\mathsf{P}_{t})-q_{n}(\\mathsf{P}_{t})),\\nu\\rangle}\\\\ &{\\quad\\mathrm{(Equation~}(\\mathsf{B}.6))\\leq\\langle\\mathsf{P}_{T}q_{n}(\\mathsf{P}_{T}),\\nu\\rangle+\\langle\\mathsf{P}_{t}(q(\\mathsf{P}_{t})-q_{n}(\\mathsf{P}_{t})),\\nu\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\langle\\mathsf{P}_{T}q_{n}(\\mathsf{P}_{T}),\\nu\\rangle+\\langle\\mathsf{P}_{t}(q(\\mathsf{P}_{t})-q_{n}(\\mathsf{P}_{t})),\\nu\\rangle+\\langle\\mathsf{P}_{T}(q_{n}(\\mathsf{P}_{T})-q(\\mathsf{P}_{T})),\\nu\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq J(\\mathsf{P}_{T})+\\varepsilon_{t}+\\varepsilon_{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "so that ", "page_idx": 21}, {"type": "equation", "text": "$$\nJ(\\mathsf{P}_{*})-J(\\mathsf{P}_{T})\\leq\\varepsilon_{T}+\\frac{1}{T}\\sum_{t=0}^{T-1}J(\\mathsf{P}_{*})-J(\\mathsf{P}_{t})+\\varepsilon_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining this with (B.10) we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\nI(\\mathsf{P}_{*})-J(\\mathsf{P}_{T})\\leq\\varepsilon_{T}+\\frac{1}{T}\\left[\\left(1+\\frac{2}{(1-\\gamma)^{2}}\\right)\\sum_{t=0}^{T-1}\\varepsilon_{t}+\\frac{2\\|r\\|_{\\infty}}{(1-\\gamma)^{2}}+\\frac{1}{\\eta(1-\\gamma)}\\left<D(\\pi_{*};\\pi_{0}),d_{\\nu}(\\mathsf{P}_{*})\\right>\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "leading to the desired bound. ", "page_idx": 21}, {"type": "text", "text": "C POWR Convergence Rates ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we prove the convergence of POWR. To do so, we need to first show that under the choice of spaces $\\mathcal{F}$ and $\\mathcal{G}$ proposed in this work, the resulting PMD iterations are well defined. Then, we need to bound the approximation error of the estimates for the action-value functions of the iterates produced by the inexact PDM algorithm, which appear in the rates of Theorem 7. ", "page_idx": 22}, {"type": "text", "text": "C.1 POWR is Well-defined ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In order to guarantee that the iterations of POWR generate policies $\\pi_{t}$ for which we can compute an estimator according to the formula in Proposition 3, we need to guarantee that all such policies are $(\\mathcal{G},\\mathcal{F})$ -compatible. In particular, we restrict to the case of the separable spaces introduced in Proposition 4, for which it turns out that it is sufficient to show that all policies belong to the space $\\mathcal{H}$ characterizing $\\mathcal{F}=\\mathcal{H}\\otimes\\mathcal{H}$ and $\\mathcal{G}=\\mathbb{R}^{|\\mathcal{A}|}\\otimes\\mathcal{H}$ . The following results provide a candidate for choosing such a space. ", "page_idx": 22}, {"type": "text", "text": "Theorem 5. Let $\\mathcal{X}\\subset\\mathbb{R}^{d}$ be a compact set and let $\\mathcal{H}=W^{2,s}(\\mathcal{X})$ be the Sobolev space of smoothness $s>0$ (see e.g. $I37J)$ . Let $\\pi_{t}(a|\\cdot)$ and $\\hat{q}_{\\pi_{t}}(\\cdot,a)$ belong to $\\mathcal{H}$ for any $a\\in A$ and $\\pi_{t}(a|x)>0$ for any $x\\in\\mathscr{X}$ . Then the policy $\\pi_{t+1}$ solution to the PMD update in (14) belongs to $\\mathcal{H}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We recall that Sobolev spaces [37] over a compact subset $\\mathcal{X}$ of RD are closed with respect to the operations of sum, multiplication, exponentiation or inversion (if the function is supported on the entire domain $\\mathcal{X}$ ), namely for any two $f,f^{\\prime}\\in\\mathcal{H},\\,f+f^{\\prime},f f^{\\prime},e^{f}\\in\\mathcal{H}$ and, if $f(x)>0$ for all $x\\in\\mathcal{X},1/f\\in\\mathcal{H}$ . This follows by applying the chain rule and the boundedness of derivatives over the compact $\\mathcal{X}$ (see for instance [45, Lemma E.2.2]). The proof follows by observing that the one-step update $\\pi_{t+1}$ in (14) is expressed precisely in terms of these operations and the hypothesis that $\\pi_{t}(a|\\cdot)$ and $\\hat{q}_{\\pi_{t}}(\\cdot,a)$ belong to $\\mathcal{H}$ for any $a\\in A$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Combining the choice of space $\\mathcal{H}$ according to the above result and combining with the PMD iterations of Algorithm 1 we have the following corollary. ", "page_idx": 22}, {"type": "text", "text": "Corollary 6. With the hypothesis of Proposition $^{4}$ let $\\mathcal{H}=W^{2,s}(\\mathcal{X})$ with $s\\,>\\,d/2$ . Let ${\\sf T}_{n}\\in$ $\\mathsf{H S}(\\mathcal{F},\\mathcal{G})$ and $r_{n}\\in{\\mathcal{G}}$ characterized as in Proposition 3. Let $\\pi_{0}(a|\\cdot)\\propto e^{\\eta q_{0}(\\cdot,a)}$ for $q_{0}$ such that $q_{0}(\\cdot,a)\\in\\mathcal{H}$ any $a\\in A$ . Then, for any $t\\in{\\mathbb N}$ the PMD iterates $\\pi_{t}$ generated by Algorithm $^{\\,l}$ are such that $\\pi_{t}(a|\\cdot)\\in\\mathcal{H}$ and hence are $(\\mathcal{G},\\mathcal{F})$ -compatible. ", "page_idx": 22}, {"type": "text", "text": "Proof. We proceed by induction. Since $\\bar{q}(\\cdot,a)\\in\\mathcal{H}$ we can apply the same reasoning in Theorem 5 to guarantee that $\\pi_{0}(\\dot{a}|\\cdot)\\in\\mathcal{H}$ for any $a\\in A$ . Moreover, $\\pi_{0}(a|\\cot)>0$ for any $a\\in A$ since it is the (normalized) exponential of a function. Hence $\\pi_{0}$ is $(\\mathcal{G},\\mathcal{F})$ -compatible. Therefore, $\\hat{q}_{0}$ obtained according to Proposition 3 is well defined and belongs to $\\mathcal{G}$ , implying $\\hat{q}_{0}(\\cdot,a)\\in\\mathcal{H}$ for any $a\\in A$ . Now, assume by the inductive hypothesis that the policy $\\pi_{t}(a|\\cdot)$ generated by POWR at time $t$ and the corresponding estimator $\\hat{q}_{\\pi_{t}}(\\cdot,a)$ of the action value function belong to $\\mathcal{H}$ and that $\\pi_{t}(a|x)>0$ for any $(x,a)\\in\\Omega$ . Then, by Theorem 5 we have that also $\\pi_{t+1}$ the solution to the PMD update in (14) belongs to $\\mathcal{H}$ (and is therefore $(\\mathcal{G},\\mathcal{F})$ -compatible). Additionally, since $\\pi_{t+1}$ can be expressed as the softmax of a (finite) sum of functions in $\\mathcal{H}$ , we have also $\\pi_{t+1}(a|x)>0$ for al $(x,\\bar{a})\\in\\Omega$ , proving the inductive hypothesis and concluding the proof. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "The above corollary guarantees us that if we are able to learn our estimates for the action-value function in $\\mathcal{H}$ a suitably regular Sobolev space, then POWR is well-defined. This is a necessary condition to then being able to study its theoretical behavior in our main result. ", "page_idx": 22}, {"type": "text", "text": "C.2 Controlling the Action-value Estimation Error ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We now show how to control the estimation error for the action-value function. We start by considering the following application of the (generalized) Simulation lemma in Corollary A.7. ", "page_idx": 22}, {"type": "text", "text": "Lemma 8 (Implications of the Simulation Lemma). Let ${\\mathsf{T}}_{n}$ and $r_{n}$ the empirical estimators of the transfer operator T and reward function $r$ as defined in Proposition $^3$ , respectively. If T ", "page_idx": 22}, {"type": "text", "text": "satisfies Assumption $^{\\,l}$ , $r\\in\\mathcal G$ , and $\\gamma\\|\\mathsf T_{n}\\|<\\gamma^{\\prime}<1$ , then, for every $(\\mathcal{G},\\mathcal{F})$ -compatible policy $\\pi$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\hat{q}_{\\pi}-q_{\\pi}\\right\\|_{\\infty}\\leq\\frac{1}{1-\\gamma^{\\prime}}\\left[c o n s t_{\\psi}\\left\\|r_{n}-r\\right\\|_{\\mathcal{G}}+\\frac{\\gamma\\left\\|r\\right\\|_{\\infty}}{1-\\gamma}\\left\\|\\mathsf{T}|_{\\mathcal{F}}-\\mathsf{T}_{n}\\right\\|_{\\mathsf{H S}}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Recall that in the notation of these appendices, the action value of a policy and its estimator via the world model CME framework are denoted $q_{\\pi}=q(\\mathsf{P})$ and ${\\hat{q}}_{\\pi}=q_{n}(\\mathsf{P})$ respectively. We can apply Corollary A.7 to obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\nq_{n}(\\mathsf{P})-q(\\mathsf{P})=(\\mathsf{I d}-\\gamma\\mathsf{T}_{n}\\mathsf{P})^{-1}(r_{n}-r)+\\gamma(\\mathsf{I d}-\\gamma\\mathsf{T}_{n}\\mathsf{P})^{-1}(\\mathsf{T}-\\mathsf{T}_{n})v(\\mathsf{P}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, by Lemma A.5, point 5, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\left\\|{q_{n}(\\mathsf{P})-q(\\mathsf{P})}\\right\\|_{\\infty}}\\leq\\frac{1}{1-\\gamma^{\\prime}}\\Big[{\\left\\|{r_{n}-r}\\right\\|_{\\infty}}+\\gamma{\\left\\|{(\\mathsf{T}-\\mathsf{T}_{n})v(\\mathsf{P})}\\right\\|_{\\infty}}\\Big],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $v(\\mathsf{P})\\,:=\\,\\mathsf{P}(\\mathsf{I d}-\\gamma\\mathsf{T}\\mathsf{P})^{-1}r$ is the value function of the MDP, and we used that $\\gamma\\left\\Vert\\mathsf{T}_{n}\\right\\Vert<$ $\\gamma^{\\prime}$ . Because of Assumption 1, $r\\,\\in\\,{\\mathcal{G}}$ , and $\\mathsf{P}$ being $(\\mathcal{G},\\mathcal{F})$ -compatible, it holds that $v({\\mathsf{P}})\\,\\in\\,{\\mathcal{F}}$ , while Proposition 3 implies $r_{n}\\in{\\mathcal{G}}$ , and $(\\mathsf{T}-\\mathsf{T}_{n}){\\boldsymbol{v}}(\\mathsf{P})\\in{\\mathcal{G}}$ as well. Therefore, using the reproducing property ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|r_{n}-r\\right\\|_{\\infty}=\\operatorname*{sup}_{(x,a)\\in\\Omega}|\\left\\langle\\psi(x,a),r_{n}-r\\right\\rangle_{\\mathcal{G}}|\\leq\\left\\|r_{n}-r\\right\\|_{\\mathcal{G}}\\operatorname*{sup}_{(x,a)\\in\\Omega}\\left\\|\\psi(x,a)\\right\\|_{\\mathcal{G}}=C_{\\psi}\\left\\|r_{n}-r\\right\\|_{\\mathcal{G}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we assumed a bounded kernel $\\langle\\psi(x,a),\\psi(x,a)\\rangle\\leq C_{\\psi}$ for all $(x,a)\\in\\Omega$ . Similarly, for the term depending on ${\\sf T}_{n}-{\\sf T}$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|(\\mathsf{T}-\\mathsf{T}_{n})v(\\mathsf{P})\\right\\|_{\\infty}=}&{\\underset{(x,a)\\in\\Omega}{\\operatorname*{sup}}|[(\\mathsf{T}|_{\\mathcal{F}}-\\mathsf{T}_{n})v(\\mathsf{P})](x,a)|}\\\\ {=}&{\\underset{(x,a)\\in\\Omega}{\\operatorname*{sup}}|\\left\\langle\\psi(x,a),(\\mathsf{T}|_{\\mathcal{F}}-\\mathsf{T}_{n})v(\\mathsf{P})\\right\\rangle_{\\mathcal{Q}}|}\\\\ {=}&{\\underset{(x,a)\\in\\Omega}{\\operatorname*{sup}}\\left|\\mathsf{T}\\left[(v(\\mathsf{P})\\otimes\\psi(x,a))(\\mathsf{T}|_{\\mathcal{F}}-\\mathsf{T}_{n})\\right]\\right|}\\\\ {\\leq}&{\\left\\|\\mathsf{T}|_{\\mathcal{F}}-\\mathsf{T}_{n}\\right\\|_{\\mathsf{H S}}\\underset{(x,a)\\in\\Omega}{\\operatorname*{sup}}|v(\\mathsf{P})(x,a)|}\\\\ {\\leq\\frac{\\left\\|r\\right\\|_{\\infty}}{1-\\gamma}\\|\\mathsf{T}|_{\\mathcal{F}}-\\mathsf{T}_{n}\\right\\|_{\\mathsf{H S}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining the previous two bounds, we get to ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\left\\|q_{n}(\\boldsymbol{\\mathsf{P}})-q(\\boldsymbol{\\mathsf{P}})\\right\\|_{\\infty}}\\le\\frac{1}{1-\\gamma^{\\prime}}\\left[C_{\\psi}\\left\\|r_{n}-r\\right\\|_{\\mathcal{G}}+\\frac{\\gamma\\left\\|r\\right\\|_{\\infty}}{1-\\gamma}{\\left\\|\\mathsf{T}|_{\\mathcal{F}}}-\\mathsf{T}_{n}\\right\\|_{\\mathsf{H S}}\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "as desired. ", "page_idx": 23}, {"type": "text", "text": "According to the result above, we can control the approximation error for the action value function in terms of the approximation errors \u2225rn \u2212r\u2225G and  T|F \u2212Tn  HS . This can be done by leveraging state-of-the-art statistical learning rates for the ridge regression and CME estimators from [39, 21, 46]. The following lemma connects Assumption 2 with the notation used in [39] which enables us to use the required result. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.1 (Relation between (A.8) and [39]\u2019s definition). The following two facts are equivalent ", "page_idx": 23}, {"type": "text", "text": "1. $g\\in{\\mathcal{G}}$ satisfies the strong source condition Assumption 2 with parameter $\\beta$ on the probability distribution $\\rho$ .   \n2. $g\\in[\\mathcal{G}]_{\\rho}^{1+2\\beta}$ as in the notation of $[39].$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Using the same notations as in [39], we have ", "page_idx": 23}, {"type": "equation", "text": "$$\ng\\in[\\mathcal{G}]_{\\rho}^{1+2\\beta}\\iff g=\\sum_{i\\in\\mathbb{N}}a_{i}\\mu_{i}^{\\frac{1}{2}+\\beta}e_{i}\\,\\,\\,\\mathrm{and}\\,\\,\\left\\|(a_{i})_{i\\in\\mathbb{N}}\\right\\|_{\\ell^{2}}<\\infty.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $1\\implies2$ we have that that for $g\\in[\\mathcal{G}]_{\\rho}^{1+2\\beta}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\nC_{\\rho}^{-\\beta}g=\\sum_{i\\in\\mathbb{N}}a_{i}\\mu_{i}^{\\frac{1}{2}}e_{i}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "whose $\\mathcal{G}$ -norm is $\\left\\|C_{\\rho}^{-\\beta}g\\right\\|_{\\mathcal{G}}=\\left\\|(a_{i})_{i\\in\\mathbb{N}}\\right\\|_{\\ell^{2}}<\\infty.$ ", "page_idx": 24}, {"type": "text", "text": "For $2\\implies1$ , let $\\begin{array}{r}{g\\,=\\,\\sum_{i\\in\\mathbb{N}}{b_{i}\\mu_{i}^{\\frac{1}{2}}e_{i}}}\\end{array}$ with $\\|(b_{i})_{i\\in\\mathbb{N}}\\|_{\\ell^{2}}\\,<\\,\\infty$ (since). Now, $\\left\\|C_{\\rho}^{-\\beta}g\\right\\|_{\\mathcal{G}}<\\infty$ is equivalent to $\\|(b_{i}\\mu_{i}^{-\\beta})_{i\\in\\mathbb{N}}\\|_{\\ell^{2}}<\\infty$ . By letting $b_{i}=\\mu_{i}^{\\beta}a_{i}$ we have that $\\|(a_{i})_{i\\in\\mathbb{N}}\\|_{\\ell^{2}}<\\infty$ and that ", "page_idx": 24}, {"type": "equation", "text": "$$\ng=\\sum_{i\\in\\mathbb{N}}a_{i}\\mu_{i}^{\\frac{1}{2}+\\beta}e_{i},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "that is $g\\in[\\mathcal{G}]_{\\rho}^{1+2\\beta}$ . ", "page_idx": 24}, {"type": "text", "text": "With the connection between [39] and Assumption 2 in place we can characterize the bound on the approximation error for the world model-based estimation of the action-value function. ", "page_idx": 24}, {"type": "text", "text": "Proposition C.2. Let ${\\mathsf{T}}_{n}$ and $r_{n}$ the empirical estimators of the transfer operator $\\top$ and reward function r as defined in Proposition 3, respectively. When $\\mathsf{P}$ is a $(\\mathcal{G},\\mathcal{F})$ -compatible policy as in Definition $^{\\,l}$ and the strong source condition Assumption 2 is attained with parameter $\\beta$ , it holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|q_{n}(\\mathsf{P})-q(\\mathsf{P})\\right\\|_{\\infty}\\le O(\\delta^{2}n^{-\\alpha}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with rates $\\begin{array}{r}{\\alpha\\in\\left(\\frac{\\beta}{2+2\\beta},\\frac{\\beta}{1+2\\beta}\\right)}\\end{array}$ and probability not less than $1-4e^{-\\delta}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. We use Lemma C.1 to apply Theorem 3.1 (ii) from [39] to show that under Assumption 2 with parameter $\\beta$ it holds, with probability not less than $1-\\bar{4}e^{-\\delta}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|r_{n}-r\\right\\|_{\\mathcal{G}}\\leq\\delta^{2}c_{r}n^{-\\alpha_{r}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The rate $\\begin{array}{r}{\\alpha_{r}\\,\\in\\,\\left(\\frac{\\beta}{2+2\\beta},\\frac{\\beta}{1+2\\beta}\\right)}\\end{array}$ is determined by the properties of the inclusion $\\mathcal{G}\\hookrightarrow B_{b}(\\Omega)$ , and the constant $c_{r}>0$ is independent of $n$ and $\\delta$ . Similarly, point (2.) of [21, Theorem 2] shows that under Assumption 2 ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\lVert\\mathsf{T}|_{\\mathcal{F}}-\\mathsf{T}_{n}\\rVert_{\\mathsf{H S}(\\mathcal{F},\\mathcal{G})}\\leq\\delta^{2}c_{\\mathsf{T}}n^{-\\alpha_{\\mathsf{T}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "again with probability not less than $1\\!-\\!4e^{-\\delta}$ , rates $\\begin{array}{r}{\\alpha_{\\mathsf{T}}\\in\\left(\\frac{\\beta}{2+2\\beta},\\frac{\\beta}{1+2\\beta}\\right)}\\end{array}$ and with $c_{\\mathsf{T}}>0$ independent of $n$ and $\\delta$ . Combining every bound and denoting $\\alpha:=\\operatorname*{min}(\\alpha_{r},\\alpha_{\\mathsf{T}})$ , we conclude ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|q_{n}(\\mathsf{P})-q(\\mathsf{P})\\right\\|_{\\infty}\\le\\frac{\\delta^{2}}{1-\\gamma^{\\prime}}\\left[C_{\\psi}c_{r}+\\frac{\\gamma c_{\\mathsf{T}}\\|r\\|_{\\infty}}{1-\\gamma}\\right]n^{-\\alpha}=O(\\delta^{2}n^{-\\alpha}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "as required. ", "page_idx": 24}, {"type": "text", "text": "C.3 Convergence Rates for POWR ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "With a bound on the estimation error of the action-value function by Algorithm 1, we are finally ready to state the complexity bounds for POWR. ", "page_idx": 24}, {"type": "text", "text": "Theorem 9. Let $(\\pi_{t})_{t\\in\\mathbb{N}}$ be a sequence of policies generated by Algorithm $^{\\,l}$ in the same setting of Corollary 6. If the action-value functions $\\hat{q}_{\\pi_{t}}$ are estimated from a dataset $(x_{i},a_{i};x_{i}^{\\prime})_{i=1}^{n}$ with $(x_{i},a_{i})\\sim\\rho\\in\\mathscr{P}(\\Omega)$ such that Assumption 2 holds with parameter $\\beta$ , the iterates of Algorithm $^{\\,l}$ converge to the optimal policy as ", "page_idx": 24}, {"type": "equation", "text": "$$\nJ(\\pi_{*})-J(\\pi_{T})\\le O\\left(\\frac{1}{T}+\\delta^{2}n^{-\\alpha}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with probability not less than $1-4e^{-\\delta}$ . Here, $\\begin{array}{r}{\\alpha\\,\\in\\,\\left(\\frac{\\beta}{2+2\\beta},\\frac{\\beta}{1+2\\beta}\\right)}\\end{array}$ and $\\pi_{*}\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\Delta(\\mathcal{A})$ is $a$ measurable maximizer of (8). ", "page_idx": 24}, {"type": "text", "text": "Proof. Since the setting of Corollary 6 implies that $\\mathsf{P}_{t}$ are $(\\mathcal{G},\\mathcal{F})$ -compatible for all $t$ , and Assumption 2 is holding, then $q(\\mathsf{P}_{t})$ and $q_{n}(\\mathsf{P}_{t})$ belong to $\\mathcal{G}$ for all $(\\mathsf{P}_{t})_{t\\in\\mathbb{N}}$ . This assures that we can use the statistical learning bounds Proposition C.2 into Theorem 7, yielding the final bound. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "D Experimental details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1 Additional Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In Fig. 2 we show the average timestep at which a reward threshold is met during the training phase. The testing environments are the same as introduced previously, with reward thresholds being the standard ones given in [25], except for the Taxi- $\\mathtt{v3}$ environment, where it is marginally lower. Interestingly, in this environment, only DQN and our algorithm are capable of achieving the original threshold within $1.5\\times10^{6}$ timesteps during the training. On the other hand, the new lower threshold is also reached by the PPO algorithm. ", "page_idx": 25}, {"type": "text", "text": "Our approach attain the desired reward quicker than the competing algorithms. Furthermore, the timestep at which POWRreaches the threshold exhibits a lower variance compared to other techniques. This implies that our approach requires a stable amount of timesteps to learn how to solve a specific environment. ", "page_idx": 25}, {"type": "image", "img_path": "kbBjVMcJ7G/tmp/df48ceb28a51f166b136f64f9a1a975a000016b3f79b6a8f8c45d4ac94e9bd99.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 2: Mean timestep at which various algorithms attain a specified reward threshold during their training. The reward targets are set at 0.8 for FrozenLake-v1, 6 for Taxi-v3, and $-110$ for MountainCar-v0. The absence of a box indicates that the corresponding algorithm was unable to meet the reward threshold within the training process. ", "page_idx": 25}, {"type": "text", "text": "D.2 Other methods ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We compare the performance of our algorithm with several baselines. In particular, we considered A2C [40], DQN [4], TRPO [7] and PPO [6], which we implemented using the stable baselines library [47]. We used the standard hyperparameters in [47]. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: we provide theoretical and experimental proofs of our claims. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: see the conclusion section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We give all the proofs in the Appendix. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In the paper, we provided the algorithm description and analysis, including the hyperparameters of the baselines and our proposed algorithm. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provided the link to the code and all the hyperparameters we tested. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provided a description of the open-source environments, in which we tested our POWR and the baselines. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We showed seven independent runs for each experiment we reported to prove the effectiveness of our method. Moreover, in the plot, we can observe the bands based on the minimum and maximum values reached by the different runs. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provided the code. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We respected the ethics guidelines for the development of this work. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We believe that this work doesn\u2019t have any societal impact. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This work doesn\u2019t have such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We implemented our approach from scratch. We mentioned the baselines and the environments for testing involved in this work, that are open-source ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provided the code of our algorithm. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]