[{"type": "text", "text": "gRNAde: Geometric Deep Learning for 3D RNA inverse design ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Computational RNA design tasks are often posed as inverse problems, where   \n2 sequences are designed based on adopting a single desired secondary structure   \n3 without considering 3D geometry and conformational diversity. We introduce   \n4 gRNAde, a geometric RNA design pipeline operating on 3D RNA backbones to   \n5 design sequences that explicitly account for structure and dynamics. Under the   \n6 hood, gRNAde is a multi-state Graph Neural Network that generates candidate   \n7 RNA sequences conditioned on one or more 3D backbone structures where the   \n8 identities of the bases are unknown. On a single-state fixed backbone re-design   \n9 benchmark of 14 RNA structures from the PDB identified by Das et al. [2010],   \n0 gRNAde obtains higher native sequence recovery rates $56\\%$ on average) compared   \n11 to Rosetta $45\\%$ on average), taking under a second to produce designs compared   \n2 to the reported hours for Rosetta. We further demonstrate the utility of gRNAde on   \n3 a new benchmark of multi-state design for structurally flexible RNAs, as well as   \n4 zero-shot ranking of mutational fitness landscapes in a retrospective analysis of a   \n15 recent RNA polymerase ribozyme structure. Open source code and tutorials are   \n16 available at: anonymous.4open.science/r/geometric-rna-design ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Why RNA design? Historical efforts in computational drug discovery have focussed on designing   \n19 small molecule or protein-based medicines that either treat symptoms or counter the end stages   \n20 of disease processes. In recent years, there is a growing interest in designing new RNA-based   \n21 therapeutics that intervene earlier in disease processes to cut off disease-causing information flow   \n22 in the cell [Damase et al., 2021, Zhu et al., 2022]. Notable examples of RNA molecules at the   \n23 forefront of biotechnology today include mRNA vaccines [Metkar et al., 2024] and CRISPR-based   \n24 genomic medicine [Doudna and Charpentier, 2014]. Of particular interest for structure-based design   \n25 are ribozymes and riboswitches in the untranslated regions of mRNAs [Mandal and Breaker, 2004,   \n26 Leppek et al., 2018]. In addition to coding for proteins (such as the spike protein in the Covid vaccine),   \n27 naturally occurring mRNAs contain riboswitches that are responsible for cell-state dependent protein   \n28 expression of the mRNA. Riboswitches act by \u2018switching\u2019 their 3D structure from an unbound   \n29 conformation to a bound one in the presence of specific metabolites or small molecules. Rational   \n30 design of riboswitches will enable translation to be dependent on the presence or absence of partner   \n31 molecules, essentially acting as \u2018on-off\u2019 switches for highly targeted mRNA therapies in the future   \n32 [Felletti et al., 2016, Mustafina et al., 2019, Mohsen et al., 2023].   \n33 Challenges of RNA modelling. Despite the promises of RNA therapeutics, proteins have instead   \n34 been the primary focus in the 3D biomolecular modelling community. Availability of a large number   \n35 of protein structures from the PDB combined with advances in deep learning for structured data   \n36 [Bronstein et al., 2021, Duval et al., 2023] have revolutionized protein 3D structure prediction [Jumper   \n37 et al., 2021] and rational design [Dauparas et al., 2022, Watson et al., 2023]. Applications of deep   \n38 learning for computational RNA design are underexplored compared to proteins due to paucity of   \n39 3D structural data [Schneider et al., 2023]. Most tools for RNA design primarily focus on secondary   \n40 structure without considering 3D geometry [Churkin et al., 2018] and use non-learnt algorithms for   \n41 aligning 3D RNA fragments [Han et al., 2017, Yesselman et al., 2019], which can be restrictive due   \n42 to the hand-crafted nature of the heuristics used.   \n43 In addition to limited 3D data for training deep learning models, the key technical challenge is that   \n44 RNA is more dynamic than proteins. The same RNA can adopt multiple distinct conformational   \n45 states to create and regulate complex biological functions [Ganser et al., 2019, Hoetzel and Suess,   \n46 2022, Ken et al., 2023]. Computational RNA design pipelines must account for both the 3D geometric   \n47 structure and conformational flexibility of RNA to engineer new biological functions.   \n48 Our contributions. This paper introduces gRNAde, a geometric deep learning-based pipeline for   \n49 RNA inverse design conditioned on 3D structure, analogous to ProteinMPNN for proteins [Dauparas   \n50 et al., 2022]. As illustrated in Figure 1, gRNAde generates candidate RNA sequences conditioned   \n51 on one or more backbone 3D conformations, enabling both single- and multi-state fixed-backbone   \n52 sequence design. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/2fe04073ecdde6fca23a1f516938ba95c1e8b2caa9fc726123f3ddca5c29c200.jpg", "img_caption": ["Figure 1: The gRNAde pipeline for 3D RNA inverse design. gRNAde is a generative model for RNA sequence design conditioned on backbone 3D structure(s). gRNAde processes one or more RNA backbone graphs (a conformational ensemble) via a multi-state GNN encoder which is equivariant to 3D roto-translation of coordinates as well as conformer order, followed by conformer order-invariant pooling and autoregressive sequence decoding. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "53 We demonstrate the utility of gRNAde for the following design scenarios: ", "page_idx": 1}, {"type": "text", "text": "54 \u2022 Improved performance and speed over Rosetta. We compare gRNAde to Rosetta [Leman   \n55 et al., 2020], the state-of-the-art physically based tool for 3D RNA inverse design, for single  \n56 state fixed backbone design of 14 RNA structures of interest from the PDB identified by Das   \n57 et al. [2010]. We obtain higher native sequence recovery rates with gRNAde ( $56\\%$ on average)   \n58 compared to Rosetta ( $45\\%$ on average). Additionally, gRNAde is significantly faster than Rosetta   \n59 for inference; e.g. sampling $^{100+}$ designs in 1 second for an RNA of 60 nucleotides on an A100   \n60 GPU, compared to the reported hours for Rosetta.   \n61 \u2022 Enables multi-state RNA design, which was previously not possible with Rosetta. gRNAde   \n62 with multi-state GNNs improves sequence recovery over an equivalent single-state model on   \n63 a benchmark of structurally flexible RNAs, especially for surface nucleotides which undergo   \n64 positional or secondary structural changes.   \n65 \u2022 Zero-shot learning of RNA fitness landscape. In a retrospective analysis of mutational ftiness   \n66 landscape data for an RNA polymerase ribozyme [McRae et al., 2024], we show how gRNAde\u2019s   \n67 perplexity, the likelihood of a sequence folding into a backbone structure, can be used to   \n68 rank mutants based on fitness in a zero-shot/unsupervised manner and outperforms random   \n69 mutagenesis for improving fitness over the wild type in low throughput scenarios. ", "page_idx": 1}, {"type": "text", "text": "70 2 The gRNAde pipeline ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "71 2.1 The 3D RNA inverse folding problem ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "72 Figure 1 illustrates the RNA inverse folding problem: the task of designing new RNA sequences   \n73 conditioned on a structural backbone. Given the 3D coordinates of a backbone structure, machine   \n74 learning models must generate sequences that are likely to fold into that shape. The underlying   \n75 assumption behind inverse folding (and rational biomolecule design) is that structure determines   \n76 function [Huang et al., 2016]. To the best of our knowledge, gRNAde is the first explicitly multi-state   \n77 inverse folding pipeline, allowing users to design sequences for backbone conformational ensembles   \n78 (a set of 3D backbone structures) as opposed to a single structure. ", "page_idx": 2}, {"type": "text", "text": "79 2.2 RNA conformational ensembles as geometric multi-graphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "80 Featurization. The input to gRNAde is an RNA to be re-designed. For instance, this could be a   \n81 set of PDB files with 3D backbone structures for the given RNA (a conformational ensemble) and   \n82 the corresponding sequence of $n$ nucleotides. As shown in Appendix Figure 11, gRNAde builds a   \n83 geometric graph representation for each input structure:   \n84 1. We start with a 3-bead coarse-grained representation of the RNA backbone, retaining the   \n85 coordinates for P, $C4^{\\circ}$ , N1 (pyrimidine) or N9 (purine) for each nucleotide [Dawson et al., 2016].   \n86 This \u2018pseudotorsional\u2019 representation describes RNA backbones completely in most cases while   \n87 reducing the size of the torsional space to prevent overfitting [Wadley et al., 2007].   \n88 2. Each nucleotide $i$ is assigned a node in the geometric graph with the 3D coordinate $\\vec{\\pmb{x}}_{i}\\in\\mathbb{R}^{3}$   \n89 corresponding to the centroid of the 3 bead atoms. Random Gaussian noise with standard   \n90 deviation $0.1\\bar{\\overset{\\circ}{\\mathsf{A}}}$ is added to coordinates during training to prevent overfitting on crystallisation   \n91 artifacts, following Dauparas et al. [2022]. Each node is connected by edges to its 32 nearest   \n92 neighbours as measured by the pairwise distance in 3D space, $\\lVert\\pmb{\\vec{x}}_{i}-\\pmb{\\dot{\\vec{x}}}_{j}\\rVert_{2}$ .   \n93 3. Nodes are initialized with geometric features analogous to the featurization used in protein   \n94 inverse folding [Ingraham et al., 2019, Jing et al., 2020]: (a) forward and reverse unit vectors   \n95 along the backbone from the $5'$ end to the $3^{\\,\\cdot}$ end, $(\\pmb{\\vec{x}}_{i+1}-\\pmb{\\vec{x}}_{i}$ and $\\pmb{\\vec{x}}_{i}-\\pmb{\\vec{x}}_{i-1})$ ; and (b) unit   \n96 vectors, distances, angles, and torsions from each $\\mathrm{C4}\\\"$ to the corresponding $\\mathbf{P}$ and N1/N9.   \n97 4. Edge features for each edge from node $j$ to $i$ are initialized as: (a) the unit vector from the   \n98 source to destination node, $\\vec{\\pmb{x}}_{j}-\\vec{\\pmb{x}}_{i}$ ; (b) the distance in 3D space, $\\lVert\\vec{\\pmb{x}}_{j}-\\vec{\\pmb{x}}_{i}\\rVert_{2}$ , encoded by 32   \n99 radial basis functions; and (c) the distance along the backbone, $j-i$ , encoded by 32 sinusoidal   \n100 positional encodings.   \n101 Multi-graph representation. As described in the previous section, given a set of $k$ (conformer)   \n102 structures in the input conformational ensemble, each RNA backbone is featurized as a separate   \n103 geometric graph $\\bar{\\mathcal{G}^{(k)}}=(\\mathbf{A}^{(k)},S^{(k)},\\vec{V}^{(k)})$ with the scalar features $S^{(k)}\\in\\mathbb{R}^{n\\times f}$ , vector features   \n104 $\\vec{V}^{(k)}\\in\\mathbb{R}^{n\\times f^{\\prime}\\times3}$ , and $A^{(k)}$ , an $n\\times n$ adjacency matrix. For clear presentation and without loss of   \n105 generality, we omit edge features and use $f,f^{\\prime}$ to denote scalar/vector feature channels.   \n106 The input to gRNAde is thus a set of geometric graphs $\\{\\mathcal{G}^{(1)},\\ldots,\\mathcal{G}^{(k)}\\}$ which is merged into what we   \n107 term a \u2018multi-graph\u2019 representation of the conformational ensemble, $\\mathcal{\\bar{M}}=(A,S,\\vec{V})$ , by stacking the   \n108 set of scalar features $\\{S^{(1)},\\ldots,S^{(k)}\\}$ into one tensor $\\mathbf{S}\\in\\mathbb{R}^{n\\times k\\times f}$ along a new axis for the set size   \n109 $k$ . Similarly, the set of vector features $\\{\\vec{V}^{(1)},\\cdot\\cdot\\cdot,\\vec{V}^{(k)}\\}$ is stacked into one tensor $\\vec{V}\\in\\mathbb{R}^{n\\times k\\times f^{\\prime}\\times3}$ .   \n110 Lastly, the set of adjacency matrices $\\{\\pmb{A}^{(1)},\\cdot\\cdot\\cdot,\\pmb{A}^{(k)}\\}$ are merged via a union $\\cup$ into one single joint   \n111 adjacency matrix $\\pmb{A}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "112 2.3 Multi-state GNN for representation learning on conformational ensembles ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "113 The gRNAde model, illustrated in Appendix Figure 12, processes one or more RNA backbone graphs   \n114 via a multi-state GNN encoder which is equivariant to 3D roto-translation of coordinates as well as to   \n115 the ordering of conformers, followed by conformer order-invariant pooling and sequence decoding.   \n116 We describe each component in the following sections.   \n117 Multi-state GNN encoder. When representing conformational ensembles as a multi-graph, each   \n118 node feature tensor contains three axes: (#nodes, #conformations, feature channels). We perform   \n119 message passing on the multi-graph adjacency to independently process each conformer, while   \n120 maintaining permutation equivariance of the updated feature tensors along both the first (#nodes)   \n121 and second (#conformations) axes. This works by operating on only the feature channels axis and   \n122 generalising the PyTorch Geometric [Fey and Lenssen, 2019] message passing class to account for   \n123 the extra conformations axis; see Appendix Figure 14 and the pseudocode for details.   \n124 We use multiple rotation-equivariant GVP-GNN [Jing et al., 2020] layers to update scalar features   \n125 $\\pmb{s}_{i}\\in\\mathbb{R}^{k\\times f}$ and vector features $\\vec{v}_{i}\\in\\mathbb{R}^{k\\times f^{\\prime}\\times3}$ for each node $i$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{m}_{i},\\vec{\\pmb{m}}_{i}:=\\displaystyle\\sum_{j\\in\\mathcal{N}_{i}}\\mathbf{M}\\mathrm{SG}\\Big(\\left(\\pmb{s}_{i},\\vec{v}_{i}\\right),\\left(\\pmb{s}_{j},\\vec{v}_{j}\\right),\\pmb{e}_{i j}\\Big),}\\\\ &{\\quad\\pmb{s}_{i}^{\\prime},\\vec{\\pmb{v}}_{i}^{\\prime}:=\\mathbf{U}\\mathrm{PD}\\Big(\\left(\\pmb{s}_{i},\\vec{v}_{i}\\right)\\ ,\\left(\\pmb{m}_{i},\\vec{m}_{i}\\right)\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "126 where MSG, UPD are Geometric Vector Perceptrons, a generalization of MLPs to take tuples of   \n127 scalar and vector features as input and apply $O(3)$ -equivariant non-linear updates. The overall GNN   \n128 encoder is $S O(3)$ -equivariant due to the use of reflection-sensitive input features (dihedral angles)   \n129 combined with $O(3)$ -equivariant GVP-GNN layers.   \n130 Our multi-state GNN encoder is easy to implement in any message passing framework and can be   \n131 used as a plug-and-play extension for any geometric GNN pipeline to incorporate the multi-state   \n132 inductive bias. It serves as an elegant alternative to batching all the conformations, which we found   \n133 required major alterations to message passing and pooling depending on downstream tasks.   \n134 Conformation order-invariant pooling. The final encoder representations in gRNAde account for   \n135 multi-state information while being invariant to the permutation of the conformational ensemble. To   \n136 achieve this, we perform a Deep Set pooling [Zaheer et al., 2017] over the conformations axis after the   \n137 final encoder layer to reduce $\\pmb{S}\\in\\mathbb{R}^{n\\times k\\times f}$ and $\\vec{V}\\in\\mathbb{R}^{n\\times k\\times f^{\\prime}\\times3}$ to $S^{\\prime}\\in\\mathbb{R}^{n\\times f}$ and $\\vec{V}^{\\prime}\\in\\mathbb{R}^{n\\times f^{\\prime}\\times3}$ : ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nS^{\\prime},\\vec{V}^{\\prime}:=\\frac{1}{k}\\sum_{i=1}^{k}\\Big(S[:,\\:i],\\vec{V}[:,\\:i]\\Big)\\:.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "138 A simple sum or average pooling does not introduce any new learnable parameters to the pipeline and   \n139 is flexible to handle a variable number of conformations, enabling both single-state and multi-state   \n140 design with the same model.   \n141 Sequence decoding and loss function. We feed the final encoder representations after pooling,   \n142 ${\\bar{S^{\\prime}}},{\\bar{V^{\\prime}}}$ , to autoregressive GVP-GNN decoder layers to predict the probability of the four possible base   \n143 identities $(\\mathrm{A},\\mathrm{G},\\mathrm{C},\\mathrm{U})$ for each node/nucleotide. Decoding proceeds according to the RNA sequence   \n144 order from the $5'$ end to $\\3^{\\circ}$ end. gRNAde is trained in a self-supervised manner by minimising a   \n145 cross-entropy loss (with label smoothing value of 0.05) between the predicted probability distribution   \n146 and the ground truth identity for each base. During training, we use autoregressive teacher forcing   \n147 [Williams and Zipser, 1989] where the ground truth base identity is fed as input to the decoder at   \n148 each step, encouraging the model to stay close to the ground-truth sequence.   \n149 Sampling. When using gRNAde for inference and designing new sequences, we iteratively sample   \n150 the base identity for a given nucleotide from the predicted conditional probability distribution,   \n151 given the partially designed sequence up until that nucleotide/decoding step. We can modulate the   \n152 smoothness or sharpness of the probability distribution by using a temperature parameter. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "153 2.4 Evaluation metrics for designed sequences ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "154 In principle, inverse folding models can be sampled from to obtain a large number of designed   \n155 sequences for a given backbone structure. Thus, in-silico metrics to determine which sequences are   \n156 useful and which ones to prioritise in wet lab experiments are a critical part of the overall pipeline. We   \n157 currently use the following metrics to evaluate gRNAde\u2019s designs, visualised in Appendix Figure 13:   \n158 \u2022 Native sequence recovery, which is the average percentage of native (ground truth) nucleotides   \n159 correctly recovered in the sampled sequences. Recovery is the most widely used metric for   \n160 biomolecule inverse design [Dauparas et al., 2022] but can be misleading in the case of RNAs   \n161 where alternative nucleotide base pairings can form the same structural patterns.   \n162 \u2022 Secondary structure self-consistency score, where we \u2018forward fold\u2019 the sampled sequences   \n163 using a secondary structure prediction tool (we used EternaFold [Wayment-Steele et al., 2022])   \n164 and measure the average Matthew\u2019s Correlation Coefficient (MCC) to the groundtruth secondary   \n165 structure, represented as a binary adjacency matrix. MCC values range between -1 and $+1$ ,   \n166 where $+1$ represents a perfect prediction, 0 an average random prediction and $^-1$ an inverse   \n167 prediction. This measures how well the designs recover base pairing patterns.   \n168 \u2022 Tertiary structure self-consistency scores, where we \u2018forward fold\u2019 the sampled sequences   \n169 using a 3D structure prediction tool (we used RhoFold [Shen et al., 2022]) and compute the   \n170 average RMSD, TM-score and GDT_TS to the groundtruth $\\mathrm{C4}\\\"$ coordinates to measure how   \n171 well the designs recover global structural similarity and 3D conformations.   \n172 \u2022 Perplexity, which can be thought of as the average number of bases that the model is selecting   \n173 from for each nucleotide. Formally, perplexity is the average exponential of the negative   \n174 log-likelihood of the sampled sequences. A perfect model would have perplexity of 1, while   \n175 a perplexity of 4 means that the model is making random predictions (the model outputs a   \n176 uniform probability over 4 possible bases). Perplexity does not require a ground truth structure   \n177 to calculate, and can also be used for ranking sequences as it is the model\u2019s estimate of the   \n178 compatibility of a sequence with the input backbone structure.   \n179 Significance and limitations. Self-consistency metrics, termed \u2018designability\u2019 (eg. scRMSD ${\\leq}2\\mathring\\mathrm{A}$ ),   \n180 as well as perplexity have been found to correlate with experimental success in protein design   \n181 [Watson et al., 2023]. While precise designability thresholds are yet to be established for RNA,   \n182 pairs of structures with TM-score ${\\geq}0.45$ or $\\mathrm{GDT\\_TS\\_}=0.5\\$ are known to correspond to roughly the   \n183 same fold [Zhang et al., 2022]. Another major limitation for in-silico evaluation of 3D RNA design   \n184 compared to proteins is the relatively worse state of structure prediction tools [Schneider et al., 2023]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "185 3 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "186 3D RNA structure dataset. We create a machine learning-ready dataset for RNA inverse design   \n187 using RNASolo [Adamczyk et al., 2022], a novel repository of RNA 3D structures extracted from   \n188 solo RNAs, protein-RNA complexes, and DNA-RNA hybrids in the PDB. We used structures at   \n189 resolution $\\bar{<}\\bar{4}.0\\mathring\\mathrm{A}$ resulting in 4,223 unique RNA sequences for which a total of 12,011 structures   \n190 are available (RNASolo date cutoff: 31 October 2023). Dataset statistics are available in Appendix   \n191 Figure 15, illustrating the diversity of our dataset in terms of sequence length, number of structures   \n192 per sequence, as well as structural variations among conformations per sequence.   \n193 Structural clustering. In order to ensure that we evaluate gRNAde\u2019s generalization ability to novel   \n194 RNAs, we cluster the 4,223 unique RNAs into groups based on structural similarity. We use US-align   \n195 [Zhang et al., 2022] with a similarity threshold of TM-score ${>}0.45$ for clustering, and ensure that   \n196 we train, validate and test gRNAde on structurally dissimilar clusters (see next paragraph). We also   \n197 provide utilities for clustering based on sequence homology using CD-HIT [Fu et al., 2012], which   \n198 leads to splits containing biologically dissimilar clusters of RNAs.   \n199 Splits to evaluate generalization. After clustering, we split the RNAs into training ${\\sim}4000$ samples),   \n200 validation and test sets (100 samples each) to evaluate two different design scenarios:   \n201 1. Single-state split. This split is used to fairly evaluate gRNAde for single-state design on a   \n202 set of RNA structures of interest from the PDB identified by Das et al. [2010], which mainly   \n203 includes riboswitches, aptamers, and ribozymes. We identify the structural clusters belonging to   \n204 the RNAs identified in Das et al. [2010] and add all the RNAs in these clusters to the test set   \n205 (100 samples). The remaining clusters are randomly added to the training and validation splits.   \n206 2. Multi-state split. This split is used to test gRNAde\u2019s ability to design RNA with multiple   \n207 distinct conformational states. We order the structural clusters based on median intra-sequence   \n208 RMSD among available structures within the cluster1. The top 100 samples from clusters with   \n209 the highest median intra-sequence RMSD are added to the test set. The next 100 samples are   \n210 added to the validation set and all remaining samples are used for training.   \n211 Validation and test samples come from clusters with at most 5 unique sequences, in order to ensure   \n212 diversity. Any samples that were not assigned clusters are directly appended to the training set. We ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/29a25de804c768e4437f6481315b0c53f30ec949491169e1fd004b1e9ba6b8a4.jpg", "img_caption": ["(a) gRNAde outperforms Rosetta. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/53b1b6648cabc66738f0c67fcb9331467740b3b30347b7934af059d25528c3ab.jpg", "img_caption": ["(b) gRNAde\u2019s perplexity correlates with recovery. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: gRNAde compared to Rosetta for single-state design. (a) We benchmark native sequence recovery of gRNAde, Rosetta, FARNA and ViennaRNA on 14 RNA structures of interest identified by Das et al. [2010]. gRNAde obtains higher native sequence recovery rates ( $56\\%$ on average) compared to Rosetta $(45\\%)$ . (b) Sequence recovery per sample for Rosetta and gRNAde, shaded by gRNAde\u2019s perplexity for each sample. gRNAde\u2019s perplexity is correlated with native sequence recovery for designed sequences. Full results are available in Appendix Table 2. ", "page_idx": 5}, {"type": "text", "text": "213 also directly add very large RNAs $>1000$ nts) to the training set, as it is unlikely that we want to   \n214 design very large RNAs. We exclude very short RNA strands $\\zeta10$ nts).   \n215 Evaluation metrics. For a given data split, we evaluate models on the held-out test set by designing   \n216 16 sequences (sampled at temperature 0.1) for each test data point and computing averages for each of   \n217 the metrics described in Section 2.4: native sequence recovery, structural self-consistency scores and   \n218 perplexity. We employ early stopping by reporting test set performance for the model checkpoint for   \n219 the epoch with the best validation set recovery. Standard deviations are reported across 3 consistent   \n220 random seeds for all models.   \n221 Hyperparameters. All models use 4 encoder and 4 decoder GVP-GNN layers, with 128 scalar/16   \n222 vector node features, 64 scalar/4 vector edge features, and drop out probability 0.5, resulting in   \n223 2,147,944 trainable parameters. All models are trained for a maximum of 50 epochs using the Adam   \n224 optimiser with an initial learning rate of 0.0001, which is reduced by a factor 0.9 when validation   \n225 performance plateaus with patience of 5 epochs. Ablation studies of key modelling decisions are   \n226 available in Appendix Table 1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "227 4 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "228 4.1 Single-state RNA design benchmark ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "229 We set out to compare gRNAde to Rosetta, a state-of-the-art physically based toolkit for biomolecular   \n230 modelling and design [Leman et al., 2020]. We reproduced the benchmark setup from Das et al.   \n231 [2010] for Rosetta\u2019s fixed backbone RNA sequence design workflow on 14 RNA structures of   \n232 interest from the PDB, which mainly includes riboswitches, aptamers, and ribozymes (full listing in   \n233 Table 2). We trained gRNAde on the single-state split detailed in Section 3, explicitly excluding the   \n234 14 RNAs as well as any structurally similar RNAs in order to ensure that we fairly evaluate gRNAde\u2019s   \n235 generalization abilities vs. Rosetta.   \n236 gRNAde improves sequence recovery over Rosetta. In Figure 2, we compare gRNAde\u2019s native   \n237 sequence recovery for single-state design with numbers taken from Das et al. [2010] for Rosetta,   \n238 FARNA (a predecessor of Rosetta), and ViennaRNA (the most popular 2D inverse folding method).   \n239 gRNAde has higher recovery of $56\\%$ on average compared to $45\\%$ for Rosetta, $32\\%$ for FARNA, and   \n240 $27\\%$ for ViennaRNA. See Appendix Table 2 for per-RNA results.   \n241 gRNAde is significantly faster than Rosetta. In addition to superior sequence recovery, gRNAde   \n242 is significantly faster than Rosetta for high-throughout design pipelines. Training gRNAde from   \n243 scratch takes roughly 2\u20136 hours on a single A100 GPU, depending on the exact hyperparameters.   \n244 Once trained, gRNAde can design hundreds of sequences for backbones with hundreds of nucleotides ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/5793d836b8b4c66acc26fc712001bd2f21f4db474bd6dc359a69b392d06a6560.jpg", "img_caption": ["Design 1: Design 2: Design 3: perplexity: 1.310 perplexity: 1.382 perplexity: 1.425 recovery: 0.591 (27 edits) recovery: 0.409 (37 edits) recovery: 0.515 (30 edits) $\\mathrm{sc}2\\mathrm{D}=0.923$ , $\\mathrm{scRMSD}=1.384$ $\\mathrm{sc}2\\mathrm{D}=0.922$ , $\\mathrm{scRMSD}=2.125$ $\\mathrm{sc}2\\mathrm{D}=0.923$ , $\\mathrm{scRMSD}=3.213$ $\\mathrm{scTM}=0.831$ , $\\mathrm{scGDT}=0.830$ $\\mathrm{scTM}=0.687$ , $\\mathrm{scGDT}=0.678$ $\\mathrm{scTM}=0.512$ , $\\mathrm{scGDT}=0.526$ "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Cherry-picked designs for Guanine riboswitch aptamer (PDB: 4FE5). We show the RhoFold-predicted 3D structure in colour overlaid on the groundtruth structure in grey. Designs recover the base pairing patterns and tertiary structure of the RNA, as measured by high selfconsistency score. gRNAde\u2019s perplexity is correlated well with 3D self-consistency scores and can be useful for ranking designs. More design visualisations are available in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "245 in ${\\sim}1$ second with GPU acceleration. On the other hand, Rosetta takes order of hours to produce   \n246 a single design due to performing expensive Monte Carlo optimisations2. Deep learning methods   \n247 like gRNAde are arguably easier to use since no expert customization is required and setup is easier   \n248 compared to Rosetta [Dauparas et al., 2022], potentially making RNA design more broadly accessible.   \n249 gRNAde\u2019s perplexity correlates with sequence and structural recovery. In Figure 2b, we plot   \n250 native sequence recovery per sample for Rosetta vs. gRNAde, shaded by gRNAde\u2019s average perplexity   \n251 for each sample. Perplexity is an indicator of the model\u2019s confidence in its own prediction (lower   \n252 perplexity implies higher confidence) and appears to be correlated with native sequence recovery.   \n253 Additionally, visualisations of gRNAde\u2019s designs for a riboswitch in Figure 3 show that perplexity   \n254 is also correlated with structural self-consistency scores. In the subsequent Section 4.3, we further   \n255 demonstrate the utility of gRNAde\u2019s perplexity for zero-shot ranking of RNA fitness landscapes. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "256 4.2 Multi-state RNA design benchmark ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "257 Structured RNAs often adopt multiple distinct conformational states to perform biological functions   \n258 [Ken et al., 2023]. For instance, riboswitches adopt at least two distinct functional conformations: a   \n259 ligand bound (holo) and unbound (apo) state, which helps them regulate and control gene expression   \n260 [Stagno et al., 2017]. If we were to attempt single-state inverse design for such RNAs, each backbone   \n261 structure may lead to a different set of sampled sequences. It is not obvious how to select the   \n262 input backbone as well as designed sequence when using single-state models for multi-state design.   \n263 gRNAde\u2019s multi-state GNN, descibed in Section 2.3, directly \u2018bakes in\u2019 the multi-state nature of   \n264 RNA into the architecture and designs sequences explicitly conditioned on multiple states.   \n265 In order to evaluate gRNAde\u2019s multi-state design capabilities, we trained equivalent single-state and   \n266 multi-state gRNAde models on the multi-state split detailed in Section 3, where the validation and   \n267 test sets contain progressively more structurally flexible RNAs as measured by median RMSD among   \n268 multiple available states for an RNA.   \n269 Multi-state gRNAde boosts sequence recovery. In Figure 4a, we compared a single-state variant   \n270 of gRNAde with otherwise equivalent multi-state models (with up to 3 and 5 states, respectively) in   \n271 terms of native sequence recovery. Multi-state variants show marginal improvements, overall. As a   \n272 caveat, it is worth noting that multi-state models consume more GPU memory than an equivalent   \n273 single-state model during mini-batch training (approximate peak GPU usage for max. number of   \n274 states $=1$ : 12GB, 3: 28GB, 5: 50GB on a single A100 with at most 3000 total nodes in a mini-batch). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/c9eb17ee5ab022dc371621421d7de5df710936492fd76f69af1e700f04f91809.jpg", "img_caption": ["(a) Per-sample sequence recovery "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/f6bb44970bfb8b52406312a8c58ccae61fdafe5de400280ec681e6f04c610d48.jpg", "img_caption": ["(b) Per-nucleotide recovery vs. structural flexibility measures "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Multi-state design benchmark. (a) Multi-state gRNAde show marginal improvement over an equivalent single-state model in terms of average per-sample sequence recovery over all test RNAs. (b) When plotting sequence recovery per-nucleotide, multi-state gRNAde improves over a single-state model for structurally flexible regions of RNAs, as characterised by nucleotides that tend to undergo changes in base pairing (left), nucleotides with greater average solvent accessible surface area (centre), and nucleotides with higher average RMSD (right) across multiple states. Marginal histograms in blue show the distribution of values. We plot performance for one consistent random seed across all models; collated results and ablations are available in Appendix Table 1. ", "page_idx": 7}, {"type": "text", "text": "275 Improved recovery in structurally flexible regions. In Figure 4b, we evaluated gRNAde\u2019s   \n276 multi-state sequence recovery at a fine-grained, per-nucleotide level. Multi-state GNNs improve   \n277 sequence recovery over the single-state variant on structurally flexible nucleotides, as characterised   \n278 by undergoing changes in base pairing/secondary structure, higher average RMSD between 3D   \n279 coordinates across states, and larger solvent accessible surface area. ", "page_idx": 7}, {"type": "text", "text": "280 4.3 Zero-shot ranking of RNA fitness landscape ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "281 Lastly, we explored the use of gRNAde as a zero-shot ranker of mutants in RNA engineering   \n282 campaigns. Given the backbone structure of a wild type RNA of interest as well as a candidate set of   \n283 mutant sequences, we can compute gRNAde\u2019s perplexity of whether a given sequence folds into the   \n284 backbone structure. Perplexity is inversely related to the likelihood of a sequence conditioned on a   \n285 structure, as described in Section 2.4. We can then rank sequences based on how \u2018compatible\u2019 they   \n286 are with the backbone structure in order to select a subset to be experimentally validated in wet labs.   \n287 Retrospective analysis on ribozyme fitness landscape. A recent study by McRae et al. [2024]   \n288 determined a cryo-EM structure of a dimeric RNA polymerase ribozyme at $5\\mathrm{\\mathring{A}}$ resolution3, along   \n289 with fitness landscapes of ${\\sim}75\\mathrm{K}$ mutants for the catalytic subunit 5TU and ${\\sim}48\\mathrm{K}$ mutants for the   \n290 scaffolding subunit t1. We design a retrospective study using this data of (sequence, fitness value)   \n291 pairs where we simulate an RNA engineering campaign with the aim of improving catalytic subunit   \n292 fitness over the wild type 5TU sequence.   \n293 We consider various design budgets ranging from hundreds to thousands of sequences selected for   \n294 experimental validation, and compare 4 unsupervised approaches for ranking/selecting variants: (1)   \n295 random choice from all ${\\sim}75\\small{,}000$ sequences; (2) random choice from all 449 single mutant sequences;   \n296 (3) random choice from all single and double mutant sequences (as sequences with higher mutation   \n297 order tend to be less fit); and (4) negative gRNAde perplexity (lower perplexity is better). For each   \n298 design budget and ranking approach, we compute the expected maximum change in ftiness over the   \n299 wild type that could be achieved by screening as many variants as allowed in the given design budget.   \n300 We run 10,000 simulations to compute confidence intervals for the 3 random baselines.   \n301 gRNAde outperforms random baselines in low design budget scenarios. Figure 5 illustrates the   \n302 results of our retrospective study. At low design budgets of up to hundreds of sequences, which are   \n303 relevant in the case of a low throughput fitness screening assay, gRNAde outperforms all random   \n304 baselines in terms of the maximum change in ftiness over the wild type. The top 10 mutants as ranked   \n305 by gRNAde contain a sequence with 4-fold improved fitness, while the top 200 leads to a 5-fold   \n306 improvement. Note that gRNAde is used zero-shot here, i.e. it was not fine-tuned on any assay data.   \n307 Perspective. Overall, it is promising that gRNAde\u2019s perplexity correlates with experimental   \n308 fitness measurements out-of-the-box (zero-shot) and can be a useful ranker of mutant fitness in   \n309 our retrospective study. In realistic design scenarios, improvements could likely be obtained by   \n310 fine-tuning gRNAde on a low amount of experimental ftiness data. For example, latent features from   \n311 gRNAde may be finetuned or used as input to a prediction head with supervised learning on fitness   \n312 landscape data. This study acts as a sanity check before committing to wet lab validation of $\\operatorname{gRNAde}$   \n313 designs. We see random mutagenesis and directed evolution-based approaches as complementary to   \n314 de-novo design and inverse folding approaches like gRNAde. Random mutagenesis can be thought   \n315 of as local exploration around a wild type sequence, optimising fitness within an \u2018island\u2019 of activity.   \n316 Structure-based design approaches are akin to global jumps in sequence space, with the potential to   \n317 find new islands further away from the wild type [Huang et al., 2016]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/e424777aba764ea25551a78fdda35b12967e644a48a5e3fb4f3369d612799ef6.jpg", "img_caption": ["Figure 5: Retrospective study of gRNAde for ranking ribozyme mutant fitness. Using the backbone structure and mutational ftiness landscape data from an RNA polymerase ribozyme [McRae et al., 2024], we retrospectively analyse how well we can rank variants at multiple design budgets using random selection vs. gRNAde\u2019s perplexity for mutant sequences conditioned on the backbone structure (catalytic subunit 5TU). Note that gRNAde is used zero-shot here, i.e. it was not fine-tuned on any assay data. For stochastic strategies, bars indicate median values, and error bars indicate the interquartile range estimated from 10,000 simulations per strategy and design budget. At low throughput design budgets of up to ${\\sim}500$ sequences, selecting mutants using gRNAde outperforms random baselines in terms of the expected maximum improvement in fitness over the wild type. In particular, gRNAde performs better than single site saturation mutagenesis, even when all single mutants are explored (total of 449 single mutants, 10,493 double mutants for the catalytic subunit 5TU in McRae et al. [2024]). See Appendix Figure 10 for results on scaffolding subunit t1. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "318 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "319 We introduce gRNAde, a geometric deep learning pipeline for RNA sequence design conditioned   \n320 on one or more 3D backbone structures. gRNAde is superior to the physically based Rosetta for 3D   \n321 RNA inverse folding in terms of performance, inference speed, and ease of use. Further, gRNAde   \n322 enables explicit multi-state design for structurally flexible RNAs which was previously not possible   \n323 with Rosetta. gRNAde\u2019s perplexity correlates with native sequence and structural recovery, and   \n324 can be used for zero-shot ranking of mutants in RNA engineering campaigns. To the best of our   \n325 knowledge, gRNAde is also the first geometric deep learning architecture for multi-state biomolecule   \n326 representation learning; the model is generic and can be repurposed for other learning tasks on   \n327 conformational ensembles, including multi-state protein design.   \n328 Limitations. Key avenues for future development of gRNAde include supporting multiple interacting   \n329 chains, accounting for partner molecules with RNAs, and supporting negative design against undesired   \n330 conformations. We discuss practical tradeoffs to using gRNAde in real-world RNA design scenarios   \n331 in Appendix B, including limitations due to the current state of 3D RNA structure prediction tools. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "332 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "333 B. Adamczyk, M. Antczak, and M. Szachniuk. Rnasolo: a repository of cleaned pdb-derived rna 3d   \n334 structures. Bioinformatics, 2022. (Cited on page 5)   \n335 M. Baek, F. DiMaio, I. Anishchenko, J. Dauparas, S. Ovchinnikov, G. R. Lee, J. Wang, Q. Cong,   \n336 L. N. Kinch, R. D. Schaeffer, et al. Accurate prediction of protein structures and interactions using   \n337 a three-track neural network. Science, 2021. (Cited on page 13)   \n338 M. Baek, R. McHugh, I. Anishchenko, H. Jiang, D. Baker, and F. DiMaio. Accurate prediction of   \n339 protein\u2013nucleic acid complexes using rosettafoldna. Nature Methods, 2024. (Cited on page 13)   \n340 E. Bonnet, P. Rzazewski, and F. Sikora. Designing rna secondary structures is hard. Journal of   \n341 Computational Biology, 2020. (Cited on page 13)   \n342 M. M. Bronstein, J. Bruna, T. Cohen, and P. Velickovic. Geometric deep learning: Grids, groups,   \n343 graphs, geodesics, and gauges. arXiv preprint, 2021. (Cited on page 1)   \n344 J. Chen, Z. Hu, S. Sun, Q. Tan, Y. Wang, Q. Yu, L. Zong, L. Hong, J. Xiao, T. Shen, et al. Inter  \n345 pretable rna foundation model from unannotated data for highly accurate rna structure and function   \n346 predictions. arXiv preprint, 2022. (Cited on page 13)   \n347 A. Churkin, M. D. Retwitzer, V. Reinharz, Y. Ponty, J. Waldisp\u00fchl, and D. Barash. Design of rnas:   \n348 comparing programs for inverse rna folding. Briefings in bioinformatics, 2018. (Cited on page 2,   \n349 13)   \n350 T. R. Damase, R. Sukhovershin, C. Boada, F. Taraballi, R. I. Pettigrew, and J. P. Cooke. The limitless   \n351 future of rna therapeutics. Frontiers in bioengineering and biotechnology, 2021. (Cited on page 1)   \n352 R. Das, J. Karanicolas, and D. Baker. Atomic accuracy in predicting and designing noncanonical rna   \n353 structure. Nature methods, 2010. (Cited on page 1, 2, 5, 6, 13, 14, 15, 19)   \n354 J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. Wicky, et al. Robust   \n355 deep learning based protein sequence design using proteinmpnn. Science, 2022. (Cited on page 2,   \n356 3, 4, 7, 14)   \n357 W. K. Dawson, M. Maciejczyk, E. J. Jankowska, and J. M. Bujnicki. Coarse-grained modeling of rna   \n358 3d structure. Methods, 2016. (Cited on page 3)   \n359 K. Didi, F. Vargas, S. Mathis, V. Dutordoir, E. Mathieu, U. J. Komorowska, and P. Lio. A framework   \n360 for conditional diffusion modelling with applications in motif scaffolding for protein design. In   \n361 NeurIPS 2023 Machine Learning for Structural Biology Workshop, 2023. (Cited on page 13)   \n362 J. A. Doudna and E. Charpentier. The new frontier of genome engineering with crispr-cas9. Science,   \n363 2014. (Cited on page 1)   \n364 A. Duval, S. V. Mathis, C. K. Joshi, V. Schmidt, S. Miret, F. D. Malliaros, T. Cohen, P. Lio, Y. Bengio,   \n365 and M. Bronstein. A hitchhiker\u2019s guide to geometric gnns for 3d atomic systems. arXiv preprint,   \n366 2023. (Cited on page 1)   \n367 M. Felletti, J. Stifel, L. A. Wurmthaler, S. Geiger, and J. S. Hartig. Twister ribozymes as highly   \n368 versatile expression platforms for artificial riboswitches. Nature communications, 2016. (Cited on   \n369 page 1)   \n370 M. Fey and J. E. Lenssen. Fast graph representation learning with pytorch geometric. ICLR 2019   \n371 Representation Learning on Graphs and Manifolds Workshop, 2019. (Cited on page 4)   \n372 L. Fu, B. Niu, Z. Zhu, S. Wu, and W. Li. Cd-hit: accelerated for clustering the next-generation   \n373 sequencing data. Bioinformatics, 2012. (Cited on page 5)   \n374 L. R. Ganser, M. L. Kelly, D. Herschlag, and H. M. Al-Hashimi. The roles of structural dynamics in   \n375 the cellular functions of rnas. Nature reviews Molecular cell biology, 2019. (Cited on page 2)   \n376 D. Han, X. Qi, C. Myhrvold, B. Wang, M. Dai, S. Jiang, M. Bates, Y. Liu, B. An, F. Zhang, et al.   \n377 Single-stranded dna and rna origami. Science, 2017. (Cited on page 2, 13)   \n378 S. He, R. Huang, J. Townley, R. C. Kretsch, T. G. Karagianes, D. B. Cox, H. Blair, D. Penzar, V. Vyalt  \n379 sev, E. Aristova, et al. Ribonanza: deep learning of rna structure through dual crowdsourcing.   \n380 bioRxiv, 2024. (Cited on page 13)   \n381 J. Hoetzel and B. Suess. Structural changes in aptamers are essential for synthetic riboswitch   \n382 engineering. Journal of Molecular Biology, 2022. (Cited on page 2)   \n383 P.-S. Huang, S. E. Boyken, and D. Baker. The coming of age of de novo protein design. Nature, 2016.   \n384 (Cited on page 3, 9)   \n385 J. Ingraham, V. Garg, R. Barzilay, and T. Jaakkola. Generative models for graph-based protein design.   \n386 NeurIPS, 2019. (Cited on page 3, 20)   \n387 J. B. Ingraham, M. Baranov, Z. Costello, K. W. Barber, W. Wang, A. Ismail, V. Frappier, D. M.   \n388 Lord, C. Ng-Thow-Hing, E. R. Van Vlack, et al. Illuminating protein space with a programmable   \n389 generative model. Nature, 2023. (Cited on page 13)   \n390 B. Jing, S. Eismann, P. Suriana, R. J. L. Townshend, and R. Dror. Learning from protein structure   \n391 with geometric vector perceptrons. In International Conference on Learning Representations,   \n392 2020. (Cited on page 3, 4, 20)   \n393 C. K. Joshi, C. Bodnar, S. V. Mathis, T. Cohen, and P. Lio. On the expressive power of geometric   \n394 graph neural networks. In International Conference on Machine Learning, 2023. (Cited on page   \n395 17)   \n396 J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool,   \n397 R. Bates, A. Zidek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold.   \n398 Nature, 2021. (Cited on page 1, 13)   \n399 M. L. Ken, R. Roy, A. Geng, L. R. Ganser, A. Manghrani, B. R. Cullen, U. Schulze-Gahmen,   \n400 D. Herschlag, and H. M. Al-Hashimi. Rna conformational propensities determine cellular activity.   \n401 Nature, 2023. (Cited on page 2, 7)   \n402 J. K. Leman, B. D. Weitzner, S. M. Lewis, J. Adolf-Bryfogle, N. Alam, R. F. Alford, M. Aprahamian,   \n403 D. Baker, K. A. Barlow, P. Barth, et al. Macromolecular modeling and design in rosetta: recent   \n404 methods and frameworks. Nature methods, 2020. (Cited on page 2, 6)   \n405 K. Leppek, R. Das, and M. Barna. Functional $5'$ utr mrna structures in eukaryotic translation   \n406 regulation and how to find them. Nature reviews Molecular cell biology, 2018. (Cited on page 1)   \n407 S. Li, S. Moayedpour, R. Li, M. Bailey, S. Riahi, L. Kogler-Anele, M. Miladi, J. Miner, D. Zheng,   \n408 J. Wang, et al. Codonbert: Large language models for mrna design and optimization. bioRxiv,   \n409 2023a. (Cited on page 13)   \n410 Y. Li, C. Zhang, C. Feng, R. Pearce, P. Lydia Freddolino, and Y. Zhang. Integrating end-to  \n411 end learning with deep geometrical potentials for ab initio rna structure prediction. Nature   \n412 Communications, 2023b. (Cited on page 13)   \n413 M. Mandal and R. R. Breaker. Gene regulation by riboswitches. Nature reviews Molecular cell   \n414 biology, 2004. (Cited on page 1)   \n415 E. K. McRae, C. J. Wan, E. L. Kristoffersen, K. Hansen, E. Gianni, I. Gallego, J. F. Curran, J. Attwater,   \n416 P. Holliger, and E. S. Andersen. Cryo-em structure and functional landscape of an rna polymerase   \n417 ribozyme. Proceedings of the National Academy of Sciences, 2024. (Cited on page 2, 8, 9, 19)   \n418 M. Metkar, C. S. Pepin, and M. J. Moore. Tailor made: the art of therapeutic mrna design. Nature   \n419 Reviews Drug Discovery, 2024. (Cited on page 1)   \n420 M. G. Mohsen, M. K. Midy, A. Balaji, and R. R. Breaker. Exploiting natural riboswitches for aptamer   \n421 engineering and validation. Nucleic Acids Research, 2023. (Cited on page 1)   \n422 K. Mustafina, K. Fukunaga, and Y. Yokobayashi. Design of mammalian on-riboswitches based on   \n423 tandemly fused aptamer and ribozyme. ACS Synthetic Biology, 2019. (Cited on page 1)   \n424 R. J. Penic, T. Vlasic, R. G. Huber, Y. Wan, and M. Sikic. Rinalmo: General-purpose rna language   \n425 models can generalize well on structure prediction tasks. arXiv preprint, 2024. (Cited on page 13)   \n426 F. Runge, D. Stoll, S. Falkner, and F. Hutter. Learning to design RNA. In ICLR, 2019. (Cited on   \n427 page 13)   \n428 B. Schneider, B. A. Sweeney, A. Bateman, J. Cerny, T. Zok, and M. Szachniuk. When will rna get its   \n429 alphafold moment? Nucleic Acids Research, 2023. (Cited on page 2, 5)   \n430 T. Shen, Z. Hu, Z. Peng, J. Chen, P. Xiong, L. Hong, L. Zheng, Y. Wang, I. King, S. Wang, et al.   \n431 E2efold-3d: End-to-end deep learning method for accurate de novo rna 3d structure prediction.   \n432 arXiv preprint, 2022. (Cited on page 5)   \n433 J. Stagno, Y. Liu, Y. Bhandari, C. Conrad, S. Panja, M. Swain, L. Fan, G. Nelson, C. Li, D. Wendel,   \n434 et al. Structures of riboswitch rna reaction states by mix-and-inject xfel serial crystallography.   \n435 Nature, 2017. (Cited on page 7)   \n436 C. Tan, Y. Zhang, Z. Gao, H. Cao, and S. Z. Li. Hierarchical data-efficient representation learning for   \n437 tertiary structure-based rna design. arXiv preprint, 2023. (Cited on page 13, 14)   \n438 R. J. Townshend, S. Eismann, A. M. Watkins, R. Rangan, M. Karelina, R. Das, and R. O. Dror.   \n439 Geometric deep learning of rna structure. Science, 2021. (Cited on page 13)   \n440 Q. Vicens and J. S. Kieft. Thoughts on how to think (and talk) about rna structure. Proceedings of   \n441 the National Academy of Sciences, 2022. (Cited on page 13, 17)   \n442 L. M. Wadley, K. S. Keating, C. M. Duarte, and A. M. Pyle. Evaluating and learning from rna   \n443 pseudotorsional space: quantitative validation of a reduced representation for rna structure. Journal   \n444 of molecular biology, 2007. (Cited on page 3)   \n445 W. Wang, C. Feng, R. Han, Z. Wang, L. Ye, Z. Du, H. Wei, F. Zhang, Z. Peng, and J. Yang. trrosettarna:   \n446 automated prediction of rna 3d structure with transformer network. Nature Communications, 2023.   \n447 (Cited on page 13)   \n448 M. Ward, E. Courtney, and E. Rivas. Fitness functions for rna structure design. Nucleic Acids   \n449 Research, 2023. (Cited on page 13)   \n450 A. M. Watkins, R. Rangan, and R. Das. Farfar2: improved de novo rosetta prediction of complex   \n451 global rna folds. Structure, 2020. (Cited on page 13)   \n452 J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst,   \n453 R. J. Ragotte, L. F. Milles, et al. De novo design of protein structure and function with rfdiffusion.   \n454 Nature, 2023. (Cited on page 2, 5, 13)   \n455 H. K. Wayment-Steele, W. Kladwang, A. I. Strom, J. Lee, A. Treuille, A. Becka, E. Participants, and   \n456 R. Das. Rna secondary structure packages evaluated and improved by high-throughput experiments.   \n457 Nature methods, 2022. (Cited on page 4)   \n458 R. J. Williams and D. Zipser. A learning algorithm for continually running fully recurrent neural   \n459 networks. Neural computation, 1989. (Cited on page 4)   \n460 J. D. Yesselman, D. Eiler, E. D. Carlson, M. R. Gotrik, A. E. d\u2019Aquino, A. N. Ooms, W. Kladwang,   \n461 P. D. Carlson, X. Shi, D. A. Costantino, et al. Computational design of three-dimensional rna   \n462 structure and function. Nature nanotechnology, 2019. (Cited on page 2, 13, 14)   \n463 M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep sets.   \n464 NeurIPS, 2017. (Cited on page 4, 20)   \n465 C. Zhang, M. Shine, A. M. Pyle, and Y. Zhang. Us-align: universal structure alignments of proteins,   \n466 nucleic acids, and macromolecular complexes. Nature methods, 2022. (Cited on page 5)   \n467 Y. Zhu, L. Zhu, X. Wang, and H. Jin. Rna-based therapeutics: An overview and prospectus. Cell   \n468 Death & Disease, 2022. (Cited on page 1) ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "469 A Related Work ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "470 We attempt to briefly summarise recent developments in RNA structure modelling and design, with   \n471 an emphasis on deep learning-based approaches.   \n472 RNA inverse folding. Most tools for RNA inverse folding focus on secondary structure without   \n473 considering 3D geometry [Churkin et al., 2018, Runge et al., 2019] and approach the problem from   \n474 the lens of energy optimisation [Ward et al., 2023]. Rosetta fixed backbone re-design [Das et al.,   \n475 2010] is the only energy optimisation-based approach that accounts for 3D structure. Deep neural   \n476 networks such as gRNAde can incorporate 3D structural constraints and are orders of magnitude   \n477 faster than optimisation-based approaches; this is particularly attractive for high-throughput design   \n478 pipelines as solving the inverse folding optimisation problem is NP hard [Bonnet et al., 2020].   \n479 RNA structure design. Inverse folding models for protein design have often been coupled with   \n480 backbone generation models which design structural backbones conditioned on various design   \n481 constraints [Watson et al., 2023, Ingraham et al., 2023, Didi et al., 2023]. Current approaches for   \n482 RNA backbone design use classical (non-learnt) algorithms for aligning 3D RNA motifs [Han et al.,   \n483 2017, Yesselman et al., 2019], which are small modular pieces of RNA that are believed to fold   \n484 independently. Such algorithms may be restricted by the use of hand-crafted heuristics and we plan   \n485 to explore data-driven generative models for RNA backbone design in future work.   \n486 RNA structure prediction. There have been several recent efforts to adapt protein folding   \n487 architectures such as AlphaFold2 [Jumper et al., 2021] and RosettaFold [Baek et al., 2021] for RNA   \n488 structure prediction [Li et al., 2023b, Wang et al., 2023, Baek et al., 2024]. A previous generation of   \n489 models used GNNs as ranking functions together with Rosetta energy optimisation [Watkins et al.,   \n490 2020, Townshend et al., 2021]. None of these architectures aim at capturing conformational flexibility   \n491 of RNAs, unlike gRNAde which represents RNAs as multi-state conformational ensembles. Neither   \n492 can structure prediction tools be used for RNA design tasks as they are not generative models.   \n493 RNA language models. Self-supervised language models have been developed for predictive and   \n494 generative tasks on RNA sequences, including general-purpose models such as RNA FM [Chen   \n495 et al., 2022] and RiNaLMo [Penic et al., 2024] as well as mRNA-specific CodonBERT [Li et al.,   \n496 2023a]. RNA sequence data repositories are orders of magnitude larger than those for RNA structure   \n497 (eg. RiNaLMo is trained on 36 million sequences). However, standard language models can only   \n498 implicitly capture RNA structure and dynamics through sequence co-occurence statistics, which   \n499 can pose a chellenge for designing structured RNAs such as riboswitches, aptamers, and ribozymes.   \n500 RibonanzaNet [He et al., 2024] represents a recent effort in developing structure-informed RNA   \n501 language models by supervised training on experimental readouts from chemical mapping, although   \n502 RibonanzaNet cannot be used for RNA design. Inverse folding methods like gRNAde are language   \n503 models conditioned on 3D structure, making them a natural choice for structure-based design.   \n504 Comparison to contemporaneous work. Concurrently, Tan et al. [2023] also developed a deep   \n505 learning-based 3D RNA inverse folding model. We want to emphasize that this is independent work,   \n506 but for completeness we include a discussion on key differences to gRNAde:   \n508 \u2013 New capabilities: gRNAde enables explicit multi-state design to generate sequences   \n509 conditioned on multiple backbone structures, which is not possible with Rosetta nor Tan   \n510 et al. [2023]\u2019s approach. We have also demonstrated the utility of gRNAde\u2019s perplexity for   \n511 zero-shot ranking of mutants in RNA engineering campaigns.   \n512 \u2013 Decoding: gRNAde uses an autoregressive decoder with rotation-equivariant GNN layers,   \n513 while Tan et al. [2023] use a non-autoregressive (one-shot) decoder with rotation-invariant   \n514 layers. In our ablation study (Appendix D), we found autoregressive decoding to show   \n515 significantly higher 2D and 3D self-consistency scores than non-autoregressive decoding,   \n516 even though non-autoregressive decoding lead to higher sequence recovery. Autoregressive   \n517 decoding is more expressive and can condition predictions at each decoding step on past   \n518 predictions, while one-shot decoders sample from independent probability distributions for   \n519 each nucleotide. We find autoregressive decoding to be a better inductive bias for predicting   \n520 base pairing and base stacking interactions that are drivers of RNA structure [Vicens and   \n521 Kieft, 2022]. For instance, G-C and A-U pairs can often be swapped for one another, but   \n522 non-autoregressive decoding does not capture such paired constraints.   \n523 \u2022 Evaluation:   \n524 \u2013 Evaluation metrics: Tan et al. [2023] focus on measuring native sequence recovery, only.   \n525 We have additionally introduced structural self-consistency metrics at the 2D and 3D level,   \n526 which have been shown to better correlate with experimental success in protein design.   \n527 \u2013 Perplexity: We found gRNAde\u2019s perplexity to be correlated with sequence and structural   \n528 recovery, as well as demonstrated its utility for zero-shot ranking of mutants in RNA   \n529 engineering. On the other hand, Tan et al. [2023] do not report perplexity and claim that   \n530 perplexity is an unsuitable metric for RNA design.   \n531 \u2013 Data splitting: While both studies use structural clustering to evaluate generalisation to   \n532 structurally dissimilar RNAs, Tan et al. [2023]\u2019s test splits are determined randomly. Our   \n533 experiments use currated test splits from Das et al. [2010] to fairly compare gRNAde   \n534 to physically based Rosetta, as well as split based on structural flexibility to benchmark   \n535 multi-state design.   \n536 \u2022 Usage and reproducibility: We release open source training and inference code as well as model   \n537 checkpoints to enable complete reproducibility. We also release Colab notebooks and detailed   \n538 tutorials to make gRNAde broadly applicable and useful in real-world RNA design campaigns.   \n539 At present, it is not possible to reproduce the results in Tan et al. [2023] or compare to gRNAde   \n540 directly as no training code is available. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "541 B FAQs on using gRNAde ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "542 How to chose the number of states to provide as input to gRNAde? In general, this would depend   \n543 on the design objective. For instance, designing riboswitches may necessitate multi-state design,   \n544 while a single-state pipeline may be more sensible for locking an aptamer into its bound conformation   \n545 [Yesselman et al., 2019]. Note that it may be possible to benefit from multi-state gRNAde models   \n546 even when performing single-state design by using slightly noised variations of the same backbone   \n547 structure as an input conformational ensemble.   \n548 How to prioritise or chose amongst designed sequences? We have currently provided 3 types   \n549 of evaluation metrics: native sequence recovery, structural self-consistency scores and perplexity,   \n550 towards this end. We suspect that recovery may not be the ideal choice, except for design scenarios   \n551 where we require certain regions of the RNA sequence to be conserved or native-like. Self-consistency   \n552 scores may provide an overall more holistic evaluation metric as they accounts for alternative base   \n553 pairings which still lead to similar structures as well as better capture the recovery of structural motifs   \n554 responsible for functionality. However, structural self-consistency scores inherit the limitations of   \n555 the structure prediction methods used as part of their computation. For instance, computing the self  \n556 consistency score between an RNA backbone and its own native sequence provides an upper bounds   \n557 on the maximum score that designs can obtain under a given structure prediction method. Lastly,   \n558 gRNAde\u2019s perplexity estimates the likelihood of a sequence given a backbone and can be useful for   \n559 ranking designs and mutants in RNA engineering campaigns (especially for design scenarios where   \n560 structure prediction tools are not performant).   \n561 In real-world design scenarios, we can pair gRNAde with another machine learning model (an   \n562 \u2018oracle\u2019) for ranking or predicting the suitability of designed sequences for the objective (for instance,   \n563 binding affinity or some other notion of ftiness). We hope to conduct further experimental validation   \n564 of gRNAde designs in the wet lab in order to better understand these tradeoffs.   \n565 Why not average single-state logits over multiple states for multi-state design? ProteinMPNN   \n566 [Dauparas et al., 2022] proposes to average logits from multiple backbones for multi-state protein   \n567 design. Here is a simple example to highlight issues with such an approach: Consider two states A   \n568 and B, and choice of labels X, Y, and Z. For state A: X, Y, Z are assigned probabilities $75\\%$ , $20\\%$ ,   \n569 $5\\%$ . For state B: X, Y, Z are assigned probabilities $5\\%$ , $20\\%$ , $75\\%$ . Logically, label Y is the only one   \n570 that is compatible with both states. However, averaging the probabilities would lead to label $\\Chi$ or $Z$   \n571 being more likely to be sampled in designs. As an alternative, gRNAde is based on multi-state GNNs   \n572 which can take as input one or more backbone structures and generate sequences conditioned on the   \n573 conformational ensemble directly. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/e0509735ba653a2c43b77b2784cffd5f6dedc9ea130e0c5ff10119a97f7eea36.jpg", "img_caption": ["Figure 6: 3D self-consistency scores for 3 representative RNAs from Das et al. [2010]. We use RhoFold to \u2018forward fold\u2019 100 designs sampled at temperature $=0.5$ and plot self-consistency TMscore and GDT_TS. Each dot corresponds to one designed sequence and is coloured by gRNAde\u2019s perplexity (normalised per RNA). Designs with lower relative perplexity generally have higher 3D self-consistency and can be considered more \u2018designable\u2019. Dotted lines represent TM-score and GDT_TS thresholds of 0.45 and 0.50, repsectively. Pairs of structures scoring higher than the threshold correspond to roughly the same fold. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/f5e393652eb0fd7e80a494a0a4de1c3300011055cb5a704bd170fa9bfa731140.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 7: Cherry-picked designs for Guanine riboswitch aptamer (PDB: 4FE5, sequence: GGACAUAUAAUCGCGUGGAUAUGGCACGCAAGUUUCUACCGGGCACCGUAAAUGUCCGACUAUGUCC). ", "page_idx": 14}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/c852ac83c61c487203172eb283f800ae080b1f933b78a94dee62a3ae5324d729.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Design 1:   \nGGGGCUCCGGCGACGCAGUCGAAAG   \nCCCAGCAGUACCAAGCCUCAGGGGA   \nAACUUUGAGGUGGCCUAACAAAGGA   \nUACGGUAAUAAGCUGCGGGAAAAGG   \nUUGUAAGCCGGAGCGAAGACCUAAG   \nGCACCGCUUUUGGCGGUGCUAUGGU   \nUGAAGUUAA   \nperplexity: 1.2462   \nrecovery: 0.7170 (44 edits)   \n$\\mathrm{sc}2\\mathrm{D}=0.8301$   \n$\\mathrm{scRMSD}=5.4562$   \n$\\mathrm{scTM}=0.6481$   \n$\\mathrm{scGDT}=0.4465$   \nDesign 2:   \nGGGGUACCGGCGACGCAGUCGAAUG   \nCCCUGUGGUACCAAGCCCCGGGGGA   \nAACUUCGGGGUGGCCUUACCAAGGA   \nCACGGUAAUAAGCCACGGGAAAUGG   \nUUGUAAGCCGGUCCGAAGCCCUAAG   \nGCCGCGCUUUGGGCGCGGCUAUGGG   \nUGAAGGCAA   \nperplexity: 1.3273   \nrecovery: 0.6226 (58 edits)   \n$\\mathrm{sc}2\\mathrm{D}=0.6896$   \n$\\mathrm{scRMSD}=6.7239$   \n$\\mathrm{scTM}=0.6300$   \n$\\mathrm{scGDT}=0.4513$   \nDesign 3:   \nGAGGCCACGGCAACGCAGUCUAACG   \nCCCUGUGGUACCAAGUCUUAGGAGA   \nAAUUUUAAGAUGGCCUAAUAAAGGA   \nUAUGGUAAUAAGCCACGGGAAAAGG   \nUUGUAAGACGUGACGAAGUCCUAAG   \nGCCACAGUUUUGCUGUGGCUAUGGA   \nUGGAGUACA   \nperplexity: 1.3204   \nrecovery: 0.7044 (45 edits)   \n$\\mathrm{sc}2\\mathrm{D}=0.7922$   \n$\\mathrm{scRMSD}=8.8211$   \n$\\mathrm{scTM}=0.4582$   \n$\\mathrm{scGDT}=0.2909$ ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Figure 8: Cherry-picked designs for Tetrahymena Ribozyme P4-P6 domain (PDB: 2R8S, sequence: GGAAUUGCGGGAAAGGGGUCAACAGCCGUUCAGUACCAAGUCUCAGGGGAAACUUUGAGAUGGCCUUGCAAAGGGU AUGGUAAUAAGCUGACGGACAUGGUCCUAACACGCAGCCAAGUCCUAAGUCAACAGAUCUUCUGUUGAUAUGGAUGCAGUUCA). ", "page_idx": 15}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/d90b6c077047b781a285b79bf0d7b1d27e7b1e64c98084846af04e6728f42c33.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/2e0b5764a27106ff42df53fcd0f15e819d0414d5886fd12134496478152784cb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/b2a79745f04e906bc6febf5184811d3d2178e258ca1a755a155feabeb1def369.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Design 1:   \nGUCAAACGCAGCCGAAA   \nGCGCGAUAGUCCCAGGAA   \nperplexity $=1.6237$   \nrecovery $=0.4571$ (16 edits)   \n$\\mathrm{sc}2\\mathrm{D}=-0.0074$   \nscRMSD $=3.9505$   \n$\\mathrm{scTM}=0.2597$   \nscGDT $=0.4786$   \nDesign 2:   \nGGCAAACGCGGCCGAAA   \nGCGCGUGAGUCCCCGGAC   \nperplexity $=1.6630$   \nrecovery $=0.4857$ (16 edits)   \n$\\mathrm{sc}2\\mathrm{D}=-0.0099$   \n$\\mathrm{scRMSD}=3.3549$   \n$\\mathrm{scTM}=0.2526$   \n$\\mathrm{scGDT}=0.5000$   \nDesign 3:   \nCGUAGUCGGAGCCGAAG   \nGGCCGUUAGUCCCAGGAG   \nperplexity $=1.7020$   \nrecovery $=0.4000$ (17 edits)   \n$\\mathrm{sc}2\\mathrm{D}=0.4035$   \nscRMSD = 16.4102   \n$\\mathrm{scTM}=0.0319$   \n$\\mathrm{scGDT}=0.0571$ ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Figure 9: Cherry-picked designs for Vitamin B12 binding aptamer (PDB: 1ET4, sequence: GGAACCGGUGCGCAUAACCACCUCAGUGCGAGCAA). ", "page_idx": 15}, {"type": "table", "img_path": "Fm4FkfGTLu/tmp/96298005c849f82c8bdf6832738f849df5a3f9ec414617e580458911c03cc961.jpg", "table_caption": ["Table 1: Ablation study and aggregated benchmark results for gRNAde. We report metrics averaged over 100 test sets samples and standard deviations across 3 consistent random seeds. The percentages reported in brackets for the 3D self-consistency scores are the percentage of designed samples within the \u2018designability\u2019 threshold values $(\\mathrm{scRMSD}{\\leq}2\\mathring{\\mathrm{A}}$ , $\\mathrm{scTM}{\\ge}0.45$ , $\\mathrm{scGDT}{\\scriptstyle\\geq0.5})$ . "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "575 D Ablation Study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "576 Table 1 presents an ablation study as well as aggregated benchmark for various configurations of   \n577 gRNAde. Key takeaways are highlighted below. Note that all results in the main paper are reported   \n578 for models trained on the maximum length of 5000 nucleotides using autoregressive decoding and   \n579 rotation-equivariant GNN layers, as this lead to the lowest perplexity values.   \n580 Max. train RNA length Limiting the maximum length of RNAs used for training can be seen   \n581 as ablating the use of ribosomal RNA families (which are thousands of nucleotides long and form   \n582 complexes with specialised ribosomal proteins). We find that training on only short RNAs fewer than   \n583 1000s of nucleotides leads to worse sequence recovery and 3D self-consistency scores, even though it   \n584 improves 2D self-consistency across both evaluation splits. This suggests that tertiary interactions   \n585 learnt from ribosomal RNAs can generalise to other RNA families to some extent (large ribosomal   \n586 RNAs were excluded from test sets).   \n587 GNN We ablated whether the internal representations of the GVP-GNN are rotation invariant or   \n588 equivariant. Equivariant GNNs are theoretically more expressive [Joshi et al., 2023] and we do find   \n589 them more capable at ftiting the training distribution (as shown by lower perplexity). However, we do   \n590 not find significant differences in terms of other performance metrics across different GNN layers.   \n591 Model \u2018AR\u2019 implies autoregressive decoding (described in Section 2.3, uses 4 encoder and 4   \n592 decoder layers), while \u2018NAR\u2019 implies non-autoregressive, one-shot decoding using an MLP (uses 8   \n593 encoder layers). Across both evaluation splits, AR models show significantly higher self-consistency   \n594 scores than NAR, even though NAR lead to higher sequence recovery. AR is more expressive and   \n595 can condition predictions at each decoding step on past predictions, while one-shot NAR samples   \n596 from independent probability distributions for each nucleotide. Thus, AR is a better inductive bias   \n597 for predicting base pairing and base stacking interactions that are drivers of RNA structure [Vicens   \n598 and Kieft, 2022]. For instance, G-C and A-U pairs can often be swapped for one another, but   \n599 non-autoregressive decoding does not capture such paired constraints.   \n600 Max. #states We evaluate the impact of increasing the maximum number of states as input to   \n601 gRNAde. Multi-state models marginally improve native sequence recovery as well as structural   \n602 self-consistency scores over an equivalent single state variant, even for the single-state benchmark   \n603 where the multi-state model is being used with only one state as input. This suggests that seeing   \nmultiple states during training can be useful for gRNAde\u2019s performance even for single-state design   \n605 tasks.   \nNon-learnt baselines. We report the performance of two non-learnt baselines to contextualise   \n607 gRNAde\u2019s performance: for each test sample, simply predicting the groundtruth sequence back   \n608 and predicting a random sequence. Structural self-consistency scores for the Groundtruth baseline   \n609 provides a rough upper bounds on the maximum score that any gRNAde designs can theoretically   \n610 obtain given the current state of 2D/3D structure predictors being used. gRNAde always performs   \n611 better than the random baseline and often reaches 2D self-consistency scores close to the upper bound.   \n612 Both 2D and 3D self-consistency scores are inherently limited by the performance of the structure   \n613 prediction methods used.   \n614 2D inverse folding baseline. We additionally report results for ViennaRNA\u2019s 2D-only inverse   \n615 folding method to further demonstrate the utility of 3D inverse folding. ViennaRNA has improved   \n616 2D self-consistency scores over gRNAde but fails to capture tertiary interactions in its designs, as ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "617 evident by poor recovery and 3D self-consistency scores similar to the random baseline. ", "page_idx": 17}, {"type": "text", "text": "618 Split. Single- and multi-state splits are described in Section 3; the multi-state split is relatively harder   \n619 than the single-state split based on overall reduced performance for all baselines and models. Models   \n620 trained on \u2018All data\u2019 use all RNASolo samples for training, solely for the purpose of releasing the best   \n621 possible gRNAde checkpoints for real-world usage. Evaluation metrics for \u2018All data\u2019 are reported on   \n622 the single-state test set. ", "page_idx": 17}, {"type": "text", "text": "Table 2: Full results for Figure 2 comparing gRNAde to Rosetta, FARNA and ViennaRNA for single-state design on $14~\\mathrm{RNA}$ structures of interest identified by Das et al. [2010]. Rosetta and FARNA recovery values are taken from Das et al. [2010], Supplementary Table 2. ", "page_idx": 18}, {"type": "table", "img_path": "Fm4FkfGTLu/tmp/911b2ec3f0742b77321d3a0e68b8a03639a4c8bb30c8b527f2b91ee803ba5d6e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/6fdae333843701d9813e302ba7f643a90fd89d4652cf3ed6724146d67536691b.jpg", "img_caption": ["Figure 10: Retrospective study of gRNAde for ranking ribozyme mutant fitness (t1 subunit). Using the backbone structure and mutational fitness landscape data from an RNA polymerase ribozyme [McRae et al., 2024], we retrospectively analyse how well we can rank variants at multiple design budgets using random selection vs. gRNAde\u2019s perplexity for mutant sequences conditioned on the backbone structure (scaffolding subunit t1). gRNAde performs better than single site saturation mutagenesis, even when all single mutants are explored (total of 403 single mutants, 17,027 double mutants for the scaffolding subunit t1 in McRae et al. [2024]). See Section 4.3 for results on catalytic subunit 5TU and further discussions. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "624 F Additional Figures ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "625 Figure 11: RNA backbone featurization.   \n626 Figure 12: gRNAde model architecture.   \n627 Figure 13: In-silico evaluation metrics for gRNAde.   \n628 Figure 14: Multi-graph tensor representation of RNA conformational ensembles.   \n629 Listing 1: Pseudocode for multi-state GNN encoder layer.   \n630 Figure 15: RNASolo data statistics. ", "page_idx": 19}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/7cf4b3b9fa47c6bae314e406b81fc73de3f12ff830ff3d0b8a7a7da353489367.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 11: gRNAde featurizes RNA backbone structures as 3D geometric graphs. Each RNA nucleotide is a node in the graph, consisting of 3 coarse-grained beads for the coordinates for P, $\\mathrm{C4}'$ , N1 (pyrimidines) or N9 (purines) which are used to compute initial geometric features and edges to nearest neighbours in 3D space. Backbone chain figure adapted from Ingraham et al. [2019]. ", "page_idx": 19}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/91163dde212656065896f3083d45ea38eeea913ddcd4d8e0b9d6b798de0cce03.jpg", "img_caption": ["Figure 12: gRNAde model architecture. One or more RNA backbone geometric graphs are encoded via a series of SE(3)-equivariant Graph Neural Network layers [Jing et al., 2020] to build latent representations of the local 3D geometric neighbourhood of each nucleotide within each state. Representations from multiple states for each nucleotide are then pooled together via permutation invariant Deep Sets [Zaheer et al., 2017], and fed to an autoregressive decoder to predict a probabilities over the four possible bases (A, G, C, U). The probability distribution can be sampled to design a set of candidate sequences. During training, the model is trained end-to-end by minimising a cross-entropy loss between the predicted probability distribution and the true sequence identity. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/8dd2368badf4f770712a0848f99478dafaeb8246f292672a37bdff7c88de3107.jpg", "img_caption": ["Figure 13: In-silico evaluation metrics for gRNAde designed sequences. We consider (1) sequence recovery, the percentage of native nucleotides recovered in designed samples, (2) self-consistency scores, which are measured by \u2018forward folding\u2019 designed sequences using a structure predictor and measuring how well 2D and 3D structure are recovered (we use EternaFold and RhoFold for 2D/3D structure prediction, respectively). We also report (3) perplexity, the model\u2019s estimate of the likelihood of a sequence given a backbone. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/e397940d73da1bcc0eaaf535e612bae1528382d86ad6c18f34d186d666ccc15b.jpg", "img_caption": ["Figure 14: Multi-graph tensor representation of RNA conformational ensembles, and the associated symmetry groups acting on each axis. We process a set of $k$ RNA backbone conformations with $n$ nodes each into a tensor representation. Each multi-state GNN layer updates the tensor while being equivariant to the underlying symmetries; pseudocode is available in Listing 1. Here, we show a tensor of 3D vector-type features with shape $n\\times k\\times3$ . As depicted in the equivariance diagram, the updated tensor must be equivariant to permutation $S_{n}$ of $n$ nodes for axis 1, permutation $S_{k}$ of $k$ conformers for axis 2, and rotation $S O(3)/O(3)$ of the 3D features for axis 3. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "class MultiGVPConv( MessagePassing ): \u2019\u2019GVPConv for handling multiple conformations \u2019\u2019 ", "page_idx": 21}, {"type": "text", "text": "def __init__(self , ...): ", "page_idx": 21}, {"type": "text", "text": "def forward(self , x_s , x_v , edge_index , edge_attr): # stack scalar feats along axis 1: # [n_nodes , n_conf , d_s] -> [n_nodes , n_conf $^*$ d_s] x_s = x_s.view(x_s.shape [0], x_s.shape [1] \\* x_s.shape [2]) # stack vector feat along axis 1: # [n_nodes , n_conf , d_v , 3] -> [n_nodes , n_conf $^*$ $\\mathsf{d}_{-}\\mathsf{v}*3]$ x_v = x_v.view(x_v.shape [0], x_v.shape [1] $^*$ x_v.shape $[2]*3!$ ) # message passing and aggregation message $=$ self.propagate( edge_index , ${\\bf S}={\\bf x}_{-}\\,{\\bf s}$ , $\\mathtt{v}\\!=\\!\\mathtt{x}_{-}\\mathtt{v}$ , edge_attr $=$ edge_attr) # split scalar and vector channels return _split_multi(message , d_s , d_v , n_conf) ", "page_idx": 21}, {"type": "text", "text": "def message(self , s_i , v_i , s_j , v_j , edge_attr): ", "page_idx": 21}, {"type": "text", "text": "# unstack scalar feats:   \n# [n_nodes , n_conf $^*$ d] -> [n_nodes , n_conf , d_s]   \ns_i $=$ s_i.view(s_i.shape [0], s_i.shape [1]// d_s , d_s)   \ns_j = s_j.view(s_j.shape [0], s_j.shape [1]// d_s , d_s)   \n# unstack vector feats:   \n# [n_nodes , n_conf $^*$ d_v \\*3] $->$ [n_nodes , n_conf , d_v , 3]   \nv_i = v_i.view(v_i.shape [0], v_i.shape [1]//( d_v \\*3), d_v , 3)   \nv_j = v_j.view(v_j.shape [0], v_j.shape [1]//( d_v \\*3), d_v , 3)   \n# message function for edge j-i   \nmessage $=$ tuple_cat ((s_j , v_j), edge_attr , (s_i , v_i))   \nmessage $=$ self.message_func(message) # GVP   \n# merge scalar and vector channels along axis 1   \nreturn _merge_multi (\\* message) ", "page_idx": 21}, {"type": "text", "text": "def _split_multi(x, d_s , d_v , n_conf): ", "page_idx": 21}, {"type": "text", "text": "Splits a merged representation of (s, v) back into a tuple. s x[. :-3 \\* d_v \\* n_conf ]. view(x.shape [0], n_conf , d_s) v x[ -3 \\* d_v \\* n_conf :]. view(x.shape [0], n_conf , d_v , 3) return s v ", "page_idx": 21}, {"type": "text", "text": "Merges a tuple (s, v) into a single \u2018torch.Tensor \u2018   \nwhere the vector channels are flattened and   \nappended to the scalar channels.   \n\uff0c\uff0c\uff0c   \n$\\begin{array}{r l}&{\\#\\mathrm{~\\sigma_8~:~\\Delta~[n_-nodes~,~\\pi_{n}~c o n f~\\sigma,~\\pi~d]~\\mathcal~\\mathcal~{~-~}~>~\\Delta~[n_-n o d e s~,~\\pi_{n}~\\subset~n_{-}~c o n f~\\sigma~*~\\Delta~d_{-}~s]~}}\\\\ &{\\bullet\\;=\\;\\mathrm{~\\sigma_8~,~v~i~e~w~(~\\mathbf8~.~s~hape~[0]~,~\\sigma~s~.~s~hape~[1]~\\sigma~*~\\Delta~\\bullet~\\bullet~\\bullet~shape~[2]~)~}}\\\\ &{\\#\\mathrm{~\\sigma~\\in~[\\pi_{-n}~n o~d e s~,~\\pi_{n}~\\subset~n_{-}~c o n f~\\sigma,~\\pi~d~,~\\sigma~3]~\\;~-~\\Delta~[n_-n o d e s~,~\\pi_{n}~\\subset~o n~f~\\sigma~*~\\Delta~d_{-}~v\\neq3]~}}\\\\ &{\\forall\\mathrm{~\\sigma~=~\\mathbb{v}~.~\\forall~i~e~w~(~\\mathbf{v}~.~s~h a p e~[0]~,~\\pi~\\cdot~s h a p e~[1]~\\sigma~*~\\Delta~v~.~s h a p e~[2]~*~3~)~}}\\\\ &{\\;r\\in\\mathrm{turn~\\pi~to~r\\Delta~cat~(~[\\delta_{s}~,~\\delta~v]~,~\\sigma~-1~)~}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Listing 1: PyG-style pseudocode for a multi-state GVP-GNN layer. We update node features for each conformer independently while maintaining permutation equivariance of the updated feature tensors along both the first (no. of nodes) and second (no. of conformations) axes. ", "page_idx": 21}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/958a737636839d3582f4675840e394f2ad452fb15c44a6bd42f3aaa749bd185b.jpg", "img_caption": ["(a) Sequence length. The dataset is long-tailed in terms of RNA sequence length, with many short sequences including aptamers, riboswitches, ribozymes, and tRNAs (fewer than 200 nucleotides). The dataset also includes several longer ribosomal RNAs (thousands of nucleotides). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/8524350a1c6d55ba2e87c176eef1a04479f36724728f8e0257f4fe9e434a3b9d.jpg", "img_caption": ["(b) Number of structures per sequence. The dataset covers a wide range of RNA conformation ensembles, with on average 3 structures per sequence. There are multiple structures available for 1,547 sequences. The remaining 2,676 sequences have one corresponding structure. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/ce5df9f5dc5e6c66cd68c5ceba792e1be01d6e58ed907053f1778156a5a41c40.jpg", "img_caption": ["(c) Average pairwise RMSD per sequence. For (d) Bivariate distribution for sequence length vs. 1,547 sequences with multiple structures, there is avg. RMSD. The joint plot illustrates how structural significant structural diversity among conformations. diversity (measured by avg. pairwise RMSD) varies On average, the pairwise $\\mathrm{C4}^{\\circ}$ RMSD among the set across sequence lengths. We notice similar structural of structures for a sequence is greater than $\\bar{1}\\mathring{\\mathrm{A}}$ . variations regardless of sequence length. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Fm4FkfGTLu/tmp/a018708848e610e909ef8f2fd878226736acc6868773de90a8477baa2712a8db.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 15: RNASolo data statistics. We plot histograms to visualise the diversity of RNAs available in terms of (a) sequence length, (b) number of structures available per sequence, as well as (c) structural variation among conformations for those RNA that have multiple structures. The bivariate distribution plot (d) for sequence length vs. average pairwise RMSD illustrates structural diversity regardless of sequence lengths. ", "page_idx": 22}, {"type": "text", "text": "631 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "632 1. Claims   \n633 Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s   \n634 contributions and scope?   \n635 Answer: [Yes]   \n636 Justification: Yes, the main claims made in the abstract are that the proposed 3D RNA design   \n637 method, gRNAde, improves sequence recovery over Rosetta, is capable of multi-state design,   \n638 and can be useful for zero-shot ranking of RNA ftiness landscapes. All the claims are supported   \n639 by empirical results and expanded upon in detail in the rest of the paper.   \n640 2. Limitations   \n641 Question: Does the paper discuss the limitations of the work performed by the authors?   \n642 Answer: [Yes]   \n643 Justification: Yes, we have discussed limitations at several places, including the conclusion   \n644 section, an FAQ section, as well as a detailed ablation study in the appendix.   \n645 3. Theory Assumptions and Proofs   \n646 Question: For each theoretical result, does the paper provide the full set of assumptions and a   \n647 complete (and correct) proof?   \n648 Answer: [NA]   \n649 Justification: The paper does not include theoretical results.   \n650 4. Experimental Result Reproducibility   \n651 Question: Does the paper fully disclose all the information needed to reproduce the main   \n652 experimental results of the paper to the extent that it affects the main claims and/or conclusions   \n653 of the paper (regardless of whether the code and data are provided or not)?   \n654 Answer: [Yes]   \n655 Justification: All code, data, and pretrained models are publicly available, along with detailed   \n656 instructions on installation, reproducing results, and real-world usage.   \n657 5. Open access to data and code   \n658 Question: Does the paper provide open access to the data and code, with sufficient instructions   \n659 to faithfully reproduce the main experimental results, as described in supplemental material?   \n660 Answer: [Yes]   \n661 Justification: All data and code is publicly available, including the exact commands and environ  \n662 ments needed to access the raw data, process the data, and reproduce the results.   \n663 6. Experimental Setting/Details   \n664 Question: Does the paper specify all the training and test details (e.g., data splits, hyperparame  \n665 ters, how they were chosen, type of optimizer, etc.) necessary to understand the results?   \n666 Answer: [Yes]   \n667 Justification: The experimental setup is presented in detail as a dedicated section in the main   \n668 text, as well as extensively documented in the code.   \n669 7. Experiment Statistical Significance   \n670 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n671 information about the statistical significance of the experiments?   \n672 Answer: [Yes]   \n673 Justification: All results are accompanied by error bars and confidence intervals, along with the   \n674 factors of variability that the error bars capture.   \n675 8. Experiments Compute Resources   \n676 Question: For each experiment, does the paper provide sufficient information on the computer   \n677 resources (type of compute workers, memory, time of execution) needed to reproduce the   \n678 experiments?   \n679 Answer: [Yes]   \n680 Justification: Yes, the paper and code provide information on the computer resources used for   \n681 this work. However, we currently do not have estimes on the total compute used.   \n682 9. Code Of Ethics   \n683 Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS   \n684 Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n685 Answer: [Yes]   \n686 Justification: The research conforms with the NeurIPS Code of Ethics.   \n687 10. Broader Impacts   \n688 Question: Does the paper discuss both potential positive societal impacts and negative societal   \n689 impacts of the work performed?   \n690 Answer: [Yes]   \n691 Justification: We hope that our tools contributes to the development of RNA-based therapeutics   \n692 towards improving health outcomes. We have attempted to make gRNAde as convinient to use   \n693 as possible towards this end. We do not foresee any immediate negative societal impact of our   \n694 work.   \n695 11. Safeguards   \n696 Question: Does the paper describe safeguards that have been put in place for responsible release   \n697 of data or models that have a high risk for misuse (e.g., pretrained language models, image   \n698 generators, or scraped datasets)?   \n699 Answer: [NA]   \n700 Justification: The paper poses no such risks.   \n701 12. Licenses for existing assets   \n702 Question: Are the creators or original owners of assets (e.g., code, data, models), used in the   \n703 paper, properly credited and are the license and terms of use explicitly mentioned and properly   \n704 respected?   \n705 Answer: [Yes]   \n706 Justification: Original owners of any assets used as part of our stidy are appropriately credited.   \n707 13. New Assets   \n708 Question: Are new assets introduced in the paper well documented and is the documentation   \n709 provided alongside the assets?   \n710 Answer: [Yes]   \n711 Justification: Our datasets, code, and model checkpoints are publicly available under the   \n712 permissive MIT License.   \n713 14. Crowdsourcing and Research with Human Subjects   \n714 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n715 include the full text of instructions given to participants and screenshots, if applicable, as well as   \n716 details about compensation (if any)?   \n717 Answer: [NA]   \n718 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n719 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n720 Subjects   \n721 Question: Does the paper describe potential risks incurred by study participants, whether such   \n722 risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or   \n723 an equivalent approval/review based on the requirements of your country or institution) were   \n724 obtained?   \n725 Answer: [NA]   \n726 Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}]