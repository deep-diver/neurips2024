[{"type": "text", "text": "Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anchit Jain\u2217 Rozhin Nobahari Aristide Baratin University of Cambridge MILA - Quebec AI institute Samsung - SAIT AI Lab Montreal aj625@cantab.ac.uk Universit\u00e9 de Montr\u00e9al ", "page_idx": 0}, {"type": "text", "text": "Stefano Sarao Mannelli Gatsby & SWC - University College London s.saraomannelli@ucl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations. Current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics. To address this gap, this paper explores the evolution of bias in a teacher-student setup modeling different data sub-populations with a Gaussian-mixture model. We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setting, which we prove to be exact in high dimension. Notably, our analysis reveals how different properties of sub-populations influence bias at different timescales, showing a shifting preference of the classifier during training. Applying our findings to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias. We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real datasets, including CIFAR10, MNIST, and CelebA. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Over the past decade, the problem of assessing the fairness of classifiers has garnered significant attention, revealing that machine learning (ML) systems not only reproduce existing biases in the data but also tend to amplify them [21, 40, 11]. Given the complexity of the ML pipeline, isolating and characterising the key drivers of this amplification is challenging. Recent studies have begun to disentangle the contributions from architectural design choices, including overparameterisation [37], model complexity, activation functions [5, 12], learning protocols [43, 13], post-processing practices such as pruning [19], and intrinsic aspects of the data like its geometrical properties [38]. ", "page_idx": 0}, {"type": "text", "text": "Theoretical results in this area (e.g., [37, 38]) are mostly based on asymptotic analysis, leaving the transient learning regime poorly understood. Due to limitations on computational resources, a trained ML system may operate far from the asymptotic regime and hence existing results may not always apply. Insights from class imbalance literature [43, 12] indicate that classifiers converge faster for classes with more data, but how this applies to fairness, where datasets might be balanced by label but imbalanced by demographics, remains unclear. ", "page_idx": 0}, {"type": "text", "text": "Our analysis addresses this gap by providing a precise characterisation of the transient dynamics of online stochastic gradient descent (SGD) in a high dimensional prototypical model of linear classification. We use the teacher-mixture (TM) framework [38], where different data sub-populations are modeled with a mixture of Gaussians, each having its own linear rule (teacher) for determining the labels. Adjusting the parameters of the data distribution in our framework connects models of fairness and spurious correlations, providing a unifying framework and a general set of results applicable to both domains. Remarkably, our study reveals a rich behaviour divided into three learning phases, where different features of data bias the classifier and causing significant deviations from asymptotic predictions. We reproduce our theoretical findings through numerical experiments in more complex settings, demonstrating validity beyond the simplicity of our model. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our key contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 High-dimensional analysis: We demonstrate that in the high-dimensional limit, relevant properties of the classifier, such as the generalisation error, can be expressed using a few sufficient statistics. We prove that their evolution converges to a set of ordinary differential equations (ODEs) that can be solved explicitly in our setting.   \n\u2022 Bias evolution characterisation: Using our solution, we characterise the evolution of bias throughout training, showing a three-phase learning process where bias exhibits nonmonotonic behaviour. Specifically: 1. Initial phase: The classifier is initially influenced by sub-populations with strong class imbalance. 2. Intermediate phase: The dynamics shifts towards the saliency, or norm, of the samples in a sub-population. 3. Final phase: Sub-population imbalance, or relative representation, becomes the dominant factor.   \n\u2022 Empirical validation: We validate and extend our theoretical results through numerical experiments in both synthetic and real datasets, including CIFAR10, MNIST, and CelebA. ", "page_idx": 1}, {"type": "text", "text": "Altogether, our study reveals a complex time-dependence of learning with structured data that previous theoretical studies have failed to capture. This characterisation is crucial for developing effective bias mitigation strategies, especially under limited computational resources. ", "page_idx": 1}, {"type": "text", "text": "1.1 Further related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Class imbalance and fairness. A key element in our study is the presence of heterogeneous data distributions within the dataset. In the context of fairness, these distributions model different groups in a population. Sampling unbalance is particularly critical, as minority groups are often misclassified [9, 20]. However, theoretical studies on group imbalance have been limited to asymptotic analyses [38], which may not apply in practical settings. Related questions have been explored in the label imbalance literature [22], where it has long been known [1, 17] that underrepresented classes have slower convergence rate and may even experience increased errors early in training. Our work shows that pre-asymptotic analysis can reveal complex transient dynamics, which is practically relevant when learning slows down or training to convergence is not possible. Similar to our analysis, [12] has shown that supposedly neutral choices, like activation functions or pooling operations, can generate strong biases. In contrast to prior work, our focus on data properties identifies several timescales associated to different data features relevant to bias generation. ", "page_idx": 1}, {"type": "text", "text": "Simplicity bias. Several studies [31, 16, 41, 10, 32] have highlighted a bias of deep neural networks (DNNs) towards simple solutions, suggesting this bias is a key to their generalisation performance. Simplicity bias also influences learning dynamics: [4, 32, 28, 30, 33] have showed that DNNs learn progressively more complex functions during training, with a notion of complexity often defined implicitly by other DNNs or observations like the time to memorisation. Our results connect with simplicity bias by identifying interpretable properties of the data that make samples appear \u201csimple\u201d to a shallow network. Interestingly, our findings reveal that different phases of learning experience simplicity in different ways, leading to forgetting of previously learned features. ", "page_idx": 1}, {"type": "text", "text": "Spurious correlations. Simplicity bias can also lead to shortcomings [39] by excessively relying of spurious features in the data, possibly hurting generalisation, especially in out-of-distribution contexts [14]. Theoretical works [29, 37, 18] have identified statistical properties that cause a classifier to favour spurious features over potentially more complex but more predictive features. ", "page_idx": 1}, {"type": "text", "text": "Various methods have been proposed to address this problem using explicit partitioning of the data [2, 36]; some approaches implicitly infer subgroups with various degrees of correlation as spurious features. Notably, [26, 42] rely on early stages of learning to detect bias and adjust sample importance accordingly. Our study provides a unifying view of learning in fairness and spurious correlation problems, highlighting the presence of ephemeral biases characterised by multiple timescales during training. This adds complexity to the understanding of learning dynamics and points out potential confounding effects in existing mitigation methods. ", "page_idx": 2}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/def822e853ffb051067e25f4500f93b2ca4e97d0e7ed589a567c1ea4764db13e.jpg", "img_caption": ["(a) ODEs vs simulations (b) Robustness model (c) Centered fairness (d) General fairness "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Teacher-Mixture in fairness and robustness. Panel (a) shows the generalisation errors\u2014 for the subpopulations $^+$ (blue) and \u2212(red)\u2014obtained through simulation (crosses) and predicted by the theory (solid lines) for a network with linear activation. The inset shows the same comparison for the order parameters: $R_{+}$ (blue), $R_{-}$ (red), $M$ (green), and $Q$ (orange). Panels $(b{-}d)$ exemplify the different scenarios achievable in the TM model investigated in Sec. 4. Panel $(b)$ represent a model for robustness where a spurious feature\u2014given by the shift vector\u2014can mislead the classifier, see Sec. 4.1. Panels $(c,d)$ are instead discussed in Sec. 4.2 and represent two models of fairness. First, Panel $(b)$ has no shift, $v=0$ , allowing us to remove the confounding effects. Finally, Panel (d) shows the general fairness problem. ", "page_idx": 2}, {"type": "text", "text": "2 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Data distribution. We consider a standard supervised learning setup where the training data consists of pairs of a feature vector $\\pmb{x}\\in\\mathbb{R}^{d}$ and a binary label $y=\\pm1$ . To model subgroups within the data [35], we assume that the feature vectors are structured as clusters $c_{1},\\ldots,c_{m}$ , respectively centered on some fixed attribute vectors $\\pmb{v}_{1},\\cdot\\cdot\\cdot\\mathrm{\\boldmath~,~}\\pmb{v}_{m}\\in\\mathbb{R}^{d}$ . Specifically, $\\pmb{x}$ is sampled from a mixture of $m$ isotropic Gaussians: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{x}\\sim\\sum_{j=1}^{m}\\rho_{j}\\mathcal{N}(\\pmb{v}_{j}/\\sqrt{d},\\Delta_{j}\\mathbb{I}_{d\\times d}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with mixing probabilities $\\rho_{1},\\cdots,\\rho_{m}$ and scalar varian\u221aces $\\Delta_{1},\\cdot\\cdot\\cdot,\\Delta_{m}$ . Assuming the entries of $\\pmb{v}_{j}$ are of order 1 as $d$ gets large, the scaling factor $1/\\sqrt{d}$ ensures that the Euclidean norm of the renormalised vector is of order 1. This prevents the problem from becoming either trivial or overly challenging in the high-dimensional limit [25, 24]. We adopt a teacher-mixture (TM) scenario [38] where each cluster has its own teacher rule: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{x}\\in c_{j}\\quad\\Longrightarrow\\quad y=\\mathrm{sign}(\\overline{{\\pmb{w}}}_{j}^{\\top}\\pmb{x}/\\sqrt{d}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This rule is characterised by the teacher vectors $\\overline{{\\pmb{w}}}_{j}\\in\\mathbb{R}^{d}$ , ensuring linear separability within each cluster. Fig. 1b-d illustrate the data distribution for two clusters with opposite mean vectors $\\pm v$ , which will be the primary case study for our analysis. ", "page_idx": 2}, {"type": "text", "text": "Model. In this study we analyse a linear model applied to the above data distribution. We aim to learn a vector parameter $\\pmb{w}$ , referred to as the \u2018student\u2019, such that predictions are given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{y}(\\pmb{x})={\\pmb w}^{\\top}{\\pmb x}/\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The training process involves applying online SGD on the squared loss $\\hat{\\epsilon}=(y-\\hat{y})^{2}$ . At the $k$ -th iteration, a feature vector $\\pmb{x}^{k}$ is sampled from (1), the ground truth label $y^{k}$ and current model ", "page_idx": 2}, {"type": "text", "text": "prediction $\\hat{y}^{k}$ are respectively given by (2) and (3), and the parameter is updated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta\\pmb{w}^{k}:=\\pmb{w}^{k+1}-\\pmb{w}^{k}=-\\frac{\\eta}{2}\\nabla\\hat{\\epsilon}^{k}(\\pmb{w}^{k})=\\frac{\\eta}{\\sqrt{d}}(y^{k}-\\hat{y}^{k})\\,\\pmb{x}^{k}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\eta/2>0$ denotes the learning rate. It is important to note that in this online setting the number of time steps is equivalent to the number of training examples. In our analysis, the model is evaluated by its generalisation error, or population loss, $\\boldsymbol{\\epsilon}:=\\mathbb{E}[\\boldsymbol{\\hat{\\epsilon}}]$ . ", "page_idx": 3}, {"type": "text", "text": "3 SGD analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We study the evolution of the generalisation error during training with SGD with constant learning rate in the high dimensional setting (i.e. large $d_{.}$ ). Following a classical approach [34, 8], we streamline the problem by focusing on a small set of summary statistics, referred to as \u2018order parameters\u2019, which fully characterises the dynamics. As the dimension increases, it can be shown by concentration arguments that the evolution of these order parameters converges to the deterministic solution of a system of ODEs [15, 6, 3]. Notably, in our setting, we achieve an analytical solution of this ODE system. We sketch our main results below, referring to the Appendix for derivations and proofs. ", "page_idx": 3}, {"type": "text", "text": "3.1 Order parameters ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the setup described in Section 2, consider the following $2m+1$ variables: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{j}={\\frac{1}{d}}{\\pmb w}^{\\top}{\\pmb w}_{j},\\quad M_{j}={\\frac{1}{d}}{\\pmb w}^{\\top}{\\pmb v}_{j},\\quad Q={\\frac{1}{d}}\\|{\\pmb w}\\|^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for $1\\leq j\\leq m$ . These variables correspond to key statistics of the student, namely its alignment to the cluster teachers, its alignment to the cluster centers, and its magnitude, respectively. ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.1. The generalisation error can be written as an average $\\begin{array}{r}{\\epsilon=\\sum_{j=1}^{m}\\rho_{j}\\epsilon_{j}}\\end{array}$ over the clusters, where $\\epsilon_{j}$ is a degree 2 polynomial in $R_{j},M_{j}$ and $Q$ taking the form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\epsilon_{j}=1-2\\alpha_{j}M_{j}+M_{j}^{2}-\\beta_{j}R_{j}+Q\\Delta_{j}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha_{j},\\beta_{j}$ are constants independent of the parameter $\\pmb{w}$ . ", "page_idx": 3}, {"type": "text", "text": "We present the derivation of this result and the explicit form of the constants $\\alpha_{j},\\beta_{j}$ in Appendix B.1. Our problem thus reduces to characterising the evolution of order parameters (5). Using the gradient update of the parameter in Eq. 4 and the notation $\\delta^{k}:=y^{k}-\\hat{y}^{k}$ , we can write update equations for the order parameters as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta M_{j}^{k}=\\frac{\\eta}{d}\\delta^{k}\\frac{\\pmb{v}_{j}^{\\top}\\pmb{x}^{k}}{\\sqrt{d}},\\quad\\Delta R_{j}^{k}=\\frac{\\eta}{d}\\delta^{k}\\frac{\\pmb{\\overline{{w}}}_{j}^{\\top}\\pmb{x}^{k}}{\\sqrt{d}},\\quad\\Delta Q^{k}=\\frac{2\\eta}{d}\\delta^{k}\\frac{\\pmb{w}_{j}^{\\top}\\pmb{x}^{k}}{\\sqrt{d}}+\\frac{\\eta^{2}}{d^{2}}(\\delta^{k})^{2}\\|\\pmb{x}^{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 High dimensional dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We build upon classic results [34, 8], recently put on rigorous grounds [15, 6, 3], leveraging the self-averaging property of the order parameters in the high dimensional limit $d\\to\\infty$ . As a result, as the dimension gets large, the discrete, stochastic evolution (7) of the order parameters can be effectively described in terms of the deterministic solution of the average continuous-time dynamics. Let $S:=(S_{i})_{1\\leq i\\leq2m+1}$ denote the collection of order parameters. The following lemma shows that the average of the updates (7) over the sample $\\pmb{x}^{k}$ can be expressed solely in terms of $S^{k}$ . ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.2. $\\begin{array}{r}{\\mathbb{E}[\\Delta S_{i}^{k}]=\\frac{1}{d}f_{i}(S^{k}).}\\end{array}$ for some functions $(f_{i}(S))_{1\\leq i\\leq2m+1}$ in $O(l)$ as $d\\to\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "The theorem below states that as $d$ gets large, the stochastic evolution $S^{k}$ of the order parameter gets uniformly close, with high probability, to the average continuous-time dynamics described by the ODE system: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d\\bar{S}_{i}(t)}{d t}=f_{i}(\\bar{S}(t)),\\qquad1\\leq i\\leq2m+1,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the continuous time is given by the example number divided by the input dimension, $t=k/d$ . Formally, ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.3. Fix a time horizon $T>0$ . For $1\\leq i\\leq2m+1,$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{0\\leq k\\leq d T}|S_{i}^{k}-\\bar{S}_{i}(k/d)|\\stackrel{P}{\\longrightarrow}0\\quad a s\\ d\\rightarrow\\infty.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\xrightarrow{P}$ denotes convergence in probability. A proof is provided in Appendix B. We provide the explicit expression of the functions $f_{i}$ in the ODEs (8) in Appendix C, focusing on $m=2$ clusters for clarity. ", "page_idx": 4}, {"type": "text", "text": "3.3 Solving the ODEs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here we present the explicit solution of the ODEs (8) in the case of two clusters ( $(m\\,=\\,2)$ ) with opposite mean vectors $\\pm\\pmb{v}$ , as in [38]. Henceforth, we refer to $\\pmb{v}$ as the shift vector and to the two clusters as the \u2018positive\u2019 and \u2018negative\u2019 sub-populations, with mixing probabilities $\\rho$ and $(1-\\rho)$ , variances $\\Delta_{\\pm}$ and teacher vectors $\\overline{{\\pmb{w}}}_{\\pm}$ , respectively. The order parameters introduced in Eq. 5 are specifically denoted as $M={\\pmb w}^{\\top}{\\pmb v}/d$ , $\\bar{R}_{+}\\doteq{\\pmb w}^{\\top}\\overline{{\\pmb w}}_{+}\\bar{\\slash}d$ , and $R_{-}\\stackrel{\\cdot}{=}{\\pmb w}^{\\top}{\\pmb\\overline{w}}_{-}/d$ in this setting. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4. In the above setting, solutions to the order parameter evolution take the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M(t)=M_{0}e^{-\\eta(v+\\Delta^{m i x})t}+M^{\\infty}(1-e^{-\\eta(v+\\Delta^{m i x})t}),}\\\\ &{R\\pm(t)=R_{\\pm}^{0}e^{-\\eta\\Delta^{m i x}t}+R_{\\pm}^{\\infty}(1-e^{-\\eta\\Delta^{m i x}t})+k_{1}^{\\pm}(e^{-\\eta\\Delta^{m i x}t}-e^{-\\eta(v+\\Delta^{m i x})t}),}\\\\ &{\\quad Q(t)=Q_{0}e^{-\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})t}+Q^{\\infty}(1-e^{-\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})t})}\\\\ &{\\quad\\quad+k_{2}(e^{-t(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})\\eta}-e^{-t\\Delta^{m i x}\\eta})+k_{3}(e^{-t(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})\\eta}-e^{-t(v+\\Delta^{m i x})\\eta})}\\\\ &{\\quad\\quad+k_{4}(e^{-t(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})\\eta}-e^{-t(2v+2\\Delta^{m i x})\\eta}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The remaining constants are less significant and are reported in Appendix D.1 and discussed further in Appendix E. This solution allows us to describe important observables such as the generalisation error (via Lemma 3.1) at any timestep. Fig. 1a plots the theoretical closed-form solutions along with values obtained through simulation when we set $d=1000$ . Note the remarkable agreement between the analytical ODE solution and simulations of the online SGD dynamics in this high dimensional data limit. ", "page_idx": 4}, {"type": "text", "text": "4 Insights ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we delve deeper into the solution derived in Theorem 3.4. By examining the exponents in Eqs. 10- 12, we can identify the relevant training timescales. Notably, $M$ follows a straightforward behaviour dominated by a single timescale, whereas $R_{\\pm}$ and $Q$ exhibit multiple timescales, leading to significant implications for the emergence and evolution of bias during training. ", "page_idx": 4}, {"type": "text", "text": "In the following sections, we analyse increasingly complex scenarios to understand how bias develops and evolves. Parameters specifying these different scenarios are the shift norm $v=\\dot{||\\pmb{v}||}^{\\frac{2}{2}}/\\dot{d}$ and relative representation $\\rho$ , the subpopulation variances $\\Delta_{\\pm}$ , and the teacher overlap $T_{\\pm}=\\bar{\\pmb{w}}_{+}^{\\top}\\overline{{\\pmb{w}}}_{-}/d$ . For simplicity we fix the teacher norm $\\|\\pmb{w}_{\\pm}\\|_{2}=\\sqrt{d}$ , so that $T_{\\pm}$ is the cosine similarity between the two teachers. ", "page_idx": 4}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/f20e7151de24cd91eda54f126a91f10ac2c42ce6420ad7707112e069100b526c.jpg", "img_caption": ["Figure 2: Spurious correlations transient alignment. Time-evolution of loss (purple), student-teacher (red) and student-shift (green) cosine similarities. The initial phase (green background) of learning aligns classifier and shift vector before aligning with the teacher (red background), Sec. 4.1. Parameters: $v=$ $16,\\bar{\\rho^{\\circ}}=\\,0.5,\\Delta_{-}\\,=\\,\\Delta_{+}\\,=\\,0.1,T_{\\pm}\\,=\\,$ $1,\\eta=0.5$ . For these parameters, spurious features allow the correct classification of $90\\%$ of the samples. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.1 Spurious correlations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The emergence of spurious correlations during training exemplifies a type of bias where a classifier favours a spurious feature over a core one. To isolate the impact of spurious correlation in our model while ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/f1ccbc5360bc579b3e0a3117e5e17fcc60318dd4bc03d02309f563a7372a3cec.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: The crossing phenomenon. Panel (a) (left side) shows the loss curves of sub-population \u2212 (in red) and sub-population $^+$ in blue along with the overall loss (in purple). We observe a crossing cause by a higher variance but lower representation in sub-population \u2212. The background colours represent the different phases of bias that are characterised by the evolution of the order parameters shown in Panel (a) (right side). Panel $(b)$ shows the presence of the crossing phenomenon in a large portion of the parameter space using a phase diagram. Blue indicates an asymptotic preference for sub-population $^+$ and red the opposite. Dark colours indicates regions where bias is consistent across training, while regions in light colours undergo a crossing phenomenon. White indicates that learning rate was too high and training diverged. Parameters: $v=0,\\Delta_{+}=1,T_{\\pm}=0.9,\\eta=0.1$ . ", "page_idx": 5}, {"type": "text", "text": "avoiding confounding effects, we consider perfectly overlapping teachers $(\\overline{{{\\pmb w}}}_{+}\\,=\\,\\overline{{{\\pmb w}}}_{-})$ ) and subpopulations with equal variance and representation $(\\rho\\,=\\,0.5,\\Delta_{+}\\,=\\,\\Delta_{-})$ ). With non-perfectly overlapping clusters $v\\neq0$ , we introduce a spurious correlation by adding a small cosine similarity between the shift vector and the teacher, creating a label imbalance\u2014an imbalance between the proportion of positive and negative labels\u2014within each sub-population. The setting is illustrated in Fig. 1b. ", "page_idx": 5}, {"type": "text", "text": "From Eqs. 10-12, two relevant timescales for the problem are observed: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tau_{M}=\\frac{1}{\\eta(v+\\Delta^{m i x})},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tau_{R}=1/\\eta\\Delta^{m i x}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The shortest timescale, $\\tau_{M}$ , associated with $M$ , indicates that the student first aligns with the spurious feature. By aligning with the shift vector, the student can predict most examples correctly, but not all. The effect of spurious correlations is transient; at $t\\sim\\tau_{R}$ , the student starts disaligning from the spurious feature and aligns with the teacher vector, eventually achieving nearly perfect alignment. This is illustrated in Fig. 2, where the student initially picks up on the spurious correlation (green) and achieves almost perfect alignment with the shift vector during intermediate times before aligning nearly perfectly with the teacher (red). ", "page_idx": 5}, {"type": "text", "text": "4.2 Fairness ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we identify the properties of sub-populations that determine the bias during learning and show how bias evolves in three phases. To quantify bias, we use the overall accuracy equality metric [7], which measures the discrepancy in accuracy across groups. Intuitively, we aim for equal loss on both groups, considering any deviation from this condition as bias. ", "page_idx": 5}, {"type": "text", "text": "4.2.1 Zero shift ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first consider a simplified case where we assume that both clusters are centered at the origin $v=0$ as shown in Fig. 1c. We will later reintroduce the shift and analyse the transient dynamics it introduces as per the discussion in section 4.1. The zero shift case represents an extreme situation where not only is the classification not solvable if $\\overline{{{\\pmb w}}}_{+}\\not=\\overline{{{\\pmb w}}}_{-}$ , but it is also difficult to identify which cluster generated a given data point since the shift provides no information. This setting is particularly suited to analysing the effects of \u2018group level\u2019 features, such as group variance and relative representation, on the preference of the classifier. ", "page_idx": 5}, {"type": "text", "text": "In this simplified setting, $M(t)$ is always zero and the constants $k_{1}^{\\pm},k_{3},k_{4}$ presented in equations 11 and 12 are zero. Thus, the dynamics only involve two relevant timescales given by $\\tau_{R}$ in Eq. 14 and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tau_{Q}=1/(\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Fig. 3a illustrates the changing preference of the classifier. Specifically, we observe that the variance of the sub-population is particularly relevant initially and the sub-population with higher variance (red) is learnt faster, i.e. its generalisation error drops faster. However, asymptotically we observe that the relative representation becomes more important wherein the student aligns itself with the teacher that has a higher product of representation and standard deviation (blue), i.e. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho\\sqrt{\\Delta_{+}}\\gtrsim(1-\\rho)\\sqrt{\\Delta_{-}}\\iff R_{+}^{\\infty}\\gtrsim R_{-}^{\\infty}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thus, the network can advantage the cluster with higher variance initially but asymptotically advantage the other cluster if its representation is high enough. This leads to the interesting behaviour shown in Fig. 3 wherein we observe a \u2018crossing\u2019 of the losses on the two sub-populations. A more detailed analysis of the \u2018crossing\u2019 is presented in Appendix E.2. ", "page_idx": 6}, {"type": "text", "text": "Initial dynamics. Starting from small initialisation, the initial rate of change of the generalisation error for sub-population $^+$ is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{d\\epsilon_{g+}}{d t}\\bigg|_{t=0}=-\\eta^{2}\\Delta^{m i x}\\Delta_{+}\\left(\\sqrt{\\frac{2}{\\pi\\Delta_{+}}}\\frac{R_{+}^{\\infty}}{\\eta}-1\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and analogously for $-$ . The learning rate $\\eta$ must be chosen to be small enough such that the generalisation errors decrease and hence the first term in the brackets must dominate over the 1. Since $\\left.\\bar{R}_{+}^{\\infty}/R_{-}^{\\infty}\\in[T_{\\pm};1/T_{\\pm}]\\right.$ (for $T_{\\pm}>0$ ), the ratio between generalisation error rates is bounded by ", "page_idx": 6}, {"type": "equation", "text": "$$\nT_{\\pm}\\sqrt{\\frac{\\Delta_{+}}{\\Delta_{-}}}\\leq\\frac{d\\epsilon_{g+}/d t\\big|_{t=0}}{d\\epsilon_{g-}/d t\\big|_{t=0}}\\leq\\frac{1}{T_{\\pm}}\\sqrt{\\frac{\\Delta_{+}}{\\Delta_{-}}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "When the teachers are only slightly misaligned\u2014 $-T_{\\pm}\\lessapprox1$ \u2014the bound is tight and we can see that it is the ratio of the square roots of the variances that determines which cluster is learnt faster initially. As precisely detailed below, the initial bias can substantially differ from the asymptotic bias of the classifier. Indeed, Fig. 3b shows in a phase diagram the existence of \u2018bias crossing\u2019 across a wide range of variances and representations. The transition between the phases that represent a initial preference for the positive sub-population (light red and dark blue) and the phases that represent an initial preference for negative sub-population (dark red and light blue) is approximately given by the line ${\\Delta_{-}}={\\Delta_{+}}=1$ , independent of the representation as predicted by Eq. 18. The portion of the dark blue phase just above the white divergent phase marks a \u2018quasi-divergent\u2019 region wherein the generalisation error on the negative sub-population rises even at $t=0$ because the learning rate is too large for such high variances. It hence marks a region of impractical behaviour that is only observed with poorly optimised learning rates. ", "page_idx": 6}, {"type": "text", "text": "Asymptotic preference. In the limit of small learning rates $\\eta\\rightarrow0$ , the student will asymptotically exhibit lower loss on whichever sub-population\u2019s teacher it has better alignment with. Thus, Eq. 16 provides a simple characterisation of asymptotic preference from representations and standard deviations in the small learning rate limit. However, the situation is more complex in the case of finite learning rate, which may disrupt learning in one or both clusters. Without indulging further into the discussion of asymptotic performance, which is not the main goal of the paper, we refer to Appendix E.3 for more in depth and analysis and additional phase diagrams. ", "page_idx": 6}, {"type": "text", "text": "4.2.2 General case ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now consider the general case shown in Fig. 1d, where the shift is non zero and all three timescales identified so far play a role. ", "page_idx": 6}, {"type": "text", "text": "As observed in Sec. 4.1, when the shift norm $v$ is large, the effect of spurious correlations becomes significant and the timescale associated with the spurious correlations is the fastest. In general, when $v\\neq0$ we observe an additional phase due to the effect of spurious correlation. In this new first phase, the student advantages the cluster with higher representation and lower variance since the salient information received from this cluster is more coherent and easier to access. ", "page_idx": 6}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/dd2477fa91934c51b8739a8ac80284b7b2ca699ad10c839f0d8d89a9f6cc85d6.jpg", "img_caption": ["Figure 4: Double crossing phenomenon. (Left panel) shows the loss for the two sub-populations (blue and red lines) and the global one (in purple). (Right panel) shows the value of the order parameters across time. The behaviour of the order parameters across time provides a precise characterisation and understanding of the different phases. Parameters: $v=100,\\rho=0.75,\\Delta_{+}=$ $0.1,\\Delta_{-}=0.5,\\eta=0.03,T_{\\pm}=0.\\bar{9},\\alpha_{+}=0.343,\\alpha_{-}^{-}=0.12$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "More precisely, in high dimensions the shift and the teachers are likely to exhibit a small cosine similarity leading to a class imbalance in the clusters and creating spurious correlation. The amount of label imbalance within a cluster is characterised by the value of $\\alpha$ , as detailed in Appendix A. For smaller variances, $\\alpha$ takes more extreme values leading to stronger spurious correlation of that cluster with the shift. If a cluster has more positive examples, we would observe a reduction in loss for that cluster if the student aligns with the mean of that cluster (and opposite to the mean if the cluster has mostly negative examples). When both clusters have different majority classes, the direction of spurious correlation for the two are same. However, when the majority classes are the same, we have competing directions for spurious correlation. The expression for $M_{\\infty}$ in Appendix D.1 Eq. D.42 shows that in this case the relative representation comes into play and the mean of the cluster with greater representation and class imbalance will be chosen by the teacher to align with. Fig. 4 shows such a scenario with three phase bias evolution: ", "page_idx": 7}, {"type": "text", "text": "\u2022 The green phase is driven by spurious correlation where the positive cluster is advantaged since it has greater representation and class imbalance.   \n\u2022 Then, the red phase is driven by greater variance where the negative cluster is learnt faster as discussed through Eq. 18.   \n\u2022 Finally, we observe the orange phase wherein the student starts aligning with the positive cluster as per the asymptotic rule in Eq. 16. ", "page_idx": 7}, {"type": "text", "text": "In summary, the student first advantages the sub-population with higher representation and lower variance. Next, it advantages the sub-population with higher variance. Asymptotically, it advantages the sub-population with higher representation and variance. Our analysis thus shows that bias is a dynamical quantity that can vary non-monotonically during training and cannot be characterised by simply the initial and asymptotic values. ", "page_idx": 7}, {"type": "text", "text": "5 Ablations using numerical simulations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Rotated MNIST ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We train a 2-layer neural network with 200 hidden units, ReLU activation, and sigmoidal readout activation, using online SGD on MSE loss in a MNIST classification task. Data are centered and the variance is set to 1 following standard pre-processing practices. We consider a variation of the MNIST dataset that mimics the TM model, allowing us to verify our theory when network and the data structure are mismatched. Digits 0 to 4 and 5 to 9 are grouped to form the two subpopulations. With probability $p_{+}$ and $p_{-}$ , digits of both subpopulations are rotated with a subpopulation-specific angle\u2014i.e. Fig. 5a uses angles of rotation $\\theta_{-}=45^{o}$ and $\\theta_{-}=-90^{o}$ . The goal of the classifier is to detect rotations. ", "page_idx": 7}, {"type": "text", "text": "The experimental framework gives a correspondence between parameters of the generative model and properties of a real dataset. We can control relative representation by subsampling, teacher similarity by playing with angle difference, label imbalance by changing the probability of rotation, and saliency by increasing and decreasing the norm of the subpopulation using multiplicative factors $\\Delta_{\\pm}$ . The only parameter that we cannot control is the shift $\\pmb{v}$ which is a property of the data. ", "page_idx": 7}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/59c27e13b985deae2df237fdbeb3bb91fecb19b37dc03a8266eff5b2f58df00a.jpg", "img_caption": ["(a) Crossing phenomenon (b) Double crossing phenomenon "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/b3c181934c6f9b70132393d944452b4597f9f16e4b6a4771eebd40ce167045f5.jpg", "img_caption": ["(c) Effect of $\\Delta_{-}$ on the initial learning phase "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Numerical simulations on MNIST. The figure shows the average (solid lines) and standard deviation (shaded area) of 100 simulations run in this framework. In particular the upper plots show the test loss and lower plots the test accuracy for subpop\u221aulation $^+$ (\u221ablue) and \u2212(red). Panel (a) an example of crossing phenomenon obtained by imposing $\\sqrt{\\Delta}_{+}=1$ , $\\sqrt{\\Delta}_{-}=0.2$ , and $\\rho=0.1$ . Panel $(b)$ shows the double crossing, obtained by introducing an additional timescale to the previous case by tuning label imbalance. Panel (c) explore the effect of changing $\\Delta_{-}$ while keeping a constant $\\Delta_{+}=1$ . ", "page_idx": 8}, {"type": "text", "text": "Therefore, in order to reproduce the zero-shift case of Sec. 4.2, we remove the label imbalance by setting the probability of rotation $p_{+}=p_{-}=0.5$ . By properly calibrating the saliency $\\Delta$ and the relative representation $\\rho$ , it is possible to bias the classifier towards one subpopulation at the beginning of training and the other in the end. This is shown in Fig. 5a where $\\rho=0.1$ and $\\Delta_{+}>\\Delta_{-}$ . The saliency difference favours subpopulation $^+$ initially while setting $\\rho$ small enough advantages subpopulation \u2212later in training. This is precisely what we observe in the plot. ", "page_idx": 8}, {"type": "text", "text": "In Fig. 5b, we extend the MNIST experiment by varying the average image brightness of subpopulation \u2212, which reflects the group variance in our theory. The results show that greater brightness leads to faster learning in the second phase and an increasing asymptotic preference, consistent with our predictions. ", "page_idx": 8}, {"type": "text", "text": "Finally, we consider the general fairness case. By creating label imbalance, i.e. setting $p_{+}=0.3$ and $p_{-}=0.7$ , we observe an additional phase of bias evolution, wherein the classifier prefers dense regions with consistent labels. This advantages subpopulation \u2212and indeed it is what we see in Fig. 5c. The result of the simulations matches the theory displaying a double crossing phenomenon. ", "page_idx": 8}, {"type": "text", "text": "5.2 CIFAR10 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We consider the same architecture and pre-processing described for MNIST on a CIFAR10 classification task. We select 8 classes and assign 4 of them to the positive group and 4 to the negative group. Inside each group, 2 classes are labelled as negative and 2 as positive. This simulation framework is similar to the one considered by [5] where the authors used sub-populations with only 2 classes each. ", "page_idx": 8}, {"type": "text", "text": "The average brightness of the samples in each cluster plays the same role as the parameter $\\Delta$ in the synthetic model. Our theory predicts that the classifier will advantage the group with highest average brightness, see Eq. 16. In order to achieve the same generalisation error on both subpopulations, the less bright group needs more samples (larger $\\rho$ ). This is shown in Fig. 6a, where the three panels correspond to different assignment of the classes: in the top panel classes are randomly assigned to ", "page_idx": 8}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/0d03b00cdb79eb64c20849ae10a950bca08766e111b1fb8c440b384d22630584.jpg", "img_caption": ["(a) Group brightness ", "(b) Crossing and $\\rho$ dependence "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 6: Numerical simulations on CIFAR10. The figure shows experiments of a $2\\mathrm{L}$ neural network on CIFAR10 where classes were grouped together to form the subpopulations. The plots show the average performance\u2014measure by loss or accuracy\u2014achieved over 100 simulations (for Panel (a)) and 10 simulations (for Panel $(b)$ , respectively) using the shaded area to quantify the standard deviation. Panel (a) shows the result at the end of training changing relative representation $\\rho$ , while Panel $(b)$ shows the training trajectories as $\\rho$ changes as indicated in the colour-bar, see text for more details. ", "page_idx": 9}, {"type": "text", "text": "the two groups; in the middle panel classes are randomly partitioned in two groups and the brighter one is assigned to group \u2212; finally the last panel assigns the brightest classes to group \u2212and least bright to group $^+$ . As predicted, we need increasingly high relative representation $\\rho$ to achieve a balance in losses at the end of training. ", "page_idx": 9}, {"type": "text", "text": "When labels are balanced, our theory predicts that the classifier is initially attracted by the larger $\\Delta$ and eventually\u2014if the relative representation of the group with smaller $\\Delta$ is large enough\u2014it switches and favours the other group. This effect is indeed verified in the CIFAR10 experiments. Starting from the partitioning in Fig. 6a (bottom) with $\\rho=0.8$ , the dynamics is initially attracted by group \u2013 before advantaging the other group, giving rise to a crossing that can be observed\u2013among other things\u2013in Fig. 6b. ", "page_idx": 9}, {"type": "text", "text": "Fig.6b highlights another prediction of our theory concerning the timescale when $\\rho$ becomes relevant. Our theory predicts that $\\rho$ should become relevant only in the final stages of the dynamics and indeed the curves are almost perfectly overlapping until for most of training. As already noticed, $\\rho$ sufficiently large gives rise to a crossing behaviour as indicated demonstrated in the synthetic model. ", "page_idx": 9}, {"type": "text", "text": "5.3 Additional numerical experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Appendix F, we provide additional experiments within the TM model and the CelebA dataset, exploring different architectures and losses. Even under these new conditions, we observe that bias presents different timescales and shows crossing behaviours. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper examined the dynamics of bias in a high dimensional synthetic framework, showing that it can be explicitly characterised to reveal transient behaviour. Our findings reveal that classifiers exhibit biases toward different data features during training, possibly alternating sub-population preference. Although our analysis is based on certain assumptions, numerical experiments that violate these assumptions still display the behaviour predicated by our theory. ", "page_idx": 9}, {"type": "text", "text": "While this paper centered on bias propagation in a controlled synthetic setting, the study of bias in ML systems is a significant issue with profound societal implications. We believe this line of research will have practical impacts in the medium term, aiding the design of mitigation strategies that account for transient dynamics. Future research will further explore this connection, proposing theory-based dynamical protocols for bias mitigation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Andrew Saxe and Simon Lacoste-Julien for numerous discussions and insightful comments.Anchit Jain is grateful to the Gatsby Computational Neuroscience Unit for the research internship opportunity. This work was supported by the Sainsbury Wellcome Centre Core Grant from Wellcome (219627/Z/19/Z) and the Gatsby Charitable Foundation (GAT3755). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rangachari Anand, Kishan G Mehrotra, Chilukuri K Mohan, and Sanjay Ranka. An improved algorithm for neural network classification of imbalanced training sets. IEEE transactions on neural networks, 4(6):962\u2013969, 1993.   \n[2] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[3] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro. From highdimensional & mean-field dynamics to dimensionless odes: A unifying approach to sgd in two-layers networks. In Gergely Neu and Lorenzo Rosasco, editors, Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 1199\u20131227. PMLR, 12\u201315 Jul 2023.   \n[4] Devansh Arpit, Stanis\u0142aw Jastrz\u02dbebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning, pages 233\u2013242. PMLR, 2017.   \n[5] Samuel James Bell and Levent Sagun. Simplicity bias leads to amplified performance disparities. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 355\u2013369, 2023.   \n[6] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. High-dimensional limit theorems for sgd: Effective dynamics and critical scaling. Advances in Neural Information Processing Systems, 35:25349\u201325362, 2022.   \n[7] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research, 50(1):3\u201344, 2021.   \n[8] Michael Biehl and Holm Schwarze. Learning by on-line gradient descent. Journal of Physics A: Mathematical and general, 28(3):643, 1995.   \n[9] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pages 77\u201391. PMLR, 2018.   \n[10] Farzan Farnia, Jesse Zhang, and David Tse. A spectral approach to generalization and optimization in neural networks. 2018.   \n[11] Yunhe Feng and Chirag Shah. Has ceo gender bias really been fixed? adversarial attacking and improving gender fairness in image search. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11882\u201311890, 2022.   \n[12] Emanuele Francazi, Aurelien Lucchi, and Marco Baity-Jesi. Initial guessing bias: How untrained networks favor some classes. arXiv preprint arXiv:2306.00809, 2023.   \n[13] Prakhar Ganesh, Hongyan Chang, Martin Strobel, and Reza Shokri. On the impact of machine learning randomness on group fairness. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 1789\u20131800, 2023.   \n[14] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673, 2020.   \n[15] Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborov\u00e1. Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup. Advances in neural information processing systems, 32, 2019.   \n[16] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. Advances in neural information processing systems, 31, 2018.   \n[17] Guo Haixiang, Li Yijing, Jennifer Shang, Gu Mingyun, Huang Yuanyue, and Gong Bing. Learning from class-imbalanced data: Review of methods and applications. Expert systems with applications, 73:220\u2013239, 2017.   \n[18] Katherine L Hermann, Hossein Mobahi, Thomas Fel, and Michael C Mozer. On the foundations of shortcut learning. In The Twelfth International Conference on Learning Representations, 2024.   \n[19] Eugenia Iofinova, Alexandra Peste, and Dan Alistarh. Bias in pruned vision models: In-depth analysis and countermeasures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24364\u201324373, 2023.   \n[20] Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 1548\u20131558, 2021.   \n[21] Matthew Kay, Cynthia Matuszek, and Sean A Munson. Unequal representation and gender stereotypes in image search results for occupations. In Proceedings of the 33rd annual acm conference on human factors in computing systems, pages 3819\u20133828, 2015.   \n[22] Bartosz Krawczyk. Learning from imbalanced data: open challenges and future directions. Progress in Artificial Intelligence, 5(4):221\u2013232, 2016.   \n[23] Yann LeCun, L\u00e9on Bottou, Genevieve B Orr, and Klaus-Robert M\u00fcller. Efficient backprop. In Neural networks: Tricks of the trade, pages 9\u201350. Springer, 2002.   \n[24] Marc Lelarge and L\u00e9o Miolane. Asymptotic bayes risk for gaussian mixture in a semi-supervised setting. In 2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), pages 639\u2013643. IEEE, 2019.   \n[25] Thibault Lesieur, Caterina De Bacco, Jess Banks, Florent Krzakala, Cris Moore, and Lenka Zdeborov\u00e1. Phase transitions and optimal algorithms in high-dimensional gaussian mixture clustering. In 2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 601\u2013608. IEEE, 2016.   \n[26] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In International Conference on Machine Learning, pages 6781\u20136792. PMLR, 2021.   \n[27] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba) dataset. Retrieved August, 15(2018):11, 2018.   \n[28] Karttikeya Mangalam and Vinay Uday Prabhu. Do deep neural networks learn shallow learnable examples first? In ICML 2019 Workshop on Identifying and Understanding Deep Learning Phenomena, 2019.   \n[29] Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure modes of out-of-distribution generalization. In International Conference on Learning Representations, 2021.   \n[30] Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L Edelman, Fred Zhang, and Boaz Barak. Sgd on neural networks learns functions of increasing complexity. Advances in Neural Information Processing Systems, 33, 2020.   \n[31] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.   \n[32] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In International conference on machine learning, pages 5301\u20135310. PMLR, 2019.   \n[33] Maria Refinetti, Alessandro Ingrosso, and Sebastian Goldt. Neural networks trained with sgd learn distributions of increasing complexity. In International Conference on Machine Learning, pages 28843\u201328863. PMLR, 2023.   \n[34] David Saad and Sara Solla. Dynamics of on-line gradient descent learning for multilayer neural networks. Advances in neural information processing systems, 8, 1995.   \n[35] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2020.   \n[36] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. In International Conference on Learning Representations (ICLR), 2020.   \n[37] Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. In International Conference on Machine Learning, pages 8346\u20138356. PMLR, 2020.   \n[38] Stefano Sarao Mannelli, Federica Gerace, Negar Rostamzadeh, and Luca Saglietti. Unfair geometries: exactly solvable data model with fairness implications. arXiv preprint arXiv:2205.15935, 2022.   \n[39] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. Advances in Neural Information Processing Systems, 33:9573\u20139585, 2020.   \n[40] Harini Suresh and John Guttag. A framework for understanding sources of harm throughout the machine learning life cycle. In Equity and access in algorithms, mechanisms, and optimization, pages 1\u20139. 2021.   \n[41] Zhiqin John Xu. Understanding training and generalization in deep learning by fourier analysis. arXiv preprint arXiv:1808.04295, 2018.   \n[42] Yu Yang, Eric Gan, Gintare Karolina Dziugaite, and Baharan Mirzasoleiman. Identifying spurious biases early in training through the lens of simplicity bias. In International Conference on Artificial Intelligence and Statistics, pages 2953\u20132961. PMLR, 2024.   \n[43] Han-Jia Ye, De-Chuan Zhan, and Wei-Lun Chao. Procrustean training for imbalanced deep learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 92\u2013102, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Problem setup and notation 15 ", "page_idx": 13}, {"type": "text", "text": "B Proof of the main theorems 16 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Lemma 3.1. 16   \nB.2 Proof of Lemma 3.2. 16   \nB.3 Proof of Thm 3.3. 16 ", "page_idx": 13}, {"type": "text", "text": "C Derivation of the ODEs 17 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Useful Averages . . 17   \nC.2 ODEs . . 18   \nStudent-shift overlap $M$ . . . 18   \nStudent-teacher $^+$ overlap $R_{+}$ . . . . 19   \nStudent-teacher \u2212overlap $R_{-}$ . . . . 19   \nSelf-overlap $Q$ . . . 19   \nContinuous limit. 21 ", "page_idx": 13}, {"type": "text", "text": "D ODE solutions 23 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 General case 23 ", "page_idx": 13}, {"type": "text", "text": "$M$ 23   \n$R_{+}$ 23   \n$R_{-}$ 23   \n$Q$ 23   \nD.2 Spurious correlations setting 24   \nD.3 Fairness setting 25 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "E Deeper analysis of the learning dynamics equations 25 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E.1 Single centered cluster . 25   \nE.2 Analysis of teacher alignment $(\\tau_{R})$ and student magnitude $(\\tau_{Q})$ timescales 25   \nE.3 Asymptotic preference . . 26 ", "page_idx": 13}, {"type": "text", "text": "F Additional numerical simulations 27 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "F.1 CelebA 27   \nF.2 Simulations on Synthetic Data and Deeper Networks 29 ", "page_idx": 13}, {"type": "text", "text": "A Problem setup and notation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We begin by refreshing the problem description and notation introduced in the main body for the two cluster case (Sec. 3.3) as well as defining some new notation to make the presentations of the results more compact. ", "page_idx": 14}, {"type": "text", "text": "1. $\\left({\\pmb x},y\\right)$ denotes a training example with $\\pmb{x}\\in\\mathbb{R}^{d}$ and $y\\in\\{-1,1\\}$ .   \n2. $\\pmb{x}$ is drawn from a mixture of two Gaussians with means $\\pmb{v}/\\sqrt{d}$ and $-v/\\sqrt{d}$ respectively, covariances $\\Delta_{+}I_{d\\times d}$ and $\\Delta_{-}I_{d\\times d}$ respectively. These two Gaussians are henceforth referred to as the positive and negative Gaussians respectively.   \n3. $\\rho$ represents the probability of the data being drawn from the positive Gaussian.   \n4. $\\langle\\rangle$ denotes an average over $x$ , $\\langle\\rangle_{\\oplus}$ denotes an average over the positive Gaussian and $\\langle\\rangle_{\\Theta}$ denotes an average over the negative Gaussian.   \n5. $\\overline{{{w}}}_{+}$ and $\\overline{{{w}}}_{-}$ denote the teachers for the positive Gaussian and negative Gaussian respectively. $\\pmb{w}$ is the learnt classifier (\"the student\").   \n6. The true labels, $y$ , are then given by: \u2022 $\\mathrm{y}=\\mathrm{sign}(\\overline{{\\pmb{w}}}_{+}\\cdot\\pmb{x}/\\sqrt{d})$ for the positive cluster; \u2022 $\\mathrm{y}=\\mathrm{sign}(\\overline{{\\pmb{w}}}_{-}\\cdot\\pmb{x}/\\sqrt{d})$ for the negative cluster.   \n7. Our predictions are $\\hat{y}={\\pmb w}\\cdot{\\pmb x}/\\sqrt{d}$ .   \n8. The student is trained to minimise $\\mathrm{L}2\\,\\mathrm{loss}=(y-\\hat{y})^{2}$ .   \n9. The student learns using online stochastic gradient descent.   \n10. $\\eta/2$ is the learning rate.   \n11. $\\epsilon$ denotes the generalisation error.   \n12. $a\\cdot b$ denotes the dot product between vectors $a$ and $b$ . ", "page_idx": 14}, {"type": "text", "text": "13. We now define the following Order Parameters (where only the first 4 change with training): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;Q=w\\cdot w/d;}\\\\ &{\\bullet\\;R_{+}=w\\cdot\\overline{{\\boldsymbol{w}}}_{+}/d;}\\\\ &{\\bullet\\;R_{-}=w\\cdot\\overline{{\\boldsymbol{w}}}_{-}/d;}\\\\ &{\\bullet\\;M=w\\cdot\\boldsymbol{v}/d;}\\\\ &{\\bullet\\;T_{\\pm}=\\overline{{\\boldsymbol{w}}}_{+}\\cdot\\overline{{\\boldsymbol{w}}}_{-}/d;}\\\\ &{\\bullet\\;M_{+}^{\\ast}=\\overline{{\\boldsymbol{w}}}_{+}\\cdot\\boldsymbol{v}/d;}\\\\ &{\\bullet\\;M_{-}^{\\ast}=\\overline{{\\boldsymbol{w}}}_{-}\\cdot\\boldsymbol{v}/d;}\\\\ &{\\bullet\\;\\boldsymbol{v}=\\pm\\,\\mathbf{\\boldsymbol{v}}/d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "14. For algebraic simplicity, we assume $||\\overline{{{\\pmb w}}}_{+}||_{2}=||\\pmb{\\overline{{w}}}_{-}||_{2}=\\sqrt{d}$ (and thus, $\\overline{{\\pmb{w}}}_{+}\\cdot\\overline{{\\pmb{w}}}_{+}/d=1$ and $\\overline{{\\pmb{w}}}_{-}^{-}\\cdot\\overline{{\\pmb{w}}}_{-}/d=1)$ . This has the consequence that $T_{\\pm}$ exactly equals the cosine similarity between the two teachers. ", "page_idx": 14}, {"type": "text", "text": "15. We also define $\\Delta^{m i x}=\\rho\\Delta_{+}+(1-\\rho)\\Delta_{-}$ and $\\Delta^{2m i x}=\\rho\\Delta_{+}^{2}+(1-\\rho)\\Delta_{-}^{2}$ ", "page_idx": 14}, {"type": "text", "text": "16. For notational convenience we define: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\alpha_{+}=\\langle y\\rangle_{\\oplus}=1-2\\Phi\\bigg(\\frac{-M_{+}^{*}}{\\sqrt{\\Delta_{+}}}\\bigg),}\\\\ {\\displaystyle\\alpha_{-}=\\langle y\\rangle_{\\ominus}=1-2\\Phi\\bigg(\\frac{-(-M_{-}^{*})}{\\sqrt{\\Delta_{-}}}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note, $\\alpha_{+}$ also has an intuitive meaning. It represents the difference between the probability that an example drawn from the positive cluster has positive true label and the probability that an example drawn from the positive cluster has negative true label. It is hence 0 when the positive cluster has equal positive and negative examples, positive when the cluster has more positive examples than negative, negative when the cluster has more negative examples than positive. Similarly, $\\alpha_{-}$ represents the difference in these probabilities for the negative cluster. ", "page_idx": 14}, {"type": "text", "text": "17. Finally, we also define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c c c}{\\displaystyle{\\beta_{+}=\\sqrt{\\frac{2\\Delta_{+}}{\\pi}}\\exp{\\left(\\frac{-M_{+}^{*2}}{2\\Delta_{+}}\\right)},}}\\\\ {\\displaystyle{\\beta_{-}=\\sqrt{\\frac{2\\Delta_{-}}{\\pi}}\\exp{\\left(\\frac{-M_{-}^{*2}}{2\\Delta_{-}}\\right)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "18. Lastly, we use $t$ to denote continuous time given by (epoch number/ $d$ ). ", "page_idx": 15}, {"type": "text", "text": "B Proof of the main theorems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Lemma 3.1. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Denote with $\\left<\\cdot\\right>_{j}$ the expectation over samples from cluster $j$ . The generalisation error reads $\\epsilon=$ $\\textstyle\\sum_{j=1}^{m}\\rho_{j}\\epsilon_{j}$ with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\epsilon_{j}:=\\left\\langle\\left(y-\\hat{y}\\right)^{2}\\right\\rangle_{j}=\\left\\langle\\left(y-\\displaystyle\\frac{\\pmb{w}\\cdot\\pmb{x}}{\\sqrt{d}}\\right)^{2}\\right\\rangle_{j}=\\left\\langle y^{2}\\right\\rangle_{j}+\\left\\langle\\left(\\displaystyle\\frac{\\pmb{w}\\cdot\\pmb{x}}{\\sqrt{d}}\\right)^{2}\\right\\rangle_{j}-2\\left\\langle y\\displaystyle\\frac{\\pmb{w}\\cdot\\pmb{x}}{\\sqrt{d}}\\right\\rangle_{j}}\\\\ {=1+(Q\\Delta_{j}+M_{j}^{2})-2(\\alpha_{j}M_{j}+R_{j}\\beta_{j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second term comes from: isolating the mean and the definition of $M_{j}$ , and the isotropy of $x$ . The third term comes from the useful identity Integral $^{\\,I}$ Eq. C.31, derived in Appendix C.1, and the constants are given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{j}=1-2\\Phi\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\left(\\frac{-M_{j}^{*}}{\\sqrt{\\Delta_{j}}}\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\right),\\quad\\beta_{j}=\\sqrt{\\frac{2\\Delta_{j}}{\\pi}}\\exp\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\left(\\frac{-(M_{j}^{*})^{2}}{2\\Delta_{j}}\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $M_{j}^{*}:=\\overline{{\\pmb{w}}}_{j}^{\\top}\\pmb{v}_{j}/d$ and $\\begin{array}{r}{\\Phi(x)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x}e^{-u^{2}/2}d u}\\end{array}$ is the cumulative distribution function of the standard normal. ", "page_idx": 15}, {"type": "text", "text": "The formula for the generalisation error specializes to the case of two clusters with opposite means as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\epsilon=1+M^{2}-(2\\rho\\alpha_{+}-2(1-\\rho)\\alpha_{-})\\,M}}\\\\ {{-\\,2\\rho\\beta_{+}R_{+}-2(1-\\rho)\\beta_{-}R_{-}+\\Delta^{m i x}Q,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Notably, $\\alpha_{\\pm}$ has an intuitive meaning wherein it represents the difference between the fraction of positive and negatives in a cluster, i.e., $\\alpha_{+}=\\langle y\\rangle_{c=+}$ and $\\alpha_{-}=\\langle{y}\\rangle_{c=-}$ . ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Lemma 3.2. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Explicit computations are carried out in Appendix C.2 below for the case of two clusters. ", "page_idx": 15}, {"type": "text", "text": "B.3 Proof of Thm 3.3. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Using the notation of Section 3.2 and assuming Lemma 3.2, we examine the update equations (7) written as a stochastic iterative process ", "page_idx": 15}, {"type": "equation", "text": "$$\nS^{k+1}=S^{k}+\\mathbb{E}\\frac{1}{d}f(S^{k})+\\frac{1}{\\sqrt{d}}\\xi_{d}^{k},\\qquad\\xi_{d}^{k}:=\\sqrt{d}(\\Delta S^{k}-\\mathbb{E}[\\Delta S^{k}])\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the expectation is over the new sample $\\pmb{x}^{k}$ and conditional on the past samples. The noise term $\\xi_{d}^{k}$ has zero mean $\\mathbb{E}[\\xi_{d}^{k}]=0$ and conditional covariance $\\Sigma_{d}:=\\mathbb{E}[\\xi_{d}^{k}\\xi_{d}^{k\\top}]$ . ", "page_idx": 15}, {"type": "text", "text": "Define the continuous-time rescaled process $S_{d}(t)$ ) as the linear interpolation of $S^{\\lfloor t d\\rfloor}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nS_{d}(t)=S^{\\lfloor t d\\rfloor}+(t d-\\lfloor t d\\rfloor)(S^{\\lfloor t d\\rfloor+1}-S^{\\lfloor t d\\rfloor})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here we leverage existing stochastic process convergence results (e.g., [6], Theorem 2.3]) showing that, if $\\Sigma_{d}$ converges to the matrix valued function $\\Sigma(S)$ as $d\\to\\infty$ in some appropriate sense, then ", "page_idx": 15}, {"type": "text", "text": "the sequence $S_{d}(t)$ converges weakly as $d\\rightarrow\\infty$ to the solution ${\\tilde{S}}_{t}$ of the stochastic differential equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd\\tilde{S}_{t}=f(\\tilde{S}_{t})d t+\\sqrt{\\Sigma(\\tilde{S}_{t})}d B_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $B_{t}$ is a standard Brownian motion in $\\mathbb{R}^{2m+1}$ . In our case, we can show that $\\Sigma_{d}\\in\\mathcal{O}(d^{-1})$ as $d\\to\\infty$ , so that $\\Sigma=0$ and Eq. B.27 reduces to the ODE in Eq. 8. ", "page_idx": 16}, {"type": "text", "text": "Let us sketch the scaling argument. Algebraic manipulations similar to those in Section C.2 show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Sigma_{d}=\\nabla{\\mathcal{S}}^{k\\top}\\mathbb{E}[\\Phi^{k}\\Phi^{k\\top}]\\nabla{\\mathcal{S}}^{k}(1+{\\mathcal{O}}(d^{-1})),\\qquad\\Phi^{k}:=\\eta(\\delta^{k}x^{k}-\\mathbb{E}[\\delta^{k}x^{k}])\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\nabla$ denotes the gradient with respect to the student vector $\\mathbfit{w}$ . Recall that $S^{k}$ has $2m$ components that are linear in $\\pmb{w}$ (corresponding to the order parameters $R_{j}$ and $M_{j}$ in Eq. 5) and one that is quadratic (corresponding to $Q$ ). By making the gradients $\\nabla{S}^{k}$ explicit using Eq. 5), we see that at leading order, the matrix entries $\\Sigma_{d}^{i j},1\\le i,j\\le2m+1$ take the form ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Sigma_{d}^{i j}=\\frac{1}{d}\\mathbb{E}[\\Phi_{{\\pmb{a}}_{i}}^{k}\\Phi_{{\\pmb{a}}_{j}}^{k\\top}],\\qquad\\Phi_{{\\pmb{a}}_{i}}^{k}=\\eta(\\delta^{k}\\frac{{\\pmb{a}}_{i}^{\\top}{\\pmb{x}}^{k}}{\\sqrt{d}}-\\mathbb{E}[\\delta^{k}\\frac{{\\pmb{a}}_{i}^{\\top}{\\pmb{x}}^{k}}{\\sqrt{d}}])\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the vector $\\pmb{a}_{i}$ is either one of the teacher vectors $\\overline{{\\pmb{w}}}_{j}$ , one of the shift vector ${\\pmb v}_{j}$ , or the student vector $\\pmb{w}$ , depending on the entry $i=1,\\cdot\\cdot\\cdot,2m+1$ . As can be sh\u221aown explic\u221aitly as in App\u221aendix C.1 below, $\\Phi_{{\\pmb a}_{i}}^{k}$ depend on $\\pmb{x}^{k}$ only through auxiliary variables $\\overline{{{\\pmb w}}}_{j}^{\\top}{\\pmb x}/\\sqrt{d},\\overline{{{\\pmb v}}}_{j}^{\\top}\\dot{\\pmb x}/\\sqrt{d},{\\pmb w}^{k\\top}{\\pmb x}^{k}/\\sqrt{d}$ , which jointly follow a multivariate distribution whose parameters depend on the student vector $\\pmb{w}^{k}$ only through $S^{k}$ and are in $O(1)$ as $d\\to\\infty$ . As a result, $\\Sigma_{d}^{i j}\\in O(d^{\\overline{{-}}1})$ . ", "page_idx": 16}, {"type": "text", "text": "Finally, the weak convergence of $S_{d}(t)_{t}$ to $\\bar{S}_{t}$ implies convergence in probability for the supremum norm on the interval $[0,T]$ for any $T>0$ . Specifically, for each $1\\leq i\\leq2m+1$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T}|S_{d i}(t)-\\bar{S}_{i}(t)|\\stackrel{P}{\\longrightarrow}0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\xrightarrow{P}$ denotes convergence in probability. This result directly leads to Eq. 9, thereby proving the theorem. ", "page_idx": 16}, {"type": "text", "text": "C Derivation of the ODEs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we are going to explicitly derive the ODE describing the dynamics of the order parameters. Starting from the discrete updates of the order parameters, Eqs. 7, we are going to consider the thermodynamic limit, $d\\to\\infty$ . As proven in Thm. 3.3, the updates concentrate to their typical value and the discrete evolution converges to differential equations. Therefore, the rest of the section is devoted to performing averages over the Gaussians in order to evaluate the typical values. Before proceeding with the evaluation of Eqs. 7, it is useful to introduce two identities. ", "page_idx": 16}, {"type": "text", "text": "C.1 Useful Averages ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Integral 1: ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle a\\cdot x\\ \\mathrm{sign}(b\\cdot x+c)\\rangle=(a\\cdot\\mu)(1-2\\Phi\\left(\\frac{-(b\\cdot\\mu+c)}{\\sqrt{\\Delta b\\cdot b}}\\right))+a\\cdot b\\sqrt{\\frac{2\\Delta}{b\\cdot b\\pi}}\\exp\\left(\\frac{-(b\\cdot\\mu+c)^{2}}{2\\Delta b\\cdot b}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $x$ is multivariate normal distribution with mean $\\mu$ and covariance $\\Delta I$ , and the angular bracket notation indicates average with respect to $x$ . ", "page_idx": 16}, {"type": "text", "text": "Derivation. Define the auxiliary random variables $z_{1}\\,=\\,a\\cdot x$ and $z_{2}=b\\cdot x+c$ , that follow a multivariate normal distribution ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\binom{z_{1}}{z_{2}}}\\sim{\\mathcal{N}}\\!\\left(\\left[{\\begin{array}{r}{a\\cdot\\mu}\\\\ {b\\cdot\\mu+c}\\end{array}}\\right],\\Delta\\left[{\\begin{array}{r r}{a\\cdot a}&{a\\cdot b}\\\\ {a\\cdot b}&{b\\cdot b}\\end{array}}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the law of iterated expectation, our average can be written as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle a\\cdot x\\,\\mathrm{sign}(b\\cdot x+c)\\rangle=\\mathbb{E}_{z_{2}}[\\mathrm{sign}(z_{2})\\mathbb{E}_{z_{1}|z_{2}}[z_{1}]]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{z_{2}}[\\mathrm{sign}(z_{2})(a\\cdot\\mu+\\frac{a\\cdot b}{b\\cdot b}(z_{2}-(b\\cdot\\mu+c))]}\\\\ &{=(a\\cdot\\mu-\\frac{a\\cdot b}{b\\cdot b}(b\\cdot\\mu+c))\\mathbb{E}_{z_{2}}[\\mathrm{sign}(z_{2})]+\\frac{a\\cdot b}{b\\cdot b}\\mathbb{E}_{z_{2}}[z_{2}\\mathrm{sign}(z_{2})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first expectation follows from the definition of the cumulative distribution function $\\Phi$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z_{2}}[\\mathrm{sign}(z_{2})]=(1-2\\Phi\\!\\left(\\frac{-(b\\cdot\\mu+c)}{\\sqrt{\\Delta b\\cdot b}}\\right)).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The second term is simply the mean of a folded normal distribution ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z_{2}}[z_{2}\\mathrm{sign}(z_{2})]=(\\sqrt{\\Delta b\\cdot b})\\sqrt{\\frac{2}{\\pi}}\\exp\\left(\\frac{-(b\\cdot\\mu+c)^{2}}{2\\Delta b\\cdot b}\\right)+(b\\cdot\\mu+c)(1-2\\Phi\\bigg(\\frac{-(b\\cdot\\mu+c)}{\\sqrt{\\Delta b\\cdot b}}\\bigg)).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining these three expressions we obtain the identity. ", "page_idx": 17}, {"type": "text", "text": "Integral 2: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle a\\cdot x\\,b\\cdot x\\rangle=(a\\cdot\\mu)(b\\cdot\\mu)+\\Delta(a\\cdot b)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $x$ is defined as for the previous identity. ", "page_idx": 17}, {"type": "text", "text": "Derivation. We proceed as in the previous case. Define the auxiliary random variables $z_{1}=a\\cdot x$ and $z_{2}=b\\cdot x$ . They follow a multivariate normal distribution ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[z_{1}\\right]\\sim\\mathcal{N}\\Bigg(\\left[\\!\\!\\begin{array}{c}{a\\cdot\\mu}\\\\ {b\\cdot\\mu}\\end{array}\\!\\!\\right],\\Delta\\left[\\!\\!\\begin{array}{c c}{a\\cdot a}&{a\\cdot b}\\\\ {a\\cdot b}&{b\\cdot b}\\end{array}\\!\\!\\right]\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the law of iterated expectation, our average may be written as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle a\\cdot x\\ b\\cdot x\\rangle=\\mathbb{E}_{z_{2}}[z_{2}\\mathbb{E}_{z_{1}|z_{2}}[z_{1}]]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{z_{2}}[z_{2}(a\\cdot\\mu+\\frac{a\\cdot b}{b\\cdot b}(z_{2}-(b\\cdot\\mu))]}\\\\ &{\\quad\\quad\\quad=(a\\cdot\\mu-\\frac{a\\cdot b}{b\\cdot b}(b\\cdot\\mu))\\mathbb{E}_{z_{2}}[z_{2}]+\\frac{a\\cdot b}{b\\cdot b}\\mathbb{E}_{z_{2}}[z_{2}^{2}]}\\\\ &{\\quad\\quad\\quad=(a\\cdot\\mu-\\frac{a\\cdot b}{b\\cdot b}(b\\cdot\\mu))(b\\cdot\\mu)+\\frac{a\\cdot b}{b\\cdot b}(\\Delta b\\cdot b+(b\\cdot\\mu)^{2})}\\\\ &{\\quad\\quad\\quad=(a\\cdot\\mu)(b\\cdot\\mu)+\\Delta(a\\cdot b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2 ODEs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We have now the building blocks to evaluate the expected values of Eqs. 7. We refresh the notation that $\\delta^{\\mu}\\,=\\,y^{\\mu}\\,-\\,\\hat{y}^{\\mu}$ , $y^{\\mu}\\,=\\,\\mathrm{sign}\\left({\\pmb x}^{\\mu}\\cdot{\\pmb\\overline{{{w}}}}_{\\mu}/\\sqrt{d}\\right)$ , and $\\hat{y}^{\\mu}\\,=\\,{\\pmb x}^{\\mu}\\cdot{\\pmb w}/\\sqrt{d}$ . Final step is to take the continuous limit. This is obtained by noticing that the RHS of the equations is factorised by $1/d$ . Therefore by taking as time unit $1/d$ and defining time as $t=\\mu/d$ the discrete updates converge to continuous increments as $d\\to\\infty$ . ", "page_idx": 17}, {"type": "text", "text": "Student-shift overlap $M$ . ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle\\Delta M\\rangle=\\frac{\\eta}{d}\\left(\\rho v\\alpha_{+}+\\rho M_{+}^{*}\\beta_{+}-(1-\\rho)v\\alpha_{-}+(1-\\rho)M_{-}^{*}\\beta_{-}-\\left(M(v+\\Delta^{m i x})\\right)\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Derivation. Starting from the definition in Eq. 7 for $M$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle\\Delta M\\rangle=\\frac{\\eta}{d}\\bigg(\\bigg\\langle y\\frac{\\pmb{x}\\cdot\\pmb{v}}{\\sqrt{d}}\\bigg\\rangle-\\bigg\\langle\\frac{\\pmb{w}\\cdot\\pmb{x}}{\\sqrt{d}}\\frac{\\pmb{x}\\cdot\\pmb{v}}{\\sqrt{d}}\\bigg\\rangle\\bigg).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first term can be evaluated using integral 1 and the second term using integral 2 yielding the result. ", "page_idx": 17}, {"type": "text", "text": "Student-teacher $^+$ overlap $R_{+}$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\langle\\Delta R_{+}\\rangle=\\displaystyle\\frac{\\eta}{d}\\Big(\\rho(M_{+}^{*}\\alpha_{+}+\\beta_{+})+(1-\\rho)(-M_{+}^{*}\\alpha_{-}+T_{\\pm}\\beta_{-})}}\\\\ {{-\\rho(M M_{+}^{*}+R_{+}\\Delta_{+})-(1-\\rho)(M M_{+}^{*}+R_{+}\\Delta_{-})\\Big)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Derivation. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Delta R_{+}\\rangle=\\frac{\\eta}{d}\\left\\langle\\left(y-\\frac{w\\cdot x}{\\sqrt{d}}\\right)\\left(\\frac{\\mathbf{x}\\cdot\\overline{{w}}_{+}}{\\sqrt{d}}\\right)\\right\\rangle}\\\\ {\\quad\\quad=\\frac{\\eta}{d}\\left(\\rho\\left\\langle y\\frac{x\\cdot\\overline{{w}}_{+}}{\\sqrt{d}}\\right\\rangle_{\\oplus}+(1-\\rho)\\left\\langle y\\frac{\\mathbf{x}\\cdot\\overline{{w}}_{+}}{\\sqrt{d}}\\right\\rangle_{\\ominus}-\\rho\\left\\langle\\frac{w\\cdot x}{\\sqrt{d}}\\frac{x\\cdot\\overline{{w}}_{+}}{\\sqrt{d}}\\right\\rangle_{\\oplus}-(1-\\rho)\\left\\langle\\frac{w\\cdot x}{\\sqrt{d}}\\frac{x\\cdot\\overline{{w}}_{-}\\cdot\\overline{{w}}_{+}}{\\sqrt{d}}\\right\\rangle_{\\oplus}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "These 4 terms can be computed using integrals 1 and 2 yielding the result. ", "page_idx": 18}, {"type": "text", "text": "Student-teacher \u2212overlap $R_{-}$ . ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\langle\\Delta R_{-}\\rangle=\\displaystyle\\frac{\\eta}{d}\\Big(\\rho(M_{-}^{*}\\alpha_{+}+T_{\\pm}\\beta_{+})+(1-\\rho)(-M_{-}^{*}\\alpha_{-}+\\beta_{-})}}\\\\ {{-\\rho(M M_{-}^{*}+R_{-}\\Delta_{+})-(1-\\rho)(M M_{-}^{*}+R_{-}\\Delta_{-})\\Big)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Derivation. Same as for $R_{+}$ . ", "page_idx": 18}, {"type": "text", "text": "Self-overlap $Q$ . ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\langle\\Delta Q\\rangle=\\displaystyle\\frac{2\\eta}{d}\\left(\\rho(\\alpha_{+}M+\\beta_{+}R_{+})+(1-\\rho)(-\\alpha_{-}M+\\beta_{-}R_{+})-M^{2}-Q\\Delta^{m i x}\\right)}}\\\\ {{+\\displaystyle\\frac{\\eta^{2}}{d}\\Big(\\Delta^{m i x}+Q\\Delta^{2m i x}+M^{2}\\Delta^{m i x}}}\\\\ {{-\\displaystyle2\\left(\\rho\\Delta_{+}(\\alpha_{+}M+\\beta_{+}R_{+})+(1-\\rho)\\Delta_{-}(-\\alpha_{-}M+\\beta_{-}R_{+})\\right)\\Big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Derivation. This update requires additional steps with respect to the previous ones. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\langle\\Delta Q\\right\\rangle=\\frac{2\\eta}{d}\\left\\langle\\delta\\frac{\\pmb{w}_{j}^{\\top}\\pmb{x}}{\\sqrt{d}}\\right\\rangle+\\frac{\\eta^{2}}{d}\\left\\langle(\\delta^{\\mu})^{2}\\frac{\\|\\pmb{x}^{\\mu}\\|^{2}}{d}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first term is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\frac{{2\\eta}}{d}\\left\\langle{\\delta\\frac{{\\pmb{\\psi}}_{j}^{\\top}{\\pmb{x}}}{\\sqrt{d}}}\\right\\rangle=\\frac{2\\eta}{d}\\left\\langle{\\pmb{y}\\frac{{\\pmb{w}}\\cdot{\\pmb{x}}}{\\sqrt{d}}-\\left(\\frac{{\\pmb{w}}\\cdot{\\pmb{x}}}{\\sqrt{d}}\\right)^{2}}\\right\\rangle}}\\\\ {{\\quad\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\frac{2\\eta}{d}\\left(M(\\rho\\alpha_{+}-(1-\\rho)\\alpha_{-})+\\rho\\beta_{+}R_{+}+(1-\\rho)\\beta_{-}R_{-}-M^{2}-Q\\Delta^{m i x}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second term ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\eta^{2}}{d}\\left<(\\delta^{\\mu})^{2}\\frac{\\|\\pmb{x}^{\\mu}\\|^{2}}{d}\\right>=\\displaystyle\\frac{\\eta^{2}}{d}\\left<\\left(y-\\frac{\\pmb{w}\\cdot\\pmb{x}}{\\sqrt{d}}\\right)^{2}\\frac{\\pmb{x}\\cdot\\pmb{x}}{d}\\right>}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\frac{\\eta^{2}}{d}\\left<y^{2}\\frac{\\pmb{x}\\cdot\\pmb{x}}{d}+\\left(\\frac{\\pmb{w}\\cdot\\pmb{x}}{\\sqrt{d}}\\right)^{2}\\frac{\\pmb{x}\\cdot\\pmb{x}}{d}-2y\\frac{\\pmb{w}\\cdot\\pmb{x}}{\\sqrt{d}}\\frac{\\pmb{x}\\cdot\\pmb{x}}{d}\\right>}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "requires additional steps. We consider the three terms in the expression above, starting from the first one ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\langle y^{2}\\frac{\\pmb{x}\\cdot\\pmb{x}}{d}\\right\\rangle=\\left\\langle\\frac{\\pmb{x}\\cdot\\pmb{x}}{d}\\right\\rangle=\\frac{1}{d}(\\sum_{i=1}^{d}\\left\\langle x_{i}^{2}\\right\\rangle)=\\frac{1}{d}(\\sum_{i=1}^{d}\\rho\\left\\langle x_{i}^{2}\\right\\rangle_{\\oplus}+(1-\\rho)\\left\\langle x_{i}^{2}\\right\\rangle_{\\ominus})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{=\\displaystyle\\frac{1}{d}(\\sum_{i=1}^{d}\\rho(\\Delta_{+}+v_{i}^{2}/d)+(1-\\rho)(\\Delta_{-}+v_{i}^{2}/d))=\\Delta^{m i x}+v/d}}\\\\ {{=\\Delta^{m i x}+O(d^{-1}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where we used the simplification $y^{2}\\,=\\,1\\qquad$ independently of the cluster\u2019s teacher. However, the remaining terms require us to split the expectation considering the probability of sampling from each cluster. The second term ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\langle{\\frac{\\pm\\cdot x}{d}}{\\left({\\frac{{\\pmb w}\\cdot{\\pmb x}}{\\sqrt{d}}}\\right)}^{2}\\right\\rangle=\\rho\\left\\langle{\\frac{\\pmb x\\cdot x}{d}}{\\left({\\frac{{\\pmb w}\\cdot{\\pmb x}}{\\sqrt{d}}}\\right)}^{2}\\right\\rangle_{\\oplus}+(1-\\rho)\\left\\langle{\\frac{\\pmb x\\cdot x}{d}}{\\left({\\frac{{\\pmb w}\\cdot{\\pmb x}}{\\sqrt{d}}}\\right)}^{2}\\right\\rangle_{\\ominus}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We begin by analysing the average over the positive Gaussian and split $\\pmb{x}$ as $\\pmb{x}=\\pmb{v}/\\sqrt{d}+\\tilde{\\pmb{x}}$ such that $\\tilde{\\pmb{x}}$ has zero mean. Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{\\pmb{x}\\cdot\\pmb{x}}{d}\\bigg(\\frac{\\pmb{w}\\cdot\\pmb{x}}{\\sqrt{d}}\\bigg)^{2}\\right\\rangle_{\\oplus}=\\left\\langle\\bigg[\\frac{\\pmb{v}\\cdot\\pmb{v}}{d^{2}}+\\frac{2\\pmb{v}\\cdot\\tilde{\\pmb{x}}}{d\\sqrt{d}}+\\frac{\\tilde{\\pmb{x}}\\cdot\\tilde{\\pmb{x}}}{d}\\bigg]\\bigg[\\bigg(\\frac{\\pmb{w}\\cdot\\pmb{v}}{d}\\bigg)^{2}+2\\frac{\\pmb{w}\\cdot\\pmb{v}}{d}\\frac{\\pmb{w}\\cdot\\tilde{\\pmb{x}}}{\\sqrt{d}}+\\bigg(\\frac{\\pmb{w}\\cdot\\tilde{\\pmb{x}}}{\\sqrt{d}}\\bigg)^{2}\\bigg]\\right\\rangle_{\\oplus}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Multiplying the terms in the brackets \u221awill give rise to 9 terms. We can see that the $3+3{=}6$ terms corresponding to ${\\pmb v}\\cdot{\\pmb v}/d^{2}$ and $2{\\pmb v}\\cdot\\tilde{\\pmb x}/d\\sqrt{d}$ will tend to 0 in the limit of infinite d due to their scaling. We now analyse the other 3 terms: ", "page_idx": 19}, {"type": "text", "text": "Term 1: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\cfrac{\\tilde{\\pmb{x}}\\cdot\\tilde{\\pmb{x}}}{d}\\left(\\cfrac{\\pmb{w}\\cdot\\pmb{v}}{d}\\right)^{2}\\right\\rangle_{\\oplus}=\\left(\\cfrac{\\pmb{w}\\cdot\\pmb{v}}{d}\\right)^{2}\\left\\langle\\cfrac{\\tilde{\\pmb{x}}\\cdot\\tilde{\\pmb{x}}}{d}\\right\\rangle_{\\oplus}+O(d^{-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=M^{2}\\Delta_{+}+O(d^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Term 2: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\left\\langle\\cfrac{\\tilde{\\pmb{x}}\\cdot\\tilde{\\pmb{x}}}{d}\\cfrac{\\pmb{w}\\cdot\\pmb{v}}{d}\\cfrac{\\pmb{w}\\cdot\\tilde{\\pmb{x}}}{\\sqrt{d}}\\right\\rangle_{\\oplus}=2R\\left\\langle\\cfrac{\\tilde{\\pmb{x}}\\cdot\\tilde{\\pmb{x}}}{d}\\cfrac{\\pmb{w}\\cdot\\tilde{\\pmb{x}}}{\\sqrt{d}}\\right\\rangle_{\\oplus}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2R\\left\\langle\\cfrac{\\tilde{\\pmb{x}}\\cdot\\tilde{\\pmb{x}}}{d}\\right\\rangle_{\\oplus}\\left\\langle\\cfrac{\\pmb{w}\\cdot\\tilde{\\pmb{x}}}{\\sqrt{d}}\\right\\rangle_{\\oplus}+O(d^{-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=0+O(d^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Term 3: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\langle\\frac{\\tilde{\\mathbf{x}}\\cdot\\tilde{\\mathbf{x}}}{d}\\bigg(\\frac{\\mathbf{w}\\cdot\\tilde{\\mathbf{x}}}{\\sqrt{d}}\\bigg)^{2}\\right\\rangle_{\\oplus}=\\bigg\\langle\\frac{\\tilde{\\mathbf{x}}\\cdot\\tilde{\\mathbf{x}}}{d}\\bigg\\rangle_{\\oplus}\\bigg\\langle\\bigg(\\frac{\\mathbf{w}\\cdot\\tilde{\\mathbf{x}}}{\\sqrt{d}}\\bigg)^{2}\\bigg\\rangle_{\\oplus}+O(d^{-1})}}\\\\ &{}&{=\\Delta_{+}(\\Delta_{+}Q)+O(d^{-1})=Q\\Delta_{+}^{2}+O(d^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus finally, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\left<{{\\frac{x\\cdot x}{d}}{\\left({\\frac{w\\cdot x}{\\sqrt{d}}}\\right)}^{2}}}\\right>=\\rho(M^{2}\\Delta_{+}+Q\\Delta_{+}^{2})+(1-\\rho)(M^{2}\\Delta_{-}+Q\\Delta_{-}^{2})}}\\\\ {{=M^{2}\\Delta^{m i x}+Q\\Delta^{2m i x}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the the third term ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\langle y{\\frac{\\pmb w\\cdot\\pmb x\\cdot x}{\\sqrt d}}\\right\\rangle=\\rho\\left\\langle y{\\frac{\\pmb w\\cdot\\pmb x}{\\sqrt d}}{\\frac{\\pmb x\\cdot\\pmb x}{d}}\\right\\rangle_{\\oplus}+(1-\\rho)\\left\\langle y{\\frac{\\pmb w\\cdot\\pmb x}{\\sqrt d}}{\\frac{\\pmb x\\cdot\\pmb x}{d}}\\right\\rangle_{\\ominus}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As before, we analyse the average over the positive Gaussian first and split $\\textbf{\\em x}$ into its mean and a zero mean component: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\langle y{\\frac{x\\cdot x}{d}}{\\frac{\\pmb w\\cdot x}{\\sqrt{d}}}\\right\\rangle_{\\oplus}=\\left\\langle y\\left[{\\frac{\\pmb v\\cdot\\pmb v}{d^{2}}}+{\\frac{2\\pmb v\\cdot{\\tilde{x}}}{d\\sqrt{d}}}+{\\frac{{\\tilde{x}}\\cdot{\\tilde{x}}}{d}}\\right]\\left[{\\frac{\\pmb w\\cdot\\pmb v}{d}}+{\\frac{\\pmb w\\cdot{\\tilde{x}}}{\\sqrt{d}}}\\right]\\right\\rangle_{\\oplus}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This gives rise to 6 terms. We can see that the $2+2{=}4$ terms corresponding to ${\\pmb v}\\cdot{\\pmb v}/d^{2}$ and $2{\\pmb v}\\cdot\\tilde{\\pmb x}/d\\sqrt{d}$ will tend to 0 in the limit of infinite d due to their scaling. We now analyse the other 2 terms: ", "page_idx": 20}, {"type": "text", "text": "Term 1: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle y\\frac{\\tilde{x}\\cdot\\tilde{x}}{d}\\frac{\\boldsymbol{w}\\cdot\\boldsymbol{v}}{d}\\right\\rangle_{\\circled{0}}=M\\left\\langle y\\frac{\\tilde{x}\\cdot\\tilde{x}}{d}\\right\\rangle_{\\circled{0}}}\\\\ &{\\qquad\\qquad\\qquad=M\\left\\langle\\mathrm{sign}(\\frac{\\tilde{x}\\cdot\\overline{{\\boldsymbol{w}}}_{+}}{\\sqrt{d}}+\\frac{\\overline{{\\boldsymbol{w}}}_{+}\\cdot\\boldsymbol{v}}{d})\\frac{\\tilde{x}\\cdot\\tilde{\\boldsymbol{x}}}{d}\\right\\rangle_{\\circled{0}}}\\\\ &{\\qquad\\qquad\\qquad=M\\left\\langle\\mathrm{sign}(\\frac{\\tilde{x}\\cdot\\overline{{\\boldsymbol{w}}}_{+}}{\\sqrt{d}}+\\frac{\\overline{{\\boldsymbol{w}}}_{+}\\cdot\\boldsymbol{v}}{d})\\right\\rangle_{\\circled{0}}\\left\\langle\\frac{\\tilde{x}\\cdot\\tilde{\\boldsymbol{x}}}{d}\\right\\rangle_{\\circled{0}}+O(d^{-1})}\\\\ &{\\qquad\\qquad\\qquad=M\\left\\langle y\\right\\rangle_{\\circled{0}}\\Delta_{+}+O(d^{-1})}\\\\ &{\\qquad=M\\alpha_{+}\\Delta_{+}+O(d^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Term 2: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle y\\left(\\frac{\\widetilde{\\pmb{x}}\\cdot\\widetilde{\\pmb{x}}}{d}\\right)\\left(\\frac{{\\pmb{w}}\\cdot\\widetilde{\\pmb{x}}}{\\sqrt{d}}\\right)\\right\\rangle_{\\oplus}=\\left\\langle y\\left(\\frac{{\\pmb{w}}\\cdot\\widetilde{\\pmb{x}}}{\\sqrt{d}}\\right)\\right\\rangle_{\\oplus}\\left\\langle\\left(\\frac{\\widetilde{\\pmb{x}}\\cdot\\widetilde{\\pmb{x}}}{d}\\right)\\right\\rangle_{\\oplus}+O(d^{-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\Delta_{+}\\left\\langle y\\left(\\frac{{\\pmb{w}}\\cdot\\widetilde{\\pmb{x}}}{\\sqrt{d}}\\right)\\right\\rangle_{\\oplus}+O(d^{-1})}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\Delta_{+}R_{+}\\beta_{+}+O(d^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Where the last equality follows using integral 1. Thus: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\langle y\\frac{\\pmb{x}\\cdot\\pmb{x}}{d}\\frac{\\pmb{w}\\cdot\\pmb{x}}{\\sqrt{d}}\\right\\rangle_{\\oplus}=\\Delta_{+}(\\alpha_{+}M+\\beta_{+}R_{+})+O(d^{-1}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We repeat the same analysis for the negative gaussian and get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\langle y\\frac{\\pmb{x}\\cdot\\pmb{x}}{d}\\frac{\\pmb{x}\\cdot\\pmb{x}}{\\sqrt{d}}\\right\\rangle=\\rho\\Delta_{+}(\\alpha_{+}M+\\beta_{+}R_{+})+(1-\\rho)\\Delta_{-}(-\\alpha_{-}M+\\beta_{-}R_{+})+O(d^{-1}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Collecting everything together and taking the infinite dimensional limit: ", "page_idx": 20}, {"type": "text", "text": "\u27e8 $\\Delta{w}\\cdot\\Delta{w}/d)=\\frac{\\eta^{2}}{d}\\left(\\Delta^{m i x}+Q\\Delta^{2m i x}+M^{2}\\Delta^{m i x}-2\\left(\\rho\\Delta_{+}(\\alpha_{+}M+\\beta_{+}R_{+})+(1-\\rho)\\Delta_{-}(-\\alpha_{-}\\Delta_{+}R_{+})\\right)\\right),$ M + \u03b2\u2212R+)) ", "page_idx": 20}, {"type": "text", "text": "Thus, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Delta Q\\rangle=\\displaystyle\\frac{2\\eta}{d}\\left(\\rho(\\alpha_{+}M+\\beta_{+}R_{+})+(1-\\rho)(-\\alpha_{-}M+\\beta_{-}R_{+})-M^{2}-Q\\Delta^{m i x}\\right)}}\\\\ {{+\\displaystyle\\frac{\\eta^{2}}{d}\\left(\\Delta^{m i x}+Q\\Delta^{2m i x}+M^{2}\\Delta^{m i x}-2\\left(\\rho\\Delta_{+}(\\alpha_{+}M+\\beta_{+}R_{+})+(1-\\rho)\\Delta_{-}(-\\alpha_{-}M+\\beta_{-}R_{+})\\right)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Continuous limit. Final step of the derivation is taking the termodynamics limit that leads to the ODEs implicitely defined in Thm. 3.3: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{M}(M,R_{+},R_{-},Q)=\\eta\\Big(\\rho v\\alpha_{+}+\\rho M_{+}^{*}\\beta_{+}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad-\\left(1-\\rho\\right)v\\alpha_{-}+(1-\\rho)M_{-}^{*}\\beta_{-}-(M(v+\\Delta^{m i x}))\\Big),}\\\\ &{f_{R_{+}}(M,R_{+},R_{-},Q)=\\eta\\Big(\\rho(M_{+}^{*}\\alpha_{+}+\\beta_{+})+(1-\\rho)(-M_{+}^{*}\\alpha_{-}+T_{\\pm}\\beta_{-})}\\\\ &{\\qquad\\qquad\\qquad\\quad-\\rho(M M_{+}^{*}+R_{+}\\Delta_{+})-(1-\\rho)(M M_{+}^{*}+R_{+}\\Delta_{-})\\Big),}\\\\ &{f_{R_{-}}(M,R_{+},R_{-},Q)=\\eta\\Big(\\rho(M_{-}^{*}\\alpha_{+}+T_{\\pm}\\beta_{+})+(1-\\rho)(-M_{-}^{*}\\alpha_{-}+\\beta_{-})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad-\\rho(M M_{-}^{*}+R_{-}\\Delta_{+})-(1-\\rho)(M M_{-}^{*}+R_{-}\\Delta_{-})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{Q}(M,R_{+},R_{-},Q)=2\\eta\\left(\\rho(\\alpha_{+}M+\\beta_{+}R_{+})+(1-\\rho)(-\\alpha_{-}M+\\beta_{-}R_{+})-M^{2}-Q\\Delta^{m i x}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\eta^{2}\\Big(\\Delta^{m i x}+Q\\Delta^{2m i x}+M^{2}\\Delta^{m i x}-2\\big(\\rho\\Delta_{+}(\\alpha_{+}M+\\beta_{+}R_{+})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(1-\\rho\\right)\\Delta_{-}\\big(-\\alpha_{-}M+\\beta_{-}R_{+})\\big)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D ODE solutions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we first present the general solutions of the ODEs sketched in Theorem 3.4, then we specialise to the two scenarios discussed in the main text. ", "page_idx": 22}, {"type": "text", "text": "D.1 General case ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "From the previous section, we have a system of coupled ODEs for the order parameters of the form: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d M}{d t}=c_{1}+c_{2}M,}\\\\ {\\displaystyle\\frac{d R_{-}}{d t}=c_{3-}+c_{4-}M+c_{5-}R_{-},}\\\\ {\\displaystyle\\frac{d R_{+}}{d t}=c_{3+}+c_{4+}M+c_{5+}R_{+},}\\\\ {\\displaystyle\\frac{d Q}{d t}=c_{6}+c_{7}M+c_{8}M^{2}+c_{9+}R_{+}+c_{9-}R_{-}+c_{10}Q.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This represent a linear system of ODEs which can be solved using standard methods like Laplace transform, leading to Eqs. 10-12. We now report the equations including the exact expression of their coefficients. ", "page_idx": 22}, {"type": "text", "text": "M : ", "page_idx": 22}, {"type": "equation", "text": "$$\nM(t)=M_{0}e^{-t\\eta(v+\\Delta^{m i x})}+M_{\\infty}(1-e^{-t\\eta(v+\\Delta^{m i x})}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where, ", "page_idx": 22}, {"type": "equation", "text": "$$\nM_{\\infty}=\\frac{(\\rho M_{+}^{*}\\beta_{+}+(1-\\rho)M_{-}^{*}\\beta_{-})+v(\\rho\\alpha_{+}-(1-\\rho)\\alpha_{-})}{v+\\Delta^{m i x}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$R_{+}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nR_{+}(t)=R_{+}^{0}e^{-t\\eta\\Delta^{m i x}}+R_{+}^{\\infty}(1-e^{-t\\eta\\Delta^{m i x}})+k_{1+}(e^{-t\\eta\\Delta^{m i x}}-e^{-t\\eta(v+\\Delta^{m i x})}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{+}^{\\infty}=\\frac{\\displaystyle(\\rho\\beta_{+}+T_{\\pm}(1-\\rho)\\beta_{-})+M_{+}^{*}(\\rho\\alpha_{+}-(1-\\rho)\\alpha_{-}-M_{\\infty})}{\\Delta^{m i x}},}\\\\ &{k_{1+}=\\frac{\\displaystyle M_{+}^{*}(M_{\\infty}-M_{0})}{v}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$R_{-}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nR_{-}(t)=R_{-}^{0}e^{-t\\eta\\Delta^{m i x}}+R_{-}^{\\infty}(1-e^{-t\\eta\\Delta^{m i x}})+k_{1-}(e^{-t\\eta\\Delta^{m i x}}-e^{-t\\eta(v+\\Delta^{m i x})}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{-}^{\\infty}=\\frac{(T_{\\pm}\\rho\\beta_{+}+(1-\\rho)\\beta_{-})+M_{-}^{*}(\\rho\\alpha_{+}-(1-\\rho)\\alpha_{-}-M_{\\infty})}{\\Delta^{m i x}},}\\\\ &{k_{1-}=\\frac{M_{-}^{*}\\left(M_{\\infty}-M_{0}\\right)}{v}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$Q$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q(t)=Q_{0}e^{-t\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})}+Q_{\\infty}(1-e^{-t\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})})}\\\\ &{\\qquad+k_{2}\\big(e^{-t\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})}-e^{-t\\eta\\Delta^{m i x}}\\big)}\\\\ &{\\qquad+k_{3}\\big(e^{-t\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})}-e^{-t\\eta(v+\\Delta^{m i x})}\\big)}\\\\ &{\\qquad+k_{4}\\big(e^{-t\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})}-e^{-t\\eta(2v+2\\Delta^{m i x})}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where, ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ_{\\infty}=\\frac{\\eta\\Delta^{m i x}+2\\rho\\beta_{+}R_{+}^{\\infty}(1-\\eta\\Delta_{+})+2(1-\\rho)\\beta_{-}R_{-}^{\\infty}(1-\\eta\\Delta_{-})}{2\\Delta^{m i x}-\\eta\\Delta^{2m i x}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad+\\frac{M_{\\infty}(M_{\\infty}(\\eta\\Delta^{m i x}-2)+2\\rho\\alpha_{+}(1-\\eta\\Delta_{+})-2(1-\\rho)\\alpha_{-}(1-\\eta\\Delta_{-}))}{2\\Delta^{m i x}-\\eta\\Delta^{2m i x}},}\\\\ &{k_{2}=\\frac{2\\rho\\beta_{+}(1-\\eta\\Delta_{+})(R_{+}^{\\infty}-R_{+}^{0}-k_{1+})+2(1-\\rho)\\beta_{-}(1-\\eta\\Delta_{-})(R_{-}^{\\infty}-R_{-}^{0}-k_{1-})}{\\Delta^{m i x}-\\eta\\Delta^{2m i x}},}\\\\ &{k_{3}=\\frac{2\\rho\\beta_{+}(1-\\eta\\Delta_{+})k_{1+}+2(1-\\rho)\\beta_{-}(1-\\eta\\Delta_{-})k_{1-}}{\\Delta^{m i x}-\\eta\\Delta^{2m i x}+v},}\\\\ &{\\qquad\\qquad+\\frac{(M_{\\infty}-M_{0})(M_{\\infty}(\\eta\\Delta^{m i x}-2)+2\\rho\\alpha_{+}(1-\\eta\\Delta_{+})-2(1-\\rho)\\alpha_{-}(1-\\eta\\Delta_{-}))}{\\Delta^{m i x}-\\eta\\Delta^{2m i x}+v},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\nk_{4}=\\frac{(\\eta\\Delta^{m i x}-2)(M_{\\infty}-M_{0})^{2}}{\\eta\\Delta^{2m i x}+2v}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.2 Spurious correlations setting ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Under the setting discussed in the Sec. 4.1 $(\\rho=0.5,\\Delta_{+}=\\Delta_{-}=\\Delta,T_{\\pm}=1)$ ), we can make the following simplifications: ", "page_idx": 23}, {"type": "text", "text": "The equations then take the form: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M(t)=M_{0}e^{-t\\eta(v+\\Delta)}+M_{\\infty}(1-e^{-t\\eta(v+\\Delta)}),}\\\\ &{R(t)=R^{0}e^{-t\\eta\\Delta}+R^{\\infty}(1-e^{-t\\eta\\Delta})+k_{1}(e^{-t\\eta\\Delta}-e^{-t\\eta(v+\\Delta)}),}\\\\ &{Q(t)=Q_{0}e^{-t\\eta(2\\Delta-\\eta\\Delta^{2})}+Q_{\\infty}(1-e^{-t\\eta(2\\Delta-\\eta\\Delta^{2})})}\\\\ &{\\qquad\\quad+\\,k_{2}(e^{-t\\eta(2\\Delta-\\eta\\Delta^{2})}-e^{-t\\eta\\Delta})}\\\\ &{\\qquad\\quad+\\,k_{3}(e^{-t\\eta(2\\Delta-\\eta\\Delta^{2})}-e^{-t\\eta(v+\\Delta)})}\\\\ &{\\qquad\\quad+\\,k_{4}(e^{-t\\eta(2\\Delta-\\eta\\Delta^{2})}-e^{-t\\eta(2v+2\\Delta)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Where, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{\\infty}=\\frac{M^{\\star}\\beta+\\nu\\alpha}{\\nu+\\Delta},}\\\\ &{R_{\\infty}=\\frac{\\beta+M^{\\star}(\\alpha-M_{\\infty})}{\\Delta},}\\\\ &{k_{1}=\\frac{(M_{\\infty}-M_{0})}{\\nu},}\\\\ &{Q_{\\infty}=\\frac{\\eta\\Delta+2\\beta R_{\\infty}(1-\\eta\\Delta)}{2\\Delta-\\eta\\Delta^{2}}+\\frac{M_{\\infty}(M_{\\infty}(\\eta\\Delta-2)+2\\alpha(1-\\eta\\Delta))}{2\\Delta-\\eta\\Delta^{2}},}\\\\ &{k_{2}=\\frac{2\\beta(1-\\eta\\Delta)(R_{\\infty}-R_{0}-k_{1})}{\\Delta-\\eta\\Delta^{2}},}\\\\ &{k_{3}=\\frac{2\\beta(1-\\eta\\Delta)k_{1}}{\\Delta-\\eta\\Delta^{2}}+\\frac{(M_{\\infty}-M_{0})(M_{\\infty}(\\eta\\Delta-2)+2\\alpha(1-\\eta\\Delta))}{\\Delta-\\eta\\Delta^{2}+\\nu},}\\\\ &{k_{4}=\\frac{(\\eta\\Delta-\\eta\\Delta)(R_{\\infty}-R_{0})}{2\\Delta-\\eta\\Delta^{2}+2\\nu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.3 Fairness setting ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The general fairness case coincides with the general case discussed above (D.1), therefore we limit our discussion to the simplified case with centered clusters. ", "page_idx": 24}, {"type": "text", "text": "Under the zero shift $v\\,=\\,0$ , the equations take the simplified form wherein $M,v,M_{\\pm}^{\\ast}$ are 0, the transient term in $R_{\\pm}$ vanishes and $Q$ only has one transient term. Specifically: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak l_{+}(t)=R_{+}^{0}e^{-t\\eta\\Delta^{m i x}}+R_{+}^{\\infty}(1-e^{-t\\eta\\Delta^{m i x}}),}\\\\ &{\\mathfrak l_{-}(t)=R_{-}^{0}e^{-t\\eta\\Delta^{m i x}}+R_{-}^{\\infty}(1-e^{-t\\eta\\Delta^{m i x}}),}\\\\ &{\\mathcal{I}(t)=Q_{0}e^{-t\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})}+Q_{\\infty}(1-e^{-t\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})})+Q_{t r a n s}(e^{-t\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})}-e^{-t\\eta\\Delta^{m i x}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R}_{+}^{\\infty}=\\sqrt{\\frac{2}{\\pi}}\\frac{\\rho\\sqrt{\\Delta_{+}}+T_{\\pm}(1-\\rho)\\sqrt{\\Delta_{-}}}{\\Delta^{m i x}},}\\\\ &{{R}_{-}^{\\infty}=\\sqrt{\\frac{2}{\\pi}}\\frac{T_{\\pm}\\rho\\sqrt{\\Delta_{+}}+(1-\\rho)\\sqrt{\\Delta_{-}}}{\\Delta^{m i x}},}\\\\ &{{Q}_{\\infty}=\\frac{\\eta\\Delta^{m i x}+2\\sqrt{\\frac{2}{\\pi}}\\rho\\sqrt{\\Delta_{+}}R_{+}^{\\infty}(1-\\eta\\Delta_{+})+2\\sqrt{\\frac{2}{\\pi}}(1-\\rho)\\sqrt{\\Delta_{-}}R_{-}^{\\infty}(1-\\eta\\Delta_{-})}{2\\Delta^{m i x}-\\eta\\Delta^{2m i x}},}\\\\ &{{Q}_{t r a n s}=\\sqrt{\\frac{2}{\\pi}}\\frac{2\\rho\\sqrt{\\Delta_{+}}(1-\\eta\\Delta_{+})(R_{+}^{\\infty}-R_{+}^{0})+2(1-\\rho)\\sqrt{\\Delta_{-}}(1-\\eta\\Delta_{-})(R_{-}^{\\infty}-R_{-}^{0})}{\\Delta^{m i x}-\\eta\\Delta^{2m i x}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "E Deeper analysis of the learning dynamics equations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This section provides insights into the learning dynamics \u2014 particularly those relevant to bias evolution \u2014 that arise out of the expressions for order parameter evolution. We shall provide intuitive explanations behind the various mathematical terms that appear. ", "page_idx": 24}, {"type": "text", "text": "E.1 Single centered cluster ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Consider first a single cluster centered at the origin\u2013i.e. $\\rho=1,v=0$ with variance $\\Delta$ . In this setting, the minimum generalisation error is achieved when the student perfectly aligns with the teacher and optimises its norm such that $\\begin{array}{r}{Q_{o p t}=\\frac{2}{\\pi\\Delta}}\\end{array}$ , achieving the generalisation error $\\begin{array}{r}{\\epsilon_{\\operatorname*{min}}=1-\\frac{2}{\\pi}}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Importantly, this is not 0 since the student and the teacher are mismatched \u2013i.e. the student is linear whereas the teacher has a $s i g n(\\cdot)$ activation function. From the equations, we observe that the asymptotic generalisation error when training using online stochastic gradient descent in this setting is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\epsilon_{\\infty}=\\frac{1-2/\\pi}{1-\\eta\\Delta/2}=\\left(1-\\frac{2}{\\pi}\\right)\\left(1+\\frac{\\eta\\Delta}{2}+O(\\eta^{2}\\Delta^{2})\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, as the learning rate increases, the generalisation error increases until it reaches the critical learning rate beyond which training is unstable and the loss grows unboundedly. In the single cluster case, Eq. E.54 this is $2/\\Delta$ which matches the classical result from convex optimisation [23]. We can similarly find the critical learning rate for two clusters to be $2\\Delta^{m i x}/\\Delta^{2m i x}$ by ensuring exponential terms decay to zero in equation 12. ", "page_idx": 24}, {"type": "text", "text": "E.2 Analysis of teacher alignment $(\\tau_{R})$ and student magnitude $(\\tau_{Q})$ timescales ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We now consider the fairness setting with zero shift as illustrated in Fig. 1c. As discussed in section 4.2, the relevant timescales in this setting are ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tau_{\\cal R}=\\frac{1}{\\eta\\Delta^{m i x}},\\qquad\\tau_{\\cal Q}=\\frac{1}{\\eta(2\\Delta^{m i x}-\\eta\\Delta^{2m i x})},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "since $M(t)$ is always zero. Fig. 7 shows the crossing phenomena of the loss curves along with the order parameter evolution and other insightful terms. The alignment of the student is governed by the timescale $\\tau_{R}$ and the change in its magnitude is governed by the timescale $\\tau_{Q}$ . Initially, the classifier has a small magnitude and its alignment roughly matches the two teachers which are themselves quite similar $T_{\\pm}=0.9)$ ). Indeed, we see that the $R_{+}$ and $R_{-}$ have very similar trajectories. However, smaller magnitudes advantage higher variances as discussed in Appendix E.1 $\\mathcal{Q}_{o p t}$ is inversely proportional to the cluster variance). ", "page_idx": 24}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/09bb7c89c22af04d8e717d85c8feeb35591ede4e8d7a6444a12056b71a817c21.jpg", "img_caption": ["Figure 7: The Crossing Phenomenon The left shows the \u2018crossing\u2019 of the loss curves on the negative sub-population in red (higher variance and lower representation) and positive sub-population in blue (lower variance but greater representation) along with the overall loss in purple obtained as a weighted average of the two. It also marks $\\tau_{R}$ as the dashed vertical line and $\\tau_{Q}$ as the dotted vertical line. The right side shows the evolution of the order parameters and a transient term. The horizontal blue and red dash-dotted line mark the optimal value of Q for the positive-subpopulation and negative sub-populations respectively. The parameters are $v=0,\\rho=\\bar{0}.8,\\Delta_{+}=\\bar{0}.\\bar{1},\\Delta_{-}=$ $1,T_{\\pm}=0.9,\\eta=0.1$ . "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "We mark the optimal values of $Q$ using horizontal lines in Fig.7 on the left side with blue for the positive sub-population (lower variance) and red for the negative sub-population (higher variance). As the magnitude of the student grows, we observe a sharp drop in the generalisation error on the higher variance sub-population till $Q$ crosses the horizontal red line. Beyond this point, the generalisation error on the higher variance sub-population rises since the magnitude of the student has exceeded the optimal value (horizontal red line) and the generalisation error on the lower variance sub-population continues to fall as the magnitude of the student approaches the horizontal blue line. Finally, an inspection of the timescales reveals that $\\tau_{Q}$ (vertical dotted line) is less than $t_{R}$ (vertical dashed line) and hence we may expect the student magnitude to saturate before its alignment. However, $Q_{t r a n s}$ , the transient term associated with $Q$ (third line of equation 12), is always negative and hence suppresses the growth of $Q$ initially. ", "page_idx": 25}, {"type": "text", "text": "In summary, we observe a two phase behaviour. First the student shifts its alignment and increases magnitude leading to a sharper drop in the higher variance generalisation error. Second, we observe that as the student continues increasing magnitude while keeping its alignment fixed, it advantages the lower variance cluster. ", "page_idx": 25}, {"type": "text", "text": "E.3 Asymptotic preference ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This section discusses the asymptotic generalisation errors of our classifier when $v=0$ as a function of representation and variances. Firstly, as discussed in section 4.2, ", "page_idx": 25}, {"type": "equation", "text": "$$\nR_{+}^{\\infty}>R_{-}^{\\infty}\\iff\\rho\\sqrt{\\Delta_{+}}>(1-\\rho)\\sqrt{\\Delta_{-}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Intuitively, one might expect that the asymptotically lower generalisation error is achieved on the population whose teacher has better asymptotic alignment with the student. Indeed, when the learning rate tends to 0, we observe exactly this as illustrated by the two dark phases in Fig. 8 on the left side. However, when the learning rate is greater than zero, we observe more complex behaviour. Fig. 8 (right) shows the emergence two new phases (light red and light blue) wherein the classifier exhibits higher generalisation error on a sub-population despite having better alignment with its corresponding teacher. This behaviour can be traced back to equation E.54 wherein the increase in asymptotic generalisation error due to non-zero learning rates is amplified by the cluster variance. ", "page_idx": 25}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/11c26726ba1e806f65433bda3ba53de7b96498ff9b016c4fd3675f63334034bb.jpg", "img_caption": ["Figure 8: Initial and Asymptotic student preferences We set $v=0,\\Delta_{+}=1,T_{\\pm}=0.9,\\eta=0.1$ and study the values of $\\rho,\\Delta_{-}$ . The figure studies only asymptotic preferences under $v=0,\\Delta_{+}=$ $1,T_{\\pm}=0.9$ . When the learning rate is small $(\\eta\\to\\bar{0}^{+}$ on left side), the cluster which has better alignment with the teacher must also have lower generalisation error. However, for non-zero learning rates $\\eta=0.1$ on right side), behaviour is more complicated leading to the light colored phases where despite better asymptotic alignment with the teacher, the generalisation error is higher. Parameters: $\\eta\\rightarrow0^{+}$ (left) vs $\\eta=0.1$ (right). "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Thus, our analysis shows how a large learning rate can also become a source of bias in our classifier by advantaging the sub-population with smaller variance. ", "page_idx": 26}, {"type": "text", "text": "F Additional numerical simulations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "F.1 CelebA", "page_idx": 26}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/cbbce4a1d24d4b165d936c1ed0919d292bb4955dbca7ce41cd52cc81e24e2808.jpg", "img_caption": ["", "", ""], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 9: Numerical simulations in the CelebA dataset. Figure shows the average accuracy (solid lines) and standard deviation (shaded area) of 4 different runs in this framework. The top row depicts the test accuracy over the course of training for different pairs of target and group attributes. The bottom row illustrates the difference in test accuracies between the $^+$ and \u2212subpopulations, highlighting the crossing phenomenon observed during training. Panels (a), $(b)$ , and (c) depict this for the pairs of target and group attributes of (Eye glass, Bags under eyes), (Bangs, Blurry), and (Young, Blond Hair), respectively. ", "page_idx": 26}, {"type": "text", "text": "The goal of this experiment is to show the emergence of different timescales in realist scenarios of relevance for the fairness literature. ", "page_idx": 26}, {"type": "text", "text": "The CelebA dataset [27] contains over $200\\mathbf{k}$ celebrity images annotated with 40 attribute labels, covering a wide range of facial attributes such as gender, age, and expressions. For this experiment, we consider different pairs as the target and group attributes. The task is to predict the target attribute while the group attribute defines the $^+$ and \u2212subpopulations. ", "page_idx": 27}, {"type": "text", "text": "For the model, we select a pretrained ResNet-18 model on ImageNet and add an additional fully connected layer, with only the latter being optimised during training. We use cross-entropy as the loss objective and train via online SGD. ", "page_idx": 27}, {"type": "text", "text": "We randomly selected target-label pairs, making sure to avoid attributes that are pathologically underrepresented in the dataset and would hinder the significance of the result. In the plots shown in Fig. 9 we show some of the pairs that show a crossing phenomenon. Each panel in Fig. 9 show the accuracy and accuracy gap over the course of training. Notice how the classifier favours sub-population \u2212in the initial phase of training before changing preference. ", "page_idx": 27}, {"type": "text", "text": "This result shows that bias can change over the course of training even in standard setting. This does not imply that it will always occur and indeed several of the pairs in the dataset do not show a crossing phenomenon. However, understanding when and why this phenomenon occurs can affect the algorithmic choices that we make in our ML pipeline. ", "page_idx": 27}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/fbf0fa9fb1b584fc996ffea4ded3ea94b9c69e3d676e632484387a8e76a9a075.jpg", "img_caption": ["Figure 10: Synthetic Data Simulation with alternate Training Protocols We observe the \u2018doublecrossing\u2019 phenomena in not only the loss curves, but also the error curves for the positive subpopulation (blue) and the negative sub-population (red) (left). The shaded areas quantify the standard deviation obtained across 10 seeds. We observe similar behavior when using Adam (middle) and weight decay (right). The data distribution parameters are $d\\;=\\;100,v\\;=\\;4,\\rho\\;=\\;0.7,\\Delta_{+}\\;=\\;$ $0.1,\\Delta_{-}=1,T_{\\pm}=0.9,\\eta=0.01,\\alpha_{+}=0.471,\\alpha_{-}=-0.188$ "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "In this section we test the validity of the prediction of our model across various networks and training protocols. We consider a data distribution with parameters as detailed in the caption of Fig. 10. We train a 2-layer MLP with ReLU activation in the hidden layer(s) and sigmoidal activation at the output with 128 neurons in the hidden layer with online SGD using BCE loss. We refer to this as the \u2018standard configuration\u2019 for further comparisons. We sample training and test data from the data distribution and use the test data to obtain estimates of the loss as well as error rates (percentage of test examples misclassified). We visualize these in Fig. 10 on the left. ", "page_idx": 28}, {"type": "text", "text": "We observe the three phase behaviour predicted by our model. The positive sub-population is initially advantaged more since it exhibits stronger spurious correlation. Then, the negative sub-population is advantaged since it has a higher variance. Finally, as per Eq. 16, the positivesub-population is advantaged once more since it has sufficiently high representation. We not only observe the \u2018double-crossing\u2019 phenomena in the losses, but also in the test errors demonstrating the robustness of our model beyond the linearity and MSE loss assumptions. ", "page_idx": 28}, {"type": "text", "text": "We also observe similar behavior using Adam optimization in Fig. 10 (center). We note that we had to use a smaller learning rate of 0.001 to keep training stable since Adam leads to faster optimization manifesting in a higher effective learning rate than SGD. ", "page_idx": 28}, {"type": "text", "text": "When we train using a weight decay penalty of 0.1 (Fig. 10 right), we observe that asymptotically the higher variance cluster is now preferred. This behaviour can be explained using the theoretical model. As discussed in Appendix E, smaller student magnitude advantages the higher variance group and weight decay encourages smaller student weights leading to an asymptotic preference for the negative sub-population. ", "page_idx": 28}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/d32620a6fe824d2fa6723ec7d92bb5683ef76a88e13545c0d7b386988c9ba997.jpg", "img_caption": ["Figure 11: Ablations with Deeper Networks We observe the \u2018double-crossing\u2019 phenomena across deeper networks as well. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "We now train deeper networks with 2 to 5 layers and visualise results in Fig. 11. The \u2018double-crossing\u2019 phenomena persists across deeper networks. We also train wider networks with 2, 16, 128 and 1024 units in the hidden layer and visualise results in Fig. 12. The \u2018double-crossing\u2019 phenomena persists ", "page_idx": 28}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/df47f3bfc488f53f35502581bf94fcd0e1a609f22ace00a359204f0b3e01f017.jpg", "img_caption": ["Figure 12: Ablations with Wider Networks We observe the \u2018double-crossing\u2019 phenomena across wider networks as well. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "QUYLbzwtTV/tmp/36a24df86e84359c485a011e89900f9d610e03e857246bcbefb95048bafa468a.jpg", "img_caption": ["Figure 13: Ablations across Learning Rates Larger learning rates can lead to instability (left). If training is stable however, we observe the \u2018crossing\u2019 phenomena as usual, just at different time scales due to different speeds of training. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "across wider networks. ", "page_idx": 29}, {"type": "text", "text": "Finally, we vary the learning rate as well from our standard configuration and note that when the learning rate is too high, the training is unstable for the higher variance cluster (Fig. 13 left). For stable training, however, we observe the \u2018double-crossing\u2019 phenomena as usual, just at longer timescales for slower learning rates as predicted by our theory. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: the model is introduced in Sec. 2, analytical characterisation in provided in Sec. 3, their analysis are provided in Sec. 4, and implications are provided in Sec. 5. The appendix complements these sections. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Limits of the applicability of the discussed in the assumptions of the model, Sec. 2, and additional limitations are reported in the conclusions Sec. 6. Furthermore, we comment on how these limitations may be conservative by performing numerical experiments in settings that violate the assumptions of the model. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Proofs and derivations are reported in the appendix for space constraint in Appendix B, C and D.1. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Yes, the derivation is provided as stated in the previous questions and all the parameters to reproduce our results are reported in the text and/or the caption of the figures. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide code in the SM to simulate the dynamics in the TM model and reproduce the analytical solution. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Yes, see Sec. 5 and Appendix F. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The variance and number of seeds is reported in the text/caption of the figures.   \nWe use shaded area around line to indicate the standard deviation (instead of error bars). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: All the simulations were performed on personal laptops. Computational resources needed to reproduce our results are quite available. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Discussion reported in the conclusion Sec. 6. This paper focuses on theoretical research, however we believe that the phenomena observed and characterised in our framework extend to a broader class of classifiers and datasets, potentially leading to a redesign of aspects of the ML pipeline in order to account for bias as a dynamical quantity. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: literature relevant for this work is properly cited, mostly in Sec. 1. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA]   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}]