[{"type": "text", "text": "NRGBoost: Energy-Based Generative Boosted Trees ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Despite the rise to dominance of deep learning in unstructured data domains, tree  \n2 based methods such as Random Forests (RF) and Gradient Boosted Decision Trees   \n3 (GBDT) are still the workhorses for handling discriminative tasks on tabular data.   \n4 We explore generative extensions of these popular algorithms with a focus on   \n5 explicitly modeling the data density (up to a normalization constant), thus enabling   \n6 other applications besides sampling. As our main contribution we propose an   \n7 effective energy-based generative boosting algorithm that is analogous to the second   \n8 order boosting algorithm implemented in popular packages like XGBoost. We   \n9 show that, despite producing a generative model capable of handling inference tasks   \n10 over any input variable, our proposed algorithm can achieve similar discriminative   \n11 performance to GBDT algorithms on a number of real world tabular datasets and   \n12 outperform competing approaches for sampling. ", "page_idx": 0}, {"type": "text", "text": "13 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "14 Generative models have achieved tremendous success in computer vision and natural language   \n15 processing, where the ability to generate synthetic data guided by user prompts opens up many   \n16 exciting possibilities. While generating synthetic table records does not necessarily enjoy the same   \n17 wide appeal, this problem has still received considerable attention as a potential avenue for bypassing   \n18 privacy concerns when sharing data. Estimating the data density, $p(\\mathbf x)$ , is another typical application   \n19 of generative models which enables a host of different use cases that can be particularly interesting   \n20 for tabular data. Unlike discriminative models which are trained to perform inference over a single   \n21 target variable, density models can be used more flexibly for inference over different variables or for   \n22 out of distribution detection. They can also handle inference with missing data in a principled way by   \n23 marginalizing over unobserved variables.   \n24 The development of generative models for tabular data has mirrored its progression in computer   \n25 vision with many of its Deep Learning (DL) approaches being adapted to the tabular domain [Jordon   \n26 et al., 2018, Xu et al., 2019, Engelmann and Lessmann, 2020, Fan et al., 2020, Zhao et al., 2021,   \n27 Kotelnikov et al., 2022]. Unfortunately, these methods are only useful for sampling as they either   \n28 don\u2019t model the density explicitly or can\u2019t evaluate it due to untractable marginalization over high   \n29 dimensional latent variable spaces. Furthermore, despite growing in popularity, DL has still failed to   \n30 displace tree-based ensemble methods as the tool of choice for handling tabular discriminative tasks   \n31 with gradient boosting still being found to outperform neural-network-based methods in many real   \n32 world datasets [Grinsztajn et al., 2022, Borisov et al., 2022a].   \n33 While there have been recent efforts to extend the success of tree-based models to generative modeling   \n34 [Correia et al., 2020, Wen and Hang, 2022, Nock and Guillame-Bert, 2022, Watson et al., 2023,   \n35 Nock and Guillame-Bert, 2023, Jolicoeur-Martineau et al., 2023], we find that direct extensions of   \n36 Random Forests (RF) and Gradient Boosted Decision Tree (GBDT) are still missing. It is this gap   \n37 that we try to address, seeking to keep the general algorithmic structure of these popular algorithms   \n38 but replacing the optimization of their discriminative objective with a generative counterpart. Our   \n39 main contributions in this regard are:   \n40 \u2022 Proposing NRGBoost, a novel energy-based generative boosting model that, analogously to   \n41 the boosting algorithms implemented in popular GBDT packages, is trained to maximize a   \n42 local second order approximation to the likelihood at each boosting round.   \n43 \u2022 Proposing an approximate sampling algorithm to speed up the training of any tree-based   \n44 multiplicative generative boosting model.   \n45 \u2022 Exploring the use of bagged ensembles of Density Estimation Trees (DET) [Ram and Gray,   \n46 2011] with feature subsampling as the generative counterpart to RF.   \n47 The longstanding popularity of GBDT models in machine learning practice can, in part, be attributed   \n48 to the strength of its empirical results and the efficiency of its existing implementations. We therefore   \n49 focus on an experimental evaluation in real world datasets spanning a range of use cases, number   \n50 of samples and features. We find that, on smaller datasets, our implementation of NRGBoost can   \n51 be trained in a few minutes on a mid-range consumer CPU and achieve similar discriminative   \n52 performance to a standard GBDT model while also being able to generate samples that are generally   \n53 harder to distinguish from real data than state of the art neural-network-based models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "ojFEP11Cqj/tmp/cf86a4b040cd19ceabff700a1d6612dc3f72c87f0bde0da2e7cdaf4d5cc33ebe.jpg", "img_caption": ["Figure 1: Downsampled MNIST samples generated by NRGBoost and two tabular DL methods. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "54 2 Energy Based Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "55 An Energy-Based Model (EBM) parametrizes the logarithm of a probability density function directly   \n56 (up to an unspecified normalizing constant): ", "page_idx": 1}, {"type": "equation", "text": "$$\nq_{f}(\\mathbf{x})=\\frac{\\exp\\left(f(\\mathbf{x})\\right)}{Z[f]}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "57 Here $f(\\mathbf{x})\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\mathbb{R}$ is a real function over the input domain.1 We will avoid introducing any   \n58 parametrization, instead treating the function $f\\in{\\mathcal{F}}({\\mathcal{X}})$ lying in an appropriate function space over   \n59 the input space as our model parameter directly. $\\begin{array}{r}{Z[\\dot{f}]=\\dot{\\sum_{\\mathbf{x}\\in\\mathcal{X}}}\\exp\\bar{(f(\\mathbf{x}))}}\\end{array}$ , known as the partition   \n60 function, is then a functional of $f$ giving us the necessary  normalizing constant.   \n61 This is the most flexible way one could represent a probability density function making essentially   \n62 no compromises on its structure. The downside to this is that for most interesting choices of $\\mathcal{F}$ ,   \n63 computing or estimating this normalizing constant is untractable which makes training these models   \n64 difficult. Their unnormalized nature however does not prevent EBMs from being useful in a number   \n65 of applications besides sampling. Performing inference over a small enough subset of variables   \n66 requires only normalizing over the set of their possible values and for anomaly or out of distribution   \n67 detection, knowledge of the normalizing constant is not necessary.   \n68 One common way to train an energy-based model to approximate a data generating distribution, $p(\\mathbf x)$ ,   \n69 is to minimize the Kullback-Leibler divergence between $p$ and $q_{f}$ , or equivalently, maximize the   \n70 expected log likelihood functional: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nL[f]=\\mathbb{E}_{\\mathbf{x}\\sim p}\\log q_{f}(\\mathbf{x})=\\mathbb{E}_{\\mathbf{x}\\sim p}f(\\mathbf{x})-\\log Z[f]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "71 This optimization is typically carried out by gradient descent over the parameters of $f$ , but due to   \n72 the untractability of the partition function, one must rely on Markov Chain Monte Carlo (MCMC)   \n73 sampling to estimate the gradients [Song and Kingma, 2021]. ", "page_idx": 2}, {"type": "text", "text": "74 3 NRGBoost ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "75 Expanding the increase in log-likelihood in equation 2 due to a variation $\\delta f$ around an energy function   \n76 $f$ up to second order we have ", "page_idx": 2}, {"type": "equation", "text": "$$\nL[f+\\delta f]-L[f]\\approx\\mathbb{E}_{\\mathbf{x}\\sim p}\\delta f(\\mathbf{x})-\\mathbb{E}_{\\mathbf{x}\\sim q_{f}}\\delta f(\\mathbf{x})-\\frac{1}{2}\\mathrm{Var}_{\\mathbf{x}\\sim q_{f}}\\delta f(\\mathbf{x})=:\\Delta L_{f}[\\delta f]\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "77 The $\\delta f$ that maximizes this quadratic approximation should thus have a large positive difference   \n78 between the expected value under the data and under $q_{f}$ while having low variance under $q_{f}$ . We   \n79 note that just like the original log-likelihood, this Taylor expansion is invariant to adding an overall   \n80 constant to $\\delta f$ . This means that, in maximizing equation 3 we can consider only functions that have   \n81 zero expectation under $q_{f}$ in which case we can simplify $\\Delta L_{f}[\\delta f]$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta L_{f}[\\delta f]=\\mathbb{E}_{\\mathbf{x}\\sim p}\\delta f(\\mathbf{x})-\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}\\sim q_{f}}\\delta f^{2}(\\mathbf{x})\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "82 We thus formulate our boosting algorithm as modelling the data density with an additive energy   \n83 function. At each boosting iteration we improve upon the current energy function $f_{t}$ by finding an   \n84 optimal step $\\delta f_{t}^{*}$ that maximizes $\\Delta L_{f_{t}}[\\delta f]$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta f_{t}^{*}=\\arg\\operatorname*{max}_{\\delta f\\in\\mathcal{H}_{t}}\\Delta L_{f_{t}}[\\delta f]\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "85 where $\\mathcal{H}_{t}$ is an appropriate space of functions (satisfying $\\mathbb{E}_{\\mathbf{x}\\sim q_{f_{t}}}\\delta f(\\mathbf{x})=0$ if equation 4 is used).   \n86 The solution to this problem can be interpreted as a Newton step in the space of energy functions.   \n87 Because for an energy-based model, the Fisher Information matrix with respect to the energy function   \n88 and the hessian of the expected log-likelihood are the same, we can also interpret the solution to   \n89 equation 5 as a natural gradient step (see the Appendix A). This approach is essentially analogous   \n90 to the second order step implemented in modern discriminative gradient boosting libraries such as   \n91 XGBoost [Chen and Guestrin, 2016] and LightGBM [Ke et al., 2017] and which can be traced back   \n92 to Friedman et al. [2000].   \n93 In updating the current iterate, $f_{t+1}=f_{t}+\\alpha_{t}\\cdot\\delta f_{t}^{*}$ , we scale $\\delta f_{t}^{*}$ by an additional scalar step-size   \n94 $\\alpha_{t}$ . This can be interpreted as a globalization strategy to account for the fact that the quadratic   \n95 approximation in equation 3 is not necessarily valid over large steps in function space. A common   \n96 strategy in nonlinear optimization would be to select $\\alpha_{t}$ via a line search based on the original   \n97 log-likelihood. Common practice in discriminative boosting however is to interpret this step size   \n98 as a regularization parameter and to select a fixed value in $]0,1]$ with (more) smaller steps typically   \n99 outperforming fewer larger ones when it comes to generalization. We choose to adopt a hybrid   \n100 strategy, first selecting an optimal step size by line search and then shrinking it by a fixed factor. We   \n101 find that this typically accelerates convergence allowing the algorithm to take comparatively larger   \n102 steps that increase the likelihood in the initial phase of boosting. For a starting point, $f_{0}$ , we can   \n103 choose the logarithm of any probability distribution over $\\mathcal{X}$ as long as it is easy to evaluate. Sensible   \n104 choices are a uniform distribution (i.e., $f\\equiv0$ ), the product of marginals for the training set, or any   \n105 mixture distribution between these two. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "106 3.1 Weak Learners ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "107 As a weak learner we will consider functions defined by trees over the input space. I.e., letting   \n108 $\\textstyle\\bigcup_{j=1}^{J}X_{j}={\\mathcal{X}}$ be the partitioning of the input space induced by the leaves of a binary tree whose   \n109 internal nodes represent a split along one dimension into two disjoint partitions, we take as $\\mathcal{H}$ the set   \n110 of functions such as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta f({\\bf x})=\\sum_{j=1}^{J}w_{j}{\\bf1}_{X_{j}}({\\bf x})\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "111 where ${\\mathbf{1}}_{X}$ denotes the indicator function of a subset $X$ and $w_{j}$ are values associated with each   \n112 leaf $j~\\in~[1..J]$ . In a standard decision tree these values would typically encode an estimate of   \n113 $p(\\boldsymbol{y}|\\mathbf{x}\\in X_{j})$ , with $y$ being a special target variable that is never considered for splitting. In our   \n114 generative approach they encode unconditional densities (or more precisely energies) over each leaf\u2019s   \n115 support and every variable can be used for splitting. Note that our functions $\\delta f$ are thus parametrized   \n116 by the values $w_{j}$ as well the structure of the tree and the variables and values for the split at each   \n117 node which ultimately determine the $X_{j}$ . We omit these dependencies for brevity.   \n118 Replacing the definition in equation 6 in our objective (equation 4) we get the following optimization   \n119 problem to find the optimal decision tree: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{w_{1},\\ldots,w_{J},X_{1},\\ldots,X_{J}}}&{~\\displaystyle\\sum_{j=1}^{J}\\left(w_{j}P(X_{j})-\\frac{1}{2}w_{j}^{2}Q_{f}(X_{j})\\right)}\\\\ {\\mathrm{s.t.}}&{~\\displaystyle\\sum_{j=1}^{J}w_{j}Q_{f}(X_{j})=0\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "120 where $P(X_{j})$ and $Q_{f}(X_{j})$ denote the probability of the event $\\mathbf{x}\\in X_{j}$ under the respective distribution   \n121 and the constraint ensures that $\\delta f$ has zero expectation under $q_{f}$ . With respect to the leaf weights this   \n122 is a quadratic program whose optimal solution and objective values are respectively given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{j}^{*}=\\frac{P(X_{j})}{Q_{f}(X_{j})}-1\\,,\\qquad\\qquad\\Delta L_{f}^{*}\\left(X_{1},\\ldots,X_{J}\\right)=\\frac{1}{2}\\left(\\sum_{j=1}^{J}\\frac{P^{2}(X_{j})}{Q_{f}(X_{j})}-1\\right)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "123 Because carrying out the maximization of this optimal value over the tree structure that determines   \n124 the $X_{j}$ is hard, we approximate its solution by greedily growing a tree that maximizes it when   \n125 considering how to split each node individually. A parent leaf with support $X_{P}$ is thus split into 2   \n126 child leaves, with disjoint support, $X_{L}\\cup X_{R}=X_{P}$ , so as to maximize over all possible partitionings   \n127 along a single dimension, $\\mathcal{P}\\left(X_{P}\\right)$ , the following objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{X_{L},X_{R}\\in{\\mathcal{P}}(X_{P})}{\\frac{P^{2}(X_{L})}{Q_{f}(X_{L})}}+{\\frac{P^{2}(X_{R})}{Q_{f}(X_{R})}}-{\\frac{P^{2}(X_{P})}{Q_{f}(X_{P})}}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "128 Note that when using parametric weak learners, computing a second order step would typically   \n129 involve solving a linear system with a full Hessian. As we can see, this is not the case when the   \n130 weak learners are decision trees where the optimal value to assign to a leaf $j$ does not depend on   \n131 any information from other leaves and, likewise, the optimal objective value is a sum of terms, each   \n132 depending only on information from a single leaf. This would have not been the case had we tried to   \n133 optimize the likelihood functional in Equation 2 directly instead of its quadratic approximation. ", "page_idx": 3}, {"type": "text", "text": "134 3.2 Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "135 To compute the leaf values in equation 8 and the splitting criterion in equation 9 we would have to   \n136 know $P(X)$ and be able to compute $Q_{f}(X)$ which is infeasible due to the untractable normalization   \n137 constant. We therefore estimate these quantities, with recourse to empirical data for $P(X)$ , and to   \n138 samples approximately drawn from the model with MCMC. Because even if the input space is not   \n139 partially discrete, $f$ is still discontinuous and constant almost everywhere we can\u2019t use gradient based   \n140 samplers and therefore rely on Gibbs sampling instead. This only requires evaluating each $f_{t}$ along   \n141 one dimension at a time, while keeping all others fixed which can be computed efficiently for a tree   \n142 by traversing it only once. However, since at boosting iteration $t$ our energy function is a sum of $t$   \n143 trees, this computation scales linearly with the iteration number. This makes the overall time spent   \n144 sampling quadratic in the number of iterations and thus precludes us from training models with a   \n145 large number of trees.   \n146 In order to reduce the burden associated with this sampling, which can dominate the runtime of   \n147 training the model, we propose a new sampling approach that leverages the cumulative nature of   \n148 boosting. The intuition behind this approach is that the set of samples used in the previous boosting   \n149 round are (approximately) drawn from a distribution that is already close to the new model distribution.   \n150 It could therefore be helpful to keep some of those samples, especially those that conform the best to   \n151 the new model. Rejection sampling allows us to do just that. The boosting update in terms of the   \n152 densities takes the following multiplicative form: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nq_{t}(\\mathbf{x})=k_{t}\\,q_{t-1}(\\mathbf{x})\\exp\\left(\\alpha_{t}\\delta f_{t}(\\mathbf{x})\\right)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "153 Here, $k$ is an unknown multiplicative constant and since $\\delta f_{t}$ is given by a tree, we can easily bound   \n154 the exponential factor by finding the leaf with the largest value. We can therefore use the previous   \n155 model, $q_{t-1}(\\mathbf{x})$ , as a proposal distribution for which we already have a set of samples and keep each   \n156 sample, $\\mathbf{x}$ , with an acceptance probability of: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{a c c e p t}(\\mathbf{x})=\\exp\\left[\\alpha_{t}\\left(\\delta f_{t}(\\mathbf{x})-\\underset{\\mathbf{x}}{\\operatorname*{max}}\\,\\delta f_{t}(\\mathbf{x})\\right)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "157 We note that knowledge of the constant $k_{t}$ is not necessary to compute this acceptance probability.   \n158 After removing samples from the pool, we can use Gibbs sampling to draw a new set of samples in   \n159 order to keep a fixed total number of samples per round of boosting. Note also that $q_{0}$ is typically a   \n160 simple model for which we can both directly evaluate the desired quantities (i.e., $Q_{0}(X)$ for a given   \n161 partition $X$ ) and cheaply draw exact samples from. As such, no sampling is required for the first   \n162 iteration of boosting and for the second we can draw exact samples from $q_{1}$ with rejection sampling   \n163 using $q_{0}$ as a proposal distribution.   \n164 This approach works better when either the range of $f_{t}$ is small or when the step sizes $\\alpha_{t}$ are small as   \n165 this leads to larger acceptance probabilities. Note that in practice it can be helpful to independently   \n166 refresh a fixed fraction samples, $p_{r e f r e s h}$ , at each round of boosting in order to encourage more   \n167 diverse samples between rounds. This can be accomplished by keeping each sample with a probability   \n168 $p_{a c c e p t}(\\mathbf{x})(\\bar{1}-p_{r e f r e s h})$ instead. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "169 3.3 Regularization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "170 The simplest way to regularize a boosting model is to stop training when overfitting is detected by   \n171 monitoring a suitable performance metric on a validation set. For NRGBoost this could be the increase   \n172 in log-likelihood at each boosting round. However, estimating this quantity would require drawing   \n173 additional validation samples from the model (see Appendix A). An alternative viable validation   \n174 strategy which needs no additional samples is to simply monitor a discriminative performance metric   \n175 (over one or more variables). This essentially amounts to monitoring the quality of $\\bar{q}_{f}(x_{i}|\\mathbf{x}_{-i})$ instead   \n176 of the full $q_{f}(\\mathbf{x})$ .   \n177 Besides early stopping, the decision trees themselves can be regularized by limiting the depth or total   \n178 number of leaves of each tree. Additionally we can rely on other strategies such as disregarding splits   \n179 that would result in a leaf with too little training data, $P(X)$ , model data, $Q_{f}(X)$ , volume $V(X)$ or   \n180 too high of a ratio between training and model data $P(X){\\big/}Q_{f}(X)$ . We found the latter to be the most   \n181 effective of these, not only yielding better generalization performance than other approaches, but also   \n182 having the added benefit of allowing us to lower bound the acceptance probability of our rejection   \n183 sampling scheme. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "184 4 Density Estimation Trees and Density Estimation Forests ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "185 Density Estimation Trees (DET) were proposed by Ram and Gray [2011] as an alternative to   \n186 histograms and kernel density estimation but have received little attention as generative models   \n188 support of each leaf in a bi , $\\begin{array}{r}{q=\\sum_{j=1}^{J}\\frac{\\hat{P}(X_{j})}{V(X_{j})}{\\bf1}_{X_{j}}}\\end{array}$ , with ${\\hat{P}}(X)$ ing an empirical estimate   \n189 of probability of the event $\\mathbf{x}\\in X$ and $V(X)$ denoting the volume of $X$ . Note that it is possible   \n190 to draw an exact sample from this type of model by randomly selecting a leaf, $j\\,\\in\\,[1..J]$ , given   \n191 probabilities ${\\hat{P}}(X_{j})$ , and then drawing a sample from a uniform distribution over $X_{j}$ .   \n192 To fti a DET, Ram and Gray [2011] propose optimizing the Integrated Squared Error (ISE) between the   \n193 data and model distributions which, following a similar approach to Section 3.1, leads the following   \n194 optimization problem when considering how to split a leaf node: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{X_{L},X_{R}\\in\\mathcal{P}(X_{P})}D(P(X_{L}),V(X_{L}))+D(P(X_{R}),V(X_{R}))-D(P(X_{P}),V(X_{P}))\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "195 For the ISE, $D$ should be taken as the function $D_{I S E}(P,V)=P^{2}/V$ which leads to a similar splitting   \n196 criterion to Equation 12 but replacing the previous model\u2019s distribution with the volume measure $V$   \n197 which can be interpreted as the uniform distribution on $\\mathcal{X}$ (up to a multiplicative constant).   \n198 Maximum Likelihood Often generative models are trained to maximize the likelihood of the   \n199 observed data. This was left for future work in Ram and Gray [2011] but, as we show in Appendix   \n200 B, can be accomplished by replacing the $D$ in Equation 12 with $D_{K L}(P,V)=P\\,\\log\\left({P\\bar{/}\\bar{V}}\\right)$ .This   \n201 choice of minimization criterion can be seen as analogous to the choice between Gini impurity and   \n202 Shannon entropy in the computation of the information gain in decision trees.   \n203 Bagging and Feature Subsampling Following the common approach in decision trees, Ram and   \n204 Gray [2011] suggest the use of pruning for regularization of DET models. Practice has however   \n205 evolved to prefer bagging as a form of regularization rather than relying on single decision trees. We   \n206 employ same principle to DETs by ftiting many trees on bootstrap samples of the data. We also adopt   \n207 the common practice from Random Forests of randomly sampling a subset of features to consider   \n208 when splitting any leaf node in order to encourage independence between the different trees in the   \n209 ensemble. The ensemble model, which we call Density Estimation Forests (DEF) in the sequence,   \n210 is thus an additive mixture of DETs with uniform weights, therefore still allowing for normalized   \n211 density computation and exact sampling. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "212 5 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "213 Generative Boosting Most prior work on generative boosting focuses on unstructured data and   \n214 the use of parametric weak learners and is split between two approaches: (i) Additive methods that   \n215 model the density function as an additive mixture of weak learners such as Rosset and Segal [2002],   \n216 Tolstikhin et al. [2017]. (ii) Those that take a multiplicative approach modeling the density function as   \n217 an unnormalized product of weak learners. The latter is equivalent to the energy based approach that   \n218 writes the energy function (log density) as an additive sum of weak learners. Welling et al. [2002] in   \n219 particular also approach boosting from the point of view of functional optimization of the likelihood   \n220 or the logistic loss of an energy-based model. However, they rely on a first order local approximation   \n221 of the objective since they focus on parametric weak learners such as restricted boltzman machines   \n222 for which a second order step would be impractical.   \n223 Greedy Multiplicative Boosting Another more direct multiplicative boosting framework was first   \n224 proposed by Tu [2007]. At each boosting round a discriminative classifier is trained to distinguish   \n225 between empirical data and data generated by the current model by estimating the likelihood ratio   \n226 $p(\\mathbf{x})/_{q_{t}(\\mathbf{x})}$ . This estimated ratio is used as a direct multiplicative factor to update the current model   \n227 $q_{t}$ (after being raised to an appropriate step size). In ideal conditions this greedy procedure would   \n228 converge in a single iteration if a step size of 1 would be used. While Tu [2007] does not prescribe a   \n229 particular choice of classifier to use, Grover and Ermon [2017] proposes a similar concept where the   \n230 ratio is estimated based on an adversarial bound for an $f$ -divergence and Cranko and Nock [2019]   \n231 provides additional analysis on this method. In Appendix C we dive deeper into the differences   \n232 between NRGBoost and this approach when it is adapted to use trees as weak learners. We note, how  \n233 ever, that the main difference is that NRGBoost attempts to update the current density proportionally   \n234 to an exponential of the ratio, $\\exp{\\left(\\alpha_{t}\\cdot p(x)\\middle/\\right.}q_{t}(x)\\right)$ , instead of the ratio directly.   \n235 Tree-Based Density Modelling Other authors have proposed tree-based density models similar to   \n236 DET [Nock and Guillame-Bert, 2022] or additive mixtures of tree-based models [Correia et al., 2020,   \n237 Wen and Hang, 2022, Watson et al., 2023] but perhaps surprisingly, the natural idea of creating an   \n238 ensemble of DET models through bagging has not been explored before as far as we are aware. Two   \n239 distinguishing features of some of these alternative approaches are: (i) Unlike DETs, the partitioning   \n240 of each tree is not driven directly by a density estimation goal. Correia et al. [2020] leverages   \n241 a standard discriminative Random Forest, therefore giving special treatment to a particular input   \n242 variable whose conditional estimation drives the choice of partitions and Wen and Hang [2022]   \n243 proposes using a mid-point random tree partitioning. (ii) Besides modelling the density function as   \n244 uniform at the leaf of each tree, other authors propose leveraging more complex models [Correia   \n245 et al., 2020, Watson et al., 2023] which can allow for the use of trees that are more representative   \n246 with a smaller number of leaves. (iii) Nock and Guillame-Bert [2022] and Watson et al. [2023] both   \n247 propose generative adversarial frameworks where the generator and discriminator are both a tree or   \n248 an ensemble of trees respectively. Note that, unlike with boosting, in these approaches the new model   \n249 doesn\u2019t add to the previous one but replaces it instead.   \n250 Other Recent Tree-Based approaches Nock and Guillame-Bert [2023] proposes a different   \n251 ensemble approach where each tree does not have their own leaf values that get added or multiplied   \n252 to produce the final density, but instead serve to collectively define the partitioning of the input space.   \n253 To train such models the authors propose a boosting framework where, rather than adding a new tree   \n254 to the ensemble at every iteration, the model is initialized with a fixed number of tree root nodes and   \n255 each iteration adds a split to an existing leaf node. Finally Jolicoeur-Martineau et al. [2023] propose   \n256 a diffusion model where a tree-based model (e.g., GBDT) is used to regress the score function. Being   \n257 a diffusion model, however, means that computing densities is untractable. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/7dcb34f225481da401710cf7ce35b364525a8ae2bae77cc749daaa43023fe685.jpg", "table_caption": ["Table 1: Single variable inference results. The reported values are the averages over 5 cross-validation folds. The corresponding sample standard deviations are reported in Appendix G. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "258 6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "259 For our experiments we use 5 tabular datasets from the UCI Machine Learning Repository [Dheeru   \n260 and Karra Taniskidou, 2017]: Abalone (AB), Physicochemical Properties of Protein Tertiary Structure   \n261 (PR), Adult (AD), MiniBooNE (MBNE) and Covertype (CT) as well as the California Housing (CH)   \n262 available through the Scikit-Learn package [Pedregosa et al., 2011]. We also include a downsampled   \n263 version of MNIST (by $2\\mathbf{x}$ along each dimension) which allows us to visually assess the quality of   \n264 individual samples, something that is generally not possible with structured tabular data, and provides   \n265 an example of the performance that can be achieved in an unstructured dataset with many features   \n266 that are correlated among themselves. More details about these datasets are given in Appendix E.   \n267 We split our experiments into two sections, the first to evaluate the quality of density models directly   \n268 on a single variable inference task and the second to investigate the performance of our proposed   \n269 models when used for sampling. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "270 6.1 Single Variable Inference ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "271 In this section we test the ability of a generative model, trained to learn the density over all input   \n272 variables, $q(\\mathbf{x})$ , to infer the value of a single one. I.e., we wish to test how good is its estimate of   \n273 $q(x_{i}|\\mathbf{x}_{-i})$ . For this purpose we pick $x_{i}=y$ as the original target of the dataset, noting that the   \n274 models that we train do not treat this variable in any special way, except for the selection of the best   \n275 model in validation. As such, we would expect that the model\u2019s performance in inference over this   \n276 particular variable is indicative of its strength on any other single variable inference task and also   \n277 indicative of the quality of the full $q(\\mathbf{x})$ from which the conditional probability estimate is derived.   \n278 We use XGBoost [Chen and Guestrin, 2016] as a baseline for what should be achievable by a very   \n279 strong discriminative model. Note that this model is trained to maximize the discriminative likelihood,   \n280 $\\mathbb{E}_{{\\bf x}\\sim p}\\log q(x_{i}|{\\bf x}_{-i})$ , directly, not wasting model capacity in learning other aspects of the full data   \n281 distribution. As another generative baseline we use our own implementation of RFDE [Wen and   \n282 Hang, 2022] which allows us to gauge the impact of the guided partitioning used in the DEF models   \n283 over a random partitioning of the input space.   \n284 We use random search to tune the hyperparameters of the XGBoost model and a grid search to tune the   \n285 most important hyperparameters of the generative density models. We employ 5-fold cross-validation,   \n286 repeating the hyperparameter tuning on each fold for all datasets except for the largest one (CT) for   \n287 which we report results on a single fold. For the full details of the experimental protocol please refer   \n288 to Appendix F.   \n289 We find that NRGBoost performs better than the additive ensemble models (see Table 1) despite   \n290 producing more compact ensembles. It often achieves comparable performance to XGBoost on the   \n291 smaller datasets and with a small gap on the three larger ones. We note also that for the regression   \n292 datasets the generative models provide an estimate of the full conditional distribution over the target   \n293 variable rather than a point estimate like XGBoost. While there are other variants of discriminative   \n294 boosting that also provide an estimate of the aleatoric uncertainty [Duan et al., 2020], they rely on a   \n295 parametric assumption about $p(y|\\mathbf x)$ that needs to hold for any $\\mathbf{x}$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/8876448b27b5c7f0e06dbe083be2f38e14bcf7c4f187a0f2ee1b61bd94c89029.jpg", "table_caption": ["Table 2: ML Efficiency results. The reported values are the averages over 5 different datasets generated by the same model. The best methods for each dataset are in bold and methods whose difference is $<2\\sigma$ away from zero are underlined. The performance of XGBoost trained on the real data is also reported for reference. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "296 6.2 Sampling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "297 In this section, we compare the sampling performance of our proposed methods to neural-network  \n298 based methods TVAE [Xu et al., 2019] and TabDDPM [Kotelnikov et al., 2022] on two metrics.   \n299 Machine Learning Efficiency The Machine Learning (ML) efficiency has been a popular way   \n300 to measure the quality of generative models for sampling [Xu et al., 2019, Kotelnikov et al., 2022,   \n301 Borisov et al., 2022b]. It relies on using samples from the model to train a discriminative model which   \n302 is then evaluated on the real data. Note that this is similar to the single variable inference performance   \n303 from Section 6.1. In fact, if the density model\u2019s support covers that of the full data, one would expect   \n304 the discriminative model to recover the generator\u2019s $q(y|\\mathbf x)$ , and therefore its performance, in the limit   \n305 where infinite generated data is used to train it.   \n306 We use an XGBoost model (with the hyperparameters tuned in real data) as the discriminative model   \n307 and train it using a similar number of training and validation samples as in the original data. For   \n308 the density models, we generate samples from the best model found in the previous section and   \n309 for non-density models we select their hyperparameters by evaluating the ML Efficiency in the   \n310 real validation set. Note that this leaves the sampling models at a potential advantage since the   \n311 hyperparameter selection is based on the metric that is being evaluated rather than the direct inference   \n312 performance of the previous section.   \n313 Discriminator Measure Similar to Borisov et al. [2022b] we test the capacity of a discriminative   \n314 model to distinguish between real and generated data. We use the original validation set as the real   \n315 part of the training data in order to avoid benefiting generative methods that overfit their original   \n316 training set. A new validation set is carved out of the original test set $(20\\%)$ and used to tune the   \n317 hyperparameters of an XGBoost model which we use as our choice of discriminator, evaluating its   \n318 AUC on the remainder of the real test data.   \n319 We repeat all experiments 5 times, with 5 different generated datatsets from each model. Results are   \n320 reported in Tables 2 and 3 showing that (i) NRGBoost outperforms all other methods by substantial   \n321 margins in the discriminator measure except for the PR and the MBNE datasets. (ii) On the ML   \n322 Efficiency metric, TabDDPM outperforms NRGBoost by small margins on the small datasets which   \n323 could in part be explained by the denser hyperparameter tuning favouring models that perform   \n324 particularly well at inferring the target variable at the expense of the others. Nevertheless, NRGBoost   \n325 still significantly outperforms all other models on MNIST and CT. Its samples also look visually   \n326 similar to the real data in both the MNIST and California datasets (see Figures 1 and 2). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 3: Discriminator measure results. All results are the AUC of an XGBoost model trained to distinguish real from generated data an therefore lower means better. The reported values are the averages over 5 different datasets generated by the same model. ", "page_idx": 8}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/b1086040ce4e1cd248045b5562cca9906cda039d9d57055466b39129224b5722.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ojFEP11Cqj/tmp/3ef0503183fc5b453e31a3df6b027b6c7fae0a264f9ba61ec4a1fdd0fe538baa.jpg", "img_caption": ["Figure 2: Joint histogram for the latitude and longitude for the California Housing dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "327 7 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "328 While the additive tree models like DEF require no sampling to train and are easy to sample from, we   \n329 find that in practice they require very deep trees to model the data well which, in turn, also requires   \n330 using a large number of trees in the ensemble to regularize. In our experiments we found that their   \n331 performance was often capped by the maximum number of leaves we allowed them to grow to $(2^{14})$ .   \n332 In contrast, we find that NRGBoost is able to model the data better while using shallower trees   \n333 and in fewer number. Its main downside is that it can only be sampled from approximately using   \n334 more expensive MCMC and also requires sampling during the training process. While our fast   \n335 Gibbs sampling implementation coupled with our proposed sampling approach were able to mitigate   \n336 the slow training, making these models much more usable in practice they are still cumbersome to   \n337 use for sampling due to autocorrelation between samples from the same Markov Chain. We argue   \n338 however that unlike in image or text generation where fast sampling is necessary for an interactive   \n339 user experience, this can be less of a concern for the task of generating synthetic datasets where the   \n340 one time cost of sampling is not as important as faithfully capturing the data generating distribution.   \n341 We also find that tuning the hyperparameters of tree-based models is easier and less crucial than DL   \n342 models for which many trials fail to produce a reasonable model. In particular we found NRGBoost   \n343 to be rather robust, with different hyperparameters leading to small differences in performance.   \n344 Finally, we note that like any other machine learning models, generative models are susceptible to   \n345 overftiting and are thus liable to leak information about their training data when generating synthetic   \n346 samples. In this respect, we believe that NRGBoost offers better tools to monitor and control   \n347 overfitting than other alternatives (see Section 3.3) but, still, due consideration for this risk must be   \n348 taken into account when sharing synthetic data. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "349 8 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "350 In this work, we extend the two most popular tree-based discriminative methods for use in generative   \n351 modeling. We find that our boosting approach, in particular, offers generally good discriminative   \n352 performance and better overall sampling performance than alternatives. We hope that these results   \n353 encourage further research into generative boosting approaches for tabular data, in particular exploring   \n354 other applications besides sampling that are enabled by density models. ", "page_idx": 8}, {"type": "text", "text": "355 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "356 Vadim Borisov, Tobias Leemann, Kathrin Se\u00dfler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci.   \n357 Deep Neural Networks and Tabular Data: A Survey. IEEE Transactions on Neural Networks and Learning   \n358 Systems, pages 1\u201321, 2022a. ISSN 2162-237X, 2162-2388. doi: 10.1109/TNNLS.2022.3229161. URL   \n359 http://arxiv.org/abs/2110.01889. arXiv:2110.01889 [cs].   \n360 Vadim Borisov, Kathrin Se\u00dfler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language Mod  \n361 els are Realistic Tabular Data Generators, October 2022b. URL http://arxiv.org/abs/2210.06280.   \n362 arXiv:2210.06280 [cs].   \n363 Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM   \n364 SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785\u2013794, August 2016.   \n365 doi: 10.1145/2939672.2939785. URL http://arxiv.org/abs/1603.02754. arXiv:1603.02754 [cs].   \n366 Alvaro Correia, Robert Peharz, and Cassio P de Campos. Joints in random forests. In H. Larochelle, M. Ranzato,   \n367 R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,   \n368 pages 11404\u201311415. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_   \n369 files/paper/2020/file/8396b14c5dff55d13eea57487bf8ed26-Paper.pdf.   \n370 Zac Cranko and Richard Nock. Boosted Density Estimation Remastered. In Proceedings of the 36th International   \n371 Conference on Machine Learning, pages 1416\u20131425. PMLR, May 2019. URL https://proceedings.mlr.   \n372 press/v97/cranko19b.html. ISSN: 2640-3498.   \n373 Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing   \n374 Magazine, 29(6):141\u2013142, 2012.   \n375 Dua Dheeru and Ef iKarra Taniskidou. UCI machine learning repository, 2017. URL http://archive.ics.   \n376 uci.edu/ml.   \n377 Tony Duan, Anand Avati, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu, Andrew Y. Ng, and Alejandro Schuler.   \n378 NGBoost: Natural Gradient Boosting for Probabilistic Prediction, June 2020. URL http://arxiv.org/   \n379 abs/1910.03225. arXiv:1910.03225 [cs, stat].   \n380 Justin Engelmann and Stefan Lessmann. Conditional Wasserstein GAN-based Oversampling of Tabular Data for   \n381 Imbalanced Learning, August 2020. URL http://arxiv.org/abs/2008.09202. arXiv:2008.09202 [cs].   \n382 Ju Fan, Junyou Chen, Tongyu Liu, Yuwei Shen, Guoliang Li, and Xiaoyong Du. Relational data synthesis using   \n383 generative adversarial networks: a design space exploration. Proceedings of the VLDB Endowment, 13(12):   \n384 1962\u20131975, August 2020. ISSN 2150-8097. doi: 10.14778/3407790.3407802. URL https://dl.acm.org/   \n385 doi/10.14778/3407790.3407802.   \n386 Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statisti  \n387 cal view of boosting (With discussion and a rejoinder by the authors). The Annals of Statis  \n388 tics, 28(2):337\u2013407, April 2000. ISSN 0090-5364, 2168-8966. doi: 10.1214/aos/1016218223.   \n389 URL https://projecteuclid.org/journals/annals-of-statistics/volume-28/issue-2/   \n390 Additive-logistic-regression--a-statistical-view-of-boosting-With/10.1214/aos/   \n391 1016218223.full. Publisher: Institute of Mathematical Statistics.   \n392 L\u00e9o Grinsztajn, Edouard Oyallon, and Ga\u00ebl Varoquaux. Why do tree-based models still outperform deep learning   \n393 on tabular data?, July 2022. URL http://arxiv.org/abs/2207.08815. arXiv:2207.08815 [cs, stat].   \n394 Aditya Grover and Stefano Ermon. Boosted Generative Models, December 2017. URL http://arxiv.org/   \n395 abs/1702.08484. arXiv:1702.08484 [cs, stat].   \n396 Charles R. Harris, K. Jarrod Millman, St\u00e9fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau,   \n397 Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer,   \n398 Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\u00e1ndez del R\u00edo, Mark Wiebe, Pearu Peter  \n399 son, Pierre G\u00e9rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph   \n400 Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357\u2013362, September   \n401 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2.   \n402 Alexia Jolicoeur-Martineau, Kilian Fatras, and Tal Kachman. Generating and imputing tabular data via diffusion   \n403 and flow-based gradient-boosted trees, 2023.   \n404 James Jordon, Jinsung Yoon, and Mihaela van der Schaar. PATE-GAN: Generating Synthetic Data with Differ  \n405 ential Privacy Guarantees. December 2018. URL https://openreview.net/forum?id=S1zk9iRqF7.   \n406 Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.   \n407 LightGBM: A Highly Efficient Gradient Boosting Decision Tree. In Advances in Neural Information   \n408 Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.   \n409 cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html.   \n410 Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. TabDDPM: Modelling Tabular Data   \n411 with Diffusion Models, September 2022. URL http://arxiv.org/abs/2209.15421. arXiv:2209.15421   \n412 [cs].   \n413 Richard Nock and Mathieu Guillame-Bert. Generative trees: Adversarial and copycat. In Kamalika Chaudhuri,   \n414 Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th   \n415 International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research,   \n416 pages 16906\u201316951. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/nock22a.   \n417 html.   \n418 Richard Nock and Mathieu Guillame-Bert. Generative forests, 2023.   \n419 Melissa E. O\u2019Neill. Pcg: A family of simple fast space-efficient statistically good algorithms for random number   \n420 generation. Technical Report HMC-CS-2014-0905, Harvey Mudd College, Claremont, CA, September 2014.   \n421 F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,   \n422 V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:   \n423 Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \n424 Parikshit Ram and Alexander G. Gray. Density estimation trees. In Proceedings of the 17th ACM SIGKDD   \n425 international conference on Knowledge discovery and data mining, pages 627\u2013635, San Diego California   \n426 USA, August 2011. ACM. ISBN 978-1-4503-0813-7. doi: 10.1145/2020408.2020507. URL https:   \n427 //dl.acm.org/doi/10.1145/2020408.2020507.   \n428 Saharon Rosset and Eran Segal. Boosting Density Estimation. In Advances in Neural Information Processing   \n429 Systems, volume 15. MIT Press, 2002. URL https://papers.nips.cc/paper_files/paper/2002/   \n430 hash/3de568f8597b94bda53149c7d7f5958c-Abstract.html.   \n431 Yang Song and Diederik P. Kingma. How to Train Your Energy-Based Models. arXiv:2101.03288 [cs, stat],   \n432 January 2021. URL http://arxiv.org/abs/2101.03288. arXiv: 2101.03288.   \n433 Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Sch\u00f6lkopf. AdaGAN:   \n434 Boosting Generative Models, May 2017. URL http://arxiv.org/abs/1701.02386. arXiv:1701.02386   \n435 [cs, stat].   \n436 Zhuowen Tu. Learning Generative Models via Discriminative Approaches. In 2007 IEEE Conference on   \n437 Computer Vision and Pattern Recognition, pages 1\u20138, June 2007. doi: 10.1109/CVPR.2007.383035. ISSN:   \n438 1063-6919.   \n439 David S. Watson, Kristin Blesch, Jan Kapar, and Marvin N. Wright. Adversarial random forests for density   \n440 estimation and generative modeling. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors,   \n441 Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of   \n442 Proceedings of Machine Learning Research, pages 5357\u20135375. PMLR, 25\u201327 Apr 2023. URL https:   \n443 //proceedings.mlr.press/v206/watson23a.html.   \n444 Max Welling, Richard Zemel, and Geoffrey E Hinton. Self Supervised Boosting. In Advances in Neural   \n445 Information Processing Systems, volume 15. MIT Press, 2002. URL https://papers.nips.cc/paper_   \n446 files/paper/2002/hash/cd0cbcc668fe4bc58e0af3cc7e0a653d-Abstract.html.   \n447 Hongwei Wen and Hanyuan Hang. Random forest density estimation. In Kamalika Chaudhuri, Stefanie Jegelka,   \n448 Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International   \n449 Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 23701\u2013   \n450 23722. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/wen22c.html.   \n451 Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling Tabu  \n452 lar data using Conditional GAN. In Advances in Neural Information Processing Systems, vol  \n453 ume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/hash/   \n454 254ed7d2de3b23ab10936522dd547b78-Abstract.html.   \n455 Zilong Zhao, Aditya Kunar, Hiek Van der Scheer, Robert Birke, and Lydia Y. Chen. CTAB-GAN: Effective   \n456 Table Data Synthesizing, May 2021. URL http://arxiv.org/abs/2102.08369. arXiv:2102.08369 [cs]. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "457 A Additional Derivations ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "458 The expected log-likelihood for an energy-based model (EBM), ", "page_idx": 11}, {"type": "equation", "text": "$$\nq_{f}(\\mathbf{x})=\\frac{\\exp\\left(f(\\mathbf{x})\\right)}{Z[f]}\\,,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "459 is given by ", "page_idx": 11}, {"type": "equation", "text": "$$\nL[f]=\\mathbb{E}_{\\mathbf{x}\\sim p}\\log q_{f}(\\mathbf{x})=\\mathbb{E}_{\\mathbf{x}\\sim p}f(\\mathbf{x})-\\log Z[f]\\,.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "460 The first variation of $L$ can be computed as ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\delta L[f;g]:=\\left.\\frac{d L[f+\\epsilon g]}{d\\epsilon}\\right|_{\\epsilon=0}=\\mathbb{E}_{\\mathbf{x}\\sim p}\\,g(\\mathbf{x})-\\delta\\log Z[f;g]=\\mathbb{E}_{\\mathbf{x}\\sim p}\\,g(\\mathbf{x})-\\mathbb{E}_{\\mathbf{x}\\sim q_{f}}\\,g(\\mathbf{x})\\,.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "461 This is a linear functional of its second argument, $g$ , and can be regarded as a directional derivative   \n462 of $L$ at $f$ along a variation $g$ . The last equality comes from the following computation of the first   \n463 variation of the log-partition function: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\delta\\log Z[f;g]=\\frac{\\delta Z[f;g]}{Z[f]}}\\\\ {\\displaystyle=\\frac{1}{Z[f]}\\sum_{\\mathbf{x}}\\exp^{\\prime}\\left(f(\\mathbf{x})\\right)g(\\mathbf{x})}\\\\ {\\displaystyle=\\sum_{\\mathbf{x}}\\frac{\\exp\\left(f(\\mathbf{x})\\right)}{Z[f]}g(\\mathbf{x})}\\\\ {\\displaystyle=\\mathbb{E}_{\\mathbf{x}\\sim q_{f}}g(\\mathbf{x})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "464 Analogous to a Hessian, we can differentiate Equation 15 again along a second independent variation   \n465 $h$ of $f$ yielding a symmetric bilinear functional which we will write as $\\delta^{2}L[f;g,\\bar{h}]$ . Note that the   \n466 first term in equation 2 is linear in $f$ and thus has no curvature, so we only have to consider the log   \n467 partition function itself: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\delta^{2}L[f;g,h]=\\displaystyle\\frac{\\partial^{2}L[f+g+\\varepsilon h]}{\\partial\\varepsilon\\partial\\varepsilon}\\Big\\vert_{(\\varepsilon,\\varepsilon)=0}}\\\\ {=-\\delta^{2}\\log Z[f;g,h]=-\\delta\\left\\{\\delta\\log Z[f;g]\\right\\}[f;h]}\\\\ {=-\\delta\\left\\{\\displaystyle\\frac{1}{Z[f]}\\displaystyle\\sum_{\\kappa}\\exp\\left(f(\\mathbf{x})\\right)g(\\mathbf{x})\\right\\}[f;h]}\\\\ {=\\displaystyle\\frac{\\delta Z[f;h]}{Z[f]}\\displaystyle\\sum_{\\mathbf{x}=\\log{g}(\\mathbf{x})}(f(\\mathbf{x}))\\,g(\\mathbf{x})-\\displaystyle\\frac{1}{Z[f]}\\displaystyle\\sum_{\\mathbf{x}}\\exp^{\\prime}\\left(f(\\mathbf{x})\\right)g(\\mathbf{x})h(\\mathbf{x})}\\\\ {=\\displaystyle\\frac{\\delta Z[f;h]}{Z[f]}\\cdot\\mathbb{E}_{\\mathrm{x}\\sim\\log{g}(\\mathbf{x})}-\\displaystyle\\frac{1}{Z[f]}\\displaystyle\\sum_{\\mathbf{x}}\\exp\\left(f(\\mathbf{x})\\right)g(\\mathbf{x})h(\\mathbf{x})}\\\\ {=\\mathbb{E}_{\\mathrm{x}\\sim\\mathcal{H}}h(\\mathbf{x})\\cdot\\mathbb{E}_{\\mathrm{x}\\sim q,\\mathcal{F}}(\\mathbf{x})-\\mathbb{E}_{\\mathrm{x}\\sim q,h}(\\mathbf{x})g(\\mathbf{x})}\\\\ {=-\\mathbb{C}\\mathrm{ov}_{\\mathrm{sec},\\forall(g\\mathbf{x}),h(\\mathbf{x})})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "468 Note that this functional is negative semi-definite for all $f$ , i.e. $\\delta^{2}L[f;h,h]\\leq0$ , meaning that the   \n469 log-likelihood is a concave functional of $f$ .   \n470 Using these results, we can now compute the Taylor expansion of the increment in log-likelihood $L$   \n471 from a change $f\\rightarrow f+\\delta f$ up to second order in $\\delta f$ : ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Delta L_{f}[\\delta f]=\\delta L[f;\\delta f]+\\frac{1}{2}\\delta^{2}L[f;\\delta f,\\delta f]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{\\mathbf{x}\\sim p}\\delta f(\\mathbf{x})-\\mathbb{E}_{\\mathbf{x}\\sim q_{f}}\\delta f(\\mathbf{x})-\\frac{1}{2}\\mathrm{Var}_{\\mathbf{x}\\sim q_{f}}\\delta f(\\mathbf{x})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "472 As an aside, defining the functional derivative, \u03b4\u03b4fJ([fx]), of a functional $J$ implicitly by: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{\\bf x}\\frac{\\delta J[f]}{\\delta f({\\bf x})}g({\\bf x})=\\delta J[f;g]\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "473 we can formally define, by analogy with the parametric case, the Fisher Information \"Matrix\" (FIM)   \n474 at $f$ as the following bilinear functional of two independent variations $g$ and $h$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F[f;g,h]:=-\\displaystyle\\sum_{\\mathbf{y},\\mathbf{z}}\\left[\\mathbb{E}_{\\mathbf{x}\\sim q_{f}}\\frac{\\delta^{2}\\log q_{f}(\\mathbf{x})}{\\delta f(\\mathbf{y})\\delta f(\\mathbf{z})}\\right]g(\\mathbf{y})h(\\mathbf{z})}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{\\mathbf{y},\\mathbf{z}}\\frac{\\delta^{2}\\log Z[f]}{\\delta f(\\mathbf{y})\\delta f(\\mathbf{z})}g(\\mathbf{y})h(\\mathbf{z})}\\\\ &{\\qquad=\\delta^{2}\\log Z[f;g,h]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "475 The only difference to the second-order variation of 2 computed in equation 20 would be that the   \n476 expectation is taken under the model distribution, $q_{f}$ , instead of the data distribution $p$ . However,   \n477 because the only term in $\\log q_{f}(\\mathbf{x})$ that is non-linear in $f$ is the log-partition functional, which is not   \n478 a function of $\\mathbf{x}$ , this expectation plays no role in the computation and we get the result that the FIM is   \n479 the same as the negative Hessian of the log-likelihood for these models. ", "page_idx": 12}, {"type": "text", "text": "480 A.1 Application to Piecewise Constant Functions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "481 Considering a weak learner such as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\delta f({\\bf x})=\\sum_{j=1}^{J}w_{j}{\\bf1}_{X_{j}}({\\bf x})\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "482 where the subsets $X_{j}$ are disjoint and cover the entire input space, $\\mathcal{X}$ , we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{\\mathbf{x}\\sim q}\\delta f(\\mathbf{x})=\\sum_{\\mathbf{x}\\in\\mathcal{X}}q(\\mathbf{x})\\sum_{j=1}^{J}w_{j}\\mathbf{1}_{X_{j}}(\\mathbf{x})}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad=\\sum_{j=1}^{J}w_{j}\\sum_{\\mathbf{x}\\in X_{j}}q(\\mathbf{x})\\ =\\sum_{j=1}^{J}w_{j}Q(X_{j})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "483 Similarly, making use of the fact that $\\mathbf{1}_{X_{i}}(\\mathbf{x})\\mathbf{1}_{X_{j}}(\\mathbf{x})=\\delta_{i j}\\mathbf{1}_{X_{i}}(\\mathbf{x})$ , we can compute ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}\\sim q}\\delta f^{2}(\\mathbf{x})=\\sum_{\\mathbf{x}\\in\\mathcal{X}}q(\\mathbf{x})\\left(\\sum_{j=1}^{J}w_{j}\\mathbf{1}_{X_{j}}(\\mathbf{x})\\right)^{2}=\\sum_{j=1}^{J}w_{j}^{2}Q(X_{j})\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "484 In fact, we can extend this to any ordinary function of $\\delta f$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{x}\\sim q}\\,g\\left(\\delta f(\\mathbf{x})\\right)=\\displaystyle\\sum_{\\mathbf{x}\\in\\mathcal{X}}q(\\mathbf{x})\\sum_{j=1}^{J}\\mathbf{1}_{X_{j}}(\\mathbf{x})g\\left(\\delta f(\\mathbf{x})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{j=1}^{J}\\sum_{\\mathbf{x}\\in\\mathcal{X}_{j}}q(\\mathbf{x})g(w_{j})}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{j=1}^{J}g(w_{j})Q(X_{j})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "485 where we made use of the fact that the ${\\mathbf{1}}_{X_{j}}$ constitute a partition of unity: ", "page_idx": 12}, {"type": "equation", "text": "$$\n1=\\sum_{j=1}^{J}\\mathbf{1}_{X_{j}}(\\mathbf{x})\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "486 Finally, we can compute the increase in likelihood from a step $f\\rightarrow f+\\alpha\\cdot\\delta f$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L[f+\\alpha\\cdot\\delta f]-L[f]=\\mathbb{E}_{\\mathbf{x}\\sim p}\\left[\\alpha\\cdot\\delta f(\\mathbf{x})\\right]-\\log Z[f+\\alpha\\cdot\\delta f]+\\log Z[f]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\alpha\\mathbb{E}_{\\mathbf{x}\\sim p}\\delta f(\\mathbf{x})-\\log\\mathbb{E}_{\\mathbf{x}\\sim q_{f}}\\exp(\\alpha\\delta f(\\mathbf{x}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\alpha\\displaystyle\\sum_{j=1}^{J}w_{j}P\\left(X_{j}\\right)-\\log\\displaystyle\\sum_{j=1}^{J}Q_{f}\\left(X_{j}\\right)\\exp\\left(\\alpha w_{j}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "487 where in equation 42 we made use of the equality: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log Z[f+\\alpha\\cdot\\delta f]-\\log Z[f]=\\log\\frac{\\sum_{\\mathbf{x}}\\exp(f(\\mathbf{x})+\\alpha\\delta f(\\mathbf{x}))}{Z[f]}=\\log\\sum_{\\mathbf{x}}q_{f}(\\mathbf{x})\\exp(\\alpha\\delta f(\\mathbf{x}))\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "488 and of the result in equation 39 in the final step. ", "page_idx": 13}, {"type": "text", "text": "489 This result can be used to conduct a line search over the step size using training data and to estimate   \n490 an increase in likelihood at each round of boosting for the purpose of early stopping, using validation   \n491 data. ", "page_idx": 13}, {"type": "text", "text": "492 B Training Density Estimation Trees ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "493 Density Estimation Trees (DET) [Ram and Gray, 2011] model the density function as a piecewise   \n494 constant function, ", "page_idx": 13}, {"type": "equation", "text": "$$\nq(\\mathbf{x})=\\sum_{j=1}^{J}v_{j}\\mathbf{1}_{X_{j}}(\\mathbf{x})\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "495 where $X_{j}$ are given by a partitioning of the input space $\\mathcal{X}$ induced by a binary tree and the $v_{j}$ are the   \n496 density values associated with each leaf that, for the time being, we will only require to be such that   \n497 $q(\\mathbf{x})$ sums to one.   \n498 Ram and Gray [2011] proposes fitting DET models to directly minimize a generative objective, the   \n499 Integrated Squared Error (ISE) between the data generating distribution, $p(\\mathbf{x})$ and the model: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{q\\in\\mathcal{Q}}\\sum_{\\mathbf{x}\\in\\mathcal{X}}\\left(p(\\mathbf{x})-q(\\mathbf{x})\\right)^{2}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "500 Noting that $q$ is a function as in Equation 45 and that $\\textstyle\\bigcup_{j=1}^{J}X_{j}={\\mathcal{X}}$ , we can rewrite this as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\boldsymbol{v}_{1},\\ldots,\\boldsymbol{v}_{J},X_{1},\\ldots,X_{J}}}&{~\\displaystyle\\sum_{\\mathbf{x}\\in\\mathcal{X}}p^{2}(\\mathbf{x})+\\displaystyle\\sum_{j=1}^{J}\\sum_{\\mathbf{x}\\in X_{j}}\\left(v_{j}^{2}-2v_{j}p(\\mathbf{x})\\right)}\\\\ {\\mathrm{s.t.}}&{~\\displaystyle\\sum_{j=1}^{J}\\sum_{\\mathbf{x}\\in X_{j}}v_{j}=1\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "501 Since the first term in the objective does not depend on the model this optimization problem can be   \n502 further simplified as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\boldsymbol{v}_{1},\\ldots,\\boldsymbol{v}_{J},\\boldsymbol{X}_{1},\\ldots,\\boldsymbol{X}_{J}}}&{\\displaystyle\\sum_{j=1}^{J}\\left(v_{j}^{2}V(X_{j})-2v_{j}P(X_{j})\\right)}\\\\ {\\mathrm{s.t.}}&{\\displaystyle\\sum_{j=1}^{J}v_{j}V(X_{j})=1\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "503 where $V(X)$ denotes the volume of a subset $X$ . Solving this quadratic program for the $v_{j}$ we obtain   \n504 the following optimal leaf values and objective: ", "page_idx": 13}, {"type": "equation", "text": "$$\nv_{j}^{*}=\\frac{P(X_{j})}{V(X_{j})}\\,,\\qquad\\qquad\\qquad\\mathrm{ISE}^{*}\\left(X_{1},\\ldots,X_{J}\\right)=-\\sum_{j=1}^{J}\\frac{P^{2}(X_{j})}{V_{f}(X_{j})}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "505 One can therefore grow a tree by greedily choosing to split a parent leaf with support $X_{P}$ into two   \n506 leaves with supports $X_{L}$ and $X_{R}$ so as to maximize the following criterion: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{X_{L},X_{R}\\in\\mathcal{P}(X_{P})}\\frac{P^{2}(X_{L})}{V(X_{L})}+\\frac{P^{2}(X_{R})}{V(X_{R})}-\\frac{P^{2}(X_{P})}{V(X_{P})}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "507 B.1 Maximum Likelihood ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "508 To maximize the likelihood, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{q}\\mathbb{E}_{\\mathbf{x}\\sim p}\\log q(\\mathbf{x})\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "509 rather than the ISE one can use the same approach. Here the optimization problem to solve is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{v_{1},\\ldots,v_{J},X_{1},\\ldots,X_{J}}}&{~\\displaystyle\\sum_{j=1}^{J}P(X_{j})\\log v_{j}}\\\\ {\\mathrm{s.t.}}&{~\\displaystyle\\sum_{j=1}^{J}v_{j}V(X_{j})=1\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "510 This is, again, easy to solve for $v_{j}$ since it is separable over $j$ after removing the constraint using   \n511 Lagrange multipliers. The optimal leaf values and objective are in this case: ", "page_idx": 14}, {"type": "equation", "text": "$$\nv_{j}^{*}=\\frac{P(X_{j})}{V(X_{j})}\\,,\\qquad\\qquad L^{*}\\left(X_{1},\\ldots,X_{J}\\right)=\\sum_{j=1}^{J}P(X_{j})\\log\\frac{P(X_{j})}{V_{f}(X_{j})}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "512 The only change is therefore to the splitting criterion which should become: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{X_{L},X_{R}\\in\\mathcal{P}(X_{P})}P(X_{L})\\log\\frac{P(X_{L})}{V(X_{L})}+P(X_{R})\\log\\frac{P(X_{R})}{V(X_{R})}-P(X_{P})\\log\\frac{P(X_{P})}{V(X_{P})}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "513 C Greedy Tree Based Multiplicative Boosting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "514 In multiplicative generative boosting an unnormalized current density model, $\\tilde{q}_{t-1}(\\mathbf x)$ , is updated at   \n515 each boosting round by multiplication with a new factor $\\delta q_{t}^{\\alpha_{t}}(\\mathbf{x})$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widetilde{q}_{t}(\\mathbf{x})=\\widetilde{q}_{t-1}(\\mathbf{x})\\cdot\\delta q_{t}^{\\alpha_{t}}(\\mathbf{x})\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "516 For our proposed NRGBoost, this factor is chosen in order to maximize a local quadratic approx  \n517 imation of the log likelihood around $q_{t-1}$ as a functional of the log density (see Section 3). The   \n518 motivation behind the greedy approach of Tu [2007] or Grover and Ermon [2017] is to instead make   \n519 the update factor $\\delta q_{t}(\\mathbf{x})$ proportional to the likelihood ratio $r_{t}(\\mathbf{x}):=\\left.p(\\mathbf{x})\\middle/q_{t-1}(\\mathbf{x})\\right.$ directly, which   \n520 under ideal conditions would mean that the method converges immediately when choosing a step size   \n521 $\\alpha_{t}=1$ . In more realistic setting, however, this method has been shown to converge under conditions   \n522 on the performance of the individual $\\delta q_{t}$ as discriminators between real and generated data [Tu, 2007,   \n523 Grover and Ermon, 2017, Cranko and Nock, 2019].   \n524 While in principle this desired $r_{t}(\\mathbf{x})$ could be derived from any binary classifier that is trained to   \n525 predict a probability of a datapoint being generated (e.g., by training it to minimize a strictly proper   \n526 loss) and Tu [2007] does not prescribe any particular choice, Grover and Ermon [2017] propose   \n527 relying on the following variational bound of an $f$ -divergence to derive an estimator for this ratio: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{f}(P\\|Q_{t-1})\\geq\\operatorname*{sup}_{u\\in\\mathcal{U}_{t}}\\left[\\mathbb{E}_{\\mathbf{x}\\sim p}\\,u(\\mathbf{x})-\\mathbb{E}_{\\mathbf{x}\\sim q_{t-1}}f^{*}(u(\\mathbf{x}))\\right]\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "528 Here $f^{*}$ denotes the convex conjugate of $f$ . This bound is tight, with the optimum being achieved for   \n529 $u_{t}^{*}(\\mathbf{x})=f^{\\prime}\\big(p(\\mathbf{x})\\big/q_{t-1}(\\mathbf{x})\\big)$ , if $\\mathcal{U}_{t}$ is capable of representing this function. $\\bar{(f^{\\prime})}^{-1}\\,(u_{t}^{*}(\\mathbf{x})\\bar{)}$ can thus be   \n530 interpreted as an approximation of $r_{t}(\\mathbf{x})$ .   \n531 Adapting this method to use trees as weak learners can be accomplished by considering $\\mathcal{U}_{t}$ in Equation   \n532 56 to be defined by tree functions $\\begin{array}{r}{u=1/J\\sum_{j=1}^{J}w_{j}\\mathbf{1}_{X_{j}}}\\end{array}$ with leaf values $w_{j}$ and leaf supports $X_{j}$ .   \n533 At each boosting iteration a new tree, $\\boldsymbol{u}_{t}^{*}$ can thus be grown to greedily optimize the lower bound in   \n534 the r.h.s. of Equation 56 and setting $\\delta q_{t}(\\mathbf{x})=\\left(f^{\\prime}\\right)^{-1}\\left(u_{t}^{*}(\\mathbf{x})\\right)$ which is thus also a tree with the same   \n535 leaf supports and leaf values given by $v_{j}:=\\left(f^{\\prime}\\right)^{-1}(w_{j})$ . This leads to the seaprable optimization   \n536 problem: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{w_{1},\\ldots,w_{J},X_{1},\\ldots,X_{J}}\\sum_{j}^{J}\\left[P(X_{j})w_{j}-Q(X_{j})f^{*}(w_{j})\\right]\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/5e35ee8039569f82778590625a1d1454cdd6c4e4e3ebe9a5b40775699ae30b05.jpg", "table_caption": ["Table 4: Comparison of splitting criterion and leaf weights for the different versions of boosting. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "537 Note that we drop the iteration indices from this point onward for brevity. Maximizing over $w_{j}$ with   \n538 the $X_{j}$ fixed we have that $w_{j}^{*}=f^{\\prime}\\left(P(X_{j})\\middle/Q(X_{j})\\right)$ which yields the optimal value ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ^{*}(X_{1},\\ldots,X_{j})=\\sum_{j}\\left[P(X_{j})f^{\\prime}\\left({\\frac{P(X_{j})}{Q(X_{j})}}\\right)-Q(X_{j})(f^{*}\\circ f^{\\prime})\\left({\\frac{P(X_{j})}{Q(X_{j})}}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "539 that in turn determines the splitting criterion as a function of the choice of $f$ . Finally, the optimal   \n540 density values for the leaves are given by ", "page_idx": 15}, {"type": "equation", "text": "$$\nv_{j}^{*}=\\left(f^{\\prime}\\right)^{-1}(w_{j}^{*})=\\frac{P(X_{j})}{Q(X_{j})}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "541 It is interesting to note two particular choices of $f$ -divergences. For the $\\mathrm{KL}$ divergence, $f(t)=t\\log t$   \n542 and $f^{\\prime}(t)=1+\\log t=\\left(f^{*}\\right)^{-1}(t)$ . This leads to ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ_{K L}(X_{1},\\dots,X_{j})=\\sum_{j}P(X_{j})\\log{\\frac{P(X_{j})}{Q(X_{j})}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "543 as the splitting criterion. The Pearson $\\chi^{2}$ divergence, with $f(t)=(t-1)^{2}$ , leads to the same splitting   \n544 criterion as NRGBoost. Note however that for NRGBoost the leaf values for the multiplicative update   \n545 of the density are given by $\\exp{\\left(P(X_{j})/Q(X_{j})-1\\right)}$ instead of the ratio directly. Table 4 summarizes   \n546 these results.   \n547 Another interesting observation is that a DET model can be interpreted as a single round of greedy   \n548 multiplicative boosting starting from a uniform initial model. The choice of the ISE as the criterion to   \n549 optimize the DET corresponds to the choice of Pearson\u2019s $\\chi^{2}$ divergence and likelihood to the choice   \n550 of KL divergence. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "551 D Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "552 Discretization In our practical implementation of tree based methods we first discretize the input   \n553 space by binning continuous numerical variables by quantiles. Furthermore we also bin discrete   \n554 numerical variables in order to keep their cardinalities smaller than 256. This can also be interpreted   \n555 as establishing a priori a set of discrete values to consider when splitting on each numerical variable   \n556 and is done for computational efficiency, being inspired by LightGBM [Ke et al., 2017].   \n557 Categorical Splitting For splitting on a categorical variable we once again take inspiration from   \n558 LightGBM. Rather than relying on one-vs-all splits we found it better to first order the possible   \n559 categorical values at a leaf according to a pre-defined sorting function and then choose the optimal   \n560 many-vs-many split as if the variable was numerical. The function used to sort the values is the leaf   \n561 value function. E.g., for splitting on a categorical variable $x_{i}$ we order each possible categorical value   \n562 $k$ by $\\hat{P}(x_{i}{=}k,X_{-i})\\overline{{\\hat{Q}}}(x_{i}{=}\\bar{k,}X_{-i})$ in the case of NRGBoost where $X_{-i}$ denotes the leaf support over the   \n563 remaining variables.   \n564 Tree Growth Strategy We always grow trees in best first order. I.e., we always split the current   \n565 leaf node that yields the maximum gain in the chosen objective value.   \n566 Line Search As mentioned in Section 3, we perform a line search to find the optimal step size after   \n567 each round of boosting in order to maximize the likelihood gain in Equation 43. Because evaluating   \n568 multiple possible step sizes, $\\alpha_{t}$ , is inexpensive, we simply do a grid search over 101 different step   \n569 sizes in the range $[10^{\\bar{-}3},10]$ with their logarithm uniformly distributed.   \n570 Random Forest Density Estimation (RFDE) We implement the RFDE method [Wen and Hang,   \n571 2022] after quantile discretization of the dataset and therefore split at the midpoint of the discretized   \n572 dimension instead of the original one. When a leaf support has odd cardinality over the splitting   \n573 dimension a random choice is made over the two possible splitting values. Finally, the original paper   \n574 does not mention how to split over categorical domains. We therefore choose to randomly split the   \n575 possible categorical values for a leaf evenly as we found that this yielded slightly better results than a   \n576 random one vs all split.   \n577 Code Our implementation of the proposed tree-based methods is mostly Python code using the   \n578 NumPy library [Harris et al., 2020]. We implement the tree evaluation and Gibbs sampling in C,   \n579 making use of the PCG library [O\u2019Neill, 2014] for random number generation. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/ac06a4dbe5f424f94de262502adda14c808413b84506434d9759a7beb54497e8.jpg", "table_caption": ["Table 5: Dataset Information. We respect the original test sets of each dataset when provided, otherwise we set aside $20\\%$ of the original dataset as a test set. $20\\%$ of the remaining data is set aside as a validation set used for hyperparameter tuning. "], "table_footnote": ["\u2217Original test set was respected. "], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "580 E Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "581 We use 5 datasets from the UCI Machine Learning Repository [Dheeru and Karra Taniskidou, 2017]:   \n582 Abalone, Physicochemical Properties of Protein Tertiary Structure (referred to as Protein in the   \n583 sequence), Adult, MiniBooNE and Covertype. We also use the California Housing dataset which was   \n584 downloaded through the Scikit-Learn package Pedregosa et al. [2011] and a downsampled version of   \n585 the MNIST dataset Deng [2012]. Table 5 summarizes the main details of these datasets as well as the   \n586 approximate number of samples used for train/validation/test for each cross-validation fold. ", "page_idx": 16}, {"type": "text", "text": "587 F Experimental Setup ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "588 F.1 XGBoost Hyperparameter Tuning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "589 To tune the hyperparameters of XGBoost we use 100 trials of random search with the search space   \n590 defined in Table 6.   \n591 Each model was trained for 1000 boosting rounds on regression and binary classification tasks. For   \n592 multi-class classification tasks a maximum number of 200 rounds of boosting was used due to the   \n593 larger size of the datasets and because a separate tree is built at every round for each class. The   \n594 best model was selected based on the validation set, together with the boosting round where the best   \n595 performance was attained. The test metrics reported correspond to the performance of the selected   \n596 model at that boosting round on the test set. ", "page_idx": 16}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/d417502865746dd7788802e764e08daef80c47d58a0ac2ed9cd199110ccde314.jpg", "table_caption": ["Table 6: XGBoost hyperparameter tuning search space. $\\delta(0)$ denotes a point mass distribution at 0. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "597 F.2 TVAE Hyperparameter Tuning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "598 To tune the hyperparameters of TVAE we use 50 trials of random search with the search spaces   \n599 defined in Table 7.   \n600 The TVAE implementations used are from the latest version of the SDV package (https://github.   \n601 com/sdv-dev/SDV) available at the time. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Table 7: TVAE hyperparameter tuning search space. We set both compress_dims and decompress_dims to have the number of layers specified by num_layers, with hidden_dim hidden units in each layer. We use larger batch sizes and smaller number of epochs for the larger datasets (MBNE, MNIST, CO). ", "page_idx": 17}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/c2bfeee57693f5fcc6aef4a26cecf96e9c2df29a602ad69989ea753b0e301f66.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "602 F.3 TabDDPM Hyperparameter Tuning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "603 To tune the hyperparameters of TabDDPM we use 50 trials of random search with the same search   \n604 space that the original authors use in their paper [Kotelnikov et al., 2022].   \n605 We use the official implementation (https://github.com/yandex-research/tab-ddpm)   \n606 adapted to use our datasets and validation setup. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "607 F.4 Random Forest Density Estimation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "608 For RFDE models we train a total of 1000 trees. The only hyperparameter that we tune is the   \n609 maximum number of leaves per tree for which we test the values $[{\\dot{2}}^{6},2^{7},\\dots,2^{1}4]$ . For the Adult   \n610 dataset, due to limitations of our tree evaluation implementation we only values test up to $2^{1}3$ . ", "page_idx": 17}, {"type": "text", "text": "611 F.5 Density Estimation Forests Hyperparameter Tuning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "612 We train ensembles with 1000 DET models. Only three hyperparameters are tuned, using three nested   \n613 loops. Every loop runs over the possible values of a single parameter in a pre-defined order with early   \n614 stopping triggering if a value fails to improve the validation metric over the previous one. The tuned   \n615 parameters along with their possible values are reported in Table 8 ", "page_idx": 17}, {"type": "text", "text": "616 F.6 NRGBoost ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "617 We train NRGBoost models for a maximum of 200 rounds of boosting. The starting point of each   \n618 NRGBoost model was selected as a mixture model between a uniform distribution $(10\\%)$ and the ", "page_idx": 17}, {"type": "text", "text": "Table 8: DEF models grid search space. Rows are in order of outermost loop to innermost loop. Note that for the Adult dataset, due to limitations of the implementation a maximum number of 8192 leaves is used instead of 16384. ", "page_idx": 18}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/4c1b877933de0d116fe9571141602c318af478557ebe9f72f72827fcf612403d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "619 product of training marginals $(90\\%)$ on the discretized input space. We observed that this mixture   \n620 coefficient does not have much impact on the results however. ", "page_idx": 18}, {"type": "text", "text": "621 We only tune two parameters for NRGBoost Models: ", "page_idx": 18}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/499e3fae7979d9628f61ce353b15f5f74e3c2a99d42d6c9e5947549eae204bd3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "629 This means that at maximum we train only 24 NRGBoost models (30 for CT). ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "630 All other relevant parameters are fixed and their values, along with a short description, is given in   \n631 Table 9. ", "page_idx": 18}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/44f85c53b18fd298290b2bab75aa1f32ca681315d5a6390b6397fd3dd2646a86.jpg", "table_caption": ["Table 9: NRGBoost fixed parameters. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "632 F.7 Evaluation Setup ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "633 Single variable inference For the single variable inference evaluation, the best models are selected   \n634 by their discriminative performance on a validation set. The entire setup is repeated five times with   \n635 different cross-validation folds and with different seeds for all sources of randomness except on the   \n636 CT dataset due to its large size. For the Adult and MNIST datasets the test set is fixed but training   \n637 and validation splits are still rotated.   \n638 Sampling For the sampling evaluation we use a single train/validation/test split of the real data   \n639 (corresponding to the first fold in the previous setup) for training the generative models. The density   \n640 models used are those previously selected based on their single variable inference performance   \n641 on the validation set. For the sampling models (TVAE and TabDDPM) we directly evaluate their   \n642 ML Efficiency using the validation data by training an XGBoost model on generated data. The   \n643 hyperparameters used for this XGBoost model are those selected on the real data in the previous   \n644 experiment. We only use a generated validation set in order to select the best stopping point for   \n645 XGBoost.   \n646 ML Efficiency For each selected model we sample a train and validation sets with the same number   \n647 of samples as those used on the original data. For NRGBoost we generate these samples by running   \n648 64 chains in parallel with 100 steps of burn in and downsampling their outputs by 30 (for the smaller   \n649 datasets) or 10 (for MBNE, MNIST and CT). The setup is repeated 5 times with 5 different datasets   \n650 generated for each method.   \n651 Discriminator Measure We create the training, validation and test sets to train an XGBoost model   \n652 to discriminate between real and generated data using the following process:   \n653 \u2022 The original validation set is used as the real part of the training set in order to avoid   \n654 benefitting generative methods that overfit their training set.   \n655 \u2022 The original test set is split $20\\%/80\\%$ . The $20\\%$ portion is used as the real part of the   \n656 validation set and the $80\\%$ portion as the real part of the test set.   \n657 \u2022 To form the generated part of the training, validation and test sets for the smaller datasets   \n658 we sample data according to the original number of samples in the train, validation and   \n659 test splits on the real data. Note that this makes the ratio of real to synthetic data 1:4 in the   \n660 training set. This is deliberate because for these smaller datasets the original validation has   \n661 few samples and adding extra synthetic data helps the discriminator.   \n662 \u2022 For the larger datasets we generate the same number of synthetic samples as there are real   \n663 samples on each split, therefore making every ratio 1:1 because the discriminator is typically   \n664 already too powerful and doesn\u2019t need extra data.   \n665 Because, in contrast to the previous metric, having a lower number of effective samples helps rather   \n666 than hurts we take extra precautions to not generate correlated data with NRGBoost. We draw each   \n667 sample by running its own independent chain for 100 steps starting from an independent sample from   \n668 the initial model which is a rather slow process. The setup is repeated 5 times with 5 different sets of   \n669 generated samples from each method. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "670 F.8 Computational Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "671 The experiments were run on a machine equipped with an AMD Ryzen 7 7700X 8 core CPU and 32   \n672 GB of RAM. The comparisons with TVAE and TabDDPM further made use of a GeForce RTX 3060   \n673 GPU with 12 GB of VRAM. ", "page_idx": 19}, {"type": "text", "text": "674 G Additional Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "675 G.1 Standard Errors ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "676 In Tables 10, 11 and 12 we report the sample standard deviations obtained for the main tables   \n677 presented in the paper. ", "page_idx": 19}, {"type": "text", "text": "678 G.2 Samples ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "679 In Figure G.2 we show the convergence of a Gibbs sampler sampling from a NRGBoost model. In   \n680 only a few samples each chain appears to have converged to the data manifold after starting at a   \n681 random sample from the initial model (a mixture between the product of training marginals and a   \n682 uniform). Note how consecutive samples are autocorrelated. In particular it can be rare for a chain   \n683 to switch between two different modes of the distribution (e.g., switching digits) even though a few   \n684 such transitions can be observed. ", "page_idx": 19}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/cffd04c640c7040d9aba8134bf5fdea61d0c34f62756e15ad54fe6dbbd151475.jpg", "table_caption": ["Table 10: Single variable inference sample standard deviations. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/9a4e2ac35bc3628d98baa32e956b8c144f21b872c9f1f0d684d6599d86341b48.jpg", "table_caption": ["Table 11: ML Efficiency results sample standard deviations. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/26d92f1a059acb25083245b8a94ed151cb5813fb5759c38c799a952b0fe6fc6c.jpg", "table_caption": ["Table 12: Discriminator measure sample standard deviations. "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "ojFEP11Cqj/tmp/3b1106b15e354b9db90832495d2c861d636349b70f762c081d8d53e2e51f36dd.jpg", "img_caption": ["Figure 3: Downsampled MNIST samples generated by Gibbs sampling from a NRGBoost model. Each row corresponds to an independent chain initialized with a sample from the initial model $f_{0}$ (first column). Each column represents a consecutive sample from the chain. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 13: Best NRGBoost model parameters per dataset and the wall time taken to train it. The format is minutes:seconds. ", "page_idx": 21}, {"type": "table", "img_path": "ojFEP11Cqj/tmp/efc31f7ae2d48c905ee935c185f3047d8f3cb2960e4e46054523774e3e59ea6c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "685 G.3 Time ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "686 In Table 13 we report the best hyperparameters found for NRGBoost for the first cross-validation   \n687 fold together with the time taken to train this best model. ", "page_idx": 21}, {"type": "text", "text": "688 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "689   \n690 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n691 paper\u2019s contributions and scope?   \n692 Answer: [Yes]   \n693 Justification: Claims about proposal of novel methods are justified in Sections 3 and 4.   \n694 Claims about empirical results are justified in Section 6 and Appendix G.   \n695 Guidelines:   \n696 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n697 made in the paper.   \n698 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n699 contributions made in the paper and important assumptions and limitations. A No or   \n700 NA answer to this question will not be perceived well by the reviewers.   \n701 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n702 much the results can be expected to generalize to other settings.   \n703 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n704 are not attained by the paper.   \n705 2. Limitations   \n706 Question: Does the paper discuss the limitations of the work performed by the authors?   \n707 Answer: [Yes]   \n708 Justification: We discuss limitations of our proposed method both in the section that intro  \n709 duces it (Section 3) as well as in the experiments (Section 6) and discussion (Section 7)   \n710 sections.   \n711 Guidelines:   \n712 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n713 the paper has limitations, but those are not discussed in the paper.   \n714 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n715 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n716 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n717 model well-specification, asymptotic approximations only holding locally). The authors   \n718 should reflect on how these assumptions might be violated in practice and what the   \n719 implications would be.   \n720 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n721 only tested on a few datasets or with a few runs. In general, empirical results often   \n722 depend on implicit assumptions, which should be articulated.   \n723 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n724 For example, a facial recognition algorithm may perform poorly when image resolution   \n725 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n726 used reliably to provide closed captions for online lectures because it fails to handle   \n727 technical jargon.   \n728 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n729 and how they scale with dataset size.   \n730 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n731 address problems of privacy and fairness.   \n732 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n733 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n734 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n735 judgment and recognize that individual actions in favor of transparency play an impor  \n736 tant role in developing norms that preserve the integrity of the community. Reviewers   \n737 will be specifically instructed to not penalize honesty concerning limitations.   \n738 3. Theory Assumptions and Proofs   \n739 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n740 a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes]   \nJustification: All results presented in the main paper are justified in Appendices A and B. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "43   \n44 \u2022 The answer NA means that the paper does not include theoretical results.   \n45 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n46 referenced.   \n47 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n48 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n49 they appear in the supplemental material, the authors are encouraged to provide a short   \n50 proof sketch to provide intuition.   \n51 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n52 by formal proofs provided in appendix or supplemental material.   \n53 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "754 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "755 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n756 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n757 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Justification: Additional implementation details of our method are provided in Appendix D and the full experimental setup is described in detail in Appendix F. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "793 5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "94 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n95 tions to faithfully reproduce the main experimental results, as described in supplemental   \n96 material?   \n97 Answer: [No]   \n98 Justification: Unfortunately we did not have time to clean up the code and document it   \n99 so that it could be useful at the time of the paper deadline. But we intend to make our   \n00 implementations of the proposed algorithms available as a python library as soon as possible   \n01 and will also open source the full experimental setup on Github.   \n02 Guidelines:   \n03 \u2022 The answer NA means that paper does not include experiments requiring code.   \n04 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n05 public/guides/CodeSubmissionPolicy) for more details.   \n06 \u2022 While we encourage the release of code and data, we understand that this might not be   \n07 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n08 including code, unless this is central to the contribution (e.g., for a new open-source   \n09 benchmark).   \n10 \u2022 The instructions should contain the exact command and environment needed to run to   \n11 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n12 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n13 \u2022 The authors should provide instructions on data access and preparation, including how   \n14 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n15 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n16 proposed method and baselines. If only a subset of experiments are reproducible, they   \n17 should state which ones are omitted from the script and why.   \n18 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n19 versions (if applicable).   \n20 \u2022 Providing as much information as possible in supplemental material (appended to the   \n21 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The experimental setup is described in as much detail as the space allows in Section 6. The full setup is described in Appendix F. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "835 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "36 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n7 information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Sample standard deviations for all experiments are reported in Appendix G. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "845 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n846 example, train/test split, initialization, random drawing of some parameter, or overall   \n847 run with given experimental conditions).   \n848 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n849 call to a library function, bootstrap, etc.)   \n850 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n851 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n852 of the mean.   \n853 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n854 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n855 of Normality of errors is not verified.   \n856 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n857 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n858 error rates).   \n859 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n860 they were calculated and reference the corresponding figures or tables in the text.   \n861 8. Experiments Compute Resources   \n862 Question: For each experiment, does the paper provide sufficient information on the com  \n863 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n864 the experiments?   \n865 Answer: [Yes]   \n866 Justification: This information is provided in Appendix G.   \n867 Guidelines:   \n868 \u2022 The answer NA means that the paper does not include experiments.   \n869 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n870 or cloud provider, including relevant memory and storage.   \n871 \u2022 The paper should provide the amount of compute required for each of the individual   \n872 experimental runs as well as estimate the total compute.   \n873 \u2022 The paper should disclose whether the full research project required more compute   \n874 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n875 didn\u2019t make it into the paper).   \n876 9. Code Of Ethics   \n877 Question: Does the research conducted in the paper conform, in every respect, with the   \n878 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n879 Answer: [Yes]   \n880 Justification: As far as we are aware there are no violations of the Code of Ethics.   \n881 Guidelines:   \n882 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n883 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n884 deviation from the Code of Ethics.   \n885 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n886 eration due to laws or regulations in their jurisdiction).   \n887 10. Broader Impacts   \n888 Question: Does the paper discuss both potential positive societal impacts and negative   \n889 societal impacts of the work performed?   \n890 Answer: [Yes]   \n891 Justification: We discuss the main potential misuse of our work in Section 7.   \n892 Guidelines:   \n893 \u2022 The answer NA means that there is no societal impact of the work performed.   \n894 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n895 impact or why the paper does not address societal impact.   \n896 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n897 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n898 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n899 groups), privacy considerations, and security considerations.   \n900 \u2022 The conference expects that many papers will be foundational research and not tied   \n901 to particular applications, let alone deployments. However, if there is a direct path to   \n902 any negative applications, the authors should point it out. For example, it is legitimate   \n903 to point out that an improvement in the quality of generative models could be used to   \n904 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n905 that a generic algorithm for optimizing neural networks could enable people to train   \n906 models that generate Deepfakes faster.   \n907 \u2022 The authors should consider possible harms that could arise when the technology is   \n908 being used as intended and functioning correctly, harms that could arise when the   \n909 technology is being used as intended but gives incorrect results, and harms following   \n910 from (intentional or unintentional) misuse of the technology.   \n911 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n912 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n913 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n914 feedback over time, improving the efficiency and accessibility of ML).   \n915 11. Safeguards   \n916 Question: Does the paper describe safeguards that have been put in place for responsible   \n917 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n918 image generators, or scraped datasets)?   \n919 Answer: [NA]   \n920 Justification: We do not believe our proposed models have a high risk of misuse but will   \n921 nonetheless highlight the potential risks in the documentation when we release the code.   \n922 Guidelines:   \n923 \u2022 The answer NA means that the paper poses no such risks.   \n924 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n925 necessary safeguards to allow for controlled use of the model, for example by requiring   \n926 that users adhere to usage guidelines or restrictions to access the model or implementing   \n927 safety filters.   \n928 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n929 should describe how they avoided releasing unsafe images.   \n930 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n931 not require this, but we encourage authors to take this into account and make a best   \n932 faith effort.   \n933 12. Licenses for existing assets   \n934 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n935 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n936 properly respected?   \n937 Answer: [Yes]   \n938 Justification: As far as we are aware we cite all the sources of the data used in our experiments   \n939 as well the main software packages used.   \n940 Guidelines:   \n941 \u2022 The answer NA means that the paper does not use existing assets.   \n942 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n943 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n944 URL.   \n945 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n946 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n947 service of that source should be provided.   \n948 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n949 package should be provided. For popular datasets, paperswithcode.com/datasets   \n950 has curated licenses for some datasets. Their licensing guide can help determine the   \n951 license of a dataset.   \n952 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n953 the derived asset (if it has changed) should be provided.   \n954 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n955 the asset\u2019s creators.   \n956 13. New Assets   \n957 Question: Are new assets introduced in the paper well documented and is the documentation   \n958 provided alongside the assets?   \n959 Answer: [NA]   \n960 Justification: We don\u2019t release any new assets at the time of submission. We plan to release   \n961 the code later and will fully document it.   \n962 Guidelines:   \n963 \u2022 The answer NA means that the paper does not release new assets.   \n964 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n965 submissions via structured templates. This includes details about training, license,   \n966 limitations, etc.   \n967 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n968 asset is used.   \n969 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n970 create an anonymized URL or include an anonymized zip file.   \n971 14. Crowdsourcing and Research with Human Subjects   \n972 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n973 include the full text of instructions given to participants and screenshots, if applicable, as   \n974 well as details about compensation (if any)?   \n975 Answer: [NA]   \n976 Justification: We don\u2019t conduct any experiments involving human subjects.   \n977 Guidelines:   \n978 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n979 human subjects.   \n980 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n981 tion of the paper involves human subjects, then as much detail as possible should be   \n982 included in the main paper.   \n983 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n984 or other labor should be paid at least the minimum wage in the country of the data   \n985 collector.   \n986 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n987 Subjects   \n988 Question: Does the paper describe potential risks incurred by study participants, whether   \n989 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n990 approvals (or an equivalent approval/review based on the requirements of your country or   \n991 institution) were obtained?   \n992 Answer: [NA]   \n993 Justification: We don\u2019t conduct any experiments involving human subjects.   \n994 Guidelines:   \n995 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n996 human subjects.   \n997 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n998 may be required for any human subjects research. If you obtained IRB approval, you   \n999 should clearly state this in the paper. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]