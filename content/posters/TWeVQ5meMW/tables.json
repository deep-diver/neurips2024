[{"figure_path": "TWeVQ5meMW/tables/tables_7_1.jpg", "caption": "Table 1: Quantitative comparison for the number of iterations, subject fidelity and prompt fidelity.", "description": "This table compares the performance of several subject-driven text-to-image generation methods on the DreamBench dataset.  The metrics used are DINO (image-to-image similarity), CLIP-I (image-to-text alignment), and CLIP-T (text-to-image alignment). The table shows the number of iterations required for training each model, along with the scores achieved on each metric.  It highlights that the proposed RPO method achieves state-of-the-art results using fewer training steps and less computational resources than existing methods.", "section": "5.2 Results"}, {"figure_path": "TWeVQ5meMW/tables/tables_8_1.jpg", "caption": "Table 2: Ablation study on regularization to evaluate fidelity across multiple subjects and prompts. Standard deviation is included.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different components of the proposed Reward Preference Optimization (RPO) method on the fidelity of subject-driven text-to-image generation. The study compares four different configurations:\n\n1.  **Pure Lsim:** Only the similarity loss is minimized.\n2.  **Lpref w/o early-stopping:** The preference loss is included, but early stopping based on the \u03bb-Harmonic reward is not used.\n3.  **Early-stopping w/o Lpref:** Early stopping is used, but the preference loss is excluded.\n4.  **RPO (\u03bbval = 0.3):** Both the preference loss and early stopping are used with a validation \u03bb-Harmonic reward of 0.3.\n\nThe table reports the mean and standard deviation of DINO, CLIP-I, CLIP-T, and \u03bb-Harmonic scores across multiple subjects and prompts for each configuration.  The results show the effectiveness of combining both the preference loss and early stopping in achieving high-quality image generation.", "section": "5.3 Ablation Study and Method Analysis"}, {"figure_path": "TWeVQ5meMW/tables/tables_8_2.jpg", "caption": "Table 3: Different validation \u03bbval-Harmonic reward comparison for evaluating fidelity over multiple subjects and prompts. Standard deviation is included.", "description": "This table shows the results of an ablation study comparing different values of \u03bbval (a hyperparameter in the \u03bb-Harmonic reward function) on the performance of the RPO model.  It evaluates the model's fidelity across multiple subjects and prompts using DINO, CLIP-I, CLIP-T, and \u03bbval-Harmonic scores, providing standard deviations for each metric.  The study demonstrates how the choice of \u03bbval affects the balance between model regularization and performance.", "section": "5.3 Ablation Study and Method Analysis"}, {"figure_path": "TWeVQ5meMW/tables/tables_14_1.jpg", "caption": "Table 1: Quantitative comparison for the number of iterations, subject fidelity and prompt fidelity.", "description": "This table compares several subject-driven text-to-image generation methods (DreamBooth, SuTI, Textual Inversion, Re-Imagen, DisenBooth, Custom Diffusion, ELITE, IP-Adapter, SSR-Encoder, and RPO) across three metrics: DINO (image-to-image similarity), CLIP-I (image-to-text similarity), and CLIP-T (text-to-image similarity).  It also indicates the number of iterations required for training each method and the backbone model used.", "section": "5.2 Results"}, {"figure_path": "TWeVQ5meMW/tables/tables_15_1.jpg", "caption": "Table 5: Different \u03bbtrain-\u03bb-Harmonic reward comparison for evaluating fidelity over multiple subjects and prompts.", "description": "This table presents the results of an ablation study on the effect of different \u03bbtrain values on the performance of the proposed RPO method.  \u03bbtrain is a hyperparameter controlling the weight of the harmonic mean reward function, influencing the balance between image and text similarity. The table shows the DINO, CLIP-I, and CLIP-T scores for several \u03bbtrain values (0.0, 0.3, 0.5, 0.7), demonstrating how changing the \u03bbtrain value affects the model's ability to generate images that are faithful to both the reference images and textual prompts.  The results indicate that a \u03bbtrain value of 0.7 achieves the best CLIP-I score, suggesting that a stronger emphasis on text-image alignment improves performance on DreamBench.", "section": "5.3 Ablation Study and Method Analysis"}, {"figure_path": "TWeVQ5meMW/tables/tables_15_2.jpg", "caption": "Table 6: Comparison for harmonic mean and arithmetic mean reward function.", "description": "This table compares the performance of the A-Harmonic reward function using both harmonic and arithmetic means for different Aval values (0.3, 0.5, and 0.7).  The results are evaluated across multiple subjects and prompts using DINO, CLIP-I, and CLIP-T scores. The harmonic mean is shown to be more robust and less susceptible to assigning high rewards due to high scores on only one of the two metrics (ALIGN-I and ALIGN-T).", "section": "5.2 Results"}, {"figure_path": "TWeVQ5meMW/tables/tables_16_1.jpg", "caption": "Table 1: Quantitative comparison for the number of iterations, subject fidelity and prompt fidelity.", "description": "This table compares different methods for subject-driven text-to-image generation on the DreamBench dataset.  It shows the number of iterations each method required during training, as well as the performance of each method in terms of DINO (image-to-image similarity), CLIP-I (image-to-text similarity), and CLIP-T (text-to-image similarity).  The table highlights that the proposed RPO method achieves comparable or superior results to existing state-of-the-art methods, while requiring significantly fewer resources.", "section": "5.2 Results"}, {"figure_path": "TWeVQ5meMW/tables/tables_16_2.jpg", "caption": "Table 1: Quantitative comparison for the number of iterations, subject fidelity and prompt fidelity.", "description": "This table presents a quantitative comparison of different subject-driven text-to-image generation methods on the DreamBench dataset.  It compares RPO against several baselines, including DreamBooth and SuTI, across three metrics: DINO (image-to-image similarity), CLIP-I (image-to-image similarity), and CLIP-T (text-to-image similarity). For each method, it indicates the backbone model used, the number of iterations (or training steps) performed, and the resulting scores for each metric. The table highlights RPO's performance relative to existing state-of-the-art methods, showcasing its efficiency and effectiveness in subject-driven image generation.", "section": "5.2 Results"}]