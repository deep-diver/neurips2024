{"references": [{"fullname_first_author": "Shengjia Zhao", "paper_title": "Bias and generalization in deep generative models: An empirical study", "publication_date": "2018-12-03", "reason": "This paper is foundational for understanding bias and generalization in deep generative models, a topic directly relevant to the paper's investigation of diffusion models."}, {"fullname_first_author": "Irina Higgins", "paper_title": "beta-vae: Learning basic visual concepts with a constrained variational framework", "publication_date": "2017-04-24", "reason": "This paper introduces the beta-VAE, a model that addresses disentanglement in latent representations, which is relevant to the paper's study of factorized representations in diffusion models."}, {"fullname_first_author": "Christopher P. Burgess", "paper_title": "Understanding disentangling in B-vae", "publication_date": "2018-04-03", "reason": "This paper delves deeper into disentanglement in variational autoencoders, providing theoretical context and furthering the discussion started by Higgins et al. (2017) regarding disentanglement, a key concept in the paper's context."}, {"fullname_first_author": "Maya Okawa", "paper_title": "Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task", "publication_date": "2023-10-09", "reason": "This paper directly explores compositional generalization in diffusion models, a key focus of the present paper, providing empirical evidence on a toy model."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-06", "reason": "This paper introduces denoising diffusion probabilistic models (DDPMs), the core generative model used in the research presented."}]}