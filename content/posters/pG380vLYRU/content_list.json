[{"type": "text", "text": "Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhenwei Lin Qi Deng Shanghai University of Finance and Economics Shanghai Jiao Tong University zhenweilin@163.sufe.edu.cn qdeng24@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we introduce faster accelerated primal-dual algorithms for minimizing a convex function subject to strongly convex function constraints. Prior to our work, the best complexity bound was $\\bar{\\mathcal{O}}(1/\\varepsilon)$ , regardless of the strong convexity of the constraint function. It is unclear whether the strong convexity assumption can enable even better convergence results. To address this issue, we have developed novel techniques to progressively estimate the strong convexity of the Lagrangian function. Our approach, for the first time, effectively leve\u221arages the constraint strong convexity, obtaining an improved complexity of $\\mathcal{\\dot{O}}(1/\\sqrt{\\varepsilon})$ . This rate matches the complexity lower bound for strongly-convex-concave saddle point optimization and is therefore order-optimal. We show the superior performance of our methods in sparsity-inducing constrained optimization, notably Google\u2019s personalized PageRank problem. Furthermore, we show that a restarted version of the proposed methods can effectively identify the optimal solution\u2019s sparsity pattern within a finite number of steps, a result that appears to have independent significance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we are interested in the following convex function-constrained problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\quad f(\\mathbf{x})\\quad\\mathrm{s.t.}\\quad g_{i}(\\mathbf{x})\\leq0,\\,1\\leq i\\leq m,}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is a convex continuous function and bounded from below and $g_{i}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ , $i=1,2,\\dots,m$ , are strongly convex continuous functions. An important application of this problem, commonly encountered in statistics and engineering, involves the objective $f(\\mathbf{x})$ as a proximalfriendly regularizer and $g_{i}(\\mathbf{x})$ as a data-driven loss function used to gauge model fidelity. ", "page_idx": 0}, {"type": "text", "text": "To apply first-order methods for the above function-constrained problems, a common strategy involves a double-loop procedure that repeatedly employs fast first-order methods, such as Nesterov\u2019s accelerated method, to solve specific strongly convex proximal subproblems. Popular methods among this category include Augmented Lagrangian methods [18, 33], level-set methods [21], penalty methods [17]. When both $f(\\mathbf{x})$ and $g_{i}(\\mathbf{x})$ are convex and smooth (or composite), it has been found that these double-loop algorithms can attain an iteration complexity of ${\\mathcal{O}}(1/\\varepsilon)$ to achieve an $\\varepsilon$ -error in both the optimality gap and constraint vi\u221aolation. When the objective is strongly convex, the complexity can be further improved to $\\mathcal{O}(1/\\sqrt{\\varepsilon})$ ([33, 21]). ", "page_idx": 0}, {"type": "text", "text": "In contrast to these double-loop algorithms, single-loop algorithms remain popular due to their simplicity in implementation. Along this research line, [32] developed a first-order algorithm based on linearizing the augmented Lagrangian function, which obtains an iteration complexity of ${\\mathcal{O}}(1/\\varepsilon)$ . [34] extended the augmented Lagrangian method to stochastic function-constrained problems where both objective and constraint exhibit an expectation form. Viewing (1) as a special case of the min-max problem: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\operatorname*{max}_{\\mathbf{y}\\in\\mathbb{R}^{m}}\\,\\,\\mathcal{L}(\\mathbf{x},\\mathbf{y}):=f(\\mathbf{x})+\\sum_{i=1}^{m}y_{i}g_{i}(\\mathbf{x}),\\quad\\mathrm{s.t.~}y_{i}\\geq0,\\;i=1,2,\\dotsc,m,}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "[11] proposed to solve (1) and (2) by an accelerated primal-dual method (APD), which generalizes the primal-dual hybrid gradient method [6] initially developed for saddle point optimization with bilinear coupling term. Under mild conditions, APD achieves the best iteration comple\u221axity of ${\\mathcal{O}}(1/\\varepsilon)$ for general convex constrained problem and a further improved complexity of $O(1/\\sqrt{\\varepsilon})$ when $f(\\mathbf{x})$ is strongly convex. [4] proposed a unified constrained extrapolation method that can be applied to both deterministic and stochastic constrained optimization problems. ", "page_idx": 1}, {"type": "text", "text": "Despite these recent progresses, to the best of our knowledge, all available algorithms are suboptimal in the presence of strongly convex function constraints (1). Specifically, direct applicatio\u221ans of previously discussed algorithms yield an ${\\mathcal{O}}(1/\\varepsilon)$ complexity, which is inferior to the $\\textstyle{\\mathit{O}}(1/{\\sqrt{\\varepsilon}})$ optimal bound for the strongly-convex-concave saddle point problem [22]. It is somewhat unsatisfactory that the strong convexity of $g\\mathbf{(x)}$ has not been found helpful in further algorithmic acceleration. The core underlying issue arises from the dynamics of saddle point optimization: it is the strong convexity of $\\mathcal{L}(\\cdot,\\mathbf{y})$ that offers more potential acceleration advantages, yet the strong convexity of $\\mathcal{L}(\\cdot,\\mathbf{y})$ is substantially harder to estimate than that of $g\\mathbf{(x)}$ . This difficulty is compounded by the interplay between $g\\mathbf{(x)}$ and the varying dual sequence $\\left\\{\\mathbf{y}_{k}\\right\\}$ . The challenge naturally leads us to question: $I_{S}$ it possible to further improve the convergence rate of first-order methods for solving the strongly convex constrained problem (1)? ", "page_idx": 1}, {"type": "text", "text": "Key intuitions We make an assumption that the minimizer of $f(\\mathbf{x})$ is infeasible for the function constraint $g_{i}(\\mathbf{x})\\leq0$ , $1\\leq i\\leq m$ . If this assumption were not made, we would be dealing with an unconstrained optimization problem that would not depend on $g\\mathbf{(x)}$ . This assumption also implies that the optimal dual variables are non-zero, and as a result, the Lagrangian function is strongly convex with respect to x. By leveraging the strong convexity, we can use more aggressive step sizes and achieve faster convergence rates compared to other state-of-the-art algorithms. ", "page_idx": 1}, {"type": "text", "text": "Applications in sparsity-constrained optimization We consider the constrained Lasso-type problem, which minimizes a sparsity-inducing regularizer while explicitly ensuring data-driven error remains controlled: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\ \\|\\mathbf{x}\\|_{1}\\quad\\mathrm{s.t.}\\ g(\\mathbf{x})\\leq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $g(\\cdot)$ is a convex smooth loss term. A motivating application is the approximate personalized PageRank problem [8], where $g(\\mathbf{x})\\;=\\;{\\textstyle\\frac{1}{2}}\\langle\\mathbf{x},Q\\mathbf{x}\\rangle\\,-\\,\\langle\\dot{\\mathbf{b}},\\mathbf{x}\\rangle$ is strongly convex quadratic and $Q$ integrates the graph Laplacian with an identity matrix. Compared to the standard Lasso problem [30], $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}g(\\mathbf{x})\\!+\\!\\bar{\\lambda}\\|\\mathbf{x}\\|_{1}^{-}}\\end{array}$ , the constrained problem (3) offers enhanced control over the data ftiting error. This advantage, however, is counterbalanced by the challenge of dealing with a nonlinear constraint. Besides concerns about the efficiency in solving (3), it is often desired to show the active set (or sparsity) identification, namely, the nonzero patterns of the optimal solution $\\mathbf{x}^{*}$ can be identified by the solution sequence $\\left\\{{\\bf x}_{k}\\right\\}$ in a finite number of iterations. Identifying the embedded solution structure within a broader context is referred to as the manifold identification problem [31, 12]. Exploiting the sparsity pattern is particularly desirable in large-scale PageRank problems, as it could result in significant runtime savings. For the regularized Lasso-type problem, it has been known that proximal gradient methods (e.g. [14, 19, 24]) possess the finite active-set identification property. Specifically, [24] introduced \u201cactive set complexity\u201d, which is defined as the number of iterations required before an algorithm is guaranteed to have reached the optimal manifold, and they proved the proximal gradient method with constant stepsize can identify the optimal manifold in a finite number of iterations. However, for the problem (3), it remains unclear whether first-order methods can identify the sparsity pattern in finite time. ", "page_idx": 1}, {"type": "text", "text": "Contributions We address the theoretical questions about strongly convex constrained optimization and the application of sparse optimization. Our contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "First, we present a new accelerated primal-dual algorithm with progressive strong convexity estimation (APDPro) for solving problem (1). APDPro employs a novel strategy to estimate the lower bound of the dual variables, which leads to a gradually refined estimated strong convexity modulus of $\\mathcal{L}(\\cdot,\\mathbf{y})$ . With additional cut constraints on the dual update, APDPro is able to separate the dual search space from the origin point, which is critical for maintaining the desired strong convexity over the en\u221atire solution path. With these two important ingredients, APDPro exhibits an $\\bar{\\mathcal O}\\bigl((\\|\\mathbf x_{0}-\\dot{\\mathbf x}^{*}\\|+D_{Y})/\\sqrt\\varepsilon\\bigr)$ complexity bound to obtain an $\\varepsilon$ -error on the function value gap and constraint violation, where $D_{Y}$ is a known upper-bound of $\\|\\mathbf{y}_{0}-\\mathbf{y}^{*}\\|$ . Moreover, we show that for the last iterate to have an $\\varepsilon$ e\u221arror (i.e., $\\|\\mathbf{x}_{K}-\\mathbf{x}^{*}\\|^{2}\\,\\le\\,\\varepsilon)$ , APDPro requires a total iteration of $\\mathcal{O}\\big((\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|+\\|\\mathbf{y}_{0}-\\mathbf{y}^{*}\\|)/\\sqrt\\varepsilon\\big)$ Both complexity results appear new in the literature for strongly convex-constrained optimization. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Second, we present a new restart algorithm (rAPDPro) which calls APDPro repeatedly with the input parameters properly changing over time. Different from APDPro, rAPDPro dynamically adjusts the iteration number of APDPro in each epoch based on the prog\u221aressive stron\u221ag convexity estimation. We show that rAPDPro exhibits a complexity of $\\mathcal{O}\\big(\\log(D_{X}/\\sqrt\\varepsilon)+D_{Y}/\\sqrt\\varepsilon\\big)$ to ensure $\\varepsilon$ -error in the last iterate convergence where $D_{X}$ is the e\u221astimated diameter of the primal feasible domain. While it is difficult to improve the overall $O(1/\\sqrt{\\varepsilon})$ bound, rAPDPro appears to be more advantageous when $D_{X}$ and $D_{Y}$ are the same order of $\\lVert\\mathbf{x}_{0}-\\mathbf{x}^{*}\\rVert$ and $\\|\\mathbf{y}_{0}-\\mathbf{y}^{*}\\|$ , respectively, and $D_{X}\\gg D_{Y}$ . In addition, we show that a similar restart strategy can further accelerate the st\u221aandard APD. The multistage-accelerated primal dual method (msAPD) obtains a comparable $\\mathcal{O}(1/\\sqrt{\\varepsilon})$ complexity of APDPro without introducing additional cut constraint. ", "page_idx": 2}, {"type": "text", "text": "Third, we apply our proposed methods to the s\u221aparse learning problem (3). In view of the theoretical analysis, all our methods converge at an $O(1/\\Bar{\\sqrt{\\varepsilon}})$ rate, which is substantially better than the rates of state-of-the-art first-order algorithms. Moreover, we conduct a new analysis to show that the restart algorithm rAPDPro has the favorable feature of identifying the optimal sparsity pattern. Note that such active-set/manifold identification is substantially more challenging to prove due to the coupling of dual variables and constraint functions. To establish the desired property, we develop asymptotic convergence of the dual sequence to the optimal solution, which can be of independent interest. ", "page_idx": 2}, {"type": "text", "text": "Outline Section 2 sets notations and assumptions for the later analysis. Section 3 presents the APDPro algorithm and develops its stepsize rule and complexity rate. Section 4 presents the restart APDPro (rAPDPro) algorithm. Section 5 applies our proposed methods for sparsity-inducing optimization and shows the sparsity identification result for rAPDPro. Section 6 empirically examine the convergence performance and sparsity identification of our proposed algorithms. Finally, we draw the conclusion in Section 7. All the missing proofs are provided in the appendix sections. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We use bold letters like $\\mathbf{x}$ to represent vectors. Suppose $\\textbf{x}\\in\\mathbb{R}^{n}$ , $q~\\geq~1$ , we use $\\left\\|\\mathbf{x}\\right\\|_{q}\\;=$ $(\\textstyle\\sum_{i=1}^{n}|\\mathbf{x}_{(i)}|^{q})^{1/q}$ to represent the $l_{q}$ -norm, where ${\\bf x}_{(i)}$ is the $i$ -th element of $\\mathbf{x}$ . For brevity, $\\|\\mathbf{x}\\|$ stands for $l_{2}$ -norm. For a matrix $A$ , we denote the matrix norm induced by 2-norm as $\\|A\\|=\\operatorname*{sup}_{\\|\\mathbf{x}\\|\\leq1}\\left\\|A\\mathbf{x}\\right\\|$ . The normal cone of $\\boldsymbol{\\mathcal{U}}$ at $\\mathbf{u}$ is denoted as $\\mathcal{N}_{\\mathcal{U}}(\\mathbf{u}):=\\left\\{\\mathbf{v}\\ |\\ \\langle\\mathbf{v},\\mathbf{x}-\\mathbf{u}\\rangle\\le0,\\forall\\mathbf{x}\\in\\mathcal{U}\\right\\}$ . Let $B(\\mathbf{x},r)$ be the closed ball centered at $\\mathbf{x}$ with radius $r>0$ , i.e., $B(\\mathbf{x},r)=\\left\\{\\mathbf{y}\\mid\\|\\mathbf{y}-\\mathbf{x}\\|\\leq r\\right\\}$ . We denote the set of feasible solutions by $\\mathcal{X}_{G}:=\\{\\mathbf{x}\\mid g_{i}(\\mathbf{x})\\leq0,\\forall i\\in[m]\\}$ and write the constraint function as $G(\\mathbf{x}):=[g_{1}(\\mathbf{x}),\\ldots,g_{m}(\\mathbf{x})]^{\\top}$ . We assume each $g_{i}(\\mathbf{x})$ is a $\\mu_{i}$ strongly convex function, and denote $\\pmb{\\mu}:=[\\mu_{1},\\ldots,\\mu_{m}]^{\\top}$ . Let $[m]:=\\{1,\\ldots,m\\}$ for integer $m$ . We denote minimum and maximum strongly convexity $\\underline{{\\mu}}:=\\operatorname*{min}_{j\\in[m]}\\{\\mu_{j}\\}$ , and $\\bar{\\mu}:=\\operatorname*{max}_{j\\in[m]}\\{\\mu_{j}\\}$ and the vector of elements 0 by 0. The Lagrangian function of problem (1) is given by $\\mathcal{L}(\\mathbf{x},\\mathbf{\\dot{y}}):=f(\\mathbf{x})+\\langle\\mathbf{y},G(\\mathbf{x})\\rangle$ where $\\mathbf{y}\\in\\mathbb{R}_{+}^{m}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (KKT condition). We say that $\\mathbf{x}^{*}$ satisfies the KKT condition of (1) if there exists $a$ Lagrangian multiplier vector $\\mathbf{y}^{*}\\in\\mathbb{R}_{+}^{m}$ such that $\\mathbf{0}\\in\\partial_{x}\\mathcal{L}(\\mathbf{x}^{*},\\mathbf{y}^{*})$ and $\\langle\\mathbf{y}^{*},G(\\mathbf{x}^{*})\\rangle=0$ . ", "page_idx": 2}, {"type": "text", "text": "The KKT condition is necessary for optimality when a constraint qualification (CQ) holds at $\\mathbf{x}^{*}$ . We assume Slater\u2019s CQ (Assumption 1) holds, which guarantees that an optimal solution is also a KKT point [3]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. There exists a strictly feasible point $\\widetilde{\\mathbf{x}}\\in\\mathbb{R}^{n}$ such that $G(\\widetilde{\\mathbf{x}})<\\mathbf{0}$ . ", "page_idx": 2}, {"type": "text", "text": "We use $\\tilde{\\bf x}$ to denote a strictly feasible point throughout the paper. Moreover, we require Assumption 2 to circumvent any trivial solution. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2. For any $\\begin{array}{r}{\\mathbf{x}_{0}^{*}\\in\\mathrm{argmin}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\;f(\\mathbf{x})}\\end{array}$ , there exists an $i\\in[m]$ such that $g_{i}(\\mathbf{x}_{0}^{*})>0$ . ", "page_idx": 2}, {"type": "text", "text": "Remark 1. Assumption 2 is essential for our analysis. While verifying Assumption 2 can be indeed challenging, it is achievable for the sparsity-inducing problem considered in our paper. In this example, the solution $\\mathbf{x}_{0}^{*}=\\mathbf{0}$ is the single minimizer of the sparsity penalty. ", "page_idx": 3}, {"type": "text", "text": "Next, we give several useful properties about the optimal solutions of problem (1). Please refer to Appendix D.1 for the proof of Proposition 1 and Appendix D.2 for the proof of Proposition 2. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Suppose Assumption 1 holds. Then, for any optimal solution $\\mathbf{x}^{*}$ of problem (1), there exists $\\mathbf{y}^{*}\\in\\mathbb{R}^{m}$ such that KKT condition holds. Moreover, $\\mathbf{y}^{*}$ falls into set $\\mathcal{V}:=\\left\\{\\mathbf{y}\\mid\\|\\mathbf{y}\\|_{1}\\leq\\bar{c}\\right\\}$ , where c\u00af := f( x)\u2212minx\u2208Rn f(x). ", "page_idx": 3}, {"type": "text", "text": "Proposition 2. Under Assumption 2, $\\mathbf{x}^{*}$ is the unique solution of (1). Furthermore, set $y^{\\ast}=$ $\\operatorname{argmax}_{\\mathbf{y}\\in\\mathbb{R}_{+}^{m}}\\mathcal{L}(\\mathbf{x}^{*},\\mathbf{y})$ is convex and bounded. ", "page_idx": 3}, {"type": "text", "text": "In view of Assumption 2, Proposition 2, and closedness of the subdifferential set of proper convex functions [2, Theorem 3.9], [27, Chapter 23], we know that $\\mathbf{dist}(\\partial f(\\mathbf{x}^{*}),\\mathbf{0})\\;>\\;0$ , where $\\begin{array}{r}{\\mathbf{dist}(\\partial f(\\mathbf{x}^{*}),\\mathbf{0}):=\\operatorname*{min}_{\\xi\\in\\partial f(\\mathbf{x}^{*})}\\left\\lVert\\xi\\right\\rVert}\\end{array}$ . Furthermore, we make the following assumption: ", "page_idx": 3}, {"type": "text", "text": "Assumption 3. Throughout the paper, suppose that a constant $r$ satisfying ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{dist}(\\partial f(\\mathbf{x}^{*}),\\mathbf{0})\\geq r>0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is known. ", "page_idx": 3}, {"type": "text", "text": "We give some important examples for which the lower bound $r$ can be estimated. Suppose $f(\\mathbf{x})$ is a Lasso regularizer, i.e., $f(\\mathbf{x})=\\|\\mathbf{x}\\|_{1}$ , then $r=1$ satisfies (4). More general, consider the group Lasso regularizer, i.e., $\\begin{array}{r}{f(\\mathbf{x})=\\sum_{i=1}^{B}p_{i}\\Vert\\mathbf{x}_{(i)}\\Vert.}\\end{array}$ , where $\\mathbf{x}_{(i)}\\in\\mathbb{R}^{b_{i}}$ and $\\textstyle\\sum_{i=1}^{B}b_{i}=n$ , $B$ is the number of blocks, then $r=\\mathrm{min}_{i\\in[B]}\\{p_{i}\\}$ when $\\mathbf{x}^{*}\\neq\\mathbf{0}$ . Another example is $f(\\mathbf{x})=\\mathbf{c}^{\\top}\\mathbf{x}$ , then we have $r=\\|\\mathbf{c}\\|$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 2. Condition (4) is similar to the bounded gradient assumption that has been used for accelerating the convergence of the Frank-Wolfe algorithm. See Appendix B for more discussions. ", "page_idx": 3}, {"type": "text", "text": "When considering the Lipschitz continuity of function in $\\mathbb{R}^{n}$ , even quadratic functions are not Lipschitz continuous. However, the Lipschitz continuity of $g_{i}(x)$ is crucial for algorithm convergence. Therefore, we define the bounded feasible region in the following proposition, with its proof provided in Appendix D.3. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3. Let $\\begin{array}{r}{\\mathcal{X}:=\\mathcal{B}\\big(\\widetilde{\\mathbf{x}},\\operatorname*{min}_{i\\in[m]}2\\sqrt{\\frac{-2g_{i}(\\mathbf{x}_{i}^{*})}{\\mu_{i}}}\\big)}\\end{array}$ , where $\\begin{array}{r}{\\mathbf{x}_{i}^{\\ast}=\\operatorname*{argmin}_{\\mathbf{x}\\in\\mathbb{R}^{n}}g_{i}(\\mathbf{x})}\\end{array}$ . Then under Assumptions $^{\\,I}$ and 2, we have $\\mathbf{x}^{*}\\in\\mathbf{int}\\,\\mathcal{X}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 4. There exist $L_{X},L_{G}>0$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|\\nabla G({\\bf x})-\\nabla G(\\bar{\\bf x})\\|\\leq L_{X}\\|{\\bf x}-\\bar{\\bf x}\\|,\\;\\;\\forall{\\bf x},\\bar{\\bf x}\\in\\mathcal{X},}\\\\ &{}&{\\|G({\\bf x})-G(\\bar{\\bf x})\\|\\leq L_{G}\\|{\\bf x}-\\bar{\\bf x}\\|,\\;\\;\\forall{\\bf x},\\bar{\\bf x}\\in\\mathcal{X},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\nabla G(\\mathbf{x}):=[\\nabla g_{1}(\\mathbf{x}),\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\nabla g_{m}(\\mathbf{x})]\\in\\mathbb{R}^{n\\times m}$ and $\\mathcal{X}$ is defined in Proposition 3. ", "page_idx": 3}, {"type": "text", "text": "The Lipschitz smoothness of the Lagrangian function with respect to the primal variable $\\mathbf{x}$ is crucial for the convergence of algorithms. Given that the dual variable y is bounded from above, and considering the smoothness of the constraint functions, we can derive the smoothness of the Lagrangian function. Combining (5) and the fact $\\|\\mathbf{y}\\|\\leq\\|\\mathbf{y}\\|_{1}\\leq\\bar{c},\\forall\\mathbf{y}\\in\\mathcal{Y}$ , we obtain that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla G(\\mathbf{x})\\mathbf{y}-\\nabla G(\\bar{\\mathbf{x}})\\mathbf{y}\\|\\leq L_{X Y}\\|\\mathbf{x}-\\bar{\\mathbf{x}}\\|\\ \\forall\\mathbf{x},\\bar{\\mathbf{x}}\\in\\mathcal{X},\\ \\forall\\mathbf{y}\\in\\mathcal{Y},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $L_{X Y}=\\bar{c}L_{X}$ . For set $\\mathcal{X},\\mathcal{Y}$ , we use $D_{X}$ and $D_{Y}$ to denote their diameters, respectively, i.e., $\\begin{array}{r}{D_{X}:=\\operatorname*{max}_{\\mathbf{x}_{1},\\mathbf{x}_{2}\\in\\mathcal{X}}\\left\\|\\mathbf{x}_{1}-\\mathbf{x}_{2}\\right\\|}\\end{array}$ and $\\begin{array}{r}{D_{Y}:=\\operatorname*{max}_{\\mathbf{y}_{1},\\mathbf{y}_{2}\\in\\mathcal{Y}}\\left\\|\\mathbf{y}_{1}-\\mathbf{y}_{2}\\right\\|}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "3 APD with progressive strong convexity estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We present the Accelerated Primal-Dual Algorithm with Progressive Strong Convexity Estimation (APDP\u221aro) to solve problem (1). For problem (1), APDPro achieves the improved convergence rate $\\mathcal{O}(1/\\sqrt{\\varepsilon})$ without relying on the uniform strong convexity assumption [11, 22]. For the rest of this ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Accelerated Primal-Dual Algorithm with Progressive Strong Convexity Estimation (APDPro) ", "page_idx": 4}, {"type": "text", "text": "Require: $\\tau_{0}>0,\\sigma_{0}>0,\\mathbf{x}_{0}\\in\\mathcal{X},\\mathbf{y}_{0}\\in\\mathcal{Y},\\rho_{0}\\geq0,N>0$   \n213:::   fISonerit $\\left(\\mathbf{x}_{-1},\\mathbf{y}_{-1}\\right)\\gets\\left(\\mathbf{x}_{0},\\mathbf{y}_{0}\\right),\\bar{\\mathbf{x}}_{0}\\gets\\mathbf{x}_{0},\\sigma_{-1}\\gets\\sigma_{0},T_{0}=0$   \n$\\begin{array}{r}{\\Delta_{X Y}=\\frac{1}{2\\tau_{0}}D_{X}^{2}+\\frac{1}{2\\sigma_{0}}D_{Y}^{2}}\\end{array}$   \n$k=0,1,\\ldots,N$   \n4: $\\mathcal{V}_{k}\\gets\\left\\{\\mathbf{y}\\in\\mathbb{R}_{+}^{m}\\;|\\;\\|\\mathbf{y}\\|_{1}\\cdot\\underline{{\\mu}}\\geq\\rho_{k}\\right\\}\\bigcap\\mathcal{V},$   \n5: $\\begin{array}{r}{\\mathbf{\\bar{z}}_{k}\\leftarrow(1+\\sigma_{k-1}/\\sigma_{k})G(\\mathbf{x}_{k})-(\\sigma_{k-1}/\\sigma_{k})G(\\mathbf{x}_{k-1}}\\end{array}$ )   \n6: $\\begin{array}{r}{\\mathsf{y}_{k+1}\\gets\\mathrm{argmin}_{\\mathbf{y}\\in\\mathcal{y}_{k}}\\,\\|\\mathbf{y}-\\left(\\mathbf{y}_{k}+\\sigma_{k}\\mathbf{z}_{k}\\right)\\|^{2}}\\end{array}$   \n7: $\\begin{array}{r}{\\mathbf{x}_{k+1}\\leftarrow\\mathrm{prox}_{f,\\mathcal{X}}(\\mathbf{x}_{k}-\\tau_{k}\\nabla G(\\mathbf{x}_{k})\\mathbf{y}_{k+1},\\tau_{k})}\\end{array}$   \n98:: CUopdmapteu $\\mathrm{te}\\ t_{k},\\quad\\bar{\\mathbf{x}}_{k+1}\\gets(T_{k}\\bar{\\mathbf{x}}_{k}+t_{k}\\mathbf{x}_{k+1})/(T_{k}+t_{k}),\\quad T_{k+1}\\gets T_{k}+t_{k}$ $\\rho_{k+1}\\gets\\mathrm{IMPROVE}(\\mathbf{x}_{k},\\bar{\\mathbf{x}}_{k}$ $\\frac{\\sigma_{0}\\tau_{k-1}\\Delta_{X Y}}{\\sigma_{k-1}}$   \n10: Update $\\tau_{k+1}$ and $\\sigma_{k+1}$ depending on $\\rho_{k+1}$   \n11: end for   \n12: Output: ${\\bf x}_{N+1},{\\bf y}_{N+1}$   \n13: procedure $\\begin{array}{r l}&{\\frac{\\mathbf{\\mu}_{\\mathrm{{NIPROVE}}}}{\\mathbf{IMPROVE}}(\\mathbf{x},\\bar{\\mathbf{x}},\\beta,\\bar{\\beta},\\rho_{\\mathrm{old}})}\\\\ &{\\mathbf{e}\\rho=\\underline{{\\mu}}\\cdot\\operatorname*{max}\\left\\{r\\big[\\|\\nabla G(\\mathbf{x})\\|+L_{X}\\sqrt{2\\beta}\\big]^{-1},\\Big[\\frac{L_{X}}{r}\\sqrt{\\frac{\\bar{\\beta}}{2\\underline{{\\mu}}}}+\\sqrt{\\frac{L_{X}^{2}\\bar{\\beta}}{2\\underline{{\\mu}}r^{2}}+\\frac{\\|\\nabla G(\\bar{\\mathbf{x}})\\|}{r}}\\Big]^{-2}\\right\\}}\\\\ &{\\mathbf{\\mu}=\\operatorname*{max}\\{\\rho_{\\mathrm{old}},\\rho\\}}\\end{array}$   \n14: Comput   \n15: Set   \n16: return \u03c1new   \n17: end procedure ", "page_idx": 4}, {"type": "text", "text": "paper, we denote $\\begin{array}{r}{\\mathrm{prox}_{f,\\mathcal{X}}(\\mathbf{x}-\\eta\\mathbf{z},\\eta):=\\mathrm{argmin}_{\\hat{\\mathbf{x}}\\in\\mathcal{X}}\\,f(\\hat{\\mathbf{x}})+\\langle\\mathbf{z},\\hat{\\mathbf{x}}\\rangle+\\frac{1}{2\\eta}\\|\\hat{\\mathbf{x}}-\\mathbf{x}\\|^{2}}\\end{array}$ as the proximal mapping. ", "page_idx": 4}, {"type": "text", "text": "We describe APDPro in Algorithm 1. The main component of APDPro contains a dual ascent step to update $\\mathbf{y}_{k}$ based on the extrapolated gradient, followed by a primal proximal step to update $\\mathbf{x}_{k}$ Compared with standard APD [11], APDPro has two more steps. First, line 4 of Algorithm 1 applies a novel cut constraint to separate the dual sequence $\\left\\{\\mathbf{y}_{k}\\right\\}$ from the origin, which allows us to leverage the strong convexity of the Lagrangian function and hence obtain a faster rate of convergence than APD. Second, to use the strong convexity more effectively, in line 9, we perform a progressive estimation of the strong convexity by using the latest iterates $\\mathbf{x}_{k}$ and $\\bar{\\bf x}_{k}$ . Throughout the algorithm process, we use a routine IMPROVE to construct a non-decreasing sequence $\\{\\bar{\\rho_{k}}\\}$ , which provides increasingly refined lower bounds of the strong convexity of the Lagrangian function. ", "page_idx": 4}, {"type": "text", "text": "The IMPROVE step In order to estimate the strong convexity of the Lagrangian function, we rely on the subdifferential separation (eq. (4)) to bound the dual variables. From the first-order optimality condition in minimizing $\\mathcal{L}(\\mathbf{x},\\mathbf{y}^{*})$ and the fact that $\\mathbf{x}^{*}\\in\\mathbf{int}\\,\\mathcal{X}$ (Proposition 3), we have $\\begin{array}{r}{\\overline{{\\mathbf{0}^{\\prime}}}\\in\\partial f(\\dot{\\mathbf{x}^{*}})+\\nabla G(\\mathbf{x}^{*})\\mathbf{y}^{*}+\\mathcal{N}_{\\boldsymbol{\\chi}}(\\mathbf{\\bar{x}^{*}})=\\partial f(\\mathbf{x}^{*})+\\nabla G(\\mathbf{x}^{*})\\mathbf{y}^{*}.}\\end{array}$ It follows from (4) that ", "page_idx": 4}, {"type": "equation", "text": "$$\nr\\leq\\|\\nabla G(\\mathbf{x}^{*})\\mathbf{y}^{*}\\|\\leq\\|\\nabla G(\\mathbf{x}^{*})\\|\\cdot\\|\\mathbf{y}^{*}\\|\\leq\\|\\mathbf{y}^{*}\\|_{1}\\|\\nabla G(\\mathbf{x}^{*})\\|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the last inequality use the fact that $\\|\\cdot\\|\\leq\\|\\cdot\\|_{1}$ . Note that the bound $\\|\\mathbf{y}^{*}\\|_{1}\\geq r/\\|\\nabla G(\\mathbf{x}^{*})\\|$ can not be readily used in the algorithm implementation because $\\mathbf{x}^{*}$ is generally unknown. To resolve this issue, we develop more concrete dual lower bounds by using the generated solution $\\hat{\\bf x}$ in the proximity of $\\mathbf{x}^{*}$ . As we will show in the analysis, APDPro keeps track of two primal sequences $\\left\\{{\\bf x}_{k}\\right\\}$ and $\\left\\{\\bar{\\bf x}_{k}\\right\\}$ , for which we can establish bounds on $\\lVert\\mathbf{x}_{k}-\\dot{\\mathbf{x}}^{*}\\rVert^{2}$ and $(\\mathbf{y}^{*})^{\\top}{\\mathbf{\\bar{\\mu}}}\\cdot\\|\\hat{{\\mathbf{x}}}-\\mathbf{\\bar{x}}^{*}\\|^{2}/2,$ respectively. This drives us to develop the following lower bound property, with the proof provided in Appendix E.1. ", "page_idx": 4}, {"type": "text", "text": "Proposition 4. Suppose Assumption 4 holds. Let $\\mathbf{y}^{*}\\in\\mathcal{V}^{*}$ be a dual optimal solution. ", "page_idx": 4}, {"type": "text", "text": "1. Suppose that $\\|\\hat{\\mathbf{x}}-\\mathbf{x}^{*}\\|^{2}\\leq2\\beta,$ , then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{y}^{*}\\|_{1}\\geq h_{1}(\\hat{\\mathbf{x}},\\beta):=r\\big[\\|\\nabla G(\\hat{\\mathbf{x}})\\|+L_{X}\\sqrt{2\\beta}\\big]^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2. Suppose $(\\mathbf{y}^{*})^{\\top}{\\boldsymbol{\\mu}}\\cdot\\|\\hat{\\mathbf{x}}-\\mathbf{x}^{*}\\|^{2}\\leq2\\beta$ , then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{y}^{*}\\|_{1}\\geq h_{2}(\\hat{\\mathbf{x}},\\beta):=\\left[\\frac{L_{X}}{r}\\sqrt{\\frac{\\beta}{2\\underline{{\\mu}}}}+\\sqrt{\\frac{L_{X}^{2}\\beta}{2\\underline{{\\mu}}r^{2}}+\\frac{\\|\\nabla G(\\hat{\\mathbf{x}})\\|}{r}}\\right]^{-2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our next goal is to conduct the convergence analysis for APDPro in Theorem 1 and Corollary 1.   \nComplete proof details are provided in Appendix E.2 and E.3. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Suppose for any $\\mathbf{y}^{*}\\in\\mathcal{V}^{*}$ , $(\\mathbf{y}^{*})^{\\top}\\pmb{\\mu}\\geq\\rho_{0}$ holds, and let the sequence $\\{\\tau_{k},\\sigma_{k},t_{k},\\rho_{k+1}\\}$ generated by Algorithm $^{\\,I}$ satisfy: ", "page_idx": 5}, {"type": "equation", "text": "$$\nt_{k+1}(\\tau_{k+1}^{-1}-\\rho_{k+1})\\leq t_{k}\\tau_{k}^{-1},\\quad t_{k+1}\\sigma_{k+1}^{-1}\\leq t_{k}\\sigma_{k}^{-1},\\quad L_{X Y}+L_{G}^{2}\\sigma_{k}\\leq\\tau_{k}^{-1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, the set $\\mathcal{V}_{k}$ is nonempty and $\\mathcal{V}^{*}\\subseteq\\mathcal{V}_{k}$ . Let $\\begin{array}{r}{\\Delta(\\mathbf{x},\\mathbf{y}):=\\frac{1}{2\\tau_{0}}\\|\\mathbf{x}-\\mathbf{x}_{0}\\|^{2}+\\frac{1}{2\\sigma_{0}}\\|\\mathbf{y}-\\mathbf{y}_{0}\\|^{2},\\bar{\\mathbf{y}}_{K}=}\\end{array}$ $T_{K}^{-1}\\sum_{s=0}^{K-1}t_{s}\\mathbf{y}_{s}$ . The sequence $\\left\\{{\\bar{\\bf x}}_{k},{\\bf x}_{k},{\\bar{\\bf y}}_{k}\\right\\}$ generated by APDPro satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{t_{K-1}\\tau_{K-1}^{-1}}{2T_{K}}\\|\\mathbf{x}^{*}-\\mathbf{x}_{K}\\|^{2}+\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y}^{*})-\\mathcal{L}(\\mathbf{x}^{*},\\bar{\\mathbf{y}}_{K})\\leq\\frac{1}{T_{K}}\\Delta(\\mathbf{x}^{*},\\mathbf{y}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next, we develop more concrete complexity results in Corollary 1. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. Suppose that $\\sigma_{k},\\tau_{k},t_{k}$ satisfy: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\tau_{0}^{-1}\\geq L_{X Y}+L_{G}^{2}\\sigma_{0},\\ t_{k}=\\sigma_{k}/\\sigma_{0},}}\\\\ {{\\tau_{k+1}=\\tau_{k}/\\sqrt{1+\\rho_{k+1}\\tau_{k}},\\ \\sigma_{k+1}=\\sigma_{k}\\tau_{k}/\\tau_{k+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\bar{\\mathbf{x}}_{K})-f(\\mathbf{x}^{*})\\leq\\frac{6}{6+\\tau_{0}\\tilde{\\rho}_{K}(K+1)K}\\Big(\\frac{1}{2\\tau_{0}}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}+\\frac{D_{Y}^{2}}{2\\sigma_{0}}\\Big),}\\\\ &{\\quad\\|[G(\\bar{\\mathbf{x}}_{K})]_{+}\\|\\leq\\frac{6}{c^{*}(6+\\tau_{0}\\tilde{\\rho}_{K}(K+1)K)}\\Big(\\frac{1}{2\\tau_{0}}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}+\\frac{D_{Y}^{2}}{2\\sigma_{0}}\\Big),}\\\\ &{\\frac{1}{2}\\|\\mathbf{x}_{K}-\\mathbf{x}^{*}\\|^{2}\\leq\\frac{3\\sigma_{0}}{\\hat{\\rho}_{K}^{2}\\tau_{0}^{2}K^{2}+9(\\sigma_{0}/\\tau_{0})}\\Delta(\\mathbf{x}^{*},\\mathbf{y}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{c^{\\ast}:=\\big(f(\\mathbf{x}^{\\ast})-\\operatorname*{min}_{\\mathbf{x}}f(\\mathbf{x})\\big)/\\operatorname*{min}_{i\\in[m]}\\{-g_{i}(\\widetilde\\mathbf{x})\\}>0,\\,\\widetilde{\\rho}_{k}=2\\sum_{s=0}^{k}\\widehat{\\rho}_{s}s/\\big(k(k+1)\\big)}\\end{array}$ and $\\tilde{\\rho}_{k}$ satisfy the following condition, $\\hat{\\rho}_{k+1}:=\\sqrt{\\hat{\\rho}_{k}^{2}k^{2}+(3\\rho_{k+1}\\hat{\\rho}_{k})k}/(k+1),\\hat{\\rho}_{1}=3\\sqrt{{\\rho}_{1}/{\\tau}_{0}}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 3. In view of Corollary $^{\\,I}$ , APDPro obtains an iteration complexity of $O(1/\\sqrt{\\tilde{\\rho}_{K}\\varepsilon})$ , which is substantially better than the ${\\mathcal{O}}(1/\\varepsilon)$ bound of APD [11] and ConEx [4] when the strong convexity parameter $\\tilde{\\rho}_{K}$ is relatively large compared with $\\varepsilon$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 4. Additionally, we argue that even when $\\tilde{\\rho}_{K}~=~{\\cal O}(\\varepsilon)$ , APDPro can obtain the matching ${\\mathcal{O}}(1/\\varepsilon)$ bound of the state-of-the-art algorithms. Specifically, using the definition of $\\sigma_{k},\\tau_{k}$ , we can easily derive the monotonicity of $\\bar{\\{\\sigma_{k}\\}}$ . It follows from $\\sigma_{k+1}\\,=\\,\\tau_{k}\\sigma_{k}/\\tau_{k+1}\\,=$ $\\tau_{k}\\sigma_{k}/(\\tau_{k}/\\sqrt{1+\\rho_{k+1}\\tau_{k}})\\ \\geq\\ \\sigma_{k}$ , that $\\begin{array}{r}{T_{k}~=~\\sum_{s=0}^{k-1}t_{k}~=~\\sigma_{0}^{-1}\\sum_{s=0}^{k-1}\\sigma_{k}~\\geq~k~}\\end{array}$ . Using a similar argument to that of Corollary , we ob tain the bound $f(\\bar{\\bf x}_{K})\\,-\\,f({\\bf x}^{*})\\;\\le\\;{\\mathcal O}(1/K)$ and $\\|[G(\\bar{\\mathbf{x}}_{K})]_{+}\\|\\leq\\mathcal{O}(1/K)$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 5. The implementation of APDPro requires knowing an upper bound on $\\|\\mathbf{y}^{*}\\|$ . When the bound is unavailable, $I I I J$ developed an adaptive APD which still ensures the boundedness of dual sequence via line search. Since our main goal of this paper is to exploit the lower-bound rather than the upper bound of $\\|\\mathbf{y}^{*}\\|$ , we leave the extension for the future work. ", "page_idx": 5}, {"type": "text", "text": "4 APDPro with a restart scheme ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Note that in the worst case, APDPro exhibits an iteration complexity of $\\mathcal{O}\\big((D_{X}+D_{Y})/\\sqrt{\\varepsilon}\\big)$ , which has a linear dependence on the diameter. While the $O(1/\\sqrt{\\varepsilon})$ is optimal [25], it is possible to improve the complexity with respect to the primal part from $\\mathcal{O}(D_{X}/\\sqrt{\\varepsilon})$ to ${\\mathcal{O}}{\\bigl(}\\log{\\bigl(}D_{X}/{\\sqrt{\\varepsilon}}{\\bigr)}{\\bigr)}$ . To achieve this goal, we propose a restart scheme (rAPDPro) that calls APDPro repeatedly and present the details in Algorithm 2. Inspired by [16], we set the iteration number as a function of the estimated strong convexity, detailed in the TERMINATEITER procedure. For convenience in describing a double-loop algorithm, we use superscripts for the number of epochs and subscripts for the number of sub-iterations in parameters $\\mathbf{x},\\mathbf{y},\\tau,\\sigma$ , e.g., $\\mathbf{x}_{1}^{S}$ meaning the $\\mathbf{x}$ output of first iterations at $S$ -th epoch. To avoid redundancy in the Algorithm 2, we call the APDPro iteration directly. Note that the notation system here is identical to that of APDPro, with the only difference being the use of superscripts to distinguish the number of epochs. ", "page_idx": 5}, {"type": "text", "text": "In Theorem 2, we show the overall convergence complexity of rAPDPro with the proof provided in Appendix F.1. ", "page_idx": 5}, {"type": "text", "text": "Require: $\\rho_{N_{-1}}^{-1}\\geq0,\\bar{\\sigma}>0$ , \u03bd0 \u2208(0, 1), \u03b4 \u2208(0, 1), x\u2212N1\u22121, yN\u22121\u22121, S   \n1: Compute $\\bar{\\tau}=\\left(1-\\nu_{0}\\right)\\left(L_{X Y}+L_{G}^{2}\\bar{\\sigma}/\\delta\\right)^{-1}$   \n2: for $s=0,1,\\ldots,S\\,\\,\\mathbf{0}$ o   \n3: $\\begin{array}{r l}&{\\quad=\\bar{\\tau},\\sigma_{0}^{s}=\\bar{\\sigma},(\\mathbf{x}_{-1}^{s},\\mathbf{y}_{-1}^{s})\\gets(\\mathbf{x}_{N_{s-1}}^{s-1},\\mathbf{y}_{N_{s-1}}^{s-1}),(\\mathbf{x}_{0}^{s},\\mathbf{y}_{0}^{s})\\gets(\\mathbf{x}_{N_{s-1}}^{s-1},\\mathbf{y}_{N_{s-1}}^{s-1}),\\rho_{0}^{s}=\\rho_{N_{s-1}}^{s-1}}\\\\ &{\\mathfrak{t}\\,\\Delta_{X Y}=\\frac{1}{\\tau^{s}}D_{X}^{2}+\\frac{1}{2\\sigma_{0}^{s}}D_{Y}^{2},\\sigma_{-1}^{s}\\gets\\sigma_{0}^{s},T_{0}^{s}=0,k=0,\\hat{\\rho}_{0}^{s}=1,N_{s}=\\infty}\\end{array}$   \n4: Se   \n5: while $k<N_{s}$ do   \n6: Run line 4-10 of APDPro with index set $(s,k)$   \n7: Update $N_{s}$ , $\\hat{\\rho}_{k+1}^{s}\\gets$ TERMINATEITER $\\hat{\\rho}_{k}^{s},\\rho_{k+1}^{s},s,k)$ , $k\\gets k+1$   \n8: end while   \n109::  eOnudt pfourt: $\\mathbf{x}_{N_{S}}^{S},\\mathbf{y}_{N_{S}}^{S}$   \n11: procedure TERMINATEITER $(\\hat{\\rho}_{\\mathrm{old}},\\rho,s,k)$   \n12: Compute $\\begin{array}{r l}&{\\hat{\\rho}_{\\mathrm{new}}=\\left\\{\\frac{1}{k+1}\\sqrt{\\hat{\\rho}_{\\mathrm{old}}^{2}k^{2}+3\\rho\\hat{\\rho}_{\\mathrm{old}}k}\\quad k>1\\right.}\\\\ &{\\left.\\qquad\\qquad\\qquad\\quad\\times=1\\right.}\\\\ &{N=\\lceil\\operatorname*{max}\\{6(\\hat{\\rho}_{\\mathrm{new}}\\tau_{0}^{s})^{-1},\\sqrt{2}^{s}\\cdot3\\sqrt{2}D_{Y}/\\big(\\hat{\\rho}_{\\mathrm{new}}D_{X}\\sqrt{\\tau_{0}^{s}\\sigma_{0}^{s}}\\big)\\}\\rceil}\\end{array}$   \n13: Compute   \n14: return $N$ , \u03c1\u02c6new   \n15: end procedure ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Let $\\{\\mathbf{x}_{0}^{s}\\}_{s\\ge0}$ be the sequence generated by rAPDPro, then we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{0}^{s}-\\mathbf{x}^{*}\\|^{2}\\leq\\Delta_{s}\\equiv D_{X}^{2}\\cdot2^{-s},\\ \\ \\forall s\\geq0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As a consequence, rAPDPro will find a solution $\\mathbf{x}_{0}^{S}$ such that $\\|\\mathbf{x}_{0}^{S}-\\mathbf{x}^{*}\\|^{2}\\leq\\varepsilon$ for any $\\varepsilon\\in(0,D_{X}^{2})$ in at most $S:=\\left\\lceil\\log_{2}(D_{X}^{2}/\\varepsilon)\\right\\rceil$ epochs. Moreover, The iteration number of rAPDPro to find $\\mathbf{x}_{0}^{S}$ such that $\\|\\mathbf{x}_{0}^{S}-\\mathbf{x}^{*}\\|^{2}\\leq\\varepsilon$ is bounded by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\varepsilon}:=\\left(\\frac{12}{\\varpi_{1}\\tau_{0}^{s}}+2\\right)\\left\\lceil\\log_{2}\\frac{D_{X}}{\\sqrt{\\varepsilon}}+1\\right\\rceil+\\left(\\frac{6(\\sqrt{2}+2)}{\\varpi_{2}\\sqrt{\\tau_{0}^{s}\\sigma_{0}^{s}}}\\right)\\cdot\\left(\\frac{D_{Y}}{\\sqrt{\\varepsilon}}\\right)\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\varpi_{1}$ and $\\varpi_{2}$ satisfy $\\begin{array}{r l r}{\\sum_{s=0}^{S}(\\hat{\\rho}_{N_{s}}^{s})^{-1}}&{{}=}&{(\\varpi_{1})^{-1}(S\\;+\\;1)}\\end{array}$ and $\\begin{array}{r l}{\\sum_{s=0}^{S}\\sqrt{2}^{s}/\\hat{\\rho}_{N_{s}}^{s}}&{{}=}\\end{array}$ $(\\varpi_{2})^{-1}\\textstyle\\sum_{s=0}^{S}{\\sqrt{2}}^{s}$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "Remark 6. The bound $T_{\\varepsilon}$ depends on $\\varepsilon$ , $\\varpi_{1}$ and $\\varpi_{2}$ . I $f\\varpi_{1}=O\\big((-\\log_{2}\\sqrt{\\varepsilon})^{-1}\\big)$ or $\\varpi_{2}=O(\\sqrt\\varepsilon)$ , then we have $T_{\\varepsilon}=\\infty$ , which implies that we can not guarantee $\\|\\mathbf{x}_{0}^{s}-\\mathbf{x}^{*}\\|\\leq\\varepsilon$ at finite iterations. $T_{\\varepsilon}=\\infty$ implies that there exists an epoch with infinite sub-iterations. Hence, rAPDPro is reduced to APDPro $i f$ we only consider that epoch. ", "page_idx": 6}, {"type": "text", "text": "Remark 7. Comparison of rAPDPro and APDPro involves a number of factors. In particular, rAPDPro compares favorably against APDPro if $\\|\\mathbf{x}_{0}\\,-\\,\\mathbf{x}^{*}\\|\\,=\\,\\widetilde\\Omega(\\sqrt\\varepsilon\\log D_{X})$ . Moreover, the complexity (16) can be slightly improved if $D_{X}$ is replaced by any ti ghter upper bound of $\\lVert\\mathbf{x}_{0}^{s}-\\mathbf{x}^{*}\\rVert$ . However, it is still unknown whether we can directly replace $D_{X}$ with $\\lVert\\mathbf{x}_{0}^{s}-\\mathbf{x}^{*}\\rVert$ in (16). ", "page_idx": 6}, {"type": "text", "text": "Dual Convergence For dual variables, we establish asymptotic convergence to the optimal solution, a key condition for developing the active-set identification in the later section. For ease in notation, it is more convenient to label the generated solution as a whole sequence using a single subscript index: $\\mathbf{x}_{1},\\mathbf{x}_{2},...\\,,\\mathbf{x}_{N};\\mathbf{y}_{1},\\mathbf{y}_{2},,...\\,,\\mathbf{y}_{N}.$ $\\{\\mathbf{x}_{0}^{s+1},\\mathbf{y}_{0}^{s+1}\\}$ $\\{\\mathbf{x}_{N_{s}+1}^{s},\\mathbf{y}_{N_{s}+1}^{s}\\}$ correspond to the same $j$ air of $(s,k)$ s. We present the dual asymptotic result in the following theorem, with the proof provided in Appendix F.2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Assume $\\bar{\\tau}^{-1}>\\overline{{\\rho}}$ and choose $\\nu_{0}>0$ such that $1>\\operatorname*{inf}_{j\\geq0}\\{\\sigma_{j-1}/\\sigma_{j}\\}\\geq\\delta+\\nu_{0}$ . We have $(\\mathbf{x}^{*},\\mathbf{y}^{*})$ satisfy the KKT condition, where $\\mathbf{y}^{*}$ is any limit point of $\\{\\mathbf{y}_{j}\\}$ generated by rAPDPro. ", "page_idx": 6}, {"type": "text", "text": "Remark 8. To establish the asymptotic convergence of the dual variable, we introduce an additional constant $\\delta\\in(0,1)$ , which implies that the initial step size must meet a stricter requirement than the convergence condition specified in Corollary $^{\\,l}$ . Since $\\sigma_{k}^{s}/\\sigma_{k-1}^{s}\\;=\\;\\sqrt{1+\\rho_{k}^{s}\\tau_{k}^{s}},\\;\\{\\rho$ $\\{\\rho_{k}^{s}\\}$ is bounded due to the boundedness of the dual variable, $\\{\\tau_{k}^{s}\\}$ is monotonically decreasing, then $\\operatorname*{inf}_{0\\le k\\le N_{s}}\\{\\sigma_{k-1}^{s}/\\sigma_{k}^{s}\\}\\ge(1+\\overline{{\\rho}}\\bar{\\tau})^{-1/2}$ . Hence, inequality, $1>\\operatorname*{inf}_{j\\geq0}\\{\\sigma_{j-1}/\\sigma_{j}\\}\\geq\\delta+\\nu_{0},$ , is always satisfiable if we choose proper $\\delta$ , $\\nu_{0}$ such that $(1+\\overline{{\\rho}}\\bar{\\tau})^{-1/2}\\geq\\delta\\!+\\!\\nu_{0}$ . Furthermore, Assumption $(\\bar{\\tau})^{-1}>\\bar{\\rho}$ is mild. Since we always choose $\\bar{\\sigma}$ large enough in rAPDPro, $\\bar{\\tau}$ can be sufficiently small. ", "page_idx": 6}, {"type": "text", "text": "Remark 9. Both algorithms proposed previously require solving quadratic optimization with linear constraints when updating dual variables, which may introduce implementation overheads when the constraint number is high. Inspired by the multi-stage algorithm, we additionally propose an algorithm (Multi-Stage APD, msAPD) that uses different step sizes in different stages and dynamically adjusts the number of iterations in each stage by leveraging strong convexity, as detailed in Appendix $H$ . ", "page_idx": 7}, {"type": "text", "text": "5 Active-set identification in sparsity-inducing optimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we apply our proposed algorithms to the aforementioned sparse learning problem: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}f(\\mathbf{x}),\\;\\mathrm{s.t.}\\;g(\\mathbf{x})\\leq0,\\;\\mathbf{x}=\\mathbf{x}_{(1)}\\times\\ldots\\times\\mathbf{x}_{(B)},\\;\\mathbf{x}_{(i)}\\in\\mathbb{R}^{n_{i}},1\\leq i\\leq B,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{f(\\mathbf{x})=\\sum_{i=1}^{B}p_{i}\\|\\mathbf{x}_{(i)}\\|}\\end{array}$ is the group Lasso regularizer and $g\\mathbf{(x)}$ is a strongly convex function. We use $\\mathbf{x}_{(i)}$ to express the $i$ -th block coordinates of $\\mathbf{x}$ . The goal of this section is to show that rAPDPro can identify the sparsity pattern of the optimal solution of (17) in a finite number of iterations. ", "page_idx": 7}, {"type": "text", "text": "In general, suppose that $f(\\mathbf{x})$ has a separable structure $\\begin{array}{r}{f(\\mathbf{x})=\\sum_{i=1}^{B}f_{i}(\\mathbf{x}_{(i)})}\\end{array}$ , we define the active set $\\mathcal{A}(\\mathbf{x})$ for $f(\\mathbf{x})$ by $A(\\mathbf{x})\\,:=\\,\\{i\\,:\\,\\partial f_{i}(\\mathbf{x}_{(i)})$ is not a singleton}. For $\\begin{array}{r}{f(\\mathbf{x})\\,=\\,\\sum_{i=1}^{B}p_{i}\\|\\mathbf{x}_{(i)}\\|}\\end{array}$ , it is easy to see that $\\mathcal{A}(\\mathbf{x})$ is the index set of the zero blocks: $A(\\mathbf{x}^{*})=\\{i:\\mathbf{x}_{(i)}^{*}=\\mathbf{0}\\}$ . Next, we describe one property for the optimal solution of (17) in Proposition 5 with the proof provided in Appendix G.1. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5. Under Assumptions 1 and 2, the KKT point for (17) is unique. ", "page_idx": 7}, {"type": "text", "text": "To identify the sparsity pattern (active set) of the optimal solution, it is common to assume the existence of a non-degenerate optimal solution, which is stronger than the standard optimality condition [24, 29]. We say that $\\mathbf{x}^{*}$ is non-degenerate if $\\mathbf{0}\\in\\mathbf{ri}\\,\\partial\\mathcal{L}(\\mathbf{x}^{*},\\mathbf{y}^{*})=\\mathbf{ri}(\\partial f(\\mathbf{x}^{*})\\!+\\!\\nabla\\bar{g}(\\mathbf{x}^{*})\\mathbf{y}^{*})$ for the Lagrangian multiplier $\\mathbf{y}^{*}$ , where ri stands for the relative interior. More specifically, $(\\mathbf{x}^{*},\\mathbf{y}^{*})$ satisfies the block-wise optimality condition ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{-[\\nabla g(\\mathbf{x}^{*})\\mathbf{y}^{*}]_{(i)}=\\nabla f_{i}(\\mathbf{x}_{(i)}^{*}),}&{\\mathrm{if~}i\\notin{\\cal A}(\\mathbf{x}^{*}),}\\\\ {-[\\nabla g(\\mathbf{x}^{*})\\mathbf{y}^{*}]_{(i)}\\in\\mathbf{int}\\left(\\partial f_{i}(\\mathbf{x}_{(i)}^{*})\\right),}&{\\mathrm{if~}i\\in{\\cal A}(\\mathbf{x}^{*}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Inspired by [24], we use the radius $\\begin{array}{r}{\\eta:=\\operatorname*{min}_{i\\in{\\cal A}(\\mathbf{x}^{*})}\\left\\{p_{i}-\\|[\\nabla g(\\mathbf{x}^{*})\\mathbf{y}^{*}]_{(i)}\\|\\right\\}}\\end{array}$ , which describes the certain distance between the gradient and \"subdifferential boundary\" of the active set. We demonstrate in the following theorem that the optimal sparsity pattern is identified when the iterates fall in a neighborhood dependent on $\\eta$ , with the proof provided in Appendix G.2. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4. Se $\\begin{array}{r}{\\mathrm{\\boldmath{\\it~t}}\\,\\mathrm{\\boldmath{\\it~\\mathcal{X}}}:=\\ B\\Big(\\tilde{\\mathbf{x}},\\operatorname*{min}_{i\\in[m]}2\\sqrt{\\frac{-2g_{i}\\left({\\bf x}_{i}^{*}\\right)}{\\mu_{i}}}+\\zeta\\Big)\\,w i t h\\,\\zeta>0\\,a n d\\,3L_{X Y}\\cdot\\big(\\bar{\\tau}+(2L_{X Y})^{-1}\\big)\\cdot\\zeta>0\\,,}\\\\ {\\mathrm{\\boldmath{\\it~then\\,we\\,have\\,exists\\,a~epoch}}\\,\\hat{S_{0}}\\;s u c h\\,t h a t\\,{\\bf x}_{(i)}^{*}={\\bf x}_{k(i)}^{s},s\\geq\\hat{S}_{0},\\,\\forall k\\in[N_{s}],\\,\\forall i\\in\\{1,2,3\\},}\\end{array}$ \u03b7\u03c4\u00af in rAPDPro,   \n$\\boldsymbol{A}(\\mathbf{x}^{*})$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 10. The active-set identification result is achieved using the optimality condition at the next iterate xik+1. To ensure xik $\\mathbf{x}_{i}^{k+1}\\in\\operatorname{int}\\mathcal{X}$ , we define an expanded region, which prevents cases where the normal cone differs from $\\{{\\bf0}\\}$ . ", "page_idx": 7}, {"type": "text", "text": "6 Numerical study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we examine the empirical performance of our proposed algorithms for solving the sparse Personalized PageRank [8, 9, 23]. The constrained form of Personalized PageRank can be written as follows: $\\mathrm{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\quad\\|D^{1/2}\\mathbf{x}\\|_{1}$ s.t. $\\begin{array}{r}{\\frac{1}{2}\\left\\langle\\mathbf{x},Q\\mathbf{x}\\right\\rangle-\\alpha\\langle\\mathbf{s},D^{-1/2}\\mathbf{x}\\rangle\\,\\le\\,b}\\end{array}$ , where $Q,D$ and s are generated by graph. We implement both rAPDPro and msAPD. We skip APDPro as we observe that the restart strategy consistently improves the algorithm performance. For comparison, we consider the state-of-the-art accelerated primal-dual (APD) method [11], APD with restart mechanism at fixed iterations (APD+restart) and Mirror-Prox [13]. 6 small to medium-scale datasets from various domains in the Network Datasets [28] are selected in our experiments. All experiments are implemented on Mac mini M2 Pro, 32GB. Due to the page limit, we only report results on three datasets and leave more details in the last Appendix I. ", "page_idx": 7}, {"type": "image", "img_path": "pG380vLYRU/tmp/a16d4dcca321277da916c2a0bb52fb8a72ecbd7c52bf24b2a179e866015dd8b5.jpg", "img_caption": ["Figure 1: The first row describes the convergence to optimum, where the $y$ -axis reports $\\log_{10}((\\|D^{1/2}\\mathbf{x}_{k}\\|_{1}\\,-\\,\\|D^{1/2}\\mathbf{x}^{*}\\|_{1})/\\|D^{1/2}\\mathbf{x}^{*}\\|_{1})$ for rAPDPro, and $\\log_{10}((\\|D^{1/2}\\bar{\\mathbf{x}}_{k}\\|_{1}\\ -$ $\\lVert D^{1/2}\\mathbf{x}^{*}\\rVert_{1})/\\lVert D^{1/2}\\mathbf{x}^{*}\\rVert_{1})$ for APD, APD+restart, msAPD and Mirror-Prox $\\mathbf{x}^{*}$ is computed by MOSEK [1]). The second row describes feasibility violation, where $y$ -axis reports the feasibility gap $\\log_{10}(\\operatorname*{max}\\{0,G(\\mathbf{x}_{k})\\})$ for rAPDPro, and $\\log_{10}(\\operatorname*{max}\\{0,G(\\bar{\\mathbf{x}}_{k})\\})$ for APD, msAPD and Mirror-Prox. Datasets (Left-Right order) correspond to bio-CE-HT, bio-CE-LC and econ-beaflw. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We plot the relative function value gap $|f(\\mathbf{x})~-~f(\\mathbf{x}^{*})|/|f(\\mathbf{x}^{*})|$ and the feasibility violation $\\operatorname*{max}\\lbrace G(\\mathbf{x}),0\\rbrace$ over the iteration number in Figure 1, respectively. Firstly, in terms of both optimality gap and constraint violation, the performance of rAPDPro and msAPD is significantly better than that of APD, APD+restart and Mirror-Prox. Additionally, rAPDPro and msAPD often converge to high-precision solutions. Secondly, based on the experimental results, it is indeed observed that msAPD exhibits a periodic variation in convergence performance, which aligns with our algorithm theory. ", "page_idx": 8}, {"type": "image", "img_path": "pG380vLYRU/tmp/1a8adfdc5e179ac4552533b9403aa0abc4bd1bf9e0a641bce34820bf0fd1d6ef.jpg", "img_caption": ["Figure 2: The experimental results on active-set identification. Datasets (Left-Right order) correspond to bio-CE-HT, bio-CE-LC and econ-beaflw. The $x$ -axis reports the iteration number and the $y$ -axis reports accuracy in active-set identification. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Next, we examine the algorithm\u2019s effectiveness in identifying sparsity patterns. We computed a nearly optimal solution $\\mathbf{x}^{*}$ from MOSEK. Note that $\\mathbf{x}^{*}$ is a dense vector. For numerical consideration, we truncate the coordinate values of $\\mathbf{x}^{*}$ to zero if the absolute value is below $10^{-8}$ and perform the same truncation to all the generated solutions of the compared algorithms. Then we use $\\langle|A(\\mathbf{x})\\cap$ $A(\\mathbf{x}^{*})\\vert+\\vert A^{c}(\\mathbf{x})\\cap A^{c}(\\mathbf{x}^{*})\\vert)/n$ to measure the accuracy of identifying the active set, where $|\\cdot|$ denotes the set cardinality. For rAPDPro, we consider the last iterate $\\mathbf{x}_{k}$ while for APD, msAPD and Mirror-Prox, we plot the result on $\\bar{\\bf x}_{k}$ , as these are the solutions where the convergence rates are established. Figure 2 plots the experiment result, from which we observe that rAPDPro and msAPD are highly effective in identifying the active set. Often, they are able to recognize the structure of the active set within a small number of iterations. Overall, the experimental results show the great potential of our proposed algorithms in identifying the sparsity structure and are consistent with our theoretical analysis. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The key contribution of this paper is that we develop several new first-order primal-dual algorithms for convex optimization with strongly convex constraints. Using some novel strategies to exploit the strong convexity o\u221af the Lagrangian function, we substantially improve the best convergence rate from ${\\mathcal{O}}(1/\\varepsilon)$ to $\\mathcal{O}(1/\\sqrt{\\varepsilon})$ . In the application of constrained sparse learning problems, the experimental study confirms the advantage of our proposed algorithms against state-of-the-art first-order methods for constrained optimization. Moreover, we show that one of our proposed algorithms rAPDPro has the favorable feature of identifying the sparsity pattern in the optimal solution. For future work, one direction is to apply the adaptive strategy, such as line search, to our framework to deal with cases when the dual bound is unavailable. Another interesting direction is to further exploit the active set identification property in a general setting. For example, it would be interesting to incorporate our algorithm with active constraint identification, which could be highly desirable when there are a large number of constraints. It would also be interesting to consider a more general convex objective when the proximal operator is not easy to compute. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is partially supported by the Major Program of National Natural Science Foundation of China (Grant 72394360, 72394364). We sincerely thank all the reviewers for their valuable suggestions, which have significantly improved the quality of our article. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Mosek ApS. Mosek optimization toolbox for matlab. User\u2019s Guide and Reference Manual, Version, 4(1), 2019.   \n[2] Amir Beck. First-order methods in optimization. SIAM, 2017.   \n[3] Dimitri P. Bertsekas. Nonlinear programming. Athena Scientific, 1999.   \n[4] Digvijay Boob, Qi Deng, and Guanghui Lan. Stochastic first-order methods for convex and nonconvex functional constrained optimization. Mathematical Programming, pages 1\u201365, 2022.   \n[5] G\u00e1bor Braun, Alejandro Carderera, Cyrille W Combettes, Hamed Hassani, Amin Karbasi, Aryan Mokhtari, and Sebastian Pokutta. Conditional gradient methods. arXiv preprint arXiv:2211.14103, 2022.   \n[6] Antonin Chambolle and Thomas Pock. On the ergodic convergence rates of a first-order primal\u2013dual algorithm. Mathematical Programming, 159(1):253\u2013287, 2016.   \n[7] Joseph C Dunn. Rates of convergence for conditional gradient algorithms near singular and nonsingular extremals. SIAM Journal on Control and Optimization, 17(2):187\u2013211, 1979.   \n[8] Kimon Fountoulakis, Farbod Roosta-Khorasani, Julian Shun, Xiang Cheng, and Michael W Mahoney. Variational perspective on local graph clustering. Mathematical Programming, 174:553\u2013573, 2019.   \n[9] Kimon Fountoulakis and Shenghao Yang. Open problem: Running time complexity of accelerated $\\ell_{1}$ -regularized pagerank. In Conference on Learning Theory, pages 5630\u20135632. PMLR, 2022.   \n[10] Dan Garber and Elad Hazan. Faster rates for the frank-wolfe method over strongly-convex sets. In International Conference on Machine Learning, pages 541\u2013549. PMLR, 2015.   \n[11] Erfan Yazdandoost Hamedani and Necdet Serhat Aybat. A primal-dual algorithm with line search for general convex-concave saddle point problems. SIAM Journal on Optimization, 31(2):1299\u20131329, 2021.   \n[12] Warren L Hare and Adrian S Lewis. Identifying active constraints via partial smoothness and prox-regularity. Journal of Convex Analysis, 11(2):251\u2013266, 2004.   \n[13] Niao He, Anatoli Juditsky, and Arkadi Nemirovski. Mirror prox algorithm for multi-term composite minimization and semi-separable problems. Computational Optimization and Applications, 61(2):275\u2013319, 2015.   \n[14] Franck Iutzeler and J\u00e9r\u00f4me Malick. Nonsmoothness in machine learning: specific structure, proximal identification, and applications. Set-Valued and Variational Analysis, 28(4):661\u2013678, 2020.   \n[15] Michel Journ\u00e9e, Yurii Nesterov, Peter Richt\u00e1rik, and Rodolphe Sepulchre. Generalized power method for sparse principal component analysis. Journal of Machine Learning Research, 11(2), 2010.   \n[16] Guanghui Lan. First-order and stochastic optimization methods for machine learning. Springer, 2020.   \n[17] Guanghui Lan and Renato DC Monteiro. Iteration-complexity of first-order penalty methods for convex programming. Mathematical Programming, 138(1):115\u2013139, 2013.   \n[18] Guanghui Lan and Renato DC Monteiro. Iteration-complexity of first-order augmented lagrangian methods for convex programming. Mathematical Programming, 155(1):511\u2013547, 2016.   \n[19] Sangkyun Lee, Stephen J Wright, and L\u00e9on Bottou. Manifold identification in dual averaging for regularized stochastic online learning. Journal of Machine Learning Research, 13(6), 2012.   \n[20] Evgeny S Levitin and Boris T Polyak. Constrained minimization methods. USSR Computational mathematics and mathematical physics, 6(5):1\u201350, 1966.   \n[21] Qihang Lin, Selvaprabu Nadarajah, and Negar Soheili. A level-set method for convex optimization with a feasible solution path. SIAM Journal on Optimization, 28(4):3290\u20133311, 2018.   \n[22] Tianyi Lin, Chi Jin, and Michael I Jordan. Near-optimal algorithms for minimax optimization. In Conference on Learning Theory, pages 2738\u20132779. PMLR, 2020.   \n[23] David Mart\u00ednez-Rubio, Elias Wirth, and Sebastian Pokutta. Accelerated and sparse algorithms for approximate personalized pagerank and beyond. arXiv preprint arXiv:2303.12875, 2023.   \n[24] Julie Nutini, Mark Schmidt, and Warren Hare. \u201cactive-set complexity\u201d of proximal gradient: How long does it take to find the sparsity pattern? Optimization Letters, 13(4):645\u2013655, 2019.   \n[25] Yuyuan Ouyang and Yangyang Xu. Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems. Mathematical Programming, 185(1):1\u201335, 2021.   \n[26] H. Robbins and D. Siegmund. A convergence theorem for non negative almost supermartingales and some applications. In Jagdish S. Rustagi, editor, Optimizing Methods in Statistics, pages 233\u2013257. Academic Press, 1971.   \n[27] R Tyrrell Rockafellar. Convex analysis, volume 18. Princeton university press, 1970.   \n[28] Ryan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph analytics and visualization. In AAAI, 2015.   \n[29] Yifan Sun, Halyun Jeong, Julie Nutini, and Mark Schmidt. Are we there yet? manifold identification of gradient-related proximal methods. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1110\u20131119. PMLR, 2019.   \n[30] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):267\u2013288, 1996.   \n[31] Stephen J Wright. Identifiable surfaces in constrained optimization. SIAM Journal on Control and Optimization, 31(4):1063\u20131079, 1993.   \n[32] Yangyang Xu. First-order methods for constrained convex programming based on linearized augmented lagrangian function. Informs Journal on Optimization, 3(1):89\u2013117, 2021.   \n[33] Yangyang Xu. Iteration complexity of inexact augmented lagrangian methods for constrained convex programming. Mathematical Programming, 185(1):199\u2013244, 2021.   \n[34] Liwei Zhang, Yule Zhang, Jia Wu, and Xiantao Xiao. Solving stochastic optimization with expectation constraints efficiently by a stochastic augmented lagrangian-type algorithm. INFORMS Journal on Computing, 34(6):2989\u20133006, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Limitations 14 ", "page_idx": 12}, {"type": "text", "text": "B Comparison with Frank-Wolfe 14 ", "page_idx": 12}, {"type": "text", "text": "C Auxiliary lemmas 14 ", "page_idx": 12}, {"type": "text", "text": "D Proof details in Section 2 15 ", "page_idx": 12}, {"type": "text", "text": "D.1 Proof of Proposition 1 . 15   \nD.2 Proof of Proposition 2 . . . 15   \nD.3 Proof of Proposition 3 . 15 ", "page_idx": 12}, {"type": "text", "text": "E Convergence analysis of APDPro 16 ", "page_idx": 12}, {"type": "text", "text": "E.1 Proof of Proposition 4 . 16   \nE.2 Proof of Theorem 1 . 16   \nE.3 Proof of Corollary 1 . 19 ", "page_idx": 12}, {"type": "text", "text": "F Convergence analysis of rAPDPro 20 ", "page_idx": 12}, {"type": "text", "text": "F.1 Proof of Theorem 2 20   \nF.2 Proof of Theorem 3 . 22 ", "page_idx": 12}, {"type": "text", "text": "G Proof details for sparsity identification 23 ", "page_idx": 12}, {"type": "text", "text": "G.1 Proof of Proposition 5 . 24   \nG.2 Proof of Theorem 4 24 ", "page_idx": 12}, {"type": "text", "text": "H A multi-stage accelerated primal-dual algorithm 25 ", "page_idx": 12}, {"type": "text", "text": "I Experiment details 27 ", "page_idx": 12}, {"type": "text", "text": "Structure of the Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The appendix is structured as follows: Appendix A introduces some limitations of our methods, primarily concerning the application scenarios of our algorithm. Appendix B includes comparisons between ours and some related Frank-Wolfe methods. We give some auxiliary lemmas in Appendix C, which are very important for the proofs presented later. Appendix D, E, F and G present the proof of conclusion in Section 2, 3, 4 and 5, respectively. Furthermore, Appendix H introduces a new algorithm to obtain a convergence rate without complicated dual updating. Finally, Appendix I offers more extensive details on our experiments. ", "page_idx": 12}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this paper, we focus on the theoretical analysis of convex optimization. Although our proposed algorithms for the convex optimization \u221awith strongly convex constraints can theoretically improve the existing results from ${\\mathcal{O}}(1/\\varepsilon)$ to $\\mathcal{O}(1/\\sqrt{\\varepsilon})$ . However, we still need to point out that our optimization algorithm has the following limitations. One is the algorithm needs a lower bound on the norm of sub-gradients of the objective function in the optimal solution, which may not be satisfied for all functions. On the other hand, we require consistent smoothness of the constraints to ensure convergence, and how to use the line search method to ensure convergence is a future direction. ", "page_idx": 13}, {"type": "text", "text": "B Comparison with Frank-Wolfe ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We note that the strongly convex function constraint in (1) is a special case of a strongly convex set constraint, as demonstrated in [15]. Over the strongly convex set, it has been shown that Frank-Wolfe Algorithm (FW) can obtain convergence rates substantially better than the worst-case ${\\mathcal{O}}(1/\\varepsilon)$ rate. Under the bounded gradient assumption, [7, 20] show that FW obtains linear convergence over a strongly convex set. Nevertheless, the uniform bounded gradient assumption appears to be stronger than ours, as we only impose the lower boundedness assumption on the optimal solution $\\mathbf{x}^{*}$ an\u221ad allow the objective to be non-differentiable. More recently, [10] shows that FW obtains an $\\mathcal{O}(1/\\sqrt{\\varepsilon})$ rate when the gradient is the order of the square root of the function value gap. For more recent progress, please refer to [5]. Despite the attractive convergence property, FW exhibits certain limitations when applied to the general function constraints (1) addressed in this paper. Specifically, FW involves a sequence of linear optimization problems throughout the iterations. While linear optimization over certain strongly convex sets, such as $\\ell_{p}$ -ball, admits a closed-form solution, there exists no efficient routine to handle general function constraints explored in this paper. ", "page_idx": 13}, {"type": "text", "text": "C Auxiliary lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The following three-point property is important in the convergence analysis. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ be a closed strongly convex function with modulus $\\mu\\geq0$ . Give $\\bar{\\mathbf{x}}\\in\\mathcal{X}$ , where $\\mathcal{X}$ is a compact convex set and $t\\geq0$ , let $\\begin{array}{r}{\\mathbf{\\dot{x}}^{+}=\\operatorname*{argmin}_{x\\in\\mathcal{X}}f(\\mathbf{x})+\\frac{t}{2}\\|\\mathbf{x}-\\bar{\\mathbf{x}}\\|^{2}}\\end{array}$ , then for all $\\mathbf{x}\\in\\mathcal{X}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f({\\mathbf x})+\\frac{t}{2}\\|{\\mathbf x}-\\bar{{\\mathbf x}}\\|^{2}\\geq f({\\mathbf x}^{+})+\\frac{t+\\mu}{2}\\|{\\mathbf x}^{+}-{\\mathbf x}\\|^{2}+\\frac{t}{2}\\|{\\mathbf x}^{+}-\\bar{{\\mathbf x}}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Since $\\mathcal{X}$ is a convex compact set, $\\begin{array}{r}{\\phi(x):=I_{\\mathcal{X}}(\\mathbf{x})+f(\\mathbf{x})\\!+\\!\\frac{t}{2}\\|\\mathbf{x}\\!-\\!\\bar{\\mathbf{x}}\\|^{2}}\\end{array}$ is lower-semi-continuous and $(\\mu+t)$ -strongly convex, where $I_{\\mathcal{X}}(\\mathbf{x})=\\left\\{0\\begin{array}{l l}{\\phantom{\\frac{1}{2}}\\mathbf{\\nabla}\\mathbf{x}\\in\\mathcal{X}}\\\\ {\\infty}&{\\mathbf{x}\\notin\\mathcal{X}}\\end{array}\\right.$ . Using the optimality $(\\mathbf{0}\\in\\phi(\\mathbf{x}^{+}))$ and strong convexity, we have $\\begin{array}{r}{\\phi(\\mathbf{x})\\geq\\phi(\\mathbf{x}^{+})+\\langle\\mathbf{0},\\mathbf{x}-\\mathbf{x}^{+}\\rangle+\\frac{t+\\mu}{2}\\|\\mathbf{x}^{+}-\\mathbf{x}\\|^{2}}\\end{array}$ , for any $\\mathbf{x}\\in\\mathcal{X}$ . This immediately gives the desired relation. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "The following result is adjusted from the classic supermartingale convergence theorem [26, Theorem 1]. We give proof for completeness. ", "page_idx": 13}, {"type": "text", "text": "Lemma 2. Let $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ be a probability space and ${\\mathcal{F}}_{1}\\subset{\\mathcal{F}}_{2}\\subset\\cdot\\cdot\\cdot$ be a sequence of sub- $\\sigma$ -algebras of $\\mathcal{F}$ . For each $j=1,2,\\cdot\\cdot\\cdot$ , let $a_{j},b_{j}$ and $c_{j}$ be non-negative ${\\mathcal{F}}_{n}$ -measure random variables such $\\mathbb{E}[a_{j+1}\\mid\\mathcal{F}_{j}]\\le a_{j}-b_{j}+c_{j}$ , then we have $\\textstyle\\operatorname*{lim}_{j\\to\\infty}a_{j}<\\infty$ exists and $\\textstyle\\sum_{j=1}^{\\infty}b_{j}<\\infty$ a.s. when $\\textstyle\\sum_{j=1}^{\\infty}c_{j}<\\infty$ ", "page_idx": 13}, {"type": "text", "text": "Proof. Define $\\begin{array}{r}{d_{j}=a_{j}-\\sum_{l=1}^{j-1}(c_{l}-b_{l})}\\end{array}$ and for any $\\bar{a}>0$ , define $\\begin{array}{r}{t=\\operatorname*{inf}\\{t:\\sum_{l=1}^{t}c_{l}>\\bar{a}\\}}\\end{array}$ . If $j<t$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[d_{j+1}\\mid\\mathcal{F}_{j}]=\\mathbb{E}[a_{j+1}-\\sum_{l=1}^{j}(c_{l}-b_{l})\\mid\\mathcal{F}_{j}]\\overset{(a)}{\\le}a_{j}-\\sum_{l=1}^{j-1}(c_{l}-b_{l})=:d_{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(a)$ holds by $\\mathbb{E}[a_{j+1}\\mid{\\mathcal{F}}_{j}]\\le a_{j}+c_{j}-b_{j}$ , and hence ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[d_{\\operatorname*{min}\\{t,(j+1)\\}}\\mid\\mathcal{F}_{j}]=d_{t}\\mathbb{I}_{\\{t\\leq j\\}}+\\mathbb{E}[d_{j+1}\\mid\\mathcal{F}_{j}]\\mathbb{I}_{\\{t>j\\}}\\overset{(a)}{\\leq}d_{\\operatorname*{min}\\{t,j\\}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(a)$ holds by (18). Therefore, we have $\\{d_{\\operatorname*{min}\\{t,(j+1)\\}},\\mathcal{F}_{j},1\\leq j\\leq\\infty\\}$ is a supermartingale. Since ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{\\operatorname*{min}\\{t,j\\}}=a_{\\operatorname*{min}\\{t,j\\}}-\\sum_{l=1}^{\\operatorname*{min}\\{t,j\\}-1}(c_{l}-b_{l})\\overset{(a)}{\\geq}-\\sum_{l=1}^{\\operatorname*{min}\\{t,(j-1)\\}}c_{l}\\geq-\\bar{a},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "holds for all $j$ , where $(a)$ holds by $a_{\\operatorname*{min}\\{t,j\\}},b_{l}\\geq0$ . Then it follows from the martingale convergence theorem that $\\textstyle\\operatorname*{lim}_{j\\to\\infty}d_{\\operatorname*{min}\\{t,j\\}}$ exists and is finite a.s., i.e., $\\operatorname{lim}_{j\\to\\infty}d_{j}$ exists and is finite on $\\{t=$ $\\begin{array}{r}{\\infty\\}\\,=\\,\\{\\sum_{j=1}^{\\infty}c_{j}\\,\\le\\,\\bar{a}\\}}\\end{array}$ . Since $\\bar{a}$ is arbitrary, we see that $\\operatorname{lim}_{j\\to\\infty}d_{j}$ exists and is finite a.s. on $\\{\\textstyle\\sum_{j=1}^{\\infty}c_{j}\\ <\\ \\infty\\}$ . By $\\begin{array}{r}{d_{j}\\,=\\,a_{j}\\,-\\,\\sum_{l=1}^{j-1}(c_{l}\\,-\\,b_{l})}\\end{array}$ , we have $\\operatorname{lim}_{j\\to\\infty}a_{j}$ exists and is finite and $\\textstyle\\sum_{j=1}^{\\infty}b_{j}<\\infty$ when $\\scriptstyle\\{\\sum_{j=1}^{\\infty}c_{j}\\;<\\;\\infty\\}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "D Proof details in Section 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Under Slater\u2019s CQ, it is standard to show that any optimal solution $\\mathbf{x}^{*}$ will also satisfy the KKT condition. For example, one can refer to [3]. For any $\\mathbf{x}\\in\\mathcal{X}_{G}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(\\mathbf{x})+\\langle\\mathbf{y}^{*},G(\\mathbf{x})\\rangle\\geq f(\\mathbf{x}^{*})+\\langle\\mathbf{y}^{*},G(\\mathbf{x}^{*})\\rangle=f(\\mathbf{x}^{*}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the equality is from the complementary slackness. In view of the above result and the Slater\u2019s condition (i.e., $G(\\tilde{\\mathbf{x}})<\\mathbf{0})$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(\\tilde{\\mathbf{x}})>f(\\tilde{\\mathbf{x}})+\\langle\\mathbf{y}^{*},G(\\tilde{\\mathbf{x}})\\rangle\\geq f(\\mathbf{x}^{*}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining with fact $\\begin{array}{r}{\\|\\mathbf{y}^{*}\\|_{1}\\operatorname*{min}_{i\\in[m]}\\big\\{-g_{i}(\\tilde{\\mathbf{x}})\\big\\}\\leq-\\langle\\mathbf{y}^{*},G(\\tilde{\\mathbf{x}})\\rangle}\\end{array}$ , then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{y}^{*}\\|\\leq\\|\\mathbf{y}^{*}\\|_{1}\\leq\\frac{f(\\tilde{\\mathbf{x}})-f(\\mathbf{x}^{*})}{\\operatorname*{min}_{i\\in[m]}\\left\\{-g_{i}(\\tilde{\\mathbf{x}})\\right\\}}=\\bar{c},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality is by $\\begin{array}{r}{f(\\mathbf{x}^{*})\\geq\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\,f(\\mathbf{x})}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "D.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We prove the uniqueness property by contradiction. Suppose that there exist $(\\mathbf{x}^{*},\\mathbf{y}^{*})$ , $(\\tilde{\\mathbf{x}}^{*},\\tilde{\\mathbf{y}}^{*})$ satisfying the KKT condition, then from the complementary slackness, optimality of $\\mathbf{x}^{*}$ and $\\tilde{\\mathbf{x}}^{*}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{x}^{*},\\mathbf{y}^{*})=f(\\mathbf{x}^{*})=f(\\tilde{\\mathbf{x}}^{*})=\\mathcal{L}(\\tilde{\\mathbf{x}}^{*},\\tilde{\\mathbf{y}}^{*}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, we have $\\begin{array}{r}{\\mathcal L(\\widetilde\\mathbf x^{*},\\widetilde\\mathbf y^{*})\\;\\leq\\;\\mathcal L(\\mathbf x^{*},\\widetilde\\mathbf y^{*})\\;\\leq\\;\\mathcal L(\\mathbf x^{*},\\mathbf y^{*})}\\end{array}$ . Hence, we must have $\\mathcal{L}(\\tilde{\\mathbf{x}}^{*},\\tilde{\\mathbf{y}}^{*})\\;=$ $\\mathcal{L}(\\mathbf{x}^{*},\\tilde{\\mathbf{y}}^{*})$ . However, since Assumption 2 implies $\\tilde{\\mathbf{y}}^{*}\\neq\\mathbf{0}$ , the strongly convex function $\\mathcal{L}(\\cdot,\\tilde{\\mathbf{y}}^{*})$ has a unique optimizer. Therefore, we conclude that $\\mathbf{x}^{*}=\\tilde{\\mathbf{x}}^{*}$ . ", "page_idx": 14}, {"type": "text", "text": "Next, we show that the set of optimal dual variables for problem (1) is convex. Suppose that there exist two optimal dual variables $\\mathbf{y}_{1}^{*}$ and $\\mathbf{y}_{2}^{*}$ for the unique primal variable $\\mathbf{x}^{*}$ , both satisfying the KKT condition, then we have $\\langle\\mathbf{y}_{1}^{*},G(\\mathbf{x}^{*})\\rangle=\\langle\\mathbf{y}_{2}^{*},G(\\mathbf{x}^{*})\\rangle=0$ . This implies that any linear combination of $\\mathbf{y}_{1}^{*}$ and $\\mathbf{y}_{2}^{*}$ satisfy KKT condition, i.e., $\\left\\langle a\\mathbf{y}_{1}^{*}+b\\mathbf{y}_{2}^{*},G(\\mathbf{x}^{*})\\right\\rangle=0,\\forall a,b$ . From Proposition 1, we know any optimal dual variable falls into a bounded convex set $\\boldsymbol{\\wp}$ . The intersection of two convex sets is also a convex set. Hence, we complete our proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "D.3 Proof of Proposition 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. From the strong convexity of $g_{i}(\\mathbf{x})$ , we have $\\begin{array}{r}{g_{i}(\\mathbf{x})\\ge g_{i}(\\mathbf{x}_{i}^{*})+\\frac{\\mu_{i}}{2}\\|\\mathbf{x}-\\mathbf{x}_{i}^{*}\\|^{2}}\\end{array}$ , which implies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{\\mathbf{x}}-\\mathbf{x}_{i}^{*}\\|^{2}\\leq(g_{i}(\\tilde{\\mathbf{x}})-g_{i}(\\mathbf{x}_{i}^{*}))\\frac{2}{\\mu_{i}}\\overset{(a)}{<}\\frac{-2g_{i}(\\mathbf{x}_{i}^{*})}{\\mu_{i}},}\\\\ {\\|\\mathbf{x}^{*}-\\mathbf{x}_{i}^{*}\\|^{2}\\leq(g_{i}(\\mathbf{x}^{*})-g_{i}(\\mathbf{x}_{i}^{*}))\\frac{2}{\\mu_{i}}\\leq\\frac{-2g_{i}(\\mathbf{x}_{i}^{*})}{\\mu_{i}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(a)$ holds by $g_{i}(\\tilde{\\mathbf{x}})<0$ . In view of the triangle inequality and the above result, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{\\mathbf{x}}-\\mathbf{x}^{*}\\|\\leq\\|\\mathbf{x}_{i}^{*}-\\mathbf{x}^{*}\\|+\\|\\tilde{\\mathbf{x}}-\\mathbf{x}_{i}^{*}\\|<2\\sqrt{\\frac{-2g_{i}(\\mathbf{x}_{i}^{*})}{\\mu_{i}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, $\\mathbf{x}^{\\ast}\\in$ int $\\begin{array}{r}{\\mathcal{B}\\Big(\\tilde{\\mathbf{x}},\\operatorname*{min}_{i\\in[m]}2\\sqrt{\\frac{-2g_{i}(\\mathbf{x}_{i}^{*})}{\\mu_{i}}}\\Big)}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "E Convergence analysis of APDPro ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Proof of Proposition 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Using the triangle inequality and (5), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla G(\\mathbf{x}^{*})\\|-\\|\\nabla G(\\hat{\\mathbf{x}})\\|\\leq\\|\\nabla G(\\mathbf{x}^{*})-\\nabla G(\\hat{\\mathbf{x}})\\|\\leq L_{X}\\|\\hat{\\mathbf{x}}-\\mathbf{x}^{*}\\|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining the above inequality and (8), we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{r}{\\|\\mathbf{y}^{*}\\|_{1}}\\leq L_{X}\\|\\hat{\\mathbf{x}}-\\mathbf{x}^{*}\\|+\\|\\nabla G(\\hat{\\mathbf{x}})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we develop more\u221a specific lower bounds on $\\|\\mathbf{y}\\|_{1}.\\,\\mathrm{i})$ . Inequality (9) can be easily verified since we have $\\|\\hat{\\mathbf x}-\\mathbf x^{*}\\|\\leq\\sqrt{2\\beta}$ . ii). Suppose $(\\mathbf{y}^{*})^{\\top}{\\boldsymbol{\\mu}}\\cdot\\|\\hat{\\mathbf{x}}-\\mathbf{x}^{*}\\|^{2}\\leq2\\beta$ , then together with (22) we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{r}{\\|\\mathbf{y}^{*}\\|_{1}}\\leq L_{X}\\sqrt{\\frac{2\\beta}{(\\mathbf{y}^{*})^{\\top}\\mu}}+\\|\\nabla G(\\hat{\\mathbf{x}})\\|\\leq L_{X}\\sqrt{\\frac{2\\beta}{\\underline{{\\mu}}\\|\\mathbf{y}^{*}\\|_{1}}}+\\|\\nabla G(\\hat{\\mathbf{x}})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the above inequality can be expressed as $a t^{2}-b t-c\\leq0$ with $t=\\|\\mathbf{y}^{*}\\|_{1}^{-1/2}$ , $a=r,b=$ $L_{X}\\sqrt{2\\beta/\\underline{{\\mu}}}$ and $c=\\|\\nabla G(\\hat{\\mathbf{x}})\\|$ . Standard analysis implies that $t\\leq(b+\\sqrt{b^{2}+4a c})/2a$ , which gives the desired bound (10). \u53e3 ", "page_idx": 15}, {"type": "text", "text": "E.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. First, it is easy to verify by our construction that $\\{{\\mathcal{V}}_{k}\\}$ is a monotone sequence: $y_{1}\\supseteq y_{2}\\supseteq$ $\\cdots\\supseteq\\mathfrak{H}_{k}\\cdot\\cdot\\cdot$ . Our goal is to show $\\mathcal{V}^{*}\\subseteq\\mathcal{V}_{k}$ holds for any $k\\geq0$ by induction. Note that $\\mathcal{V}^{*}\\subseteq\\mathcal{V}_{0}$ immediately follows from our assumption that $(\\mathbf{y}^{*})^{\\top}\\bar{\\pmb{\\mu}}\\geq\\rho_{0}$ , for any $\\mathbf{y}^{\\ast}\\,\\in\\,\\mathcal{V}^{\\ast}$ . Suppose that $\\mathcal{V}^{*}\\subseteq\\mathcal{V}_{k}$ holds for $k=0,\\ldots,K-1$ , we claim: ", "page_idx": 15}, {"type": "text", "text": "1. For any $\\mathbf{x}\\in\\mathcal{X}$ and $\\mathbf{y}\\in\\mathcal{Y}^{*}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y})-\\mathcal{L}(\\mathbf{x},\\bar{\\mathbf{y}}_{K})\\leq\\frac{1}{T_{K}}\\Delta(\\mathbf{x},\\mathbf{y})-\\frac{t_{K-1}\\tau_{K-1}^{-1}}{2T_{K}}\\|\\mathbf{x}-\\mathbf{x}_{K}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "2. $y^{*}\\subseteq y_{K}$ ", "page_idx": 15}, {"type": "text", "text": "Part 1. For $k=0,1,2,\\ldots,K-1$ , taking ${\\bf\\Gamma}-\\langle{\\bf z}_{k},\\cdot\\rangle$ and $f(\\cdot)+\\langle\\nabla G(\\mathbf{x}_{k})\\mathbf{y}_{k+1},\\cdot\\rangle$ in Lemma 1, the following relations ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{-\\langle\\mathbf{y}_{k+1}-\\mathbf{y},\\mathbf{z}_{k}\\rangle\\leq A_{k+1},}\\\\ {f(\\mathbf{x}_{k+1})+\\langle\\mathbf{y}_{k+1},\\nabla G(\\mathbf{x}_{k})^{\\top}(\\mathbf{x}_{k+1}-\\mathbf{x})\\rangle\\leq f(\\mathbf{x})+B_{k+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k+1}\\triangleq\\frac{1}{2\\sigma_{k}}\\left(\\|\\mathbf{y}-\\mathbf{y}_{k}\\|^{2}-\\|\\mathbf{y}-\\mathbf{y}_{k+1}\\|^{2}-\\|\\mathbf{y}_{k+1}-\\mathbf{y}_{k}\\|^{2}\\right),}\\\\ &{B_{k+1}\\triangleq\\frac{1}{2\\tau_{k}}\\left(\\|\\mathbf{x}-\\mathbf{x}_{k}\\|^{2}-\\|\\mathbf{x}-\\mathbf{x}_{k+1}\\|^{2}-\\|\\mathbf{x}_{k+1}-\\mathbf{x}_{k}\\|^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "hold for any $\\textbf{x}\\in\\:\\mathcal{X}$ and $\\mathbf{y}\\,\\in\\,\\bigcap_{0\\leq s\\leq k}{\\mathcal{V}}_{s}$ . The existence of such $\\mathbf{y}$ follows from our induction hypothesis. Since $\\mathbf{y}_{k+1}^{\\top}G(\\cdot)$ is $\\rho_{k}$ -strongly convex, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\mathbf{y}_{k+1},\\nabla G(\\mathbf{x}_{k})^{\\top}(\\mathbf{x}_{k+1}-\\mathbf{x})\\right\\rangle}\\\\ &{\\geq\\left\\langle\\mathbf{y}_{k+1},\\nabla G(\\mathbf{x}_{k})^{\\top}(\\mathbf{x}_{k+1}-\\mathbf{x}_{k})\\right\\rangle}\\\\ &{\\ +\\left\\langle\\mathbf{y}_{k+1},G(\\mathbf{x}_{k+1})-G(\\mathbf{x})\\right\\rangle-\\left\\langle\\mathbf{y}_{k+1},G(\\mathbf{x}_{k+1})-G(\\mathbf{x}_{k})\\right\\rangle+\\frac{\\rho_{k}}{2}\\|\\mathbf{x}-\\mathbf{x}_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining this result and (25), we have ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r l}&{f(\\mathbf{x}_{k+1})-f(\\mathbf{x})+\\langle\\mathbf{y}_{k+1},G(\\mathbf{x}_{k+1})-G(\\mathbf{x})\\rangle}\\\\ &{\\leq B_{k+1}-\\langle\\mathbf{y}_{k+1},\\nabla G(\\mathbf{x}_{k})^{\\top}(\\mathbf{x}_{k+1}-\\mathbf{x}_{k})\\rangle+\\langle\\mathbf{y}_{k+1},G(\\mathbf{x}_{k+1})-G(\\mathbf{x}_{k})\\rangle-\\frac{\\rho_{k}}{2}\\|\\mathbf{x}-\\mathbf{x}_{k}\\|^{2}.}\\end{array}$ On the other hand, by the definition of $\\mathbf{z}_{k}$ , we have $\\begin{array}{r l}&{\\langle\\mathbf{y}-\\mathbf{y}_{k+1},\\mathbf{z}_{k}\\rangle}\\\\ &{=\\langle\\mathbf{y}-\\mathbf{y}_{k+1},G(\\mathbf{x}_{k})-G(\\mathbf{x}_{k+1})\\rangle+\\langle\\mathbf{y}-\\mathbf{y}_{k+1},G(\\mathbf{x}_{k+1})\\rangle}\\\\ &{\\ \\ +\\left(\\sigma_{k-1}/\\sigma_{k}\\right)\\langle\\mathbf{y}-\\mathbf{y}_{k},G(\\mathbf{x}_{k})-G(\\mathbf{x}_{k-1})\\rangle+\\left(\\sigma_{k-1}/\\sigma_{k}\\right)\\langle\\mathbf{y}_{k}-\\mathbf{y}_{k+1},G(\\mathbf{x}_{k})-G(\\mathbf{x}_{k-1})\\rangle.}\\end{array}$ 29) ", "page_idx": 15}, {"type": "text", "text": "Let us denote $\\mathbf{q}_{k}=G(\\mathbf{x}_{k})-G(\\mathbf{x}_{k-1})$ for brevity. Combining (24) and (29) yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{y}-\\mathbf{y}_{k+1},G(\\mathbf{x}_{k+1})\\rangle}\\\\ &{\\leq A_{k+1}+\\langle\\mathbf{y}-\\mathbf{y}_{k+1},G(\\mathbf{x}_{k+1})-G(\\mathbf{x}_{k})\\rangle-(\\sigma_{k-1}/\\sigma_{k})\\langle\\mathbf{y}-\\mathbf{y}_{k},\\mathbf{q}_{k}\\rangle-(\\sigma_{k-1}/\\sigma_{k})\\langle\\mathbf{y}_{k}-\\mathbf{y}_{k+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Putting (28) and (30) together, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\mathbf{x}_{k+1},\\mathbf{y})-\\mathcal{L}(\\mathbf{x},\\mathbf{y}_{k+1})}\\\\ &{\\quad\\leq A_{k+1}+B_{k+1}-\\langle\\mathbf{y}_{k+1},\\nabla G(\\mathbf{x}_{k})^{\\top}(\\mathbf{x}_{k+1}-\\mathbf{x}_{k})\\rangle+\\langle\\mathbf{y}_{k+1},G(\\mathbf{x}_{k+1})-G(\\mathbf{x}_{k})\\rangle}\\\\ &{\\quad+\\,\\langle\\mathbf{y}-\\mathbf{y}_{k+1},\\mathbf{q}_{k+1}\\rangle-\\,(\\sigma_{k-1}/\\sigma_{k})\\langle\\mathbf{y}-\\mathbf{y}_{k},\\mathbf{q}_{k}\\rangle+(\\sigma_{k-1}/\\sigma_{k})\\langle\\mathbf{y}_{k+1}-\\mathbf{y}_{k},\\mathbf{q}_{k}\\rangle-\\,\\frac{\\rho_{k}}{2}\\|\\mathbf{x}-\\mathbf{x}_{k}\\|^{2}}\\\\ &{\\quad\\leq A_{k+1}+B_{k+1}+\\frac{L_{X Y}}{2}\\|\\mathbf{x}_{k+1}-\\mathbf{x}_{k}\\|^{2}-\\frac{\\rho_{k}}{2}\\|\\mathbf{x}-\\mathbf{x}_{k}\\|^{2}}\\\\ &{\\quad+\\,\\langle\\mathbf{y}-\\mathbf{y}_{k+1},\\mathbf{q}_{k+1}\\rangle-\\,(\\sigma_{k-1}/\\sigma_{k})\\langle\\mathbf{y}-\\mathbf{y}_{k},\\mathbf{q}_{k}\\rangle+(\\sigma_{k-1}/\\sigma_{k})\\langle\\mathbf{y}_{k+1}-\\mathbf{y}_{k},\\mathbf{q}_{k}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality is by Lipschitz smoothness of $\\langle\\mathbf{y}_{k+1},G(\\cdot)\\rangle$ ", "page_idx": 16}, {"type": "text", "text": "Next, we bound the term $\\langle\\mathbf q_{k},\\mathbf y_{k+1}-\\mathbf y_{k}\\rangle$ by Young\u2019s inequality, which gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\langle\\mathbf{y}_{k+1}-\\mathbf{y}_{k},\\mathbf{q}_{k}\\right\\rangle\\leq\\frac{1}{2\\sigma_{k-1}}\\|\\mathbf{y}_{k+1}-\\mathbf{y}_{k}\\|^{2}+\\frac{\\sigma_{k-1}}{2}\\|\\mathbf{q}_{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It follows from (31) and $\\begin{array}{r}{\\frac{\\sigma_{k}}{2}\\|\\mathbf q_{k+1}\\|^{2}\\leq\\frac{L_{G}^{2}\\sigma_{k}}{2}\\|\\mathbf x_{k+1}-\\mathbf x_{k}\\|^{2}}\\end{array}$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\mathbf{x}_{k+1},\\mathbf{y})-\\mathcal{L}(\\mathbf{x},\\mathbf{y}_{k+1})}\\\\ &{\\quad\\leq\\frac{\\tau_{k}^{-1}-\\rho_{k}}{2}\\|\\mathbf{x}-\\mathbf{x}_{k}\\|^{2}-\\frac{\\tau_{k}^{-1}}{2}\\|\\mathbf{x}-\\mathbf{x}_{k+1}\\|^{2}+\\frac{(\\sigma_{k-1}/\\sigma_{k})\\sigma_{k-1}}{2}\\|\\mathbf{q}_{k}\\|^{2}-\\frac{\\sigma_{k}}{2}\\|\\mathbf{q}_{k+1}\\|^{2}}\\\\ &{\\quad+\\,\\frac{1}{2\\sigma_{k}}\\left(\\|\\mathbf{y}-\\mathbf{y}_{k}\\|^{2}-\\|\\mathbf{y}-\\mathbf{y}_{k+1}\\|^{2}\\right)+\\langle\\mathbf{y}-\\mathbf{y}_{k+1},\\mathbf{q}_{k+1}\\rangle-\\left(\\sigma_{k-1}/\\sigma_{k}\\right)\\langle\\mathbf{y}-\\mathbf{y}_{k},\\mathbf{q}_{k}\\rangle}\\\\ &{\\quad-\\,\\frac{\\sigma_{k}^{-1}-(\\sigma_{k-1}/\\sigma_{k})/\\sigma_{k-1}}{2}\\|\\mathbf{y}_{k+1}-\\mathbf{y}_{k}\\|^{2}+\\frac{L_{G}^{2}\\sigma_{k}}{2}\\|\\mathbf{x}_{k+1}-\\mathbf{x}_{k}\\|^{2}-\\frac{\\tau_{k}^{-1}-L_{X Y}}{2}\\|\\mathbf{x}_{k+1}-\\mathbf{x}_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Multiply both sides of the above relation by $t_{k}$ and sum up the result for $k=0,1,\\ldots,K-1$ . In view of the parameter relation (11), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{k=0}^{K-1}t_{k}\\big[\\mathcal{L}(\\mathbf{x}_{k+1},\\mathbf{y})-\\mathcal{L}(\\mathbf{x},\\mathbf{y}_{k+1})\\big]}\\\\ &{\\overset{(a)}{\\leq}\\frac{t_{0}(\\tau_{0}^{-1}-\\rho_{0})}{2}\\|\\mathbf{x}-\\mathbf{x}_{0}\\|^{2}-\\frac{t_{K-1}\\tau_{K-1}^{-1}}{2}\\|\\mathbf{x}-\\mathbf{x}_{K}\\|^{2}-\\frac{t_{K-1}\\sigma_{K-1}}{2}\\|\\mathbf{q}_{K}\\|^{2}}\\\\ &{\\quad+\\,\\frac{t_{0}\\sigma_{0}^{-1}}{2}\\|\\mathbf{y}-\\mathbf{y}_{0}\\|^{2}-\\frac{t_{K-1}\\sigma_{K-1}^{-1}}{2}\\|\\mathbf{y}-\\mathbf{y}_{K}\\|^{2}+t_{K-1}\\big\\langle\\mathbf{y}-\\mathbf{y}_{K},\\mathbf{q}_{K}\\big\\rangle-t_{0}\\big\\langle\\mathbf{y}-\\mathbf{y}_{0},\\mathbf{q}_{0}\\big\\rangle}\\\\ &{\\overset{(b)}{\\leq}\\frac{1}{2\\eta_{0}}\\|\\mathbf{x}-\\mathbf{x}_{0}\\|^{2}+\\frac{1}{2\\sigma_{0}}\\|\\mathbf{y}-\\mathbf{y}_{0}\\|^{2}-\\frac{t_{K-1}\\tau_{K-1}^{-1}}{2}\\|\\mathbf{x}-\\mathbf{x}_{K}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(a)$ uses ${\\bf q}_{0}={\\bf0}$ and $\\mathbf{x}_{-1}=\\mathbf{x}_{0}$ , and $(b)$ holds by $\\rho_{0}=0,t_{0}=1$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t_{K-1}\\left\\langle\\mathbf{y}-\\mathbf{y}_{K},\\mathbf{q}_{K}\\right\\rangle\\leq\\frac{t_{K-1}}{2\\sigma_{K-1}}\\|\\mathbf{y}-\\mathbf{y}_{K}\\|^{2}+\\frac{t_{K-1}}{2/\\sigma_{K-1}}\\|\\mathbf{q}_{K}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\mathcal{L}(\\mathbf{x},\\mathbf{y})$ is convex in $\\mathbf{x}$ and linear in $\\mathbf{y}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{K}\\big[\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y})-\\mathcal{L}(\\mathbf{x},\\bar{\\mathbf{y}}_{K})\\big]\\leq\\sum_{k=0}^{K-1}t_{k}\\big[\\mathcal{L}(\\mathbf{x}_{k+1},\\mathbf{y})-\\mathcal{L}(\\mathbf{x},\\mathbf{y}_{k+1})\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining (33) and (34), we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{K}\\left[\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y})-\\mathcal{L}(\\mathbf{x},\\bar{\\mathbf{y}}_{K})\\right]\\leq\\frac{1}{2\\tau_{0}}\\|\\mathbf{x}-\\mathbf{x}_{0}\\|^{2}-\\frac{t\\kappa-1\\tau_{K-1}^{-1}}{2}\\|\\mathbf{x}-\\mathbf{x}_{K}\\|^{2}+\\frac{1}{2\\sigma_{0}}\\|\\mathbf{y}-\\mathbf{y}_{0}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Dividing both sides by $T_{K}$ , we obtain the desired result (23). ", "page_idx": 16}, {"type": "text", "text": "Part 2. Next we show $y^{*}\\subseteq y_{K}$ . Let $\\mathbf{y}^{*}$ be any point in $\\mathcal{V}^{*}$ . Since (35) holds for any $\\mathbf{x}\\in\\mathcal{X}$ and $\\mathbf{y}\\in\\cap_{0\\leq k\\leq K-1}\\mathcal{V}_{k}\\supseteq\\mathcal{V}^{*}$ , we can place $\\mathbf{x}=\\mathbf{x}^{*},\\mathbf{y}=\\mathbf{y}^{*}\\in\\mathcal{V}^{*}$ in (23) to obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{t_{K-1}\\tau_{K-1}^{-1}}{2T_{K}}\\|\\mathbf{x}^{*}-\\mathbf{x}_{K}\\|^{2}+\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y}^{*})-\\mathcal{L}(\\mathbf{x}^{*},\\bar{\\mathbf{y}}_{K})\\leq\\frac{1}{T_{K}}\\Delta(\\mathbf{x}^{*},\\mathbf{y}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, the strong convexity of $\\mathcal{L}(\\cdot,\\mathbf{y}^{*})$ implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y}^{*})\\geq\\mathcal{L}(\\mathbf{x}^{*},\\mathbf{y}^{*})+\\frac{(\\mathbf{y}^{*})^{\\top}\\mu}{2}\\|\\bar{\\mathbf{x}}_{K}-\\mathbf{x}^{*}\\|^{2}\\geq\\mathcal{L}(\\mathbf{x}^{*},\\bar{\\mathbf{y}}_{K})+\\frac{(\\mathbf{y}^{*})^{\\top}\\mu}{2}\\|\\bar{\\mathbf{x}}_{K}-\\mathbf{x}^{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying the above two inequalities yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{({\\mathbf y}^{*})^{\\top}\\boldsymbol\\mu}{2}\\|\\bar{\\mathbf x}_{K}-{\\mathbf x}^{*}\\|^{2}\\leq\\frac{1}{T_{K}}\\Delta({\\mathbf x}^{*},{\\mathbf y}^{*}),\\ \\frac12\\|{\\mathbf x}_{K}-{\\mathbf x}^{*}\\|^{2}\\leq\\frac{\\tau_{K-1}\\sigma_{0}}{\\sigma_{K-1}}\\Delta({\\mathbf x}^{*},{\\mathbf y}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In view of (36) and Proposition 4, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbf{y}^{*})^{T}{\\boldsymbol\\mu}\\geq\\underline{{\\mu}}\\,\\|\\mathbf{y}^{*}\\|_{1}=\\underline{{\\mu}}\\operatorname*{max}\\left\\{h_{1}(\\mathbf{x}_{K},\\frac{\\sigma_{0}\\tau_{K-1}\\Delta_{X Y}}{\\sigma_{K-1}}),h_{2}(\\bar{\\mathbf{x}}_{K},\\frac{\\Delta_{X Y}}{T_{K}})\\right\\}:=\\hat{\\rho}_{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, since ${\\mathfrak{V}}^{*}\\subseteq{\\mathfrak{V}}_{K-1}$ , we have $(\\mathbf{y}^{*})^{T}\\pmb{\\mu}\\geq\\rho_{K-1}$ . Hence we have $(\\mathbf{y}^{*})^{T}\\pmb{\\mu}\\geq\\rho_{K}$ where $\\rho_{K}=\\operatorname*{max}\\{\\hat{\\rho}_{K},\\rho_{K-1}\\}$ is the output of the IMPROVE procedure. Due to the construction of $\\mathcal{V}_{K}$ , we immediately see that $\\mathbf{y}^{*}\\in\\mathcal{V}_{K}$ . This implies $y^{*}\\subseteq y_{K}$ and completes our induction proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Next, we specify the stepsize selection in Lemma 3 and develop more concrete complexity results in Corollary 1. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. Let \u03c1\u02c6k+1 := \u03c1\u02c62kk2+k(+3\u03c11k+1 \u03c1\u02c6k)kf or $k\\geq1$ and $\\begin{array}{r}{\\hat{\\rho}_{1}=3\\sqrt{\\frac{\\rho_{1}}{\\tau_{0}}}}\\end{array}$ . Suppose $\\sigma_{k},\\tau_{k}$ satisfy: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tau_{0}^{-1}\\geq L_{X Y}+L_{G}^{2}\\sigma_{0},\\;\\;\\tau_{k+1}=\\tau_{k}(1+\\rho_{k+1}\\tau_{k})^{-\\frac{1}{2}},\\;\\;\\sigma_{k+1}=\\frac{\\tau_{k}\\sigma_{k}}{\\tau_{k+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\tau_{k}^{2}}\\ge\\frac{\\hat{\\rho}_{k}^{2}}{9}k^{2}+\\frac{1}{\\tau_{0}^{2}},\\;\\;T_{k}\\ge1+\\frac{\\tau_{0}}6\\tilde{\\rho}_{k}(k+1)k,\\;\\;\\hat{\\rho}_{k}\\ge\\operatorname*{min}\\{\\rho_{1},\\hat{\\rho}_{1}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\tilde{\\rho}_{k}=2\\sum_{s=0}^{k}\\frac{\\hat{\\rho}_{s}s}{k(k+1)}}\\end{array}$ for $k\\geq1$ . Moreover, suppose $\\bar{\\rho}\\tau_{0}\\le2$ , where $\\bar{\\rho}=\\bar{c}\\cdot\\bar{\\mu}$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma_{k}^{2}\\leq\\sigma_{0}^{2}(k+1)^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We first use induction to show that $\\begin{array}{r}{\\frac{1}{\\tau_{k}^{2}}\\geq\\frac{\\hat{\\rho}_{k}^{2}}{9}k^{2}+\\frac{1}{\\tau_{0}^{2}}}\\end{array}$ . It is easy to see that $\\begin{array}{r}{\\frac{1}{\\tau_{k}^{2}}\\geq\\frac{\\hat{\\rho}_{k}^{2}}{9}k^{2}+\\frac{1}{\\tau_{0}^{2}}}\\end{array}$ holds for $k=1$ by the definition $\\hat{\\rho}_{1}=3\\sqrt{\\rho_{1}/\\tau_{0}}$ and $\\tau_{1}=\\tau_{0}(1\\!+\\!\\rho_{1}\\tau_{0})^{-\\frac12}$ . Assume $\\begin{array}{r}{\\frac{1}{\\tau_{k}^{2}}\\geq\\frac{\\hat{\\rho}_{k}^{2}}{9}k^{2}+\\frac{1}{\\tau_{0}^{2}}}\\end{array}$ holds for all $k=0,\\ldots,K$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\tau_{K+1}^{2}}=\\frac{1}{\\tau_{K}^{2}}+\\frac{\\rho_{K+1}}{\\tau_{K}}}\\\\ &{\\qquad\\ge\\frac{\\hat{\\rho}_{K}^{2}}{9}K^{2}+\\frac{1}{\\tau_{0}^{2}}+\\rho_{K+1}\\sqrt{\\frac{\\hat{\\rho}_{K}^{2}}{9}K^{2}+\\frac{1}{\\tau_{0}^{2}}}}\\\\ &{\\qquad\\ge\\frac{\\hat{\\rho}_{K}^{2}}{9}K^{2}+\\frac{1}{\\tau_{0}^{2}}+\\frac{\\rho_{K+1}\\hat{\\rho}_{K}K}{3}}\\\\ &{\\qquad\\ge\\frac{\\hat{\\rho}_{K+1}^{2}}{9}(K+1)^{2}+\\frac{1}{\\tau_{0}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which completes our induction. It follows from $\\begin{array}{r}{\\frac{1}{\\tau_{k}^{2}}\\geq\\frac{\\hat{\\rho}_{k}^{2}}{9}k^{2}+\\frac{1}{\\tau_{0}^{2}}}\\end{array}$ and the relation among $T_{k},t_{k},\\sigma_{k},\\tau_{k}$ that, for any $k\\geq1$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{k}=\\sum_{s=0}^{k-1}t_{s}=1+\\sum_{s=1}^{k-1}t_{s}\\geq1+\\sum_{s=1}^{k-1}\\frac{\\sigma_{s}}{\\sigma_{0}}=1+\\sum_{s=1}^{k-1}\\frac{\\tau_{0}}{\\tau_{s}}\\geq1+\\tau_{0}\\sum_{s=1}^{k-1}\\sqrt{\\frac{\\rho_{s}^{2}s^{2}}{9}+\\frac{1}{\\tau_{0}^{2}}}}\\\\ &{\\qquad>1+\\tau_{0}\\sum_{s=1}^{k-1}\\frac{\\hat{\\rho}_{s}s}{3}=1+\\frac{\\tau_{0}}{6}\\tilde{\\rho}_{k}(k+1)k.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, we use induction to prove ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\rho}_{k}\\geq\\operatorname*{min}\\{\\rho_{1},\\hat{\\rho}_{1}\\},\\forall k\\geq1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It is easy to find that $\\hat{\\rho}_{1}\\geq\\operatorname*{min}\\{\\rho_{1},\\hat{\\rho}_{1}\\}$ . We assume that $\\hat{\\rho}_{k}\\geq\\operatorname*{min}\\{\\rho_{1},\\hat{\\rho}_{1}\\},\\forall k\\geq1$ holds for any $k=1,\\ldots,K$ . Considering $\\hat{\\rho}_{K+1}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\rho}_{K+1}\\geq\\frac{1}{K+1}\\sqrt{\\hat{\\rho}_{K}^{2}K^{2}+3\\rho_{1}\\hat{\\rho}_{K}K}}\\\\ &{\\qquad\\geq\\frac{1}{K+1}\\sqrt{\\left(\\operatorname*{min}\\left\\{\\rho_{1},\\hat{\\rho}_{1}\\right\\}\\right)^{2}K^{2}+3\\rho_{1}\\cdot\\operatorname*{min}\\left\\{\\rho_{1},\\hat{\\rho}_{1}\\right\\}K}\\geq\\operatorname*{min}\\left\\{\\rho_{1},\\hat{\\rho}_{1}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which completes the induction. Moreover, we use induction to show $\\sigma_{k}^{2}\\leq\\sigma_{0}^{2}(k+1)^{2}$ . It is obvious that the inequality holds for $k=0$ . Assume the inequality holds for all $k=0,\\ldots,K$ , then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sigma_{K+1}^{2}=\\sigma_{K}^{2}(1+\\rho_{K+1}\\frac{\\tau_{0}\\sigma_{0}}{\\sigma_{K}})}&{}\\\\ {=\\sigma_{K}^{2}+\\rho_{K+1}\\tau_{0}\\sigma_{0}\\sigma_{K}}&{}\\\\ {\\leq\\sigma_{0}^{2}\\left((K+1)^{2}+\\rho_{K+1}\\tau_{0}(K+1)\\right)}&{}\\\\ {\\leq\\sigma_{0}^{2}(K+2)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality use the relation $\\rho_{k}\\leq\\bar{\\rho},\\forall k$ , and $\\bar{\\rho}\\tau_{0}\\le2$ . ", "page_idx": 18}, {"type": "text", "text": "E.3 Proof of Corollary 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. First, we show that the sequences $\\{\\tau_{k},\\sigma_{k},t_{k},\\rho_{k}\\}$ generated by APDPro satisfy the relationship in (11) in Theorem 1. The first part of (11) can be derived using the monotonicity of $\\{\\rho_{k}\\}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{t_{k+1}\\big(\\tau_{k+1}^{-1}-\\rho_{k+1}\\big)=\\sigma_{0}^{-1}\\big(\\sigma_{k+1}\\tau_{k+1}-\\sigma_{k+1}\\rho_{k+1}\\big)}&{}\\\\ {=\\sigma_{0}^{-1}\\big(\\sigma_{k}\\tau_{k}\\tau_{k+1}^{-2}-\\sigma_{k+1}\\rho_{k+1}\\big)}&{}\\\\ {=\\sigma_{0}^{-1}\\big(\\sigma_{k}\\big(1+\\rho_{k+1}\\tau_{k}\\big)/\\tau_{k}-\\sigma_{k+1}\\rho_{k+1}\\big)}&{}\\\\ {=\\sigma_{0}^{-1}\\big(\\sigma_{k}/\\tau_{k}+\\rho_{k+1}\\sigma_{k}-\\sigma_{k+1}\\rho_{k+1}\\big)}&{}\\\\ {\\leq t_{k}\\tau_{k}^{-1}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second part of (11) can be easily verified using the parameters setting. ", "page_idx": 18}, {"type": "text", "text": "Next, we prove the last term in (11) by induction. Firstly, it easy to verify that for any $\\sigma_{0}\\,>\\,0$ , there exists $\\tau_{0}\\in(0,(L_{X Y}+L_{G}^{2}\\sigma_{0})^{-1}\\bar{]}$ such that last term of (11) holds. Hence, when $k=0$ , the last term of (11) is directly from the first term of (13). Suppose that the last term of (11) holds for $k=0,\\ldots,K-1$ . From $\\sigma_{K-1}/\\sigma_{K}=\\tau_{K}/\\tau_{K-1}\\leq1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\tau_{K}}=\\frac{\\sigma_{K}}{\\tau_{K-1}\\sigma_{K-1}}\\geq\\frac{L_{X Y}}{\\sigma_{K-1}/\\sigma_{K}}+L_{G}^{2}\\sigma_{K}\\geq L_{X Y}+L_{G}^{2}\\sigma_{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Without loss of generality, place $\\mathbf{x}=\\mathbf{x}^{*}$ , $\\begin{array}{r}{\\mathbf{y}=\\mathbf{y}^{+}:=\\left(\\|\\mathbf{y}^{*}\\|_{1}+c^{*}\\right)\\frac{[G(\\bar{\\mathbf{x}}_{K})]_{+}}{\\|[G(\\bar{\\mathbf{x}}_{K})]_{+}\\|}}\\end{array}$ in (23), and using $\\|\\mathbf{y}^{*}\\|_{1}\\leq\\bar{c}$ in Proposition 1. It is easy to see $\\|\\mathbf{y}^{+}\\|\\,=\\,\\|\\mathbf{y}^{*}\\|_{1}+c^{*}\\,\\le\\,\\bar{c}$ , and $\\|\\mathbf{y}^{+}\\|_{1}\\,\\geq\\,\\|\\mathbf{y}^{+}\\|\\,=$ $\\|\\mathbf{y}^{*}\\|_{1}+c^{*}\\geq\\|\\mathbf{y}^{*}\\|_{1}$ , Hence, we conclude that $\\mathbf{y}^{+}\\in\\mathcal{V}_{k},\\forall k\\geq0$ . ", "page_idx": 18}, {"type": "text", "text": "Now observe that $\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y}^{*})-\\mathcal{L}(\\mathbf{x}^{*},\\mathbf{y}^{*})\\geq0$ , which implies $f\\big(\\bar{\\mathbf{x}}_{K}\\big)+\\langle\\mathbf{y}^{*},G(\\bar{\\mathbf{x}}_{K})\\rangle-f\\big(\\mathbf{x}^{*}\\big)\\geq0$ . In view of $\\langle\\mathbf{y}^{*},G(\\bar{\\mathbf{x}}_{K})\\rangle\\leq\\langle\\mathbf{y}^{*},[G(\\bar{\\mathbf{x}}_{K})]_{+}\\rangle\\leq\\|\\mathbf{y}^{*}\\|\\cdot\\|[G(\\bar{\\mathbf{x}}_{K})]_{+}\\|$ , then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\bar{\\bf x}_{K})+\\|{\\bf y}^{*}\\|\\cdot\\|[G(\\bar{\\bf x}_{K})]_{+}\\|-f({\\bf x}^{*})\\ge0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, it follows from $\\|\\mathbf{y}^{*}\\|_{1}\\geq\\|\\mathbf{y}^{*}\\|$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y}^{+})-\\mathcal{L}(\\mathbf{x}^{*},\\bar{\\mathbf{y}}_{K})\\geq\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y}^{+})-\\mathcal{L}(\\mathbf{x}^{*},\\mathbf{y}^{*})}\\\\ &{\\qquad\\qquad\\qquad\\geq f(\\bar{\\mathbf{x}}_{K})+(\\|\\mathbf{y}^{*}\\|+c^{*})\\,\\|[G(\\bar{\\mathbf{x}}_{K})]_{+}\\|-f(\\mathbf{x}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining (45), (46) and (23), we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}\\left\\{c^{*}\\|[G(\\bar{\\mathbf{x}}_{K})]_{+}\\|,f(\\bar{\\mathbf{x}}_{K})-f(\\mathbf{x}^{*})\\right\\}\\leq\\frac{1}{T_{K}}\\big(\\frac{1}{2\\tau_{0}}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}+\\frac{D_{Y}^{2}}{2\\sigma_{0}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In view of the bound in (38) and the relation between $\\tau_{k},\\sigma_{k}$ , we can get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\tau_{k}}{\\sigma_{k}}\\,\\le\\,\\frac{3}{\\hat{\\rho}_{k}^{2}\\tau_{0}^{2}k^{2}+9\\sigma_{0}/\\tau_{0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In view of (47) and (38), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}\\big\\{c^{*}\\|[G(\\bar{\\mathbf{x}}_{K})]_{+}\\|,f(\\bar{\\mathbf{x}}_{K})-f(\\mathbf{x}^{*})\\big\\}\\leq\\frac{6}{6+\\tau_{0}\\tilde{\\rho}_{K}(K+1)K}\\big(\\frac{1}{2\\tau_{0}}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}+\\frac{D_{\\gamma}^{2}}{2\\sigma_{0}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining (23) and (48) yields $\\begin{array}{r}{\\frac{1}{2}\\|\\mathbf{x}_{K}-\\mathbf{x}^{*}\\|^{2}\\leq3\\sigma_{0}\\Delta(\\mathbf{x}^{*},\\mathbf{y}^{*})/(\\hat{\\rho}_{K}^{2}\\tau_{0}^{2}K^{2}+9\\sigma_{0}/\\tau_{0}).}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "F Convergence analysis of rAPDPro ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. First, we show that the choice of $\\tau_{0}^{s}\\,=\\,\\bar{\\tau},\\sigma_{0}^{s}\\,=\\,\\bar{\\sigma},\\forall s\\,\\geq\\,0$ satisfy the condition (13) in Corollary 1: $(\\tau_{0}^{s})^{-1}\\geq(1-\\nu_{0})(\\tau_{0}^{s})^{-1}=\\check{L_{X Y}}+c\\check{L_{G}^{\\check{\\sigma}}}\\sigma_{0}^{s}/\\delta\\geq L_{X Y}+c L_{G}^{2}\\sigma_{0}^{s}.$ ", "page_idx": 19}, {"type": "text", "text": "Next, we show (15) holds by induction. Clearly, (15) holds for $s=0$ . Assume $\\|\\mathbf{x}_{0}^{s}-\\mathbf{x}^{*}\\|^{2}\\leq\\Delta_{s}$ holds for $s=0,\\ldots,S-1$ . Then by Theorem 1, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{x}_{0}^{S}-\\mathbf{x}^{*}\\|^{2}\\leq\\frac{\\sigma_{0}^{S}\\tau_{N_{S}}^{S}}{\\sigma_{N_{S}}^{S}}\\Big(\\frac{2}{\\tau_{0}^{S}}\\Delta_{S}+\\frac{1}{\\sigma_{0}^{S}}D_{Y}^{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In view of the first bound in (38) and the relation between $\\tau_{N_{s}}^{s},\\sigma_{N_{s}}^{s}$ , we can get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\tau_{N_{s}}^{s}}{\\sigma_{N_{s}}^{s}}\\,\\leq\\,\\frac{9}{\\sigma_{0}^{s}\\tau_{0}^{s}(\\hat{\\rho}_{N_{s}}N_{s})^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining (49) and (50) yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{x}_{0}^{S}-\\mathbf{x}^{*}\\|^{2}\\leq\\frac{18}{(\\hat{\\rho}_{N_{s}}\\tau_{0}^{s}N_{s})^{2}}+\\frac{9D_{Y}^{2}}{\\sigma_{0}^{s}\\tau_{0}^{s}(\\hat{\\rho}_{N_{s}}N_{s})^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the algorithm sets $N_{s}=\\lceil\\operatorname*{max}\\{6(\\hat{\\rho}_{N_{s}}\\tau_{0}^{s})^{-1},\\sqrt{2}^{s}\\cdot3\\sqrt{2}D_{Y}/\\big(\\hat{\\rho}_{N_{s}}D_{X}\\sqrt{\\tau_{0}^{s}\\sigma_{0}^{s}}\\big)\\}\\rceil$ , it follows that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{18}{(\\hat{\\rho}_{N_{s}}\\tau_{0}^{s}N_{s})^{2}}\\leq\\frac{18}{(\\hat{\\rho}_{N_{s}}\\tau_{0}^{s})^{2}}\\cdot\\frac{(\\hat{\\rho}_{N_{s}}\\tau_{0}^{s})^{2}}{36}=\\frac{1}{2},\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {\\frac{9D_{Y}^{2}}{\\sigma_{0}^{s}\\tau_{0}^{s}(\\hat{\\rho}_{N_{s}}N_{s})^{2}}\\leq\\frac{9D_{Y}^{2}}{\\sigma_{0}^{s}\\tau_{0}^{s}\\hat{\\rho}_{N_{s}}^{2}}\\cdot\\frac{\\hat{\\rho}_{N_{s}}^{2}\\sigma_{0}^{s}\\tau_{0}^{s}D_{X}^{2}}{18D_{Y}^{2}2^{s}}=\\frac{1}{2}\\cdot2^{-s}D_{X}^{2}=\\frac{1}{2}\\Delta_{S},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies the desired result (15). ", "page_idx": 19}, {"type": "text", "text": "Let the algorithm run for $S=\\left\\lceil\\log_{2}(D_{X}^{2}/\\varepsilon)\\right\\rceil$ epochs, then $\\|\\mathbf{x}_{0}^{S}-\\mathbf{x}^{*}\\|^{2}\\leq D_{X}^{2}\\cdot2^{-S}\\leq\\varepsilon$ . The total iteration number required by Algorithm 2 for attaining a solution $\\mathbf{x}_{0}^{S}$ such that $\\|\\mathbf{x}_{0}^{S}-\\mathbf{x}^{*}\\|^{2}\\leq\\varepsilon$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{s=0}^{S}N_{s}\\leq\\sum_{s=0}^{S}\\bigg\\{\\frac{6}{\\tilde{\\rho}_{N_{s}}^{\\delta}\\tau_{0}^{s}}+\\frac{3\\sqrt{2}D\\gamma}{\\tilde{\\rho}_{N_{s}}^{\\delta}D_{N}\\sqrt{\\tau_{0}^{s}\\sigma_{0}^{s}}}\\sqrt{2}^{s}+1\\bigg\\}}\\\\ &{\\quad\\quad\\quad\\quad\\frac{(a)}{=\\left(\\frac{6}{\\pi_{1}\\tau_{0}^{s}}+1\\right)(S+1)}+\\frac{3\\sqrt{2}D\\gamma}{\\pi_{2}D_{N}\\sqrt{\\tau_{0}^{s}\\sigma_{0}^{s}}}\\sum_{s=0}^{S}\\sqrt{2}^{s}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\left(\\frac{12}{\\pi_{1}\\tau_{0}^{s}}+2\\right)\\left[\\log_{2}\\frac{D\\chi}{\\sqrt{\\varepsilon}}+1\\right]+\\frac{3\\sqrt{2}D\\gamma}{\\varpi_{2}D_{N}\\sqrt{\\tau_{0}^{s}\\sigma_{0}^{s}}}\\cdot\\frac{\\sqrt{2}^{S+1}-1}{\\sqrt{2}-1}}\\\\ &{\\quad\\quad\\quad\\leq\\left(\\frac{12}{\\pi_{1}\\tau_{0}^{s}}+2\\right)\\left[\\log_{2}\\frac{D\\chi}{\\sqrt{\\varepsilon}}+1\\right]+\\frac{3\\sqrt{2}D\\gamma(\\sqrt{2}+1)}{\\varpi_{2}D_{N}\\sqrt{\\tau_{0}^{s}\\sigma_{0}^{s}}}\\cdot\\left(\\sqrt{2}^{\\log_{2}(D_{N}^{2}/\\varepsilon)+2}-1\\right)}\\\\ &{\\quad\\quad\\quad\\leq\\left(\\frac{12}{\\pi_{1}\\tau_{0}^{s}}+2\\right)\\left[\\log_{2}\\frac{D\\chi}{\\sqrt{\\varepsilon}}+1\\right]+\\frac{6D\\gamma(\\sqrt{2}+2)}{\\varpi_{2}\\sqrt{\\tau_{0}^{s}\\sigma_{0}^{s}}}\\cdot\\frac{1}{\\sqrt{\\varepsilon}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(a)$ holds by $\\begin{array}{r}{\\sum_{s=0}^{S}(\\hat{\\rho}_{N_{s}}^{s})^{-1}=(\\varpi_{1})^{-1}(S+1)}\\end{array}$ and $\\begin{array}{r}{\\sum_{s=0}^{S}\\sqrt{2}^{s}/\\hat{\\rho}_{N_{s}}^{s}=(\\varpi_{2})^{-1}\\sum_{s=0}^{S}\\sqrt{2}^{s}.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Now, we give some proof details in dual convergence results. Let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{Q}_{j}(\\mathbf{x},\\mathbf{y}):=\\frac{(\\tau_{j})^{-1}-\\rho_{j}}{2}\\|\\mathbf{x}-\\mathbf{x}_{j}\\|^{2}+\\frac{1}{2\\sigma_{j}}\\|\\mathbf{y}-\\mathbf{y}_{j}\\|^{2}+\\big(\\sigma_{j-1}/\\sigma_{j}\\big)\\left\\langle\\mathbf{y}_{j}-\\mathbf{y},G(\\mathbf{x}_{j})-G(\\mathbf{x}_{j-1})\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{(\\sigma_{j-1}/\\sigma_{j})}{2/\\sigma_{j-1}}\\|G(\\mathbf{x}_{j})-G(\\mathbf{x}_{j-1})\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then we establish an important property about the solution sequence in the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma 4. Assume $\\bar{\\tau}^{-1}>\\overline{{\\rho}}$ and choose $\\nu_{0}>0$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n1>\\operatorname*{inf}_{j\\geq0}\\{\\sigma_{j-1}/\\sigma_{j}\\}\\geq\\delta+\\nu_{0}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then there exists an $\\nu_{1}>0$ such that for any $j\\geq0$ and any KKT point $(\\mathbf{x}^{*},\\tilde{\\mathbf{y}}^{*})$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq t_{j}Q_{j}(\\mathbf x^{*},\\widetilde{\\mathbf y}^{*})-t_{j+1}Q_{j+1}(\\mathbf x^{*},\\widetilde{\\mathbf y}^{*})-\\nu_{1}t_{j}\\Big[\\frac{1}{2\\tau_{j}}\\|\\mathbf x_{j+1}-\\mathbf x_{j}\\|^{2}+\\frac{1}{2\\sigma_{j}}\\|\\mathbf y_{j+1}-\\mathbf y_{j}\\|^{2}\\Big],}\\\\ &{0<t_{j}Q_{j}(\\mathbf x^{*},\\widetilde{\\mathbf y}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. First, we give some results that will be used repeatedly in the following. For notation simplicity, we denote $\\theta_{j}\\,=\\,\\sigma_{j-1}/\\sigma_{j}$ . In view of Lemma 3, and the parameter ergodic sequence generated by rAPDPro, we have $\\left\\{\\bar{(\\tau_{k}^{s})}^{-1},\\sigma_{k}^{s}\\right\\}$ is monotonically increasing sequence in $k$ , $\\bar{\\tau}=$ $\\tau_{0}^{s},\\bar{\\sigma}=\\sigma_{0}^{s},t_{0}^{s}=1,\\forall s\\geq0$ , and there exist a $\\nu_{3}>0$ such that $\\bar{\\sigma}+\\nu_{3}\\leq\\underline{{\\sigma}}:=\\operatorname*{min}_{s}\\{\\sigma_{N_{s}}^{s}\\}$ . Now, for rAPDPro, we claim that there exist $\\nu_{1},\\nu_{2}>0$ such that the following two conditions hold ", "page_idx": 20}, {"type": "text", "text": "1. For any $j\\geq0$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{1-\\delta,(\\tau_{j}^{-1}-L_{X Y}-L_{G}^{2}\\sigma_{j})\\tau_{j}\\right\\}\\geq\\nu_{1}>0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t_{j}\\operatorname*{min}\\left\\{\\tau_{j}^{-1}-\\rho_{j},\\frac{1}{\\sigma_{j}}-\\frac{\\delta}{\\sigma_{j-1}}\\right\\}\\geq\\nu_{2}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "2. For any $j\\geq0$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\leq t_{j}Q_{j}(\\mathbf x^{*},\\widetilde{\\mathbf y}^{*})-t_{j+1}Q_{j+1}(\\mathbf x^{*},\\widetilde{\\mathbf y}^{*})-\\nu_{1}t_{j}\\big((2\\tau_{j})^{-1}\\|\\mathbf x_{j+1}-\\mathbf x_{j}\\|^{2}\\big)+(2\\sigma_{j})^{-1}\\|\\mathbf y_{j+1}-\\mathbf y_{j}\\|^{2}\\big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Part 1. We first consider two subsequent points $\\mathbf{x}_{j}$ and $\\mathbf{x}_{j+1}$ within the same epoch, and assume $j\\sim(s,k)$ . Then, it follows from $\\theta_{k}^{s}=\\sigma_{k-1}^{s}/\\sigma_{k}^{s}$ that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\sigma_{k}^{s})^{-1}-\\theta_{k}^{s}\\delta(\\sigma_{k-1}^{s})^{-1}=(\\sigma_{k}^{s})^{-1}-\\delta(\\sigma_{k}^{s})^{-1}=\\frac{1-\\delta}{\\sigma_{k}^{s}}\\overset{(51)}{\\geq}\\frac{\\nu_{0}}{\\sigma_{k}^{s}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we use induction to show ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1-\\nu_{0}}{\\tau_{k}^{s}}\\geq L_{X Y}+L_{G}^{2}\\sigma_{k}^{s}\\delta^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "When $k\\ =\\ 0$ , inequality (56) degenerates as the definition of $\\tau_{0}^{s},\\sigma_{0}^{s}$ . Suppose (56) holds for $k=0,1,\\ldots,K-1$ . Then, from $\\theta_{K}^{s}=\\sigma_{K-1}^{s}/\\sigma_{K}^{s}=\\tau_{K}^{s}/\\tau_{K-1}^{s}\\le\\bar{1}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\nu_{0})(\\tau_{K}^{s})^{-1}=(1-\\nu_{0})(\\tau_{K-1}^{s}\\theta_{K}^{s})^{-1}\\ge\\frac{L_{X Y}}{\\theta_{K}^{s}}+\\frac{L_{G}^{2}\\sigma_{K-1}^{s}\\delta^{-1}}{\\theta_{K}^{s}}\\ge L_{X Y}+L_{G}^{2}\\sigma_{K}^{s}\\delta^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which completes our induction proof. Hence, combining (55) and (56), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{1-\\delta,\\,\\big((\\tau_{k}^{s})^{-1}-L_{X Y}-L_{G}^{2}\\sigma_{k}^{s}/\\delta\\big)\\tau_{k}^{s}\\right\\}\\geq\\nu_{0},\\,\\,\\,\\forall k\\in[N_{s}].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, when switching to the next epoch $\\ {\\mathit{s}}\\to s+1{\\mathit{\\Omega}}$ ), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\sigma_{0}^{s+1}\\big((\\sigma_{0}^{s+1})^{-1}-\\theta_{0}^{s+1}\\delta/\\sigma_{N_{s}}^{s}\\big)\\overset{(a)}{\\geq}\\sigma_{0}^{s+1}((\\sigma_{0}^{s+1})^{-1}-(\\sigma_{N_{s}}^{s})^{-1})\\overset{(b)}{\\geq}1-\\sigma_{0}^{s+1}\\underline{{\\sigma}}^{-1}=1-\\bar{\\sigma}\\underline{{\\sigma}}^{-1}}\\\\ {((\\tau_{0}^{s+1})^{-1}-L_{X Y}-L_{G}^{2}\\delta^{-1}\\sigma_{0}^{s+1})\\tau_{0}^{s+1}\\overset{(c)}{\\geq}\\nu_{0}\\tau_{0}^{s+1}=\\nu_{0}\\bar{\\tau},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(a)$ holds by $\\theta_{0}^{s}=1,\\delta<1,(b)$ follows from $(\\sigma_{N_{s}}^{s})^{-1}\\geq\\underline{{\\sigma}}^{-1}$ . Hence, combining (55), (57) and (58), we completes our proof of (52) by setting $\\nu_{1}=\\operatorname*{min}\\{1-\\bar{\\sigma}\\underline{{\\sigma}}^{-1},\\nu_{0}\\bar{\\tau},\\nu_{0}\\}$ . ", "page_idx": 20}, {"type": "text", "text": "Since rAPDPro reset the stepsize periodically and $\\{t_{k}^{s},(\\tau_{k}^{s})^{-1}\\}_{k\\in[N_{s}]}$ are two monotonically increasing sequences, hence ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{j\\geq0}t_{j}(\\tau_{j}^{-1}-\\rho_{j})\\geq t_{0}^{s}(\\bar{\\tau}^{-1}-\\overline{{\\rho}})=\\bar{\\tau}^{-1}-\\overline{{\\rho}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consider $\\operatorname*{inf}_{k\\in[N_{s}]}t_{k}^{s}\\sigma_{k}^{s}(1-\\delta\\sigma_{k}^{s}/\\sigma_{k-1}^{s})$ . Combining $\\delta+\\nu_{0}\\leq\\operatorname*{inf}_{k\\in[N_{s}]}\\left\\{\\theta_{k}^{s}\\right\\}$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in[N_{s}]}t_{k}^{s}\\sigma_{k}^{s}(1-\\delta\\frac{\\sigma_{k}^{s}}{\\sigma_{k-1}^{s}})=\\operatorname*{inf}_{k\\in[N_{s}]}t_{k}^{s}\\sigma_{k}^{s}(1-\\delta/\\theta_{k}^{s})\\geq\\nu_{0}\\bar{\\sigma}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, when switching to the next epoch $\\mathbf{\\chi}^{\\prime}s\\to s+1$ ), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{s\\geq0}t_{0}^{s+1}\\sigma_{0}^{s+1}(1-\\delta\\sigma_{0}^{s+1}(\\sigma_{N_{s}}^{s})^{-1})=\\bar{\\sigma}^{2}\\operatorname*{inf}_{s\\geq0}(\\bar{\\sigma}^{-1}-\\delta(\\sigma_{N_{s}}^{s})^{-1})\\geq\\bar{\\sigma}(1-\\delta),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality holds by $\\bar{\\sigma}=\\sigma_{0}^{s}\\le\\sigma_{N_{s}}^{s}$ . Hence, it follows from (59), (60) and (61) that there exist $\\nu_{2}=\\operatorname*{min}\\{\\bar{\\tau}^{-1}-\\overline{{\\rho}},\\nu_{0}\\bar{\\sigma},\\bar{\\sigma}(1-\\delta)\\}$ such (53) holds. ", "page_idx": 20}, {"type": "text", "text": "Part 2. for any $j\\geq0$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t_{j+1}Q_{j+1}(\\mathbf x^{*},\\mathbf{\\tilde{y}}^{*})\\leq t_{j}\\big(\\frac{(\\tau_{j})^{-1}}{2}\\|\\mathbf x^{*}-\\mathbf x_{j+1}\\|^{2}+\\langle G(\\mathbf x_{j+1})-G(\\mathbf x_{j}),\\mathbf y_{j+1}-\\tilde{\\mathbf y}^{*}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left(2\\sigma_{j}\\right)^{-1}\\|\\mathbf{\\tilde{y}}^{*}-\\mathbf y_{j+1}\\|^{2}+\\frac{\\sigma_{j}}{2}\\|G(\\mathbf x_{j+1})-G(\\mathbf x_{j})\\|^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consider $k\\;\\in\\;\\{0,1,\\ldots,N_{s}\\}$ . Inequality (51) implies (11) holds (see proof of Corollary 1 in Section E.3). Hence, for $0\\leq k\\leq N_{s}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t_{j+1}Q_{j+1}(\\mathbf x^{*},\\tilde{\\mathbf y}^{*})\\leq t_{k}^{s}\\big(\\frac{(\\tau_{k}^{s})^{-1}}{2}\\|\\mathbf x^{*}-\\mathbf x_{k+1}^{s}\\|^{2}+\\big\\langle G(\\mathbf x_{k+1}^{s})-G(\\mathbf x_{k}^{s}),\\mathbf y_{k+1}^{s}-\\tilde{\\mathbf y}^{*}\\big\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\frac1{2\\sigma_{k}^{s}}\\big\\|\\tilde{\\mathbf y}^{*}-\\mathbf y_{k+1}^{s}\\big\\|^{2}+\\frac{\\sigma_{k}^{s}}{2\\delta}\\big\\|G(\\mathbf x_{k+1}^{s})-G(\\mathbf x_{k}^{s})\\big\\|^{2}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $j$ corresponds to $(s,k)$ . Furthermore, consider switching to next epoch $'s\\rightarrow s+1$ ). Since $t_{k}^{s}(\\tau_{k}^{s})^{-1}$ is an increasing sequence in $k$ , $\\rho_{0}^{s+1}>0,t_{0}^{s+1}=1$ , hence ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t_{N_{s}}^{s}(\\tau_{N_{s}}^{s})^{-1}\\geq t_{0}^{s+1}(\\tau_{0}^{s+1})^{-1}-\\rho_{0}^{s+1}t_{0}^{s+1},\\forall s\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{t_{N_{s}}^{s}}{\\sigma_{N_{s}}^{s}}\\overset{(a)}{=}\\frac{t_{0}^{s+1}}{\\sigma_{0}^{s+1}},\\ t_{N_{s}}^{s}\\overset{(b)}{\\geq}t_{0}^{s+1}\\overset{(c)}{=}t_{0}^{s+1}\\theta_{0}^{s+1},t_{N_{s}}^{s}\\sigma_{N_{s}}^{s}\\overset{(b)}{\\geq}t_{0}^{s+1}\\sigma_{0}^{s+1}\\overset{(c)}{=}t_{0}^{s+1}\\sigma_{0}^{s+1}\\theta_{0}^{s+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(a)$ holds by the definition of $\\begin{array}{r}{t_{k}^{s}=\\frac{\\sigma_{k}^{s}}{\\sigma_{0}^{s}}}\\end{array}$ , $(b)$ holds by $\\{t_{k}^{s},\\sigma_{k}^{s}\\}$ is an increasing sequence in $k$ , and $(c)$ holds by $\\theta_{0}^{s+1}=1$ . Hence, by (64) and (65), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t_{j+1}Q_{j+1}(\\mathbf{x}^{*},\\mathbf{\\tilde{y}}^{*})\\leq t_{N_{s}}^{s}\\left(\\frac{1}{2\\tau_{N_{s}}^{s}}\\|\\mathbf{x}^{*}-\\mathbf{x}_{0}^{s+1}\\|^{2}+\\frac{\\sigma_{N_{s}}^{s}}{2}\\|G(\\mathbf{x}_{0}^{s+1})-G(\\mathbf{x}_{N_{s}}^{s})\\|^{2}\\right.}\\\\ &{\\left.\\qquad\\qquad\\qquad\\qquad+\\left.\\frac{1}{2\\sigma_{N_{s}}^{s}}\\|\\tilde{\\mathbf{y}}^{*}-\\mathbf{y}_{0}^{s+1}\\|^{2}+\\left\\langle G(\\mathbf{x}_{0}^{s+1})-G(\\mathbf{x}_{N_{s}}^{s}),\\mathbf{y}_{0}^{s+1}-\\tilde{\\mathbf{y}}^{*}\\right\\rangle\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $j$ corresponds to $(s,N_{s})$ . By putting (63) and (66) together, we complete the proof of (62). ", "page_idx": 21}, {"type": "text", "text": "Placing $(\\mathbf x,\\mathbf y)=(\\mathbf x^{*},\\mathbf\\tilde{\\mathbf y}^{*}),(\\mathbf x_{k+1},\\mathbf y_{k+1})=(\\mathbf x_{j+1},\\mathbf y_{j+1})$ in (32) and multiplying $t_{j}$ on both sides, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq t_{j}[\\mathcal{L}(\\mathbf{x}_{j+1},\\widetilde{\\mathbf{y}}^{*})-\\mathcal{L}(\\mathbf{x}^{*},\\mathbf{y}_{j+1})]}\\\\ &{\\quad\\leq t_{j}\\[\\frac{\\tau_{j}^{-1}-\\rho_{j}}{2}\\|\\mathbf{x}-\\mathbf{x}_{j}\\|^{2}-\\frac{\\tau_{j}^{-1}}{2}\\|\\mathbf{x}-\\mathbf{x}_{j+1}\\|^{2}+\\frac{\\theta_{j}}{2\\delta/\\sigma_{j-1}}\\|\\mathbf{q}_{j}\\|^{2}-\\frac{1}{2\\delta/\\sigma_{j}}\\|\\mathbf{q}_{j+1}\\|^{2}}\\\\ &{\\quad\\quad+\\left(2\\sigma_{j}\\right)^{-1}\\left(\\|\\mathbf{y}-\\mathbf{y}_{j}\\|^{2}-\\|\\mathbf{y}-\\mathbf{y}_{j+1}\\|^{2}\\right)+\\left\\langle\\mathbf{y}-\\mathbf{y}_{j+1},\\mathbf{q}_{j+1}\\right\\rangle-\\theta_{j}\\langle\\mathbf{y}-\\mathbf{y}_{j},\\mathbf{q}_{j}\\rangle}\\\\ &{\\quad\\quad-\\frac{\\sigma_{j}^{-1}-\\theta_{j}\\delta/\\sigma_{j-1}}{2}\\|\\mathbf{y}_{j+1}-\\mathbf{y}_{j}\\|^{2}+\\frac{L_{G}^{2}}{2\\delta/\\sigma_{j}}\\|\\mathbf{x}_{j+1}-\\mathbf{x}_{j}\\|^{2}-\\frac{\\tau_{j}^{-1}-L_{X Y}}{2}\\|\\mathbf{x}_{j+1}-\\mathbf{x}_{j}\\|^{2}\\right]}\\\\ &{\\quad\\leq t_{j}Q_{j}(\\mathbf{x}^{*},\\widetilde{\\mathbf{y}}^{*})-t_{j+1}Q_{j+1}(\\mathbf{x}^{*},\\widetilde{\\mathbf{y}}^{*})-\\nu_{1}t_{j}[(2\\tau_{j})^{-1}\\|\\mathbf{x}_{j+1}-\\mathbf{x}_{j}\\|^{2}+(2\\sigma_{j})^{-1}\\|\\mathbf{y}_{j+1}-\\mathbf{y}_{j}\\|_{\\cdot}^{2}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality holds by (62) and (52). It follows from (53), $\\sigma_{j-1}/\\sigma_{j}\\leq1$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\mathbf{y}_{j}-\\tilde{\\mathbf{y}}^{\\ast},\\mathbf{q}_{j}\\rangle\\geq-\\frac{\\sigma_{j-1}}{2\\delta}\\|\\mathbf{q}_{j}\\|^{2}-\\frac{\\delta/\\sigma_{j-1}}{2}\\|\\tilde{\\mathbf{y}}^{\\ast}-\\mathbf{y}_{k}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t_{j}Q_{j}(\\mathbf x^{*},\\widetilde{\\mathbf y}^{*})\\geq t_{j}\\big((2\\tau_{j})^{-1}\\|\\mathbf x^{*}-\\mathbf x_{j}\\|^{2}+(2\\sigma_{j})^{-1}\\|\\widetilde{\\mathbf y}^{*}-\\mathbf y_{j}\\|^{2}-\\frac{\\delta}{2\\sigma_{j-1}}\\|\\mathbf y_{j}-\\widetilde{\\mathbf y}^{*}\\|^{2}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\nu_{2}\\big(\\frac{1}{2}\\|\\mathbf x^{*}-\\mathbf x_{j}\\|^{2}+\\frac{1}{2}\\|\\mathbf y_{j}-\\widetilde{\\mathbf y}^{*}\\|^{2}\\big)>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining (67) and (68), we complete our proof of (54). ", "page_idx": 21}, {"type": "text", "text": "F.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. Since $\\left\\{(\\mathbf{x}_{j},\\mathbf{y}_{j})\\right\\}$ located in set $\\mathcal X\\times\\mathcal Y$ is a bounded sequence, it must have a convergent subsequence $\\begin{array}{r l}{\\operatorname*{lim}_{n\\to\\infty}(\\mathbf{\\hat{x}}_{j_{n}},\\mathbf{y}_{j_{n}})\\;=\\;\\;(\\mathbf{x}^{*},\\mathbf{y}^{*})}\\end{array}$ , where $\\mathbf{y}^{*}$ is the limit point. We claim that limit point $(\\mathbf{x}^{*},\\mathbf{y}^{*})$ satisfies the KKT condition. Placing $a_{j}=t_{j}Q_{j}(\\mathbf x^{*},\\tilde{\\mathbf{y}}^{*}),b_{j}=\\nu_{1}t_{j}[(2\\tau_{j})^{-1}\\|\\mathbf{x}_{j+1}-$ $\\mathbf{x}_{j}\\|^{2}+(2\\sigma_{j})^{-1}\\|\\mathbf{y}_{j+1}-\\mathbf{y}_{j}\\|^{2}\\big]$ and $c_{j}\\,=\\,0$ in Lemma 2. It follows from (54) in Lemma 4 that $a_{j}\\geq0,b_{j}>0$ . Hence, we have $\\begin{array}{r}{\\sum_{j=0}^{\\infty}\\|\\mathbf{x}_{j+1}-\\mathbf{x}_{j}\\|^{2}<\\infty}\\end{array}$ , and $\\begin{array}{r}{\\sum_{j=0}^{\\infty}\\|\\mathbf{y}_{j+1}-\\mathbf{y}_{j}\\|^{2}<\\infty}\\end{array}$ , which implies $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\|\\mathbf{x}_{j_{n}}-\\mathbf{x}_{j_{n}+1}\\|^{2}=0}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\|\\mathbf{y}_{j_{n}}-\\mathbf{y}_{j_{n}+1}\\|^{2}=0}\\end{array}$ . There are two different cases for $\\tau_{j_{n}}$ when $j_{n}\\rightarrow\\infty$ , and we discuss the value of $B_{j_{n}+1}$ in (25) decided by $\\tau_{j_{n}}$ in each of the two cases below. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Case 1: $\\tau_{j_{n}}^{-1}<\\infty$ . By the definition of $B_{j_{n}+1}$ in (27) and $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\|\\mathbf{x}_{j_{n}}-\\mathbf{x}_{j_{n}+1}\\|^{2}=0}\\end{array}$ , we have $B_{j_{n}+1}\\leq\\|\\mathbf{x}-\\mathbf{x}_{j_{n}+1}\\|\\cdot\\|\\mathbf{x}_{j_{n}+1}-\\mathbf{x}_{j_{n}}\\|/\\tau_{j_{n}}\\overset{n\\to\\infty}{\\longrightarrow}0.$ . ", "page_idx": 22}, {"type": "text", "text": "Case 2: $\\tau_{j_{n}}^{-1}=\\infty$ . It follows from (39) that $\\tau_{j_{n}}^{-1}$ increases at order $\\Theta(k)$ , where $j_{n}\\sim(s,k)$ . By (23), we obtain $\\|\\mathbf{x}-\\mathbf{x}_{j_{n}}\\|$ decreases at order ${\\mathcal O}(1/k)\\;(j_{n}\\sim(s,k))$ . Hence, combining $\\ensuremath{\\operatorname{lim}}_{n\\to\\infty}\\|\\ensuremath{\\mathbf{x}}_{j_{n}}-$ $\\mathbf{x}_{j_{n}+1}\\|^{2}\\,=\\,0$ , we have $\\begin{array}{r}{B_{j_{n}+1}\\,\\leq\\,\\frac{1}{\\tau_{j_{n}}}\\big(\\|\\mathbf{x}-\\mathbf{x}_{j_{n}+1}\\|\\|\\mathbf{x}_{j_{n}+1}-\\mathbf{x}_{j_{n}}\\|\\big)\\,\\overset{n\\to\\infty}{\\longrightarrow}\\,0}\\end{array}$ . It follows from $\\operatorname*{lim}_{n\\to\\infty}\\mathbf{x}_{j_{n}}=\\mathbf{x}^{*}$ , $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}B_{j_{n}+1}=0}\\end{array}$ and (25) that ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(\\mathbf{x}^{*})+\\langle\\nabla G(\\mathbf{x}^{*})\\mathbf{y}^{*},\\mathbf{x}^{*}\\rangle\\leq f(\\mathbf{x})+\\langle\\nabla G(\\mathbf{x}^{*})\\mathbf{y}^{*},\\mathbf{x}\\rangle,\\forall\\mathbf{x}\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, according to the first-order optimality condition, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{0}\\in\\partial f(\\mathbf{x}^{*})+\\nabla G(\\mathbf{x}^{*})\\mathbf{y}^{*}+\\mathcal{N}_{\\mathcal{X}}(\\mathbf{x}^{*}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next, we show the complementary slackness holds for $(\\mathbf{x}^{*},\\mathbf{y}^{*})$ . Since $\\sigma_{j_{n}}^{-1}$ has an upper bound $\\bar{\\sigma}^{-1}$ , $\\|\\mathbf{y}-\\mathbf{y}_{j_{n}+1}\\|\\leq D_{Y}$ , $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\|\\mathbf{y}_{j_{n}}-\\mathbf{y}_{j_{n}+1}\\|^{2}=0}\\end{array}$ and the definition of $A_{j_{n}+1}$ in (26), hence we obtain $\\begin{array}{r}{A_{j_{n}+1}\\leq\\frac{1}{\\sigma_{j_{n}}}\\big(\\|\\mathbf{y}_{j_{n}}-\\mathbf{y}_{j_{n}+1}\\|\\|\\mathbf{y}-\\mathbf{y}_{j_{n}+1}\\|\\big)\\overset{n\\to\\infty}{\\longrightarrow}0.}\\end{array}$ Combining above, $\\operatorname*{lim}_{n\\to\\infty}\\mathbf{y}_{j_{n}}=\\mathbf{y}^{*}$ and (24), we have $\\begin{array}{r}{\\bar{0}^{\\leftarrow}\\!<-\\langle G(\\mathbf{x}^{*}),\\mathbf{y}^{*}\\rangle\\leq-\\langle G(\\mathbf{x}^{*}),\\mathbf{y}\\rangle,}\\end{array}$ , $\\forall\\mathbf{y}\\in{\\mathcal{Y}}$ . Moreover, due to the complementary slackness, there exists an $\\hat{\\mathbf{y}}^{\\ast}\\,\\in\\,{\\mathcal{V}}^{\\ast}\\,\\subseteq\\,{\\mathcal{V}}$ such that $-\\langle G(\\mathbf{x}^{*}),\\hat{\\mathbf{y}}^{*}\\rangle\\,=\\,0$ . Hence, we must have $\\langle G(\\mathbf{x}^{*}),\\mathbf{y}^{*}\\rangle=0$ , which, together with (69), implies that $(\\mathbf{x}^{*},\\mathbf{y}^{*})$ is KKT point. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "G Proof details for sparsity identification ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our proof strategy of active-set identification in rAPDPro is similar to those in unconstrained optimization [24]. Namely, we show that the optimal sparsity pattern is identified when the iterates fall in a properly defined neighborhood dependent on $\\eta$ . The next lemma shows that the primal and dual sequences indeed converge to the neighborhood of the optimal primal and dual solutions, respectively, in a finite number of iterations. ", "page_idx": 22}, {"type": "text", "text": "Lemma 5. There exists an $\\hat{S}_{1}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{0}^{s}-\\mathbf{x}^{*}\\|\\leq\\|\\mathbf{x}_{0}^{\\hat{S}_{1}}-\\mathbf{x}^{*}\\|\\ a n d\\ \\|\\mathbf{y}_{0}^{s}-\\mathbf{y}^{*}\\|\\leq\\|\\mathbf{y}_{0}^{\\hat{S}_{1}}-\\mathbf{y}^{*}\\|,\\forall s\\geq\\hat{S}_{1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(\\mathbf{x}^{*},\\mathbf{y}^{*})$ is the unique solution of problem (17). Moreover, there exists an epoch $\\hat{S}_{0}\\geq\\hat{S}_{1}$ such that $\\forall s\\geq\\hat{S}_{0}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|{\\mathbf{y}}_{k}^{s}-{\\mathbf{y}}^{*}\\|\\leq\\frac{\\eta}{3\\|\\nabla g({\\mathbf{x}}^{*})\\|},\\,\\|{\\mathbf{x}}_{k}^{s}-{\\mathbf{x}}^{*}\\|\\leq\\frac{\\eta}{3L_{X Y}}\\frac{\\tau_{k}^{s}}{\\tau_{k}^{s}+(2L_{X Y})^{-1}},\\,\\forall k=0,1,\\ldots N_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. From Theorem 2 and 3, we have $\\begin{array}{r}{\\operatorname*{lim}_{j\\to\\infty}(\\mathbf{x}_{j},\\mathbf{y}_{j})=(\\mathbf{x}^{*},\\mathbf{y}^{*})}\\end{array}$ , where $j$ corresponds to $(s,0)$ .   \nIt implies that there exists an epoch $\\hat{S}_{1}$ such that (70) holds. ", "page_idx": 22}, {"type": "text", "text": "It follows from (35) that $\\begin{array}{r}{\\|\\mathbf{x}_{1}^{s}-\\mathbf{x}^{*}\\|\\leq\\sqrt{\\sigma_{0}^{s}\\tau_{0}^{s}/\\sigma_{1}^{s}(\\|\\mathbf{x}_{0}^{s}-\\mathbf{x}^{*}\\|^{2}/\\tau_{0}^{s}+\\|\\mathbf{y}_{0}^{s}-\\mathbf{y}^{*}\\|^{2}/\\sigma_{0}^{s})}}\\end{array}$ . Hence, in order to prove $\\begin{array}{r}{\\|\\mathbf{x}_{1}^{s}-\\mathbf{x}^{*}\\|\\leq\\frac{\\eta}{3L_{X Y}}\\cdot\\frac{\\tau_{k}^{s}}{\\tau_{k}^{s}+(2L_{X Y})^{-1}}}\\end{array}$ , we need to prove ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\frac{\\sigma_{0}^{s}\\tau_{0}^{s}}{\\sigma_{1}^{s}}\\big(\\frac{1}{\\tau_{0}^{s}}\\big\\|\\mathbf{x}_{0}^{s}-\\mathbf{x}^{*}\\big\\|^{2}+\\frac{1}{\\sigma_{0}^{s}}\\big\\|\\mathbf{y}_{0}^{s}-\\mathbf{y}^{*}\\big\\|^{2}\\big)}\\leq\\frac{\\eta}{3L_{X Y}}\\frac{\\tau_{1}^{s}}{\\tau_{1}^{s}+(2L_{X Y})^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From Corollary 1 and Theorem 2, 3, we know that the left hand side of (72) converges to 0 and right hand side of (72) is a positive constant. Hence, there exist a $\\hat{S}_{2}$ such that (72) holds, which implies (71) holds for $k=1,s=\\hat{S}_{2}$ . Now we use induction to prove, for $\\forall k\\in[N_{\\hat{S}_{2}}]$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big(\\frac{\\sigma_{0}^{\\delta_{2}}\\tau_{k}^{\\delta_{2}}}{\\sigma_{k}^{\\delta_{2}}}\\big(\\frac{1}{\\tau_{0}^{\\delta_{2}}}\\|\\mathbf{x}_{0}^{\\hat{S}_{2}}-\\mathbf{x}^{*}\\|^{2}+\\frac{1}{\\sigma_{0}^{\\delta_{2}}}\\|\\mathbf{y}_{0}^{\\hat{S}_{2}}-\\mathbf{y}^{*}\\|^{2}\\big)\\big)^{1/2}\\leq\\frac{\\eta}{3L\\_{X Y}}\\frac{\\tau_{k}^{\\delta_{2}}}{\\tau_{k}^{\\delta_{2}}+(2L_{X Y})^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When $k=1$ , inequality (73) coincides with (72) with $s=\\hat{S}_{2}$ . Now, assume (73) holds for $k$ , we aim to prove (73) holds for $k+1$ . It follows from (35) that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{x}_{k+1}^{\\hat{S}_{2}}-\\mathbf{x}^{*}\\|\\overset{(a)}{\\leq}\\sqrt{\\frac{\\tau_{k+1}^{\\hat{S}_{2}}}{\\sigma_{k+1}^{\\hat{S}_{2}}}\\cdot\\frac{\\sigma_{k}^{\\hat{S}_{2}}}{\\tau_{k}^{\\hat{S}_{2}}}}\\cdot\\frac{\\eta}{3L\\_{X Y}}\\cdot\\frac{\\tau_{k}^{\\hat{S}_{2}}}{\\tau_{k}^{\\hat{S}_{2}}+(2L_{X Y})^{-1}}\\overset{(b)}{=}\\frac{\\eta}{3L_{X Y}}\\cdot\\frac{\\tau_{k+1}^{\\hat{S}_{2}}}{\\tau_{k}^{\\hat{S}_{2}}+(2L_{X Y})^{-1}}}\\\\ &{\\overset{(c)}{\\leq}\\frac{\\eta}{3L_{X Y}}\\cdot\\frac{\\tau_{k+1}^{\\hat{S}_{2}}}{\\tau_{k+1}^{\\hat{S}_{2}}+(2L_{X Y})^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(a)$ follows from induction, $(b)$ holds by $\\tau_{k}^{\\hat{S}_{2}}\\sigma_{k}^{\\hat{S}_{2}}=\\tau_{k+1}^{\\hat{S}_{2}}\\sigma_{k+1}^{\\hat{S}_{2}}$ and $(c)$ holds by $\\tau_{k+1}^{\\hat{S}_{2}}\\leq\\tau_{k}^{\\hat{S}_{2}}$ . Hence, we complete our proof of (73). From Theorem 2, we have $\\|\\mathbf{x}_{0}^{s}-\\mathbf{x}^{*}\\|^{2}\\leq D_{X}^{2}\\cdot2^{-s}$ , which implies that there exists a $\\begin{array}{r}{\\hat{S}_{3}=\\left\\lceil2\\log_{2}\\left\\{D_{X}\\big(\\frac{\\eta}{3L_{X Y}}\\cdot\\frac{\\bar{\\tau}}{\\bar{\\tau}+(2L_{X Y})^{-1}}\\big)^{-1}\\right\\}\\right\\rceil}\\end{array}$ such that $\\|\\mathbf{x}_{0}^{\\hat{S}_{3}}-\\mathbf{x}^{*}\\|\\leq$ $\\begin{array}{r}{D_{X}\\cdot\\sqrt{2}^{-\\hat{S}_{3}}\\leq\\frac{\\eta}{3L_{X Y}}\\frac{\\bar{\\tau}}{\\bar{\\tau}+(2L_{X Y})^{-1}}}\\end{array}$ , which implies that $\\begin{array}{r}{\\|{\\bf x}_{0}^{s}-{\\bf x}^{*}\\|\\le D_{X}^{2}\\cdot2^{-s}\\le\\frac{\\eta}{3L_{X Y}}\\frac{\\bar{\\tau}}{\\bar{\\tau}+(2L_{X Y})^{-1}}}\\end{array}$ holds for any $s\\geq\\hat{S}_{3}$ . ", "page_idx": 23}, {"type": "text", "text": "It follows from the definition of $\\hat{S}_{1}$ in (70) and stepsize will be reset at different epoch, then we have (72) holds for $s\\geq\\operatorname*{max}\\{\\hat{S}_{1},\\hat{S}_{2}\\}$ , which implies that (73) holds with substituting $\\hat{S}_{2}$ as any $s\\ \\geq\\ \\operatorname*{max}\\{\\hat{S}_{1},\\hat{S}_{2}\\}$ . Furthermore, it follows from Theorem 3 that $\\mathrm{lim}_{j\\to\\infty}\\,{\\bf y}_{j}\\;=\\;{\\bf y}^{*}$ , where $j$ corresponds to $(s,k)$ . Then there exists a ${\\hat{S}}_{4}$ such that the first term in (71) holds. Hence, we can obtain that there exist a $\\hat{S}_{0}=\\operatorname*{max}\\{\\hat{S}_{1},\\hat{S}_{2},\\hat{S}_{3},\\hat{S}_{4}\\}$ such that (71) holds. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "It is worth noting that the primal neighborhood defined by the second term of (71) is a bit different from the fixed neighborhood in the standard analysis [24], which involves a constant stepsize. As APDPro sets $\\tau_{k}^{s}\\,=\\,\\mathcal{O}(1/k)$ , both the point distance and neighborhood radius decay at the same ${\\mathcal{O}}(1/k)$ rate. Hence, we use a substantially different analysis to show the sparsity identification in the constrained setting. ", "page_idx": 23}, {"type": "text", "text": "G.1 Proof of Proposition 5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. The uniqueness of primal optimal solution $\\mathbf{x}^{*}$ follows from Proposition 2. The KKT condition (ensured by Slater\u2019s CQ) implies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{0}\\in\\partial f(\\mathbf{x}^{*})+\\nabla g(\\mathbf{x}^{*})\\mathbf{y}^{*}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "According to Assumption 2, we have $\\mathbf{x}^{*}\\neq\\mathbf{0}$ , hence $\\mathcal{A}^{c}(\\mathbf{x}^{*})=\\{1,2,\\dots,B\\}\\setminus\\mathcal{A}(\\mathbf{x}^{*})\\neq\\emptyset$ . In view of (74), for any $i\\in\\mathcal{A}^{c}(\\mathbf{x})$ , we have $p_{i}\\mathbf{x}_{(i)}^{*}/\\lVert\\mathbf{x}_{(i)}^{*}\\rVert=-\\dot{\\nabla}_{(i)}g(\\mathbf{x}^{*})\\mathbf{y}^{*}$ , which gives a unique $\\mathbf{y}^{*}$ . ", "page_idx": 23}, {"type": "text", "text": "G.2 Proof of Theorem 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. It follows from the Lipschitz smoothness of $g(\\cdot)$ and property (71) that for any $s\\geq\\hat{S}_{0}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big\\|\\left[\\nabla g(\\mathbf{x}_{k}^{s})\\mathbf{y}_{k+1}^{s}\\right]_{(i)}\\big\\|-\\big\\|\\left[\\nabla g(\\mathbf{x}^{*})\\mathbf{y}_{k+1}^{s}\\right]_{(i)}\\big\\|}\\\\ &{\\leq\\big\\|\\nabla g(\\mathbf{x}_{k}^{s})\\mathbf{y}_{k+1}^{s}-\\nabla g(\\mathbf{x}^{*})\\mathbf{y}_{k+1}^{s}\\big\\|}\\\\ &{\\leq L_{X Y}\\big\\|\\mathbf{x}_{k}^{s}-\\mathbf{x}^{*}\\big\\|\\leq\\frac{\\eta}{3}\\frac{\\tau_{k}^{s}}{\\tau_{k}^{s}+(2L_{X Y})^{-1}},\\,k=0,\\dots{N_{s}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recall that the primal update has the following form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{k+1}^{s}=\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathrm{argmin}}\\,\\Big\\{\\sum_{i=1}^{B}p_{i}\\|\\mathbf{x}_{(i)}\\|+\\big\\langle\\nabla g(\\mathbf{x}_{k}^{s})\\mathbf{y}_{k+1}^{s},\\mathbf{x}\\big\\rangle+\\frac{1}{2\\tau_{k}^{s}}\\|\\mathbf{x}-\\mathbf{x}_{k}^{s}\\|^{2}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\tau_{k}^{s}/(\\tau_{k}^{s}+(2L_{X Y})^{-1})$ is monotonically increasing with respect to $\\tau_{k}^{s}$ , for the strictly feasible point $\\tilde{\\bf x}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\mathbf{x}}_{k+1}^{s}-\\tilde{{\\mathbf{x}}}\\right\\|\\overset{(a)}{\\leq}\\frac{\\eta}{3L_{X Y}}\\cdot\\frac{\\bar{\\tau}}{\\bar{\\tau}+(2L_{X Y})^{-1}}+\\left\\|{\\mathbf{x}}^{*}-\\tilde{{\\mathbf{x}}}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\overset{(b)}{<}\\zeta+\\operatorname*{min}_{i\\in[m]}2\\sqrt{\\frac{-2g_{i}(\\mathbf{x}_{i}^{*})}{\\mu_{i}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(a)$ holds by (71), $\\bar{\\tau}\\ge\\tau_{k}^{s}$ and $(b)$ follows from the definition of ${\\bf x}^{*},\\tilde{\\bf x}$ and $\\zeta$ . Inequality (76) implies that $\\mathbf{x}_{k+1}^{s}\\in\\mathbf{int}\\,\\mathcal{X}$ , and hence $\\mathcal{N}_{\\mathcal{X}}(\\mathbf{x}_{k+1}^{s})=\\{\\mathbf{0}\\}$ . In view of the optimality condition, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\frac{1}{\\tau_{k}^{s}}(\\mathbf{x}_{k}^{s}-\\mathbf{x}_{k+1}^{s})-\\nabla g(\\mathbf{x}_{k}^{s})\\mathbf{y}_{k+1}^{s}\\right]_{(i)}\\in p_{i}\\partial\\|[\\mathbf{x}_{k+1}^{s}]_{(i)}\\|,\\,1\\le i\\le B.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Our next goal is to show $[\\mathbf{x}_{k+1}^{S}]_{(i)}=\\mathbf{x}_{(i)}^{*}$ satisfies condition (77) for $i\\in\\mathcal{A}(\\mathbf{x}^{*})$ . Placing $\\mathbf{x}_{(i)}=\\mathbf{x}_{(i)}^{*}$ in $\\begin{array}{r}{\\left\\|\\left[\\nabla g(\\mathbf{x}_{k}^{S})\\mathbf{y}_{k+1}^{S}+\\frac{1}{\\tau_{k}^{s}}(\\mathbf{x}-\\mathbf{x}_{k}^{s})\\right]_{(i)}\\right\\|}\\end{array}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\big[\\nabla g(\\mathbf{x}_{k}^{s})\\mathbf{y}_{k+1}^{s}+\\frac{1}{\\tau_{s}^{s}}(\\mathbf{x}^{*}-\\mathbf{x}_{k}^{s})\\big]_{(i)}\\right\\|}\\\\ &{\\leq\\big\\|\\left[\\nabla g(\\mathbf{x}_{k}^{s})\\mathbf{y}_{k+1}^{s}\\right]_{(i)}\\big\\|+\\big\\|\\frac{1}{\\tau_{s}^{s}}(\\mathbf{x}_{(i)}^{*}-\\mathbf{x}_{k(i)}^{s})\\big)\\big\\|}\\\\ &{\\stackrel{(a)}{\\leq}\\frac{\\eta}{3}\\frac{\\tau_{k}^{s}}{\\tau_{k}^{s}+(2L_{X Y})^{-1}}+\\big\\|\\left[\\nabla g(\\mathbf{x}^{*})\\mathbf{y}_{k+1}^{s}\\right]_{(i)}\\big\\|+\\frac{\\eta}{3}\\frac{(L_{X Y})^{-1}}{\\tau_{k}^{s}+(2L_{X Y})^{-1}}}\\\\ &{\\stackrel{(b)}{\\leq}\\frac{\\eta}{3}\\big[\\frac{\\tau_{k}^{s}+2(2L_{X Y})^{-1}}{\\tau_{s}^{s}+(2L_{X Y})^{-1}}+1\\big]+\\big\\|\\left[\\nabla g(\\mathbf{x}^{*})\\mathbf{y}^{*}\\right]_{(i)}\\big\\|}\\\\ &{<\\eta+\\big\\|\\left[\\nabla g(\\mathbf{x}^{*})\\mathbf{y}^{*}\\right]_{(i)}\\big\\|\\stackrel{(c)}{\\leq}p_{i},\\forall i\\in\\mathcal{A}(\\mathbf{x}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In above, $(a)$ follows from (71) and (75), $(b)$ follows from ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|[\\nabla g({\\mathbf x}^{*}){\\mathbf y}_{k+1}^{S}]_{(i)}\\|-\\|[\\nabla g({\\mathbf x}^{*}){\\mathbf y}^{*}]_{(i)}\\|\\le\\|{\\mathbf y}_{k+1}^{S}-{\\mathbf y}^{*}\\|\\|\\nabla g({\\mathbf x}^{*})\\|\\le\\frac{\\eta}{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and $(c)$ holds by the definition of $\\eta$ . Combining (77) and (78), we have $\\mathcal{A}(\\mathbf{x}^{*})\\subseteq\\mathcal{A}(\\mathbf{x}_{k+1}^{s}),s\\geq$ $\\hat{S}_{0},\\forall k\\in[N_{s}]$ , which completes our proof. \u53e3 ", "page_idx": 24}, {"type": "table", "img_path": "pG380vLYRU/tmp/56c04390aa11a748e6a58ed10c0a50f817fca1787ebfef2785823cff5b9fb96a.jpg", "table_caption": ["Table 1: Datasets description and parameter settings "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "H A multi-stage accelerated primal-dual algorithm ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Both the previous algorithms need to solve a complicated dual problem that involves a linear cut constraint, posing a potential issue: the associated sub-problem might lack a closed-form solution. To resolve this issue, we present the Multi-\u221aStage Accelerated Primal-Dual Algorithm (msAPD) in Algorithm 3, which obtains the same $\\mathcal{O}(1/\\sqrt{\\varepsilon})$ complexity without introducing a new cut constraint. Our new method is a double-loop procedure for which an accelerated primal-dual algorithm with a pending sub-iteration number (APDPi) is running in each stage. While both APDPi and APDPro employ the IMPROVE step to estimate the dual lower bound, APDPi only relies on the lower bound estimation to change the inner-loop iteration number adaptively, but not the stepsize selection. ", "page_idx": 24}, {"type": "text", "text": "We develop the convergence property of APDPi, which paves the path to proving our main theorem. For the convergence analysis, it suffices to verify that the initial stepsize parameter $\\tau_{0}^{s},\\sigma_{0}^{s}$ satisfy assumptions in Theorem 5. ", "page_idx": 24}, {"type": "text", "text": "Theorem 5. Let $\\{\\bar{\\mathbf{x}}_{k}^{s},\\bar{\\mathbf{y}}_{k}^{s}\\}$ be the sequence generated by APDPi, then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\bar{\\mathbf{x}}_{K}^{s},\\mathbf{y}^{*})-\\mathcal{L}(\\mathbf{x}^{*},\\bar{\\mathbf{y}}_{K}^{s})\\leq\\frac{1}{K}\\Delta^{s}(\\mathbf{x}^{*},\\mathbf{y}^{*}),\\ \\ \\frac{1}{2}\\||\\bar{\\mathbf{x}}_{K}^{s}-\\mathbf{x}^{*}||^{2}\\leq\\frac{1}{(\\mathbf{y}^{*})^{\\top}\\mu K}\\Delta^{s}(\\mathbf{x}^{*},\\mathbf{y}^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$\\begin{array}{r}{\\Delta^{s}(\\mathbf{x}^{*},\\mathbf{y}^{*})\\triangleq\\frac{1}{2\\tau_{0}^{s}}\\|\\mathbf{x}_{0}^{s}-\\mathbf{x}^{*}\\|^{2}+\\frac{1}{2\\sigma_{0}^{s}}\\|\\mathbf{y}_{0}^{s}-\\mathbf{y}^{*}\\|^{2}\\,a n d\\,(\\mathbf{x}^{*},\\mathbf{y}^{*})\\,i s\\,a\\,K K T p o i n t.}\\end{array}$ ", "text_format": "latex", "page_idx": 24}, {"type": "image", "img_path": "pG380vLYRU/tmp/7efc75c927f213284dd4a23f8f3fc70b1851ff0d57550ec5f807fc5cc11e2295.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 3: The first row is the results of objective convergence to optimum, where the $y$ -axis reports $\\log_{10}((\\|D^{1/2}\\mathbf{x}_{k}\\|_{1}\\,-\\,\\|D^{1/2}\\mathbf{x}^{*}\\|_{1})/\\|\\bar{D}^{1/2}\\mathbf{x}^{*}\\|_{1})$ for rAPDPro, and $\\log_{10}((\\|D^{1/2}\\bar{\\mathbf{x}}_{k}\\|_{1}\\ -$ $\\|D^{1/2}\\mathbf{x}^{*}\\|_{1})/\\|D^{1/2}\\mathbf{x}^{*}\\|_{1})$ for APD, msAPD and Mirror-Prox. The second row is the results of feasibility violation, where $y$ -axis reports the feasibility gap $\\log_{10}(\\operatorname*{max}\\{0,G(\\mathbf{x}_{k})\\})$ for rAPDPro, and $\\log_{10}(\\operatorname*{max}\\{0,G(\\bar{\\mathbf{x}}_{k})\\})$ for APD, APD+restart msAPD and Mirror-Prox. Datasets (Left-Right order) correspond to DD68, DD242 and peking-1. ", "page_idx": 25}, {"type": "text", "text": "Algorithm 3 Multi-Stage APD (msAPD) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Require: $\\bar{\\mathbf{x}}^{0}\\in\\mathcal{X},\\bar{\\mathbf{y}}^{0}\\in\\mathcal{Y},\\tilde{\\sigma},S$   \n1: Initialize: $\\rho_{0}^{0}=0$   \n2: for $s=0,\\ldots,S\\,\\bullet$ do   \n3: Compute \u03c4 0s = LXY + L2G\u03c3\u02dc \u00b7 22 \u22121, \u03c30s = \u03c3\u02dc \u00b7 22   \n4: $(\\bar{\\mathbf{x}}^{s+1},\\bar{\\mathbf{y}}^{s+1},\\rho_{0}^{s+1})\\gets\\mathbf{APDPI}(\\tau_{0}^{s},\\sigma_{0}^{s},\\bar{\\mathbf{x}}^{s},\\bar{\\mathbf{y}}^{s},\\rho_{0}^{s},s)$   \n5: end for   \n6: Output: \u00afxS+1, y\u00afS+1   \n7: procedure AP $\\mathrm{~\\mathsf~{~DPI}}(\\tau_{0}^{s},\\sigma_{0}^{s},\\mathbf{x}_{0},\\mathbf{y}_{0},\\rho_{0}^{s},s)$   \n8: Initialize: $\\begin{array}{r}{\\left({\\bf x}_{-1},{\\bf y}_{-1}\\right)\\leftarrow\\left({\\bf x}_{0},{\\bf y}_{0}\\right),\\bar{\\bf x}_{0}={\\bf x}_{0},k=0,N_{s}=\\infty,\\Delta_{X Y}=\\frac{1}{2\\tau_{0}^{s}}D_{X}^{2}+\\frac{1}{2\\sigma_{0}^{s}}D_{Y}^{2}}\\end{array}$   \n9: while $k<N_{s}$ do   \n11: 10: $\\begin{array}{r l}&{\\mathbf{z}_{k}\\leftarrow2G(\\mathbf{x}_{k})-G(\\mathbf{x}_{k-1})}\\\\ &{\\mathbf{y}_{k+1}\\leftarrow\\mathrm{argmin}_{\\mathbf{y}\\in\\mathcal{Y}}\\left\\|\\mathbf{y}-(\\mathbf{y}_{k}+\\sigma_{k}\\mathbf{z}_{k})\\right\\|^{2}}\\\\ &{\\mathbf{x}_{k+1}\\leftarrow\\mathrm{prox}_{f,\\mathcal{X}}(\\mathbf{x}_{k}-\\tau_{0}^{s}\\nabla G(\\mathbf{x}_{k})\\mathbf{y}_{k+1},\\tau_{0}^{s})}\\\\ &{\\bar{\\mathbf{x}}_{k+1}\\leftarrow(k\\bar{\\mathbf{x}}_{k}+\\mathbf{x}_{k+1})/(k+1),}\\\\ &{\\rho_{k+1}^{s}\\leftarrow\\mathrm{IMPROV}(\\mathbf{x}_{k},\\bar{\\mathbf{x}}_{k},\\frac{1}{2}D_{\\boldsymbol{X}}^{2},\\frac{\\Delta_{\\boldsymbol{X}}\\gamma}{k},\\rho_{k}^{s})}\\\\ &{\\mathrm{Compute}\\ N_{s}=\\lceil\\operatorname*{max}\\left\\{\\frac{4}{\\rho_{k+1}^{s}\\tau_{0}^{s}},\\frac{D_{\\boldsymbol{Y}}^{2}}{\\rho_{k+1}^{s}\\sigma_{0}^{s}D_{\\boldsymbol{X}}^{2}}\\cdot2^{-}\\right.}\\\\ &{\\left.k\\leftarrow k+1\\right.}\\end{array}$   \n12:   \n13:   \n14:   \n15:   \n16:   \n17: end while   \n18: return $\\bar{\\mathbf{x}}_{N_{s}},\\bar{\\mathbf{y}}_{N_{s}},\\rho_{k}^{s}$   \n19: end procedure ", "page_idx": 25}, {"type": "text", "text": "Proof. The stepsize $\\tau_{k}^{s}=\\tau_{0}^{s},\\sigma_{k}^{s}=\\sigma_{0}^{s}$ are unchanged at one epoch, which implies that $\\rho_{k+1}=0$ , i.e., (37) are satisfied. By the definition of $\\tau_{0}^{s}$ and $\\sigma_{0}^{s}$ , we have $(\\tau_{0}^{s})^{-1}\\,=\\,L_{X Y}\\,+\\,L_{G}^{2}\\tilde{\\sigma}\\sqrt{2}^{s}\\,=$ $L_{X Y}+L_{G}^{2}\\sigma_{0}^{s}$ , which means equality holds at the first term in (37). ", "page_idx": 25}, {"type": "text", "text": "Since $g_{i}(\\mathbf{x})$ is a strongly convex function with modulus $\\mu_{i}$ , then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y}^{*})\\geq\\mathcal{L}(\\mathbf{x}^{*},\\mathbf{y}^{*})+\\frac{(\\mathbf{y}^{*})^{\\top}\\mu}{2}\\|\\bar{\\mathbf{x}}_{K}-\\mathbf{x}^{*}\\|^{2},\\;\\;\\mathcal{L}(\\mathbf{x}^{*},\\mathbf{y}^{*})\\geq\\mathcal{L}(\\mathbf{x}^{*},\\bar{\\mathbf{y}}_{K}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Summing up the two inequalities above, we can get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\bar{\\mathbf{x}}_{K},\\mathbf{y}^{*})-\\mathcal{L}(\\mathbf{x}^{*},\\bar{\\mathbf{y}}_{K})\\geq\\frac{(\\mathbf{y}^{*})^{\\top}\\mu}{2}\\|\\bar{\\mathbf{x}}_{K}-{\\mathbf{x}}^{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We show msAPD obtains an $\\mathcal{O}(1/\\sqrt{\\varepsilon})$ convergence rate, which matches the complexity of APDPro. Theorem 6. Let $\\left\\{\\bar{\\bf x}_{0}^{s}\\right\\}$ be the sequence computed by msAPD. Then, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\bar{\\mathbf{x}}_{0}^{s}-\\mathbf{x}^{*}\\|^{2}\\leq\\Delta_{s}\\equiv D_{X}^{2}\\cdot2^{-s},\\quad\\forall s\\geq0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For any $\\varepsilon\\,\\in\\,(0,D_{X}^{2})$ , msAPD will find a solution $\\bar{\\mathbf{x}}_{0}^{s}\\in\\mathcal{X}$ such that $\\|\\bar{\\mathbf{x}}_{0}^{s}-\\mathbf{x}^{*}\\|^{2}\\leq\\varepsilon$ in at most $\\left\\lceil\\log_{2}D_{X}^{2}/\\varepsilon\\right\\rceil$ epochs. Moreover, the overall iteration number performed by msAPD to find such $a$ solution is bounded by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\varepsilon}=\\left(\\frac{8L_{X Y}}{\\rho_{N_{0}}^{0}}+2\\right)\\left\\lceil\\log_{2}\\frac{D_{X}}{\\sqrt{\\varepsilon}}+1\\right\\rceil+(2+\\sqrt{2})\\left(\\tilde{\\sigma}L_{G}^{2}+\\frac{2D_{Y}^{2}}{\\rho_{N_{0}}^{0}\\tilde{\\sigma}D_{X}^{2}}\\right)\\frac{D_{X}}{\\sqrt{\\varepsilon}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We first show that (81) holds by induction. It is easy to verify that (81) holds for $s\\,=\\,0$ . Assume $\\|\\bar{\\mathbf{x}}_{0}^{s}-\\mathbf{x}^{*}\\|^{2}\\leq\\Delta_{s}=D_{X}^{2}\\cdot2^{-s}$ holds for $s=0,\\ldots,S-1$ . By Theorem 5, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\bar{\\mathbf{x}}_{0}^{S}-\\mathbf{x}^{*}\\|^{2}\\leq\\frac{1}{(\\mathbf{y}^{*})^{\\top}\\mu N_{S-1}}\\big(\\frac{2}{\\tau_{0}^{S-1}}\\Delta_{S}+\\frac{1}{\\sigma_{0}^{S-1}}D_{Y}^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As the algorithm sets $N_{S-1}=\\left\\lceil\\operatorname*{max}\\left\\{4/(\\rho_{N_{S-1}}^{S-1}\\tau_{0}^{S-1}),2D_{Y}^{2}/(\\rho_{N_{S-1}}^{S-1}\\sigma_{0}^{S-1}\\Delta_{S})\\right\\}\\right\\rceil$ , the following ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad2\\big(({\\mathbf{y}^{*}})^{\\top}\\mu N_{S-1}\\tau_{0}^{S-1}\\big)^{-1}\\leq2\\big(\\rho_{N_{S-1}}^{S-1}N_{S-1}\\tau_{0}^{S-1}\\big)^{-1}\\leq\\frac{1}{2},}\\\\ &{D_{Y}^{2}\\big(({\\mathbf{y}^{*}})^{\\top}\\mu N_{S-1}\\sigma_{0}^{S-1}\\big)^{-1}\\leq D_{Y}^{2}\\big(\\rho_{N_{S-1}}^{S-1}N_{S-1}\\sigma_{0}^{S-1}\\big)^{-1}\\leq\\frac{1}{2}\\Delta_{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Putting these pieces together, we have $\\begin{array}{r}{\\|\\bar{\\mathbf{x}}_{S}-\\mathbf{x}^{*}\\|^{2}\\leq\\frac12\\Delta_{S}+\\frac12\\Delta_{S}=\\Delta_{S}}\\end{array}$ . Suppose the algorithm runs for $S$ epochs to achieve the desired accuracy $\\varepsilon$ , i.e., $\\|\\mathbf{x}_{0}^{S}-\\mathbf{x}^{*}\\|^{2}\\leq D_{X}^{2}\\cdot2^{-S}\\leq\\varepsilon$ . Then the overall iteration number can be bounded by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{s=0}^{S}N_{s}\\overset{(a)}{\\leq}\\sum_{s=0}^{S}\\Big\\{\\frac{4}{\\rho_{N_{0}}^{0}\\tau_{0}^{S-1}}+\\frac{2D_{Y}^{2}}{\\rho_{N_{0}}^{0}\\sigma_{0}^{S-1}\\Delta_{S}}+1\\Big\\}}\\\\ &{\\qquad\\qquad\\quad\\overset{(b)}{\\leq}\\sum_{s=0}^{S}\\Big\\{\\Big(\\frac{4L X Y}{\\rho_{N_{0}}^{0}}+1\\Big)+\\Big(\\tilde{\\sigma}L_{G}^{2}+\\frac{2D_{Y}^{2}}{\\rho_{N_{0}}^{0}\\tilde{\\sigma}D_{X}^{2}}\\Big)\\sqrt{2}^{s}\\Big\\}}\\\\ &{\\qquad\\qquad\\quad\\leq\\Big(\\frac{8L x Y}{\\rho_{N_{0}}^{0}}+2\\Big)\\Big[\\log_{2}\\frac{D x}{\\sqrt{\\varepsilon}}+1\\Big]+(2+\\sqrt{2})\\Big(\\tilde{\\sigma}L_{G}^{2}+\\frac{2D_{Y}^{2}}{\\rho_{N_{0}}^{0}\\tilde{\\sigma}D_{X}^{2}}\\Big)\\frac{D_{X}}{\\sqrt{\\varepsilon}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $(a)$ holds by $\\rho_{N_{S}}^{s}\\geq\\rho_{N_{0}}^{0},\\forall s\\geq0,\\,($ (b) follows from the definition of $\\tau_{0}^{s}$ and $\\sigma_{0}^{s}$ . ", "page_idx": 26}, {"type": "text", "text": "Remark 11. Theorem $6$ shows that msAPD obtains a worst-case complexity of ${\\mathcal O}(\\log(D_{X}/\\sqrt{\\varepsilon})+$ $(D_{X}+D_{Y}^{2}/D_{X})/\\sqrt{\\varepsilon})$ , which is an upper bound of the complexity of rAPDPro (see Theorem 2). The complexities of msAPD and rAPDPro match when $D_{X}=\\Omega(1)D_{Y}$ . Otherwise, rAPDPro appears to be much better in terms of dependence on $D_{X}/\\sqrt{\\varepsilon}$ . On the other hand, msAPD has a simpler subproblem, which does not involve an additional cut constraint on the dual update. ", "page_idx": 26}, {"type": "text", "text": "I Experiment details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We examine the empirical performance for solving sparse Personalized PageRank. Let $G=(V,E)$ be a connected undirected graph with $n$ vertices. Denote the adjacency matrix of $G$ by $A$ , that is, $A_{i,j}=1$ if $i\\sim j$ and 0 otherwise. Let $D=\\mathrm{diag}(d_{1},\\ldots,d_{n})$ be the matrix with the degrees $\\{d_{i}\\}_{i=1}^{n}$ in its diagonal. Then the constrained form of Personalized PageRank can be written as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}~\\|D^{1/2}\\mathbf{x}\\|_{1}~\\mathrm{{s.t.}}~\\frac{1}{2}\\left\\langle\\mathbf{x},Q\\mathbf{x}\\right\\rangle-\\alpha\\langle\\mathbf{s},D^{-1/2}\\mathbf{x}\\rangle\\leq b,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\begin{array}{r}{Q=D^{-1/2}\\big(D-\\frac{1-\\alpha}{2}(D+A)\\big)D^{-1/2}}\\end{array}$ , $\\alpha\\in(0,1)$ , $\\mathbf{s}\\in{\\Delta^{n}}$ is a teleportation distribution over the nodes of the graph $G$ and $b$ is a pre-specific target level. ", "page_idx": 26}, {"type": "image", "img_path": "pG380vLYRU/tmp/91507e091fcd12383f4a106bcae25d75dea89d046ced55ca2649ed290085b54a.jpg", "img_caption": ["Figure 4: The experimental results on active-set identification. Datasets (Left-Right order) correspond to DD68, DD242 and peking-1. The $x$ -axis reports the iteration number and the $y$ -axis reports accuracy in active-set identification. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Datasets We selected 6 small-to-median scale datasets from various domains in the Network Datasets [28]. We skip large-scale networks as MOSEK struggles to achieve the optimal solution, making it unsuitable for subsequent comparison of the optimality gap. We briefly describe these datasets in Table 1. For more details, please refer to the network repository. ", "page_idx": 27}, {"type": "text", "text": "Parameter tuning For all experiments, we set $r\\,=\\,\\mathrm{min}_{i\\in[n]}\\,|d_{i}|$ , $\\underline{{\\mu}}\\;=\\;\\lambda_{\\operatorname*{min}}(Q)$ and $L_{X}~=$ $\\lambda_{\\operatorname*{max}}(Q)$ , with $\\lambda_{\\operatorname*{min}}(\\cdot),\\lambda_{\\operatorname*{max}}(\\cdot)$ denoting the smallest and largest eigenvalue, respectively. For msAPD, we have made additional parameter adjustments. Based on our observations, due to a small estimated strongly convex coefficient, msAPD could not switch to the next cycle $s$ early enough. To prevent msAPD from degrading to APD, we iterate according to the predef\u221ained number of sub\u221a-iterations and manually switch to the next set of parameters. We divide $\\tau$ by $\\sqrt{2}$ , multiply $\\sigma$ by $\\sqrt{2}$ , and increase the number of sub-iterations in the next period by a factor of $\\sqrt{2}$ . For all experiments, we tune the stepsize $\\tau,\\sigma,\\gamma$ from $\\left\\{0.0001,0.0005,\\bar{0.001},0.\\dot{005},0.01\\right\\}$ , where $\\tau,\\sigma$ are the initial stepsizes of rAPDPro, msAPD and APD, $\\gamma$ is the constant stepsize of Mirror-Prox. All algorithms start with the primal variables initialized as zero vectors and the dual variables initialized as ones. ", "page_idx": 27}, {"type": "text", "text": "Additional experiment results Figure 3 and Figure 4 describe the convergence performance and active set identification results on the last three datasets: DD68, DD242 and peking-1. Furthermore, we report the time consumption for the Personalized PageRank problem in Table 2. The table indicates that, although rAPDPro and msAPD require moderately complex computations to determine the lower bound of the strong convexity parameter, the two methods still accelerate the algorithm\u2019s convergence and can significantly reduce the overall convergence time. ", "page_idx": 27}, {"type": "table", "img_path": "pG380vLYRU/tmp/8e7eeced4fb43fe4a3c653800359d4ffb49b91efda1a2217ff0c812da34ade64.jpg", "table_caption": ["Table 2: Time summary when $\\operatorname*{max}\\{|f(\\mathbf{x})-f(\\mathbf{x}^{*})|/|f(\\mathbf{x}^{*})|,\\operatorname*{max}\\{G(\\mathbf{x}),0\\}\\}\\leq10^{-3}$ . All experiments were conducted five times, and the results are reported as mean (standard deviation). $^*$ means that upon completion of all iterations, the algorithms still fails to meet the criteria for both error measures. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Nonetheless, we observe that Mosek achieves significantly faster computational efficiency for small-scale problems than our algorithm. Therefore, we test the efficiency of rAPDPro on some large-scale instances. For large-scale instances, we consider the following problem $\\operatorname{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\|\\mathbf{x}-$ $1\\|_{1}$ s.t. $\\begin{array}{r}{\\cdot\\frac{1}{2}\\mathbf{x}^{\\top}Q_{i}\\mathbf{x}+c_{i}^{\\top}\\mathbf{x}+d_{i}\\leq0,i=1,\\ldots,m}\\end{array}$ , where $Q_{i}$ are dense and positive definite matrix and generated randomly and $c_{i}$ are generated randomly. Furthermore, we set proper $d_{i}$ to make the feasible region is non-empty. When $n=5000$ and $m>10$ , MOSEK crashes on our computer, which means we can not get $\\mathbf{x}^{*}$ for calculating the optimality gap. Therefore, we report the time required for the algorithm to satisfy $\\operatorname*{max}\\{|f(\\mathbf{x})-f(\\mathbf{x}^{*})|/|f(\\mathbf{x}^{*})|,\\operatorname*{max}\\{G(\\mathbf{x}),0\\}\\}\\,\\le\\,10^{-3}$ and the time taken by the algorithm to complete 10,000 iterations. On this problem, results from small datasets indicate that the performance of the 10,000-step algorithm should be sufficient to meet our specified termination criteria. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Table 3: Comparison of computational time in seconds between rAPDPro and MOSEK ", "page_idx": 28}, {"type": "table", "img_path": "pG380vLYRU/tmp/7aa7d286347951988b7d60afe269e6d401ce916b0badb4ea3e0598c2d9863a2f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We state the complete contributions in the Introduction section. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Appendix A ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We give all assumptions needed for the theorems we are proving, such as Assumption 1, 2, 3 and 4, to ensure the conclusion is correct. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our experimental reproduction scripts have been placed in the attachment. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 30}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our experiments use entirely publicly available datasets, and we are committed to making our code completely open source. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All details can be found in the paper and supplemental material. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Since our algorithm is deterministic, our experimental results do not report standard deviation correlation results, but we have experimented on a wide range of datasets to demonstrate the robustness of our algorithm. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All experiments are run on Mac mini M2 Pro, 32GB. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The code in submission is fully compliant with the NeurIPS code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The creators or original owners of assets are properly credited, and the license and terms of use are explicitly mentioned and respected in the paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide a complete document of our code. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]