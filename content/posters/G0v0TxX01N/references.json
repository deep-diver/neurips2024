{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This paper is foundational for demonstrating the capabilities of large language models (LLMs) and their ability to perform complex reasoning tasks with few-shot learning, which is a key aspect discussed in the current research."}, {"fullname_first_author": "Jacob Austin", "paper_title": "Structured denoising diffusion models in discrete state-spaces", "publication_date": "2021-12-06", "reason": "This paper introduces a novel approach to diffusion models, applying them to discrete spaces rather than continuous ones, which is highly relevant to the current research using discrete diffusion models for text generation."}, {"fullname_first_author": "Shansan Gong", "paper_title": "DiffuSeq: Sequence to sequence text generation with diffusion models", "publication_date": "2023-00-00", "reason": "This paper presents DiffuSeq, a significant foundation for the current work.  It adapts diffusion models for conditional language generation, directly influencing the proposed Diffusion-of-Thought method."}, {"fullname_first_author": "Ishaan Gulrajani", "paper_title": "Likelihood-based diffusion language models", "publication_date": "2023-05-18", "reason": "This paper introduces Plaid, a large-scale diffusion language model which serves as a crucial base model for the proposed technique in the current research."}, {"fullname_first_author": "Aaron Lou", "paper_title": "Discrete diffusion language modeling by estimating the ratios of the data distribution", "publication_date": "2023-10-16", "reason": "This paper introduces SEDD, another significant diffusion language model used as a baseline model and compared against in the current research."}]}