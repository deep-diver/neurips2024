[{"type": "text", "text": "Score matching through the roof: linear, nonlinear, and latent variables causal discovery ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Causal discovery from observational data holds great promise, but existing methods   \n2 rely on strong assumptions about the underlying causal structure, often requiring   \n3 full observability of all relevant variables. We tackle these challenges by leveraging   \n4 the score function $\\nabla\\log p(X)$ of observed variables for causal discovery and   \n5 propose the following contributions. First, we generalize the existing results of   \n6 identifiability with the score to additive noise models with minimal requirements   \n7 on the causal mechanisms. Second, we establish conditions for inferring causal   \n8 relations from the score even in the presence of hidden variables; this result is   \n9 two-faced: we demonstrate the score\u2019s potential as an alternative to conditional   \n10 independence tests to infer the equivalence class of causal graphs with hidden   \n11 variables, and we provide the necessary conditions for identifying direct causes in   \n12 latent variable models. Building on these insights, we propose a flexible algorithm   \n13 for causal discovery across linear, nonlinear, and latent variable models, which we   \n14 empirically validate. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 The inference of causal effects from observations holds the potential for great impact arguably in any   \n17 domain of science, where it is crucial to be able to answer interventional and counterfactual queries   \n18 from observational data [1, 2, 3]. Existing causal discovery methods can be categorized based on   \n19 the information they can extract from the data [4], and the assumptions they rely on. Traditional   \n20 causal discovery methods (e.g. PC, GES [5, 6]) are general in their applicability but limited to the   \n21 inference of an equivalence class. Additional assumptions on the structural equations generating   \n22 effects from the cause are, in fact, imposed to ensure the identifiability of a causal order [7, 8, 9, 10].   \n23 As a consequence, existing methods for causal discovery require specialized and often untestable   \n24 assumptions, preventing their application to real-world scenarios.   \n25 Further, the majority of existing approaches are hindered by the assumption that all relevant causes   \n26 of the measured data are observed, which is necessary to interpret associations in the data as causal   \n27 relationships. Despite the convenience of this hypothesis, it is often not met in practice, and the solu  \n28 tions relaxing this requirement face substantial limitations. The FCI algorithm [11] can only return an   \n29 equivalence class from the data. Appealing to additional restrictions ensures the identifiability of some   \n30 direct causal effects in the presence of latent variables: RCD [12] relies on the linear non-Gaussian   \n31 additive noise model, whereas CAM-UV [13] requires nonlinear additive mechanisms. Nevertheless,   \n32 the strict conditions on the structural equations hold back their applicability to more general settings.   \n33 Our paper tackles these challenges and can be put in the context of a recent line of work that   \n34 derives a connection between the score function $\\operatorname{\\nabla}\\log p(X)$ and the causal graph underlying the   \n35 data-generating process [14, 15, 16, 17, 18, 19]. The use of the score for causal discovery is   \n36 practically appealing, as it yields advantages in terms of scalability to high dimensional graphs [16]   \n37 and guarantees of finite sample complexity bounds [20]. Instead of imposing assumptions that ensure   \n38 strong, though often impractical, theoretical guarantees, we organically demonstrate different levels of   \n39 identifiability based on the strength of the modeling hypotheses, always relying on the score function   \n40 to encode all the causal information in the data. Starting from results of Spantini et al. [21] and Lin   \n41 [22], we show how constraints on the Jacobian of the score $\\nabla^{2}\\log{p(X)}$ can be used as an alternative   \n42 to conditional independence testing to identify the Markov equivalence class of causal models with   \n43 hidden variables. Further, we prove that the score function identifies the causal direction of additive   \n44 noise models, with minimal assumptions on the causal mechanisms. This extends the previous findings   \n45 of Montagna et al. [17], limited by the assumption of nonlinearity of the causal effects, and Ghoshal   \n46 and Honorio [14], limited to linear mechanisms. On these results, we build the main contributions   \n47 of our work, enabling the identification of direct causal effects in hidden variables models.   \n48 Our main contributions are as follows: $(i)$ We present the necessary conditions for the identifiability   \n49 of direct causal effects and the presence of hidden variables with the score in the case of latent   \n50 variables models. (ii) We propose AdaScore (Adaptive Score-based causal discovery), a flexible   \n51 algorithm for causal discovery based on score matching estimation of $\\nabla\\log{p(X)}$ [23]. Based on the   \n52 user\u2019s belief about the plausibility of several modeling assumptions on the data, AdaScore can output   \n53 a Markov equivalence class, a directed acyclic graph, or a mixed graph, accounting for the presence   \n54 of unobserved variables. To the best of our knowledge, the broad class of causal models handled by   \n55 our method is unmatched by other approaches in the literature. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "56 2 Model definition and related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "57 In this section, we introduce the formalism of structural causal models (SCMs), separately for the the   \n58 cases with and without hidden variables. ", "page_idx": 1}, {"type": "text", "text": "59 2.1 Causal model with observed variables ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "60 Let $X$ be a set of random variables in $\\mathbb{R}$ defined according to the set of structural equations ", "page_idx": 1}, {"type": "equation", "text": "$$\nX_{i}:=f_{i}(X_{\\mathrm{PA}_{i}^{g}},N_{i}),\\enspace\\forall i=1,\\ldots,k.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "61 $N_{i}\\in\\mathbb{R}$ are mutually independent random variables with strictly positive density, known as noise   \n62 or error terms. The function $f_{i}$ is the causal mechanism mapping the set of direct causes $X_{\\mathrm{PA}_{i}^{\\mathcal{G}}}$   \n63 of $X_{i}$ and the noise term $N_{i}$ , to $X_{i}$ \u2019s value. A structural causal model (SCM) is defined as the   \n64 tuple $(X,N,\\mathcal{F},\\mathbb{P}_{N})$ , where $\\mathcal{F}\\,=\\,(f_{i})_{i=1}^{k}$ is the set of causal mechanisms, and $\\mathbb{P}_{N}$ is the joint   \n65 distribution relative to the density $p_{N}$ over the noise terms $N\\in\\mathbb{R}^{k}$ . We define the causal graph $\\mathcal{G}$   \n66 as a directed acyclic graph (DAG) with nodes $X=\\{X_{1},\\ldots,X_{k}\\}$ , and the set of edges defined as   \n67 $\\{X_{j}\\rightarrow X_{i}:X_{j}\\in X_{\\mathrm{PA}_{i}^{\\mathcal{G}}}\\}$ , such that $\\mathrm{PA}_{i}^{\\mathcal{G}}$ are the indices of the parent nodes of $X_{i}$ in the graph   \n68 $\\mathcal{G}$ . (In the remainder of the paper, we adopt the following notation: given a set of random variables   \n69 $Y=\\{Y_{1},\\ldots,Y_{n}\\}$ and a set of indices $Z\\subset\\mathbb{N}$ , then $Y_{Z}\\bar{=}\\{Y_{i}|i\\in\\bar{Z},Y_{i}\\in Y\\}$ .)   \n70 Under this model, the probability density of $X$ satisfies the Markov factorization (e.g. Peters et al.   \n71 [1] Proposition 6.31): ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\np(x)=\\prod_{i=1}^{k}p(x_{i}|x_{\\mathrm{PA}_{i}^{\\mathcal{G}}}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where we adopt the convention of lowercase letters referring to realized random variables, and use $p$ to denote the density of different random objects, when the distinction is clear from the argument. This factorization is equivalent to the global Markov condition (e.g. Peters et al. [1] Proposition 6.22) that demands that for all $\\{X_{i},X_{j}\\}\\in{\\bar{X}},X_{Z}\\subseteq X\\setminus\\{X_{i},X_{j}\\}$ , then ", "page_idx": 1}, {"type": "equation", "text": "$$\nX_{i}\\vartriperp_{\\boldsymbol{\\mathcal{G}}}^{d}X_{j}|X_{\\boldsymbol{Z}}\\implies X_{i}\\vartriangle X_{j}|X_{\\boldsymbol{Z}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "72 where $\\left(\\cdot\\,\\underline{{\\u}}\\perp\\quad\\cdot\\,\\vert\\cdot\\right)$ denotes probabilistic conditional independence of $X_{i},X_{j}$ given $X_{Z}$ , and $\\big(\\cdot\\frac{\\mid\\mid}{\\mathscr{G}}\\cdot\\mid\\cdot\\big)$   \n73 is the notation for $^d$ -separation, a criterion of conditional independence defined on the graph $\\mathcal{G}$   \n74 (Definition 5 of the appendix). As it is commonly done, we assume that the reverse direction   \n75 $X_{i}\\perp X_{j}|X_{Z}\\;\\Longrightarrow\\;\\hat{X_{i}}\\perp\\perp\\frac{d}{g}\\dot{X_{j}}|X_{Z}$ hold, and we say that the density $p$ is faithful to the graph $\\mathcal{G}$   \n76 [2, 24] (hence the faithfulness assumption). Together with the global Markov condition, faithfulness   \n77 implies an equivalence between the probabilistic and graphical notions of conditional independence: ", "page_idx": 1}, {"type": "equation", "text": "$$\nX_{i}\\perp X_{j}|X_{Z}\\Longleftrightarrow X_{i}\\perp\\perp_{\\mathcal{G}}^{d}X_{j}|X_{Z}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "78 In general, several DAGs may entail the same set of d-separations: graphs sharing such common   \n79 structure form a Markov equivalence class (see Definition 6 in the appendix).   \n80 The above model assumes that there aren\u2019t any unobserved causes of variables in $X$ , other than the   \n81 noise terms in $N$ . As we are interested in distributions with potential hidden variables, we will now   \n82 generalize our model to represent data-generating processes that may involve latent causes.   \n83 Definitions on graphs. As graphs play a central role in our work, Appendix A.1 provides a   \n84 detailed overview of the fundamental notation and definitions that we rely on in the remainder of   \n85 the paper. For the next section, we advise the reader to be comfortable with the notions of ancestors   \n86 (Definition 2) and inducing paths (Definition 3) in DAGs.   \n87 Closely related works. Several methods for the causal discovery of fully observable models using   \n88 the score have been recently proposed. Ghoshal and Honorio [14] demonstrates the identifiability of   \n89 the linear non-Gaussian model from the score, and it is complemented by Rolland et al. [15], which   \n90 shows the connection between score matching estimation of ${\\nabla\\log{p(X)}}$ and the inference of causal   \n91 graphs underlying nonlinear additive noise models with Gaussian noise terms, also allowing for   \n92 sample complexity bounds [20]. Montagna et al. [17] provides identifiability results in the nonlinear   \n93 setting, without posing any restriction on the distribution of the noise terms. Montagna et al. [16]   \n94 is the first to show that the Jacobian of the score provides information equivalent to conditional   \n95 independence testing in the context of causal discovery, limited to the case of additive noise models.   \n96 All of these studies make specialized assumptions to find theoretical guarantees of identifiability,   \n97 whereas our paper provides a unifying view of causal discovery with the score function, which   \n98 generalizes and expands the existing results. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "99 2.2 Causal model with unobserved variables ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "100 Under the model (1), we consider the case where the set of variables $X$ is partitioned into the disjoint   \n101 subsets of observed random variables $V=\\{V_{1},\\ldots,V_{d}\\}$ and unobserved (or latent) random variables   \n102 $U=\\{U_{1},\\ldots,U_{p}\\}$ . We assume that the following set of structural equations is satisfied: ", "page_idx": 2}, {"type": "equation", "text": "$$\nV_{i}:=f_{i}(V_{\\mathrm{PA}_{i}^{\\mathcal{G}}},U^{i},N_{i}),\\,\\,\\,\\forall i=1,\\dots,d,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "103 where $U^{i}$ stands for the set of unobserved parents of $V_{i}$ , and $V_{\\mathrm{PA}_{i}^{g}}=\\{V_{k}|k\\in\\mathrm{PA}_{i}^{g},V_{k}\\in V\\}$ are   \n104 the observed direct causes of $V_{i}$ . Some of the causal relations and the conditional independencies   \n105 implied by the set of equations (4) can be summarized in a graph obtained as a marginalization of the   \n106 DAG $\\mathcal{G}$ onto the observable nodes $V$ .   \n107 Definition 1 (Marginal graph, Zhang [25]). Let $X=V\\dot{\\cup}U$ and $\\mathcal{G}$ be a DAG over $X$ . The following   \n108 construction gives the marginal graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ , with nodes $V$ and edges found as follows:   \n109 \u2022 pair of nodes $V_{i},V_{j}$ are adjacent in the graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ if and only if there is an inducing path   \n110 between them relative to $U$ in $\\mathcal{G}$ ;   \n11 \u2022 for each pair of adjacent nodes $V_{i},V_{j}$ in $\\mathcal{M}_{V}^{\\mathcal{G}}$ , orient the edge as $V_{i}\\to V_{j}$ if $V_{i}$ is an ancestor   \n12 of $V_{j}$ in $\\mathcal{G}$ , else orient it as $V_{i}\\leftrightarrow V_{j}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "113 We define the map ${\\mathcal{G}}\\mapsto{\\mathcal{M}}_{V}^{\\mathcal{G}}$ as the marginalization of the DAG $\\mathcal{G}$ onto $V$ , the observable nodes. ", "page_idx": 2}, {"type": "text", "text": "114 The graph resulting from the above construction is a maximal ancestral graph (MAG, Definition 4),   \n115 hence we will often refer to it as the marginal $M A G$ of $\\mathcal{G}$ . Intuitively, a directed edge denotes the   \n116 presence of an ancestorship relation, whereas bidirected edges represent dependencies that can not be   \n117 removed by conditioning on any of the variables in the graph.   \n118 In the case of DAGs, d-separation encodes the probabilistic conditional independence relations   \n119 between the variables of $X$ in the graph $\\mathcal{G}$ , as explicit by Equation (3). Such notion of graphical sepa  \n120 ration has a natural generalization to maximal ancestral graphs, known as $m$ -separation (Definition 5   \n121 of the appendix). Zhang [25] shows that $m$ -separation and $^d$ -separation are in fact equivalent (see   \n122 Lemma 1 of the appendix), such that given $V_{Z}\\subset V$ and $\\{V_{i},V_{j}\\}\\subset V$ , the following holds: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nV_{i}\\,\\mathbb{\\perp}\\,\\mathbb{\\perp}\\,\\mathbb{\\ell}_{j}\\,|V_{Z}\\setminus\\{V_{i},V_{j}\\}\\iff V_{i}\\,\\mathbb{\\perp}\\,\\mathbb{\\perp}_{\\mathcal{M}_{V}^{\\mathcal{G}}}^{m}\\,V_{j}|V_{Z}\\setminus\\{V_{i},V_{j}\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123 where $\\big(\\cdot\\perp_{\\mathcal{M}_{V}^{\\mathcal{G}}}\\cdot\\mid\\cdot\\big)$ denotes $m$ -separation relative to the graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ . Just like with DAGs, MAGs   \n124 that imply the same set of conditional independencies define an equivalence class. Usually, the   \n125 common structure of these graphs is represented by partial ancestral graphs (PAGs, Definition 7 of   \n126 the appendix). We use $\\mathcal{P}_{\\mathcal{M}_{V}^{\\mathcal{G}}}$ to denote the PAG relative to $\\mathcal{M}_{V}^{\\mathcal{G}}$ . ", "page_idx": 3}, {"type": "text", "text": "Problem definition. In this work, our goal is to provide theoretical guarantees for the identifiability of the Markov equivalence class of the marginal graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ and its direct causal effects with the score, where variables $V_{i}$ are defined according to Equation (4). ", "page_idx": 3}, {"type": "text", "text": "128 Without further assumptions on the data-generating process, we can identify the graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ only up   \n129 to its partial ancestral graph, as discussed in the next section.   \n130 Closely related works. Causal discovery with latent variables have been first studied in the context   \n131 of constraint-based approaches with the FCI algorithm [11], which shows the identifiability of the   \n132 equivalence class of a marginalized graph via conditional independence testing. The RCD and   \n133 CAM-UV [12, 13] approaches instead demonstrate the inferrability of directed causal edges via   \n134 regression and residuals independence testing. Both methods rely on strong assumptions on the   \n135 causal mechanisms: their theoretical guarantees apply to models where the effects are generated by a   \n136 linear (RCD) or nonlinear (CAM-UV) additive contribution of each cause. Our work demonstrates   \n137 that using the score function for causal discovery unifies and generalizes these results, presenting   \n138 an alternative to conditional independence testing for constraint-based methods, and being agnostic   \n139 about the class of causal mechanisms of the observed variables, under the weaker requirement of   \n140 additivity of the noise terms. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "141 3 Theory for a score-based test of separation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "142 In this section, we show that for $V\\subseteq X$ generated according to Equation (4) the Hessian matrix of   \n143 $\\log p(V)$ identifies the equivalence class of the marginal MAG $\\mathcal{M}_{V}^{\\mathcal{G}}$ . It has already been proven that   \n144 cross-partial derivatives of the log-likelihood are informative about a set of conditional independence   \n145 relationships between random variables: Spantini et al. [21] (Lemma 4.1) shows that, given $V_{Z}\\subseteq X$   \n146 such that $\\{V_{i},V_{j}\\}\\subseteq V_{Z}$ , then ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\frac{\\partial^{2}}{\\partial V_{i}\\partial V_{j}}}\\log p(V_{Z})=0\\iff V_{i}\\bot\\bot V_{j}|V_{Z}\\setminus\\{V_{i},V_{j}\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 Equation (3) resulting from faithfulness and the directed global Markov property immediately   \n148 implies that this expression can be used as a test of conditional independence to identify the Markov   \n149 equivalence class of the graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ , as commonly done in constraint-based causal discovery (for   \n150 reference, see e.g. Section 3 in Glymour et al. [4]). This result generalizes Lemma 1 of Montagna et al.   \n151 [16], where it is used to define constraints to infer edges in the causal structure without latent variables. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 (Adapted1 from [21]). Let $V$ be a set of random variables with strictly positive density generated according to model (4). For each set $V_{Z}\\subseteq V$ of nodes in $\\mathcal{M}_{V}^{\\mathcal{G}}$ such that $\\{V_{i},V_{j}\\}\\subseteq V_{Z}$ , the following holds for each supported value $v_{Z}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial V_{i}\\partial V_{j}}\\log p(v_{Z})=0\\;\\Longleftrightarrow\\;V_{i}\\:\\underset{M_{V}^{\\cal Q}}{\\longrightarrow}\\:{\\cal V}_{i}\\:\\underset{V_{i}}{\\stackrel{m}{\\longrightarrow}}\\:V_{j}|V_{Z}\\setminus\\{V_{i},V_{j}\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "152 The result of Proposition 1 presents an alternative to conditional independence testing in constraint  \n153 based approaches to causal discovery, showing that the equivalence class of the graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ can be   \n154 identified using the cross partial derivatives of the log-likelihood as a test of conditional independence   \n155 between variables, much in the spirit of the Fast Causal Inference algorithm [11]. Identifying the   \n156 Markov equivalence class is the most we can hope to achieve without further hypotheses. As we will   \n157 see in the next section, the score function can also help leverage additional restrictive assumptions on   \n158 the causal mechanisms of Equation (4) to identify direct causal effects. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "159 4 A theory of identifiability from the score ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "160 In this section, we show that, under additional assumptions on the data-generating process, we can   \n161 identify the direct causal relations that are not influenced by unobserved variables, as well as the   \n162 presence of unobserved active paths (Definition 5) between nodes in the marginalized graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ .   \n163 As a preliminary step before diving into causal discovery with latent variables, we show how the   \n164 properties of the score function identify edges in directed acyclic graphs, that is in the absence of   \n165 latent variables (when $U=\\emptyset$ and $\\mathcal{G}=\\bar{M}_{V}^{\\mathcal{G}})$ . The goal of the next section is two-sided: first, it   \n166 introduces the fundamental ideas connecting the score function to causal discovery that also apply to   \n167 hidden variable models, second, it extends the existing theory of causal discovery with score matching   \n168 to additive noise models with both linear and nonlinear mechanisms. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "169 4.1 Warm up: identifiability without latent confounders ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "170 In this section, we summarise and extend the theoretical findings presented in Montagna et al. [17],   \n171 where the authors show how to derive constraints on the score function that identify the causal order of   \n172 the DAG $\\mathcal{G}$ where all the variables in the set $X$ are observed. Define the structural relations of (1) as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{i}:=h_{i}(X_{\\mathrm{PA}_{i}^{g}})+N_{i},i=1,\\ldots,k,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "173 with three times continuously differentiable mechanisms $h_{i}$ , noise terms centered at zero, and strictly   \n174 positive density $p_{X}$ . Given the Markov factorization of Equation (2), the components of the score   \n175 function $\\nabla\\log{p(x)}$ are: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{X_{i}}\\log{p(x)}=\\partial_{X_{i}}\\log{p(x_{i}|x_{\\mathrm{PA}_{i}^{\\mathcal{G}}})}+\\displaystyle\\sum_{j\\in\\mathrm{CH}_{i}^{\\mathcal{G}}}\\partial_{X_{i}}\\log{p(x_{j}|x_{\\mathrm{PA}_{j}^{\\mathcal{G}}})}}\\\\ &{\\qquad\\qquad\\qquad=\\partial_{N_{i}}\\log{p(n_{i})}-\\displaystyle\\sum_{j\\in\\mathrm{CH}_{i}^{\\mathcal{G}}}\\partial_{X_{i}}h_{j}(x_{\\mathrm{PA}_{j}^{\\mathcal{G}}})\\partial_{N_{j}}\\log{p(n_{j})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 where $\\mathrm{CH}_{i}^{\\mathcal{G}}$ denotes the set of children of node $X_{i}$ . We observe that if a node $X_{s}$ is a sink, i.e. a   \n177 node satisfying $\\mathrm{CH}_{s}^{\\mathcal{G}}=\\emptyset$ , then the summation over the children vanishes, implying that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\partial_{X_{s}}\\log p(x)=\\partial_{N_{s}}\\log p(n_{s}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "178 The key point is that the score component of a sink node is a function of its structural equation noise   \n179 term, such that one could learn a consistent estimator of $\\partial_{X_{s}}\\log{p_{X}}$ from a set of observations of the   \n180 noise term $N_{s}$ . Given that, in general, one has access to $X$ samples rather than observations of the   \n181 noise random variables, authors in Montagna et al. [17] show that $N_{s}$ of a sink node can be consistently   \n182 estimated from i.i.d. realizations of $X$ . For each node $X_{1},\\ldots,X_{k}$ , we define the quantity: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{i}:=X_{i}-\\mathbf{E}[X_{i}|X_{\\setminus X_{i}}],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 where $X_{\\backslash X_{i}}$ are the random variables in the set $X\\setminus\\{X_{i}\\}.\\;{\\bf E}[X_{i}|X_{\\backslash X_{i}}]$ is the optimal least squares   \n184 predictor of $X_{i}$ from all the remaining nodes in the graph, and $R_{i}$ is the regression residual. For   \n185 a sink node $X_{s}$ , the residual satisfies: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{s}=N_{s},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "186 which can be seen by rewriting $\\begin{array}{r l r}{{\\bf E}[X_{s}|X_{\\backslash X_{s}}]}&{=}&{h_{s}(X_{\\mathrm{PA}_{s}^{g}})\\;+\\;{\\bf E}[N_{s}|X_{\\mathrm{DE}_{s}^{g}},X_{\\mathrm{ND}_{s}^{g}}]\\;\\;=\\;}\\end{array}$   \n187 $h_{s}(X_{\\mathrm{PA}_{s}^{\\mathcal{G}}})+\\mathbf{E}[N_{s}]$ , where $X_{\\mathrm{DE}_{s}^{\\mathcal{G}}}$ and $X_{\\mathrm{ND}_{s}^{\\mathcal{G}}}$ denotes the descendants and non-descendants of $X_{s}$ ,   \n188 respectively. Equations (9) and (11) together imply that the score $\\partial_{N_{s}}\\log p(N_{s})$ is a function of $R_{s}$ ,   \n189 such that it is possible to find a consistent approximator of the score of a sink from observations of $R_{s}$ .   \n190 Proposition 2 (Generalization of Lemma 1 in Montagna et al. [17]). Let $X$ be a set of random   \n191 variables, generated by a restricted additive noise model (Definition $^{\\,g}$ ) with structural equations (7),   \n192 and let $X_{j}\\in X$ . Consider $r_{j}$ in the support of $R_{j}$ . Then: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{j}{\\mathrm{~is~a~sink}}\\Longleftrightarrow\\mathbf{E}\\left[\\left(\\mathbf{E}\\left[\\partial_{X_{j}}\\log p(X)\\mid R_{j}=r_{j}\\right]-\\partial_{X_{j}}\\log p(X)\\right)^{2}\\right]=0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "193 Our result generalizes Lemma 1 in Montagna et al. [17], as they assume $X$ generated by an   \n194 identifiable additive noise model with nonlinear mechanisms. Instead, we remove the nonlinearity   \n195 assumption and make the weaker hypothesis of a restricted additive noise model, which is provably   \n196 identifiable [9], in the formal sense defined in the appendix (Definition 8). This result doesn\u2019t come   \n197 as a surprise, given the previous findings of Ghoshal and Honorio [14] showing that the score infers   \n198 linear non-Gaussian additive noise models: Proposition 2 provides a unifying and general theory   \n199 for the identifiability of models with potentially mixed linear and nonlinear mechanisms.   \n200 Based on these insights, Montagna et al. [17] propose the NoGAM algorithm to exploit the con  \n201 dition in (12) for identifying the causal order of the graph: being $\\mathbf{E}\\left[\\bar{\\partial}_{X_{i}}\\log p(X)\\,\\,\\bar{|}\\,R_{i}\\right]$ the opti  \n202 mal least squares estimator of the score of node $X_{i}$ from $R_{i}$ , a sink node is characterized as the   \n203 $\\operatorname*{argmin}_{i}{\\mathbf{E}}\\left[{\\mathbf{E}}\\left[\\partial_{X_{i}}\\log p(X)~|~R_{i}\\right]-\\partial_{X_{i}}\\log p(X)\\right]^{2}$ , where in practice the residuals $R_{i}$ , the score   \n204 components and the least squares estimators are replaced by their empirical counterparts. After a   \n205 sink node is identified, it is removed from the graph and assigned a position in the order, and the   \n206 procedure is iteratively repeated up to the source nodes. Being the score estimated by score matching   \n207 techniques [23], we usually make reference to score matching-based causal discovery.   \n208 In the next section, we show how we can generalize these results to identify direct causal effects   \n209 between a pair of variables in the marginal MAG $\\mathcal{M}_{V}^{\\mathcal{G}}$ when $U\\neq\\emptyset$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "210 4.2 Identifiability in the presence of latent confounders ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "211 We now introduce the last of our main theoretical results, that is: given a pair of nodes $V_{i}$ , $V_{j}$ that   \n212 are adjacent in the graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ with $U\\neq\\emptyset$ , we can use the score function to identify the presence   \n213 of a direct causal effect between $V_{i}$ and $V_{j}$ , or that of an active path that is influenced by unobserved   \n214 variables. Given that the causal model of Equation (4) ensures identifiability only up to the equivalence   \n215 class, we need additional restrictive assumptions. In particular, we enforce an additive noise model   \n216 with respect to both the observed and unobserved noise variables. This corresponds to an additive   \n217 noise model on the observed variables with the noise terms recentered by the latent causal effects.   \n218 Assumption 1 (SCM assumptions). The set of structural equations of the observable variables   \n219 specified in (4) is now defined as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{i}:=f_{i}(V_{\\mathrm{PA}_{i}^{\\mathcal{G}}})+g_{i}(U^{i})+N_{i},\\forall i=1,\\ldots,d,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "220 assuming the mechanisms $f_{i}$ to be of class $\\mathcal{C}^{3}(\\mathbb{R}^{|V_{\\mathrm{PA}_{i}^{\\mathcal{G}}}|})$ , and mutually independent noise terms with   \n221 strictly positive density function. The $N_{i}$ \u2019s are assumed to be non-Gaussian when $f_{i}$ is linear in some   \n222 of its arguments.   \n223 Crucially, our hypothesis is weaker than those required by two state-of-the-art approaches, CAM-UV   \n224 [13] and RCD [12]: CAM-UV assumes a Causal Additive Model (CAM) with structural equations   \n225 with nonlinear mechanisms in the form $\\begin{array}{r}{V_{i}:=\\sum_{k\\in\\mathrm{PA}_{i}^{\\mathcal G}}f_{i k}(V_{k})+\\sum_{U_{k}^{i}}g_{i k}(U_{k}^{i})+N_{i}}\\end{array}$ , and RCD   \n226 requires an additive noise model with linear effects of both the latent and observed causes. Thus,   \n227 our model encompasses and extends the nonlinear and linear settings of CAM-UV and RCD, such   \n228 that the theory developed in the remainder of the section is valid for a broader class of causal models. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "229 Our first step is rewriting the structural relations in (13) as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{i}:=f_{i}(V_{\\mathrm{PA}_{i}^{g}})+\\tilde{N}_{i},}\\\\ &{\\tilde{N}_{i}:=g_{i}(U^{i})+N_{i},\\forall i=1,\\dots,d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "230 which provides an additive noise model in the form of (7). Next, we define the following regression   \n231 residuals for any node $V_{k}$ in the graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{k}(V_{Z}):=V_{k}-{\\mathbf E}[V_{k}\\mid V_{Z\\backslash\\{k\\}}],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "232 where $V_{Z\\backslash\\{k\\}}$ denotes the set of random variables $V_{Z}\\setminus\\{V_{k}\\}$ . ", "page_idx": 5}, {"type": "text", "text": "233 Given these definitions, we are ready to show how directed edges, and the presence of unobserved   \n234 variables can be identified from the score of linear and nonlinear additive noise models. ", "page_idx": 5}, {"type": "text", "text": "235 4.2.1 Identifiability of directed edges ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "236 Consider $V_{i},V_{j}$ adjacent nodes in the PAG $\\mathcal{P}_{\\mathcal{M}_{V}^{\\mathcal{G}}}$ : we want to investigate when a direct causal   \n237 effect $V_{i}~\\in~V_{\\mathrm{PA}_{j}^{\\mathcal G}}$ can be identified from the score. We make the following observations: for   \n238 $V_{Z}=V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\cup\\{V_{j}\\}$ and $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\perp\\!\\!\\!\\perp_{d}^{\\mathcal{G}}U^{j}$ , by Equation (15) it follows ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{j}(V_{Z})=\\tilde{N_{j}}-\\mathbf{E}[\\tilde{N_{j}}],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "239 where we use $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\perp\\!\\!\\!\\perp_{d}^{\\mathcal{G}}U^{j}$ to write $\\mathbf{E}[\\tilde{N}_{j}|V_{Z\\backslash\\{j\\}}]=\\mathbf{E}[\\tilde{N}_{j}]$ . Moreover, we note that $V_{j}$ is a sink node   \n240 relative to $\\mathcal{M}_{V_{Z}}^{\\mathcal{G}}$ , the marginalization of $\\mathcal{G}$ onto $V_{Z}$ . In analogy to the case without latent variables, we   \n241 can show that $\\partial_{V_{j}}\\log{p(V_{Z})}$ is a function of $\\tilde{N_{j}}$ , the error term in the additive noise model of Equation   \n242 (14), such that the score of $V_{j}$ can be consistently predicted from observations of the residual $R_{j}(V_{Z})$ .   \n243 Proposition 3. Let $X$ be generated by a restricted additive noise model with structural equations (7),   \n244 and causal graph $\\mathcal{G}$ . Consider $V_{i},V_{j}$ adjacent in $\\mathcal{M}_{V}^{\\mathcal{G}}$ , marginalization of $\\mathcal{G}$ . Further, assume that   \n245 the score component $\\partial_{V_{j}}\\log{p(V_{Z})}$ is not constant for uncountable values of $V_{Z}$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{\\mathrm{PA}_{j}^{\\sigma}}\\overset{\\mathrm{d}}{\\underset{\\mathcal{G}}{=}}U^{j}\\wedge V_{i}\\in V_{\\mathrm{PA}_{j}^{\\sigma}}\\Longleftrightarrow\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})-\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})=r_{j}]]^{2}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "247 (ii) Let $V_{Z}\\subseteq V$ , such that $\\{V_{i},V_{j}\\}\\subseteq V_{Z}$ . Then: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{\\mathrm{PA}_{j}^{G}}\\mathrm{~\\mathcal{A}~}_{\\mathcal{G}}^{d}U^{j}\\mathrm{~V~}V_{i}\\notin V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\Longleftrightarrow\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})-\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})=r_{j}]]^{2}\\neq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "248 Intuitively, the proposition has two essential implications. Part $(i)$ provides the condition for the   \n249 identifiability of the potential direct causal effect between a pair $V_{i},V_{j}$ , that is, when the association   \n250 between $V_{j}$ and its observed parents is not influenced by active paths that involve latent variables.   \n251 This condition is necessary: given an active path such that $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\bot\\!\\!\\!\\!\\slash\\,\\mathit{d}_{\\mathcal{G}}^{\\mathit{d}}U^{j}$ , the score could not identify   \n252 a direct causal effect $V_{i}\\to V_{j}$ , which is the content of the second part of the proposition.   \n253 We have established theoretical guarantees of identifiability for linear and nonlinear additive noise   \n254 models, even in the presence of hidden variables: we find that the score function is a means for the   \n255 identifiability of all direct parental relations that are not influenced by unobserved variables; all the   \n256 remaining arrowheads of the edges in the graph $\\mathcal{M}_{V}^{\\mathcal{G}}$ are identified no better than in the equivalence   \n257 class. Based on these insights, we propose AdaScore, a score matching-based algorithm for the   \n258 inference of Markov equivalence classes, direct causal effects, and the presence of latent variables. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "259 4.3 A score-based algorithm for causal discovery ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "260 Building on our theory, we propose AdaScore, a generalization of NoGAM to linear and nonlinear   \n261 additive noise models with latent variables. The main strength of our approach is its adaptivity   \n262 with respect to structural assumptions: based on the user\u2019s belief about the plausibility of several   \n263 modeling assumptions on the data, AdaScore can output an equivalence class (using the condition   \n264 of Proposition 1 instead of conditional independence testing in an FCI-like algorithm), a directed   \n265 acyclic graph (as in NoGAM), or a mixed graph, accounting for the presence of unobserved variables.   \n266 We now describe the version of our algorithm whose output is a mixed graph, where we rely on score   \n267 matching estimation of the score and its Jacobian (Appendix C.2). At an intuitive level, we find   \n268 unoriented edges using Proposition 1, i.e. checking for dependencies in the form of non-zero entries   \n269 in the Jacobian of the score via hypothesis testing on the mean, and find the edges\u2019 directions via the   \n270 condition of Proposition 3, i.e. by estimating residuals of each node $X_{i}$ and checking whether they can   \n271 correctly predict the $i$ -th score entry (the vanishing mean squared errors are verified by hypothesis test   \n272 of zero mean). It would be tempting to simply find the skeleton (i.e. the graphical representation of   \n273 the constraints of an equivalence class) first via the well-known adjacency search of the FCI algorithm   \n274 and then iterate through all neighborhoods of all nodes to orient edges using Proposition 3. This   \n275 would be prohibitively expensive, as finding the skeleton is well-known to have super-exponential   \n276 computational complexity [11]. Instead, we propose an alternative solution: exploiting the fact that   \n277 some nodes may not be influenced by latent variables, we first use Proposition 2 to find sink nodes   \n278 that are not affected by latents (using hypothesis testing to find vanishing mean squared error in the   \n279 score predictions from the residuals), in the spirit of the NoGAM algorithm. If there is such a sink,   \n280 we search all its adjacent nodes via Proposition 1 (plus an optional pruning step for better accuracy,   \n281 Appendix C.2), and orient the inferred edges towards the sink. Else, if no sink can be found, we pick   \n282 a node in the graph and find its neighbors by Proposition 1, orienting its edges using the condition in   \n283 Proposition 3 (score estimation by residuals under latent effects). This way, we get an algorithm that   \n284 is polynomial in the best case (Appendix C.3). Details on AdaScore are provided in Appendix C,   \n285 while a pseudo-code summary is provided in the Algorithm 1 box. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "oRZN9a53ag/tmp/e95a09cbc2d72367a54658daaaaf780f68eca323fade378b8602ab709b2d64f6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "286 5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "287 We use the causally2 Python library [26] to generate synthetic data with known ground truths,   \n288 created as Erd\u00f6s-R\u00e9nyi sparse and dense graphs, respectively with probability of edge between pair   \n289 of nodes equals 0.3 and 0.5. We sample the data according to linear and nonlinear mechanisms with   \n290 additive noise, where the nonlinear functions are parametrized by a neural network with random   \n291 weights, a common approach in the literature [18, 26, 27, 28, 29]. Noise terms are sampled from a   \n292 uniform distribution in the $[-2,2]$ range. Hidden causal effects are obtained by randomly picking   \n293 two nodes and dropping the corresponding column from the data matrix. See Appendix D.1 for   \n294 further details on the data generation. As metric, we consider the structural Hamming distance (SHD)   \n295 [30, 31], a simple count of the number of incorrect edges, where missing and wrongly directed   \n296 edges count as one error. We fix the level of the hypothesis tests of AdaScore to 0.05, which is a   \n297 common choice in the absence of prior knowledge. We compare AdaScore to NoGAM, CAM-UV,   \n298 RCD, and DirectLiNGAM, whose assumptions are detailed in Table 1. In the main manuscript, we   \n299 comment on the results on datasets of 1000 observations from dense graphs, with and without latent   \n300 variables. Additional experiments including those on sparse networks are presented in Appendix E.   \n301 Our synthetic data are standardized by their empirical variance to remove shortcuts in the data [18, 32].   \n302 Discussion. Our experimental results on models without latent variables of Figure 1a show that when   \n303 causal relations are linear, AdaScore can recover the causal graph with accuracy that is comparable   \n304 with all the other benchmarks, with the exception of DirectLiNGAM. On nonlinear data AdaScore   \n305 presents better performance than CAM-UV, RCD, and DirectLiNGAM while being comparable   \n306 to NoGAM in accuracy. This is in line with our expectations: in the absence of finite sample   \n307 errors and in the fully observable setting, NoGAM and AdaScore are indeed the same algorithms.   \n308 When inferring under latent causal effects, Figure 1b, our method performs comparably to CAM  \n309 UV and RCD on graphs up to seven nodes while slightly degrading on nine nodes. Additionally,   \n310 AdaScore outperforms NoGAM in this setting, as we would expect according to our theory. Overall,   \n311 we observe that our method is robust to a variety of structural assumptions, with accuracy that is   \n312 often comparable and sometimes better than competitors (as in nonlinear observable settings). We   \n313 remark that although AdaScore does not clearly outperform the other baselines, its broad theoretical   \n314 guarantees of identifiability are not matched by any available method in the literature; this makes it   \n315 an appealing option for inference in realistic scenarios that are hard to investigate with synthetic data,   \n316 where the structural assumptions of the causal model underlying the observations are unknown. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "oRZN9a53ag/tmp/9993bf320c3afd836538461090602afce6e5ad3820cd81065828317bd643ca3f.jpg", "img_caption": ["(b) Latent variables model "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: Empirical results on dense graphs with different numbers of nodes, on fully observable (no hidden variables) and latent variable models. We report the SHD accuracy (the lower, the better). We note that DirectLiNGAM is surprisingly robust to different structural assumptions, and AdaScore is generally comparable or better (as in nonlinear observable data) than the other benchmarks. ", "page_idx": 8}, {"type": "table", "img_path": "oRZN9a53ag/tmp/4fab407bf144cff15b67fc76f79232dcdcc48a7a81b83ae03da266fe6247ba36.jpg", "table_caption": ["Table 1: Experiments causal discovery algorithms. The content of the cells denotes whether the method supports $(\\pmb{\\mathscr{S}})$ or not $({\\pmb X})$ the condition specified in the corresponding row. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "317 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "18 The existing literature on causal discovery shows a connection between score matching and structure   \n19 learning in the context of nonlinear ANMs: in this paper, (i) we formalize and extend these results   \n20 to linear SCMs, and (ii) we show that the score retains information on the causal structure even in the   \n21 presence of unobserved variables. Additionally, while previous works posit the accent on finding the   \n22 causal order through the score, we study its potential to identify the Markov equivalence class with a   \n23 constraint-based strategy that does not explicitly require tests of conditional independence, as well as   \n24 to identify direct causal effects. Our theoretical insights result in AdaScore: unlike existing approaches   \n25 for the estimation of causal directions, our algorithm provides theoretical guarantees for a broad class   \n26 of identifiable models, namely linear and nonlinear, with additive noise, in the presence of latent   \n27 variables. Even though AdaScore does not clearly outperform the existing baselines on our synthetic   \n28 benchmark, its adaptivity to different structural hypotheses is a step towards causal discovery that is   \n2 less reliant on prior assumptions, which are often untestable and thus hindering reliable inference in   \n30 real-world problems. While we do not touch on the task of causal representation learning [33], where   \n31 causal variables are learned from data, we believe this is a promising research direction in relation   \n32 to our work due to the specific interplay between score-matching estimation and generative models. ", "page_idx": 8}, {"type": "text", "text": "333 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "334 [1] Jonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Elements of causal inference: founda  \n335 tions and learning algorithms. The MIT Press, 2017.   \n336 [2] Judea Pearl. Causality. Cambridge university press, 2009.   \n337 [3] Peter Spirtes. Introduction to causal inference. Journal of Machine Learning Research, 11(54):   \n338 1643\u20131662, 2010. URL http://jmlr.org/papers/v11/spirtes10a.html.   \n339 [4] Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on   \n340 graphical models. Frontiers in Genetics, 10, 2019. ISSN 1664-8021. doi: 10.3389/fgene.2019.   \n341 00524. URL https://www.frontiersin.org/articles/10.3389/fgene.2019.00524.   \n342 [5] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press, 2nd   \n343 edition, 2000.   \n344 [6] David Maxwell Chickering. Optimal structure identification with greedy search. J. Mach. Learn.   \n345 Res., 3(null):507\u2013554, mar 2003. ISSN 1532-4435. doi: 10.1162/153244303321897717. URL   \n346 https://doi.org/10.1162/153244303321897717.   \n347 [7] Shohei Shimizu, Patrik O. Hoyer, Aapo Hyv\u00e4rinen, and Antti Kerminen. A linear non-gaussian   \n348 acyclic model for causal discovery. J. Mach. Learn. Res., 7:2003\u20132030, dec 2006. ISSN   \n349 1532-4435.   \n350 [8] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Sch\u00f6lkopf. Non  \n351 linear causal discovery with additive noise models. In D. Koller, D. Schuurmans, Y. Bengio,   \n352 and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 21. Cur  \n353 ran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2008/file/   \n354 f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf.   \n355 [9] Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Causal discovery   \n356 with continuous additive noise models. J. Mach. Learn. Res., 15(1):2009\u20132053, jan 2014. ISSN   \n357 1532-4435.   \n358 [10] Kun Zhang and Aapo Hyv\u00e4rinen. On the identifiability of the post-nonlinear causal model. In   \n359 Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909,   \n360 page 647\u2013655, Arlington, Virginia, USA, 2009. AUAI Press. ISBN 9780974903958.   \n361 [11] Peter Spirtes. An anytime algorithm for causal inference. In Thomas S. Richardson and Tommi S.   \n362 Jaakkola, editors, Proceedings of the Eighth International Workshop on Artificial Intelligence   \n363 and Statistics, volume R3 of Proceedings of Machine Learning Research, pages 278\u2013285.   \n364 PMLR, 04\u201307 Jan 2001. URL https://proceedings.mlr.press/r3/spirtes01a.html.   \n365 Reissued by PMLR on 31 March 2021.   \n366 [12] Takashi Nicholas Maeda and Shohei Shimizu. Rcd: Repetitive causal discovery of linear   \n367 non-gaussian acyclic models with latent confounders. In Silvia Chiappa and Roberto Calandra,   \n368 editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and   \n369 Statistics, volume 108 of Proceedings of Machine Learning Research, pages 735\u2013745. PMLR,   \n370 26\u201328 Aug 2020. URL https://proceedings.mlr.press/v108/maeda20a.html.   \n371 [13] Takashi Nicholas Maeda and Shohei Shimizu. Causal additive models with unobserved variables.   \n372 In Uncertainty in Artificial Intelligence, pages 97\u2013106. PMLR, 2021.   \n373 [14] Asish Ghoshal and Jean Honorio. Learning linear structural equation models in polynomial   \n374 time and sample complexity. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings   \n375 of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84   \n376 of Proceedings of Machine Learning Research, pages 1466\u20131475. PMLR, 09\u201311 Apr 2018.   \n377 URL https://proceedings.mlr.press/v84/ghoshal18a.html.   \n378 [15] Paul Rolland, Volkan Cevher, Matth\u00e4us Kleindessner, Chris Russell, Dominik Janzing, Bernhard   \n379 Sch\u00f6lkopf, and Francesco Locatello. Score matching enables causal discovery of nonlinear   \n380 additive noise models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,   \n381 Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on   \n382 Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 18741\u2013   \n383 18753. PMLR, 17\u201323 Jul 2022.   \n384 [16] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello.   \n385 Scalable causal discovery with score matching. In 2nd Conference on Causal Learning and   \n386 Reasoning, 2023. URL https://openreview.net/forum?id $\\cdot$ 6VvoDjLBPQV.   \n387 [17] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello.   \n388 Causal discovery with score matching on additive models with arbitrary noise. In 2nd Conference   \n389 on Causal Learning and Reasoning, 2023. URL https://openreview.net/forum?id $=$   \n390 rVO0Bx90deu.   \n391 [18] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, and Francesco Locatello. Shortcuts   \n392 for causal discovery of nonlinear models by score matching, 2023.   \n393 [19] Pedro Sanchez, Xiao Liu, Alison Q O\u2019Neil, and Sotirios A. Tsaftaris. Diffusion models for   \n394 causal discovery via topological ordering. In The Eleventh International Conference on Learning   \n395 Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ Idusfje4-Wq.   \n396 [20] Zhenyu Zhu, Francesco Locatello, and Volkan Cevher. Sample complexity bounds for score  \n397 matching: Causal discovery and generative modeling. Advances in Neural Information Process  \n398 ing Systems, 36, 2024.   \n399 [21] Alessio Spantini, Daniele Bigoni, and Youssef Marzouk. Inference via low-dimensional   \n400 couplings, 2018.   \n401 [22] Juan Lin. Factorizing multivariate function classes. In M. Jordan, M. Kearns, and   \n402 S. Solla, editors, Advances in Neural Information Processing Systems, volume 10. MIT   \n403 Press, 1997. URL https://proceedings.neurips.cc/paper_files/paper/1997/   \n404 file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf.   \n405 [23] Aapo Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. J. Mach.   \n406 Learn. Res., 6:695\u2013709, 2005. URL https://api.semanticscholar.org/CorpusID:   \n407 1152227.   \n408 [24] Caroline Uhler, G. Raskutti, Peter B\u00fchlmann, and B. Yu. Geometry of the faithfulness assump  \n409 tion in causal inference. The Annals of Statistics, 41, 07 2012. doi: 10.1214/12-AOS1080.   \n410 [25] Jiji Zhang. Causal reasoning with ancestral graphs. Journal of Machine Learning Research, 9   \n411 (7), 2008.   \n412 [26] Francesco Montagna, Atalanti Mastakouri, Elias Eulig, Nicoletta Noceti, Lorenzo Rosasco,   \n413 Dominik Janzing, Bryon Aragam, and Francesco Locatello. Assumption violations   \n414 in causal discovery and the robustness of score matching. In A. Oh, T. Neumann,   \n415 A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural   \n416 Information Processing Systems, volume 36, pages 47339\u201347378. Curran Associates,   \n417 Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/   \n418 93ed74938a54a73b5e4c52bbaf42ca8e-Paper-Conference.pdf.   \n419 [27] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without   \n420 acyclicity constraints. In International Conference on Learning Representations, 2022. URL   \n421 https://openreview.net/forum?id $=$ eYciPrLuUhG.   \n422 [28] Nan Rosemary Ke, Silvia Chiappa, Jane X Wang, Jorg Bornschein, Anirudh Goyal, Melanie Rey,   \n423 Theophane Weber, Matthew Botvinick, Michael Curtis Mozer, and Danilo Jimenez Rezende.   \n424 Learning to induce causal structure. In International Conference on Learning Representations,   \n425 2023. URL https://openreview.net/forum?id=hp_RwhKDJ5.   \n426 [29] Philippe Brouillard, S\u00e9bastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and   \n427 Alexandre Drouin. Differentiable causal discovery from interventional data. In Proceedings of   \n428 the 34th International Conference on Neural Information Processing Systems, NIPS \u201920, Red   \n429 Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.   \n430 [30] Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. The max-min hill-climbing   \n431 bayesian network structure learning algorithm. Machine learning, 65:31\u201378, 2006.   \n432 [31] Sofia Triantaflilou and Ioannis Tsamardinos. Score-based vs constraint-based causal learning in   \n433 the presence of confounders. In Cfa@ uai, pages 59\u201367, 2016.   \n434 [32] Alexander G. Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated dag!   \n435 causal discovery benchmarks may be easy to game. In Neural Information Processing Systems,   \n436 2021. URL https://api.semanticscholar.org/CorpusID:239998404.   \n437 [33] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Ke, Nal Kalchbrenner, Anirudh   \n438 Goyal, and Y. Bengio. Toward causal representation learning. Proceedings of the IEEE, PP:   \n439 1\u201323, 02 2021. doi: 10.1109/JPROC.2021.3058954.   \n440 [34] Peter Spirtes and Thomas Richardson. A polynomial time algorithm for determining dag   \n441 equivalence in the presence of latent variables and selection bias. In Proceedings of the 6th   \n442 International Workshop on Artificial Intelligence and Statistics, pages 489\u2013500. Citeseer, 1996.   \n443 [35] Yingzhen Li and Richard E Turner. Gradient estimators for implicit models. arXiv preprint   \n444 arXiv:1705.07107, 2017.   \n445 [36] Peter B\u00fchlmann, Jonas Peters, and Jan Ernest. CAM: Causal additive models, high-dimensional   \n446 order search and penalized regression. The Annals of Statistics, 42(6), dec 2014. URL   \n447 https://doi.org/10.1214%2F14-aos1260. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "448 A Useful results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "449 In this section, we provide a collection of results and definitions relevant to the theory of this paper. ", "page_idx": 12}, {"type": "text", "text": "450 A.1 Definitions over graphs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "451 Let $X=X_{1},\\ldots,X_{d}$ a set of random variables. A graph ${\\mathcal{G}}=(X,E)$ consists of finitely many nodes   \n452 or vertices $X$ and edges $E$ . We now provide additional definitions, separately for directed acyclic   \n453 and mixed graphs.   \n454 Directed acyclic graph. In a directed graph, nodes can be connected by a directed edge $(\\rightarrow)$ , and   \n455 between each pair of nodes there is at most one directed edge. We say that $X_{1}$ is a parent of $X_{j}$ if   \n456 $X_{i}\\rightarrow X_{j}\\in E$ , in which case we also say that $X_{j}$ is a child of $X_{i}$ . Two nodes are adjacent if they   \n457 are connected by an edge. Three nodes are called a $\\nu$ -structure if one node is a child of the other   \n458 two, e.g. as $X_{i}\\rightarrow X_{k}\\leftarrow X_{j}$ is a collider. A path in $\\mathcal{G}$ is a sequence of at least two distinct vertices   \n459 $X_{i_{1}},\\ldots,X_{i_{m}}$ such that there is an edge between $X_{i_{k}}$ and $X_{i_{k+1}}$ . If $X_{i_{k}}\\to X_{i_{k+1}}$ for every node   \n460 in the path, we speak of a directed path, and call $X_{i_{k}}$ an ancestor of $X_{i_{k+1}}$ , $X_{i_{k+1}}$ a descendant of   \n461 $X_{i_{k}}$ . Given the set $\\mathrm{DE}_{i}^{\\mathcal{G}}$ of descendants of a node $X_{i}$ , we define the set of non-descendants of $X_{i}$ as   \n462 $\\mathrm{ND}_{i}^{\\mathcal{G}}=X\\setminus(\\mathrm{DE}_{i}^{\\mathcal{G}}\\cup\\{X_{i}\\})$ . A node without parents is called a source node. A node without children   \n463 is called a sink node. A directed acyclic graph is a directed graph with no cycles.   \n464 Mixed graph. In a mixed graph nodes can be connected by a directed edge $(\\rightarrow)$ or a bidirected   \n465 edge $(\\leftrightarrow)$ , and between each pair of nodes there is at most one directed edge. Two vertices are said   \n466 to be adjacent in a graph if there is an edge (of any kind) between them. The definitions of parent,   \n467 child, ancestor, descendant, path provided for directed acyclic graph also apply in the case of mixed   \n468 graphs. Additionally, $X_{i}$ is a spouse of $X_{j}$ (and vice-versa) if $X_{i}\\leftrightarrow X_{j}\\in E$ . An almost directed   \n469 cycle occurs when $X_{i}\\leftrightarrow X_{j}\\in E$ and $X_{i}$ is an ancestor of $X_{j}$ in $\\mathcal{G}$ .   \n470 For ease of reference from the main text, we separately provide the definition of inducing paths and ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "471 ancestors in directed acyclic graphs. ", "page_idx": 12}, {"type": "text", "text": "472 Definition 2 (Ancestor). Consider a DAG $\\mathcal{G}$ with set of nodes $X$ , and $X_{i},X_{j}$ elements of $X$ . We   \n473 say that $X_{i}$ is an ancestor of $X_{j}$ if there is a directed path from $X_{i}$ to $X_{j}$ in the graph, as in   \n474 $X_{i}\\rightarrow...\\rightarrow X_{j}$ .   \n475 Definition 3 (Inducing path). Consider a DAG $\\mathcal{G}$ with set of nodes $X$ , and $Y,Z$ disjoint subsets such   \n476 that $X=Y\\dot{\\cup}Z$ . We say that there is an inducing path relative to $Z$ between the nodes $Y_{i},Y_{j}$ if every   \n477 node on the path that is not in $Z\\cup\\{Y_{i},Y_{j}\\}$ is a collider on the path (i.e. for each $Y_{k}\\in Y$ on the path   \n478 the sequence $Y_{i}\\dots.\\,\\cdot\\,Y_{k}\\gets..\\,.\\,Y_{j}$ appears) and every collider on the path is an ancestor of $Y_{i}$ or $Y_{j}$ .   \n479 One natural way to encode inducing paths and ancestral relationships between variables is represented   \n480 by maximal ancestral graphs.   \n481 Definition 4 (MAG). A maximal ancestral graph (MAG) is a mixed graph such that:   \n482 1. there are no directed cycles and no almost directed cycles;   \n483 2. there are no inducing paths between two non-adjacent nodes.   \n484 Next, we define conditional independence in the context of graphs.   \n485 Definition 5 (m-separation). Let $\\mathcal{M}$ be a mixed graph with nodes $X$ . A path $\\pi$ in $\\mathcal{M}$ between $X_{i},X_{j}$   \n486 elements of $X$ is active w.r.t. $Z\\subseteq X\\setminus\\{X_{i},X_{j}\\}$ if:   \n487 1. every non-collider on $\\pi$ is not in $Z$   \n488 2. every collider on $\\pi$ is an ancestors of a node in $Z$ .   \n489 $X_{i}$ and $X_{j}$ are said to be $m$ -separated by $Z$ if there is no active path between $X_{i}$ and $X_{j}$ relative to $Z$ .   \n490 Two disjoint sets of variables $W$ and $Y$ are $m$ -separated by $Z$ if every variable in $W$ is $\\mathbf{m}$ -separated   \n491 from every variable in $Y$ by $Z$ .   \n492 If m-separation is applied to DAGs, it is called $d$ -separation.   \n493 The set of directed acyclic graphs that satisfy the same set of conditional independencies form an   \n494 equivalence class, known as the Markov equivalence class.   \n495 Definition 6 (Markov equivalence class of a DAG). Let $\\mathcal{G}$ be a DAG with nodes $X$ . We denote with   \n496 $[\\mathcal{G}]$ the Markov equivalence class of $\\mathcal{G}$ . A DAG $\\tilde{\\mathcal{G}}$ with nodes $X$ is in $[\\mathcal G]$ if the following conditions   \n497 are satisfied for each pair $X_{i},X_{j}$ of distinct nodes in $X$ : ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "499 ", "page_idx": 13}, {"type": "text", "text": "500 ", "page_idx": 13}, {"type": "text", "text": "\u2022 there is an edge between $X_{i}$ , $X_{j}$ in $\\mathcal{G}$ if and only if there is an edge between $X_{i},X_{j}$ in $\\tilde{\\mathcal{G}}$ ;   \n\u2022 $\\mathrm{et}\\;Z\\subseteq X\\;\\backslash\\;\\{X_{i},X_{j}\\}.\\;\\mathrm{Then}\\;X_{i}\\;\\underline{{{1}}}\\;\\underline{{{d}}}\\;X_{j}|Z\\iff X_{i}\\;\\underline{{{1}}}\\;\\underline{{{d}}}\\;X_{j}|Z;$   \n\u2022 let $\\pi$ be a path between $X_{i}$ and $X_{j}$ . $X_{k}$ is a collider in the path $\\pi$ in $\\mathcal{G}$ if and only if it is a collider in the path $\\pi$ in $\\tilde{\\mathcal{G}}$ . ", "page_idx": 13}, {"type": "text", "text": "502 In summary, graphs in the same equivalence class share the edges up to direction, the set of d  \n503 separations, and the set of colliders.   \n504 Just as for DAGs, there may be several MAGs that imply the same conditional independence   \n505 statements. Denote the Markov-equivalence class of a MAG $\\mathcal{M}$ with $[\\mathcal{M}]$ : this is represented by a   \n506 partial mixed graph, the class of graphs that can contain four kinds of edges: $\\rightarrow,\\leftrightarrow$ , $\\circ$ \u2212\u2212 $\\cdot\\bigcirc$ and $\\hookrightarrow$ ,   \n507 and hence three kinds of end marks for edges: arrowhead $(>)$ , tail $(-)$ and circle (\u25e6).   \n508 Definition 7 (PAG, Definition 3 of Zhang [25]). Let $[\\mathcal{M}]$ be the Markov equivalence class of an   \n509 arbitrary MAG $\\mathcal{M}$ . The partial ancestral graph (PAG) for $[\\mathcal{M}],P_{\\mathcal{M}}$ , is a partial mixed graph such   \n510 that: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "\u2022 $P_{\\mathcal{M}}$ has the same adjacencies as $\\mathcal{M}$ (and any member of $[\\mathcal{M}])$ does; \u2022 A mark of arrowhead is in $P_{\\mathcal{M}}$ if and only if it is shared by all MAGs in $[\\mathcal{M}]$ ; and \u2022 A mark of tail is in $P_{\\mathcal{M}}$ if and only if it is shared by all MAGs in $[\\mathcal{M}]$ . ", "page_idx": 13}, {"type": "text", "text": "514 Intuitively, a PAG represents an equivalence class of MAGs by displaying all common edge marks   \n515 shared by all members of the class and displaying circles for those marks that are not in common. ", "page_idx": 13}, {"type": "text", "text": "516 A.2 Equivalence between m-separation and d-separation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "517 In this section, we provide a proof for equation (5), stating the equivalence between m-separation and   \n518 d-separation in a formal sense. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1 (Adapted from Zhang [25]). Let $\\mathcal{G}$ be a DAG with nodes $X=V\\cup U$ , with $V$ and $U$ disjoint sets, and $\\mathcal{M}_{V}^{\\mathcal{G}}$ the marginalization of $\\mathcal{G}$ onto $V$ . For any $\\{V_{i},V_{j}\\}\\in V$ and $V_{Z}\\subseteq V\\backslash\\{V_{i},V_{j}\\}$ , the following equivalence holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\nV_{i}\\operatorname{\\perp}_{\\sqrt{\\mathcal{G}}}^{d}V_{j}|V_{Z}\\iff V_{i}\\operatorname{\\perp}_{\\mathcal{M}_{V}^{\\mathcal{G}}}^{m}V_{j}|V_{Z}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "519 Proof. The implication $V_{i}\\operatorname{\\perp}_{\\mathcal{G}}^{d}V_{j}|V_{Z}\\implies V_{i}\\operatorname{\\perp}_{\\mathcal{M}_{V}^{\\mathcal{G}}}V_{j}|V_{Z}$ is a direct consequence of Lemma 18   \n520 from Spirtes and Richardson [34], where we set $S=\\emptyset$ , since we do not consider selection bias. The   \n521 implication $V_{i}$ $\\perp_{\\mathcal{G}}^{d}V_{j}|V_{Z}\\;\\Longleftarrow\\;V_{i}\\perp_{\\mathcal{M}_{V}^{\\mathcal{G}}}^{m}V_{j}|V_{Z}$ follows from Lemma 17 by Spirtes and Richardson   \n522 [34], again with $S\\,=\\,\\emptyset$ . Note, that in their terminology \u201cd-separation in MAGs\u201d is what we call   \n523 m-separation. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "524 A.3 Additive noise model identifiability ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "525 We study the identifiability of the additive noise model, reporting results from Peters et al. [9]. We   \n526 start with a formal definition of identifiability in the context of causal discovery.   \n527 Definition 8 (Identifiable causal model). Let $(X,N,{\\mathcal{F}},p_{N})$ be an SCM with underlying graph $\\mathcal{G}$ and   \n528 $p_{X}$ joint density function of the variables of $X$ . We say that the model is identifiable from observa  \n529 tional data if the distribution $p_{X}$ can not be generated by a structural causal model with graph $\\tilde{\\mathcal{G}}\\neq\\mathcal{G}$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "530 First, we consider the case of models of two random variables ", "page_idx": 13}, {"type": "equation", "text": "$$\nX_{2}:=f(X_{1})+N,\\quad X_{1}\\bot\\underline{{{\\cal N}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "531 Condition 1 (Condition 19 of Peters et al. [9]). Consider an additive noise model with structural   \n532 equations (17). The triple $(f,p_{X_{1}},p_{N})$ does not solve the following differential equation for all pairs   \n533 $x_{1},x_{2}$ with $f^{\\prime}(x_{2})\\nu^{\\prime\\prime}(x_{2}-f(x_{1}))\\neq0$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\xi^{\\prime\\prime\\prime}=\\xi^{\\prime\\prime}\\left({\\frac{f^{\\prime\\prime}}{f^{\\prime}}}-{\\frac{\\nu^{\\prime\\prime\\prime}f^{\\prime}}{\\nu^{\\prime\\prime}}}\\right)+{\\frac{\\nu^{\\prime\\prime\\prime}\\nu^{\\prime}f^{\\prime\\prime}f^{\\prime}}{\\nu^{\\prime\\prime}}}-{\\frac{\\nu^{\\prime}(f^{\\prime\\prime})^{2}}{f^{\\prime}}}-2\\nu^{\\prime\\prime}f^{\\prime\\prime}f^{\\prime}+\\nu^{\\prime}f^{\\prime\\prime\\prime},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "534 Here, $\\xi:=\\log p_{X_{1}}$ , $\\nu:=\\log p_{N}$ , the logarithms of the strictly positive densities. The arguments   \n535 $x_{2}-f(x_{1})$ , $x_{1}$ , and $x_{1}$ of $\\nu$ , $\\xi$ and $f$ respectively, have been removed to improve readability. ", "page_idx": 14}, {"type": "text", "text": "536 Next, we show that a structural causal model satisfying Condition 1 is identifiable, as in Definition 8 ", "page_idx": 14}, {"type": "text", "text": "537 Theorem 1 (Theorem 20 of Peters et al. [9]). Let $p_{X_{1},X_{2}}$ the joint distribution of a pair of random   \n538 variables generated according to the model of equation (17) that satisfies Condition $^{\\,l}$ , with graph $\\mathcal{G}$ .   \n539 Then, $\\mathcal{G}$ is identifiable from the joint distribution.   \n540 Finally, we show an important fact, holding for identifiable bivariate models, which is that the score   \n541 \u2202\u2202X1 log p(x1, x2) is nonlinear in x1. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Lemma 2 (Sufficient variability of the score). Let $p_{X_{1},X_{2}}$ the joint distribution of a pair of random variables generated according to a structural causal model that satisfies Condition $^{\\,I}$ , with graph $\\mathcal{G}$ . Then: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial~}{\\partial X_{1}}(\\xi^{\\prime}(x_{1})-f^{\\prime}(x_{1})\\nu^{\\prime}(x_{2}-f(x_{1})))\\neq0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "542 for all pairs $\\left(x_{1},x_{2}\\right)$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. By contradiction, assume that there exists $(x_{1},x_{2})$ such that $\\begin{array}{r}{\\frac{\\partial}{\\partial X_{1}}(\\xi^{\\prime}(x_{1})-f^{\\prime}(x_{1})\\nu^{\\prime}(x_{2}-}\\end{array}$ $f(x_{1}))=0$ . Then: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial X_{1}}\\left(\\frac{\\frac{\\partial^{2}}{\\partial X_{1}^{2}}\\pi(x_{1},x_{2})}{\\frac{\\partial^{2}}{\\partial X_{1}\\partial X_{2}}\\pi(x_{1},x_{2})}\\right)=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "543 where $\\pi(x_{1},x_{2})=\\log p(x_{1},x_{2})$ . By explicitly computing all the partial derivatives of the above   \n544 equation, we obtain that equation 18 is satisfied, which violates Condition 1. \u53e3   \n545 These results guaranteeing the identifiability of the bivariate additive noise model can be generalized   \n546 to the multivariable case, with a set of random variables $X=\\{X_{1},\\ldots,X_{k}\\}$ that satisfy: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nX_{i}:=f_{i}(X_{\\mathrm{PA}_{i}^{g}})+N_{i},i=1,\\ldots,k,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "547 where $\\mathcal{G}$ is the resulting causal graph directed and acyclic. The intuition is that, rather than studying   \n548 the multivariate model as a whole, we need to ensure that Condition 1 is satisfied for each pair of   \n549 nodes, adding restrictions on their marginal conditional distribution. ", "page_idx": 14}, {"type": "text", "text": "Definition 9 (Definition 27 of Peters et al. [9]). Consider an additive noise model with structural equations (19). We call this SCM a restricted additive noise model if for all $X_{j}\\in X,X_{i}\\in X_{\\mathrm{PA}_{j}^{g}}$ , and all sets $X_{S}\\subseteq X$ , $S\\subset\\mathbb{N}$ , with $X_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\setminus\\{X_{i}\\}\\subseteq X_{S}\\subseteq X_{\\mathrm{ND}_{j}}^{\\mathcal{G}}\\setminus\\{X_{i},X_{j}\\}$ , there is a value $x_{S}$ with $p(x_{S})>0$ , such that the triplet ", "page_idx": 14}, {"type": "equation", "text": "$$\n(f_{j}(x_{\\mathrm{PA}_{j}^{\\mathcal{G}}\\backslash\\{i\\}},\\cdot),p_{X_{i}|X_{S}=x_{S}},p_{N_{j}})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "550 satisfies Condition 1. Here, $f_{j}(x_{\\mathrm{PA}_{j}^{\\mathcal{G}}\\backslash\\{i\\}},\\cdot)$ denotes the mechanism function $x_{i}\\;\\mapsto\\;f_{j}\\big(x_{\\mathrm{PA}_{j}^{\\mathcal G}}\\big)$ .   \n551 Additionally, we require the noise variables to have positive densities and the functions $f_{j}$ to be   \n552 continuous and three times continuously differentiable. ", "page_idx": 14}, {"type": "text", "text": "553 Then, for a restricted additive noise model, we can identify the graph from the distribution. ", "page_idx": 14}, {"type": "text", "text": "554 Theorem 2 (Theorem 28 of Peters et al. [9]). Let $X$ be generated by a restricted additive noise   \n555 model with graph $\\mathcal{G}$ , and assume that the causal mechanisms $f_{j}$ are not constant in any of the input   \n556 arguments, i.e. for $X_{i}\\in X_{\\mathrm{PA}_{j}^{\\mathcal{G}}}$ , there exist $x_{i}\\neq x_{i}^{\\prime}$ such that $\\check{f}_{j}(x_{\\mathrm{PA}_{j}^{\\mathcal{G}}\\backslash\\{i\\}},x_{i})\\neq f_{j}(\\dot{x}_{\\mathrm{PA}_{j}^{\\mathcal{G}}\\backslash\\{i\\}},\\dot{x}_{i}^{\\prime})$ .   \n557 Then, $\\mathcal{G}$ is identifiable. ", "page_idx": 14}, {"type": "text", "text": "558 A.4 Other auxiliary results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "559 We state several results that hold for a pair of random variables that are not connected by an active path   \n560 that includes unobserved variables (active paths are introduced in Definition 5). For the remainder of   \n561 the section, let $V,U$ be a pair of disjoint sets of random variables, $X=V\\cup U$ generated according   \n562 to the structural causal model defined by the set of equations (1), $\\mathcal{G}$ the associated causal graph, and   \n563 $\\mathcal{M}_{V}^{\\mathcal{G}}$ the marginalization onto $V$ .   \n564 The first statement provides under which condition the unobserved parents of two variables in the   \n565 marginal MAG are mutually independent random vectors.   \n566 Lemma 3. Let $V_{j}\\,\\in\\,V$ , and $Z\\subset\\mathbb{N}$ such that $V_{Z}=V_{\\mathrm{PA}_{j}^{g}}\\cup\\{V_{j}\\}$ . Assume $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\perp\\!\\!\\!\\perp V^{j}$ . Then   \n567 $U^{j}\\perp\\!\\!\\!\\perp U^{Z_{k}}$ for each index $Z_{k}\\neq j$ .   \n568 Proof. The assumption $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\overset{\\underset{\\mathrm{\\/|}}{d}}{=}\\mathcal{G}^{U^{j}}$ implies that there is no active path in $\\mathcal{G}$ between nodes in $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}$   \n569 and nodes in $U^{j}$ . Given that for each $Z_{k}\\in Z$ , $Z_{k}\\neq Z$ , nodes in $U^{Z_{k}}$ are direct causes of at least   \n570 one node in $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}$ , any active path between nodes in $U^{Z_{k}}$ and nodes in $U^{j}$ would also be an active   \n571 path between $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}$ and $U^{j}$ , which is a contradiction. Hence $U^{j}\\perp\\!\\!\\!\\perp U^{Z_{k}}$ . \u53e3   \n572 The previous lemmas allow proving the following result, which will be fundamental to demonstrate   \n573 the theory of Proposition 3. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Lemma 4. Let $V_{j}\\in V$ , and $Z\\subset\\mathbb{N}$ such that $V_{Z}=V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\cup\\{V_{j}\\}$ . Assume $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\overset{\\underset{\\mathrm{\\/|}}{d}}{=}\\mathcal{G}^{U^{j}}$ . W.l.o.g., let the $j$ -th element of $V_{Z}$ be $V_{Z_{j}}=V_{j}$ . Denote as $U^{Z}$ the set of unobserved parents of nodes in $V_{Z}$ , and $U^{Z\\setminus\\{j\\}}$ the unobserved parents of nodes in $V_{Z\\setminus\\{j\\}}:=V_{Z}\\setminus V_{j}$ . Then, the following holds for each $\\boldsymbol{v}_{Z},\\boldsymbol{u}^{Z}$ values: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\log p(v_{Z})=\\log p(v_{j}|v_{\\mathrm{PA}_{j}^{\\mathcal{G}}})+\\log Q(v_{Z}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ(v_{Z})=\\sum_{u^{Z\\setminus\\{j\\}}}p(u^{Z\\setminus\\{j\\}})\\prod_{k\\neq j}^{|Z|}p(v_{Z_{k}}|v_{Z_{1}},\\dotsc,v_{Z_{k-1}},u^{Z_{k}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "574 Proof. By the law of total probability and the chain rule, we can write $p(v_{Z})$ as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{p(v_{Z})=\\sum_{u}p(v_{Z}|u)p(u)}}\\\\ &{}&{=\\sum_{u}p(u)p(v_{Z_{j}}|u,v_{Z\\backslash\\{j\\}})p(v_{Z\\backslash\\{j\\}}|u).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "575 By Lemma 3, $U^{Z_{j}}\\perp\\!\\!\\!\\perp U^{Z_{k}}$ , $k\\neq j$ , where $U^{Z_{k}}$ denotes unobserved parents of the node $V_{Z_{k}}$ . Then,   \n576 we can factorize $p(u)=p\\left(u^{Z_{j}}\\right)p\\left(u^{Z\\backslash\\{j\\}}\\right)$ . Plugging the factorization in equation (20) we find ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{p(\\boldsymbol{v}_{Z})=\\sum_{u}p\\left(u^{Z_{j}}\\right)p\\left(u^{Z\\backslash\\{j\\}}\\right)p(\\boldsymbol{v}_{Z_{j}}|u,\\boldsymbol{v}_{Z\\backslash\\{j\\}})p(\\boldsymbol{v}_{Z\\backslash\\{j\\}}|u)}}\\\\ &{}&{=\\sum_{u}p\\left(u^{Z_{j}}\\right)p\\left(u^{Z\\backslash\\{j\\}}\\right)p(\\boldsymbol{v}_{Z_{j}}|u^{Z_{j}},\\boldsymbol{v}_{\\mathrm{PA}_{Z_{j}}^{g}})p(\\boldsymbol{v}_{Z\\backslash\\{j\\}}|u),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "577 where the latter equation comes from the global Markov property on the graph $\\mathcal{G}$ . Further, by assump  \n578 tion of $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\perp\\!\\!\\!\\perp V^{j}$ , we know that $U^{Z_{j}}\\perp\\!\\!\\!\\perp V_{Z_{k}}$ , $k\\neq j$ , such that $p(v_{Z\\backslash\\{j\\}}|u)=p(v_{Z\\backslash\\{j\\}}|u^{Z\\backslash\\{j\\}})$ .   \n579 Then: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(v_{Z})=\\displaystyle\\sum_{u}p\\left(u^{Z\\backslash}\\right)p\\left(u^{Z\\backslash\\{j\\}}\\right)p\\big(v_{Z_{j}}|u^{Z_{j}},v_{\\mathrm{PA}_{Z_{j}}^{\\sigma}}\\big)p\\big(v_{Z\\backslash\\{j\\}}|u^{Z\\backslash\\{j\\}}\\big)}\\\\ &{\\qquad=\\displaystyle\\sum_{u^{Z_{j}}}p\\left(u^{Z_{j}}\\right)p\\big(v_{Z_{j}}|u^{Z_{j}},v_{\\mathrm{PA}_{Z_{j}}^{\\sigma}}\\big)\\displaystyle\\sum_{u^{Z\\backslash\\{j\\}}}p\\left(u^{Z\\backslash\\{j\\}}\\right)p\\big(v_{Z\\backslash\\{j\\}}|u^{Z\\backslash\\{j\\}}\\big)}\\\\ &{\\qquad=p\\big(v_{Z_{j}}|v_{\\mathrm{PA}_{Z_{j}}^{\\sigma}}\\big)\\displaystyle\\sum_{u^{Z\\backslash\\{j\\}}}p\\left(u^{Z\\backslash\\{j\\}}\\right)p\\big(v_{Z\\backslash\\{j\\}}|u^{Z\\backslash\\{j\\}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "580 which proves the claim. ", "page_idx": 15}, {"type": "text", "text": "581 Intuitively, Lemma 4 shows that given a node $V_{j}$ without children and bidirected edges in a marginal  \n582 ized graph $\\mathcal{M}_{V_{Z}}^{\\mathcal{G}}$ , the kernel of node $V_{j}$ in the Markov factorization of $p(v_{Z})$ is equal to the kernel of   \n583 the same node in the Markov factorization of $p(x)$ of equation (2), relative to the graph without latent   \n584 confounders $\\mathcal{G}$ . ", "page_idx": 16}, {"type": "text", "text": "585 B Proofs of theoretical results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "586 B.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . Observe that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial V_{i}\\partial V_{j}}\\log p(v_{Z})=0\\iff V_{i}\\bot\\bot_{\\mathcal{G}}^{d}V_{j}|V_{Z}\\setminus\\{V_{i},V_{j}\\}\\iff V_{i}\\bot\\bot_{\\mathcal{M}_{V}^{\\mathcal{G}}}^{m}V_{j}|V_{Z}\\setminus\\{V_{i},V_{j}\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "587 where the first equivalence holds by a combination of the faithfulness assumption with the global   \n588 Markov property, as explicit in equation (3), and the second due to Lemma 1. Then, the claim is   \n589 proven. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "590 B.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The forward direction is immediate from equation (9) and $R_{j}\\,=\\,N_{j}$ , when $X_{j}$ is a sink (equation (11)). Thus, we focus on the backward direction. Given ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}\\left[\\left(\\mathbf{E}\\left[\\partial_{X_{j}}\\log p(X)\\mid R_{j}=r_{j}\\right]-\\partial_{X_{j}}\\log p(X)\\right)^{2}\\right]=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "591 we want to show that $X_{j}$ has no children, which we prove by contradiction. ", "page_idx": 16}, {"type": "text", "text": "Let us introduce a function $q:\\mathbb{R}\\to\\mathbb{R}$ such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\partial_{X_{j}}\\log p(X)~|~R_{j}=r_{j}\\right]=q(r_{j}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $s_{j}:\\mathbb{R}^{|X|}\\to\\mathbb{R}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\ns_{j}(x)=\\partial_{X_{j}}\\log p(x).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The mean squared error equal to zero implies that $s_{j}(X)$ is a constant, once $R_{j}$ is observed. Formally, under the assumption of $p(x)>0$ for each $x\\in\\mathbb{R}^{k}$ , this implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\np(s_{j}(x)\\neq q(R_{j})|R_{j}=r_{j})=0,\\forall x\\in\\mathbb{R}^{k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By contradiction, we assume that $X_{j}$ is not a leaf, and want to show that $s_{j}(X)$ is not constant in $X$ , given $R_{j}$ fixed. Let $X_{i}$ such that $\\bar{X_{j}}\\in X_{\\mathrm{PA}_{i}^{\\mathcal{G}}}$ . Being the structural causal model identifiable, there is no model with distribution $p_{X}$ whose graph has a backward edge $X_{i}\\rightarrow X_{j}$ : thus, the Markov factorization of equation (2) is unique and implies: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\partial_{X_{j}}\\log p(X)=\\partial_{N_{j}}\\log p(N_{j})-\\sum_{k\\in\\mathrm{CH}_{j}^{\\mathcal{G}}}\\partial_{X_{j}}h_{k}\\bigl(X_{\\mathrm{PA}_{k}}\\bigr)\\partial N_{k}\\log p(N_{k}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We note that, by definition of residual in equation (10), $R_{j}=r_{j}$ fixes the following distance: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{j}=N_{j}-{\\bf E}[N_{j}|X_{\\backslash X_{j}}].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, conditioning on $R_{j}$ doesn\u2019t restrict the support of $X$ : given $R_{j}=r_{j}$ , for any $x_{\\backslash X_{j}}$ (value of the vector of elements in $X\\setminus\\{X_{j}\\},\\exists n_{j}$ with $p(n_{j}>0)$ ) (by the hypothesis of strictly positive densities of the noise terms) that satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\nr_{j}=n_{j}-{\\bf E}[N_{j}|x_{\\backslash{X_{j}}}].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "592 Next, we condition on all the parents of $X_{i}$ , except for $X_{j}$ , to reduce our problem to the simpler   \n593 bivariate case. Let $S\\subset\\mathbb{N}$ and $X_{S}\\ \\subseteq\\ X$ such that $X_{\\mathrm{PA}_{i}^{\\mathcal{G}}}\\setminus\\{X_{j}\\}\\subseteq X_{S}\\,\\subseteq\\,X_{\\mathrm{ND}_{i}^{\\mathcal{G}}}\\setminus\\{X_{i},X_{j}\\}$ ,   \n594 and consider $x_{S}$ such that $p(x_{S}\\;>\\;0)$ . Let $X_{\\mathrm{PA}_{i}^{\\mathcal{G}}}\\;=\\;x_{\\mathrm{PA}_{i}^{\\mathcal{G}}}$ hold under $X_{S}\\;=\\;x_{S}$ . We define   \n595 $X_{j_{|x_{s}}}\\,:=\\,X_{j}|(X_{S}\\,=\\,x_{S})$ , and similarly $X_{|{x_{s}}}\\;:=\\;X|(X_{S}\\;=\\;x_{S})$ . Being the SCM a restricted   \n596 additive noise model, by Definition 9, the triplet $(g_{i},p_{X_{j}}_{|_{x_{s}}},p_{N_{i}})$ satisfies Condition 1, where   \n597 $g_{i}(x_{j})=h_{i}(x_{\\mathrm{PA}_{i}^{\\mathcal{G}}\\backslash\\{X_{j}\\}},x_{j})$ . Consider $X_{i}=x_{i}$ , and the pair of values $(\\boldsymbol{x}_{j},\\boldsymbol{x}_{j}^{*})$ such that $x_{j}\\neq x_{j}^{*}$   \n598 and ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nu_{N_{i}}^{\\prime\\prime}(x_{i}-g_{i}(x_{j}))g_{i}^{\\prime}(x_{j})\\neq0,}\\\\ {\\nu_{N_{i}}^{\\prime\\prime}(x_{i}-g_{i}(x_{j}^{*}))g_{i}^{\\prime}(x_{j}^{*})\\neq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "599 where we resort to the usual notation $\\nu_{N_{i}}:=\\log p_{N_{i}}$ . By Lemma 2, $(x_{i},x_{j})$ and $(\\boldsymbol{x}_{i},\\boldsymbol{x}_{j}^{*})$ satisfy: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{X_{j}}(\\xi^{\\prime}(x_{j})-\\nu_{N_{i}}^{\\prime}(x_{i}-g_{i}(x_{j}))g_{i}^{\\prime}(x_{j}))\\neq0,}\\\\ &{\\partial_{X_{j}}(\\xi^{\\prime}(x_{j}^{*})-\\nu_{N_{i}}^{\\prime}(x_{i}-g_{i}(x_{j}^{*}))g_{i}^{\\prime}(x_{j}^{*}))\\neq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "600 where $\\xi:=\\log p_{X_{j}}_{|_{x_{s}}}$ . Thus, we can fix $x_{j}$ and $\\boldsymbol{x}_{j}^{*}$ (which are arbitrarily chosen) such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\partial_{X_{j}}(\\xi^{\\prime}(x_{j})-\\nu_{N_{i}}^{\\prime}(x_{i}-g_{i}(x_{j}))g_{i}^{\\prime}(x_{j}))-\\partial_{X_{j}}(\\xi^{\\prime}(x_{j}^{*})-\\nu_{N_{i}}^{\\prime}(x_{i}-g_{i}(x_{j}^{*}))g_{i}^{\\prime}(x_{j}^{*}))\\neq0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Fixing $X_{\\vert x_{S},x_{j}}=x$ and $X_{\\vert_{x_{S},x_{j}^{*}}}=x^{*}$ , where the two values differ only in their j- $^{t h}$ component, we find the following difference: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{:}(x)-s_{j}(x^{*})=\\partial_{X_{j}}\\big(\\xi^{\\prime}(x_{j})-\\nu_{N_{i}}^{\\prime}(x_{i}-g_{i}(x_{j}))g_{i}^{\\prime}(x_{j})\\big)-\\partial_{X_{j}}\\big(\\xi^{\\prime}(x_{j}^{*})-\\nu_{N_{i}}^{\\prime}(x_{i}-g_{i}(x_{j}^{*}))g_{i}^{\\prime}(x_{j}^{*})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "601 which is different from 0 by equation (21). This contradicts the fact that the score $s_{j}$ is constant once   \n602 $R_{j}$ is fixed, which proves our claim. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "603 B.3 Proof of Proposition 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "604 In this proof, we use several ideas from the demonstration of Proposition 2. We demonstrate the   \n605 forward and the backward parts of the two statements separately.   \n606 Proof of part $(i)$ , forward direction. Given $V_{Z}=V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\cup\\{V_{i},V_{j}\\}$ and $r_{j}\\in\\mathbb{R}$ in the image of $R_{j}$ ,   \n607 we want to show: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{\\mathrm{PA}_{j}^{G}}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{d}{g}U^{j}\\wedge V_{i}\\in V_{\\mathrm{PA}_{j}^{G}}\\Longrightarrow\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})-\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})=r_{j}]]^{2}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "608 By Lemma 4, the score of $V_{j}$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{V_{j}}\\log p(V_{Z})=\\partial_{V_{j}}\\log p(V_{j}|V_{\\mathrm{PA}_{j}^{\\mathcal{G}}})+\\partial_{V_{j}}\\log Q(V_{Z})}\\\\ &{\\qquad\\qquad\\qquad=\\log p(\\tilde{N}_{j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for some $Q$ map acting on $V_{Z}$ . The latter equality holds because all variables in $V_{Z}$ are nondescendants of $V_{j}$ , such that $\\partial_{V_{j}}Q(V_{Z})=0$ . Further, by equation (16) we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\nR_{j}(V_{Z})=\\tilde{N}_{j}+c,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $c=-\\mathbf{E}[\\tilde{N}_{j}]$ is a constant. It follows that the least square estimator of the score of $V_{j}$ from $R_{j}(V_{Z})$ satisfies the following equation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})]=\\mathbf{E}[\\partial_{V_{j}}\\log p(\\Tilde{N}_{j})|\\Tilde{N}_{j}]=\\partial_{V_{j}}\\log p(\\Tilde{N}_{j}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first equality holds because $\\mathbf{E}[\\cdot|\\tilde{N}_{j}]=\\mathbf{E}[\\cdot|\\tilde{N}_{j}+c]$ . Then, we find ", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})-\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})=r_{j}]]^{2}=\\mathbf{E}[\\partial_{V_{j}}\\log p(\\Tilde{N}_{j})-\\partial_{V_{j}}\\log p(\\Tilde{N}_{j})]^{2}=0,}\\end{array}$ 609 which is exactly our claim. ", "page_idx": 17}, {"type": "text", "text": "610 Proof of part $(i)$ , backward direction. Given $V_{Z}=V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\cup\\{V_{i},V_{j}\\},r_{j}\\in\\mathbb{R}$ in the image of $R_{j}$ , and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})-\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})=r_{j}]]^{2}=0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "611 we want to show that $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\perp\\!\\!\\!\\perp_{\\mathcal{G}}^{d}U^{j}\\wedge V_{i}\\,\\in V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}$ , meaning that there is a direct causal effect that   \n612 is not biased by unobserved variables. We provide the proof by contradiction, in analogy to the   \n613 demonstration of the backward direction of Proposition 2. ", "page_idx": 17}, {"type": "text", "text": "Let us introduce $s_{j}:\\mathbb{R}^{|V z|}\\to\\mathbb{R}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\ns_{j}(v_{Z})=\\partial_{V_{j}}\\log p(V_{Z}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The mean squared error equal to zero implies that $s_{j}(V_{Z})$ is constant in $V_{Z}$ , once $R_{j}$ is observed. By contradiction, we assume that $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}$ $^{d}_{\\mathcal{G}}U^{j}\\vee V_{i}\\notin V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}$ , and want to show that $s_{j}(V_{Z})$ is not constant in $V_{Z}$ , given $R_{j}$ fixed. In this regard, we make the following observation: by definition of residual in equation (15), $R_{i}(V_{Z})=r_{i}$ fixes the following distance: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{j}(V_{Z})=\\tilde{N}_{j}-{\\bf E}[\\tilde{N}_{j}|V_{Z\\backslash\\{j\\}}].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, conditioning on $R_{j}(V_{Z})$ doesn\u2019t restrict the support of $V_{Z}$ : given $R_{j}(V_{Z})=r_{j}$ , $\\exists\\tilde{n}_{j}$ with $p(\\tilde{n}_{j})>0$ (by assumption of strictly positive densities $p_{N_{j}}$ and $p_{X}$ ), that satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\nr_{j}=\\tilde{n}_{j}-\\mathbf{E}[\\tilde{N}_{j}|v_{Z\\backslash\\{j\\}}],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "614 for all $v_{Z\\setminus\\{j\\}}$ . Hence, the random variable $V_{Z}|R_{j}(V_{Z})=r_{j}$ has strictly positive density on all points   \n615 $v_{Z}$ where $p_{V_{Z}}(v_{Z})>0$ . Now, consider $v_{Z}$ and $v_{Z}^{*}$ , taken from the set of uncountable values such that   \n616 the score $s_{j}$ function is not a constant, meaning that $s_{j}(v_{Z})\\neq s_{j}(v_{Z}^{*})$ , where $V_{Z}$ is sampled given   \n617 $\\bar{R_{j}}({V_{Z}})=\\bar{r}_{j}$ . Given that different $v_{Z}$ and $v_{Z}^{*}$ are selected from an uncountable subset of the support,   \n618 we conclude that the score $s_{j}|(R_{j}(V_{Z})=r_{j}^{-})=\\partial_{V_{j}}\\log p(V_{Z}|R_{j}(V_{Z})=r_{j})$ is not a constant for at   \n619 least an uncountable set of points, which contradicts equation (22). \u53e3   \n620 Proof of part $(i i)$ , forward direction. Given that $V_{i}$ is connected to $V_{j}$ in the marginal MAG and that   \n621 $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}$ $^{d}_{\\mathcal{G}}U^{j}\\ \\vee V_{i}\\ \\not\\in\\ V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}$ , we want to show that for each $V_{Z}\\ \\subseteq\\ V$ with $\\{V_{i},V_{j}\\}\\subseteq V_{Z}$ , the   \n622 following holds: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})-\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})=r_{j}]]^{2}\\neq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let us introduce $h:\\mathbb{R}\\to\\mathbb{R}$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\bf E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})=r_{j}]=h(r_{j}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and further define: ", "page_idx": 18}, {"type": "equation", "text": "$$\ns_{j}(V_{Z})=\\partial_{V_{j}}\\log p(V_{Z}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Having the mean squared error in equation (23) equals zero implies that $s_{j}(V_{Z})$ is a constant, once $R_{j}(V_{Z})$ is observed. Thus, the goal of the proof is to show that there are values of $V_{Z}$ such that the score is not a constant once $R_{j}$ is fixed. By definition of residual in equation (15), $R_{j}(V_{Z})=r_{j}$ fixes the following distance: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{j}(V_{Z})=\\tilde{N}_{j}-{\\bf E}[\\tilde{N}_{j}|V_{Z\\backslash\\{j\\}}].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, conditioning on $R_{j}(V_{Z})$ doesn\u2019t restrict the support of $V_{Z}$ : given $\\begin{array}{r}{R_{j}(V_{Z})=r_{j},}\\end{array}$ $\\exists\\tilde{n}_{j}$ with $p(\\tilde{n}_{j})>0$ (by assumption of positive density of the noise $N_{j}$ on the support $\\mathbb{R}$ ), that satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\nr_{j}=\\tilde{n}_{j}-\\mathbf{E}[\\tilde{N}_{j}|v_{Z\\backslash\\{j\\}}],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "623 for all $v_{Z\\setminus\\{j\\}}$ . Hence, the random variable $V_{Z}|R_{j}(V_{Z})=r_{j}$ has strictly positive density on all points   \n624 $v_{Z}$ where $p_{V_{Z}}(v_{Z})>0$ . Now, consider $v_{Z}$ and $v_{Z}^{*}$ , taken from the set of uncountable values such that   \n625 the score $s_{j}$ function is not a constant, meaning that $s_{j}(v_{Z})\\neq s_{j}(v_{Z}^{*})$ , where $V_{Z}$ is sampled given   \n626 $\\bar{R_{j}}({V_{Z}})=\\bar{r}_{j}$ . Given that different $v_{Z}$ and $v_{Z}^{*}$ are selected from an uncountable subset of the support,   \n627 we conclude that the score $s_{j}|(R_{j}(V_{Z})=r_{j}\\stackrel{\\_}{=}\\partial_{V_{j}}\\log p(V_{Z}|R_{j}(V_{Z})=r_{j})$ is not a constant for at   \n628 least an uncountable set of points, such that the claim follows. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Proof of part $(i i)$ , backward direction. Given that $\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})-\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})\\,=\\,]$ $r_{j}]]^{2}\\neq0$ for all $V_{Z}\\subseteq V$ such that $\\{V_{i},V_{j}\\}\\in V_{Z}$ , and given $V_{i}$ and $V_{j}$ adjacent in the marginal MAG, we want to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\not\\perp\\!\\!\\!\\!/\\:_{\\mathcal{G}}^{d}U^{j^{j}}\\vee V_{i}\\not\\in V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "629 The prove comes easily by contradiction: say that $V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\perp\\!\\!\\!\\perp_{\\mathcal{G}}^{d}U^{j}\\wedge V_{i}\\in V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}$ . Then, by the forward   \n630 direction of part $(i)$ of Proposition 3, we know that $V_{Z}^{\\'}=V_{\\mathrm{PA}_{j}^{\\mathcal{G}}}\\cup\\{V_{j}\\}$ satisfies ${\\bf E}[\\partial_{V_{j}}\\log p(V_{Z})-$   \n631 $\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})=r_{j}]|^{2}=0$ , leading to a contradiction. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "632 C Algorithm ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "633 C.1 Detailed description of our algorithm ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "634 In Proposition 1 we have seen that score matching can detect $m$ -separations and therefore the skeleton   \n635 of the PAG describing the data. If one is willing to make the assumptions required for Proposition 3   \n636 it could be desirable to use this to orient edges, since the interpretation of PAG edges might be   \n637 cumbersome for people not familiar with ancestral models. Therefore, one could simply find the   \n638 skeleton of the PAG using the fast adjacency search [5] and then orient the edges by applying   \n639 Proposition 3 on every subset of the neighbourhood of every node. This would yield a very costly   \n640 algorithm. But if we make the assumptions required to orient edges with Proposition 3 we can do a   \n641 bit better. In Algorithm 2 we present an algorithm that still has the same worst case runtime but runs   \n642 polynomially in the best case. The main intuition is that we iteratively remove irrelevant nodes in the   \n643 spirit of the original SCORE algorithm [15]. To this end, we first check if the is any unconfounded   \n644 sink if we consider the set of all remaining variables. If there is one, we can orient its parents and   \n645 ignore it afterwards. If there is no such set, we need to fall back to the procedure proposed above, i.e.   \n646 we need to check the condition of Proposition 3 on all subsets of the neighbourhood of a node, until   \n647 we find no node with a direct outgoing edge. In Proposition 4 we show that this way we do not fail   \n648 orient edge or fail to remove any adjacency. In the following discussion, we will use the notation ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta_{i}(X_{Z}):=\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})-\\mathbf{E}[\\partial_{V_{j}}\\log p(V_{Z})|R_{j}(V_{Z})=r_{j}]]^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "649 for the second residual from Proposition 3 and also ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta_{i,j}(X_{Z}):=\\frac{\\partial^{2}}{\\partial V_{i}\\partial V_{j}}\\log p(v_{Z})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "650 for the cross-partial derivative, where $X_{i},X_{j}\\in V$ and $Z\\subseteq V$ . ", "page_idx": 19}, {"type": "text", "text": "651 Proposition 4 (Correctness of algorithm). Let $X=V\\dot{\\cup}U$ be generated by the SCM in Equation (4)   \n652 with non-constant scores for uncountably many values. Let $g_{X}$ be the causal $D A G$ of $X$ and ${\\mathcal{G}}_{V}$ be   \n653 the marginal MAG of $g_{X}$ . Then Algorithm 2 outputs a directed edge from $X_{i}\\in V$ to $X_{j}\\in V$ iff   \n654 there is a direct edge in $g_{X}$ between them and no unobserved backdoor path w.r.t. $U$ . Further, the   \n655 output of Algorithm 2 has the same skeleton as ${\\mathcal{G}}_{V}$ .   \n656 Proof. We proof the statement by induction over the steps of the algorithm. Let $S$ be the set of   \n657 remaining nodes in an arbitrary step of the algorithm. Our induction hypothesis is that for $X_{i},X_{j}\\in S$   \n658 and $X_{k}\\in B_{i}$ we have   \n659   \n660 ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": ". $X_{i}$ is an unconfounded sink w.r.t. to some set $S^{\\prime}\\subseteq S$ iff $X_{i}$ is an unconfounded sink w.r.t. some $S^{\\prime\\prime}\\subseteq V$ ", "page_idx": 19}, {"type": "text", "text": "661 ", "page_idx": 19}, {"type": "text", "text": "662 Clearly, this holds in the initial step as $S=V$ . ", "page_idx": 19}, {"type": "text", "text": "663 Suppose we find $\\delta_{i}(X_{S})\\;=\\;0$ for $X_{i}~\\in~S$ . If $X_{i}$ has at least one adjacent node in $\\mathcal{M}_{V}^{\\mathcal{G}}$ , by   \n664 Proposition 3, we know that $X_{i}$ does not have any children and is also not connected to any other   \n665 node in $S$ via a hidden mediator or unobserved confounder. This means, all nodes that are not   \n666 separable from $X_{i}$ must be direct parents of $X_{i}$ , which are by our induction hypothesis 2) the nodes   \n667 in $B_{i}$ . Since $X_{i}$ does not have children, it also suffices to check $X_{i}\\perp\\!\\!\\!\\perp X_{j}|S\\setminus\\{X_{i},X_{j}\\}$ for $X_{j}\\in B_{i}$   \n668 (instead of conditioning on all subsets of $B_{i}$ ). So we can already add these direct edges to the output.   \n669 If, on the other hand, $X_{i}$ has no adjacent nodes in $\\mathcal{M}_{V}^{\\mathcal{G}}$ , we have $X_{i}\\perp\\!\\!\\!\\perp X_{j}|S\\backslash\\{X_{i},X_{j}\\}$ for $X_{j}\\in B_{i}$ ,   \n670 so in both cases we add the correct set of parents. Since $X_{i}$ is not an ancestor of any of the nodes in   \n671 $S\\setminus\\{X_{i}\\}$ , $X_{i}$ cannot be a hidden mediator or hidden confounder between nodes in $S\\setminus\\{X_{i}\\}$ and   \n672 conditioning on $X_{i}$ cannot block an open path. Thus, the induction hypothesis still holds in the next   \n673 step.   \n674 Suppose now there is no unconfounded sink and we explore $X_{i}$ . By our induction hypothesis 2), $B_{i}$   \n675 contains the parents of $X_{i}$ and by Proposition 3 it suffices to only look at subsets of $B_{i}$ to orient direct   \n676 edges. And also due to the induction hypothesis 2) $B_{i}$ contains all nodes that are not separable from   \n677 $X_{i}$ . So by adding bidirected edges to all nodes in $B_{i}$ can only add too many edges but not miss some.   \n678 Now it remains to show that the induction hypothesis holds if we set $S$ to $S\\setminus\\{X_{i}\\}$ . For 1) we need   \n679 to show that $X_{i}$ cannot be a hidden mediator or hidden confounder w.r.t. $S\\setminus\\{X_{i}\\}$ (since ignoring   \n680 $X_{i}$ won\u2019t change whether there is a direct edge or not). Suppose $X_{i}$ is on a unobserved causal path   \n681 $X_{k}\\to\\cdots\\to U^{m}\\to X_{l}$ with $X_{k},X_{l}\\in S\\setminus\\bar{\\{X_{i}\\}}$ and $U^{m}\\,\\bar{\\in}\\,X\\setminus(S\\setminus\\{X_{i}\\})$ . This path must have   \n682 been a unobserved causal path before, unless $X_{i}=U^{m}$ . But then there is a direct edge $X_{i}\\to X_{l}$ .   \n683 We would not remove $X_{i}$ from $S$ if this edge was unconfounded, so there must a hidden confounder   \n684 between $X_{i}$ and $X_{l}$ . But in this case, Proposition 3 wouldn\u2019t allow us to direct the edge anyway, since   \n685 $V_{\\mathrm{PA}_{l}}\\nperp\\!\\!\\!\\perp_{\\mathcal{G}}^{d}U_{l}$ . Suppose there is confounding path $X_{k}\\gets\\cdot\\cdot\\cdot\\rightarrow U^{m}\\rightarrow X_{l}$ with $X_{k},X_{l}\\in S\\setminus\\{X_{i}\\}$   \n686 and $\\bar{U}^{m}\\overset{\\mathcal{\\sigma}}{\\in}X\\setminus\\bar{(S\\setminus\\{X_{i}\\})}$ . If $X_{i}\\neq U^{m}$ the path was already been a confounding path without $X_{i}$   \n687 being unobserved. So again, there must be a confounder between $X_{i}$ and $X_{l}$ , as otherwise we would   \n688 not remove $X_{i}$ . And analogously to before, we could not have oriented the edge even with $X_{i}\\in S$   \n689 since $V_{\\mathrm{PA}_{l}}$ \u0338 ${}_{\\mathcal{G}}^{d}U_{l}$ . For 2) we only have to see that we just remove nodes from $B_{i}$ if we found an   \n690 independence.   \n691 For $|S|<2$ , the algorithm enters the final pruning stage. From the discussion above it is clear,   \n692 that we already have the correct result, up to potentially too many bidirected edges. In the final   \n693 step we certainly remove all these edges $X_{i}\\leftrightarrow X_{j}$ , as we check $m$ -separation for all subsets of the   \n694 neighbourhoods $\\operatorname{Adj}(X_{i})$ and $\\mathrm{Adj}(\\bar{X_{j}})$ , which are supersets of the true neighbourhoods.   \n697 All theoretical results in the paper have assumed that we know the density of our data. Obviously, in   \n698 practise we have to deal with a finite sample instead. Especially, in Proposition 1 and Proposition 3   \n699 we derived criteria that compare random variables with zero. Clearly, this condition is never met in   \n700 practise. Therefore, we need find ways to reasonably set thresholds for these random quantities.   \n701 First note, that we use the Stein gradient estimator [35] to estimate the score function. This means   \n702 especially that for a node $V_{i}$ we get a vector ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "oRZN9a53ag/tmp/f1d771251af7c4ebae72262bac4d132e5bfb904380bd4b47be05e940c0e07d71.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left((\\frac{\\partial}{\\partial V_{i}}\\log p(v))_{l}\\right)_{l=1,\\dots,m},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "703 i.e. an estimate of the score for every one of the $m$ samples. Analogously, we get a $m\\times d\\times d$ tensor   \n704 for the estimates of\u2202V\u2202i\u22022V j log p(v). ", "page_idx": 21}, {"type": "text", "text": "705 In Proposition 1 we showed that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial V_{i}\\partial V_{j}}\\log p(v_{Z})=0\\iff X_{i}\\bot\\Pi_{\\mathcal{M}_{V}^{G}}^{m}V_{j}|V_{Z}\\setminus\\{V_{i},V_{j}\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "706 In the finite sample version, we use a one sample t-test on the vector of estimated cross-partial   \n707 derivatives with the null-hypothesis that the means is zero. Due to the central limit theorem, the   \n708 sample mean follows approximately a Gaussian distribution, regardless of the true distribution of the   \n709 observations.   \n710 For Proposition 3 we need to do some additional steps. Recall, that the relevant quantity in Propo  \n711 sition 3 is the mean squared error of a regression, which is always positive. Therefore, a test for   \n712 mean zero is highly likely to reject in any case. We decided to employ a two-sample test in a similar   \n713 (but different) manner as Montagna et al. [17]. As test, we used the Mann-Whitney U-test. Note,   \n714 that Algorithm 2 employs Proposition 3 in two different ways: first, to decide whether there is an   \n715 unconfounded sink and second, to orient edges in case there is no unconfounded sink. We pick a   \n716 different sample as second sample of the Mann-Whitney U-test. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "717 Analogously to before, this is a vector with $m$ entries, one for every sample. ", "page_idx": 21}, {"type": "text", "text": "718 Note, that in the case where we want to check if there is an unconfounded sink, we do not make any   \n719 mistake by rejecting too few hypotheses, i.e. if we miss some unconfounded sinks (instead, we only   \n720 lose efficiency, as we do the costly iteration over all possible sets of parents). Therefore, for this test   \n721 we chose a a second sample that yields a \u201cconservative\u201d test result.   \n722 As candidate sink for set $S\\subseteq V$ , we pick the node $X_{i}=\\operatorname*{min}_{i}\\operatorname*{mean}(\\delta_{i}(X_{S}))$ . In fact, we want to   \n723 know whether the mean of $\\delta_{i}$ is significantly lower than all other means. But we empirically observed   \n724 that choosing the concatenated $\\delta\\mathbf{s}$ of all nodes as second sample makes the test reject with very high   \n725 probability, which would lead our algorithm to falsely assume the existence of an unconfoudned sink.   \n726 Instead, we then pick as second \u201creference node\u201d $\\begin{array}{r}{X_{j}=\\operatorname*{min}_{j\\neq i}\\operatorname*{mean}(\\delta_{j}(X_{Z}))}\\end{array}$ . We then do the two   \n727 sample test between $\\delta_{i}(X_{Z})$ and $\\delta_{j}(X_{Z})$ . The intuition is that the test will reject the hypothesis of   \n728 identical means, if $X_{i}$ is an unconfounded sink but $X_{j}$ is not.   \n729 In the case where we use Proposition 3 to orient edges, we only need to decide whether an not   \n730 previsouly directed edge $X_{i}-X_{j}$ needs to be oriented one way, the other way, or not at all. Instead,   \n731 here the issue lies in the fact that we need to iterate over possible sets of parents of the nodes. Let   \n732 $B_{i}$ be the set of nodes that have not been $m$ -separated from $X_{i}$ by any test so far. We pick the   \n733 subset $Z_{i}=\\operatorname*{min}_{Z^{\\prime}\\subseteq B_{i}}$ $\\mathrm{mean}(\\delta_{i}^{Z^{\\prime}})$ , i.e. the set with the lowest mean error. We then conduct the test   \n734 with $\\delta_{i}(X_{Z_{i}})$ and $\\bar{\\delta_{j}}(X_{Z_{j}})$ . If there is a directed edge between them, one of the residuals will be   \n735 significantly lower than the other.   \n736 Just like Montagna et al. [17] we use a cross-validation scheme to generate the residuals, in order to   \n737 prevent overfitting. We split the dataset into several equally sized, disjoint subsamples. For every   \n738 residual we fit the regression on all subsamples that don\u2019t contain the respective target.   \n739 Also, just like in the NoGAM algorithm Montagna et al. [17] we add a pruning step for the directed   \n740 edges to the end. The idea is to use a feature selection method to remove insignificant edges. Just like   \n741 Montagna et al. [17], we use the CAM-based pruning step proposed by B\u00fchlmann et al. [36], which   \n742 fits a generalised additive regression model from the parents to a child and test whether one of the   \n743 additive components is significantly non-zero. All parents for which the test rejects this hypothesis   \n744 are removed. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "745 C.3 Complexity ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "746 Proposition 5. Complexity Let n be the number of samples and d the number of observable nodes.   \n747 Algorithm 2 runs in ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Omega\\left((d^{2}-d)\\cdot(r(n,d)+s(n,d))\\right)\\quad\\;a n d\\quad\\mathcal{O}\\left(d^{2}\\cdot2^{d}(r(n,d)+s(n,d))\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "748 where $r(n,d)$ is the time required to solve a regression problem and $s(n,d)$ is the time for calculating   \n749 the score. With e.g. kernel-ridge regression and the Stein-estimator, both run in ${\\mathcal{O}}(n^{3})$ .   \n750 Proof. Algorithm 2 runs its main loop $d$ times. It first checks for the existence of an unconfounded   \n751 sink, which involves solving $2d$ regression problems (including cross-validation prediction) and   \n752 calculating the score, adding up to $\\bar{(2d^{2}-d)}$ regressions and $d$ score evaluations. In the worst case,   \n753 we detect no unconfounded sink and iterate through all subsets of the neighbourhood of a node   \n754 (which is in the worst case of size $d-1,$ ) and for all other nodes in the neighbourhood we solve $2d$   \n755 regression problems and evaluate the score. For each subset we calculate two regression functions,   \n756 the score and calculate the entries in the Hessian of the log-density, i.e. $d\\cdot2^{d}$ regressions, $d\\cdot2^{d-1}$   \n757 scores and additionally $2^{d-1}$ Hessians. If we are unlucky, this node has a directed outgoing edge   \n758 and we continue with this node (with the same size of nodes). This can happen $d-1$ times. So we   \n759 get $(d^{2}-d)\\cdot2^{d}$ regressions and $(d^{2}-d)\\cdot2^{d-1}$ scores and Hessians. In the final pruning step we   \n760 calculate for every bidirected edge (of which there can be $(d^{2}-d)/2)$ a Hessian for all subsets of the   \n761 neighbourhoods, which can again be $2^{d-1}$ subsets. Using the pruning procedure from CAM for the   \n762 directed edges we also spend at most $O(n d^{3})$ steps.   \n763 In the best case, we always find an unconfounded sink. Then our algorithm reduces to NoGAM.   \n764 ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "765 D Experimental details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "766 In this section, we present the details of our experiments in terms of synthetic data generation and   \n767 algorithms hyperparameters. ", "page_idx": 22}, {"type": "text", "text": "768 D.1 Synthetic data generation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "769 In this work, we rely on synthetic data to benchmark AdaScore\u2019s finite samples performance. For   \n770 each dataset, we first sample the ground truth graph and then generate the observations according to   \n771 the causal graph.   \n772 Erd\u00f6s-Renyi graphs. The ground truth graphs are generated according to the Erd\u00f6s-Renyi model.   \n773 It allows specifying the number of nodes and the probability of connecting each pair of nodes). In ER   \n774 graphs, a pair of nodes has the same probability of being connected.   \n775 Nonlinear causal mechanisms. Nonlinear causal mechanisms are parametrized by a neural network   \n776 with random weights. We create a fully connected neural network with one hidden layer with 10   \n777 units, Parametric ReLU activation function, followed by one normalizing layer before the final fully   \n778 connected layer. The weights of the neural network are sampled from a standard Gaussian distribution.   \n779 This strategy for synthetic data generation is commonly adopted in the literature [26, 18, 28, 29, 27].   \n780 Linear causal mechanisms. For the linear mechanisms, we define a simple linear regression model   \n781 predicting the effects from their causes and noise terms, weighted by randomly sampled coefficients.   \n782 Coefficients are generated as samples from a Uniform distribution supported in the range $[-3,-0.5]\\cup$   \n783 [0.5, 3]. We avoid too small coefficients to avoid close to unfaithful datasets Uhler et al. [24].   \n784 Noise terms distribution. The noise terms are sampled from a Uniform distribution supported   \n785 between $-2$ and 2.   \n786 Finally, we remark that we standardize the data by their empirical data. This is known to remove   \n787 shortcuts that allow finding a correct causal order sorting variables by their marginal variance, as in   \n788 varsortability, described in Reisach et al. [32], or sorting variables by the magnitude of their score   \n789 $|\\partial_{X_{i}}\\log p({\\dot{X_{}}})|$ , a phenomenon known as scoresortability analyzed by Montagna et al. [18]. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "790 D.2 AdaScore hyperparameters ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "791 For AdaScore, we set the $\\alpha$ level for the required hypothesis testing at 0.05. For the CAM-pruning   \n792 step, the level is instead set at 0.001, the default value of the dodidscover Python implementation of   \n793 the method, and commonly found in all papers using CAM-pruning for edge selection [15, 16, 17, 36].   \n794 For the remaining parameters. The regression hyperparameters for the estimation of the residuals are   \n795 found via cross-validation during inference: tuning is done minimizing the generalization error on   \n796 the estimated residuals, without using the performance on the causal graph ground truth. Finally, for   \n797 the score matching estimation, the regularization coefficients are set to 0.001. ", "page_idx": 23}, {"type": "text", "text": "798 D.3 Computer resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "799 All experiments have been run on an AWS EC2 instance of type p3.2xlarge. These machines   \n800 contain Intel Xeon E5-2686-v4 processors with $2.3\\:\\mathrm{GHz}$ and 8 virtual cores as well as 61 GB RAM.   \n801 All experiments can be run within a day. ", "page_idx": 23}, {"type": "text", "text": "802 E Additional Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "803 In this section, we provide additional experimental results. All synthetic data has been generated as   \n804 described in Appendix D.1. ", "page_idx": 23}, {"type": "text", "text": "805 E.1 Non-additive mechanisms ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "806 In Figure 1 we have demonstrated the performance of our proposed method on data generated by   \n807 linear SCMs and non-linear SCMs with additive noise. But Proposition 1 also holds for any faithful   \n808 distribution generated by an acyclic model. Thus, we employed as mechanism a neural network-based   \n809 approach similar to the non-linear mechanism described in Appendix D. Instead of adding the noise   \n810 term, we feed it as additional input into the neural network. Results in this setting are reported in   \n811 Figure 2. As neither AdaScore nor any of the baseline algorithms has theoretical guarantees for the   \n812 orientation of edges in this scenario, we report the $F_{1}$ -score (popular in classification problems) w.r.t.   \n813 to the existence of an edge, regardless of orientation. Our experiments show that AdaScore can, in   \n814 general, correctly recover the graph\u2019s skeleton in all the scenarios, with an $F_{1}$ score median between   \n815 1 and $\\sim0.75$ , respectively for small and large numbers of nodes. ", "page_idx": 23}, {"type": "text", "text": "816 E.2 Sparse graphs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "817 In this section, we present the experiments on sparse Erd\u00f6s-Renyi graphs where each pair of nodes   \n818 is connected by an edge with probability 0.3. The results are illustrated in Figure 3. For sparse   \n819 graphs, recovery results are similar to the dense case, with AdaScore generally providing comparable   \n820 performance to the other methods. ", "page_idx": 23}, {"type": "text", "text": "821 E.3 Increasing number of samples ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "822 In the following series of plots we demonstrate the scaling behaviour of our method w.r.t. to the   \n823 number of samples. Figure 5 shows results with edge probability 0.5 and Figure 4 with 0.3. All   \n824 graphs contain seven observable nodes. As before we observe that AdaScore performs comparably to   \n825 other methods. E.g. in Figures 4a and 5b we can see that the median error AdaScore improves with   \n826 additional samples and in all plots we see that no other algorithm seems to gain an advantage over   \n827 AdaScore with increasing sample size. ", "page_idx": 23}, {"type": "image", "img_path": "oRZN9a53ag/tmp/65cfd3f8d48d84aced3315466389eb25ab79008d3c882b980f544599e45f1855.jpg", "img_caption": ["(a) Fully observable model "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "oRZN9a53ag/tmp/fe0680a8d82aee52f9eab9d832fd145752eb26b9beb59dbc9e65361c873cdcbe.jpg", "img_caption": ["(b) Latent variables model "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 2: Empirical results for non-additive causal mechanisms on sparse graphs with different numbers of nodes, on fully observable (no hidden variables) and latent variable models. We report the $F_{1}$ score w.r.t. the existence of edges (the higher, the better). ", "page_idx": 24}, {"type": "image", "img_path": "oRZN9a53ag/tmp/8122584153a38da33b62f1fcbb116d8b052673c842e662429240e2f6e01117f9.jpg", "img_caption": ["(a) Fully observable model "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "oRZN9a53ag/tmp/a9aba7b7f2f21f2119feae36916cf37bc2b59e7c04f5c356afa3f8e332e45a62.jpg", "img_caption": ["(b) Latent variables model "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 3: Empirical results on sparse graphs with different numbers of nodes, on fully observable (no hidden variables) and latent variable models. We report the SHD accuracy (the lower, the better). ", "page_idx": 24}, {"type": "image", "img_path": "oRZN9a53ag/tmp/3eecfe00f3bec9165a27d3bca985c011ba725c2755b677da33a7da20a7621c2f.jpg", "img_caption": ["(a) Fully observable model "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "oRZN9a53ag/tmp/1b0206524dd99ff4d6400243c70652e6987c12cf3840f693fa35acd3db51d1f3.jpg", "img_caption": ["(b) Latent variables model "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 4: Empirical results on sparse graphs with different numbers of samples and seven nodes, on fully observable (no hidden variables) and latent variable models. We report the SHD accuracy (the lower, the better). ", "page_idx": 25}, {"type": "image", "img_path": "oRZN9a53ag/tmp/49e3772894325099bd946261285bd5f26b9ba62325e9b243943a1a636568c650.jpg", "img_caption": ["(a) Fully observable model "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "oRZN9a53ag/tmp/675dc6bb75b59aedb092ee1950712868222604c87aa7285b8eff7d519120715e.jpg", "img_caption": ["(b) Latent variables model "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 5: Empirical results on dense graphs with different numbers of samples and seven nodes, on fully observable (no hidden variables) and latent variable models. We report the SHD accuracy (the lower, the better). ", "page_idx": 25}, {"type": "text", "text": "828 E.4 Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "829 In this section, we remark the limitations of our empirical study. It is well known that causal discovery   \n830 lacks meaningful, multivariate benchmark datasets with known ground truth. For this reason, it is   \n831 common to rely on synthetically generated datasets. We believe that results on synthetic graphs should   \n832 be taken with care, as there is no strong reason to believe that they should mirror the benchmarked   \n833 algorithms\u2019 behaviors in real-world settings, where often there is no prior knowledge about the   \n834 structural causal model underlying available observations.   \n836 The checklist is designed to encourage best practices for responsible machine learning research,   \n837 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n838 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n839 follow the references and precede the (optional) supplemental material. The checklist does NOT   \n840 count towards the page limit.   \n841 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n842 each question in the checklist:   \n843 \u2022 You should answer [Yes] , [No] , or [NA] .   \n844 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n845 relevant information is Not Available.   \n846 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n847 The checklist answers are an integral part of your paper submission. They are visible to the   \n848 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n849 (after eventual revisions) with the final version of your paper, and its final version will be published   \n850 with the paper.   \n851 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n852 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n853 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n854 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n855 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n856 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n857 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n858 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n859 please point to the section(s) where related material for the question can be found.   \n861 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n862 paper\u2019s contributions and scope?   \n863 Answer: [Yes]   \n864 Justification: In the abstract, we claim that we connect the properties of the score function   \n865 to causal structure learning. In the paper, particularly sections 3 and 4, we present the   \n866 theoretical results supporting our claim. Further, in the abstract we mention that based on   \n867 our theory we propose an algorithm for causal discovery from score matching estimation,   \n868 algorithm that we define in Section 4.3 and we empirically validate in Section 5 and   \n869 Appendix E.   \n870 Guidelines:   \n871 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n872 made in the paper.   \n873 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n874 contributions made in the paper and important assumptions and limitations. A No or   \n875 NA answer to this question will not be perceived well by the reviewers.   \n876 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n877 much the results can be expected to generalize to other settings.   \n878 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n879 are not attained by the paper.   \n880 2. Limitations   \n881 Question: Does the paper discuss the limitations of the work performed by the authors?   \n882 Answer: [Yes]   \n883 Justification: The main limitation of our work is on the experimental side: our experiments   \n884 are limited to synthetic data, which are not an ideal probing ground. Additionally, our   \n85 method does not provide performance that clearly improves on the existing literature. These   \n86 limitations of our work are discussed in the discussion of the experiments in Section 5, as   \n887 well as in the \"Limitations\" appendix section E.4. Concerning the assumptions required by   \n88 our method, we thoroughly discuss them in the theoretical sections of the paper, where we   \n89 define the results that are later used for the definition of the AdaScore. Finally, computational   \n90 complexity is discussed in Appendix C.3.   \n891 Guidelines:   \n92 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n893 the paper has limitations, but those are not discussed in the paper.   \n94 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n95 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n96 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n97 model well-specification, asymptotic approximations only holding locally). The authors   \n98 should reflect on how these assumptions might be violated in practice and what the   \n99 implications would be.   \n00 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n01 only tested on a few datasets or with a few runs. In general, empirical results often   \n02 depend on implicit assumptions, which should be articulated.   \n03 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n904 For example, a facial recognition algorithm may perform poorly when image resolution   \n05 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n06 used reliably to provide closed captions for online lectures because it fails to handle   \n07 technical jargon.   \n08 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n09 and how they scale with dataset size.   \n10 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n911 address problems of privacy and fairness.   \n12 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n13 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n14 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n15 judgment and recognize that individual actions in favor of transparency play an impor  \n16 tant role in developing norms that preserve the integrity of the community. Reviewers   \n17 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: All our theoretical results make explicit the assumptions for which they are valid. Plus, we in section Appendix B we provide the proofs of our theoretical results. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "935 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: In Appendix D, we provide all the details to reproduce the data generation of our experiments and the hyperparameters used in AdaScore for our experimental runs. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. \u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. \u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "939   \n940   \n941   \n942   \n943   \n944   \n945   \n946   \n947   \n948   \n949   \n950   \n951   \n952   \n953   \n954   \n955   \n956   \n957   \n958   \n959   \n960   \n961   \n962   \n963   \n964   \n965   \n966   \n967   \n968   \n969   \n970   \n971   \n972   \n973   \n974   \n975   \n976   \n977   \n978   \n979   \n980   \n981   \n982   \n983   \n984   \n985   \n986   \n987   \n988   \n989   \n990   \n991   \n992   \n993 ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We provide the code for the experiments and the data generation in a zip file. Further, we describe all the details for reproducing our experimental results in Appendix D. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In our experiments section 5, we present all the necessary details on the data generation procedure and a description of the empirical results that are necessary for understanding our findings. Additionally, a comprehensive overview of our experimental design is presented in Appendix D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "994   \n995   \n996   \n997   \n998   \n999   \n1000   \n1001   \n1002   \n1003   \n1004   \n1005   \n1006   \n1007   \n1008   \n1009   \n1010   \n1011   \n1012   \n1013   \n1014   \n1015   \n1016   \n1017   \n1018   \n1019   \n1020   \n1021   \n1022   \n1023   \n1024   \n1025   \n1026   \n1027   \n1028   \n1029   \n1030   \n1031   \n1032   \n1033   \n1034   \n1035   \n1036   \n1037   \n1038   \n1039   \n1040   \n1041   \n1042   \n1043   \n1044   \n1045 ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We report all our experimental results in the form of boxplots. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do not believe any of the concerns in the Code of Ethics apply to our work. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Justification: In this work, we present a novel causal discovery method from observational data. We believe there are no specific negative societal impacts, while positive impacts are those generally recognized to causal discovery, as discussed in the Introduction section 1. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "1099 11. Safeguards   \n1100 Question: Does the paper describe safeguards that have been put in place for responsible   \n1101 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1102 image generators, or scraped datasets)?   \n1103 Answer: [NA]   \n1104 Justification: We release a causal discovery model from observational data which does not   \n1105 poses such risks.   \n1106 Guidelines:   \n1107 \u2022 The answer NA means that the paper poses no such risks.   \n1108 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1109 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1110 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1111 safety filters.   \n1112 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1113 should describe how they avoided releasing unsafe images.   \n1114 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1115 not require this, but we encourage authors to take this into account and make a best   \n1116 faith effort.   \n1117 12. Licenses for existing assets   \n1118 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1119 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1120 properly respected?   \n1121 Answer: [Yes]   \n1122 Justification: We use our proprietary assets, as well as public assets available under MIT   \n1123 license, which we correctly cite in our work.   \n1124 Guidelines:   \n1125 \u2022 The answer NA means that the paper does not use existing assets.   \n1126 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1127 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1128 URL.   \n1129 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1130 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1131 service of that source should be provided.   \n1132 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1133 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1134 has curated licenses for some datasets. Their licensing guide can help determine the   \n1135 license of a dataset.   \n1136 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1137 the derived asset (if it has changed) should be provided.   \n1138 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1139 the asset\u2019s creators.   \n1140 13. New Assets   \n1141 Question: Are new assets introduced in the paper well documented and is the documentation   \n1142 provided alongside the assets?   \n1143 Answer: [Yes]   \n1144 Justification: We release the code for AdaScore, submitted in the form of a zip flie containing   \n1145 the necessary documentation for usage. Moreover, an extensive description of the data   \n1146 generation is provided in the paper, as well as a description of the method itself.   \n1147 Guidelines:   \n1148 \u2022 The answer NA means that the paper does not release new assets.   \n1149   \n1150   \n1151   \n1152   \n1153   \n1154   \n1155   \n1156   \n1157   \n1158   \n1159   \n1160   \n1161   \n1162   \n1163   \n1164   \n1165   \n1166   \n1167   \n1168   \n1169   \n1170   \n1171   \n1172   \n1173   \n1174   \n1175   \n1176   \n1177   \n1178   \n1179   \n1180   \n1181   \n1182   \n1183   \n1184   \n1185   \n1186   \n1187   \n1188   \n1189 ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. \u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. \u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.   \n14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not work with human subjects or crowdsourcing. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.   \n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not work with human subjects or crowdsourcing. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]