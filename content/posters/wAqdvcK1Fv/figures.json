[{"figure_path": "wAqdvcK1Fv/figures/figures_3_1.jpg", "caption": "Figure 1: Visualisation of a typical state space of a tabular dataset: Numerical entries taking values in R<sup>d</sup>, cyclical categorical entries (e.g. season), ordinal categorical entries (e.g. age), unstructured categorical entries, and variables with an absorbing state associated with masking the entry.", "description": "This figure illustrates different types of state spaces that can be represented in a tabular dataset.  It shows how numerical variables (represented by a continuous space R<sup>d</sup>), cyclical categorical variables (like seasons), ordinal categorical variables (like age), and unstructured categorical variables can all be included in a single dataset. The figure also shows how an \"absorbing state\" can be used to indicate a missing or masked value.  The different graph structures illustrate how various dependencies among variables can be represented.", "section": "3.1 Heat Equation in Structured Discrete Spaces"}, {"figure_path": "wAqdvcK1Fv/figures/figures_6_1.jpg", "caption": "Figure 2: Comparison of energy discrepancy and contrastive divergence on the dataset with 16 dimensions and 5 states. Rows 1 and 2 show the estimated density and synthesised samples, respectively.", "description": "This figure compares the performance of energy discrepancy (ED) and contrastive divergence (CD) in estimating a probability distribution with 16 dimensions and 5 states.  The top row shows the true data distribution (Data), the ED estimated density, and the CD estimated density. The bottom row displays samples generated from the ED and CD models, respectively. The figure visually demonstrates that energy discrepancy produces a more accurate estimate of the density and generates samples that better reflect the true distribution's characteristics.", "section": "6.1 Discrete Density Estimation"}, {"figure_path": "wAqdvcK1Fv/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison of the energy discrepancy and contrastive divergence on the synthetic tabular datasets.", "description": "This figure shows a comparison of samples generated using contrastive divergence (CD) and energy discrepancy (ED) methods on a synthetic tabular dataset.  The leftmost panel shows the original data distribution, which exhibits a clear ring structure with distinct color clusters within each ring. The middle panel shows samples generated using contrastive divergence. These samples are dispersed more randomly, indicating that the CD method is not capturing the underlying structure of the data as effectively. The rightmost panel shows samples generated using energy discrepancy. These samples are more concentrated around the ring structure and maintain the color clustering more accurately, showing that the ED method successfully captures the data's underlying structure. This visual comparison illustrates the superior performance of energy discrepancy in generating realistic samples from complex data distributions.", "section": "6.2 Tabular Data Synthesising"}, {"figure_path": "wAqdvcK1Fv/figures/figures_8_1.jpg", "caption": "Figure 4: Calibration results comparison between the baseline (left) and energy discrepancy (right) on the adult dataset.", "description": "This figure compares the calibration results of two classification models: a baseline model (left) and a model trained using energy discrepancy (right), both applied to the 'adult' dataset.  The Expected Calibration Error (ECE) plots show that the model trained with energy discrepancy exhibits better calibration than the baseline model, evidenced by a lower ECE (2.62% vs. 1.03%). This suggests that the energy discrepancy method produces more reliable confidence scores in its predictions.", "section": "Improving Calibration"}, {"figure_path": "wAqdvcK1Fv/figures/figures_18_1.jpg", "caption": "Figure 5: Scaling limit of the introduced perturbations. Top: Convergence of rescaled cyclical and ordinal perturbations ys2t/S for base time parameters t = 0.01 and t = 0.05 to Gaussian [0, 1) with non-trivial boundary conditions. One can see that the perturbation converges to a fixed shape on the normalised state space. Bottom: Convergence of rescaled cyclical and ordinal perturbation (yst - E[yst])/\u221aS for base time parameters t = 0.1 and t = 0.5 to Gaussian on R (red line). The orange mark indicates the initial state. One can see that the perturbation remains non-trivial as the state space grows to infinity at rate \u221aS.", "description": "This figure shows the convergence of cyclical and ordinal perturbations to a Gaussian distribution as the state space size increases. The top row shows the convergence of the rescaled perturbations to a fixed shape on the normalized state space for different base time parameters. The bottom row shows the convergence of the rescaled and centered perturbations to a Gaussian distribution on R.", "section": "3.1 Heat Equation in Structured Discrete Spaces"}, {"figure_path": "wAqdvcK1Fv/figures/figures_21_1.jpg", "caption": "Figure 2: Comparison of energy discrepancy and contrastive divergence on the dataset with 16 dimensions and 5 states. Rows 1 and 2 show the estimated density and synthesised samples, respectively.", "description": "This figure compares the performance of energy discrepancy (ED) and contrastive divergence (CD) in estimating the probability density and generating samples from a dataset with 16 dimensions and 5 states.  The top row shows the true data distribution and then the estimated density functions produced by ED and CD methods. The bottom row displays samples generated by each method, illustrating the differences in their ability to capture the multi-modal nature of the distribution.", "section": "6.1 Discrete Density Estimation"}, {"figure_path": "wAqdvcK1Fv/figures/figures_21_2.jpg", "caption": "Figure 2: Comparison of energy discrepancy and contrastive divergence on the dataset with 16 dimensions and 5 states. Rows 1 and 2 show the estimated density and synthesised samples, respectively.", "description": "This figure compares the performance of energy discrepancy (ED) and contrastive divergence (CD) in estimating probability distributions on a dataset with 16 dimensions and 5 states. The top row shows the true data distribution and the ED and CD estimated densities.  The bottom row shows samples generated from the learned models using ED and CD. The figure visually demonstrates that ED produces a more accurate density estimation and generates samples that more closely resemble the true data distribution compared to CD.", "section": "6.1 Discrete Density Estimation"}, {"figure_path": "wAqdvcK1Fv/figures/figures_23_1.jpg", "caption": "Figure 2: Comparison of energy discrepancy and contrastive divergence on the dataset with 16 dimensions and 5 states. Rows 1 and 2 show the estimated density and synthesised samples, respectively.", "description": "This figure compares the performance of energy discrepancy and contrastive divergence methods for estimating probability density and generating synthetic samples from a dataset with 16 dimensions and 5 states. The top row displays the estimated probability density functions obtained by using the two different methods. The bottom row shows samples synthesized using the same methods.  It showcases how the energy discrepancy method produces more accurate density estimations and better-quality synthetic samples compared to the contrastive divergence method.", "section": "6.1 Discrete Density Estimation"}, {"figure_path": "wAqdvcK1Fv/figures/figures_24_1.jpg", "caption": "Figure 2: Comparison of energy discrepancy and contrastive divergence on the dataset with 16 dimensions and 5 states. Rows 1 and 2 show the estimated density and synthesised samples, respectively.", "description": "This figure compares the performance of energy discrepancy (ED) and contrastive divergence (CD) in estimating the probability distribution of a dataset with 16 dimensions and 5 states for each dimension. The top row displays the estimated probability density learned by each method.  The bottom row presents samples generated from the learned models. Visually, energy discrepancy produces a sharper, more accurate representation of the density, with samples that more closely reflect the true data distribution. Contrastive divergence, in contrast, shows a less refined density estimation and produces samples less aligned with the true data distribution.", "section": "6.1 Discrete Density Estimation"}, {"figure_path": "wAqdvcK1Fv/figures/figures_26_1.jpg", "caption": "Figure 5: Scaling limit of the introduced perturbations. Top: Convergence of rescaled cyclical and ordinal perturbations ys2t/S for base time parameters t = 0.01 and t = 0.05 to Gaussian [0, 1) with non-trivial boundary conditions. One can see that the perturbation converges to a fixed shape on the normalised state space. Bottom: Convergence of rescaled cyclical and ordinal perturbation (yst - E[yst])/\u221aS for base time parameters t = 0.1 and t = 0.5 to Gaussian on R (red line). The orange mark indicates the initial state. One can see that the perturbation remains non-trivial as the state space grows to infinity at rate \u221aS.", "description": "This figure shows the results of an experiment to verify the scaling limit of the cyclical and ordinal perturbations, as described in Theorem 2. The top row shows that as the size of the state space (S) increases, both cyclical and ordinal perturbations converge to a Gaussian distribution on the interval [0,1), which is consistent with Theorem 2. The bottom row demonstrates that rescaling the perturbation by \u221aS results in convergence to a Gaussian distribution on the real numbers.", "section": "3.1 Heat Equation in Structured Discrete Spaces"}, {"figure_path": "wAqdvcK1Fv/figures/figures_27_1.jpg", "caption": "Figure 12: Visualisation of the training data and samples drawn from the energy-based models learned by the variants of our approaches on the Ego-small dataset.", "description": "This figure visualizes the results of graph generation using different methods.  Subfigure (a) shows examples from the Ego-small training dataset. Subfigures (b) and (c) display graphs generated using the ED-Bern and ED-Grid methods, respectively, showcasing the models' ability to learn and generate realistic graph structures similar to the training data.", "section": "D.5 Graph Generation"}]