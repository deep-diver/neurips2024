[{"figure_path": "0AwMciNShl/tables/tables_5_1.jpg", "caption": "Table 1: Comprehensive evaluation criteria for T2V models. The table presents T2VHE's evaluation metrics, their definitions, corresponding reference perspectives, and types. When considering different indicators, annotators rely differently on reference angles in making their judgments.", "description": "This table presents the six evaluation metrics used in the Text-to-Video Human Evaluation (T2VHE) protocol. For each metric, the table provides a detailed definition, the corresponding reference perspectives to guide annotators in making judgements, and the type of metric (objective or subjective).  The reference perspectives offer specific viewpoints and examples to help annotators make consistent and reliable evaluations, especially when dealing with subjective metrics. The table shows that some metrics rely more heavily on reference perspectives than others when making judgements.", "section": "4.1 Evaluation metrics"}, {"figure_path": "0AwMciNShl/tables/tables_6_1.jpg", "caption": "Table 2: Comparison of annotation consensus under different annotator qualifications. We compute Krippendorff's a [47] as an IAA measure. Higher values represent more consensus among annotators.", "description": "This table compares the inter-annotator agreement (IAA) scores, using Krippendorff's alpha, across different annotator groups for various evaluation metrics (Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, Human Preference).  The groups compared are: AMT and pre-training LRAs, AMT and post-training LRAs, and AMT annotators alone.  Higher IAA values indicate stronger agreement between annotators, suggesting higher annotation quality and reliability. The table shows a significant improvement in IAA after the post-training of LRAs, bringing their scores closer to those of the AMT annotators.", "section": "4.3 Evaluators"}, {"figure_path": "0AwMciNShl/tables/tables_8_1.jpg", "caption": "Table 2: Comparison of annotation consensus under different annotator qualifications. We compute Krippendorff's \u03b1 [47] as an IAA measure. Higher values represent more consensus among annotators.", "description": "This table compares the inter-annotator agreement (IAA), measured using Krippendorff's alpha, across three groups of annotators: AMT annotators (professional crowdworkers), pre-training LRAs (laboratory-recruited annotators before training), and post-training LRAs (laboratory-recruited annotators after training).  Higher values of Krippendorff's alpha indicate better agreement among annotators for a given metric. The table shows that post-training LRAs achieve much higher consensus for all the evaluation metrics (Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, and Human Preference) compared to the pre-training LRAs.  This highlights the significant impact of the training program on improving the quality and reliability of human evaluations.", "section": "4.3 Evaluators"}, {"figure_path": "0AwMciNShl/tables/tables_8_2.jpg", "caption": "Table 4: Type and number of model pairs discarded in dynamic evaluation.", "description": "This table shows the number of video pairs that were excluded from the human evaluation process by the dynamic evaluation module. The model pairs are grouped by the two models being compared.  The dynamic evaluation module prioritizes the annotation of video pairs considered more deserving of manual evaluation during the static annotation phase and discards those with less significance based on the differences in model scores. The \"Count\" column indicates how many times each pair was discarded across different evaluation metrics. This illustrates how the dynamic module optimizes annotation efficiency by reducing unnecessary manual annotations.", "section": "4.4 Dynamic evaluation module"}, {"figure_path": "0AwMciNShl/tables/tables_23_1.jpg", "caption": "Table 5: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refer to the annotation results of Post-training LRAs using the dynamic evaluation component. A higher score represents a better performance of the model on that dimension.", "description": "This table presents a comparison of the scores and rankings of five different text-to-video (T2V) models across six evaluation metrics (Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, and Human Preference).  The comparison is made using three different groups of annotators: pre-trained laboratory-recruited annotators (LRAs), Amazon Mechanical Turk (AMT) annotators, and post-trained LRAs.  Additionally, it includes results from post-trained LRAs using a dynamic evaluation module (Dyn), which aims to optimize annotation efficiency.  Higher scores indicate better model performance in each metric.  The ranking (in parentheses) shows the model's relative position among the five models for each metric and annotator group.", "section": "5 Human evaluation of existing models"}, {"figure_path": "0AwMciNShl/tables/tables_25_1.jpg", "caption": "Table 5: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refer to the annotation results of Post-training LRAs using the dynamic evaluation component. A higher score represents a better performance of the model on that dimension.", "description": "This table presents a comparison of the scores and rankings of five different text-to-video (T2V) models across six evaluation metrics.  Three types of annotators were used: AMT annotators (trained and compensated), pre-training LRAs (laboratory-recruited annotators with minimal training), and post-training LRAs (laboratory-recruited annotators with comprehensive training).  The table shows how the scores and rankings vary based on the annotator type and the specific evaluation metric, allowing for an analysis of model performance consistency and the impact of annotator training.", "section": "5 Human evaluation of existing models"}, {"figure_path": "0AwMciNShl/tables/tables_26_1.jpg", "caption": "Table 5: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refer to the annotation results of Post-training LRAs using the dynamic evaluation component. A higher score represents a better performance of the model on that dimension.", "description": "This table presents a comparison of the scores and rankings of five different text-to-video (T2V) models across six evaluation metrics (Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, Human Preference) using three different groups of annotators: AMT annotators, pre-training LRAs, and post-training LRAs.  It also shows the results for the post-training LRAs when using the dynamic evaluation module.  Higher scores indicate better performance on the metric.", "section": "5 Human evaluation of existing models"}, {"figure_path": "0AwMciNShl/tables/tables_31_1.jpg", "caption": "Table 8: Prompt Category - Animal", "description": "This table presents the results of human evaluation for the \"Animal\" prompt category.  It shows the average scores and rankings of five different text-to-video models (Gen2, Pika, Latte, TF-T2V, and Videocrafter2) across six evaluation metrics: Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, and Human Preference.  The numbers in parentheses indicate the ranking of each model for that specific metric.", "section": "4.1 Evaluation metrics"}, {"figure_path": "0AwMciNShl/tables/tables_31_2.jpg", "caption": "Table 5: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refer to the annotation results of Post-training LRAs using the dynamic evaluation component. A higher score represents a better performance of the model on that dimension.", "description": "This table presents a comparison of the scores and rankings of five different text-to-video (T2V) models across six evaluation dimensions.  The scores are averages from three different groups of annotators: those who received pre-training, those who used Amazon Mechanical Turk (AMT), and those with post-training. The \"Dyn\" column indicates scores from post-training annotators who used a dynamic evaluation module, a new efficiency enhancement proposed in the paper.  The table helps to demonstrate the consistency of the proposed evaluation method across different annotator groups and highlights which models perform best in each dimension, offering a nuanced view of each model's strengths and weaknesses.", "section": "5 Human evaluation of existing models"}, {"figure_path": "0AwMciNShl/tables/tables_31_3.jpg", "caption": "Table 5: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refer to the annotation results of Post-training LRAs using the dynamic evaluation component. A higher score represents a better performance of the model on that dimension.", "description": "This table presents the scores and rankings of five different text-to-video (T2V) models across six evaluation metrics: Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, and Human Preference.  The scores are obtained from three different groups of annotators: AMT annotators, pre-trained laboratory-recruited annotators (LRAs), and post-trained LRAs.  The table also shows a comparison of results when using a dynamic evaluation module (Post-training LRAs (Dyn)). Higher scores indicate better performance on the given metric.", "section": "5 Human evaluation of existing models"}, {"figure_path": "0AwMciNShl/tables/tables_31_4.jpg", "caption": "Table 5: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refer to the annotation results of Post-training LRAs using the dynamic evaluation component. A higher score represents a better performance of the model on that dimension.", "description": "This table presents a comparison of the scores and rankings of five different text-to-video (T2V) models across six evaluation metrics.  Three different groups of annotators were used: AMT annotators, pre-training LRAs, and post-training LRAs.  Post-training LRAs (Dyn) refers to the results when the dynamic evaluation component was used, which aims to reduce annotation cost.  Each metric assesses a different aspect of T2V model quality, such as video quality, temporal quality, motion quality, text alignment, ethical robustness, and human preference.  The numbers in parentheses show the rank of each model for each metric.", "section": "5 Human evaluation of existing models"}, {"figure_path": "0AwMciNShl/tables/tables_32_1.jpg", "caption": "Table 5: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refer to the annotation results of Post-training LRAs using the dynamic evaluation component. A higher score represents a better performance of the model on that dimension.", "description": "This table presents a comparison of the scores and rankings of five different text-to-video (T2V) models across six evaluation metrics: Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, and Human Preference.  The scores are obtained from three groups of annotators: AMT Annotators, pre-training LRAs, and post-training LRAs.  A separate column shows the results for post-training LRAs who used the dynamic evaluation module.  Higher scores indicate better model performance on each metric.", "section": "5 Human evaluation of existing models"}, {"figure_path": "0AwMciNShl/tables/tables_32_2.jpg", "caption": "Table 5: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refer to the annotation results of Post-training LRAs using the dynamic evaluation component. A higher score represents a better performance of the model on that dimension.", "description": "This table presents a comparison of the scores and rankings of five different text-to-video (T2V) models across six evaluation dimensions: Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, and Human Preference.  The comparison is made across three groups of annotators: AMT Annotators, pre-training LRAs, and post-training LRAs.  The table also includes a separate column for Post-training LRAs who used the dynamic evaluation module. A higher score indicates better performance on that particular metric.", "section": "5 Human evaluation of existing models"}, {"figure_path": "0AwMciNShl/tables/tables_32_3.jpg", "caption": "Table 5: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refer to the annotation results of Post-training LRAs using the dynamic evaluation component. A higher score represents a better performance of the model on that dimension.", "description": "This table presents a comparison of the scores and rankings of five different text-to-video (T2V) models across six evaluation metrics (Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, Human Preference).  The comparison is made across three different groups of annotators:  AMT annotators (trained Amazon Mechanical Turk workers), pre-training LRAs (laboratory-recruited annotators before training), and post-training LRAs (laboratory-recruited annotators after training). A fourth group, Post-training LRAs (Dyn), used the dynamic evaluation module for efficiency. Higher scores indicate better performance on each metric.", "section": "5 Human evaluation of existing models"}, {"figure_path": "0AwMciNShl/tables/tables_32_4.jpg", "caption": "Table 5: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refer to the annotation results of Post-training LRAs using the dynamic evaluation component. A higher score represents a better performance of the model on that dimension.", "description": "This table presents the scores and rankings of five different text-to-video (T2V) models across six evaluation metrics: Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, and Human Preference.  The scores are based on evaluations from three types of annotators: AMT annotators, pre-training LRAs, and post-training LRAs.  The table also includes a comparison for post-training LRAs using the dynamic evaluation module.", "section": "5 Human evaluation of existing models"}]