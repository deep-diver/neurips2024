[{"figure_path": "32g9BWTndc/figures/figures_2_1.jpg", "caption": "Figure 1: Framework of TEA-GLM", "description": "The figure illustrates the TEA-GLM framework, which consists of three main stages: contrastive learning of GNN, alignment tuning of projector, and zero-shot tasks.  The contrastive learning stage uses two views of a raw graph (generated by RE and MF methods) to train a GNN, enhancing self-supervised learning using LLM token embeddings. A linear projector then aligns the GNN's representations with the LLM's token embeddings, creating graph token embeddings. Finally, these embeddings are used in a unified instruction for various zero-shot tasks (node classification and link prediction) on different datasets (Arxiv, Cora, Pubmed).", "section": "2 Methodology"}, {"figure_path": "32g9BWTndc/figures/figures_15_1.jpg", "caption": "Figure 1: Framework of TEA-GLM", "description": "The figure illustrates the TEA-GLM framework, showing its two main components: a Graph Neural Network (GNN) and a Large Language Model (LLM).  The GNN processes the raw graph data to generate node or edge representations.  These representations are then passed through a linear projector that maps them into graph token embeddings. Finally, these embeddings, along with a unified instruction, are fed into the LLM to perform zero-shot tasks such as node classification and link prediction. The figure also highlights the contrastive learning methods used to enhance the self-supervised learning of the GNN and the alignment tuning of the projector to enhance LLM's graph comprehension. Different tasks (node classification, link prediction) using data from different datasets (Cora, Pubmed, Arxiv) are also illustrated.", "section": "2 Methodology"}, {"figure_path": "32g9BWTndc/figures/figures_15_2.jpg", "caption": "Figure 1: Framework of TEA-GLM", "description": "This figure illustrates the framework of the proposed method TEA-GLM, which consists of two main components: a Graph Neural Network (GNN) and a Large Language Model (LLM). The GNN is used to derive node representations from the graph, while the LLM is used to perform zero-shot tasks. The framework involves two key stages: enhanced self-supervised learning of the GNN and training a linear projector to map graph representations into a fixed number of graph token embeddings.  The figure shows the contrastive learning of the GNN, the alignment tuning of the projector, and the zero-shot tasks performed using the LLM.", "section": "2 Methodology"}, {"figure_path": "32g9BWTndc/figures/figures_16_1.jpg", "caption": "Figure 1: Framework of TEA-GLM", "description": "The figure illustrates the TEA-GLM framework, showing its three main stages: contrastive learning of GNN, alignment tuning of the projector, and zero-shot tasks.  The contrastive learning stage uses two views of the graph (RE and MF) and LLM token embeddings to generate enhanced node representations.  These are then mapped to graph token embeddings using a linear projector, which is trained to align the GNN representations with the LLM's embedding space. Finally, these graph token embeddings are used within unified instructions for zero-shot node classification and link prediction tasks across various datasets.", "section": "2 Methodology"}]