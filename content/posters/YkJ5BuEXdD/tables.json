[{"figure_path": "YkJ5BuEXdD/tables/tables_5_1.jpg", "caption": "Table 1: The idealized ranking accuracy of existing algorithms is not perfect, but preference-tuned models exhibit ranking accuracies far even from this idealized case. We provide both the length-normalized (R) and non-length-normalized (R) ranking accuracies for a variety of open-access preference-tuned models on the Alpaca Farm [9] validation dataset (described in App. B.2). We also provide the idealized ranking accuracy (R* or R*, Corollary 3.3). Since idealized ranking accuracy can be computed with a variety of values of \u03b2, we provide the minimum, median, and maximum idealized ranking accuracy values for a range of \u03b2. For more details, see App. B.4.", "description": "This table shows the idealized ranking accuracy and the actual ranking accuracy achieved by several open-access preference-tuned LLMs on the Alpaca Farm validation dataset. The idealized accuracy is calculated assuming perfect optimization of the DPO or RLHF objective with ground-truth preference data.  The table highlights a significant discrepancy between the idealized and achieved ranking accuracies, indicating a substantial \"alignment gap\".  Minimum, median, and maximum values for the idealized accuracy are presented due to the influence of the hyperparameter \u03b2.", "section": "3.2 Idealized Ranking Accuracy"}, {"figure_path": "YkJ5BuEXdD/tables/tables_21_1.jpg", "caption": "Table 2: Length-normalized and non-length-normalized ranking accuracies for the Anthropic Helpful and Harmless (HH-RLHF; Bai et al. [3]) and Synthetic Instruct GPT-J Pairwise [1] datasets. The latter contains only a training split.", "description": "This table presents the ranking accuracy of various language models on two different preference datasets: Anthropic HH-RLHF and Synthetic Instruct GPT-J Pairwise.  Ranking accuracy is calculated with and without length normalization. The table shows that most models have similar length-normalized ranking accuracy across datasets, with length-normalized accuracy consistently higher than non-length-normalized accuracy.  Note that the Synthetic Instruct GPT-J Pairwise dataset only has a training split available.", "section": "3.3 Measuring the Alignment Gap"}, {"figure_path": "YkJ5BuEXdD/tables/tables_21_2.jpg", "caption": "Table 3: Length-normalized and non-length-normalized ranking accuracies for the StackExchange Preferences [28] and UltraFeedback [7] datasets. Both datasets contain only a training split.", "description": "This table presents the length-normalized (\u0158) and non-length-normalized (R) ranking accuracies for a variety of open-access LLMs on the StackExchange Preferences and UltraFeedback datasets.  Both datasets only include training splits.  The table shows the performance of various models in ranking the preferred continuation over less preferred continuations for a given prompt, which is a key aspect of preference learning.", "section": "3. Measuring the Alignment Gap"}, {"figure_path": "YkJ5BuEXdD/tables/tables_22_1.jpg", "caption": "Table 1: The idealized ranking accuracy of existing algorithms is not perfect, but preference-tuned models exhibit ranking accuracies far even from this idealized case. We provide both the length-normalized (R) and non-length-normalized (R) ranking accuracies for a variety of open-access preference-tuned models on the Alpaca Farm [9] validation dataset (described in App. B.2). We also provide the idealized ranking accuracy (R* or R*, Corollary 3.3). Since idealized ranking accuracy can be computed with a variety of values of \u00df, we provide the minimum, median, and maximum idealized ranking accuracy values for a range of \u00df. For more details, see App. B.4.", "description": "This table presents a comparison of the actual ranking accuracy of several preference-tuned LLMs against their idealized ranking accuracy.  The idealized accuracy represents what would be achievable if the models perfectly optimized the DPO or RLHF objective with ground-truth preference data. The table shows a significant gap between the actual and idealized accuracies, indicating a substantial limitation of current preference learning techniques.", "section": "3.2 Idealized Ranking Accuracy"}, {"figure_path": "YkJ5BuEXdD/tables/tables_22_2.jpg", "caption": "Table 1: The idealized ranking accuracy of existing algorithms is not perfect, but preference-tuned models exhibit ranking accuracies far even from this idealized case. We provide both the length-normalized (R) and non-length-normalized (R) ranking accuracies for a variety of open-access preference-tuned models on the Alpaca Farm [9] validation dataset (described in App. B.2). We also provide the idealized ranking accuracy (R* or R*, Corollary 3.3). Since idealized ranking accuracy can be computed with a variety of values of \u03b2, we provide the minimum, median, and maximum idealized ranking accuracy values for a range of \u03b2. For more details, see App. B.4.", "description": "This table presents a comparison of the actual ranking accuracy of several preference-tuned LLMs against their idealized ranking accuracy (the accuracy they would achieve if they perfectly optimized the DPO or RLHF objective). The table highlights the significant \"alignment gap\" between the observed and idealized accuracies, indicating that current models are far from achieving optimal performance. Length-normalized and non-length-normalized accuracies are provided, along with minimum, median, and maximum idealized accuracies for a range of \u03b2 values.", "section": "3.2 Idealized Ranking Accuracy"}, {"figure_path": "YkJ5BuEXdD/tables/tables_23_1.jpg", "caption": "Table 6: We provide both the length-normalized (R) and non-length-normalized (R) ranking accuracies for a variety of open-access preference-tuned models on the Alpaca Farm [9] validation dataset (described in App. B.2). We also provide the idealized ranking accuracy (Corollary 3.3). Unlike Table 1, we include examples with ties in this table.", "description": "This table presents the length-normalized and non-length-normalized ranking accuracies for four different preference-tuned LLMs on the Alpaca Farm validation dataset.  It also shows the minimum, median, and maximum idealized ranking accuracies for a range of beta values, offering a comparison between the actual performance of these models and the theoretical maximum achievable under ideal conditions (considering ties in the data). The difference highlights the \"alignment gap\" discussed in the paper.", "section": "3.3 Measuring the Alignment Gap"}, {"figure_path": "YkJ5BuEXdD/tables/tables_29_1.jpg", "caption": "Table 1: The idealized ranking accuracy of existing algorithms is not perfect, but preference-tuned models exhibit ranking accuracies far even from this idealized case. We provide both the length-normalized (R) and non-length-normalized (R) ranking accuracies for a variety of open-access preference-tuned models on the Alpaca Farm [9] validation dataset (described in App. B.2). We also provide the idealized ranking accuracy (R* or R*, Corollary 3.3). Since idealized ranking accuracy can be computed with a variety of values of \u03b2, we provide the minimum, median, and maximum idealized ranking accuracy values for a range of \u03b2. For more details, see App. B.4.", "description": "This table presents the idealized and actual ranking accuracies for several preference-tuned LLMs.  It compares the ranking accuracy achievable under perfect conditions (idealized) to the real-world performance of existing models on the Alpaca Farm validation dataset.  The table highlights the significant \"alignment gap\" between idealized and actual performance, which is a key finding of the paper.", "section": "3.2 Idealized Ranking Accuracy"}, {"figure_path": "YkJ5BuEXdD/tables/tables_30_1.jpg", "caption": "Table 1: The idealized ranking accuracy of existing algorithms is not perfect, but preference-tuned models exhibit ranking accuracies far even from this idealized case. We provide both the length-normalized (R) and non-length-normalized (R) ranking accuracies for a variety of open-access preference-tuned models on the Alpaca Farm [9] validation dataset (described in App. B.2). We also provide the idealized ranking accuracy (R* or R*, Corollary 3.3). Since idealized ranking accuracy can be computed with a variety of values of \u00df, we provide the minimum, median, and maximum idealized ranking accuracy values for a range of \u00df. For more details, see App. B.4.", "description": "This table presents the idealized and actual ranking accuracies for several open-access, preference-tuned LLMs on the Alpaca Farm validation dataset.  It highlights a significant difference (alignment gap) between the idealized accuracy (what would be achieved with perfect optimization and ground-truth data) and the observed accuracy of existing models.  The table also shows the minimum, median, and maximum idealized ranking accuracies for a range of beta values (hyperparameter in RLHF/DPO objectives).", "section": "3.2 Idealized Ranking Accuracy"}, {"figure_path": "YkJ5BuEXdD/tables/tables_31_1.jpg", "caption": "Table 1: The idealized ranking accuracy of existing algorithms is not perfect, but preference-tuned models exhibit ranking accuracies far even from this idealized case. We provide both the length-normalized (R) and non-length-normalized (R) ranking accuracies for a variety of open-access preference-tuned models on the Alpaca Farm [9] validation dataset (described in App. B.2). We also provide the idealized ranking accuracy (R* or R*, Corollary 3.3). Since idealized ranking accuracy can be computed with a variety of values of \u03b2, we provide the minimum, median, and maximum idealized ranking accuracy values for a range of \u03b2. For more details, see App. B.4.", "description": "This table compares the actual ranking accuracy of several open-access, preference-tuned LLMs against their idealized ranking accuracy (i.e., the accuracy they would achieve if they perfectly optimized the DPO or RLHF objective).  The table highlights the significant gap between the observed and idealized ranking accuracies, illustrating a key finding of the paper that even under ideal conditions, these models struggle to achieve high ranking accuracy.", "section": "3.2 Idealized Ranking Accuracy"}, {"figure_path": "YkJ5BuEXdD/tables/tables_32_1.jpg", "caption": "Table 1: The idealized ranking accuracy of existing algorithms is not perfect, but preference-tuned models exhibit ranking accuracies far even from this idealized case. We provide both the length-normalized (R) and non-length-normalized (R) ranking accuracies for a variety of open-access preference-tuned models on the Alpaca Farm [9] validation dataset (described in App. B.2). We also provide the idealized ranking accuracy (R* or R*, Corollary 3.3). Since idealized ranking accuracy can be computed with a variety of values of \u00df, we provide the minimum, median, and maximum idealized ranking accuracy values for a range of \u00df. For more details, see App. B.4.", "description": "This table shows the idealized ranking accuracy compared to the actual ranking accuracy achieved by several open-access preference-tuned LLMs. The idealized accuracy represents the performance a model would achieve if it perfectly optimized the DPO or RLHF objective using ground truth preference data.  The table highlights a significant \"alignment gap\" between the idealized and observed ranking accuracies, indicating that current preference learning algorithms struggle to achieve high ranking accuracies even under ideal conditions.  Both length-normalized and non-length-normalized results are provided for different models, along with the range of idealized accuracies for various beta values.", "section": "3 Measuring the Alignment Gap"}, {"figure_path": "YkJ5BuEXdD/tables/tables_33_1.jpg", "caption": "Table 1: The idealized ranking accuracy of existing algorithms is not perfect, but preference-tuned models exhibit ranking accuracies far even from this idealized case. We provide both the length-normalized (R) and non-length-normalized (R) ranking accuracies for a variety of open-access preference-tuned models on the Alpaca Farm [9] validation dataset (described in App. B.2). We also provide the idealized ranking accuracy (R* or R*, Corollary 3.3). Since idealized ranking accuracy can be computed with a variety of values of \u03b2, we provide the minimum, median, and maximum idealized ranking accuracy values for a range of \u03b2. For more details, see App. B.4.", "description": "This table presents the length-normalized and non-length-normalized ranking accuracies for various open-access preference-tuned models on the Alpaca Farm validation dataset.  It also shows the idealized ranking accuracy achievable under perfect conditions, highlighting the significant gap between the observed and idealized performance. The table emphasizes the limitations of current preference-tuning methods in achieving high ranking accuracy, even when the objective is perfectly optimized.", "section": "3.2 Idealized Ranking Accuracy"}]