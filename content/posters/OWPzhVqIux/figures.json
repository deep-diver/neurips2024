[{"figure_path": "OWPzhVqIux/figures/figures_2_1.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure presents the results of two experiments using GPT-4 in a 5-armed bandit setting. The top row shows an experiment where GPT-4 fails to explore effectively, exhibiting behavior similar to a greedy algorithm.  The bottom row shows a successful exploration experiment where GPT-4, with a modified prompt, converges to the optimal arm.  The figure uses three visualizations: histograms showing how many times the best arm was chosen across multiple trials, plots of the suffix failure frequency (how often the best arm was never chosen after a certain point), and plots of the cumulative reward across trials. This demonstrates the impact of prompt engineering on exploration capability of LLMs.", "section": "Representative experiments"}, {"figure_path": "OWPzhVqIux/figures/figures_3_1.jpg", "caption": "Figure 2: Prompt designs; see Figure 9 for a more detailed view. A prompt is generated by traversing the graph from top to bottom.", "description": "This figure is a graph that shows how the prompts are generated. The different prompt designs are created by combining elements such as scenarios (buttons or advertisements), framing (neutral or suggestive), history presentation (raw or summarized), return type (single action or distribution), and the inclusion of chain-of-thought reasoning.  The figure shows the different options and how they combine to create a total of 32 different prompt designs.  Figure 9 provides more detailed text examples of each.", "section": "2 Experimental setup"}, {"figure_path": "OWPzhVqIux/figures/figures_5_1.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure displays the results of two experiments using GPT-4 on a 5-armed bandit problem. The top row shows an experiment where GPT-4 fails to explore effectively, while the bottom row shows a successful exploration.  Three different algorithms (Upper Confidence Bound, Thompson Sampling, and Greedy) are used as baselines for comparison. The left panel of each row is a histogram showing how often the best arm was selected, the center panel plots the frequency of \"suffix failures\" (never choosing the best arm after a certain time step), and the right panel shows the time-averaged reward.", "section": "3 Experimental results"}, {"figure_path": "OWPzhVqIux/figures/figures_5_2.jpg", "caption": "Figure 3: Scatter plot summarizing all experiments with T=100. We plot suffix failures (via SuffFailFreq(T/2)) vs. uniform-like failures (via K*MinFrac(T)). Each LLM/configuration pair maps to a dot (some dots overlap). The only successful GPT-4 configuration (BSSC0) is labeled with a star. We also plot e-GREEDY, tracing out the tradeoffs for different e.", "description": "This scatter plot visualizes the results of experiments comparing different Large Language Models (LLMs) and configurations on a multi-armed bandit (MAB) problem.  The x-axis represents the frequency of suffix failures (where the best arm is never chosen after a certain point), and the y-axis shows the frequency of uniform-like failures (where arms are chosen almost equally). Each point represents a specific LLM and prompt configuration.  The plot reveals that most LLMs and configurations exhibit substantial exploration failures, failing to converge to the best arm.  The successful GPT-4 configuration (BSSC0) is highlighted, demonstrating that successful exploration is possible but requires specific prompt engineering and LLM capabilities. The e-GREEDY baseline curve helps to contextualize the results.", "section": "Experimental results"}, {"figure_path": "OWPzhVqIux/figures/figures_6_1.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure shows the results of experiments using GPT-4 on a 5-armed bandit problem.  Two different prompt configurations are compared: one that leads to exploration failure and one that leads to success. The figure includes histograms showing how often the best arm was chosen, plots of suffix failure frequency (the percentage of times the best arm was never chosen after a certain point in time), and plots of cumulative average rewards.  These are compared against three standard bandit algorithms (UCB, Thompson Sampling, and Greedy) to illustrate the success or failure of the GPT-4 configurations.", "section": "Representative experiments"}, {"figure_path": "OWPzhVqIux/figures/figures_6_2.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure shows the results of experiments comparing GPT-4's performance on a 5-armed bandit problem with two different prompt configurations and three baseline algorithms (UCB, TS, and GREEDY). The top row displays an exploration failure, while the bottom row shows a successful exploration. The visualizations include histograms of best arm selections, suffix failure frequencies, and cumulative time-averaged rewards.  The figure demonstrates the impact of prompt design on LLM exploration capabilities.", "section": "Experimental results"}, {"figure_path": "OWPzhVqIux/figures/figures_7_1.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure shows the results of experiments comparing GPT-4's performance on a 5-armed bandit problem with two different prompt configurations. The top row demonstrates exploration failure, while the bottom row shows successful exploration.  The figure uses histograms to show the number of times the best arm was selected across multiple trials, plots showing the frequency of 'suffix failures' (where the best arm is never selected after a certain point in time), and graphs of cumulative time-averaged rewards.  These visualizations allow for a comparison of GPT-4's performance against standard bandit algorithms such as UCB, Thompson Sampling, and a greedy algorithm.", "section": "Representative experiments"}, {"figure_path": "OWPzhVqIux/figures/figures_8_1.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure shows the results of experiments comparing GPT-4's performance in a 5-armed bandit problem with two different prompt configurations.  The top row illustrates a case of exploration failure, where GPT-4 fails to consistently select the best arm, mirroring the performance of a greedy algorithm.  The bottom row shows a successful exploration, where GPT-4, aided by a more informative prompt, exhibits performance comparable to optimal bandit algorithms like UCB and Thompson Sampling.  The visualizations include histograms of best-arm selections, suffix failure frequency, and time-averaged rewards, providing a comprehensive view of the model's exploration behavior under different conditions.", "section": "Representative experiments"}, {"figure_path": "OWPzhVqIux/figures/figures_16_1.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure shows the results of experiments comparing GPT-4's performance on a 5-armed bandit problem with two different prompt configurations.  The top row demonstrates an exploration failure where the model does not consistently select the best arm. The bottom row illustrates successful exploration, showing the model successfully converging to the optimal arm.  The figure includes histograms showing the number of times the best arm is chosen, plots of the suffix failure frequency, and plots of the cumulative time-averaged rewards for both the GPT-4 configurations and for baseline bandit algorithms (UCB, Thompson Sampling, and Greedy).", "section": "Representative experiments"}, {"figure_path": "OWPzhVqIux/figures/figures_19_1.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure displays the results of two experiments using GPT-4 on a 5-armed bandit problem. The top row shows an experiment where GPT-4 fails to explore effectively, while the bottom row shows an experiment where GPT-4 successfully explores.  The figure compares GPT-4's performance to three baseline algorithms: Upper Confidence Bound (UCB), Thompson Sampling (TS), and a greedy algorithm.  Three visualizations are provided for each experiment: a histogram showing the number of times the best arm was chosen across multiple trials, a plot of the suffix failure frequency (the probability of never selecting the best arm after a certain point in time), and a plot of the cumulative time-averaged rewards.", "section": "Representative experiments"}, {"figure_path": "OWPzhVqIux/figures/figures_20_1.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure shows the results of experiments comparing GPT-4's performance on a 5-armed bandit problem with two different prompt configurations. One configuration resulted in exploration failure, while the other was successful.  The figure presents three visualizations for each configuration: a histogram showing the number of times the best arm was chosen across multiple repetitions of the experiment, a plot of the suffix failure frequency (the proportion of trials where the best arm was never chosen after a certain point), and a plot showing the cumulative time-averaged rewards.  These results highlight the impact of prompt design on the LLM's ability to explore effectively.", "section": "3 Experimental results"}, {"figure_path": "OWPzhVqIux/figures/figures_20_2.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure shows the results of experiments using GPT-4 on a 5-armed bandit problem.  The top row shows an exploration failure, where the model does not consistently choose the best arm.  The bottom row shows successful exploration with a modified prompt.  Histograms illustrate the number of times the best arm was chosen, while plots show suffix failure frequency and cumulative reward, comparing GPT-4's performance to standard bandit algorithms (UCB, TS, and Greedy).", "section": "Experimental results"}, {"figure_path": "OWPzhVqIux/figures/figures_27_1.jpg", "caption": "Figure 1: Representative experiments: Two prompt configurations for GPT-4 on a 5-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the GREEDY algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each t, we plot the suffix failure frequency, the fraction of replicates for which the best arm is never chosen after time-step t, and (Right) cumulative time-averaged rewards, averaged over replicates (\u00b12 standard errors).", "description": "This figure presents a comparison of exploration performance between GPT-4 with different prompt configurations and standard bandit algorithms (UCB, TS, and Greedy) in a 5-armed bandit problem. The top row shows an exploration failure, where GPT-4 rarely selects the best arm, while the bottom row illustrates successful exploration, reaching comparable performance to the baseline algorithms.  Three visualizations are provided for each scenario to show the distribution of best arm selections, the frequency of suffix failures (where the best arm is never chosen after a certain point), and the cumulative average rewards.", "section": "Representative experiments"}]