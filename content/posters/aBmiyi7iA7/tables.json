[{"figure_path": "aBmiyi7iA7/tables/tables_9_1.jpg", "caption": "Table 1: The average acceptance rate, average test MSE, and average test MSE at the best acceptance rate for the three activation functions on the UTKFace dataset. The sigmoid network has better average acceptance rate and MSE than the ReLU-based networks, although all activation functions have nearly the same MSE at their best acceptance rate.", "description": "This table presents the results of experiments on the UTKFace dataset using three different activation functions (Sigmoid, ReLU, and Leaky ReLU). It compares the average acceptance rate, the overall average Mean Squared Error (MSE), and the MSE achieved at the best acceptance rate for each activation function.  The results show that the Sigmoid activation function leads to a better average acceptance rate and lower overall MSE compared to ReLU and Leaky ReLU.  However, when comparing MSE at their best acceptance rate, there is little difference among the three activation functions.", "section": "4.2 UTKFace Dataset"}, {"figure_path": "aBmiyi7iA7/tables/tables_17_1.jpg", "caption": "Table 2: Acceptance rates of HMC on the synthetic dataset with respect to the number of leapfrog steps L and step size e on BNNs with different activation functions.", "description": "This table presents the acceptance rates of the Hamiltonian Monte Carlo (HMC) algorithm using different activation functions (Sigmoid, ReLU, Leaky ReLU) on a synthetic dataset.  The acceptance rates are shown for various numbers of leapfrog steps (L) and step sizes (\u03b5). The data demonstrates how the acceptance rate changes as these hyperparameters are varied, offering insights into the efficiency of HMC with different activation functions and hyperparameter settings.", "section": "4.1 Synthetic Dataset"}]