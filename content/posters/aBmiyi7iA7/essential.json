{"importance": "This paper is crucial for researchers employing Hamiltonian Monte Carlo (HMC) for Bayesian neural network inference, especially those using ReLU activation functions.  It reveals the inefficiency of HMC with ReLU networks due to high rejection rates, impacting the reliability and scalability of Bayesian deep learning. The findings necessitate a re-evaluation of HMC's applicability and encourage exploration of alternative sampling methods, influencing both theoretical understanding and practical applications of Bayesian neural networks. The proposed optimal scaling guideline for HMC with non-differentiable log-densities provides a new avenue for tuning HMC, enhancing the method's efficiency in a broader range of scenarios. ", "summary": "Hamiltonian Monte Carlo struggles with ReLU neural networks: high rejection rates hinder Bayesian deep learning.", "takeaways": ["HMC's high rejection rate with ReLU networks stems from a large local error rate.", "The inefficiency is not easily mitigated by derivative adjustments at ReLU's non-differentiable point.", "Optimal HMC tuning guidelines for non-differentiable log-densities are proposed (step size scaling as d\u207b\u00b9/\u00b2 and optimal acceptance probability of 0.45)."], "tldr": "Bayesian neural networks (BNNs) are increasingly used, but their high dimensionality necessitates efficient inference methods like Hamiltonian Monte Carlo (HMC).  However, the non-differentiability of activation functions like ReLU poses challenges for HMC.  Classical HMC analysis assumes smooth energy functions, leading to inaccurate error rate estimations and inefficient sampling when applied to ReLU networks. This paper addresses these issues by analyzing HMC's error rates in non-smooth settings and proposing solutions.\nThe researchers show that HMC for ReLU networks exhibits a much higher local error rate (\u03a9(\u03b5)) than the classical rate (O(\u03b5\u00b3)), causing a high rejection rate. This inefficiency is verified through both theoretical analysis and experiments on synthetic and real-world data. Importantly, they offer new guidelines for tuning HMC with non-differentiable components, suggesting an optimal acceptance probability of 0.45 and a step size scaling of d\u207b\u00b9/\u00b2. This work highlights the limitations of relying on classical HMC guidelines for ReLU networks and provides valuable insights for researchers working on Bayesian deep learning.", "affiliation": "University of Delaware", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "aBmiyi7iA7/podcast.wav"}