[{"type": "text", "text": "Hamiltonian Monte Carlo on ReLU Neural Networks is Inefficient ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vu C. Dinh Department of Mathematical Sciences University of Delaware vucdinh@udel.edu ", "page_idx": 0}, {"type": "text", "text": "Lam Si Tung Ho Department of Mathematics and Statistics Dalhousie University lam.ho@dal.ca ", "page_idx": 0}, {"type": "text", "text": "Cuong V. Nguyen Department of Mathematical Sciences Durham University viet.c.nguyen@durham.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We analyze the error rates of the Hamiltonian Monte Carlo algorithm with leapfrog integrator for Bayesian neural network inference. We show that due to the nondifferentiability of activation functions in the ReLU family, leapfrog HMC for networks with these activation functions has a large local error rate of $\\bar{\\Omega}(\\epsilon)$ rather than the classical error rate of $\\mathcal{O}(\\epsilon^{3})$ . This leads to a higher rejection rate of the proposals, making the method inefficient. We then verify our theoretical findings through empirical simulations as well as experiments on a real-world dataset that highlight the inefficiency of HMC inference on ReLU-based neural networks compared to analytical networks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, there has been a growing interest in doing full Bayesian analyses for neural networks and deep learning (Hern\u00e1ndez-Lobato and Adams, 2015; Huber, 2020; Cobb and Jalaian, 2021; Dhulipala et al., 2023). Since neural network models are typically high-dimensional, Hamiltonian Monte Carlo (HMC) (Neal, 2011) is a natural choice over other Markov Chain Monte Carlo approaches (Duane et al., 1987; Neal, 2011). When the activation function of a network is analytic (e.g., sigmoid function), one could rely on the theoretical foundations and practical guidelines of HMC in classical settings with smooth energy functions for computational designs (Neal, 2011; Beskos et al., 2013; Betancourt et al., 2017). ", "page_idx": 0}, {"type": "text", "text": "For ReLU-based neural networks, the situation is not as clear since the ReLU activation has a point of non-differentiability at zero. The derivative of ReLU at zero, in principle, is not well-defined, although often set to be zero in most computational platforms (Bertoin et al., 2021). Since non-differentiability only happens on a set of measure zero on the parameter space, it is natural to assume that this technical singularity does not pose a problem beyond theoretical considerations. For example, when training ReLU networks, it is known that the vast majority of stochastic gradient descent sequences produced by minimizing the loss function are not meaningfully impacted by changing the value of the derivative of ReLU at zero (Berner et al., 2019; Bolte and Pauwels, 2020; Bertoin et al., 2021; Bianchi et al., 2022). ", "page_idx": 0}, {"type": "text", "text": "For HMC, this line of thought is also not completely misguided, as we will show in this paper that HMC with leapfrog integrator is correct (i.e., it samples from the correct distribution) as long as the computed derivatives during sampling are well-defined up to the second order and are compatible with the chain rule. Since the backpropagation algorithm to compute gradients for neural networks is designed with the chain rule as its foundation, HMC with ReLU-based networks is thus correct regardless of how the derivative of ReLU is defined at zero, as long as it is defined deterministically. From a theoretical perspective, if Hamiltonian dynamics can be simulated exactly, the acceptance probability of HMC is always one and non-differentiability is also not an issue. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In practice, however, Hamiltonian systems can rarely be exactly integrated and are often approximated by a symplectic integrator (e.g., the leapfrog integrator) (Neal, 2011). The approximation errors of the Hamiltonian during integration lead to a decay in the acceptance probability of the proposals. The focus of our analyses in this work is on the efficiency of practical implementations of HMC: we show that when a leapfrog HMC particle crosses a surface of non-differentiability (which corresponds to a single activation/deactivation of a neuron in the network), the Hamiltonian is likely to incur a local error rate of order $\\Omega(\\epsilon)$ , leading to uncontrollable error accumulation along the Hamiltonian path. This is in contrast with the classical local error rate $\\mathcal{O}(\\epsilon^{3})$ for smooth Hamiltonian (Neal, 2011) and thus renders HMC inference on ReLU networks inefficient compared to its analytic counterparts. Since the issue is due to the differences in the derivatives of the potential energy on two domains across a surface of non-differentiability, it cannot be resolved through the choices of the derivative value of ReLU at zero.1 ", "page_idx": 1}, {"type": "text", "text": "Our contributions. In this work, we analyze the HMC algorithm with the leapfrog integrator for Bayesian neural network inference: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We formulate the theoretical conditions under which HMC with leapfrog integrator is correct, even when derivatives are not defined in classical senses.   \n\u2022 We provide an upper bound of order $O(\\epsilon)$ for the error of Hamiltonian dynamics on ReLUbased networks, and establish that outside a small set of starting points in the parameter space, this error is also bounded from below by $\\Omega(\\epsilon)$ .   \n\u2022 We analyze the optimal dimensional scaling of the step size and acceptance probability of HMC for target distributions consisting of $d\\gg1$ independent and identically distributed (i.i.d.) dimensions with piece-wise affine non-differentiable log-density components. From this result, we obtain a new guideline for tuning HMC with a first-order symplectic integrator that suggests a scaling of $\\bar{d}^{-1/2}$ for the step size and an optimal acceptance probability of 0.45.   \n\u2022 Through experiments with both synthetic and real datasets, we validate our theoretical analyses and highlight the inefficiency of ReLU neural networks compared to analytical networks. ", "page_idx": 1}, {"type": "text", "text": "Related works. Classical theoretical analyses of HMC algorithms are often performed under the assumptions that the potential functions (the negative log-likelihood of the posterior distributions) are smooth (Neal, 2011; Beskos et al., 2013; Betancourt et al., 2017). Another research direction considers the cases where the energy function is discontinuous (Pakman and Paninski, 2013; Afshar and Domke, 2015) or contains discrete parameters (Nishimura et al., 2020; Zhou, 2020) by introducing momentum adjustments near its discontinuity in a way that preserves the total energy. These results proved that the constructed algorithms preserve the correct stationary distribution but did not consider the efficiency of the approach, and they do not apply directly to the neural network models where the potential function is continuous. It is generally recognized that if the leapfrog transition is not effective in preserving the Hamiltonian, HMC is inefficient and the issue needs to be remedied by algorithmic modifications, for example, by using surrogate functions (Dinh et al., 2017) or non-volume-preserving proposals (Afshar et al., 2021). Other theoretical analyses of HMC also considered the global geometry such as the curvature of the HMC manifolds (Seiler et al., 2014), which is different from the local smoothness property that we consider in this paper. ", "page_idx": 1}, {"type": "text", "text": "2 Hamiltonian Monte Carlo for Neural Networks and Its Efficiency ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Bayesian neural networks (BNNs). We consider a general Bayesian feed-forward neural network model (Neal, 1995). Formally, given a $d_{0}$ -dimensional input $x$ in a bounded open set ", "page_idx": 1}, {"type": "text", "text": "$\\mathcal{X}\\subset\\mathbb{R}^{d_{0}}$ , the output $f_{q}(x)\\;\\in\\;\\mathcal{V}$ of an $M$ -layer feed-forward neural network with parameters $\\boldsymbol{q}=(A_{1},b_{1},A_{2},b_{2},\\dots,\\dot{A}_{M},b_{M})$ is defined through several layers: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{h_{0}(x)=x,\\quad}&{\\mathrm{(input~layer)}}\\\\ &{h_{j}(x)=\\sigma(A_{j}\\cdot h_{j-1}(x)+b_{j}),\\;\\;\\mathrm{for}\\;j=1,2,\\dots,M-1,\\quad}&{\\quad(M-1\\mathrm{~hidden~layers})}\\\\ &{f_{q}(x)=h_{M}(x)=A_{M}\\cdot h_{M-1}(x)+b_{M},\\quad}&{\\quad}&{\\mathrm{(output~layer)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\sigma$ is an activation function, $A_{j}\\in\\mathbb{R}^{d_{j}\\times d_{j-1}}$ and $b_{j}\\in\\mathbb{R}^{d_{j}}$ , with $d_{j}$ being the number of nodes in the $j$ -th layer. Throughout this paper, we assume the parameter vector $q$ belongs to a compact set $\\mathcal{Q}$ . In the Bayesian setting, given a dataset $D=\\left\\{(x_{1},y_{1}),(x_{2},y_{2}),\\ldots,(x_{n},y_{n})\\right\\}$ with inputs $x_{i}\\in\\mathcal{X}$ and labels $y_{i}\\in\\mathcal{V}$ , the posterior distribution for the model parameters is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(q)\\propto\\pi(q)\\prod_{i=1}^{n}\\ell(q|x_{i},y_{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pi(q)$ is the prior density and $\\ell(q|x_{i},y_{i})$ is the likelihood function given the data point $\\left({x_{i},y_{i}}\\right)$ . Using the posterior, we obtain the posterior predictive distribution of any new data point $(x,y)$ by $\\begin{array}{r}{P(x,\\stackrel{\\bullet}{y}|D)\\stackrel{\\bullet}{=}\\int\\ell(q|x,y)P(q)d q}\\end{array}$ , which can be used for prediction. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we consider the general setting with a smooth (i.e., infinitely differentiable) loss function $\\bar{\\mathcal{L}}_{x,y}(q)$ that holds for several classification and regression problems. A simple case of this setting is regression with square loss (which corresponds to a Gaussian noise assumption with fixed, known variance) where $\\dot{\\mathcal{L}_{x,y}}(q)=(\\dot{f}_{q}(x)-y)^{2}$ and $\\log\\ell(q|x,y)\\propto-(f_{q}(x)-y)^{\\hat{2}}$ . Another common case is classification with cross-entropy loss where $\\mathcal{L}_{x,y}(q)=-\\log(\\operatorname{softmax}(f_{q}(x))_{y})$ and $\\log\\ell(q|x,y)\\,=\\,f_{q}(x)_{y}\\,+c$ for some constant $c$ . The analyses of our work can also be extended easily to other Bayesian learning settings such as unsupervised learning and generative modeling with smooth losses. ", "page_idx": 2}, {"type": "text", "text": "Hamiltonian Monte Carlo for BNNs with leapfrog integrator. Since computing the posterior predictive distribution $P(x,y|D)$ and other integrals over the posterior is generally intractable, especially for complex models like neural networks, we usually compute these quantities approximately. Among the approximation methods, HMC (Neal, 1995, 2011) is a popular choice for neural network models due to the availability of gradients and the effectiveness of the method when exploring the parameter space. In general, HMC uses a Hamiltonian dynamical system to sample $m$ parameter vectors q\u02dc1, q\u02dc2, . . . , q\u02dcm from the posterior P(q) and then approximate g(q)P(q)dq \u2248 m1  im=1 g(q\u02dci) for any function $g$ of interest. To sample from $P(q)$ , HMC proposes to extend the state space to include auxiliary momentum variables $p$ of the same dimension as $q$ and study the canonical distribution: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(q,p)\\propto\\exp\\left(-H(q,p)\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $H(q,p)\\,=\\,U(q)+K(p)$ , with $U(q)\\,=\\,-\\log P(q)$ and $K(p)\\,=\\,\\frac{1}{2}\\|p\\|^{2}$ . Here we refer to $H({\\mathfrak{q}},{\\mathfrak{p}}),\\,U({\\mathfrak{q}})$ , and $K(p)$ as respectively the Hamiltonian, the potential energy function, and the kinetic energy function of the Hamiltonian system at the state $(q,p)$ . We assumed that $p\\sim\\mathcal{N}(0,I)$ in the formulation above, although in theory $p$ could have a more general distribution. After defining $P(q,p)$ , we can then sample ${\\tilde{q}}_{1},{\\tilde{q}}_{2},\\dots,{\\tilde{q}}_{m}$ successively from this canonical distribution using Hamiltonian dynamics. Specifically, given the current position ${\\tilde{q}}_{i}$ , we sample $\\tilde{q}_{i+1}$ in two steps, both of which leave the canonical distribution invariant (i.e., the canonical distribution is the invariant distribution of the Markov kernels associated with those samplers). In the first step, we randomly draw a vector $\\tilde{p}_{i}$ of new values for the momentum variables from their Gaussian distribution, independently of the current values ${\\tilde{q}}_{i}$ of the position variables. In the second step, a Metropolis update is performed where a new sample is proposed by simulating Hamiltonian dynamics for $L$ steps using the leapfrog method (Skeel, 1999; Leimkuhler and Reich, 2005; Sanz-Serna and Calvo, 2018) with a step size of $\\epsilon$ . In particular, starting from the initial state $(q_{0},p_{0})=(\\tilde{q}_{i},\\tilde{p}_{i})$ , we perform the following updates for $L$ steps: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p_{1/2}=p_{0}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{0}),}\\\\ {\\displaystyle q_{1}=q_{0}+\\epsilon\\,p_{1/2},}\\\\ {\\displaystyle p_{1}=p_{1/2}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The momentum variables at the end of this trajectory are then negated to obtain a proposed state $(\\tilde{q},\\tilde{p})$ , which is accepted with probability $\\operatorname*{min}\\{1,\\exp\\left(-H(\\tilde{q},\\tilde{p})+\\bar{H}(q_{0},p_{0})\\right)\\}$ , giving the new state $(\\tilde{q}_{i+1},\\tilde{p}_{i+1})$ . The negation of the momentum at the end of this $L$ -step trajectory makes the Metropolis proposal symmetrical, but is not necessarily needed in practice, since $K(p)\\,=\\,K(-p)$ and the momentum will be replaced before it is used again in the next iteration. ", "page_idx": 3}, {"type": "text", "text": "HMC offers an attractive Monte Carlo method, especially in high dimensions: an HMC particle can travel a long distance across the state space while Hamiltonian dynamics keep the Hamiltonian $H(q,p)$ relatively constant, leading to a high acceptance rate (Neal, 2011). When the energy functions are smooth, the algorithm has a well-established theoretical foundation that guarantees its correctness and efficiency through two main observations: (i) the leapfrog integrator is reversible and preserves volume, and (ii) the local error of a leapfrog step is $\\bar{\\mathcal{O}}(\\bar{\\epsilon}^{3})$ (Neal, 2011). Property (ii) essentially allows the HMC particles to travel a length of $L\\epsilon$ while maintaining a small rejection rate of order $\\mathcal{O}(L\\epsilon^{3})$ . In practice, HMC is often tuned according to a fixed travel time $T$ ; that is, $L=T/\\epsilon$ for a fixed constant $T$ . In this case, the global error of HMC (and thus, the rejection rate of the proposals) is of order ${\\mathcal{O}}(T\\epsilon^{2})$ . ", "page_idx": 3}, {"type": "text", "text": "Efficiency of HMC and optimal tuning. One important aspect of implementing HMC is tuning the two main parameters: the step size $\\epsilon$ and the travel time $T$ . The main considerations are: (i) how to scale $\\epsilon$ with the dimension of the problem, and (ii) how to choose both $\\epsilon$ and $T$ to achieve a balance between the effort to simulate a long trajectory and the acceptance probability of the resulting proposal. The analyses for such optimal acceptance probability are often done via a proxy case where the model parameters consist of $d\\gg1$ smooth and i.i.d. components (Beskos et al., 2013; Betancourt et al., 2014). In this setting, Beskos et al. (2013) rely on the global error rate ${\\mathcal{O}}(\\epsilon^{2})$ to show that HMC with leapfrog requires $L\\approx\\mathcal{O}(d^{1/4})$ steps to traverse the state space and the step size $\\epsilon$ is generally scaled as $d^{-1/4}$ for an average acceptance probability of $\\Omega(1)$ . If we let $\\epsilon=l d^{-1/4}$ for some tunable parameter $l$ , the number of leapfrog steps is $L=T/\\epsilon=T/(l d^{-1/4})$ , and the computational cost to compute a single proposal will be approximately: ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{0}\\cdot d\\cdot\\frac{T}{l d^{-1/4}}=C_{0}\\frac{T}{l}d^{5/4},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C_{0}$ measures the cost of one leapfrog step in one dimension. Given a starting location $q$ , the number of proposals until acceptance follows a geometric distribution with some probability of success $A(q,l)$ . Using Jensen\u2019s inequality, the expected cost until the first accepted proposal in stationary is bounded from below by: ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{0}\\,\\frac{T}{l\\cdot\\mathbb{E}[A(q,l)]}\\,d^{5/4}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here the quantity $l\\cdot\\mathbb{E}[A(q,l)]$ is called the efficiency of the HMC algorithm, and a sensible approach for optimal tuning of HMC is to choose $l$ such that this efficiency function is minimized. In the setting with i.i.d. smooth parameters, Beskos et al. (2013) show that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{d\\to\\infty}\\mathbb{E}[A(q,l)]=2\\Phi(-l^{2}\\sqrt{\\Sigma}/2):=a(l),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Phi$ is the c.d.f. of the standard normal distribution and $\\Sigma$ is an unknown constant. While $\\Sigma$ and the optimal $l_{o p t}$ of the function $l\\cdot a(l)$ generally depend on the specific target distribution under consideration, it can be shown that $a(l_{o p t})$ does not vary with the selected target distribution. Thus, in practice, we can compute the efficiency by setting $\\Sigma=1$ ; that is, the efficiency is computed as $2l:\\Phi(-l^{2}/2)$ . This leads to an optimal acceptance probability of $a(l_{o p t})=0.651$ , which is often used in practice for HMC tuning (Neal, 2011; Campbell et al., 2021; Hoffman et al., 2021; Nijkamp et al., 2021). Subsequently, Betancourt et al. (2014) extend this analysis to include an upper bound and suggest the target average acceptance probability can be relaxed to $0.6\\leq a(l)\\leq0.9$ . ", "page_idx": 3}, {"type": "text", "text": "3 Correctness and Efficiency of Leapfrog HMC on ReLU Neural Networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we shall present our theoretical results on the correctness and efficiency of HMC with leapfrog integrator on Bayesian ReLU neural networks. First, we prove a general result in Theorem 3.1 on the correctness of leapfrog HMC on models that use backpropagation. As a special case of this theorem, leapfrog HMC on neural networks with the ReLU activation is correct (i.e., it samples from the correct distribution), even when derivatives are not defined in a classical sense. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. If the derivatives of the potential energy function $U$ are well-defined up to the second order and are compatible with the chain rule, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial q}\\left(\\frac{\\partial U}{\\partial q}(\\phi(q))\\right)=\\frac{\\partial^{2}U}{\\partial q^{2}}(\\phi(q))\\frac{\\partial\\phi}{\\partial q}(q)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for all smooth functions $\\phi,$ , then the leapfrog integrator is reversible and preserves volume. As a consequence, the HMC sampler with leapfrog integrator leaves the canonical distribution invariant. ", "page_idx": 4}, {"type": "text", "text": "Since the backpropagation algorithm is designed with the chain rule as its foundation, Theorem 3.1 tells us that leapfrog HMC on ReLU neural networks is correct regardless of how the derivative of ReLU is defined at 0, as long as it is defined consistently. A complete proof of this theorem is provided in Appendix A.1, where deterministic definitions of first derivatives are needed to guarantee reversibility, and the chain rule concerning second derivatives of the potential function appears when computing the Jacobian of the leapfrog transformation to ensure volume-preservation. ", "page_idx": 4}, {"type": "text", "text": "Having established the correctness of leapfrog HMC on ReLU neural networks, we now turn to the main emphasis of our paper: the efficiency of this algorithm. Note that from our discussion in Section 2, the efficiency of HMC depends on the acceptance probability of the proposals and the approximation error of the Hamiltonian. Thus, our idea here is to show that when the HMC particles cross a surface of non-differentiability (e.g., that corresponds to a single activation/deactivation of a neuron in the ReLU network), the Hamiltonian will likely incur a local error rate of order $\\Omega(\\epsilon)$ , leading to uncontrollable error accumulation along the Hamiltonian path. ", "page_idx": 4}, {"type": "text", "text": "To analyze the efficiency of HMC, we need a result about the (ir)regularity of the potential function of ReLU-based networks, which is stated in Lemma 3.2 below. Essentially, the output of a ReLU feed-forward neural network at a node (before activation) can be characterized by the activation patterns (on-off for ReLU) of all nodes from previous layers feeding into it. When these activation patterns are fixed, the output at a node is a multilinear/polynomial function of the network parameters. Non-differentiability thus arises when the values of these functions cross zero, leading to \u201cjumps\u201d in values of partial derivatives of the potential functions that drive Hamiltonian dynamics. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2. If the activation function \u03c3 is piece-wise affine with a single point of non-differentiability at 0, then there exists a finite union of smooth surface $\\textstyle S=\\bigcup_{i\\in I}S_{i}$ with $S_{i}=\\{q:f_{i}(q)=0\\}$ and analytic functions $f_{i}$ , such that \u2202U/\u2202q is non-smooth on $S$ but locally smooth everywhere else. ", "page_idx": 4}, {"type": "text", "text": "The proof of this lemma is in Appendix A.2, which uses an induction argument on the layers of the feed-forward neural networks. As HMC explores the parameter space, these patterns change as the dynamics cross a surface of non-differentiability. Since the on-off ReLU activation patterns are discrete in nature, the behaviors of HMC with ReLU networks resemble those of models with discrete parameters (Dinh et al., 2017; Nishimura et al., 2020; Zhou, 2020) rather than a purely continuous one. Lemma 3.2 extends naturally for all piece-wise affine functions with only one point of non-differentiability at 0 such as the leaky ReLU activation function. ", "page_idx": 4}, {"type": "text", "text": "Analysis of local errors. The next step in our analysis is to derive the local error rate of the leapfrog HMC algorithm on ReLU networks. To give an intuition for our result, we will demonstrate an error analysis for the simpler case where the particle crosses a surface of non-differentiability only once. In this case, we consider a single leapfrog step starting at $(q_{0},p_{0})$ and ending at $(q_{1},p_{1})$ after performing the updates in Equations (1)-(3). If we assume that along this linear path, the particle crosses a surface of non-differentiability exactly once at a point $z$ , then we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Delta K=K(p_{1})-K(p_{0})=\\frac{1}{2}\\left(\\|p_{1}\\|^{2}-\\|p_{0}\\|^{2}\\right)}\\\\ {\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{1}{2}\\left(\\bigg\\|p_{0}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{0})-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{1})\\bigg\\|^{2}-\\|p_{0}\\|^{2}\\right)}\\\\ {\\quad\\quad\\quad\\quad\\quad\\quad\\quad=-\\frac{\\epsilon}{2}p_{0}\\cdot\\bigg(\\frac{\\partial U}{\\partial q}(q_{0})+\\frac{\\partial U}{\\partial q}(q_{1})\\bigg)+\\mathcal{O}(\\epsilon^{2})}\\\\ {\\quad\\quad\\quad\\quad\\quad\\quad=-\\frac{\\epsilon}{2}p_{1/2}\\cdot\\bigg(\\frac{\\partial U}{\\partial q}(q_{0})+\\frac{\\partial U}{\\partial q}(q_{1})\\bigg)+\\mathcal{O}(\\epsilon^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Delta U=U(q_{1})-U(q_{0})}\\\\ {\\displaystyle\\qquad=U(q_{1})-U(z)+U(z)-U(q_{0})}\\\\ {\\displaystyle\\qquad=\\int_{\\epsilon_{1}}^{\\epsilon}\\frac{\\partial U}{\\partial q}(q_{0}+t\\,p_{1/2})\\cdot p_{1/2}\\,d t+\\int_{0}^{\\epsilon_{1}}\\frac{\\partial U}{\\partial q}(q_{0}+t\\,p_{1/2})\\cdot p_{1/2}\\,d t}\\\\ {\\displaystyle\\qquad=\\frac{\\epsilon_{2}}{2}\\,p_{1/2}\\cdot\\left[\\frac{\\partial U^{+}}{\\partial q}(z)+\\frac{\\partial U}{\\partial q}(q_{1})\\right]+\\frac{\\epsilon_{1}}{2}\\,p_{1/2}\\cdot\\left[\\frac{\\partial U}{\\partial q}(q_{0})+\\frac{\\partial U^{-}}{\\partial q}(z)\\right]+\\mathcal{O}(\\epsilon^{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\epsilon_{1}$ and $\\epsilon_{2}$ are the time spent along the path before and after crossing the surface of nondifferentiability, respectively. Similarly, \u2202\u2202U q $\\frac{\\partial U^{-}}{\\partial q}(z)$ and $\\frac{\\partial U^{+}}{\\partial q}(z)$ denote the gradients at $z$ on the two differentiable regions before and after the incidence, respectively. Thus, we can deduce the local error of the Hamiltonian: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{3}H=H(q_{1},p_{1})-H(q_{0},p_{0})=\\cfrac{p_{1/2}}{2}\\cdot\\bigg[{-\\epsilon_{2}\\left(\\cfrac{\\partial U}{\\partial q}(q_{0})-\\cfrac{\\partial U^{-}}{\\partial q}(z)\\right)-\\epsilon_{1}\\left(\\cfrac{\\partial U}{\\partial q}(q_{1})-\\cfrac{\\partial U^{+}}{\\partial q}(z)\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+(\\epsilon_{2}-\\epsilon_{1})\\left(\\cfrac{\\partial U^{+}}{\\partial q}(z)-\\cfrac{\\partial U^{-}}{\\partial q}(z)\\right)\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\cfrac{(\\epsilon_{2}-\\epsilon_{1})}{2}\\;p_{1/2}\\cdot\\left(\\cfrac{\\partial U^{+}}{\\partial q}(z)-\\cfrac{\\partial U^{-}}{\\partial q}(z)\\right)+{\\mathcal O}(\\epsilon^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "since $\\partial U/\\partial q\\left(z\\right)$ is Lipschitz in each domain of continuity due to $U$ being analytic in these bounded domains. ", "page_idx": 5}, {"type": "text", "text": "Our analysis above shows that even if the particle crosses a surface of non-differentiability once, the incurred local error $\\Delta H$ will be of order $\\Omega(\\epsilon)$ . This analysis can be generalized to the case when the linear path from $\\left(q_{0},p_{0}\\right)$ to $(q_{1},p_{1})$ crosses multiple regions. We thus have the following general result with the proof given in Appendix A.3. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3. Consider a leapfrog step starting at $\\left(q_{0},p_{0}\\right)$ and ending at $(q_{1},p_{1})$ that crosses the surfaces of non-differentiability at $z_{1},z_{2},\\dots,z_{k}$ at time $\\epsilon_{1},\\epsilon_{2},\\ldots,\\epsilon_{k}$ . The local approximation error incurred can be estimated by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta H=H(q_{1},p_{1})-H(q_{0},p_{0})=p_{1/2}\\cdot\\sum_{i=1}^{k}\\left(\\frac{\\epsilon}{2}-\\epsilon_{i}\\right)\\left(\\frac{\\partial U^{+}}{\\partial q}(z_{i})-\\frac{\\partial U^{-}}{\\partial q}(z_{i})\\right)+\\mathcal{O}(\\epsilon^{2}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "awnhde raef $\\frac{\\partial U^{-}}{\\partial q}\\big(z_{i}\\big)$ nagn $\\frac{\\partial U^{+}}{\\partial q}(z_{i})$ idveelnyo.te the gradients of $U$ at $z_{i}$ on the two differentiable regions before $z_{i}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 indicates that, in general, $\\Delta H=\\Omega(\\epsilon)$ and is difficult to control. $\\Delta H$ can only be small (i.e., smaller than $O(\\epsilon))$ if the leapfrog path crosses the regions at a very specific time. For example, when $k=1$ , this corresponds to the path crossing a boundary approximately at time $t\\approx\\epsilon/2$ . The following lemma shows that this happens only at a very small subset of the state space. The proof of this lemma is given in Appendix A.4. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.4. For $M>0$ and $a\\,>\\,0,$ , define $\\mathcal{A}_{M,a}$ as the set of all augmented states $(q,p)$ such that: $\\|p\\|\\leq M$ and a single leapfrog step starting at $(q,p)$ crosses a surface of non-differentiability exactly once at time $\\epsilon_{1}\\in\\left((1-a)\\frac{\\epsilon}{2},(1+a)\\frac{\\epsilon}{2}\\right).L e t\\,\\mathcal{B}_{M},$ be the set of all augmented states $(q,p)$ such that the $L$ -step leapfrog Hamiltonian trajectory starting at $(q,p)$ belongs to $\\mathcal{A}_{M,a}$ at some point along the path. There exist $C>0$ and $\\alpha\\geq2$ that depend only on the potential energy function (and are independent of $\\alpha,\\,\\epsilon,\\,L$ , and $M$ ) such that: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu({\\cal A}_{M,a})\\leq(C a M\\epsilon)^{1/\\alpha}\\;\\;\\;a n d\\;\\;\\;\\mu({\\cal B}_{M,a})\\leq(C a M\\epsilon)^{1/\\alpha}{\\cal L},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mu$ denotes the Lebesgue measure on $\\mathcal{Q}\\times\\mathbb{R}^{d}$ , with $\\mathcal{Q}$ being the parameter space. ", "page_idx": 5}, {"type": "text", "text": "Another direct consequence of Theorem 3.3 is that for a fixed travel time $T$ (i.e., $L=T/\\epsilon$ for a fixed constant $T$ ), as $\\epsilon$ goes to zero, the global errors of leapfrog HMC can be controlled by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\epsilon}{2}\\sum_{z\\in A}p(z)\\cdot\\left(\\frac{\\partial U^{+}}{\\partial q}(z)-\\frac{\\partial U^{-}}{\\partial q}(z)\\right)+\\mathcal{O}(T\\epsilon),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{A}}$ denotes the set of points of non-differentiability along the Hamiltonian path when the system is integrated exactly with the same initial state for an amount of time $T$ . ", "page_idx": 6}, {"type": "text", "text": "Tuning HMC on ReLU neural networks. Our result above indicates that for ReLU networks, HMC proposals may accumulate a rejection rate of order $\\Omega(N\\epsilon)$ , where $N$ is associated with the number of times the dynamics cross a surface of non-differentiability (i.e., when a ReLU neuron is activated or deactivated). This means that the classical guidance for implementations of HMC is no longer valid for ReLU networks. However, with our theoretical result above, we can still adapt the analyses of Beskos et al. (2013) and Betancourt et al. (2014) to obtain similar guidelines for tuning HMC with a first-order symplectic integrator. These guidelines are stated in the following proposition, with the proof given in Appendix A.5. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.5. Consider a target distribution on vectors consisting of $d\\gg1$ i.i.d. piece-wise affine non-differentiable log-density components. The following statements hold: ", "page_idx": 6}, {"type": "text", "text": "(a) HMC with the leapfrog integrator has a global error rate of $O(\\epsilon)$ .   \n(b) The step size \u03f5 should be scaled as $d^{-1/2}$ for an $\\Omega(1)$ average acceptance probability.   \n(c) The optimal acceptance probability of leapfrog HMC is approximately 0.45.   \n(d) The range of target average acceptance probability can be relaxed to [0.4, 0.75]. ", "page_idx": 6}, {"type": "text", "text": "As a consequence of this proposition, for piece-wise affine non-differentiable log-densities, as the dimension increases, the computational cost to maintain a constant average acceptance probability grows as $d^{3/2}$ as opposed to $\\breve{d}^{5/4}$ in classical cases (Beskos et al., 2013). Statement (d) of the proposition follows the discussions from Betancourt et al. (2014), where the detailed analysis uses both the upper and lower bounds of the efficiency function. This analysis is illustrated in Figures 4 and 5 in the appendix. ", "page_idx": 6}, {"type": "text", "text": "It is worth noting that for the random-walk Metropolis (RWM) algorithm, the scaling and optimal acceptance probability are $d^{-1}$ and 0.23, respectively (Yang et al., 2020). The corresponding quantities for the Metropolis-adjusted Langevin algorithm (MALA) are $d^{-1/3}$ and 0.57 when the potential energy function is seven times differentiable (Roberts and Rosenthal, 1998). Thus, the scaling of the step size and the optimal acceptance probability for HMC in this setting are still more efficient than RWM but are less ideal than those of analytic cases. ", "page_idx": 6}, {"type": "text", "text": "We want to reiterate that these analyses above are only for the proxy case of $d\\gg1$ independent and identically distributed vector components. In this setting, the number of critical events (where the HMC particle crosses a surface of non-differentiability) increases linearly as the dimension $d$ increases. For general ReLU networks, it is still unclear how the number of critical events would change with increasing dimension of the parameter space. It is worth noting that for a fixed generic parameter configuration, it is suggested that for any line segment through the input space, the average number of regions (of the input space) intersecting with it is linear in the number of neurons, which is far below the exponential number of regions that is theoretically attainable (Hanin and Rolnick, 2019). However, we are not aware of similar results for the number of (polynomial) regions on the parameter space for fixed inputs. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Synthetic Dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we shall conduct empirical simulations to validate our theoretical analyses. For the simulations, we generate a synthetic dataset with 100 examples where $x\\sim\\operatorname{uniform}(0,4)$ and $y\\sim\\mathcal{N}(\\cos(2x),0.1^{\\bar{2}})$ . The dataset is shown in Figure 6 in Appendix A.6. Using this dataset, we investigate the influences of several hyper-parameters on the average acceptance rate and the efficiency of HMC on ReLU-based and analytic networks, including the step size $\\epsilon$ , the number of leapfrog steps $L$ and the dimension $d$ of the model parameters. Our simulations are implemented using the Autograd package (Maclaurin et al., 2015). ", "page_idx": 6}, {"type": "text", "text": "This section will focus on computational aspects of HMC with BNNs that are not directly obtainable from our theoretical analysis. First, as highlighted in Section 3, we can qualitatively compare the efficiency of ReLU-based and analytic networks through their optimal acceptance rates, as a lower optimal acceptance rate indicates that the model is less efficient. However, since they are only known up to a multiplicative constant in the exponents, the efficiency functions of two different models, such as between ReLU and analytic networks, are not quantitatively comparable through these theoretical analyses. We aim to validate such direct comparisons in our simulations. Second, some of our theoretical results are analyzed through the proxy cases of independent and identically distributed vector components. While this approach, as often done in analyses of HMC, provides good insights about the behaviors of HMC when the complexity of the models increases, this also leaves some uncertainties about the extent to which the results apply to neural network models, and we aim to complement those results through practical simulations. ", "page_idx": 6}, {"type": "image", "img_path": "aBmiyi7iA7/tmp/791bc6e523929457ae5e685fa43b39e7ae6aba79bf842dd2056e093c6586c886.jpg", "img_caption": ["Figure 1: Acceptance rates of HMC with respect to the number of leapfrog steps $L$ (left) and step size $\\epsilon$ (right) on BNNs with different activation functions. The decay in acceptance rates of sigmoid networks is much more moderate than those of ReLU-based networks. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Effects of number of steps $L$ and step size $\\epsilon$ . In this simulation, we investigate the effects of $L$ and $\\epsilon$ on the acceptance rate of HMC on BNNs with different types of activation functions. In particular, we consider one hidden layer neural networks with 50 hidden nodes that use either the sigmoid, ReLU, or leaky ReLU activation. We choose a standard normal prior $\\pi(q)=\\mathcal{N}(0,I)$ and sample 2,000 parameter vectors from the posterior after a burn-in period of 100 samples. We vary the number of steps $L\\in\\{200,400,600,\\mathring{\\mathrm{s00}},1000\\}$ together with the step size $\\epsilon\\in\\mathsf{\\bar{\\{0.0005,0.0010,0.0015,0.0020,0.0025\\}}}$ and record the corresponding acceptance rates. We repeat this procedure 5 times with different random seeds to obtain the average acceptance rates and their standard errors. The full results of this simulation are reported in Table 2 in Appendix A.6, with some typical trends shown in Figure 1. ", "page_idx": 7}, {"type": "text", "text": "From Figure 1, we observe that the average acceptance rate of sigmoid networks, across different values of the step size $\\epsilon$ , is generally higher than those of ReLU and leaky ReLU networks. As $\\epsilon$ increases, the decay in average acceptance rate of sigmoid networks is moderate and stable across different values of $L$ . On the other hand, the drops in average acceptance rate for ReLU and leaky ReLU networks are significant, and the problem exacerbates for large values of $L$ . ", "page_idx": 7}, {"type": "text", "text": "We also note that for sigmoid networks, the average acceptance rate for fixed step sizes are relatively constant as $L$ increases. This behavior is somewhat expected for symplectic integrators with smooth target distributions, as high-order symplectic integrators (i.e., those with global error at most $\\mathcal{O}(\\epsilon^{2}))$ are known to not only approximate the flow of the Hamiltonian $H$ corresponding to the canonical distributions, but also exactly simulate the flow for some modified Hamiltonian H\u02dc (Betancourt et al., 2014). This makes the approximation errors bounded in $L$ for an appropriately small step size $\\epsilon$ with smooth target distributions. For ReLU-based networks, the regularity conditions for such asymptotic behaviors do not hold, and as presented in Figure 1, the leapfrog integrator becomes unstable, manifesting in numerical divergences that pull the approximation errors to infinity and the average acceptance rate to zero as $L$ increases. ", "page_idx": 7}, {"type": "image", "img_path": "aBmiyi7iA7/tmp/29e73a770f9e2738a861299fb22a4fbbe9278f94d26273231ee70fc6cd213fa3.jpg", "img_caption": ["Figure 2: Efficiency of HMC with respect to acceptance rate on BNNs with different activation functions. On both synthetic and UTKFace datasets, HMC inference with sigmoid networks is more efficient than with ReLU-based networks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Efficiency vs. acceptance rate. In this simulation, we investigate the efficiency as a function of the acceptance rate. We use the same setting as the previous simulation, except that here we fix the travel time $T=\\epsilon L=0.1$ and vary $\\epsilon\\in\\{0.0005,0.0010,0.0015,\\ldots,0.0040\\}$ . Recall from Section 2 that for a fixed $d$ (the dimension of the problem), the expected computational cost until the first accepted proposal in stationary is inversely proportional to the $\\epsilon\\,\\mathbb{E}[A(q,\\epsilon)]$ . Thus, in Figure 2a, we plot the average curves (after interpolating 5 random runs) of the efficiency (up to a multiplicative constant) function $f(a_{\\epsilon})=\\epsilon\\,a_{\\epsilon}$ , where $a_{\\epsilon}$ is the acceptance rate of the step size $\\epsilon$ . ", "page_idx": 8}, {"type": "text", "text": "Figure 2a shows that HMC for analytic networks is much more efficient than their ReLU counterparts. For every reasonable value of the target acceptance rate, the computational costs for ReLU networks are much higher than that of the sigmoid network. At the empirically optimal acceptance rate $(\\sim0.75)$ , the difference in performance is by a factor of more than 4. We also note that the optimal empirical acceptance rates for sigmoid, ReLU, and leaky ReLU networks in this simulation are 0.755, 0.6525, and 0.724, respectively. These empirical values are higher than their theoretical quantities (0.651 for sigmoid networks and 0.45 for ReLUbased networks), but are consistent with the relaxed ranges using upper and lower bounds of the efficiency $(0.6\\leq a_{\\epsilon}\\leq0.9$ for sigmoid and $0.4\\leq a_{\\epsilon}\\leq0.75$ for ReLU-based networks). ", "page_idx": 8}, {"type": "text", "text": "Effects of dimensionality. This simulation aims to study the effects of the dimension $d$ of the parameter space as well as the network architecture on the acceptance rate of HMC. For this purpose, we consider two types of architectures: (i) shallow networks with one hidden layer containing either 10, 50, 100, 200, or 400 nodes, and (ii) deep networks with 1, 2, 3, or 4 hidden layers, ", "page_idx": 8}, {"type": "image", "img_path": "aBmiyi7iA7/tmp/03f7dd603aa710cac809d7921d9c222364855455d06b02012d0f97a1888882a6.jpg", "img_caption": ["Figure 3: Acceptance rate of HMC with respect to the number of model parameters on shallow and deep neural networks with different activation functions. HMC on shallow networks generally has lower acceptance rates than deep networks of the same size. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "each of which contains 20 nodes. For each network, we run HMC with $L=200$ and $\\epsilon=0.001$ while keeping other hyper-parameters the same as in previous simulations. ", "page_idx": 8}, {"type": "text", "text": "We plot the average acceptance rate with respect to the number of model parameters in Figure 3. From this figure, when $d$ increases (either by increasing width or depth), all models exhibit some decays in acceptance rates. However, ReLU and leaky ReLU networks become unstable rather quickly, while the sigmoid network can perform relatively well in the same settings. The results also indicate that Bayesian learning with HMC for wide networks seems more difficult than for deep networks of the same number of parameters, hinting that there are other geometric forces in place other than the dimensionality of the problem. ", "page_idx": 8}, {"type": "table", "img_path": "aBmiyi7iA7/tmp/0c0eed7ce2dd943f436dc126b1518cf37081fcb749484c1a28fafcb3a8e36dbb.jpg", "table_caption": ["Table 1: The average acceptance rate, average test MSE, and average test MSE at the best acceptance rate for the three activation functions on the UTKFace dataset. The sigmoid network has better average acceptance rate and MSE than the ReLU-based networks, although all activation functions have nearly the same MSE at their best acceptance rate. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.2 UTKFace Dataset ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In addition to the synthetic dataset above, we also conduct experiments to validate our theoretical findings on a subset of the real-world UTKFace dataset (Zhang et al., 2017). This is an image regression dataset where we need to predict the age of a person given an image of their face. Using this dataset, we will compare the efficiency curves, the acceptance rates, and the mean squared errors (MSEs) of HMC sampling on the sigmoid, ReLU, and leaky ReLU networks. Details of our experiment settings are given in Appendix A.6. ", "page_idx": 9}, {"type": "text", "text": "In Figure 2b, we plot the efficiency versus acceptance rate curves in an experiment similar to that with the synthetic data above. The figure shows that the sigmoid curve is more efficient than the ReLU and leaky ReLU curves. The optimal acceptance rates for sigmoid, ReLU, and leaky ReLU networks are 0.813, 0.797, and 0.627 respectively. In Table 1, we show the average acceptance rate, average MSE, and the average MSE at the best acceptance rate for each network type. From the table, the sigmoid network has the highest acceptance rate as well as the lowest MSE. However, if we tune HMC to the best empirical acceptance rate in each network type, their MSEs become very similar. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and Future Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We analyzed the error rates of the HMC algorithm with leapfrog integrator for Bayesian neural network inference and showed, through theoretical analyses and experiments, that HMC on ReLUbased networks is inefficient compared to analytical networks. Our results highlight that for HMC, non-differentiability is not an issue that can be ignored, even if singularity only occurs on a set of measure zero. Several aspects of the paper could be the subjects of future works. First, since HMC accumulates a rejection rate of order $\\bar{\\Omega}(N\\epsilon)$ , where $N$ is associated with the number of times the dynamics cross a surface of non-differentiability, the characterization of this quantity and its dependency on the network architecture play a central role in studying the efficiency of this algorithm. As noted in Section 3, it is known that the average number of input regions intersecting with a line segment through the input space is linear in the number of neurons (Hanin and Rolnick, 2019). Thus, a similar result for the number of polynomial regions on the parameter space for fixed inputs would shed light into the decays of the acceptance rates in Figure 3. Another potential future work is to extend the general ideas in this paper to other models with non-differentiable components, such as the max-pooling layers in convolutional neural networks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "LSTH was supported by the Canada Research Chairs program and the Engineering Research Council of Canada (NSERC) Discovery Grants. VD was supported by a startup fund from the University of Delaware, a University of Delaware Research Foundation\u2019s Strategic Initiatives Grant, and National Science Foundation grant DMS-1951474. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Afshar, H. M. and J. Domke (2015). Reflection, refraction, and Hamiltonian Monte Carlo. In Conference on Neural Information Processing Systems (NeurIPS).   \nAfshar, H. M., R. Oliveira, and S. Cripps (2021). Non-volume preserving Hamiltonian Monte Carlo and No-U-Turn samplers. In International Conference on Artificial Intelligence and Statistics (AISTATS).   \nBerner, J., D. Elbr\u00e4chter, P. Grohs, and A. Jentzen (2019). Towards a regularity theory for ReLU networks \u2013 chain rule and global error estimates. In International Conference on Sampling Theory and Applications (SampTA).   \nBertoin, D., J. Bolte, S. Gerchinovitz, and E. Pauwels (2021). Numerical influence of ReLU\u2019(0) on backpropagation. In Conference on Neural Information Processing Systems (NeurIPS).   \nBeskos, A., N. Pillai, G. Roberts, J. Sanz-Serna, and A. Stuart (2013). Optimal tuning of the hybrid Monte Carlo algorithm. Bernoulli 19(5A), 1501\u20131534.   \nBetancourt, M., S. Byrne, and M. Girolami (2014). Optimizing the integrator step size for Hamiltonian Monte Carlo. arXiv preprint arXiv:1411.6669.   \nBetancourt, M., S. Byrne, S. Livingstone, and M. Girolami (2017). The geometric foundations of Hamiltonian Monte Carlo. Bernoulli 23(4A), 2257\u20132298.   \nBianchi, P., W. Hachem, and S. Schechtman (2022). Convergence of constant step stochastic gradient descent for non-smooth non-convex functions. Set-Valued and Variational Analysis 30(3), 1117\u20131147.   \nBolte, J. and E. Pauwels (2020). A mathematical model for automatic differentiation in machine learning. In Conference on Neural Information Processing Systems (NeurIPS).   \nCampbell, A., W. Chen, V. Stimper, J. M. Hernandez-Lobato, and Y. Zhang (2021). A gradient based strategy for Hamiltonian Monte Carlo hyperparameter optimization. In International Conference on Machine Learning (ICML).   \nCobb, A. D. and B. Jalaian (2021). Scaling Hamiltonian Monte Carlo inference for Bayesian neural networks with symmetric splitting. In Conference on Uncertainty in Artificial Intelligence (UAI).   \nDhulipala, S. L., Y. Che, and M. D. Shields (2023). Efficient Bayesian inference with latent Hamiltonian neural networks in No-U-Turn sampling. Journal of Computational Physics 492, 112425.   \nDinh, V., A. Bilge, C. Zhang, and F. A. Matsen IV (2017). Probabilistic path Hamiltonian Monte Carlo. In International Conference on Machine Learning (ICML).   \nDuane, S., A. D. Kennedy, B. J. Pendleton, and D. Roweth (1987). Hybrid Monte Carlo. Physics Letters B 195(2), 216\u2013222.   \nHanin, B. and D. Rolnick (2019). Complexity of linear regions in deep networks. In International Conference on Machine Learning (ICML).   \nHern\u00e1ndez-Lobato, J. M. and R. Adams (2015). Probabilistic backpropagation for scalable learning of Bayesian neural networks. In International Conference on Machine Learning (ICML).   \nHoffman, M., A. Radul, and P. Sountsov (2021). An adaptive-MCMC scheme for setting trajectory lengths in Hamiltonian Monte Carlo. In International Conference on Artificial Intelligence and Statistics (AISTATS).   \nHuber, M. F. (2020). Bayesian perceptron: Towards fully Bayesian neural networks. In IEEE Conference on Decision and Control (CDC).   \nJi, S., J. Koll\u00e1r, and B. Shiffman (1992). A global Lojasiewicz inequality for algebraic varieties. Transactions of the American Mathematical Society 329(2), 813\u2013818.   \nLeimkuhler, B. and S. Reich (2005). Simulating Hamiltonian Dynamics. Cambridge University Press.   \nMaclaurin, D., D. Duvenaud, and R. P. Adams (2015). Autograd: Effortless gradients in Numpy. In ICML AutoML Workshop.   \nNeal, R. M. (1995). Bayesian learning for neural networks. Ph. D. thesis, University of Toronto.   \nNeal, R. M. (2011). MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 113\u2013162.   \nNijkamp, E., R. Gao, P. Sountsov, S. Vasudevan, B. Pang, S.-C. Zhu, and Y. N. Wu (2021). MCMC should mix: Learning energy-based model with neural transport latent space MCMC. In International Conference on Learning Representations (ICLR).   \nNishimura, A., D. B. Dunson, and J. Lu (2020). Discontinuous Hamiltonian Monte Carlo for discrete parameters and discontinuous likelihoods. Biometrika 107(2), 365\u2013380.   \nPakman, A. and L. Paninski (2013). Auxiliary-variable exact Hamiltonian Monte Carlo samplers for binary distributions. In Conference on Neural Information Processing Systems (NeurIPS).   \nRoberts, G. O. and J. S. Rosenthal (1998). Optimal scaling of discrete approximations to Langevin diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 60(1), 255\u2013268.   \nSanz-Serna, J.-M. and M.-P. Calvo (2018). Numerical Hamiltonian problems, Volume 7. Courier Dover Publications.   \nSeiler, C., S. Rubinstein-Salzedo, and S. Holmes (2014). Positive curvature and Hamiltonian Monte Carlo. In Conference on Neural Information Processing Systems (NeurIPS).   \nSkeel, R. D. (1999). Integration schemes for molecular dynamics and related applications. The Graduate Student\u2019s Guide to Numerical Analysis\u2019 98: Lecture Notes from the VIII EPSRC Summer School in Numerical Analysis, 119\u2013176.   \nYang, J., G. O. Roberts, and J. S. Rosenthal (2020). Optimal scaling of random-walk Metropolis algorithms on general target distributions. Stochastic Processes and their Applications 130(10), 6094\u20136132.   \nZhang, Z., Y. Song, and H. Qi (2017). Age progression/regression by conditional adversarial autoencoder. In IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR).   \nZhou, G. (2020). Mixed Hamiltonian Monte Carlo for mixed discrete and continuous variables. In Conference on Neural Information Processing Systems (NeurIPS). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We consider the formulation of one leapfrog step: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p_{1/2}=p_{0}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{0})}\\\\ {\\displaystyle q_{1}=q_{0}+\\epsilon p_{1/2}}\\\\ {\\displaystyle p_{1}=p_{1/2}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Reversibility. If we reverse the momentum and go from $(q_{1},-p_{1})$ to $(q_{2},p_{2})$ then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{p_{1/2}^{\\prime}=\\displaystyle-p_{1}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{1})}}\\\\ {{q_{2}=q_{1}+\\epsilon p_{1/2}^{\\prime}}}\\\\ {{p_{2}=p_{1/2}^{\\prime}-\\displaystyle\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{2}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We note that ", "page_idx": 12}, {"type": "equation", "text": "$$\nq_{2}=q_{1}+\\epsilon\\left(-p_{1}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{1})\\right)=q_{1}-\\epsilon p_{1/2}=q_{0}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{2}=p_{1/2}^{\\prime}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{2})=-p_{1}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{1})-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{0})=-p_{0}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, leapfrog is reversible as long as $\\frac{\\partial U}{\\partial q}$ is a well-defined deterministic function. ", "page_idx": 12}, {"type": "text", "text": "Volume preservation. We consider $(q_{1},p_{1})$ as function of $(q_{0},p_{0})$ and note that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial q_{1}}{\\partial q_{0}}(q_{0},p_{0})=1-\\frac{\\epsilon^{2}}{2}\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{0}),\\;\\;\\;\\;\\;\\frac{\\partial q_{1}}{\\partial p_{0}}(q_{0},p_{0})=\\epsilon,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\gamma_{1}}{\\gamma_{q_{0}}}(q_{0},p_{0})=-\\frac{\\epsilon}{2}\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{0})-\\frac{\\epsilon}{2}\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{1})\\frac{\\partial q_{1}}{\\partial q_{0}}(q_{0},p_{0}),\\quad\\frac{\\partial p_{1}}{\\partial p_{0}}(q_{0},p_{0})=1-\\frac{\\epsilon}{2}\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{1})\\frac{\\partial q_{1}}{\\partial p_{0}}(q_{0},p_{0}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, the Jacobian of the transformation is ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left[1-\\frac{\\epsilon^{2}}{2}\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{0})\\right]\\left[1-\\frac{\\epsilon^{2}}{2}\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{1})\\right]-\\epsilon\\left[-\\frac{\\epsilon}{2}\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{0})-\\frac{\\epsilon}{2}\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{1})\\left(1-\\frac{\\epsilon^{2}}{2}\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{0})\\right)\\right],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which is equal to 1. ", "page_idx": 12}, {"type": "text", "text": "We thus can conclude that leapfrog transformation preserves volume, as long as the second derivative of $U$ is well-defined and is compatible with the chain rule. Specifically, we need ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial q_{0}}\\left(\\frac{\\partial U}{\\partial q}(q_{1})\\right)=\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{1})\\frac{\\partial q_{1}}{\\partial q_{0}}(q_{0})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial p_{0}}\\left(\\frac{\\partial U}{\\partial q}(q_{1})\\right)=\\frac{\\partial^{2}U}{\\partial q^{2}}(q_{1})\\frac{\\partial q_{1}}{\\partial p_{0}}(q_{0}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We will use an induction argument on the layers (indexed by $l$ ) of the feed-forward neural networks to prove that for a fixed input $x$ , $h_{l}(x)$ is locally smooth except on a union of multilinear surfaces ", "page_idx": 12}, {"type": "equation", "text": "$$\nS_{l}=\\bigcup_{i\\in I_{l}}S_{i}^{l},\\ \\ S_{i}^{l}=\\{q:f_{i}^{l}(q)=0\\},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which decomposes the parameter space into open regions ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Omega\\setminus S_{l}=\\bigcup_{j\\in J_{l}}T_{j}^{l}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "on which $h_{l}(x)$ is multilinear. ", "page_idx": 13}, {"type": "text", "text": "For $l=1$ , we note that $h_{1}$ (as a function of parameters) is only non-differentiable on ", "page_idx": 13}, {"type": "equation", "text": "$$\nS_{1}=\\bigcup_{j=1}^{d_{1}}\\left\\{(A_{1},b_{1}):A_{1}^{j}\\cdot x+b_{1}^{j}=0\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Omega\\setminus S_{1}=\\bigcup_{\\sigma\\in\\{-1,1\\}^{d_{1}}}\\{(A_{1},b_{1}):\\mathrm{sign}(A_{1}^{j}\\cdot x+b_{1}^{j})=\\sigma_{j},\\ \\ \\forall1\\leq j\\leq d_{1}\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Assume that the statement is correct for $l$ , we recall that ", "page_idx": 13}, {"type": "equation", "text": "$$\nh_{l+1}=\\sigma(A_{l+1}\\cdot h_{l}+b_{l+1}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We consider ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bigcup_{j\\in J_{l}}\\bigcup_{\\sigma\\in\\{-1,1\\}^{d_{1}}}T_{j}^{l}\\times\\{(A_{l+1},b_{l+1}):\\mathrm{sign}(A_{1}^{k}\\cdot h_{l}+b_{1}^{k})=\\sigma_{k},\\ \\ \\forall1\\leq k\\leq d_{l+1}\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and note that on each of the sets in the expression above, $h_{l+1}$ is a parametric linear transformation of $h_{l}(x)$ , and is thus multilinear. ", "page_idx": 13}, {"type": "text", "text": "The complement of this set is a subset of the union of ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bigcup_{i\\in I_{l}}[S_{i}^{l}\\times\\mathbb{R}^{d_{l+1}}]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bigcup_{j\\in J_{l}}\\bigcup_{k=1}^{d_{l+1}}T_{j}^{l}\\times\\{(A_{l+1},b_{l+1}):A_{1}^{j}\\cdot h_{l}+b_{1}^{j}=0\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This completes the proof. ", "page_idx": 13}, {"type": "text", "text": "A.3 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Consider a leapfrog step starting at $\\left(q_{0},p_{0}\\right)$ and ending at $(q_{1},p_{1})$ and crosses the surfaces of non-differentiability at $z_{1},z_{2},\\ldots,z_{k}$ at time $\\epsilon_{1},\\epsilon_{2},\\ldots,\\epsilon_{k}$ , where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p_{1/2}=p_{0}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{0})}\\\\ {\\displaystyle q_{1}=q_{0}+\\epsilon p_{1/2}}\\\\ {\\displaystyle p_{1}=p_{1/2}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Delta K=K(p_{1})-K(p_{0})=\\frac{1}{2}\\left(||p_{1}||^{2}-||p_{0}||^{2}\\right)}\\\\ {\\displaystyle=\\frac{1}{2}\\left(\\left|\\left|p_{0}-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{0})-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q_{1})\\right|^{2}-||p_{0}||^{2}\\right)}\\\\ {\\displaystyle=-\\frac{\\epsilon}{2}p_{0}\\cdot\\left(\\frac{\\partial U}{\\partial q}(q_{0})+\\frac{\\partial U}{\\partial q}(q_{1})\\right)+\\left\\|\\frac{\\partial U}{\\partial q}\\right\\|_{\\infty}^{2}\\mathcal{O}(\\epsilon^{2})}\\\\ {\\displaystyle=-\\frac{\\epsilon}{2}p_{1/2}\\cdot\\left(\\frac{\\partial U}{\\partial q}(q_{0})+\\frac{\\partial U}{\\partial q}(q_{1})\\right)+\\left\\|\\frac{\\partial U}{\\partial q}\\right\\|_{\\infty}^{2}\\mathcal{O}(\\epsilon^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Delta U=\\sum_{i=0}^{k}\\int_{\\epsilon_{i}}^{\\epsilon_{i+1}}\\frac{\\partial U}{\\partial q}(q_{0}+t p_{1/2})\\cdot p_{1/2}d t}\\\\ {\\displaystyle=p_{1/2}\\cdot\\left(\\sum_{i=0}^{k}\\big(\\epsilon_{i+1}-\\epsilon_{i}\\big)\\frac{\\partial U^{+}}{\\partial q}(z_{i})\\right)+\\left\\|\\frac{\\partial^{2}U}{\\partial q^{2}}\\right\\|_{\\infty}\\mathcal{O}(\\epsilon^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\epsilon_{0}=0,\\epsilon_{k+1}=\\epsilon,z_{0}=q_{0},z_{k+1}=q_{1}$ . Here, $\\frac{\\partial U^{-}}{\\partial q}(z)$ and $\\frac{\\partial U^{+}}{\\partial q}(z)$ denote the gradients at $z$ on the two differentiable regions before and after the incidence, respectively. ", "page_idx": 14}, {"type": "text", "text": "We note that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{k}(\\varepsilon_{i+1}-\\varepsilon_{i})\\frac{\\partial U^{+}}{\\partial q}(z_{i})}\\\\ &{=\\displaystyle\\sum_{i=0}^{k}\\varepsilon_{i+1}\\frac{\\partial U^{+}}{\\partial q}(z_{i})-\\sum_{i=0}^{k}\\varepsilon_{i}\\frac{\\partial U^{+}}{\\partial q}(z_{i})}\\\\ &{=\\displaystyle\\sum_{i=0}^{k}\\varepsilon_{i+1}\\frac{\\partial U^{-}}{\\partial q}(z_{i+1})-\\sum_{i=0}^{k}\\varepsilon_{i}\\frac{\\partial U^{+}}{\\partial q}(z_{i})+\\left\\|\\frac{\\partial U^{-}}{\\partial q^{2}}\\right\\|_{\\infty}\\mathcal{O}(\\varepsilon^{2})}\\\\ &{=\\displaystyle\\sum_{i=1}^{k+1}\\varepsilon_{i}\\frac{\\partial U^{+}}{\\partial q}(z_{i})-\\sum_{i=0}^{k}\\varepsilon_{i}\\frac{\\partial U^{+}}{\\partial q}(z_{i})+\\left\\|\\frac{\\partial U^{-}}{\\partial q^{2}}\\right\\|_{\\infty}\\mathcal{O}(\\varepsilon^{2})}\\\\ &{=\\displaystyle\\sum_{i=1}^{k}\\varepsilon_{i}\\frac{\\partial U^{-}}{\\partial q}(z_{i})-\\sum_{i=0}^{k}\\varepsilon_{i}\\frac{\\partial U^{+}}{\\partial q}(z_{i})-\\epsilon_{i}\\frac{\\partial U^{-}}{\\partial q}(z_{i})+\\epsilon_{i+1}\\frac{\\partial U^{-}}{\\partial q}(z_{i+1})+\\left\\|\\frac{\\partial U^{-}}{\\partial q^{2}}\\right\\|_{\\infty}\\mathcal{O}(\\varepsilon^{2})}\\\\ &{=-\\sum_{i=1}^{k}\\epsilon_{i}\\left(\\frac{\\partial U^{+}}{\\partial q}(z_{i})-\\frac{\\partial U^{-}}{\\partial q}(z_{i})\\right)+\\epsilon_{i}\\frac{\\partial U^{-}}{\\partial q}(z_{i})+\\left\\|\\frac{\\partial\\mathcal{O}^{T}}{\\partial q^{2}}\\right\\|_{\\infty}\\mathcal{O}(\\varepsilon^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the third equality comes form the fact that $\\frac{\\partial U}{\\partial q}(z)$ is Lipschitz in each domain of continuity. We deduce that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H(q_{1})-H(q_{0})}\\\\ &{=p_{1/2}\\cdot\\left(-\\displaystyle\\sum_{i=1}^{k}\\epsilon_{i}\\left(\\frac{\\partial U^{+}}{\\partial q}(z_{i})-\\frac{\\partial U^{-}}{\\partial q}(z_{i})\\right)+\\frac{\\epsilon}{2}\\left(\\frac{\\partial U}{\\partial q}(q_{1})-\\frac{\\partial U}{\\partial q}(q_{0})\\right)\\right)+\\left\\|\\frac{\\partial^{2}U}{\\partial q^{2}}\\right\\|_{\\infty}\\mathcal{O}(\\epsilon^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial U}{\\partial q}(q_{1})-\\frac{\\partial U}{\\partial q}(q_{0})=\\sum_{i=1}^{k+1}\\left(\\frac{\\partial U^{-}}{\\partial q}(z_{i})-\\frac{\\partial U^{+}}{\\partial q}(z_{i-1})\\right)+\\displaystyle\\sum_{i=1}^{k}\\left(\\frac{\\partial U^{+}}{\\partial q}(z_{i})-\\frac{\\partial U^{-}}{\\partial q}(z_{i})\\right)}\\\\ {\\displaystyle=\\sum_{i=1}^{k}\\left(\\frac{\\partial U^{+}}{\\partial q}(z_{i})-\\frac{\\partial U^{-}}{\\partial q}(z_{i})\\right)+\\left\\|\\frac{\\partial^{2}U}{\\partial q^{2}}\\right\\|_{\\infty}\\mathcal{O}(\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, the local approximation error incurred can be estimated by ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{1/2}\\cdot\\sum_{i=1}^{k}\\left(\\frac{\\epsilon}{2}-\\epsilon_{i}\\right)\\left(\\frac{\\partial U^{+}}{\\partial q}(z_{i})-\\frac{\\partial U^{-}}{\\partial q}(z_{i})\\right)+\\left\\|\\frac{\\partial^{2}U}{\\partial q^{2}}\\right\\|_{\\infty}\\mathcal{O}(\\epsilon^{2}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.4 Proof of Lemma 3.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We note that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\{(q,p):f\\left(q_{0}+\\frac{\\epsilon}{2}(p-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q))\\right)=0\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "aBmiyi7iA7/tmp/26c25f7129bfe3173be703eec7340e6ec14483871ee6bf04e116a889be3d8d40.jpg", "img_caption": ["Figure 4: Efficiency as functions of acceptance probability for symplectic integrator of second order (left, error rate $\\bar{\\mathcal{O}}(\\bar{\\epsilon^{2}}))$ and first order (right, error rate $\\Theta(\\epsilon))$ . The $y$ -axes of the graphs are presented up to unknown multiplicative constants and cannot be directly compared. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "aBmiyi7iA7/tmp/75f4ac8f04dc1a271ea5cac1ff0e35bb90b8805970daedcd8024f0bf2b896a33.jpg", "img_caption": ["Figure 5: Upper bound on efficiency as functions of acceptance probability for symplectic integrator of second order (left, error rate $\\bar{\\mathcal{O}}(\\epsilon^{2}))$ and first order (right, error rate $\\Theta(\\epsilon))$ . The $y$ -axes of the graphs are presented up to unknown multiplicative constants and cannot be directly compared. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "has measure zero. ", "page_idx": 15}, {"type": "text", "text": "Consider $(q,p)\\in A$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nf\\left(q+\\epsilon_{1}(p-\\frac\\epsilon2\\frac{\\partial U}{\\partial q}(q))\\right)=0,\\;\\;\\;\\epsilon_{1}\\in\\left((1-a)\\frac{\\epsilon}{2},(1+a)\\frac{\\epsilon}{2}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The Lojasewicz inequality (Ji et al., 1992) implies that there exists $\\alpha\\geq2$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C\\left.d((q,p),\\mathcal{V})^{\\alpha}\\leq\\displaystyle\\left|f\\left(q_{0}+\\frac{\\epsilon}{2}(p-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q))\\right)\\right|\\leq\\left|\\epsilon_{1}-\\frac{\\epsilon}{2}\\right|\\left\\|p-\\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q)\\right|}\\\\ {\\leq\\frac{a\\epsilon}{2}\\left(M+\\frac{\\epsilon}{2}\\|\\frac{\\partial U}{\\partial q}\\|_{\\infty}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We conclude that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu(A)\\leq(C a M\\epsilon)^{1/\\alpha}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since leapfrog steps preserve Lebesgue volume, the bound on $B_{M,b}$ follows from a simple union bound. ", "page_idx": 15}, {"type": "text", "text": "A.5 Proof of Proposition 3.5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Using similar arguments as those of Theorem 3.3 and Lemma 3.4, we can show that HMC with leapfrog integrator with ", "page_idx": 15}, {"type": "equation", "text": "$$\nP(q)=\\exp\\left(-\\sum_{i=1}^{d}V(q_{i})\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "has a global error rate of $\\Theta(\\epsilon)$ since $V(q)$ is piece-wise affine. ", "page_idx": 16}, {"type": "text", "text": "Using the same arguments presented in the proof of Theorem 3.6 of Beskos et al. (2013), the acceptance probability $A(q)$ of HMC (starting at $q$ ) is given by $\\operatorname*{max}(1,e^{R_{d}(q)})$ where ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{d}(q)=-\\sum_{i=1}^{d}\\Delta(q_{i},\\epsilon)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $\\Delta(q_{i},\\epsilon)$ is the acceptance probability of a Hamitonian particle for the (one-dimensional) potential function $V(q)$ starting at $q_{i}$ . ", "page_idx": 16}, {"type": "text", "text": "We first use Lemma 3.3 of Beskos et al. (2013), which shows that for any volume-preserving, timereversible numerical integrator of the Hamiltonian equations of order $O(\\epsilon^{v})$ , the expectation and the variation of the acceptance probability (with respect to $P$ ) is of order $\\mathcal{O}(\\epsilon^{2v})$ . Moreover, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to0}\\frac{\\mathbb{E}[\\Delta(q_{i},\\epsilon)]}{\\epsilon^{2}}=\\mu,\\ \\ \\operatorname*{lim}_{\\epsilon\\to0}\\frac{\\mathbb{E}[\\Delta(q_{i},h)^{2}]}{\\epsilon^{2}}=\\Sigma\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mu=\\Sigma/2$ . ", "page_idx": 16}, {"type": "text", "text": "Due to the structure of the target density and stationarity, the terms $\\Delta(q_{i},h)$ are i.i.d. random variables. Since the expectation and variance of $\\Delta(q_{i},h)$ are both ${\\mathcal{O}}(\\epsilon^{2})$ and we have $d$ terms, the natural scaling to obtain a distributional limit is given by $\\epsilon=l d^{-1/2}$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{d}\\rightarrow\\mathcal{N}\\left(-\\frac{1}{2}l^{2}\\Sigma,l^{2}\\Sigma\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "in distribution, and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{d\\to\\infty}\\mathbb{E}[A(q)]=2\\Phi(-l{\\sqrt{\\Sigma}}/2):=a(l)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Phi$ is the cumulative distribution function of standard normal distributions. ", "page_idx": 16}, {"type": "text", "text": "We note that the number of leapfrog steps of length $\\epsilon$ needed to compute a proposal is $L=T/\\epsilon$ , and the computing time for a single proposal will be approximately ", "page_idx": 16}, {"type": "equation", "text": "$$\nC_{0}\\cdot d\\cdot\\frac{T}{l d^{-1/2}}=\\frac{T}{l}d^{3/2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $C_{0}$ measures the cost for one leapfrog step in one dimension. Given a location $q$ , the number of proposals until acceptance follows a geometric distribution with probability of success $e^{R_{d}(q)}$ . Using Jensen\u2019s inequality, the expected computing time until the first accepted proposal in stationary is bounded from below by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{T}{l\\cdot a(l)}d^{3/2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A sensible approach for optimal tuning of HMC is to minimize the efficiency $l\\cdot a(l)$ . Similar to classical cases, while $\\Sigma$ is unknown, the optimal acceptance rate is independent of $\\Sigma$ and can be obtained using simple numerical investigations of this function. The result is presented in Figure 4, where the optimal acceptance probability of leapfrog HMC in piece-wise affine cases is approximately 0.45. ", "page_idx": 16}, {"type": "text", "text": "Betancourt et al. (2014) extend the analyses of Beskos et al. (2013) to include an upper bound and suggest that the target average acceptance probability of 0.651 can be relaxed to $0.6\\,\\leq\\,\\alpha\\,\\leq\\,0.9$ . Following the discussions from Betancourt et al. (2014), we can also relax the range of target average acceptance probability to $0.4\\leq\\alpha\\leq0.75$ , where the optimum upper bounds (0.8 for second order and 0.715 for first order) of Betancourt et al. (2014) are presented in Figure 5. ", "page_idx": 16}, {"type": "text", "text": "A.6 Additional Details of the Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In all experiments, the error bars are one standard error of the mean over 5 different runs of the algorithm with different random seeds. The experiments were run on a single CPU machine. Each experiment took 3-4 hours to complete, except for the first simulation on synthetic data, which took around 5 days on one CPU machine. ", "page_idx": 16}, {"type": "table", "img_path": "aBmiyi7iA7/tmp/aab6d11302db21aef0a3d3718ff119e0c55e9734e27d93aeeb6bc47d4e113b5f.jpg", "table_caption": ["Table 2: Acceptance rates of HMC on the synthetic dataset with respect to the number of leapfrog steps $L$ and step size $\\epsilon$ on BNNs with different activation functions. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "aBmiyi7iA7/tmp/33a2d61ae7abc0366a62ea2033a3e0d10fd2f37d88512a507cf18889842a1aba.jpg", "img_caption": ["Figure 6: The synthetic dataset used in our simulations. This dataset contains 100 data points where $x\\sim$ uniform $(0,4)$ and $y\\sim\\mathcal{N}(\\cos(2x),0.1^{2})$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "For the UTKFace dataset, we select the subset that contains male Asian faces with age between 6 and 92. The subset is then randomly split into a training set (167 images) and a test set (100 images). All the input images are converted into grayscale and resized to $32\\times32$ , which are then flattened into a vector of length 1024 to be used with MLP models. All the labels are scaled by 1/100 so that they are in the range $[0,1]$ . In this experiment, we fix $T\\:=\\:0.01$ and vary $\\epsilon\\in\\{0.00005,0.0001\\dot{0},0.00015,\\dots,\\dot{0.}00\\dot{0}40\\}$ . For each run of the HMC, we sample 300 parameter vectors from the posterior after a burn-in period of 50 samples. The MSEs are computed on the test set using these samples. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Sections 3 and 4. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See the last paragraph of Section 3 and the conclusion section. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Sections 2, 3 and the appendix. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] Justification: See Section 4 and Appendix A.6. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: We would like to keep the code confidential. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Section 4 and Appendix A.6. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Section 4 and Appendix A.6. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Appendix A.6. ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and made an effort to ensure that every aspect of the work conforms with it. ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: The topic of our work is mainly theoretical with no direct societal impacts. ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: We do not release any data or model. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We cited the paper that introduced the Autograd package used in our experiments in Section 4. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not release new assets. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve human subjects. ", "page_idx": 19}]