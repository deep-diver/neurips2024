[{"type": "text", "text": "Once Read is Enough: Domain-specific Pretraining-free Language Models with Cluster-guided Sparse Experts for Long-tail Domain Knowledge ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fang Dong1 Mengyi Chen1,\u2217 Jixian Zhou1,\u2217 Yubin Shi1 Yixuan Chen2 Mingzhi Dong3,1 Yujiang Wang2,\u2020 Dongsheng $\\mathbf{Li^{4}}$ Xiaochen Yang5 Rui Zhu6 Robert Dick7 Qin Lv8 Fan Yang9 Tun Lu1 Ning $\\mathbf{Gu^{1}}$ Li Shang1,\u2020 ", "page_idx": 0}, {"type": "text", "text": "1China and Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University 2 Oxford Suzhou Centre for Advanced Research, Suzhou, China 3 University of Bath 4 Microsoft Research Asia, Shanghai, China 5 School of Mathematics & Statistics, University of Glasgow 6 Bayes Business School, City St George\u2019s, University of London 7 Department of Electrical Engineering and Computer Science, University of Michigan 8 Department of Computer Science, University of Colorado Boulder 9 School of Microelectronics, Fudan University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Language models (LMs) only pretrained on a general and massive corpus usually cannot attain satisfying performance on domain-specific downstream tasks, and hence, applying domain-specific pretraining to LMs is a common and indispensable practice. However, domain-specific pretraining can be costly and time-consuming, hindering LMs\u2019 deployment in real-world applications. In this work, we consider the incapability to memorize domain-specific knowledge embedded in the general corpus with rare occurrences and \u201clong-tail\u201d distributions as the leading cause for pretrained LMs\u2019 inferior downstream performance. Analysis of Neural Tangent Kernels (NTKs) reveals that those long-tail data are commonly overlooked in the model\u2019s gradient updates and, consequently, are not effectively memorized, leading to poor domain-specific downstream performance. Based on the intuition that data with similar semantic meaning are closer in the embedding space, we devise a Cluster-guided Sparse Expert (CSE) layer to actively learn long-tail domain knowledge typically neglected in previous pretrained LMs. During pretraining, a CSE layer efficiently clusters domain knowledge together and assigns long-tail knowledge to designate extra experts. CSE is also a lightweight structure that only needs to be incorporated in several deep layers. With our training strategy, we found that during pretraining, data of long-tail knowledge gradually formulate isolated, \u201coutlier\u201d clusters in an LM\u2019s representation spaces, especially in deeper layers. Our experimental results show that only pretraining CSE-based LMs is enough to achieve superior performance than regularly pretrained-finetuned LMs on various downstream tasks, implying the prospects of domain-specific-pretraining-free language models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In natural language processing, it is a prevalent paradigm to pretrain language models (LMs) on a large-scale unlabeled corpus covering a plethora of knowledge, and those pretrained LMs have exhibited impressive performance in language tasks in the general domain [41]. When it comes to downstream tasks requiring specialized domain knowledge, e.g., legal search or medical question answering [23, 7], those models usually fail to expertise in such knowledge and cannot acquire desirable performance. As such, domain-specific pretraining on domain-specific datasets is deemed essential to fulfill pretrained LMs\u2019 potential in various downstream tasks [21, 14, 42, 36]. However, applying domain-specific pretraining on an LM could require domain expertise from humans, for instance, the involvement of a doctor for healthcare tasks [31], which can be costly and laborious. The associated catastrophic forgetting issue [26] could further complicate the domain-specific pretraining process. ", "page_idx": 1}, {"type": "image", "img_path": "manHbkpIW6/tmp/9a719521935d7337f01df055fa490857a4124188de98d70db2041bc3ed05ce29.jpg", "img_caption": ["Figure 1: a) The top 20 subreddits with the highest amount of data in the Reddit Comments Dataset, where a typical long-tail distribution can be observed. b) Language Models struggle to memorize long-tail domain knowledge during pretraining. The less frequently a sentence appears in the training corpus, the higher its perplexity, indicating that it is not effectively memorized. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this work, we re-visit the pretraining- domain-specific-pretraining paradigm and raise the following question: is domain-specific pretraining indispensable to LMs? Notably, the domain-specific knowledge necessary for various downstream tasks is usually embedded in the pretraining corpus of extensive information sources. Those pieces of domain-specific information may only appear a few times in the massive corpus, significantly less frequently than other ubiquitous and general knowledge, and there can be numerous pieces of such rare information, a distribution usually defined as \u201clongtail\u201d. In Fig 1(a), we plot the frequency of the top-20 subreddits count on Reddit Comments Dataset [2], and a typical long-tail distribution can be observed. Previous works have verified that LMs are not good learners of long-tail knowledge in the pretraining dataset with Question-Answering as the downstream task[20]. Our experiments, as shown in Figure 1(b), further illustrate that pretrained LMs do not adequately retain domain-specific knowledge in long-tail sequences which is evidenced by a surge in perplexity corresponding to decreased frequency score. This could result in inferior performance on downstream tasks. Domain-specific pretraining improves LMs\u2019 domain performance by providing a second lesson, which could be avoided if the first (pretraining) is appropriately delivered. ", "page_idx": 1}, {"type": "text", "text": "To unveil the hidden mechanisms under LM\u2019s incapability to learn long-tail domain-specific knowledge, we investigate the behaviors of a GPT on the Wikipedia dataset. We examine LMs\u2019 learning capabilities on long-tail data by analyzing the Neural Tangent Kernels (NTKs) of long-tail data and all data. Recent research [5, 40] has indicated that the updating of deep networks can be governed by the gradient direction corresponding to the principle eigenvector of an NTK matrix, which reflects the most common gradient-descending direction across the entire input space. Following those works, we consider an NTK\u2019s principle eigenvector (PE) gradient direction as a primary indicator of an LM\u2019s overall gradient-updating direction over a data space. Our analysis has revealed that the PE gradient direction of long-tail data, indicating the gradient-descending direction from long-tail knowledge, is generally diverged from that of overall data, which rules the overall updating of network parameters. The observation that long-tail data cannot substantially impact LMs\u2019 parametric updates under regular pretraining settings explains pretrained LMs\u2019 incompetence on domain-specific knowledge of rare occurrences, necessitating an effective solution. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To this end, we propose the Cluster-guided Sparse Expert (CSE) layer, an effective, efficient, and easy-to-implement approach to improve LMs\u2019 long-tail knowledge awareness. In a CSE layer, with intuition such that data with similar semantic meaning are closer in the embedding space, we perform efficient clustering on the embeddings to group data from different domains, and additional experts will be assigned to explicitly and appropriately memorize the information within those clusters. Models trained with CSE show pronounced cluster structure in the embedding space, where long-tail data forms small, outlier clusters. We empirically demonstrate that converting several deep layers into CSE ones can be enough to achieve satisfying results, such as the last two layers of GPT[28] or BERT[10], and the incurred computational costs are comparatively small and arguably acceptable. We have verified that pretrained CSE-based LMs have outperformed regularly pretrained domain-specific pretrained LMs on downstream tasks from various domains, which implies that domain-specific pretraining may not be essential if long-tail knowledge can be sufficiently learned. ", "page_idx": 2}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We have presented that datasets show a long-tail distribution, with domain-specific data in the long-tail, and revealed that long-tail data cannot substantially affect LMs\u2019 training, which is a leading cause of LMs\u2019 incompetence in learning rare, domain-specific knowledge. \u2022 We have devised a Cluster-guided Sparse Expert (CSE) architecture to better pretrain LMs to memorize the long-tail domain knowledge. With such a training strategy, LMs can effectively capture long-tail domain data in the representation space as outlier clusters, thereby enhancing their ability to handle less frequent contexts efficiently. \u2022 Promising performance on downstream tasks has verified the effectiveness of the proposed method, indicating that domain-specific pretraining may not be indispensable to LMs. ", "page_idx": 2}, {"type": "text", "text": "2 Analysis of Long-Tail Domain Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first elucidate the challenges associated with learning from long-tail data through gradient analysis. We then explore the embedding space using the Cluster-guided Sparse Expert (CSE) layer, which effectively captures the structural nuances of long-tail data. Furthermore, we examine the dynamics of these clustering structures, offering insights into how the learning processes of long-tail clusters adapt and evolve across various training stages and model layers. ", "page_idx": 2}, {"type": "text", "text": "2.1 Challenges in Learning Long-Tail Domain Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This subsection explores the significant challenges posed by long-tail domain data within language models (LMs). The primary issue stems from the divergence in gradient directions between long-tail data and the general gradient-updating trajectory of these models, which critically hampers effective learning. ", "page_idx": 2}, {"type": "text", "text": "2.1.1 Preliminaries and Definitions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Informed by seminal works [12, 19], we utilize Neural Tangent Kernels (NTKs) to scrutinize the gradient behavior of neural networks under a gradient descent training regime. The NTK represented as $\\Theta(\\mathcal{X},\\mathcal{X})$ , is defined as the outer product of the gradients of network outputs relative to its parameters $\\Theta(\\mathcal{X},\\mathcal{X})=J_{\\theta}(\\mathcal{X})J_{\\theta}(\\mathcal{X})^{\\top}$ , where $J_{\\theta}=\\nabla_{\\theta}f(\\mathcal{X};\\theta)$ denotes the Jacobian matrix of the function $f$ at the data points $\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "To determine the predominant gradient-descending direction across the input space, which is influenced by the gradient direction associated with the principal eigenvector of the NTK matrix, we first perform an eigenvalue decomposition of the NTK matrix. Recognized as a positive semi-definite real symmetric matrix, the NTK decomposes into $\\begin{array}{r}{\\Theta=\\mathbf{U}\\mathbf{A}\\mathbf{U}^{\\top}\\stackrel{\\smile}{=}\\sum_{i=1}^{n}\\lambda_{i}\\mathbf{\\dot{u}}_{i}\\mathbf{u}_{i}^{\\top}}\\end{array}$ . Here, $n$ represents the total number of training instances. The principal eigenvect or $\\mathbf{u}_{m a x}$ is identified as the vector corresponding to the maximum eigenvalue. Then the primary gradient direction for a given input set $\\mathcal{X}$ is $\\mathbf{g}_{\\theta}(\\bar{\\mathcal{X}})=\\mathbf{u}_{m a x}J_{\\theta}(\\mathcal{X})$ . Building upon the above preliminaries, we introduce the metric of ", "page_idx": 2}, {"type": "text", "text": "Gradient Consistency (GC) to evaluate the alignment between gradient directions for specific data subsets and the overall dataset. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Gradient Consistency $(G C)_{\\prime}$ ). Let $\\mathcal{X^{\\prime}}$ be a specific subset of the training set $\\mathcal{X}$ . The gradient consistency of $\\mathcal{X^{\\prime}}$ is evaluated by computing the cosine similarity between the most prevalent gradient direction of $\\mathcal{X^{\\prime}}$ and that of the entire dataset $\\mathcal{X}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nG C_{\\theta}(X^{\\prime})=\\frac{\\mathbf{g}_{\\theta}(\\mathcal{X})\\cdot\\mathbf{g}_{\\theta}(\\mathcal{X}^{\\prime})}{\\|\\mathbf{g}_{\\theta}(\\mathcal{X})\\|\\|\\mathbf{g}_{\\theta}(\\mathcal{X}^{\\prime})\\|}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A higher GC value indicates that the model\u2019s optimization updates are well-aligned with the needs of the specific subset $X^{\\prime}$ , suggesting focused and effective learning of this data. Conversely, a lower value indicates suboptimal learning of these data, pointing to potential areas for improvement in model training strategies. ", "page_idx": 3}, {"type": "text", "text": "2.1.2 Gradient Consistency (GC) Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We assess the sentences from Wikipedia on a standard GPT model using a sentence frequency score to gauge how frequently each sentence appears in the corpus. This score is calculated by averaging the frequency of its constituent tokens. Figure 2(a) displays the relationship between GC and sentence frequency score. Additionally, the figure includes a histogram that details how many percentages of sentences across the whole dataset fall into each frequency bin. ", "page_idx": 3}, {"type": "text", "text": "There is a significant correlation between gradient consistency and the frequency with which sentences appear in the corpus. Notably, for sentences less frequently encountered in the dataset, the model demonstrates substantial ineffectiveness in learning. As demonstrated, the GC value sharply declines from 0.8 to 0.4 as the sentence frequency score decreases from 0.3 to 0.2. Furthermore, the GC value continues to diminish as the sentence frequency score decreases further, indicating that the model\u2019s gradient descent direction struggles to align with the requirements of these rare sentences. ", "page_idx": 3}, {"type": "text", "text": "Our analysis indicates that the optimization requirements for long-tail sentences are significantly overlooked during standard pretraining, largely due to gradient conflicts between long-tail and common data. Research [38, 30] has demonstrated that these conflicts lead to suboptimal learning outcomes for the affected data. In typical pretraining, the gradient descent direction is dominated by common data, which prevents the model from effectively capturing the unique characteristics of long-tail domain data. This oversight significantly impairs the performance of LMs in learning domain-specific knowledge, particularly when dealing with rare occurrences, highlighting the need for a more effective solution. ", "page_idx": 3}, {"type": "image", "img_path": "manHbkpIW6/tmp/63c031c375b30e6e1c598d16199e774afc84cbffc9db4b31cde53dc4205a2da0.jpg", "img_caption": ["Figure 2: a) The correlation between sentence frequency score and gradient consistency in the baseline model. A histogram is also included showing how many percentage of sentences across the whole dataset fall into each frequency bin. For further analysis using 2/3-gram averages, please refer to the appendix D.1. b) A sampled embedding space containing 4 long-tail clusters, taken from our CSE layers. For more information on the detailed cluster contents, please refer to the appendix D.2. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.2 Embedding Space Analysis With Cluster-guided Sparse Expert (CSE) layer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Prior research[2] has shown that extensive domain-specific data reside within the long-tail distribution of a general pretraining corpus, as illustrated in Figure 1(a). These data, often semantically similar, are likely to cluster closely within the embedding space, facilitating potential aggregation for dedicated learning. However, our analysis in Section 2.1 underscores significant challenges in learning from long-tail data. Specifically, the model\u2019s gradient updates frequently fail to align with the optimization needs of these data, leading to their under-representation in the embedding space. Such misalignment obscures the inherent group structures that these domain data form based on their semantic similarities, thereby impeding dedicated learning efforts. ", "page_idx": 4}, {"type": "text", "text": "To address the issues outlined above and to facilitate a more effective examination of long-tail domain data in the embedding space, we propose the Cluster-guided Sparse Expert (CSE) layer. This layer groups proximate long-tail data points into clusters and directs them to specialized experts for dedicated learning. As demonstrated in Figure 3(a), the GC value of long-tail data in the baseline model initially increases at the beginning of the training stage but rapidly declines thereafter, indicating that the model\u2019s inability to capture the learning dynamics of long-tail data begins early in the training process. Our CSE layer capitalizes on the clustering structure at the point where the GC value peaks, subsequently taking effect to channel domain-specific clusters into dedicated learning pathways. Further details about this approach are provided in Section 3. ", "page_idx": 4}, {"type": "text", "text": "The clustering results from the CSE-based LM, shown in Figure 2(b), reveal four smaller clusters alongside a predominant one. Detailed analysis shows high domain coherence within the smaller clusters, each comprising sentences closely related to specific domains. The average sentence frequency score of these domain clusters falls into the long tail of the sentence frequency distribution, as shown in Figure 2(a). In contrast, the predominant cluster, colored in purple, contains a diverse mix of more common data and exhibits a higher average sentence frequency compared to the smaller clusters. Further analysis of sentences with frequency scores below 0.2 shows their random distribution across clusters, suggesting these extremely infrequent sentences may serve as noise in the learning process. ", "page_idx": 4}, {"type": "text", "text": "This analysis demonstrates that our proposed CSE-based architecture effectively groups long-tail data from the same domains for dedicated learning, fostering a domain-specific clustering structure within the embedding space. The long-tail domain clusters, distinct from clusters containing common data, show a higher degree of compactness and are clearly separated, highlighting the unique features embodied by these clusters. ", "page_idx": 4}, {"type": "text", "text": "2.3 Dynamic of Long-Tail Domain Clusters ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we explore the learning dynamics of long-tail domain data by tracking how clusters evolve across different training stages and model layers. We utilize DBSCAN clustering[29] to determine the number of clusters. ", "page_idx": 4}, {"type": "text", "text": "Long-tail clusters can be seen early in the training stage. As shown in Figure 3(b) and Figure 3(c), the number of clusters quickly peaks early in the training stage, along with a peak in the ratio of cluster distances to cluster radii. This indicates that our CSE-based architecture effectively promotes the formation of a clustering structure early on. ", "page_idx": 4}, {"type": "text", "text": "The swift emergence of these clusters signifies substantial model adaptation to global features at the start of training, allowing for effective differentiation between clusters. As training progresses, inter-cluster distances gradually decrease, suggesting a stabilization in the learning dynamics and a potential shift in focus toward refining intra-cluster nuances. ", "page_idx": 4}, {"type": "text", "text": "Long-tail clusters become more pronounced with increasing network depth. Figures 3(b) and 3(c) demonstrate that the number of clusters is consistently higher in the deeper layers compared to the lower layers, with the ratio of cluster distances to cluster radii escalating significantly in the last two layers and reaching its maximum in the final layer. This pattern indicates that clusters become increasingly distinct and better separated as they progress through the network\u2019s layers. The enhanced separation of clusters in deeper layers can be attributed to the hierarchical feature extraction inherent in deep neural networks. As data moves through successive layers, the network abstracts and compiles more complex features, transitioning from general to more specific attributes. This hierarchical processing allows the final layers to capture and enhance subtle distinctions between different data groups, leading to more defined and isolated clusters. This process not only underscores the capability of deep layers to refine and emphasize key features but also illustrates the network\u2019s efficiency in encoding progressively finer-grained information as layer depth increases. ", "page_idx": 4}, {"type": "image", "img_path": "manHbkpIW6/tmp/51cf56d67d37da6b27f1148c42e096f9f74ccc9601dd42f58b10f22032d26b9d.jpg", "img_caption": ["Figure 3: a) Evolution of the Gradient Consistency (GC) of long-tail data in the baseline model over the first 8000 training steps. GC scores beyond this range are omitted, as they consistently remain below 0.2. For details on the method used to select long-tail data, please refer to the appendix D.3. b) Evolution of number of clusters over training steps. c) The ratio of cluster distances to cluster radii over training steps, providing a measure of cluster structure clarity independent of norm values. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "manHbkpIW6/tmp/671c5ea2767f8e547efe839196aa7198f419a4e55d48161d904dfa2d6ed88981.jpg", "img_caption": ["Figure 4: a) Overview of the Cluster-guided Sparse Expert (CSE) layer. b) The cluster number fluctuation is mainly caused by the big common cluster. These four figures arranged sequentially from top to bottom, were sampled at every 10,000 steps throughout the process from the FFN of the 10-th layer in a GPT model. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3 Cluster-guided Sparse Expert (CSE) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To avoid the troublesome and costly domain-specific pretraining, we design a novel strategy, named Clsuter-guided Sparse Expert (CSE), to help the model capture the long-tail domain knowledge during pretraining. Since long-tail domain data show poor gradient consistency with overall data, we employ a sparse expert architecture within the Transformer model to assign data to different parameters, thereby avoiding the gradient conflict in each parameter group. This strategy can be applied to either attention or FFN. To dispatch data, with a straightforward and generally accepted intuition such that data with similar semantic meaning are closer in the embedding space, we design a very simple, efficient but effective online clustering algorithm operating concurrently with the language model pretraining, separate embeddings into different clusters, and use the outcome of this algorithm to instruct the dispatching of embeddings. The proposed algorithm is outlined in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Dimension Reduction In high-dimensional vector clustering, computational efficiency poses a significant challenge due to the $O(d^{2})$ complexity of computing vector distance where $d$ denotes the dimensionality. So, we employ the same way of dimension reduction as is discussed in Section 2 before applying clustering on the embeddings, using a Gaussian random initialized matrix to project embeddings to a low-dimensional space[22]. This process, grounded in the Johnson-Lindenstrauss Lemma, effectively preserves the pairwise distances between embeddings while reducing their dimensionality, thereby enhancing the efficiency of our clustering algorithm. ", "page_idx": 6}, {"type": "text", "text": "Initialization We commence by training a baseline dense model devoid of any expert structure. Our findings in Section 2 illuminate an initial rise in gradient consistency between long-tail domain data and the general dataset at the onset of training, subsequently followed by a downturn. Consequently, we adopt a warmup stage, letting the model learn the common features of long-tail and non-long-tail data. In our experiments, this process typically accounts for no more than $1\\%$ of the overall training. We then sample $N$ instances from the dataset and use its clustering result to initialize the cluster structure. We utilized DBSCAN ", "page_idx": 6}, {"type": "table", "img_path": "manHbkpIW6/tmp/467a75e21cd1dc43f0b0c2e0a3132144f980383cc766870a88e649358260f69f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "[29] in our experiments, a clustering algorithm that does not explicitly require the number of clusters. For every identified cluster, we document its centroid and define its radius as the average distance of all constituent data points from this central point ", "page_idx": 6}, {"type": "text", "text": "After this warm-up period, we fix the number of clusters and copy the module into cluster number copies. The module selection is introduced in the next paragraph. In our experiments, we noticed that the variations in the number of clusters were primarily driven by the splitting and merging of larger clusters, as illustrated in Figure 4(b); the smaller, long-tail clusters, however, remained largely unchanged. Consequently, adopting the initial clustering configuration directly, without further adjustments during training, was found to have no detrimental effect on model performance or the distribution of data handling. This approach capitalizes on the stability of the long-tail clusters and the dynamics of the larger ones, ensuring efficient data processing without compromising accuracy. ", "page_idx": 6}, {"type": "text", "text": "Select Layer Our motivation for performing clustering is rooted in the premise that semantically similar data tends to be closer. However, it is important to note that models learn the semantics of data progressively through layers; as we delve deeper into the model layers, the semantic information becomes increasingly rich, which may in turn amplify distinctions between data points. To quantify this variation, we apply our strategy only on layers with larger inter-cluster distances. Since the last 2 layers show a significant increase in inter-cluster distance, we apply our strategy in the last 2 layers, which is also the empirical best practice observed in existing moe-related works.[11, 25] ", "page_idx": 6}, {"type": "text", "text": "Dispatch Embeddings For each coming embedding, we decide the index of the expert it is dispatched to with $i=\\arg\\operatorname*{min}_{j=1}^{\\bar{n}}||v^{\\prime}-c_{j}||/r_{j}$ , where $c_{j}$ denotes the center of cluster $j$ , and $r_{j}$ denotes the radius of cluster $j$ . Note that the $v^{\\prime}$ here is the sequence embedding rather than a token embedding and is defined as the mean of all token embeddings in the sequence[17], and the dispatching also happens on the sequence level. The rationale for defining and analyzing long-tail at the sequence level stems from the fact that when we discuss long-tails, we are essentially referring to semantics rather than individual words or tokens. Such semantics can only be observed within the context provided by sequences as a whole. ", "page_idx": 6}, {"type": "text", "text": "Update cluster center The model\u2019s parameter space undergoes gradual updates throughout training, causing a slow drift in the embedding space as the parameters evolve. To tackle this, we incorporate a dynamic mechanism to update the cluster centers concurrently with the assignment of clusters. For a given cluster $m c_{i}$ , let its center at time $t$ be denoted as $c_{i}^{t}$ . When a new embedding $v$ arrives and is assigned to $m c_{i}$ , we update $c_{i}^{t}$ with: $c_{i}^{t+1}=\\alpha\\cdot c_{i}^{t}+(\\mathrm{i}-\\alpha)\\cdot v^{\\prime}$ , where, $\\alpha\\in[0,1]$ is a center update factor that determines the influence of the new embedding $v^{\\prime}$ on the existing center $c_{i}^{t}$ . This adaptive updating scheme ensures that cluster centers remain representative of the current state of the embedding space, even as it evolves through the training process. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4 Related Works ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Long-Tail Prior research addressing the issue of long-tail learning has predominantly been conducted within the domain of computer vision. The objective is to accurately recognize and classify rare or infrequently occurring classes in a given dataset together with frequently occurring classes [44]. There are several approaches to address the problem, including re-weighting [8], logit adjustment [4, 45], robust distributional matching [18, 35], and knowledge transfer [39, 34]. [37] declare that as the number of samples increases, the diminishing phenomenon suggests that there is a decreasing marginal benefit for a model to extract additional information from the data due to the presence of information overlap. Research in natural language processing has identified significant limitations in language models\u2019 capacity to learn long-tail knowledge [27, 3]. Furthermore, [46] suggests that attempting to address this issue during the domain-specific pretraining stage is often too late. ", "page_idx": 7}, {"type": "text", "text": "Domain-Specific Pretraining Domain-specific pretraining, also known as domain-specific pretraining, is highly advantageous to assist language models in requiring specialized domain knowledge. In one approach, contextualized embeddings are adapted to text from the target domain using masked language modeling, as detailed by Han and Eisenstein [16]. The concept of multi-phase pretraining involves secondary-stage unsupervised pretraining, exemplified by broad-coverage domain-specific BERT variants like BioBERT [24]. Research by Gururangan et al. [15] extends this by proposing domain-adaptive pretraining (DAPT) from a broader corpus and task-specific pretraining (TAPT) which uses unlabeled data increasingly aligned with the task distribution. These studies underscore the importance of domain-relevant data for pretraining in both high and low-resource scenarios [16, 15]. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section presents the experimental results of our model and other methods. In the experiments, our model only undergoes a pretrained phase, reading domain-specific data once. Other methods are pretrained on the same dataset and then continue-pretrained on domain-specific datasets. Subsequently, all models are used as embedding models with all parameters frozen to generate embeddings for downstream tasks. ", "page_idx": 7}, {"type": "text", "text": "Dataset and Evaluation We employ Wikipedia [13] as our pretraining dataset, which is also widely accepted in other works [24, 10]. We adopt some legal and medical domain-specific downstream tasks to show the effectiveness of our model. To ensure that the pretraining data do contain domain knowledge required by the downstream tasks, we mixed a relatively small amount (less than $8\\%$ ) of legal-domain-specific data [1] and medical-domain-specific data [9] into the pretraining data to simulate a long-tail distribution. The datasets selected are listed in Table 4 in Appendix A. Concurrently, we report the test perplexity of each model after the pretraining phase, serving as evidence of model convergence. Task performances are reported by accuracy. Although we use the method of mixing small-scale domain-specific datasets into pretraining data to simulate the long-tail distribution in those huge corpora, we cannot fully simulate the extremely rich pretraining data used on LLMs due to the limited training resources. ", "page_idx": 7}, {"type": "text", "text": "Baselines Since our strategy is not restricted to a specific model structure, we adopt both BERT [10] and GPT [28] as the base models and compare all the strategies on these base models respectively. We also compare with a Switch-MoE [11] version of them to show the effectiveness of our routing strategy. More Detailed implementation setting is listed in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "5.1 Main Result ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 9 and Table 2 show the performance of all models/strategies under our experiment setting with a trainable linear classifier for downstream tasks. \\*/med means a model continue-pretrained on medical-domain-specific data, and \\*/legal means a model continue-pretrained on legal-domainspecific data. We tested Cluster-guided Sparse Expert on Attention and FFN respectively, denoted as MoA and MoF. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "manHbkpIW6/tmp/fc7d2ffbc9b502fc52c59326972df09e62f64d14ed5e3ff7b003188d166fc704.jpg", "table_caption": ["Table 1: Results of strategies applied on BERT "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "manHbkpIW6/tmp/ad74acd94770905183e6f1c3d0cf96ec5a4c30e3cfa40078e4d23fe08b18418f.jpg", "table_caption": ["Table 2: Results of strategies applied on GPT "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Our method outperforms other models/strategies on almost all tasks, with an average improvement of around $3\\%$ , showing an ability to learn long-tail data from the pretraining dataset. Our method can be applied to either the Attention module or the FFN module, and both ways will yield a better result compared with the domain-specific pretrained baselines, showing potential for eliminating the need for domain-specific pretraining. While in certain scenarios, domain-specific pretraining remains indispensable due to the privacy concerns associated with proprietary data, we argue that when pretraining datasets encompass domains similar to the proprietary one, our approach can still facilitate an enhanced domain-specific pretraining performance. We also present the result of a larger scale model, in Table 3. We used larger models (330M GPT-style models trained with 20B tokens) and domain-specific tasks from the academic, environmental, and financial domains to demonstrate the generalization capability of our method. The results indicate that our method, even without fine-tuning, consistently outperforms baselines by an average of $3.5\\%$ in accuracy on domain-specific tasks while maintaining comparable performance on general tasks. It is also notable that domainspecific pretraining leads to overfitting and even catastrophic forgetting, resulting in a decrease in performance on tasks from non-related domains. More details are shown in Appendix A. Further experiments show that our approach does not cause a reduction in general knowledge acquisition, and more details are shown in Appendix C. ", "page_idx": 8}, {"type": "table", "img_path": "manHbkpIW6/tmp/b29bd91b3d04b47c6bd3fe8ed4ed2f94141310a43c6f0bddb6d42b6b5891334b.jpg", "table_caption": ["Table 3: Results of strategies applied on 330M GPT "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Expert analysis We analyze our model\u2019s embedding space to determine if our method dispatches embeddings correctly. We sample data and perform a forward inference pass through the model, visualizing the dispatching path of our model. As is shown in Figure 5, our distribution strategy correctly and effectively dispatches data from different long-tail clusters to different experts. We further visualize the NTK in each expert of our model, and it can be observed that by dispatching long-tail data separately, the NTK in each expert becomes more consistent. Whereas in a baseline model, its NTK matrix shows a poor consistency of the batch data, since long-tail and non-long-tail data are not separated. ", "page_idx": 9}, {"type": "image", "img_path": "manHbkpIW6/tmp/ae09e62eb11c2df610f58daa0c2197d5f1250f9bb3dde59c86ceed306f675f7d.jpg", "img_caption": ["Figure 5: a) The embedding space and routing result of our model. b) The NTK in each expert in our model. c) The NTK in baseline. b) and $\\mathrm{\\Phi_{c}^{-}}$ ) are sampled from the FFN in the 10th layer. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "For time overhead, the cluster initialization step is the additional computation introduced by our method throughout the training phase, relative to both the baseline and MoE. The time complexity of this operation will be $O(n\\log n d^{\\prime})$ and is bounded by $O(n^{2}d^{\\prime})$ for the worst case, where $n$ represents the number of sampled points. Notably, during subsequent inference stages, the quantum of parameters activated per iteration by our algorithm aligns identically with that of both MoE and the baseline. Since MoE and the baseline necessitate an additional fine-tuning phase, it becomes evident that the time cost of this phase far surpasses the $O(n^{2}d^{\\prime})$ complexity of cluster initialization. To provide more straightforward data, the wall-clock time of training $\\&$ domain-specific pretraining of 330M models for baseline GPT, MoE, and our CSE method and are 177 hours, 205 hours, and 160 hours respectively. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we seek to elucidate why language models require domain-specific pretraining despite the presence of domain knowledge in their pretraining data. Our investigation uncovers that Sentences with lower frequency scores show diminished gradient consistency, resulting in increased test perplexity. This misalignment, particularly pronounced in low-frequency sentences, culminates in elevated test perplexity, suggesting a deficiency in effectively leveraging domain-specific information. To address this challenge, we introduce Cluster-guided Sparse Experts (CSE), grouping diverse long-tail domain data and dispatching them to different experts to enhance gradient consistency within each expert, thereby enabling the model to incorporate long-tail domain knowledge during pretraining. Experiments suggest that our approach has the potential to supplant the need for a dedicated domain-specific pretraining stage. Through this approach, long-tail domain instances promote the formation of small, outlier clusters in the representation space, exhibiting a characteristic signature across varying stages of training and architectural depths. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China under Grant 62090025, and the Natural Science Foundation of Jiangsu Province (BK20240414). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[1] Caselaw access project. https://case.law/\u201e 2024.   \n[2] Reddit comments dataset. https://clickhouse.com/docs/en/getting-started/ example-datasets/reddit-comments\u201e 2024.   \n[3] Mallen A., Asai A., Zhong V, Das R., Khashabi D., and Hajishirzi H. When not to trust language models: Investigating effect. in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023. [4] Krishna Menon Aditya, Jayasumana Sadeep, Singh Rawat Ankit, Jain Himanshu, Veit Andreas, and Kumar. Sanjiv. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314, 2020.   \n[5] Benjamin Bowman and Guido Mont\u00fafar. Spectral bias outside the training set for deep networks in the kernel regime. ArXiv, abs/2206.02927, 2022. URL https://api.semanticscholar. org/CorpusID:249431476.   \n[6] \u00c0lex Bravo, Janet Pi\u00f1ero, N\u00faria Queralt-Rosinach, Michael Rautschka, and Laura I Furlong. Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research. BMC Bioinformatics, 16(1), February 2015. doi: 10.1186/s12859-015-0472-9. URL https://doi.org/10.1186/s12859-015-0472-9. [7] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. Legal-bert: \u201cpreparing the muppets for court\u2019\u201d. ArXiv, abs/2010.02559, 2020. URL https://api.semanticscholar.org/CorpusID:222141043.   \n[8] Huang Chen, Li Yining, Change Loy Chen, and Tang Xiaoou. Learning deep representation for imbalanced classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.   \n[9] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 615\u2013621, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2097. URL https://aclanthology.org/N18-2097.   \n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[11] W. Fedus, B. Zoph, , and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 2022.   \n[12] Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M. Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and HsuanTien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 405075699f065e43581f27d67bb68478-Abstract.html.   \n[13] Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.   \n[14] Zhen Guo and Yining Hua. Continuous training and fine-tuning for domain-specific language models in medical question answering. ArXiv, abs/2311.00204, 2023. URL https://api. semanticscholar.org/CorpusID:264832958.   \n[15] Suchin Gururangan, Ana Marasovic\u00b4, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. ArXiv, abs/2004.10964, 2020. URL https://api.semanticscholar.org/CorpusID: 216080466.   \n[16] Xiaochuang Han and Jacob Eisenstein. Unsupervised domain adaptation of contextualized embeddings for sequence labeling. In Conference on Empirical Methods in Natural Language Processing, 2019. URL https://api.semanticscholar.org/CorpusID:202541481.   \n[17] Junjie Huang, Duyu Tang, Wanjun Zhong, Shuai Lu, Linjun Shou, Ming Gong, Daxin Jiang, and Nan Duan. Whiteningbert: An easy unsupervised sentence embedding approach. arXiv preprint arXiv:2104.01767, 2021.   \n[18] Zheng Huangjie, Chen Xu, Yao Jiangchao, Yang Hongxia, Li Chunyuan, Zhang Ya, Zhang Hao, Tsang Ivor, Zhou Jingren, and Zhou. Mingyuan. Contrastive attraction and contrastive repulsion for representation learning. Transactions on Machine Learning Research, 2023.   \n[19] Arthur Jacot, Cl\u00e9ment Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and generalization in neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pp. 8580\u20138589, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html.   \n[20] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pp. 15696\u201315707. PMLR, 2023.   \n[21] Zixuan Ke, Yijia Shao, Haowei Lin, Hu Xu, Lei Shu, and Bing Liu. Adapting a language model while preserving its general knowledge. arXiv preprint arXiv:2301.08986, 2023.   \n[22] Kasper Green Larsen and Jelani Nelson. The johnson-lindenstrauss lemma is optimal for linear dimensionality reduction. arXiv preprint arXiv:1411.2404, 2014.   \n[23] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36:1234 \u2013 1240, 2019. URL https://api.semanticscholar. org/CorpusID:59291975.   \n[24] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240, 2020.   \n[25] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021.   \n[26] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. ArXiv, abs/2308.08747, 2023. URL https://api.semanticscholar.org/CorpusID: 261031244.   \n[27] Kandpal N., Deng H., Roberts A., Wallace E., and Raffel C. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, 2023.   \n[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[29] Erich Schubert, J\u00f6rg Sander, Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu. Dbscan revisited, revisited. ACM Transactions on Database Systems (TODS), 42:1 \u2013 21, 2017. URL https://api.semanticscholar.org/CorpusID:5156876.   \n[30] Guangyuan Shi, Qimai Li, Wenlong Zhang, Jiaxin Chen, and Xiao-Ming Wu. Recon: Reducing confilcting gradients from the root for multi-task learning, 2023. URL https://arxiv.org/ abs/2302.11289.   \n[31] K. Singhal, Shekoofeh Azizi, Tao Tu, Said Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather J. Cole-Lewis, Stephen J. Pfohl, P A Payne, Martin G. Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, P. A. Mansfield, Blaise Ag\u00fcera y Arcas, Dale R. Webster, Greg S. Corrado, Yossi Matias, Katherine Hui-Ling Chou, Juraj Gottweis, Nenad Tomaev, Yun Liu, Alvin Rajkomar, Jo\u00eblle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. Nature, 620:172 \u2013 180, 2022. URL https://api.semanticscholar.org/CorpusID:255124952.   \n[32] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170.   \n[33] Erik M. van Mulligen, Annie Fourrier-Reglat, David Gurwitz, Mariam Molokhia, Ainhoa Nieto, Gianluca Trifiro, Jan A. Kors, and Laura I. Furlong. The eu-adr corpus: Annotated drugs, diseases, targets, and their relationships. Journal of Biomedical Informatics, 45(5): 879\u2013884, 2012. ISSN 1532-0464. doi: https://doi.org/10.1016/j.jbi.2012.04.004. URL https: //www.sciencedirect.com/science/article/pii/S1532046412000573. Text Mining and Natural Language Processing in Pharmacogenomics.   \n[34] Chen Xu, Chen Siheng, Yao Jiangchao, Zheng Huangjie, Zhang Ya, and W. Tsang. Ivor. Learning on attribute-missing graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \n[35] Chen Xu, Pan Yuangang, Tsang Ivor, and Zhang. Ya. Learning node representations against perturbations. Pattern Recognition, 145:109976, 2024.   \n[36] Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng Ann Heng, and Wai Lam. Unveiling the generalization power of fine-tuned large language models. ArXiv, abs/2403.09162, 2024. URL https://api.semanticscholar.org/CorpusID:268385476.   \n[37] Cui Yin, Jia Menglin, Lin Tsung-Yi, Song Yang, and Belongie. Serge. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.   \n[38] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning, 2020. URL https://arxiv.org/abs/2001.06782.   \n[39] Wang Yu-Xiong, Ramanan Deva, and Hebert. Martial. Learning to model the tail. Advances in Neural Information Processing Systems, 2017.   \n[40] Shi Yubin, Chen Yixuan, Dong Mingzhi, Yang Xiaochen, Li Dongsheng, Wang Yujiang, P. Dick Robert, Lv Qin, Zhao Yingying, Yang Fan, Lu Tun, Gu Ning, and Shang Li. Train faster, perform better: Modular adaptive training in over-parameterized models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[41] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen. A survey of large language models. ArXiv, abs/2303.18223, 2023. URL https://api.semanticscholar.org/CorpusID:257900969.   \n[42] Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, and Shikai Wu. Finetuning large language models for domain-specific machine translation. ArXiv, abs/2402.15061, 2024. URL https://api.semanticscholar.org/CorpusID:267897581.   \n[43] Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, and Daniel E. Ho. When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset. arXiv e-prints, art. arXiv:2104.08671, April 2021. doi: 10.48550/arXiv.2104.08671.   \n[44] Zhou Zhihan, Yao Jiangchao, Wang Yan-Feng, Han Bo, and Zhang Ya. Contrastive learning with boosted memorization. In International Conference on Machine Learning, 2022.   \n[45] Zhou Zhihan, Yao Jiangchao, Hong Feng, Zhang Ya, Han Bo, and Wang. Yanfeng. Combating representation learning disparity with geometric harmonization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[46] Zeyuan Allen Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 4 shows the datasets used in our experiments. Table 5 shows the hyperparameters used in our implementations. We use a machine with 8 NVIDIA GeForce RTX 3090 GPUs with 24GB GPU memory as our experiment platform. Pretraining costs about 24 hours. ", "page_idx": 14}, {"type": "table", "img_path": "manHbkpIW6/tmp/fa04a74ac71ffbb24c8e9acf60f6b321c088fbb1d98ed9d23dedd0bf37084fe9.jpg", "table_caption": ["Table 4: Datasets used for experiments "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "manHbkpIW6/tmp/b0b730ba5f182c2658fa0416211d30695fbfac1b495a1703eaf3dd4283e85fcf.jpg", "table_caption": ["Table 5: Hyperparameters of Models "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "By monitoring the validation loss of the pretraining dataset(Figure 6), we show the Catastrophic Forgetting problem of the BERT model and its MOE method in the domain-specific finetuning phase. Despite our attempts at various combinations of generic data and domain-specific data during domain finetuning, the best outcome among these still resulted in a decline in model performance on domains unrelated to its fine-tuning, indicating a limitation in the generalizability of the adapted model. As domain-specific finetuning proceeds, the validation loss of pretraining dataset has a significant rise and stays well above the convergence position of pretraining. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "manHbkpIW6/tmp/f2a964b8ac40170413120460abfef14d59e59e6449d38846c868ff125649a9a8.jpg", "img_caption": ["Figure 6: The validation loss of the pretraining dataset during the domain-specific finetuning phase. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Our training process strictly followed the basic framework of pretraining and domain-specific pretraining, and we didn\u2019t control the occurrence of forgetting, focusing more on the best performance. However as shown in the appendix, we monitored the forgetting phenomenon using the test loss on pretraining data. Specifically, BERT/med(further trained in medical), exhibited a more severe forgetting issue, with test perplexity on pretraining data increasing by 24.33, compared to a smaller increase of 8.41 for BERT/legal(further trained in legal). After correcting this imbalance by utilizing early-stop to select checkpoints where each model showed similar degrees of forgetting but not the best performance, the results came out that each domain-finetuned model outperformed on its respective domain tasks. This further underscores the challenges posed by the fine-tuning stage, affirming the value of our approach in not requiring domain-specific pretraining. ", "page_idx": 15}, {"type": "table", "img_path": "manHbkpIW6/tmp/9ce12e3d50f3b7814265a768df15aaa8247c19d6b10770a5fc0648c155a614b0.jpg", "table_caption": ["Table 6: Checkpoints selected with early-stop where two models show the same degree of forgetting. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Limitations Discussions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Although we use the method of mixing small-scale domain-specific datasets into pretraining data to simulate the long-tail distribution in those huge corpora, we cannot fully simulate the extremely rich pretraining data used on LLMs due to the limited training resources. ", "page_idx": 15}, {"type": "text", "text": "C Discussions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Large Scale The maximum model scale presented in the experiment section in this paper is 330M GPT. So, we discuss what if our proposed CSE method is applied to a larger scale. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Model Size: Larger models have greater learning capacities, which can enhance the GC for long-tail knowledge given a fixed training data size.   \n\u2022 Window Size: A larger window size allows the model to capture longer-range relationships within each training sequence, potentially highlighting more subtle distinctions and leading to a more long-tail distribution.   \n\u2022 Training Data Size: Larger data sizes and larger window sizes might result in a higher proportion of common data relative to long-tail clusters. The increased presence of common data and the potential subdivision of long-tail clusters into smaller groups could, paradoxically, reduce the GC for long-tail knowledge. ", "page_idx": 16}, {"type": "text", "text": "Performance on Common Datasets Additionally, we have included results on general knowledge tasks to confirm that our dedicated long-tail learning method does not compromise general knowledge acquisition. ", "page_idx": 16}, {"type": "table", "img_path": "manHbkpIW6/tmp/b756c11a98abe1f16d43b07f68bf4c18a86b2f5c580c46ad2073ae85e9574eac.jpg", "table_caption": ["Table 7: Results of general tasks on BERT with the same small-scale setting in the paper "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "manHbkpIW6/tmp/c70d81ebb06c7ead6aecb6fcd3852d5f2f2fe56bc979f91c148dfddd205d0564.jpg", "table_caption": ["Table 8: Results of general tasks tested on GPT 330M trained with 20B tokens "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Experiments on A Pre-trained Model We also conducted experiments on a pre-trained 110M scale model, wherein all methods continue training from a single pre-trained checkpoint. The outcomes are presented in the following table. Results show that directly applying our method to a pre-trained model still yields superior performance compared to the baseline and MoE models. ", "page_idx": 16}, {"type": "table", "img_path": "manHbkpIW6/tmp/a9431a4d419dc1f59298dcb35e8b616ba8b7fbb2122e0f9e67ffe988fa4e4ef7.jpg", "table_caption": ["Table 9: Results of strategies applied on pre-trained model "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Using both MoA and MoF We tried this architecture using both MoA and MoF and tested it in the same small-scale setting reported in the paper, and found that $\\scriptstyle\\mathrm{MoA+MoF}$ yields improvements in the overall performance compared to MoA/MoF only. ", "page_idx": 16}, {"type": "table", "img_path": "manHbkpIW6/tmp/cb7e9a7f63fa2907959658883947aacd23a456ecaae4075941182fee52983846.jpg", "table_caption": ["Table 10: Updated results of strategies applied on BERT "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Analysis of Sentence Frequency Distribution and Cluster Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Sentence Frequency Distribution and Gradient Consistency ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We analyze the sentence frequency distribution using $2/3$ -gram averages, comparing it to gradient consistency across different training stages. Figure 7(a) and Figure 7(b) illustrate the frequency distribution of sentences based on their 2-gram and 3-gram patterns, respectively. The results are consistent with the 1-gram analysis, confirming that despite its simplicity, the 1-gram method remains effective in capturing sentence frequency trends. The gradient consistency results align with these findings, reinforcing the robustness of this approach. ", "page_idx": 17}, {"type": "image", "img_path": "manHbkpIW6/tmp/dd54aae8e67842b3ec080eeb00a97d7d08557707b86d5a7afb095e8c046c2840.jpg", "img_caption": ["Figure 7: a) 2-gram sentence frequency distribution. b) 3-gram sentence frequency distribution. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.2 Cluster Details and Sentence Content ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Figure 8 shows the detailed content of sentences within each small cluster. For example, in the \u2019Java Network Debug\u2019 cluster, terms such as HTML, Javascript, and $\\tt{<}1i n k>$ frequently appear, indicating strong domain coherence. This example highlights how specific topics tend to cluster together, demonstrating the effectiveness of the clustering process. ", "page_idx": 17}, {"type": "text", "text": "Furthermore, we identified low-frequency sentences that are scattered across clusters, lacking clear semantic correlation. These sentences often include irregular content such as misprinted formulas or non-English text, which prevents them from forming coherent clusters. Figure 8 also illustrates the distribution and content of these low-frequency sentences, providing insight into their irregular placement across clusters. ", "page_idx": 17}, {"type": "text", "text": "D.3 Methodology for Long-Tail Data Selection ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We applied the Elbow method to determine a threshold of $9.37\\%$ on the curve of domain proportions, classifying any domain with a data proportion at or below this level as a long-tail domain. By plotting the curve of Wikipedia domains and their corresponding proportions, we identified the point $(9.37\\%)$ where the slope changes significantly, marking the transition from the head of the distribution to the tail. Following this, we randomly selected sentences from these long-tail domains for further analysis. ", "page_idx": 17}, {"type": "image", "img_path": "manHbkpIW6/tmp/ca382b2a72be6de5bdcdc22756f0faa1d714276873fbfe0a3c39e3e72edaa812.jpg", "img_caption": ["Figure 8: Detailed content of sentences within clusters, showing high domain coherence (e.g., \u2019Java Network Debug\u2019 cluster) and the distribution of low-frequency, irregular sentences. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We claim clearly in Abstract and Introduction 1. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We write them in Appendix B. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution ", "page_idx": 18}, {"type": "text", "text": "is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We achieved this part in the Analysis 2, and the relevant assumptions are indicated in the Reference. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The Table 5 in the Appendix A shows the random seeds used in our experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \nWhile NeurIPS does not require releasing code, the conference does require all submis  \nsions to provide some reasonable avenue for reproducibility, which may depend on the   \nnature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We will release our source code and detailed instructions for reproducing our results upon acceptance of this paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We show our experimental setting/details both in our Experiments 5 and in the Appendix A ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We show the standard deviation of accuracy in Table 9 and Table 2. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We show these in Appendix A ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We have checked it. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We cite the creators and introduce assets in the Appendix A. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]