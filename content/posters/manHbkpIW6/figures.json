[{"figure_path": "manHbkpIW6/figures/figures_1_1.jpg", "caption": "Figure 1: a) The top 20 subreddits with the highest amount of data in the Reddit Comments Dataset, where a typical long-tail distribution can be observed. b) Language Models struggle to memorize long-tail domain knowledge during pretraining. The less frequently a sentence appears in the training corpus, the higher its perplexity, indicating that it is not effectively memorized.", "description": "This figure shows two subfigures. Subfigure (a) is a bar chart illustrating the top 20 subreddits in the Reddit Comments Dataset, demonstrating a long-tail distribution where a few subreddits have a significantly higher number of comments than the vast majority.  Subfigure (b) is a line graph showing the relationship between sentence frequency and perplexity in language models. It indicates that as the frequency of a sentence in the training corpus decreases (long-tail data), its perplexity (a measure of how well the model predicts the sentence) increases, illustrating the difficulty language models have in memorizing less frequent domain-specific knowledge.", "section": "1 Introduction"}, {"figure_path": "manHbkpIW6/figures/figures_3_1.jpg", "caption": "Figure 2: a) The correlation between sentence frequency score and gradient consistency in the baseline model. A histogram is also included showing how many percentage of sentences across the whole dataset fall into each frequency bin. For further analysis using 2/3-gram averages, please refer to the appendix D.1. b) A sampled embedding space containing 4 long-tail clusters, taken from our CSE layers. For more information on the detailed cluster contents, please refer to the appendix D.2.", "description": "Figure 2(a) shows the negative correlation between the frequency of a sentence and its gradient consistency in the baseline model.  Sentences that appear infrequently in the dataset show low gradient consistency, indicating that the model struggles to capture their learning dynamics effectively. Figure 2(b) illustrates the embedding space of a model with the Cluster-guided Sparse Expert (CSE) layer.  The figure highlights four distinct long-tail clusters along with a more central cluster of common data. Each long-tail cluster contains semantically similar sentences from a specific domain.", "section": "Analysis of Long-Tail Domain Data"}, {"figure_path": "manHbkpIW6/figures/figures_5_1.jpg", "caption": "Figure 3: a) Evolution of the Gradient Consistency (GC) of long-tail data in the baseline model over the first 8000 training steps. GC scores beyond this range are omitted, as they consistently remain below 0.2. For details on the method used to select long-tail data, please refer to the appendix D.3. b) Evolution of number of clusters over training steps. c) The ratio of cluster distances to cluster radii over training steps, providing a measure of cluster structure clarity independent of norm values.", "description": "This figure shows the evolution of gradient consistency (GC) of long-tail data, the number of clusters, and the ratio of cluster distances to cluster radii over training steps in a baseline model.  Panel (a) demonstrates the decreasing GC of long-tail data over time. Panels (b) and (c) illustrate the dynamic evolution of cluster formation and structure throughout training.  The number of clusters changes, and their relative compactness shifts over time.  The changes in cluster structure clarity independent of norm values suggest an evolution in how the model learns and groups the long-tail data over the course of training.", "section": "2.3 Dynamic of Long-Tail Domain Clusters"}, {"figure_path": "manHbkpIW6/figures/figures_5_2.jpg", "caption": "Figure 4: a) Overview of the Cluster-guided Sparse Expert (CSE) layer. b) The cluster number fluctuation is mainly caused by the big common cluster. These four figures arranged sequentially from top to bottom, were sampled at every 10,000 steps throughout the process from the FFN of the 10th layer in a GPT model.", "description": "This figure illustrates the Cluster-guided Sparse Expert (CSE) layer architecture. Subfigure (a) shows the overall CSE layer structure, highlighting the dispatcher mechanism that routes input embeddings to different sparse experts. Subfigure (b) visualizes the cluster evolution in the embedding space during the training process, showing how clusters form and evolve across different training steps (sampled every 10,000 steps from the 10th layer's FFN in a GPT model). The evolution showcases the dynamic nature of clustering, with the formation and merging of clusters, particularly driven by a large common cluster.", "section": "2.2 Embedding Space Analysis With Cluster-guided Sparse Expert (CSE) layer"}, {"figure_path": "manHbkpIW6/figures/figures_9_1.jpg", "caption": "Figure 5: a) The embedding space and routing result of our model. b) The NTK in each expert in our model. c) The NTK in baseline. b) and c) are sampled from the FFN in the 10th layer.", "description": "This figure visualizes the embedding space and neural tangent kernel (NTK) analysis of the proposed Cluster-guided Sparse Expert (CSE) model.  Panel (a) shows the embedding space, illustrating how the CSE layer effectively clusters and routes long-tail data to designated experts. Panels (b) and (c) present NTK matrices for the CSE model (experts 1 and 2) and the baseline model, respectively, demonstrating that the CSE model achieves more consistent NTK within each expert, unlike the baseline.", "section": "5.2 Analysis"}, {"figure_path": "manHbkpIW6/figures/figures_15_1.jpg", "caption": "Figure 6: The validation loss of the pretraining dataset during the domain-specific finetuning phase.", "description": "This figure shows the validation loss of the pretraining dataset during the domain-specific finetuning phase for four different models: BERT/legal, MoE/legal, MoE/med, and BERT/med.  The x-axis represents the training steps, and the y-axis represents the validation loss. The plot illustrates the phenomenon of catastrophic forgetting, where the model's performance on the pretraining dataset degrades significantly as it is further fine-tuned on the domain-specific tasks.  Notice that the models fine-tuned on medical data show a greater increase in validation loss than those fine-tuned on legal data. This highlights the challenges posed by domain-specific finetuning on pretrained language models.", "section": "B Limitations Discussions"}, {"figure_path": "manHbkpIW6/figures/figures_17_1.jpg", "caption": "Figure 7: a) 2-gram sentence frequency distribution. b) 3-gram sentence frequency distribution.", "description": "The figure shows the frequency distribution of sentences based on their 2-gram and 3-gram patterns, respectively.  The distributions are shown as histograms, with the x-axis representing the average frequency and the y-axis representing the percentage of sentences.  A second line graph shows the gradient consistency for each frequency range.  The results confirm that the 1-gram method used earlier is robust, as the gradient consistency aligns with the frequency trends across 2-gram and 3-gram analysis.", "section": "D.1 Sentence Frequency Distribution and Gradient Consistency"}, {"figure_path": "manHbkpIW6/figures/figures_18_1.jpg", "caption": "Figure 8: Detailed content of sentences within clusters, showing high domain coherence (e.g., \u2018Java Network Debug\u2019 cluster) and the distribution of low-frequency, irregular sentences.", "description": "This figure visualizes the content of sentences within different clusters identified by the CSE layer.  It demonstrates that sentences within the same cluster share semantic similarity, indicating the effectiveness of the clustering approach.  The figure also highlights the presence of low-frequency, irregular sentences that are scattered across clusters, lacking clear semantic coherence.  This observation supports the paper's argument that CSE effectively groups semantically similar sentences, even those with low frequency, improving the language model's ability to learn long-tail knowledge.", "section": "D.2 Cluster Details and Sentence Content"}]