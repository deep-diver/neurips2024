[{"figure_path": "o863gX6DxA/figures/figures_1_1.jpg", "caption": "Figure 1: Left: The tree of possible refinements is infinitely deep and has infinite branching factor. Each node is a program and each edge is an LLM sample. Right: Explore-Exploit tradeoff for a search state after performing 3 node expansions. Exploit by sampling another child of a program that is nearly correct, or Explore by sampling a child of a program that has been expanded fewer times.", "description": "The figure illustrates the exploration-exploitation tradeoff in program refinement using LLMs. The left panel shows a tree representing the infinite possibilities of iterative refinement, where each node is a program and each edge is a language model sample. The right panel illustrates the tradeoff, showing a choice between exploiting a program that is almost correct (80%) or exploring a less-explored program (60%), which could potentially lead to a better solution.", "section": "1 Introduction"}, {"figure_path": "o863gX6DxA/figures/figures_4_1.jpg", "caption": "Figure 2: How the model's beliefs about the benefit of refining a program, \u03b8, change as we vary (1) N, the number of times it was previously refined, and (2) h, the heuristic estimate of how close we are to satisfying the specification (larger h is better). Left: Expected benefit of refining decreases the more we refine, and asymptotically decays to zero (Eq. 11). Middle/Right: Posterior beliefs initially center around h and shift toward zero with each additional refinement. Same colored curves show same values of h for different values of N. The hyperparameter C modulates the rate of decay with each additional refinement, and also affects the initial concentration of the density around h.", "description": "This figure shows how the model's belief about the benefit of refining a program changes based on the number of times it has been refined (N) and its heuristic value (h). The left panel shows that the expected benefit of refining decreases as N increases and asymptotically decays to zero. The middle and right panels show how the posterior beliefs, initially centered around h, shift towards zero with each refinement. The hyperparameter C controls the rate of decay and the initial concentration of the density around h.", "section": "Understanding the behavior of REx"}, {"figure_path": "o863gX6DxA/figures/figures_5_1.jpg", "caption": "Figure 1: Left: The tree of possible refinements is infinitely deep and has infinite branching factor. Each node is a program and each edge is an LLM sample. Right: Explore-Exploit tradeoff for a search state after performing 3 node expansions. Exploit by sampling another child of a program that is nearly correct, or Explore by sampling a child of a program that has been expanded fewer times.", "description": "The figure on the left shows the tree-like structure that results from the iterative refinement process, where each node represents a program and each edge represents a language model call that results in a refined version of the code. This tree is infinitely deep and wide due to the stochastic nature of the refinement process.  The figure on the right illustrates the tradeoff between exploration (sampling a program that has not been explored thoroughly) and exploitation (sampling a program that is close to the solution). This tradeoff is crucial because it determines how the iterative refinement process proceeds.", "section": "1 Introduction"}, {"figure_path": "o863gX6DxA/figures/figures_5_2.jpg", "caption": "Figure 2: How the model's beliefs about the benefit of refining a program, \u03b8, change as we vary (1) N, the number of times it was previously refined, and (2) h, the heuristic estimate of how close we are to satisfying the specification (larger h is better). Left: Expected benefit of refining decreases the more we refine, and asymptotically decays to zero (Eq. 11). Middle/Right: Posterior beliefs initially center around h and shift toward zero with each additional refinement. Same colored curves show same values of h for different values of N. The hyperparameter C modulates the rate of decay with each additional refinement, and also affects the initial concentration of the density around h.", "description": "This figure shows how the model's belief about the benefit of refining a program changes based on the number of times it has been refined (N) and its heuristic value (h).  The left panel illustrates the decay of the expected benefit of refinement with increasing refinements, asymptotically approaching zero. The middle and right panels display the shift of posterior beliefs from the heuristic value towards zero as the program is refined more, demonstrating an exploration-exploitation tradeoff. The hyperparameter C influences both the initial concentration of the posterior belief around the heuristic value and the rate of its decay.", "section": "4 REx: Refine, Explore, Exploit"}, {"figure_path": "o863gX6DxA/figures/figures_6_1.jpg", "caption": "Figure 4: Comparing REx with alternatives using GPT-4 (temp=1). BFS and FW are Breadth First Search and Fixed Width, respectively. AUC denotes Area Under the Curve, and Final denotes the success rate at the maximum # LLM calls (64 for ARC and 300 for others due to domain conventions). Dark lines show performance with the best hyper-parameter setting for each method. Light lines show each run on each hyperparameter. The inset box plots show the distribution while varying the hyper-parameters. APPS baselines: Parsel [25], AlphaCode [14], and Olausson et al. [3]. Nonlinear Loop Invariant baselines: Z3/GSpacer [26] and Yao et al. [21]. ARC baseline: Hypo. Search [23]. More results on APPS Interview-Level and ARC in Figure 11 and Figure 13", "description": "The figure compares the performance of REx against three baselines (Greedy, Breadth-First Search, and Fixed-Width) across three different problem domains (Nonlinear Loop Invariant, APPS Competition, and ARC) in terms of the number of problems solved given a certain number of LLM calls.  It shows that REx consistently outperforms or is competitive with the best baseline in all three domains. The box plots illustrate the robustness of REx to variations in hyperparameters.", "section": "5 Experimental Results"}, {"figure_path": "o863gX6DxA/figures/figures_7_1.jpg", "caption": "Figure 5: Comparing REx with alternatives with other LLMs on competition programming (APPS Competition-Level). More results on ARC are available in Appendix in Figure 12.", "description": "This figure compares the performance of REx against three baseline methods (BFS, Greedy, and Fixed-Width) across three different LLMs (GPT-3.5-turbo, Claude-3.5-Sonnet, and Llama-3.1-405B) on the APPS Competition-Level dataset.  The x-axis represents the sample budget (number of LLM requests), and the y-axis represents the success rate (percentage of problems solved).  The figure shows that REx consistently outperforms or is competitive with the best-performing baseline methods across all three LLMs.  The insets show box plots illustrating the distribution of AUC (Area Under the Curve) values for each method. Appendix Figure 12 shows similar results on the ARC dataset.", "section": "Experimental Results"}, {"figure_path": "o863gX6DxA/figures/figures_7_2.jpg", "caption": "Figure 4: Comparing REx with alternatives using GPT-4 (temp=1). BFS and FW are Breadth First Search and Fixed Width, respectively. AUC denotes Area Under the Curve, and Final denotes the success rate at the maximum # LLM calls (64 for ARC and 300 for others due to domain conventions). Dark lines show performance with the best hyper-parameter setting for each method. Light lines show each run on each hyperparameter. The inset box plots show the distribution while varying the hyper-parameters. APPS baselines: Parsel [25], AlphaCode [14], and Olausson et al. [3]. Nonlinear Loop Invariant baselines: Z3/GSpacer [26] and Yao et al. [21]. ARC baseline: Hypo. Search [23].", "description": "This figure compares the performance of REx against three baseline methods (Breadth-First Search, Fixed-Width, and Greedy) across four different problem sets: Nonlinear Loop Invariants, ARC, APPS Competition-Level, and APPS Introductory-Level.  The y-axis represents the success rate (percentage of problems solved) and the x-axis shows the number of LLM calls (budget).  The figure demonstrates that REx consistently outperforms or matches the best-performing baseline method for each problem set. The inset box plots illustrate the variation in performance for different hyperparameter settings for each algorithm, showing that REx is more robust to hyperparameter choices compared to baselines. This robustness is a key claim of the paper.", "section": "Experimental Results"}, {"figure_path": "o863gX6DxA/figures/figures_8_1.jpg", "caption": "Figure 7: Example search tree generated by REx. Blue-Yellow gradient indicates heuristic value (blue: low, yellow: high). The order of children from left to right indicates the sequence of generations (left: older, right: newer). See also appendix Fig. 14-17", "description": "This figure shows a search tree generated by the REx algorithm.  Each node represents a program, and the color gradient (blue to yellow) represents the heuristic value of that program (blue being low, yellow being high). The order of child nodes from left to right shows the order in which programs were generated. The figure illustrates how the algorithm explores and exploits different program refinements, guiding the search towards high-quality solutions.", "section": "4 REx: Refine, Explore, Exploit"}, {"figure_path": "o863gX6DxA/figures/figures_13_1.jpg", "caption": "Figure 1: Left: The tree of possible refinements is infinitely deep and has infinite branching factor. Each node is a program and each edge is an LLM sample. Right: Explore-Exploit tradeoff for a search state after performing 3 node expansions. Exploit by sampling another child of a program that is nearly correct, or Explore by sampling a child of a program that has been expanded fewer times.", "description": "The figure on the left shows the tree-like structure that results from iteratively refining a program using an LLM.  The figure on the right illustrates the exploration-exploitation tradeoff in program refinement.  The tradeoff is between exploiting refinements of programs that are closer to being correct (passing more test cases) and exploring less-explored programs. ", "section": "1 Introduction"}, {"figure_path": "o863gX6DxA/figures/figures_16_1.jpg", "caption": "Figure 2: How the model's beliefs about the benefit of refining a program, \u03b8, change as we vary (1) N, the number of times it was previously refined, and (2) h, the heuristic estimate of how close we are to satisfying the specification (larger h is better). Left: Expected benefit of refining decreases the more we refine, and asymptotically decays to zero (Eq. 11). Middle/Right: Posterior beliefs initially center around h and shift toward zero with each additional refinement. Same colored curves show same values of h for different values of N. The hyperparameter C modulates the rate of decay with each additional refinement, and also affects the initial concentration of the density around h.", "description": "This figure illustrates the exploration-exploitation tradeoff in program refinement.  The left panel shows how the expected reward of refining a program decreases as the number of refinements (N) increases, asymptotically approaching zero. This demonstrates the exploitation aspect \u2013 refining a program many times yields diminishing returns. The middle and right panels show how posterior beliefs about a program's optimality (\u03b8) evolve with both N and the heuristic estimate of its correctness (h).  Initially, beliefs center around h, but shift towards zero with each refinement. This shows the exploration aspect; programs with lower initial h values are still given a chance to improve.", "section": "Understanding the behavior of REx"}, {"figure_path": "o863gX6DxA/figures/figures_17_1.jpg", "caption": "Figure 4: Comparing REx with alternatives using GPT-4 (temp=1). BFS and FW are Breadth First Search and Fixed Width, respectively. AUC denotes Area Under the Curve, and Final denotes the success rate at the maximum # LLM calls (64 for ARC and 300 for others due to domain conventions). Dark lines show performance with the best hyper-parameter setting for each method. Light lines show each run on each hyperparameter. The inset box plots show the distribution while varying the hyper-parameters. APPS baselines: Parsel [25], AlphaCode [14], and Olausson et al. [3]. Nonlinear Loop Invariant baselines: Z3/GSpacer [26] and Yao et al. [21]. ARC baseline: Hypo. Search [23].", "description": "The figure compares the performance of REx against three other methods (Breadth-First Search, Fixed-Width, and Greedy) across three different problem domains. The x-axis represents the number of LLM calls (compute budget), and the y-axis represents the success rate.  The figure shows that REx consistently outperforms or is competitive with the best-performing baseline across the three domains.  Inset boxplots illustrate the performance variability across different hyperparameter settings for each method.", "section": "5 Experimental Results"}, {"figure_path": "o863gX6DxA/figures/figures_18_1.jpg", "caption": "Figure 5: Comparing REx with alternatives with other LLMs on competition programming (APPS Competition-Level). More results on ARC are available in Appendix in Figure 12.", "description": "This figure compares the performance of REx against other baselines (BFS, Greedy, and Fixed-Width) across three different LLMs (GPT-3.5-turbo, Claude-3.5-Sonnet, and Llama-3.1-405B) on the APPS Competition-Level dataset.  The x-axis represents the sample budget (number of LLM requests), and the y-axis shows the success rate.  The inset box plots show the distribution of success rates across multiple runs with different hyperparameter settings for each method.  The figure demonstrates REx's consistent superior performance across different LLMs and hyperparameter choices. More detailed results for ARC are given in Appendix Figure 12.", "section": "Experimental Results"}, {"figure_path": "o863gX6DxA/figures/figures_18_2.jpg", "caption": "Figure 2: How the model's beliefs about the benefit of refining a program, \u03b8, change as we vary (1) N, the number of times it was previously refined, and (2) h, the heuristic estimate of how close we are to satisfying the specification (larger h is better). Left: Expected benefit of refining decreases the more we refine, and asymptotically decays to zero (Eq. 11). Middle/Right: Posterior beliefs initially center around h and shift toward zero with each additional refinement. Same colored curves show same values of h for different values of N. The hyperparameter C modulates the rate of decay with each additional refinement, and also affects the initial concentration of the density around h.", "description": "This figure demonstrates how the model's belief in the effectiveness of refining a program changes based on the number of times it's been refined (N) and its heuristic value (h).  The left panel shows that the expected reward from refinement decreases and approaches zero as N increases.  The middle and right panels illustrate that the model's posterior belief about the program's potential (\u03b8) starts near the heuristic value (h) and shifts towards zero with each refinement attempt. The hyperparameter C controls the rate at which this belief shifts.", "section": "Understanding the behavior of REx"}, {"figure_path": "o863gX6DxA/figures/figures_19_1.jpg", "caption": "Figure 13: Results on APPS-Interview. Denotations are the same as Figure 4.", "description": "This figure compares the performance of REx against three baseline methods (BFS, Greedy, and FW) and the state-of-the-art method from Olausson et al. 2023 on the APPS Interview-Level dataset.  The left panel shows the success rate (percentage of problems solved) as a function of the number of LLM calls (sample budget).  The right panel provides box plots summarizing the final success rate, the area under the curve (AUC), and the number of LLM requests required to achieve similar performance as Olausson et al. 2023. REx demonstrates competitive or better performance compared to the baselines in terms of success rate and AUC, and significantly fewer LLM requests are needed to match the state-of-the-art method.", "section": "5 Experimental Results"}, {"figure_path": "o863gX6DxA/figures/figures_20_1.jpg", "caption": "Figure 1: Left: The tree of possible refinements is infinitely deep and has infinite branching factor. Each node is a program and each edge is an LLM sample. Right: Explore-Exploit tradeoff for a search state after performing 3 node expansions. Exploit by sampling another child of a program that is nearly correct, or Explore by sampling a child of a program that has been expanded fewer times.", "description": "The figure illustrates the exploration-exploitation tradeoff in program refinement using LLMs.  The left panel shows a tree representing the potentially infinite space of program refinements generated iteratively. The right panel depicts a simplified search state after three refinement steps. It highlights the choice between exploiting a refinement that is already quite close to being correct (80% correct) versus exploring a less refined program (60% correct) that might still lead to a better solution in subsequent steps.  This tradeoff is central to the paper's proposed algorithm.", "section": "1 Introduction"}, {"figure_path": "o863gX6DxA/figures/figures_22_1.jpg", "caption": "Figure 1: Left: The tree of possible refinements is infinitely deep and has infinite branching factor. Each node is a program and each edge is an LLM sample. Right: Explore-Exploit tradeoff for a search state after performing 3 node expansions. Exploit by sampling another child of a program that is nearly correct, or Explore by sampling a child of a program that has been expanded fewer times.", "description": "The figure on the left shows a tree representing the iterative refinement process of improving a program with LLMs. Each node is a program and each edge represents an LLM sample generating a new, hopefully better program.  The right side illustrates the exploration-exploitation tradeoff.  Should refinement focus on the most promising program (exploit) or explore a less-explored program, even if it\u2019s currently less promising?", "section": "1 Introduction"}, {"figure_path": "o863gX6DxA/figures/figures_23_1.jpg", "caption": "Figure 2: How the model's beliefs about the benefit of refining a program, \u03b8, change as we vary (1) N, the number of times it was previously refined, and (2) h, the heuristic estimate of how close we are to satisfying the specification (larger h is better). Left: Expected benefit of refining decreases the more we refine, and asymptotically decays to zero (Eq. 11). Middle/Right: Posterior beliefs initially center around h and shift toward zero with each additional refinement. Same colored curves show same values of h for different values of N. The hyperparameter C modulates the rate of decay with each additional refinement, and also affects the initial concentration of the density around h.", "description": "This figure shows how the model's belief about the benefit of refining a program changes based on the number of times it has been refined and its heuristic value. The left plot shows that the expected benefit of refining decreases and asymptotically decays to zero as the refinement count increases. The middle and right plots illustrate how posterior beliefs initially center around the heuristic value and shift towards zero with each refinement, showing that a program that has been heavily refined is less likely to be refined further. The hyperparameter C controls the rate of decay.", "section": "4 REx: Refine, Explore, Exploit"}, {"figure_path": "o863gX6DxA/figures/figures_23_2.jpg", "caption": "Figure 4: Comparing REx with alternatives using GPT-4 (temp=1). BFS and FW are Breadth First Search and Fixed Width, respectively. AUC denotes Area Under the Curve, and Final denotes the success rate at the maximum # LLM calls (64 for ARC and 300 for others due to domain conventions). Dark lines show performance with the best hyper-parameter setting for each method. Light lines show each run on each hyperparameter. The inset box plots show the distribution while varying the hyper-parameters. APPS baselines: Parsel [25], AlphaCode [14], and Olausson et al. [3]. Nonlinear Loop Invariant baselines: Z3/GSpacer [26] and Yao et al. [21]. ARC baseline: Hypo. Search [23]. More results on APPS Interview-Level and ARC in Figure 11 and Figure 13", "description": "This figure compares the performance of four different program refinement methods (REX, BFS, Greedy, and Fixed Width) across three different datasets (APPS Competition, APPS Introductory, and Nonlinear Loop Invariants) and a visual reasoning puzzle dataset (ARC).  The x-axis represents the number of LLM calls (compute budget), and the y-axis represents the percentage of problems solved. Darker lines represent results with optimal hyperparameters, while lighter lines show performance across a range of hyperparameter settings, emphasizing the robustness of REx.  The inset box plots show the distribution of results for each method and different hyperparameters. Baselines included for comparison are listed in the caption.", "section": "Experimental Results"}, {"figure_path": "o863gX6DxA/figures/figures_24_1.jpg", "caption": "Figure 4: Comparing REx with alternatives using GPT-4 (temp=1). BFS and FW are Breadth First Search and Fixed Width, respectively. AUC denotes Area Under the Curve, and Final denotes the success rate at the maximum # LLM calls (64 for ARC and 300 for others due to domain conventions). Dark lines show performance with the best hyper-parameter setting for each method. Light lines show each run on each hyperparameter. The inset box plots show the distribution while varying the hyper-parameters. APPS baselines: Parsel [25], AlphaCode [14], and Olausson et al. [3]. Nonlinear Loop Invariant baselines: Z3/GSpacer [26] and Yao et al. [21]. ARC baseline: Hypo. Search [23]. More results on APPS Interview-Level and ARC in Figure 11 and Figure 13", "description": "The figure compares the performance of REx against three baselines (Greedy, Breadth-First Search, and Fixed-Width) across three different problem domains (APPS Competition, Nonlinear Loop Invariants, and ARC).  The x-axis represents the number of LLM calls (budget), and the y-axis represents the success rate (percentage of problems solved).  The main observation is that REx consistently achieves higher success rates for the same compute budget, particularly for challenging datasets. The figure includes box plots illustrating the effect of varying hyperparameters within each method and shows that REx is more robust to hyperparameter settings.  Comparative baselines for each domain are also cited.", "section": "5 Experimental Results"}, {"figure_path": "o863gX6DxA/figures/figures_24_2.jpg", "caption": "Figure 4: Comparing REx with alternatives using GPT-4 (temp=1). BFS and FW are Breadth First Search and Fixed Width, respectively. AUC denotes Area Under the Curve, and Final denotes the success rate at the maximum # LLM calls (64 for ARC and 300 for others due to domain conventions). Dark lines show performance with the best hyper-parameter setting for each method. Light lines show each run on each hyperparameter. The inset box plots show the distribution while varying the hyper-parameters. APPS baselines: Parsel [25], AlphaCode [14], and Olausson et al. [3]. Nonlinear Loop Invariant baselines: Z3/GSpacer [26] and Yao et al. [21]. ARC baseline: Hypo. Search [23].", "description": "The figure compares the performance of four different program synthesis methods (REX, Breadth-First Search, Greedy, and Fixed-Width) across three different problem domains: APPS Competition, APPS Introductory, and Nonlinear Loop Invariants.  The x-axis represents the number of LLM calls (compute budget), and the y-axis shows the success rate (percentage of problems solved). The figure shows that REx consistently outperforms other methods, solving more problems with fewer LLM calls. Box plots show the distribution of results across different hyperparameter settings for each method.", "section": "5 Experimental Results"}, {"figure_path": "o863gX6DxA/figures/figures_31_1.jpg", "caption": "Figure 4: Comparing REx with alternatives using GPT-4 (temp=1). BFS and FW are Breadth First Search and Fixed Width, respectively. AUC denotes Area Under the Curve, and Final denotes the success rate at the maximum # LLM calls (64 for ARC and 300 for others due to domain conventions). Dark lines show performance with the best hyper-parameter setting for each method. Light lines show each run on each hyperparameter. The inset box plots show the distribution while varying the hyper-parameters. APPS baselines: Parsel [25], AlphaCode [14], and Olausson et al. [3]. Nonlinear Loop Invariant baselines: Z3/GSpacer [26] and Yao et al. [21]. ARC baseline: Hypo. Search [23].", "description": "The figure compares the performance of REx against three baseline methods (Greedy, Breadth-First Search, and Fixed-Width) across three different problem domains: APPS Competition, Nonlinear Loop Invariant synthesis, and ARC.  The x-axis represents the number of LLM calls (compute budget), and the y-axis represents the success rate in solving the problems.  The figure shows that REx consistently outperforms or competes with the best baseline methods across all datasets, demonstrating its efficiency and robustness.", "section": "Experimental Results"}, {"figure_path": "o863gX6DxA/figures/figures_32_1.jpg", "caption": "Figure 1: Left: The tree of possible refinements is infinitely deep and has infinite branching factor. Each node is a program and each edge is an LLM sample. Right: Explore-Exploit tradeoff for a search state after performing 3 node expansions. Exploit by sampling another child of a program that is nearly correct, or Explore by sampling a child of a program that has been expanded fewer times.", "description": "The figure consists of two parts. The left part shows the tree structure of possible program refinements using LLMs. The right part illustrates the exploration-exploitation tradeoff in program refinement.  It shows that one can choose to either exploit by refining a program that is close to being correct (passing more tests) or explore by refining a less-explored program.", "section": "1 Introduction"}, {"figure_path": "o863gX6DxA/figures/figures_33_1.jpg", "caption": "Figure 7: Example search tree generated by REx. Blue-Yellow gradient indicates heuristic value (blue: low, yellow: high). The order of children from left to right indicates the sequence of generations (left: older, right: newer). See also appendix Fig. 14-17", "description": "This figure shows a search tree generated by the REx algorithm.  Nodes represent programs, and edges represent refinements. The color of each node indicates the heuristic value of the program it represents, with blue representing low heuristic values and yellow representing high heuristic values. The order of the nodes from left to right on each level of the tree shows the order in which the program refinements were explored.  The figure illustrates the explore-exploit tradeoff employed by REx, where it explores programs with lower heuristic values but also prioritizes refining programs with high heuristic values.  The appendix shows more example search trees.", "section": "4 REx: Refine, Explore, Exploit"}, {"figure_path": "o863gX6DxA/figures/figures_34_1.jpg", "caption": "Figure 1: Left: The tree of possible refinements is infinitely deep and has infinite branching factor. Each node is a program and each edge is an LLM sample. Right: Explore-Exploit tradeoff for a search state after performing 3 node expansions. Exploit by sampling another child of a program that is nearly correct, or Explore by sampling a child of a program that has been expanded fewer times.", "description": "The figure on the left shows the search space for iterative code refinement using LLMs.  The search space is an infinitely large tree because each refinement can lead to infinitely many other refinements. The figure on the right illustrates the explore-exploit tradeoff in choosing which branch to explore further during refinement.  'Exploit' focuses on refining programs that are close to correct, while 'Explore' explores programs that have not been explored enough.", "section": "1 Introduction"}, {"figure_path": "o863gX6DxA/figures/figures_35_1.jpg", "caption": "Figure 7: Example search tree generated by REx. Blue-Yellow gradient indicates heuristic value (blue: low, yellow: high). The order of children from left to right indicates the sequence of generations (left: older, right: newer). See also appendix Fig. 14-17", "description": "This figure shows an example of a search tree generated by the REx algorithm. Each node in the tree represents a program, and the edges represent refinements performed by the algorithm. The color of each node represents the heuristic value of the corresponding program, with blue indicating a low heuristic value and yellow indicating a high heuristic value. The order of the children of each node indicates the order in which the refinements were performed. The figure also shows that the algorithm explores multiple refinement paths before converging on a solution.", "section": "4 REx: Refine, Explore, Exploit"}]