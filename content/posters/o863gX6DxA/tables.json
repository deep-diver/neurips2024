[{"figure_path": "o863gX6DxA/tables/tables_12_1.jpg", "caption": "Table 1: Analyze the hyperparameter sensitivity of methods by their performance rankings on different benchmarks. The rankings are determined by (AUC + final_success_rate) / 2. The hyperparameter, empty value for Greedy, denotes the heuristic value assigned for the empty solution. The hyperparameter choices for ARC are different from the others because of its domain convention (maximum # LLM requests=64 instead of 300). REx consistently outperforms or competes with the best baselines when C = 20 for all difficult benchmarks.", "description": "This table analyzes how sensitive different program synthesis methods are to their hyperparameters.  The ranking is based on a combined AUC and final success rate. It shows that REx is relatively insensitive to its hyperparameter (C), consistently performing well across different problem types.  Greedy and BFS (breadth-first search) show more significant changes with differing parameter settings.", "section": "A.2 Hyperparameter analysis"}, {"figure_path": "o863gX6DxA/tables/tables_14_1.jpg", "caption": "Table 2: Benchmark evaluation on the NLA programs. For programs with multiple loops, each loop is labeled with a number, top to down, from outer to inner.", "description": "This table presents the results of evaluating several programs on the NLA (non-linear arithmetic) benchmark.  Each program has one or more loops that require non-linear loop invariants for verification. The table shows the type of invariant (NL = Non-linear or Linear), and whether each method (G-CLN, GSpacer, Loopy-GPT4, and REx) successfully found an invariant for each loop.  A checkmark indicates success. The table helps to compare the effectiveness of different approaches in discovering non-linear loop invariants.", "section": "A.3 Loop Invariants"}, {"figure_path": "o863gX6DxA/tables/tables_15_1.jpg", "caption": "Table 2: Benchmark evaluation on the NLA programs. For programs with multiple loops, each loop is labeled with a number, top to down, from outer to inner.", "description": "This table presents the results of evaluating several programs on a benchmark called NLA, which focuses on non-linear loop invariants. The table shows whether each method (G-CLN, GSpacer, Loopy-GPT4, and REx) successfully finds the loop invariants for different programs. Each program may have multiple loops, which are numbered from outer to inner.  The \"InvType\" column specifies whether the loop invariant is non-linear (\"NL\") or linear (\"Linear\"). This helps in understanding the complexity of the task.", "section": "A.3 Loop Invariants"}, {"figure_path": "o863gX6DxA/tables/tables_17_1.jpg", "caption": "Table 1: Analyze the hyperparameter sensitivity of methods by their performance rankings on different benchmarks. The rankings are determined by (AUC + final_success_rate) / 2. The hyperparameter, empty value for Greedy, denotes the heuristic value assigned for the empty solution. The hyperparameter choices for ARC are different from the others because of its domain convention (maximum # LLM requests=64 instead of 300). REx consistently outperforms or competes with the best baselines when C = 20 for all difficult benchmarks.", "description": "This table analyzes how sensitive different code generation methods are to changes in their hyperparameters.  It ranks methods based on a combined score of AUC (Area Under the Curve) and final success rate.  The table shows that REx, a new method presented in the paper, is relatively insensitive to hyperparameter choices, unlike other methods such as Greedy, BFS, and Fixed-Width, and outperforms or is competitive with these other methods when its hyperparameter C is set to 20. This is particularly true for challenging benchmarks.", "section": "A.2 Hyperparameter analysis"}, {"figure_path": "o863gX6DxA/tables/tables_21_1.jpg", "caption": "Table 1: Analyze the hyperparameter sensitivity of methods by their performance rankings on different benchmarks. The rankings are determined by (AUC + final_success_rate) / 2. The hyperparameter, empty value for Greedy, denotes the heuristic value assigned for the empty solution. The hyperparameter choices for ARC are different from the others because of its domain convention (maximum # LLM requests=64 instead of 300). REx consistently outperforms or competes with the best baselines when C = 20 for all difficult benchmarks.", "description": "This table analyzes how sensitive different code refinement methods are to their hyperparameters.  The methods are evaluated across multiple benchmarks (APPS Competition, APPS Interview, APPS Introductory, ARC, and Nonlinear Loop Invariants), using the area under the curve (AUC) and final success rate. The table shows that the performance of methods other than REx is highly dependent on the specific hyperparameter values chosen. In contrast, REx demonstrates consistent outperformance or competitive performance, even when varying the hyperparameter C.", "section": "A.2 Hyperparameter analysis"}, {"figure_path": "o863gX6DxA/tables/tables_26_1.jpg", "caption": "Table 1: Analyze the hyperparameter sensitivity of methods by their performance rankings on different benchmarks. The rankings are determined by (AUC + final_success_rate) / 2. The hyperparameter, empty value for Greedy, denotes the heuristic value assigned for the empty solution. The hyperparameter choices for ARC are different from the others because of its domain convention (maximum # LLM requests=64 instead of 300). REx consistently outperforms or competes with the best baselines when C = 20 for all difficult benchmarks.", "description": "This table analyzes how sensitive different code generation methods are to their hyperparameters by showing their performance rankings across various benchmarks. The ranking is calculated using a combination of AUC and final success rate.  The table highlights that REx demonstrates consistent top-tier performance or competitiveness against other leading methods, particularly when its hyperparameter C is set to 20, across challenging benchmarks.", "section": "A.2 Hyperparameter analysis"}, {"figure_path": "o863gX6DxA/tables/tables_27_1.jpg", "caption": "Table 1: Analyze the hyperparameter sensitivity of methods by their performance rankings on different benchmarks. The rankings are determined by (AUC + final_success_rate) / 2. The hyperparameter, empty value for Greedy, denotes the heuristic value assigned for the empty solution. The hyperparameter choices for ARC are different from the others because of its domain convention (maximum #LLM requests=64 instead of 300). REx consistently outperforms or competes with the best baselines when C = 20 for all difficult benchmarks.", "description": "This table presents a hyperparameter analysis of various code generation methods, including REx, across different benchmarks (LoopInv, ARC, APPS-Comp, APPS-Inter, APPS-Intro).  The analysis focuses on the impact of adjusting hyperparameters on the methods' performance, using a ranking system based on the average of AUC and final success rate. It shows REx's robustness to hyperparameter variations, particularly when C = 20.", "section": "A.2 Hyperparameter analysis"}]