[{"figure_path": "6Wm4202Wvc/figures/figures_3_1.jpg", "caption": "Figure 1: A visualization of top-2 principal components of gradients (top) and activations (bottom) from different fine-tuning steps (left to right). Color indicates the training labels (binary).", "description": "This figure visualizes the top two principal components of gradients and activations during the fine-tuning process of a model.  The gradients represent the change in model parameters with respect to the loss function, while the activations are the model's intermediate outputs.  The visualization shows how these gradients and activations evolve over various training steps.  The color coding of the points represents the binary labels (0 or 1) associated with the training data, illustrating how the model's internal representations (gradients and activations) are related to the labels during training.  The separation of points based on label can reveal information about label privacy during training.", "section": "3.2 Label Leakage of Standard Split Learning"}, {"figure_path": "6Wm4202Wvc/figures/figures_4_1.jpg", "caption": "Figure 2: An intuitive illustration of the proposed fine-tuning protocol.", "description": "This figure illustrates the proposed fine-tuning protocol, showing the interaction between the server and the client. The server holds n model instances with unique LoRA weights (\u03b81, \u03b82,...\u03b8n).  The client holds the local layers and the model head.  The process starts with the client sending inputs to the server. The server then performs forward passes through each of its n models, resulting in n sets of activations (h1, h2,...hn).  These activations are then weighted (using matrices W1, W2,...Wn) and aggregated by the client. The client then applies its local layers and model head, computing the loss (L). Backpropagation then occurs, with the server receiving weighted gradients from the client, performing backward passes through each of its n models and returning the gradients with respect to the adapter parameters (\u03b8).  This iterative process refines the model parameters while maintaining label privacy.", "section": "3.3 Privacy-preserving backpropagation"}, {"figure_path": "6Wm4202Wvc/figures/figures_7_1.jpg", "caption": "Figure 1: A visualization of top-2 principal components of gradients (top) and activations (bottom) from different fine-tuning steps (left to right). Color indicates the training labels (binary).", "description": "This figure visualizes the top two principal components of gradients and activations during the fine-tuning process of a model.  The top row shows gradients, and the bottom row shows activations.  Each column represents a different stage of the fine-tuning process (Step: 0, Step: 1000, Step: 4000, Step: 16000).  The color of each point represents the binary training label associated with that data point. The figure demonstrates how gradients and activations evolve during training and how they relate to the training labels.", "section": "3.2 Label Leakage of Standard Split Learning"}]