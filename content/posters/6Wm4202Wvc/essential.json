{"importance": "This paper is crucial for researchers in federated learning and AI privacy.  It directly addresses the significant challenge of **maintaining label privacy during model fine-tuning via APIs**, a common practice with growing security concerns. The proposed solution, **P3EFT**, provides a practical and effective method with proven performance improvements. This opens new avenues for secure collaborative AI development and will likely influence the design of future privacy-preserving protocols for large models.", "summary": "P3EFT: A novel multi-party split learning algorithm ensures label privacy during parameter-efficient fine-tuning over APIs, achieving higher accuracy than existing methods.", "takeaways": ["P3EFT, a novel multi-party split learning algorithm, achieves high accuracy while maintaining label privacy during API-based model fine-tuning.", "Low-rank adaptation, a popular fine-tuning method, leaks label information even with minimal parameter updates, highlighting a critical privacy vulnerability.", "P3EFT outperforms existing privacy-preserving methods in both two-party and multi-party settings across various NLP tasks and large language models."], "tldr": "Fine-tuning large language models (LLMs) via APIs is convenient but raises privacy concerns, especially regarding user data. Existing privacy-preserving techniques are often insufficient, particularly for protecting training labels.  This paper focuses on addressing label leakage during parameter-efficient fine-tuning (PEFT). The work is motivated by scenarios where input data is public but labels are private (e.g., social media engagement with known click data). \nThis paper introduces P3EFT, a novel protocol for privacy-preserving PEFT. P3EFT utilizes existing PEFT properties to obfuscate gradients and parameters.  Extensive experimentation on several NLP tasks and LLMs (DeBERTa, Flan-T5, LLaMA) demonstrates P3EFT's effectiveness in maintaining privacy while maintaining high accuracy. Results show that P3EFT outperforms existing methods in various scenarios. The findings highlight the importance of considering label privacy in API-based LLM fine-tuning and offer a practical solution for secure collaborative AI development.", "affiliation": "string", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "6Wm4202Wvc/podcast.wav"}