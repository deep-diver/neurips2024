[{"figure_path": "FYm8coxdiR/tables/tables_6_1.jpg", "caption": "Table 1: Cosine similarity of image features before and after flipping on Clean and Typographic datasets.", "description": "This table presents the cosine similarity results of image features before and after applying a horizontal flip operation. The data is separated into two groups: 'Original' representing images without added text, and 'Typographic' representing images with superimposed text, simulating a typographic attack. The cosine similarity values are calculated across ten different datasets (ImageNet, Caltech101, Food101, Flowers102, OxfordPets, EuroSAT, DTD, StanfordCars, FGVCAircraft, SUN397). Lower cosine similarity in the 'Typographic' set suggests that the presence of text significantly affects the horizontal flip invariance property of CLIP's image feature representation, forming a basis for the proposed disentanglement method.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/tables/tables_7_1.jpg", "caption": "Table 2: Results of image classification on original datasets.", "description": "This table presents the performance of various methods on image classification tasks using original datasets (without typographic attacks).  The methods compared include CLIP (the baseline), Materzynska et al. [16], PAINT [11], Defense Prefix [1], and the proposed MirrorCLIP method.  The table shows the accuracy achieved by each method across 10 different datasets, providing a comprehensive comparison of their performance on standard image classification benchmarks.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/tables/tables_8_1.jpg", "caption": "Table 3: Results of image classification on synthetic typographic attack datasets.", "description": "This table presents the results of image classification experiments conducted on synthetic typographic attack datasets.  It compares the performance of several methods, including CLIP, Materzynska et al. [16], PAINT [11], Defense Prefix [1], and the proposed MirrorCLIP method. The results are shown as the average accuracy across various datasets (ImageNet, Caltech, Food, Flowers, Pets, SAT, DTD, Cars, Aircraft, SUN).  The table highlights the improvement achieved by MirrorCLIP in defending against typographic attacks, where misleading text is overlaid on images.", "section": "5.3 Evaluation on visual features disentanglement"}, {"figure_path": "FYm8coxdiR/tables/tables_8_2.jpg", "caption": "Table 4: Results of image classification on real-world typographic attack datasets.", "description": "This table presents the results of image classification experiments conducted on three real-world typographic attack datasets.  It compares the performance of different methods, including CLIP, Materzynska et al. [16], PAINT [11], Defense Prefix [1], and the proposed MirrorCLIP method.  The results show the accuracy of each method in correctly classifying images that have been tampered with by adding misleading text, demonstrating the effectiveness of MirrorCLIP in handling such adversarial examples.", "section": "5.3 Evaluation on visual features disentanglement"}, {"figure_path": "FYm8coxdiR/tables/tables_8_3.jpg", "caption": "Table 5: Results of text recognition on typographic attack datasets.", "description": "This table presents the results of text recognition experiments conducted on typographic attack datasets.  It compares the performance of the baseline CLIP model against the proposed MirrorCLIP framework.  The results are broken down by dataset (Imagenet, Flowers, Food, and three real-world datasets from [16], [11], and RTA-100) showing a significant improvement in accuracy for MirrorCLIP.", "section": "5.4 Evaluation on textual features disentanglement"}, {"figure_path": "FYm8coxdiR/tables/tables_8_4.jpg", "caption": "Table 6: Results of different features on various tasks.", "description": "This table presents the performance comparison of different feature representations (image features, flipped image features, textual features, textual features (hard), visual features, visual features (hard)) on three tasks: image recognition (original and typographic images), and text recognition (real-world typographic attacks).  The results show the effectiveness of disentangled features on each of these tasks, highlighting that the proposed disentanglement method improves performance, particularly in text recognition.", "section": "5.5 Analysis and Ablation"}, {"figure_path": "FYm8coxdiR/tables/tables_12_1.jpg", "caption": "Table A: Templates of synthetic typographic attack datasets for image recognition", "description": "This table lists the templates used for image recognition in synthetic typographic attack datasets.  For each dataset (ImageNet, Caltech101, Food101, Flowers102, OxfordPets, EuroSAT, DTD, StanfordCars, FGVCAircraft, SUN397), a text prompt template is provided that was used with CLIP.  These templates are designed to guide CLIP in its image recognition task, especially in the context of typographic attacks where misleading text is superimposed on the image.", "section": "5.1 Experimental Setup"}, {"figure_path": "FYm8coxdiR/tables/tables_14_1.jpg", "caption": "Table B: Image features\u2019 cosine similarity before and after flipping on Clean and Typographic datasets with RN50\u00d74 as image encoder.", "description": "This table presents the cosine similarity of image features before and after horizontal flipping for both clean and typographic datasets.  The experiment uses ResNet50x4 as the image encoder.  It shows how the cosine similarity changes after adding text to the images, indicating the difference in the \"mirror effect\" between visual objects and text.  Lower cosine similarity after flipping for typographic images indicates that the textual elements lack horizontal flip invariance in CLIP.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/tables/tables_15_1.jpg", "caption": "Table 2: Results of image classification on original datasets.", "description": "This table presents the results of image classification experiments performed on original datasets (without typographic attacks).  It compares the performance of the proposed MirrorCLIP method against several baseline methods, including CLIP, Materzynska et al., PAINT, and Defense Prefix.  The results are shown for 10 different datasets, showcasing the accuracy of each method in correctly classifying images. The performance is measured as the average classification accuracy across these datasets.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/tables/tables_15_2.jpg", "caption": "Table 3: Results of image classification on synthetic typographic attack datasets.", "description": "This table presents the results of image classification experiments conducted on synthetic typographic attack datasets.  It compares the performance of the proposed MirrorCLIP method against the baseline CLIP model and other state-of-the-art typographic defense methods (Materzynska et al., PAINT, Defense Prefix). The results are shown for 10 different datasets, illustrating the effectiveness of MirrorCLIP in mitigating the impact of typographic attacks on image recognition.", "section": "5.3 Evaluation on visual features disentanglement"}, {"figure_path": "FYm8coxdiR/tables/tables_15_3.jpg", "caption": "Table 4: Results of image classification on real-world typographic attack datasets.", "description": "This table presents the results of image classification experiments conducted on three real-world typographic attack datasets. It compares the performance of the proposed MirrorCLIP method against the baseline CLIP model and three other state-of-the-art typographic defense methods: Materzynska et al. [16], PAINT [11], and Defense Prefix [1].  The results show that MirrorCLIP significantly improves the accuracy of image classification on these datasets compared to the other methods, highlighting its effectiveness in handling text-visual images.", "section": "5.3 Evaluation on visual features disentanglement"}, {"figure_path": "FYm8coxdiR/tables/tables_15_4.jpg", "caption": "Table 3: Results of image classification on synthetic typographic attack datasets.", "description": "This table presents the results of image classification experiments conducted on synthetic typographic attack datasets.  It compares the performance of the proposed MirrorCLIP method against the baseline CLIP model. The datasets used are synthetic versions of 10 public datasets, where misleading text has been added to the images. The table showcases a significant improvement in accuracy achieved by MirrorCLIP compared to CLIP, highlighting the effectiveness of its disentanglement technique in mitigating the negative impact of misleading text.", "section": "5.3 Evaluation on visual features disentanglement"}, {"figure_path": "FYm8coxdiR/tables/tables_15_5.jpg", "caption": "Table G: Results of image classification on original datasets with different feature representations.", "description": "This table presents the results of image classification on 10 original datasets using different feature representations.  The rows represent the performance of image classification using different feature sets: original image features, flipped image features, textual features, textual features (zero), visual features, and visual features (zero). The columns represent the 10 different datasets, and the final column shows the average performance across all datasets. The table compares the effectiveness of each feature set in the context of image recognition.", "section": "5.5 Analysis and Ablation"}, {"figure_path": "FYm8coxdiR/tables/tables_15_6.jpg", "caption": "Table 3: Results of image classification on synthetic typographic attack datasets.", "description": "This table presents the results of image classification experiments conducted on synthetic typographic attack datasets.  The experiments evaluate the performance of different methods, including the proposed MirrorCLIP and several baselines,  in correctly classifying images that have been modified with superimposed text that is typographically designed to mislead the model. The table shows the accuracy of each method on various datasets, highlighting the effectiveness of MirrorCLIP in mitigating the impact of typographic attacks.", "section": "5.3 Evaluation on visual features disentanglement"}, {"figure_path": "FYm8coxdiR/tables/tables_16_1.jpg", "caption": "Table I: Results of image classification on real-world typographic attack datasets with different feature representations.", "description": "This table presents the results of image classification experiments conducted on three real-world typographic attack datasets using different feature representations obtained from the proposed MirrorCLIP framework. The datasets represent scenarios where text is superimposed on images, posing a challenge to image recognition models. The features compared are: original image features, flipped image features, textual features (with and without zeroing), and visual features (with and without zeroing).  The performance is measured as the accuracy percentage in correctly identifying the images despite textual interference. The comparison highlights the impact of disentangling textual and visual features in improving robustness to typographic attacks. ", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/tables/tables_16_2.jpg", "caption": "Table J: Results of text recognition on real-world typographic attack datasets with different feature representations.", "description": "This table presents the results of text recognition experiments conducted on real-world typographic attack datasets using different feature representations.  The representations tested include: original image features, flipped image features, textual features (with and without zeroing), and visual features (with and without zeroing).  The performance is evaluated across three different datasets from the literature ([16], [11], RTA-100 [1]). The average accuracy across these three datasets is reported for each feature representation, showing the impact of disentanglement on the performance of text recognition.", "section": "5.4 Evaluation on textual features disentanglement"}, {"figure_path": "FYm8coxdiR/tables/tables_16_3.jpg", "caption": "Table K: Results of image recognition on ordinary and special palindromes datasets.", "description": "This table presents the results of image recognition experiments conducted on two types of palindrome datasets: ordinary and special.  Ordinary palindromes are those where the word's visual appearance changes upon horizontal flipping (e.g., 'did' to 'bib'), while special palindromes retain their visual form after flipping (e.g., 'mom' to 'mom'). The results are shown for both CLIP and MirrorCLIP, comparing the accuracy of image recognition on original (non-typographic) and typographic images for each dataset and palindrome type.  The 'Avg.' row provides average accuracy across the three datasets.", "section": "5.5 Analysis and Ablation"}, {"figure_path": "FYm8coxdiR/tables/tables_16_4.jpg", "caption": "Table L: Results of different features on image recognition with flipped text.", "description": "This table presents the results of image recognition experiments using different feature representations (image features, flipped image features, and visual features) on both original and typographic datasets where images have been horizontally flipped.  The accuracy for each representation is shown separately for original and typographic images indicating the robustness of each feature representation to the typographic attacks.", "section": "5.5 Analysis and Ablation"}]