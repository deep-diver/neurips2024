[{"figure_path": "FYm8coxdiR/figures/figures_1_1.jpg", "caption": "Figure 1: The zero-shot prediction of CLIP before and after disentanglement, (a): prediction of text recognition, text of \u201ceraser\u201d is misclassified as \u201cegg\u201d before disentanglement, (b): prediction of image recognition, visual object of a dog is misclassified as a cat before disentanglement", "description": "This figure demonstrates the zero-shot prediction performance of CLIP on text-visual images before and after applying the proposed disentanglement method.  Subfigure (a) shows the text recognition task, highlighting the misclassification of \"eraser\" as \"egg\" before disentanglement and the correct classification after disentanglement. Subfigure (b) shows the image recognition task, illustrating the misclassification of a dog as a cat before disentanglement and the accurate classification after disentanglement. The figure visually represents the effectiveness of the MirrorCLIP framework in disentangling textual and visual features, improving the accuracy of CLIP's predictions on ambiguous inputs.", "section": "1 Introduction"}, {"figure_path": "FYm8coxdiR/figures/figures_2_1.jpg", "caption": "Figure 2: The cosine similarity of the image features encoded by the CLIP image encoder before and after horizontal flipping. Adding text to the image leads to a significant decrease in cosine similarity, indicating that CLIP does not exhibit horizontal flip invariance for textual factors.", "description": "This figure demonstrates the mirror effect of CLIP on text and visual features. The left side shows images of a dog and an apple, both before and after horizontal flipping. The cosine similarity between the original and flipped images is close to 1, indicating that CLIP is invariant to horizontal flipping for visual objects. The right side shows the same images, but with text added (cat and earphones).  In these images, the cosine similarity is significantly lower after flipping, demonstrating that CLIP is not invariant to horizontal flipping for text features. This difference is crucial for the MirrorCLIP model, as it allows it to distinguish between visual and textual features based on this horizontal flip invariance.", "section": "3 CLIP's Mirror Effect and Disentangling Masks"}, {"figure_path": "FYm8coxdiR/figures/figures_3_1.jpg", "caption": "Figure 3: Results of mirror effect experiments, (a) Cosine similarity of image features before and after flipping on Original and Typographic datasets, (b) Proportion of textual mask on Original and Typographic datasets.", "description": "This figure presents the results of experiments designed to demonstrate the difference in mirror effects between visual objects and text using CLIP.  The left panel (a) shows the cosine similarity of image features before and after horizontal flipping, for both original datasets (containing only images) and typographic datasets (images with added text).  A high cosine similarity indicates that the features remain largely unchanged after flipping.  The right panel (b) shows the proportion of textual mask generated by MirrorCLIP for each dataset, reflecting the algorithm's ability to identify text within an image. The significant drop in cosine similarity in the typographic datasets and the increase in textual mask proportion demonstrate the effectiveness of MirrorCLIP in distinguishing text from visual elements based on the horizontal flip invariance of images, but not text.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/figures/figures_4_1.jpg", "caption": "Figure 4: Generation of disentangling mask, (a) The disentangling mask is generated by contrasting the sign of corresponding positions in the image features before and after flipping. (b) Input images and generated disentangling masks (resized from 1 \u00d7 512 to 16 \u00d7 32). After adding text to the image, the proportion of textual mask increases significantly.", "description": "This figure shows how the disentangling mask is generated and how it changes with the addition of text to an image.  (a) illustrates the process of generating the mask by comparing the signs of corresponding elements in the image features before and after horizontal flipping.  Positive values indicate visual features, negative values indicate text features. (b) visually demonstrates the mask's application to sample images, showing a clear increase in the textual mask proportion after text is added to the image.", "section": "4 Zero-shot Disentanglement Framework"}, {"figure_path": "FYm8coxdiR/figures/figures_4_2.jpg", "caption": "Figure 5: Pipeline of zero-shot dual-stream disentanglement framework. The framework takes flipped and original images as input, generates disentangling masks by comparing their image features in the latent space, then utilizes the proposed textual filter and visual filter to generate textual and visual features, achieving disentanglement and completing downstream tasks.", "description": "This figure illustrates the MirrorCLIP framework, a zero-shot method for disentangling textual and visual features from images. It uses both original and flipped versions of an image as input. The image encoders generate features for each, and a mask generation module compares these to identify textual and visual regions.  These masks then filter the features, separating textual and visual information which allows for separate textual and visual predictions.", "section": "4 Zero-shot Disentanglement Framework"}, {"figure_path": "FYm8coxdiR/figures/figures_6_1.jpg", "caption": "Figure 6: Visualization for textual and non-textual features for typographic attacked data using class activation map.", "description": "This figure shows the visualization of textual and visual features using Class Activation Mapping (CAM) for images that have been subjected to typographic attacks.  The CAM highlights the regions of the image that contribute most strongly to the classification of the image. The left column shows the input image with overlaid text (a typographic attack). The middle column shows the CAM for the textual features, highlighting the regions corresponding to the added text. The right column shows the CAM for the visual features, highlighting the regions corresponding to the actual image content (the visual object). This demonstrates the effectiveness of the MirrorCLIP framework in separating textual and visual features from images.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/figures/figures_7_1.jpg", "caption": "Figure 7: Results of image variation using Stable unCLIP.", "description": "This figure visualizes the results of image variation experiments using the Stable unCLIP model.  It demonstrates the effectiveness of the MirrorCLIP framework in disentangling textual and visual features. The top row shows the input image, the features extracted by the image encoder, the disentangled visual and textual features, and the images generated using only the visual or textual features.  The results show that the disentangled visual features generate images similar to the original but without text, while the disentangled textual features generate images containing only text, thus demonstrating the success of the disentanglement process.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/figures/figures_9_1.jpg", "caption": "Figure 8: Potential Application Examples of MirrorCLIP. (a) Using MirrorCLIP to disentangle region features of RegionCLIP, before disentanglement, RegionCLIP mistakenly identified a price tag with text \u201cpapaya\u201d as papaya and a laptop monitor as a television set because of the interference of text \u201ctelevision\u201d. (b) Textual features disentangled by MirrorCLIP are used to provide prompts for SAM, achieving text region segmentation.", "description": "This figure demonstrates two potential applications of MirrorCLIP. In (a), it shows how MirrorCLIP improves RegionCLIP's accuracy by disentangling text and visual features. Before using MirrorCLIP, RegionCLIP misclassified a price tag with the text \"papaya\" as a papaya and a laptop monitor as a television because of the text interference. After using MirrorCLIP, RegionCLIP's accuracy is improved. In (b), it shows how MirrorCLIP enhances SAM by providing disentangled textual features as prompts, achieving accurate text region segmentation.", "section": "Potential application"}, {"figure_path": "FYm8coxdiR/figures/figures_13_1.jpg", "caption": "Figure A: Typographic datasets. (a) generation of synthetic typographic datasets. (b) a sample of real typographic datasets.", "description": "This figure shows examples of typographic attack datasets used in the paper.  Panel (a) illustrates the process of creating synthetic typographic datasets: starting with an original image (e.g., a dog),  irrelevant text is added on top. Panel (b) displays examples of real-world typographic datasets, where images already contain overlaid or nearby text that could be misleading to a model performing image recognition.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/figures/figures_13_2.jpg", "caption": "Figure B: Samples of synthetic typographic attack datasets.", "description": "This figure shows examples of images from synthetic typographic attack datasets.  The original images have been modified by adding text that is irrelevant to the actual visual content of the image. This is done to simulate real-world scenarios where misleading text could confuse a model trying to identify the object in the image. The goal is to test the robustness of the model against these typographic attacks.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/figures/figures_14_1.jpg", "caption": "Figure C: Samples of real-world typographic attack datasets.", "description": "This figure shows examples of real-world typographic attacks used in the paper's experiments.  These images depict objects with misleading text labels added to them, demonstrating a challenging scenario for image recognition models. The labels are handwritten and purposefully incorrect to mimic real-world situations where visual and textual information may conflict.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/figures/figures_14_2.jpg", "caption": "Figure 6: Visualization for textual and non-textual features for typographic attacked data using class activation map.", "description": "This figure shows the visualization of textual and visual features using Class Activation Mapping (CAM) for images with typographic attacks.  The CAM highlights the regions of the image that contribute most strongly to the classification of textual or visual content.  For each image, there are three sub-images: the input image with added text, the CAM for the disentangled textual features, and the CAM for the disentangled visual features. This visualizes how effectively MirrorCLIP separates textual and visual regions in text-visual images.", "section": "5.2 Validation Experiments for MirrorCLIP"}, {"figure_path": "FYm8coxdiR/figures/figures_14_3.jpg", "caption": "Figure 7: Results of image variation using Stable unCLIP.", "description": "This figure shows the results of image generation using the Stable unCLIP model with different input conditions.  The top row shows the input images, which include images with and without added typographic text.  The subsequent rows display the results of image generation using the original image features, disentangled visual features, and disentangled textual features from MirrorCLIP, respectively. The results demonstrate the ability of MirrorCLIP to effectively separate visual and textual features in images, as the visual features generate images similar to the original but without text, while the textual features generate images that only contain text.", "section": "5.3 Evaluation on visual features disentanglement"}]