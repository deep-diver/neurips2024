[{"type": "text", "text": "CLIP in Mirror: Disentangling text from visual images through reflection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tiancheng Wang1\u2020 Yuguang Yang2\u2020 Linlin Yang4\u2217 Shaohui Lin5 Juan Zhang1,3 Guodong Guo6 Baochang Zhang1,3 ", "page_idx": 0}, {"type": "text", "text": "1Institute of Artificial Intelligence, Beihang University, Beijing, China 2School of Electronic Information Engineering, Beihang University, Beijing, China 3Zhongguancun Laboratory, Beijing, China 4State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China ", "page_idx": 0}, {"type": "text", "text": "5School of Computer Science and Technology, East China Normal University, Shanghai, China 6Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The CLIP network excels in various tasks, but struggles with text-visual images i.e., images that contain both text and visual objects; it risks confusing textual and visual representations. To address this issue, we propose MirrorCLIP, a zero-shot framework, which disentangles the image features of CLIP by exploiting the difference in the mirror effect between visual objects and text in the images. Specifically, MirrorCLIP takes both original and flipped images as inputs, comparing their features dimension-wise in the latent space to generate disentangling masks. With disentangling masks, we further design fliters to separate textual and visual factors more precisely, and then get disentangled representations. Qualitative experiments using stable diffusion models and class activation mapping (CAM) validate the effectiveness of our disentanglement. Moreover, our proposed MirrorCLIP reduces confusion when encountering text-visual images and achieves a substantial improvement on typographic defense, further demonstrating its superior ability of disentanglement. Our code is available at https://github.com/tcwangbuaa/MirrorCLIP. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The CLIP network [19] has demonstrated remarkable success, leading to its widespread application in real-world scenarios. However, it still struggles with text-visual images [7; 16; 11; 1], i.e., images that contain both text and visual objects, where CLIP can become confused when processing an image with misleading text in it. For instance, as shown in Figure 1, when asked to describe a visual object, the model might mistake an image of a dog for a cat due to the presence of the \u201ccat\u201d text. In contrast, when asked to recognize text in an image, the model might mistake the text of \u201ceraser\u201d for \u201cegg\u201d due to the visual object of eggs. Can we disentangle the textual and visual1 factors within CLIP? Achieving this disentanglement for CLIP can reduce confusion when encountering such images and enhance its robustness against typographic attacks [1]. ", "page_idx": 0}, {"type": "text", "text": "Existing work emphasizes extracting visual features from image features and exploring additional structures [16] or training strategies [1; 11]. Specifically, Materzynska et al. [16] introduce a learnable projection, Defense Prefix [1] introduces a learnable prefix in the prompt, and PAINT [11] introduces fine-tuning with linearly interpolating neural network weights. However, these methods require training with specified data and overlook textual features. Instead, we aim at a zero-shot architecture without retraining for CLIP and emphasize both textual and visual features via disentanglement. ", "page_idx": 0}, {"type": "image", "img_path": "FYm8coxdiR/tmp/a6f1cb10ce4e5f1ba7389c8d5e92684bd8b6604ea8238103536be850ad00a16f.jpg", "img_caption": ["Figure 1: The zero-shot prediction of CLIP before and after disentanglement, (a): prediction of text recognition, text of \u201ceraser\u201d is misclassified as \u201cegg\u201d before disentanglement, (b): prediction of image recognition, visual object of a dog is misclassified as a cat before disentanglement "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose MirrorCLIP, a simple yet efficient text-visual disentanglement framework to enhance CLIP\u2019s robustness in text-visual images. Our innovation is leveraging the differences of mirror effect between text and visual elements in the image - when images are horizontally flipped, visual objects maintain semantic consistency after flipping, while text typically becomes a nonsensical string. For instance, after being filpped, an image of a dog still remains its recognizability, while an image containing the word \u201ccat\u201d turns into a nonsensical string like \u201ctac\u201d, resulting in the disappearance of its meaning. Based on this observation, we propose to decompose the image features of CLIP into textual and visual factors by contrasting them before and after flipping in the latent space. Specifically, MirrorCLIP employs a dual-stream zero-shot framework. The process begins by inputting both the original and horizontally flipped images into the image encoder to generate corresponding image features. By comparing these features, we generate a disentangling mask that identifies textual and visual regions of the latent variable. This mask is then used to separate textual and visual features. Specifically, the textual features are derived by excluding visual features from the original image features using the mask, while the visual features are obtained by combining image features of the original images with their flipped version. ", "page_idx": 1}, {"type": "text", "text": "Extensive experiments validate our proposed method. For text-visual disentanglement, the class activation maps (CAMs) [24] show that the disentangled textual and visual features correspond precisely to the regions of text and visual objects, respectively. Using the stable diffusion model [21; 20], visual features generate images similar to the original but without text, while textual features generate textual images (i.e., images only contain text), demonstrating the effectiveness of our method. To quantitatively evaluate the effectiveness of visual feature disentanglement, we compared the state-of-the-art typographic defense methods Defense Prefix [1] in 10 synthetic and 3 real-world typographic attack datasets using disentangled features. Typographic attacks add text on top of visuals, testing the model\u2019s robustness against textual perturbations. MirrorCLIP achieves substantial performance improvements, with a $+4.17\\%$ increase in real-world datasets and a $+5.89\\%$ increase in synthetic datasets. To further evaluate the disentangled textual features, we propose to recognize the typographic attack text. The results show that with disentangled textual features, the accuracy improves to $73.95\\%$ , compared to $39.32\\%$ without disentanglement. In summary, the contributions of our work are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We observed that CLIP exhibits horizontal filp invariance for the visual factors of images but not for the textual factors, and propose a simple yet efficient solution to disentangle textual features from visual features in the latent space of CLIP accordingly. \u2022 We propose MirrorCLIP, a zero-shot text-visual disentanglement framework, which can effectively achieve the disentanglement of visual and textual features without any additional training and significantly reduce confusion in text-visual images while improving the robustness of CLIP against typographic attacks. ", "page_idx": 1}, {"type": "image", "img_path": "FYm8coxdiR/tmp/b6046e777120e546ca366ad3d3a8f6d2de91263a6e3d9e1a3af489d4803da581.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: The cosine similarity of the image features encoded by the CLIP image encoder before and after horizontal filpping. Adding text to the image leads to a significant decrease in cosine similarity, indicating that CLIP does not exhibit horizontal flip invariance for textual factors. ", "page_idx": 2}, {"type": "text", "text": "\u2022 We qualitatively demonstrate the effectiveness of our disentangled representations through the salient regions of CAMs. Moreover, with stable diffusion models and our disentangled representations, we enable generation based on visual and textual factors. \u2022 By evaluating on typographic images, we show that MirrorCLIP effectively achieves disentangled representations and greatly improves performance compared to CLIP without disentanglement, including a whopping $\\bar{1}6.82\\%$ improvement on image recognition and $34.63\\%$ improvement on text recognition, surpassing state-of-the-art methods on defense against typographic attacks. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vision-language models have advanced significantly, learning generalized visual representations that align with textual descriptions [19]. This capability enables VLMs to make few-shot or zeroshot decisions in open-world settings [8; 25; 26], making them highly effective for downstream tasks. However, this broad generalization also raises concerns about robustness, especially when dealing with images containing rich text elements, which can mislead the model\u2019s decision results. MirrorCLIP further explores this setting, aiming to disentangle textual and visual features from text-visual images to improve CLIP\u2019s robustness in these challenging scenarios. ", "page_idx": 2}, {"type": "text", "text": "Typographic attacks were first introduced by Goh et al. [7], who revealed that the performance of vision-language models drops dramatically when input images contain misleading text. To mitigate this, Materzynska et al.[16] applied a linear projection matrix to disentangle visual from textual features. Ilharco et al.[11] interpolated between fine-tuned and original CLIP models, and Azuma et al. [1] introduced a learnable defense prefix. We utilize this task to evaluate the disentangled textual and visual features: visual features are used in typographic defense experiments, and textual features are used in typographic text recognition experiments. ", "page_idx": 2}, {"type": "text", "text": "Disentangled representations of CLIP have been studied to separate different types of information encoded in embeddings. Ramesh et al. [20] used PCA to reconstruct CLIP embeddings and generated related images through diffusion models, revealing distinct semantic dimensions. Lemesle et al. [14] found that textual and visual factors of an image do not share semantic representations in CLIP. Materzynska et al. [16] trained projection matrices to disentangle visual and textual features. MirrorCLIP further explores the way to uncover the textual and visual components of the representations of text-visual images. ", "page_idx": 2}, {"type": "text", "text": "3 CLIP\u2019s Mirror Effect and Disentangling Masks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Contrastive Language-Image Pretraining. CLIP [19] aims to learn robust associations between text and images without requiring explicit labeling or supervision for specific tasks. It is pretrained on a dataset including 400 million image-text pairs without human annotation, which provides a broad spectrum of possible text-image associations. During training, through contrastive learning, ", "page_idx": 2}, {"type": "image", "img_path": "FYm8coxdiR/tmp/67e5bda27331c8130dc23e44b995270d72bb7d56b7ca2e69b67dc277600dbfca.jpg", "img_caption": ["Figure 3: Results of mirror effect experiments, (a) Cosine similarity of image features before and after flipping on Original and Typographic datasets, (b) Proportion of textual mask on Original and Typographic datasets. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "CLIP optimizes to maximize the cosine similarity of embeddings between matching text-image pairs. This enables CLIP to learn the embeddings of images and text within a joint latent space, thereby allowing CLIP to extract the semantics of images. However, recent work has revealed that CLIP can become confused when faced with text-visual images [7; 16; 11; 1]. To address this issue, we leverage CLIP\u2019s mirror effect to achieve the disentanglement of textual and visual components within the image embeddings. ", "page_idx": 3}, {"type": "text", "text": "CLIP\u2019s Mirror Effect. When we observe objects in the mirror, we are able to identify their reflected presence. However, this may not be the case with text. Because the text that is mirrored appears as a string of non-sensical characters due to the letter distortion and the reversed writing order. CLIP is a joint image and text embedding model designed to recognize concepts in images. Does CLIP act like a human and exhibit a similar phenomenon? ", "page_idx": 3}, {"type": "text", "text": "To determine whether the distinct mirror effects between visual objects and text affect the representation of CLIP, we input both the original and filpped images into the encoder and calculate the cosine similarity between the resulting features. As shown in Figure 2, for clean input images, such as a dog, the cosine similarity remains approximately 1, indicating semantic invariance. However, when text is added, the similarity between the original and flipped images significantly decreases. Furthermore, our quantitative experiments on 10 public datasets reveal that similarity drops significantly from 0.9846 to 0.8225 once text is added to the images as shown in Figure 3 (a). This demonstrates that the image features of visual objects in CLIP have horizontal filp invariance, whereas that of text does not, which can be further exploited to disentangle these two factors from image representations. ", "page_idx": 3}, {"type": "text", "text": "Disentangling Mask. Given $X$ and $X^{f}$ represent the image features before and after image filpping, their cosine similarity can be written as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\cos\\left(X,X^{f}\\right)=\\frac{\\sum_{i=1}^{n}\\left(X_{i}\\odot X_{i}^{f}\\right)}{\\left\\|X\\right\\|\\times\\left\\|X^{f}\\right\\|}=\\sum_{i=1}^{n}\\left(\\frac{X_{i}}{\\left\\|X\\right\\|}\\odot\\frac{X_{i}^{f}}{\\left\\|X^{f}\\right\\|}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\odot$ denotes Hadamard product, $n$ denotes the dimensionality of features. The cosine similarity is influenced by the product of elements at different positions after normalization. If the signs of elements at the corresponding positions change after filpping, the product becomes negative, leading to a decrease in cosine similarity. Previous research [20] has shown that different dimensions of CLIP\u2019s image embeddings encode distinct semantic information. Therefore, we use the change in the signs of elements at different positions before and after filpping to determine whether a position belongs to the textual or visual factors. Subsequently, we generate the disentangling mask for textual and visual factors with this characteristic. As shown in Figure 4 (a), the disentangling mask $M$ , and its corresponding textual mask $M^{t}$ and visual mask $M^{v}$ , are calculated as ", "page_idx": 3}, {"type": "equation", "text": "$$\nM=S i g n\\left(X\\odot X^{f}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\nM^{t}=-\\frac{1}{2}\\times(M-1),\\quad M^{v}=\\frac{1}{2}\\times(M+1),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "FYm8coxdiR/tmp/e12cf1ed8ebc97df9802433992c20c396310e75fca4de2c0b5e755af4548c6d3.jpg", "img_caption": ["Figure 4: Generation of disentangling mask, (a) The disentangling mask is generated by contrasting the sign of corresponding positions in the image features before and after filpping. (b) Input images and generated disentangling masks (resized from $1\\times512$ to $16\\times32)$ ). After adding text to the image, the proportion of textual mask increases significantly. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "FYm8coxdiR/tmp/edc1e31d6af7a5d4ecd90b6c5f64b01c9cdafcaa3c187101efc0836659e1a5b2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 5: Pipeline of zero-shot dual-stream disentanglement framework. The framework takes filpped and original images as input, generates disentangling masks by comparing their image features in the latent space, then utilizes the proposed textual filter and visual filter to generate textual and visual features, achieving disentanglement and completing downstream tasks. ", "page_idx": 4}, {"type": "text", "text": "$M^{t}$ and $M^{v}$ are obtained by mapping $\\{-1,1\\}$ in $M$ to $\\{1,0\\}$ and $\\{0,1\\}$ , respectively. Figure 4 (b) shows the disentangling masks for different images, where the black areas represent the textual mask and the white areas represent the visual mask. It can be observed that when text is added to the image, whether handwritten or printed, the area of the textual mask increases significantly. As shown in Figure 3 (b), we conduct experiments across 10 public datasets and reveal that the proportion of textual mask increases significantly from 0.0683 to 0.2431 after adding text to images, which confirms the validity of our mask generation method. ", "page_idx": 4}, {"type": "text", "text": "4 Zero-shot Disentanglement Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Utilizing the disentangling masks, we propose a zero-shot dual-stream disentanglement framework. The pipeline of the disentanglement framework is illustrated in Figure 5. The framework is straightforward and does not require any training. We generate the disentangling mask by comparing the image features before and after image filpping based on Eqs. 2 and 3. Due to the entangling between textual and visual features, any dimension of latent space may contain both textual and visual factors. Therefore, separating with a \u201cboolean\u201d disentangling mask is rough. For example, directly setting the visual mask area to 0 would lead to the loss of the textual semantic information within it. Instead, we propose a \u201csoft\u201d textual filter to remove visual features in the image features as below: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nX^{t}=\\left(X\\odot M^{t}\\right)+\\left(X-X^{f}\\right)\\odot M^{v},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $X^{t}$ denotes textual features. The textual fliter in Equation 4 consists of two parts: the first part ensures the retention of textual features in the textual mask region, and the second part preserves textual features in the visual mask region while filtering out visual features. ", "page_idx": 5}, {"type": "text", "text": "Similarly, we propose a visual filter to get visual features $X^{v}$ as below: ", "page_idx": 5}, {"type": "equation", "text": "$$\nX^{v}=X^{f}+X\\odot M^{v}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Equation 5, the first part uses filpped image features as visual features due to the disappearance of textual semantics after filpping, and the second part enhances robustness against images with filpped text by adding a visual mask region of original image features. After disentangling, either textual features or visual features can be used to replace original image features for inference depending on the specific task. As shown in Figure 1, our disentanglement framework can correct errors made by CLIP in image and text recognition. ", "page_idx": 5}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Overview. To validate the effectiveness of the proposed MirrorCLIP, we conduct the following experiments. In Section 5.2, we first validate the difference in the mirror effect of text and visual elements in images with 10 clean public datasets and their corresponding synthetic typographic datasets. We then qualitatively visualize the quality of disentangled text and visual elements using CAMs and stable diffusion models. In Section 5.3, we primarily validate the disentanglement effectiveness for visual elements by performing a typographic defense experiment on 13 typographic datasets (10 synthetic datasets and 3 real-world datasets), following [1]. In Section 5.4, we evaluate the disentanglement effectiveness for text elements by ensuring that the disentangled textual features can correctly recognize the text added to visual elements in the typographic datasets. ", "page_idx": 5}, {"type": "text", "text": "Datasets. Clean public classification datasets contain rich visual elements from the real world, which can be used to evaluate the robustness and performance of MirrorCLIP. These include ImageNet [4], Caltech101 [6], OxfordPets [18], StanfordCars [13], Flowers102 [17], Food101 [2], FGVCAircraft [15], DTD [3], SUN397 [23], and EuroSAT [10]. Synthetic typographic Datasets add text of incorrect categories to the images. We follow [1] to construct synthetic typographic datasets using the 10 clean public datasets mentioned above. Real-world typographic datasets include three publicly available real-world typographic attack datasets from Materzynska et al. [16], PAINT[11], and Defense Prefix (RTA-100) [1]. ", "page_idx": 5}, {"type": "text", "text": "Baselines. To evaluate MirrorCLIP \u2019s disentanglement performance for visual elements, we benchmark against CLIP [19], Materzynska et al. [16], PAINT [11] and Defense Prefix [1]. To evaluate MirrorCLIP \u2019s disentanglement performance for text elements, we mainly compare it with the vanilla CLIP. ", "page_idx": 5}, {"type": "text", "text": "5.2 Validation Experiments for MirrorCLIP ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Validation of different mirror effects for visual and text elements. To validate our observation that CLIP exhibits horizontal filp invariance for visual features but not for textual features, we conducted experiments on the cosine similarity of image features before and after filpping across 10 clean public image classification datasets and their corresponding typographic datasets. ", "page_idx": 5}, {"type": "text", "text": "The average cosine similarity of image features before and after filpping for all samples in all datasets is shown in Table 1. According to the results, before adding text, the cosine similarity of image features before and after flipping is 0.9846 across 10 datasets, which is close to 1 and confirms CLIP\u2019s horizontal filp invariance for visual features. However, after adding text, the cosine similarity significantly decreases to 0.8225, which also validates CLIP\u2019s lack of horizontal flip invariance for textual features. ", "page_idx": 5}, {"type": "table", "img_path": "FYm8coxdiR/tmp/46e67b4ae41c7137510f77dde73f67e069b128275ff089c7ab69701201eae090.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "FYm8coxdiR/tmp/16ce345474c47ccebf8940bdd62f52edba3edd568fa1df3dbdf0df4994856e5d.jpg", "img_caption": ["Figure 6: Visualization for textual and non-textual features for typographic attacked data using class activation map. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Visualization of attribution maps for textual and visual features. We employ CAM to showcase a more intuitive visualization towards the disentangled textual and visual features in Equations 4 and 5, respectively. CAMs illuminate the salient regions within an input image that contribute the most strongly to the target concept [22]. Specifically, we employ DecomCAM [24], which is the state-of-the-art CAM method that strongly reduces the noise effect of typical CAM methods. Our interpretation target is constructed as $\\bar{X}^{t}X^{\\mathrm{T}}$ , $X^{c}X^{\\mathrm{{T}}}$ for textual and visual features, respectively. As shown in Figure 6, our disentangled features can effectively separate regions of text and visual elements. ", "page_idx": 6}, {"type": "text", "text": "Similar images generation with textual and visual features. To visualize the effectiveness of disentanglement, we utilized Stable Diffusion models, Stable UnCLIP [20], with disentangled textual and visual features for image generation [20; 16]. We use image features, disentangled visual and textual features as image embedding conditions. The generated images are depicted in Figure 7. ", "page_idx": 6}, {"type": "text", "text": "When using images with irrelevant text for image variations, the generated images mix the semantics of text and visual elements. For example, an image of an apple with the text \u201cearphones\u201d might produce an image with the logo of Apple Inc. and non-sensical text. Similarly, a cat labeled as \u201cdog\u201d could result in an image that combines a cat and a dog\u2019s face with non-sensical text. Besides, after disentangling, visual features generate accurate visual content (e.g., an apple or a cat) without non-sensical strings, indicating successful flitering of textual features. Textual features, on the other hand, generate images filled with text without visual semantics, demonstrating effective separation. ", "page_idx": 6}, {"type": "text", "text": "In addition, we conducted controlled experiments. for images without text, visual features generate image-related patterns, while textual features produce nonsensical characters. For text-only images, visual features generate non-sensical patterns, whereas textual features generate coherent text and patterns, showcasing the effectiveness of our disentanglement framework. ", "page_idx": 6}, {"type": "text", "text": "5.3 Evaluation on visual features disentanglement ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To further evaluate the effectiveness of the disentangled visual features, we perform typographic defense experiments. Here, MirrorCLIP is required to exclude the interference of the text added and correctly recognize visual elements. The performance of visual features on clean public classification datasets, synthetic typographic datasets, and real-world typographic datasets is shown in Tables 2, 3, and 4, respectively. ", "page_idx": 6}, {"type": "image", "img_path": "FYm8coxdiR/tmp/ed6ed66093f91eb38d32c28533f13c4e0c33e71b1c0b4b3d6b61e9da4a8e4b12.jpg", "img_caption": ["Figure 7: Results of image variation using Stable unCLIP. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "FYm8coxdiR/tmp/a9807ec1887b4379f04e4ff041e802fbc0cbbee62fb59144d4ce36f5910f66ec.jpg", "table_caption": ["Table 2: Results of image classification on original datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "From Tables 3 and 4, it is evident that CLIP exhibits poor robustness when faced with typographic attacks, resulting in significant performance degradation. However, using the visual features obtained from our disentanglement framework to replace the original image features significantly improves performance on both synthetic and real-world typographic attack datasets $\\left(+{\\bar{1}}5.49\\%$ on synthetic typographic attack datasets and $+21.27\\%$ on real-world typographic attack datasets). Compared to Materzynska et al. [16], PAINT [11], and Defense Prefix [1], which introduce additional parameters for training, our zero-shot method surpasses their performance without any additional training. This demonstrates the strong robustness of the visual features obtained from our simple but effective disentanglement framework across various types of images. ", "page_idx": 7}, {"type": "text", "text": "Additionally, we test the performance of disentangled visual features on clean datasets. According to Table 2, our zero-shot disentanglement framework slightly improves performance compared to the original CLIP model in clean images without text. This indicates that our method does not cause performance degradation when handling images without text elements. ", "page_idx": 7}, {"type": "text", "text": "5.4 Evaluation on textual features disentanglement ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To evaluate the performance of the textual features obtained from our framework, we employ these features to recognize the text elements in the typographic datasets. Our results are shown in Table 5. Based on the results, CLIP struggles to achieve high performance in text recognition within images due to the confusion between text and visual elements. However, after applying our disentanglement framework, substituting image features with textual features significantly improves CLIP\u2019s performance in text recognition (from $\\bar{3}9.32\\%$ to $73.95\\%$ ). This indicates that the disentangled textual features can precisely represent the text elements in the images, confirming the effectiveness of the proposed framework. ", "page_idx": 7}, {"type": "table", "img_path": "FYm8coxdiR/tmp/60f1d3632940489a7ed78459e8c12bdeb56cc0070bb443d6a707141892a58458.jpg", "table_caption": ["Table 3: Results of image classification on synthetic typographic attack datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "FYm8coxdiR/tmp/db89070d68267d350bae8917a32981c8fdc267cd3e4a48649813fc97a0dff949.jpg", "table_caption": ["Table 4: Results of image classification on real-world typographic attack datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "FYm8coxdiR/tmp/69c7702b85763a63e5d7c66a05e1d11e980b19376b0ce5b954662638b7787af3.jpg", "table_caption": ["Table 5: Results of text recognition on typographic attack datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "FYm8coxdiR/tmp/0b6eab2ee4dda3a142f261b11502b4a89734b9560da65673a7f1b04966c10a52.jpg", "table_caption": ["Table 6: Results of different features on various tasks. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.5 Analysis and Ablation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Structure of image encoder. To investigate whether the horizontal filp invariance is a characteristic of ViT [5], we replaced CLIP\u2019s image encoder with ResNet [9] $({\\bf R}{\\bf N}50{\\times}4)$ and conducted the same experiment; the experimental results confirm that our observations and disentanglement framework are also applicable to CLIP models using CNN-based image encoders. This indicates that the characteristic of horizontal flip invariance is mainly related to the contrastive learning strategy employed by CLIP. ", "page_idx": 8}, {"type": "text", "text": "Filter of textual and visual features. In order to figure out the appropriate representation of textual and visual features, we conducted experiments on various tasks using different feature representations obtained after disentanglement. The results of the experiments are shown in Table 6, where \u201coriginal\u201d represents the average accuracy on 10 original image classification datasets, \u201ctypographic\u201d represents the average accuracy on 13 real-world and synthetic typographic datasets, \u201creal-world\u201d represents the average accuracy on 3 real-world typographic datasets. Textual features (hard) and visual features (hard) are obtained through $X\\odot M^{t}$ and $X\\odot M^{v}$ , respectively, where $X$ denotes image features, $M^{t}$ and $M^{v}$ represent the textual mask and the visual mask. ", "page_idx": 8}, {"type": "text", "text": "According to Table 6, as analyzed in Section 4, directly zeroing out the textual or visual parts based on the disentangling mask would lead to information loss and consequently performance degradation. So we proposed the textual fliter in Equation 4 to obtain textual features, aiming to reduce information loss. Although directly using filpped image features as visual features achieve the highest performance in defending against typographic attacks, it lacks robustness for filpped images (i.e., if all samples are filpped before being inputted, the accuracy would decrease to $37.56\\%$ , detailed results are shown in ", "page_idx": 8}, {"type": "image", "img_path": "FYm8coxdiR/tmp/68f1a66a5c92a837d79f664686f5c8450b793d41d04300f4d501244d31def8df.jpg", "img_caption": ["Figure 8: Potential Application Examples of MirrorCLIP. (a) Using MirrorCLIP to disentangle region features of RegionCLIP, before disentanglement, RegionCLIP mistakenly identified a price tag with text \u201cpapaya\" as papaya and a laptop monitor as a television set because of the interference of text \u201ctelevision\". (b) Textual features disentangled by MirrorCLIP are used to provide prompts for SAM, achieving text region segmentation. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table L). Therefore, we proposed the visual filter in Equation 5 to obtain visual features, aiming to ensure robustness against flipped images. ", "page_idx": 9}, {"type": "text", "text": "Moreover, as seen in Table 6, while textual features notably boost text recognition, they yield negligible accuracy in image recognition tasks. Conversely, visual features significantly enhance image recognition but have minimal impact on text recognition, which validates the effectiveness of our proposed filter in isolating visual and textual features. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations. Although our proposed framework achieves excellent disentanglement results with a simple approach, due to the deep entanglement between visual and textual features, our method cannot fully separate them. It does not affect performance in recognition tasks but may influence the results of image generation, as seen in Figure 7 with the examples of apple in the second row and textual features results in the first row. What\u2019s more, when facing extreme scenarios such as palindromes in the images, MirrorCLIP still work for normal palindromes, where the shape of the words changes before and after flipping (e.g., \u201cdid\u201d to \u201cbib\u201d). However, for special palindromes, where the shape of the words remains basically unchanged (e.g., \u201cmom\u201d to \u201cmom\u201d), MirrorCLIP struggles to achieve disentanglement, although special palindromes are quite rare compared to other word, detailed experimental results are shown in Appendix F. ", "page_idx": 9}, {"type": "text", "text": "Potential application. We have initially explored object detection and text segmentation by combining MirrorCLIP with RegionCLIP [27] and SAM [12]. The results show the potential of MirrorCLIP for different downstream tasks or applications. Relevant examples are shown in Figure 8. By using MirrorCLIP to get the disentangled visual region features of RegionCLIP, we can reduce the influence of textual factors and get more accurate detection results. By using the textual features obtained from MirrorCLIP to generate prompts for SAM, we can achieve text localization within images and perform preliminary text segmentation. ", "page_idx": 9}, {"type": "text", "text": "Conclusion. In this paper, we first discovered and verified that CLIP exhibits horizontal filp invariance for visual features while lacking this property for textual features. Leveraging this observation, we proposed a simple yet effective zero-shot dual-stream disentanglement framework MirrorCLIP by contrasting image features before and after flipping. We demonstrated the effectiveness of this framework through the visualization of attention maps with CAMs and similar image generation with stable diffusion models. Additionally, we conducted experiments on 13 synthetic and realworld typographic attack datasets to further validate the excellent disentanglement performance and robustness of our method across different samples. Furthermore, we surpass state-of-the-art methods in defense against typographic attacks without any additional training. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work is supported by the National Key Research and Development Program of China (No. 2023YFC3300029), Zhejiang Provincial Natural Science Foundation of China (No. LD24F020007), Beijing Natural Science Foundation (No. L223024 and L244043), National Natural Science Foundation of China (No. 62076016, 12201024 and 62406298), \u201cOne Thousand Plan\u201d projects in Jiangxi Province (JXSQ2023102268), Taiyuan City \u201cDouble hundred Research action\u201d (2024TYJB0127). Thanks for National Key Laboratory on Automatic Target Recognition 220402. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hiroki Azuma and Yusuke Matsui. Defense-prefix for preventing typographic attacks on clip. In IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2023.   \n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 - mining discriminative components with random forests. In European Conference on Computer Vision (ECCV), 2014.   \n[3] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2013.   \n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2009.   \n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021.   \n[6] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (CVPRW), 2004.   \n[7] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Christopher Olah. Multimodal neurons in artificial neural networks. Distill, 2021.   \n[8] Jie Guo, Qimeng Wang, Yan Gao, Xiaolong Jiang, Xu Tang, Yao Hu, and Baochang Zhang. Mvp-seg: Multi-view prompt learning for open-vocabulary semantic segmentation. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV), 2023.   \n[9] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2015.   \n[10] Patrick Helber, Benjamin Bischke, Andreas R. Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2017.   \n[11] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[12] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross B. Girshick. Segment anything. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 3992\u20134003, 2023.   \n[13] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In IEEE International Conference on Computer Vision Workshops (ICCVW), 2013.   \n[14] Yoann Lemesle, Masataka Sawayama, Guillermo Valle P\u00e9rez, Maxime Adolphe, H\u00e9l\u00e8ne Sauz\u00e9on, and Pierre-Yves Oudeyer. Language-biased image classification: evaluation based on semantic representations. In International Conference on Learning Representations (ICLR), 2022.   \n[15] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B. Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. ArXiv, 2013.   \n[16] Joanna Materzynska, Antonio Torralba, and David Bau. Disentangling visual and written concepts in clip. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[17] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics & Image Processing, 2008.   \n[18] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2012.   \n[19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021.   \n[20] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv, 2022.   \n[21] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \n[22] Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. International Journal of Computer Vision (IJCV), 2016.   \n[23] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision (IJCV), 2014.   \n[24] Yuguang Yang, Runtang Guo, Sheng Wu, Yimi Wang, Linlin Yang, Bo Fan, Jilong Zhong, Juan Zhang, and Baochang Zhang. Decomcam: Advancing beyond saliency maps through decomposition and integration. Neurocomputing, 2024.   \n[25] Yuguang Yang, Yiming Wang, Shupeng Geng, Runqi Wang, Yiming Wang, Shen-Te Wu, and Baochang Zhang. Self-enhancement improves text-image retrieval in foundation visual-language models. ArXiv, 2023.   \n[26] Renrui Zhang, Zhang Wei, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Jiao Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In European Conference on Computer Vision (ECCV), 2022.   \n[27] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel C. F. Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based languageimage pretraining. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16772\u201316782, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Implementation details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Benchmark model. During the experiments, we used the ViT-B/32 version of CLIP as a pre-trained model and all parameters of CLIP were frozen. We informed the CLIP model of our recognition intent by adjusting the text prompt. For text recognition, we used the template \"text of $\\{\\}^{\\prime\\prime}$ across all datasets. For image recognition, we used the template \"a photo of $\\{\\}^{\\dprime}$ across all real-world typographic attack datasets, which is shown in Figure 5, and the templates we use across synthetic typographic attack datasets are shown in Table A. All experiments were conducted on NVIDIA A800. ", "page_idx": 12}, {"type": "table", "img_path": "FYm8coxdiR/tmp/8168f1483452895d368d1b8f769f490a4242fd75648b44ae6198d30a16b4d505.jpg", "table_caption": ["Table A: Templates of synthetic typographic attack datasets for image recognition "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Generating similar images for validation. The model we employed for image generation in Section 5.2 is Stable unCLIP [20], a new stable diffusion model fine-tuned at $768\\times768$ resolution, based on SD2.1-768 [21]. This model allows for image variations, conditioned on CLIP image features. During the image variation, we use \u2018\u2019 as prompt condition. ", "page_idx": 12}, {"type": "text", "text": "B Typographic attack datasets ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We will explain the details of the test data in Section 5. For synthetic typographic attack datasets, we add text to images from ten classification datasets: ImageNet [4], Caltech101 [6], OxfordPets [18], StanfordCars [13], Flowers102 [17], Food101 [2], FGVCAircraft [15], DTD [3], SUN397 [23], EuroSAT [10]. To generate typographic attack datasets, we followed PAINT [11] and Defense Prefix [1], as shown in Figure A (a). We resize the short dimension to 224 pixels using bicubic interpolation and center-crop the images by $224\\!\\times\\!224$ . We randomly select the font from three options: Roman, Courier, and Times. The font size is chosen randomly between 20 and 40 points. Additionally, we use one of eight colors: red, green, blue, cyan, magenta, yellow, white, or black. The text includes a 1-point shadow in a different color from the main font color. Text is placed randomly in the image, ensuring that entire words remain visible. The text content is chosen from the class labels of the dataset, excluding the correct labels for the images. The samples of synthetic typographic attack datasets are shown in Figure B. ", "page_idx": 12}, {"type": "text", "text": "For real-world typographic attack datasets, we use datasets made by Materzynska et al. [16], PAINT [11] and Defense Prefix [1]. The samples of real-world typographic attack datasets are shown in Figure C, in which objects are labeled with tags of incorrect classes. ", "page_idx": 12}, {"type": "text", "text": "C Visualization of attention map ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "More visualization results using CAMs to show attention maps in Section 5.2 are presented in Figure D. ", "page_idx": 12}, {"type": "text", "text": "D Generation of similar images ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "More similar images generated in Section 5.2 are shown in Figure E. ", "page_idx": 12}, {"type": "image", "img_path": "FYm8coxdiR/tmp/e30f7f14e2572efa1e41c2b2ee59ba820d9cbee5a13ff9127a3eab8d9c4c6e02.jpg", "img_caption": ["Figure A: Typographic datasets. (a) generation of synthetic typographic datasets. (b) a sample of real typographic datasets. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "FYm8coxdiR/tmp/dcaf2134c1807764637835bf744133a554d8ed37e0515a4a626b0642fa4ec230.jpg", "img_caption": ["Figure B: Samples of synthetic typographic attack datasets. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "E Detailed results of ablation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Results of CNN-based image encoder. The detailed results of the experiments for the CNN-based image encoder in Section 5.5 are as follows. The cosine similarity of the image features before and after the flip of the Clean and Typographic datasets is shown in Table B. The results of image recognition across 10 original datasets are shown in Table C. Results of image recognition across 10 synthetic typographic attack datasets and 3 real-world typographic attack datasets are shown in Table D and Table E, respectively. Results of text recognition across 3 synthetic typographic attack datasets and 3 real-world typographic attack datasets are shown in Table F. ", "page_idx": 13}, {"type": "text", "text": "Results of different feature representations. The detailed results of experiments we conduct for different feature representations in Section 5.5 are as follows. Results of image recognition across 10 original datasets are shown in Table G. Results of image recognition across 10 synthetic ", "page_idx": 13}, {"type": "image", "img_path": "FYm8coxdiR/tmp/114cb13742e520e497c5216c006f47c88dffb05a001ea9a138cfdcdfe38bbf00.jpg", "img_caption": ["Figure C: Samples of real-world typographic attack datasets. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "FYm8coxdiR/tmp/20d3a969c63f0c51b73566f6807ddc1f9195996f35d784ce63bb1c8791d13edd.jpg", "img_caption": ["Figure D: More results of activation map visualization. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "FYm8coxdiR/tmp/bb9bfcd157a4144565d78fd9398e8343daa2451105e2cffc490c7cc2cd1475b6.jpg", "img_caption": ["Figure E: More results of image variation using Stable unCLIP. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table B: Image features\u2019 cosine similarity before and after filpping on Clean and Typographic datasets with $\\mathrm{RN}50\\!\\times\\!4$ as image encoder. ", "page_idx": 14}, {"type": "table", "img_path": "FYm8coxdiR/tmp/2cf13e2c5e2ab1901df0e584c7659e1093976b9338de8b319b030cbba8d04b50.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table C: Results of image classification on original datasets with $\\mathrm{RN}50\\!\\times\\!4$ as image encoder. ", "page_idx": 15}, {"type": "table", "img_path": "FYm8coxdiR/tmp/c7acf8d6988de60c3b29f70ead9b193d1785896cae091a163bb65211ad899152.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table D: Results of image classification on synthetic typographic attack datasets with $\\mathsf{R N}50\\!\\times\\!4$ as image encoder. ", "page_idx": 15}, {"type": "table", "img_path": "FYm8coxdiR/tmp/f057f0e601ad4464b00e70d360c8daa91dfaa21d5d13b1f28b1f322d1089b0aa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table E: Results of image classification on real-world typographic attack datasets with $\\mathrm{RN}50\\!\\times\\!4$ as image encoder. ", "page_idx": 15}, {"type": "table", "img_path": "FYm8coxdiR/tmp/b6122af87c13340d0ecdd487117d7aed5d9fe9bd1ff8804f0b6618d338a0810a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table F: Results of text recognition on typographic attack datasets with $\\mathsf{R N50}\\!\\times\\!4$ as image encoder. ", "page_idx": 15}, {"type": "table", "img_path": "FYm8coxdiR/tmp/8caaf8fe987d3f22576f4ed46e09289c6125c9300ab2d5967c326f92b60afab5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "FYm8coxdiR/tmp/a1f363fd3303363e7dfb7e621970ca53b2f683f64ec6fa4190b8ecb0653d7f7a.jpg", "table_caption": ["Table G: Results of image classification on original datasets with different feature representations. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table H: Results of image classification on synthetic typographic attack datasets with different feature representations. ", "page_idx": 15}, {"type": "table", "img_path": "FYm8coxdiR/tmp/393797cb087cf6e38f71807223bc72c1eaeb4bcc4987eaed06c2dbfbfaf260aa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table I: Results of image classification on real-world typographic attack datasets with different feature representations. ", "page_idx": 16}, {"type": "table", "img_path": "FYm8coxdiR/tmp/bd8529eb10d0aecb745deeb7194e7abbeac90745de41a2594aaa35d3bf56028b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "FYm8coxdiR/tmp/83e4fbf49d9d1cd454551c9959496073a29ef207dfef18b54c9b7a644a4902b6.jpg", "table_caption": ["Table J: Results of text recognition on real-world typographic attack datasets with different feature representations. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "FYm8coxdiR/tmp/444b2428a851ed5a3eddce7cbdfc8f5b3140dd1b7ec0dbeeb42425e42954b838.jpg", "table_caption": ["Table K: Results of image recognition on ordinary and special palindromes datasets. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "FYm8coxdiR/tmp/1563207aa4afe46f06e4c4b3ffc36fdb29c7c4903911c35177896e0ff473c583.jpg", "table_caption": ["Table L: Results of different features on image recognition with flipped text. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "typographic attack datasets and 3 real-world typographic attack datasets are shown in Table H and Table I, respectively. Results of text recognition across 3 real-world typographic attack datasets are shown in Table J. ", "page_idx": 16}, {"type": "text", "text": "F Experiments for palindromes ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For the case of palindromes, we categorized them into two types: ordinary palindromes, where the shape of the words changes before and after flipping (e.g., \"did\" to \"bib\"), and special palindromes, where the shape of the words remains basically unchanged (e.g., \"mom\" to \"mom\"). We constructed corresponding datasets: the ordinary palindrome dataset includes 26 words (\"dad\", \"did\", \"eve\", \"eye\", \"ewe\", \"gig\", \"madam\", \"pip\", \"pop\", \"pup\", \"radar\", \"redder\", \"deified\", \"rotator\", \"repaper\", \"reviver\", \"sees\", \"tat\", \"tenet\", \"tot\", \"refer\", \"deed\", \"peep\", \"civic\", \"racecar\", \"level\"), while the special palindrome dataset includes 5 words (\"wow\", \"noon\", \"mom\", \"nun\", \"minim\"). The results are shown in Table K. For ordinary palindromes, MirrorCLIP achieves disentanglement with 13.85 improvements compared to the baseline. This is a comparable improvement like other words. However, for special palindromes, MirrorCLIP struggles to achieve disentanglement and only improves the accuracy by 5.29.As special palindromes are quite rare compared to other words, according to the results in Table K, their impact is limited. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 17}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 17}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 17}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 17}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 17}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Sec. 1 ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Sec. 6 ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Sec. 3, Sec. 4 and Sec. 5 ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Appendix A and Appendix B ", "page_idx": 18}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Appendix A and Appendix B ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 19}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Appendix A and Appendix B Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Appendix A ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our proposed framework enhances the defense against typographic attacks 6. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification:   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our source code is submitted via a zip file. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]