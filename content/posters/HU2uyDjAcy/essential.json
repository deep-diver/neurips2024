{"importance": "This paper is important because it presents **LocalOMD**, a novel algorithm for learning optimal strategies in extensive-form games. It addresses the high variance issue in existing methods by employing a **fixed sampling approach and adaptive online mirror descent**. This offers **near-optimal sample complexity** and opens up new avenues for research in game theory and reinforcement learning.", "summary": "LocalOMD: Adaptive OMD in extensive-form games achieves near-optimal sample complexity by using fixed sampling and local updates, reducing variance and generalizing well.", "takeaways": ["LocalOMD algorithm provides a new approach to learning in extensive-form games.", "LocalOMD achieves a near-optimal sample complexity, improving efficiency.", "The fixed sampling approach significantly reduces variance in loss estimation."], "tldr": "Many existing methods for learning in extensive-form games suffer from high variance due to importance sampling. This paper addresses this issue by proposing a fixed sampling approach where the sampling policy is fixed and not updated over time. This reduces variance in gain estimates, but traditional methods for regret minimization still suffer from large variance, even with a fixed sampling policy. \nThe proposed algorithm, LocalOMD, uses an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set with individually decreasing learning rates.  It leverages a regularized loss function to ensure stability. Importantly, LocalOMD avoids the use of importance sampling, addressing the high-variance problem and offering a convergence rate of \u00d5(T-1/2) with high probability. The algorithm shows near-optimal dependence on the game parameters when using optimal learning rates and sampling policies.", "affiliation": "CREST - FairPlay, ENSAE Paris", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "HU2uyDjAcy/podcast.wav"}