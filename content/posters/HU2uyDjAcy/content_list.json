[{"type": "text", "text": "Local and Adaptive Mirror Descents in Extensive-Form Games ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Come Fiegel Pierre Menard Tadashi Kozuno Remi Munos CREST - FairPlay, ENSAE Paris ENS Lyon OMRON SINIC X Google DeepMind Palaiseau, France Lyon, France Tokyo, Japan Paris, France come.fiege@normalesup.org ", "page_idx": 0}, {"type": "text", "text": "VianneyPerchet Michal Valko CREST - FairPlay, ENSAE Paris, Criteo AI Lab INRIA Paris, France ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study how to learn $\\varepsilon$ -optimal strategies in zero-sum imperfect information games (IIG) with trajectory feedback. In this setting, players update their policies sequentially, based on their observations over a fixed number of episodes denoted by $T$ . As noted by Steinberger et al. (2020) and McAleer et al. (2022), most existing procedures suffer from high variance due to the use of importance sampling over sequences of actions. To reduce this variance, we consider a fixed sampling approach, where players still update their policies over time, but with observations obtained through a given fixed sampling policy. Our approach is based on an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set, using individually decreasing learning rates and a regularized loss. We show that this approach guarantees a convergence rate of $\\tilde{\\mathcal{O}}(T^{-\\bar{1}/2})$ with high probability and has a near-optimal dependence on the game parameters when applied with the best theoretical choices of learning rates and sampling policies. To achieve these results, we generalize the notion of OMD stabilization, allowing for time-varying regularization with convex increments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The extensive-form representation of a game (Osborne & Rubinstein, 1994) can be depicted as a tree whose nodes correspond to the game states. At each state, the players choose some available actions and, based on these choices, the game transitions to the next state among the current state's children. ", "page_idx": 0}, {"type": "text", "text": "In imperfect information games (HIGs), players may only have access to partial information about the current game state upon taking action. Therefore, the state space is partitioned for each player into multiple information sets, which consist of indistinguishable states from the player's perspective. With perfect recall (Kuhn, 1950), when players remember their previous moves, each space of information sets also has a tree structure. ", "page_idx": 0}, {"type": "text", "text": "We focus more specifically on zero-sum IIGs represented in an extensive form under the perfect recall assumption, where the gains of one player, conventionally called the max-player, are equal to the losses of his opponent, the min-player. The primary goal is to design an algorithm learning $\\varepsilon$ optimal strategies (von Neumann, 1928). To achieve this, one can use the self-play framework, where an agent controls both players for $T$ episodes. At the beginning of each episode, the agent prescribes a strategy for each player. The agent then observes the play and updates the players\u2019 strategies for the next episode based on the outcome of the game. After $T$ episodes, this protocol returns a guess of strategies with a small exploitability gap (Ponsen et al., 2011). In this learning framework, the agent has very limited feedback, only observing the rewards along each sampled trajectory, as opposed to richer feedback that would for example include all possible rewards and all transition probabilities, (Zinkevich et al., 2007; Hoda et al., 2010; Tammelin, 2014; Kroer et al., 2015; Burch et al., 2019) unrealistic in large games. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To deal with this learning framework, a well-studied approach is to unilaterally minimize the regret of each player during the interactions with the game, i.e. the difference between the cumulative gain the player would have obtained had he played the best fixed a posteriori policy and the cumulative gain obtained by following the sequence of policies. The key observation is that by minimizing the regret of both players, the average policies over the sequence of policies generated during the process converge toward optimal strategies at the rate of order ${\\mathcal{O}}(1/{\\sqrt{T}})$ (Cesa-Bianchi & Lugosi, 2006; Kozuno et al., 2021). Regret minimizers such as CFR-based algorithm or online mirror descent (OMD) (Hoda et al., 2010; Kroer et al., 2015) can be used, leading to optimal rates (with respect to the game size) with the latter option (Bai et al., 2022; Fiegel et al., 2023). ", "page_idx": 1}, {"type": "text", "text": "Since the agent only observes trajectories of the game, an importance sampling estimate (Auer et al., 2003) of gain (or loss) is fed to the regret minimizer. However, the estimate of this loss usually suffers from high variance due to two reasons. First, the same sequence of policies is used to minimize the regret and to collect the trajectories, making the players strive to fulfill two competing goals: play a policy with small regret and play a policy leading to a small variance gain estimate. Second, importance sampling is applied to sequences of actions, that have in large games a very small probability of being played, leading to empirically large importance sampling weights and ultimately inflating the variance of the gain estimates. ", "page_idx": 1}, {"type": "text", "text": "To mitigate this issue, regularization and biasing the estimates can help (Kozuno et al., 2021; Bai et al., 2020). However, the high variance of the gain estimates remains problematic with large games, for which the algorithms are generally coupled with function approximation (Steinberger et al., 2020; McAleer et al., 2022). For instance, neural networks are particularly susceptible to noise (Zhang et al., 2021). A natural question is thus whether it is possible to learn optimal strategies without relying on importance-sampling over the sequence of actions. ", "page_idx": 1}, {"type": "text", "text": "To this aim, we consider a particular case of the self-play framework: the fixed policy sampling framework (Lanctot et al., 2009). In this setting, a fixed policy is used to collect the trajectories of the game. Precisely, at each round, one player, let's say the min-player, follows the fixed sampling policy to play against the current policy of the max-player. The collected trajectory is then used to update the current policy of the min-player. In the next episode, the max-player will follow a sampling policy against the current policy of the min-player, and so on. The outcome sampling MCCFR algorithm adopts this framework to update the two players\u2032 policy by regret minimization, feeding the CFR algorithm with gain estimated via importance sampling (Lanctot et al., 2009; Bai et al., 2020; Farina et al.,2021b). ", "page_idx": 1}, {"type": "text", "text": "Recently, McAleer et al. (2022) proposed the ESCHER algorithm that removes the need for importance sampling in this framework. In particular, as the CFR algorithm is invariant by re-scaling of the gains and the weights of the sampling policy are fixed, ESCHER can directly operate with the unweighted history cumulative gain (Bai et al., 2020). Unfortunately, it still requires access to an oracle that provides this history of cumulative gains at an arbitrary information set. ", "page_idx": 1}, {"type": "text", "text": "Nonetheless, the insight of McAleer et al. (2022) cannot be used directly for OMD-based algorithms as they are not scale-invariant. Furthermore, the OMD-based algorithms generally work at the global game level whereas CFR-based algorithms work at the local level of the information set (Bai et al., 2020), making local adaptation to the problem easier. ", "page_idx": 1}, {"type": "text", "text": "Contributions  We make the following main contributions: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u00b7 We propose the LocalOMD algorithm, in the fixed policy sampling framework, that allows adaptive learning rates and does not require importance-sampling over the sequence of actions but only for the current action. We explain how it can simply be seen as a regret minimization procedure applied to a local loss on each information set, similarly to Farina et al. (2019b). ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We prove that LocalOMD achieves, in this fixed sampling framework, a $\\widetilde{\\mathcal{O}}\\left(1/\\varepsilon^{2}\\right)^{1}$ sample complexity with any choice of non-degenerate sampling policy, ignoring the game and policy-dependent parameters.   \n\u00b7 With an appropriate sampling policy and choice of learning rates, we prove that LocalOMD, recover the $\\widetilde{\\mathcal{O}}\\left(H^{3}(A_{\\mathcal{X}}+B_{\\mathcal{Y}})/\\varepsilon^{2}\\right)$ near-optimal sample complexity for learning $\\varepsilon$ -optimal strategies in a tabular setting, where $H$ is the height of the tree, $A_{\\mathcal{X}}$ the total number of available actions for the min-player and $B{y}$ the same quantity for the max-player. This sample complexity was also achieved in the fixed policy framework by BalancedCFR (Bai et al., 2022), but with a less generalizable procedure that updates the policy at one depth at a time.   \n\u00b7 We generalize the dual-stabilization technique introduced by Fang et al. (2020) to analyze OMD with a time-varying regularization as long as the increments of the regularization are convex.   \n\u00b7 Our tabular experiments reveal that our algorithm yields comparable results to existing baselines while demonstrating a reduced variance in loss estimation. ", "page_idx": 2}, {"type": "text", "text": "2   Settings and fixed sampling procedure ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1  Extensive-form games and regret ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Game definition We consider a finite zero-sum HIG game $({\\cal S},{\\boldsymbol{\\mathcal{X}}},{\\boldsymbol{\\mathcal{Y}}},{\\mathcal{A}},{\\boldsymbol{B}},{\\boldsymbol{p}},{\\boldsymbol{\\ell}})$ with perfect recall. Given two behavioral policies $\\mu=(\\mu(\\cdot|x))_{x\\in\\mathcal{X}}$ and $\\boldsymbol{\\nu}=(\\nu(\\cdot|\\boldsymbol{y}))_{\\boldsymbol{y}\\in\\mathcal{y}}$ , one episode of such game proceeds as follows: An initial game state $s_{1}\\sim p(\\cdot|s_{0})$ is first sampled in the set of states $\\boldsymbol{S}$ according to the transition function $p$ , starting from the root $s_{0}$ of the tree. At depth $h$ , the min- and maxplayers respectively observe the information set $x_{h}$ and $y_{h}$ associated with the current state $s_{h}$ in the spaces of information sets $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ (these spaces being two partitions of $\\boldsymbol{S}$ ), then simultaneously choose and execute actions $a_{h}\\sim\\mu(\\cdot|x_{h})$ and $b_{h}\\sim\\nu(\\cdot|y_{h})$ in the sets of legal actions ${\\mathcal{A}}(x_{h})$ and $B(y_{h})$ . As a result, the state transitions to a new state $s_{h+1}\\sim p(\\cdot|s_{h},a_{h},b_{h})$ in $\\boldsymbol{S}$ , with the min- and max- players getting respectively the losses $\\ell_{h}\\sim\\ell(\\cdot|s_{h},a_{h},b_{h})$ in $[0,1]$ and $1-\\ell_{h}$ according to the loss distribution l. This is repeated until a final state $s_{H}$ of a fixed depth $H$ is reached, after which the episode finishes. ", "page_idx": 2}, {"type": "text", "text": "Policies and actions We will denote by $\\Pi_{\\mathrm{min}}$ and $\\Pi_{\\mathrm{max}}$ the set of behavioral policies of the minand max- players. Because of the perfect recall assumption, such policies, with an independent stochastic choice of action for each information set, are enough to describe the entire set of strategies (Laraki et al., 2019). We will also denote by $A_{\\mathcal{X}}$ and $B{y}$ the total number of actions for respectively the min- and max- players, i.e. $\\begin{array}{r}{A_{\\mathcal{X}}:=\\sum_{x\\in\\mathcal{X}}|\\mathcal{A}(x)|}\\end{array}$ and $\\begin{array}{r}{B_{\\mathcal{V}}=\\sum_{y\\in\\mathcal{Y}}\\left|\\mathcal{B}(y)\\right|}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Regret and $\\varepsilon$ -optimal strategies  We are interested in learning $\\varepsilon$ -optimal policies through selfplay over multiple episodes. A useful notion for this objective is the regret as explained in the introductionWe frst dene te vale $V^{\\mu,\\nu}=\\mathbb{E}^{\\mu,\\nu}[\\sum_{h=1}^{H}\\dot{\\ell}_{h}]$ as the expected sum of loss forthe min-player) with respect to a pair of policies $(\\mu,\\nu)\\in\\Pi_{\\operatorname*{min}}\\times\\Pi_{\\operatorname*{max}}$ Given a sequence $(\\mu^{t},\\nu^{t})_{t\\in[T]}$ in $\\Pi_{\\mathrm{min}}\\times\\Pi_{\\mathrm{max}}$ , the regrets of the min- and max- players are then defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathfrak{R}_{\\operatorname*{min}}^{T}:=\\operatorname*{max}_{\\mu^{\\dagger}\\in\\Pi_{\\operatorname*{min}}}\\sum_{t=1}^{T}(V^{\\mu^{t},\\nu^{t}}-V^{\\mu^{\\dagger},\\nu^{t}})\\quad\\mathrm{and}\\quad\\mathfrak{R}_{\\operatorname*{max}}^{T}:=\\operatorname*{max}_{\\nu^{\\dagger}\\in\\Pi_{\\operatorname*{max}}}\\sum_{t=1}^{T}(V^{\\mu^{t},\\nu^{t}}-V^{\\mu^{t},\\nu^{t}})\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Minimizing the regret of both players leads to the computation of an $\\varepsilon$ -optimal profile (equivalent to an $\\varepsilon$ -Nash equilibrium for two players zero-sum games) through the computation of an average of the policies. The following theorem quantifies this statement under the perfect recall assumption. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1. (Cesa-Bianchi & Lugosi, 2006; Kozuno et al, 2021) From a sequence $(\\mu^{t},\\nu^{t})_{t\\in[T]}\\ i$ .n $\\Pi_{\\mathrm{min}}\\times\\Pi_{\\mathrm{max}}$ a certaintime-averaged profile $(\\overline{{\\mu}},\\overline{{\\nu}})$ is $\\varepsilon$ optimalwith $\\varepsilon=\\left(\\Re_{\\operatorname*{min}}^{T}+\\Re_{\\operatorname*{max}}^{T}\\right)/\\dot{T}$ It especially shows that both averaged strategies converge to the set of optimal strategies as long as the regret of both players is sub-linear. We now focus on the min-player point of view because of the symmetry of the game. Indeed, the following ideas will apply exactly the same way to the max-player, using the losses $1-\\ell_{h}$ instead. ", "page_idx": 2}, {"type": "text", "text": "1: Input: Fixed sampling policies $\\mu^{s}$ and $\\nu^{s}$ . Initial policies $\\mu^{1}$ and $\\nu^{1}$ and update procedure for each player   \n2: For $t=1$ to $T$ The min-player observes the full outcome of an episode with the policies $(\\mu^{s},\\nu^{t})$ The max-player observes the full outcome of an episode with the policies $(\\mu^{t},\\nu^{s})$ The min- and max-player respectively update $\\mu^{t+1}$ and $\\nu^{t+1}$ based on their past observations   \n3: Output: The time-averaged policies $\\overline{{\\mu}},\\overline{{\\nu}}$ of Theorem 2.1 ", "page_idx": 3}, {"type": "text", "text": "Perfect recall and realization plan _ Under the perfect recall assumption, players do not forget their past observations and actions. We can then assume, for any information set $x\\in\\mathscr{X}$ and action $a\\in A(x)$ , the existence of a unique depth $h\\in[H]$ and history $(x_{1},a_{1},...,x_{h},a_{h})$ such that $x_{h}=x$ and $a_{h}=a$ . Using this unique history, we define the realization plan $\\mu_{1:}\\in\\mathbb{R}^{A_{\\mathcal{X}}}$ (von Stengel, 1996) associated to a policy $\\mu\\in\\Pi_{\\operatorname*{min}}$ with, for any $x\\in\\mathscr{X}$ and $a\\in A(x)$ by $\\mu_{1:}(x,a):=\\Pi_{i=1}^{h}\\,\\mu(a_{i}|x_{i})$ It denotes the combined probability of choosing actions that lead to $(x,a)$ . We will especially define $Q_{\\operatorname*{max}}:=\\{\\mu_{1:},\\mu\\in\\Pi_{\\operatorname*{min}}\\}$ the treeplex, i.e. the set of all possible realization plans. ", "page_idx": 3}, {"type": "text", "text": "Loss and regret linearization  For $\\nu$ a max-player policy, the unique history also allows for the definition of adversarial transitions $p_{1:}^{\\nu}\\in\\mathbb{R}^{\\chi}$ and adversarial losses $\\bar{\\ell}^{\\bar{\\nu}}\\in\\mathbb{R}^{A_{\\lambda}}$ with: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{1:}^{\\nu}(x):=p(x_{1}|s_{0})\\prod_{i=2}^{h}p^{\\nu}(x_{i}|x_{i-1},a_{i-1})\\quad{\\mathrm{and}}\\quad\\ell^{\\nu}(x,a):=p_{1:}^{\\nu}(x)\\ell_{h}^{\\nu}(x,a)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p(x_{1}|s_{0})$ is the probability that $x_{1}$ is initially observed by the min-player, and, assuming that the max-player policy is set to $\\nu$ \uff0c $p^{\\nu}(\\cdot|(x_{i-1},a_{i-1}))$ denotes the probability of transitioning to $x_{i}$ when $(x_{i-1},a_{i-1})$ is reached, and $\\ell_{h}^{\\nu}$ the average loss $\\ell_{h}$ associated to $a$ when $x$ is reached. Similarly to the realization plan, the adversarial transitions denote the combined probability of both Nature and max-player actions that lead to $x$ , assuming that the min-player plays the actions $(a_{1},...,a_{h-1})$ \uff1a ", "page_idx": 3}, {"type": "text", "text": "Using a chain-rule argument, we get the relation. $V^{\\mu,\\nu}~=~\\langle\\ell^{\\nu},\\mu_{1:}\\rangle$ given a pair of policies $(\\mu,\\nu)\\in\\Pi_{\\operatorname*{min}}\\times\\Pi_{\\operatorname*{max}}$ ,where $\\langle\\cdot,\\cdot\\rangle$ is the standard inner product of $\\mathbb{R}^{A_{\\lambda}}$ , defined by $\\langle z_{1},z_{2}\\rangle:=$ $\\begin{array}{r}{\\sum_{x\\in\\mathcal{X}}\\sum_{a\\in\\mathcal{A}(x)}z_{1}(x,a)z_{2}(x,a)}\\end{array}$ . The regret can then be rewritten ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Re_{\\mathrm{min}}^{T}=\\operatorname*{max}_{\\mu^{\\dagger}\\in\\Pi_{\\mathrm{min}}}\\sum_{t=1}^{T}\\left\\langle\\ell^{t},\\mu_{1:}^{t}-\\mu_{1:}^{\\dagger}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\ell^{t}:=\\ell^{\\nu^{t}}$ , which effectively reduces the problem to a linear regret problem over the convex polytope $Q_{\\mathrm{min}}$ of realization plans. ", "page_idx": 3}, {"type": "text", "text": "Several techniques exist to sequentially choose policies $(\\mu^{t})_{t\\in[T]}$ minimizing $\\Re_{\\operatorname*{min}}^{T}$ , assuming that the losses $\\ell^{t}$ are observed after each round $t$ (Hoda et al., 2010). However, in the trajectory feedback setting, these losses are not observed, and can only be estimated from the observation of the trajectories $(x_{1}^{t},a_{1}^{\\check{t}},...,x_{H}^{t},a_{H}^{t})$ and partial losses $(\\ell_{1}^{t},...,\\ell_{H}^{t})$ of each round. ", "page_idx": 3}, {"type": "text", "text": "2.2  Fixed sampling policy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the fixed sampling framework (Lanctot et al., 2009), both players always use the same policy for the observations of the trajectory. However, the two observations can not be done simultaneously with such an approach, as the learning would then be quite naive. The solution, summarized in Algorithm 1, is for the two players to take turns between an observation phase, in which they play their fixed sampling policy $\\mu^{s}$ or $\\nu^{s}$ , and an interaction phase, in which they play their updated policy $\\mu^{t}$ or $\\nu^{t}$ . The underlying idea is that the observation phase lets each player observe how the game unfolds against the opponent in its interaction phase, playing its updated policy. Given upper-bounds of the regrets min and 9Ta associated to the sequence $(\\mu^{t},\\bar{\\nu^{t}})_{t\\in[T]}$ , the previous Theorem 2.1 then characterizes the optimality of the outputted time-averaged profile $(\\overline{{\\mu}},\\overline{{\\nu}})$ ", "page_idx": 3}, {"type": "text", "text": "While theoretically optimal algorithms already exist using simultaneous regret minimization procedures (Bai et al., 2022; Fiegel et al., 2023), this framework allows for the removal of the global importance sampling term of the loss, which reduces the variance to make algorithms more suitable beyond the tabular setting (McAleer et al., 2022). Indeed, as the probability of choosing a sequence of action reaching a given information set is fixed, the average estimations of the losses do not need to be re-weighted based on the inverse of a changing probability. This re-weighting eventually leads to unstable function approximation, e.g. with neural networks, as this probability can be very small. Furthermore, the fixed sampling framework also allows aggressive policies more focused on exploitation, as the observation side is handled by the sampling strategy. The downside is that this sampling policy must be fixed in advance, which requires defining a good sampling policy beforehand. From now on, we again focus on the min-player for the same symmetry reasons. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Estimated regret Based on the min-player observations, we defne $\\hat{\\Re}_{\\operatorname*{min}}^{T}$ the estimated regret by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathfrak{R}}_{\\operatorname*{min}}^{T}:=\\operatorname*{max}_{\\mu^{\\dagger}\\in\\Pi_{\\operatorname*{min}}}\\sum_{t=1}^{T}\\left\\langle\\widehat{\\ell}^{t},\\,\\mu_{1:}^{t}-\\mu_{1:}^{\\dagger}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Wwherethe $\\widehat{\\ell}^{t}$ are the importance-sampling estimated loss vectors, defined for each information set $x$ of depth $h$ and action $\\bar{a}\\in\\mathcal{A}(x)$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\ell}^{t}(x,a):=\\frac{\\mathbb{I}_{\\left\\{x=x_{h}^{t},a=a_{h}^{t}\\right\\}}}{\\mu_{1:}^{s}(x,a)}\\ell_{h}^{t}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\v x_{h}^{t}$ the visited information set, $a_{h}^{t}$ the chosen action and $\\ell_{h}^{t}$ the loss at depth $h$ of episode $t$ ", "page_idx": 4}, {"type": "text", "text": "The following theorem states that upper-bounding this estimated regret is enough to upper-bound the actual regret, up to an additional additive term. Its proof is given in Appendix B and relies on Bernstein-type inequalities. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2. Assume that the estimated losses are obtained with a fixed positive sampling policy $\\mu^{s}$ as above.Then, for any sequence $(\\mu^{t})_{t\\in[T]}$ of $\\Pi_{\\mathrm{min}}$ andany $\\delta\\in(0,1)$ thefollowingboundholds withaprobabilityatleast $1-\\delta$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Re_{m i n}^{T}\\leq\\operatorname*{max}\\left\\{\\hat{\\Re}_{m i n}^{T},0\\right\\}+4\\sqrt{\\iota H\\kappa(\\mu^{s})T}}\\\\ &{\\kappa(\\mu^{s}):=\\operatorname*{max}_{\\mu\\in\\Pi_{\\operatorname*{min}}}\\sum_{x\\in\\mathcal{X}}\\sum_{a\\in\\mathcal{A}_{x}}\\frac{\\mu_{1:}(x,a)}{\\mu_{1:}^{s}(x,a)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where t := log (Ax\u00b11) and k(\u03bc) := max\u03bcellmin \u2265zex aeAn \u03bc:(\u00b1a)- ", "page_idx": 4}, {"type": "text", "text": "A similar proposition is proved by Farina et al. (2020). Our bound is specific to the importancesampling loss estimator, but tighter by a factor $\\sqrt{\\kappa(\\mu^{s})/H}$ ", "page_idx": 4}, {"type": "text", "text": "Remark 2.3. The quantity $\\kappa(\\mu^{s})$ can be efficiently computed recursively for each of the sub-trees induced by an information set $x\\in\\mathscr{X}$ , and we will denote by $\\kappa(\\mu^{s}|{\\boldsymbol{x}})$ the associated quantities. The same recursion shows that the balanced policy $\\mu^{\\star}$ , which plays proportionally to the total number of actions of each sub-tree, minimizes all these local quantities and satisfies $\\kappa({\\dot{\\mu}}^{\\star})=A_{\\mathcal{X}}$ . The related computations are provided in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "3 Adaptive Mirror Descent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We shall now focus on the update procedure the min-player can use to minimize this estimated regret.   \nLet us first define some important notions of convex optimization. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1. Let $\\Omega\\;\\subset\\;\\mathbb{R}^{n}$ be a non-empty open convex, and $\\overline{\\Omega}$ be its closure. A function $\\Psi:\\overline{{\\Omega}}\\rightarrow\\mathbb{R}$ is said to be Legendre if $\\Psi$ is strictly convex, continuously differentiable on $\\Omega$ and $\\begin{array}{r}{'y\\in\\overline{{\\Omega}}\\backslash\\Omega,\\;\\operatorname*{lim}_{x\\rightarrow y}\\|\\nabla\\Psi(x)\\|=+\\infty}\\end{array}$ . The Bregman divergence $\\mathbf{D}_{\\Psi}:\\dot{\\overline{{\\Omega}}}\\times\\Omega\\rightarrow\\mathbb{R}$ of a Legendre function $\\Psi$ is defined as $\\mathbf{D}_{\\Psi}^{}(x,y):=\\,\\Psi(x)-\\,\\mathbf{\\bar{\\Psi}}(y)\\,-\\,\\langle\\nabla\\Psi(y),x-y\\rangle$ . The Fenchel conjugate $\\Psi^{\\star}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ of $\\Psi$ is defined by $\\Psi^{\\star}(\\xi)=\\operatorname*{sup}_{x\\in{\\overline{{\\Omega}}}}\\left\\langle\\xi,x\\right\\rangle-\\Psi(x)$ ", "page_idx": 4}, {"type": "text", "text": "3.1  Online Mirror Descent and dilated entropy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In an extensive-form game with perfect recall, algorithms based on the Online Mirror Descent (OMD) typically compute at each time step $t$ theupdate ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mu^{t+1}=\\underset{\\mu\\in\\Pi_{\\operatorname*{min}}}{\\arg\\operatorname*{min}}\\left<\\widehat{\\ell}^{t},\\mu_{1:}\\right>+\\mathbf{D}_{\\Psi}(\\mu_{1:},\\mu_{1:}^{t})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widehat{\\ell}^{t}$ is the estimated loss and $\\Psi:Q_{\\mathrm{min}}\\rightarrow\\mathbb{R}$ a Legendre regularizer. ", "page_idx": 5}, {"type": "text", "text": "Dilated entropy _ A common choice of such regularizer is the dilated entropy (Hoda et al., 2010; Kroer et al., 2015). It requires for each $x\\in\\mathscr{X}$ a Legendre regularizer $\\Psi_{x}$ over a convex domain $\\overline{{\\Omega_{x}}}\\,\\subset\\,\\mathbb{R}_{\\ge0}^{|\\mathcal{A}(x)|}$ that conainsthe simplex $\\Delta_{{\\cal A}(x)}:=\\left\\{\\mu,\\,\\sum_{a\\in\\cal A(x)}\\mu(a)=1\\right\\}$ Fora givn lis f positive weights $\\alpha=(\\alpha(x))_{x\\in\\mathcal{X}}$ , the dilated entropy $\\Psi_{\\alpha}^{\\mathrm{dil}}$ satisfies for any $\\mu\\in\\Pi_{\\operatorname*{min}}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Psi_{\\alpha}^{\\mathrm{dil}}(\\mu_{1:}):=\\sum_{x\\in\\mathcal{X}}\\alpha(x)\\mu_{1:}(x)\\Psi_{x}\\left(\\mu(\\cdot|x)\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\textstyle\\mu_{1:}(x):=\\sum_{a\\in{\\cal{A}}(x)}\\mu_{1:}(x,a)$ Using thisdilatd enoy as theregularizer, the OMuat become ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu^{t+1}=\\underset{\\mu\\in\\Pi_{\\operatorname*{min}}}{\\arg\\operatorname*{min}}\\left\\langle\\widehat{\\ell}^{t},\\mu_{1:}\\right\\rangle+\\mathbf{D}_{\\alpha}^{\\mathrm{dil}}(\\mu_{1:},\\mu_{1:}^{t})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Where $\\begin{array}{r}{\\mathbf{D}_{\\alpha}^{\\mathrm{dil}}(\\mu_{1:},\\mu_{1:}^{t}):=\\sum_{x\\in\\mathcal{X}}\\alpha(x)\\mu_{1:}(x)\\mathbf{D}_{x}(\\mu_{1:}(\\cdot|x),\\mu_{1:}^{t}(\\cdot|x))}\\end{array}$ and $(\\mathbf{D}_{x})_{x\\in\\mathcal{X}}$ are the individual Bregman divergences of the $(\\Psi_{x})_{x\\in\\mathcal{X}}$ . The benefits of this regularization are that it efficiently suits the structure of the game and that the associated updates are easily computed recursively, starting from the final states. ", "page_idx": 5}, {"type": "text", "text": "3.2  Stabilized OMD algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The regularizer $\\Psi$ sometimes needs to change over time. For example, when $T$ is unknown, a regularizer of the form $\\Psi^{t}=\\Psi/\\eta^{t}$ is usually considered, with $\\eta^{t}=t^{-1/2}$ the learning rate. Fiegel et al. (2023) gives another example of time-varying regularization, adapting the regularization to the game structure that is assumed to be initially unknown. The previous updates (OMD) do not however allow adaptive regularization in general. In fact, even the simple learning rate decrease $\\eta^{t+1}=t^{-1/2}$ can lead to a linear regret dependence with time (Orabona & Pal, 2018). ", "page_idx": 5}, {"type": "text", "text": "In this part, we shall consider more generally a sequence of Legendre regularizers $(\\Psi^{t})_{t\\in[T]}$ defined on a convex domain ${\\overline{{\\Omega}}}\\subset\\mathbb{R}^{n}$ , and that the player chooses a sequence of primal iterates $(w^{t})_{t\\in[T]}$ (respectively the updated realization plans $(\\mu_{1:}^{t})_{t\\in[T]}$ of our settings) in a closed convex set $\\mathcal{C}$ (respectively the treeplex $Q_{\\mathrm{min}},$ included in $\\overline{\\Omega}$ , according to a sequence of dual increments $(\\xi^{t})_{t\\in[T]}$ in $\\mathbb{R}^{n}$ (respectively the estimated losses $(\\widehat{\\ell}^{t})_{t\\in[T]})$ observed sequentially. ", "page_idx": 5}, {"type": "text", "text": "Fang et al. (2020) proposed in the presence of non-increasing learning rates, to use a technique called dual-stabilization to recover the classical OMD bounds. We noticed that their updates can be interpreted as ", "page_idx": 5}, {"type": "equation", "text": "$$\nw^{t+1}=\\operatorname*{arg\\,min}_{w\\in\\mathcal{C}}\\left<\\xi^{t},w\\right>+\\mathbf{D}_{\\Psi^{t}}\\left(w,w^{t}\\right)+\\mathbf{D}_{\\Psi^{t+1}-\\Psi^{t}}\\left(w,w^{1}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\Psi^{t+1}-\\Psi^{t}$ incremental functions assumed tobe convex, generalizing their special case $\\Psi^{t+1}=$ $\\Psi/\\eta^{t+1}$ . The following theorem, proven in Appendix $\\mathrm{D}$ shows that classical OMD guarantees can be recovered with these updates. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. Let $(w^{t})_{t\\in[T]}$ be a sequence of primal iterates generated by the updates (GDS-OMD), with convex incremental functions. Then for any $w^{\\dagger}\\in{\\overline{{\\Omega}}}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\langle\\xi^{t},w^{t}-w^{\\dagger}\\right\\rangle\\leq\\mathbf{D}_{\\Psi^{T}}(w^{\\dagger},w^{1})+\\sum_{t=1}^{T}\\mathbf{D}_{\\Psi^{t,\\star}}\\left(\\nabla\\Psi^{t}(w^{t})-\\xi^{t},\\nabla\\Psi^{t}(w^{t})\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the $(\\Psi^{t,\\star})_{t\\in[T]}$ are therespetivFenelngatf $\\left(\\Psi^{t}\\right)_{t\\in[T]}$ ", "page_idx": 5}, {"type": "text", "text": "Compared to the guarantees obtained with previous adaptive procedures, such as Ada-MD (Joulani et al., 2017), the first term of the bound is stated with respect to $w^{1}$ instead of the sequence $(\\boldsymbol{w}^{t})_{t}$ which is important for some $(\\Psi^{t})_{t}$ sequences (Orabona & Pal, 2018). Remark 3.3. AdaGrad for stochastic gradient descent (Duchi et al., 2011) is an interesting example of regularizatiom with convex increments (and not only through a decreasing learning rate). It uses the adaptive regularization $\\Psi^{t+1}=\\left\\lVert\\cdot\\right\\rVert_{(G^{t})^{1/2}}^{2}$ , where $G^{t}$ is a positive semi-definite matrix defined with the gradients $g_{k}$ by either $\\begin{array}{r}{G^{t}=\\sum_{k=1}^{t}g_{k}g_{k}^{T}}\\end{array}$ or, more effciently, by $\\begin{array}{r}{G^{t}=\\mathrm{Diag}\\left(\\sum_{k=1}^{t}g_{k}g_{k}^{T}\\right)}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: Input: Sampling policy $\\mu^{s}\\in\\Pi_{\\operatorname*{min}}$ and initial policy $\\mu^{1}\\in\\Pi_{\\operatorname*{min}}$ Bregman divergences $\\mathbf{D}_{x}$ for each information set $x\\in\\mathscr{X}$ Sequences of (possibly adaptive) learning rates $(\\eta^{t}(x))_{t,x}$ for each round $t$ and information set $x$   \n2: For $t=1$ to $T$ Observes the outcome of an episode using the fixed strategy $\\mu^{s}$ $q_{H+1}^{t}\\gets0$ For $h=H$ to 1: $\\begin{array}{r l}&{\\widetilde{\\ell}_{h}^{t}\\leftarrow\\mathbb{I}_{\\left\\{a=a_{h}^{t}\\right\\}}\\left(\\ell_{h}^{t}+q_{h+1}^{t}\\right)\\biggr/\\mu^{s}(a_{h}^{t}|x_{h}^{t})}\\\\ &{\\mu^{t+1}(\\cdot|x)\\leftarrow\\arg\\operatorname*{min}_{\\mu\\in\\Delta_{A(x)}}h_{x}^{t}(\\mu)}\\\\ &{q_{h}^{t}\\leftarrow\\operatorname*{min}_{\\mu\\in\\Delta_{A(x)}}h_{x}^{t}(\\mu)}\\\\ &{\\mathrm{re}\\,h_{x}^{t}(\\mu):=\\left\\langle\\widetilde{\\ell}_{h}^{t},\\mu\\right\\rangle\\!+\\!\\frac{1}{\\eta^{t}(x_{h}^{t})}\\!\\mathbf{D}_{x}\\left(\\mu,\\mu^{t}(\\cdot|x_{h}^{t})\\right)\\!+\\!\\left(\\frac{1}{\\eta^{t+1}(x_{h}^{t})}-\\frac{1}{\\eta^{t^{\\prime}+1}(x_{h}^{t})}\\right)\\mathbf{D}_{x}\\left(\\mu,\\mu^{1}(\\cdot|x_{h}^{t})\\right)}\\end{array}$ and $t^{\\prime}$ is the last round in which $\\v x_{h}^{t}$ was visited For all non-visited $x\\in\\mathscr{X}$ $\\mu^{t+1}(\\cdot|x)\\gets\\mu^{t}(\\cdot|x)$   \n3: Output: The time-averaged policy $\\overline{{\\mu}}$ ", "page_idx": 6}, {"type": "text", "text": "Adaptive dilatation  In the extensive-form game setting based on the dilated entropy $\\Psi_{\\alpha}^{\\mathrm{dil}}$ ,this stabilization can be applied to have weights $(\\bar{\\alpha}^{t}(x))_{x\\in\\mathcal{X},t\\in[T]}$ that vary with times. The convexity assumption of wdit $\\Psi_{\\alpha^{t+1}}^{\\mathrm{dil}}-\\Psi_{\\alpha^{t}}^{\\mathrm{dil}}$ then rewrites to having locallynon-decreasing weights for each $x\\in\\mathscr{X}$ In this particular case, the updates are obtained with the formula ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mu^{t+1}=\\operatorname*{arg\\,min}_{\\mu\\in\\Pi_{\\mathrm{min}}}\\left\\langle\\widehat{\\ell}^{t},\\mu_{1:}\\right\\rangle+\\mathbf{D}_{\\alpha^{t}}^{\\mathrm{dil}}(\\mu,\\mu^{t})+\\mathbf{D}_{\\alpha^{t+1}-\\alpha^{t}}^{\\mathrm{dil}}(\\mu,\\mu^{1})\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 LocalOMD algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1  Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Let us now consider the fixed sampling framework introduced in Section 2.2. Given a sequence $(\\eta^{t}(x))_{t\\in[T]}$ of locally non-increasing learning rates for each $x\\in\\mathscr{X}$ , we introduce the Local0MD algorithm described in Algorithm 2, that uses the updates (DDS-OMD) above with the adaptive weights $\\alpha^{t}(x)\\,=\\,1/(\\mu_{1:}^{s}(\\bar{x^{}})\\eta^{t}(x))$ . Dividing the loss by the importance sampling term $1/\\bar{\\mu_{1;}^{s}}(x)$ through the learning rates lets it bypass the large variance that this rate can introduce. ", "page_idx": 6}, {"type": "text", "text": "Local loss This algorithm can be interpreted as one that locally applies the updates (GDS-OMD) using the local loss $\\widetilde{\\ell}_{h}^{t}$ , a regularized version of the sum of subsequent losses. Even though this algorithm results from a global minimization procedure, the local loss only uses the probability $\\mu^{s}(a|x)$ of choosing the last action $a\\in A(x)$ in the important sampling, instead of the combined probability $\\mu_{1;}^{s}(x,a)$ of the realization plan. A similar decomposition was observed by Farina et al. (2019a) for the non-stochastic setings, in which both players directly observe the gradient associated with their policies. ", "page_idx": 6}, {"type": "text", "text": "For this reason, the local loss will consistently be at most of order $\\mathcal{O}(H A)$ . Meanwhile, the loss used by Fiegel et al. (2023) can be of order ${\\mathcal{O}}(A_{{\\mathcal{X}}})$ (approximately $A^{H}$ in the worst case), even with IX exploration attempting to alleviate the importance sampling issue. This presents a challenge for potential applications involving function approximation, where $A_{\\mathcal{X}}$ becomes very large (McAleer et al., 2022). For instance, such high-variance estimates could lead to highly unstable training dynamics of a policy parametrized with a neural network. ", "page_idx": 6}, {"type": "text", "text": "Complexity  At each iteration, the algorithm only needs to update the policy along the observed trajectory, so the time complexity per iteration is only $H$ times the cost of a local update. If $\\mathbf{D}_{x}$ is the Kullback-Leibler divergence, the local updates then simplify to some Exponential Weights updates and the total time complexity of an iteration becomes $\\mathcal{O}(H A)$ ,where $A$ is an upper-bound on the local number of actions. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2  Theoretical analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The analysis of LocalOMD, detailed in Appendix $\\mathrm{E}$ is derived from Theorem 3.2 that bounds the estimated regret. The results on the real regret are then obtained with Theorem 2.2. We now present two choices of regularization and their associated guarantees. ", "page_idx": 7}, {"type": "text", "text": "Adaptive rates  As LocalOMD treats each information set $x\\in\\mathscr{X}$ as a separate problem through the local losses $\\widetilde{\\ell}_{h}^{t}$ , an interesting choice is to consider the same adaptive rates that would be used in the $K$ -armed bandit problems. The following theorem provides an upper bound in this case. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1. (Informal, exact statement in Appendix $E$   \nFor a large class of regularizers $(\\Psi_{x})_{x\\in\\mathcal{X}}$ and learning rates $(\\eta^{t}(x)_{x\\in\\mathcal{X},t\\in[T]})$ , the regret has $a$ $\\mathcal{O}(\\sqrt{T\\log(1/\\delta}))$ upper bound (hiding the game-dependent terms) with a probability at least $1-\\delta$ Suchlearningratesinclude,forall $x\\in\\mathscr{X}$ of depth $h_{i}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta^{t}(x)=\\eta\\Biggl/\\sqrt{\\sum_{k=1}^{t}\\mathbb{I}_{\\left\\{x=x_{h}^{k}\\right\\}}}\\;,\\quad o r\\;t h e\\;a d a p t i v e\\;v e r s i o n\\;\\eta^{t}(x)=\\eta\\Biggl/\\sqrt{\\sum_{k=1}^{t}\\mathbb{I}_{\\left\\{x=x_{h}^{k}\\right\\}}\\left(\\tilde{\\ell}_{h}^{k}\\right)^{2}}\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The adaptive learning rates mentioned for this theorem generally enjoy better performances in practice.   \nFurthermore, they require no initial computation and are easily updated. ", "page_idx": 7}, {"type": "text", "text": "Optimal rates  The following theorem uses a constant learning rate that locally depends on the $\\kappa(\\mu^{s}|{\\boldsymbol{x}})$ quantities of Remark 2.3, and on the $A:=\\operatorname*{max}_{x\\in\\mathcal{X}}|\\mathcal{A}(\\bar{x})|$ quantity that upper bounds the local number of available actions on the whole tree. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2. Using Local0MD with $\\mu^{1}$ as the uniform policy, with the learning rates $\\eta^{t}(x)\\,=$ $\\eta/\\kappa(\\mu^{s}|\\boldsymbol{x})$ where $\\eta~=~\\sqrt{\\log(A)\\kappa(\\mu^{s})/(3H T)}$ . and with $\\Psi_{x}$ the Shannon entropy $\\Psi_{x}(\\mu)\\;=\\;$ $\\begin{array}{r}{\\sum_{a\\in\\mathcal{A}(x)}\\mu(a)\\log(\\mu(a))}\\end{array}$ we have with a probability at least $1-\\delta$ and $\\iota=\\log(2(A_{\\mathcal{X}}+1)/\\delta)$ \uff0c ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Re_{\\mathrm{min}}^{T}\\le\\Big(4+2\\sqrt{3}\\Big)\\,\\,H^{3/2}\\sqrt{\\log(A)\\iota\\kappa(\\mu^{s})T}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that these rates are not adaptive and thus do not require the stabilization introduced in Section 3.2. When using the balanced policy $\\mu^{\\star}$ as the sampling policy, for which $\\kappa(\\mu^{\\star})=A_{\\mathcal{X}}$ , we obtain with Theorem 2.1 the rate $\\widetilde{\\mathcal{O}}\\left(H^{3/2}\\sqrt{A\\chi T}\\right)$ , near-optimal up to the $H$ dependency (Bai et al., 2022). ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We implemented Local0MD, with the parameters of Theorem 4.1 and Theorem 4.2, then tested it against the theoretically optimal BalancedCFR (Bai et al., 2022) using the balanced policy as the sample policy, and BalancedFTRL (Fiegel et al., 2023). The algorithms were compared on three standard benchmark games: Kuhn poker (Kuhn, 1950), Leduc poker (Southey et al., 2005) and liars dice, using the version 1.4 of the OpenSpiel library (Lanctot et al., 20i9) under the Apache 2.0 license. The learning rates (and the $I X$ parameters for the relevant algorithms) were optimized independently for each algorithm using a grid search. The code is available at https : //github.com/anon5493/Local0MD-experiments. ", "page_idx": 7}, {"type": "text", "text": "The results are given with respect to the total number of episodes used for learning. This technically disadvantages the fixed sampling algorithms, as these require more than one episode at each round $t$ while still performing a single update on the policy of each player. The exploitability gap, along with the variance across the different instances of the simulation, is plotted in Figure 1, top. Note that this variance across the instances is different from the variance of the estimated loss vector $\\widehat{\\ell}^{t}$ ourmethod tries to reduce, which is plotted in the Figure 1, down. ", "page_idx": 7}, {"type": "text", "text": "Focusing on the exploitability gap, we observe that the two versions of LocalOMD behave similarly and constantly beat BalancedCFR, mainly because the latter needs to update each depth with independent samples, thus needing $H$ times more episodes overall. The results of BalancedFTRL are more comparable, exhibiting for example better performances on liars dice but worse on Leduc poker. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In the second figure, we observe that the algorithms based on a fixed sampling procedure indeed get a smaller variance in their loss estimation as the sampling policy stays consistently balanced. BalancedCFR again gets worse results compared to LocalOMD as the losses of each depth are only estimatedevery $H$ iteration, which increases its variance. ", "page_idx": 8}, {"type": "image", "img_path": "HU2uyDjAcy/tmp/9c3758594834e950faa2dcc98d126579fb26894e42e87b119e1e3cb8eb10edf2.jpg", "img_caption": ["Figure 1: Performances over 5 simulations of various algorithms with respect to the total number of episodes. The vertical axis denotes the exploitability gap $\\begin{array}{r}{\\operatorname*{max}_{(\\mu,\\nu)\\in\\Pi_{\\operatorname*{min}}\\times\\Pi_{\\operatorname*{max}}}V^{\\overline{{\\mu}},\\nu}-V^{\\mu,\\overline{{\\nu}}}}\\end{array}$ (top) and the empirical variance of the $\\widehat{\\ell}^{t}$ vectors over time (bottom), with all rewards scaled between 0 and 1. The total numbers of actions are $A_{\\mathcal{X}}=B_{\\mathcal{Y}}=12$ for Kuhn poker, $A_{\\mathcal{X}}=B_{\\mathcal{Y}}=1092$ for Leduc poker, and $A_{\\mathcal{X}}=B_{\\mathcal{Y}}=24570$ for Liars dice. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We studied the use of a fixed sampling OMD procedure for the computation of $\\varepsilon$ -optimal strategies. This approach relies, for each player, on an uncoupling between the observation policy and the interaction policy as described in Algorithm 1. This uncoupling is in direct contrast with the more restrictive semi-bandit setting usually considered for self-play, where these two policies must coincide by design. Notice that this is not the standard exploration/exploitation trade-off, as even in the expert setting (with full information), some kind of exploration is still required. ", "page_idx": 8}, {"type": "text", "text": "While the balanced observation policy gets the optimal rates in the worst case, it may not always be the best one for a given game. An alternate choice is to instead use for the observations the current average policy (Gibson et al., 2012). This choice can be adapted to the fixed sampling framework, by restarting the algorithm after a certain number of episodes and using the computed average as the new sampling policy. ", "page_idx": 8}, {"type": "text", "text": "The proposed algorithm LocalOMD also enjoys simultaneously two interpretations: one as a Mirror Descent type algorithm working at the global level, with a single update performed at each iteration over the whole tree; and one as regret minimizers working locally at each information set, which makes it very similar to a CFR algorithm despite a fundamentally different approach. ", "page_idx": 8}, {"type": "text", "text": "We would like to conclude by providing the following interesting research directions. ", "page_idx": 9}, {"type": "text", "text": "Problem-dependent optimality  For a given game structure and fixed sampling policy $\\mu^{s}$ , is there a policy-dependent lower bound $O(\\sqrt{\\kappa(\\mu^{s})T})$ on the regret? We wonder if the $\\kappa(\\mu^{s})$ quantity of Remark 2.3 denotes some sort of complexity related to the problem. ", "page_idx": 9}, {"type": "text", "text": "General sum game  Using the same techniques as Bai et al. (2022), in a general sum game with potentially more than two players, LocalOMD can be shown to converge to an $\\varepsilon$ -approximatenormalform coarse correlated equilibrium. Are convergences to other forms of correlated equilibrium possible using this fixed sampling policy framework? ", "page_idx": 9}, {"type": "text", "text": "On-policy algorithms  Is it also possible to remove the importance-sampling of the previous actions in the usual semi-bandit framework that observes with the current policy? The answer is not obvious since the current approach heavily relies on the fact that the sampling policy is fixed. ", "page_idx": 9}, {"type": "text", "text": "7  Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported in part by the French National Research Agency (ANR) in the framework of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003) and through the grant DOOM ANR23-CE23-0002. It was also funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily refect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. The Nonstochastic Multiarmed Bandit Problem. SIAM Journal on Computing, 32(1):48-77, January 2003. ISSN 0097-5397. doi: 10.1137/S0097539701398375.   \nBai, Y., Jin, C., and Yu, T. Near-optimal reinforcement learning with self-play. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2159-2170. Curran Associates, Inc., 2020. URL https : / /proceedings . neurips.cc/paper/2020/file/172ef5a94b4dd0aa120c6878fc29f70c-Paper.pdf.   \nBai, Y., Jin, C., Mei, S., and Yu, T. Near-optimal learning of extensive-form games with imperfect information. In International Conference on Machine Learning, 2022.   \nBurch, N., Moravcik, M., and Schmid, M. Revisiting $\\mathrm{CFR+}$ and Alternating Updates. Journal of Artificial Intelligence Research, 64:429-443, 2019.   \nCesa-Bianchi, N. and Lugosi, G. Prediction, Learning, and Games. Cambridge University Press, Cambridge, 2006. ISBN 978-0-521-84108-5. doi: 10.1017/CBO9780511546921.   \nDaskalakis, C., Foster, D. J., and Golowich, N. Independent policy gradient methods for competitive reinforcement learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 5527-5540. Curran Associates, Inc., 2020. URL https: //proceedings .neurips.cc/paper/2020/file/ 3b2acfe2e38102074656ed938abf4ac3-Paper.pdf.   \nDuchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121-2159, 2011. URL http : //jmlr.org/papers/v12/duchi11a.html.   \nFang, H., Harvey, N., Portella, V., and Friedlander, M. Online mirror descent and dual averaging: Keeping pace in the dynamic case. In Proceedings of the 37th International Conference on Machine Learning, Pp. 3008-3017. PMLR, November 2020.   \nFarina, G., Kroer, C., and Sandholm, T. Optimistic Regret Minimization for Extensive-Form Games via Dilated Distance-Generating Functions. In Advances in Neural Information Processing Systems, 2019a.   \nFarina, G., Kroer, C., and Sandholm, T. Regret circuits: Composability of regret minimizers. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 1863-1872. PMLR, 2019b. URL http: //proceedings.mlr.press/v97/farina19b.html.   \nFarina, G., Kroer, C., and Sandholm, T. Stochastic Regret Minimization in Extensive-Form Games. In International Conference on Machine Learning, 2020.   \nFarina, G., Kroer, C., and Sandholm, T. Faster Game Solving via Predictive Blackwell Approachability: Connecting Regret Matching and Mirror Descent. In AAAI Conference on Artificial Intelligence, 2021a. URL https: //arxiv . org/abs/2007 .14358.   \nFarina, G, Kroer, C, and Sandholm,T. Bandit Linar Optimization for quential DeisinMaking and Extensive-Form Games. In AAAI Conference on Artificial Intelligence, 2021b.   \nFiegel, C., Menard, P, Kozuno, T., Munos, R., Perchet, V., and Valko, M. Adapting to game trees in zero-sum imperfect information games, 2023.   \nGibson, R., Burch, N., Lanctot, M., and Szafron, D. Effcient monte carlo counterfactual regret minimization in games with many player actions. volume 3, 12 2012.   \nGordon, G. J. No-regret Algorithms for Online Convex Programs. In Advances in Neural Information Processing Systems, 2007.   \nHart, S. and Mas-Colell, A. A Simple Adaptive Procedure Leading to Correlated Equilibrium. Econometrica, 68(5):1127-1150, 2000.   \nHoda, S., Gilpin, A., Pena, J., and Sandholm, T. Smoothing Techniques for Computing Nash Equilibria of Sequential Games. Mathematics of Operations Research, 2010. URL https : //kilthub.cmu.edu/ndownloader/files/12101699.   \nJohanson, M, Bard, N, Lanctot, M., Gibson, R., and Bowling, M. Effcient nash equilibrium approximation through monte carlo counterfactual regret minimization. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems -Volume 2, AAMAS '12, pp. 837-846, Richland, SC, 2012. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 0981738125.   \nJoulani, P, Gyorgy, A., and Szepesvari, C. A modular analysis of adaptive (non-)convex optimization: Optimism, composite objectives, and variational bounds. In International Conference on Algorithmic Learning Theory, 2017. URL https : //api .semanticscholar.org/CorpusID : 28926102.   \nKoller, D., Megido, N., and Von Stengel, B. Effcient Computation of Equilibria for Extensive Two-Person Games. Games and Economic Behavior, 14(2):247-259, 1996.   \nKozuno, T, Menard, P, Munos, R., and Valko, M. Learning in two-player zero-sum partially observable Markov games with perfect recall. In Neural Information Processing Systems, 2021.   \nKroer, C., Waugh, K., Kilinc-Karzan, F., and Sandholm, T. Faster first-order methods for extensiveform game solving. In Economics and Computation, 2015. ISBN 978-1-4503-3410-5. doi: 10.1145/2764468.2764476.   \nKroer, C., Farina, G., and Sandholm, T. Solving large sequential games with the excessive gap technique. In Neural Information Processing Systems, 2018.   \nKroer, C., Waugh, K., Klnc-Karzan, F, and Sandholm, T. Faster algorithms for extensive-form game solving via improved smoothing functions. Mathematical Programming, 179(1):385-417, 2020.   \nKuhn, H. W. Extensive games. Proceedings of the National Academy of Sciences, 36(10):570-576, 1950.   \nKuhn, H. W. Extensive Games and the Problem of Information. Annals of Mathematics Studies, 28: 193-216 1953 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Lanctot, M., Waugh, K., Zinkevich, M., and Bowling, M. Monte-Carlo sampling for regret minimization in extensive games. In Neural Information Processing Systems, 2009. ", "page_idx": 11}, {"type": "text", "text": "Lanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V., Upadhyay, S., Perolat, J., Srinivasan, S., Timbers, F., Tuyls, K., Omidshafiei, S., Hennes, D., Morrill, D., Muller, P., Ewalds, T., Faulkner, R., Kramar, J., De Vylder, B., Saeta, B., Bradbury, J., Ding, D., Borgeaud, S., Lai, M., Schrittwieser, J., Anthony, T., Hughes, E., Danihelka, I., and Ryan-Davis, J. Openspiel: A framework for reinforcement learning in games, 2019. URL https : //arxiv . org/abs/1908 . 09453.   \nLaraki, R., Renault, J., and Sorin, S. Mathematical Foundations of Game Theory. Springer, October 2019. doi: 10.1007/978-3-030-26646-2. URL https : //hal . science/hal-03070434.   \nLattimore, T. and Szepesvari, C. Bandit Algorithms. Cambridge University Press, 2020. doi: 10.1017/9781108571401.   \nLee, C.-W., Kroer, C., and Luo, H. Last-iterate convergence in extensive-form games. In Neural Information Processing Systems, 2021. URL https: //proceedings .neurips . cc/paper/ 2021/file/77bb14f6132ea06dea456584b7d5581e-Paper.pdf.   \nLiu, Q., Yu, T., Bai, Y., and Jin, C. A sharp analysis of model-based reinforcement learning with self-play. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 7001-7010. PMLR, 18-24 Jul 2021. URL https ://proceedings .mlr .press/v139/liu21z.html.   \nMcAleer, S., Farina, G., Lanctot, M., and Sandholm, T. ESCHER: Eschewing importance sampling in games by computing a history value function to estimate regret. CoRR, abs/2206.04122, 2022. doi: 10.48550/arXiv.2206.04122. URL https : //doi . org/10 . 48550/arXiv .2206 . 04122.   \nMcMahan, H. A survey of algorithms and analysis for adaptive online learning. Journal of Machine Learning Research, 18:1-50, 08 2017.   \nMunos, R., Perolat, J., Lespiau, J-B., Rowland, M., De Vylder, B., Lanctot, M., Timbers, F., Hennes, D., Omidshafiei, S., Gruslys, A., Azar, M. G., Lockhart, E., and Tuyls, K. Fast computation of nash equilibria in imperfect information games. In International Conference on Machine Learning, 2020.   \nNemirovski, A. Prox-Method with Rate of Convergence $O(1/t)$ for Variational Inequalities with Lipschitz Continuous Monotone Operators and Smooth Convex-Concave Saddle Point Problems. SIAM Journal on Optimization, 15(1):229-251, 2004.   \nNesterov, Y. Smooth minimization of non-smooth functions. Mathematical programming, 103(1): 127-152, 2005.   \nOrabona, F. and Pal, D. Scale-free online learning. Theor. Comput. Sci., 716:50-69, 2018. doi: 10.1016/j.tcs.2017.11.021. URL https : //doi . org/10.1016/j.tcs.2017.11.021.   \nOsborne, M. J. and Rubinstein, A. A Course in Game Theory. The MIT Press, 1994. ISBN 0-262-65040-1.   \nPonsen, M., De Jong, S., and Lanctot, M. Computing Approximate Nash Equilibria and Robust Best-Responses Using Sampling. Journal of Artificial Intelligence Research, 42:575-605, 2011.   \nRomanovsky, J. V. Reduction of a game with complete memory to a matricial game. Dokl. Akad. Nauk SSSR, 144:62-64, 1962.   \nSchmid, M., Burch, N., Lanctot, M., Morav?ik, M., Kadlec, R., and Bowling, M. Variance reduction in monte carlo counterfactual regret minimization (VR-MCCFR) for extensive form games using baselines. CoRR, abs/1809.03057, 2018. URL http : //arxiv.org/abs/1809 . 03057.   \nSidford, A., Wang, M., Yang, L., and Ye, Y. Solving discounted stochastic two-player games with nearoptimal time and sample complexity. In Chiappa, S. and Calandra, R. (eds.), The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], volume 108 of Proceedings of Machine Learning Research, pp. 2992-3002. PMLR, 2020. URL http: //proceedings .mlr.press/v108/sidford20a.htmi.   \nSouthey, F., Bowling, M., Larson, B., Piccione, C., Burch, N., Billings, D., and Rayner, C. Bayes' bluff: Opponent modelling in poker. In Proceedings of the Twenty-First Conference on Uncertaintyin Artificial Intelligence (UAI), pp. 550-558, 2005.   \nSteinberger, E., Lerer, A., and Brown, N. DREAM: deepregret minimization with advantage baselines and model-free learning. CoRR, abs/2006.10410, 2020. URL https : //arxiv . org/abs/2006 . 10410.   \nStrens, M. A Bayesian Framework for Reinforcement Learning. In International Conference on Machine Learning, 2000.   \nTammelin, 07.s0 Solving large imperfect information games using $\\mathrm{CFR+}$ : arXiv preprint arXiv:1407.5042, 2014.   \nvon Neumann, J. Zur Theorie der Gesellschaftsspiele. Mathematische Annalen, 100:295-320, 1928. ISSN 0025-5831; 1432-1807/e.   \nvon Stengel, B. Efficient computation of behavior strategies. Games and Economic Behavior, 14(2): 220-246, 1996.   \nWaugh, K. and Bagnell, J. A. A unified view of large-scale zero-sum equilibrium computation. CoRR, abs/1411.5007, 2014. URL http : //arxiv. org/abs/1411.5007.   \nWei, C., Lee, C., Zhang, M., and Luo, H. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games. In Belkin, M. and Kpotufe, S.(eds.), Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA, volume 134 of Proceedings of Machine Learning Research, pp. 4259-4299. PMLR, 2021. URL http://proceedings.mlr.press/v134/wei21a.html.   \nWei, C.-Y, Hong, Y-T, and Lu, C.-J. Online reinforcement learning in stochastic games. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper/2017/file/ 36e729ec173b94133d8fa552e4029f8b-Paper.pdf.   \nXie, Q., Chen, Y, Wang, Z., and Yang, Z. Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium. In Abernethy, J. and Agarwal, S. (eds.), Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pp. 3674-3682. PMLR, 09-12 Jul 2020. URL https://proceedings.mlr.press/v125/xie20a.html.   \nZhang, B. H. and Sandholm, T. Finding and Certifying (Near-) Optimal Strategies in Black-Box Extensive-Form Games. In AAAl Conference on Artificial Intelligence, 2021.   \nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning (still) requires rethinking generalization. Commun. ACM, 64(3):107-115, feb 2021. ISSN 0001-0782. doi: 10.1145/3446776. URL https : //doi . org/10 . 1145/3446776.   \nZhang, K., Kakade, S., Basar, T., and Yang, L. Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1166-1178. Curran Associates,Inc.,2020. URL https: //proceedings .neurips. cc/paper/2020/file/ Occ6ee01c82fc49c28706e0918f57e2d-Paper.pdf.   \nZhou, Y., Li, J., and Zhu, J. Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information. In International Conference on Learning Representations, 2020. URL https: //openreview.net/pdf?id $=$ Syg-ET4FPS.   \nZinkevich, M., Johanson, M., Bowling, M., and Piccione, C. Regret minimization in games with incomplete information. Neural Information Processing Systems, 2007. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix and Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Related works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we review previous works on learning an $\\varepsilon$ -optimal strategy in IIGs. ", "page_idx": 13}, {"type": "text", "text": "Full feedback  When the game is known, that is the information set structure space, transitions probability, and reward function are provided, a first line of work recasts the setting through the sequence-form representation of a game as a linear program which can be solved efficiently (Romanovsky, 1962; von Stengel, 1996; Koller et al., 1996). A second line of work relies on first-order optimization methods for saddle point computation (Hoda et al., 2010; Kroer et al., 2015, 2018, 2020; Munos et al., 2020; Lee et al., 2021). In particular Hoda et al. (2010); Kroer et al. (2018) relies on the Nesterov smoothing technique Nesterov (2005) whereas Kroer et al. (2015, 2020) use the MirrorProx algorithm (Nemirovski, 2004). These methods have a rate of convergence of order $\\widetilde{\\mathcal{O}}(\\mathrm{poly}(H,A_{\\mathcal{X}},B_{\\mathcal{Y}})/\\varepsilon)$ ", "page_idx": 13}, {"type": "text", "text": "A third approach, counterfactual regret minimization (Zinkevich et al., 2007), leverages local regret minimization, i.e. minimizing a type of regret at each information set. Popular algorithms are based on the regret-matching algorithm (Hart & Mas-Colell, 2000; Gordon, 2007) such as CFR algorithm (Zinkevich et al., 2007) or based on a close variant of regret-matching, e.g. $\\tt C F R+$ (Tammelin, 2014; Burch et al., 2019; Farina et al., 2021a). Note that other local regret minimizers could be used, see for example Waugh & Bagnell (2014); Farina et al. (2019b). These algorithms enjoy a guarantee of convergence of order $\\widetilde{\\mathcal{O}}(\\mathrm{poly}(H,A_{\\mathcal{X}},B_{\\mathcal{Y}})/\\varepsilon^{2})$ ", "page_idx": 13}, {"type": "text", "text": "Nevertheless, all the methods described above need to explore the whole information set tree (or the whole state space) in order to compute one update. The cost of one traversal is of order $\\mathcal{O}(X+Y)$ if the transitions and the actions of the other player are sampled; see for example the external-sampling MCCFR algorithm (Lanctot et al., 2009). ", "page_idx": 13}, {"type": "text", "text": "Trajectory feedback A way to tackle the aforementioned issues is to consider the agnostic setting where the agent has no prior knowledge of the game and only observes trajectories of the game. Precisely, the rewards and the transition probabilities are unknown. ", "page_idx": 13}, {"type": "text", "text": "Model-based  A first method to deal with this limited feedback is to build a model of the game and then run any full feedback algorithm in this model. For example, Zhou et al. (2020) use posterior sampling (PS, Strens, 2000) to learn a model and then use the CFR algorithm in games sampled from the posterior. They obtain a convergence rate of order $\\widetilde{\\mathcal{O}}(\\mathrm{poly}(H,S,A,B)/\\varepsilon^{2})$ but onlywhen the games are actually sampled according to the known prior. Instead, Zhang & Sandholm (2021) relies on the principle of optimism in the presence of uncertainty to incrementally build a model of the game. Then, the CFR algorithm is fed with optimistic estimates of the local regrets. They prove a high-probability sample complexity of order $\\mathcal{\\widetilde{O}}(\\mathrm{poly}(H,S,A,B)/\\varepsilon^{2})$ ", "page_idx": 13}, {"type": "text", "text": "Model-free  Another line of work (Lanctot et al., 2009; Johanson et al., 2012; Schmid et al., 2018; Farina et al., 2020) directly estimates the local regret via importance sampling that is then fed to the CFR algorithm. In particular, the outcome-sampling MCCFR (Lanctot et al., 2009; Farina et al., 2020) builds an importance sampling estimate of the counterfactual regret by playing according to a well-chosen balanced policy. Intuitively, this policy should ensure to explore all the information sets. Note that, depending on the structure of the information set space, playing uniformly over the actions at each information set is not necessarily a good choice. Instead, Farina et al. (2020) propose as a balanced policy to play action with probability proportional to the number of leaves in the sub-tree of possible next information sets. In particular, the outcome-sampling MCCFR algorithm requires the knowledge of the information set space structure to build its balanced policy. Nonetheless, in order toobtain $\\varepsilon,$ -optimal strategies with high probability, MCCFR needs at most $\\widetilde{\\mathcal{O}}(H^{3}(A_{\\mathcal{X}}+B_{\\mathcal{Y}})/\\varepsilon^{2})$ realizations of the game (Farina et al., 2020; Bai et al., 2022). ", "page_idx": 13}, {"type": "text", "text": "Later, Kozuno et al. (2021) proposed to combine Online Mirror Descent (0MD) with dilated Shannon entropy as regularizer and importance sampling estimate of the losses of a player, see also Farina et al. (2021b). They prove a sample complexity, for the proposed algorithm, Ix0MD, of order $\\widetilde{\\mathcal{O}}(H^{2}(X A_{\\mathcal{X}}+Y B_{\\mathcal{Y}})/\\varepsilon^{2})$ . Interestingly, they do not need to know in advance the structure of the information set space to obtain this bound. However, the sample complexity of IxOMD does not match the lower bound for this setting which is of order $\\mathcal{O}((A_{\\mathcal{X}}\\,\\dot{+}\\,B_{\\mathcal{Y}})/\\dot{\\varepsilon}^{2})$ . Recently, Bai et al. (2022) proposed the Balanced OMD algorithm that enjoys also relies on OMD but with a dilated entropy weighted by the realization plans of balanced policies as regularizers. For this algorithm, they prove a sample complexity of order $\\widetilde{\\mathcal{O}}(H^{3}(A_{\\mathcal{X}}+\\Bar{B_{\\mathcal{Y}}})/\\varepsilon^{2})$ ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Perfect information Markov game Another line of work considers Markov game Kuhn (1953) with perfect information and limited feedback. However, it does not assume perfect recall. Sidford et al. (2020); Zhang et al. (2020); Daskalakis et al. (2020); Wei et al. (2021) consider the case where a generative model is available whereas Wei et al. (2017); Bai et al. (2020); Xie et al. (2020); Liu et al. (2021) deal with the trajectory feedback case. Although this setting is related to ours there is no direct comparison between the two. ", "page_idx": 14}, {"type": "text", "text": "B  Regret estimation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we aim to establish Theorem 2.2 of the main paper. We start by stating a Bernstein-type inequality that we will use multiple times. It can be found e.g. in Exercise 5.15 by Lattimore & Szepesvari (2020). We provide a short proof below as we did not find any for this precise statement. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1. Let $(U^{t})_{t\\in[T]}$ bea sequence ofrandomvariableswithrespect toafiltration $\\mathcal{F}$ and $\\gamma>0$ be a fixed constant such that for all t, $\\gamma U^{t}\\leq1$ .Then with a probability of at least $1-\\delta^{\\prime}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(U^{t}-{\\mathbb{E}}\\left[U^{t}{\\big|}{\\mathcal{F}}^{t-1}\\right]\\right)\\leq\\gamma\\sum_{t=1}^{T}{\\mathbb{E}}\\left[(U^{t})^{2}{\\big|}{\\mathcal{F}}^{t-1}\\right]+{\\frac{1}{\\gamma}}\\log({\\frac{1}{\\delta^{\\prime}}})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For any $t\\in[T]$ , using the inequalities $\\exp(x)\\leq1+x+x^{2}$ for all $x\\leq1$ and $1+x\\leq\\exp(x)$ for all $x\\in\\mathbb R$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp\\left(\\gamma U^{t}\\right)\\middle|\\mathcal{F}^{t-1}\\right]\\leq\\mathbb{E}\\left[1+\\gamma U^{t}+\\gamma^{2}(U^{t})^{2}\\middle|\\mathcal{F}^{t-1}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=1+\\gamma\\mathbb{E}\\left[U^{t}\\middle|\\mathcal{F}^{t-1}\\right]+\\gamma^{2}\\mathbb{E}\\left[(U^{t})^{2}\\middle|\\mathcal{F}^{t-1}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\exp\\left(\\gamma\\mathbb{E}\\left[U^{t}\\middle|\\mathcal{F}^{t-1}\\right]+\\gamma^{2}\\mathbb{E}\\left[(U^{t})^{2}\\middle|\\mathcal{F}^{t-1}\\right]\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This implies that the random process $(S_{t})_{t\\in[T]}$ defined by ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{t}:=\\exp\\left(\\sum_{k=1}^{t}\\gamma\\left(U^{k}-\\mathbb{E}\\left[U^{k}\\big|\\mathcal{F}^{k-1}\\right]\\right)-\\sum_{k=1}^{t}\\gamma^{2}\\mathbb{E}\\left[(U^{k})^{2}\\big|\\mathcal{F}^{k-1}\\right]\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is a super-martingale, with $S_{0}=1$ . Using the Markov inequality, we then get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{1}{\\gamma}\\log(S_{T})>\\frac{1}{\\gamma}\\log\\left(\\frac{1}{\\delta^{\\prime}}\\right)\\right)=\\mathbb{P}\\left(S_{T}>\\frac{1}{\\delta^{\\prime}}\\right)\\le\\delta^{\\prime}\\,\\mathbb{E}(S_{T})\\le\\delta^{\\prime}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which immediately yields the stated inequality with probability at least $1-\\delta^{\\prime}$ ", "page_idx": 14}, {"type": "text", "text": "This lemma is then used for Theorem 2.2. The fitration $({\\mathcal{F}}^{t})_{t\\in[T]}$ will be used, such that $\\mathcal{F}^{t}$ isthe sigma-algebra of all variables of the self-play algorithm up to the execution of episode $t+1$ ", "page_idx": 14}, {"type": "text", "text": "Theorem B.2. Assume that the estimated losses are obtained with a fixed positive sampling policy $\\mu^{s}$ as above. Then, for any sequence $(\\mu^{t})_{t\\in[T]}$ of $\\Pi_{\\mathrm{min}}$ andany $\\delta\\in(0,1)$ thefollowingbound holds with aprobabilityatleast $1-\\delta$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathfrak{R}_{m i n}^{T}\\leq\\operatorname*{max}\\left\\{\\hat{\\Re}_{m i n}^{T},0\\right\\}+4\\sqrt{\\iota H\\kappa(\\mu^{s})T}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\iota:=\\log\\left(\\frac{A_{X}+1}{\\delta}\\right)\\quad a n d\\quad\\kappa(\\mu^{s}):=\\operatorname*{max}_{\\mu\\in\\Pi_{\\operatorname*{min}}}\\sum_{x\\in\\mathcal{X}}\\sum_{a\\in\\mathcal{A}_{x}}\\frac{\\mu_{1:}(x,a)}{\\mu_{1:}^{s}(x,a)}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We want to show that, with probability at least $1-\\delta$ , that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{t}\\left\\langle\\ell^{t}-\\widehat\\ell^{t},\\mu_{1:}^{t}-\\mu_{1:}\\right\\rangle\\leq4\\sqrt{\\iota H\\kappa(\\mu^{s})T}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds for all $\\mu\\in\\Pi_{\\operatorname*{min}}$ . Then the property follows after re-organizing the inequality and maximizing over $\\mu$ . In order to do so, we divide this term into two parts: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\langle\\ell^{t}-\\widehat{\\ell}^{t},\\mu_{1:}^{t}-\\mu_{1:}\\right\\rangle=\\underbrace{\\sum_{t=1}^{T}\\left\\langle\\widehat{\\ell}^{t}-\\ell^{t},\\mu_{1:}\\right\\rangle}_{\\mathrm{ESTI}}+\\underbrace{\\sum_{t=1}^{T}\\left\\langle\\ell^{t}-\\widehat{\\ell}^{t},\\mu_{1:}^{t}\\right\\rangle}_{\\mathrm{ESTI}}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We will furthermore assume that $H T\\,\\geq\\,\\iota\\kappa(\\mu^{s})$ , as otherwise, $4{\\sqrt{\\iota H\\kappa(\\mu^{s})T}}\\,\\leq\\,4H T$ and the property immediately follows from $\\Re_{\\operatorname*{min}}^{T}\\leq H T$ ", "page_idx": 15}, {"type": "text", "text": "Upper bound of EST $I$ For all $x\\in\\mathscr{X}$ of depth $h$ and $a\\in A(x)$ , we apply Lemma B.1 to the random process ", "page_idx": 15}, {"type": "equation", "text": "$$\nU_{x,a}^{t}=\\ell_{h}^{t}\\mathbb{I}_{\\{x=x_{h}^{t},a=a_{h}^{t}\\}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $\\delta^{\\prime}=\\delta/(A X+1)$ and a fixed $\\gamma_{1}\\in(0,1]$ we will specify later. This yields, with a probability at least $1-\\delta^{\\prime}$ , that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{=1}^{T}\\left(\\ell_{h}^{t}\\mathbb{I}_{\\left\\{x=x_{h}^{t},a=a_{h}^{t}\\right\\}}-\\mathbb{E}\\left[\\ell_{h}^{t}\\mathbb{I}_{\\left\\{x=x_{h}^{t},a=a_{h}^{t}\\right\\}}\\bigg|\\mathcal{F}^{t-1}\\right]\\right)\\leq\\gamma_{1}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left(\\ell_{h}^{t}\\right)^{2}\\mathbb{I}_{\\left\\{x=x_{h}^{t},a=a_{h}^{t}\\right\\}}\\bigg|\\mathcal{F}^{t-1}\\right]+\\frac{t}{\\gamma_{1}}}}\\\\ &{}&{\\leq\\gamma_{1}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\ell_{h}^{t}\\mathbb{I}_{\\left\\{x=x_{h}^{t},a=a_{h}^{t}\\right\\}}\\bigg|\\mathcal{F}^{t-1}\\right]+\\frac{t}{\\gamma_{1}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By definition of the estimated loss, $\\ell_{h}^{t}\\mathbb{I}_{\\left\\{x=x_{h}^{t},a=a_{h}^{t}\\right\\}}/\\mu_{1:}^{s}(x,a)\\;=\\;\\widehat{\\ell}^{t}(x,a)$ .We thus divide by $\\mu_{1:}^{s}(x,a)$ both sides of the inequality, and the unbiasedness of the loss estimator yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left[\\widehat{\\ell}^{t}(x,a)-\\ell^{t}(x,a)\\right]\\leq\\gamma_{1}\\sum_{t=1}^{T}\\ell^{t}(x,a)+\\frac{\\iota}{\\gamma_{1}\\mu_{1}^{s}:(x,a)}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This inequality holds for all $(x,a)$ with a probability of at least $1-\\delta A_{\\mathcal{X}}/(A_{\\mathcal{X}}+1)$ . Taking the scalar product with any $\\mu\\in\\Pi_{\\operatorname*{min}}$ then gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\Bigl\\langle\\widehat{\\ell}^{t}-\\ell^{t},\\mu_{1:}\\Bigr\\rangle\\le\\gamma_{1}\\sum_{t=1}^{T}\\bigl\\langle\\ell^{t},\\mu_{1:}\\bigr\\rangle+\\frac{1}{\\gamma_{1}}\\sum_{x\\in\\mathcal{X}}\\sum_{a\\in\\mathcal{A}(x)}\\frac{\\mu_{1:}(x,a)}{\\mu_{1:}^{s}(x,a)}}\\quad}&{}\\\\ &{\\qquad\\qquad\\qquad\\le\\gamma_{1}H T+\\frac{\\iota}{\\gamma_{1}}\\kappa(\\mu^{s})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using $\\gamma_{1}=\\sqrt{\\iota\\kappa(\\mu^{s})/(H T)}\\le1$ (by assumption), finally yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{EST}1\\leq2\\sqrt{\\iota H\\kappa(\\mu^{s})T}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Upper bound of EST II For this upper bound, we apply Lemma B.1 directly to the sequence $U^{t}=$ $\\left<-\\widehat{\\ell}^{t},\\mu_{1:}^{t}\\right>$ Wenowchoose $\\gamma_{2}\\in\\mathbb{R}_{+}$ (no further assumption is needed on $\\gamma_{2}$ as the sequence is negative) and apply the lemma to get with probability at least $1-\\delta/(A_{\\mathcal{X}}+1)$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sum_{t=1}^{T}\\left(e^{t}-\\bar{\\sigma}_{t}\\mu_{t}^{*}\\right)\\leq\\eta\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\langle\\sigma_{t}\\mu_{t}^{*}\\right\\rangle\\right\\|^{2}e^{t-\\frac{1}{2}}\\right]+\\frac{\\lambda}{\\eta}}\\\\ {=\\eta\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\langle\\sum_{t=1}^{T}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\\Delta)}\\left\\vert\\sigma_{t,\\sigma}(\\mu_{t,\\sigma}(\\Delta))\\right\\vert^{2}\\right]^{-1}+\\frac{\\lambda}{\\eta}}\\\\ {\\leq m a k\\sum_{t=1}^{T}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\\Delta)}\\left\\vert e^{\\frac{\\lambda_{t}}{2}}\\sum_{t=1}^{T}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\\Delta)}\\left\\vert\\sigma_{t,\\sigma}^{*}\\right\\vert^{2}\\left\\vert\\left\\vert\\sigma_{t,\\sigma}^{*}\\right\\vert\\right\\vert^{2}}\\\\ {\\leq m\\beta\\sum_{t=1}^{T}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\\Delta)}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\\Delta)}\\left\\vert\\sigma_{t,\\sigma}^{*}\\right\\vert^{2}\\left\\vert\\left\\vert\\sigma_{t,\\sigma}^{*}\\right\\vert\\right\\vert^{2}}\\\\ {=\\eta\\sum_{t=1}^{T}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\\Delta)}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\\Delta)}\\left\\vert\\sigma_{t,\\sigma}^{*}\\right\\vert^{2}\\left\\vert\\sigma_{t,\\sigma}^{*}\\right\\vert\\right\\vert^{2}\\bigg]+\\frac{\\lambda}{\\eta}}\\\\ {=\\eta\\sum_{t=1}^{T}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\\Delta)}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\\Delta)}\\left\\vert\\sigma_{t,\\sigma}^{*}\\right\\vert^{2}\\left\\vert\\sigma_{t,\\sigma}^{*}\\right\\vert^{2}+\\frac{\\lambda}{\\eta}}\\\\ {=\\eta\\sum_{t=1}^{T}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\\Delta)}\\rho_{t}^{*}(\\Delta)_{t}^{2}\\left\\vert\\sigma_{t,\\sigma}^{*}\\right\\vert^{2}+\\frac{\\lambda}{\\eta}}\\\\ {=\\eta\\sum_{t=1}^{T}\\sum_{\\sigma\\in\\sigma_{t,\\sigma}(\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking 2 = $\\gamma_{2}=\\sqrt{\\frac{\\iota}{H\\kappa(\\mu^{\\mathrm{s}})T}}$ then leads to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\langle\\ell^{t}-\\widehat\\ell^{t},\\mu_{1:}^{t}\\right\\rangle\\leq2\\sqrt{\\iota H\\kappa(\\mu^{s})T}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing the two inequalities yields the inequality of the theorem with a probability of at least $1-\\delta$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C Balanced policy and $\\kappa$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section deals with the $\\kappa(\\mu^{s})$ and local $\\kappa(\\mu^{s}|{\\boldsymbol{x}})$ of the main paper, and links it to the balanced policy $\\mu^{\\star}$ ", "page_idx": 16}, {"type": "text", "text": "Recursive $\\kappa$ computation Let $\\mu^{s}$ be the positive sample policy. For any $\\mu\\in\\Pi_{\\operatorname*{min}}$ and $x\\in\\mathscr{X}$ of depth $h$ , we define $\\kappa_{\\mu}(\\mu^{s}|x)$ the local sum of ratios against $\\mu$ in the subtree induced by $x$ , i.e. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\kappa_{\\mu}(\\mu^{s}|x):=\\sum_{x^{\\prime}\\in\\mathcal{X},x\\mathrm{~is~in~the~history~of~}x^{\\prime}}\\sum_{a^{\\prime}\\in A(x^{\\prime})}\\frac{\\mu_{h:}(x^{\\prime},a^{\\prime})}{\\mu_{h:}^{s}(x^{\\prime},a^{\\prime})}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where, if $(x_{1}^{\\prime},a_{1}^{\\prime}...,x_{h^{\\prime}}^{\\prime},a^{\\prime})$ is the history of $(x^{\\prime},a^{\\prime})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{h:}(x^{\\prime},a^{\\prime}):=\\Pi_{i=h}^{h^{\\prime}}\\,\\mu(a_{i}^{\\prime}|x_{i}^{\\prime})\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We then formally define $\\kappa(\\mu^{s}|{\\boldsymbol{x}})$ as $\\kappa(\\mu^{s}\\vert x)\\;:=\\;\\operatorname*{max}_{\\mu\\in\\Pi_{\\mathrm{min}}}\\kappa_{\\mu}(\\mu^{s}\\vert x)$ . For any $\\mu\\,\\in\\,\\Pi_{\\operatorname*{min}}$ ,the following recursive formula stands ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\kappa_{\\mu}(\\mu^{s}|x)=\\sum_{a\\in A(x)}\\frac{\\mu(a|x)}{\\mu^{s}(a|x)}\\left(1+\\sum_{x^{\\prime}\\in\\mathcal{X},x^{\\prime}\\mathrm{~directly}\\mathrm{~follows}\\;(x,a)}\\kappa_{\\mu}(\\mu^{s}|x^{\\prime})\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "that follows from the definition of $\\kappa_{\\mu}(\\mu^{s}|x)$ . The same kind of recursion can then be obtained for $\\kappa(\\mu^{s}|{\\boldsymbol{x}})$ , because each appearance of $\\mu$ in the previous equality can be maximized independently (depending on different information sets). This yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\kappa(\\mu^{s}|x)=\\displaystyle\\operatorname*{max}_{\\mu\\in\\Delta_{A(x)}}\\sum_{a\\in A(x)}\\displaystyle\\frac{\\mu(a)}{\\mu^{s}(a|x)}\\left(1+\\sum_{x^{\\prime}\\in\\mathcal{X},x^{\\prime}\\mathrm{~directly~follows~}(x,a)}\\kappa(\\mu^{s}|x^{\\prime})\\right)}\\\\ &{\\quad=\\displaystyle\\operatorname*{max}_{a\\in A(x)}\\displaystyle\\frac{1}{\\mu^{s}(a|x)}\\left(1+\\sum_{x^{\\prime}\\in\\mathcal{X},x^{\\prime}\\mathrm{~directly~follows~}(x,a)}\\kappa(\\mu^{s}|x^{\\prime})\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which allows for a simple recursive computation of $\\kappa(\\mu^{s}|{\\boldsymbol{x}})$ . Finally, once the whole recursive computation is done, $\\kappa(\\mu^{s})$ itself can be computed by, defining $\\chi_{1}$ the information sets of depth 1, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\kappa(\\mu^{s})=\\sum_{x_{1}\\in\\mathcal{X}_{1}}\\kappa(\\mu^{s}|x_{1})\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Balanced policy $\\kappa(\\mu^{s}|{\\boldsymbol{x}})$ can also be minimized over $\\mu^{s}\\in\\Pi_{\\operatorname*{min}}$ recursively from the leaves using the tree structure. Indeed, for each $x\\in\\mathscr{X}$ , assuming that the minimizers of $\\kappa(\\mu^{s}|\\boldsymbol{x}^{\\prime})$ are already known for subsequent $x^{\\prime}$ , the policy $\\mu^{s}\\in\\Delta_{\\mathcal{A}(x)}$ that minimizes the maximum along the actions $a\\in A(x)$ can be computed from (1). Furthermore, if we define $A^{\\tau}(x,a)$ and $A^{\\tau}(x)$ the total number of actions in the subtrees respectively induced by $(x,a)$ and $x$ , i.e. ", "page_idx": 17}, {"type": "equation", "text": "$$\nA^{\\tau}(x,a):=1\\sum_{\\substack{x^{\\prime}\\in\\mathcal{X},(x,a)\\mathrm{~is~in~the~history~of~}x^{\\prime}}}|A(x^{\\prime})|\\quad\\mathrm{and}\\quad A^{\\tau}(x):=\\sum_{a\\in A(x)}A^{\\tau}(x,a)\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we can show that $\\begin{array}{r}{\\operatorname*{min}_{\\mu^{s}\\in\\Pi_{\\operatorname*{min}}}\\kappa(\\mu^{s}|x)=A^{\\tau}(x)}\\end{array}$ , and that the minimum is attained by the balanced policy $\\mu^{\\star}$ defined by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu^{\\star}(a|x):=\\frac{A^{\\tau}(x,a)}{A^{\\tau}(x)}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Indeed, if we assume in (1) that the previous property holds for the $\\kappa(\\mu^{s}|\\boldsymbol{x}^{\\prime})$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\kappa(\\mu^{s}|x)=\\operatorname*{max}_{a\\in A(x)}\\frac{1}{\\mu^{s}(a|x)}\\left(1+\\sum_{\\substack{x^{\\prime}\\in\\mathcal{X},x^{\\prime}\\mathrm{\\:directly\\:follows}\\;(x,a)}}A^{\\tau}(x^{\\prime})\\right)\\,=\\operatorname*{max}_{a\\in A(x)}\\frac{A^{\\tau}(x,a)}{\\mu^{s}(a|x)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the previous equality is minimized when the $\\mu^{s}(a|x)$ are proportional to the $A^{\\tau}(x,a)$ , achieved by the balanced policy $\\mu^{\\star}$ .With this policy, the same equality gives $\\kappa(\\mu^{\\star}|x)\\,=\\,A^{\\tau}(x)$ which concludes the induction. ", "page_idx": 17}, {"type": "text", "text": "Finally, computing $\\kappa(\\mu^{\\star})$ yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\kappa(\\mu^{\\star})=\\sum_{x_{1}\\in\\mathcal{X}_{1}}\\kappa(\\mu^{\\star}|x_{1})=\\sum_{x_{1}\\in\\mathcal{X}_{1}}A^{\\tau}(x_{1})=A_{\\mathcal{X}}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D  Generalized dual stabilized online mirror descent ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section will establish the bound related to the updates (GDS-OMD) obtained with any Legendre function. ", "page_idx": 17}, {"type": "text", "text": "D.1  General Bregman divergence properties ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We start this section by stating multiple properties of the Bregman divergence $\\mathbf{D}_{\\Psi}$ for $\\Psi$ aconvex function, continuously differentiable on an open $\\Omega$ and defined on $\\overline{\\Omega}$ , proved by Cesa-Bianchi & Lugosi (2006). ", "page_idx": 17}, {"type": "text", "text": "Law of cosines : For any $x\\in{\\overline{{\\Omega}}}$ and $w,z\\in\\Omega$ , the following equality holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ensuremath{\\mathbf{D}}_{\\Psi}(x,w)=\\ensuremath{\\mathbf{D}}_{\\Psi}(x,z)+\\ensuremath{\\mathbf{D}}_{\\Psi}(z,w)-\\ensuremath{\\left\\langle\\nabla\\Psi(w)-\\nabla\\Psi(z),x-z\\right\\rangle}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "1: Input: A sequence of dual increments $\\xi^{t}$ An open subset $\\Omega\\in\\mathbb{R}^{n}$ and a closed convex $\\mathcal{C}$ of $\\overline{\\Omega}$ A sequence of Legendre regularizers $(\\Psi^{t})_{t\\in[T]}$ on $\\overline{\\Omega}$ such that for all $t\\in[T]$ \uff0c $\\Psi^{t+1}-\\Psi^{t}$ is convex An initial primal iterate $w^{1}\\in\\mathcal{C}$   \n2: Output: A sequence $(w^{t})_{t\\in[T]}$ of primal iterates   \n3: Algorithm: For $t=1$ to $T$ $\\begin{array}{r l}&{\\mathbf{\\Phi}_{\\mathcal{L}}^{1}=\\mathbf{\\Phi}^{\\mathbf{\\alpha}_{1}}\\mathbf{\\Phi}(\\mathbf{\\Phi}w^{t})}\\\\ &{\\mathbf{\\Phi}_{\\mathcal{L}}^{2}=\\nabla\\Psi^{t}(w^{t})}\\\\ &{y^{t+1}=z^{t}-\\xi^{t}+\\nabla\\Psi^{t+1}(w_{1})-\\nabla\\Psi^{t}(w^{1})}\\\\ &{\\hat{w}^{t+1}=\\nabla\\Psi^{t+1,\\star}(y^{t+1})}\\\\ &{w^{t+1}=\\Pi_{\\mathcal{L}}^{\\Psi^{t+1}}(\\hat{w}^{t+1})}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Bregman projection : For $\\mathcal{C}$ a closed convex of $\\overline{\\Omega}$ , and $\\Psi$ strictly convex, we can define the Bregman projection $\\Pi_{\\mathcal{C}}^{\\Psi}$ over $\\overline{\\Omega}$ by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{C}}^{\\Psi}(w)=\\underset{z\\in\\mathcal{C}}{\\arg\\operatorname*{min}}\\,\\mathbf{D}_{\\Psi}(z,w)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This Bregman projection satisfies a generalized Pythagorean inequality, for $w\\in\\Omega$ and $z\\in{\\mathcal{C}}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\ensuremath{\\mathbf{D}}_{\\Psi}(z,w)\\geq\\ensuremath{\\mathbf{D}}_{\\Psi}(z,\\Pi_{\\mathcal{C}}^{\\Psi}(w))+\\ensuremath{\\mathbf{D}}_{\\Psi}(\\Pi_{\\mathcal{C}}^{\\Psi}(w),w)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Fenchel dual : We defined the Fenchel dual $\\Psi^{\\star}$ of a Legendre function $\\Psi$ for any $\\xi\\in\\mathbb{R}^{n}$ by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Psi^{\\star}(\\xi)=\\operatorname*{sup}_{w\\in\\overline{{\\Omega}}}\\left\\langle\\xi,w\\right\\rangle-\\Psi(w)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If we consider $\\Omega^{\\star}:=\\nabla\\Psi(\\Omega)$ , it can be shown that $\\nabla\\Psi^{\\star}$ is the inverse function of $\\nabla\\Psi$ over $\\Omega^{\\star}$ ,i.e. for any $w\\in\\Omega$ \uff0c $\\nabla\\Psi^{\\star}(\\nabla\\Psi(w))=w$ . Furthermore, for $w,z\\in\\Omega$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ensuremath{\\mathbf{D}}_{\\Psi}(w,z)=\\ensuremath{\\mathbf{D}}_{\\Psi^{\\star}}(\\nabla\\Psi^{\\star}(z),\\nabla\\Psi^{\\star}(y))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Strong convexity: $\\Psi$ is said to be 1-strongly convex with respect to a norm $\\lVert\\cdot\\rVert$ if for all $w,z\\in\\Omega$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Psi(z)\\geq\\Psi(w)+\\langle\\nabla\\Psi(w),z-w\\rangle+\\frac{1}{2}\\|w-z\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In this case, the Bregman divergence of the Fenchel dual $\\Psi^{\\star}$ satisfies for any $\\xi_{1},\\xi_{2}\\in\\Omega^{\\star}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{D}_{\\Psi^{\\star}}(\\xi_{1},\\xi_{2})\\leq\\left\\lVert\\xi_{1}-\\xi_{2}\\right\\rVert_{\\star}^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\left\\|\\cdot\\right\\|_{\\star}$ is the dual norm of $\\lVert\\cdot\\rVert$ ", "page_idx": 18}, {"type": "text", "text": "D.2 GDS-OMD Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We will assume in the following parts that the updates of the following algorithm are properly defined, which happens when all vectors $y^{t+1}$ belong to the Fenchel dual space $\\Omega^{t+1,\\star}:=\\dot{\\nabla}\\dot{\\Psi}^{t+1}(\\Omega)$ We make the same assumption on the regular OMD iterates $z^{t}-\\xi^{t}$ ", "page_idx": 18}, {"type": "text", "text": "We start by giving an equivalent formulation of the updates (GDS-OMD) through Algorithm 3. ", "page_idx": 18}, {"type": "text", "text": "Proposition D.1. Algorithm $\\boldsymbol{\\beta}$ computes the updates (GDS-OMD) if they are properly defined, i.e. computes the sequence of primal iterates defined by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\boldsymbol{w}^{t+1}=\\operatorname*{arg\\,min}_{\\boldsymbol{w}\\in\\mathcal{C}}\\left(\\boldsymbol{\\xi}^{t},\\boldsymbol{w}\\right)+\\mathbf{D}_{\\Psi^{t}}\\left(\\boldsymbol{w},\\boldsymbol{w}^{t}\\right)+\\mathbf{D}_{\\Psi^{t+1}-\\Psi^{t}}\\left(\\boldsymbol{w},\\boldsymbol{w}^{1}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. By definition of $\\hat{w}^{t+1}$ in Algorithm 3, we have for all iterations $t\\in[T]$ and $w\\in\\mathcal{C}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf D_{\\Psi^{t+1}}(w,\\hat{w}^{t+1})=\\Psi^{t+1}(w)-\\left\\langle\\nabla\\Psi^{t+1}(\\hat{w}^{t+1}),w\\right\\rangle+C_{1}}\\\\ &{\\qquad\\qquad\\qquad=\\Psi^{t}(w)+\\left(\\Psi^{t+1}(w)-\\Psi^{t}(w)\\right)-\\left\\langle y^{t+1},w\\right\\rangle+C_{1}}\\\\ &{\\qquad\\qquad\\qquad=\\left\\langle\\xi^{t},w\\right\\rangle+\\left(\\Psi^{t}(w)-\\left\\langle\\nabla\\Psi^{t}(w^{t}),w\\right\\rangle\\right)+}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left(\\Psi^{t+1}(w)-\\Psi^{t}(w)-\\left\\langle\\nabla\\Psi^{t+1}(w^{1})-\\nabla\\Psi^{t}(w^{1}),w\\right\\rangle\\right)+C_{1}}\\\\ &{\\qquad\\qquad\\qquad=\\left\\langle\\xi^{t},w\\right\\rangle+\\mathbf D_{\\Psi^{t}}\\left(w,w^{t}\\right)+\\mathbf D_{\\Psi^{t+1}-\\Psi^{t}}\\left(w,w^{1}\\right)+C_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{1}$ and $C_{2}$ are constants independent of the choice of $w$ (but not independent of the other variables). As $\\boldsymbol{w}^{t+1}\\,=\\,\\arg\\operatorname*{min}_{\\boldsymbol{w}\\in\\mathcal{\\bar{C}}}\\mathbf{\\bar{D}}_{\\Psi^{t+1}}\\big(\\boldsymbol{w},\\hat{\\boldsymbol{w}}^{t+1}\\big)$ theupdatsoflgoritmcoincidewih the updates (GDS-OMD), as both minimize the same function at each iteration up to an additive constant. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "The updates of Algorithm 3 are then used to show Theorem 3.2 below. Compared to the ones of McMahan (2017) that also allow adaptive regularization, these updates do not suffer from the potential linear rates observed by Orabona & Pal (2018). ", "page_idx": 19}, {"type": "text", "text": "Theorem D.2. Let $(w^{t})_{t\\in[T]}$ be a sequence of primal iterates generated by the updates (GDS-OMD), with convex incremental functions. Then for any $w^{\\dagger}\\in{\\overline{{\\Omega}}}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\langle\\xi^{t},w^{t}-w^{\\dagger}\\right\\rangle\\leq\\mathbf{D}_{\\Psi^{T}}(w^{\\dagger},w^{1})+\\sum_{t=1}^{T}\\mathbf{D}_{\\Psi^{t},\\star}\\left(\\nabla\\Psi^{t}(w^{t})-\\xi^{t},\\nabla\\Psi^{t}(w^{t})\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We can assume, without any incidence on the $(w^{t})_{t\\in[T]}$ sequence, that $\\Psi^{T+1}=\\Psi^{T}$ .We also define for all $t\\in[T]$ the notations $\\varphi^{t}=\\Psi^{t+1}-\\Psi^{t}$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{q}^{t}=\\left\\langle\\xi^{t},\\hat{w}^{t+1}\\right\\rangle+\\mathbf{D}_{\\Psi^{t}}(\\hat{w}^{t+1},w^{t})+\\mathbf{D}_{\\varphi^{t}}(\\hat{w}^{t+1},w^{1})\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We then divide the sum into a stability and a penalty terms: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\langle\\xi^{t},w^{t}-w^{\\dagger}\\right\\rangle=\\underbrace{\\sum_{t=1}^{T}\\left(\\hat{q}^{t}-\\left\\langle\\xi^{t},w^{\\dagger}\\right\\rangle\\right)}_{\\mathrm{penalty}}+\\underbrace{\\sum_{t=1}^{T}\\left(\\left\\langle\\xi^{t},w^{t}\\right\\rangle-\\hat{q}^{t}\\right)}_{\\mathrm{stability}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and we look at upper-bounding these two terms. ", "page_idx": 19}, {"type": "text", "text": "Penalty term: For all $t\\in[T]$ , using the law of cosines on the Bregman divergences of $\\Psi^{t}$ and $\\varphi^{t}$ we have the two equalities: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{D}_{\\Psi^{t}}(w^{\\dagger},w^{t})=\\mathbf{D}_{\\Psi^{t}}(w^{\\dagger},\\hat{w}^{t+1})+\\mathbf{D}_{\\Psi^{t}}(\\hat{w}^{t+1},w^{t})-\\left\\langle\\nabla\\Psi^{t}(w^{t})-\\nabla\\Psi^{t}(\\hat{w}^{t+1}),w^{\\dagger}-\\hat{w}^{t+1}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{D}_{\\varphi^{t}}(w^{\\dagger},w^{1})=\\mathbf{D}_{\\varphi^{t}}(w^{\\dagger},\\hat{w}^{t+1})+\\mathbf{D}_{\\varphi^{t}}(\\hat{w}^{t+1},w^{1})-\\left\\langle\\nabla\\varphi^{t}(w^{1})-\\nabla\\varphi^{t}(\\hat{w}^{t+1}),w^{\\dagger}-\\hat{w}^{t+1}\\right\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Summing these two equalities, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}_{\\Psi^{t}}(w^{\\dagger},w^{t})+\\mathbf{D}_{\\varphi^{t}}(w^{\\dagger},w^{1})}\\\\ &{\\qquad=\\mathbf{D}_{\\Psi^{t+1}}(w^{\\dagger},\\hat{w}^{t+1})+\\mathbf{D}_{\\Psi^{t}}(\\hat{w}^{t+1},w^{t})+\\mathbf{D}_{\\varphi^{t}}(\\hat{w}^{t+1},w^{1})-\\big\\langle\\xi^{t},w^{\\dagger}-\\hat{w}^{t+1}\\big\\rangle}\\\\ &{\\qquad=\\mathbf{D}_{\\Psi^{t+1}}(w^{\\dagger},\\hat{w}^{t+1})+\\hat{q}^{t}-\\big\\langle\\xi^{t},w^{\\dagger}\\big\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as by definition of $\\hat{w}^{t+1}$ and $y^{t+1}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla\\Psi^{t+1}(\\hat{w}^{t+1})=y^{t+1}=-\\xi_{t}+\\nabla\\Psi^{t}(w^{t})+\\nabla\\varphi^{t}(w^{1})\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Furthermore, as $w^{t+1}=\\Pi_{C}^{t+1}(\\hat{w}^{t+1})$ , the Pythagorean inequality for the Bregman divergence yields that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{D}_{\\Psi^{t+1}}(w^{\\dagger},\\hat{w}^{t+1})\\ge\\mathbf{D}_{\\Psi^{t+1}}(w^{\\dagger},w^{t+1})+\\mathbf{D}_{\\Psi^{t+1}}(w^{t+1},\\hat{w}^{t+1})\\ge\\mathbf{D}_{\\Psi^{t+1}}(w^{\\dagger},w^{t+1})\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Injecting this in the previous equality and telescoping leads to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left(\\hat{q}^{t}-\\left\\langle\\xi^{t},w^{\\dagger}\\right\\rangle\\right)=\\displaystyle\\sum_{t=1}^{T}\\left(\\mathbf{D}_{\\Psi^{t}}(w^{\\dagger},w^{t})+\\mathbf{D}_{\\varphi^{t}}(w^{\\dagger},w^{1})-\\mathbf{D}_{\\Psi^{t+1}}(w^{\\dagger},\\hat{w}^{t+1})\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\sum_{t=1}^{T}\\left(\\mathbf{D}_{\\Psi^{t}}(w^{\\dagger},w^{t})+\\mathbf{D}_{\\varphi^{t}}(w^{\\dagger},w^{1})-\\mathbf{D}_{\\Psi^{t+1}}(w^{\\dagger},w^{t+1})\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbf{D}_{\\Psi^{T+1}}(w^{\\dagger},w^{1})-\\mathbf{D}_{\\Psi^{T+1}}(w^{\\dagger},w^{t+1})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathbf{D}_{\\Psi^{T}}(w^{\\dagger},w^{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as $\\Psi^{T}=\\Psi^{T+1}$ by definition. ", "page_idx": 20}, {"type": "text", "text": "Stability term: We first notice, for all $t\\in[T]$ , that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\xi^{t},w^{t}\\right\\rangle-\\hat{q}^{t}=\\left\\langle\\xi^{t},w^{t}-\\hat{w}^{t+1}\\right\\rangle-\\mathbf{D}_{\\Psi^{t}}(\\hat{w}^{t+1},w^{t})-\\mathbf{D}_{\\varphi^{t}}(\\hat{w}^{t+1},w^{1})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\langle\\xi^{t},w^{t}-\\hat{w}^{t+1}\\right\\rangle-\\mathbf{D}_{\\Psi^{t}}(\\hat{w}^{t+1},w^{t})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\langle\\xi^{t},w^{t}-\\tilde{w}^{t+1}\\right\\rangle-\\mathbf{D}_{\\Psi^{t}}(\\hat{w}^{t+1},w^{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{w}^{t+1}:=\\operatorname*{arg\\,min}_{\\tilde{w}\\in\\Omega}\\left[\\left\\langle\\xi^{t},\\tilde{w}\\right\\rangle+\\mathbf{D}_{\\Psi^{t}}\\big(\\tilde{w},w^{t}\\big)\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "isthe $\\tilde{w}^{t+1}$ iterate that would be obtained using a classical OMD step with $\\Psi^{t}$ , without the stabilization. By optimality, it verifies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla\\Psi^{t}(\\tilde{w}^{t+1})=\\nabla\\Psi^{t}(w^{t})-\\xi^{t}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the law of cosines then yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}_{\\Psi^{t}}(w^{t},w^{t})=\\mathbf{D}_{\\Psi^{t}}(w^{t},\\tilde{w}^{t+1})+\\mathbf{D}_{\\Psi^{t}}(\\tilde{w}^{t+1},w^{t})-\\left\\langle\\nabla\\Psi^{t}(w^{t})-\\nabla\\Psi^{t}(\\tilde{w}^{t+1}),w^{t}-\\tilde{w}^{t+1}\\right\\rangle}\\\\ &{\\qquad\\qquad(0)=\\mathbf{D}_{\\Psi^{t}}(w^{t},\\tilde{w}^{t+1})+\\mathbf{D}_{\\Psi^{t}}(\\tilde{w}^{t+1},w^{t})-\\left\\langle\\xi^{t},w^{t}-\\tilde{w}^{t+1}\\right\\rangle\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging this in the first inequality, we directly get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\langle\\xi^{t},w^{t}\\right\\rangle-\\hat{q}^{t}\\leq\\mathbf{D}_{\\Psi^{t}}(w^{t},\\tilde{w}^{t+1})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and we conclude using ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}_{\\Psi^{t}}(w^{t},\\tilde{w}^{t+1})=\\mathbf{D}_{\\Psi^{t,\\star}}(\\nabla\\Psi^{t}(\\tilde{w}^{t+1}),\\nabla\\Psi^{t}(w^{t}))}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{D}_{\\Psi^{t,\\star}}(\\nabla\\Psi^{t}(w^{t})-\\xi^{t},\\nabla\\Psi^{t}(w^{t}))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "E  LocalOMD analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section will focus on the dilated entropy approach to extensive-form games, and especially on theupdates ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mu^{t+1}=\\operatorname*{arg\\,min}_{\\mu\\in\\Pi_{\\operatorname*{min}}}\\left\\langle\\widehat{\\ell}^{t},\\mu_{1:}\\right\\rangle+\\mathbf{D}_{\\alpha^{t}}^{\\mathrm{dil}}(\\mu,\\mu^{t})+\\mathbf{D}_{\\alpha^{t+1}-\\alpha^{t}}^{\\mathrm{dil}}(\\mu,\\mu^{1})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "that are used by LocalOMD. ", "page_idx": 20}, {"type": "text", "text": "E.1 General analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The following proposition shows that each update of this form can be computed recursively starting from the leaves of the tree. It requires for any $t\\in[T]$ the vector $q^{t}$ that satisfies for any $x\\in\\mathscr{X}$ of depth $h$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nq^{t}(x)=\\operatorname*{min}_{\\mu\\in\\Pi_{\\mathrm{min}}}\\left\\langle\\widehat{\\ell}^{i,\\to x},\\mu_{h:}^{\\to x}\\right\\rangle+\\mathbf{D}_{\\alpha^{t}}^{\\mathrm{dil},\\to\\mathrm{x}}(\\mu,\\mu^{t})+\\mathbf{D}_{\\alpha^{t+1}-\\alpha^{t}}^{\\mathrm{dil},\\to\\mathrm{x}}(\\mu,\\mu^{1})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\rightarrow x$ means that the quantity is considered on the sub-tree induced by $x$ rather than the full information set tree, and $\\mu_{h}$ : is defined in Appendix $\\textrm{C}$ ", "page_idx": 20}, {"type": "text", "text": "Proposition E.1. Consider the previous updates (GDS-OMD dilated) and the vectors $(q^{t})_{t\\in[T]}$ above.Both $\\mu^{t+1}$ and $q^{t}$ can be computed recursively starting from the leaves of the tree through ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu^{t+1}=\\arg\\operatorname*{min}_{\\mu\\in\\Delta_{\\mathcal{A}(x)}}h_{x}^{t}(\\mu)\\quad a n d\\quad q^{t}(x)=\\operatorname*{min}_{\\mu\\in\\Delta_{\\mathcal{A}(x)}}h_{x}^{t}(\\mu)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\nh_{x}^{t}(\\mu)=\\left\\langle\\widetilde{\\ell}^{t}(x,\\cdot),\\mu\\right\\rangle+(1/\\alpha^{t}(x))\\,\\mathbf{D}_{x}(\\mu,\\mu^{t}(\\cdot|x))+\\left(1/\\alpha^{t+1}(x)-1/\\alpha^{t}(x)\\right)\\,\\mathbf{D}_{x}(\\mu,\\mu^{1}(\\cdot|x))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the regularized loss $\\widetilde{\\ell}^{t}(x,a)$ is defined by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}^{t}(x,a):=\\widehat{\\ell}^{t}(x,a)\\+\\sum_{x^{\\prime}\\in\\mathcal{X}|x^{\\prime}\\atop x^{\\prime}\\in\\mathcal{X}|x^{\\prime}\\,d i r e c t l y\\,f o l l o w s\\,(x,a)}q^{t}(x^{\\prime})\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. First, note that $\\mu^{t+1}$ is the unique minimizer associated to each $q^{t}(x_{1})$ for $x_{1}$ the information set of depth 1. Indeed, each of the sub-tree induced by the $x_{1}$ can be considered as an independent problem. The idea will be to recursively minimize the $q^{t}(x)$ , starting from the leaves (i.e. the final information sets $x_{H}$ ), and compute $\\mu^{t+\\bar{1}}(|x)$ as the associated minimizer at each information set. ", "page_idx": 21}, {"type": "text", "text": "This minimization is done through, at each $x\\in\\mathscr{X}$ of depth $h$ , with a decomposition of $q^{t}(x)$ . Indeed, separating the induced tree by $x$ between the root and the rest of the tree leads to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta^{t}(x)=\\frac{\\arg\\operatorname*{min}}{\\mu\\in\\operatorname*{max}_{0}}\\left\\langle\\hat{\\mathcal{E}}^{(x,\\cdot)},\\mu(\\cdot|x)\\right\\rangle+(1/\\alpha^{t}(x))\\mathbf{D}_{x}\\left\\langle\\mu(\\cdot|x),\\mu^{t}(\\cdot|x)\\right\\rangle}\\\\ &{\\qquad+\\left(1/\\alpha^{t+1}(x)-1/\\alpha^{t}(x)\\right)\\mathbf{D}_{x}\\left\\langle\\mu(\\cdot|x),\\mu^{t}(\\cdot|x)\\right\\rangle}\\\\ &{\\qquad+\\underbrace{\\sum_{\\substack{\\mathbf{x}\\in\\mathcal{M}(x)}}\\underbrace{\\sum_{j=1}^{t}\\left(\\mu(x)-z^{\\prime}(x)\\right)}\\left[\\left\\langle\\hat{\\mathcal{E}}^{(\\hat{\\mu}_{-},\\cdot)\\,x^{\\prime}},\\mu\\hat{\\mathcal{E}}^{(\\hat{\\mu}_{-},\\cdot)}+\\mathbf{D}_{\\alpha^{t}}^{\\hat{\\mathbf{i}}_{\\parallel},-\\,\\times^{\\prime}}(\\mu,\\mu^{t})+\\mathbf{D}_{\\alpha^{t+1}-\\,\\infty}^{\\hat{\\mathbf{i}}_{\\parallel},\\parallel\\sim\\sim\\mathcal{E}^{\\prime}}\\right]}_{x\\in\\mathcal{N}_{x}\\times0}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\left\\langle\\hat{\\mathcal{E}}^{(\\hat{\\mu}_{-},\\cdot)\\,\\hat{\\mathcal{E}}^{(\\hat{\\mu}_{-},\\cdot)}}+(1/\\alpha^{t+1}(x)-1/\\alpha^{t}(x))\\right\\rangle\\mathbf{D}_{x}\\left\\langle\\mu,\\mu^{1}(\\cdot|x)\\right\\rangle}\\\\ &{\\qquad+\\underbrace{\\sum_{\\substack{\\mathbf{x}\\in\\mathcal{M}(x)}}\\left\\langle\\hat{\\mathcal{E}}^{(\\hat{\\mu}_{-},\\cdot)\\,\\mu}(\\cdot)+(1/\\alpha^{t}(x))\\mathbf{D}_{x}\\left\\langle\\mu,\\mu^{t}(\\cdot|x)\\right\\rangle+\\left(1/\\alpha^{t+1}(x)-1/\\alpha^{t}(x)\\right)\\mathbf{D}_{x}\\left\\langle\\mu,\\mu^{1}(\\cdot|x)\\right\\rangle}_{x\\in\\mathcal{M}(x)}}\\\\ &{\\qquad\\qquad+\\underbrace{\\sum_{\\substack{\\mathbf{x}\\in\\mathcal{M}(x)}}\\underbrace{\\sum_{j=1}^{t}\\left(\\mu(x)-z^{\\prime}(x)\\right)}_{x\\in\\mathcal{N}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as each minimization on $\\mu\\in\\Pi_{\\operatorname*{min}}$ is done on independent components. This justifies the recursive computation of both $\\mu^{t+1}$ and $q^{t}$ \u53e3 ", "page_idx": 21}, {"type": "text", "text": "This proposition directly provides the proof of correctness of LocalOMD, for which the regularized losses at time step $t$ are non-null only on the trajectory with ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}^{t}(x,a)=\\frac{\\mathbb{I}_{\\left\\{x=x_{h}^{t},a=a_{h}^{t}\\right\\}}}{\\mu_{1:}^{s}(x)}\\widetilde{\\ell}_{h}^{t}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We now want to upper(bound the regret associated with this sequence $\\mu^{t}$ . The following lemma gives a valuable property that links the regularized loss and the estimated loss. ", "page_idx": 21}, {"type": "text", "text": "Lemma E.2. For any policy $\\mu^{\\prime}\\in\\Pi_{\\operatorname*{min}}$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\langle\\widetilde{\\ell}^{t},\\mu_{1:}^{\\prime}\\right\\rangle-\\sum_{x\\in\\mathcal{X}}\\mu_{1:}^{\\prime}(x)q^{t}(x)=\\left\\langle\\widehat{\\ell}^{t},\\mu_{1:}^{\\prime}\\right\\rangle-\\hat{q}^{t}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$\\begin{array}{r}{\\hat{q}^{t}=\\operatorname*{min}_{\\mu\\in\\Pi_{\\operatorname*{min}}}\\left\\langle\\widehat{\\ell}^{t},\\mu_{1:}\\right\\rangle+\\mathcal{D}_{\\alpha^{t}}^{\\mathrm{dil}}(\\mu,\\mu^{t})+\\mathcal{D}_{\\alpha^{t+1}-\\alpha^{t}}^{\\mathrm{dil}}(\\mu,\\mu^{1})}\\end{array}$ ", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. By definition of $\\widetilde{\\ell}^{t}$ we have, for any $\\mu\\in\\Pi_{\\operatorname*{min}}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left<\\widetilde{\\ell}^{t},\\mu_{1:}^{\\prime}\\right>=\\left<\\widehat{\\ell}^{t},\\mu_{1:}^{\\prime}\\right>+\\displaystyle\\sum_{x\\in\\mathcal{X}}\\displaystyle\\sum_{a\\in\\mathcal{A}_{x}}\\mu_{1:}^{\\prime}(x,a)\\displaystyle\\sum_{x^{\\prime}\\mid(x,a)\\to x^{\\prime}}q^{t}(x^{\\prime})}\\\\ &{\\qquad\\qquad=\\left<\\widehat{\\ell}^{t},\\mu_{1:}^{\\prime}\\right>+\\displaystyle\\sum_{x\\in\\mathcal{X}}\\displaystyle\\sum_{a\\in\\mathcal{A}_{x}}\\displaystyle\\sum_{x^{\\prime}\\mid(x,a)\\to x^{\\prime}}\\mu_{1:}^{\\prime}(x^{\\prime})q^{t}(x^{\\prime})}\\\\ &{\\qquad=\\left<\\widehat{\\ell}^{t},\\mu_{1:}^{\\prime}\\right>+\\displaystyle\\sum_{x^{\\prime}\\in\\mathcal{X}\\backslash\\mathcal{X}_{1}}\\mu_{1:}^{\\prime}(x^{\\prime})q^{t}(x^{\\prime})}\\\\ &{\\qquad=\\left<\\widehat{\\ell}^{t},\\mu_{1:}^{\\prime}\\right>+\\displaystyle\\sum_{x^{\\prime}\\in\\mathcal{X}}\\mu_{1:}^{\\prime}(x^{\\prime})q^{t}(x^{\\prime})-\\displaystyle\\sum_{x^{\\prime}\\in\\mathcal{X}_{1}}q^{t}(x^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "in which we identified the components of the second sum as the set of non-initial information sets. Wethen concludeusing $\\textstyle\\sum_{x\\in{\\mathcal{X}}_{1}}q^{t}(x)={\\hat{q}}^{t}$ by definionof the $q^{t}$ terms. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "This lemma is then used to upper bound the estimated regret of the sequence generated by the updates (GDS-OMD dilated). Indeed, while we could apply Theorem 3.2, the associated stability term, which depends on the Fenchel dual of the dilated entropy, is not easy to upper bound. Nonetheless, the proof of the following theorem is mostly the same but with a slightly different definition of the stability and penaltyterms. ", "page_idx": 22}, {"type": "text", "text": "TheoremE.3.Let $(\\mu^{t})_{t\\in[T]}$ be the sequence of policies generated by the updates (GDS-OMD dilated). The following bound holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{R}^{T}\\leq\\operatorname*{sup}_{\\mu^{\\dagger}\\in\\Pi_{\\operatorname*{min}}}\\mathbf D_{\\alpha^{T}}^{\\mathrm{dil}}(\\mu_{1}^{\\dagger},\\mu_{1:}^{1})+\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\alpha^{t}(x)\\mu_{1:}^{t}(x)\\mathbf D_{x}^{\\star}\\left(\\nabla\\Psi_{x}(\\mu_{1:}^{t}(\\cdot|x))-\\frac{1}{\\alpha^{t}(x)}\\widetilde{\\ell}^{t}(x,\\cdot),\\nabla\\Psi_{x}(\\mu_{1:}^{t}(\\cdot|x))\\right)+\\hat{R}^{T},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. The separation between the stability and the penalty terms is the same as in Theorem 3.2, but With $\\hat{q}^{t}$ (of Lemma E.2) defined after the projection rather than before. This leads to the decomposition ", "page_idx": 22}, {"type": "image", "img_path": "HU2uyDjAcy/tmp/82ec9208cd19135e3ad2fca458851f8f9f6b396dd7f325ccfac3d2caebc500ac.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Penalty term: This part is similar to the general theorem. The optimality of $\\mu^{t+1}$ leads to, for any $t\\in[T]$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla\\Psi^{t+1}(\\mu_{1:}^{t+1})=-\\widehat{\\ell}^{t}-g^{t}+\\nabla\\Psi^{t}(\\mu_{1:}^{t})+\\nabla\\varphi^{t}(\\mu_{1:}^{1})\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $g^{t}\\in Q_{\\operatorname*{max}}^{\\perp}$ and $\\varphi^{t}=\\Psi^{t+1}-\\Psi^{t}$ . We use the same two law of cosines as in Theorem 3.2 ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}_{\\Psi^{t}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{t})=\\mathbf{D}_{\\Psi^{t}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{t+1})+\\mathbf{D}_{\\Psi^{t}}(\\mu_{1:}^{t+1},\\mu_{1:}^{t})-\\left\\langle\\nabla\\Psi^{t}(\\mu_{1:}^{t})-\\nabla\\Psi^{t}(\\mu_{1:}^{t+1}),\\mu_{1:}^{\\dagger}-\\mu_{1:}^{t+1}\\right\\rangle}\\\\ &{\\mathbf{D}_{\\Psi^{t}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{1})=\\mathbf{D}_{\\varphi^{t}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{t+1})+\\mathbf{D}_{\\varphi^{t}}(\\mu_{1:}^{t+1},\\mu_{1:}^{1})-\\left\\langle\\nabla\\varphi^{t}(\\mu_{1:}^{1})-\\nabla\\varphi^{t}(\\mu_{1:}^{t+1}),\\mu_{1:}^{\\dagger}-\\mu_{1:}^{t+1}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which yields by summing ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}_{\\Psi^{t}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{t})+\\mathbf{D}_{\\mathcal{S}^{t}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{1})}\\\\ &{\\quad\\quad=\\mathbf{D}_{\\Psi^{t+1}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{t+1})+\\mathbf{D}_{\\Psi^{t}}(\\mu_{1:}^{t+1},\\mu_{1:}^{t})+\\mathbf{D}_{\\mathcal{S}}(\\mu_{1:}^{t+1},\\mu_{1:}^{1})-\\left\\langle\\widehat{\\ell}^{t}+g^{t},\\mu_{1:}^{\\dagger}-\\mu_{1:}^{t+1}\\right\\rangle}\\\\ &{\\quad\\quad=\\mathbf{D}_{\\Psi^{t+1}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{t+1})+\\widehat{q}^{t}-\\left\\langle\\widehat{\\ell}^{t},\\mu_{1:}^{\\dagger}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "wherewe used $\\left\\langle g^{t},\\mu_{1:}^{\\dagger}-\\mu_{1:}^{t+1}\\right\\rangle=0$ from the orthogoality Suming ver $t\\in[T]$ then gves,by telescoping simiiarly to the general theorem, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left(\\hat{q}^{t}-\\left\\langle\\widehat{\\ell}^{t},\\mu_{1:}^{\\dagger}\\right\\rangle\\right)=\\displaystyle\\sum_{t=1}^{T}\\left(\\mathbf D_{\\Psi^{t}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{t})+\\mathbf D_{\\varphi^{t}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{1})-\\mathbf D_{\\Psi^{t+1}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{t+1})\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbf D_{\\Psi^{T+1}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{1})-\\mathbf D_{\\Psi^{T+1}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{t+1})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathbf D_{\\Psi^{T}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Stability term: From Lemma E.2 used with $\\mu^{\\prime}=\\mu^{t}$ , we get an alternative expression of the stability term ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\langle\\widehat{\\ell}^{t},\\mu_{1:}^{t}\\right\\rangle-\\hat{q}^{t}=\\left\\langle\\widetilde{\\ell}^{t},\\mu_{1:}^{t}\\right\\rangle-\\sum_{x\\in\\mathcal{X}}\\mu_{1:}^{t}(x)q^{t}(x)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This shows the stability term can be decomposed in a positive linear combination ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\langle\\widehat{\\ell}^{t},\\mu_{1:}^{t}\\right\\rangle-\\widehat{q}^{t}=\\sum_{x\\in\\mathcal{X}}\\mu_{1:}^{t}(x)\\left[\\left\\langle\\widetilde{\\ell}^{t}(x,\\cdot),\\mu^{t}(\\cdot|x)\\right\\rangle-q^{t}(x)\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and we will individually upperbound each of the terms of the combination. The method is again similar to the general theorem, but locally with the regularized loss. Defining $\\Psi_{x}^{t}:=\\alpha^{t}(x)\\Psi_{x}$ and $\\varphi_{x}^{t}:=\\Psi_{x}^{t+1}-\\Psi_{x}^{t}$ wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\widetilde{\\ell}^{t}(x,\\cdot),\\mu^{t}(\\cdot|x)\\right\\rangle-q^{t}(x)}\\\\ &{\\qquad=\\left\\langle\\widetilde{\\ell}^{t}(x,\\cdot),\\mu^{t}(\\cdot|x)-\\mu^{t+1}(\\cdot|x)\\right\\rangle-\\mathbf{D}_{\\Psi_{x}^{t}}(\\mu^{t+1}(\\cdot|x),\\mu^{t}(\\cdot|x))-\\mathbf{D}_{\\varphi_{x}^{t}}(\\mu^{t+1}(\\cdot|x),\\mu^{1}(\\cdot|x))}\\\\ &{\\qquad\\leq\\left\\langle\\widetilde{\\ell}^{t}(x,\\cdot),\\mu^{t}(\\cdot|x)-\\mu^{t+1}(\\cdot|x)\\right\\rangle-\\mathbf{D}_{\\Psi_{x}^{t}}(\\mu^{t+1}(\\cdot|x),\\mu^{t}(\\cdot|x))}\\\\ &{\\qquad\\leq\\left\\langle\\widetilde{\\ell}^{t}(x,\\cdot),\\mu^{t}(\\cdot|x)-\\widetilde{\\mu}^{t+1}(\\cdot|x)\\right\\rangle-\\mathbf{D}_{\\Psi_{x}^{t}}(\\widetilde{\\mu}^{t+1}(\\cdot|x),\\mu^{t}(\\cdot|x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\mu}^{t+1}(\\cdot|x):=\\arg\\operatorname*{min}_{\\tilde{\\mu}\\in\\Omega_{x}}\\left[\\left\\langle\\widetilde{\\ell}^{t}(x,\\cdot),\\tilde{\\mu}\\right\\rangle+\\mathbf{D}_{\\Psi_{x}^{t}}(\\tilde{\\mu},\\mu^{t}(\\cdot|x))\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By optimality, $\\tilde{\\mu}^{t+1}(\\cdot|x)$ verifies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla\\Psi_{x}^{t}(\\tilde{\\mu}^{t+1}(\\cdot|x))=\\nabla\\Psi_{x}^{t}(\\mu^{t}(\\cdot|x))-\\widetilde{\\ell}^{t}(x,\\cdot)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the law of cosines yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\mathbf{D}_{\\Psi_{x}^{t}}(\\mu^{t}(\\cdot|x),\\mu^{t}(\\cdot|x))}\\\\ &{\\quad=\\mathbf{D}_{\\Psi_{x}^{t}}(\\mu^{t}(\\cdot|x),\\tilde{\\mu}^{t+1}(\\cdot|x))+\\mathbf{D}_{\\Psi_{x}^{t}}(\\tilde{\\mu}^{t+1}(\\cdot|x),\\mu^{t}(\\cdot|x))-}\\\\ &{\\qquad\\qquad\\qquad\\langle\\nabla\\Psi_{x}^{t}(\\mu^{t}(\\cdot|x))-\\nabla\\Psi_{x}^{t}(\\tilde{\\mu}^{t+1}(\\cdot|x)),\\mu^{t}(\\cdot|x)-\\tilde{\\mu}^{t+1}(\\cdot|x)\\rangle}\\\\ &{\\quad=\\mathbf{D}_{\\Psi_{x}^{t}}(\\mu^{t}(\\cdot|x),\\tilde{\\mu}^{t+1}(\\cdot|x))+\\mathbf{D}_{\\Psi_{x}^{t}}(\\tilde{\\mu}^{t+1}(\\cdot|x),\\mu^{t}(\\cdot|x))-\\left\\langle\\tilde{\\ell}^{t}(x,\\cdot),\\mu^{t}(\\cdot|x)-\\tilde{\\mu}^{t+1}(\\cdot|x)\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Plugging this in the first inequality, we directly get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\langle\\widetilde{\\ell}^{t}(x,\\cdot),\\mu^{t}(\\cdot|x)\\right\\rangle-q^{t}(x)\\leq\\mathbf{D}_{\\Psi_{x}^{t}}(\\mu^{t}(\\cdot|x),\\tilde{\\mu}^{t+1}(\\cdot|x))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and we get the individual upper bounds with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf D_{\\Psi_{x}^{t}}(\\mu^{t}(\\cdot|x),\\tilde{\\mu}^{t+1}(\\cdot|x))=\\alpha^{t}(x)\\mathbf D_{\\Psi_{x}}(\\mu^{t}(\\cdot|x),\\tilde{\\mu}^{t+1}(\\cdot|x))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\alpha^{t}(x)\\mathbf D_{\\Psi_{x}^{\\star}}(\\nabla\\Psi_{x}(\\tilde{\\mu}^{t+1}(\\cdot|x)),\\nabla\\Psi_{x}(\\mu^{t}(\\cdot|x)))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\alpha^{t}(x)\\mathbf D_{\\Psi_{x}^{\\star}}\\left(\\nabla\\Psi_{x}(\\mu^{t}(\\cdot|x))-\\frac{1}{\\alpha^{t}(x)}\\tilde{\\ell}^{t}(x,\\cdot),\\nabla\\Psi_{x}(\\mu^{t}(\\cdot|x))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This upper bound on the estimated regret is then used with the learning rates considered in the main article. ", "page_idx": 23}, {"type": "text", "text": "E.2   Optimal rates analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We first consider the optimal rates of the main paper. ", "page_idx": 24}, {"type": "text", "text": "Theorem E.4. Using Local0MD with $\\mu^{1}$ as the uniform policy, with the learning rates $\\eta^{t}(x)\\,=$ $\\eta/\\kappa(\\mu^{s}|\\boldsymbol{x})$ where $\\eta~=~\\sqrt{\\log(A)\\kappa(\\mu^{s})/(3H T)},$ andwith $\\Psi_{x}$ the Shannon entropy $\\Psi_{x}(\\mu)\\;=\\;$ $\\begin{array}{r}{\\sum_{a\\in\\mathcal{A}(x)}\\mu(a)\\log(\\mu(a))}\\end{array}$ the regret is bounded with a probability at least $1-\\delta$ by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathfrak{R}_{\\operatorname*{min}}^{T}\\le\\left(4+2\\sqrt{3}\\right)\\,H^{3/2}\\sqrt{\\log(A)\\iota\\kappa(\\mu^{s})T}\\quad w h e r e\\quad\\iota=\\log(2(A_{\\mathcal{X}}+1)/\\delta)\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We apply Theorem E.3, using the relations $\\alpha^{t}(x)\\,=\\,1/(\\mu_{1:}^{s}(x)\\eta^{t}(x))$ and $\\mathbb{I}_{\\{x=x_{h}^{t}\\}}\\widetilde{\\ell}_{h}^{t}\\,=$ $\\mu_{1:}^{s}(x)\\widetilde{\\ell}^{t}(x,\\cdot)$ . We again separately bound the penalty and stability terms. ", "page_idx": 24}, {"type": "text", "text": "Penaltyterm $\\therefore$ We will denote by PEN this term defined by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{PEN}:=\\operatorname*{sup}_{\\mu^{\\dagger}\\in\\Pi_{\\operatorname*{min}}}{\\bf D}_{\\alpha^{T}}^{\\mathrm{dil}}\\left(\\mu_{1:}^{\\dagger},\\mu_{1:}^{1}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By definition of the dilated entropy, we have, using that $\\mu^{1}$ is the uniform policy and that the Bregman divergence of the Shannon entropy is the Kullback-Leibler divergence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla^{2}\\Delta x=}&{\\frac{x^{2}}{r^{2}(\\Delta)}\\displaystyle\\sum_{s=1}^{r-1}\\frac{\\partial^{2}(1+|x|^{s})}{\\partial x^{2}}\\Vert_{\\mathcal{H}}\\left(y^{2}\\middle)\\Delta x\\middle(\\rho^{2}\\middle)\\right.}\\\\ &{\\qquad\\displaystyle=}&{\\frac{1}{r^{2}\\mu_{0}}\\displaystyle\\sum_{s=1}^{r-1}\\frac{\\partial^{2}(1+|x|^{s})}{\\partial x^{2}}\\sum_{i=1}^{r-1}y^{2}\\left(\\sigma_{i,i+1}^{s}\\right)\\left(y^{2}\\middle)\\Delta x^{2}\\middle(\\rho^{2}\\middle)\\left(x^{2}\\middle)\\right.}\\\\ &{\\qquad\\displaystyle\\leq\\frac{1}{r^{2}\\mu_{0}}\\displaystyle\\sum_{s=1}^{r-1}\\frac{\\partial^{2}(1+|x|^{s})}{\\partial x^{2}}\\left(y^{2}\\middle)\\left(x^{2}\\middle)\\sum_{s=1}^{r-1}y^{2}\\left(\\sigma_{i,i+1}^{s}\\right)\\left(y^{2}\\middle)\\middle(x^{2}\\middle)\\right.}\\\\ &{\\qquad\\displaystyle=}&{\\ln\\frac{\\delta(1-\\delta)}{r}\\displaystyle\\sum_{s=1}^{r-1}\\frac{\\partial^{2}(1+|x|^{s})}{\\partial x^{2}}\\left(y^{2}\\middle)\\left(x^{2}\\right)}\\\\ &{\\qquad\\displaystyle=}&{\\ln\\frac{\\delta(1-\\delta)}{r}\\displaystyle\\sum_{s=1}^{r-1}\\frac{\\partial^{2}(1+|x|^{s})}{\\partial x^{2}}\\left(y^{2}\\middle)\\left(x^{2}\\right)}\\\\ &{\\qquad\\displaystyle=\\frac{\\delta(1-\\delta)}{r}\\displaystyle\\sum_{s=1}^{r-1}\\frac{\\partial^{2}(1+|x|^{s})}{\\partial x^{2}}\\left(y^{2}\\middle)\\frac{\\partial_{x}\\partial^{2}(x)}{\\partial x^{2}}\\left(x^{2}\\middle)\\sum_{s=t+1}^{r-1}\\frac{\\partial^{2}(1+|x|^{s})}{\\partial x^{2}}\\left(y^{2}\\middle)\\right.}\\\\ &{\\qquad\\displaystyle\\leq\\frac{1}{r^{2}\\mu_{0}}\\displaystyle\\sum_{s=1}^{r-1}\\frac{\\partial^{2}(1+|x|^{s})}\n$$$$\n\\begin{array}{r l}&{\\mathrm{{udance}})=\\displaystyle\\frac{{\\log(A)}}{\\eta}\\displaystyle\\sum_{h=1}^{H}\\operatorname*{sup}_{\\mu^{\\uparrow}\\in\\Pi_{\\operatorname*{min}}}\\sum_{x\\in\\mathcal{X}_{h}}\\displaystyle\\frac{\\mu_{1;\\cdot}^{\\uparrow}(x)}{\\mu_{1;\\cdot}^{\\uparrow}(x)}\\sum_{\\substack{x^{\\prime}\\in\\mathcal{X}\\mid x\\mathrm{~is~in~thener~of~}x^{\\prime}}\\,a^{\\prime}\\in A(x^{\\prime})}\\displaystyle\\sum_{\\mu_{1;\\cdot}^{\\uparrow}(x^{\\prime},a^{\\prime})}}\\\\ &{\\phantom{\\mathrm{{udance}}}=\\displaystyle\\frac{\\log(A)}{\\eta}\\displaystyle\\sum_{h=1}^{H}\\operatorname*{sup}_{\\mu^{\\downarrow}\\in\\Pi_{\\operatorname*{min}}}\\sum_{x\\in\\mathcal{X}_{h}}\\displaystyle\\sum_{\\tau\\in\\mathcal{X}\\backslash x\\mathrm{~in~the~hisary~of~}x^{\\prime}\\,a^{\\prime}\\in A(x^{\\prime})}\\displaystyle\\sum_{\\mu_{1;\\cdot}^{\\downarrow}(x^{\\prime},a^{\\prime})}}\\\\ &{\\phantom{\\mathrm{{udance}}}=\\displaystyle\\frac{\\log(A)}{\\eta}\\displaystyle\\sum_{h=1}^{H}\\operatorname*{sup}_{\\mu^{\\downarrow}\\in\\Pi_{\\operatorname*{min}}}\\sum_{x^{\\prime}\\in\\mathcal{X}\\,a^{\\prime}\\in A(x^{\\prime})}\\displaystyle\\sum_{\\mu_{1;\\cdot}^{\\downarrow}(x^{\\prime},a^{\\prime})}}\\\\ &{\\phantom{\\mathrm{{udance}}}=\\displaystyle\\frac{\\log(A)}{\\eta}H\\kappa(\\mu^{s})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\chi_{h}$ is the set of information sets of depth $h$ , the two sums being later merged on the basis of perfect recall. We now look at the stability term. ", "page_idx": 24}, {"type": "text", "text": "Stability term : We will denote by STA this term defined by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{STA}:=\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\alpha^{t}(x)\\mu_{1:}^{t}(x)\\mathbf{D}_{x}^{\\star}\\left(\\nabla\\Psi_{x}(\\mu_{1:}^{t}(\\cdot|x))-\\frac{1}{\\alpha^{t}(x)}\\widetilde{\\ell}^{t}(x,\\cdot),\\nabla\\Psi_{x}(\\mu_{1:}^{t}(\\cdot|x))\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We frt lok a an upper-bound of $\\begin{array}{r}{\\mathbf{D}_{x}^{\\star}\\Big(\\nabla\\Psi_{x}(\\mu_{1:}^{t}(\\cdot|x))-\\frac{1}{\\alpha^{t}(x)}\\widetilde{\\ell}^{t}(x,\\cdot),\\nabla\\Psi_{x}(\\mu_{1:}^{t}(\\cdot|x))\\Big)}\\end{array}$ .In order to do so, we upper-bound (in the symmetric matrix sense) the Hessian of $\\Psi_{x}^{\\star}$ on $I\\ :=$ $\\begin{array}{r l}{\\Bigl\\{\\nabla\\Psi_{x}(\\mu_{1:}^{t}(\\cdot|x))-\\frac{\\widetilde{\\gamma}}{\\alpha^{t}(x)}\\widetilde{\\ell}^{t}(x,\\cdot)\\Bigl|\\gamma\\in[0,1]\\Bigr\\}}&{{}}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Because $\\begin{array}{r}{\\Psi_{x}(\\mu)=\\sum_{a\\in\\mathcal{A}(x)}\\mu(a)\\log(\\mu(a))}\\end{array}$ is the Shannon entropy, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\Psi_{x}(\\mu)(a)=\\log(\\mu(a))+1\\quad\\mathrm{and\\;thus}\\quad\\nabla^{2}\\Psi_{x}(\\mu)=\\mathrm{Diag}\\{(1/\\mu(a))\\}_{a\\in A(x)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and the Hessian of $\\Psi_{x}^{\\star}$ is given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla^{2}\\Psi^{\\star}(y)=\\nabla^{2}\\Psi_{x}(y)^{-1}={\\mathrm{Diag}}\\{y(a)\\}_{a\\in A(x)}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In particular, it is upper bounded on $I$ by the matrix $D_{\\mu}$ defined by ", "page_idx": 25}, {"type": "equation", "text": "$$\nD_{\\mu}:=\\mathrm{Diag}\\{\\mu(a)\\}_{a\\in\\mathcal{A}(x)}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This yields that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}_{x}^{\\star}\\bigg(\\nabla\\Psi_{x}(\\mu_{1:}^{t}(\\cdot|x))-\\frac{1}{\\alpha^{t}(x)}\\widetilde{\\ell}^{t}(x,\\cdot),\\nabla\\Psi_{x}(\\mu_{1:}^{t}(\\cdot|x))\\bigg)\\leq\\frac{1}{2}\\|\\frac{1}{\\alpha^{t}(x)}\\widetilde{\\ell}^{t}(x,\\cdot)\\|_{D_{\\mu}^{t}(\\cdot|x)}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{2\\alpha^{t}(x)^{2}}\\displaystyle\\sum_{a\\in A(x)}\\mu^{t}(a|x)\\widetilde{\\ell}^{t}(x,a)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which leads to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{STA}\\leq\\displaystyle\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\frac{\\mu_{1:}^{t}(x)}{2\\alpha^{t}(x)}\\sum_{a\\in\\mathcal{A}(x)}\\mu^{t}(a|x)\\widetilde{\\ell}^{t}(x,a)^{2}}\\\\ &{\\quad=\\displaystyle\\frac{\\eta}{2}\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\mathbb{I}_{\\left\\{x=x_{h}^{t}\\right\\}}\\frac{\\mu_{1:}^{t}(x)}{\\mu_{1:}^{s}(x)}\\frac{1}{\\kappa(\\mu^{s}|x)}\\sum_{a\\in\\mathcal{A}(x)}\\mathbb{I}_{\\left\\{a=a_{h}^{t}\\right\\}}\\mu^{t}(a|x)\\widetilde{\\ell}_{h}^{t}(a)^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can first notice from recursively comparing the minimizer $\\mu^{t+1}(\\cdot|x_{h}^{t})$ With $\\mu^{t}(\\cdot|x_{h}^{t})$ that the regularized loss $\\widetilde{\\ell}_{h}^{t}(a_{h}^{t})$ ,satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{h}^{t}(a_{h}^{t})\\leq\\left\\langle\\widehat{\\ell}^{t,\\rightarrow x},\\mu_{h+1:}^{t,\\rightarrow x}\\right\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "re-using the notation at the beginning of the section, because the regularization does not evolve with time. The difficulty is now to upper bound STA with high probability. In order to do so, we use the Lemma B.1 on the sequence $({\\bar{U}}^{\\bar{t}})_{t\\in[T]}$ defined by ", "page_idx": 25}, {"type": "equation", "text": "$$\nU^{t}:=\\sum_{x\\in\\mathcal{X}}\\mathbb{I}_{\\left\\{x=x_{h}^{t}\\right\\}}\\frac{\\mu_{1:}^{t}(x)}{\\mu_{1:}^{s}(x)}\\frac{1}{\\kappa(\\mu^{s}|x)}\\sum_{a\\in\\mathcal{A}(x)}\\mathbb{I}_{\\left\\{a=a_{h}^{t}\\right\\}}\\mu^{t}(a|x)\\widetilde{\\ell}_{h}^{t}(a)^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with $\\gamma^{\\prime}=\\gamma\\in(0,1/(H^{2}\\kappa(\\mu^{s}))]$ and $\\delta^{\\prime}=\\delta/2$ . This yields with probability at least $1-\\delta/2$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}U^{t}\\leq\\sum_{t=1}^{T}\\mathbb{E}\\left[U^{t}\\big|\\mathcal{F}^{t-1}\\right]+\\gamma\\sum_{t=1}^{T}\\mathbb{E}\\left[(U^{t})^{2}\\big|\\mathcal{F}^{t-1}\\right]+\\iota/\\gamma\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "On the one hand, we have, using $\\widehat{\\ell}_{h}^{t}(a_{h}^{t})\\leq\\kappa(\\mu^{s}|x)$ and the previous inequality that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[U^{t}\\big|\\mathcal{F}^{t-1}\\right]\\leq\\displaystyle\\sum_{x\\in\\mathcal{X}}p^{t}(x)\\mu^{t}(x)\\sum_{a\\in A(x)}\\left\\langle\\ell^{t,\\rightarrow x},\\mu_{h:}^{t,\\rightarrow x}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{x\\in\\mathcal{X}}p^{t}(x)\\mu^{t}(x)H}\\\\ &{\\qquad\\qquad\\qquad\\leq H^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "On the other hand, using the same inequality, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\{U^{\\dagger}\\}^{2}\\}^{\\big|}F^{t-1}\\right]=\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{\\tau\\leq K}\\|_{\\mu_{1}^{t}}\\Big(x_{t}\\Big)\\frac{\\mu_{1}^{t}}{\\alpha}(x)\\displaystyle\\sum_{\\alpha\\in A(s)\\atop\\mu_{1}^{t}(x)}\\mathbb{I}_{\\{\\alpha=u_{k}^{\\prime}\\}}\\left\\langle\\widehat{\\tilde{\\ell}}_{\\mu}^{t}(\\alpha),\\mu^{t}(\\alpha)\\right\\rangle\\right)^{2}\\right]\\mathcal{H}^{t-1}\\right]}\\\\ {\\leq}&{H\\mathbb{E}\\left[\\displaystyle\\sum_{\\tau\\leq K}\\mathbb{I}_{\\{\\alpha=x_{k}^{\\prime}\\}}\\frac{\\mu_{1}^{t}}{\\mu_{1}^{t}(x)}\\Big(z^{2}\\sum_{\\alpha\\in A(s)\\atop\\mu_{1}^{t}(x)}\\mathbb{I}_{\\{\\alpha=u_{k}^{\\prime}\\}}\\left\\langle\\widehat{\\ell}_{\\kappa}^{t}(\\alpha),\\mu^{t}(\\alpha)\\right\\rangle\\right)^{2}\\Bigg|\\mathcal{H}^{t-1}\\right]}\\\\ &{\\leq H\\kappa(\\mu^{3})\\mathbb{E}\\left[\\displaystyle\\sum_{\\tau\\leq K}\\mathbb{I}_{\\{\\alpha=x_{k}^{\\prime}\\}}\\frac{\\mu_{1}^{t}}{\\mu_{1}^{t}(x)}\\Big(x\\sum_{\\alpha\\in A(s)\\atop\\mu_{1}^{t}(x)}\\prod_{\\beta_{1}^{t}(\\alpha)}\\mu_{1}^{t}(\\alpha),\\mu^{t}(\\alpha)\\Big)\\Bigg|\\mathcal{H}^{t-1}\\right]}\\\\ &{\\leq H\\kappa(\\mu^{3})\\sum_{\\tau\\leq K}p^{t}(x)\\mu^{t}(x)\\displaystyle\\sum_{\\alpha\\in A(s)\\atop\\mu_{1}^{t}(x)}\\left\\langle\\ell^{t-1,\\alpha},\\mu_{1}^{t-\\alpha}\\right\\rangle}\\\\ &{\\leq H\\kappa(\\mu^{3})\\sum_{\\alpha\\in B(s)\\atop\\mu_{1}^{t}(K)}p^{t}(x)\\mu^{t}(x)H}\\\\ &{\\leq H^{3}(\\kappa(\\mu^{3})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The following upper bound on the stability term thus holds ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{STA}\\leq\\eta\\left(H^{2}T+\\gamma H^{3}\\kappa(\\mu^{s})T+\\frac{\\iota}{\\gamma}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking $\\gamma=1/(H^{2}\\kappa(\\mu^{s}))$ , we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{STA}\\leq\\eta\\left(2H^{2}T+H^{2}\\iota\\kappa(\\mu^{s})\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As the bound of the theorem trivially holds if $T<\\iota\\kappa(\\mu^{s})$ (the regret being bounded by $T$ anyway), we even have assuming $T\\geq\\iota\\kappa(\\mu^{s})$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{STA}\\leq3\\eta H^{2}T\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Conclusion: Combining all the previous bounds, the estimated regret is bounded, with a probability of at least $1-\\delta/2$ by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\Re}^{T}\\le\\frac{\\log(A)}{\\eta}H\\kappa(\\mu^{s})+3\\eta H^{2}T\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking $\\eta=\\sqrt{\\log(A)\\kappa(\\mu^{s})/(3H T)}$ we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\mathfrak{R}}^{T}\\le2\\sqrt{3}\\,H^{3/2}\\sqrt{\\log(A)\\iota\\kappa(\\mu^{s})T}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We finally conclude by combining this bound with Theorem 2.2 for the true regret, using $\\delta^{\\prime}=\\delta/2$ such that the two inequalities hold with a probability at least $1-\\delta$ \u53e3 ", "page_idx": 26}, {"type": "text", "text": "E.3  Adaptive rates analysis ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We end this appendix by considering the adaptive setting. We will assume that all regularizers $\\Psi_{x}$ are 1-strongly convex with respect to some norms $\\left\\|\\cdot\\right\\|_{x}$ , and we will define ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{\\Psi}:=\\underset{x\\in\\mathcal{X},\\mu\\in\\Delta_{A_{x}}}{\\operatorname*{sup}}\\mathbf{D}_{x}(\\mu,\\mu^{1}(\\cdot|x))}\\\\ &{C_{\\Psi}^{\\star}:=\\underset{x\\in\\mathcal{X},a\\in A_{x}}{\\operatorname*{sup}}\\|\\mathbb{I}_{\\{x,a\\}}\\|_{x}^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mu^{1}$ is the initial policy considered in the algorithm and $\\mathbb{I}_{\\{x,a\\}}$ is the loss vector $\\ell(x,\\cdot)$ equal to 1 for $a\\in A(x)$ and 0 for $a^{\\prime}\\in\\mathcal{A}(x)\\backslash\\{a\\}$ . The following theorem is the formal statement of Theorem 4.1 in the main article. While being quite general, the upper bound is unsurprisingly not as tight as the previous one. ", "page_idx": 26}, {"type": "text", "text": "Theorem E.5. With such regularizers, assume that the learning rates are locally decreasing and let $\\lambda_{1},\\lambda_{2}\\in\\mathbb{R}_{>0}$ betwo constants such thatfor all information set $x\\in\\mathscr{X}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{t\\in[T-1]}\\left[\\frac{1}{\\eta^{t+1}(x)}-\\frac{1}{\\eta^{t}(x)}\\right]\\le\\lambda_{1}\\quad\\mathrm{and}\\quad1/\\eta^{T}(x)+\\sum_{t=1}^{T}\\eta^{t}(x)\\mathbb{I}_{\\left\\{x=x_{h}^{t}\\right\\}}\\le\\lambda_{2}\\sqrt{T}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then with a probability at least $1-\\delta$ , the regret of Algorithm 2 is upper-bounded by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{R}_{\\operatorname*{max}}^{T}\\le\\left[2\\left[\\left(1+\\lambda_{1}\\right)C_{\\Psi}C_{\\Psi}^{\\star}\\kappa(\\mu^{s})\\right]^{2}\\lambda_{2}|\\mathcal{X}|+4\\sqrt{H\\kappa(\\mu^{s})\\iota}\\right]\\sqrt{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\iota=\\log((A_{\\mathcal{X}}+1)/\\delta)$ ", "page_idx": 27}, {"type": "text", "text": "The proof of this theorem will be based on the following lemma that bounds the regularized loss usingthe $\\lambda_{1}$ constant above. ", "page_idx": 27}, {"type": "text", "text": "Lemma E.6. For all $t\\in[T]$ and $h\\in[H]$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\ell}_{h}^{t}(a_{h}^{t})\\le(1+\\lambda_{1}C_{\\Psi})\\kappa(\\mu^{s}|x_{h}^{t})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The proof is done recursively on $h$ , starting from the leaves. Indeed, for $h=H$ , the property is immediate as $\\widetilde{\\ell}_{h}^{t}(a_{h}^{t})\\le1/\\mu^{s}(a_{H}^{t}|x_{H}^{t})\\le\\kappa(\\mu^{s}|x_{H}^{t})$ . If we assume that the property holds for a depth $h>1$ , then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{h}^{t}=\\underset{\\mu\\in\\Delta_{A(x_{h}^{t})}}{\\operatorname*{min}}\\left\\langle\\widetilde{\\ell}_{h}^{t},\\mu\\right\\rangle+\\frac{1}{\\eta^{t}(x_{h}^{t})}\\mathbf{D}_{x}\\left(\\mu,\\mu^{t}(\\cdot|x_{h}^{t})\\right)+\\left(\\frac{1}{\\eta^{t+1}(x_{h}^{t})}-\\frac{1}{\\eta^{t}(x_{h}^{t})}\\right)\\mathbf{D}_{x}\\left(\\mu,\\mu^{1}(\\cdot|x_{h}^{t})\\right)}\\\\ &{\\quad\\leq\\left\\langle\\widetilde{\\ell}_{h}^{t},\\mu^{t}(\\cdot|x_{h}^{t})\\right\\rangle+\\left(\\frac{1}{\\eta^{t+1}(x_{h}^{t})}-\\frac{1}{\\eta^{t}(x_{h}^{t})}\\right)\\mathbf{D}_{x}\\left(\\mu^{t}(\\cdot|x_{h}^{t}),\\mu^{1}(\\cdot|x_{h}^{t})\\right)}\\\\ &{\\quad\\leq\\widetilde{\\ell}_{h}^{t}(a_{h}^{t})+\\lambda_{1}C_{\\Psi}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\ell}_{h-1}^{t}(a_{h-1}^{t})=(\\ell_{h-1}^{t}+q_{h}^{t})/\\mu^{s}(a_{h-1}^{t}|x_{h-1}^{t})}\\\\ &{\\qquad\\qquad\\leq(1+\\lambda_{1}C_{\\Psi}+\\widetilde{\\ell}_{h}^{t}(a_{h}^{t}))/\\mu^{s}(a_{h-1}^{t}|x_{h-1}^{t})}\\\\ &{\\qquad\\qquad\\leq(1+\\lambda_{1}C_{\\Psi})(1+\\kappa(\\mu^{s}|x_{h}^{t}))/\\mu^{s}(a_{h-1}^{t}|x_{h-1}^{t})}\\\\ &{\\qquad\\qquad\\leq(1+\\lambda_{1}C_{\\Psi})\\kappa(\\mu^{s}|x_{h-1}^{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which concludes the induction. ", "page_idx": 27}, {"type": "text", "text": "Proof. We now prove the theorem. We start with the estimated regret, that we decompose between the penalty term and the stability term using theorem E.3. ", "page_idx": 27}, {"type": "text", "text": "Penalty term: The penalty term PEN is bounded by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{PEN}\\leq\\underset{\\mu^{\\dagger}\\in\\Pi_{\\operatorname*{min}}}{\\operatorname*{sup}}\\mathbf{D}_{\\alpha^{T}}^{\\mathrm{dil}}(\\mu_{1:}^{\\dagger},\\mu_{1:}^{1})}\\\\ &{\\leq\\underset{\\mu^{\\dagger}\\in\\Pi_{\\operatorname*{min}}}{\\operatorname*{sup}}\\sum_{x\\in\\mathcal{X}}\\frac{1}{\\eta^{T}(x)}\\frac{\\mu_{1:}^{\\dagger}(x)}{\\mu_{1:}^{\\circ}(x)}\\mathbf{D}_{x}(\\mu^{\\dagger}(\\cdot|x),\\mu^{1}(\\cdot|x))}\\\\ &{\\leq C_{\\Psi}\\lambda_{2}\\sqrt{T}\\underset{\\mu^{\\dagger}\\in\\Pi_{\\operatorname*{min}}}{\\operatorname*{sup}}\\sum_{x\\in\\mathcal{X}}\\frac{\\mu_{1:}^{\\dagger}(x)}{\\mu_{1:}^{\\circ}(x)}}\\\\ &{\\leq C_{\\Psi}\\lambda_{2}\\kappa(\\mu^{s})\\sqrt{T}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Stability term: For the stability term STA, we rely on Lemma E.6 and the 1-strong convexity of $\\Psi_{x}$ withrespecto $\\left\\|\\cdot\\right\\|_{x}$ (seeAppendix $\\mathrm{D}.1$ )and get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathrm{STA}}=\\displaystyle\\sum_{t=1}^{\\infty}\\sum_{u=1}^{t}\\sum_{u=1}^{u}\\alpha^{(t)}(x)\\alpha^{\\top}\\left(\\nabla\\Phi_{u}(u_{t}^{t}(x))-\\frac{1}{\\alpha(u^{t})}\\bar{\\rho}(x,\\cdot)\\nabla\\Phi_{v}\\right)}\\\\ &{\\lesssim\\displaystyle\\sum_{t=1}^{\\infty}\\sum_{u=1}^{t}\\beta^{(t)}(x)\\alpha^{\\top}\\left(x\\right)\\mathbb{I}_{\\rho}^{\\lambda_{t}},}\\\\ &{\\lesssim\\displaystyle\\sum_{t=1}^{\\infty}\\sum_{u=1}^{t}\\gamma^{(t)}(x)\\|\\bar{\\rho}_{v+1}\\|_{\\rho}^{\\lambda_{t}}\\|\\bar{\\rho}_{u,v}^{\\lambda_{t}},}\\\\ &{\\lesssim\\displaystyle\\sum_{t\\in\\mathbb{R}_{+}}^{t}\\sum_{u=1}^{\\infty}\\gamma^{(t)}(x)\\|\\bar{\\rho}_{v+1}\\|_{\\rho}^{\\lambda_{t}}\\|\\bar{\\rho}_{u,v}^{\\lambda_{t}}\\|}\\\\ &{\\lesssim\\displaystyle\\left(\\|\\bar{\\rho}_{v+1}^{(t)}\\|_{L^{\\infty}(\\mathbb{R}^{T})}^{2}+\\sum_{t=1}^{t}\\bar{\\rho}_{v,v}^{\\lambda_{t}}\\|\\bar{\\rho}_{v,v}^{\\lambda_{t}}\\|\\right)^{2}}\\\\ &{\\leq\\|(1+\\lambda)C\\epsilon\\bar{C}\\|_{L^{\\infty}(\\mathbb{R}^{T})}^{2}\\displaystyle\\sum_{t=1}^{t}\\sum_{u=1}^{t}\\gamma^{(t)}(x)\\|\\bar{\\rho}_{v,v}^{\\lambda_{t}}\\|\\bar{\\rho}^{\\lambda_{t}}\\|v^{\\lambda_{t}}\\|^{2}}\\\\ &{\\lesssim\\|(1+\\lambda)C\\epsilon\\bar{C}\\|_{L^{\\infty}(\\mathbb{R}^{T})}^{2}\\displaystyle\\sum_{t=1}^{t}\\sum_{u=1}^{t}\\gamma^{(t)}(x)\\|\\bar{\\rho}_{v,v}^{\\lambda_{t}}\\|\\bar{\\rho}^{\\lambda_{t}}\\|v^{\\lambda_{t}}\\|}\\\\ &{\\lesssim\\|(1+\\lambda)C\\epsilon\\bar{C}\\|_{L^{\\infty}(\\mathbb{R}^{T})}^{2}\\displaystyle\\sum_{t=1}^{t}\\sum \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Conclusion: By summing these two upper bounds we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\Re}^{T}\\leq\\Big[C_{\\Psi}\\lambda_{2}\\kappa(\\mu^{s})+\\left[\\left(1+\\lambda_{1}\\right)C_{\\Psi}C_{\\Psi}^{\\star}\\kappa(\\mu^{s})\\right]^{2}\\lambda_{2}|\\boldsymbol{\\mathcal{X}}|\\Big]\\,\\sqrt{T}}\\\\ &{\\quad\\quad\\leq2\\left[\\left(1+\\lambda_{1}\\right)C_{\\Psi}C_{\\Psi}^{\\star}\\kappa(\\mu^{s})\\right]^{2}\\lambda_{2}|\\boldsymbol{\\mathcal{X}}|\\sqrt{T}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The bound is finally obtained using the Theorem 2.2 that holds with a probability of at least $1-\\delta$ and links the estimated regret to the true regret. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The main claims are proven in the theorems or observed in the experiments. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Some limitations and possible improvements are mentioned in the conclusion. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The proofs of the theorem (and the exact statement of Theorem 4.1) are provided in the Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All implemented algorithms are fully specified. An open-access library (OpenSpiel) is used for an implementation of the games. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: A link to an anonymous repository with instructions to run the code is provided. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The few details necessary to redo the experiments are provided. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The variance over the multiple simulation is visible on the figures. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The experiments were run on a personal computer as they do not require heavy computations. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: the research conducted in the paper fully conforms with the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This work is mostly theoretical and we are not aware of any societal impact. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This work presents no risk of misuse. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The original paper behind OpenSpiel is cited. The version used and the license are specified. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]