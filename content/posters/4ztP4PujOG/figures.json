[{"figure_path": "4ztP4PujOG/figures/figures_0_1.jpg", "caption": "Figure 1: (A) Hard cases which cannot be properly modeled by most existing representations. (B) Motion graph transforms single-frame patches into interconnected nodes, describing the spatial-temporal relationships. Future per-pixel motion dynamic vectors are then predicted on this graph.", "description": "This figure demonstrates the challenges of existing motion representations and introduces the proposed motion graph. Part (A) showcases scenarios that are difficult for existing methods to model accurately, including motion blur, distortion/deformation, occlusion, and expansion.  Part (B) illustrates the motion graph, a novel approach where video frames are broken into patches (nodes) interconnected based on their spatial-temporal relationships.  The future motion of each pixel is predicted using this graph.", "section": "1 Introduction"}, {"figure_path": "4ztP4PujOG/figures/figures_3_1.jpg", "caption": "Figure 2: Motion graph node construction: Cosine similarity, denoted by (,), between patch features in consecutive frames is computed to further choose top k directions for each patch. Tendency  and location features are then generated based on these k vectors and the patch location.", "description": "This figure illustrates the process of constructing a node in the motion graph.  It starts with computing cosine similarity between patch features from consecutive frames (It and It+1) to identify the top k most likely motion directions for each patch. These directions, along with the patch's location, are used to generate the tendency (tf(m)) and location (lf(m)) features for the node, which represent the node's motion-related attributes and its spatial position within the frame, respectively. These features are then combined to form the final node motion feature (vf(m)).", "section": "3.2 Motion Graph"}, {"figure_path": "4ztP4PujOG/figures/figures_5_1.jpg", "caption": "Figure 3: Inside the interaction module for the m-th view \u03a6(m). The spatial and temporal message passing are iteratively conducted and repeated T \u2212 1 times, where T is the observed frame number.", "description": "This figure illustrates the inner workings of the motion graph interaction module.  The spatial message passing is shown as a 2D convolution, while temporal message passing utilizes a graph neural network. The process iterates T-1 times (where T is the number of observed frames), alternating between spatial and temporal message passing (forward and backward). The goal is to ensure complete information flow across all frames, even influencing the last frame's nodes with information from the beginning.", "section": "3.2 Motion Graph"}, {"figure_path": "4ztP4PujOG/figures/figures_5_2.jpg", "caption": "Figure 4: Pipeline overview. After decoding per-pixel motion features into dynamic vectors, we perform multi-flow forward warping for future frame generation.", "description": "This figure illustrates the three main steps of the video prediction pipeline using motion graphs. Step I shows the process of learning motion features from the observed video frames using multiple motion graph interaction modules.  The motion features from different scales are fused to create a unified representation. Step II involves upsampling the motion features to the original image resolution using a motion upsampler (OSR) and decoding them into dynamic vectors using a motion decoder (\u03a9dec). Finally, step III performs the multi-flow forward warping to generate the predicted future frame from the past frames and the dynamic vectors, resulting in the synthesis of future frames (IT+1).", "section": "3.3 Motion-graph-empowered Video Prediction"}, {"figure_path": "4ztP4PujOG/figures/figures_8_1.jpg", "caption": "Figure 5: On the UCF Sports dataset, our method recovers richer image details than MMVP [2].", "description": "This figure presents a qualitative comparison of video prediction results between the proposed method and MMVP on the UCF Sports dataset. It showcases that the proposed method, compared to MMVP, better preserves the details of the image. This improved detail preservation is particularly noticeable in areas with complex motion or blurring, suggesting that the proposed method is superior in handling challenging motion scenarios.", "section": "4.1 Public Benchmark Comparison"}, {"figure_path": "4ztP4PujOG/figures/figures_8_2.jpg", "caption": "Figure 6: Qualitative comparisons with OPT [53] and DMVFN [13]. Our method maintains the object structures better than both methods while holding a higher motion prediction accuracy.", "description": "This figure shows a qualitative comparison of video prediction results between the proposed method and two state-of-the-art methods (OPT and DMVFN) on KITTI and Cityscapes datasets.  The results demonstrate that the proposed method better preserves object structures and achieves higher motion prediction accuracy compared to the other methods, especially in handling perspective effects and occlusions. Each column represents a different video sequence, showing the last observed frame, predictions by OPT, DMVFN, the proposed method, and the ground truth (GT).", "section": "4.1 Public Benchmark Comparison"}, {"figure_path": "4ztP4PujOG/figures/figures_14_1.jpg", "caption": "Figure 7: Architecture of the image encoder", "description": "The image encoder consists of four ResBlock Downsample layers followed by a pixel unshuffle layer.  The ResBlock Downsample layers use a combination of convolutional layers, downsampling, and residual connections to extract multi-scale features from the input image (3 x H x W). The pixel unshuffle layer reshapes the output feature maps to the desired resolution (8Cimg x Hs x Ws).", "section": "A.2 Network Architecture"}, {"figure_path": "4ztP4PujOG/figures/figures_15_1.jpg", "caption": "Figure 8: Inner structure of spatial and temporal block in motion graph interaction module", "description": "This figure shows the architecture of the spatial and temporal message passing within the motion graph interaction module.  The spatial message passing uses a 2D convolution to update node features based on spatial relationships.  The temporal message passing involves a linear projection to create prediction vectors from previous nodes, and a linear layer to combine these predictions with information from the successor nodes.", "section": "3.2 Motion Graph"}, {"figure_path": "4ztP4PujOG/figures/figures_15_2.jpg", "caption": "Figure 9: Inner structure of the motion upsampler and the motion decoder.", "description": "This figure shows the architecture of the motion upsampler and decoder used in the video prediction pipeline.  The upsampler takes the fused multi-view motion features (ffuse) as input and progressively increases the resolution using ResBlock Upsample layers to match the original video frame resolution (H x W). Finally, a 1x1 convolution layer converts the upsampled features into dynamic vectors (P) representing pixel-level motion.", "section": "3.3 Motion-graph-empowered Video Prediction"}, {"figure_path": "4ztP4PujOG/figures/figures_17_1.jpg", "caption": "Figure 10: Failure cases in UCF Sports Dataset", "description": "This figure showcases examples where the video prediction model struggles.  The examples depict scenarios involving fast, unpredictable movements such as kicking and diving. These actions present challenges for the model's ability to accurately predict future frames due to their dynamic and less easily predictable nature.  The figure visually demonstrates limitations of the approach when confronted with complex and sudden motion, highlighting areas for future improvements.", "section": "A.6 Failure case demonstration"}, {"figure_path": "4ztP4PujOG/figures/figures_18_1.jpg", "caption": "Figure 11: Tendency feature visualization using KITTI dataset", "description": "This figure visualizes the tendency features extracted from the KITTI dataset using K-means clustering.  The top row shows example frames from the dataset. The bottom two rows display the results of the clustering, using 2 and 3 clusters respectively.  Each color represents a cluster, indicating different motion patterns detected by the model within the image patches.  Areas of similar motion are grouped into the same color. The visualization demonstrates the model's ability to differentiate between static and dynamic regions of the frames, and its capability of clustering similar movement patterns together.", "section": "A.7 Node Feature Visualization"}, {"figure_path": "4ztP4PujOG/figures/figures_18_2.jpg", "caption": "Figure 12: Locaiton feature visualization on three datasets.", "description": "This figure visualizes the location features extracted from three different datasets: Cityscapes, UCF Sports, and KITTI.  The location feature represents the spatial position of image patches.  A K-means clustering algorithm was applied to the location features, resulting in a color-coded visualization showing the distribution of the spatial patterns. The differences in the patterns across datasets demonstrate the varying characteristics of these datasets.", "section": "A.7 Node Feature Visualization"}]