[{"figure_path": "4ztP4PujOG/tables/tables_2_1.jpg", "caption": "Table 1: Motion representation comparison. We assess the representative ability by judging how accurate the motion can be described, especially for the hard cases demonstrated in Fig 1(A).", "description": "This table compares different motion representations used in video prediction, namely image difference, keypoint trace, optical flow/voxel flow, motion matrix, and the proposed motion graph.  For each method, it indicates whether an external model is needed to generate the motion representation, its ability to represent complex motion patterns (representative ability), and its space complexity.  The table highlights that while the motion matrix has high representative ability, it also has a high space complexity; conversely, the proposed motion graph offers a good balance between representative ability and space efficiency.", "section": "2 Related Works"}, {"figure_path": "4ztP4PujOG/tables/tables_6_1.jpg", "caption": "Table 2: Dataset configurations", "description": "This table presents the configurations used for each dataset in the video prediction experiments.  It shows the resolution of the images, the size of the smallest feature maps (Hs and Ws), the number of input frames used for prediction, the number of output frames predicted, the type of loss function used during training, and the hyperparameter k (which influences the number of dynamic vectors, temporal edges per node, and output dynamic vectors per pixel).", "section": "4 Experiments"}, {"figure_path": "4ztP4PujOG/tables/tables_7_1.jpg", "caption": "Table 3: Performance comparison on UCF Sports STRPM split", "description": "This table compares the performance of the proposed method with other state-of-the-art (SOTA) methods on the UCF Sports STRPM split dataset.  The performance is evaluated using two metrics: Peak Signal-to-Noise Ratio (PSNR) and Learned Perceptual Image Patch Similarity (LPIPS).  Results are shown for both the 5th (t=5) and 10th (t=10) future frames, providing a comprehensive comparison across various methods.", "section": "4.1 Public Benchmark Comparison"}, {"figure_path": "4ztP4PujOG/tables/tables_7_2.jpg", "caption": "Table 4: Performance comparison on the UCF Sports MMVP split.", "description": "This table presents a performance comparison of different video prediction methods on the UCF Sports MMVP dataset split into three categories based on the structural similarity index (SSIM) score: easy (SSIM \u2265 0.9), intermediate (0.6 < SSIM < 0.9), and hard (SSIM < 0.6).  The metrics used for comparison include SSIM, PSNR, and LPIPS for each category and the full dataset.  The model sizes of each method are also listed, demonstrating the efficiency of the proposed approach.", "section": "4.1 Public Benchmark Comparison"}, {"figure_path": "4ztP4PujOG/tables/tables_7_3.jpg", "caption": "Table 5: Evaluation on Cityscapes [39] and KITTI [37] datasets. \"RGB\", \"F\", \"S\" and \"I\" denote video frames, optical flow, semantic map, and instance map; t + 3 and t + 5 respectively indicate the average performance of the next 3 and 5 frames. Results with * are copied from [13].", "description": "This table presents a quantitative comparison of the proposed method with other state-of-the-art video prediction methods on the Cityscapes and KITTI datasets.  The evaluation metrics include Multi-scale Structural Similarity Index Measure (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) for different prediction horizons (t+1, t+3, t+5 frames).  The input modalities used in each method are also listed (RGB, optical flow (F), semantic map (S), instance map (I)).", "section": "4 Experiments"}, {"figure_path": "4ztP4PujOG/tables/tables_8_1.jpg", "caption": "Table 6: Model consumption analysis on three datasets, compared with the SOTA methods.", "description": "This table compares the model size and GPU memory usage of the proposed method with state-of-the-art (SOTA) methods on three datasets: UCF Sports, KITTI, and Cityscapes.  It highlights the significant reduction in model size and GPU memory utilization achieved by the proposed approach, demonstrating its efficiency.", "section": "4 Experiments"}, {"figure_path": "4ztP4PujOG/tables/tables_9_1.jpg", "caption": "Table 7: Ablation study on the value of k.", "description": "This table presents the ablation study on the impact of the hyperparameter k on the performance of the video prediction model.  The hyperparameter k controls multiple aspects of the model: the number of dynamic vectors each node initially embeds, the number of temporal forward/backward edges for each node, and the number of dynamic vectors to be decoded from the upsampled motion features. The table shows the SSIM, PSNR, LPIPS, and GPU memory usage for different values of k (1, 5, 8, 10, and 20).  It demonstrates how changing k affects the balance between performance and resource consumption.", "section": "4.2 Ablation Studies"}, {"figure_path": "4ztP4PujOG/tables/tables_9_2.jpg", "caption": "Table 8: Ablation study on the number of motion graph views and on the location feature  floc.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of the number of motion graph views and the inclusion of the location feature (floc) on the model's performance.  The study assesses the effect on SSIM, PSNR, and LPIPS metrics.  The results show that increasing the number of views improves performance, while excluding the location feature leads to a decrease in performance.", "section": "4.2 Ablation Studies"}, {"figure_path": "4ztP4PujOG/tables/tables_14_1.jpg", "caption": "Table 2: Dataset configurations", "description": "This table shows the configuration settings used for each dataset in the video prediction experiments.  It lists the resolution of the input videos, the size of the smallest feature maps (Hs x Ws) generated by the image encoder, the number of input frames used for prediction, the number of output (predicted) frames, the type of loss function used during training, and the value of the hyperparameter 'k'.  The 'k' value determines several aspects of the model, including the number of dynamic vectors per node in the motion graph, the number of temporal edges per node, and the number of dynamic vectors per pixel in the output.", "section": "4 Experiments"}, {"figure_path": "4ztP4PujOG/tables/tables_15_1.jpg", "caption": "Table 5: Evaluation on Cityscapes [39] and KITTI [37] datasets. \"RGB\", \"F\", \"S\" and \"I\" denote video frames, optical flow, semantic map, and instance map; t + 3 and t + 5 respectively indicate the average performance of the next 3 and 5 frames. Results with * are copied from [13].", "description": "This table presents a comparison of the proposed method's performance against existing methods on the Cityscapes and KITTI datasets for video prediction.  It shows the Multi-scale Structural Similarity Index Measure (MS-SSIM) and Peak Signal-to-Noise Ratio (PSNR) for the first frame (t+1), the average of the next three frames (t+3), and the average of the next five frames (t+5). The input modalities used are also specified (RGB, RGB+F, RGB+S, RGB+S+I, RGB+S+F). Results marked with * are taken directly from the cited DMVFN paper.", "section": "4 Experiments"}, {"figure_path": "4ztP4PujOG/tables/tables_16_1.jpg", "caption": "Table 11: Ablation study on the number of the predicted vectors. The experiments are conducted on UCF Sports MMVP splits. The listed results are from the models trained for 100 epochs (models were all trained for 300 epochs in the main manuscript).", "description": "This table presents the ablation study result on the number of predicted vectors used in the model. It shows the impact of varying the number of predicted vectors (k) on the model's performance, measured by SSIM, PSNR, and LPIPS metrics.  The study was conducted on the UCF Sports MMVP dataset split, and results are shown for both the full dataset and the subset with SSIM less than 0.6. Memory consumption is also listed to indicate the efficiency of the model with different k values.", "section": "4.2 Ablation Studies"}, {"figure_path": "4ztP4PujOG/tables/tables_16_2.jpg", "caption": "Table 12: Ablation study on graph interaction module. The experiments are conducted on KITTI and metrics show evaluation on the t + 1 results.", "description": "This table presents the ablation study results on the KITTI dataset for the motion graph interaction module. It shows the impact of using spatial edges, backward edges, and both on the performance metrics PSNR, MS-SSIM, and LPIPS for predicting the next frame (t+1). The results demonstrate that incorporating both spatial and temporal information significantly improves the video prediction accuracy.", "section": "4.2 Ablation Studies"}, {"figure_path": "4ztP4PujOG/tables/tables_16_3.jpg", "caption": "Table 1: Motion representation comparison. We assess the representative ability by judging how accurate the motion can be described, especially for the hard cases demonstrated in Fig 1(A).", "description": "This table compares different motion representation methods used in video prediction, including image difference, keypoint trace, optical flow/voxel flow, motion matrix, and the proposed motion graph.  For each method, it indicates whether an out-of-shell model is required, its representative ability (assessed by how well it captures complex motions), and its space complexity. The table highlights the advantages of the motion graph in terms of representational power and efficiency.", "section": "2 Related Works"}]