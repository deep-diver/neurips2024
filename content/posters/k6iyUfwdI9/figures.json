[{"figure_path": "k6iyUfwdI9/figures/figures_3_1.jpg", "caption": "Figure 1: Single-label queries with low epistemic uncertainty: Conditional normalized probability of the correct completion given repetitions of an incorrect response. Each figure shows the query and the considered two responses with their initial probabilities, as a response for the query, in parentheses (the first response is the correct one).", "description": "This figure shows the results of an experiment where the model is given a single-label query with low epistemic uncertainty.  The model is then prompted with the correct answer and multiple repetitions of an incorrect answer. The normalized probability of the correct response is plotted against the number of repetitions of the incorrect answer.  The results demonstrate that even with many repetitions of the incorrect answer, the probability of the correct response remains relatively high, indicating low epistemic uncertainty. Each subplot represents a different query.", "section": "Single-label queries with low epistemic uncertainty"}, {"figure_path": "k6iyUfwdI9/figures/figures_3_2.jpg", "caption": "Figure 2: Single-label queries with high epistemic uncertainty: Conditional normalized probability of the correct completion given repetitions of an incorrect response. Each figure shows the query and the considered two responses with their initial probabilities, as a response for the query, in parentheses (the first response is the correct one).", "description": "This figure displays four examples of single-label queries where the language model exhibits high epistemic uncertainty.  The graphs show how the normalized probability of the correct answer changes as an incorrect response is repeatedly added to the prompt.  In each case, the correct answer's probability significantly decreases as the incorrect response is repeated, indicating a lack of robust knowledge from the model.", "section": "Single-label queries with high epistemic uncertainty"}, {"figure_path": "k6iyUfwdI9/figures/figures_3_3.jpg", "caption": "Figure 3: Multi-label queries with aleatoric uncertainty: Conditional normalized probability of the first of the two provided responses, both of which are correct, given repetitions of the second response in the prompt. Each figure shows the query and the considered two responses with their initial probabilities, as a response for the query, in parentheses.", "description": "This figure shows four examples of multi-label queries with high aleatoric uncertainty.  The normalized probability of the first (correct) response is plotted against the number of repetitions of the second (also correct) response in the prompt.  The results demonstrate that even when multiple correct answers exist, the probability of a correct answer does not collapse to zero as the incorrect answer is repeated in the prompt. This contrasts with the behavior observed in single-label queries with high epistemic uncertainty, where the probability of the correct answer decreases significantly as the incorrect answer is repeated.", "section": "3 Probability amplification by iteratively prompting"}, {"figure_path": "k6iyUfwdI9/figures/figures_6_1.jpg", "caption": "Figure 1: Single-label queries with low epistemic uncertainty: Conditional normalized probability of the correct completion given repetitions of an incorrect response. Each figure shows the query and the considered two responses with their initial probabilities, as a response for the query, in parentheses (the first response is the correct one).", "description": "This figure displays the results of an experiment on single-label queries with low epistemic uncertainty.  The normalized probability of the correct answer is shown, given varying numbers of repetitions of an incorrect answer in the prompt. Each subplot represents a different query, showing the initial probabilities of the correct and incorrect answers. The x-axis represents the number of repetitions of the incorrect answer, and the y-axis represents the normalized probability of the correct answer. The low drop in probability of the correct response even with many repetitions of the incorrect response highlights that the model has low epistemic uncertainty regarding these queries.", "section": "Single-label queries with low epistemic uncertainty"}, {"figure_path": "k6iyUfwdI9/figures/figures_9_1.jpg", "caption": "Figure 4: PR-curve for the baseline and the proposed methods on various datasets. On the TriviaQA and AmbigQA datasets, M.I. and S.E. perform nearly identically, but they outperform the T0 and S.V. baselines. For the S.E. and M.I. methods, the responses for a large number of queries can be clustered into a single group, and therefore the semantic entropy and mutual information scores are zero. This is why the starting point of their curves is at a higher recall values. On the TriviaQA+WordNet and AmbigQA+WordNet datasets with a significant number of high entropy multi-label queries, M.I. outperforms the S.E. baseline. The methods perform nearly identical on the not shown recall area.", "description": "This figure presents the precision-recall curves for four different methods used for hallucination detection on four different datasets. The methods are compared based on their precision and recall in detecting hallucinated responses.  The datasets used are TriviaQA, AmbigQA, TriviaQA+WordNet, and AmbigQA+WordNet. The results show that the proposed mutual information (MI) method performs comparably to the semantic entropy (S.E.) method on single-label datasets (TriviaQA and AmbigQA), but significantly outperforms it on multi-label datasets (TriviaQA+WordNet and AmbigQA+WordNet).  The difference in performance is more pronounced when the recall is high.", "section": "Experiments"}, {"figure_path": "k6iyUfwdI9/figures/figures_9_2.jpg", "caption": "Figure 4: PR-curve for the baseline and the proposed methods on various datasets. On the TriviaQA and AmbigQA datasets, M.I. and S.E. perform nearly identically, but they outperform the T0 and S.V. baselines. For the S.E. and M.I. methods, the responses for a large number of queries can be clustered into a single group, and therefore the semantic entropy and mutual information scores are zero. This is why the starting point of their curves is at a higher recall values. On the TriviaQA+WordNet and AmbigQA+WordNet datasets with a significant number of high entropy multi-label queries, M.I. outperforms the S.E. baseline. The methods perform nearly identical on the not shown recall area.", "description": "This figure displays precision-recall curves for four different methods used to detect hallucination in language models. The methods are compared on four different datasets: TriviaQA, AmbigQA, TriviaQA+WordNet, and AmbigQA+WordNet. The TriviaQA and AmbigQA datasets consist primarily of single-label queries, while the TriviaQA+WordNet and AmbigQA+WordNet datasets contain a mix of single-label and multi-label queries. The results show that the mutual information (MI) and semantic entropy (SE) methods perform similarly well on the datasets with mainly single-label queries, significantly outperforming the baseline methods. However, on datasets containing a substantial proportion of multi-label queries, the MI method surpasses the SE method.", "section": "Experiments"}, {"figure_path": "k6iyUfwdI9/figures/figures_31_1.jpg", "caption": "Figure 6: Distributions of bounds on the missing mass. The left figure for each dataset presents the empirical distribution of the upper bounds on the missing mass E[Uk]. The middle figure presents the empirical distribution of \u016ak, the missing mass computed on a finite support approximation (where the support is obtained by taking samples from the LLM until a cumulative probability of 95% or 1000 samples are achieved). The right graph shows the empirical distribution of P(X), the cumulative probabilities of all responses generated by the language model. For each figure, one observation (sample) corresponds to a single query. The black curves represent the corresponding empirical cumulative distribution functions for the upper bounds on EUk and for Uk, and the empirical survival function (1 minus the empirical distribution function) for the distribution of P(X).", "description": "This figure shows the empirical distributions of three different quantities related to the missing mass for TriviaQA and AmbigQA datasets. The leftmost plot displays the distribution of upper bounds on the expected missing mass E[Uk].  The central plot shows the distribution of the missing mass \u016ak, computed using a finite support approximation of the LLM's output. Finally, the rightmost plot displays the distribution of the cumulative probabilities of all responses generated by the LLM.", "section": "F.4 Expected missing mass under Zipf distribution"}, {"figure_path": "k6iyUfwdI9/figures/figures_33_1.jpg", "caption": "Figure 4: PR-curve for the baseline and the proposed methods on various datasets. On the TriviaQA and AmbigQA datasets, M.I. and S.E. perform nearly identically, but they outperform the T0 and S.V. baselines. For the S.E. and M.I. methods, the responses for a large number of queries can be clustered into a single group, and therefore the semantic entropy and mutual information scores are zero. This is why the starting point of their curves is at a higher recall values. On the TriviaQA+WordNet and AmbigQA+WordNet datasets with a significant number of high entropy multi-label queries, M.I. outperforms the S.E. baseline. The methods perform nearly identical on the not shown recall area.", "description": "This figure compares the performance of different methods for detecting hallucination in LLMs across four datasets: TriviaQA, AmbigQA, TriviaQA+WordNet, and AmbigQA+WordNet.  The plots show precision-recall curves, where precision represents the accuracy of the hallucination detection, and recall represents the percentage of queries correctly identified as hallucinations.  The results indicate that the mutual information (MI) based method performs similarly to the semantic entropy (SE) method on datasets with mostly single-label queries but outperforms it significantly on datasets with a higher proportion of multi-label queries, highlighting the MI-based method's advantage in handling multiple valid answers.", "section": "Experiments"}, {"figure_path": "k6iyUfwdI9/figures/figures_33_2.jpg", "caption": "Figure 4: PR-curve for the baseline and the proposed methods on various datasets. On the TriviaQA and AmbigQA datasets, M.I. and S.E. perform nearly identically, but they outperform the T0 and S.V. baselines. For the S.E. and M.I. methods, the responses for a large number of queries can be clustered into a single group, and therefore the semantic entropy and mutual information scores are zero. This is why the starting point of their curves is at a higher recall values. On the TriviaQA+WordNet and AmbigQA+WordNet datasets with a significant number of high entropy multi-label queries, M.I. outperforms the S.E. baseline. The methods perform nearly identical on the not shown recall area.", "description": "This figure presents the Precision-Recall (PR) curves for four different methods used for hallucination detection in LLMs: the proposed mutual information (M.I.) method, semantic entropy (S.E.), greedy response probability (T0), and self-verification (S.V.).  The results are shown for four datasets: TriviaQA, AmbigQA, TriviaQA+WordNet (TriviaQA combined with WordNet data), and AmbigQA+WordNet (AmbigQA combined with WordNet data). The graphs show that the M.I. and S.E. methods generally outperform T0 and S.V., particularly on datasets with high entropy multi-label queries. The near-identical performance of M.I. and S.E. on the TriviaQA and AmbigQA datasets is explained by the fact that responses to many queries cluster closely, resulting in similar entropy and mutual information scores.", "section": "6 Experiments"}]