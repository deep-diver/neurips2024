[{"figure_path": "gnnmB7y0Xx/tables/tables_6_1.jpg", "caption": "Table 1: Performance of state vector optimization. The best results in the zero shot setting are in underline and the best results in the few shot setting are in bold. The result of basic state vector is mathematically equivalent to task vector. Note that we only present the results across six tasks here and leave the rest in the Appendix. We also report standard deviation and the results are passed with significance test (p < .05).", "description": "This table presents the performance comparison of different methods for in-context learning (ICL) on several tasks, using Llama-2-7B and GPT-J-6B language models.  The methods compared include regular baselines (zero-shot and few-shot ICL), using the task vector, function vector, and the proposed state vector method with inner and momentum optimization.  The table displays the average performance across different tasks and models, highlighting the improvements achieved by the proposed state vector optimization methods.  Statistical significance (p<.05) is indicated. ", "section": "5.3 Inner Optimization(RQ1)"}, {"figure_path": "gnnmB7y0Xx/tables/tables_7_1.jpg", "caption": "Table 2: Performance comparison of gradient optimization algorithms. The method means the optimization algorithm applied to the opt (.) in Eqn. 9.", "description": "This table compares the performance of different gradient optimization algorithms applied to the state vector optimization method proposed in the paper.  It shows the zero-shot and few-shot performance for several methods. The best performing method is highlighted, demonstrating the effectiveness of the chosen optimization strategy in improving ICL performance.  The results illustrate that the momentum-based optimization method yields the best performance, exceeding other first-order methods. ", "section": "5.4 Momentum Optimization (RQ2)"}, {"figure_path": "gnnmB7y0Xx/tables/tables_14_1.jpg", "caption": "Table 3: Text portability of momentum optimized state vector. The templates are provided with \"X\" replaced by a query word. \"+SV\" denotes adding momentum optimized state vector", "description": "This table presents the results of an experiment evaluating the effectiveness of the momentum-optimized state vector in generating natural text completions. Three different prompts were used, each containing a variable \"X\" representing a query word.  The table compares the performance of Llama-2-7B with and without the addition of the momentum-optimized state vector (+SV).  The results show a significant improvement in accuracy when the optimized state vector is included.", "section": "5.3 Inner Optimization(RQ1)"}, {"figure_path": "gnnmB7y0Xx/tables/tables_14_2.jpg", "caption": "Table 1: Performance of state vector optimization. The best results in the zero shot setting are in underline and the best results in the few shot setting are in bold. The result of basic state vector is mathematically equivalent to task vector. Note that we only present the results across six tasks here and leave the rest in the Appendix. We also report standard deviation and the results are passed with significance test (p < .05).", "description": "This table presents the performance comparison of different methods for in-context learning (ICL), including baselines like regular and ICL, task vector, and function vector, and the proposed state vector with inner and momentum optimization.  The results are shown for both zero-shot and few-shot settings across six tasks, indicating improvements achieved by the proposed methods in terms of accuracy. Statistical significance (p<0.05) is reported.", "section": "5.3 Inner Optimization(RQ1)"}, {"figure_path": "gnnmB7y0Xx/tables/tables_15_1.jpg", "caption": "Table 1: Performance of state vector optimization. The best results in the zero shot setting are in underline and the best results in the few shot setting are in bold. The result of basic state vector is mathematically equivalent to task vector. Note that we only present the results across six tasks here and leave the rest in the Appendix. We also report standard deviation and the results are passed with significance test (p < .05).", "description": "This table shows the performance comparison of different methods for in-context learning (ICL) on six tasks, including zero-shot and few-shot settings.  It compares the regular approach, function vector, task vector, and the proposed state vector methods (with inner and momentum optimization). The best results for each setting are highlighted. Standard deviations are reported, and statistical significance (p<0.05) is indicated.", "section": "5.3 Inner Optimization(RQ1)"}, {"figure_path": "gnnmB7y0Xx/tables/tables_15_2.jpg", "caption": "Table 1: Performance of state vector optimization. The best results in the zero shot setting are in underline and the best results in the few shot setting are in bold. The result of basic state vector is mathematically equivalent to task vector. Note that we only present the results across six tasks here and leave the rest in the Appendix. We also report standard deviation and the results are passed with significance test (p < .05).", "description": "This table presents the performance comparison of different methods for in-context learning (ICL) on six tasks using Llama-2-7B and GPT-J-6B language models.  The methods compared are regular ICL, function vector, task vector, state vector with inner optimization, state vector with momentum optimization, and the ICL baseline. Results are shown for both zero-shot and few-shot settings, with statistically significant improvements demonstrated by the proposed state vector optimization methods.", "section": "5.3 Inner Optimization(RQ1)"}, {"figure_path": "gnnmB7y0Xx/tables/tables_16_1.jpg", "caption": "Table 1: Performance of state vector optimization. The best results in the zero shot setting are in underline and the best results in the few shot setting are in bold. The result of basic state vector is mathematically equivalent to task vector. Note that we only present the results across six tasks here and leave the rest in the Appendix. We also report standard deviation and the results are passed with significance test (p < .05).", "description": "This table presents the performance comparison of different methods for in-context learning (ICL). The methods compared are the Regular baseline, Function Vector, Task Vector, State Vector with inner optimization, State Vector with momentum optimization, and the ICL baseline. The performance is evaluated on six different tasks across two different large language models (LLMs). The results show the effectiveness of the proposed state vector optimization methods.", "section": "5.3 Inner Optimization(RQ1)"}]