[{"figure_path": "fYa6ezMxD5/figures/figures_1_1.jpg", "caption": "Figure 1: MatFormer introduces nested structure into the Transformer's FFN block & trains all the submodels, enabling free extraction of hundreds of accurate submodels for elastic inference.", "description": "MatFormer modifies the standard Transformer architecture by incorporating a nested Feed Forward Network (FFN) block structure.  During training, it optimizes the parameters of multiple nested FFN blocks with varying sizes. This allows for the extraction of many accurate smaller models without incurring additional computational costs.  The figure illustrates this nested structure, showing how submodels of different sizes can be extracted from a single, larger model, enabling elastic inference across various deployment environments.", "section": "3 MatFormer"}, {"figure_path": "fYa6ezMxD5/figures/figures_5_1.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure presents the results comparing MatLM (MatFormer Language Model) and baseline models.  It shows validation loss, one-shot evaluation scores, and consistency with the largest model (XL) across different model sizes. The key takeaway is that MatLM, especially when using the Mix'n'Match technique, outperforms baselines and produces models with better accuracy and consistency.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_7_1.jpg", "caption": "Figure 3: We train various decoder-only MatLM models at a range of sizes from 78M to 850M parameters and observe the scaling trends of all granularities (S, M, L, XL) for validation loss and 1-shot downstream evaluation scores. We find that the MatLM-XL models across scales mimic the training trends of Baseline-XL models. Interestingly, we also note that that validation loss and downstream evaluations follow the scaling trends of the XL-models across all granularities.", "description": "This figure shows the scaling behavior of MatLM models of different sizes (78M to 850M parameters) compared to baseline Transformer Language Models. It demonstrates that the MatLM-XL models (largest models) across different sizes follow the same scaling trend as the baseline XL models, indicating good scalability.  The validation loss and 1-shot downstream evaluation scores also show similar scaling trends across all granularities (S, M, L, XL), highlighting the efficacy and consistency of the MatFormer approach across various model sizes.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_7_2.jpg", "caption": "Figure 5: MatViT natively enables elastic encoders for adaptive retrieval that can be used for real-time query side computation while retaining strong accuracy on ImageNet-1K, unlike the baselines.", "description": "This figure shows the results of an experiment comparing the performance of MatViT (a nested Transformer architecture) to baseline ViT models for adaptive image retrieval on ImageNet-1K.  MatViT's elastic inference allows extraction of submodels with varying parameter sizes, adapting to compute constraints.  The plot demonstrates that MatViT submodels, selected using the Mix'n'Match heuristic, retain high accuracy while significantly reducing compute cost compared to independently trained baseline ViT models of similar size. This highlights the advantage of MatViT's nested architecture in providing flexibility and efficiency.", "section": "4.2 MatViT: MatFormer Vision Transformers"}, {"figure_path": "fYa6ezMxD5/figures/figures_7_3.jpg", "caption": "Figure 4: MatViT variants match or outperform standard ViT models on ImageNet-1K classification and provide free extracted models that span the accuracy-compute curve through Mix'n'Match.", "description": "This figure shows the results of image classification experiments on ImageNet-1K using MatViT (MatFormer-based Vision Transformer) models.  Two versions of MatViT are presented: B/16 (trained on ImageNet-1K with AugReg) and L/16 (pretrained on ImageNet-21K and finetuned on ImageNet-1K).  The figure demonstrates that MatViT models, along with submodels extracted via the Mix'n'Match technique, either match or surpass the performance of standard ViT models.  The plots illustrate the accuracy-vs.-parameters trade-off, showing that smaller models extracted from MatViT efficiently cover the performance spectrum.", "section": "4.2 MatViT: MatFormer Vision Transformers"}, {"figure_path": "fYa6ezMxD5/figures/figures_8_1.jpg", "caption": "Figure 5: MatViT natively enables elastic encoders for adaptive retrieval that can be used for real-time query side computation while retaining strong accuracy on ImageNet-1K, unlike the baselines.", "description": "This figure compares the performance of MatViT (MatFormer-based Vision Transformer) and baseline ViT models for adaptive image retrieval on ImageNet-1K.  The x-axis represents the total number of parameters (in millions) used in the query encoder, and the y-axis represents the 1-NN (1-Nearest Neighbor) accuracy. The figure shows that MatViT models (both those explicitly trained and those extracted using Mix'n'Match) achieve comparable or better accuracy than baseline ViT models with significantly fewer parameters, demonstrating the effectiveness of MatViT for resource-efficient retrieval tasks.", "section": "4.2 MatViT: MatFormer Vision Transformers"}, {"figure_path": "fYa6ezMxD5/figures/figures_8_2.jpg", "caption": "Figure 5: MatViT natively enables elastic encoders for adaptive retrieval that can be used for real-time query side computation while retaining strong accuracy on ImageNet-1K, unlike the baselines.", "description": "This figure shows the results of using MatViT (MatFormer-based Vision Transformer) for adaptive image retrieval on ImageNet-1K.  It compares the 1-nearest neighbor (NN) accuracy of MatViT, Mix'n'Match (a submodel selection heuristic for MatFormer), and baseline ViT models across a range of model sizes (total parameters in millions). The results demonstrate that MatViT and Mix'n'Match models maintain high accuracy even with significantly fewer parameters compared to the baseline, highlighting their efficiency for real-time query-side computation.", "section": "4.2 MatViT: MatFormer Vision Transformers"}, {"figure_path": "fYa6ezMxD5/figures/figures_16_1.jpg", "caption": "Figure 6: We search for optimal architectrure configuration using evolutionary search [48] and with Mix'n'Match. We find that Mix'n'Match yields architectures lying on pareto-optimal curve, and are at least as good as those found using the evolutionary search.", "description": "This figure compares the performance of Mix'n'Match against Neural Architecture Search (NAS) for finding optimal submodels within a given parameter budget.  It shows that Mix'n'Match achieves comparable results to the more computationally expensive NAS approach, while also offering a simpler heuristic.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_19_1.jpg", "caption": "Figure 7: The smoother variant of consistency measures the KL divergence between the smaller models and the corresponding XL model. This metric, unlike the exact match accuracy variant, also accounts for different sampling strategies on the output distribution during deployment. In this figure, we plot KL divergence of S, M, L granularities with respect to XL for the 850M parameter model.", "description": "The figure shows the KL divergence between smaller models (S, M, L) and the largest model (XL) for different model sizes.  The KL divergence is a measure of the difference in probability distributions between two models. Lower KL divergence indicates higher consistency between models. The figure shows that MatFormer and Mix'n'Match consistently have smaller KL divergences compared to baselines, suggesting that MatFormer and Mix'n'Match produce smaller models that are more consistent with the largest model.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_20_1.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure shows the validation loss and one-shot downstream evaluation scores for an 850M parameter MatLM (Matryoshka Transformer Language Model) and its baseline models.  The results demonstrate that MatLM's submodels, obtained through a technique called Mix'n'Match, achieve better validation loss and one-shot downstream evaluation performance than independently trained baseline models.  Additionally, it highlights the consistency of the Mix'n'Match submodels compared to the full model, indicating they fall on the accuracy-parameter tradeoff curve.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_21_1.jpg", "caption": "Figure 9: We train various decoder-only MatLM models at a range of sizes from 78M to 850M parameters and observe the scaling trends for each model granularity on validation loss. We observe that the gap between MatLM and the baseline appears to be constant at each granularity. The consistency between the submodels of granularities and the XL models shows the effect of MatFormer joint training on natively ensuring similar behavior across submodels.", "description": "This figure shows the scaling behavior of MatLM models of various sizes (78M to 850M parameters) and granularities (S, M, L, XL).  It plots validation loss against the number of non-embedding parameters.  The key observation is that the performance difference between MatLM and baseline models remains relatively constant across all granularities and model sizes.  The figure also highlights the consistency of submodels (smaller models extracted from the largest MatLM model) in validation loss which is consistent with the findings of the paper.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_21_2.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure shows the results of experiments comparing MatLM (MatFormer Language Model) with baseline Transformer Language Models.  The plots illustrate validation loss, one-shot downstream evaluation scores, and consistency with the largest model (XL).  It demonstrates that MatLM outperforms baselines and that a simple heuristic called Mix'n'Match can create many accurate submodels.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_22_1.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure displays three sub-figures showing the validation loss, the one-shot downstream evaluation scores, and the consistency with the XL model for the 850M MatLM and its baseline models.  The results demonstrate that MatFormer (and Mix'n'Match) outperforms the baselines in terms of validation loss and downstream performance while maintaining consistency with the largest model.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_22_2.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure presents a comparison of validation loss and one-shot downstream evaluation scores for an 850M parameter MatLM (Matryoshka Transformer Language Model) and its baseline counterparts.  It demonstrates that MatLM, particularly when combined with the Mix'n'Match submodel selection technique, achieves better performance than independently trained models across various sizes. The results show that the Mix'n'Match approach generates models that follow the accuracy-versus-parameter trade-off curve established by the explicitly trained submodels, indicating a cost-effective way to obtain accurate models for diverse compute budgets.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_22_3.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure shows the comparison of validation loss and downstream evaluation scores between 850M parameter MatLM and its baseline models with different sizes.  It demonstrates that MatLM outperforms baselines across all granularities.  The Mix'n'Match technique, which enables the extraction of additional submodels, helps in generating accurate and consistent models, demonstrating the efficacy of MatFormer in creating models that span the performance versus compute trade-off curve.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_22_4.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure shows the validation loss and one-shot downstream evaluation results for an 850M parameter MatLM model and its baseline counterparts.  It demonstrates that MatLM outperforms the baselines across different model sizes (S,M,L,XL),  and that the Mix'n'Match approach used for MatLM yields models that are both accurate and highly consistent with the full model. The consistency of smaller models created using Mix'n'Match is particularly highlighted in a separate graph.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_22_5.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure presents the validation loss and one-shot downstream evaluation scores for an 850M parameter MatLM (Matryoshka Transformer Language Model) and its corresponding baseline models.  It demonstrates that MatFormer's Mix'n'Match technique produces smaller, more consistent submodels that maintain high accuracy across different computational budgets. The submodels are shown to fall on the curve defined by the explicitly trained models, highlighting the efficacy of the MatFormer approach.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_23_1.jpg", "caption": "Figure 1: MatFormer introduces nested structure into the Transformer's FFN block & trains all the submodels, enabling free extraction of hundreds of accurate submodels for elastic inference.", "description": "MatFormer introduces a nested structure into the feed-forward network (FFN) block of the Transformer.  During training, MatFormer optimizes multiple nested FFN blocks of varying sizes. This allows for the extraction of numerous accurate, smaller models without incurring additional computational costs.  The figure illustrates this nested structure, highlighting how the submodels can be mixed and matched during inference to achieve elastic inference across various deployment constraints.", "section": "3 MatFormer"}, {"figure_path": "fYa6ezMxD5/figures/figures_23_2.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure shows the comparison of validation loss and one-shot downstream evaluation scores between the 850M parameter MatLM and its baseline models.  The leftmost panel (a) displays validation loss, demonstrating that MatLM outperforms baselines across all granularities (S, M, L, XL).  The center panel (b) shows one-shot downstream evaluation scores, where again MatLM surpasses baselines.  Importantly, the rightmost panel (c) illustrates the consistency of submodels extracted via Mix'n'Match, highlighting that these models exhibit high consistency with the full MatLM-XL model.  This consistency is a key finding, suggesting that smaller, more efficient models can be obtained without sacrificing performance.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_23_3.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure shows the results of the experiments comparing MatLM and baseline models, in terms of validation loss, one-shot evaluation scores, and consistency with the largest model.  It highlights how MatLM outperforms baseline models, especially in terms of achieving better results with smaller models, and how the Mix'n'Match strategy helps in creating accurate and consistent submodels within a given parameter budget.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/figures/figures_23_4.jpg", "caption": "Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix'n'Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.", "description": "This figure shows three graphs comparing the performance of MatLM (MatFormer Language Model) with baseline Transformer models.  The first graph displays validation loss against the number of non-embedding parameters. The second graph shows one-shot downstream evaluation scores against the number of non-embedding parameters. The third graph presents the consistency of the smaller MatLM models with the largest MatLM model.  The results demonstrate that MatLM, particularly when using the Mix'n'Match technique, produces more accurate and consistent models that perform as well as or better than independently trained counterparts across a range of sizes. These models effectively occupy the optimal accuracy vs. parameters trade-off space.", "section": "4.1 MatLM: MatFormer Language Models"}]