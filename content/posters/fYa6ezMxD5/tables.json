[{"figure_path": "fYa6ezMxD5/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of MatFormer with comparable techniques across training and inference. We emphasize that in contrast with earlier work, MatFormer requires optimizing fewer models to obtain an exponential number of models at inference time without the need for post-training or NAS. Moreover, MatFormer subnetworks are nested, allowing for adaptive retrieval & colocation of models during inference. Here, l is the number of layers in the model and exp(l) refers to exponential in l.", "description": "This table compares MatFormer with other methods for obtaining multiple submodels from a single model.  It highlights MatFormer's advantages: requiring optimization of fewer models, producing an exponential number of models at inference time without post-training or Neural Architecture Search (NAS), and its nested subnetwork structure enabling adaptive retrieval and colocation. It also lists the number of models optimized and obtained, whether the subnetworks are nested, the model selection method, if post-training is needed, the architecture type (CNN or Transformer), and if the model is a decoder-only model.", "section": "Related Work"}, {"figure_path": "fYa6ezMxD5/tables/tables_6_1.jpg", "caption": "Table 2: Inference time speed-ups over a standard 850M model through speculative decoding using a 393M (S) draft and 850M (XL) verifier model.", "description": "This table presents the inference time speedup achieved by using speculative decoding with the smaller (393M parameters) and larger (850M parameters) submodels extracted from the 850M MatLM model, compared to standard autoregressive decoding of the 850M model. The speedup is shown for two downstream tasks: LAMBADA and TriviaQA.  The table also includes a row showing the additional speedup achieved by sharing the attention cache across the smaller and larger models.", "section": "4.1.1 Elastic Inference with MatLM"}, {"figure_path": "fYa6ezMxD5/tables/tables_7_1.jpg", "caption": "Table 3: Fitted parameters for the scaling equation: Loss(N, D) = a \u2022 (ND)^b + c", "description": "This table presents the fitted parameters (a, b, c) for the scaling law equation Loss(N, D) = a * (ND)^b + c, which describes the relationship between the loss, the number of non-embedding parameters (N), and the number of training tokens (D).  The parameters are fitted separately for both Baseline and MatFormer models, offering a comparison of their scaling behavior.", "section": "4.1.2 MatLM Scales as well as Vanilla Transformer LMs"}, {"figure_path": "fYa6ezMxD5/tables/tables_14_1.jpg", "caption": "Table 4: Model details for the models scales used to conduct the experiments described in Section 4.1, with a breakdown of total parameter counts, non-embedding parameter counts and FFN parameter counts for each model granularity.", "description": "This table provides detailed information about the language models used in the experiments of section 4.1.  For each model size (78M, 180M, 310M, 463M, and 850M parameters), it breaks down the total parameter count into the number of non-embedding parameters and FFN (Feed-Forward Network) parameters. It also specifies the model dimension (dmodel) and the number of training tokens used for each model.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/tables/tables_17_1.jpg", "caption": "Table 5: On 850M MatFormer model, while running NAS we observe it prefers balanced granularities across layers rather than skewed. On a few parameter constraints we list the MnM heuristic configuration, and configuration predicted with NAS.", "description": "This table compares the configurations of submodels generated by the Mix'n'Match heuristic and Neural Architecture Search (NAS) for various parameter budgets on an 850M MatFormer model.  The results show that NAS tends to favor balanced granularities (even distribution of different submodel granularities across layers), aligning with the Mix'n'Match heuristic, which prioritizes balanced configurations for optimal performance.", "section": "D Search Techniques"}, {"figure_path": "fYa6ezMxD5/tables/tables_19_1.jpg", "caption": "Table 6: For 850M model, we experiment with modifying {ps,pm,pl,pxl} to sample submodels from a non-uniform distribution during training and report the results across all granularities. We find that all strategies that upweight the loss for the largest granularity perform well, with modest degradation on the M and S granularties.", "description": "This table presents the results of experiments on an 850M parameter model where different probabilities were used for sampling submodels during training.  The goal was to investigate the impact of non-uniform sampling on model performance across different granularities. The results show that strategies which emphasize the largest granularity generally perform well, with only minor negative effects on smaller granularities.", "section": "F.3 Tuning Sampling Probability"}, {"figure_path": "fYa6ezMxD5/tables/tables_20_1.jpg", "caption": "Table 7: 2 \u00d7 2 grid of pairs to evaluate (top-1 accuracy (%)) the effects of MatFormer and standard training on the pretraining (PT) on ImageNet-21K and finetuning (FT) on ImageNet-1K using a L/16 architecture. Using a MatFormer during pretraining helps bring more accurate, and elastic encoders for downstream uses.", "description": "This table presents the top-1 accuracy results of different ViT models trained with and without MatFormer on ImageNet-1K.  It shows the impact of using MatFormer during pretraining and finetuning phases on the model's accuracy, demonstrating its effectiveness in producing more accurate and elastic models. Different model sizes are compared for both ViT and MatViT architectures.", "section": "4.2 MatViT: MatFormer Vision Transformers"}, {"figure_path": "fYa6ezMxD5/tables/tables_21_1.jpg", "caption": "Table 8: Image retrieval 1-NN accuracy (%) when the query and document encoders are the same model. Similar to the image classification results, MatViT variants either match or outperform the corresponding standard ViT counterparts. Note that all the smaller models of a given model in MatViT are extracted for free while the baselines have to be explicitly trained for the constraints.", "description": "This table presents the 1-nearest neighbor (NN) accuracy results for image retrieval experiments on ImageNet-1K.  The query and document encoders used are the same model.  The results show that the MatViT models (variants of Vision Transformers with a nested structure) either match or outperform the standard ViT (Vision Transformer) counterparts in terms of accuracy. A key point is that the smaller MatViT models are extracted at no additional cost, while the corresponding baseline ViT models of the same sizes would need to be explicitly trained.", "section": "4.2 MatViT: MatFormer Vision Transformers"}, {"figure_path": "fYa6ezMxD5/tables/tables_24_1.jpg", "caption": "Table 9: Downstream Eval numbers and development set log perplexity loss on 78M model size granularities.", "description": "This table presents the performance of different language models on 25 downstream tasks.  It compares the performance of baseline Transformer models with MatFormer models across four different granularities (S, M, L, XL) representing different model sizes.  The table includes both downstream evaluation metrics and development set log perplexity loss for each model and granularity, allowing for a comprehensive comparison of accuracy and efficiency.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/tables/tables_25_1.jpg", "caption": "Table 9: Downstream Eval numbers and development set log perplexity loss on 78M model size granularities.", "description": "This table presents the results of evaluating the 78M parameter MatLM model and its corresponding baseline models on a variety of downstream tasks.  The evaluation metrics include accuracy scores on tasks such as TriviaQA, Natural Questions, and HellaSwag, along with the development set log perplexity loss. The table shows the performance of MatLM compared to the baseline for each model granularity (S, M, L, XL).", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/tables/tables_25_2.jpg", "caption": "Table 9: Downstream Eval numbers and development set log perplexity loss on 78M model size granularities.", "description": "This table presents the results of downstream evaluation tasks and development set log perplexity loss for different model sizes (78M) and granularities (S, M, L, XL).  It compares the performance of baseline models with MatFormer models across various tasks, offering a detailed breakdown of the model's performance at different scales.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/tables/tables_26_1.jpg", "caption": "Table 9: Downstream Eval numbers and development set log perplexity loss on 78M model size granularities.", "description": "This table presents the results of evaluating various language models on 25 downstream tasks.  It compares the performance of baseline transformer models (Baseline-S, Baseline-M, Baseline-L, Baseline-XL) against MatFormer models (MatLM-S, MatLM-M, MatLM-L, MatLM-XL) of four different sizes (granularities) \u2014 78M parameters. The metrics used are accuracy scores for each task and the average log perplexity across all tasks for each model. The table highlights the relative performance differences between baseline and MatFormer models. ", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/tables/tables_26_2.jpg", "caption": "Table 9: Downstream Eval numbers and development set log perplexity loss on 78M model size granularities.", "description": "This table presents the performance of different models (Baseline and MatLM with granularities S, M, L, XL) on 25 downstream tasks.  It shows the accuracy and log perplexity loss for each model on each task, allowing for a comparison of the different model sizes and architectures. The log perplexity loss is a metric used to evaluate the language model's performance, where lower values indicate better performance.", "section": "4.1 MatLM: MatFormer Language Models"}, {"figure_path": "fYa6ezMxD5/tables/tables_27_1.jpg", "caption": "Table 9: Downstream Eval numbers and development set log perplexity loss on 78M model size granularities.", "description": "This table presents the performance of different language models on various downstream tasks.  It compares the performance of baseline models (Baseline-S, Baseline-M, Baseline-L, Baseline-XL) with MatFormer models (MatLM-S, MatLM-M, MatLM-L, MatLM-XL) at the 78M parameter scale.  Each model's performance is evaluated using various metrics (e.g., Exact Match, Accuracy) for 25 English tasks. The table also includes the development set log perplexity loss for each model, providing a comprehensive evaluation of their performance across diverse language understanding tasks.", "section": "4.1 MatLM: MatFormer Language Models"}]