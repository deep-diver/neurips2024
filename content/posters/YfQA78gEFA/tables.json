[{"figure_path": "YfQA78gEFA/tables/tables_7_1.jpg", "caption": "Table 1: Performance evaluation over 1K test instances. The bracket includes the number of models.", "description": "This table presents the performance evaluation results of different methods on 1000 test instances for two problem sizes (n=100 and n=200).  The methods compared include traditional solvers (Concorde, LKH3, HGS), a baseline neural method (POMO), and several robust versions of POMO obtained using different adversarial training techniques (POMO_AT, POMO_HAC, POMO_DivTrain) and the proposed collaborative neural framework (CNF). Performance is measured in terms of optimality gap and inference time across clean instances and instances subjected to different adversarial attacks.  The number of models used in ensemble methods is indicated in parentheses.", "section": "5 Experiments"}, {"figure_path": "YfQA78gEFA/tables/tables_9_1.jpg", "caption": "Table 1: Performance evaluation over 1K test instances. The bracket includes the number of models.", "description": "This table presents the performance comparison of various methods (traditional and neural) on three different scenarios: clean instances, instances with fixed adversarial attacks, and instances with adversarial attacks generated against the specific model.  It evaluates the optimality gap (percentage difference from the optimal solution) and the inference time for each method across different problem sizes (n=100 and n=200) for TSP and CVRP. The results show the impact of the collaborative neural framework (CNF) proposed in the paper compared with baseline methods.", "section": "5 Experiments"}, {"figure_path": "YfQA78gEFA/tables/tables_18_1.jpg", "caption": "Table 3: Robustness study of DIMES [56] on 1000 TSP100 instances. G, S, MCTS, and AS denote greedy, sample, Monte Carlo tree search, and active search, respectively.", "description": "This table presents the results of a robustness study on the DIMES method for solving the Traveling Salesman Problem (TSP) with 100 cities.  It compares the performance of DIMES using different search strategies (greedy, sample, Monte Carlo tree search, and active search) on both clean and adversarial instances (Fixed Adv.). The \"Gap\" column represents the difference between the solution cost obtained by DIMES and the optimal solution cost, expressed as a percentage. The \"Time\" column indicates the computation time required for each method. The table demonstrates the effect of different search methods on the robustness of DIMES to adversarial attacks.", "section": "5. Experiments"}, {"figure_path": "YfQA78gEFA/tables/tables_18_2.jpg", "caption": "Table 1: Performance evaluation over 1K test instances. The bracket includes the number of models.", "description": "This table presents the performance comparison of various methods on 1000 test instances of TSP and CVRP problems.  The methods compared include traditional optimization solvers (Concorde, LKH3, HGS) and neural network-based methods (POMO, POMO_AT, POMO_AT (3), POMO_HAC (3), POMO_DivTrain (3), CNF_Greedy (3), CNF (3)). The evaluation metrics are optimality gap and inference time.  The optimality gap represents the difference between the obtained solution cost and the optimal solution cost, showing the solution quality.  The table shows performance on clean instances and adversarial instances generated by various attack methods (Uniform, Fixed Adv, Adv) to assess the robustness of different methods. The number in brackets indicates the number of models used in ensemble-based methods.", "section": "5 Experiments"}, {"figure_path": "YfQA78gEFA/tables/tables_24_1.jpg", "caption": "Table 1: Performance evaluation over 1K test instances. The bracket includes the number of models.", "description": "This table presents the performance comparison of different methods on 1000 test instances of TSP and CVRP problems, including traditional methods (Concorde, LKH3, HGS) and neural methods (POMO, POMO_AT, POMO_HAC, POMO_DivTrain, CNF).  The performance metrics shown are optimality gap and inference time. Results are shown for clean instances and instances with adversarial attacks generated using uniform and fixed adversarial attacks and adversarial attacks based on the tested model. The table allows readers to directly compare the robustness and efficiency of various methods against different adversarial attacks.", "section": "5 Experiments"}, {"figure_path": "YfQA78gEFA/tables/tables_24_2.jpg", "caption": "Table 1: Performance evaluation over 1K test instances. The bracket includes the number of models.", "description": "This table presents the performance comparison of various methods on 1000 test instances of TSP and CVRP problems.  The methods include traditional solvers (Concorde, LKH3, HGS) and neural methods (POMO, POMO_AT, POMO_AT (3), POMO_HAC (3), POMO_DivTrain (3), CNF_Greedy (3), CNF (3)). The performance is evaluated under three scenarios: clean instances, instances with fixed adversarial attacks, and instances with adaptive adversarial attacks. For each method and scenario, the optimality gap and inference time are reported. The numbers in brackets indicate the number of models used in ensemble methods.", "section": "5 Experiments"}, {"figure_path": "YfQA78gEFA/tables/tables_25_1.jpg", "caption": "Table 1: Performance evaluation over 1K test instances. The bracket includes the number of models.", "description": "This table presents the performance comparison of different methods on 1000 test instances of TSP and CVRP problems with different sizes (n=100 and n=200).  The methods compared include traditional optimization solvers (Concorde, LKH3, HGS), the baseline neural VRP method (POMO), and various robust versions of POMO incorporating different defense strategies (POMO_AT, POMO_HAC, POMO_DivTrain).  The proposed CNF method is also included for comparison.  The table shows the optimality gap (percentage difference between obtained solution and the optimal solution) and the inference time for each method.  Separate results are presented for clean instances and instances that have been adversarially perturbed using different attacks, namely uniform, fixed adversarial, and adversarial attacks.", "section": "5 Experiments"}, {"figure_path": "YfQA78gEFA/tables/tables_26_1.jpg", "caption": "Table 1: Performance evaluation over 1K test instances. The bracket includes the number of models.", "description": "This table presents the performance comparison of different methods on various TSP and CVRP instances.  The methods compared include traditional optimization solvers (Concorde, LKH3, HGS),  a baseline neural VRP method (POMO), and several variations of POMO incorporating adversarial training techniques (POMO_AT, POMO_HAC, POMO_DivTrain).  The proposed CNF method is also included. Performance is measured by the optimality gap (percentage difference between the solution cost and the optimal cost) and the inference time.  Different adversarial attack methods are tested against each method (Uniform, Fixed Adv., Adv.), representing variations in the difficulty of the test cases.", "section": "5 Experiments"}, {"figure_path": "YfQA78gEFA/tables/tables_27_1.jpg", "caption": "Table 1: Performance evaluation over 1K test instances. The bracket includes the number of models.", "description": "This table presents the performance comparison of different methods on 1000 test instances of TSP and CVRP problems. The methods compared include traditional solvers (Concorde, LKH3, HGS), a baseline neural method (POMO), and several variations of the baseline incorporating adversarial training techniques (POMO_AT, POMO_HAC, POMO_DivTrain). The proposed method (CNF) is also included. Performance is evaluated on three types of instances: clean instances, instances with fixed adversarial attacks, and instances with adaptive adversarial attacks.  The results show the optimality gap (percentage difference between the solution found and the optimal solution) and the computation time for each method.", "section": "5 Experiments"}, {"figure_path": "YfQA78gEFA/tables/tables_27_2.jpg", "caption": "Table 1: Performance evaluation over 1K test instances. The bracket includes the number of models.", "description": "This table presents the performance comparison of different methods on 1000 test instances of TSP and CVRP problems with various attack settings (clean, uniform adversarial, fixed adversarial, and adaptive adversarial).  The methods compared include traditional solvers (Concorde, LKH3, HGS), the baseline neural method (POMO), and variants incorporating adversarial training (POMO_AT, POMO_HAC, POMO_DivTrain), as well as the proposed collaborative neural framework (CNF).  The table shows the optimality gap and inference time for each method and problem type. The bracket indicates the number of models used for ensemble methods.", "section": "5 Experiments"}]