{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-09", "reason": "This paper is foundational for the field of instruction-following LLMs, a key element in improving reasoning capabilities."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduces Llama 2, a large language model used as the backbone for the experiments in the current paper, significantly impacting its results."}, {"fullname_first_author": "Freda Shi", "paper_title": "Language models are multilingual chain-of-thought reasoners", "publication_date": "2023-05-05", "reason": "This paper introduces MGSM, a key multilingual mathematical reasoning dataset used in evaluating the proposed MindMerger method."}, {"fullname_first_author": "Longhui Yu", "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models", "publication_date": "2023-09-12", "reason": "This paper presents a method for generating mathematical reasoning questions, contributing to the resources available for multilingual reasoning research."}, {"fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "publication_date": "2023-10-06", "reason": "This paper introduces Mistral 7B, another large language model used in the supplementary experiments to validate the generality of the proposed MindMerger method."}]}