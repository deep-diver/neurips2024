{"references": [{"fullname_first_author": "Vardan Papyan", "paper_title": "Prevalence of neural collapse during the terminal phase of deep learning training", "publication_date": "2020-09-22", "reason": "This paper is the seminal work introducing the concept of neural collapse and its properties, which forms the foundation of the current research."}, {"fullname_first_author": "Ronen Eldan", "paper_title": "TinyStories: How small can language models be and still speak coherent english?", "publication_date": "2023-05-00", "reason": "This paper introduces the TinyStories dataset, a crucial resource used in the current study for its controlled nature and suitability for investigating neural collapse in language models."}, {"fullname_first_author": "Tomer Galanti", "paper_title": "On the role of neural collapse in transfer learning", "publication_date": "2022-12-00", "reason": "This paper extends the understanding of neural collapse by exploring its relationship to transfer learning, a relevant concept given the pre-training nature of language models."}, {"fullname_first_author": "Weiyang Liu", "paper_title": "Generalizing and decoupling neural collapse via hyperspherical uniformity gap", "publication_date": "2023-03-00", "reason": "This paper proposes a generalization of the neural collapse framework, addressing limitations of the original model which are relevant to the high-dimensional nature of language modeling."}, {"fullname_first_author": "Jiachen Jiang", "paper_title": "Generalized neural collapse for a large number of classes", "publication_date": "2023-10-00", "reason": "This paper directly addresses the challenge of applying neural collapse to scenarios with numerous classes, similar to the vocabulary size in language models."}]}