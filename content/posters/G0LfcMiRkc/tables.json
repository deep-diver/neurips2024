[{"figure_path": "G0LfcMiRkc/tables/tables_8_1.jpg", "caption": "Table 1: Permutation test of NC measurements with respect to validation loss. Twenty-one (21) identical two-layer 768-wide models were trained with different data shuffling seeds and permuted with 104 trials. The p-values below 0.05 (bolded) show those properties to be statistically significant.", "description": "This table presents the results of a permutation test performed to assess the statistical significance of the correlation between neural collapse (NC) properties and generalization performance (validation loss).  Twenty-one identical models were trained with different random data shuffles, and a permutation test was run with 104 trials to control for the effects of random data shuffling.  The table shows the R-squared value of the correlation, the correlation coefficient, and the p-value for each NC property, indicating which correlations are statistically significant (p < 0.05).", "section": "5.1 Neural Collapse's Relationship with Generalization"}, {"figure_path": "G0LfcMiRkc/tables/tables_20_1.jpg", "caption": "Table 2: Sample architectural configuration for a 12-layer 1024-dimensional causal language model (CLM) based on [2] and GPT-Neo [80]. Shallower models have configurations with attention_layers and attention_types truncated. Narrower models adjust hidden_size to their width (d). All other configuration values are the same across models.", "description": "This table shows the hyperparameters used to train the causal language models in the experiments.  The base configuration is for a 12-layer model with a hidden size of 1024.  Variations in depth (number of layers) and width (hidden size) are indicated in the caption.", "section": "Model Architectural Details"}, {"figure_path": "G0LfcMiRkc/tables/tables_21_1.jpg", "caption": "Table 3: Batch sizes used to train models on a single NVIDIA A100 (40GB) GPU. Width (d) and depth (L) correspond to hidden_size and length of attention_layers, respectively, in Table 2.", "description": "This table shows the batch sizes used for training the causal language models on a single NVIDIA A100 GPU with 40GB memory.  The batch size varied depending on the model's depth (number of layers) and width (embedding dimension).  The table is organized to show how batch size changed across different model configurations.", "section": "3 Optimization"}, {"figure_path": "G0LfcMiRkc/tables/tables_34_1.jpg", "caption": "Table 1: Permutation test of NC measurements with respect to validation loss. Twenty-one (21) identical two-layer 768-wide models were trained with different data shuffling seeds and permuted with 104 trials. The p-values below 0.05 (bolded) show those properties to be statistically significant.", "description": "This table presents the results of a statistical test investigating the correlation between Neural Collapse (NC) properties and generalization performance (measured by validation loss).  Twenty-one identical models were trained with different random initializations (data shuffling seeds), and a permutation test was conducted to determine the statistical significance of any observed correlations.  The table shows the R-squared values (measuring the strength of the correlation), and p-values (indicating statistical significance).  Bold p-values below 0.05 indicate statistically significant correlations between the specific NC property and generalization performance.", "section": "5.1 Neural Collapse's Relationship with Generalization"}, {"figure_path": "G0LfcMiRkc/tables/tables_35_1.jpg", "caption": "Table 5: Under TinyStories-12x1024_10L, the variability and interference of some English first names were far below those of the average token. This might be because names are distinct from one another and are not typically used in the same contexts as other words (aside from articles). The only names to have CDNV close to that of the average token are \u201cAnna\u201d and \u201cTim\u201d. Note that the positive interference of the last row (average token) is not a typo.", "description": "This table shows the variability (CDNV) and interference of several English first names.  It demonstrates that most names have lower variability and interference compared to the average token, which could be due to their distinctness and limited contextual overlap with other words.  The exception is the names \"Anna\" and \"Tim\", which exhibit variability and interference closer to the average.", "section": "Q Examples for Interpretability"}]