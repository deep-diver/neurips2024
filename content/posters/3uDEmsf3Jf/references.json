{"references": [{"fullname_first_author": "Rafael Figueiredo Prudencio", "paper_title": "A survey on offline reinforcement learning: Taxonomy, review, and open problems", "publication_date": "2023", "reason": "This paper provides a comprehensive overview of offline reinforcement learning, which is the foundation of the current research in offline safe reinforcement learning."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-04-22", "reason": "This paper introduces D4RL, a benchmark dataset widely used in offline reinforcement learning, which forms the basis of many experiments in this paper."}, {"fullname_first_author": "Ilya Kostrikov", "paper_title": "Offline reinforcement learning with Fisher divergence critic regularization", "publication_date": "2021", "reason": "This paper addresses the issue of distribution shift in offline reinforcement learning by using Fisher divergence, which is a key idea that this paper builds upon."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "Off-policy deep reinforcement learning without exploration", "publication_date": "2019", "reason": "This paper introduces BCQ, a crucial algorithm used as a baseline in this paper's experiments, demonstrating its importance in the field."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020", "reason": "This paper introduces Conservative Q-learning, another important baseline algorithm used in this paper's experiments, highlighting its significance in the field."}]}