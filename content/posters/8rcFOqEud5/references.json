{"references": [{"fullname_first_author": "Hunter Lightman", "paper_title": "Let's verify step by step", "publication_date": "2023-05-20", "reason": "This paper introduces a method for verifying the reasoning steps of LLMs, a key concept in ReST-MCTS* for enhancing the quality of reasoning traces."}, {"fullname_first_author": "Eric Zelikman", "paper_title": "Star: Bootstrapping reasoning with reasoning", "publication_date": "2022-12-01", "reason": "STaR is a key baseline in ReST-MCTS*, providing a comparative approach for LLM self-training based on reward signals."}, {"fullname_first_author": "Avi Singh", "paper_title": "Beyond human data: Scaling self-training for problem-solving with language models", "publication_date": "2023-12-23", "reason": "ReSTEM, another key baseline, is compared with ReST-MCTS* in terms of LLM self-training effectiveness."}, {"fullname_first_author": "Arian Hosseini", "paper_title": "V-star: Training verifiers for self-taught reasoners", "publication_date": "2024-02-24", "reason": "V-STaR offers a comparative self-training approach that utilizes a verifier, which is similar to one of the aspects of ReST-MCTS*."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "Chain-of-thought prompting is a crucial technique for LLM reasoning, and it is explicitly mentioned and used in ReST-MCTS*."}]}