[{"figure_path": "8rcFOqEud5/tables/tables_1_1.jpg", "caption": "Table 1: Key differences between existing self-improvement methods and our approach. Train refers to whether to train a reward model.", "description": "This table compares several existing self-improvement methods for LLMs with the proposed ReST-MCTS*.  It highlights key differences in reasoning policy methods (e.g., Chain of Thought, Best-of-N, MCTS), reward guidance (e.g., final outcome reward, per-step process reward), the source of reward labels (e.g., human annotation, ground-truth, inferred), and whether a reward model was trained.  The table shows that ReST-MCTS* differs from previous methods by using process rewards inferred from tree search and employing multi-iterative training for both policy and reward models.", "section": "1 Introduction"}, {"figure_path": "8rcFOqEud5/tables/tables_3_1.jpg", "caption": "Table 2: Primary results by training both policy and value model for multiple iterations. For each backbone, different self-training approaches are conducted separately. This means each approach has its own generated train data and corresponding reward (value) model. Our evaluation is zero-shot only, the few-shot baseline only serves as a comparison.", "description": "This table presents the results of training policy and value models across multiple iterations using various self-training methods.  It compares the performance of ReST-MCTS* against ReSTEM and Self-Rewarding methods on three different LLM backbones (LLaMA-3-8B-Instruct, Mistral-7B: MetaMATH, and SciGLM-6B). The evaluation is zero-shot, with few-shot results provided as a baseline.  The table highlights the continuous improvement of ReST-MCTS* across iterations.", "section": "4 Experiments"}, {"figure_path": "8rcFOqEud5/tables/tables_4_1.jpg", "caption": "Table 2: Primary results by training both policy and value model for multiple iterations. For each backbone, different self-training approaches are conducted separately. This means each approach has its own generated train data and corresponding reward (value) model. Our evaluation is zero-shot only, the few-shot baseline only serves as a comparison.", "description": "This table presents the results of experiments comparing different self-training approaches across multiple iterations.  The approaches are evaluated using three different Large Language Model (LLM) backbones: LLaMA-3-8B-Instruct, Mistral-7B, and SciGLM-6B.  Each approach undergoes two iterations of self-training, and results are shown for zero-shot and few-shot evaluations. The metrics used include MATH, GPQA, Diamond, and CEval-Hard, providing a comprehensive comparison of the performance of different methods.", "section": "4 Experiments"}, {"figure_path": "8rcFOqEud5/tables/tables_6_1.jpg", "caption": "Table 2: Primary results by training both policy and value model for multiple iterations. For each backbone, different self-training approaches are conducted separately. This means each approach has its own generated train data and corresponding reward (value) model. Our evaluation is zero-shot only, the few-shot baseline only serves as a comparison.", "description": "This table presents the zero-shot and few-shot experimental results of three different LLMs (LLaMA-3-8B-Instruct, Mistral-7B: MetaMATH, and SciGLM-6B) across various self-training methods (ReSTEM, Self-Rewarding, and ReST-MCTS*) over two iterations.  It shows how each self-training method performs on MATH, GPQA, Diamond, and CEval-Hard benchmarks and highlights the continuous improvement of ReST-MCTS* across iterations.", "section": "4 Experiments"}, {"figure_path": "8rcFOqEud5/tables/tables_7_1.jpg", "caption": "Table 3: Accuracy of different verifiers on GSM8K test set and MATH500. SC: Self-Consistency, MS: MATH-SHEPHERD. Verification is based on 256 outputs.", "description": "This table compares the accuracy of different verification methods (Self-Consistency, Outcome Reward Model, Self-Consistency + Outcome Reward Model, MATH-SHEPHERD, Self-Consistency + MATH-SHEPHERD, and Self-Consistency + ReST-MCTS*) on two datasets (GSM8K and MATH500).  Each method's accuracy is evaluated based on 256 outputs.  The table highlights the superior performance of ReST-MCTS* in achieving higher accuracy on both datasets.", "section": "4.2 Evaluating Self-Improvement of ReST-MCTS*"}, {"figure_path": "8rcFOqEud5/tables/tables_8_1.jpg", "caption": "Table 2: Primary results by training both policy and value model for multiple iterations. For each backbone, different self-training approaches are conducted separately. This means each approach has its own generated train data and corresponding reward (value) model. Our evaluation is zero-shot only, the few-shot baseline only serves as a comparison.", "description": "This table presents the results of training both the policy and value models for multiple iterations using different self-training methods.  The performance is evaluated using zero-shot settings on several benchmarks, comparing against a few-shot baseline.  Each self-training method uses its own generated training data and corresponding reward model.", "section": "4 Experiments"}, {"figure_path": "8rcFOqEud5/tables/tables_16_1.jpg", "caption": "Table 1: Key differences between existing self-improvement methods and our approach. Train refers to whether to train a reward model.", "description": "This table compares the proposed ReST-MCTS* approach with existing LLM self-improvement methods such as STaR, ReSTEM, RFT, and V-STaR.  The comparison highlights key differences in reasoning policies (e.g., whether they use chain-of-thought, best-of-N, or Monte Carlo Tree Search), reward guidance (type of reward signals used), and whether a reward model is trained. ReST-MCTS* is shown to be unique in its use of process reward guidance with MCTS* for collecting high-quality reasoning traces.", "section": "1 Introduction"}, {"figure_path": "8rcFOqEud5/tables/tables_25_1.jpg", "caption": "Table 6: The average running time of different algorithms (under our basic experiment settings) on a single question.", "description": "This table presents the average running time, in seconds, for four different reasoning methods: CoT + SC, ORM + BoN, PRM + BoN, and MCTS*.  The times are broken down for MATH problems.  It highlights the computational cost differences between the methods, showing that MCTS* requires significantly more time than the others.", "section": "E Experimental Details"}, {"figure_path": "8rcFOqEud5/tables/tables_26_1.jpg", "caption": "Table 8: Overall performance comparison with representative models on SciEval.", "description": "This table presents a comparison of the overall performance of three different reasoning methods (CoT, ToT, and ReST-MCTS*) on the SciEval benchmark.  The results are broken down by four parts of the benchmark (Part I, Part II, Part III, Part IV) and an overall average.  The table shows the performance of these methods using two different large language models (LLMs): GLM4 and GPT-3.5-Turbo.  The purpose is to demonstrate the superior performance of the ReST-MCTS* method compared to other more traditional reasoning methods.", "section": "4 Experiments"}]