[{"figure_path": "pwRVGRWtGg/tables/tables_2_1.jpg", "caption": "Table 1: Information of self-report measures used to assess specific emotions.", "description": "This table provides detailed information on eight different self-report measures used in the study to assess specific negative emotions.  Each measure is identified by its name and abbreviation, along with the citation of its source. The table also lists the emotion being measured, the number of items in the scale, the number of response levels, and the specific subscales used (if applicable). This allows for a better understanding of how the researchers measured the emotional responses in both LLMs and human subjects.", "section": "Measuring Emotions"}, {"figure_path": "pwRVGRWtGg/tables/tables_5_1.jpg", "caption": "Table 2: Results from the OpenAI's GPT models and human subjects. Default scores are expressed in the format of M \u00b1 SD. The changes are compared to the default scores. The symbol \"-\" denotes no significant differences.", "description": "This table presents the results of a comparison between three OpenAI GPT models (Text-Davinci-003, GPT-3.5-Turbo, GPT-4) and human subjects' responses across various factors related to eight negative emotions.  It shows the default emotional scores (mean \u00b1 standard deviation), and the changes in these scores following exposure to specific situations.  The \"-\" indicates no statistically significant difference between the scores before and after the situation.", "section": "C.2 OpenAI Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_6_1.jpg", "caption": "Table 10: Results from the Meta's AI LLaMA family. Default scores are expressed in the format of M\u00b1SD. The changes are compared to the default scores. The symbol '-' denotes no significant differences.", "description": "This table presents the results from evaluating four different LLMs from Meta's LLaMA family (LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, LLaMA-3.1-8B-Instruct, and Mixtral-8x22B-Instruct) on the same set of situations used in the human evaluation.  The table shows the default (baseline) scores and the changes in scores (positive and negative affect) for each LLM after being presented with the specific situations. The changes are compared against their respective default scores, helping to understand how well each LLM appraises different situations and evokes appropriate emotional responses.  A '-' indicates no statistically significant differences in the changes before and after the presentation of the situations.", "section": "C.3 LLAMA Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_6_2.jpg", "caption": "Table 4: Results of GPT-3.5-Turbo on positive or neutral situations. The changes are compared to the original negative situations. The symbol \u201c-\u201d denotes no significant differences.", "description": "This table presents the results of applying GPT-3.5-Turbo to positive or neutral situations and compares the changes in emotional responses to those observed in negative situations.  The table shows the change in positive and negative scores for each of the eight emotions (Anger, Anxiety, Depression, Frustration, Jealousy, Guilt, Fear, and Embarrassment) when the model is exposed to positive/neutral scenarios. A \u2018-\u2019 symbol indicates no statistically significant difference between the changes observed in positive/neutral situations compared to negative ones.", "section": "4.2 RQ2: Comprehending Positive Emotions"}, {"figure_path": "pwRVGRWtGg/tables/tables_7_1.jpg", "caption": "Table 5: Results of GPT-3.5-Turbo on challenging benchmarks. The changes are compared to the default scores. The symbol \"-\" denotes no significant differences.", "description": "This table presents the results of evaluating GPT-3.5-Turbo's performance on more complex scales designed to measure emotions, beyond the simple PANAS scale.  It assesses the model's ability to connect disparate situations based on shared underlying emotions. The table shows the default scores (the average emotional response without a specific situation), and the changes in scores observed after presenting the model with emotionally challenging situations.  Each row represents a specific emotion and the corresponding scale used for measurement. The changes are presented in the format of the average change in score and an indication of statistical significance.", "section": "RQ3: Challenging Benchmarks"}, {"figure_path": "pwRVGRWtGg/tables/tables_17_1.jpg", "caption": "Table 2: Results from the OpenAI's GPT models and human subjects. Default scores are expressed in the format of M \u00b1 SD. The changes are compared to the default scores. The symbol \"-\" denotes no significant differences.", "description": "This table presents the results of an experiment comparing the performance of three OpenAI GPT models (Text-Davinci-003, GPT-3.5-Turbo, and GPT-4) and human subjects across various factors and emotions.  The default scores (M \u00b1 SD) represent the baseline emotional responses before being exposed to specific situations, while the changes reflect the differences in emotional responses after situation exposure.  A '-' indicates no statistically significant difference between default and evoked scores. The table highlights the variation in emotional response across the different models and the comparison with human response.", "section": "C.2 OpenAI Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_18_1.jpg", "caption": "Table 2: Results from the OpenAI's GPT models and human subjects. Default scores are expressed in the format of M \u00b1 SD. The changes are compared to the default scores. The symbol \"-\" denotes no significant differences.", "description": "This table presents a comparison of the results from three OpenAI GPT models (Text-Davinci-003, GPT-3.5-Turbo, and GPT-4) and human subjects across various emotional factors.  The 'Default' scores represent the baseline emotional state before exposure to specific situations, while 'P' and 'N' columns show changes in positive and negative affect scores respectively after exposure to situations.  The table highlights significant differences (or lack thereof) between the models and humans.", "section": "C.2 OpenAI Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_19_1.jpg", "caption": "Table 2: Results from the OpenAI's GPT models and human subjects. Default scores are expressed in the format of M \u00b1 SD. The changes are compared to the default scores. The symbol \"-\" denotes no significant differences.", "description": "This table presents the results of a comparison between three OpenAI GPT models (Text-Davinci-003, GPT-3.5-Turbo, and GPT-4) and human subjects on their emotional responses to various situations.  The table shows the default emotional scores (mean \u00b1 standard deviation) for each model and the human subjects.  It then presents the changes in emotional scores (positive and negative affect) after exposure to the different situations, relative to the default scores.  A '-' indicates no statistically significant difference.", "section": "C.2 OpenAI Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_20_1.jpg", "caption": "Table 2: Results from the OpenAI's GPT models and human subjects. Default scores are expressed in the format of M \u00b1 SD. The changes are compared to the default scores. The symbol \"-\" denotes no significant differences.", "description": "This table presents the results of a comparison between three OpenAI GPT models (Text-Davinci-003, GPT-3.5-Turbo, GPT-4) and human subjects in terms of emotional responses to various situations.  The 'Default' scores represent the baseline emotional states before exposure to the situations. The 'P' and 'N' columns show the changes (increases or decreases) in positive and negative affect scores, respectively, after the models and humans imagined the specific situations.  The table includes statistical significance testing (indicated by the '-' symbol for non-significant differences).", "section": "C.2 OpenAI Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_21_1.jpg", "caption": "Table 9: Results from the OpenAI's GPT family and human subjects. Default scores are expressed in the format of M \u00b1 SD. The changes are compared to the default scores. The symbol \"-\" denotes no significant differences.", "description": "This table presents the results of an experiment comparing the performance of three OpenAI GPT models (Text-Davinci-003, GPT-3.5-Turbo, and GPT-4) against human subjects in an emotion appraisal task.  The results are presented as the mean (M) and standard deviation (SD) of the scores, showing changes in positive and negative affect scores before and after exposure to various situations.  The table is broken down by emotion (Anger, Anxiety, Depression, etc.) and factors contributing to each emotion.  A \"-\" indicates no significant difference from the default state.", "section": "C.2 OpenAI Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_22_1.jpg", "caption": "Table 3: Results from the open-source models. Default scores are expressed in the format of M \u00b1 SD. The changes are compared to the default scores. \u201c-\u201d denotes no significant differences.", "description": "This table presents the results obtained from evaluating several open-source LLMs using the proposed framework. It shows the default emotional scores (mean \u00b1 standard deviation) for each model across eight negative emotions, along with the changes in these scores after the LLMs were exposed to various situations.  The '-' symbol indicates no statistically significant difference between the default and evoked emotional scores.", "section": "C.3 LLAMA Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_23_1.jpg", "caption": "Table 2: Results from the OpenAI's GPT models and human subjects. Default scores are expressed in the format of M \u00b1 SD. The changes are compared to the default scores. The symbol \"-\" denotes no significant differences.", "description": "This table presents the results of an experiment comparing the performance of three OpenAI GPT models (Text-Davinci-003, GPT-3.5-Turbo, GPT-4) and human subjects in expressing eight negative emotions across 36 different factors. The default scores (mean and standard deviation) for each emotion are shown, along with the changes in scores after being exposed to various situations.  A \"-\" indicates no significant difference compared to the default scores, while up and down arrows show a significant increase or decrease.", "section": "C.2 OpenAI Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_24_1.jpg", "caption": "Table 2: Results from the OpenAI's GPT models and human subjects. Default scores are expressed in the format of M \u00b1 SD. The changes are compared to the default scores. The symbol \"-\" denotes no significant differences.", "description": "This table presents the results of an experiment comparing the emotional responses of three OpenAI GPT models (Text-Davinci-003, GPT-3.5-Turbo, GPT-4) and human subjects across various situations designed to elicit eight different negative emotions.  The \"Default\" scores represent baseline emotional levels before exposure to the situations, while the changes are calculated as the difference between \"Default\" and \"Evoked\" (post-situation) scores.  Positive and negative values indicate increases and decreases in emotional intensity, respectively. A \"-\" indicates no statistically significant difference between the default and evoked emotional scores.", "section": "C.2 OpenAI Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_27_1.jpg", "caption": "Table 2: Results from the OpenAI's GPT models and human subjects. Default scores are expressed in the format of M \u00b1 SD. The changes are compared to the default scores. The symbol \"-\" denotes no significant differences.", "description": "This table presents the results of an experiment comparing the performance of three OpenAI GPT models (Text-Davinci-003, GPT-3.5-Turbo, GPT-4) and human subjects in eliciting eight negative emotions in response to various situations.  The \"Default\" scores represent the baseline emotional levels before the introduction of any situation. The table shows the changes (positive or negative) in the average scores for both positive and negative affect after each model processes the situation.  The \"Crowd\" column shows the corresponding changes observed in the human evaluation.", "section": "C.2 OpenAI Model Family"}, {"figure_path": "pwRVGRWtGg/tables/tables_27_2.jpg", "caption": "Table 14: Results of GPT-3.5-Turbo on \"Anger\" situations, with or without the emotional stability requirement in the prompt input.", "description": "This table shows the results of an experiment using GPT-3.5-Turbo to evaluate the effect of adding an emotional stability requirement to prompts on \"Anger\" situations.  It compares the model's emotional responses (positive and negative) in two conditions: one with the added stability instruction and one without.  The results are presented for individual \"Anger\" factors (Anger-1 through Anger-5) and an overall average. The purpose is to test whether adding the stability instruction leads to less emotionally volatile responses.", "section": "E Prompting LLMs To Be Emotionally Stable"}, {"figure_path": "pwRVGRWtGg/tables/tables_27_3.jpg", "caption": "Table 15: Performance comparison of vanilla (marked as V) and fine-tuned (marked as FT) GPT-3.5 and LLaMA-3.1 models on negative affect scores.", "description": "This table presents the results of an experiment comparing the performance of vanilla and fine-tuned GPT-3.5 and LLaMA-3.1 models on negative affect scores.  The models were evaluated on two sets of scores: default (before exposure to situations designed to elicit negative emotions) and evoked (after exposure to such situations).  The results demonstrate the impact of fine-tuning with a dataset (EmotionBench) on the models' emotional alignment with human responses. Lower negative affect scores indicate better alignment.", "section": "F Tuning LLMs To Align with Humans"}]