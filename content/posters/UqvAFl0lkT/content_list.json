[{"type": "text", "text": "EReLELA: Exploration in Reinforcement Learning via Emergent Language Abstractions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Instruction-following from prompts in Natural Languages (NLs) is an impor  \n2 tant benchmark for Human-AI collaboration. Training Embodied AI agents for   \n3 instruction-following with Reinforcement Learning (RL) poses a strong explo  \n4 ration challenge. Previous works have shown that NL-based state abstractions can   \n5 help address the exploitation versus exploration trade-off in RL. However, NLs   \n6 descriptions are not always readily available and are expensive to collect. We   \n7 therefore propose to use the Emergent Communication paradigm, where artificial   \n8 agents are free to learn an emergent language (EL) via referential games, to bridge   \n9 this gap. ELs constitute cheap and readily-available abstractions, as they are the   \n0 result of an unsupervised learning approach. In this paper, we investigate (i) how   \n11 EL-based state abstractions compare to NL-based ones for RL in hard-exploration,   \n2 procedurally-generated environments, and (ii) how properties of the referential   \n3 games used to learn ELs impact the quality of the RL exploration and learning.   \n14 Results indicate that the EL-guided agent, namely EReLELA, achieves similar   \n5 performance as its NL-based counterparts without its limitations. Our work shows   \n16 that Embodied RL agents can leverage unsupervised emergent abstractions to   \n7 greatly improve their exploration skills in sparse reward settings, thus opening new   \n18 research avenues between Embodied AI and Emergent Communication. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 Natural Languages (NLs) have some properties, such as compositionality and recursive syntax, that   \n21 allow us to talk about infinite meanings while only using a finite number of words (or even letters,   \n22 or phonemes...). In other words, it enables us to be as expressive as one might needs. However,   \n23 it may be interesting sometimes to use language to abstract away from the details and only focus   \n24 on the essence of a specific experience, or a specific sensory stimulus. Thus, even though NLs can   \n25 sometimes be used with high expressiveness, they also can work as abstractions. For instance, using a   \n26 unique utterance to refer to a lot of semantically-similar but (visually) different situations, such as the   \n27 one presented in Figure 1 where the utterance \u2018one can see a purple key and a green ball\u2019 can refer   \n28 to many of the first-person perspective of the embodied agent, irrespective of the actual perspective   \n29 under which each object is seen.   \n30 Tam et al. [61] referred to that aspect as compacting/clustering a state/observation space, which is   \n31 in effect segmenting it into a set of less-detailed but more-meaningful sub-spaces. We employ the   \n32 term meaningful with respect the task that the embodied agent is possibly trained for. For instance,   \n33 if the task consists of picking and placing objects, then it is meaningful for utterances to contain   \n34 information about objects and places, but not so much to contain information about other agents in   \n35 the environment, if any. In this paradigm, Tam et al. [61] and Mu et al. [51] provided some arguments   \n36 towards the compacting/clustering assumption of NLs, as they used NLs oracle to build an abstraction   \n37 over a 3D and 2D environments. They relied upon state-of-the-art exploration algorithms, such as   \n38 Random Network Distillation (RND - Burda et al. [9]) and Never-Give-Up (NGU - Badia et al. [1]),   \n39 which can be difficult to deploy.   \n40 Thus, in this work, we aim to simplify the process of using   \n41 languages as abstractions and address the limitation of using   \n42 NLs, as they are expensive to harvest and not necessarily the   \n43 most meaningful abstraction for any given task. Indeed, instead   \n44 of state-of-the-art exploration algorithms, we show that simpler   \n45 count-based approaches combined with language abstraction   \n46 can be leveraged for hard-exploration tasks. And, in order to   \n47 remove the reliance on NLs, we look at the field of Emergent   \n48 Communication (EC) [41, 7] which have shown that artificial   \n49 languages, that we refer to as emergent languages (ELs), can   \n50 emerge through unsupervised learning algorithms, such as Ref  \n51 erential Games and variants [19], with structure and properties   \n52 similar to NLs. Our experimental evidences show that ELs,   \n53 acquired over an embodied agent\u2019s observations in an online   \n54 fashion and in parallel of its training, can be leveraged for hard  \n55 exploration tasks. We investigate what are the properties of   \n56 NLs and ELs in terms of their abstraction building abilities   \n57 by proposing a novel metric entitled Compactness Ambigu  \n58 ity Metric (CAM). Measures show that ELs abstractions are   \n59 aligned but not similar to NLs in terms of the abstractions they   \n60 perform, as the Emergent Communication context successfully   \n61 picks up on the meaningful features of the environment. Indeed,   \n62 EReLELA\u2019s abstractions reflect colors in the MultiRoom-N7-S4 ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/862eeac6500083b690c7e9d7c52438d14e3f3f5b1f1915619d81c19d62ac45a0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Top-view visualization of a wall-free 3D environment with different objects (e.g. red and blue cubes, purple and green keys, and green ball) showing the trajectory (from blue to red dots) of a randomly-walking embodied agent, with first-person perspectives highlighted at relevant timesteps using colored cones - showing the agent\u2019s viewpoint direction when a new utterance is used to describe the firstperson perspective using an oracle speaking in NL. ", "page_idx": 1}, {"type": "text", "text": "63 environment which only features coloured, unlocked doors, but no distracting objects, or shapes in   \n64 the KeyCorridor-S3-R2 environment where it is important to pickup a relevant key, among other   \n65 distractingly-shaped objects, and to open the locked door-shaped object.   \n66 We continue by reviewing EC and RL backgrounds and notations in Section 2. After detailing our   \n67 method in Section 3, we present experimental results on procedurally-generated, hard-exploration   \n68 task from the MiniGrid [15] benchmarks in Section 4. Finally, we discuss in Section 5 the results   \n69 presented in light of some related works and highlight possible future works. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "70 2 Background & Notation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "71 We provide details on our Reinforcement Learning (RL) settings and count-based exploration methods   \n72 in Section 2.1.Then, we review Emergent Communication in Section 2.2. ", "page_idx": 1}, {"type": "text", "text": "73 2.1 Exploration vs Exploitation in Reinforcement Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "74 An RL agent interacts with an environment in order to learn a mapping from states to actions that   \n75 maximises its reward signal. Initially, both the reward signal and the dynamics of the environment,   \n76 i.e. the impact that the agent actions may have on the environment, are unknown to the agent. It must   \n77 explore the environment and gather information, but, all the while it is exploring, it cannot exploit the   \n78 best strategy that it has found so far to maximise the currently-known reward signal. This dilemma is   \n79 known as the Exploration-vs-Exploitation trade-off of RL.This dilemma is only the start of the rabbit   \n80 hole, as it can even get worse. Indeed, in sparse reward environments, the reward signal is mainly   \n81 zero most of the time. This context makes it very difficult for RL agents to learn anything, because RL   \n82 algorithms derive feedback (i.e. gradients to update their parameters) from the reward signal that they   \n83 observe from the environment.It is usually referred to as extrinsic, in order to differentiate it from an   \n84 intrinsic reward signal. As the extrinsic reward is mostly zero, RL agents must exploit another signal   \n85 to derive information about the currently-unknown environment. This other signal can be found in   \n86 relation to the observation/state space, as RL agents can learn to seek novelty or surprise around the   \n87 observation/state space and attempt to manipulate it efficiently by choosing relevant actions. Focusing   \n88 on this novelty, RL agents can harvest an intrinsic reward signal, in the sense that RL agents are   \n89 building it and giving it to themself. Note that this intrinsic reward signal is very different from the   \n90 extrinsic reward signal, because it does not inform about the task that RL agents need to perform   \n91 in the environment. Ideally, though, it provides a graded and dense signal that the RL agent can   \n92 use to start learning anything about the environment. This is inspired by intrinsic motivation in   \n93 psychology [53]. Exploration driven by curiosity/novelty might be an important way for children   \n94 to grow and learn. Here, we focus on novelty, but the intrinsic rewards could be correlated with e.g.   \n95 impact [54], surprise [9] or familiarity of the state. The intrinsic reward signal is only a proxy for   \n96 RL agents to start to make progress into learning about the environment and eventually, hopefully   \n97 encounter some non-zero extrinsic reward signal along the way. It provides a denser reward signal   \n98 that can guide RL agents into learning internal representations about the environment\u2019s dynamic so   \n99 that, whenever some extrinsic reward are encountered along the way, then they can efficiently bind   \n100 their previously-learned representations to those recently-encountered extrinsic rewards.   \n101 Formally, we study a single agent in a Markov Decision Pro  \n102 cess (MDP) defined by the tuple $(S,\\mathcal{A},T,\\mathcal{R},\\gamma)$ , referring to,   \n103 respectively, the set of states, the set of actions, the transition   \n104 function $T:S\\times A\\to P(S)$ which provides the probability   \n105 distribution of the next state given a current state and action,   \n106 the reward function $\\mathcal{R}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathit{r}$ , and the discount fac  \n107 tor $\\gamma\\in[0,1]$ . The agent is modelled with a stochastic policy ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{t}=\\mathbb{E}_{s_{t+k+1}\\sim T(s_{t+k},a_{t+k})}[}\\\\ &{\\quad\\quad\\quad\\quad\\quad a_{t+k+1}\\sim\\pi(s_{t+k+1})}\\\\ &{\\displaystyle\\sum_{k=0}^{T}\\gamma^{k}R(s_{t+k+1},a_{t+k+1})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "108 $\\pi:S\\to P(A)$ from which actions are sampled at every time step of an episode of finite time horizon   \n109 $T$ . The agent\u2019s goal is to learn a policy which maximises its discounted expected return at time $t$ ,   \n110 defined in equation 1. We further define $\\mathcal{R}=\\lambda_{\\mathrm{ext}}\\mathcal{R}^{\\mathrm{ext}}+\\lambda_{\\mathrm{int}}\\mathcal{R}^{\\mathrm{int}}$ as the weighted sum of the extrinsic   \n111 and intrinsic reward functions, respectively, $\\mathcal{R}^{\\mathrm{ext}},\\mathcal{R}^{\\mathrm{int}}$ , with weights $\\lambda_{\\mathrm{ext}},\\lambda_{i n t}$ . Indeed, while the   \n112 extrinsic reward is provided by the environment, we assume that for any tuple $\\left({{s_{t}},{a_{t}},{s_{t+1}}}\\right)$ we can   \n113 compute an intrinsic reward.   \n114 Stanton and Clune [58] identifies two categories of exploration strategies, to wit across-training,   \n115 where novelty of states, for instance, is evaluated in relation to all prior training RL episodes, and   \n116 intra-life, where it is evaluated solely in relation of the current RL episode. And, historically, we   \n117 can identify two types of intrinsic motivation exploration depending on how the intrinsic reward is   \n118 computed, either relying on count-based or prediction-based methods. Prediction-based methods fti   \n19 into the across-training category and count-based methods can actually fti in both categories but they   \n120 have mainly been instantiated in the literature as across-training methods after extension of intra-life   \n121 core mechanisms. As our proposed architecture EReLELA fit into the category of count-based   \n122 methods, we detail them further.In the context of an intrinsic reward signal correlated with surprise,   \n123 then it is necessary to quantify how much of surprise each observation/state provides. Intuitively, we   \n124 can count how many times a given observation/state has been encountered and derive from that count   \n125 our intrinsic reward. The reward would guide the RL agent to prefer rarely visited/observed states   \n126 compared to common states. This is referred to as the count-based exploration method. Count-based   \n127 exploration method were originally only applicable to tabular RL where the state space is discrete   \n128 and it is easy to compare states together. When dealing with continuous or high-dimensional state   \n129 spaces, such method is not practical. Thus, Bellemare et al. [3] proposed (and extended in Ostrovski   \n130 et al. [52]) a pseudo-count approach which was derived from increasingly more efficient density   \n131 models, and they showed success in applying it to image-based exploration environments from Atari   \n132 2600 benchmark, such as Montezuma\u2019s Revenge, Private Eye, and Venture. We provide more relevant   \n133 details in Appendix B.   \n134 Nevertheless, hard-exploration task involving procedurally-generated environments are notoriously   \n135 difficult for count-based exploration methods. Indeed, when states are procedurally-generated, almost   \n136 all states will be showing \u2018novel\u2019 features, most times irrespectively of whether it is relevant to the   \n137 task or not. It will follow that their state (pseudo-)count will always be low and therefore the RL   \n138 agent will get feedback towards reaching all of them indefinitely, but if every state is \u2018novel\u2019 then   \n139 there is nothing to guide the agent in any specific direction that would entail to good exploration. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "140 2.2 Emergent Communication ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "141 Emergent Communication is at the interface of language grounding and language emergence. While   \n142 language emergence raises the question of how to make artificial languages emerge, possibly with   \n143 similar properties to NLs, such as compositionality [2, 24, 45, 55], language grounding is concerned   \n144 with the ability to ground the meaning of (natural) language utterances into some sensory processes,   \n145 e.g. the visual modality. On one hand, the compositionality of ELs has been shown to further   \n146 the learnability of said languages [38, 57, 8, 45] and, on the other hand, the compositionality of   \n147 NLs promises to increase the generalisation ability of the artificial agent that would be able to   \n148 rely on them as a grounding signal, as it has been found to produce learned representations that   \n149 generalise, when measured in terms of the data-efficiency of subsequent transfer and/or curriculum   \n150 learning [27, 49, 50, 33]. Yet, emerging languages are far from being \u2018natural-like\u2019 protolanguages   \n151 [40, 10, 11], and the questions of how to constraint them to a specific semantic or a specific syntax   \n152 remain open problems. Nevertheless, some sufficient conditions can be found to further the emergence   \n153 of compositional languages and generalising learned representations [40, 43, 17, 5, 24, 39, 12, 21].   \n154 The backbone of the field rests on games that emphasise the functionality of languages, namely,   \n155 the ability to efficiently communicate and coordinate between agents. The first instance of such   \n156 an environment is the Signaling Game or Referential Game $(R G)$ by Lewis [44], where a speaker   \n157 agent is asked to send a message to the listener agent, based on the state/stimulus of the world that it   \n158 observed. The listener agent then acts upon the observation of the message by choosing one of the   \n159 actions available to it in order to perform the \u2018best\u2019 action given the observed state depending on the   \n160 notion of \u2018best\u2019 action being defined by the interests common to both players. In RGs, typically, the   \n161 listener action is to discriminate between a target stimulus, observed by the speaker and prompting   \n162 its message generation, and some other distractor stimuli. Distractor stimuli are selected using a   \n163 distractor sampling scheme, which has been shown to impact the resulting EL [42, 43]. The listener   \n164 must discriminate correctly while relying solely on the speaker\u2019s message. The latter defined the   \n165 discriminative variant, as opposed to the generative variant where the listener agent must reconstruct/-   \n166 generate the whole target stimulus (usually played with symbolic stimuli). Visual (discriminative)   \n167 RGs have been shown to be well-suited for unsupervised representation learning, either by competing   \n168 with state-of-the-art self-supervised learning approaches on downstream classification tasks [22], or   \n169 because they have been found to further some forms of disentanglement [28, 35, 14, 46] in learned   \n170 representations [65, 18]. Such properties can enable \u201cbetter up-stream performance\u201d[63], greater   \n171 sample-efficiency, and some form of (systematic) generalization [48, 26, 59]. Thus, this paper aims   \n172 to investigate visual discriminative RGs as auxiliary tasks for RL agents. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "173 3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "174 In this section, following the acknowledgement of a gap in terms of evaluating the abstractions   \n175 that different languages perform over different state/observation space, we start by introducing in   \n176 Section 3.1 our Compactness Ambiguity Metric (CAM) that attempts to fill in that gap.Then, in   \n177 Section 3.2, we present the EReLELA architecture that leverages EL abstractions in an intra-life   \n178 count-based exploration scheme for RL agents. ", "page_idx": 3}, {"type": "text", "text": "179 3.1 Compactness Ambiguity Metric ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "180 In order to measure qualities related to the kind of abstraction that a language performs over stimuli,   \n181 we propose to rely on the temporal aspects of embodied agent\u2019s trajectories in a given environment.   \n182 We build over the following intuition, represented in Figure 2: we consider two possible languages   \n183 grounded into the first-person viewpoint of an embodied agent situated in a 3D environment populated   \n184 with objects of different shapes and colors. On one hand, we have the Blue language, which is only   \n185 concerned about blue objects and its utterances only describe that they are of color blue when they   \n186 are, while, on the other hand, we have the Color language, which is describing the color of all   \n187 visible objects. Inherently, those two languages expose different semantics about the world, and   \n188 therefore they perform different abstractions. We aim to build a metric that captures how different the   \n189 semantics they expose are. To do so, we propose to arrange their respective utterances when prompted   \n190 with the very same agent\u2019s trajectories into different timespan-focused buckets towards building   \n191 an histogram. These timespan-focused buckets reflect $\\delta(u)^{\\bar{}}$ the number of consecutive timesteps   \n192 $\\left(t_{k}\\right)_{k\\in[k_{\\mathrm{start}},k_{\\mathrm{start}}+\\delta(u)]}$ for which a specific utterance $u$ would be uttered by a speaker of each language   \n193 when prompted with the stimuli in those timesteps. We will refer to these are compactness counts. For   \n194 instance the Blue language\u2019s utterance \u2018I see a blue object\u2019 at the beginning of the trajectory occupies   \n195 twice as more consecutive timesteps as the same utterance coming from a Color language speaker (or,   \n196 its compactness count in the Blue language is twice its compactness count in the Color language).   \n197 Therefore, in the case of the Blue language, this utterance would increment the medium-length bucket,   \n198 while it would increment the short-length bucket in the case of Color language histogram. It ensues   \n199 that the histograms of timespan-focused buckets captures semantics exposed by each language, and   \n200 we will therefore refer to the resulting histogram as the histogram of semantic-clustering timespans.   \n201 As the toy example highlights, the histograms of semantic-clustering timespans will differ from one   \n202 language to another depending on the semantics each language expose or, in other words, depending   \n203 on the abstractions they perform. This is the first intuition on which the Compactness Ambiguity   \n205 Formally, we define $\\mathcal{L}$ as the set of all possible lan  \n206 guages over vocabulary $V$ with maximum sentence   \n207 length $L$ , such that for any language $l\\in{\\mathcal{L}}$ we denote   \n208 $\\mathsf{S p}_{l}:{\\mathcal{S}}\\rightarrow l$ as a speaker agent or oracle that maps   \n209 any state/observation $s\\in S$ to a caption or utterance   \n210 $u\\in l$ . Thus, we can now consider $N$ buckets whose   \n211 related timespans $(T_{i})_{i\\in[1,N]}$ are sampled relative to   \n212 the maximal length $T$ of a trajectory in the given en  \n213 vironment, and the histogram of semantic-clustering   \n214 timespans that they induce.   \nover the same observed trajectory of stimuli,   \n216 is made evident by considering the expressivity or, its and that the discrepancy in exposed semantics   \n217 inverse, the ambiguity, of a given language $l$ , defined can be captured by an histogram of semantic  \n218 as $\\begin{array}{r}{\\mathcal{E}_{l}\\;=\\;\\frac{\\mathcal{\\#}\\mathrm{unique\\;\\bar{u}t t e r a n c e s}}{\\mathcal{\\#}\\mathrm{unique\\;stimuli}}}\\end{array}$ s with # the set cardinality clustering timespans.   \n219 operator. Dealing with stimuli being states/observations of a (randomly walking) embodied agent,   \n220 gathered into a dataset $\\mathcal{D}$ , the number of unique stimuli cannot be estimated reliably when dealing   \n221 with complex, continuous stimuli. Thus, the best we can rely on is a measure of relative expressivity   \n222 over a dataset, that we define as REl(D) = #uniq#uset iutmteurlirances , with $|\\cdot|$ being the size   \n223 operator over collections (differing from sets in the sense that they allow duplicates). In those terms,   \n224 the relative expressivity is maximised if and only if (i) $\\#\\mathcal{D}=|\\mathcal{D}|$ , and (ii) $\\mathrm{Sp}_{l}$ is a bijection over   \n225 $\\mathcal{D}$ . On the other hand, considering that a language $l$ performs an abstraction over $\\mathcal{D}$ is tantamount   \n226 to some stimuli $(s,s^{\\prime})\\in\\mathcal{D}^{2}$ sharing the same utterance $u=\\mathrm{Sp}_{l}(s)=\\mathrm{Sp}_{l}(s^{\\prime})$ , i.e. consisting of   \n227 a hash collision, meaning that the mapping $\\mathrm{Sp}_{l}$ from $\\mathcal{D}$ to $l$ woud not be injective (and therefore   \n228 not bijective). Incidentally, the relative expressivity $\\mathcal{R}\\mathcal{E}_{l}(\\mathcal{D})$ cannot be maximised, leading to the   \n229 language $l$ being ambiguous over $\\mathcal{D}$ . In this consideration, we can see that the ambiguity of a   \n230 language (over a given dataset) can be impacted by either the extent to which an abstraction is   \n231 performed (meaning that most colliding states/observations are of consecutive timesteps) or the   \n232 extent to which the dataset is redundant (meaning $\\#D<<|D|)$ . Therefore it is important that our   \n233 proposed Compactness Ambiguity Metric is built to focus on sources of ambiguities that are the   \n234 result of consecutive-timesteps states colliding, more than sources of ambiguities that are the result   \n235 of redundancy in the given dataset. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/fb1d17b93d6151a85d2ecdfe37ce02ed7d491a996a6d92b0631b385be7b93dd9.jpg", "img_caption": ["Figure 2: Toy example illustration of how different languages expose different semantics "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall i\\in[1,N],\\;T_{i}=1+\\lceil\\lambda_{i}\\cdot\\mathcal{R}\\mathcal{A}_{l}(\\mathcal{D})\\rceil\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall i\\in[1,N],\\;T_{i}^{\\prime}=1+\\lceil\\lambda_{i}\\cdot T\\rceil\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "236   \n237   \n238   \n239   \n240   \n241   \n242 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall i\\in[1,N],\\;C A(\\mathcal{D})_{T_{i}}=\\sum_{u\\in l}\\frac{\\#\\delta_{\\overline{{D}}}^{\\geq T_{i}}(u)}{\\#\\delta_{\\mathcal{D}}(u)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Yet, in its currently proposed form, it is impacted by the amount of redundancy in the dataset. In order to reduce this dependence, we propose to bake some invariance to redudancy-induced ambiguity into the timespan-focused buckets. To this end, for a given language $l$ and dataset ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{D}$ , we define the buckets\u2019 related timespans in relation to the relative ambiguity $\\begin{array}{r}{\\mathcal{R}\\tilde{\\mathcal{A}}_{l}(\\tilde{\\mathcal{D}})=\\frac{1}{\\mathcal{R}\\mathcal{E}_{l}(\\mathcal{D})}=}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "43 #S|pD(|D), as shown in equation 2 with \u03bbi \u2208[0, 1] s.t. \u2200(j, k), j < k =\u21d2 \u03bbj < \u03bbk, and \u2308\u00b7\u2309being   \n44 the ceiling operator. This is in lieu of defining them in relation to the maximal length $T$ of an agent\u2019s   \n45 trajectory in the environment, as shown in equation 3. More specifically, let us first acknowledge   \n46 bdeeicnogm epitohseitri oanb sotrfa rcetliaotni voer  aremdbuingduiatnyc yo,v seur cthw toh iatn $\\mathcal{R A}_{l}=\\mathcal{R A}_{l}^{\\mathrm{redundancy}}+\\mathcal{R A}_{l}^{\\mathrm{abstract}}$ . h Tohfe int sn sooteu rtcheast   \n48 the relative ambiguity is equal to the mean number of consecutive timesteps, or compactness count,   \n49 for which a given utterance would be used when the unique utterances are uniformly distributed   \n50 over the dataset $\\mathcal{D}$ . Thus, in the metric, we propose to absorb variations of relative ambiguity due to   \n51 redundancy by changing the metric\u2019s bucket setup, from Equation 3 to Equation 2. Doing so, it is true   \n52 that the metric\u2019s bucket setup will also vary when the abstraction-induced relative ambiguity varies,   \n53 we remark that the metric would not build invariance to this source of relative ambiguity since it is   \n54 taken into accounts when sorting out the different unique utterances into their relevant bucket, based   \n255 on the maximal number of consecutive timesteps in which they occur, as shown in equation 4 with   \n256 $\\delta_{\\ensuremath{\\mathcal{D}}}:l\\to2^{\\mathbb{N}}$ is the compactness count function that associates each utterances $u\\in l$ to its related set   \n257 of compactness counts over dataset $\\mathcal{D}$ , i.e. the set that contains numbers of consecutive timesteps   \n258 for which $u\\in l$ was uttered by $\\mathrm{Sp}_{l}$ , each time it was uttered without being uttered in the previous   \n259 timestep. For instance, if we consider $u~\\in~l$ such that $\\mathbf{S}\\mathbf{p}_{l}^{-1}(u)\\,=\\,\\{s_{t_{1}},s_{t_{1}+1},s_{t_{1}+2},s_{t_{2}}\\}$ , with   \n260 $(t_{1},t_{2})\\in[0,T]^{2}$ such that $t_{2}>t_{1}+3$ , then $\\delta_{\\mathrm{D}}(u)=\\{3,1\\}$ because $u$ occurred 2 non-consecutive   \n261 times over $\\mathcal{D}$ and those occurrences lasted for, respectively, 3 and 1 consecutive timesteps, i.e. for   \n262 compactness counts of 3 and 1. The superscript $\\geq T_{i}$ in $\\delta_{\\mathcal{\\overline{{D}}}}^{\\geq\\hat{T}_{i}}$ implies flitering of the output set based   \n263 on compactness counts being greater or equal to $T_{i}$ . We provide in appendix C an analysis of the   \n264 sensitivity of our proposed metric, and in appendix E.1 experimental results that ascertain the internal   \n265 validity of our proposed metric, we consider a 3D room environment of MiniWorld [15], fliled with 5   \n266 different, randomly-placed objects, as shown in a top-view perspective in Figure 1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "267 3.2 EReLELA Architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "268 This section details the EReLELA   \n269 architecture, which stands for Ex  \n270 ploration in Reinforcement Learning   \n271 via Emergent Language Abstractions.   \n272 As a count-based exploration method,   \n273 we present here its intra-life core   \n274 mechanism, where intrinsic reward   \n275 signals are derived from novelty at   \n276 the level of language utterances de  \n277 scribing the current observation/state.   \n278 It relies on a hashing-like function   \n279 (cf. Appendix B), which takes the   \n280 form of the speaker agent of a refer  \n281 ential game (RG), to turn continuous   \n282 and high-dimensional observations/s", "page_idx": 5}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/d85aca1a39762376d9a08e0c68c659684877f029e1995026fc7be92aaebf0432.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: EReLELA architecture consisting of a stimulus/observation encoder shared between an RL agent and the speaker and listener agents of a RG, framed as an unsupervised auxiliary task [31]. The language utterances outputted by the RG speaker agent are used in a count-based exploration method to generate intrinsic rewards for the RL agent. ", "page_idx": 5}, {"type": "text", "text": "283 tates into discrete, variable-length sequences of tokens. EReLELA is built around an RL agent   \n284 augmented with an unsupervised auxiliary task, a (discriminative, here, or generative) RG, following   \n285 the UNREAL architecture from Jaderberg et al. [31], as shown in Figure 3.   \n286 We train the RG agents in a descriptive, discriminative RG with $K=256$ distractors, every $T_{R G}=$   \n287 32768 gathered RL observations, on a dataset $\\mathcal{D}_{R G}$ consisting of the most recent $|\\mathcal{D}_{R G}|\\,=\\,8192$   \n288 observations, among which 2048 are held-out for validation/testing-purpose, over a maximum of   \n289 $N_{R G-e p o c h}=32$ epochs or until they reach a validation/testing RG accuracy greater than a given   \n290 threshold $a c c_{R G-t h r e s h}=90\\%.$ . Our preliminary experiments in Appendices D.1 and D.2 show,   \n291 respectively, that increasing the RG accuracy threshold $a c c_{R G-t h r e s h}$ increases the sample-efficiency   \n292 of the EL-guided RL agent, and that the number of distractors $K\\in[15,128,256]$ is critical (even   \n293 more so than the distractor sampling scheme - which we set to be uniform unless specified otherwise),   \n294 and that it correlates positively with the performance of the RL agent. More specific details about   \n295 the RG and its agents\u2019 architectures can be found in Appendices $\\boldsymbol{\\mathrm F}$ and $\\mathrm{G}$ and our open-source   \n296 implementation1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "297 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "298 Agents Our RL agent is optimized using the R2D2 algorithm from [34] with the Adam opti  \n299 mizer Kingma and Ba [36]. Importantly, as it aims to maximise the weighted sum of the extrinsic   \n300 and intrinsic reward functions following equation 1, throughout this paper, we use $\\lambda_{i n t}=0.1$ and   \n301 $\\lambda_{e x t}=10.0$ in order to make sure that the agent pursues the external goal once the exploration of   \n302 the environment has highlighted it. Further details about the RL agent can be found in Appendix F.   \n303 For our RG agents, we consider optimization using either the Impatient-Only or the LazImpa loss   \n304 function from Rita et al. [56], but the latter is adapted to the context of a Straight-Through Gumbel  \n305 Softmax (STGS) communication channel [25, 21], as detailed in Appendix G.1, and we refer to   \n306 it as STGS-LazImpa. Indeed, the LazImpa loss function has been shown to induce Zipf\u2019s Law of   \n307 Abbreviation (ZLA) in the ELs. Thus, we can investigate in the following experiments how does   \n308 structural similarity between NLs and ELs affect the kind of abstractions they perform, as well as   \n309 the resulting RL agent. Further details about the RG in EReLELA can be found in Appendix G.   \n310 Environments. After having considered in our preliminary experiments (cf. Appendix E.4) the 2D   \n311 environment MultiRoom-N7-S4, we propose below experiments in the more challenging KeyCorridor  \n312 S3-R2 environment from MiniGrid [15]. Indeed, it involves complex object manipulations, such as   \n313 (distractors) object pickup/drop and door unlocking, which requires first picking up the relevantly  \n314 colored key object.   \n315 Natural Language Oracles. Our implementation of a NL oracle is simply describing the visible   \n316 objects in terms of their colour and shape attributes, from left to right on the agent\u2019s perspective,   \n317 whilst also taking into account object occlusions. For instance, around the end of the trajectory   \n318 presented in Figure 1, the green key would be occluded by the blue cube, therefore the NL oracle   \n319 would provide the description \u2018blue cube red cube\u2019 alone. We also implement colour-specific and   \n320 shape-specific language oracles, which consists of filtering out from the NL oracle\u2019s utterance the   \n321 information that each of those language abstract away, i.e. removing any shape-related word in the   \n322 case of the colour-specific language, and vice-versa.   \n323 Hypotheses. We seek to validate the following hypotheses. Firstly, we consider whether NL   \n324 abstractions can help for hard-exploration in RL with a simple count-based approach (H1), and refer   \n325 to the relevant agent using NL abstractions to compute intrinsic rewards as NLA. We carry on with   \n326 the hypothesis that ELs can be used similarly $\\left(\\mathbf{H}2\\right)$ , and we investigate to what extent do ELs compare   \n327 to NLs in terms of abstraction. We would expect ELs to perform more meaningful abstractions than   \n328 NLs (H3), in the sense that their abstractions would be more aligned with the relevant features of a   \n329 given environment.   \n330 Evaluation. We employ 3 random seeds for each agent. We evaluate (H1) and (H2) using both the   \n331 success rate and the manipulation count, in the hard-exploration task of KeyCorridor-S3-R2. The   \n332 manipulation count is a per-episode counter incremented each time an object is successfully picked   \n333 up or dropped by the RL agent over the course of each episode. In order to evaluate both (H3.1)   \n334 and (H3.2), we use the CAM to measure the kind of abstractions performed by ELs, and compare   \n335 those measures with those of the oracles\u2019 languages that we previously studied. We report the CAM   \n336 distances between ELs and the NL, Color language, and Shape language oracles, which is computed   \n337 as an euclidean distance in $\\mathbb{R}^{6}$ by considering the $N=6$ CAM scores for each timespans/thresholds   \n338 as vectors in this space. As we remarked that an agent\u2019s skillfullness at the task would induce very   \n339 different trajectories (e.g. in MultiRoom-N7-S4, staying in the first room and only ever seeing the   \n340 first door, for an unskillfull agent, as opposed to visiting multiple rooms and observing multiple   \n341 colored-doors, for a skillfull agent), we compute the oracle languages CAM scores on the exact same   \n342 trajectories than used to compute each EL\u2019s CAM scores.   \n345 We present in Figure 4 both the success rate of the different agents (as line plot through learning -left-,   \n346 or barplot at the end of learning -right-), and the per-episode manipulation count (middle). From   \n347 the fact that both the NLA and EReLELA agent performance converges higher or close to $80\\%$ of   \n348 success rate (except the STGS-LazImpa-10-1), we validate hypotheses (H1) and (H2), meaning that   \n349 it is possible to learn systematic exploration skills from both NL or EL abstractions with a simple   \n350 count-based exploration method, in 2D environments (cf. further evidence in Appendix D.1 with the   \n351 MultiRoom-S7-R4 environment). This result puts into perspective the directions of previous literature   \n352 designing complex exploration algorithms [9, 1].   \n353 The sample-efficiency is better for NLA than it is for most EL-based agents, except the Agnostic   \n354 STGS-LazImpa-10-1 agent, possibly because of the fact that ELs are learned online in parallel of the   \n355 RL training, as opposed to the case of NLA which makes use of a ready-to-use oracle. Concerning   \n356 the most-sample-efficient Agnostic STGS-LazImpa-10-1 agent, we interpret its success to be the   \n357 result of beneftiing from both a language structure ascribing to the ZLA and a performed abstraction   \n358 that is more optimal than NL oracle\u2019s ones, because it is learned from the stimuli themselves.   \n359 Among the different Agnostic EReLELA agents, the final performance are not statistically  \n360 significantly distinguishable, meaning that learning systematic exploration skills with EReLELA can   \n361 be done with some robustness to the anecdotical differences in qualities of the different ELs. On the   \n362 other hand, the shared/non-agnostic EReLELA agents\u2019s performance are statistically-significantly   \n363 distinguishable from each other and from their agnostic versions, achieving lower performance or   \n364 even failing to learn anything in the case of the STGS-LazImpa-10-1 EReLELA agent. We interpret   \n365 these results as being caused by some kind of interference between the RG training and the RL   \n366 training, preventing any valuable representations from being learned in the shared observation encoder   \n367 (cf. Figure 3), thus warranting the need for future works to investigate whether a synergy can be   \n368 achieved.   \n369 Finally, acknowledging the RANDOM agent, which is the ablated version of EReLELA without   \n370 RG training, enabling still a median performance around $70\\%$ of success rate, we recall the Random   \n371 Network Distillation approach from Burda et al. [9], for they both share a randomly initialised   \n372 networked from which feedback is harvested to guide an RL agent. Thus, even more so in a 2D   \n373 environment, this ablated version is not to be confused with a lower-bound baseline but rather an   \n374 interesting ablation that enables us to show the impact of the RG training, increasing the sample  \n375 efficiency and final performance of the resulting RL agent. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/83db095159af5eab41ea91b0e4f560ad0344170f0cbf1376cd25d95330e4b495.jpg", "img_caption": ["Figure 4: Success rate learning curve (left), computed as running averages over 1024 episodes each time (i.e. 32 in parallel, as there are 32 actors, over 32 running average steps), and barplot (right), along with per-episode manipulation count (middle) in KeyCorridor-S3-R2 from MiniGrid [15], for different agents: (i) the Natural Language Abstraction agent (NLA) refers to using the NL oracle to compute intrinsic reward, (ii) the STGS-LazImpa- $\\cdot\\beta_{1}\\!-\\!\\beta_{2}$ EReLELA agents with $\\beta_{1}=5$ (agnostic only) or $\\beta_{1}=10$ (shared and agnostic), and $\\beta_{2}=1$ , (iii) the Impatient-Only EReLELA agents (shared and agnostic), and (iv) the RANDOM agent referring to an ablated version of EReLELA without RG training. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "376 4.2 EReLELA learns Meaningful Abstractions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "377 Regarding hypothesis (H3), we show in Figure 5 the CAM distances between the different agent\u2019s   \n378 ELs and the natural, colour-specific, and shape-specific languages. We recall that in the KeyCorridor  \n379 S3-R2 environment, the most important feature is object shape as the agent must pickup a key from   \n380 all other distractor objects and then use it to unlock the locked door. Thus, as we observe that   \n381 most ELs\u2019 abstractions are closer to the shape-specific language than the others, we conclude that   \n382 EReLELA learns meaningful abstractions, thus validating hypothesis (H3) (cf. Appendix E.3 for   \n383 further evidence in the context of MultiRoom-N7-S4). Further, we remark that the failing STGS  \n384 LazImpa-10-1 EReLELA agent is indeed failing because its EL\u2019s abstractions are not highlighting   \n385 shape features. When considering the shared/non-agnostic agents only, we can see that they require   \n386 many more RG training epochs, meaning that they reach the accuracy threshold less often than their   \n387 agnostic counterparts. We take this as further evidence for our interpretation that there might be   \n388 interference between the RL objective and the RG objective.   \n389 We note that abstractions from ELs brought about in the contexts of the Agnostic STGS-LazImpa   \n390 agents and the Agnostic Impatient-Only agents are the closest to that of the shape-specific language   \n391 ones, and their evolution throughout learning are similar. Yet, the Agnostic STGS-LazImpa agents   \n392 achieves statistically-significantly better sample-efficiency (cf. Figure 7). We interpret this as being   \n393 caused by the ZLA structure of the ELs in the context of the Agnostic STGS-LazImpa agents, thus   \n394 showing that NL-like structure is impacting the kind of abstractions being performed in ways that are   \n395 yet to be unveiled by future works.   \n396 Limitations. With regards to the external validity of EReLELA, we acknowledge that the current   \n397 work only addresses a 2D environment and therefore, despite being procedurally-generated, it presents   \n398 less challenges to count-based exploration methods than in the context of 3D procedurally-generated   \n399 environments. Although we provide some results in Appendix E.3 showing that EReLELA is able   \n400 to learn meaningful abstractions in a 3D environment, we leave it to future work to ascertain the   \n401 external validity of EReLELA by testing it in a procedurally-generated 3D environment that pose   \n402 purely-navigational or navigational and manipulative exploration challenges. ", "page_idx": 7}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/a09b5002f1cf39466468f24e066473db39c093cc8a3c9e010eae442deed393bd.jpg", "img_caption": ["Figure 5: CAM distances to NL (left), Color language (middle), and Shape language (right), for ELs brought about in KeyCorridor-S3-R2 from MiniGrid [15], with different agents: (i) the STGSLazImpa- $J_{1}\\!-\\!\\beta_{2}$ EReLELA agents with $\\beta_{1}=5$ (agnostic only) or $\\beta_{1}=10$ (shared and agnostic), and $\\beta_{2}=1$ , (ii) the Impatient-Only EReLELA agents (shared and agnostic), and (iii) the RANDOM agent referring to an ablated version of EReLELA without RG training. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "403 5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "404 We investigated the compacting/clustering hypothesis for ELs, questioning how do NLs and ELs   \n405 compare in terms of the abstractions they perform over state/observation spaces. To answer this   \n406 question, we proposed a novel metric entitled Compactness Ambiguity Metric (CAM), for which we   \n407 analysed the sensitivity and performed internal validation.   \n408 We then leveraged this metric to show that ELs abstractions are more meaningful than NLs ones,   \n409 as the Emergent Communication context successfully picks up on the meaningful features of the   \n410 environment.   \n411 Then, we have proposed the Exploration in Reinforcement Learning via Emergent Languages   \n412 Abstractions (EReLELA) agent, which leverages ELs abstractions to generate intrinsic motivation   \n413 rewards for an RL agent to learn systematic exploration skills. Our experimental evidences showed   \n414 the performance of EReLELA in procedurally-generated, hard-exploration 2D environments from   \n415 MiniGrid [15].   \n416 Moreover, in the parallel optimization of the RG players, we evidenced how the STGS-LazImpa loss   \n417 function, which induces EL to abide by ZLA like most NLs, impacts the kind of abstraction being   \n418 performed compared to baseline Impatient-Only loss function, and yields better sample-efficiency for   \n419 the RL agent training.   \n420 Future work ought to investigate different loss functions and distractor sampling schemes, especially   \n421 if playing discriminative RGs like here, as we expect, for instance, that sampling distractors more   \n422 contrastively, e.g. like in Choi et al. [17], may induce the emergence of more complete, and therefore   \n423 more meaningful ELs. By complete, we mean that the ELs would still be abstracting away details but   \n424 also capturing more information about the underlying structure of the stimuli space, e.g. capturing   \n425 both colour- and shape-related information of visible objects. In this light, we would also expect   \n426 generative RGs to propose a possibly different picture that is worth investigating.   \n427 While we leave it to subsequent work to investigate the external validity of EReLELA and whether   \n428 it transfers similarly well to 3D environments, our results open the door to a new application   \n429 of the principles of Emergent Communication and ELs towards influencing/shaping the learned   \n430 representations and behaviours of Embodied AI agents trained with RL. ", "page_idx": 8}, {"type": "text", "text": "431 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "432 [1] A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot, S. Kapturowski, O. Tieleman,   \n433 M. Arjovsky, A. Pritzel, A. Bolt, et al. Never give up: Learning directed exploration strategies.   \n434 In International Conference on Learning Representations, 2019.   \n435 [2] M. Baroni. Linguistic generalization and compositionality in modern artificial neural networks.   \n436 mar 2019. URL http://arxiv.org/abs/1904.00157.   \n437 [3] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying   \n438 count-based exploration and intrinsic motivation. Advances in neural information processing   \n439 systems, 29, 2016.   \n440 [4] L. Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.   \n441 com/. Software available from wandb.com.   \n442 [5] B. Bogin, M. Geva, and J. Berant. Emergence of Communication in an Interactive World with   \n443 Consistent Speakers. sep 2018. URL http://arxiv.org/abs/1809.00549.   \n444 [6] D. Bouchacourt and M. Baroni. How agents see things: On visual representations in an emergent   \n445 language game. aug 2018. URL http://arxiv.org/abs/1808.10696.   \n446 [7] N. Brandizzi. Towards more human-like AI communication: A review of emergent communica  \n447 tion research. Aug. 2023.   \n448 [8] H. Brighton. Compositional syntax from cultural transmission. MIT Press, Artificial, 2002.   \n449 URL https://www.mitpressjournals.org/doi/abs/10.1162/106454602753694756.   \n450 [9] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A. Efros. Large-Scale Study of   \n451 Curiosity-Driven Learning. aug 2018. URL http://arxiv.org/abs/1808.04355.   \n452 [10] R. Chaabouni, E. Kharitonov, E. Dupoux, and M. Baroni. Anti-efficient encoding in emergent   \n453 communication. NeurIPS, may 2019. URL http://arxiv.org/abs/1905.12561.   \n454 [11] R. Chaabouni, E. Kharitonov, A. Lazaric, E. Dupoux, and M. Baroni. Word-order biases in deep  \n455 agent emergent communication. may 2019. URL http://arxiv.org/abs/1905.12330.   \n456 [12] R. Chaabouni, E. Kharitonov, D. Bouchacourt, E. Dupoux, and M. Baroni. Compositionality   \n457 and Generalization in Emergent Languages. apr 2020. URL http://arxiv.org/abs/2004.   \n458 09124.   \n459 [13] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of   \n460 the thiry-fourth annual ACM symposium on Theory of computing, pages 380\u2013388, 2002.   \n461 [14] R. T. Q. Chen, X. Li, R. Grosse, and D. Duvenaud. Isolating sources of disentanglement in   \n462 VAEs, 2018.   \n463 [15] M. Chevalier-Boisvert, B. Dai, M. Towers, R. de Lazcano, L. Willems, S. Lahlou, S. Pal, P. S.   \n464 Castro, and J. Terry. Minigrid & miniworld: Modular & customizable reinforcement learning   \n465 environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.   \n466 [16] K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and   \n467 Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine   \n468 translation. arXiv preprint arXiv:1406.1078, 2014.   \n469 [17] E. Choi, A. Lazaridou, and N. de Freitas. Compositional Obverter Communication Learning   \n470 From Raw Visual Input. apr 2018. URL http://arxiv.org/abs/1804.02341.   \n471 [18] K. Denamgana\u00ef, S. Missaoui, and J. A. Walker. Visual referential games further the emergence   \n472 of disentangled representations. arXiv preprint arXiv:2304.14511, 2023.   \n473 [19] K. Denamgana\u00ef and J. A. Walker. Referentialgym: A nomenclature and framework for language   \n474 emergence & grounding in (visual) referential games. 4th NeurIPS Workshop on Emergent   \n475 Communication, 2020.   \n476 [20] K. Denamgana\u00ef and J. A. Walker. Referentialgym: A framework for language emergence &   \n477 grounding in (visual) referential games. 4th NeurIPS Workshop on Emergent Communication,   \n478 2020.   \n479 [21] K. Denamgana\u00ef and J. A. Walker. On (emergent) systematic generalisation and compositionality   \n480 in visual referential games with straight-through gumbel-softmax estimator. 4th NeurIPS   \n481 Workshop on Emergent Communication, 2020.   \n482 [22] R. Dessi, E. Kharitonov, and M. Baroni. Interpretable agent communication from scratch (with   \n483 a generic visual processor emerging on the side). May 2021.   \n484 [23] T. Eccles, Y. Bachrach, G. Lever, A. Lazaridou, and T. Graepel. Biases for emergent communi  \n485 cation in multi-agent reinforcement learning. Dec. 2019.   \n486 [24] S. Guo, Y. Ren, S. Havrylov, S. Frank, I. Titov, and K. Smith. The emergence of compositional   \n487 languages for numeric concepts through iterated learning in neural agents. arXiv preprint   \n488 arXiv:1910.05291, 2019.   \n489 [25] S. Havrylov and I. Titov. Emergence of Language with Multi-agent Games: Learning to   \n490 Communicate with Sequences of Symbols. may 2017. URL http://arxiv.org/abs/1705.   \n491 11192.   \n492 [26] I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess, A. Pritzel, M. Botvinick, C. Blundell,   \n493 and A. Lerchner. DARLA: Improving Zero-Shot Transfer in Reinforcement Learning. URL   \n494 https://arxiv.org/pdf/1707.08475.pdf.   \n495 [27] I. Higgins, N. Sonnerat, L. Matthey, A. Pal, C. P. Burgess, M. Botvinick, D. Hassabis, and   \n496 A. Lerchner. SCAN: Learning Abstract Hierarchical Compositional Visual Concepts. jul 2017.   \n497 URL http://arxiv.org/abs/1707.03389.   \n498 [28] I. Higgins, D. Amos, D. Pfau, S. Racaniere, L. Matthey, D. Rezende, and A. Lerchner. Towards   \n499 a Definition of Disentangled Representations. dec 2018. URL http://arxiv.org/abs/1812.   \n500 02230.   \n501 [29] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):   \n502 1735\u20131780, 1997.   \n503 [30] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. Van Hasselt, and D. Silver.   \n504 Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933, 2018.   \n505 [31] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu.   \n506 Reinforcement learning with unsupervised auxiliary tasks. In International Conference on   \n507 Learning Representations, 2016.   \n508 [32] N. Jaques, A. Lazaridou, E. Hughes, C. Gulcehre, P. A. Ortega, D. Strouse, J. Z. Leibo, and   \n509 N. De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement   \n510 learning. arXiv preprint arXiv:1810.08647, 2018.   \n511 [33] Y. Jiang, S. Gu, K. Murphy, and C. Finn. Language as an Abstraction for Hierarchical Deep   \n512 Reinforcement Learning. jun 2019. URL http://arxiv.org/abs/1906.07343.   \n513 [34] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. Recurrent experience replay   \n514 in distributed reinforcement learning. In International conference on learning representations,   \n515 2018.   \n516 [35] H. Kim and A. Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983, 2018.   \n517 [36] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint   \n518 arXiv:1412.6980, 2014.   \n519 [37] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,   \n520 2013.   \n521 [38] S. Kirby. Learning, bottlenecks and the evolution of recursive syntax. 2002.   \n522 [39] T. Korbak, J. Zubek, \u0141. Kuci\u00b4nski, P. Mi\u0142o\u00b4s, and J. R\u00b8aczaszek-Leonardi. Developmentally   \n523 motivated emergence of compositional communication via template transfer. oct 2019. URL   \n524 http://arxiv.org/abs/1910.06079.   \n525 [40] S. Kottur, J. M. F. Moura, S. Lee, and D. Batra. Natural Language Does Not Emerge \u2019Naturally\u2019   \n526 in Multi-Agent Dialog. jun 2017. URL http://arxiv.org/abs/1706.08502.   \n527 [41] A. Lazaridou and M. Baroni. Emergent Multi-Agent communication in the deep learning era.   \n528 June 2020.   \n529 [42] A. Lazaridou, A. Peysakhovich, and M. Baroni. Multi-Agent Cooperation and the Emergence   \n530 of (Natural) Language. dec 2016. URL http://arxiv.org/abs/1612.07182.   \n531 [43] A. Lazaridou, K. M. Hermann, K. Tuyls, and S. Clark. Emergence of Linguistic Communication   \n532 from Referential Games with Symbolic and Pixel Input. apr 2018. URL http://arxiv.org/   \n533 abs/1804.03984.   \n534 [44] D. Lewis. Convention: A philosophical study. 1969.   \n535 [45] F. Li and M. Bowling. Ease-of-Teaching and Language Structure from Emergent Communica  \n536 tion. jun 2019. URL http://arxiv.org/abs/1906.02403.   \n537 [46] F. Locatello, S. Bauer, M. Lucic, G. R\u00e4tsch, S. Gelly, B. Sch\u00f6lkopf, and O. Bachem. A sober   \n538 look at the unsupervised learning of disentangled representations and their evaluation. Oct.   \n539 2020.   \n540 [47] R. Lowe, J. Foerster, Y.-L. Boureau, J. Pineau, and Y. Dauphin. On the Pitfalls of Measuring   \n541 Emergent Communication. mar 2019. URL http://arxiv.org/abs/1903.05168.   \n542 [48] M. L. Montero, C. J. Ludwig, R. P. Costa, G. Malhotra, and J. Bowers. The role of disentangle  \n543 ment in generalisation. In International Conference on Learning Representations, 2021. URL   \n544 https://openreview.net/forum?id $=$ qbH974jKUVy.   \n545 [49] I. Mordatch and P. Abbeel. Emergence of Grounded Compositional Language in Multi-Agent   \n546 Populations. URL https://arxiv.org/pdf/1703.04908.pdf.   \n547 [50] K. Moritz Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. M.   \n548 Czarnecki, M. Jaderberg, D. Teplyashin, M. Wainwright, C. Apps, D. Hassabis, P. Blunsom,   \n549 and D. London. Grounded Language Learning in a Simulated 3D World. URL https:   \n550 //arxiv.org/pdf/1706.06551.pdf.   \n551 [51] J. Mu, V. Zhong, R. Raileanu, M. Jiang, N. Goodman, T. Rockt\u00e4schel, and E. Grefenstette.   \n552 Improving intrinsic exploration with language abstractions. Advances in Neural Information   \n553 Processing Systems, 35:33947\u201333960, 2022.   \n554 [52] G. Ostrovski, M. G. Bellemare, A. Oord, and R. Munos. Count-based exploration with neural   \n555 density models. In International conference on machine learning, pages 2721\u20132730. PMLR,   \n556 2017.   \n557 [53] P.-Y. Oudeyer and F. Kaplan. How can we define intrinsic motivation? In the 8th International   \n558 Conference on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems. Lund   \n559 University Cognitive Studies, Lund: LUCS, Brighton, 2008.   \n560 [54] R. Raileanu and T. Rockt\u00e4schel. Ride: Rewarding impact-driven exploration for procedurally  \n561 generated environments. In International Conference on Learning Representations, 2019.   \n562 [55] Y. Ren, S. Guo, M. Labeau, S. B. Cohen, and S. Kirby. Compositional Languages Emerge in a   \n563 Neural Iterated Learning Model. feb 2020. URL http://arxiv.org/abs/2002.01365.   \n564 [56] M. Rita, R. Chaabouni, and E. Dupoux. \" lazimpa\": Lazy and impatient neural agents learn to   \n565 communicate efficiently. arXiv preprint arXiv:2010.01878, 2020.   \n566 [57] K. Smith, S. Kirby, H. B. A. Life, and U. 2003. Iterated learning: A framework for the emergence   \n567 of language. Artificial Life, 9(4):371\u2013389, 2003. URL https://www.mitpressjournals.   \n568 org/doi/abs/10.1162/106454603322694825.   \n569 [58] C. Stanton and J. Clune. Deep curiosity search: Intra-life exploration can improve performance   \n570 on challenging deep reinforcement learning problems. arXiv preprint arXiv:1806.00553, 2018.   \n571 [59] X. Steenbrugge, S. Leroux, T. Verbelen, and B. Dhoedt. Improving generalization for abstract   \n572 reasoning tasks using disentangled feature representations. Nov. 2018.   \n573 [60] U. Strauss, P. Grzybek, and G. Altmann. Word length and word frequency. Springer, 2007.   \n574 [61] A. Tam, N. Rabinowitz, A. Lampinen, N. A. Roy, S. Chan, D. Strouse, J. Wang, A. Banino,   \n575 and F. Hill. Semantic exploration from language abstractions and pretrained representations.   \n576 Advances in Neural Information Processing Systems, 35:25377\u201325389, 2022.   \n577 [62] H. Tang, R. Houthooft, D. Foote, A. Stooke, X. Chen, Y. Duan, J. Schulman, F. De Turck, and   \n578 P. Abbeel. Exploration: A study of count-based exploration for deep reinforcement learning.   \n579 arxiv e-prints, page. arXiv preprint arXiv:1611.04717, 2016.   \n580 [63] S. van Steenkiste, F. Locatello, J. Schmidhuber, and O. Bachem. Are disentangled representa  \n581 tions helpful for abstract visual reasoning? May 2019.   \n582 [64] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas. Dueling network   \n583 architectures for deep reinforcement learning. In International conference on machine learning,   \n584 pages 1995\u20132003. PMLR, 2016.   \n585 [65] Z. Xu, M. Niethammer, and C. Raffel. Compositional generalization in unsupervised com  \n586 positional representation learning: A study on disentanglement and emergent language. Oct.   \n587 2022.   \n588 [66] G. K. Zipf. Human behavior and the principle of least effort: An introduction to human ecology.   \n589 Ravenio Books, 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "590 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Justification: Contribution/Claim # 1, i.e. a comparison between emergent and natural languages with respect to the kind of abstractions they perform, is substantiated in Section E.1, where we verify the internal validity of the metric we propose for quantitative comparison, and Section E.2 where measures using our proposed metrics on different natural or emergent languages are presented and discussed. Contribution/Claim # 2, i.e. simple countbased exploration methods guided by natural or emergent language abstractions are helpful for exploration in reinforcement learning over hard-exploration, procedurally-generated environments, is substantiated in Section E.3. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We discuss limitations at the end of Section 4. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "44 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Justification: Our only theoretical results is found in Appendix C with the full set of assumptions and a complete and correct proof. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: All the information needed to reproduce the main experimental results and appendices experimental results are discussed both in Sections 3 or 4 for critical (and new) hyperparameters, and in Appendices G and F for hyperparameters introduced in previous works. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 14}, {"type": "text", "text": "697 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n698 authors are welcome to describe the particular way they provide for reproducibility.   \n699 In the case of closed-source models, it may be that access to the model is limited in   \n700 some way (e.g., to registered users), but it should be possible for other researchers   \n701 to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "02 5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "03 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n04 tions to faithfully reproduce the main experimental results, as described in supplemental   \n05 material? ", "page_idx": 15}, {"type": "text", "text": "Justification: The open-access code contains a README.md flie with sufficient instructions to faithfully reproduce the main experimental results. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "729 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "0 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n1 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n2 results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "734 Justification: All the information needed to reproduce the main experimental results and   \n735 appendices experimental results are discussed both in Sections 3 or 4 for critical (and newly  \n736 introduced) hyperparameters, and in Appendices G and F for hyperparameters introduced   \n37 in previous works. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "744 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "745 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n746 information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "747 Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "748 Justification: All plots (barplots or line plots) contains in the title the type of information   \n749 about the statistical significance of the experiments (i.e. min/median/max, meaning that the   \n750 shaded area reflect the min and max values of the distribution while the bar or line reflects   \n751 the median of the distribution).   \n752 Guidelines:   \n753 \u2022 The answer NA means that the paper does not include experiments.   \n754 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n755 dence intervals, or statistical significance tests, at least for the experiments that support   \n756 the main claims of the paper.   \n757 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n758 example, train/test split, initialization, random drawing of some parameter, or overall   \n759 run with given experimental conditions).   \n760 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n761 call to a library function, bootstrap, etc.)   \n762 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n763 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n764 of the mean.   \n765 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n766 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n767 of Normality of errors is not verified.   \n768 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n769 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n770 error rates).   \n771 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n772 they were calculated and reference the corresponding figures or tables in the text.   \n773 8. Experiments Compute Resources   \n774 Question: For each experiment, does the paper provide sufficient information on the com  \n775 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n776 the experiments?   \n777 Answer: [Yes]   \n778 Justification: Section F contains sufficient information on the computer resources needed to   \n779 reproduce the experiments.   \n780 Guidelines:   \n781 \u2022 The answer NA means that the paper does not include experiments.   \n782 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n783 or cloud provider, including relevant memory and storage.   \n784 \u2022 The paper should provide the amount of compute required for each of the individual   \n785 experimental runs as well as estimate the total compute.   \n786 \u2022 The paper should disclose whether the full research project required more compute   \n787 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n788 didn\u2019t make it into the paper).   \n789 9. Code Of Ethics   \n790 Question: Does the research conducted in the paper conform, in every respect, with the   \n791 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n792 Answer: [Yes]   \n793 Justification: The research conducted in the paper conform in every respect with the NeurIPS   \n794 Code of Ethics.   \n795 Guidelines:   \n796 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n797 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n798 deviation from the Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "801 10. Broader Impacts   \n802 Question: Does the paper discuss both potential positive societal impacts and negative   \n803 societal impacts of the work performed?   \n804 Answer: [Yes]   \n805 Justification: The paper contains a Broader Impact discussion in Appendix A.   \n806 Guidelines:   \n807 \u2022 The answer NA means that there is no societal impact of the work performed.   \n808 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n809 impact or why the paper does not address societal impact.   \n810 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n811 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n812 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n813 groups), privacy considerations, and security considerations.   \n814 \u2022 The conference expects that many papers will be foundational research and not tied   \n815 to particular applications, let alone deployments. However, if there is a direct path to   \n816 any negative applications, the authors should point it out. For example, it is legitimate   \n817 to point out that an improvement in the quality of generative models could be used to   \n818 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n819 that a generic algorithm for optimizing neural networks could enable people to train   \n820 models that generate Deepfakes faster.   \n821 \u2022 The authors should consider possible harms that could arise when the technology is   \n822 being used as intended and functioning correctly, harms that could arise when the   \n823 technology is being used as intended but gives incorrect results, and harms following   \n824 from (intentional or unintentional) misuse of the technology.   \n825 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n826 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n827 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n828 feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "34 Justification: The paper does release data or models that have any risk for misuses.   \n35 Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "851 Justification: Apart from the environments from MiniGrid [15], the paper does not use   \n852 existing assets.   \n853 Guidelines:   \n854 \u2022 The answer NA means that the paper does not use existing assets.   \n855 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n856 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n857 URL.   \n858 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n859 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n860 service of that source should be provided.   \n861 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n862 package should be provided. For popular datasets, paperswithcode.com/datasets   \n863 has curated licenses for some datasets. Their licensing guide can help determine the   \n864 license of a dataset.   \n865 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n866 the derived asset (if it has changed) should be provided.   \n867 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n868 the asset\u2019s creators.   \n869 13. New Assets   \n870 Question: Are new assets introduced in the paper well documented and is the documentation   \n871 provided alongside the assets?   \n872 Answer: [NA]   \n873 Justification: The paper does not release new assets.   \n874 Guidelines:   \n875 \u2022 The answer NA means that the paper does not release new assets.   \n876 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n877 submissions via structured templates. This includes details about training, license,   \n878 limitations, etc.   \n879 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n880 asset is used.   \n881 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n882 create an anonymized URL or include an anonymized zip file.   \n883 14. Crowdsourcing and Research with Human Subjects   \n884 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n885 include the full text of instructions given to participants and screenshots, if applicable, as   \n886 well as details about compensation (if any)?   \n887 Answer: [NA]   \n888 Justification: The paper does not involve experiments with human subjects nor crowdsourc  \n889 ing.   \n890 Guidelines:   \n891 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n892 human subjects.   \n893 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n894 tion of the paper involves human subjects, then as much detail as possible should be   \n895 included in the main paper.   \n896 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n897 or other labor should be paid at least the minimum wage in the country of the data   \n898 collector.   \n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human ", "page_idx": 18}, {"type": "text", "text": "Subjects ", "page_idx": 18}, {"type": "text", "text": "901 Question: Does the paper describe potential risks incurred by study participants, whether   \n902 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n903 approvals (or an equivalent approval/review based on the requirements of your country or   \n904 institution) were obtained?   \n905 Answer: [NA]   \n906 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n907 Guidelines:   \n908 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n909 human subjects.   \n910 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n911 may be required for any human subjects research. If you obtained IRB approval, you   \n912 should clearly state this in the paper.   \n913 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n914 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n915 guidelines for their institution.   \n916 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n917 applicable), such as the institution conducting the review.   \n919 No technology is safe from being used for malicious purposes, which equally applies to our research.   \n920 However, we view many of the ethical concerns surrounding research to be mitigated in the present   \n921 case. These include data-related concerns such as fair use or issues surrounding use of human subjects,   \n922 given that our data consists solely of simulations.   \n923 With regards to the ethical aspects related to its inclusion in the field of Artificial Intelligence, we argue   \n924 that our work aims to have positive outcomes on the development of human-machine interfaces since   \n925 we investigate, among other things, alignment of emergent languages with natural-like languages.   \nThe current state of our work does not allow extrapolation towards negative outcomes. We believe   \n927 that this work is of benefti to the research community of reinforcement learning, language emergence   \n928 and grounding, in their current state. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "929 B Further details on Count-Based Exploration ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "930 Another approach to counting states from continuous and/or high-dimensional state spaces is by   \n931 relying on hashing functions, so that states become tractable. Indeed, Tang et al. [62] have shown that   \n932 a generalisation of classical counting techniques through hashing can provide an appropriate signal   \n933 for exploration in continuous and/or high-dimensional environments where informed exploration is   \n934 required. In effect, they proposed to discretise the state space $\\boldsymbol{S}$ with a hash function $\\dot{\\phi}:S\\to\\mathbb{Z}^{k}$ ,   \n935 with $k\\in\\mathbb{N}\\setminus\\{0\\}$ , to derive an exploration bonus of the form $\\begin{array}{r}{r^{+}(s)=\\frac{\\beta}{\\sqrt{n(\\phi(s))}}}\\end{array}$ where $\\beta\\in\\mathbb{R}^{+}$ is a   \n936 bonus coefficient and $n(.)$ is a count initialised at zero for the whole range of $\\phi$ and updated at each   \n937 step $t$ of the RL loop by increasing by 1 the count $n\\big(\\phi\\big(s_{t}\\big)\\big)$ related to the current observation/state   \n938 $s_{t}$ . Performance is dependent on the hash function $\\phi$ , and especially in terms of granularity of the   \n939 discretisation it induces. Indeed, it would be desirable that the \u2018similar\u2019 states result in hashing   \n940 collisions while the \u2018distant\u2019 states would not. To this end, they propose to use locality-sensitive   \n941 hashing (LSH) such as SimHash [13], resulting in the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\phi(s)=\\operatorname{sgn}(A g(s))\\in\\{-1,1\\}^{k},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "942 where sgn is the sign function, $A\\in\\mathbb{R}^{k\\times D}$ is a matrix with each entry drawn i.i.d. from a standard   \n943 Gaussian distribution, and $g:S\\rightarrow\\mathbb{R}^{D}$ is an optional preprocessing function. Note that increasing   \n944 $k$ leads to higher granularity and therefore decreases the number of hashing collisions. Tang et al.   \n945 [62] reports great results on the Atari 2600 benchmarks, both with and without a learnable $g$ that is   \n946 modelled as the encoder of an autoencoder (AE). ", "page_idx": 20}, {"type": "text", "text": "947 C Sensitivity Analisys of the Compactness Ambiguity Metric ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "948 Based on derivative-based local sensitivity analysis, we propose an intuitive proof of our claim that   \n949 defining timespans in relation to the relative ambiguity reduces the sensibility to variations induced   \n950 by redundancy-based ambiguity in the resulting metric, compared to defining timespans in relation to   \n951 the the maximal length $T$ of an agent\u2019s trajectory in the environment. To do so, we assume:   \n52 (i) that there exists two differentiable function $f_{i}.f_{i}^{\\prime}$ such that for all $i\\,\\in\\,[1,N]$ , we have   \n53 $C A(\\mathcal{D})_{T_{i}}\\,=\\,f_{i}(\\mathcal{D},\\mathcal{R}\\mathcal{A}_{l}^{\\mathrm{re}}$ edundancy, $\\mathcal{R A}_{l}^{\\mathrm{abstract}})$ when $T_{i}$ is defined according to Equation 2,   \n54 and respectively with $f_{i}^{\\prime}$ when using $T_{i}^{\\prime}$ from Equation 3, and   \n55 (ii) that their partial derivatives with respect to $T_{i}$ or $T_{i}^{\\prime}$ are negative. Indeed, $T_{i}$ and $T_{i}^{\\prime}$ are   \n56 involved into flitering operations reducing the value of the numerator in Equation 4, therefore   \n57 any increase of their values would result in decreasing the overall metric output, which   \n58 implies that their partial derivatives with $f_{i}$ and $f_{i}^{\\prime}$ must be negative.   \n959 With those assumptions, we show that $f_{i}$ \u2019s sensitivity to redundancy-induced ambiguity $\\mathcal{R A}_{l}^{\\mathrm{re}}$ edundancy   \n960 is less than that of $f_{i}^{\\prime}$ : ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Proof. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\partial f_{i}}{\\partial\\mathcal{R}\\mathcal{A}_{l}^{\\mathrm{redundancy}}}=\\frac{\\partial f_{i}}{\\partial C C_{\\mathcal{D}}}\\cdot\\frac{\\partial C C_{\\mathcal{D}}}{\\partial\\mathcal{R}\\mathcal{A}_{l}^{\\mathrm{redundancy}}}+\\frac{\\partial f_{i}}{\\partial T_{i}}\\cdot\\frac{\\partial T_{i}}{\\partial\\mathcal{R}\\mathcal{A}_{l}^{\\mathrm{redundancy}}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(from Assump. (i) about $f_{i}$ ) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iff\\frac{\\partial f_{i}}{\\partial R\\mathcal{A}_{l}^{\\mathrm{redundancy}}}=\\frac{\\partial f_{i}^{\\prime}}{\\partial\\mathcal{R}\\mathcal{A}_{l}^{\\mathrm{redundancy}}}+\\frac{\\partial f_{i}}{\\partial T_{i}}\\cdot\\frac{\\partial T_{i}}{\\partial\\mathcal{R}\\mathcal{A}_{l}^{\\mathrm{redundancy}}}\\quad\\mathrm{(from~Assump.~(i)~about~}f_{i}^{\\prime}\\mathrm{)}}\\\\ &{\\iff\\frac{\\partial f_{i}}{\\partial R\\mathcal{A}_{l}^{\\mathrm{redundancy}}}=\\frac{\\partial f_{i}^{\\prime}}{\\partial\\mathcal{R}\\mathcal{A}_{l}^{\\mathrm{redundancy}}}+\\frac{\\partial f_{i}}{\\partial T_{i}}\\cdot\\lambda_{i}}\\\\ &{\\implies|\\frac{\\partial f_{i}}{\\partial\\mathcal{R}\\mathcal{A}_{l}^{\\mathrm{redundancy}}}|\\leq|\\frac{\\partial f_{i}^{\\prime}}{\\partial\\mathcal{R}\\mathcal{A}_{l}^{\\mathrm{rdundancy}}}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "962 D Preliminary Experiments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "963 D.1 Impact of Referential Game Accuracy ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "964 In this experiments, we investigate whether the RG accuracy impacts the RL agent training, in the   \n965 context of the MultiRoom-N7-S4 environment from MiniGrid [15], with an RL sampling budget of   \n966 $1M$ observations.   \n967 Hypothesis. We seek to validate the following hypotheses, (PH1) : the sample-efficiency of the   \n968 RL agent is dependant on the quality of the RG players, as parameterised by the accRG\u2212thresh   \n969 hyperparameter.   \n970 Evaluation. We report both the success rate and the coverage count in the hard-exploration task of   \n971 MultiRoom-N7-S4. To compute the coverage count, we overlay a grid of tiles over the environment\u2019s   \n972 possible locations/cells of the agents and we count the number of different tiles visited by the RL   \n973 agent over the course of each episode. We use 3 random seeds for each agent. In order to evaluate the   \n974 impact of the RG accuracy strictly in terms of the kind of abstractions that are being performed by the   \n975 resulting EL, we use the Impatient-Only loss function (removing the impact of the hyperparameter of   \n976 the scheduling function $\\alpha(\\cdot)$ from the Lazy term of the STGS-LazImpa loss function), and we employ   \n977 an agnostic version of our proposed EReLELA agent, i.e. without sharing the observation encoder   \n978 between the RG players and the RL agent. We present results for two different RG accuracy   \n979 threshold $a c c_{R G-t h r e s h}=60\\%$ (green) or $a c c_{R G-t h r e s h}=80\\%$ (red), and compare against, as an   \n980 upper bound the Natural Language Abstraction agent (blue), which refers to using the NL oracle to   \n981 compute intrinsic reward, and, as a lower bound an ablated version of EReLELA without RG training   \n982 (orange).   \n983 Results. We present results in Figure 6. We observe statistically significant differences between   \n984 the performances (in terms of success rate, cf. Figure 6(left)) of the two EReLELA agents with   \n985 $a c c_{R G-t h r e s h}=60\\%$ or $a c c_{R G-t h r e s h}=80\\%$ , thus validating hypothesis (PH1). We observe that   \n986 higher RG accuracy threshold lead to higher sample-efficiency.   \n987 As a sanity check, we plot the results of the ablated EReLELA agent without RG training, and we were   \n988 expecting it to perform poorer than any other agent since the quality of its RG players is the lowest, at   \n989 chance level. Yet, we observe that it performs on par with the best $a c c_{R G-t h r e s h}=80\\%.$ -EReLELA   \n990 agent. While puzzling, we propose a possible explanation in the observation that the test-time relative   \n991 expressivity of the ablated agent is higher than that of the least-performing, $a c c_{R G-t h r e s h}=60\\%.$   \n992 EReLELA agent, and on par with that of the best-performing, $a c c_{R G-t h r e s h}\\,=\\,80\\%.$ -EReLELA   \n993 agent, at the beginning of the RL agent training process. Thus, we interpret this as follows: the   \n994 randomly-initialised ablated agent\u2019s EL is possibly performing an abstraction over the observation ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/610aa15b732773e52e4553ac28b14b22766674a75060c52cc0ba395f7e51da07.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 6: Success rate (left), test-time relative expressivity (middle), and per-episode coverage count (right) in MultiRoom-N7-S4 from MiniGrid [15], computed as running averages over 256 episodes each time (i.e. 32 in parallel, as there are 32 actors, over 8 running average steps), for different agents: (i) the Natural Language Abstraction agent (blue) refers to using the NL oracle to compute intrinsic reward, the Agnostic Impatient-Only EReLELA agent refers to our proposed architecture without sharing the observation encoder between the RG players and the RL agent, using the Impatient-Only loss function to optimize the RG players, with an RG accuracy threshold $a c c_{R G-t h r e s h}=60\\%$ (ii - green) or $a c c_{R G-t h r e s h}=80\\%$ (iii - red), and (iv) an ablated version without RG training (orange). ", "page_idx": 22}, {"type": "text", "text": "995 space that is good-enough for the RL agent to start learning exploration skills, the same way the   \n996 random network in the context of the RND agent from Burda et al. [9] probably does, and increasing   \n997 the quality of the RG players may only be a sufficient condition to increasing the sample-efficiency   \n998 of the EL-guided RL agent. ", "page_idx": 23}, {"type": "text", "text": "999 D.2 Impact of Referential Game Distractors ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1000 In this experiments, we investigate whether the RG\u2019s number of distractors $K$ and distractor sampling   \n1001 scheme impacts the RL agent training, in the context of the KeyCorridor-S3-R2 environment from   \n1002 MiniGrid [15], with an RL sampling budget of $1M$ observations.   \n1003 Hypothesis. We seek to validate the following hypotheses, (PH2) : the sample-efficiency of the RL   \n1004 agent is dependant on the number of distractors $K$ and the distractor sampling scheme.   \n1005 Evaluation. We report the success rate in the hard-exploration task of KeyCorridor-S3-R2. We   \n1006 use 3 random seeds for each agent. Like previously, we use the Impatient-Only loss function (to   \n1007 remove the impact of the hyperparameter of the scheduling function $\\alpha(\\cdot)$ from the Lazy term of   \n1008 the STGS-LazImpa loss function), and we employ an agnostic version of our proposed EReLELA   \n1009 agent, i.e. without sharing the observation encoder between the RG players and the RL agent.   \n1010 We present results for three different number of distractors $K\\,\\in\\,[15,128,256]$ and two different   \n1011 sampling scheme between UnifDSS corresponding to uniformly sampling distractors over the whole   \n1012 training dataset, or Sim50DSS corresponding to sampling distractors $50\\bar{\\%}$ of the time from the same   \n1013 RL episode than the current target stimulus is from and, the rest of the time following UnifDSS.   \n1014 Following results in Appendix D.1, we set the RG accuracy threshold $a c c_{R G-t h r e s h}\\in[80\\%,90\\%]$ .   \n1015 Results. We present results in Figure 7. We observe statistically significant differences between the   \n1016 performances of the different EReLELA agents, thus validating hypothesis (PH2). Our results show   \n1017 that (i) the number of distractors $K$ is the most impactful parameter and it correlates positively with   \n1018 the resulting performance, irrespective of the distractor sampling scheme used, and, indeed, (ii) while   \n1019 9 the $S i m50D S S$ seems to provide better performance than UnifDSS for low numbers of distractors   \n1020 $K=15$ , although not statistically-significantly, the table is turned when considering high number of   \n1021 distractors $K=256$ where the UnifDSS yields statistically significantly better performance than the   \n1022 Sim50DSS. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "1023 E Further Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1024 E.1 Experiment #1: CAM Metric Internal Validity ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1025 Environment. We consider a 3D room environment of MiniWorld [15], where the agent\u2019s observation   \n1026 is egocentric, as a first-person viewpoint. The room is fliled with 5 different, randomly-placed objects,   \n1027 with different shapes (among ball, box or key) and colours (among). The dimensions simulate a 12   \n1028 by 5 meters room, like shown in a top-view perspective in Figure 1.   \n1029 Hypothesis. In this experiments, we seek to validate two hypotheses, (H1.1) : the Compactness   \n1030 Ambiguity Metric captures something that is related to the kind of abstraction a language performs,   \n1031 and (H1.2) : the Compactness Ambiguity Metric allows a graduated comparison of different kind   \n1032 of abstractions being performed, meaning that it allows discrimination between different kind of   \n1033 abstractions.   \n1034 Evaluation. In order to compute the metric, we use 5 seeds to gather random walk trajectories in our   \n1035 environment, for each language. In order to evaluate (H1.1), we propose to measure a language that   \n1036 is built to present no meaningful abstractions and we expect the measure to be close to null. We build   \n1037 a language that performs no meaningful abstraction from the natural language oracles by shuffling   \n1038 its utterances over the set of agent trajectories that are used to compute the metric, meaning that   \n1039 the mapping between temporally-sensitive stimuli and linguistic utterances is rendered completely   \n1040 random.   \n1041 Then, in order to evaluate (H1.2), we show experimental evidences that the metric allows qualitative   \n1042 discrimination between the different languages built above from the natural language oracles, which   \n1043 are build to perform different kind of abstractions.   \n1044 Results. We present results of the metric with $N=6$ timespans in Figure 8, for $\\lambda_{0}=0.0306125$ ,   \n1045 $\\lambda_{1}=0.06125$ , $\\lambda_{2}=0.125$ , $\\lambda_{3}=0.25$ , $\\lambda_{4}=0.5$ and $\\lambda_{5}=0.75$ . As the shuffled (natural) language   \n1046 measure is almost null on all timespans/thresholds, we validate hypothesis (H1.1).   \n1047 We observe that we can qualitatively discriminate between each evaluated language\u2019s measures since   \n1048 the histograms are statistically different. Moreover, language abstractions scores are inversely corre  \n1049 lated with the amount of information being abstracted away, i.e. attribute-value-specific languages\u2019   \n1050 abstraction score lower than colour/shape-specific languages abstraction, which score lower than   \n1051 natural language abstractions. Thus, we can see that the metric is graduated and that the graduation   \n1052 follows the amount of abstraction being performed by each language. This allows us to validate   \n1053 hypothesis (H1.2).   \n1055 In this experiment, we investigate what kind of abstractions do ELs perform over a 3D environment,   \n1056 in comparison to some natural languages abstractions, as detailed at the beginning of Section 4. For   \n1057 further precision, we also implement attribute-value-specific language oracles with the same flitering   \n1058 approach. For instance, for the green value on the colour attribute, we would obtain a green-only   \n1059 language oracle whose utterances could be \u2018EoS\u2019 if no visible object is green, or \u2018green green\u2019 if there   \n1060 are two green objects visible in the agent\u2019s observation. We consider the same 3D room environment   \n1061 of MiniWorld [15] as in Section E.1, i.e. the agent\u2019s observation is egocentric, as a first-person   \n1062 viewpoint and the room is filled with 5 different, randomly-placed objects, with different shapes   \n1063 (among ball, box or key) and colours (among). The dimensions simulate a 12 by 5 meters room, like   \n1064 shown in a top-view perspective in Figure 1.   \n1065 Hypothesis. We seek to validate the following hypotheses, (H2.1) : ELs build meaningful abstractions,   \n1066 and (H2.2) : ELs brought about using the STGS-LazImpa loss function (type II) perform more   \n1067 meaningful abstractions than Impatient-Only baseline (type I).   \n1068 Evaluation. In order to make the CAM measures, we use 5 seeds to gather random walk trajectories   \n1069 in our environment, for each language. In order to evaluate both (H2.1) and (H2.2), we use the CAM   \n1070 to measure the kind of abstractions performed by ELs brought about in the two different EReLELA   \n1071 settings, with Impatient-Only or STGS-LazImpa losses, and compare those measures with those of   \n1072 the oracles\u2019 languages that we previously studied.   \n1073 Results. We present results of the metric with $N=6$ timespans in Figure 9. We observe statistically   \n1074 significant differences between ELs of type I and II, with type I\u2019s abstraction being similar to a Blue  \n1075 specific language\u2019s abstraction (timespans $0-4)$ or a Ball-specific language\u2019s abstraction (timespans   \n1076 $1-3)$ , and type II\u2019s abstraction not really resembling any of the oracle languages\u2019 abstractions, but   \n1077 still being meaningful with scores increasing along with the length of the considered timespans. Thus,   \n1078 we validate hypothesis (H2.1), but cannot conclude on hypothesis (H2.2), unless we consider that   \n1079 CAM scores related to longer timespans are more meaningful, for instance. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/f0ddc4876232f3f6d72507146eb381f36e1a7f54457d54f72cfa35af9b3768e1.jpg", "img_caption": ["Figure 8: Interval validity measures of Compactness Ambiguity Metric for $N=6$ timespans/thresholds, with $\\lambda_{0}=0.0306125$ , $\\lambda_{1}=0.06125$ , $\\lambda_{2}=0.125$ , $\\lambda_{3}=0.25$ , $\\lambda_{4}=0.5$ and $\\lambda_{5}=0.75$ , for different languages built to perform different kind of abstraction. We can qualitatively discriminate between each languages, and validate that the shuffled (natural) language\u2019s meaningless abstraction scores almost null. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "1080 E.3 Experiment #3: Learning Purely-Navigational Systematic Exploration Skills from 1081 Scratch ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1082 In the following, we present an experiment in the MultiRoom-N7-S4 environment from MiniGrid [15],   \n1083 which is possibly less challenging than KeyCorridor-S3-R2, presented in the Section 4, for it does   \n1084 not involve as many complex object manipulation (e.g. only open/close doors, no unlocking of   \n1085 doors \u2013 which requires the corresponding key to be firstly picked up \u2013 nor pickup/drop keys or   \n1086 other objects as distractors), but still poses a purely-navigational hard-exploration challenge. We   \n1087 report results on the agnostic version of our proposed EReLELA architecture, that is to say without   \n1088 sharing the observation encoder between both RG players and the RL agent, in order to guard   \n1089 ourselves against the impact of possible confounders found in multi-task optimization, such as possible ", "page_idx": 25}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/dadcfd04e3927acdc80bca3daa3815da36f745511a00bf31e63f5643af701e10.jpg", "img_caption": ["Figure 9: Measures of Compactness Ambiguity Metric for $N\\,=\\,6$ timespans/thresholds, with $\\lambda_{0}=0.0306125$ , $\\lambda_{1}=0.06125$ , $\\lambda_{2}=0.125$ , $\\lambda_{3}=0.25$ , $\\lambda_{4}=0.5$ and $\\lambda_{5}=0.75$ , comparing ELs (Type I and II) with different oracles\u2019 languages built to perform different kind of abstraction. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/7dfb4a51a3c83687d0865c58698f2de946f1e7bcfac26b9497188784b886ec7c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 10: Success rate (left) and per-episode coverage count (right) in MultiRoom-N7-S4 from MiniGrid [15], computed as running averages over 1024 episodes each time (i.e. 32 in parallel, as there are 32 actors, over 32 running average steps), for different agents: (i) the Natural Language Abstraction agent (NLA) refers to using the NL oracle to compute intrinsic reward, (ii) the STGSLazImpa EReLELA agent refers to our proposed architecture, EReLELA, using the STGS-LazImpa loss function to optimize the RG players, and (iii) the Impatient-Only EReLELA agent refers to the same architecture without the lazy-speaker loss to optimize the RG players. ", "page_idx": 26}, {"type": "text", "text": "1090 interference between the RL-objective-induced gradients and the RG-training-induced gradients. We   \n1091 use an RG accuracy threshold $a c c_{R G-t h r e s h}=65\\%$ and a number of training distractors $K=3$   \n1092 (like at testing/validation time).   \n1093 Hypotheses. We consider whether $\\mathrm{NL}$ abstractions can help for a purely-navigational hard  \n1094 exploration task in RL with a count-based approach (H3.0), and refer to the relevant agent using   \n1095 NL abstractions to compute intrinsic rewards as NLA. Then, we make the hypothesis that ELs can   \n1096 be used similarly (H3.1), and we investigate to what extent do ELs compare to NLs in terms of   \n1097 abstraction performed, in this purely-navigational task. In the case of (H3.1) being verified, we would   \n1098 expect ELs to perform similar abstractions as NLs (H3.2).   \n1099 Evaluation. We evaluate (H3.0) and (H3.1) using both the success rate and the coverage count.To   \n1100 compute the coverage count, we overlay a grid of tiles over the environment\u2019s possible locations/cells   \n1101 of the agents and we count the number of different tiles visited by the RL agent over the course of   \n1102 each episode. To evaluate (H3.2), we compute the CAM scores of both the ELs and the oracles\u2019   \n1103 natural, color-specific, and shape-specific languages. As we remarked that an agent\u2019s skillfullness at   \n1104 the task would induce very different trajectories (e.g. in MultiRoom-N7-S4, staying in the first room   \n1105 and only ever seeing the first door, for an unskillfull agent, as opposed to visiting multiple rooms   \n1106 and observing multiple colored-doors, for a skillfull agent), we compute the oracle languages CAM   \n1107 scores on the exact same trajectories than used to compute each EL\u2019s CAM scores.   \n1108 Results. We present in Figure 10(left) the success rate of the different agents, and the per-episode   \n1109 coverage count in Figure 10(right).From the fact that both the NLA and EReLELA agent performance   \n1110 converges higher or close to $80\\%$ of success rate, we validate hypotheses (H0) and (H3.1), in the   \n1111 context of the MultiRoom-N7-S4 environment. We remark that the sample-efficiency is slightly better   \n1112 for NLA than it is for EL-based agents, possibly because of the fact that ELs are learned online   \n1113 in parallel of the RL training, as opposed to the case of NLA which makes use of a ready-to-use   \n1114 oracle. Among the two EReLELA agents, the learning curves are not statistically-significantly ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/e671e1b2ac82379b82e10274a7bb19e7b9a09e5fb60c236028016b903f5b46fb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 11: Performance and qualities of the ELs brought about in the context of both (i) the STGS-LazImpa EReLELA agent, and (ii) the Impatient-Only EReLELA agent, with respect to both the training- and validation/testing-time RG accuracy (left), the validation/test-time Instantaneous Coordination [32, 47, 23](middle), and the validation/testing-time length of the speaker\u2019s messages (as a ratio over the max sentence length $L=128$ - right). ", "page_idx": 26}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/7a454cd984f41f800cc880e7d3f3ffe764e1cba4f3cdfb9a6994ca791bc8e2c0.jpg", "img_caption": ["Figure 12: Comparison of Compactness Ambiguity Metric scores for $N=6$ timespans/thresholds, with $\\lambda_{0}=0.0306125$ , $\\lambda_{1}=0.06125$ , $\\lambda_{2}=0.125$ , $\\lambda_{3}=0.25$ , $\\lambda_{4}=0.5$ and $\\lambda_{5}=0.75$ , between the abstractions performed by ELs brought about in the context of both (i) the STGS-LazImpa EReLELA agent (in green, first rows) and (ii) the Impatient-Only EReLELA agent (in purple, bottom rows), and the abstractions performed by the natural, colour-specific, and shape-specific languages, computed on the very same agent trajectories. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "1115 distinguishable, meaning that learning systematic exploration skills with EReLELA can be done with   \n1116 some robustness to the anecdotical differences in qualities of the different ELs due to using different   \n1117 optimization losses. Indeed, we also report in Figure 11 both the training- and validation/testing-time   \n1118 RG accuracies (on the left), the validation/testing-time Instantaneous Coordination (in the middle \u2013   \n1119 Jaques et al. [32], Lowe et al. [47], Eccles et al. [23]), and the validation/testing-time length of the RG   \n1120 speaker\u2019s messages (on the right), showing that the ELs brought about in the two different contexts   \n1121 perform differently in terms of their RG objective and have different qualities, but these discrepancies   \n1122 do not seem to impact the RL agents learning equally well from the different abstractions they   \n1123 perform (as evidenced in the next paragraph).   \n1124 Next, with regards to hypothesis (H3.2), we investigate whether the two contexts bring about ELs   \n1125 that perform different abstractions, and how do these relate to the abstractions performed by natural,   \n1126 colour-specific, and shape-specific languages, by showing in Figure 12 their CAM scores. We   \n1127 observe that both contexts result in ELs performing abstractions similar or better than colour-specific   \n1128 languages, which is to be expected as (door) colours are the most salient features of the environment.   \n1129 Indeed, the only two shapes or objects visible are \u2018wall\u2019 and \u2018door\u2019, whereas there are more than   \n1130 7 different colours of interest. In the context of the Impatient-Only EReLELA agent, the EL\u2019s   \n1131 abstractions are scoring very similarly to NL abstractions, as we consider longer timespans (from   \n1132 timespans #2 to #5). We could hypothesise that without the lazy-ness constraint the speaker agent   \n1133 may be given enough capacity to compress/express information pertaining to the location of visible   \n1134 objects, as this information is the only one that is captured by the NL oracle but not captured by the   \n1135 shape- and colour-specific languages. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "1136 E.4 Experiment #4: Quantifying RL Agents\u2019 Learning Progress? ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1137 In the context of RGs, the speed at which a language emerges (in terms of sampled observations, or   \n1138 number of games played) may possibly remain constant, when the data and the player architectures   \n1139 are fixed. Thus, when the data changes, the rate of language emergence may change too. Incidentally,   \n1140 we are entitled to ponder whether some properties of the data, which here are RL trajectories, would   \n1141 influence the rate of language emergence and how?   \n1142 Hypothesis. We hypothesise that as the RL agent gets more skillful, the expressivity of the emergent   \n1143 language increases (H4.1). Indeed, at each RG training epoch, the size of the dataset is fixed, and as   \n1144 the stimuli gets more diverse when the RL agent gets more skillful at exploring, the RG training will   \n1145 prompt the EL to increase its expressivity.   \n1146 Evaluation. To verify our hypothesis, we propose to measure the skillfullness of the RL agent in   \n1147 terms of exploration using the per-episode coverage count metric, and we measure the expressivity of   \n1148 the EL via the test-time (Relative) Expressivity after each RG training epoch.   \n1149 Results. We present results in Figure 13, that show the (relative) expressivity of the ELs does exhibit   \n1150 variations throughout the learning process of the RL agent. And, if we perform a regression analysis   \n1151 with each runs in terms of the per-episode coverage count of the RL agent on the x-axis and the   \n1152 expressivity of the ELs on the y-axis, we obtain a high coefficient of determination between the two   \n1153 metrics, $R^{\\tilde{2}}=0.4642$ . Thus, we conclude that the (relative) expressivity of the ELs in EReLELA can   \n1154 provide a way to quantify the progress of the RL agent, at least when it comes to exploration skills.   \n1155 Limitations. Exploration skills translates directly into diversity of the stimuli being observed, and   \n1156 therefore it prompts any RG players to increase the expressivity of their communication protocol,   \n1157 but it is remains to be seen whether this effect is valid in any environment. For instance, it is unclear   \n1158 whether a skillfull player in any other video game would induce the same effect on the diversity of   \n1159 the stimuli encountered. Thus, it is worth investigating whether this correlation holds for other genre   \n1160 of environments and skills, which we leave to future works.   \n1162 The ERELELA architecture is made up of three differentiable agents, the language-conditioned RL   \n1163 agent and the two RG agents (speaker and listener). Each agent contains at least a visual/observation   \n1164 encoder module that can be shared between agents.Both RG agents contain a language module that is   \n1165 not shared. The listener agent additionally incorporates a third decision module that combines the   \n1166 outputs of the other two modules. The RL agent similarly incorporates a third decision module with   \n1167 the addition that this third module contains a recurrent network, acting as core memory module for   \n1168 the agent. Using the Straight-Through Gumbel-Softmax (STGS) approach in the communication   \n1169 channel of the RG, the speaker agent is prompted to produce the output string of symbols with a   \n1170 Start-of-Sentence symbol and the visual module\u2019s output as an initial hidden state while the listener   \n1171 agent consumes the string of symbols with the null vector as the initial hidden state. In the following   \n1172 subsections, we detail each module architecture in depth.   \n1173 Visual Module. The visual module $f(\\cdot)$ consists of the Shared Observation Encoder, which can be   \n1174 shared between all the different agents.The former consists of three blocks of convolutional layers   \n1175 of sizes 8, 4, 3 with strides $4,3,1$ , each followed by a 2D batch normalization layer and a ReLU   \n1176 non-linear activation function. The two first convolutional layers have 32 filters, whilst the last one   \n1177 has 64. The bias parameters of the convolutional layers are not used, as it is common when using   \n1178 batch normalisation layers. Inputs are stimuli consisting of RGB frames of the environment resized   \n1179 to $64\\times64$ .   \n1180 Language Module. The language module $g(\\cdot)$ consists of some learned Embedding followed by   \n1181 either a one-layer GRU network [16] in the case of the RL agent, or a one-layer LSTM network [29]   \n1182 in the case of the RG agents. In the context of the listener agent, the input message $m=(m_{i})_{i\\in[1,L]}$   \n1183 (produced by the speaker agent) is represented as a string of one-hot encoded vectors of dimension   \n1184 $\\vert V\\vert$ and embedded in an embedding space of dimension 64 via a learned Embedding. The output   \n1185 of the listener agent\u2019s language module, $g^{l}(\\cdot)$ , is the last hidden state of the RNN layer, $h_{L}^{l}\\,=$   \n1186 $g^{L}(m_{L},h_{L-1}^{l})$ . In the context of the speaker agent\u2019s language module $g^{S}(\\cdot)$ , the output is the   \n1187 message $m=(m_{i})_{i\\in[1,L]}$ consisting of one-hot encoded vectors of dimension $|V|$ , which are sampled   \n1188 using the STGS approach from a categorical distribution $C a t(p_{i})$ where $p_{i}\\,=\\,S o f t m a x(\\nu(h_{i}^{s}))$ ,   \n1189 provided $\\nu$ is an affine transformation and $h_{i}^{s}=g^{s}(m_{i-1},h_{i-1}^{s})$ . $h_{0}^{s}=f(s_{t})$ is the output of the   \n1190 visual module, given the target stimulus $s_{t}$ .   \n1191 Decision Module. From the RL agent to the RG\u2019s listener agent, the decision module are very   \n1192 different since their outputs are either, respectively, in the action space $\\boldsymbol{\\mathcal{A}}$ or the space of distributions   \n1193 over $K+1$ stimuli (i.e. discriminating between distractors and target stimuli). For the RL agent, the   \n1194 decision module takes as input a concatenated vector comprising the output of visual module, after   \n1195 it has been procesed by a 3-layer fully-connected network with 256, 128 and 64 hidden units with   \n1196 ReLU non-linear activation functions, and some other information relevant to the RL context (e.g.   \n1197 previous reward and previous action selected, following the recipe in Kapturowski et al. [34]). The   \n1198 resulting concatenated vector is then fed to the core memory module, a one-layer LSTM network [29]   \n1199 with 1024 hidden units, which feeds into the advantage and value heads of a 1-layer dueling network   \n1200 [64].   \n1201 In the case of the RG\u2019s listener agent, similarly to Havrylov and Titov [25], the decision module   \n1202 builds a probability distribution over a set of $K+1$ stimuli/images $\\left(s_{0},...,s_{K}\\right)$ , consisting of $K$   \n1203 distractor stimuli and the target stimulus, provided in a random order, given a message $m$ using the   \n1204 scalar product: ", "page_idx": 27}, {"type": "image", "img_path": "UqvAFl0lkT/tmp/2caab03383ddff3a6a951bfe20a4abc57528fddb748c04dd20d570c003c04f23.jpg", "img_caption": ["Figure 13: Relative expressivity of the EL as a function of the per-episode coverage of the RL agent, at the end of training, over multiple runs with different hyperparameters during a W&B Sweep [4]. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "equation", "text": "$$\np((d_{i})_{i\\in[0,K]}|(s_{i})_{i\\in[0,K]};m)=S o f t m a x\\biggl((h_{L}^{l}\\cdot f(s_{i})^{T})_{i\\in[0,K]}\\biggr).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1205 Regarding optimization of the RL agent, table 1 highlights the hyperparameters used for the off-policy   \n1206 RL algorithm, R2D2[34]. More details can be found, for reproducibility purposes, in our open-source   \n1207 implementation at HIDDEN-FOR-REVIEW-PURPOSES.   \n1208 Each run can be done on less than 2Gb of VRAM, and the amount of training time for a run, with e.g.   \n1209 one NVIDIA GTX1080 Ti, is between 24 and 48 hours depending on the architecture (e.g. shared or   \n1210 agnostic). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "table", "img_path": "UqvAFl0lkT/tmp/c4cc9f14a4e8f6e183c39e3ab8496e7878c47cf043821153d19ea4700dc4a6a2.jpg", "table_caption": ["Table 1: Hyper-parameter values relevant to R2D2 in the EReLELA architecture presented. All missing parameters follow the ones in Ape-X [30]. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "1211 G On the Referential Game in EReLELA ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1212 We follow the nomenclature proposed in Denamgana\u00ef and Walker [20] and focus on a descrip  \n1213 tive object-centric (partially-observable) 2-players $\\mathcal{L}=10$ -signal $\\ N=0$ -round/ $\\mathcal{K}$ -distractor RG   \n1214 variant.   \n1215 The descriptiveness implies that the target stimulus may not be passed to the listener agent, but   \n1216 instead replaced with a descriptive distractor. In effect, the listener agent\u2019s decision module therefore   \n1217 outputs a $K+2$ -logit distribution where the $K+2$ -th logit represents the meaning/prediction that a   \n1218 descriptive distractor has been introduced and none of the $K+1$ stimuli is the target stimulus that   \n1219 the speaker agent was \u2018talking\u2019 about. The addition is made following Denamgana\u00ef et al. [18] as a   \n1220 learnable logit value, $l o g i t_{n o-t a r g e t}$ , it is an extra parameter of the model. In this case the decision   \n1221 module output is no longer as specified in Equation 6, but rather as follows: ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "equation", "text": "$$\np\\big(\\big(d_{i}\\big)_{i\\in[0,K+1]}\\big|\\big(s_{i}\\big)_{i\\in[0,K]};m\\big)=S o f t m a x\\Big(\\big(h_{L}^{l}\\cdot f\\big(s_{i}\\big)^{T}\\big)_{i\\in[0,K]}\\cup\\big\\{l o g i t_{n o-t a r g e t}\\big\\}\\Big).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "1222 The descriptiveneness is ideal but not necessary in order to employ the listener agent as a predicate   \n1223 function for the hindsight experience replay scheme. Thus, in the main results of the paper, we   \n1224 present the version without descriptiveness.   \n1225 The object-centrism is achieved via application of data augmentation schemes before feeding stimuli   \n1226 to any RG agent, following Dessi et al. [22] but using Gaussian Blur transformation alone, as it was   \n1227 found sufficient in practice. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "1228 We optimize the RG agents with either the Impatient-Only STGS loss and the STGS-LazImpa loss. ", "page_idx": 30}, {"type": "text", "text": "1229 In the remainder of this section, we detail the STGS-LazImpa loss that we employed to optimize the   \n1230 referential game agents. ", "page_idx": 30}, {"type": "text", "text": "1231 G.1 STGS-LazImpa Loss ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1232 Emergent languages rarely bears the core properties of natural languages [40, 6, 43, 12], such as   \n1233 Zipf\u2019s law of Abbreviation (ZLA). In the context of natural languages, this is an empirical law which   \n1234 states that the more frequent a word is, the shorter it tends to be [66, 60]. Rita et al. [56] proposed   \n1235 LazImpa in order to make emergent languages follow ZLA.   \n1236 To do so, Lazimpa adds to the speaker and listener agents some constraints to make the speaker   \n1237 lazy and the listener impatient. Thus, denoting those constraints as $\\mathcal{L}_{S T G S-l a z y}$ and $\\mathcal{L}_{i m p a t i e n t}$ , we   \n1238 obtain the STGS-LazImpa loss as follows: ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S T G S-L a z I m p a}(m,(s_{i})_{i\\in[0,K]})=\\mathcal{L}_{S T G S-l a z y}(m)+\\mathcal{L}_{i m p a t i e n t}(m,(s_{i})_{i\\in[0,K]}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "1239 In the following, we detail those two constraints. ", "page_idx": 31}, {"type": "text", "text": "1240 Lazy Speaker. The Lazy Speaker agent has the same architecture as common speakers. The   \n1241 \u2018Laziness\u2019 is originally implemented as a cost on the length of the message $m$ directly applied to the   \n1242 loss, of the following form: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{L}_{l a z y}(m)=\\alpha(a c c)\\cdot|m|\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "1243 where acc represents the current accuracy estimates of the referential games being played, and $\\alpha$   \n1244 is a scheduling function as follows: \u03b1 : accuracy \u2208[0, 1]  \u2192 accur\u03b2a2cy\u03b21, with (\u03b21, \u03b22) = (45, 10).   \n1245 It is aimed to adaptively penalize depending on the message length. Since the lazyness loss is   \n1246 not differentiable, they ought to employ a REINFORCE-based algorithm for the purpose of credit   \n1247 assignement of the speaker agent.   \n1248 In this work, we use the STGS communication channel, which has been shown to be more sample  \n1249 efficient than REINFORCE-based algorithms [25], but it requires the loss functions to be differen  \n1250 tiable. Therefore, we modify the lazyness loss by taking inspiration from the variational autoencoders   \n1251 (VAE) literature [37].   \n1252 The length of the speaker\u2019s message is controlled by the appearance of the EoS token, wherever   \n1253 it appears during the message generation process that is where the message is complete and its   \n1254 length is fixed. Symbols of the message at each position are sampled from a distribution over all   \n1255 the tokens in the vocabulary that the listener agent outputs. Let $(W_{l})$ be this distribution over all   \n1256 tokens $w\\in V$ at position $l\\overset{\\cdot}{\\in}[1,L]$ , such that $\\bar{\\forall l}\\in[1,\\bar{L}]$ , $m_{l}\\sim(W_{l})$ . We devise the lazyness loss   \n1257 as a Kullbach-Leibler divergence $\\dot{D}_{K L}(\\cdot|\\cdot)$ between these distribution and the distribution $(W_{E o S})$   \n1258 which attributes all its weight on the EoS token. Thus, we dissuade the listener agent from outputting   \n1259 distributions over tokens that deviate too much from the EoS-focused distribution $(W_{E o S})$ , at each   \n1260 position $l$ with varying coefficients $\\beta(l)$ . The coefficient function $\\beta:\\left[1,L\\right]\\rightarrow\\mathbb{R}$ must be monotically   \n1261 increasing. We obtain our STGS-lazyness loss as follows: ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S T G S-l a z y}(m)=\\alpha(a c c)\\cdot\\sum_{l\\in[1,L]}\\beta(l)D_{K L}\\Big((W_{E o S})|(W_{l})\\Big)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "1262 Impatient Listener. Our implementation of the Impatient Listener agent follows the original work   \n1263 of Rita et al. [56]: it is designed to guess the target stimulus as soon as possible, rather than solely   \n1264 upon reading the EoS token at the end of the speaker\u2019s message $m$ . Thus, following Equation 6, the   \n1265 Impatient Listener agent outputs a probability distribution over a set of $K+1$ stimuli $\\left(s_{0},...,s_{K}\\right)$ for   \n1266 all sub-parts/prefixes of the message $m=({\\dot{m}}_{1},...,m_{l})_{l\\in[1,L]}=(m_{\\le l})_{l\\in[1,L]}:$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\forall l\\in[1,L],\\;\\;p((\\mathbf{d}_{\\mathbf{i}}^{\\leq1})_{\\mathbf{i}\\in[\\mathbf{0},\\mathbf{K}]}|(s_{i})_{i\\in[0,K]};\\mathbf{m}^{\\leq1})=S o f t m a x\\Big((\\mathbf{h}_{\\leq1}\\cdot f(s_{i})^{T})_{i\\in[0,K]}\\Big),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "1267 where $\\mathbf{h}_{\\leq1}$ is the hidden state/output of the recurrent network in the language module after consuming   \n1268 tokens of the message from position 1 to position $l$ included.   \n1269 Thus, we obtain a sequence of $L$ probability distributions, which can each be contrasted, using the   \n1270 loss of the user\u2019s choice, against the target distribution $(D_{t a r g e t})$ attributing all its weights on the   \n1271 decision $d_{t a r g e t}$ where the target stimulus was presented to the listener agent. Here, we employ   \n1272 Havrylov and Titov [25]\u2019s Hinge loss. Denoting it as $\\mathbb{L}(\\cdot)$ , we obtain the impatient loss as follows: ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i m p a t i e n t/\\mathbb{L}}(m,(s_{i})_{i\\in[0,K]})=\\frac{1}{L}\\sum_{l\\in[1,L]}\\mathbb{L}((d_{i\\in[0,K]}^{\\leq l},(D_{t a r g e t})).\n$$", "text_format": "latex", "page_idx": 31}]