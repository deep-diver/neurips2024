[{"figure_path": "eQ6VjBhevn/tables/tables_5_1.jpg", "caption": "Table 1: Natural Distribution Shifts. TTA methods are grouped according to the baseline model and top-1 accuracy is reported. Bold text is the best method within each group.", "description": "This table presents the results of the proposed ZERO method and three other Test-Time Adaptation (TTA) methods on various datasets representing natural distribution shifts.  The methods are grouped by the baseline vision-language model used (CLIP-ViT-B-16, MaPLe, and CLIP-ViT-B-16 + CLIP-ViT-L-14). Top-1 accuracy is reported for each dataset and method, showing how well each method adapts to these challenging, out-of-distribution datasets. The best performing method within each group is highlighted in bold.", "section": "4.2 Results"}, {"figure_path": "eQ6VjBhevn/tables/tables_6_1.jpg", "caption": "Table 1: Natural Distribution Shifts. TTA methods are grouped according to the baseline model and top-1 accuracy is reported. Bold text is the best method within each group.", "description": "This table presents the results of the Test-Time Adaptation (TTA) methods on datasets representing natural distribution shifts.  The table is organized by the Vision-Language Model (VLM) used as the base model (CLIP-ViT-B-16, MaPLe, and CLIP-ViT-B-16 + CLIP-ViT-L-14). Within each group, different TTA methods are compared (Zero-Shot, Ensemble, TPT, ZERO, ZERO+Ensemble, MaPLe, PromptAlign, RLCF).  The top-1 accuracy is reported for each method and dataset (ImageNet-A, ImageNet-V2, ImageNet-R, ImageNet-Sketch). The best performing method within each group is highlighted in bold.", "section": "4.2 Results"}, {"figure_path": "eQ6VjBhevn/tables/tables_8_1.jpg", "caption": "Table 1: Natural Distribution Shifts. TTA methods are grouped according to the baseline model and top-1 accuracy is reported. Bold text is the best method within each group.", "description": "This table presents the results of the Test-Time Adaptation (TTA) methods on various datasets representing natural distribution shifts.  The table is organized by the Vision-Language Model (VLM) used as a baseline for each TTA method (CLIP-ViT-B-16, MaPLe, CLIP-ViT-B-16 + CLIP-ViT-L-14).  For each VLM, several TTA methods are evaluated (Zero-Shot, Ensemble, TPT, ZERO, ZERO+Ensemble, MaPLe, PromptAlign, RLCF variants).  The top-1 accuracy is reported for each method and dataset.  Bold text indicates the best performing method within each group of similar baseline models. The datasets used include ImageNet-A, ImageNet-V2, ImageNet-R, and ImageNet-Sketch.  The table helps quantify the performance improvement achieved by different TTA approaches over a standard zero-shot baseline and allows for comparison among various TTA techniques.", "section": "4.2 Results"}, {"figure_path": "eQ6VjBhevn/tables/tables_8_2.jpg", "caption": "Table 3: Computational requirements of different TTA methods.", "description": "This table compares the computational requirements (runtime and memory usage) of different test-time adaptation (TTA) methods for vision-language models.  It shows that the proposed ZERO method is significantly faster and more memory-efficient than existing methods like Test-Time Prompt Tuning (TPT) and Reinforcement Learning from CLIP Feedback (RLCF).", "section": "4 Experiments"}, {"figure_path": "eQ6VjBhevn/tables/tables_15_1.jpg", "caption": "Table 4: Empirical evidence supporting Proposition 2.1.", "description": "This table presents empirical evidence supporting Proposition 2.1, which states that the prediction of the marginal probability distribution (p) is invariant to Marginal Entropy Minimization (MEM). The table shows the percentage of times that the arg max of the pre-TTA marginal probability distribution (pinit) equals the arg max of the post-TTA marginal probability distribution (pend) across five datasets: ImageNet-1k, ImageNet-A, ImageNet-v2, ImageNet-R, and ImageNet-Sketch. The results show that, in most cases, the prediction of p remains unchanged after MEM.", "section": "2 Understanding Marginal Entropy Minimization"}, {"figure_path": "eQ6VjBhevn/tables/tables_16_1.jpg", "caption": "Table 5: Results on Natural Distribution Shifts when adapting CLIP-ViT-B-16 pretrained on the 2B English subset of LAION-5B. Top-1 accuracy is reported, and bold text indicates the best performer.", "description": "This table presents the results of the Test-Time Adaptation (TTA) methods on five datasets representing natural distribution shifts.  The methods compared include Zero-Shot, Ensemble, Test-Time Prompt Tuning (TPT), ZERO, and ZERO+Ensemble.  The results highlight the performance of each method compared to a standard zero-shot baseline and the improvement achieved by ZERO, especially when using an ensemble of prompts.", "section": "4.2 Results"}, {"figure_path": "eQ6VjBhevn/tables/tables_16_2.jpg", "caption": "Table 6: Results on Natural Distribution Shifts when adapting OpenAI's CLIP-ViT-B-16, with Coop-learned prompts. Top-1 accuracy is reported, and bold text indicates the best performer.", "description": "This table presents the results of Test-Time Adaptation (TTA) experiments using OpenAI's CLIP-ViT-B-16 model with prompts learned using Context Optimization (Coop).  The performance of three TTA methods (Zero-Shot, TPT, and ZERO) is evaluated on five datasets representing natural distribution shifts (ImageNet, ImageNet-A, ImageNet-V2, ImageNet-R, and ImageNet-Sketch). The table shows the top-1 accuracy achieved by each method on each dataset, highlighting the best performing method in bold.", "section": "4.2 Results"}, {"figure_path": "eQ6VjBhevn/tables/tables_16_3.jpg", "caption": "Table 7: Fine-grained Classification with CLIP-ViT-B-16 pretrained on the 2B English subset of LAION-5B. Top-1 accuracy is reported, and bold text indicates the best performer.", "description": "This table presents the results of fine-grained image classification experiments using the CLIP-ViT-B-16 model pretrained on a large language dataset.  Multiple test-time adaptation (TTA) methods are compared against a zero-shot baseline.  The table shows the top-1 accuracy achieved by each method on 10 different datasets, highlighting the best-performing method for each dataset.", "section": "4.2 Results"}, {"figure_path": "eQ6VjBhevn/tables/tables_17_1.jpg", "caption": "Table 1: Natural Distribution Shifts. TTA methods are grouped according to the baseline model and top-1 accuracy is reported. Bold text is the best method within each group.", "description": "This table presents the results of applying several Test-Time Adaptation (TTA) methods to vision-language models (VLMs) on datasets designed to evaluate robustness against natural distribution shifts. The methods are grouped by the baseline VLM used, and top-1 accuracy is reported.  The table allows for a comparison of the performance of different TTA techniques across various datasets and baselines, highlighting the best performing method within each group.", "section": "4.2 Results"}, {"figure_path": "eQ6VjBhevn/tables/tables_17_2.jpg", "caption": "Table 1: Natural Distribution Shifts. TTA methods are grouped according to the baseline model and top-1 accuracy is reported. Bold text is the best method within each group.", "description": "This table presents the results of the experiment on natural distribution shifts for various test-time adaptation (TTA) methods. The methods are grouped by the baseline vision-language model used. For each group, the table shows the top-1 accuracy for each dataset (ImageNet-A, ImageNet-V2, ImageNet-R, ImageNet-Sketch), and the average accuracy across datasets.  The best performing method in each group is highlighted in bold.  The table provides a comparison of different TTA strategies in handling challenging, out-of-distribution data.", "section": "4.2 Results"}, {"figure_path": "eQ6VjBhevn/tables/tables_22_1.jpg", "caption": "Table 10: Comparison among \u2460 CLIP\u2019s zero-shot accuracy, \u2461 CLIP\u2019s accuracy on the augmented counterpart of the dataset, and \u2462 ZERO. The augmented datasets are crafted following the protocol of Section 3.1. \u201cGap\u201d is defined as CLIP\u2019s zero shot accuracy minus its accuracy on the augmented dataset. \u201cImprovement\u201d is defined as the accuracy of ZERO minus that of zero-shot CLIP. Spearman\u2019s coefficient between \u201cGap\u201d and \u201cImprovement\u201d equals -0.95: as the \u201cGap\u201d decreases (i.e., the lower the error on augmented views) ZERO provides more substantial improvements.", "description": "This table compares the zero-shot accuracy of CLIP, its accuracy on augmented datasets (created using the method described in Section 3.1), and the accuracy of ZERO (with a percentile of 0.1) on 10 fine-grained classification datasets.  It also calculates the difference in accuracy between zero-shot CLIP and the augmented version (Gap) and the improvement achieved by ZERO compared to zero-shot CLIP (Improvement). A negative Spearman's correlation (-0.95) between Gap and Improvement suggests that when the negative impact of augmentations is smaller, ZERO provides greater improvements. This implies that ZERO's effectiveness is enhanced when the quality of augmented views is higher.", "section": "4.2 Results"}, {"figure_path": "eQ6VjBhevn/tables/tables_23_1.jpg", "caption": "Table 11: Standard deviations of ZERO for Fine-grained classification. Each cell refers to Tab. 2.", "description": "This table shows the standard deviations for the ZERO method's top-1 accuracy results on 10 fine-grained classification datasets.  The standard deviations are broken down by model (CLIP-ViT-B-16, MaPLe, CLIP-ViT-B-16 + CLIP-ViT-L-14) and variant (ZERO, ZERO+Ensemble).  Each value represents the standard deviation calculated across three separate runs for each dataset.", "section": "4.2 Results"}]