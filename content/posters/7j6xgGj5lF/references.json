{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This paper is foundational for Vision Transformers (ViT), a core subject of the current paper, introducing the use of transformers in image recognition."}, {"fullname_first_author": "Kaiming He", "paper_title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "publication_date": "2015-00-00", "reason": "This paper is highly influential in deep learning, introducing ReLU which is relevant to the activation functions used in this paper's models."}, {"fullname_first_author": "Xavier Glorot", "paper_title": "Understanding the difficulty of training deep feedforward neural networks", "publication_date": "2010-00-00", "reason": "This paper discusses the challenges of training deep networks and is important because weight initialization is a crucial aspect of addressing these challenges."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-00-00", "reason": "This paper introduces Masked Autoencoders (MAE), a self-supervised learning technique relevant to the paper's approach of learning a compact module from a pre-trained model."}, {"fullname_first_author": "Zizheng Pan", "paper_title": "Fast vision transformers with hilo attention", "publication_date": "2022-00-00", "reason": "This paper proposes improvements on ViTs, focusing on computational efficiency, a factor considered in the current paper."}]}