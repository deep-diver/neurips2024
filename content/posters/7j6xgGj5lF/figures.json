[{"figure_path": "7j6xgGj5lF/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Learngene paradigm. (b) Manually-crafted and depth-only transformation. (c) Learnable transformation along both width and depth dimension.", "description": "This figure illustrates three different approaches to building variable-sized vision transformers using the Learngene framework. (a) shows the basic Learngene approach where a compact learngene module is learned from a large model and then used to initialize variable-sized descendant models. (b) shows previous methods that used manually designed and depth-only transformations to adapt the learngene to different sizes. Finally, (c) presents the proposed LeTs method, which uses learnable transformations along both width and depth dimensions for more flexible and efficient variable-sized model initialization.", "section": "1 Introduction"}, {"figure_path": "7j6xgGj5lF/figures/figures_3_1.jpg", "caption": "Figure 2: In stage 1, we construct and train an Aux-Net which is transformed from compact learngene layers using a series of learnable transformation matrices. During training, F and G learn to capture structural knowledge about how to add new neurons and layers into the compact learngene respectively. In stage 2, given the varying sizes of target Des-Nets, we select specific parameters from well-trained transformation matrices to transform learngene for initialization, which are fine-tuned lastly under different downstream scenarios.", "description": "This figure illustrates the two-stage process of LeTs. Stage 1 involves constructing and training an auxiliary model (Aux-Net) using learngene and learnable transformation matrices (F and G) to learn how to add neurons and layers. Stage 2 uses these trained matrices to adapt learngene for initializing variable-sized descendant models (Des-Nets) based on specific parameter selection strategies for different downstream tasks, followed by fine-tuning.", "section": "3 Proposed Approach"}, {"figure_path": "7j6xgGj5lF/figures/figures_4_1.jpg", "caption": "Figure 1: (a) Learngene paradigm. (b) Manually-crafted and depth-only transformation. (c) Learnable transformation along both width and depth dimension.", "description": "This figure illustrates the three different approaches for transforming a compact learngene module to initialize variable-sized descendant models. (a) shows the basic Learngene paradigm, where a compact learngene is extracted from a large model and then transformed. (b) depicts previous methods that relied on manually designed and depth-only transformations, limiting flexibility. (c) presents the proposed LeTs method, which uses learnable transformations along both width and depth dimensions for greater flexibility and effectiveness in initializing variable-sized models.", "section": "1 Introduction"}, {"figure_path": "7j6xgGj5lF/figures/figures_6_1.jpg", "caption": "Figure 4: Performance comparisons on ImageNet-1K. Number in bracket represents Params(M).", "description": "This figure compares the performance of LeTs against other methods (Scratch, TLEG, and SWS) on ImageNet-1K for various model sizes (indicated by the number of parameters in millions).  The plots show the Top-1 accuracy over epochs.  The results show that LeTs consistently achieves higher accuracy in fewer epochs compared to training from scratch, highlighting its superior performance and efficiency for initializing variable-sized vision transformer models.", "section": "4.2 Main Results"}, {"figure_path": "7j6xgGj5lF/figures/figures_6_2.jpg", "caption": "Figure 4: Performance comparisons on ImageNet-1K. Number in bracket represents Params(M).", "description": "This figure compares the performance of LeTs against other methods (Scratch, TLEG, and SWS) on ImageNet-1K.  Multiple models of varying sizes (indicated by Params(M)) are evaluated.  The graphs show the Top-1 accuracy over epochs, illustrating LeTs' superior performance and efficiency, often achieving better results with significantly fewer training epochs than the other methods.", "section": "4.2 Main Results"}, {"figure_path": "7j6xgGj5lF/figures/figures_6_3.jpg", "caption": "Figure 4: Performance comparisons on ImageNet-1K. Number in bracket represents Params(M).", "description": "This figure compares the performance of LeTs with other methods (Scratch, TLEG, SWS) on ImageNet-1K.  The x-axis represents the number of training epochs, and the y-axis represents the Top-1 accuracy. Multiple subplots show results for different model sizes (indicated by the parameter count in parentheses).  LeTs consistently outperforms other methods after a small number of epochs, demonstrating its efficiency and improved model initialization.", "section": "4.2 Main Results"}, {"figure_path": "7j6xgGj5lF/figures/figures_7_1.jpg", "caption": "Figure 5: LeTs could flexibly initialize variable-sized models that are independent of the size of learngene and Aux-Net. Compared with Scratch and MatFormer [40], LeTs demonstrates more initialization efficiency.", "description": "This figure compares the performance of LeTs, Scratch (training from scratch), and MatFormer in terms of ImageNet-1K Top-1 accuracy against the number of model parameters (in millions).  LeTs consistently achieves higher accuracy with fewer parameters than both Scratch and MatFormer, highlighting its efficiency in initializing variable-sized models. The figure also indicates the parameter counts for the learngene (15.0M) and the auxiliary model (Aux-Net, 37.7M) used in the LeTs method.", "section": "4.2 Main Results"}, {"figure_path": "7j6xgGj5lF/figures/figures_7_2.jpg", "caption": "Figure 4: Performance comparisons on ImageNet-1K. Number in bracket represents Params(M).", "description": "This figure compares the performance of LeTs with other initialization methods (Scratch, TLEG, and SWS) on ImageNet-1K.  The x-axis represents the training epochs, and the y-axis represents the Top-1 accuracy.  Multiple lines are shown, each representing a different sized Vision Transformer (Des-Net) model, indicated by the number in the brackets which represents the number of parameters (in millions).  The figure demonstrates that LeTs consistently outperforms other methods, especially after just one epoch of fine-tuning.", "section": "4.2 Main Results"}, {"figure_path": "7j6xgGj5lF/figures/figures_17_1.jpg", "caption": "Figure 4: Performance comparisons on ImageNet-1K. Number in bracket represents Params(M).", "description": "This figure presents the performance comparison on ImageNet-1K among different methods: Scratch (training from scratch for 100 epochs), TLEG, SWS, and LeTs (proposed method).  The x-axis represents the number of training epochs, while the y-axis shows the Top-1 accuracy.  Multiple subplots show the results for various model sizes (indicated by the parameter count in parentheses). The results demonstrate that LeTs significantly outperforms other methods, particularly after only one epoch of fine-tuning. The superior performance of LeTs highlights its effectiveness in producing high-quality initializations for various model sizes.", "section": "4.2 Main Results"}]