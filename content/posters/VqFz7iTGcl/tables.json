[{"figure_path": "VqFz7iTGcl/tables/tables_5_1.jpg", "caption": "Table 3: Summary of the evaluated embedders with their performance on the MTEB benchmark.", "description": "This table presents a summary of the performance of 34 different embedding models on the Massive Text Embedding Benchmark (MTEB).  The table includes the name of each model, its dimensionality, the maximum number of tokens it processes, and its information sufficiency score (Is). In addition, it shows the model's average performance across six categories of downstream tasks within MTEB: Classification, Clustering, Reranking, Retrieval, STS, and Classification 2.  This table provides a comprehensive overview of the models' performance across various tasks and their information sufficiency, aiding in the selection of suitable models for specific downstream applications.", "section": "5 Text Embeddings Evaluation"}, {"figure_path": "VqFz7iTGcl/tables/tables_5_2.jpg", "caption": "Table 3: Summary of the evaluated embedders with their performance on the MTEB benchmark.", "description": "This table presents a summary of the performance of 34 different embedding models on the Massive Text Embedding Benchmark (MTEB).  The table includes the model name, and average scores across six task categories within the MTEB: Classification, Clustering, Reranking, Retrieval, STS, and Reranking.  The table provides a quantitative overview of how each model performed across a variety of downstream NLP tasks.", "section": "5 Text Embeddings Evaluation"}, {"figure_path": "VqFz7iTGcl/tables/tables_24_1.jpg", "caption": "Table 1: Metadata of the evaluated models and their information sufficiency.", "description": "This table provides metadata for 34 embedding models evaluated in the paper, including their dimensionality, maximum number of tokens, and information sufficiency (Is) scores.  The Is score is a key metric developed in this research to evaluate the models' ability to represent information effectively, independent of specific downstream tasks.", "section": "5 Text Embeddings Evaluation"}, {"figure_path": "VqFz7iTGcl/tables/tables_25_1.jpg", "caption": "Table 2: Statistics of the datasets used as umbrella datasets for Is informativeness evaluation.", "description": "This table presents the statistics of the datasets used in the paper to evaluate the information sufficiency (Is) score.  For each dataset, it shows the split (train, validation, test), and the size of each split. The datasets encompass various natural language processing tasks and are diverse in terms of topic, style, and data modality.  These datasets provide a comprehensive testbed for evaluating the performance and generalizability of the embedders.", "section": "C NLP Experiment Details"}, {"figure_path": "VqFz7iTGcl/tables/tables_26_1.jpg", "caption": "Table 3: Summary of the evaluated embedders with their performance on the MTEB benchmark.", "description": "This table presents the performance of 34 different embedding models on the Massive Text Embedding Benchmark (MTEB).  The models are evaluated across six different categories of downstream tasks: Classification, Clustering, Reranking, Retrieval, STS (Semantic Textual Similarity), and a second Classification category. For each model, the table shows the dimension of the embedding, the maximum number of tokens used, and the information sufficiency score (Is).  The table also provides the average performance across all MTEB tasks and the performance within each task category. This allows for a comparison of model performance across various types of NLP tasks and provides context for the information sufficiency scores.", "section": "5 Text Embeddings Evaluation"}, {"figure_path": "VqFz7iTGcl/tables/tables_27_1.jpg", "caption": "Table 3: Summary of the evaluated embedders with their performance on the MTEB benchmark.", "description": "This table presents the performance of 34 different embedding models on the Massive Text Embedding Benchmark (MTEB).  For each model, it shows the dimension of the embeddings, the maximum number of tokens, the information sufficiency score (Is), and the performance on several downstream tasks (classification, clustering, retrieval, STS, reranking). The table summarizes the average performance across different subsets of tasks within each task category, providing a comprehensive overview of each model's capabilities.", "section": "5 Text Embeddings Evaluation"}, {"figure_path": "VqFz7iTGcl/tables/tables_30_1.jpg", "caption": "Table 5: Models evaluated on the ZINC dataset.", "description": "This table lists the 28 models used in the molecular modeling experiments.  For each model, it shows whether it uses SMILES, 2D-GNNs, or 3D-GNNs as input, the architecture used, the output size of the embeddings, and the size of the dataset it was trained on. The table highlights the variety of input modalities and model architectures used to represent molecules in the experiments.", "section": "Molecular Experiment Details"}, {"figure_path": "VqFz7iTGcl/tables/tables_32_1.jpg", "caption": "Table 1: Metadata of the evaluated models and their information sufficiency.", "description": "This table lists the 34 models used in the NLP experiments.  For each model, it provides the dimensionality of the embeddings, the maximum number of tokens, and the information sufficiency score (Is). The table also shows a subset of the downstream tasks used for evaluation in the NLP experiments.", "section": "5 Text Embeddings Evaluation"}, {"figure_path": "VqFz7iTGcl/tables/tables_33_1.jpg", "caption": "Table 6: ADMET tasks extracted from the Therapeutic Data Commons platform [49] considered in our experiments. We report the correlation between the informativeness score and the performances of the embedders on the downstream tasks in terms of Pearson correlation \u03c1p, Spearman correlation \u03c1s and Kendall-Tau \u03c4. We also report the average metric of the models on each task across the grid search runs, in terms of R\u00b2 for regression tasks and AUROC for classification tasks. The tasks are ordered within each category by the correlation with the informativness score (in terms of Spearman correlation).", "description": "This table presents the results of the ADMET tasks (Absorption, Distribution, Metabolism, Excretion, and Toxicity) extracted from the Therapeutic Data Commons platform, focusing on the correlation between the information sufficiency score and the model's performance on these tasks.  It shows Pearson, Spearman, and Kendall correlations, along with average metrics (R\u00b2 for regression and AUROC for classification).  The tasks within each category are ordered by the Spearman correlation with the information sufficiency score.", "section": "D.3 Complementary results on ADMET tasks"}, {"figure_path": "VqFz7iTGcl/tables/tables_35_1.jpg", "caption": "Table 8: Correlation between Is's informativness score and our clustering evaluation score Lnneighb on the four DTI datasets considered.", "description": "This table presents the correlation between the information sufficiency (Is) score and a new clustering-based evaluation metric (Lnneighb) for four different Drug-Target Interaction (DTI) datasets.  The Lnneighb metric assesses the quality of the embeddings by measuring how close the labels of a molecule are to its nearest neighbors.  Different numbers of neighbors (nneighb = 1, 2, 4, 8) were considered to see how this impacted the correlation.  The table shows Pearson (\u03c1p), Spearman (\u03c1s), and Kendall-Tau (\u03c4) correlation coefficients for each dataset and number of neighbors, indicating the strength of the relationship between the Is score and Lnneighb.", "section": "D.4 Drug target Interaction prediction"}, {"figure_path": "VqFz7iTGcl/tables/tables_36_1.jpg", "caption": "Table 1: Metadata of the evaluated models and their information sufficiency.", "description": "This table provides a comprehensive overview of the 34 embedding models evaluated in the NLP experiments.  For each model, it lists the model's name, dimensionality (Dim), the maximum number of tokens used in training (Max Tokens), and the calculated information sufficiency score (Is). The Is score represents the ability of one model to simulate another and serves as a key metric in the proposed evaluation framework. The table also includes references to the model's location on HuggingFace Hub for reproducibility.", "section": "5 Text Embeddings Evaluation"}, {"figure_path": "VqFz7iTGcl/tables/tables_41_1.jpg", "caption": "Table 3: Summary of the evaluated embedders with their performance on the MTEB benchmark.", "description": "This table presents a summary of 34 evaluated embedding models, along with their performance on the Massive Text Embedding Benchmark (MTEB).  The table shows the dimension and maximum number of tokens for each model, and its information sufficiency score (Is).  It also includes the performance of each model on several downstream tasks within the MTEB benchmark, including classification, clustering, retrieval, reranking, and semantic textual similarity (STS) tasks.  The performance is likely represented by a metric like accuracy or another relevant metric for each task.", "section": "5 Text Embeddings Evaluation"}, {"figure_path": "VqFz7iTGcl/tables/tables_41_2.jpg", "caption": "Table 10: Comparison with Baselines: Size of the Embedder, Dimension of the embedding output (d) and the l2 reconstruction error of the embeddings for Molecular Modeling datasets.", "description": "This table compares the performance of the proposed information sufficiency metric (Is) against three baseline methods for evaluating molecular embeddings.  The baselines are: the size of the embedder model, the dimension of the embedding output (d), and the L2 reconstruction error.  The table shows the Spearman (\u03c1s), Pearson (\u03c1p), and Kendall-Tau (\u03c4) correlations between each baseline and the downstream task performance across various subsets of ADMET tasks (Absorption, Distribution, Metabolism, Excretion, Toxicity) and the complete ADMET dataset. The negative correlations for size, dimension, and l2 reconstruction error highlight that those methods are less effective in ranking the embedders than the proposed Is method, which shows strong positive correlations with task performance.", "section": "E.5 Comparison with other metrics"}]