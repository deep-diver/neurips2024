[{"figure_path": "fIz8K4DJ7w/figures/figures_2_1.jpg", "caption": "Figure 1: Comparison of the optimal point in green triangle and the results obtained by diffusion models in white scatters. See details in Appendix B.", "description": "This figure compares the optimal value of a cost function related to a three-dimensional Dirichlet distribution with the results obtained by diffusion models. The optimal value is represented by a green triangle. The results from diffusion models are scattered in white. The discrepancy highlights that diffusion models implicitly promote diversity during imputation, which is counterproductive for precise imputation of missing data. Details are provided in Appendix B.", "section": "3 Motivations"}, {"figure_path": "fIz8K4DJ7w/figures/figures_8_1.jpg", "caption": "Figure 2: Parameter sensitivity of NewImp on bandwidth for kernel function (h), hidden unit of score network HUscore, NER weight \u03bb, and discretization step \u03b7 for Eq. (9) on CC dataset. Mean values and one standard deviation from mean are represented by scatters and shaded area, respectively.", "description": "This figure shows the sensitivity analysis of the NewImp model's performance with respect to four key hyperparameters: bandwidth (h) for the RBF kernel, number of hidden units (HUscore) in the score network, negative entropy regularization weight (\u03bb), and discretization step size (\u03b7) for the ODE simulation.  The results, shown for three scenarios (MAR, MCAR, and MNAR with 30% missing data) on the CC dataset, indicate how each hyperparameter affects both Mean Absolute Error (MAE) and Wasserstein-2 distance (WASS) metrics.  The scatters represent the mean values, and shaded areas represent one standard deviation from the mean.", "section": "5.4 Sensitivity Analysis Results"}, {"figure_path": "fIz8K4DJ7w/figures/figures_30_1.jpg", "caption": "Figure 1: Comparison of the optimal point in green triangle and the results obtained by diffusion models in white scatters. See details in Appendix B.", "description": "This figure compares the results of a hypothetical optimization problem solved analytically with the results obtained using diffusion models.  The goal was to maximize a cost function related to a 3D Dirichlet distribution. The figure shows that the diffusion models' results are close to but not exactly at the optimal value. This suggests that the diffusion models might have implicit terms that encourage diversity in the results, hindering precise imputation for the Missing Data Imputation task.", "section": "3 Motivations"}, {"figure_path": "fIz8K4DJ7w/figures/figures_33_1.jpg", "caption": "Figure 2: Parameter sensitivity of NewImp on bandwidth for kernel function (h), hidden unit of score network HUscore, NER weight \u03bb, and discretization step \u03b7 for Eq. (9) on CC dataset. Mean values and one standard deviation from mean are represented by scatters and shaded area, respectively.", "description": "This figure shows the sensitivity analysis of the NewImp model with respect to four hyperparameters: bandwidth (h), hidden units (HUscore), NER weight (\u03bb), and discretization step (\u03b7). For each hyperparameter, the mean MAE and WASS values, along with their standard deviations, are plotted for MAR, MCAR, and MNAR scenarios on CC dataset. The results demonstrate the impact of each hyperparameter on the model's performance.", "section": "5.4 Sensitivity Analysis Results"}, {"figure_path": "fIz8K4DJ7w/figures/figures_33_2.jpg", "caption": "Figure 2: Parameter sensitivity of NewImp on bandwidth for kernel function (h), hidden unit of score network HUscore, NER weight \u03bb, and discretization step \u03b7 for Eq. (9) on CC dataset. Mean values and one standard deviation from mean are represented by scatters and shaded area, respectively.", "description": "This figure shows the sensitivity analysis of the NewImp model's performance with respect to four key hyperparameters: bandwidth (h), hidden units (HUscore), NER weight (\u03bb), and discretization step (\u03b7).  Each subplot shows how changes in a single hyperparameter affect both MAE and WASS metrics.  The shaded area represents the standard deviation from the mean, providing an understanding of the variability in performance.", "section": "5.4 Sensitivity Analysis Results"}, {"figure_path": "fIz8K4DJ7w/figures/figures_33_3.jpg", "caption": "Figure 2: Parameter sensitivity of NewImp on bandwidth for kernel function (h), hidden unit of score network HUscore, NER weight \u03bb, and discretization step \u03b7 for Eq. (9) on CC dataset. Mean values and one standard deviation from mean are represented by scatters and shaded area, respectively.", "description": "This figure shows the sensitivity analysis of the NewImp model's performance with respect to four hyperparameters: bandwidth (h) for the RBF kernel, number of hidden units (HUscore) in the score network, negative entropy regularization strength (\u03bb), and discretization step size (\u03b7). Each subplot displays the MAE and WASS metrics for MAR, MCAR, and MNAR missing data scenarios on the CC dataset. The plots show that the model is relatively robust to changes in HUscore and \u03b7, while the performance is sensitive to the choice of bandwidth (h) and NER weight (\u03bb).", "section": "5.4 Sensitivity Analysis Results"}, {"figure_path": "fIz8K4DJ7w/figures/figures_35_1.jpg", "caption": "Figure F.3: Average computation time, where \u2018Estimate\u2019 indicates the \u2018DSM Training Algorithm\u2019 (step 5 of Algorithm 1), and \u2018Impute\u2019 indicates the imputation algorithm (step 7 of Algorithm 1). The scatters and shaded areas indicate the mean and one standard deviation from the mean, respectively.", "description": "This figure shows the average computation time of the proposed NewImp approach. The computation time is divided into two parts: \u2018Estimate\u2019 which represents the time for DSM training (step 5), and \u2018Impute\u2019 which represents the time for imputation (step 7). The x-axis represents the logarithm of the number of samples (N) and the y-axis represents the computation time. The figure shows that the computation time increases as the number of samples increases. The figure also shows that the standard deviation of the computation time increases as the number of samples increases.", "section": "F.4 Time Complexity Analysis"}, {"figure_path": "fIz8K4DJ7w/figures/figures_35_2.jpg", "caption": "Figure F.3: Average computation time, where \u2018Estimate\u2019 indicates the \u2018DSM Training Algorithm\u2019 (step 5 of Algorithm 1), and \u2018Impute\u2019 indicates the imputation algorithm (step 7 of Algorithm 1). The scatters and shaded areas indicate the mean and one standard deviation from the mean, respectively.", "description": "This figure shows the average computation time of the DSM training algorithm (Estimate) and the imputation algorithm (Impute) for different dataset sizes (N) and numbers of features (D), across different missing data mechanisms (MAR, MCAR, MNAR).  The shaded regions represent the standard deviation. It demonstrates the impact of data size and dimensionality on the algorithm's runtime.", "section": "F.4 Time Complexity Analysis"}, {"figure_path": "fIz8K4DJ7w/figures/figures_36_1.jpg", "caption": "Figure F.2: Parameter sensitivity of NewImp on bandwidth for kernel function (h), hidden unit of score network HUscore, NER weight \u03bb, and discretization step \u03b7 for Eq. (9) on CC dataset. Mean values and one standard deviation from mean are represented by scatters and shaded area, respectively.", "description": "This figure shows the sensitivity analysis of the NewImp model's performance with respect to four key hyperparameters: bandwidth (h), number of hidden units (HUscore), regularization strength (\u03bb), and discretization step size (\u03b7). Each subplot displays the impact of varying one hyperparameter while keeping the others constant, showing mean MAE and WASS values across different parameter settings. The results indicate optimal ranges for these hyperparameters, highlighting the trade-off between accuracy and computational efficiency.", "section": "F.3 Empirical Evidence for Selecting RBF Function"}, {"figure_path": "fIz8K4DJ7w/figures/figures_37_1.jpg", "caption": "Figure F.2: Parameter sensitivity of NewImp on bandwidth for kernel function (h), hidden unit of score network HUscore, NER weight \u03bb, and discretization step \u03b7 for Eq. (9) on CC dataset. Mean values and one standard deviation from mean are represented by scatters and shaded area, respectively.", "description": "This figure shows the sensitivity analysis of the NewImp model's performance to four key hyperparameters: bandwidth (h) for the RBF kernel, the number of hidden units (HUscore) in the score network, the negative entropy regularization weight (\u03bb), and the discretization step size (\u03b7) for the ODE simulation.  Each subplot represents a different hyperparameter, with the x-axis showing the hyperparameter value and the y-axis displaying the MAE and WASS metrics. The mean performance and standard deviation are shown as scatters and shaded areas, respectively. The results demonstrate how different hyperparameter settings influence model performance and provide guidance for optimal parameter selection.", "section": "F.3 Empirical Evidence for Selecting RBF Function"}, {"figure_path": "fIz8K4DJ7w/figures/figures_39_1.jpg", "caption": "Figure F.2: Parameter sensitivity of NewImp on bandwidth for kernel function (h), hidden unit of score network HUscore, NER weight \u03bb, and discretization step \u03b7 for Eq. (9) on CC dataset. Mean values and one standard deviation from mean are represented by scatters and shaded area, respectively.", "description": "This figure analyzes the sensitivity of the NewImp model's performance to variations in four key hyperparameters: bandwidth (h) of the RBF kernel, number of hidden units (HUscore) in the score network, negative entropy regularization weight (\u03bb), and discretization step size (\u03b7) used in the ordinary differential equation simulation. For each hyperparameter, a range of values were tested, and the mean and standard deviation of the MAE and WASS metrics were calculated and plotted. This figure shows how these hyperparameters influence model accuracy.  The results demonstrate the importance of selecting appropriate hyperparameter values to balance model complexity and generalization.", "section": "F.3 Empirical Evidence for Selecting RBF Function"}, {"figure_path": "fIz8K4DJ7w/figures/figures_40_1.jpg", "caption": "Figure F.2: Parameter sensitivity of NewImp on bandwidth for kernel function (h), hidden unit of score network HUscore, NER weight \u03bb, and discretization step \u03b7 for Eq. (9) on CC dataset. Mean values and one standard deviation from mean are represented by scatters and shaded area, respectively.", "description": "This figure analyzes the impact of four key hyperparameters in the NewImp approach on the model's performance in handling missing data.  The hyperparameters tested are the bandwidth of the RBF kernel, the number of hidden units in the score network, the weight of the negative entropy regularization term, and the discretization step size for simulating the ordinary differential equation. For each hyperparameter, the figure shows how changes in its value affect imputation accuracy, as measured by the mean absolute error (MAE) and the squared Wasserstein-2 distance (WASS).  The results demonstrate the impact of the chosen parameters and provide guidance for optimal hyperparameter tuning.", "section": "F.3 Empirical Evidence for Selecting RBF Function"}]