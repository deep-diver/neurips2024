[{"figure_path": "KSOkkHm9I7/tables/tables_5_1.jpg", "caption": "Table 1: Superposed Decoding is natural-sounding and has lower average perplexity than Nucleus Sampling, which typically approximates human writing.", "description": "This table presents the average perplexity scores for different decoding methods (Nucleus, Beam/Greedy, N-gram, and Superposed Decoding) across three generated drafts.  It highlights that Superposed Decoding achieves a lower average perplexity than Nucleus Sampling, indicating that it generates text that is more similar to human writing in terms of its statistical properties.  The table demonstrates that Superposed Decoding is competitive with other methods in terms of perplexity, a key metric for evaluating the quality and fluency of generated text.", "section": "4.1 Generation Quality"}, {"figure_path": "KSOkkHm9I7/tables/tables_5_2.jpg", "caption": "Table 1: Superposed Decoding is natural-sounding and has lower average perplexity than Nucleus Sampling, which typically approximates human writing.", "description": "This table presents a comparison of the average perplexity scores achieved by different decoding methods, including Nucleus Sampling, Beam Search/Greedy Decoding, N-gram, and Superposed Decoding.  The perplexity is a measure of how well a language model predicts a given sequence of words, with lower scores indicating better prediction and fluency.  The table shows that Superposed Decoding achieves a lower average perplexity than Nucleus Sampling and is on par with the best draft of the Superposed Decoding method.  This suggests that Superposed Decoding generates more natural-sounding text compared to the baseline method.", "section": "4.1 Generation Quality"}, {"figure_path": "KSOkkHm9I7/tables/tables_7_1.jpg", "caption": "Table 2: Coverage on TriviaQA and Natural Questions in a normalized compute setting comparing vanilla Nucleus Sampling (NS) to the combination of Nucleus Sampling and Superposed Decoding (NSSPDk), where k is the number of Superposed Decoding drafts generated on top of each Nucleus Sampled generation. NSSPDK results in better coverage at nearly every compute budget n.", "description": "This table presents the results of an experiment comparing the coverage of two decoding methods: vanilla Nucleus Sampling (NS) and a combination of Nucleus Sampling and Superposed Decoding (NSSPDK).  The experiment evaluates performance on two question answering datasets, TriviaQA and Natural Questions, across various compute budgets (n).  NSSPDK consistently demonstrates improved coverage compared to NS for different numbers of Superposed Decoding drafts (k) added on top of the Nucleus Sampling drafts.  The results show that combining both methods yields better overall coverage.", "section": "4.5 Inference-Time Scaling"}, {"figure_path": "KSOkkHm9I7/tables/tables_8_1.jpg", "caption": "Table 3: Superposed Decoding generalizes across language models like Mistral 7B as shown here with similar results on coherency, as Llama-2-7B, when evaluated using Llama-2-70B.", "description": "This table presents the average perplexity scores achieved by Nucleus sampling and Superposed decoding on the Mistral 7B language model. The perplexity is a measure of how well the generated text matches human language. Lower perplexity indicates better fluency and coherence.  The table shows that Superposed Decoding, despite producing multiple drafts, achieves comparable average perplexity to Nucleus sampling, and in its best-performing draft, even outperforms Nucleus sampling. This demonstrates the generalizability of the Superposed decoding approach across different language models.", "section": "5.1 Results on Mistral 7B"}, {"figure_path": "KSOkkHm9I7/tables/tables_13_1.jpg", "caption": "Table 1: Superposed Decoding is natural-sounding and has lower average perplexity than Nucleus Sampling, which typically approximates human writing.", "description": "This table presents the average perplexity scores for different decoding methods, including Superposed Decoding, Nucleus Sampling, and Greedy Decoding.  The perplexity metric measures how well a language model predicts a sequence of words. Lower perplexity generally indicates better generation quality, and the table shows that Superposed Decoding achieves comparable or better perplexity to the other methods, particularly when considering the \"best\" draft among multiple generations.  The results suggest Superposed Decoding generates text that is as natural sounding as human writing and achieves comparable quality.", "section": "4.1 Generation Quality"}, {"figure_path": "KSOkkHm9I7/tables/tables_14_1.jpg", "caption": "Table 1: Superposed Decoding is natural-sounding and has lower average perplexity than Nucleus Sampling, which typically approximates human writing.", "description": "This table compares the average perplexity scores achieved by different decoding methods on a dataset of text generation tasks.  The methods compared are Nucleus Sampling, Beam Search/Greedy Decoding, N-gram, and Superposed Decoding.  For each method (except Superposed Decoding), a single draft's perplexity is reported. For Superposed Decoding, perplexities for three individual drafts and the best of the three drafts are provided.  The table shows that Superposed Decoding achieves comparable or better average perplexity than Nucleus Sampling, while significantly outperforming other methods like N-gram.", "section": "4.1 Generation Quality"}, {"figure_path": "KSOkkHm9I7/tables/tables_14_2.jpg", "caption": "Table 5: Standard deviation of Llama-2-7B generation perplexity calculated on OpenWebText test split in Section 4.1.", "description": "This table presents the standard deviation of generation perplexities for different decoding methods, including Nucleus Sampling, Beam Search/Greedy Decoding, N-gram, and Superposed Decoding. The perplexities were calculated on the OpenWebText test split, as detailed in Section 4.1 of the paper.  The data shows the variability in perplexity scores across different drafts generated by each method.", "section": "4.1 Generation Quality"}, {"figure_path": "KSOkkHm9I7/tables/tables_14_3.jpg", "caption": "Table 6: Standard deviation of Mistral 7B generation perplexity calculated on OpenWebText test split in Section 5.1.", "description": "This table presents the standard deviation of perplexity scores for generations produced by Nucleus Sampling and Superposed Decoding on the Mistral 7B language model.  The perplexity values were calculated on the OpenWebText test set, as detailed in Section 5.1 of the paper.  The table shows that Superposed Decoding, while having a lower average perplexity in the main study, exhibits higher standard deviations compared to Nucleus Sampling.  This indicates that Superposed Decoding's generation quality might vary more widely across different inputs compared to Nucleus Sampling.", "section": "Further Analysis and Ablations"}]