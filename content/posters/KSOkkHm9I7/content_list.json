[{"type": "text", "text": "Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ethan Shen\u22c4 Alan Fan\u22c4 Sarah Pratt\u22c4 Jae Sung Park\u22c4 Matthew Wallingford\u22c4Sham Kakade\u2020 Ari Holtzman\u2021 Ranjay Krishna\u22c4 Ali Farhadi\u22c4 Aditya Kusupati\u22c4", "page_idx": 0}, {"type": "text", "text": "\u22c4University of Washington \u2020Harvard University \u2021University of Chicago {ethans03, kusupati}@cs.washington.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many applications today provide users with multiple auto-complete drafts as they type, including GitHub\u2019s code completion, Gmail\u2019s smart compose, and Apple\u2019s messaging auto-suggestions. Under the hood, language models support this by running an autoregressive inference pass to provide a draft. Consequently, providing $k$ drafts to the user requires running an expensive language model $k$ times. To alleviate the computation cost of running $k$ inference passes, we propose Superposed Decoding, a new decoding algorithm that generates $k$ drafts at the computation cost of one autoregressive inference pass. We achieve this by feeding a superposition of the most recent token embeddings from the $k$ drafts as input to the next decoding step of the language model. At every inference step we combine the $k$ drafts with the top- $\\cdot k$ tokens to get $k^{2}$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations. Our experiments show that $k$ drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least $2.44\\times$ faster for $k\\geq3$ . In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can also be combined with other decoding strategies, resulting in universal coverage gains when scaling inference time compute. Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Commercial surveys find that $80\\%$ of e-commerce websites provide autocomplete as a feature [21]. With the proliferation of large language models, autocomplete drafts are now ubiquitous in an even wider range of applications. Examples include short draft suggestions in Gmail Smart Compose [8] and code snippets in GitHub Copilot [7]. These options provide users with the ability to consider different phrasings and increase the likelihood of having at least one suggestion that reflects their intent. While language models (LMs) [34] now power these multiple suggestions, each additional suggestion necessitates another inference pass (batch size $=1$ ), making it computationally expensive. ", "page_idx": 0}, {"type": "text", "text": "Language models use autoregressive inference to generate a plausible sequence of next tokens for a given prefix [40]. The next token generated depends on the prefix and the previously generated tokens. Decoding algorithms like Greedy Decoding, Top- $k$ Sampling [12], Beam Search, and Nucleus Sampling [18] present various ways of obtaining generations during autoregressive inference. For a prefix, Greedy (maximum likelihood) Decoding picks the most probable token at every autoregressive timestep, eventually generating only one auto-completion suggestion. Instead of making a greedy choice at every timestep, we can also sample from the top- $\\cdot k$ most probable tokens to generate the sequence [12]. Also leveraging the top- $\\cdot k$ most probable tokens is Beam Search, which picks the most probable $k$ beams from the $\\bar{k}^{2}$ possible drafts at every timestep. Alternatively, Nucleus Sampling [18], particularly effective at generating natural-sounding text, grows generations by sampling from the collection of the smallest subset of tokens that form a preset probability mass $(p)$ at every timestep. Top- $k$ Sampling, Beam Search, and Nucleus Sampling offer the benefit of generating multiple suggestions. However, this comes at the cost of requiring multiple autoregressive inference passes. ", "page_idx": 0}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/9ce2bef11d3b6f2e9e837ea65fcf42178aacde3d4d6172d63a9de0cc37b342a2.jpg", "img_caption": ["Figure 1: To generate multiple $k$ auto-complete suggestions for a prefix using an LM, the existing decoding methods like Nucleus Sampling need $k$ inference passes. In contrast, Superposed Decoding can generate $k$ suggestions at the cost of a single inference pass while being as coherent and factual. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We introduce Superposed Decoding (SPD) (Figure 1), a decoding algorithm that can generate mutiple $(k)$ high-quality short generations using only a single autoregressive inference pass. At each autoregressive timestep during inference, Superposed Decoding feeds in the superposition (weighted combination) of the embeddings corresponding to the $k$ most recent drafted tokens (Section 3.1). After selecting the top- $k$ output tokens, SPD expands the existing $k$ drafts with them, resulting in $k^{2}$ potential new drafts. Each draft has a probability score assigned to it, which is further smoothed using a combination of various $n$ -gram models $\\mathit{\\Omega}(n\\in[2,6])$ [38, 25]. N-gram interpolation for smoothing helps improve coherency by selecting the most probable and coherent continuations (top- $k$ drafts) for the next autoregressive step (Section 3.2). The n-gram interpolation is computationally inexpensive and allows flexibility to change domains of interest during inference (eg., programming, healthcare, finance, etc.). Superposed Decoding\u2019s effectiveness can be attributed to the apparent linearity of representations in language models [35, 23] which we concurrently discovered (Section 3.3). ", "page_idx": 1}, {"type": "text", "text": "Our experiments demonstrate that Superposed Decoding on Llama-2-7B [41] can generate $k\\geq1$ drafts that are as coherent, in terms of perplexity, as Nucleus Sampling (Section 4.1). However, SPD achieves this with a single inference pass, making it at least $2.44\\times$ faster than any other standard decoding methods for $k\\geq3$ (Section 4.3). The ability to generate multiple drafts at the cost of one also increases the coverage significantly for fact-based evaluation tasks like TriviaQA and Natural questions \u2013 where SPD increases the chance of generating the correct answer by at least $5\\%$ when using 3 drafts (Section 4.2). Through an extensive human evaluation for a wide range of prefixes, we show that SPD generations are as preferred as Nucleus Sampling in direct comparison (1v1) while outperforming by up to $20\\%$ in a compute normalized setting (3v1 and 3v2) (Section 4.4). Finally, we find that combining Superposed Decoding and other decoding strategies (e.g. Nucleus Sampling) results in substantial accuracy improvements when scaling inference compute (Section 4.5). ", "page_idx": 1}, {"type": "text", "text": "Superposed Decoding can help improve user experience by offering significant computational efficiency while maintaining accuracy for various writing tasks that often benefit from multiple short draft suggestions. Additionally, Superposed Decoding is extremely generalizable, works across different language models like Mistral 7B, and is capable of generating long-form content reliably, all while being nearly as diverse in suggestion as Nucleus Sampling (Section 5). ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Decoding algorithms determine how tokens are selected from a language model\u2019s next token distribution. This is typically done greedily with Beam Search or Greedy Decoding, or stochastically with Nucleus Sampling [18] or Top- $k$ Sampling [12]. Greedy Decoding operates by selecting the token with the highest probability at each timestep. However, locally optimal decisions can be globally suboptimal. On the other extreme, an exhaustive exploration of all token combinations is intractable with time complexity $\\Theta(|\\mathcal{V}|^{T})$ , where $\\mathcal{V}$ is the vocabulary size and $T$ the total number of timesteps. A practical compromise is Beam Search, which continually caches the top- $B$ most likely generations, where $B$ is the number of beams or drafts. Thus, Beam Search is guaranteed to find more likely generations than Greedy Decoding while avoiding its drawbacks. ", "page_idx": 2}, {"type": "text", "text": "Alternatively, Nucleus Sampling and Top- $\\cdot k$ Sampling decode probabilistically. To avoid degeneration, Top- $k$ Sampling truncates the probability mass to the $k$ most probable tokens, whereas Nucleus Sampling truncates the probability mass to the smallest possible set of words whose cumulative probability exceeds probability $p$ . Because of Nucleus Sampling\u2019s propensity to produce unique and unrepetitive outputs, it has become the standard [4, 22, 42], though Greedy Decoding is still favored for tasks such as short question answering [41]. ", "page_idx": 2}, {"type": "text", "text": "Generating multiple drafts linearly scales the number of inference passes in these decoding methods. Multi-token prediction [14, 36] addresses this by pre-training an LM having $n$ independent softmax heads that predict $n$ future tokens in parallel. When using multi-token prediction with speculative decoding, exact inference is $3\\times$ faster. In a similar vein, Medusa [5] reduces the number of decoding steps by adding extra decoding heads, using a tree-based attention mechanism to generate candidates while simultaneously verifying them in each decoding step. Through this, Medusa achieves a $2.2\\times$ reduction in inference latency while maintaining generation quality. However, multi-token prediction requires re-training and Medusa requires additional fine-tuning for the extra decoding heads. ", "page_idx": 2}, {"type": "text", "text": "Superposed Decoding (SPD), on the other hand, can be easily integrated out of the box without any additional training on any language model. Further, Superposed Decoding is complementary to other decoding methods like Medusa and multi-token prediction, as well as efficiency techniques like speculative decoding [30, 26, 6], quantization [10, 32, 19], pruning [13, 39, 28], and architectural optimizations [37, 2, 1, 43, 27, 11]. ", "page_idx": 2}, {"type": "text", "text": "3 Superposed Decoding (SPD) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a text input as a prefix, Superposed Decoding uses an autoregressive LM $f_{\\theta}$ to produce $k$ viable completion drafts in one inference pass. ", "page_idx": 2}, {"type": "text", "text": "First Timestep. Let $x$ denote a generated token in the vocabulary $\\nu$ and $M\\,=\\,(x_{1},\\ldots,x_{m})$ represent an initial prefix of $m$ tokens. Our goal is to generate $k$ unique drafts starting from the prefix. The next token distribution at the first timestep $m+1$ is: $p_{\\theta}(x_{m+1}|x_{1},\\dots x_{m})=p_{\\theta}(x_{m+1}|M)$ . ", "page_idx": 2}, {"type": "text", "text": "For the first timestep, each of the $k$ drafts is initialized as the prefix so we use the same next token distribution for all drafts. We grow draft $d_{i}$ by greedily selecting the $i^{\\mathrm{th}}$ most probable token, making: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd_{i}=(M,x_{m+1}^{i})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\boldsymbol{x}_{t}^{i}$ is the token at timestep $t$ for the $i^{\\mathrm{th}}$ draft. We also track each draft\u2019s probability $p_{i}$ as its score, which is initially the probability of its first token $p_{\\theta}(x_{m+1}^{i}|M)$ . Consequently, the set of drafts $D$ after the first timestep is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nD=\\left(\\!\\!\\begin{array}{c}{(M,x_{m+1}^{1})}\\\\ {\\vdots}\\\\ {(M,x_{m+1}^{k})}\\end{array}\\!\\!\\right)\\mathrm{~with~probabilities~}P=\\left(\\!\\!\\begin{array}{c}{p_{\\theta}(x_{m+1}^{1}|M)}\\\\ {\\vdots}\\\\ {p_{\\theta}(x_{m+1}^{k}|M)}\\end{array}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Next Timesteps. As the input to the LM at timestep $t$ , we construct $\\tilde{x}_{t-1}$ , which is a superposition (weighted linear combination) of the embeddings for the most recent token $x_{t-1}$ of the $k$ drafts. This means that $\\tilde{x}_{m+1}$ , the input to the LM at the second timestep $m+2$ , is the superposition of the tokens $x_{m+1}^{i}$ for $i=1,\\ldots,k$ . We use this superposed embedding as a single and accurate approximation for the most recent token across all $k$ drafts at the $(t-1)^{\\mathrm{th}}$ timestep. We run autoregressive inference over $\\tilde{x}_{t}$ once for all $k$ drafts instead of the usual once per draft, allowing us to formulate inference as: ", "page_idx": 2}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/c9e8e359300f6a20d5795dc08876f680e7f421016eb01392ddcf2bb49b83e585.jpg", "img_caption": ["Figure 2: Superposed Decoding relies on feeding a superposed token embedding \u2013 based on the most recent tokens from the current $k$ drafts \u2013 as the input during the auto-regressive inference step. This generates $k^{2}$ new drafts using the existing $k$ drafts and the top- $k$ output tokens at the new timestep. Finally, keep the top- $k$ drafts after filtering with an n-gram interpolation to improve coherency. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(x_{t}|M,\\tilde{x}_{m+1},\\ldots,\\tilde{x}_{t-1})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Because each draft contains its own contextual clues and syntax, we cannot blindly use the distribution of $x_{t}$ for each draft. Instead, we interpolate the superposed distribution $p_{\\theta}(x_{t}|M,\\tilde{x}_{m+1:t-1})$ with a draft-specific next token distribution from an $\\mathbf{n}$ -gram model to get a final distribution $p_{f}(x_{t}^{i}|M,x_{m+1:t-1})$ that is unique to each draft. Next, we rank the draft options by the joint probability of their respective previous draft $p_{i}$ and their selected token. We choose the top $k$ options as drafts for the next timestep and update their probabilities. This gives: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{J}=\\left(\\!\\!\\begin{array}{c}{(M,x_{m+1}^{1},\\ldots,x_{t}^{1})}\\\\ {\\vdots}\\\\ {(M,x_{m+1}^{k},\\ldots,x_{t}^{k})}\\end{array}\\!\\!\\right)\\mathrm{~with~probabilities~}P=\\left(\\!\\!\\begin{array}{c}{p_{\\theta}(x_{m+1}^{1}|M)\\prod_{s=m+2}^{t}p_{f}(x_{s}^{1}|M,x_{m+1:s-1}^{1})}\\\\ {\\vdots}\\\\ {p_{\\theta}(x_{m+1}^{k}|M)\\prod_{s=m+2}^{t}p_{f}(x_{s}^{k}|M,x_{m+1:s-1}^{k}),}\\end{array}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We continue generation until the maximum generation length or stop token is reached. In the following sections, we break down the process in detail. We also present pseudocode in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "3.1 Token Superposition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "During training, language models learn a representation (token embedding) $z\\in\\mathcal{R}^{d}$ for every token $x\\in\\mathcal{V}$ . We leverage this representation at timestep $t$ to construct $\\tilde{x}_{t}$ , weighing the representation for $x_{t-1}^{i}$ by a coefficient $\\gamma_{i}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\tilde{x}}_{t}=\\sum_{i=1}^{k}\\gamma_{i}\\cdot z_{t-1}^{i}{\\mathrm{~where~}}\\sum_{i=1}^{k}\\gamma_{i}=1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The performance of $\\tilde{x}_{t}$ is highly dependent on choosing the appropriate weight for each embedding $z_{t-1}^{i}$ . We find that the best strategy is to set $\\gamma_{i}$ to the normalized probability of draft $i$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma_{i}=\\frac{p_{i}}{\\sum_{j=1}^{k}p_{j}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p_{i}$ is the probability of the $i^{\\mathrm{th}}$ draft, introduced in Section 3. This allows us to directly tie the weight of a token to the likelihood that it will be preserved in future timesteps, reducing drift between the superposed embeddings and the drafts they represent. ", "page_idx": 3}, {"type": "text", "text": "3.2 N-Gram Interpolation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We construct each draft\u2019s $\\mathbf{n}$ -gram distribution $p_{\\mathrm{ngram}}(x_{t}^{i}|M,x_{m+1}^{i},\\dots,x_{t-1}^{i})$ by interpolatively smoothing the next token distributions over a set of $n$ -gram models using weights $\\lambda$ , where $n\\in[2,6]$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\mathtt{n g r a m}}(x_{t}^{i}|M,x_{m+1}^{i},\\ldots,x_{t-1}^{i})=\\sum_{n=2}^{6}\\lambda_{n}\\cdot p_{n\\cdot\\mathrm{gram}}(x_{t}^{i}|M,x_{m+1}^{i},\\ldots,x_{t-1}^{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We base our interpolation weights on weights for RedPajama found by Liu et al. [31], with additional tuning (specifics in Appendix B). However, domain-specific $\\mathbf{n}$ -gram models can easily be swapped in for specific tasks such as code generation [20]. We use the exponential mean of $p_{\\theta}$ and $p_{\\mathrm{{ngram}}}$ as our final distribution $p_{f}$ , where $\\alpha$ is a hyperparameter controlling the impact of the n-gram distribution: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{f}(x_{t}^{\\sharp}|\\mathring{M},x_{m+1:t-1}^{i})=\\mathring{p}_{\\theta}\\big(\\bar{x}_{t}|M,\\tilde{x}_{m+1:t-1}\\big)^{1-\\check{\\alpha}}\\cdot p_{\\mathrm{ngram}}\\!\\big(x_{t}^{i}|M,x_{m+1:t-1}^{i}\\big)^{\\alpha}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This means that when generating, we only consider tokens appearing in both the Superposed Decoding and n-gram distributions. If there is no overlap between the distributions, then we instead calculate ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{f}(x_{t}^{i}|M,x_{m+1:t-1}^{i})=\\delta\\cdot p_{\\theta}(x_{t}|M,\\tilde{x}_{m+1:t-1})^{1-\\alpha}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "without interpolation, where $\\delta$ is a penalty term that disincentivizes selecting an uninterpolated draft for the next timestep. This approach is the backbone of Superposed Decoding (Equation 2) and allows us to create context-aware next-token distributions for each draft with only one inference pass. ", "page_idx": 4}, {"type": "text", "text": "3.3 Superposed Decoding Semantically Approximates Beam Search ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Superposed Decoding relies on the ability of the underlying model to preserve the linear relationship between $\\tilde{x}_{t}$ and its component vectors $z_{t-1}^{i}$ [23]. More formally, if $\\tilde{x}_{t}$ is the input to the LM $f_{\\theta}$ , then $\\begin{array}{r}{f_{\\theta}(\\tilde{x}_{t})=\\sum_{i=1}^{k}\\gamma_{i}\\cdot f_{\\theta}(z_{t-1}^{i})}\\end{array}$ fs ha osuuldp earlpsoo sbeed  treume b $(\\gamma_{i}$ ddienfga ualntsd -dgrraaftm  psromboaobtihliitnyg i ns hSoPulDd) . alAlso lwo nugs  taos $\\mathbf{n}$ generate completions that resemble those from methods such as Beam Search. ", "page_idx": 4}, {"type": "text", "text": "We test this linearity by computing the cosine similarity between a set of superposed embeddings $\\{\\tilde{x}\\}$ and the linear combination of their component embeddings across 20 timesteps on Llama-2-7B. At each timestep, we first use Beam Search to generate tokens for three beams. We then manually input the superposed embedding of the three tokens into a model using Superposed Decoding . Finally, we measure the alignment between $f_{\\theta}(\\tilde{x}_{t})$ and $\\textstyle\\sum_{i=1}^{k}\\gamma_{i}\\cdot f_{\\theta}(z_{t-1}^{i})$ using cosine similarity $\\cos(a,b)$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\cos(f_{\\theta}(\\tilde{x}_{t}),\\sum_{i=1}\\gamma_{i}\\cdot f_{\\theta}(z_{t-1}^{i}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We compute the cosine similarities for ten randomly sampled batches of ten prefixes each from the OpenWebText training split and plot the mean cosine similarities across batches, as well as the standard deviation (Figure 3). We find that Llama-2-7B is sufficiently linear up to 10 timesteps across all layers. However, this linearity is imperfect, and Superposed Decoding and Beam Search eventually diverge over time. Owing to this, we identify 10 timesteps as the optimal generation length. We also show how linearity changes through the layers within each timestep in Appendix H. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We evaluate Superposed Decoding by analyzing generation quality, factuality, and latency. We demonstrate that Superposed Decoding improves over Nucleus Sampling and Greedy Decoding in generation perplexity (Section 4.1), fact-based benchmarks (Section 4.2), and wall-clock time (Section 4.3). We also conduct a user study highlighting that users prefer Superposed Decoding over Nucleus Sampling in a compute-normalized setting (Section 4.4). Finally, we show that Superposed Decoding improves performance while scaling inference compute (Section 4.5). We include example generations of Superposed Decoding in Figure 4, with more in Appendix G.1. ", "page_idx": 4}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/bcce0f099de08db3a094e0f1b639878fb09cb389750ef25472ed39ddc953039d.jpg", "img_caption": ["Figure 3: Llama-2-7B maintains the linear relationship between superposed embeddings and the component token embeddings, with mean cosine similarity $\\ge0.6$ for the first 10 timesteps. "], "img_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "KSOkkHm9I7/tmp/777309223726a25e2b8b873f4ee6531a5cabfcc722346221d0673f908f596896.jpg", "table_caption": ["Figure 4: Qualitative text generations in a compute-normalized setting for Superposed Decoding and Nucleus Sampling with prefixes sampled from OpenWebText. See Appendix G.1 for more. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We implement Superposed Decoding on the base version of Llama-2-7B [42] for the majority of our experiments, running on one A40 GPU. We do not change model weights. For perplexity evaluations, we use Llama-2-70B on eight A40 GPUs. We assume a batch size of one for all experiments. ", "page_idx": 5}, {"type": "text", "text": "For n-gram interpolation, we construct n-gram models using 200,000 documents (roughly 200 million tokens) randomly sampled from the RedPajama dataset, an open-source replication of Llama-2\u2019s training dataset [9]. We represent each n-gram model internally as a frequency table, storing each unique n-gram and its corresponding count. This enables faster lookup and is the basis behind the compute reduction that we offer. Our n-gram models require approximately $14\\:\\mathrm{GB}$ of disk storage overall, split 57 MB, 433 MB, 2.15 GB, 4.7 GB, and $6.8\\:\\mathrm{GB}$ for $n=2$ to 6. While we interpolate up to $n=6$ in this work, we note that in practice the benefits of n-gram interpolation saturate past $n=4$ , suggesting that the number of n-gram models can be lowered to reduce storage (Appendix C). ", "page_idx": 5}, {"type": "text", "text": "4.1 Generation Quality ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We test generation quality on OpenWebText\u2019s test split [15], which consists of 5,000 web-scraped documents. For each document, we use the first 15 tokens as the prefix and generate $k=3$ drafts of 10 tokens with a batch size of 1. We decide to focus on short generations because drafts are typically a short-form task. Before running experiments, we identify the optimal interpolation weight $\\alpha$ and temperature $\\tau$ by optimizing for the lowest average perplexity across three drafts on the validation split. We list the specific hyperparameter values that we use in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "We only evaluate one draft of the baselines in order to match the compute used by Superposed Decoding. We find that while Nucleus Sampling and Greedy Decoding outperform Superposed Decoding on a per-draft basis, the average best perplexity from Superposed Decoding is $\\bar{\\mathbf{5}}\\%$ lower than that of Nucleus Sampling. From the point of view of a user, this means for each prefix, at least one draft can be expected to be on par with Nucleus Sampling and Greedy Decoding. The other drafts all come free of additional compute. In the following Sections 4.2 and 4.4, we show that this diversity is beneficial for both factuality and human preference. ", "page_idx": 5}, {"type": "table", "img_path": "KSOkkHm9I7/tmp/aabedf33c96ffc6435fd10629a382bd4f0038ae5ce20b84a6a9e4c957eec0e3d.jpg", "table_caption": ["Table 1: Superposed Decoding is natural-sounding and has lower average perplexity than Nucleus Sampling, which typically approximates human writing. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2 Fact-Based Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we test the ability of Superposed Decoding to generate not only coherent but also accurate completions. We assess this using exact match precision $(\\mathrm{P}@k)$ for $k=1,2,3$ on TriviaQA [24] and Natural Questions [29], two common benchmarks for short answer generations. We decide not to use multiple choice datasets such as MMLU [17] and OpenBookQA [33] because multiple choice questions are trivial when using multiple drafts, unfairly advantaging Superposed Decoding. ", "page_idx": 5}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/50bbebf65caaed292efc244f137c80d7c1b4e2e3111c948fef49a363ec636dff.jpg", "img_caption": ["Figure 5: Superposed Decoding is as accurate as Greedy Decoding for $\\mathrm{P}@1$ and increases the fact-based coverage using multiple drafts $(\\mathbf{{P}}@2,3)$ on TriviaQA (left) and Natural Questions (right). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In Figure 5, we show that Superposed Decoding outperforms Nucleus Sampling and Beam Search at $\\boldsymbol{\\mathrm{P}@1}$ in a zero-shot setting, with three drafts providing accuracy gains of up to $2.72\\%$ in TriviaQA and $1.69\\%$ in Natural Questions. These results demonstrate that Superposed Decoding substantially increases the likelihood of getting a factually correct generation in addition to one that is coherent. ", "page_idx": 6}, {"type": "text", "text": "4.3 Latency ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Superposed Decoding presents a significant theoretical reduction in latency, but it is important to investigate how well this translates to the real-world. In Figure 6, we show that dictionary lookups are the only additional cost incurred by Superposed Decoding, barring which Superposed Decoding has near-constant compute cost. Even so, the total cost of Superposed Decoding is significantly lower than other decoding methods, with Superposed Decoding $\\mathbf{2.44\\times}$ faster on three drafts and $\\mathbf{3.54\\times}$ faster on eight compared to Nucleus Sampling, the next fastest. ", "page_idx": 6}, {"type": "text", "text": "It is important to note that Superposed Decoding results are obtained using unoptimized code. Our n-gram models are implemented using single-threaded lookup on Python dictionaries, and we do not cache any lookup results. This has an enormous, visible impact. In addition, Liu et al. [31] propose the use of suffix arrays for n-gram models, allowing a near-instantaneous lookup of arbitrary-length n-grams in trillion-token corpora. These techniques open up multiple avenues for additional speedup. ", "page_idx": 6}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/fa47988d8525372c7a4c7ecd2c65fb15da65d08a11463dd3e48c4e5f24733a96.jpg", "img_caption": ["Figure 6: Average wall clock time to generate 10 token long drafts, with batch size $=1$ , from a 15 token prefix on OpenWebText. Superposed Decoding is significantly faster for all $k>1$ , with $\\mathbf{n}$ -gram lookup costs being a major factor for $k\\geq4$ , which can be optimized further. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/a9919e430b64e5e7b6abaca196e453c47bfed0d842fc00a54d7de47d84b39f84.jpg", "img_caption": ["Figure 7: Drafts are ordered by the probability they obtain during generation using SPD, which wins over Nucleus Sampling in a compute-normalized setting. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.4 Human Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While perplexity and factuality are good proxies for textual quality, real human evaluation remains the best measure of coherency. Indeed, perplexity is an imperfect evaluator of long-form language modelling [44] while factuality metrics are inherently biased towards Greedy Decoding methods [41]. Consequently, to verify our findings, we conduct a random study asking human respondents to rank Superposed Decoding and Nucleus Sampling generations based on how well they complete a provided prefix. We compare against Nucleus Sampling because n-gram distributions from Nucleus Sampling are demonstrably the closest to those of humans [18]. ", "page_idx": 7}, {"type": "text", "text": "Setup. We conduct our study using Amazon Mechanical Turk [16]. First, we randomly sample 1,000 prefixes from the OpenWebText test set, truncating to the first 15 tokens as in Section 4.1. Next, we manually remove prefixes that are grammatically incorrect, such as text from website toolbars. From the remaining prefixes, we generate three Superposed Decoding drafts and one Nucleus Sampling draft in a compute-normalized setting, randomizing their order. Finally, we filter out prefixes with duplicate or unparseable generations (e.g. emojis). After preprocessing, 707 prefixes are left. ", "page_idx": 7}, {"type": "text", "text": "It is important to note that one Nucleus Sampling generation is $20\\%$ less expensive than three Superposed Decoding drafts, shown in Section 4.3. However, two Nucleus Sampling drafts disadvantages three Superposed Decoding drafts by $60\\%$ . Therefore, we decide to run our main survey using the first setup, which is closest to equal compute, but also conduct smaller surveys in 2v3 and 1v1 settings. ", "page_idx": 7}, {"type": "text", "text": "Results. We define a Superposed Decoding \u201cwin\u201d as when one of the Superposed Decoding drafts is ranked first. As shown in Figure 7, we find that Superposed Decoding generations are preferred approximately $\\mathbf{63.6\\%}$ of the time . If every Superposed Decoding draft was consistently the same quality as Nucleus Sampling, then we would expect the rankings to be uniform, resulting in a $75\\%$ win rate. However, this is not the case, suggesting that Nucleus Sampling is more reliable than any individual draft, but the aggregate of drafts provides a diversity that is highly valuable to users. ", "page_idx": 7}, {"type": "text", "text": "We run the two smaller-scale iterations of the study with 100 prefixes each. In the 2v3 setting, we ask users to rank two nucleus drafts and three superposed drafts to investigate whether the benefits of Superposed Decoding persist even when compute favors Nucleus Sampling. In the 1v1 setting, users choose between one nucleus draft and the lowest perplexity superposed draft, removing any potential bias caused by unequal numbers of drafts. As shown in Figure 12, in both situations, users still prefer Superposed Decoding over Nucleus Sampling $\\mathbf{60.6\\%}$ and $\\mathbf{51.4\\%}$ of the time respectively. While the surveys have higher variance due to their small size, they cement Superposed Decoding as a strong alternative to Nucleus Sampling. We show details on the additional studies in Appendix F.1 and present our survey page in Figure 13. ", "page_idx": 7}, {"type": "text", "text": "4.5 Inference-Time Scaling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Superposed Decoding also provides significant benefits for inference time compute scaling. Superposed Decoding is completely complimentary to other decoding methods, expanding semantic coverage at no extra compute. For instance, if Nucleus Sampling is used to generate $n$ drafts, Superposed Decoding with $k$ drafts can be spliced in at any timestep to strategically produce nk drafts at no extra cost, using the Nucleus Sampled generations as prefixes. This bolsters every Nucleus Sampling generation with $k$ local searches (example in Appendix G.2). ", "page_idx": 7}, {"type": "table", "img_path": "KSOkkHm9I7/tmp/2bb19aaeb81edb7eaf86b73889a56a58f0d37f6b26526d163a03894704735629.jpg", "table_caption": ["Table 2: Coverage on TriviaQA and Natural Questions in a normalized compute setting comparing vanilla Nucleus Sampling $(N S)$ to the combination of Nucleus Sampling and Superposed Decoding $(N S_{S P D k})$ , where $k$ is the number of Superposed Decoding drafts generated on top of each Nucleus Sampled generation. $N S_{S P D}$ results in better coverage at nearly every compute budget $n$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "This property is valuable for repeated sampling [3], a technique where the number of generations is scaled at inference time to increase coverage \u2013 the proportion of questions that can be answered correctly using at least one of the generations. While repeated sampling typically uses Nucleus Sampling, combining Superposed Decoding and Nucleus Sampling $(N S_{S P D})$ produces even larger coverage gains. In Table 2, we compare the coverage of vanilla Nucleus Sampling and $N S_{S P D}$ in an equal compute setting up to 100 Nucleus Sampling drafts. We find that $N S_{S P D}$ results in higher coverage across the board on both TriviaQA [24] and Natural Questions [29], highlighting Superposed Decoding as a powerful method to increase the impact of scaling inference compute. ", "page_idx": 8}, {"type": "text", "text": "5 Further Analysis and Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.1 Results on Mistral 7B ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We also implement Superposed Decoding on pre-trained Mistral 7B [22] and conduct the same experiment as Section 4.1 with one change. In Section 4.1, it was possible to evaluate the perplexity of the 10 generated tokens exactly because the generating model (Llama-2-7B) and the evaluation model (Llama-2-70B) use the same tokenization. This is not the case for Mistral 7B and Llama-2-70B. Consequently, we calculate perplexity for Mistral 7B over all tokens but the first five, ensuring that the entire generation is included. While this approach also includes several tokens from the initial prefix in perplexity calculations, they are redundant across generations, thus preserving relative ordering. ", "page_idx": 8}, {"type": "text", "text": "Table 3 compares the resulting perplexities. Like with Llama-2-7B, the average best draft perplexity using Superposed Decoding is lower than that of Nucleus Sampling, demonstrating that Superposed Decoding is adaptable to other language models out of the box. ", "page_idx": 8}, {"type": "text", "text": "5.2 Textual Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we extensively investigate the diversity and repetition of Superposed Decoding in order to better understand its properties. ", "page_idx": 8}, {"type": "text", "text": "Repetition within Generations. Repetition is a well-documented issue in all decoding methods but is most prevalent when decoding greedily [18]. We explore to what extent, if any, Superposed Decoding degenerates as well. To measure repetition, we calculate the proportion of unique unigrams, bigrams, and trigrams in each generation. The lower the uniqueness, the higher the repetition. In Figure 8, we plot results for Superposed Decoding and Nucleus Sampling for several generation lengths. Because drafting is usually a short-form task, we only consider generation lengths up to 20 tokens. We find that Superposed Decoding does not repeat significantly more than Nucleus Sampling in this range, suggesting that degeneration is not an issue in most use cases. This is especially true for bigrams and trigrams, which are more reliable indicators of degeneration than unigrams. However, we qualitatively observe that repetitions become frequent after 100 tokens. To address this, we propose Superposed Decoding Resets, which we explain in Section 5.3. ", "page_idx": 8}, {"type": "text", "text": "Diversity across Drafts. To measure diversity, we apply Self-BLEU [45] on drafts and then calculate the average across prefixes. We compute Self-BLEU by calculating the BLEU score of each draft with the other $k-1$ drafts as references. Hence, a low Self-BLEU signifies high diversity, while a high Self-BLEU suggests low diversity. After calculating Self-BLEU for varying prefix lengths, generation lengths, and numbers of drafts, we find that generation length is the most impactful hyperparameter for diverse drafts. We plot Self-BLEU against generation length in Figure 8, demonstrating that shorter generations significantly increase diversity. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Superposed Decoding generalizes across language models like Mistral 7B as shown here with similar results on coherency, as Llama-2-7B, when evaluated using Llama-2-70B. ", "page_idx": 8}, {"type": "table", "img_path": "KSOkkHm9I7/tmp/e1b6f4110a61d23972a36a1b3a777c607b968734454ed7547578abc5663a0988.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/a7787ae0d50488ce079b5832ce8411a62d204b5dd50bf28c4b4c586f6b28037b.jpg", "img_caption": ["Figure 8: Left: Minimal difference in repetition for Superposed Decoding and Nucleus Sampling in short generations. Right: Generation length is an effective knob for adjusting the diversity of drafts. Both experiments use a prefix length of 15 tokens. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.3 Superposed Decoding Resets ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To reduce long-form repetition in Superposed Decoding, we propose resetting superposed tokens every $s$ timesteps, where $s$ is a user-selected hyperparameter. On each reset step we sample one of the $k$ drafts and restart draft generation. Resetting helps Superposed Decoding escape repetitive loops while reestablishing linearity, which Figure 3 shows to deteriorate over time. This is similar to a user selecting one of the $k$ drafts while typing. We find that resetting noticeably improves long-form generation quality (examples in Appendix I) and leave further investigation for future work. ", "page_idx": 9}, {"type": "text", "text": "Further, we also ablate on prefix length, generation length, and number of drafts. Figures 9, 10, and 11 in Appendix E highlight that Superposed Decoding is robust to all three hyperparameters, with lower perplexity than Nucleus Sampling in nearly all settings tested. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While we demonstrate that Superposed Decoding is an effective technique for multiple draft generation, Superposed Decoding is limited by the quality of the n-gram models used, which are essential for maintaining coherence. In addition, while Superposed Decoding drafts are syntactically diverse, they are not often semantically diverse. We suspect that the orthogonality of token embeddings discovered by Jiang et al. [23] is a potential solution. While our initial experiments did not show diversity gains, we believe that orthogonality is promising and is a logical next step for future work. We also note that mechanisms, like batching, that increase throughput of decoding algorithms are complementary to Superposed Decoding. ", "page_idx": 9}, {"type": "text", "text": "In conclusion, we present Superposed Decoding, a novel decoding method that superposes token embeddings to generate multiple short generations at the cost of one. We demonstrate that Superposed Decoding improves on Nucleus Sampling in terms of generation quality and human preference. The plurality of choices from Superposed Decoding also leads to better performance on common benchmarks and expands coverage at scale. We envision that the latency reduction from Superposed Decoding will make it practical to apply large pre-trained language models on drafting tasks where compute is often a barrier for deployability. Finally, Superposed Decoding can be deployed in messaging applications using n-grams personalized to each user, where the number of n-grams can be reduced to save storage without compromising performance. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We are grateful to Jiacheng (Gary) Liu for his assistance in using infinigram for initial experimentation. We also thank Alisa Liu, Vivek Ramanujan, Reza Salehi, Prateek Jain, Yashas Samaga B L, and Yejin Choi for their feedback. We also acknowledge the computing resources and support from HYAK at the University of Washington, FAS RC at Harvard University & Kempner Institute, Gradient AI and research GCP credit award from Google Cloud and Google Research. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence and Google. Sham Kakade acknowledges funding from the Office of Naval Research under award N00014-22-1-2377. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Adnan, A. Arunkumar, G. Jain, P. J. Nair, I. Soloveychik, and P. Kamath. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference, 2024.   \n[2] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr\u00f3n, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023.   \n[3] B. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. R\u00e9, and A. Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https: //arxiv.org/abs/2407.21787.   \n[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020.   \n[5] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads, 2024.   \n[6] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. [7] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [8] M. X. Chen, B. N. Lee, G. Bansal, Y. Cao, S. Zhang, J. Lu, J. Tsay, Y. Wang, A. M. Dai, Z. Chen, T. Sohn, and Y. Wu. Gmail smart compose: Real-time assisted writing, 2019.   \n[9] T. Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data.   \n[10] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022.   \n[11] Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hajishirzi, S. Kakade, A. Farhadi, P. Jain, et al. Matformer: Nested transformer for elastic inference. arXiv preprint arXiv:2310.07707, 2023.   \n[12] A. Fan, M. Lewis, and Y. Dauphin. Hierarchical neural story generation, 2018.   \n[13] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers, 2023.   \n[14] F. Gloeckle, B. Y. Idrissi, B. Rozi\u00e8re, D. Lopez-Paz, and G. Synnaeve. Better & faster large language models via multi-token prediction, 2024.   \n[15] A. Gokaslan and V. Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.   \n[16] V. Harinarayan, A. Rajaraman, and A. Ranganathan. Hybrid machine/human computing arrangement. United States Patent.   \n[17] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding, 2021.   \n[18] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration, 2020.   \n[19] W. Huang, Y. Liu, H. Qin, Y. Li, S. Zhang, X. Liu, M. Magno, and X. Qi. Billm: Pushing the limit of post-training quantization for llms, 2024.   \n[20] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt. CodeSearchNet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019.   \n[21] B. Institute. 9 ux best practice design patterns for autocomplete suggestions. 2023. URL https://baymard.com/blog/autocomplete-design. Accessed: 2024-05-18.   \n[22] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.   \n[23] Y. Jiang, G. Rajendran, P. Ravikumar, B. Aragam, and V. Veitch. On the origins of linear representations in large language models, 2024.   \n[24] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, 2017.   \n[25] U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.   \n[26] S. Kim, K. Mangalam, S. Moon, J. Malik, M. W. Mahoney, A. Gholami, and K. Keutzer. Speculative decoding with big little decoder. Advances in Neural Information Processing Systems, 36, 2024.   \n[27] N. Kitaev, \u0141ukasz Kaiser, and A. Levskaya. Reformer: The efficient transformer, 2020.   \n[28] A. Kusupati, V. Ramanujan, R. Somani, M. Wortsman, P. Jain, S. Kakade, and A. Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pages 5544\u20135555. PMLR, 2020.   \n[29] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.   \n[30] Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding, 2023.   \n[31] J. Liu, S. Min, L. Zettlemoyer, Y. Choi, and H. Hajishirzi. Infini-gram: Scaling unbounded n-gram language models to a trillion tokens. arXiv preprint arXiv:2401.17377, 2024.   \n[32] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei. The era of 1-bit llms: All large language models are in 1.58 bits, 2024.   \n[33] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018.   \n[34] e. a. OpenAI. Gpt-4 technical report, 2024.   \n[35] K. Park, Y. J. Choe, and V. Veitch. The linear representation hypothesis and the geometry of large language models. arXiv preprint arXiv:2311.03658, 2023.   \n[36] W. Qi, Y. Yan, Y. Gong, D. Liu, N. Duan, J. Chen, R. Zhang, and M. Zhou. Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training, 2020.   \n[37] N. Shazeer. Fast transformer decoding: One write-head is all you need, 2019.   \n[38] W. Shi, J. Michael, S. Gururangan, and L. Zettlemoyer. Nearest neighbor zero-shot inference. In Conference on Empirical Methods in Natural Language Processing, 2022. URL https: //api.semanticscholar.org/CorpusID:249152130.   \n[39] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. A simple and effective pruning approach for large language models, 2024.   \n[40] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014.   \n[41] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023.   \n[42] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.   \n[43] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear complexity, 2020.   \n[44] Q. Zhang, D. Ram, C. Hawkins, S. Zha, and T. Zhao. Efficient long-range transformers: You need to attend more, but not necessarily at every layer, 2023.   \n[45] Y. Zhu, S. Lu, L. Zheng, J. Guo, W. Zhang, J. Wang, and Y. Yu. Texygen: A benchmarking platform for text generation models. SIGIR, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "table", "img_path": "KSOkkHm9I7/tmp/63b2a811c8d050a0b1ffa8059ead9af3940c2a9d55364dad7cceb481595e8167.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Hyperparameter Choices ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "$\\alpha$ and $\\tau$ . We find that the best performing hyperparameters for coherent generation on Llama-2-7B are $\\alpha=0.54$ and $\\tau=0.06$ , which we use in Section 4.1, 4.3, and 4.4. The only exception is in Section 4.2, where we disable n-gram smoothing. ", "page_idx": 13}, {"type": "text", "text": "N-Gram Interpolation. The weights we use for interpolation are $[0.01,0.04,0.15,0.18,0.12]$ for $n=2$ to $n=6$ . ", "page_idx": 13}, {"type": "text", "text": "C Perplexity Evaluation with Fewer N-Grams ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We evaluate perplexity for Superposed Decoding on the OpenWebText test split by interpolating only $n$ -grams from $n=2$ to $n=4$ , requiring only $2.64\\:\\mathrm{GB}$ of storage. We use the same n-gram interpolation weights as in Section 3 but retune $\\alpha$ and $\\tau$ to 0.55 and 0.1 respectively. In Table 4, we show that the perplexity of the average best generation is almost identical to the average Nucleus Sampling perplexity. This demonstrates that Superposed Decoding can be effectively implemented with minimal storage overhead in storage-constrained devices like smartphones. ", "page_idx": 13}, {"type": "text", "text": "D Standard Deviations for Perplexity Evaluation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We calculate the standard deviations across the 5000 OpenWebText prefixes used for evaluation for Llama-2-7B and Mistral 7B and show them below in Tables 5 and 6. We note that Superposed Decoding has higher standard deviation than Nucleus Sampling and Beam Search when implemented ", "page_idx": 13}, {"type": "table", "img_path": "KSOkkHm9I7/tmp/2ba316ca71e98e234f7cc983713b37d8319da8e7efb2bf599bd9235e4bfb0572.jpg", "table_caption": ["Table 4: Superposed Decoding is highly competitive with Nucleus Sampling even when only $n$ -grams from n = 2 to n = 4 are interpolated to calculate pngram. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "on Llama-2-7B but that the standard deviations equalize on Mistral 7B. Despite this high variance, human evaluation results in Section 4.4 affirm that Superposed Decoding is consistently competitive in coherency. ", "page_idx": 14}, {"type": "table", "img_path": "KSOkkHm9I7/tmp/091232a20e5b7599bef028c5067789e3a1cc6c71ae4438fd79a1f364c9448647.jpg", "table_caption": ["Table 5: Standard deviation of Llama-2-7B generation perplexity calculated on OpenWebText test split in Section 4.1. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "KSOkkHm9I7/tmp/aa3b63d66794d121a9e16014ec73cf1ba9b50098dbaf80f2afdb15fdc251459e.jpg", "table_caption": ["Table 6: Standard deviation of Mistral 7B generation perplexity calculated on OpenWebText test split in Section 5.1. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "E Further Ablations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present further ablations studying the impact of number of drafts, prefix length, and generation length on Superposed Decoding perplexity (average best). We find that for scenarios such that the number of drafts $k\\geq3$ , Superposed Decoding consistently outperforms nucleus sampling (Figure 9). This is generally expected as the higher the number of drafts, the higher the likelihood that at least one draft is good quality. Similarly, we find that Superposed Decoding is robust to prefix length and generation length. Indeed, the patterns of Superposed Decoding given longer prefix and generation lengths mimic those of Nucleus Sampling in the same situation, with consistently similar perplexities suggesting good performance (Figures 10, 11). ", "page_idx": 14}, {"type": "text", "text": "F Human Evaluation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Additional Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We run the same experiment as in Section 4.4 two more times at a smaller scale. First, we compare two Nucleus Sampling drafts and three Superposed Decoding drafts. This scenario results in a significant compute advantage for Nucleus Sampling. Still, we find that Superposed Decoding wins the majority of the time, with a win rate of $60.6\\%$ . However, this is slightly lower than the $64\\bar{\\%}$ win rate in the main study. ", "page_idx": 14}, {"type": "text", "text": "Next, we compare one Nucleus Sampling draft and the lowest perplexity Superposed Decoding draft of three. This mimics the perplexity evaluation in Section 4.1 but in a human setting. We find that Superposed Decoding is still ranked first the majority of the time, with a $51\\%$ win rate. ", "page_idx": 14}, {"type": "text", "text": "For cost efficiency, we run both studies on only 100 random prompts, with five responses per prompt. ", "page_idx": 14}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/551eadbc5ec074d0c2c5a979daec4c55e12338198cbf10fa80b1d3bde9042151.jpg", "img_caption": ["Figure 9: Average perplexity of one draft of Nucleus Sampling and Beam Search compared against average best perplexity of SPD using drafts $k=\\{2,3,4,5,6,10\\}$ on OpenWebText. SPD outperforms Nucleus Sampling for all values of $k$ tested. ", ""], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/97d757d2c2d92639a46f0afc74fab437435a35db5d5b435df9e0570f549fd561.jpg", "img_caption": ["Figure 10: Average perplexity of one draft of Nucleus Sampling compared against average best perplexity of SPD for prefix lengths 5, 15, 25, 40. We find that SPD and Nucleus Sampling consistently have similar perplexities. ", "Prefix Length "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/a71a6f87f395287366567e268afc7f33fcc9545a07d0ac3e3823ee280d1dc27b.jpg", "img_caption": ["Figure 11: Average perplexity of one draft of Nucleus Sampling compared against average best perplexity of SPD for generation lengths 5, 10, 15, 20. We find that SPD is robust for a wide range of short form generation lengths. ", "Generation Length "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/3f2b46c584bdc8ee38081813bf80a803737e33e78807850d9bfca7d727bc4f44.jpg", "img_caption": ["Figure 12: Left: Study results comparing two Nucleus Sampling drafts and three Superposed Decoding drafts. Right: Study results comparing one Nucleus Sampling draft & the lowest perplexity Superposed Decoding draft out of three generations. Note that uncertainty is higher for both iterations due to smaller sample size. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "F.2 Mechanical Turk Survey ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We show a screenshot of our study page in Figure 13. We compensate workers at an approximate rate of $\\mathbb{S}15$ per hour. ", "page_idx": 16}, {"type": "text", "text": "G Example Generations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "G.1 Superposed Decoding ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figure 14, we show 12 additional example generations comparing Superposed Decoding and Nucleus Sampling in a compute-normalized setting. Prefixes are sampled from OpenWebText and truncated to the first 15 tokens, and each generated completion is 10 tokens in length. ", "page_idx": 16}, {"type": "text", "text": "G.2 Nucleus Sampling and Superposed Decoding ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figure 15, we show an example generation that outputs three drafts using Nucleus Sampling for several timesteps, followed by Superposed Decoding with $k=3$ . This results in nine total drafts at the compute of only three Nucleus Sampling drafts. The final Superposed Decoding drafts maintain the wide coverage offered by Nucleus Sampling and complement it with additional local coverage. ", "page_idx": 16}, {"type": "text", "text": "H Layer-Wise Linearity ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figure 16, we show the cosine similarity between superposed embeddings $\\{\\tilde{x}\\}$ and the linear combination of the component embeddings of three tokens, after each layer within the first five timesteps using Llama-2-7B. Within each timestep, superposed embeddings pass through an initial layer (Layer 1) that embeds token indices and weights the resulting embeddings using corresponding token weights. The weighted embedings then pass through 32 transformer blocks (Layers 2-33), then through a final linear projection layer (Layer 34) that produces the output superposed embedding of that timestep. ", "page_idx": 16}, {"type": "text", "text": "I Resetting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here, we show examples of the Superposed Decoding Resetting technique described in Section 5.3 for generations of 200 tokens using prefixes from OpenWebText. Superposed Decoding Resetting is significantly less repetitive than non-reset Superposed Decoding. However, Nucleus Sampling, which is also shown, is qualitatively still the best performing decoding method in long form settings. ", "page_idx": 17}, {"type": "text", "text": "Prompt #1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Superposed Decoding with Resetting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Sydney architect Rana Abboud won a NAWIC Scholarship for 2019. Sydney-based architect and interior designer Rana Abboud has been awarded a $\\mathbb{S}10{,}000$ scholarship to study a Masters of Architecture at the University of New South Wales. The scholarship, awarded by the National Association of Women in Construction (NAWIC) is designed to help women in the construction industry to advance their careers. Rana Abboud, a graduate of the University of Sydney, is the founder of Rana Abboud & Associates. \u201cI am honoured to be awarded this scholarship. I am passionate about design built for people and the environment. I am excited to be able to further my studies and continue to develop my skills and knowledge,\u201d she said. The NAWIC Scholarship is awarded to women who are studying or working in the construction industry. \u201cThe NAWIC Scholarship is a great opportunity for women to further their care and ", "page_idx": 17}, {"type": "text", "text": "Superposed Decoding without Resetting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Sydney architect Rana Abboud won a NAWIC Scholarship in 2017. The National Association of Women in Construction (NAWIC) has announced the winners of its 2017 scholarships. The NAWIC Scholarship Program is designed to encourage women to pursue careers in the construction industry. The 2017 scholarship recipients are: Rana Abboud, a student at the University of New South Wales, who is studying a Bachelor of Architecture. Megan Brennan, a student at the University of New South Wales, who is studying a Bachelor of Construction Management. Katee Cox, a student at the University of New South Wales, who is studying a Bachelor of Construction Management. Kate Dunn, a student at the University of New South Wales, who is studying a Bachelor of Construction Management. Kate Foster, a student at the University of New South Wales, who ", "page_idx": 17}, {"type": "text", "text": "Nucleus Sampling ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Sydney architect Rana Abboud won a NAWIC Scholarship of $\\mathbb{S}1000$ for \u2018Significant Contribution to Career Growth\u2019. Sydney architect Rana Abboud has won the National Association of Women in Construction (NAWIC) NSW/ACT Scholarship for 2017 for \u2018Significant Contribution to Career Growth\u2019. She has also just been accepted to undertake a Master of Science in Architecture with Higher Degree by Research at Sydney University. The scholarship is one of eight awards presented by NAWIC to women to further their studies in the construction industry in Australia. \u201cAs a working mother, I am very grateful for the support I am receiving to further my professional and academic development,\u201d Abboud said. Rana Abboud (middle) with associates (L-R) Tara Abdul-Khalek and Aditi Rana. Abboud is a director of Pal Architecture in Alexandria, ", "page_idx": 17}, {"type": "text", "text": "Superposed Decoding with Resetting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "NEW DELHI: Here\u2019s something ayurveda gurus have been telling for years. A new study has found that a combination of three herbs - including ashwagandha, guduchi and shatavari - can help in the treatment of diabetes. \u201cThe study was conducted to evaluate the efficacy and safety of a poly-herbal formulation containing three herbs - Ashwagandha (Withania somnifera), Guduchi (Tinospora cordifolia), and Shatavari (Asparagus racemosus) in inhibiting the progression of diabetes in alloxan-induced diabetic rats\u201d said the study published in the journal \u2018Journal of Ayurveda and Integrative Medicine\u2019. The study was conducted by research team from the Department of Pharmacology, Dr D. Patil College of Pharmacy, Pune, Maharashtra, and the Department of Pharm ", "page_idx": 18}, {"type": "text", "text": "Superposed Decoding without Resetting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "NEW DELHI: Here\u2019s something ayurveda gurus have been saying for years: Eat your way to health. A new study has found that a diet rich in fruits and vegetables can help reduce the risk of developing type 2 diabetes. The study, published in the journal BMJ Open, found that a diet rich in fruits and vegetables can help reduce the risk of developing type 2 diabetes. The study, which was conducted by researchers from the University of Oxford, found that a diet rich in fruits and vegetables can help prevent the risk of developing type 2 diabetes. The study, which was conducted by researchers from the University of Oxford, found that a diet rich in fruits and vegetables can help prevent the risk of developing type 2 diabetes. The study, which was conducted by researchers from the University of Oxford, found that a diet rich in fruits and vegetables can help prevent the risk ", "page_idx": 18}, {"type": "text", "text": "Nucleus Sampling ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "NEW DELHI: Here\u2019s something ayurveda gurus have been telling us about for years - eating seasonal fruits and vegetables helps stay healthy. Here are 7 foods which can keep you healthy during the summer. Pomegranate is a summer fruit and if you wish to have optimum health, it is necessary to eat this fruit. The fruit can be eaten in raw form, or cut into pieces and mixed with honey and taken twice a day. The skin of the fruit is itself a health tonic and can be chewed. It contains natural bleaching properties. The fruit can also be squeezed and then the pulp mixed with milk, cumin seeds, one teaspoonful of honey and a pinch of ginger and drank thrice daily. Beets are a popular root vegetable. Beetroot juice may help increase exercise endurance, potentially because it increases blood flow, said WebMD. The best fruits ", "page_idx": 18}, {"type": "text", "text": "Prompt # 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Superposed Decoding with Resetting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As part of a broad initiative to combat sexual harassment and assault, the University of California, Berkeley has launched a new website to help students, staff, and faculty report and respond to sexual harassment and assault. The new site, which went live on Monday, is the one-stop destination for all things related to sexual harassment and assault at UC Berkeley. It is the result of a collaboration between the Office of the Vice Chancellor of Student Affairs and the Office of the Vice Provost for Academic and Student Affairs. \u201cThis is a critical time for our campus and to come together to address sexual harassment and assault in our community,\u201d said Vice Chancellor for Student Affairs, Steph Sutton-Wheeler, in a statement. \u201cWe are committed to providing a safe and inclusive environment for all members of our community and this new website is a critical step in that effort.\u201d The site provides information on how to report sexual harassment and assault, as well as the for ", "page_idx": 18}, {"type": "text", "text": "As part of a broad initiative to combat sexual harassment and assault, the University of California, Berkeley, has launched a new website that provides information about the university\u2019s policies and procedures for reporting and responding to sexual harassment and sexual violence. The website, which is available at http://sexualharassment.berkeley.edu, is designed to help students, faculty and staff understand their rights and responsibilities under the university\u2019s policies and procedures. The website includes information about the university\u2019s policies and procedures for reporting and responding to sexual harassment and sexual violence, as well as information about the university\u2019s policies and procedures for investigating and responding to sexual harassment and sexual violence. The website also includes information about the university\u2019s policies and procedures for investigating and responding to sexual harassment and sexual violence, including information about the university\u2019s policies and procedures for investigating and responding to sexual harassment and sexual violence in the workplace. ", "page_idx": 19}, {"type": "text", "text": "Nucleus Sampling ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As part of a broad initiative to combat sexual harassment and misconduct on campus, a task force consisting of faculty, staff, students, and other members of the campus community will present a draft report with recommendations at the Board of Trustees meeting on Friday, October 27, 2017. Under the leadership of Executive Director for Human Resources Jesse Bozeman, the task force was formed to address concerns raised at the end of the Spring 2017 semester, which included a Student Government Association resolution and a student-driven, faculty-approved course called \u201cSexual Misconduct: Consent and Culture,\u201d taught by Professor Nancy Cantalupo and co-sponsored by Campus Life, Student Affairs, and Academic Affairs. The task force is comprised of the following members: Diana Ballin, Senior Lecturer in the Department of Anthropology, Chair; Patricia Hopkins, Dean of Students; Kelly McSweeney ", "page_idx": 19}, {"type": "text", "text": "Short instructions & Consent Form ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Summary: Your task is to rank four language model outputs based on how well they complete the provided prompt. This should take less than a minute. ", "page_idx": 20}, {"type": "text", "text": "Criteria: You should rank completions by coherency and factuality only. Coherent generations should be grammatically correct and make logical sense. Some options may seem equally preferable. In such cases, try your best to pick one over the other! ", "page_idx": 20}, {"type": "text", "text": "Examples: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Prompt: The English soccer club flew in yesterday Best Completion: to compete in a tourn Second Best: to compete in a comp Third Best: to compete with a new Fourth Best: with high hopes to sell ", "page_idx": 20}, {"type": "text", "text": "Prompt: In an interview with BBC, Best Completion: the president stated his desi Second Best: the manager explained his inter Third Best: the manager explained his favor Fourth Best: the manager defended decision ", "page_idx": 20}, {"type": "text", "text": "Data collection & sharing: We will not ask you for your name, and the data collected in this study will be made unidentifiable to the best of our extent. We will securely store the data on our servers and only share with qualified researchers (e.g., who want to further study language model text completions). If you later decide that you do not want your responses included in this study, please email so we can exclude your work. ", "page_idx": 20}, {"type": "text", "text": "Contact: If you have any questions about this study, you should feel free to ask them by contacting us below. ", "page_idx": 20}, {"type": "text", "text": "Task: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Order the completions in decreasing order such that the preferred one is at the top.   \nNote that $\"\\mathsf{M}\"$ denotes a line break. ", "page_idx": 20}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/37ec89cb0ac0f5e2a2f35cce92dd794ccc06d06c1892c95ffe3354998430f9a3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/cd5391b46dcacf412ae077c348595550d898f13b45165437bc54aee092bb0132.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 14: Additional qualitative text generations in a compute-normalized setting for Superposed Decoding and Nucleus Sampling with prefixes sampled from OpenWebText ", "page_idx": 21}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/0fb510c48c5ac55e15491da431724d8d94e3c49a7fc22cfe4e32620c9076de82.jpg", "img_caption": ["Figure 15: Example output generated by combining Nucleus Sampling and Superposed Decoding. Superposed Decoding generates three drafts per Nucleus Sampling sample, resulting in nine total drafts. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "KSOkkHm9I7/tmp/f9cbbd860dd62fc016a5f1ac703a5070b7143468c938ffafbefe55c31c6e76d3.jpg", "img_caption": ["Figure 16: Layer-wise linearity analysis of the first five timesteps on Llama-2-7B with three tokens. The relationship between superposed embeddings and the component token embeddings is initially entirely linear; linearity then degenerates over the first few layers, but gradually recovers through the subsequent transformer block layers. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All the claims made in the abstract and introduction are supported by problem formulation, solution and empirical evaluation. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We talk about the limitations of the proposed method in Section 6 along with appropriate places in the manuscript. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: N/A ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The entire algorithm, experimental setup, and evaluation methodology are clearly outlined in the paper. We shall also open-source the code. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The code will be open-sourced and all the experiments are done on public datasets and open models. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Section 4 and the Appendix. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Some of the baselines and our method are probabilistic and we have reported the means and standard deviation where applicable. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Section 4 ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper deals with decoding algorithms for autoregressive inference which does not violate any aspect of the code of ethics. ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: As with any decoding algorithm the method tries to elicit information from a large language model but n-gram interpolation has the potential to make it more factual than it currently is. That being said, there are no significant positive or negative societal impacts of this work. ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not release any potentially misusable artifacts. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We use open language models and public datasets but do not train or add any new things to them. We provide MIT License software to be used with the model and data. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: N/A ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] Justification: See Figure 13 ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We obtained an IRB exception for the user study. ", "page_idx": 24}]