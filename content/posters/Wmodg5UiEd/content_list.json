[{"type": "text", "text": "Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Learning from human feedback plays an important role in aligning generative   \n2 models, such as large language models (LLM). However, the effectiveness of   \n3 this approach can be influenced by adversaries, who may intentionally provide   \n4 misleading preferences to manipulate the output in an undesirable or harmful   \n5 direction. To tackle this challenge, we study a specific model within this problem   \n6 domain\u2013contextual dueling bandits with adversarial feedback, where the true   \n7 preference label can be flipped by an adversary. We propose an algorithm namely   \n8 robust contextual dueling bandits (RCDB), which is based on uncertainty-weighted   \n9 maximum likelihood estimation. Our algorithm achieves an $\\widetilde{O}(d\\sqrt{T}+{d C})$ regret   \n10 bound, where $T$ is the number of rounds, $d$ is the dimension of the context, and   \n11 $0\\leq C\\leq T$ is the total number of adversarial feedback. We also prove a lower   \n2 bound to show that our regret bound is nearly optimal, both in scenarios with and   \n13 without $C=0$ ) adversarial feedback. Additionally, we conduct experiments to   \n14 evaluate our proposed algorithm against various types of adversarial feedback.   \n15 Experimental results demonstrate its superiority over the state-of-the-art dueling   \n16 bandit algorithms in the presence of adversarial feedback. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Acquiring an appropriate reward proves challenging in numerous real-world applications, often   \n19 necessitating intricate instrumentation (Zhu et al., 2020) and time-consuming calibration (Yu et al.,   \n0 2020) to achieve satisfactory levels of sample efficiency. For instance, in training large language models (LLM) using reinforcement learning from human feedback (RLHF), the diverse values and   \n2 perspectives of humans can lead to uncalibrated and noisy rewards (Ouyang et al., 2022). In contrast,   \n3 preference-based data, which involves comparing or ranking various actions, is a more straightforward method for capturing human judgments and decisions. In this context, the dueling bandit model   \n5 (Yue et al., 2012) provides a problem framework that focuses on optimal decision-making through   \n26 pairwise comparisons, rather than relying on the absolute reward for each action. ", "page_idx": 0}, {"type": "text", "text": "27 However, human feedback may not always be reliable. In real-world applications, human feedback   \n28 is particularly vulnerable to manipulation through preference label flip. Adversarial feedback can   \n9 significantly increase the risk of misleading a large language model (LLM) into erroneously prioritiz  \ning harmful content, under the false belief that it reflects human preference. Despite the significant   \n1 influence of adversarial feedback, there is limited existing research on the impact of adversarial   \n32 feedback specifically within the context of dueling bandits. A notable exception is Agarwal et al.   \n33 (2021), which studies dueling bandits when an adversary can flip some of the preference labels   \n34 received by the learner. They proposed an algorithm that is agnostic to the amount of adversarial   \n35 feedback introduced by the adversary. However, their setting has the following two limitations.   \n36 First, their study was confined to a finite-armed setting, which renders their results less applicable   \n37 to modern applications such as RLHF. Second, their adversarial feedback is defined on the whole   \n38 comparison matrix. In each round, the adversary observes the outcomes of all pairwise comparisons   \n39 and then decides to corrupt some of the pairs before the agent selects the actions. This assumption   \n40 does not align well with the real-world scenario, where the adversary often filps the preference label   \n41 based on the information of the selected actions.   \n42 In this paper, to address the above challenge, we aim to develop contextual dueling bandit algorithms   \n43 that are robust to adversarial feedback. This enables us to effectively tackle problems involving   \n44 a large number of actions while also taking advantage of contextual information. We specifically   \n45 consider a scenario where the adversary knows the selected action pair and the true preference of   \n46 their comparison. In this setting, the adversary\u2019s only decision is whether to filp the preference label   \n47 or not. We highlight our contributions as follows:   \n48 \u2022 We propose a new algorithm called robust contextual dueling bandits (RCDB), which integrates   \n49 uncertainty-dependent weights into the Maximum Likelihood Estimator (MLE). Intuitively, our   \n50 choice of weight is designed to induce a higher degree of skepticism about potentially \u201cuntrust  \n51 worthy\u201d feedback. The agent is encouraged to focus more on feedback that is more likely to be   \n52 genuine, effectively diminishing the impact of any adversarial feedback.   \n53 \u2022 We analyze the regret of our algorithm under at most $C$ number of adversarial feedback. Our result   \n54 consists of two terms: a $C$ -independent term $\\widetilde{O}(d\\sqrt{T})$ , which matches the lower bound established   \n55 in Bengs et al. (2022) for uncorrupted linear contextual dueling bandits, and a $C$ -dependent term   \n56 $\\widetilde O(d C)$ . Furthermore, we establish a lower bound for dueling bandits with adversarial feedback,   \n57 demonstrating the optimality of our adversarial term. Consequently, our algorithm for dueling   \n58 bandits attains the optimal regret in both scenarios, with and without adversarial feedback.   \n59 \u2022 We conduct extensive experiments to validate the effectiveness of our algorithm RCDB. To compre  \n60 hensively assess RCDB\u2019s robustness against adversarial feedback, we evaluate its performance under   \n61 various types of adversarial feedback and compare the results with state-of-the-art dueling bandit   \n62 algorithms. Experimental results demonstrate the superiority of our algorithm in the presence of   \n63 adversarial feedback, which corroborate our theoretical analysis.   \n64 Notation. In this paper, we use plain letters such as $x$ to denote scalars, lowercase bold letters such   \n65 as $\\mathbf{x}$ to denote vectors and uppercase bold letters such as $\\mathbf{X}$ to denote matrices. For a vector $\\mathbf{x}$ , $\\|\\mathbf{x}\\|_{2}$   \n66 denotes its $\\ell_{2}$ -norm. The weighted $\\ell_{2}$ -norm associated with a positive-definite matrix $\\mathbf{A}$ is defined   \n67 as $\\|\\mathbf{x}\\|_{\\mathbf{A}}=\\sqrt{\\mathbf{x}^{\\top}\\mathbf{Ax}}$ . For two symmetric matrices A and $\\mathbf{B}$ , we use $\\mathbf A\\succeq\\mathbf B$ to denote $\\mathbf{A}-\\mathbf{B}$ is   \n68 positive semidefinite. We use $\\mathbb{1}$ to denote the indicator function and 0 to denote the zero vector. For   \n69 two actions $a,b,$ , we use $a\\succ b$ to denote $a$ is more preferable to $b$ . For a postive integer $N$ , we use   \n70 $[N]$ to denote $\\{1,2,\\ldots,N\\}$ . We use standard asymptotic notations including $O(\\cdot),\\Omega(\\cdot),\\Theta(\\cdot)$ , and   \n71 $\\widetilde{\\cal O}(\\cdot),\\widetilde{\\Omega}(\\cdot),\\widetilde{\\Theta}(\\cdot)$ will hide logarithmic factors. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "table", "img_path": "Wmodg5UiEd/tmp/45a6cb706a129f88b42b4be6364e9058257f0d447c2ccbb34b33fd7ff38a66fd.jpg", "table_caption": ["Table 1: Comparison of algorithms for robust bandits and dueling bandits. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "72 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "73 Bandits with Adversarial Reward. The multi-armed bandit problem, involving an agent making   \n74 sequential decisions among multiple arms, has been studied with both stochastic rewards (Lai   \n75 et al., 1985; Lai, 1987; Auer, 2002; Auer et al., 2002a; Kalyanakrishnan et al., 2012; Lattimore and   \n76 Szepesv\u00e1ri, 2020; Agrawal and Goyal, 2012), and adversarial rewards (Auer et al., 2002b; Bubeck   \n77 et al., 2012). Moreover, a line of works focuses on designing algorithms that can achieve near-optimal   \n78 regret bounds for both stochastic bandits and adversarial bandits simultaneously (Bubeck and Slivkins,   \n79 2012; Seldin and Slivkins, 2014; Auer and Chiang, 2016; Seldin and Lugosi, 2017; Zimmert and   \n80 Seldin, 2019; Lee et al., 2021), which is known as \u201cthe best of both worlds\u201d guarantee. Distinct from   \n81 fully stochastic and fully adversarial models, Lykouris et al. (2018) studied a setting, where only a   \n82 portion of the rewards is subject to corruption. They proposed an algorithm with a regret dependent   \n83 on the corruption level $C$ , defined as the cumulative sum of the corruption magnitudes in each round.   \n84 Their result is $C$ times worse than the regret without corruption. Gupta et al. (2019) improved the   \n85 result by providing a regret guarantee comprising two terms, a corruption-independent term that   \n86 matches the regret lower bound without corruption, and a corruption-dependent term that is linear in   \n87 $C$ . In addition, Gupta et al. (2019) proved a lower bound demonstrating the optimality of the linear   \n88 dependency on $C$ .   \n89 Contextual Bandits with Corruption. Li et al. (2019) studied stochastic linear bandits with   \n90 corruption and presented an instance-dependent regret bound linearly dependent on the corruption   \n91 level $C$ . Bogunovic et al. (2021) studied the same problem and proposed an algorithm with near  \n92 optimal regret in the non-corrupted case. Lee et al. (2021) studied this problem in a different setting,   \n93 where the adversarial corruptions are generated through the inner product of a corrupted vector   \n94 and the context vector. For linear contextual bandits, Bogunovic et al. (2021) proved that under an   \n95 additional context diversity assumption, the regret of a simple greedy algorithm is nearly optimal   \n96 with an additive corruption term. Zhao et al. (2021) and Ding et al. (2022) extended the OFUL   \n97 algorithm (Abbasi-Yadkori et al., 2011) and proved a regret with a corruption term polynomially   \n98 dependent on the total number of rounds $T$ . He et al. (2022) proposed an algorithm for known   \n99 corruption level $C$ to remove the polynomial dependency on $T$ in the corruption term, which only   \n100 has a linear dependency on $C$ . They also proved a lower bound showing the optimality of linear   \n101 dependency on $C$ for linear contextual bandits with a known corruption level. Additionally, He et al.   \n102 (2022) extended the proposed algorithm to an unknown corruption level and provided a near-optimal   \n103 performance guarantee that matches the lower bound. For more extensions, Kuroki et al. (2023)   \n104 studied best-of-both-worlds algorithms for linear contextual bandits. Ye et al. (2023) proposed a   \n105 corruption robust algorithm for nonlinear contextual bandits.   \n106 Dueling Bandits and Logistic Bandits. The dueling bandit model was first proposed in Yue   \n107 et al. (2012). Compared with bandits, the agent will select two arms and receive the preference   \n108 feedback between the two arms from the environment. For general preference, there may not exist   \n109 the \u201cbest\u201d arm that always wins in the pairwise comparison. Therefore, various alternative winners   \n110 are considered, including Condorcet winner (Zoghi et al., 2014; Komiyama et al., 2015), Copeland   \n111 winner (Zoghi et al., 2015; Wu and Liu, 2016; Komiyama et al., 2016), Borda winner (Jamieson et al.,   \n112 2015; Falahatgar et al., 2017; Heckel et al., 2018; Saha et al., 2021; Wu et al., 2023) and von Neumann   \n113 winner (Ramamohan et al., 2016; Dud\u00edk et al., 2015; Balsubramani et al., 2016), along with their   \n114 corresponding performance metrics. To handle potentially large action space or context information,   \n115 Saha (2021) studied a structured contextual dueling bandit setting. In this setting, each arm possesses   \n116 an unknown intrinsic reward. The comparison is determined based on a logistic function of the relative   \n117 rewards. In a similar setting, Bengs et al. (2022) studied contextual linear stochastic transitivity   \n118 model with contextualized utilities. Di et al. (2023) proposed a layered algorithm with variance   \n119 aware regret bound. Another line of works does not make the reward assumption. Instead, they   \n120 assume the preference feedback can be represented by a function class. Saha and Krishnamurthy   \n121 (2022) designed an algorithm that achieves the optimal regret for $K$ -armed contextual dueling bandit   \n122 problem. Sekhari et al. (2023) studied contextual dueling bandits in a more general setting and   \n123 proposed an algorithm the provides guarantees for both regret and the number of queries. Another   \n124 related area of research is the logistic bandits, where the agent selects one arm in each round and   \n125 receives a Bernoulli reward. Faury et al. (2020) studied the dependency with respect to the degree   \n126 of non-linearity of the logistic function $\\kappa$ . They proposed an algorithm with no dependency in $\\kappa$ .   \n127 Abeille et al. (2021) further improved the dependency on $\\kappa$ and proved a problem dependent lower   \n128 bound. Faury et al. (2022) proposed a computationally efficient algorithm with regret performance   \n129 still matching the lower-bound proved in Abeille et al. (2021).   \n130 Dueling Bandits with Adversarial Feedback. A line of work has focused on dueling bandits with   \n131 adversarial feedback or corruption. Gajane et al. (2015) studied a fully adversarial utility-based   \n132 version of dueling bandits, which was proposed in Ailon et al. (2014). Saha et al. (2021) considered   \n133 the Borda regret for adversarial dueling bandits without the assumption of utility. In a setting   \n134 parallel to that in Lykouris et al. (2018); Gupta et al. (2019), Agarwal et al. (2021) studied $K$ -armed   \n135 dueling bandits in a scenario where an adversary has the capability to corrupt part of the feedback   \n136 received by the learner. They designed an algorithm whose regret comprises two terms: one that   \n137 is optimal in uncorrupted scenarios, and another that is linearly dependent on the total times of   \n138 adversarial feedback $C$ . Later on, Saha and Gaillard (2022) achieved \u201cbest-of-both world\u201d result for   \n139 noncontextual dueling bandits and improved the adversarial term of Agarwal et al. (2021) in the same   \n140 setting. For contextual dueling bandits, Wu et al. (2023) proposed an EXP3-type algorithm for the   \n141 adversarial linear setting using Borda regret. For a comparison of the most related works for robust   \n142 bandits and dueling bandits, please refer to Table 1. In this paper, we study the influence of adversarial   \n143 feedback within contextual dueling bandits, particularly in a setting where only a minority of the   \n144 feedback is adversarial. Compared to previous studies, most studies have focused on the multi-armed   \n145 dueling bandit framework without integrating context information. The notable exception is Wu et al.   \n146 (2023); however, this study does not provide guarantees regarding the dependency on the number of   \n147 adversarial feedback instances. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "148 3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "149 In this work, we study linear contextual dueling bandits with adversarial feedback. In each round   \n150 $t\\in[T]$ , the agent observes the context information $x_{t}$ from a context set $\\mathcal{X}$ and the corresponding   \n151 action set $\\boldsymbol{\\mathcal{A}}$ . Utilizing this context information, the agent selects two actions, $a_{t}$ and $b_{t}$ . Subsequently,   \n152 the environment will generate a binary feedback (i.e., preference label) $l_{t}=\\mathbb{1}(a_{t}\\succ b_{t})\\in\\bar{\\{0,1\\}}$   \n153 indicating the preferable action. We assume the existence of a reward function $r^{*}(x,a)$ dependent on   \n154 the context information $x$ and action $a$ , and a monotonically increasing link function $\\sigma$ satisfying   \n155 $\\sigma(x)+\\sigma(-x)=1$ . The preference probability will be determined by the link function and the   \n156 difference between the rewards of the selected arms, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}(a\\succ b|x)=\\sigma\\big(r^{*}(x,a)-r^{*}(x,b)\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "157 We assume that the reward function is linear with respect to some known feature map $\\phi(x,a)$ . To be   \n158 more specific, we make the following assumption:   \n159 Assumption 3.1. Let $\\phi:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}^{d}$ be a known feature map, with $\\|\\phi(x,a)\\|_{2}\\,\\leq\\,1$ for any   \n160 $(x,a)\\,\\in\\,\\mathcal{X}\\,\\times\\,\\mathcal{A}$ . We define the reward function $r_{\\theta}$ parameterized by $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ , with $r_{\\theta}(x,a)\\,=$   \n161 $\\langle\\pmb{\\theta},\\pmb{\\phi}(x,a)\\rangle$ . Moreover, there exists $\\pmb{\\theta}^{*}$ satisfying $r_{\\theta^{\\ast}}=r^{\\ast}$ , with $\\lVert\\pmb{\\theta}^{*}\\rVert_{2}^{\\frac{1}{2}}\\leq B$ .   \n162 Similar assumptions have been made in the literature of dueling bandits (Saha, 2021; Bengs et al.,   \n163 2022; Xiong et al., 2023). We also make an assumption on the derivative of the link function, which   \n164 is common in the study of generalized linear models for bandits (Filippi et al., 2010). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "165 Assumption 3.2. The link function $\\sigma$ is differentiable. Furthermore, its first-order derivative satisfies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\dot{\\sigma}(\\cdot)\\geq\\kappa\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "166 for some constant $\\kappa>0$ . ", "page_idx": 3}, {"type": "text", "text": "167 In our setting, however, the agent does not directly observe the true binary feedback. Instead, an   \n168 adversary will see both the choice of the agent and the true feedback. Based on the information, the   \n169 adversary can decide whether to corrupt the binary feedback or not.1 We represent the adversary\u2019s   \n170 decision in round $t$ by an adversarial indicator $c_{t}$ , which takes values from the set $\\{0,1\\}$ . If the   \n171 adversary chooses not to corrupt the result, we have $c_{t}=0$ . Otherwise, we have $c_{t}=1$ , which means   \n172 adversarial feedback in this round. As a result, the agent will observe a filpped preference label, i.e.,   \n173 the observation $o_{t}=1-l_{t}$ . We define $C$ as the total level of adversarial feedback, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\textstyle\\sum_{t=1}^{T}c_{t}\\leq C.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "174 Remark 3.3. Adversarial corruption has been firstly studied in bandits (Lykouris et al., 2018), where   \n175 in each round $t$ , the agent selects an action $a_{t}$ and the environment generates a numerical reward   \n176 $\\boldsymbol{r}_{t}(\\boldsymbol{a}_{t})$ . The adversary observes the reward and returns a corrupted reward $\\bar{r}_{t}$ . The corruption level   \n177 $C$ is defined by $\\begin{array}{r}{\\sum_{t=1}^{T}|r_{t}(a_{t})-\\bar{r}_{t}|\\leq C}\\end{array}$ . Compared with the continuous perturbation of rewards   \n178 in bandits, the a dversary\u2019s label flipping attack method in our model is quite different. The cost   \n179 of obtaining adversarial feedback is uniformly 1, unlike in bandits where the cost depends on the   \n180 intensity of the perturbation. Additionally, adversarial feedback in our setting involves comparing two   \n181 arms, whereas in bandits it pertains to the reward of a single arm. The only previous work that studied   \n182 label-filpping is (Agarwal et al., 2021), where the adversary cannot observe the action selected by the   \n183 agent. In contrast, our setting focuses on scenarios where this information is available to adversaries,   \n184 which is common in many real-life applications.   \n185 As the context is changing, the optimal action is different in each round, denoted by $a_{t}^{*}~=$   \n186 $\\operatorname{argmax}_{a\\in\\mathcal{A}}r^{*}(x_{t},a)$ . The goal of our algorithm is to minimize the cumulative gap between the   \n187 rewards of both selected actions and the optimal action ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Regret}(T)=\\sum_{t=1}^{T}\\!2r^{*}(x_{t},a_{t}^{*})-r^{*}(x_{t},a_{t})-r^{*}(x_{t},b_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "188 This regret definition is the same as that in Saha (2021) and the average regret defined in Bengs et al.   \n189 (2022). It is typically stronger than weak regret defined in Bengs et al. (2022), which only considers   \n190 the reward gap of the better action. ", "page_idx": 4}, {"type": "text", "text": "191 4 Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "192 In this section, we present our new algorithm RCDB, designed for learning contextual linear dueling   \n193 bandits. The main algorithm is illustrated in Algorithm 1. At a high level, we incorporate uncertainty  \n194 dependent weighting into the Maximum Likelihood Estimator (MLE) to counter adversarial feedback.   \n195 Specifically, in each round $t\\in[T]$ , we construct the estimator of parameter $\\pmb{\\theta}$ by solving the following   \n196 equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda\\kappa\\pmb{\\theta}+\\sum_{i=1}^{t-1}w_{i}\\big(\\sigma(\\phi_{i}^{\\top}\\pmb{\\theta})-o_{i}\\big)\\pmb{\\phi}_{i}=\\mathbf{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "197 where we denote $\\phi_{i}=\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})$ for simplicity, $w_{i}$ is the uncertainty weight we are going   \n198 to choose. To obtain an intuitive understanding of our weight, we consider any action-observation   \n199 sequence $(x_{1},a_{1},b_{1},o_{1},x_{2},a_{2},b_{2},o_{2},\\ldots,x_{t},a_{t},b_{t},o_{t})$ up to round $t$ . For simplicity, we denote   \n200 $\\mathcal{F}_{t}=\\sigma(x_{1},a_{1},b_{1},o_{1},x_{2},a_{2},b_{2},o_{2},.~.~,x_{t},a_{t},b_{t})$ as the flitration. Suppose the estimated parameter   \n201 $\\pmb{\\theta}_{t}$ is the solution to the unweighted version equation of (4.1), i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda\\kappa\\pmb{\\theta}_{t}+\\sum_{i=1}^{t}\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\mathopen{}\\mathclose\\bgroup\\left(\\sigma(\\phi_{i}^{\\top}\\pmb{\\theta}_{t})-o_{i}\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\aftergroup\\egroup\\right)\\phi_{i}=\\bf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "202 When we receive $\\phi_{t}=\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})$ , the probability of receiving $l_{t}=1$ can be estimated   \n203 by $\\sigma(\\mathbf{\\boldsymbol{\\phi}}_{t}^{\\intercal}\\pmb{\\theta}_{t})$ . We consider the conditional variance of the estimated probability $\\sigma(\\mathbf{\\boldsymbol{\\phi}}_{t}^{\\intercal}\\pmb{\\theta}_{t})$ in round $t$ ,   \n204 i.e., $\\operatorname{Var}\\big[\\sigma(\\dot{\\phi}_{t}^{\\top}\\pmb{\\theta}_{t})|\\mathcal{F}_{t}\\big]$ , involving a posterior estimate of the prediction\u2019s variance. First, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\sigma(\\phi_{t}^{\\top}\\theta_{t})|\\mathcal{F}_{t}\\big]\\approx\\mathbb{E}\\big[\\sigma(\\phi_{t}^{\\top}\\theta^{*})+\\sigma^{\\prime}(\\phi_{t}^{\\top}\\theta^{*})\\phi_{t}^{\\top}(\\theta_{t}-\\theta^{*})|\\mathcal{F}_{t}\\big]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\big[\\underbrace{\\sigma(\\phi_{t}^{\\top}\\theta^{*})-\\sigma^{\\prime}(\\phi_{t}^{\\top}\\theta^{*})\\phi_{t}^{\\top}\\theta^{*}}_{\\mathcal{F}_{t}-\\mathrm{measurable}}|\\mathcal{F}_{t}\\big]+\\mathbb{E}\\big[\\sigma^{\\prime}(\\phi_{t}^{\\top}\\theta^{*})\\phi_{t}^{\\top}\\theta_{t}|\\mathcal{F}_{t}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "205 Moreover, using the Taylor\u2019s expansion to (4.2), we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{0}=\\lambda\\kappa\\theta_{t}+\\sum_{i=1}^{t}(\\sigma(\\phi_{i}^{\\top}\\theta_{t})-o_{i})\\phi_{i}}\\\\ &{\\approx\\Big(\\lambda\\kappa\\mathbf{I}+\\sum_{i=1}^{t}\\sigma^{\\prime}(\\phi_{i}^{\\top}\\theta^{*})\\phi_{i}\\phi_{i}^{\\top}\\Big)\\theta_{t}+\\sum_{i=1}^{t}\\big(\\sigma(\\phi_{i}^{\\top}\\theta^{*})-o_{i}\\big)\\phi_{i}-\\sum_{i=1}^{t}\\sigma^{\\prime}(\\phi_{i}^{\\top}\\theta^{*})\\phi_{i}\\phi_{i}^{\\top}\\theta^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "206 Let $\\begin{array}{r}{\\mathbf{\\boldsymbol{\\Lambda}}_{t}=\\lambda\\kappa\\mathbf{I}+\\sum_{i=1}^{t}\\sigma^{\\prime}(\\phi_{i}^{\\top}\\pmb{\\theta}^{*})\\phi_{i}\\phi_{i}^{\\top}}\\end{array}$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t}\\approx\\Lambda_{t}^{-1}\\Big[\\sum_{i=1}^{t}\\sigma^{\\prime}(\\phi_{i}^{\\top}\\theta^{*})\\phi_{i}\\phi_{i}^{\\top}\\theta^{*}-\\sum_{i=1}^{t}\\big(\\sigma(\\phi_{i}^{\\top}\\theta^{*})-o_{i}\\big)\\phi_{i}\\Big]}\\\\ &{\\quad=\\underbrace{\\Lambda_{t}^{-1}\\Big[\\sum_{i=1}^{t}\\sigma^{\\prime}(\\phi_{i}^{\\top}\\theta^{*})\\phi_{i}\\phi_{i}^{\\top}\\theta^{*}-\\sum_{i=1}^{t-1}\\big(\\sigma(\\phi_{i}^{\\top}\\theta^{*})-o_{i}\\big)\\phi_{i}-\\sigma(\\phi_{t}^{\\top}\\theta^{*})\\Big]}_{\\mathcal{F}_{t}-\\mathrm{measurable}}+o_{t}\\Lambda_{t}^{-1}\\phi_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "207 Therefore, the variance of the estimated preference probability can be approximated by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}\\big[\\sigma(\\phi_{t}^{\\top}\\theta_{t})\\big|\\mathcal{F}_{t}\\big]=\\mathbb E\\big[\\big(\\sigma(\\phi_{t}^{\\top}\\theta_{t})-\\mathbb E\\big[\\sigma(\\phi_{t}^{\\top}\\theta_{t})\\big|\\mathcal{F}_{t}\\big]\\big)^{2}\\big|\\mathcal{F}_{t}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\approx\\mathbb E\\Big[\\Big(\\mathbb E\\big[\\sigma_{t}\\sigma^{\\prime}(\\phi_{t}^{\\top}\\theta^{*})\\phi_{t}^{\\top}\\Lambda_{t}^{-1}\\phi_{t}\\big|\\mathcal{F}_{t}\\Big]\\Big)^{2}\\Big|\\mathcal{F}_{t}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\quad\\leq\\mathbb E\\big[o_{t}[\\sigma^{\\prime}(\\phi_{t}^{\\top}\\theta^{*})]^{2}\\|\\phi_{t}\\|_{\\Lambda_{t}^{-1}}^{2}\\vert\\mathcal{F}_{t}\\big]\\leq[\\sigma^{\\prime}(\\phi_{t}^{\\top}\\theta^{*})]^{2}\\|\\phi_{t}\\|_{\\Lambda_{t}^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "208 where the first inequality holds due to the Jensen\u2019s inequality and $o_{t}^{2}=o_{t}$ , and the last inequality   \n209 holds due to $\\mathbb{E}[o_{t}|\\mathcal{F}_{t}]\\le1$ . Using $\\sigma^{\\prime}(\\phi_{t}^{\\top}\\theta^{*})\\leq1,\\phi_{t}^{\\top}\\theta^{*}\\leq1,\\Lambda_{t}\\geq\\kappa\\Sigma_{t+1}\\geq\\kappa\\Sigma_{t}$ , we can see that   \n210 $\\mathrm{Var}\\big[\\sigma(\\phi_{t}^{\\top}\\pmb\\theta_{t})|\\mathcal F_{t}\\big]\\le\\kappa^{-1}\\|\\phi_{t}\\|_{\\Sigma_{t}^{-1}}^{2}$ . Since higher variance leads to larger uncertainty, which harms   \n211 the credibility of the data, it is natural to assign a smaller weight to the data with high uncertainty.   \n212 Thus, we choose the weight to cancel out the uncertainty as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{i}=\\operatorname*{min}\\lbrace1,\\alpha/\\|\\phi_{i}\\|_{\\Sigma_{i}^{-1}}\\rbrace,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "213 where $\\alpha/\\|\\phi_{i}\\|_{\\mathbf{\\pmb{\\Sigma}}_{i}^{-1}}$ normalizes the variance of the estimated probability. To prevent excessively   \n214 large weights, we apply truncation to this value. A similar weight has been used in He et al. (2022)   \n215 for linear contextual bandits under corruption. Different from their setting where the weight is an   \n216 estimate of the variance of the linear model, our weight is an estimate of a generalized linear model.   \n217 Furthermore, by selecting a proper threshold parameter, e.g., $\\alpha=\\sqrt{d}/C$ , the weighted MLE shares   \n218 the same confidence radius with that of the no-adversary scenario.   \n219 After constructing the estimator $\\theta_{t}$ from the weighted MLE, the sum of the estimated reward for   \n220 each duel $(a,b)$ can be calculated as $\\bigl(\\phi(x_{t},a)+\\bar{\\phi}(x_{t},b)\\bigr)^{\\top}\\pmb{\\theta}_{t}$ . To encourage the exploration of duel   \n221 $(a,b)$ with high uncertainty during the learning process, we introduce an exploration bonus with the   \n222 following $\\beta\\|\\bar{\\phi}(x_{t},a)-\\bar{\\phi}(x_{t},b)\\|_{\\Sigma_{t}^{-1}}$ , which follows a similar spirit to the bonus term in the context   \n223 of linear bandit problems (Abbasi-Yadkori et al., 2011). However, the reward term and the bonus term   \n224 exhibit different combinations of the feature maps $\\phi(\\boldsymbol{x}_{t},\\boldsymbol{a})$ and $\\phi(\\boldsymbol{x}_{t},\\boldsymbol{b})$ , which is the key difference   \n225 between bandits and dueling bandits. The selection of action pairs $(a,b)$ is subsequently determined   \n226 by maximizing the estimated reward with the exploration bonus term, i.e., ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\big(\\phi(x_{t},a)+\\phi(x_{t},b)\\big)^{\\top}\\pmb\\theta_{t}+\\beta\\big|\\big|\\phi(x_{t},a)-\\phi(x_{t},b)\\big|\\big|_{\\Sigma_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "227 More discussion about the selection rule was discussed in Appendix A of Di et al. (2023). ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Robust Contextual Dueling Bandit (RCDB) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Require: $\\alpha>0$ , Regularization parameter $\\lambda$ , confidence radius $\\beta$ .   \n2: for $t=1,\\dots,T$ do   \n3: Compute $\\begin{array}{r}{\\dot{\\Sigma_{t}}=\\lambda\\mathbf{I}+\\sum_{i=1}^{t-1}\\!w_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}.}\\end{array}$   \n4: Calculate the MLE $\\pmb{\\theta}_{t}$ by solving the following equation:   \n$\\begin{array}{r}{\\lambda\\kappa\\theta+\\sum_{i=1}^{t-1}w_{i}\\Big[\\sigma\\Big(\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\theta\\Big)-o_{i}\\Big]\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)=\\mathbf{0}.}\\end{array}$ (4.4)   \n5: Observe the context vector $x_{t}$ .   \n6: Choose $\\begin{array}{r}{a_{t},b_{t}=\\operatorname*{argmax}_{a,b}\\Big\\{\\big(\\phi(x_{t},a)+\\phi(x_{t},b)\\big)^{\\top}\\theta_{t}+\\beta\\big\\|\\phi(x_{t},a)-\\phi(x_{t},b)\\big\\|_{\\Sigma_{t}^{-1}}\\Big\\}.}\\end{array}$   \n7: The adversary sees the feedback ${l_{t}=\\mathbb{1}(a_{t}\\succ b_{t})}$ and decides the indicator $c_{t}$ . Observe $o_{t}=l_{t}$   \nwhen $c_{t}=0$ , otherwise observe $o_{t}=1-l_{t}$ .   \n8: Set weight $w_{t}$ as (4.3).   \n9: end for ", "page_idx": 5}, {"type": "text", "text": "228 5 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "229 5.1 Known Number of Adversarial Feedback ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "230 At the center of our algorithm design is the uncertainty-weighted MLE. When faced with adversarial   \n231 feedback, the estimation error of the weighted MLE $\\theta_{t}$ can be characterized by the following lemma.   \n232 Lemma 5.1. If we set $\\beta=\\sqrt{\\lambda}B+\\big(\\alpha C+\\sqrt{d\\log((1+2T/\\lambda)/\\delta)}\\big)/\\kappa$ , then with probability at   \n233 least $1-\\delta$ , for any $t\\in[T]$ , we have ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\|\\pmb{\\theta}_{t}-\\pmb{\\theta}^{*}\\right\\|_{\\pmb{\\Sigma}_{t}}\\leq\\beta.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "234 Remark 5.2. If we set $\\alpha=(\\sqrt{d}+\\sqrt{\\lambda}B)/C$ , then the bonus radius $\\beta$ has no direct dependency on   \n235 the number of adversarial feedback $C$ . This observation plays a key role in proving the adversarial   \n236 term in the regret without polynomial dependence on the total number of rounds $T$ .   \n237 With Lemma 5.1, we can present the following regret guarantee of our algorithm RCDB in the dueling   \n238 bandit framework.   \n239 Theorem 5.3. Under Assumption 3.1 and 3.2, let $0<\\delta<1$ , the total number of adversarial feedback   \n240 be $C$ . If we set the bonus radius to be ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta=\\sqrt{\\lambda}B+\\big(\\alpha C+\\sqrt{d\\log((1+2T/\\lambda)/\\delta)}\\big)/\\kappa,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "241 then with probability at least $1-\\delta$ , the regret in the first $t$ rounds can be upper bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Regret}(T)\\leq4\\big(\\sqrt{\\lambda}B+\\alpha C/\\kappa\\big)\\sqrt{d T\\log(1+2T/\\lambda)}}&{}\\\\ {+\\,4d\\big(\\sqrt{T}/\\kappa+\\sqrt{\\lambda}B/\\alpha+4C/\\kappa\\big)\\log\\big((1+2T/\\lambda)/\\delta\\big)}&{}\\\\ {+\\,4d^{1.5}\\sqrt{\\log^{3}\\big((1+2T/\\lambda)/\\delta\\big)}/(\\alpha\\kappa).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "242 Moreover, if we set $\\alpha=(\\sqrt{d}+\\sqrt{\\lambda}B)/C,\\lambda=1/B^{2}$ , the regret upper bound can be simplified to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)=\\widetilde{O}\\big(d\\sqrt{T}/\\kappa+d C/\\kappa\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "243 Remark 5.4. Our regret bound consists of two terms. The first one is a $C$ -independent term $\\widetilde{O}(d\\sqrt{T})$ ,   \n244 which matches the lower bound $\\widetilde\\Omega(d\\sqrt{T})$ proved in Bengs et al. (2022). This indicates that our result   \n245 is optimal in scenarios without adversarial feedback $C=0$ ). Additionally, our result include\u221as an   \n246 additive term that is linearly dependent on the number of adversarial feedback $C$ . When $C=O({\\sqrt{T}})$ ,   \n247 the order of regret will be the same as the stochastic setting. It indicates the robustness of our algorithm   \n248 to adversarial feedback. Additionally, the following theorem we present establishes a lower bound   \n249 for this adversarial term, indicating that our dependency on the number of adversarial feedback $C$   \n250 and the context dimension $d$ is also optimal.   \n251 Theorem 5.5. For any dimension $d$ , there exists an instance of dueling bandits with $|{\\mathcal{A}}|=d_{!}$ , such   \n252 that any algorithm with the knowledge of the number of adversarial feedback $C$ must incur $\\Omega(d C)$   \n253 regret with probability at least $1/2$ .   \n254 Remark 5.6. The proof of Theorem 5.5 follows Bogunovic et al. (2021). In the constructed instances,   \n255 only one action has reward 1, while others have 0. Compared with linear bandits, where the feedback   \n256 is an exact reward, dueling bandits deal with the comparison between a pair of actions. A critical   \n257 observation from our preference model, as formulated in (3.1), is that two actions with identical   \n258 rewards result in a pair that is challenging to differentiate. The lower bound can be proved by   \n259 corrupting every comparison into a random guess until the total times of adversarial feedback have   \n260 been used up. For detailed proof, please refer to Section B.2. Our proved lower bound $\\Omega(d C)$ shows   \n261 that our result is nearly optimal because of the linear dependency on $C,d$ and only logarithmic   \n262 dependency on the total number of rounds $T$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "263 5.2 Unknown Number of Adversarial Feedback ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "264 In our previous analysis, the selection of parameters depends on having prior knowledge of the total   \n265 number of adversarial feedback $C$ . In this subsection, we extend our previous result to address   \n266 the challenge posed by an unknown number of adversarial feedback $C$ . Our approach to tackle   \n267 this uncertainty follows He et al. (2022), we introduce an adversarial tolerance threshold $\\bar{C}$ for the   \n268 adversary count. This threshold can be regarded as an optimistic estimator of the actual number of   \n269 adversarial feedback $C$ . Under this situation, the subsequent theorem provides an upper bound for   \n270 regret of Algorithm 1 in the case of an unknown number of adversarial feedback $C$ . ", "page_idx": 6}, {"type": "text", "text": "271 Theorem 5.7. Under Assumptions 3.1 and 3.2, if we set the the confidence radius as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\beta=\\sqrt{\\lambda}B+\\left[\\alpha\\bar{C}+\\sqrt{d\\log\\left((1+2T/\\lambda)/\\delta\\right)}\\right]/\\kappa,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "272 with the pre-defined adversarial tolerance threshold $\\bar{C}$ and $\\alpha=(\\sqrt{d}\\!+\\!\\sqrt{\\lambda}B)/\\bar{C}$ , then with probability   \n273 at least $1-\\delta$ , the regret of Algorithm 1 can be upper bounded as following:   \n274 \u2022 If the actual number of adversarial feedback $C$ is smaller than the adversarial tolerance threshold   \n275 $\\bar{C}$ , then we have ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)=\\widetilde{\\cal O}\\big(d\\sqrt{T}/\\kappa+d\\bar{C}/\\kappa\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "276 \u2022 If the actual number of adversarial feedback $C$ is larger than the adversarial tolerance threshold $\\bar{C}$ ,   \n277 then we have Regret $(T)=O(T)$ .   \n278 Remark 5.8. The COBE framework (Wei et al., 2022) converts any algorithm with the known   \n279 adversarial level to an algorithm in the unknown case. However, such a framework only works for   \n280 weak adversaries and does not work in our strong adversary setting. In fact, He et al. (2022) proved   \n281 that any algorithm cannot simultaneously achie\u221ave near-optimal regret when uncorrupted and maintain   \n282 sublinear regret with corruption level $C=\\Omega({\\sqrt{T}})$ . Therefore, there exists a trade-off between robust   \n283 adversarial defense \u221aand near-optimal algorithmic performance. Our algorith\u221am achieves the same   \n284 nearly optimal $\\widetilde{O}(d\\sqrt{T})$ regret as the no-adversary case even when $C=\\Theta({\\sqrt{T}})$ , which indicates   \n285 that our results are optimal in the presence of an unknown number of adversarial feedback. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "286 6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "287 6.1 Experiment Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "288 Preference Model. We study the effect of adversarial feedback with the preference model deter  \n289 mined by (3.1), where $\\sigma(x)=1/(1+e^{-x})$ . We randomly generate the underlying parameter in   \n290 $[-0.5,0.5]^{d}$ and normalize it to be a vector with $\\lVert\\pmb{\\theta}^{*}\\rVert_{2}=2$ . Then, we set it to be the underlying   \n291 parameter and construct the reward utilized in the preference model as $r^{*}(x,a)\\,=\\,\\langle\\pmb{\\theta}^{*},\\phi(x,a)\\rangle$ .   \n292 We set the action set $\\boldsymbol{\\mathcal{A}}=\\left\\{\\mathbf{\\Pi}-1/\\sqrt{d},1/\\sqrt{d}\\right\\}^{d}$ . For simplicity, we assume $\\phi(x,a)\\,=\\,a$ . In our   \n293 experiment, we set the dimension $d=5$ , with the size of action set $|{\\mathcal{A}}|=2^{d}=32$ .   \n294 Adversarial Attack Methods. We study the performance of our algorithm using different adversar  \n295 ial attack methods. We categorize the first two methods as \u201cweak\u201d primarily because the adversary in   \n296 these scenarios does not utilize information about the agent\u2019s actions. In contrast, we classify the   \n297 latter two methods as \u201cstrong\u201d attacks. In these cases, the adversary leverages a broader scope of   \n298 information, including knowledge of the actions selected by the agent and the true preference model.   \n299 This enables it to devise more targeted adversarial methods.   \n300 \u2022 \u201cGreedy Attack\": The adversary will flip the preference label for the first $C$ rounds. After that, it   \n301 will not corrupt the result anymore.   \n302 \u2022 \u201cRandom Attack\": In each round, the adversary will filp the preference label with the probability of   \n303 $0<p<1$ , until the times of adversarial feedback reach $C$ .   \n304 \u2022 \u201cAdversarial Attack\": The adversary can have access to the true preference model. It will only filp   \n305 the preference label when it aligns with the preference model, i.e., the probability for the preference   \n306 model to make that decision is larger than 0.5, until the times of adversarial feedback reach $C$ .   \n307 \u2022 \u201cMisleading Attack\": The adversary selects a suboptimal action. It will make sure this arm is   \n308 always the winner in the comparison until the times of adversarial feedback reach $C$ . In this way, it   \n309 will mislead the agent to believe this action is the optimal one.   \n310 Experiment Setup. For each experiment instance, we simulate the interaction with the environment   \n311 for $T=2000$ rounds. In each round, the feedback for the action pair selected by the algorithm is   \n312 generated according to the defined preference model. Subsequently, the adversary observes both the   \n313 selected actions and their corresponding feedback and then engages in one of the previously described   \nadversarial attack methods. We report the regret defined in (3.2) averaged across 10 random runs. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "Wmodg5UiEd/tmp/7e63025103be672548ca478b2f1862fe2cad572a67c96ebe81ade2dab4518c0a.jpg", "img_caption": ["Figure 1: Comparison of RCDB (Our Algorithm 1), MaxInp (Saha, 2021), CoLSTIM (Bengs et al., 2022) and MaxPairUCB (Di et al., 2023). We report the cumulative regret with various adversarial attack methods (Greedy, Random, Adversarial, Misleading). For the baselines, the parameters are carefully tuned to achieve better results with different attack methods. The total number of adversarial feedback is $C=\\lceil\\sqrt{T}\\rceil$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "314 315 6.2 Performance Comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "316 We first introduce the algorithms studied in this section. ", "page_idx": 7}, {"type": "text", "text": "317 \u2022 MaxInP: Maximum Informative Pair by Saha (2021). It involves maintaining a standard MLE.   \n318 With the estimated model, it then identifies a set of promising arms possible to beat the rest. The   \n319 selection of arm pairs is then strategically designed to maximize the uncertainty in the difference   \n320 between the two arms within this promising set, referred to as \u201cmaximum informative\u201d.   \n321 \u2022 CoLSTIM: The method by Bengs et al. (2022). It involves maintaining a standard MLE for the   \n322 estimated model. Based on this model, the first arm is selected as the one with the highest estimated   \n323 reward, implying it is the most likely to prevail over competitors. The second arm is selected to be   \n324 the first arm\u2019s toughest competitor, with an added uncertainty bonus.   \n325 \u2022 MaxPairUCB: This algorithm was proposed in Di et al. (2023). It uses the regularized MLE to   \n326 estimate the parameter $\\theta^{*}$ . Then it selects the actions based on a symmetric action selection rule,   \n327 i.e. the actions with the largest estimated reward plus some uncertainty bonus.   \n328 \u2022 RCDB: Algorithm 1 proposed in this paper. The key difference from the other algorithms is the   \n329 use of uncertainty weight in the calculation of MLE (4.4). The we use the same symmetric action   \n330 selection rule as MaxPairUCB. Our experiment results show that the uncertainty weight is critical   \n331 in the face of adversarial feedback.   \n332 Our results are demonstrated in Figure 1. In Figure 1(a) and Figure 1(b), we observe scenarios where   \n333 the adversary is \u201cweak\u201d due to the lack of access to information regarding the selected actions and the   \n334 underlying preference model. Notably, in these situations, our algorithm RCDB outperforms all other   \n335 baseline algorithms, demonstrating its robustness. Among the other algorithms, CoLSTIM performs   \n336 as the strongest competitor.   \n337 In Figure 1(c), the adversary employs a \u2019stronger\u2019 adversarial method. Due to the inherent randomness   \n338 of the model, some labels may naturally be \u2019incorrect\u2019. An adversary with knowledge of the selected   \n339 actions and the preference model can strategically neglect these naturally incorrect labels and   \n340 selectively flip the others. This method proves catastrophic for algorithms to learn the true model,   \n341 as it results in the agent encountering only incorrect preference labels at the beginning. Our results   \n342 indicate that this leads to significantly higher regret. However, it\u2019s noteworthy that our algorithm   \n343 RCDB demonstrates considerable robustness.   \n344 In Figure 1(d), the adversary employs a strategy aimed at misleading algorithms into believing a   \n345 suboptimal action is the best choice. The algorithm CoLSTIM appears to be the most susceptible to   \n346 being cheated by this method. Despite the deployment of \u2019strong\u2019 adversarial methods, as shown   \n347 in both Figure 1(c) and Figure 1(d), our algorithm, RCDB, consistently demonstrates exceptional   \n348 robustness against these attacks. A significant advantage of RCDB lies in that our parameter is selected   \n349 solely based on the number of adversarial feedback $C$ , irrespective of the nature of the adversarial   \n350 methods employed. This contrasts with other algorithms where parameter tuning must be specifically   \nadapted for each distinct adversarial method. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "Wmodg5UiEd/tmp/e7a0aad1d6f945a7c46c4b42915284e4ddfe341e39bcac62debd27b666f8ed55.jpg", "img_caption": ["Figure 2: The relationship between cumulative regret and the number of adversarial feedback $C$ . For this specific experiment, we employ the \"greedy attack\" method to generate the adversarial feedback. $C$ is selected from the set [20, 40, 60, 80, 100, 120, 140, 160, 180, 200] (10 adversarial levels). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "352 6.3 Robustness to Different Numbers of Adversarial Feedback   \n353 In this section, we test the performance of algorithms with increasing times of adversarial feedback.   \n354 Our results show a linear dependency on the number of adversarial feedback $C$ , which is consistent   \n355 with the theoretical results we have proved in Theorem 5.3 and 5.5. In comparison to other algorithms,   \n356 RCDB demonstrates superior robustness against adversarial feedback, as evidenced by its notably   \n357 smaller regret. ", "page_idx": 8}, {"type": "text", "text": "358 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "59 In this paper, we focus on the contextual dueling bandit problem from adversarial feedback. We   \n60 introduce a novel algorithm, RCDB, which utilizes an uncertainty-weighted Maximum Likelihood   \n61 Estimator (MLE) approach. This algorithm not only achieves optimal theoretical results in scenarios   \n62 with and without adversarial feedback but also demonstrates superior performance with synthetic   \n63 data. For future direction, we aim to extend our uncertainty-weighted method to encompass more   \n64 general settings involving preference-based data. A particularly promising future direction of our   \n65 research lies in addressing adversarial feedback within the process of aligning large language models   \n66 using Reinforcement Learning from Human Feedback (RLHF).   \n367 Limitations. We assume that the reward is linear with respect to some known feature maps. Although   \n368 this setting is common in the literature, we observe that some recent works on dueling bandits can   \n369 deal with nonlinear rewards (Li et al., 2024). Therefore, it\u2019s possible to extend our results to a more   \n370 general setting. Another assumption concerns the lower bound of the derivative of the link function.   \n371 Notably, in the logistic bandit model, which shares similarities with our setting through Bernoulli   \n372 variable\u221as, some work (Abeille et al., 2021; Faury et al., 2022) can improve the dependency of $\\kappa$ from   \n373 $1/\\kappa$ to $\\sqrt{\\kappa}$ . A similar improvement might be achieved in our setting as well. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "374 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "375 ABBASI-YADKORI, Y., P\u00c1L, D. and SZEPESV\u00c1RI, C. (2011). Improved algorithms for linear   \n376 stochastic bandits. In Advances in Neural Information Processing Systems.   \n377 ABEILLE, M., FAURY, L. and CALAUZ\u00c8NES, C. (2021). Instance-wise minimax-optimal algorithms   \n378 for logistic bandits. In International Conference on Artificial Intelligence and Statistics. PMLR.   \n379 AGARWAL, A., AGARWAL, S. and PATIL, P. (2021). Stochastic dueling bandits with adversarial   \n380 corruption. In Algorithmic Learning Theory. PMLR.   \n381 AGRAWAL, S. and GOYAL, N. (2012). Analysis of thompson sampling for the multi-armed bandit   \n382 problem. In Conference on learning theory. JMLR Workshop and Conference Proceedings.   \n383 AILON, N., KARNIN, Z. and JOACHIMS, T. (2014). Reducing dueling bandits to cardinal bandits.   \n384 In International Conference on Machine Learning. PMLR.   \n385 AUER, P. (2002). Using confidence bounds for exploitation-exploration trade-offs. Journal of   \n386 Machine Learning Research 3 397\u2013422.   \n387 AUER, P., CESA-BIANCHI, N. and FISCHER, P. (2002a). Finite-time analysis of the multiarmed   \n388 bandit problem. Machine Learning 47 235\u2013256.   \n389 AUER, P., CESA-BIANCHI, N., FREUND, Y. and SCHAPIRE, R. E. (2002b). The nonstochastic   \n390 multiarmed bandit problem. SIAM journal on computing 32 48\u201377.   \n391 AUER, P. and CHIANG, C.-K. (2016). An algorithm with nearly optimal pseudo-regret for both   \n392 stochastic and adversarial bandits. In Conference on Learning Theory. PMLR.   \n393 BALSUBRAMANI, A., KARNIN, Z., SCHAPIRE, R. E. and ZOGHI, M. (2016). Instance-dependent   \n394 regret bounds for dueling bandits. In Conference on Learning Theory. PMLR.   \n395 BENGS, V., SAHA, A. and H\u00dcLLERMEIER, E. (2022). Stochastic contextual dueling bandits under   \n396 linear stochastic transitivity models. In International Conference on Machine Learning. PMLR.   \n397 BOGUNOVIC, I., LOSALKA, A., KRAUSE, A. and SCARLETT, J. (2021). Stochastic linear bandits   \n398 robust to adversarial attacks. In International Conference on Artificial Intelligence and Statistics.   \n399 PMLR.   \n400 BUBECK, S., CESA-BIANCHI, N. ET AL. (2012). Regret analysis of stochastic and nonstochastic   \n401 multi-armed bandit problems. Foundations and Trends\u00ae in Machine Learning 5 1\u2013122.   \n402 BUBECK, S. and SLIVKINS, A. (2012). The best of both worlds: Stochastic and adversarial bandits.   \n403 In Conference on Learning Theory. JMLR Workshop and Conference Proceedings.   \n404 CESA-BIANCHI, N. and LUGOSI, G. (2006). Prediction, learning, and games. Cambridge university   \n405 press.   \n406 DI, Q., JIN, T., WU, Y., ZHAO, H., FARNOUD, F. and GU, Q. (2023). Variance-aware regret bounds   \n407 for stochastic contextual dueling bandits. arXiv preprint arXiv:2310.00968 .   \n408 DING, Q., HSIEH, C.-J. and SHARPNACK, J. (2022). Robust stochastic linear contextual bandits   \n409 under adversarial attacks. In International Conference on Artificial Intelligence and Statistics.   \n410 PMLR.   \n411 DUD\u00cdK, M., HOFMANN, K., SCHAPIRE, R. E., SLIVKINS, A. and ZOGHI, M. (2015). Contextual   \n412 dueling bandits. In Conference on Learning Theory. PMLR.   \n413 FALAHATGAR, M., HAO, Y., ORLITSKY, A., PICHAPATI, V. and RAVINDRAKUMAR, V. (2017).   \n414 Maxing and ranking with few assumptions. Advances in Neural Information Processing Systems   \n415 30.   \n416 FAURY, L., ABEILLE, M., CALAUZ\u00c8NES, C. and FERCOQ, O. (2020). Improved optimistic   \n417 algorithms for logistic bandits. In International Conference on Machine Learning. PMLR.   \n418 FAURY, L., ABEILLE, M., JUN, K.-S. and CALAUZ\u00c8NES, C. (2022). Jointly efficient and optimal   \n419 algorithms for logistic bandits. In International Conference on Artificial Intelligence and Statistics.   \n420 PMLR.   \n421 FILIPPI, S., CAPPE, O., GARIVIER, A. and SZEPESV\u00c1RI, C. (2010). Parametric bandits: The   \n422 generalized linear case. Advances in Neural Information Processing Systems 23.   \n423 GAJANE, P., URVOY, T. and CL\u00c9ROT, F. (2015). A relative exponential weighing algorithm for   \n424 adversarial utility-based dueling bandits. In International Conference on Machine Learning.   \n425 PMLR.   \n426 GUPTA, A., KOREN, T. and TALWAR, K. (2019). Better algorithms for stochastic bandits with   \n427 adversarial corruptions. In Conference on Learning Theory. PMLR.   \n428 HE, J., ZHOU, D., ZHANG, T. and GU, Q. (2022). Nearly optimal algorithms for linear contextual   \n429 bandits with adversarial corruptions. Advances in Neural Information Processing Systems 35   \n430 34614\u201334625.   \n431 HECKEL, R., SIMCHOWITZ, M., RAMCHANDRAN, K. and WAINWRIGHT, M. (2018). Approximate   \n432 ranking from pairwise comparisons. In International Conference on Artificial Intelligence and   \n433 Statistics. PMLR.   \n434 JAMIESON, K., KATARIYA, S., DESHPANDE, A. and NOWAK, R. (2015). Sparse dueling bandits.   \n435 In Artificial Intelligence and Statistics. PMLR.   \n436 KALYANAKRISHNAN, S., TEWARI, A., AUER, P. and STONE, P. (2012). Pac subset selection in   \n437 stochastic multi-armed bandits. In ICML, vol. 12.   \n438 KOMIYAMA, J., HONDA, J., KASHIMA, H. and NAKAGAWA, H. (2015). Regret lower bound and   \n439 optimal algorithm in dueling bandit problem. In Conference on learning theory. PMLR.   \n440 KOMIYAMA, J., HONDA, J. and NAKAGAWA, H. (2016). Copeland dueling bandit problem:   \n441 Regret lower bound, optimal algorithm, and computationally efficient algorithm. In International   \n442 Conference on Machine Learning. PMLR.   \n443 KUROKI, Y., RUMI, A., TSUCHIYA, T., VITALE, F. and CESA-BIANCHI, N. (2023). Best-of-both  \n444 worlds algorithms for linear contextual bandits. arXiv preprint arXiv:2312.15433 .   \n445 LAI, T. L. (1987). Adaptive treatment allocation and the multi-armed bandit problem. The annals of   \n446 statistics 1091\u20131114.   \n447 LAI, T. L., ROBBINS, H. ET AL. (1985). Asymptotically efficient adaptive allocation rules. Advances   \n448 in applied mathematics 6 4\u201322.   \n449 LATTIMORE, T. and SZEPESV\u00c1RI, C. (2020). Bandit Algorithms. Cambridge University Press.   \n450 LEE, C.-W., LUO, H., WEI, C.-Y., ZHANG, M. and ZHANG, X. (2021). Achieving near instance  \n451 optimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously. In   \n452 International Conference on Machine Learning. PMLR.   \n453 LI, L., LU, Y. and ZHOU, D. (2017). Provably optimal algorithms for generalized linear contextual   \n454 bandits. In International Conference on Machine Learning. PMLR.   \n455 LI, X., ZHAO, H. and GU, Q. (2024). Feel-good thompson sampling for contextual dueling bandits.   \n456 arXiv preprint arXiv:2404.06013 .   \n457 LI, Y., LOU, E. Y. and SHAN, L. (2019). Stochastic linear optimization with adversarial corruption.   \n458 arXiv preprint arXiv:1909.02109 .   \n459 LYKOURIS, T., MIRROKNI, V. and PAES LEME, R. (2018). Stochastic bandits robust to adversarial   \n460 corruptions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing.   \n461 OUYANG, L., WU, J., JIANG, X., ALMEIDA, D., WAINWRIGHT, C., MISHKIN, P., ZHANG,   \n462 C., AGARWAL, S., SLAMA, K., RAY, A. ET AL. (2022). Training language models to follow   \n463 instructions with human feedback. Advances in Neural Information Processing Systems 35 27730\u2013   \n464 27744.   \n465 RAMAMOHAN, S. Y., RAJKUMAR, A. and AGARWAL, S. (2016). Dueling bandits: Beyond   \n466 condorcet winners to general tournament solutions. Advances in Neural Information Processing   \n467 Systems 29.   \n468 SAHA, A. (2021). Optimal algorithms for stochastic contextual preference bandits. Advances in   \n469 Neural Information Processing Systems 34 30050\u201330062.   \n470 SAHA, A. and GAILLARD, P. (2022). Versatile dueling bandits: Best-of-both world analyses for   \n471 learning from relative preferences. In International Conference on Machine Learning. PMLR.   \n472 SAHA, A., KOREN, T. and MANSOUR, Y. (2021). Adversarial dueling bandits. In International   \n473 Conference on Machine Learning. PMLR.   \n474 SAHA, A. and KRISHNAMURTHY, A. (2022). Efficient and optimal algorithms for contextual dueling   \n475 bandits under realizability. In International Conference on Algorithmic Learning Theory. PMLR.   \n476 SEKHARI, A., SRIDHARAN, K., SUN, W. and WU, R. (2023). Contextual bandits and imitation   \n477 learning via preference-based active queries. arXiv preprint arXiv:2307.12926 .   \n478 SELDIN, Y. and LUGOSI, G. (2017). An improved parametrization and analysis of the exp3 $^{++}$   \n479 algorithm for stochastic and adversarial bandits. In Conference on Learning Theory. PMLR.   \n480 SELDIN, Y. and SLIVKINS, A. (2014). One practical algorithm for both stochastic and adversarial   \n481 bandits. In International Conference on Machine Learning. PMLR.   \n482 WEI, C.-Y., DANN, C. and ZIMMERT, J. (2022). A model selection approach for corruption robust   \n483 reinforcement learning. In International Conference on Algorithmic Learning Theory. PMLR.   \n484 WU, H. and LIU, X. (2016). Double thompson sampling for dueling bandits. Advances in neural   \n485 information processing systems 29.   \n486 WU, Y., JIN, T., LOU, H., FARNOUD, F. and GU, Q. (2023). Borda regret minimization for   \n487 generalized linear dueling bandits. arXiv preprint arXiv:2303.08816 .   \n488 XIONG, W., DONG, H., YE, C., ZHONG, H., JIANG, N. and ZHANG, T. (2023). Gibbs sam  \n489 pling from human feedback: A provable kl-constrained framework for rlhf. arXiv preprint   \n490 arXiv:2312.11456 .   \n491 YE, C., XIONG, W., GU, Q. and ZHANG, T. (2023). Corruption-robust algorithms with uncertainty   \n492 weighting for nonlinear contextual bandits and markov decision processes. In International   \n493 Conference on Machine Learning. PMLR.   \n494 YU, T., QUILLEN, D., HE, Z., JULIAN, R., HAUSMAN, K., FINN, C. and LEVINE, S. (2020).   \n495 Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In   \n496 Conference on robot learning. PMLR.   \n497 YUE, Y., BRODER, J., KLEINBERG, R. and JOACHIMS, T. (2012). The $\\mathbf{k}$ -armed dueling bandits   \n498 problem. Journal of Computer and System Sciences 78 1538\u20131556.   \n499 ZHAO, H., ZHOU, D. and GU, Q. (2021). Linear contextual bandits with adversarial corruptions.   \n500 arXiv preprint arXiv:2110.12615 .   \n501 ZHU, H., YU, J., GUPTA, A., SHAH, D., HARTIKAINEN, K., SINGH, A., KUMAR, V. and   \n502 LEVINE, S. (2020). The ingredients of real-world robotic reinforcement learning. arXiv preprint   \n503 arXiv:2004.12570 .   \n504 ZIMMERT, J. and SELDIN, Y. (2019). An optimal algorithm for stochastic and adversarial bandits.   \n505 In The 22nd International Conference on Artificial Intelligence and Statistics. PMLR.   \n506 ZOGHI, M., KARNIN, Z. S., WHITESON, S. and DE RIJKE, M. (2015). Copeland dueling bandits.   \n507 Advances in neural information processing systems 28.   \n508 ZOGHI, M., WHITESON, S., MUNOS, R. and RIJKE, M. (2014). Relative upper confidence bound   \n509 for the k-armed dueling bandit problem. In International conference on machine learning. PMLR. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "510 Broader Impact ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "511 This paper studies contextual dueling bandits with adversarial feedback. Our primary objective is   \n512 to propel advancements in bandit theory by introducing a more robust algorithm backed by solid   \n513 theoretical guarantees. The uncertainty-weighted approach we have developed for dueling bandits   \n514 holds significant potential to address the issue of adversarial feedback in preference-based data, which   \n515 could be instrumental in enhancing the robustness of generative models against adversarial attacks,   \n516 thereby contributing positively to the societal impact and reliability of machine learning applications. ", "page_idx": 12}, {"type": "text", "text": "517 A Roadmap of the Proof ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "518 A.1 Uncertainty-weighted MLE with Adversarial Feedback ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "519 In this section, we offer an overview of the proof for Lemma 5.1. The general proof idea for   \n520 the uncertainty-weighted MLE with adversarial feedback lies in decomposing the estimation error   \n521 into three terms, a stochastic error term, an adversarial term, and an additional regularization term.   \n522 Following the analysis of standard (weighted) MLE (Li et al., 2017), we introduce an auxiliary   \n523 function: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{t}(\\pmb{\\theta})=\\lambda\\kappa\\pmb{\\theta}+\\sum_{i=1}^{t-1}w_{i}\\Big[\\sigma\\Big(\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\pmb{\\theta}\\Big)}\\\\ &{\\qquad-\\,\\sigma\\Big(\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\pmb{\\theta}^{*}\\Big)\\Big]\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "524 It satisfies two conditions: First, for the true parameter value $\\theta^{*}$ , $G_{t}(\\theta^{*})$ has a simple expression, i.e., ", "page_idx": 12}, {"type": "equation", "text": "$$\nG_{t}(\\pmb{\\theta}^{*})=\\lambda\\kappa\\pmb{\\theta}^{*}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "525 Second, according to (4.4), we can get the value of function $G_{t}$ at the MLE $\\theta_{t}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\nG_{t}(\\pmb{\\theta}_{t})=\\sum_{i=1}^{t-1}w_{i}\\gamma_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "526 where $\\gamma_{i}=o_{i}-\\sigma\\big((\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i}))^{\\top}\\pmb\\theta^{*}\\big)$ . To connect the desired estimation error with the   \n527 function $G_{t}$ , we use the mean value theorem. This leads to an upper bound of the estimation error: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{t}-\\theta^{*}\\|_{\\Sigma_{t}}\\leq\\displaystyle\\frac{1}{\\kappa}\\big\\|G_{t}(\\theta_{t})-G_{t}(\\theta^{*})\\big\\|_{\\Sigma_{t}^{-1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\underbrace{\\frac{1}{\\kappa}\\lambda\\|\\theta^{*}\\|_{\\Sigma_{t}^{-1}}}_{\\mathrm{Regulatization\\;term}}+\\underbrace{\\frac{1}{\\kappa}\\big\\|G_{t}(\\theta_{t})\\big\\|_{\\Sigma_{t}^{-1}}}_{I_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "528 For term $I_{1}$ , we can decompose the summation in (A.1) based on the adversarial feedback $c_{t}$ , i.e., ", "page_idx": 12}, {"type": "equation", "text": "$$\nG_{t}(\\theta_{t})=\\sum_{i<t:c_{i}=0}w_{i}\\gamma_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)+\\sum_{i<t:c_{i}=1}w_{i}\\gamma_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "529 where $I_{2}$ can be further decomposed as ", "page_idx": 12}, {"type": "equation", "text": "$$\nI_{2}=\\sum_{i<t:c_{i}=1}w_{i}\\epsilon_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)+\\sum_{i<t:c_{i}=1}w_{i}(\\gamma_{i}-\\epsilon_{i})\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "530 where $\\epsilon_{i}=l_{i}-\\sigma\\big((\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i}))^{\\top}\\pmb\\theta^{*}\\big)$ . With our notation of adversarial feedback, when   \n531 $c_{i}=0$ , we have $\\gamma_{i}=\\epsilon_{i}$ . Therefore, we have $|\\gamma_{i}-\\epsilon_{i}|\\leq1$ and ", "page_idx": 12}, {"type": "equation", "text": "$$\nI_{1}\\leq\\frac{1}{\\kappa}\\Big\\|\\sum_{i=1}^{t-1}w_{i}\\epsilon_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)\\Big\\|_{\\Sigma_{t}^{-1}}+\\frac{1}{\\kappa}\\Big\\|\\sum_{i<t:c_{i}=1}w_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)\\Big\\|_{\\Sigma_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "532 The stochastic term can be upper bounded with the concentration inequality (Lemma D.2). Addition  \n533 ally, by employing our specifically chosen weight, as (4.3), we can control the adversarial term, with   \n534 $w_{i}\\|\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\|_{\\Sigma_{t}^{-1}}\\leq\\alpha$ . Therefore, the adversarial term can be bounded by $\\alpha C/\\kappa$ . ", "page_idx": 12}, {"type": "text", "text": "535 A.2 Regret Upper Bound ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "536 With a similar discussion of the symmetric arm selection rule to Di et al. (2023), the regret defined in   \n537 (3.2) can be bounded by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)\\leq\\sum_{t=1}^{T}\\operatorname*{min}\\Big\\{4,2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{{\\Sigma}_{t}^{-1}}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "538 Note that in our selection of weight $w_{t}$ , it has two possible values. We decompose the summation   \n539 based on the two cases separately. We have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Regret}(T)\\leq\\underbrace{\\displaystyle\\sum_{w_{t}=1}\\operatorname*{min}\\left\\{4,2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}\\right\\}}_{J_{1}}}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{\\displaystyle\\sum_{w_{t}<1}\\operatorname*{min}\\left\\{4,2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}\\right\\}}_{J_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "540 We consider $J_{1},J_{2}$ separately. For the term $J_{1}$ , we define $\\begin{array}{r}{\\mathbf{\\Lambda}_{t}=\\lambda\\mathbf{I}+\\sum_{i\\leq t-1,w_{i}=1}\\left(\\phi(x_{i},a_{i})-\\right.}\\end{array}$   \n541 $\\phi(x_{i},b_{i})\\big)\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}$ . Then, we have $\\Sigma_{t}\\succeq\\mathbf{\\Lambda}_{\\mathbf{\\Lambda}}$ , and therefore ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}\\leq\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Lambda_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "542 Using Lemma D.3 with $\\mathbf{x}_{t}=\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ_{1}\\leq4\\beta\\sqrt{d T\\log(1+2T/\\lambda)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "543 For term $J_{2}$ , we note that $w_{t}<1$ implies that $\\boldsymbol{w}_{t}=\\alpha/\\|\\phi(\\boldsymbol{x}_{t},\\boldsymbol{a}_{t})-\\phi(\\boldsymbol{x}_{t},\\boldsymbol{b}_{t})\\|_{\\Sigma_{t}^{-1}}$ . Therefore, we   \n544 have ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ_{2}\\leq\\sum_{t=1}^{T}\\frac{4\\beta}{\\alpha}\\operatorname*{min}\\Big\\{1,\\|\\sqrt{w_{t}}(\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t}))\\|_{\\Sigma_{t}^{-1}}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "545 Using Lemma D.3 with $\\mathbf{x}_{t}^{\\prime}=\\sqrt{w_{t}}(\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t}))$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ_{2}\\leq\\frac{4d\\beta\\log(1+2T/\\lambda)}{\\alpha}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "546 We conclude the proof of regret by combining (A.2) and (A.3). ", "page_idx": 13}, {"type": "text", "text": "547 B Proof of Theorems in Section 5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "548 B.1 Proof of Theorem 5.3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "549 In this subsection, we provide the proof of Theorem 5.3. We condition on the high-probability event   \n550 in Lemma 5.1 ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\Big\\{\\big\\|\\theta_{t}-\\theta^{*}\\big\\|_{\\Sigma_{t}}\\leq\\beta,\\forall t\\in[T]\\Big\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "551 Let $r_{t}=2r^{*}(x_{t},a_{t}^{*})-r^{*}(x_{t},a_{t})-r^{*}(x_{t},b_{t})$ be the regret incurred in round $t$ . The following lemma   \n552 provides the upper bound of $r_{t}$ .   \n553 Lemma B.1. Let $0<\\delta<1$ . If we set $\\beta=\\sqrt{\\lambda}B+\\left(\\alpha C+\\sqrt{d\\log((1+2T/\\lambda)/\\delta)}\\right)/\\kappa$ , on event $\\mathcal{E}$ ,   \n554 the regret of Algorithm 1 incurred in round $t$ can be upper bounded by ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\nr_{t}\\leq\\operatorname*{min}\\Big\\{4,2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "555 Moreover, the regret can be upper bounded by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)\\leq\\sum_{t=1}^{T}\\operatorname*{min}\\Big\\{4,2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{{\\Sigma}_{t}^{-1}}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "556 With Lemma B.1, we can provide the proof of Theorem 5.3. ", "page_idx": 13}, {"type": "text", "text": "557 Proof of Theorem 5.3. Using Lemma B.1, the total regret can be upper bounded by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)\\leq\\sum_{t=1}^{T}\\operatorname*{min}\\Big\\{4,2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{{\\Sigma}_{t}^{-1}}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "558 Our weight $w_{t}$ has two possible values. We decompose the summation based on the two cases   \n559 separately. We have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Regret}(T)\\leq\\underbrace{\\displaystyle\\sum_{w_{t}=1}\\operatorname*{min}\\left\\{4,2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}\\right\\}}_{J_{1}}}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{\\displaystyle\\sum_{w_{t}<1}\\operatorname*{min}\\left\\{4,2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}\\right\\}}_{J_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "560 For the term $J_{1}$ , we consider a partial summation in rounds when $w_{t}~=~1$ . Let ${\\bf\\Lambda}{\\bf A}_{t}\\;=\\;\\lambda{\\bf I}+{\\bf\\Lambda}$   \n561 $\\begin{array}{r}{\\sum_{i<k-1,w_{i}=1}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}.}\\end{array}$ Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{1}\\leq4\\beta\\displaystyle\\sum_{t:w_{t}=1}^{}\\operatorname*{min}\\left\\lbrace1,\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\mathbf{X}_{t}^{-1}}\\right\\rbrace}\\\\ &{\\quad\\leq4\\beta\\displaystyle\\sum_{t:w_{t}=1}^{}\\operatorname*{min}\\left\\lbrace1,\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\mathbf{X}_{t}^{-1}}\\right\\rbrace}\\\\ &{\\quad\\leq4\\beta\\displaystyle\\sqrt{T\\displaystyle\\sum_{t:w_{t}=1}^{}\\operatorname*{min}\\left\\lbrace1,\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\mathbf{X}_{t}^{-1}}^{2}\\right\\rbrace}}\\\\ &{\\quad\\leq4\\beta\\sqrt{d T\\log(1+2T/\\lambda)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "562 where the second inequality holds due to $\\Sigma_{t}\\succeq\\mathbf{\\Lambda}_{\\mathbf{\\Lambda}}$ . The third inequality holds due to the Cauchy  \n563 Schwartz inequality, The last inequality holds due to Lemma D.3.   \n564 For the term $J_{2}$ , the weight in this summation satisfies $w_{t}<1$ , and therefore $w_{t}=\\alpha/\\|\\phi(x_{t},a_{t})-$   \n565 $\\phi(\\boldsymbol{x}_{t},b_{t})\\|_{\\boldsymbol{\\Sigma}_{t}^{-1}}$ . Then we have ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{2}=\\displaystyle\\sum_{w_{t}<1}\\operatorname*{min}\\left\\lbrace4,2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}w_{t}\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}/\\alpha\\right\\rbrace}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\operatorname*{min}\\left\\lbrace4,2\\beta/\\alpha\\|\\sqrt{w_{t}}(\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t}))\\|_{\\Sigma_{t}^{-1}}^{2}\\right\\rbrace}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\frac{4\\beta}{\\alpha}\\operatorname*{min}\\left\\lbrace1,\\|\\sqrt{w_{t}}(\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t}))\\|_{\\Sigma_{t}^{-1}}^{2}\\right\\rbrace}\\\\ &{\\leq\\displaystyle\\frac{4d\\beta\\log(1+2T/\\lambda)}{\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "566 where the first equality holds due to the choice of $w_{t}$ . The first inequality holds because each term in   \n567 the summation is positive. The last inequality holds due to Lemma D.3. Combining (B.1) and (B.2),   \n568 we complete the proof of Theorem 5.3. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "569 B.2 Proof of Theorem 5.5 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "570 Proof of Theorem 5.5. Our proof adapts the argument in Bogunovic et al. (2021) to dueling bandits.   \n571 For any dimension $d$ , we construct $d$ instances, each with $\\theta_{i}=\\mathbf{e}_{i}$ , where $\\mathbf{e}_{i}$ is the $i$ -th standard basis   \n572 vector. We set the action set $A=\\{\\mathbf{e}_{i}\\}_{i=1}^{d}$ . Therefore, in the $i$ -th instance, the reward for the $i$ -th   \n573 action will be 1. For the other actions, it will be 0. Therefore, the $i$ -th action will be more preferable   \n574 to any other action. While for other pairs, the feedback is simply a random guess.   \n575 Consider an adversary that knows the exact instance. When the comparison involves the $i$ -th action,   \n576 it will corrupt the feedback with a random guess. Otherwise, it will not corrupt. In the $i$ -th instance,   \n577 the adversary stops the adversarial attack only after $C$ times of comparison involving the $i$ -th action.   \n578 However, after $\\bar{C}d/4$ rounds, at least $d/2$ actions have not been compared for $C$ times. For the   \n579 instances corresponding to these actions, the agent learns no information and suffers from $\\Omega(d C)$   \n580 regret. This completes the proof of Theorem 5.5. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "581 B.3 Proof of Theorem 5.7 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "582 Proof of Theorem 5.7. Here, based on the relationship between $C$ and the threshold $\\bar{C}$ , we discuss   \n583 two distinct cases separately.   \n584 \u2022 In the scenario where $\\bar{C}<C$ , Algorithm 1 can ensures a trivial regret bound, with the guarantee   \n585 that Regret $(T)\\leq2T$ .   \n586 \u2022 In the scenario where $C\\le\\bar{C}$ , we know that $\\bar{C}$ is remains a valid upper bound on the number of   \n587 adversarial feedback. Under this situation, Algorithm 1 operates successfully with $\\bar{C}$ adversarial   \n588 feedback. Therefore, according to Theorem 5.3, the regret is upper bounded by ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)\\leq\\widetilde{O}(d\\sqrt{T}+d\\bar{C}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "589 ", "page_idx": 15}, {"type": "text", "text": "590 C Proof of Lemmas 5.1 and B.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "591 C.1 Proof of Lemma 5.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "592 Proof of Lemma 5.1. Using a similar reasoning in Li et al. (2017), we define some auxiliary quantities ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{t}(\\theta)=\\lambda\\kappa\\theta+\\displaystyle\\sum_{i=1}^{t-1}w_{i}\\Bigl[\\sigma\\Bigl(\\bigl(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\bigr)^{\\top}\\theta\\Bigr)}\\\\ &{\\qquad\\qquad-\\sigma\\Bigl(\\bigl(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\bigr)^{\\top}\\theta^{*}\\Bigr)\\Bigr]\\bigl(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\bigr),}\\\\ &{\\epsilon_{t}=l_{t}-\\sigma\\Bigl(\\bigl(\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\bigr)^{\\top}\\theta^{*}\\Bigr),}\\\\ &{\\gamma_{t}=o_{t}-\\sigma\\Bigl(\\bigl(\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\bigr)^{\\top}\\theta^{*}\\Bigr),}\\\\ &{Z_{t}=\\displaystyle\\sum_{i=1}^{t-1}w_{i}\\gamma_{i}\\bigl(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "593 In Algorithm 1, $\\pmb{\\theta}_{t}$ is chosen to be the solution to the following equation, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda\\kappa\\theta_{t}+\\sum_{i=1}^{t-1}w_{i}\\Big[\\sigma\\Big(\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\theta_{t}\\Big)-o_{i}\\Big]\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "594 Then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{t}(\\theta_{t})=\\lambda\\kappa\\theta_{t}+\\displaystyle\\sum_{i=1}^{t-1}w_{i}\\Big[\\sigma\\Big(\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\theta_{t}\\Big)}\\\\ &{\\qquad\\qquad-\\sigma\\Big(\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\theta^{*}\\Big)\\Big]\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)}\\\\ &{\\qquad=\\displaystyle\\sum_{i=1}^{t-1}w_{i}\\Big[o_{i}-\\sigma\\Big(\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\theta^{*}\\Big)\\Big]\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)}\\\\ &{=Z_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "595 The analysis in Li et al. (2017); Di et al. (2023) shows that this equation has a unique solution, with   \n596 $\\pmb{\\theta}_{t}=G_{t}^{\\dot{-}1}(Z_{t})$ . Using the mean value theorem, for any $\\pmb{\\theta}_{1},\\pmb{\\theta}_{2}\\in\\mathbb{R}^{d}$ , there exists $m\\in[0,1]$ and   \n597 $\\bar{\\pmb{\\theta}}=m\\pmb{\\theta}_{1}+(1-m)\\pmb{\\theta}_{2}$ , such that the following equation holds, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{t}(\\theta_{1})-G_{t}(\\theta_{2})=\\lambda\\kappa(\\theta_{1}-\\theta_{2})+\\displaystyle\\sum_{i=1}^{t-1}w_{i}\\Big[\\sigma\\Big(\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\theta_{1}\\Big)}\\\\ &{\\qquad\\qquad\\quad-\\sigma\\Big(\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\theta_{2}\\Big)\\Big]\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)}\\\\ &{\\qquad\\qquad\\quad=\\Big[\\lambda\\kappa\\mathbf{I}+\\displaystyle\\sum_{i=1}^{t-1}w_{i}\\dot{\\sigma}\\Big(\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\bar{\\theta}\\Big)}\\\\ &{\\qquad\\qquad\\quad\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}\\Big](\\theta_{1}-\\theta_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "598 We define $F(\\bar{\\pmb\\theta})$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ensuremath{\\boldsymbol{\\mathcal{T}}}(\\bar{\\theta})=\\lambda\\kappa\\mathbf{I}+\\sum_{i=1}^{t-1}w_{i}\\dot{\\sigma}\\Bigl(\\bigl(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\bigr)^{\\top}\\bar{\\theta}\\Bigr)\\bigl(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\bigr)\\bigl(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\bigr)^{\\top}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "599 Moreover, we can see that $G_{t}(\\pmb{\\theta}^{*})~~=~~\\lambda\\kappa\\pmb{\\theta}^{*}$ . Recall $\\begin{array}{r c l}{\\pmb{\\Sigma}_{t}}&{=}&{\\lambda\\mathbf{I}\\,+\\,\\sum_{i=1}^{t-1}w_{i}\\big(\\phi(x_{i},a_{i})\\,\\mathrm{~-~}}\\end{array}$   \n600 $\\phi(x_{i},b_{i})\\big)\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)^{\\top}$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|G_{t}(\\pmb{\\theta}_{t})-G_{t}(\\pmb{\\theta}^{*})\\right\\|_{\\pmb{\\Sigma}_{t}^{-1}}^{2}=(\\pmb{\\theta}_{t}-\\pmb{\\theta}^{*})^{\\top}F(\\bar{\\pmb{\\theta}})\\pmb{\\Sigma}_{t}^{-1}F(\\bar{\\pmb{\\theta}})(\\pmb{\\theta}_{t}-\\pmb{\\theta}^{*})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\kappa^{2}(\\pmb{\\theta}_{t}-\\pmb{\\theta}^{*})^{\\top}\\pmb{\\Sigma}_{t}(\\pmb{\\theta}_{t}-\\pmb{\\theta}^{*})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\kappa^{2}\\|\\pmb{\\theta}_{t}-\\pmb{\\theta}^{*}\\|_{\\pmb{\\Sigma}_{t}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "601 where the first inequality holds due to $\\dot{\\mu}(\\cdot)\\geq\\kappa>0$ and $F(\\bar{\\pmb\\theta})\\succeq\\kappa\\pmb{\\Sigma}_{t}$ . Then we have the following   \n602 estimate of the estimation error: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|\\theta_{t}-\\theta^{*}\\|_{\\Sigma_{t}}\\leq\\displaystyle\\frac{1}{\\kappa}\\big\\|G_{t}\\big(\\theta_{t}\\big)-G_{t}\\big(\\theta^{*}\\big)\\big\\|_{\\Sigma_{t}^{-1}}}\\\\ &{}&{\\qquad\\quad\\leq\\lambda\\|\\theta^{*}\\|_{\\Sigma_{t}^{-1}}+\\displaystyle\\frac{1}{\\kappa}\\|Z_{t}\\|_{\\Sigma_{t}^{-1}}}\\\\ &{}&{\\qquad\\quad\\leq\\sqrt{\\lambda}\\|\\theta^{*}\\|_{2}+\\displaystyle\\frac{1}{\\kappa}\\|Z_{t}\\|_{\\Sigma_{t}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "603 where the second inequality holds due to the triangle inequality and $G_{t}(\\pmb{\\theta}^{*})\\,=\\,\\lambda\\kappa\\pmb{\\theta}^{*}$ . The last   \n604 inequality holds due to $\\Sigma_{t}\\succeq\\lambda\\mathbf{I}$ . Finally, we need to bound the $\\|Z_{t}\\|_{\\Sigma_{t}^{-1}}$ term. To study the impact   \n605 of adversarial feedback, we decompose the summation in (A.1) based on the adversarial feedback $c_{t}$ ,   \n606 i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\nZ_{t}=\\sum_{i<t:c_{i}=0}w_{i}\\gamma_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)+\\sum_{i<t:c_{i}=1}w_{i}\\gamma_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "607 When $c_{i}=1$ , i.e. with adversarial feedback, $|\\gamma_{i}-\\epsilon_{i}|=1$ . On the contrary, when $c_{i}=0,\\gamma_{i}=\\epsilon_{i}$ .   \n608 Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i<t:c_{i}=0}w_{i}\\gamma_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)=\\sum_{\\scriptstyle i<t:c_{i}=0}w_{i}\\epsilon_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big),}\\\\ &{\\displaystyle\\sum_{\\scriptstyle i<t:c_{i}=1}w_{i}\\gamma_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)=\\sum_{\\scriptstyle i<t:c_{i}=1}w_{i}\\epsilon_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{i<t:c_{i}=1}w_{i}\\big(\\gamma_{i}-\\epsilon_{i}\\big)\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "609 Summing up the two equalties, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nZ_{t}=\\sum_{i=1}^{t-1}w_{i}\\epsilon_{i}\\bigl(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\bigr)+\\sum_{i<t:c_{i}=1}w_{i}(\\gamma_{i}-\\epsilon_{i})\\bigl(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\bigr).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "610 Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|Z_{t}\\|_{\\mathbf{X}_{t}^{-1}}\\leq\\underbrace{\\left\\|\\sum_{i=1}^{t-1}w_{i}\\epsilon_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)\\right\\|_{\\mathbf{X}_{t}^{-1}}}_{I_{1}}+\\left\\|\\sum_{i<t:c_{i}=1}w_{i}\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)\\right\\|_{\\mathbf{X}_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "611 For the term $I_{1}$ , with probability at least $1-\\delta$ , for all $t\\in[T]$ , it can be bounded by ", "page_idx": 16}, {"type": "equation", "text": "$$\nI_{1}\\leq\\sqrt{2\\log\\Big(\\frac{\\operatorname*{det}(\\Sigma_{t})^{1/2}\\operatorname*{det}(\\Sigma_{0})^{-1/2}}{\\delta}\\Big)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "612 due to Lemma D.2. Using $w_{i}\\leq1$ , we have $\\sqrt{w_{i}}\\|\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\|_{2}\\leq2.$ Moreover, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{det}(\\Sigma_{t})\\leq\\left({\\frac{\\operatorname{Tr}(\\Sigma_{t})}{d}}\\right)^{d}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\Big(\\frac{d\\lambda+\\sum_{i=1}^{t-1}w_{i}\\parallel(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i}))\\parallel_{2}^{2}}{d}\\Big)^{d}}\\\\ &{\\leq\\Big(\\frac{d\\lambda+2T}{d}\\Big)^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "613 where the first inequality holds because for every matrix $\\mathbf{A}\\in\\mathbb{R}^{d\\times d}$ , $\\operatorname*{det}\\mathbf{A}\\leq(\\operatorname{Tr}(\\mathbf{A})/d)^{d}$ . The   \n614 second inequality holds due to $\\sqrt{w_{i}}\\|\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\|_{2}\\leq2.$ . Easy to see that $\\operatorname*{det}(\\Sigma_{0})=\\lambda^{d}$ .   \n615 The term $I_{1}$ can be bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\nI_{1}\\leq\\sqrt{d\\log((1+2T/\\lambda)/\\delta)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "616 For $I_{2}$ , with our choice of the weight $w_{i}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{2}\\leq\\displaystyle\\sum_{i<t:c_{i}=1}w_{i}\\big\\|\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)\\big\\|_{\\Sigma_{t}^{-1}}}\\\\ &{~~\\leq\\displaystyle\\sum_{i<t:c_{i}=1}w_{i}\\big\\|\\big(\\phi(x_{i},a_{i})-\\phi(x_{i},b_{i})\\big)\\big\\|_{\\Sigma_{i}^{-1}}}\\\\ &{~~\\leq\\displaystyle\\sum_{i<t:c_{i}=1}\\alpha}\\\\ &{~~\\leq\\alpha C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "617 where the second inequality holds due to $\\Sigma_{t}~\\succeq~\\Sigma_{i}$ . The third inequality holds due to $w_{i}~\\leq$   \n618 $\\alpha/\\|(\\phi(x_{i},a_{i})-\\phi(x_{i},\\mathbf{\\dot{\\bar{b}}}_{i}))\\|_{\\Sigma_{i}^{-1}}$ . The last inequality holds due to the definition of $C$ . Combining   \n619 (C.2) and (C.3), we complete the proof of Lemma 5.1. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "620 C.2 Proof of Lemma B.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "621 Proof of Lemma $B.1$ . Let the regret incurred in the $t$ -th round by $r_{t}=2r^{*}(x_{t},a_{t}^{*})-r^{*}(x_{t},a_{t})\\stackrel{}{-}$   \n622 $r^{*}(x_{t},b_{t})$ . It can be decomposed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{t}=2r^{*}(x_{t},a_{t}^{*})-r^{*}(x_{t},a_{t})-r^{*}(x_{t},b_{t})}\\\\ &{\\quad=\\langle\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},a_{t}),\\theta^{*}\\rangle+\\langle\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},b_{t}),\\theta^{*}\\rangle}\\\\ &{\\quad=\\langle\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},a_{t}),\\theta^{*}-\\theta_{t}\\rangle+\\langle\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},b_{t}),\\theta^{*}-\\theta_{t}\\rangle}\\\\ &{\\quad\\quad\\quad+\\langle2\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t}),\\theta_{t}\\rangle}\\\\ &{\\quad\\le\\lVert\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},a_{t})\\rVert_{\\Sigma_{t}^{-1}}\\lVert\\theta^{*}-\\theta_{t}\\rVert_{\\Sigma_{t}}+\\lVert\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},b_{t})\\rVert_{\\Sigma_{t}^{-1}}\\lVert\\theta^{*}-\\theta_{t}\\rVert_{\\Sigma_{t}}}\\\\ &{\\quad\\quad\\quad+\\langle2\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t}),\\theta_{t}\\rangle}\\\\ &{\\quad\\le\\beta\\lVert\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},a_{t})\\rVert_{\\Sigma_{t}^{-1}}+\\beta\\lVert\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},b_{t})\\rVert_{\\Sigma_{t}^{-1}}}\\\\ &{\\quad\\quad\\quad+\\langle2\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t}),\\theta_{t}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "623 where the first inequality holds due to the Cauchy-Schwarz inequality. The second inequality holds   \n624 due to the high probability confidence event $\\mathcal{E}$ . Using our action selection rule, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},a_{t}),\\theta_{t}\\rangle+\\beta\\|\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},a_{t})\\|_{\\Sigma_{t}^{-1}}}\\\\ &{\\quad\\quad\\leq\\langle\\phi(x_{t},b_{t})-\\phi(x_{t},a_{t}),\\theta_{t}\\rangle+\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}}\\\\ &{\\langle\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},b_{t}),\\theta_{t}\\rangle+\\beta\\|\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}}\\\\ &{\\quad\\quad\\leq\\langle\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t}),\\theta_{t}\\rangle+\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "625 Adding the above two inequalities, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta\\|\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},a_{t})\\|_{\\mathbf{\\Sigma}_{t}^{-1}}+\\beta\\|\\phi(x_{t},a_{t}^{*})-\\phi(x_{t},b_{t})\\|_{\\mathbf{\\Sigma}_{t}^{-1}}}\\\\ &{\\qquad\\leq\\langle\\phi(x_{t},a_{t})+\\phi(x_{t},b_{t})-2\\phi(x_{t},a_{t}^{*}),\\theta_{t}\\rangle+2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\mathbf{\\Sigma}_{t}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "626 Therefore, we prove that the regret in round $t$ can be upper bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\nr_{t}\\leq2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{\\Sigma_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "627 With a simple observation, we have $r_{t}\\leq4$ . Therefore, the total regret can be upper bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)\\leq\\sum_{t=1}^{T}\\operatorname*{min}\\Big\\{4,2\\beta\\|\\phi(x_{t},a_{t})-\\phi(x_{t},b_{t})\\|_{{\\Sigma}_{t}^{-1}}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "628 ", "page_idx": 17}, {"type": "text", "text": "629 D Auxiliary Lemmas ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "630 Lemma D.1 (Azuma\u2013Hoeffding inequality, Cesa-Bianchi and Lugosi 2006). Let $\\{\\eta_{k}\\}_{k=1}^{K}$ be a   \n631 martingale difference sequence with respect to a flitration $\\{\\mathcal{F}_{t}\\}$ satisfying $|\\eta_{t}|\\leq R$ for some constant   \n632 $R,\\,\\eta_{t}$ is $\\mathcal{F}_{t+1}$ -measurable, $\\mathbb{E}[\\eta_{t}\\vert\\mathcal{F}_{t}]=\\bar{0}$ . Then for any $0<\\delta<1$ , with probability at least $1-\\delta$ , we   \n633 have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\eta_{t}\\leq R\\sqrt{2T\\log{1/\\delta}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "634 Lemma D.2 (Lemma 9 Abbasi-Yadkori et al. 2011). Let $\\{\\epsilon_{t}\\}_{t=1}^{T}$ be a real-valued stochastic process   \n635 with corresponding filtration $\\{\\mathcal{F}_{t}\\}_{t=0}^{T}$ such that $\\epsilon_{t}$ is $\\mathcal{F}_{t}$ -measurable and $\\epsilon_{t}$ is conditionally $R$ -sub  \n636 Gaussian, i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall\\lambda\\in\\mathbb{R},\\mathbb{E}[e^{\\lambda\\epsilon_{t}}|\\mathcal{F}_{t-1}]\\leq\\exp\\Big(\\frac{\\lambda^{2}R^{2}}{2}\\Big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "637 Let $\\{\\mathbf{x}_{t}\\}_{t=1}^{T}$ be an $\\mathbb{R}^{d}$ -valued stochastic process where $\\mathbf{x}_{t}$ is $\\mathcal{F}_{t-1}$ -measurable and for any $t\\in[T]$ ,   \n638 we further define $\\begin{array}{r}{\\pmb{\\Sigma}_{t}=\\lambda\\mathbf{I}+\\sum_{i=1}^{t}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}}\\end{array}$ . Then with probability at least $1-\\delta$ , for all $t\\in[T]$ , we   \n639 have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bigg\\|\\sum_{i=1}^{T}\\mathbf{x}_{i}\\eta_{i}\\bigg\\|_{\\mathbf{\\Sigma}_{\\Gamma}^{-1}}^{2}\\leq2R^{2}\\log\\bigg(\\frac{\\operatorname*{det}(\\Sigma_{t})^{1/2}\\operatorname*{det}(\\Sigma_{0})^{-1/2}}{\\delta}\\bigg).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "640 Lemma D.3 (Lemma 11, Abbasi-Yadkori et al. 2011). For any $\\lambda>0$ and sequence $\\{\\mathbf{x}_{t}\\}_{t=1}^{T}\\subseteq\\mathbb{R}^{d}$   \n641 for $t\\in[T]$ , define $\\begin{array}{r}{{\\bf Z}_{t}=\\lambda{\\bf I}+\\sum_{i=1}^{t-1}{\\bf x}_{i}{\\bf x}_{i}^{\\top}}\\end{array}$ . Then, provided that $\\|\\mathbf{x}_{t}\\|_{2}\\leq L$ holds for all $t\\in[T]$ , we   \n642 have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\operatorname*{min}\\left\\{1,\\|\\mathbf{x}_{t}\\|_{\\mathbf{Z}_{t}^{-1}}^{2}\\right\\}\\leq2d\\log(1+T L^{2}/(d\\lambda)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "643 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "644 1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "48 Justification: The primary contribution of this paper is addressing the challenge of adversarial   \n9 feedback within the dueling bandit model, where feedback is represented as a binary   \n0 preference label. Our research introduces a new perspective to machine learning. Unlike   \nprevious works on corruption-robust bandits, where corruption in each round affects the   \n52 single-arm exploration and exploitation process. Flipping the preference label potentially   \n53 impacts the expected reward of both actions chosen in a duel. This interaction can further   \n4 affect subsequent decisions involving only one of these arms. Compared with previous   \n55 adversarial dueling bandit work, we study the most direct label-flipping attack, which is   \n56 aligned with many real-life preference-based learning scenarios. Our uncertainty-weighted   \n57 maximum likelihood estimation method helps to solve this novel problem, in scenarios with   \n58 known and unknown adversarial feedback. All the scope has been discussed clearly in our   \n59 abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have added a Limitations setting in our main paper. We assume that the reward is linear with respect to some known feature maps. Although this setting is common in the literature, we observe that some recent works on dueling bandits can deal with nonlinear rewards (Li et al., 2024). Therefore, it\u2019s possible to extend our results to a more general setting. Another assumption concerns the lower bound of the derivative of the link function. Notably, in the logistic bandit model, which shares similarities with our setting through Bernoulli variables, some w\u221aork (Abeille et al., 2021; Faury et al., 2022) can improve the dependency of $\\kappa$ from $1/\\kappa$ to $\\sqrt{\\kappa}$ . A similar improvement might be achieved in our setting as well. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be ", "page_idx": 19}, {"type": "text", "text": "697 used reliably to provide closed captions for online lectures because it fails to handle   \n698 technical jargon.   \n699 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n700 and how they scale with dataset size.   \n701 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n702 address problems of privacy and fairness.   \n703 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n704 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n705 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n706 judgment and recognize that individual actions in favor of transparency play an impor  \n707 tant role in developing norms that preserve the integrity of the community. Reviewers   \n708 will be specifically instructed to not penalize honesty concerning limitations.   \n709 3. Theory Assumptions and Proofs   \n710 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n711 a complete (and correct) proof?   \n712 Answer: [Yes]   \n713 Justification: We have clearly stated and proved all the lemmas and theorems used in our   \n714 theoretical results. To help readers understand the proof without checking all the details, we   \n715 provide a roadmap of our proof in Appendix A. We also write explanation and clarification   \n716 for every formula in our paper.   \n717 Guidelines:   \n718 \u2022 The answer NA means that the paper does not include theoretical results.   \n719 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n720 referenced.   \n721 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n722 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n723 they appear in the supplemental material, the authors are encouraged to provide a short   \n724 proof sketch to provide intuition.   \n725 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n726 by formal proofs provided in appendix or supplemental material.   \n727 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n728 4. Experimental Result Reproducibility   \n729 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n730 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n731 of the paper (regardless of whether the code and data are provided or not)?   \n732 Answer: [Yes]   \n733 Justification: Our paper is mainly theoretical but we also do numerical experiments to justify   \n734 the correctness of our results. We provide all the information to reproduce our results in   \n735 Section 6.   \n736 Guidelines:   \n737 \u2022 The answer NA means that the paper does not include experiments.   \n738 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n739 well by the reviewers: Making the paper reproducible is important, regardless of   \n740 whether the code and data are provided or not.   \n741 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n742 to make their results reproducible or verifiable.   \n743 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n744 For example, if the contribution is a novel architecture, describing the architecture fully   \n745 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n746 be necessary to either make it possible for others to replicate the model with the same   \n747 dataset, or provide access to the model. In general. releasing code and data is often   \n748 one good way to accomplish this, but reproducibility can also be provided via detailed   \n749 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n750 of a large language model), releasing of a model checkpoint, or other means that are   \n751 appropriate to the research performed.   \n752 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n753 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n754 nature of the contribution. For example   \n755 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n756 to reproduce that algorithm.   \n757 (b) If the contribution is primarily a new model architecture, the paper should describe   \n758 the architecture clearly and fully.   \n759 (c) If the contribution is a new model (e.g., a large language model), then there should   \n760 either be a way to access this model for reproducing the results or a way to reproduce   \n761 the model (e.g., with an open-source dataset or instructions for how to construct   \n762 the dataset).   \n763 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n764 authors are welcome to describe the particular way they provide for reproducibility.   \n765 In the case of closed-source models, it may be that access to the model is limited in   \n766 some way (e.g., to registered users), but it should be possible for other researchers   \n767 to have some path to reproducing or verifying the results.   \n768 5. Open access to data and code   \n769 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n770 tions to faithfully reproduce the main experimental results, as described in supplemental   \n771 material?   \n772 Answer: [No]   \n773 Justification: Our experiments involve synthetic data generated from a generalized linear   \n774 model, which is quite simple and easy to reproduce. That\u2019s why we do not provide access   \n775 to our data and code. All the information required to reproduce the results is provided in   \n776 Section 6. Guidelines:   \n777 \u2022 The answer NA means that paper does not include experiments requiring code.   \n778 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n779 public/guides/CodeSubmissionPolicy) for more details.   \n780 \u2022 While we encourage the release of code and data, we understand that this might not be   \n781 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n782 including code, unless this is central to the contribution (e.g., for a new open-source   \n783 benchmark).   \n784 \u2022 The instructions should contain the exact command and environment needed to run to   \n785 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n786 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n787 \u2022 The authors should provide instructions on data access and preparation, including how   \n788 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n789 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n790 proposed method and baselines. If only a subset of experiments are reproducible, they   \n791 should state which ones are omitted from the script and why.   \n792 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n793 versions (if applicable).   \n794 \u2022 Providing as much information as possible in supplemental material (appended to the   \n795 paper) is recommended, but including URLs to data and code is permitted.   \n796 6. Experimental Setting/Details   \n797 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n798 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n799 results?   \n800 Answer: [Yes]   \n801 Justification:   \n802 Guidelines: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "808 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "09 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n10 information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "834 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "8 Answer: [Yes] Justification: We only have synthetic experiments and it can be reproduced on CPUs. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "849 9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper studies contextual dueling bandits with adversarial feedback. Our primary objective is to propel advancements in bandit theory by introducing a more robust algorithm backed by solid theoretical guarantees. The uncertainty-weighted approach we have developed for dueling bandits holds significant potential to address the issue of adversarial feedback in preference-based data, which could be instrumental in enhancing the robustness of generative models against adversarial attacks, thereby contributing positively to the societal impact and reliability of machine learning applications. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "855   \n856   \n857   \n858   \n859   \n860   \n861   \n862   \n863   \n864   \n865   \n866   \n867   \n868   \n869   \n870   \n871   \n872   \n873   \n874   \n875   \n876   \n877   \n878   \n879   \n880   \n881   \n882   \n883   \n884   \n885   \n886   \n887   \n888   \n889   \n890   \n891   \n892   \n893   \n894   \n895   \n896   \n897   \n898   \n899   \n900   \n901   \n902   \n903   \n904   \n905   \n906   \n907 ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 23}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "911 12. Licenses for existing assets   \n12 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n13 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n14 properly respected?   \n15 Answer: [NA]   \n16 Justification:   \n17 Guidelines:   \n18 \u2022 The answer NA means that the paper does not use existing assets.   \n19 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n20 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n21 URL.   \n22 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n23 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n24 service of that source should be provided.   \n25 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n26 package should be provided. For popular datasets, paperswithcode.com/datasets   \n27 has curated licenses for some datasets. Their licensing guide can help determine the   \n28 license of a dataset.   \n29 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n30 the derived asset (if it has changed) should be provided.   \n31 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n32 the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "947 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "948 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n949 include the full text of instructions given to participants and screenshots, if applicable, as   \n950 well as details about compensation (if any)?   \n1 Answer: [NA]   \n52 Justification:   \n53 Guidelines:   \n54 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n55 human subjects.   \n56 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n57 tion of the paper involves human subjects, then as much detail as possible should be   \n58 included in the main paper. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, ", "page_idx": 25}, {"type": "text", "text": "or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]