[{"type": "text", "text": "Graph Convolutions Enrich the Self-Attention in Transformers! ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jeongwhan Choi\u2217 Yonsei University jeongwhan.choi@yonsei.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Hyowon Wi\u2217 KAIST hyowon.wi@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Jayoung Kim KAIST jayoung.kim@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Yehjin Shin KAIST yehjin.shin@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Kookjin Lee Arizona State University kookjin.lee@asu.edu ", "page_idx": 0}, {"type": "text", "text": "Nathaniel Trask University of Pennsylvania ntrask@seas.upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Noseong Park\u2020 KAIST noseong@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers, renowned for their self-attention mechanism, have achieved state-ofthe-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph fliter and redesign it from a graph signal processing (GSP) perspective. We propose a graph-fliter-based self-attention $\\bar{(\\mathrm{GFSA})}^{1}$ to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph-level tasks, speech recognition, and code classification. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers are arguably one of the best feats in the field of deep learning. They are now showing state-of-the-art performance in various fields, ranging from computer vision to natural language processing, prediction tasks on graphs, speech recognition, and so forth [77, 16, 60, 61, 19, 74, 101, 45, 27, 41, 63, 51, 59, 38, 95, 72]. Recently, there have been several studies conducted on better understanding them [25, 3, 80]; there exists a common agreement among researchers that the self-attention is one of the keys leading to the success. ", "page_idx": 0}, {"type": "image", "img_path": "ffNrpcBpi6/tmp/bd630b858dc422a9c5a301a9a18b369dc981246bbf3ddcd0f521f1b4336fa827.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: Performance improvements $(\\%)$ of our GFSA when integrated with different Transformer backbones in various domains. We achieve these results with only tens to hundreds of additional ", "page_idx": 0}, {"type": "image", "img_path": "ffNrpcBpi6/tmp/b54e182ed202a801c2751dbeb89252e96123d434fa75f50e4a45cdb692f0be61.jpg", "img_caption": ["Figure 2: Filter frequency response, cosine similarity, and singular values on ImageNet-1k for DeiT-S and DeiT- $\\boldsymbol{\\mathrm{S}}+$ GFSA. Details and more visualizations are in Appendices C and D. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, there also exist several studies pointing out potential limitations of the self-attention [101, 18, 25, 28]. For instance, Shi et al. [71] revealed an analogy between the self-attention and the residual graph convolutional network (GCN), showing that BERT also suffers from a notorious problem of GCNs, called oversmoothing, i.e., tokens\u2019 latent representations become similar to each other at the deeper layers. In every self-attention layer, value vectors are aggregated in a weighted average manner since each row-wise sum of the attention matrix is always 1. Although each self-attention layer has its own attention matrix, this aggregation method causes the oversmoothing problem, not only in Transformers but also in graph neural networks [54, 7, 79, 66, 37, 101, 25, 92, 53, 71, 80, 3, 89, 90]. However, we confine our discussion to the oversmoothing of Transformers (see Section 2). ", "page_idx": 1}, {"type": "text", "text": "Being inspired by them, we redesign the self-attention from the perspective of graph signal processing (GSP) \u2014 in particular, we resort to GSP on directed graphs since the attention matrix is asymmetric. However, performing graph convolutions in the self-attention layer may incur non-trivial computational overheads. Therefore, our key design point is to learn a general but effective graph filter with minimal overhead. In general, a graph fliter on a graph $\\mathcal{G}$ is represented by a polynomial expression based on its adjacency or Laplacian matrix \u2014 in this regard, the existing self-attention mechanism can be understood as the simplest graph filter with $\\bar{A}$ only, where $\\bar{A}\\in[0,1]^{n\\times n}$ means a learned attention matrix and $n$ is the number of input tokens. ", "page_idx": 1}, {"type": "text", "text": "Our proposed graph fliter consists of an identity term and two matrix polynomial terms, $\\bar{A}$ and $\\bar{A}^{K}$ . One can design better fliters with more polynomial terms, but we avoid it since Transformers already require very large computation. The $K$ -th power, $\\bar{A}^{K}$ , may also require a high computation when the number of tokens is large. To avoid this, we further approximate $\\bar{A}^{K}$ using the element-wise first-order Taylor approximation. Therefore, one can consider that our proposed graph filter is the very next complicated fliter after the one used by the original self-attention mechanism. However, its efficacy is tremendous in various fields (cf. Fig. 1). ", "page_idx": 1}, {"type": "text", "text": "Our proposed fliter enriches the self-attention with more diverse frequency information (see Fig. 2(a)) \u2014 low (resp. high) frequency signals on $\\mathcal{G}$ mean neighboring nodes have similar (resp. different) values. Therefore, our method is able to not only effectively address the oversmoothing problem but also learn better latent representations for downstream tasks. ", "page_idx": 1}, {"type": "text", "text": "There exist a couple of prior works to enrich the self-attention mechanism with high frequency information [80, 4]. In comparison with them, our proposed graph fliter is distinctive in the following aspects: i) our proposed filter is more effective and shows better performance with comparable computational overheads, ii) our proposed fliter is well-aligned with recent advancements in the GCN community \u2014 in other words, some graph fliters used by recent advanced GCN methods are special cases of our proposed graph fliter, which is not the case for prior works, and iii) other methods were typically studied for certain domains only whereas we test our method in 6 domains \u2014 for instance, DiversePatch [25] works only for Vision Transformers (ViTs). ", "page_idx": 1}, {"type": "text", "text": "We replace the self-attention layer of selected Transformers in various fields with our proposed graph filter-based layer without changing other parts. Therefore, the accuracy increases in them are solely by our proposed graph filter-based self-attention. These enriched Transformers increase the model performance by $1.63\\%$ for image classification, $6.25\\%$ for natural language understanding, $0.31\\%$ for causal language modeling, $4.03\\%$ for graph regression, $4.76\\%$ for speech recognition, and $2.40\\%$ for code classification (see Fig. 1). Our core contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We provide a novel perspective on self-attention as a graph fliter. This perspective allows us to design more effective self-attention that can address the oversmoothing problem. \u2022 We propose a graph filter-based self-attention (GFSA) mechanism, integrating an identity term and two polynomial terms for general yet effective than the simple self-attention mechanism (Section 3). \u2022 We demonstrate that GFSA improves the performance of Transformers on a variety of tasks. GFSA achieves improved results on natural language processing, computer vision, speech recognition, graph-level tasks, and code classification (Sections 5.1 to 5.6). \u2022 We devise a strategy to selectively apply GFSA to even-numbered layers, effectively mitigating the computational overhead while preserving GFSA\u2019s performance (Section 6). ", "page_idx": 2}, {"type": "text", "text": "2 Background & Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Self-Attention in Transformers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The core building block of the Transformer architecture is the self-attention mechanism, which enables the model to learn attention patterns over its input tokens [77]. The self-attention mechanism, denoted as $\\operatorname{SA}:\\mathbb{R}^{n\\times d}\\rightarrow\\mathbb{R}^{n\\times d}$ , can be expressed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{SA}(X)=\\mathrm{softmax}\\Big(\\frac{X W_{\\mathrm{key}}(X W_{\\mathrm{qry}})^{\\top}}{\\sqrt{d}}\\Big)X W_{\\mathrm{val}}=\\bar{A}X W_{\\mathrm{val}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{X}\\in\\mathbb{R}^{n\\times d}$ is the input feature matrix, $W_{\\mathrm{key}}\\in\\mathbb{R}^{d\\times d}$ , $W_{\\mathrm{qry}}\\in\\mathbb{R}^{d\\times d}$ , and $W_{\\mathrm{val}}\\in\\mathbb{R}^{d\\times d}$ are the key, query, and value trainable parameters, respectively, and $d$ is the dimension of each token. The self-attention mechanism allows the model to weigh the importance of each token in the input sequence relative to the others, enabling the model to capture long-range contextual information better. The Transformer architecture includes multiple layers, each with a multi-head self-attention layer followed by a position-wise feed-forward layer. ", "page_idx": 2}, {"type": "text", "text": "2.2 Self-Attention and Graph Convolutional Filter ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The self-attention matrix used in Transformers has the form of symmetrically normalized adjacency matrix where each token become a node [71, 28] \u2014 the symmetrically normalized adjacency matrix is a special case of asymmetric (or directed) adjacency matrix where each row is normalized and is frequently used for the graph signal processing (GSP) on directed graphs [49]. A weighted graph $\\mathcal{G}$ with adjacency matrix $\\pmb{A}$ can be constructed by using the input tokens as $n$ nodes and the edge weights between node $i$ and node $j$ as $\\exp((X W_{\\mathrm{qry}})^{\\intercal}(X W_{\\mathrm{key}}))$ . We can rewrite the self-attention matrix $\\bar{A}$ as $\\frac{\\exp\\left((X W_{\\mathrm{qry}})_{i}^{\\intercal}\\left(X W_{\\mathrm{key}}\\right)_{j}\\right)}{\\sum_{k=1}^{d}\\exp\\left(X W_{\\mathrm{qry}}\\right)_{i}^{\\intercal}\\left(X W_{\\mathrm{key}}\\right)_{k}}$ . This allows $\\bar{A}$ to be interpreted as the symmetrically normalized adjacency matrix. In other words, $\\bar{\\pmb{A}}=\\pmb{D}^{-1}\\pmb{A}$ , where $D=\\mathrm{diag}(d_{1},d_{2},\\ldots,d_{n})$ and $\\begin{array}{r}{d_{i}=\\sum_{j}A_{i,j}}\\end{array}$ . Our new attention method is designed on top of GSP which has a close connection to discrete signal processing (DSP) [67, 68]. In DSP, a discrete signal with a length of $n$ can be represented by a vector $\\pmb{x}\\in\\mathbb{R}^{n}$ . Let $\\pmb{g}\\in\\mathbb{R}^{n}$ be a filter that we want to apply to $\\textbf{\\em x}$ . The convolution $^{x\\ast}\\,g$ can be written as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{y}_{i}=\\sum_{j=1}^{n}x_{j}\\pmb{g}_{i-j},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the index, denoted as $i$ , refers to the $i$ -th element in each vector. ", "page_idx": 2}, {"type": "text", "text": "GSP can be understood as a generalized concept of DSP. Signals are defined on the nodes of a graph, and the graph\u2019s structure influences signal processing operations. In addition, the linear and shift-invariant graph convolution filter $\\pmb{H}$ with $n$ nodes can be written with a shift operator $\\boldsymbol{S}$ as follows \u2014 $\\boldsymbol{S}$ can be from a directed graph [48]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\pmb y}={\\pmb H}{\\pmb x}=\\sum_{k=0}^{K}w_{k}{\\pmb S}^{k}{\\pmb x},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\textbf{\\em x}\\in\\mathbb{R}^{n}$ is a 1-dimensional graph signal, $K$ is the maximum order of polynomial, and $w_{k}\\in[-\\infty,\\infty]$ is a coefficient. $\\boldsymbol{S}$ is an $n\\times n$ matrix where $(i,j)$ -th element is non-zero if and only if there is an edge from node $i$ to $j$ . Two representative samples of $\\boldsymbol{S}$ are adjacency and Laplacian matrices. The graph filter $\\pmb{H}$ is the same as $\\sum_{k=0}^{K}w_{k}S^{k}$ with a large enough value of $K$ , which is called matrix polynomial [48]. We note t hat this graph filtering operation can be extended to $d_{\\cdot}$ -dimensional cases as in Eq. (1). Being inspired by Zou et al. [106] and Maskey et al. [49], we rely on the singular value domain analysis to understand the low/high-pass characteristics of filters on directed graphs (cf. Fig. 2). See more discussion in Appendices $\\boldsymbol{\\mathrm E}$ and F. ", "page_idx": 3}, {"type": "text", "text": "In the context of the self-attention within Transformers, the core part of the self-attention in Eq. (1), i.e., $\\bar{A}X$ , can be considered as a $d_{\\cdot}$ -dimensional graph fliter with $\\bar{A}$ only, where $H=\\bar{A}$ . Our goal in this paper is to design a simple (for computational purposes) yet effective form of $\\pmb{H}$ . ", "page_idx": 3}, {"type": "text", "text": "2.3 Oversmoothing in GCNs and Transformers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Oversmoothing is a phenomenon observed in deep learning models, especially in GCNs [39, 78]. As information is aggregated over multiple layers for multiple nodes (tokens), latent representations tend to become similar to each other, leading to a loss of distinctiveness in the representations [54, 104, 66]. ", "page_idx": 3}, {"type": "text", "text": "Surprisingly, an oversmoothing-like phenomenon is also observed in Transformers [80, 71]. Unlike CNNs, Transformers can not benefit from simply deepening layers after a certain depth. Earlier studies hypothesize that this may be due to issues such as the attention or feature collapse or due to uniformity among patches or tokens [101, 25, 92]. Dong et al. [18] also point out that the output of a pure Transformer, i.e., an attention mechanism without skip connections or MLPs, tends to converge to a rank-1 matrix [18]. This analysis is followed by [53], which suggests that rank collapses incur vanishing gradients of attention queries and keys. ", "page_idx": 3}, {"type": "text", "text": "In this context, the self-attention acts as a low-pass filter, since the self-attention calculates the weighted average of the value vectors of tokens. Wang et al. [80, Theorem 1] also reveal that the self-attention is a low-pass filter, continuously reducing high-frequency information. This nature contributes to the oversmoothing phenomenon as unique high-frequency features are lost in deeper layers of the network, further worsening the uniformity of token representations. Therefore, we extend the term \u201coversmoothing\u201d to describe the degeneration challenge observed in Transformers. ", "page_idx": 3}, {"type": "text", "text": "There have been proposed many empirical countermeasures for ViT, such as patch diversification [102, 25], rank collapse alleviation [101, 99], and training stabilization [74, 98]. Similar alleviating methods have been also proposed in the field of NLP, such as unsupervised learning [9], and resolve the oversmoothing and the token uniformity (or information diffusion) problems [18, 92]. There are studies on utilizing high frequency information via frequency domain analyses [80, 4], but they are not designed on top of graph flitering perspectives. Dovonon et al. [20] find that Transformers are not inherently low-pass filters, but oversmoothing depends on the eigenspectrum of the self-attention layers. They propose a reparametrization of the Transformer weights, ensuring that oversmoothinng does not occur. ", "page_idx": 3}, {"type": "text", "text": "Our paper addresses the oversmoothing problem with graph fliters since the self-attention mechanism is a basic graph filtering operation as seen in the previous subsection. ", "page_idx": 3}, {"type": "text", "text": "3 Graph Filter-based Self-Attention Layers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\bar{A}\\,\\in\\,[0,1]^{n\\times n}$ , where $n$ is the number of tokens in the input to the self-attention layer, be a self-attention matrix. Since Transformers use multi-head self-attentions, there are multiple such matrices. For simplicity, but without loss of generality, we discuss only one head in one layer. ", "page_idx": 3}, {"type": "text", "text": "From the GSP perspective, using $\\bar{A}$ as the shift operator, a graph fliter can be represented as a matrix polynomial filter, as mentioned in Section 2.2. We aim to design this matrix polynomial filter using the two lowest-order terms and one high-order term in Eq. (3). The following theorem shows that, despite using the three terms, the fliter can be either a low-pass fliter or a high-pass fliter, depending on the coefficient values. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Filter characteristics based on coefficient values). Let $\\bar{A}$ be a self-attention matrix interpreted as a graph with connected components. Consider the polynomial graph filter defined by $\\sum_{k=0}^{K}w_{k}\\bar{A}^{k}$ , where $w_{2},w_{3},\\dots,w_{K-1}\\,=\\,0$ and only $w_{0}$ , $w_{1}$ , and $w_{K}$ are non-zero. If the coefficients $w_{k}$ for $k=0,1,K$ are positive and their sum is $^{\\,l}$ , then the polynomial filter acts as a low-pass fliter, attenuating high-frequency components and promoting smoothness across the graph. Conversely, if $w_{k}=(-\\alpha)^{k}$ for $k=0,1,K$ and $\\alpha\\in(0,1)$ with sufficient large $K$ , the polynomial filter exhibits high-pass filter behavior. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 3.1 is in Appendix G. Based on Theorem 3.1, we propose to use the following graph filter, $H_{\\mathrm{GFSA}}$ , where the two lowest-order terms and one high-order term of the matrix polynomial are used: ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{\\mathrm{GFSA}}=w_{0}{\\cal I}+w_{1}\\bar{\\cal A}+w_{K}\\bar{\\cal A}^{K},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $w_{0},\\,w_{1},\\,w_{K}$ are coefficients and $K$ is a hyper-parameter where $K\\geq2$ . The coefficients can be learnable weights and we learn them with gradient descent algorithms. ", "page_idx": 4}, {"type": "text", "text": "Approximation of the high-order term. In Eq. (4), it is costly to calculate $\\bar{\\pmb{A}}^{K}$ when $K$ is large, so we need a way to approximate the high-order term $\\bar{A}^{K}$ in GFSA. We use the first-order Taylor approximation at point $a=1$ for this purpose: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(x)\\simeq f(a)+f^{\\prime}(a)(x-a),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "thus, we approximate $f(K)=\\bar{\\pmb{A}}^{K}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(K)=\\bar{A}^{K}\\simeq f(1)+f^{\\prime}(1)(K-1).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Computing the derivative of $\\bar{A}^{K}$ directly at the evaluation point requires high computational costs. To overcome this problem, we adopt the forward finite difference method, which approximates derivatives with the difference term: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf^{\\prime}(K)={\\frac{f(K+h)-f(K)}{h}}={\\frac{{\\bf{{A}}}^{K+h}-{\\bf{{A}}}^{K}}{h}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the approximation error is ${\\mathcal{O}}(h^{2})$ . To balance the trade-off between computational efficiency2 and accuracy, we set $h=1$ . This method is inspired by the approach in Brouwer et al. [6], which uses the difference term between two consecutive hidden states in discrete time to approximate the derivatives. Therefore, we approximate $\\bar{A}^{K}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f(K)=\\bar{\\cal A}^{K}\\simeq f(1)+(\\bar{\\cal A}^{2}-\\bar{\\cal A})(K-1)}}\\\\ {{=\\bar{\\cal A}+(K-1)(\\bar{\\cal A}^{2}-\\bar{\\cal A}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The approximation for $\\bar{A}^{K}$ with $\\bar{A}$ and $\\bar{A}^{2}$ provides a simpler computation that can significantly reduce the required computational resources and time. ", "page_idx": 4}, {"type": "text", "text": "GFSA: our graph filter-based self-attention. Our proposed graph filter-based self-attention (GFSA) is defined with the graph filter $\\tilde{H}_{\\mathrm{GFSA}}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{GFSA}(X):=\\tilde{H}_{\\mathrm{GFSA}}X W_{\\mathrm{val}},}\\\\ &{\\qquad\\tilde{H}_{\\mathrm{GFSA}}=w_{0}I+w_{1}\\bar{A}+w_{K}(\\bar{A}+(K-1)(\\bar{A}^{2}-\\bar{A})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the last term is the approximated $\\bar{A}^{K}$ from Eq. (8). We replace the original self-attention layer in various Transformers with the proposed graph filter-based layer without changing other parts. Therefore, GFSA can be plugged into any Transformers that rely on the self-attention. For pseudocode, see Appendix I. ", "page_idx": 4}, {"type": "text", "text": "4 Properties of GFSA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section analyzes the theoretical error of the $\\bar{A}^{K}$ approximation used by GFSA and how GFSA can mitigate oversmoothing. We also explain the meaning of GFSA\u2019s high-order term in the context of Transformers and provide comparisons of GFSA in other models. ", "page_idx": 4}, {"type": "text", "text": "Theoretical characteristics of approximation error in GFSA. We provide a theorem that provides an upper bound on the error introduced by approximating the power of a matrix, specifically using the first-order Taylor expansion. The following theorem specifically analyzes the error of matrix $\\bar{A}^{\\breve{K}}$ when approximated using a first-order Taylor expansion. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 (Error bound for approximated high-order term in GFSA). Define the error term, $E_{K}$ , as the difference between the exact value and approximated value of $\\bar{\\pmb{A}}^{K}$ , which is given by $\\bar{E_{K}^{-}}=||\\bar{\\pmb{A}^{K}}-(\\bar{\\pmb{A}}+(K-1)(\\bar{\\pmb{A}^{2}}-\\bar{\\pmb{A}}))||_{F},$ where $||\\cdot||_{F}$ denotes the Frobenius norm. Then, the error bound can be shown that $E_{K}\\leq2{\\sqrt{n}}K$ . ", "page_idx": 5}, {"type": "text", "text": "The error bound provides an upper limit for the difference between the actual value of $\\bar{A}^{K}$ and its approximation. The proof of Theorem 4.1 is in Appendix H. It shows theoretical validity for using approximations to the high-order term in the fliters of our GFSA. In terms of performance, we report a difference of approximately $\\bar{A}^{K}$ between the actual calculated values in Appendix J. ", "page_idx": 5}, {"type": "text", "text": "How to alleviate the oversmoothing problem? The key leading to the low/high pass filtering behavior of our proposed filter is the coefficients $\\{w_{0},w_{1},w_{K}\\}$ \u2014 note that in the self-attention of Transformers, $w_{0}=w_{K}=0$ and $w_{1}=1$ . Since our method can learn any appropriate values for them for a downstream task, it can be reduced to low-pass-only, high-pass-only, or combined fliters. According to Theorem 3.1, our graph polynomial fliter can be said to be a low-pass fliter when $w_{1},w_{K}$ are positive and a high-pass fliter when they are negative. Therefore, our method can learn the appropriate coefficients $\\{w_{0},w_{1},w_{K}\\}$ for downstream tasks, so it can be reduced to a low-pass-only, high-pass-only, or combined filter, alleviating the oversmoothing problem of self-attention. ", "page_idx": 5}, {"type": "text", "text": "The meaning of the high-order term in GFSA in the context of Transformers. Existing selfattention only captures simple pairwise similarities between tokens and is limited in capturing high-order dependencies. For example, given the two sentences, \u201cBooks are more expensive than pencils\u201d and \u201cBooks are cheaper than computers\u201d, to understand the relationship between \u201ccomputers\u201d and \u201cpencils\u201d, we need to capture the high-order dependencies connected through the \u201cBook\u201d token. However, it is difficult to capture these high-order dependencies with traditional self-attention [103]. Therefore, from a Transformer perspective, the approximated $\\bar{A}^{K}$ in GFSA can be interpreted as being able to capture these high-order dependencies. ", "page_idx": 5}, {"type": "text", "text": "Comparison to Transformers. In the field of computer vision, there has been recent research on adjusting the frequency response of ViT. HAT [4] creates adversarial examples by altering clean images with high-frequency perturbations and jointly trains the ViT on clean images and adversarial examples. Through this, they aim to solve the problem of the ViT being unable to capture highfrequency by allowing us to capture the high-frequency components of the images. However, HAT has the disadvantage of requiring more epochs than the existing ViT, as it must perform adversarial training in some initial epochs and train normally in the remaining epochs. Wang et al. [80] use the concept of DSP, which is a special case of GSP, to isolate the lowest frequency component in the Fourier domain and use a fliter learned by rescaling the low and high-frequency components. On the other hand, our GFSA extends the concept to graph signal processing and redesigns self-attention as a graph filter. While GFSA seeks to design a better graph filter by interpreting self-attention as a graph filter, Shi et al. [71] are inspired by JKNet [91], and they solve the oversmoothing problem by fusing the hidden vectors of each layer. However, their method has a limitation with memory increasing, and they only applied it to BERT. ", "page_idx": 5}, {"type": "text", "text": "Comparison to GCNs. Comparisons to GCNs that can be interpreted as graph filters [39, 15, 24] are inevitable. GFSA without a high-order term is analogous to ChebNet [15] with $K\\,=\\,1$ . In addition, GFSA reduces to the vanilla GCN [39] when $K=1$ , $w_{0}=0$ , $w_{1}=1$ . GPR-GNN [12], which approximates graph convolutions using the monomial basis, is identical to GFSA if it only considers up to first order and additionally uses a $K$ -order term and learns the coefficients. When we use only a high-order term and $w_{K}$ is learned to a negative value, GFSA can become similar to the reaction-diffusion layer of GREAD [13], $\\bar{A}X+\\beta(\\breve{\\bar{A}}-\\bar{A}^{2})$ , depending on the higher order terms. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we demonstrate the effectiveness of GFSA through a series of experiments. These experiments encompass various tasks: i) language understanding and causal language modeling, ii) image classification, iii) graph-level task, and iv) code classification. We replace the self-attention of base Transformers in those fields with our GFSA. Our modification adds only tens to hundreds of parameters, which are negligible in comparison with the original size of base models. ", "page_idx": 6}, {"type": "text", "text": "5.1 Experiments on Natural Language Understanding ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setting. We integrate GFSA into 3 pre-trained large language models: BERT, ALBERT, and RoBERTa. We evaluate them on the GLUE benchmark, which includes 3 categories of natural language understanding tasks: i) single-sentence, ii) similarity and paraphrasing, and iii) natural language inference tasks. For each task, we select the best hyperparameters for GFSA, and the other hyperparameters are fixed. The detailed experimental settings are in Appendix K.1. ", "page_idx": 6}, {"type": "text", "text": "Results. The results are shown in Table 1. When GFSA was plugged into backbones, average performance scores improved across all models over pure backbones. This indicates that GFSA is effective in both large models like BERT and RoBERTa, as well as relatively smaller models like ALBERT. It is worth mentioning that in the case of RoBERTa finetuned on the CoLA dataset; there is a significant margin increase from $60.34\\%$ to $64.11\\%$ , which is a $3.77\\%$ improvement with only 144 additional parameters. When compared to ContraNorm, GFSA shows a greater performance improvement on average. Fig. 5 in Appendix C shows that these performance enhancements can be attributed to addressing the oversmoothing issue through the designed graph filter. ", "page_idx": 6}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/5d5cff61544d5c067fcac03787258676491bab684ac2c8e02feb51c03bcdc5ed.jpg", "table_caption": ["Table 1: Results comparison on GLUE benchmark. Avg denotes the average performance. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Experiments on Causal Language Modeling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setting. We also validate the effectiveness of GFSA on causal language modeling problems. We finetune GPT2 [61] on the following 3 datasets: Penn Treebank (PTB) [47], WikiText-2, and WikiText103 [50]. Following the evaluation method in Yao et al. [93], we finetune models for 15 epochs with PTB, 4 epochs with WikiText-103, and 10 epochs with WikiText-2, and report the perplexity for sensitivity metric. The detailed experimental settings are in Appendix L.1. ", "page_idx": 6}, {"type": "text", "text": "Results. Table 2 shows the perplexity on PTB, WitiText-2, and WikiText-103. Across all datasets, GPT2 with GFSA consistently outperforms the vanilla GPT2. Our GFSA improves the average perplexity from 18.806 to 18.764. Note that perfor 1120 nt ", "page_idx": 6}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/bbc7a587c6afc767f29b4d5882371281c6387881026faab0bed319869431f71a.jpg", "table_caption": ["Table 2: Results comparison on GPT-2 finetuned with GFSA. Avg denotes the average performance. "], "table_footnote": ["are made with only 144 additional learnable parameters for 12 layers with 12 heads. "], "page_idx": 6}, {"type": "text", "text": "5.3 Experiments on Vision Transformers ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setting. We aim to demonstrate the efficacy of our GFSA across a spectrum of ViT backbones. We choose DeiT [74], CaiT [75], and Swin [45] as the backbone, and the models are trained from scratch. When training the 12-layer DeiT, we follow the same training recipe, hyperparameters, and data augmentation from Touvron et al. [74]. For detailed experimental settings, see Appendix M.1. ", "page_idx": 7}, {"type": "text", "text": "Results. The experimental evaluations are summarized in Table 3. We compare various models on the ImageNet-1k benchmark. The results show that the proposed GFSA successfully enhances DeiT, CaiT, and Swin across all depth settings and training methods. GFSA provides additional parameters less than 72 for 12-layer DeiT while improving top-1 accuracy by $1.63\\%$ . To sum up, we observed that both shallow and deep ViTs can achieve the following beneftis from GFSA: i) The fliter response shows GFSA can preserve higher-frequency representation (cf. Fig. 2 (a)) and ii) Fig. 2 (b) shows that GFSA mitigates the increase in the cosine similarity of representation as the layer gets deeper. We further compare with state-of-the-art models that use Fourier transforms rather than graph filters in Appendix M.5. We also show results under the same settings as ContraNorm [28] in Appendix M.6. ", "page_idx": 7}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/0e728b8e9a69d6cb492b460aa9c1bcda4abc5e3de8838325d10b5a446d09f7c5.jpg", "table_caption": ["Table 3: Results comparison on ImageNet-1k. Our full results with other models are in Appendix M.4. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.4 Experiments on Graph-level Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setting. To evaluate the efficacy of GFSA on graph-level tasks, we conduct experiments on a broader range of datasets. We use datasets from Long-Range Graph Benchmark (LRGB) [21] (e.g., Peptide-func and Peptide-struct), Benchmarking GNNs [22] (e.g., ZINC, MNIST, CIFAR10), Open Graph Benchmark (OGB) dataset [32] (e.g., Molhiv and MolTox21), and OGB-LSC dataset (i.e., PCQM4M-LSC) [33]. We choose Graphormer [94], Graph-ViT [30], and GPS [63] as our backbone architectures, following their original experimental protocols for fair comparison. For GPS, we replace its self-attention module with our GFSA while maintaining its best configuration and other hyperparameters. For Graph-ViT, we apply GFSA to the Hadamard self-attention method, which He et al. [30] propose as optimal. For a detailed experimental setting, see Appendix O.1. ", "page_idx": 7}, {"type": "text", "text": "Results. Tables 4, 5, and 6 show consistent performance improvements when GFSA is integrated with backbone architectures. Graph- $\\mathrm{\\cdotViT+GFSA}$ shows improvements on all datasets. On Peptidefunc, it achieves a $0.65\\%$ increase in AP. Notably, in PCQM4M, incorporating GFSA improves the validation MAE by $7.20\\%$ . Due to space constraints, the results with standard deviation are included in Appendix O.2. ", "page_idx": 7}, {"type": "text", "text": "5.5 Experiments on Automatic Speech Recognition ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setting. We conduct automatic speech recognition (ASR) experiments on the LibriSpeech 3 dataset [55], which consists of audio recordings paired with their transcriptions. We use Branch", "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": [""], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/81823a612fe08ca00d7de6f5c3f4f2f83c291a128d9ce6df3d1e4c5cd7425544.jpg", "table_caption": ["Table 5: Results on PCQM4M and PCQM4Mv2 "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 6: Experimental evalutation of GFSA plugged into GPS and Graph-ViT. Results marked with $\\dagger$ indicate settings where we conducted our own experiments due to unavailable Hadamard self-attention performance in He et al. [30]\u2019s paper. ", "page_idx": 8}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/79fda1df967349bacf2d985c37db24eb020e51f31e96c664be6eeb4b8d038dd3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "former [59] and a pure Transformer. For implementation, we follow the recipes of SpeechBrain [65] and the detailed settings are in Appendix N.1. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 7 compares word error rates (WERs) on LibriSpeech 100h and 960h. For 100h, Transformer+GFSA achieves 10.30/25.30 on the test clean/other set, which is a $6.53\\%$ improvement over the Transformer for the WER of the test clean. For 960h, Transformer+GFSA shows a WER result of 2.31 in test clean, a $4.55\\%$ improvement over Transformer and Branchformer+GFSA achieves 2.31/5.49 with an LM on the test clean/other sets. Fig. 8 in Appendix N.2 depicts the learning curves of train loss and valid loss when using GFSA, showing the effectiveness of our proposed filter. ", "page_idx": 8}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/721882d9e62d6be8e3d096232fc46aaf538ea6c11476a6949db1960303032914.jpg", "table_caption": ["Table 7: Results for ASR training on LibriSpeech 100h and 960h with GFSA "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.6 Experiments on Code Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Setting. We conduct a code defect detection task based on Devign dataset provided by Zhou et al. [105]. We use RoBERTa [44], CodeBERT [23], PLBART [2], and CodeT5 [84] as our backbone models. The detailed settings are in Appendix P.1. ", "page_idx": 8}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/21a216872ca07bf8afe15f4a08371e01f0edc04f344316c8bac35195c8360018.jpg", "table_caption": ["Table 8: Results on code classification. The number in $(\\uparrow)$ indicates the improvement rate. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Results. Table 8 shows the accuracy of all models; GFSA results better than the base models. The biggest improvement is $2.40\\%$ for RoBERTa. In the case of CodeT5-base, using GFSA shows an accuracy of 64.75, an improvement of $1.95\\%$ from 63.51. CodeT5- smal $^+$ GFSA has only about 100 additional parameters compared to CodeT5-small with 60M parameters, and even more impressively, it surpasses the accuracy of CodeT5-base. The biggest improvement is $2.40\\%$ for RoBERTa. In Appendix P.2, we include case studies for this task. We also report the results of the code clone detection task in Appendix Q. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion on Runtime Overheads ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitation. The introduction of our GFSA layer results in a slight increase in training and inference time. We report the runtimes when plugging GFSA in Appendices R and S. For GLUE benchmark, integrating GFSA into BERT enhances the average performance from $82.51\\%$ to $83.58\\%$ (see Table 1) with more overhead of less than 36 seconds per epoch based on average training time (see Table 23). Considering the improvements, the increases in training time are negligible. ", "page_idx": 9}, {"type": "text", "text": "GFSA in selected layers: a strategy to mitigate the limitation. As GFSA requires more calculation than the original self-attention, the runtime after using GFSA slightly increases. Our experiments initially applied GFSA across all Transformer layers (as discussed in Section 5); however, to reduce computational load, we propose a selective application strategy. For this purpose, GFSA is used only on even-numbered layers. In Tables 35 to 39 of Appendix T, the results show that this strategy effectively reduces runtime increases while preserving comparable performance to the full-layer GFSA integration. Notably, the selective application of GFSA cuts the per-epoch runtime increase by $26.90\\%$ relative to its full-layer application, with only a $7.39\\%$ increase in runtime per epoch compared to the backbone model in Table 39. ", "page_idx": 9}, {"type": "image", "img_path": "ffNrpcBpi6/tmp/f400247e0493741397906e86d2b7a24a9d75c4c1bb1d22112b9b27003861c110.jpg", "img_caption": ["Figure 3: Effectiveness of our selective layer strategy on ImageNet-1k. This shows out strategy\u2019s ability to maintain accuracy benefits while mitigating runtime increases. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "GFSA in linear Transformers. Although GFSA requires additional computation for calculating $\\bar{A}^{2}$ , we explore integrating GFSA with linear attention variants to maintatin efficiency and scalability. Recent approaches [36, 70] achieve linear complexity by reformulating softmax operations and reordering matrix multiplication in self-attention. We apply similar principles to compute second-order self-attention efficiently, enabling $\\tilde{H}_{\\mathrm{GFSA}}$ calculation with linear complexity with respect to sequence length. Fig. 4 shows the performance, runtime and GPU usage changes when applying our GFSA to Transformers with linear complexity. GFSA still improves performance compared to the backbone model, while the increase in time and GPU usage is minimal. Notably, when GFSA is applied to Efficient Attention [70], the performance is improved and the runtime is 11.82 times faster than when GFSA is applied to the vanilla self-attention. This shows that GFSA can be effectively implemented with linear complexity architectures while preserving its benefits and providing a solution for addressing computational concerns. ", "page_idx": 9}, {"type": "image", "img_path": "ffNrpcBpi6/tmp/317a93f8bd533f9d5d08b159713ec9c264e4ad64d8f9c1d62e180cf1c93af479.jpg", "img_caption": ["Figure 4: Performance ( $x$ -axis), runtime ( $y$ -axis), and GPU usage (circle sizes) of various Transformers and integrated GFSA on Long-Range benchmark "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our proposed GFSA achieves high performance with improvements on a variety of tasks. GFSA is a simple yet effective method that enriches self-attention in Transformers with more diverse frequency information. This enables GFSA to address the oversmoothing problem and learn better latent representations for downstream tasks. However, our GFSA does not bring significant overheads in those Transformers\u2019 empirical runtime complexity. One can use more complicated graph filters to enhance accuracy more, but our goal is to find a balance between accuracy enhancements and overheads in runtime complexity. ", "page_idx": 9}, {"type": "text", "text": "We believe that GFSA suggests a promising new direction for improving Transformers. GFSA can be implement with simple way and used in conjunction with other techniques to further improve the performance of Transformers. Considering the ongoing advancements in large language models, such as GPT-4 [1] and LLaMA [76], we hope that our approach may offer new insights for enhancing their performance and efficiency. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "N. Park was partly supported by the Korea Advanced Institute of Science and Technology (KAIST) grant funded by the Korea government (MSIT) (No. G04240001, Physics-inspired Deep Learning, $10\\%)$ , Institute for Information & Communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (No. RS-2020-II201361, Artificial Intelligence Graduate School Program (Yonsei University), $20\\%$ ; No. RS-2024-00457882, AI Research Hub Project, $50\\%$ ), and Samsung Electronics Co., Ltd. (No. G01240136, KAIST Semiconductor Research Fund (2nd), $10\\%$ ). K. Lee acknowledges support from the U.S. National Science Foundation under grant IIS 2338909. Dr. Trask acknowledges funding under the Department of Energy under the Mathematical Multifaceted Integrated Capability Centers program. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training for program understanding and generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132668, 2021.   \n[3] Ameen Ali, Tomer Galanti, and Lior Wolf. Centered self-attention layers. arXiv preprint arXiv: 2306.01610, 2023.   \n[4] Jiawang Bai, Li Yuan, Shu-Tao Xia, Shuicheng Yan, Zhifeng Li, and Wei Liu. Improving vision transformers by revisiting high-frequency components. In European Conference on Computer Vision, pages 1\u201318. Springer, 2022.   \n[5] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing textual entailment challenge. TAC, 7:8, 2009.   \n[6] Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[7] Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. arXiv preprint arXiv:2006.13318, 2020.   \n[8] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.   \n[9] Nuo Chen, Linjun Shou, Ming Gong, Jian Pei, Bowen Cao, Jianhui Chang, Daxin Jiang, and Jia Li. Alleviating over-smoothing for unsupervised sentence representation. arXiv preprint arXiv:2305.06154, 2023.   \n[10] Tianlong Chen, Zhenyu Zhang, Yu Cheng, Ahmed Awadallah, and Zhangyang Wang. The principle of diversity: Training stronger vision transformers calls for reducing all levels of redundancy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12020\u201312030, 2022.   \n[11] Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs, 2018.   \n[12] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized PageRank graph neural network. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.   \n[13] Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. Gread: Graph neural reaction-diffusion networks. In International Conference on Machine Learning (ICML), pages 5722\u20135747. PMLR, 2023.   \n[14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for selfattention layers with application to graph neural networks. In International Conference on Machine Learning (ICML), pages 2456\u20132466. PMLR, 2021.   \n[15] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral flitering. In Advances in Neural Information Processing Systems (NeurIPS), 2016.   \n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.   \n[17] Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Third International Workshop on Paraphrasing (IWP2005), 2005.   \n[18] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning (ICML), pages 2793\u20132803. PMLR, 2021.   \n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.   \n[20] Gb\u00e8tondji JS Dovonon, Michael M Bronstein, and Matt J Kusner. Setting the record straight on transformer oversmoothing. arXiv preprint arXiv:2401.04301, 2024.   \n[21] Vijay Prakash Dwivedi, Ladislav Ramp\u00e1\u0161ek, Mikhail Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id $\\equiv$ in7XC5RcjEn.   \n[22] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research, 24(43):1\u201348, 2023.   \n[23] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. CodeBERT: A pre-trained model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1536\u20131547, 2020.   \n[24] Johannes Gasteiger, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. Diffusion improves graph learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, 2019.   \n[25] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Vision transformers with patch diversification. arXiv preprint arXiv:2104.12753, 2021.   \n[26] Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In International Conference on Machine Learning (ICML), pages 1764\u20131772. PMLR, 2014.   \n[27] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020.   \n[28] Xiaojun Guo, Yifei Wang, Tianqi Du, and Yisen Wang. ContraNorm: A contrastive learning perspective on oversmoothing and beyond. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.   \n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[30] Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A generalization of vit/mlp-mixer to graphs. In International conference on machine learning (ICML), pages 12724\u201312745. PMLR, 2023.   \n[31] Nicholas J Higham. Functions of matrices: theory and computation. SIAM, 2008.   \n[32] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems (NeurIPS), 33:22118\u201322133, 2020.   \n[33] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. OGBLSC: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021.   \n[34] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[35] John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52(7):1757\u20131768, 2012.   \n[36] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156\u20135165. PMLR, 2020.   \n[37] Nicolas Keriven. Not too little, not too much: a theoretical analysis of graph (over) smoothing. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 2268\u2013 2281, 2022.   \n[38] Jayoung Kim, Yehjin Shin, Jeongwhan Choi, Hyowon Wi, and Noseong Park. Polynomialbased self-attention for table representation learning. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 24509\u201324526. PMLR, 21\u201327 Jul 2024. URL https://proceedings.mlr.press/v235/kim24ae.html.   \n[39] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.   \n[40] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.   \n[41] Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam Shoukat, and Junaid Qadir. Transformers in speech processing: A survey. arXiv preprint arXiv:2303.11607, 2023.   \n[42] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824, 2021.   \n[43] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, 2020.   \n[44] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[47] Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. 1993.   \n[48] Antonio G. Marques, Santiago Segarra, and Gonzalo Mateos. Signal processing on directed graphs: The role of edge directionality when processing and learning from network data. IEEE Signal Processing Magazine, 37(6):99\u2013116, 2020. doi: 10.1109/MSP.2020.3014597.   \n[49] Sohir Maskey, Raffaele Paolino, Aras Bacho, and Gitta Kutyniok. A fractional graph laplacian approach to oversmoothing. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[50] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.   \n[51] Luis M\u00fcller, Mikhail Galkin, Christopher Morris, and Ladislav Ramp\u00e1\u0161ek. Attending to graph transformers. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id $\\equiv$ HhbqHBBrfZ.   \n[52] Tam Nguyen, Tan Nguyen, and Richard Baraniuk. Mitigating over-smoothing in transformers via regularized nonlocal functionals. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, 2023.   \n[53] Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 27198\u201327211, 2022.   \n[54] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.   \n[55] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206\u20135210. IEEE, 2015.   \n[56] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition. Interspeech 2019, 2019.   \n[57] Badri Patro and Vijay Agneeswaran. Scattering vision transformer: Spectral mixing matters. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, 2023.   \n[58] Badri N Patro, Vinay P Namboodiri, and Vijay Srinivas Agneeswaran. Spectformer: Frequency and attention is what you need in a vision transformer. arXiv preprint arXiv:2304.06446, 2023.   \n[59] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe. Branchformer: Parallel mlp-attention architectures to capture local and global context for speech recognition and understanding. In International Conference on Machine Learning (ICML), pages 17627\u201317643. PMLR, 2022.   \n[60] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.   \n[61] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[62] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.   \n[63] Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 14501\u201314515, 2022.   \n[64] Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global filter networks for image classification. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 980\u2013993, 2021.   \n[65] Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh Chou, Sung-Lin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, Fran\u00e7ois Grondin, William Aris, Hwidong Na, Yan Gao, Renato De Mori, and Yoshua Bengio. SpeechBrain: A general-purpose speech toolkit, 2021. arXiv:2106.04624.   \n[66] T. Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graph neural networks. arXiv preprint arXiv: Arxiv-2303.10993, 2023.   \n[67] Aliaksei Sandryhaila and Jos\u00e9 MF Moura. Discrete signal processing on graphs. IEEE transactions on signal processing, 61(7):1644\u20131656, 2013.   \n[68] Aliaksei Sandryhaila and Jose MF Moura. Discrete signal processing on graphs: Frequency analysis. IEEE Transactions on Signal Processing, 62(12):3042\u20133054, 2014.   \n[69] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green AI. Communications of the ACM, 63(12):54\u201363, 2020.   \n[70] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 3531\u20133539, 2021.   \n[71] Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen Lee, and James T Kwok. Revisiting over-smoothing in bert from the perspective of graph. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.   \n[72] Yehjin Shin, Jeongwhan Choi, Hyowon Wi, and Noseong Park. An attentive inductive bias for sequential recommendation beyond the self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 8984\u20138992, 2024.   \n[73] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y $\\mathrm{Ng}$ , and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013.   \n[74] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning (ICML), pages 10347\u201310357. PMLR, 2021.   \n[75] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 32\u201342, 2021.   \n[76] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems (NeurIPS), volume 30, 2017.   \n[78] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph Attention Networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.   \n[79] Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Multi-hop attention graph neural network. In IJCAI, 2021.   \n[80] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.   \n[81] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.   \n[82] Wei Wang, Ming Yan, and Chen Wu. Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering. arXiv preprint arXiv:1811.11934, 2018.   \n[83] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. Detecting code clones with graph neural network and flow-augmented abstract syntax tree. In 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER), pages 261\u2013271, 2020. doi: 10.1109/SANER48275.2020.9054857.   \n[84] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8696\u2013 8708, 2021.   \n[85] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641, 2019.   \n[86] Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019.   \n[87] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.   \n[88] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. CvT: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 22\u201331, 2021.   \n[89] Xinyi Wu, Amir Ajorlou, Zihui Wu, and Ali Jadbabaie. Demystifying oversmoothing in attention-based graph neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[90] Xinyi Wu, Zhengdao Chen, William Wei Wang, and Ali Jadbabaie. A non-asymptotic analysis of oversmoothing in graph neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.   \n[91] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning (ICML), pages 5453\u20135462, 2018.   \n[92] Hanqi Yan, Lin Gui, Wenjie Li, and Yulan He. Addressing token uniformity in transformers via singular value transformation. In Uncertainty in Artificial Intelligence, pages 2181\u20132191. PMLR, 2022.   \n[93] Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, and Yuxiong He. Random-ltd: Random and layerwise token dropping brings efficient training for large-scale transformers. arXiv preprint arXiv:2211.11586, 2022.   \n[94] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 28877\u201328888, 2021.   \n[95] Youn-Yeol Yu, Jeongwhan Choi, Woojin Cho, Kookjin Lee, Nayong Kim, Kiseok Chang, ChangSeung Woo, ILHO KIM, SeokWoo Lee, Joon Young Yang, SOOYOUNG YOON, and Noseong Park. Learning flexible body collision dynamics with hierarchical contact mesh transformer. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=90yw2uM6J5. [96] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international conference on computer vision, pages 558\u2013567, 2021.   \n[97] Zhanpeng Zeng, Yunyang Xiong, Sathya Ravi, Shailesh Acharya, Glenn M Fung, and Vikas Singh. You only sample (almost) once: Linear cost self-attention via bernoulli sampling. In International conference on machine learning, pages 12321\u201312332. PMLR, 2021.   \n[98] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua M. Susskind. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning (ICML), volume 202, pages 40770\u201340803. PMLR, 2023. [99] Aston Zhang, Alvin Chan, Yi Tay, Jie Fu, Shuohang Wang, Shuai Zhang, Huajie Shao, Shuochao Yao, and Roy Ka-Wei Lee. On orthogonality constraints for transformers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 375\u2013382, 2021.   \n[100] Ying Zhang, Mohammad Pezeshki, Phil\u00e9mon Brakel, Saizheng Zhang, C\u00e9sar Laurent, Yoshua Bengio, and Aaron Courville. Towards end-to-end speech recognition with deep convolutional neural networks. Interspeech 2016, 2016.   \n[101] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng. DeepViT: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.   \n[102] Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, and Jiashi Feng. Refiner: Refining self-attention for vision transformers. arXiv preprint arXiv:2106.03714, 2021.   \n[103] Haoyi Zhou, Siyang Xiao, Shanghang Zhang, Jieqi Peng, Shuai Zhang, and Jianxin Li. Jump self-attention: Capturing high-order statistics in transformers. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 17899\u201317910, 2022.   \n[104] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI open, 1:57\u201381, 2020.   \n[105] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. In Advances in neural information processing systems (NeurIPS), volume 32, 2019.   \n[106] Chunya Zou, Andi Han, Lequan Lin, and Junbin Gao. A simple yet effective svd-gcn for directed graphs. arXiv preprint arXiv:2205.09335, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A Reproducibility Statement 19   \nB Broader Impact 19   \nC Oversmoothing and Additional Visualizations 19   \nD Analysis of Frequency Responses with Visualizations 20   \nE Frequency Analyses in the Singular Value Domain 21   \nF Matrix Polynomial vs. Graph Fourier Transform 21   \nG Proof of Theorem 3.1 21   \nH Proof of Theorem 4.1 22   \nI Implementation of GFSA 23   \nJ Comparison with Actual and Approximated High-order Terms 23   \nK Natural Language Understanding 24   \nL Causal Language Modeling 25   \nM Image Classification 26   \nN Automatic Speech Recognition 29   \nO Graph-level Tasks 31   \nP Code Defect Detection 32   \nQ Code Clone Detection 33   \nR Time Complexity and Empirical Runtime Analysis 34   \nS Inference Time Analysis 36   \nT Results of the Strategy for Efficiency 38   \nU GFSA in Linear Transformers 39 ", "page_idx": 17}, {"type": "text", "text": "A Reproducibility Statement ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To ensure the reproducibility and completeness of this paper, we include the Appendix with 12 sections. Appendix I provides our PyTorch-style pseudo code for our GFSA method. The pseudo code helps to implement our GFSA to any Transformers used a pure self-attention. All experiments in the paper are reproducible with additional implementation details provided in Appendices K to Q. ", "page_idx": 18}, {"type": "text", "text": "B Broader Impact ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In terms of the broader impact of this research on society, we do not see the very negative impacts that might be expected. However, this paper may have implications for the carbon footprint and accessibility of learning algorithms. The computations required for machine learning research are rapidly growing, resulting in a larger carbon footprint [69]. Our study improves performance and increases runtime very slightly, but the runtime increase is not very significant. However, in future research, it will also be important to study and improve our GFSA by taking carbon footprints into account. ", "page_idx": 18}, {"type": "text", "text": "GFSA improves the performance of existing Transformer-based models, which can have many positive impacts on society through services that utilize natural language processing, computer vision, and speech recognition. However, it will also be important to improve GFSA by considering other dimensions of AI, such as robustness to adversarial examples, fairness, and explainability. ", "page_idx": 18}, {"type": "text", "text": "C Oversmoothing and Additional Visualizations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Fig. 2, we show the visualizations of oversmoothing characteristics in DeiT. We also provide visualizations in other domains. We show the filter response, cosine similarity, and singular value of BERT finetuned on STS-B dataset of GLUE tasks in Fig. 5 and Graphormer finetuned on ZINC dataset in Fig. 6. ", "page_idx": 18}, {"type": "text", "text": "To characterize self-attention, we first analyze the filter response of self-attention in the frequency domain. We follow the method used by Wang et al. [80] for spectral visualization of the self-attention matrix. As shown in Fig. 2 (a), DeiT has a near-zero magnitude for the high frequencies, which is characteristic of a low-frequency fliter and is likely to result in oversmoothing when applied multiple times. ", "page_idx": 18}, {"type": "text", "text": "We follow the calculation method of Guo et al. [28] for cosine similarity. As shown in Fig. 2 (b), the higher similarity as the layers of the model get deeper is related to the oversmoothing problem. To further analyze this issue, we also consider the dimensionality collapse in Transformer-based models. We plot the singular value distribution of the feature in the last block. As shown in Fig. 2 (c), insignificant, near-zero values dominate the feature distribution. As layers get deeper, the similarity of features increases and dimensional collapse occurs. The oversmoothing problem is the same in BERT and Graphormer, as shown in Fig. 5 and Fig. 6. ", "page_idx": 18}, {"type": "image", "img_path": "ffNrpcBpi6/tmp/099ebdcc2cbda727d3e4149675d4e85e35aeea4f35dd109f3e0de3f6fc2675a3.jpg", "img_caption": ["Figure 5: Filter frequency response, cosine similarity, and singular values on STS-B for BERT and BERT $^+$ GFSA "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "ffNrpcBpi6/tmp/d204d30418821636a38666ff907c276e2f28c6cb422c8b2bd2a93fbe67171f32.jpg", "img_caption": ["Figure 6: Filter frequency response, cosine similarity, and singular values on ZINC for Graphormer and Graphormer+GFSA "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Analysis of Frequency Responses with Visualizations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We analyze the frequency responses, which represent the impact of learned coefficients, for all 12 layers of BERTBASE with and without GFSA. From Fig. 7, our analysis reveals that GFSA learns various fliter types between layers. In early layers, we observe a tendency towards low-pass flitering, with prominent peaks at low frequencies. This aligns with the need for broader feature extraction in initial layers. The middle layers show a mix of low-pass and high-pass characteristics, with more complex frequency responses. This suggests GFSA is learning to balance between feature extraction and refinement. In deep layers, there is a noticeable shift towards higher frequency responses, indicating a move towards high-pass flitering. This shift supports our claim that GFSA can mitigate oversmoothing in deeper layers. BERTBASE+GFSA shows a consistently higher magnitude response at higher frequencies, especially in deeper layers, compared to vanilla BERT. In other word, vanilla self-attention works primarily as a low-pass fliter, while GFSA utilizes a wider range of frequencies. ", "page_idx": 19}, {"type": "image", "img_path": "ffNrpcBpi6/tmp/c990926395d1eb221e9258bca0aa74c77e685d3cc373d6416db9162b0541b3a8.jpg", "img_caption": ["Figure 7: Visualization of the frequency responses for all 12 layers of BERT trained on STS-B dataset. The top-left figure corresponds to the first layer, and the bottom-right figure corresponds to the last layer. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Frequency Analyses in the Singular Value Domain ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Graph signal processing (GSP) [67, 68] can be understood as a generalized concept of DSP \u2014 in other words, DSP is a special case of GSP where a line graph with n nodes is used and therefore, the graph Fourier transform (GFT) of the line graph is identical to the discrete Fourier transform. ", "page_idx": 20}, {"type": "text", "text": "In the definition of GFT, we assume that the graph shift operator (GSO) $\\boldsymbol{S}$ is diagonalizable. Considering the eigendecomposition of the GSO $S=V\\uptau\\Lambda V$ with eigenvector $V$ , we can write the graph filter output as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\ny=\\sum_{k=0}^{K}w_{k}S^{k}x=\\sum_{k=0}^{K}V^{\\top}w_{k}\\Lambda^{k}V x=V^{\\top}\\big(\\sum_{k=0}^{K}w_{k}\\Lambda^{k}\\big)V x=V^{\\top}g(\\Lambda)V x,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\textbf{\\em x}\\in\\mathbb{R}^{n}$ is a 1-dimensional graph signal, $\\pmb{\\Lambda}$ is a diagonal matrix with eigenvalues, and $w_{k}\\in[-\\infty,\\infty]$ is a coefficient. ", "page_idx": 20}, {"type": "text", "text": "However, one can use the singular value decomposition, when the GSO is not diagonalizable but symmetrically normalized, instead of the eigendecomposition [49]. Both the singular value decomposition and the eigendecomposition project the original signal onto a set of basis, but they use different basis sets. In the singular value decomposition, we sort the set of basis in ascending order of their eigenvalues, and perform frequency domain-like analyses [106, 49]. ", "page_idx": 20}, {"type": "text", "text": "Since the self-attention matrix\u2019s row-wise sum is always 1, the following is the case: $\\bar{\\pmb{A}}=\\pmb{D}^{-1}\\pmb{A}=$ $\\textstyle{\\frac{1}{n}}A$ , where $n$ is the number of tokens. Maskey et al. [49] define the following symmetrically normalized adjacency matrix (SNA): $D_{i n}^{-1/2}A D_{o u t}^{-1/2}$ . Since the degree of every node is $n$ in the self-attention matrix, the following is the case: $\\begin{array}{r}{D_{i n}^{-1/2}A D_{o u t}^{-1/2}=D^{-1/2}A D^{-1/2}=\\frac{1}{\\sqrt{n}}A\\frac{1}{\\sqrt{n}}=}\\end{array}$ ${\\textstyle{\\frac{1}{n}}}A={\\bar{A}}$ . Therefore, the self-attention matrix is a special case of SNAs. ", "page_idx": 20}, {"type": "text", "text": "F Matrix Polynomial vs. Graph Fourier Transform ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "There are two paradigms of implementing graph fliters: i) matrix polynomial, which does not require diagonalizability, and ii) graph Fourier transform, which uses the eigendecomposition for diagonalizable adjacency matrices or uses the Jordan decomposition or the singular value decomposition for non-diagonalizable adjacency matrices. ", "page_idx": 20}, {"type": "text", "text": "Those two paradigms have their own weaknesses: i) the matrix polynomial approach requires explicit matrix multiplications, and ii) the graph Fourier transform approach requires expansive spectral decompositions. The matrix polynomial is preferred when there are not many matrix multiplications. Otherwise, the graph Fourier transform approach may be better since the matrix multiplication can be simplified after the decomposition. ", "page_idx": 20}, {"type": "text", "text": "Among those two, we use the first matrix polynomial approach with only three non-zero coefficients $\\{w_{0},w_{1},w_{K}\\}$ since it does not require the complicated spectral decomposition. Since we do not rely on any explicit spectral decomposition but on the matrix polynomial, any adjacency matrix can be used. ", "page_idx": 20}, {"type": "text", "text": "G Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem 3.1 (Filter characteristics based on coefficient values). Let $\\bar{A}$ be a self-attention matrix interpreted as a graph with connected components. Consider the polynomial graph filter defined by $\\sum_{k=0}^{K}w_{k}\\bar{A}^{k}$ , where $w_{2},w_{3},\\dots,w_{K-1}\\,=\\,0$ and only $w_{0},\\ w_{1}$ , and $w_{K}$ are non-zero. If the co efficients $w_{k}$ for $k=0,1,K$ are positive and their sum is $^{\\,l}$ , then the polynomial filter acts as a low-pass fliter, attenuating high-frequency components and promoting smoothness across the graph. Conversely, if $w_{k}=(-\\alpha)^{k}$ for $k=0,1,K$ and $\\alpha\\in(0,1)$ with sufficient large $K$ , the polynomial filter exhibits high-pass filter behavior. ", "page_idx": 20}, {"type": "text", "text": "Note that without filtering, the singular value ratio is $\\left|\\sigma_{i}^{0}\\right|/\\left|\\sigma_{1}^{0}\\right|\\ =\\ 1$ . In the case where $|g(\\sigma_{i})/g(\\sigma_{1})|\\,<\\,1\\;\\forall i\\,\\geq\\,2$ , it implies that after applying the graph filter $g$ , the lowest frequency component further dominates, indicating that the graph fliter acts as a low-pass fliter. Conversely, in the case where $|g(\\sigma_{i})/g(\\sigma_{1})|>1\\,\\forall i\\geq2$ , it implies that after applying the graph fliter $g$ , the lowest frequency component $\\sigma_{i}$ no longer dominates, indicating that the graph fliter acts as a high-pass fliter. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Proof. We prove the low-pass fliter result. For the case where $w_{0},\\,w_{1}$ , and $w_{K}$ are positive and their sum is 1, we show that ", "page_idx": 21}, {"type": "equation", "text": "$$\n|g(\\sigma_{1})|=|w_{0}+w_{1}+w_{K}|=1\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, proving Theorem $\\mathrm{G}$ is equivalent to show $|g(\\sigma_{i})|<1$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|g(\\sigma_{i})|=\\big|w_{0}+w_{1}\\sigma_{i}+w_{K}(\\sigma_{i}+(K-1)(\\sigma_{i}^{2}-\\sigma_{i}))\\big|<|w_{0}+w_{1}\\sigma_{i}+w_{K}\\sigma_{i}|=\\sigma_{i}<1}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nz\\sigma_{i}+(K-1)(\\sigma_{i}^{2}-\\sigma_{i})=\\sigma_{i}((K-1)\\sigma_{i}-(K-2))<\\sigma_{i}((K-1)-(K-2))=\\sigma_{i}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the high-pass filter result, when $w_{k}=(-\\alpha)^{k}/(k+1)$ where $k=0,1,K$ and $\\alpha\\in(0,1)$ , then we show that when $w_{0}=1,w_{1}=-\\alpha/2$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\frac{\\operatorname*{lim}_{K\\rightarrow\\infty}g(\\sigma_{i})}{\\operatorname*{lim}_{K\\rightarrow\\infty}g(\\sigma_{1})}\\right|=\\left|\\frac{\\operatorname*{lim}_{K\\rightarrow\\infty}w_{0}+w_{1}\\sigma_{i}+w_{K}(\\sigma_{i}+(K-1)(\\sigma_{i}^{2}-\\sigma_{i}))}{\\operatorname*{lim}_{K\\rightarrow\\infty}w_{0}+w_{1}+w_{K}(1+(K-1)(1-1))}\\right|}\\\\ &{=\\left|\\frac{\\operatorname*{lim}_{K\\rightarrow\\infty}1-\\frac{\\alpha}{2}\\sigma_{i}+\\frac{(-\\alpha)^{K}}{(K+1)}(\\sigma_{i}+(K-1)(\\sigma_{i}^{2}-\\sigma_{i}))}{\\operatorname*{lim}_{K\\rightarrow\\infty}1-\\frac{\\alpha}{2}+\\frac{(-\\alpha)^{K}}{(K+1)}}\\right|}\\\\ &{=\\left|\\frac{1-\\frac{\\alpha}{2}\\sigma_{i}+(\\operatorname*{lim}_{K\\rightarrow\\infty}\\frac{(-\\alpha)^{K}}{(K+1)}(\\sigma_{i}+(K-1)(\\sigma_{i}^{2}-\\sigma_{i})))}{1-\\frac{\\alpha}{2}}\\right|}\\\\ &{=\\left|\\frac{1-\\frac{\\alpha}{2}\\sigma_{i}}{1-\\frac{\\alpha}{2}}\\right|>1}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{lim}_{K\\to\\infty}\\frac{(-\\alpha)^{K}}{(K+1)}(\\sigma_{i}+(K-1)(\\sigma_{i}^{2}-\\sigma_{i}))=\\operatorname*{lim}_{K\\to\\infty}\\frac{(-\\alpha)^{K}}{(K+1)}(K-1)(\\sigma_{i}^{2}-\\sigma_{i})}}\\\\ &{}&{=\\operatorname*{lim}_{K\\to\\infty}(-\\alpha)^{K}\\frac{(K+1)}{(K-1)}(\\sigma_{i}^{2}-\\sigma_{i})=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It shows that the graph fliter with $w_{k}=(-\\alpha)^{k}/(k+1)$ for $k=1,2,K$ emphasizes high-frequency components and acts as a high-pass filter. ", "page_idx": 21}, {"type": "text", "text": "This proof supports that the behavior of the polynomial filter as either a low-pass or high-pass filter directly depends on the sign and values of the coefficients, as specified in Theorem 3.1. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "H Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. The Frobeinus norm of the self-attention is directly related to how far the softmax probabilities are from being uniform. For any matrix $M\\in\\mathbb{R}^{m\\times n}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathrm{softmax}(M)\\|_{F}=\\sqrt{\\frac{m+\\sum_{i=1}^{m}d_{\\chi^{2}}(S_{i},U_{n})}{n}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $S_{i}$ is the $i$ -th row of softmax $(M)$ , $U_{n}$ is the uniform distribution over $n$ elements, and $\\begin{array}{r}{d_{\\chi^{2}}(p,q)\\,=\\,\\sum_{i}q_{i}(p_{i}/q_{i}\\,-\\,1)^{2}}\\end{array}$ is the $\\chi^{2}$ -divergence between $p$ and $q$ [14]. The Frobenius norm is maximized when the whole mass of the pr\u221aobabilities is on one element, which is a case for $d_{\\chi^{2}}(S_{i},U_{n})=n-1$ and $\\Vert\\operatorname{softmax}(M)\\Vert_{F}=\\sqrt{m}$ . Therefore, we can calculate the upper bound of Frobenius norm for $\\bar{A}$ as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|{\\bar{A}}\\|_{F}\\leq{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that $\\bar{A}\\,\\in\\,\\mathbb{R}^{n\\times n}$ is a right-stochastic matrix normalized with row-wise softmax: i) all the elements of $\\bar{A}$ lie within [0, 1], and ii) the row-sum in $\\bar{A}$ is equal to 1. Since the self-attention matrix ", "page_idx": 21}, {"type": "text", "text": "is a right-stochastic matrix, the power of the self-attention is also a right-stochastic matrix. Therefore, Eq. (21) is also hold for $\\bar{A}^{K}$ as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\bar{A}^{K}\\|_{F}=\\sqrt{\\sum_{i,j}\\bar{A}_{i,j}^{K}}\\le\\sqrt{\\sum_{i,j}\\bar{A}_{i,j}}=\\|\\bar{A}\\|_{F}\\le\\sqrt{n}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, considering the error term $E_{K}$ as given by Theorem 4.1, and applying the triangle inequality for matrix norms: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{K}=\\|\\bar{\\cal A}^{K}-(\\bar{\\cal A}+(K-1)(\\bar{\\cal A}^{2}-\\bar{\\cal A}))\\|_{F}}\\\\ &{\\quad\\quad\\leq\\|\\bar{\\cal A}^{K}\\|_{F}+\\|\\bar{\\cal A}\\|_{F}+(K-1)(\\|\\bar{\\cal A}^{2}\\|_{F}+\\|\\bar{\\cal A}\\|_{F})}\\\\ &{\\quad\\quad\\leq\\sqrt{n}+\\sqrt{n}+(K-1)(\\sqrt{n}+\\sqrt{n})=2\\sqrt{n}K.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "I Implementation of GFSA ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The pseudo code of our GFSA is shown in Algorithm 1. For implementation, $w_{0}$ and $w_{1}$ can be set as hyperparameters optionally. ", "page_idx": 22}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/78631ec0f68d1a6974b7ed6291f2446a6d8984b6804abb00cb95d60225663dd6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "J Comparison with Actual and Approximated High-order Terms ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To compare the impact of the actual $\\bar{A}^{K}$ and the approximated $\\bar{A}^{K}$ in terms of accuracy, we experimented with BERT on GLUE and the results are summarized in Table 9. BERTBASE $\\mathbf{\\bar{+}}\\bar{A}^{K}$ denotes using the exactly calculated $\\bar{A}^{K}$ instead of the approximated $\\bar{A}^{K}$ . ", "page_idx": 22}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/87ab35c4a9881bd7ec5877530520a8af3119acd4681cf73f7e3a805997756e08.jpg", "table_caption": ["Table 9: Comparison of performance using the exactly calculated $\\bar{A}^{K}$ vs. the approximated $\\bar{A}^{K}$ for GLUE tasks "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "K Natural Language Understanding ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "K.1 Detailed Experimental Settings ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We integrate GFSA into 3 pre-trained large language models: BERT, ALBERT, and RoBERTa. We evaluate them on the GLUE benchmark, which includes 3 categories of natural language understanding tasks: i) single-sentence tasks CoLA and SST-2; ii) similarity and paraphrasing tasks MRPC, QQP, and STS-B; iii) natural language inference tasks MNLI, QNLI, and RTE. For MNLI task, we experiment on both the matched (MNLI-m) and mismatched (MNLI-mm) versions. Following Devlin et al. [16], we report Matthews correlation for CoLA, F1 scores for QQP and MRPC, Spearman correlations for STS-B, and accuracy scores for the other tasks. For each task, we select the best hyperparameters for GFSA, and the other hyperparameters are fixed. We compare our GFSA with ContraNorm [28], one of the related methods that address oversmoothing. We finetune ContraNorm with the recommended hyperparameters in Guo et al. [28]. We initialize with a pre-trained language model and finetune with added GFSA for 5 epochs. ", "page_idx": 23}, {"type": "text", "text": "Dataset. The benchmark datasets we used are listed below. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 CoLA. The Corpus of Linguistic Acceptability [85] consists of English acceptability judgments drawn from books and journal articles. The target task is a binary classification task, and each sentence is determined to be grammatically acceptable or not.   \n\u2022 SST-2. The Stanford Sentiment Treebank [73] is a dataset in which each sentence is sourced from movie reviews and accompanied by human annotations of their sentiment. The target task is to classify binary sentiments for a single sentence.   \n\u2022 MRPC. The Microsoft Research Paraphrase Corpus [17] is a corpus of sentence pairs, which are automatically extracted from online news sources and annotated by humans. The target is to determine whether the sentences in the pair are semantically equivalent.   \n\u2022 QQP. The Quora Question Pairs [11] dataset is a collection of question pairs from the community question-answering website Quora. The target is to determine whether the questions in the pair are semantically equivalent.   \n\u2022 STS-B. The Semantic Textual Similarity Benchmark [8] is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data with human annotation. The target is a regression task to predict a similarity score from 0 to 5.   \n\u2022 MNLI. The Multi-Genre Natural Language Inference Corpus [87] is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The standard test set consists of private labels from the authors and evaluates both the matched (in-domain) and mismatched (cross-domain) sections.   \n\u2022 QNLI. The Stanford Question Answering [82] dataset is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph contains the answer to the corresponding question written by an annotator. The task is to determine whether the context sentence contains the answer to the question.   \n\u2022 RTE. The Recognizing Textual Entailment [5] dataset comes from a series of annual textual entailment challenges. The target task is a binary entailment classification task. ", "page_idx": 23}, {"type": "text", "text": "BERT. BERT [16] consists with 12 layers, 12 heads, 768 hidden size, 512 maximum sequence length, and MLP dimension of 3072. ", "page_idx": 23}, {"type": "text", "text": "ALBERT. ALBERT [40] consists of 12 layers, 12 heads, 768 hidden dimensions, 512 maximum sequence length, 128 embedding dimensions, and MLP dimension of 3072. ", "page_idx": 23}, {"type": "text", "text": "RoBERTa. RoBERTa [44] consists of 12 layers, 12 heads, 768 hidden size, 514 maximum sequence length, and MLP dimension of 3072. ", "page_idx": 23}, {"type": "text", "text": "Training. For implementation, we adopt HuggingFace framework. We trained all models with 5 epochs with 32 batch size. The linear learning rate decay is used and initial learning rate is set to $2^{^{\\star}\\times10^{-5}}$ . We use AdamW [46] optimizer, and weight decay is set to 0. All models are trained on 1 GPU and of NVIDIA RTX A5000 24GB. ", "page_idx": 24}, {"type": "text", "text": "K.2 Sensitivity to $K$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we explore the influence of the polynomial order, denoted as $K$ , in our GFSA, conducting experiments on BERTBASE finetuned with GLUE tasks. We search for values of $K$ from 2 to 10, and the results are presented in Table 10. For each dataset, there is an optimal $K$ and the performance of models using GFSA is generally robust to changes in $K$ . ", "page_idx": 24}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/072fbfc965bf44ebe8987e9b6e8ea4dc5b322dc538b85f009fa4a5f656511329.jpg", "table_caption": ["Table 10: Sensitivity results on various $K$ with BERTBASE finetuned on GLUE tasks "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "L Causal Language Modeling ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "L.1 Detailed Experimental Settings ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Dataset. The benchmark datasets we used are listed below. ", "page_idx": 24}, {"type": "text", "text": "\u2022 PTB. Penn Treebank [47] dataset is a collection of text documents that have been extensively annotated with linguistic information, primarily syntactic and grammatical structures. \u2022 WikiText. WikiText [50] dataset is a collection of over 100 million tokens extracted from the set of verified good and featured articles on Wikipedia. Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. ", "page_idx": 24}, {"type": "text", "text": "GPT2. GPT2 [61] is a Transformer pretrained on a very large corpus of English data in a selfsupervised fashion without any human labelling on dataset. It automatically generate inputs and labels from those texts, and trained to guess the next word in sentences. For implementation, we adopt HuggingFace Framework 4. For all experiments, GPT2 has 12 layers with 12 attention heads, 768 hidden size and 1024 maximum sequence length, resulting in a total of 117 million parameters. ", "page_idx": 24}, {"type": "text", "text": "Training. We finetune GPT2 with 4 batch size, $5\\times10^{-5}$ learning rate and linear learning weight decay using adamW [46] optimizer. We also apply dropout with probability 0.1. Following [93], we train models for 15 epochs with PTB, 4 epochs with WikiText-103 and 10 epochs with WikiText-2. We use sensitivity metric, i.e., perplexity, which is a commonly used metric to evaluate the performance of language models, particularly in language modeling and text generation tasks. perplexity measures how well a language modeling can predict a sequence of words in a given text or a test dataset. All the experiments are conducted on 1 GPU and of NVIDIA RTX 3090 24GB. ", "page_idx": 24}, {"type": "text", "text": "L.2 Sensitivity to $K$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We conducted a sensitivity study on $K$ of GPT-2 across all datasets, and the results are presented in Table 11. For PTB and WikiText-2, GFSA exhibits the best performance when $K$ is high, typically ", "page_idx": 24}, {"type": "text", "text": "around 8 or 9. However, for WikiText-103, GFSA achieves the best perplexity when $K$ is small, specifically when $K$ is 3 or 4. ", "page_idx": 25}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/067c6f35461f2a2936a1db2baadb16fd18fb92a0ecd259c986f8dcdcd287fef8.jpg", "table_caption": ["Table 11: Results comparison on GPT-2 finetuned with GFSA "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "M Image Classification ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "M.1 Detailed Experimental Settings ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our code is implemented based on the timm library [86]. In the case of our training recipe, it is the same as experimental setting of Wang et al. [80] that follows the training recipes of Touvron et al. [74] and Touvron et al. [75]. To apply our GFSA to existing base models such as DeiT, Cait, and Swin, we consider a range of $K$ between 2 and 5. For 12-layer DeiT, we follow the same hyperparameters from Wang et al. [80]. We set the dropout rate to 0 and 0.2 for 12-layer and 24-layer DeiT, respectively. For CaiT, we apply our GFSA on only to the patch embedding layer. All other hyper-parameters are kept consistent with the original papers of DeiT [74], CaiT [75] and, Swin [45]. All models are trained on NVIDIA RTX 3090 24GB. ", "page_idx": 25}, {"type": "text", "text": "M.2 FLOPs & Throughput ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In Table 12, we report the number of FLOPs and throughput. With GFSA plugged in, the FLOP count is either the same or no different. For DeiT-S with 24 layers, which shows a slight FLOP increase with GFSA plugged in. However, for the rest of the settings, the models have the same number of Flops. For throughput, it tends to decrease because calculating the high-order term is an additional cost. ", "page_idx": 25}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/adc0f8042cbd2b89acbf97ab31dbfde93dc494cd7270102900d29efbdb79184e.jpg", "table_caption": ["Table 12: Experimental evalutation of GFSA plugged into DeiT-S, CaiT-S, and Swin-S "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "M.3 Sensitivity to $K$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We also perform the sensitivity analysis for $K$ . Tables 13 and 14 show the results of sensitivity analysis for DeiT-S and CaiT-S with GFSA plugged in. For 12-layer DeiT-S, GFSA performance of 81.12 is highest when $K=3$ . When the GFSA has a $K$ of 2, the performance is worse than the ", "page_idx": 25}, {"type": "text", "text": "original DeiT-S, but when the $K$ is 3 or higher, the performance is better than the original DeiT-S, and most surprisingly, the performance is better than the 24-layer DeiT-S. ", "page_idx": 26}, {"type": "text", "text": "CaiT-S shows the highest performance of 82.84 when $K=4$ . For CaiT-S, the accuracy is slightly lower than that of the original CaiT-S when $K=2$ , but it starts to exceed the accuracy of CaiT-S when $K$ is 3 or higher. ", "page_idx": 26}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/5dc0ed40cf1021ad8900806b088373c2951eece36e9e37c44b1d1b50898e5092.jpg", "table_caption": ["Table 13: Sensitivity to $K$ for 12-layer DeiT-S $^+$ Table 14: Sensitivity to $K$ for 24-layer CaiT- $\\cdot S+$ GFSA GFSA "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/b96e52464fd0ca6f9f39e9b194aeb773c38f42fce4335f65eadfd82bc1cc0d59.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "M.4 Full Experimental Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In Table 15, we consider all three classes, CNN only, $\\mathrm{CNN+}$ Transformer, and pure Transformer, to compare more different models than in Table 3. In particular, in the Transformer category, we only test with lightweight models with similar number of parameters, such as ViT-S and DeiTS. Compared to existing techniques, the improvements by GFSA already surpasses LayerScale $(0.7\\%)$ [75], LateInsertion $(0.6\\%)$ [75], and HAT [4] $(1.38\\%)$ . ", "page_idx": 26}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/09cdf45547e81308a4832e9b11a05061a5b3d5127371d94572e8ff98698f24a2.jpg", "table_caption": ["Table 15: Compared with state-of-the-art models on ImageNet-1k dataset. The number in $(\\uparrow)$ indicates the performance improvement over the base model. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Our main experiment aims to determine whether introducing the GFSA layer would help improve performance in a base model, such as DeiT. We also compare our method with the recent models: SpectFormer [58], SVT [57], NeuTRENO [52], FNet [42], and GFNet [64]. We use a 12-layer setup to ensure a fair comparison in Table 16. ", "page_idx": 27}, {"type": "text", "text": "GFNet [64] can reduce the number of parameters, but there is a performance penalty. However, the performance improvement of DeiT- $\\cdot S+$ GFSA is relatively greater than DeiT-S compared to other models. SpectFormer [58] and SVT [57] have advantages in calculation amount and model complexity, and performance is improved over DeiT-S, but Top-1 and Top-5 accuracies are lower than those using GFSA. Additionally, NeuTRENO [52] also improves as much as GFSA compared to DeiT-S, but GFSA still has higher Top-1 accuracy. ", "page_idx": 27}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/0aecd1c9394bc09b789195ebfd2f24c222a5efa724ea8ff3fb0922bc00e0354a.jpg", "table_caption": ["Table 16: Compared with state-of-the-art models on ImageNet-1k "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "M.6 Additional Experiments with Guo et al. [28]\u2019s setting ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To make a fair comparison with ContraNorm [28], one of the related studies that mitigates oversmoothing, we run additional experiments to match their experimental setup. ", "page_idx": 27}, {"type": "text", "text": "Setting. We follow the training recipe used by Guo et al. [28], which is a slightly modified version of Touvron et al. [74]\u2019s recipe. Guo et al. [28] use AdamW optimizer with cosine learning rate decay. We select the DeiT-T and DeiT-S for ImageNet-1k. \u201cT\u201d and \u201cS\u201d denote tiny and small model sizes, respectively. For all experiments, the image size is set to be $224\\!\\!\\!\\times\\!224$ . We train each model for 300 epochs and the batch size is set to 1024. For ContraNorm, we train with their recommended hyperparameters. All models are trained on 4 GPUs and of NVIDIA RTX A6000 48GB. ", "page_idx": 27}, {"type": "text", "text": "Results. In Table 17, DeiT-T and DeiT-S with GFSA outperform vanilla DeiT-T and DeiT-S in all layer settings. GFSA improves the performance of DeiT-T with 12 layers by $1.52\\%$ . The largest gain is a $4.88\\%$ improvement on 16-layer DeiT-T. This shows that the effect of GFSA is larger than the effect of ContraNorm. For DeiT-S with 16 layers, surprisingly, GFSA is able to increase the performance by $80.83\\%$ , meaning that GFSA brings benefits with a $3.23\\%$ improvement. ", "page_idx": 27}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/fd8315e9d99fdeb236afa42fd3f5beb6bdbe038856be05a212a53772432b5180.jpg", "table_caption": ["Table 17: Experiment results on ImageNet-1k "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Sensitivity to $K$ . In Table 18, we experiment with a sensitivity analysis for $K$ . For DeiT-T, the performance of GFSA generally improves when $K$ is 4 or 5. On the other hand, GFSA performs better at lower $K$ for settings that are layers 16 and 24 for DeiT-S. ", "page_idx": 27}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/1ffc3179e7c495a2380f54d17a256fad1d890e662b986df748b10e626ad13536.jpg", "table_caption": ["Table 18: Varying $K$ for DeiT-T and DeiT-S "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "N Automatic Speech Recognition ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "N.1 Detailed Experimental Settings ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Dataset. We conduct experiments on the LibriSpeech 5 dataset [55], which consists of audio recordings paired with their transcriptions. The LibriSpeech dataset has approximately 1,000 hours of read English speech with a sampling rate of $16\\,\\mathrm{kHz}$ . We keep the original 16,000Hz sampling rate and compute 80-dim log-Mel fliterbanks for a $25\\mathrm{ms}$ sliding window, strided by $10\\mathrm{ms}$ . The fliterbank features are then normalized to zero mean and unit variance per input sequence. For implementation, we follow the recipes of SpeechBrain [65]. ", "page_idx": 28}, {"type": "text", "text": "Evaluation Metric. Word error rate (WER $(\\%)$ ) is derived from the Levenshtein distance and compares a reference to a hypothesized word-level transcription. It is calculated by summing the number of word insertions, deletions, substitutions and dividing it by the total number of words in the reference transcription. ", "page_idx": 28}, {"type": "text", "text": "Vanilla Transformer. We use a vanilla Transformer to apply our GFSA. For implementation, we use a SpeechBrain [65] framework. The vanilla Transformer consists of i) 1D convolution to perform striding, ii) Transformer encoder with 12 layers, 4 heads, embedding dimension of 512, MLP dimension of 2048, and post-LayerNorm iii) decoder with 6 layers, 4 heads, embedding dimension of 512, MLP dimension of 2048, joint beamsearch, and iv) external Transformer language model with 12 layers, 12 heads, embedding dimension of 768, and MLP dimension of 3072. ", "page_idx": 28}, {"type": "text", "text": "Branchformer. We use one of the SOTA models, Branchformer [59] to plug-in our GFSA. Branchformer has two parallel branches, one for capturing global interactions using attention and the other for more localized context using convolutional gating MLP. The Branchformer architecture for speech recognition consists of i) 1D convolution to perform striding, ii) Branchformer encoder with 18 layers, 8 heads, embedding dimension of 512, and MLP dimension of 3072, iii) decoder with 6 layers, 8 heads, embedding dimension of 512, a convolutional spatial gating unit (CSGU) dimension of 3072, joint beamsearch, and iv) external Transformer language model with 12 layers, 12 heads, embedding dimension of 768, and MLP dimension of 3072. ", "page_idx": 28}, {"type": "text", "text": "Training. We follow a training recipe from SpeechBrain [65]. The standard LibriSpeech validation sets (dev-clean and dev-other) are used to tune all parameters and select the best models. Test sets (test-clean and test-other) are used only to report final WER performance. We train the pure Transformer for 100 epochs and the Branchformer for 120 epochs with a batch size of 16. We use a data augmentation method on all models using SpecAugment [56]. SpecAugment applies time and frequency masking as well as time warping to the input spectrum. For Branchformer, we use AdamW [46] optimizer with 0.9 and 0.98 coefficients for computing running averages of gradient and its square. The learning rate and weight decay in all models are 0.0008 and 0.01, respectively. We use a connectionist temporal classification (CTC) loss [26, 100]. We also apply dropout with probability 0.1 and label smoothing with weight 0.1 to mitigate overfitting. We fix the random seed as 74443 on all experiments. All models are trained on 1 GPU and of NVIDIA RTX A6000 48GB. ", "page_idx": 28}, {"type": "text", "text": "Hyperparameters. In Table 19, we describe main hyperparameters used in the automatic speech recognition task. For Transformer+GFSA and Branchformer+GFSA, we also report the best $K$ hyperparameter. ", "page_idx": 29}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/0270393ce2b7e0531a27973e00b12754eb9e27b3bf04eea1edff43a9bae650c1.jpg", "table_caption": ["Table 19: Main hyperparameters used in ASR "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "N.2 Training Curve ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We compare the training and validation curves for LibriSpeech 100h in Fig. 8. The training loss curve of GFSA is lower than the pure Transformer. GFSA stabilizes the loss curve of pure Transformer slightly earlier. ", "page_idx": 30}, {"type": "image", "img_path": "ffNrpcBpi6/tmp/91bd02a3f5618f4cd0356a64a91610e6b65c6533183c0d7bb65717847240d450.jpg", "img_caption": ["Figure 8: Training curve on LibriSpeech 100h "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "O Graph-level Tasks ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "O.1 Detailed Experimental Settings ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Experimental settings for Graphormer. We describe benchmark datasets and Graphormer as the backbone model we used. ZINC [35] is the most popular real-world molecular dataset to predict graph property regression for constrained solubility, an important chemical property for designing generative GNNs for molecules. Uniform sampling is adopted for data splitting. We use a ZINCsubset of small-scale dataset. PCQM4M-LSC [33] is 2D molecular graphs, which is one of the most practically relevant quantum chemical properties of molecule science. The task is to predict density functional theory (DFT)-calculated HOMO-LUMO energy gap of molecules given their graphs. PCQM4M-LSC is unprecedentedly large in scale comparing to other labeled graph-level prediction datasets, which contain more than 3.8M graphs. We use PCQM4M and PCQM4Mv2 large-scale datasets. ", "page_idx": 30}, {"type": "text", "text": "Following Graphormer [94], we use Graphormer for PCQM4M and GraphormerSLIM for ZINC. Graphormer consists of 12 encoder layers, 80 encoder embedding dimension, and 768 MLP dimension. It employs 32 encoder heads and 24 hidden dimension for each head. GraphormerSLIM consists of 12 encoder layers, 80 encoder embedding dimension, and 80 MLP dimension. It employs 8 encoder heads and 10 hidden dimension for each head. We use adamW [46] optimizer with 0.9 and 0.999 coefficients for running averages of gradient and its square, and use Mean Absolute Error (MAE) as loss function. We use polynomial learning rate decay, with initial learning rate set to $2\\times10^{-4}$ and end learning rate set to $\\bar{1}\\times10^{-9}$ . For ZINC, we set batch size as 256, max epochs as 10k, and warm-up stage step as $40\\mathrm{k}$ . For PCQM4M and PCQM4Mv2, we set batch size as 1024, max epochs as 300, and warm-up stage step as $60\\mathrm{k}$ . All models are trained on 1 GPU and of NVIDIA RTX 3090 24GB. We conduct experiments with 4 different seeds. ", "page_idx": 30}, {"type": "text", "text": "Experimental settings for GPS and Graph-ViT. We use various benchmark datasets to experiment with our GFSA on GPS [63] and Graph-ViT [30]: Peptide-func and Peptide-struct from Long-Range Graph Benchmark (LRGB) [21], MNIST, CIFAR10, and ZINC from Benchmarking GNNs [22], and Moltox21 and Molhiv from OGB [32]. We fix all hyperparameters of GPS and Graph-ViT as recommended in their paper to ensure a fair comparison. To plug GFSA into GPS, we replace the self-attention module of GPS with GFSA. For Graph-ViT, we apply GFSA instead of the Hadamard self-attention mechanism and compare it with this self-attention method. We conduct experiments on GPS and Graph-ViT in their open code frameworks for a fair comparison: ", "page_idx": 30}, {"type": "text", "text": "\u2022 GPS: https://github.com/rampasek/GraphGPS \u2022 Graph-ViT: https://github.com/XiaoxinHe/Graph-ViT-MLPMixer ", "page_idx": 30}, {"type": "text", "text": "O.2 Experimental Results with Standard Deviation ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We conduct experiments following the experimental environments of Graphormer [94] using 4 different seeds. Due to space constraints, only the mean values are reported in Tables 4 and 5. In Tables 20 and 21, we report the results with mean and standard deviations. ", "page_idx": 31}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/109867051f3e627e3e90bb971252c269e38f3546be09965f61d54848e5af8fa1.jpg", "table_caption": ["Table 20: Experimental results and number of parameters on ZINC "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/c68314929e0a84503e251844fa411c154a41416049f78451ec861dfd673ab597.jpg", "table_caption": ["Table 21: Experimental results and number of parameters on PCQM4M and PCQM4Mv2 "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "P Code Defect Detection ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "P.1 Detailed Experimental Settings ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Dataset. We use Devign dataset provided by [105], which is a binary classification task to evaluate whether a C language code is vulnerable to software systems or not. ", "page_idx": 31}, {"type": "text", "text": "Implementation. We build our experiments on top of the open-sourced code 6 and recipes provided by Wang et al. [84]. ", "page_idx": 31}, {"type": "text", "text": "RoBERTa. RoBERTa [44] is an encoder-only model trained with masked language modeling on code. All hyperparameters are consistent with the training method in the source code of Wang et al. [84]. ", "page_idx": 31}, {"type": "text", "text": "PLBART. PLBART [2] is an encoder-decoder model based on BART [43] architecture. PLBART can support understanding and generation tasks. All hyperparameters are consistent with the training method in the source code of Wang et al. [84]. ", "page_idx": 31}, {"type": "text", "text": "CodeBert. CodeBERT [23] is a model trained on masked language modeling and replaced token detection. CodeBERT is a bimodal pretrained model based on Transformer with 12 layers for programming language and natural language. All hyperparameters are consistent with the training method in the source code of Wang et al. [84]. ", "page_idx": 31}, {"type": "text", "text": "CodeT5. CodeT5 is an encoder-decoder framework with the same architecture as T5 [62]. It aims to derive generic representations for programming language and natural language via pre-training on unlabeled source code. CodeT5-small has 6 encoder layers, 6 decoder layers, 8 attention heads, 512 dimensional hidden states, and 60M parameters. The other models have 12 encoder layers, 12 decoder layers, 12 attention heads, 768 dimensional hidden states, and 220M parameters. All hyperparameters are consistent with the training method in the source code of Wang et al. [84]. ", "page_idx": 31}, {"type": "text", "text": "Training. The pre-trained models mentioned above are applied to this downstream task. We add GFSA directly on top of self-attention. We finetune baselines and GFSA models for 10 epochs with a batch size of 16. We use early stopping strategy with a patience of 2. Models generate binary labels from unigram sequences at the decoder for defect detection task. We employ accuracy for evaluating the code defect detection task. All models are trained on 1 GPU and of NVIDIA RTX A6000 48GB. ", "page_idx": 31}, {"type": "text", "text": "P.2 Case Study ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In Listing 1, we show one of case for code snippets of defects in QEMU 7 that CodeT5-base does not predict correctly, but that only CodeT5-base+GFSA predicts. The commit message 8 for this case is as follow: ", "page_idx": 32}, {"type": "text", "text": "Needed for changing cpu_has_work() argument type to CPUState, used in h_cede(). ", "page_idx": 32}, {"type": "text", "text": "h_cede() is the hypercall that asks the hypervisor to shut down the CPU. Previously, this hypercall simply passed the CPUID, so the hypervisor did not know what state the CPU was in. This change allows the hypervisor to know whether the CPU is actually performing work. If the CPU is performing a task, the hypervisor waits for the CPU to complete the task. ", "page_idx": 32}, {"type": "text", "text": "In this context, accurately predicting defects like the one above is very important, and applying GFSA to CodeT5-base helps in terms of performance improvement. ", "page_idx": 32}, {"type": "text", "text": "1 @@ -204,7 +204 ,7 @@ static target_ulong put_tce_emu( sPAPRTCETable $^*$ tcet , target_ulong ioba , static target_ulong h_put_tce(CPUPPCState \\*env , sPAPREnvironment spapr   \n$3+$ static target_ulong h_put_tce(PowerPCCPU $^*$ cpu , sPAPREnvironment $^*$ spapr target_ulong opcode , target_ulong args) { target_ulong liobn $=$ args [0]; target_ulong ioba $=$ args [1];   \n8 target_ulong tce $=$ args [2];   \n9 VIOsPAPRDevice $^*$ dev $=$ spapr_vio_find_by_reg (spapr ->vio_bus , liobn);   \n10 VIOsPAPR_RTCE \\*rtce;   \n11 if (!dev) {   \n12 hcall_dprintf (\" LIOBN $_{0\\,\\tt X}$ \" TARGET_FMT_lx \" does not exist\\n\", liobn) ;   \n13 return H_PARAMETER;   \n14 }   \n15 ioba $\\&=$ \\~( SPAPR_VIO_TCE_PAGE_SIZE - 1);   \n16 #ifdef DEBUG_TCE   \n17 fprintf(stderr , \" spapr_vio_put_tce on %s ioba 0x\" TARGET_FMT_lx TCE $_{0\\,\\tt X}$ \" TARGET_FMT_lx \"\\n\", dev ->qdev.id , ioba , tce);   \n18 #endif   \n19 if (ioba $>=$ dev -> rtce_window_size ) {   \n20 hcall_dprintf (\"Out -of -bounds IOBA $_{0\\,\\tt X}$ \" TARGET_FMT_lx \"\\n\", ioba);   \n21 return H_PARAMETER;   \n22 }   \n23 rtce $=$ dev ->rtce_table $^+$ (ioba >> SPAPR_VIO_TCE_PAGE_SHIFT );   \n24 rtce ->tce $=$ tce;   \n25 return H_SUCCESS;   \n26 } ", "page_idx": 32}, {"type": "text", "text": "Q Code Clone Detection", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Q.1 Detailed Experimental Settings ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Dataset. Code clone detection aims to measure the similarity between two code snippets and predict whether they have the same functionality. We experiment with the Java data provided by Wang et al. [83]. ", "page_idx": 32}, {"type": "text", "text": "Implementation. We build our experiments on top of the open-sourced code 9 and recipes provided by Wang et al. [84]. ", "page_idx": 33}, {"type": "text", "text": "Training. We finetune both CodeT5 and CodeT5 $+$ GFSA for one epoch with a batch size of 16. We also use early stopping with patience of 2. CodeT5 and CodeT5 $^{\\downarrow+}$ GFSA encode source code and take the representation to calculate similarity of two code snippets. We employ F1 score for evaluating this task. All models are trained on 1 GPU and of NVIDIA RTX A6000 48GB. ", "page_idx": 33}, {"type": "text", "text": "Q.2 Experiment Result ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Table 22 shows results comparing CodeT5 and CodeT5 with GFSA. The result shows that by using our GFSA, CodeT5 models improve their performance. CodeT5-smal $^{+}$ GFSA provides a $0.61\\%$ improvment over Code5T-small. ", "page_idx": 33}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/f7c91e2e17fe001722814033879c0f870306d0e0e8c4d2b87c7c305cc08f0d86.jpg", "table_caption": ["Table 22: Results on the code clone detection task "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "R Time Complexity and Empirical Runtime Analysis ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Time Complexity. The time complexity of original self-attention is $O(n^{2}d)$ . But our GFSA has a high order term. Therefore, the time complexity of GFSA has $O(n^{2}d+n^{3})$ . If $n$ is smaller than $d$ , the time complexity approaches $O(n^{2}d)$ , which is the complexity of original self-attention. ", "page_idx": 33}, {"type": "text", "text": "Empirical Runtime Analysis. We report the training time of various methods with GFSA in Tables 23 to 28. In general, the training time of methods with GFSA is slightly longer than that of existing methods. For example, the Transformer for the automatic speech recognition task increases from 190.5 seconds to 191.6 seconds on Librispeech 100h dataset, as increases of only 1 second. Instead of computing higher-order polynomial terms, our GFSA approximates them, with only a small increase in runtime, which is not very significant. ", "page_idx": 33}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/a0455742c1ebd974a2ef79448960476e2c35c1768a6a5d47bd07a7df776bbeed.jpg", "table_caption": ["Table 23: Training time (seconds per epoch) on GLUE tasks. s denotes the abbreviation for second. Avg denotes the average training time across all tasks. "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/7a7c28a1c9447d556b2573e897c8ed506555e7ec3841b2921b1222f264233440.jpg", "table_caption": ["Table 24: Training time (seconds per epoch) on causal language modeling tasks. "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/1944e5d3125625e95e4a00474705396478905fb2c4c8828a626b3cbd44824453.jpg", "table_caption": ["Table 25: Training time (seconds per epoch) on ImageNet-1k "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/1b4854fa263e3fc6d3a06f47d3d071d9d2fffd3f066a733182903c362407548a.jpg", "table_caption": ["Table 26: Training time (seconds per epoch) on graph-level tasks "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/f6041e22756edbe3a26ff113350e7c8bafe7d0e8666333ed8abcb051708e7642.jpg", "table_caption": ["Table 27: Training time (seconds per epoch) on LibriSpeech datasets "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "Table 28: Training time (seconds per epoch) on the code defect prediction task ", "page_idx": 34}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/3033aede7abf1c42ee51ff583258bdc9fcad1230dff61fcc143493579ea4f0a7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "S Inference Time Analysis ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We report the inference time of various methods with GFSA in Tables 29 to 34. ", "page_idx": 35}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/4d381f6dae6e353fec5a6cad4e147fc404c6c1101a4c0d8c115729bcbd7e91a8.jpg", "table_caption": ["Table 29: Inference time on GLUE tasks. s denotes the abbreviation for second. Avg denotes the average training time across all tasks. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/cd4213e0edd0a40cd4deb889a09e2ef9983d5b0c0888e36189c6561b416ec2d6.jpg", "table_caption": ["Table 30: Inference time on causal language modeling tasks "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/20346121a32f13a223d1ae68eb4a3e377be5a91cec11a2cf1027d96f8e9267ef.jpg", "table_caption": ["Table 31: Inference time on ImageNet-1k "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/b9c624ad6a9caada63daae152e1e17b96f50be223461d402e34b028ef0d3804f.jpg", "table_caption": ["Table 32: Inference time on graph-level tasks "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/745e0b087b0d50db36c7145a84106d73476edf8578e22fddae7b3f16ca8c772c.jpg", "table_caption": ["Table 33: Inference time on LibriSpeech datasets "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/12680c39de56a2f8e3cffead8df522d72857545e6227d0eb8bb24b438f01f56f.jpg", "table_caption": ["Table 34: Inference time on the code defect prediction task "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "T Results of the Strategy for Efficiency ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In Tables 35 to 39, the results show that this strategy can reduce the increase in runtime and maintain performance compared to using GFSA for all layers. ", "page_idx": 37}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/be2b237853dd14b41c5fa61a61f474e6159f01ce233dd83f660042e9016f381c.jpg", "table_caption": ["Table 35: Comparison of performance using GFSA on all layers vs. $\\mathrm{GFSA_{\\mathrm{even}}}$ on even layers for GLUE tasks "], "table_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/96438dfb41c906dfd42c249b21db0197157dfeef5a2ba681c38032450f37d3f3.jpg", "table_caption": ["Table 36: Comparison of training time (seconds per epoch) using GFSA on all layers vs. $\\mathrm{GFSA_{\\mathrm{even}}}$ on even layers for GLUE tasks "], "table_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/8521dda128d1ebaa1a4eb70c4139cc4de70418ff649b1ff6c55f4abfc9c93ee3.jpg", "table_caption": ["Table 37: Comparison of performance using GFSA on all layers vs. $\\mathrm{GFSA_{\\mathrm{even}}}$ on even layers for causal language modeling tasks "], "table_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/fec962d4e76871f0b7803ef603fb70ff0c817c1c4586c149b3af8f15442b082c.jpg", "table_caption": ["Table 38: Comparison of training time (seconds per epoch) using GFSA on all layers vs. $\\mathrm{GFSA_{\\mathrm{even}}}$ on even layers for causal language modeling tasks "], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "Table 39: Comparison of using GFSA on all layers vs. $\\mathrm{GFSA_{\\mathrm{even}}}$ on even layers for ImageNet-1k ", "page_idx": 37}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/bd6b76ddd90e9b5b3e6ecc6e7b602dc6ea6597a8c5fecf932c2b1e10b1f21cdc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "U GFSA in Linear Transformers ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Table 40 shows the accuracy, runtime, and GPU usage results on Long Range Arena benchmark using ListOps and Image datasets. ", "page_idx": 38}, {"type": "table", "img_path": "ffNrpcBpi6/tmp/27b3df3577015f22d7ec14efe6a58b8f09ee0b2f916a4bfaec106a8db7b3825c.jpg", "table_caption": ["Table 40: Comparison of accuracy $(\\%)$ , runtime ( $s$ per 1,000 steps) and GPU usage (GB) on Long Range Arena benchmark "], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We clearly stated our claims in the introduction, including the contributions of the paper. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: In Section 6, we stated the computational overhead as a limitation of our method and provided strategies to address it. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We have included proofs for Theorem 4.1 and Theorem 3.1 in Appendix H and Appendix G. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our reproducibility statements are reported in A. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: See the statement about reproducibility in Appendix A. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Experimental environments, datasets, training methods are reported in Section 5.1 to 5.6 and in Appendix K to 5.6. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We tried to follow the experimental setup for benchmarking each task. Among the tasks we considered, it is conventional to report results from four experiments in the graph-level tasks task, so we report the mean and standard deviation together in the Appendix O.2. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 41}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Our paper specifies the type of computer resource information (e.g. GPU) for the experiment. We also indicated the amount of calculation or execution time for the experiment. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We discuss the potential positive and negative social impacts of our research in Appendix B. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: There is no such risk in our paper. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: In our paper, the authors or original owners of code, data, and models used are properly credited, and their licenses and terms of use are explicitly mentioned. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: The code we provided is well documented in the README and is anonymized. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: Our research does not include crowdsourcing experiments and studies with human subjects. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: Because our research does not involve human subjects, it is not subject to IRB approval. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 44}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]