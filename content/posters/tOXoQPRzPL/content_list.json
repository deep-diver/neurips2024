[{"type": "text", "text": "An Image is Worth 32 Tokens for Reconstruction and Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qihang $\\mathrm{\\mathbf{Y}\\mathbf{u}^{1^{*}}}$ , Mark Weber1,2\\*, Xueqing Deng1, Xiaohui Shen1, Daniel Cremers2, Liang-Chieh Chen1 ", "page_idx": 0}, {"type": "text", "text": "ByteDance 2 Technical University Munich \\* equal contribution https://yucornetto.github.io/projects/titok.html ", "page_idx": 0}, {"type": "image", "img_path": "tOXoQPRzPL/tmp/c1b55da7a0dba04d03d71408fe0725ecba4f97f8d15ff603052bb54d771a83e4.jpg", "img_caption": ["Figure 1: We propose TiTok, a compact 1D tokenizer leveraging region redundancy to represent an image with only 32 tokens for image reconstruction and generation. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images. Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process. Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors. However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities. To overcome this issue, we introduce Transformer-based 1-Dimensional Tokenizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences. TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques. For example, a $256\\times256\\times3$ image can be reduced to just 32 discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches. Specifically, using the same generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet $256\\times256$ benchmark. The advantages of TiTok become even more significant when it comes to higher resolution. At ImageNet $512\\times512$ benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens by $\\mathbf{64\\times}$ , leading to $\\bf{410\\times}$ faster generation process. Our best-performing variant can significantly surpass DiT-XL/2 (gFID 2.13 vs. 3.04) while still generating high-quality samples $\\mathbf{74\\times}$ faster. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In recent years, image generation has experienced remarkable progress, driven by the significant advancements in both transformers [19, 65, 70, 10, 71, 72] and diffusion models [16, 58, 29, 52, 21]. Mirroring the trends in generative language models [51, 62], the architecture of many contemporary image generation models incorporate a standard image tokenizer and de-tokenizer. This array of models utilizes tokenized image representations\u2014ranging from continuous [35] to discrete vectors [57, 64, 19]\u2014to perform a critical function: translating raw pixels into a latent space. The latent space (e.g., $32\\times32$ ) is significantly more compact than the original image space $(256\\times256\\times3)$ . It offers a compressed yet expressive representation, and thus not only facilitates efficient training and inference of generative models but also paves the way to scale up the model size. ", "page_idx": 1}, {"type": "text", "text": "Although image tokenizers achieve great success in image generation workflows, they encounter a fundamental limitation tied to their intrinsic design. These tokenizers are based on an assumption that the latent space should retain a 2D structure, to maintain a direct mapping for locations between the latent tokens and image patches. For example, the top-left latent token directly corresponds to the top-left image patch. This restricts the tokenizer\u2019s ability to effectively leverage the redundancy inherent in images to cultivate a more compressed latent space. ", "page_idx": 1}, {"type": "text", "text": "Taking one step back, we raise the question \u201cis $2D$ structure necessary for image tokenization?\u201d To answer the question, we draw inspiration from several image understanding tasks where model predictions are based solely on high-level information extracted from input images \u2014such as in image classification [17], object detection [8, 81], segmentation [67, 34, 74, 75], and multi-modal large language models [1, 41, 11]. These tasks do not need de-tokenizers, since the outputs typically manifest in specific structures other than images. In other words, they often format a higher-level 1D sequence as output that can still capture all task-relevant information. Prior arts, such as object queries [8, 67] or the perceiver resampler [1], encode images into a 1D sequence of a predetermined number of tokens (e.g., 64). These tokens facilitate the generation of outputs like bounding boxes or captions. The success of these methods motivates us to investigate a more compact 1D sequence as image latent representation in the context of image reconstruction and generation. It is noteworthy that the synthesis of both high-level and low-level information is crucial for the generation of high-quality images, providing a challenge for extremely compact latent representations. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce a transformer-based framework [65, 17] designed to tokenize an image to a 1D discrete sequence, which can later be decoded back to the image space via a de-tokenizer. Specifically, we present Transformer-based 1-Dimensional Tokenizer (TiTok), consisting of a Vision Transformer (ViT) encoder, a ViT decoder, and a vector quantizer following the typical VectorQuantized (VQ) model designs [19]. In the tokenization phase, the image is split and flattened into a series of patches, followed by concatenation with a 1D sequence of latent tokens. After the feature encoding process of ViT encoder, these latent tokens build the latent representation of the image. Subsequent to the vector quantization step [64, 19], the ViT decoder reconstructs the input images from the masked token sequence [15, 24]. ", "page_idx": 1}, {"type": "text", "text": "Building upon TiTok, we conduct extensive experiments to probe the dynamics of 1D image tokenization. Our investigation studies the interplay between latent space size, model size, reconstruction fidelity, and generative quality. From this exploration, several compelling insights emerge: ", "page_idx": 1}, {"type": "text", "text": "1. Increasing the number of latent tokens representing an image consistently improves the reconstruction performance, yet the benefti becomes marginal after 128 tokens. Intriguingly, 32 tokens are sufficient for a reasonable image reconstruction.   \n2. Scaling up the tokenizer model size significantly improves performance of both reconstruction and generation, especially when number of tokens is limited (e.g., 32 or 64), showcasing a promising pathway towards a compact image representation at latent space.   \n3. 1D tokenization breaks the grid constraints in prior 2D image tokenizers, which not only enables each latent token to reconstruct regions beyond a fixed image grid and leads to a more flexible tokenizer design, but also learns more high-level and semantic-rich image information, especially at a compact latent space.   \n4. 1D tokenization exhibits superior performance in generative training, with not only a significant speed-up for both training and inference but also a competitive FID score compared to a typical 2D tokenizer, while using much fewer tokens. ", "page_idx": 1}, {"type": "image", "img_path": "tOXoQPRzPL/tmp/e13b4e228dd4f24ca70230b6c6903ab782c21a855879681b1412f658e6a3a3c3.jpg", "img_caption": ["Figure 2: A speed and quality comparison of TiTok and prior arts on ImageNet $256\\times256$ and $512\\times512$ generation benchmarks. Speed-up is compared against DiT-XL/2 [52]. The sampling speed (de-tokenization included) is measured with an A100 GPU. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "In light of these findings, we introduce the TiTok family, encompassing models of varying model sizes and latent sizes, capable of achieving highly compact tokenization with as few as 32 tokens. We further confirm the model\u2019s efficacy in image generation through the MaskGIT [9] framework. TiTok is demonstrated to facilitate state-of-the-art performance in image generation, while requiring latent spaces that are $8\\times$ to $64\\times$ smaller, resulting in significant accelerations during both the training and inference phases. It also generates images with similar or higher quality but up to $410\\times$ faster than state-of-the-art diffusion models such as DiT [18] (Fig. 2). ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Image Tokenization. Images have been compressed since the early days of deep learning with autoencoders [27, 66]. The general design of using an encoder that compresses high-dimensional images into a low-dimensional latent representation and then using a decoder to reverse the process, has proven to be successful over the years. Variational Autoencoders (VAEs) [35] extend the paradigm by learning to map the input to a distribution. Instead of modeling a continuous distribution, VQVAEs [50, 56] learn a discrete representation forming a categorical distribution. VQGAN [19] further improves the training process by using adversarial training [23]. The transformer design of the autoencoder is further explored in ViT-VQGAN [69] and Efficient-VQGAN [7]. Orthogonal to this, RQ-VAE [37] and MoVQ [80] study the effect of using multiple vector quantization steps per latent embedding, while MAGVIT-v2 [72] and FSQ [48] propose a lookup-free quantization. However, all aforementioned works share the same workflow of an image always being patchwise encoded into a $2D$ grid latent representation. In this work, we explore an innovative $I D$ sequence latent representation for image reconstruction and generation. ", "page_idx": 2}, {"type": "text", "text": "Tokenization for Image Understanding. For image understanding tasks (e.g., image classification [17], object detection [8, 81, 78], segmentation [67, 74, 76], and Multi-modal Large Language Models (MLLMs) [1, 41, 77]), it is common to use a general feature encoder instead of an autoencoder to tokenize the image. Specifically, many MLLMs [41, 43, 61, 32, 22, 11] uses a CLIP [54] encoder to tokenize the image into highly semantic tokens, which proves effective for image captioning [13] and VQA [2]. Some MLLMs also explore discrete tokens [32, 22] or \u201cde-tokenize\u201d the CLIP embeddings back to images through diffusion models [61, 32, 22]. However, due to the nature of CLIP models that focus on high-level information, these methods can only reconstruct an image with high-level semantic similarities (i.e., the layouts and details are not well-reconstructed due to CLIP features). Therefore, our method is significantly different from theirs, since the proposed TiTok aims to reconstruct both the high-level and low-level details of an image, same as typical VQ-VAE tokenizers [35, 57, 19]. ", "page_idx": 2}, {"type": "text", "text": "Image Generation. Image generation methods range from sampling the VAE [35], using GANs [23] to Diffusion Models [16, 58, 29, 52, 21, 44] and autoregressive models [63, 12, 50]. Prior studies that are most related to this work build on top of a learned VQ-VAE codebook to generate images. Autoregressive transformer [19, 69, 7, 37], similar to decoder-only language models, model each patch in a step-by-step fashion, thus requiring as many steps as token number, e.g., 256 or 1024. Nonautoregressive (or bidirectional) transformers [80, 72, 68], such as MaskGIT [9], generally predict more than a single token per step and thus require significantly fewer steps to predict a complete image. Apart from that, further studies looked into improved sampling strategies [39, 40, 38]. As we focus on the tokenization stage, we apply the commonly used non-autoregressive sampling scheme of MaskGIT to generate a sequence of tokens that is later decoded into an image. ", "page_idx": 2}, {"type": "image", "img_path": "tOXoQPRzPL/tmp/5afaeb9cafef3bc97aa6041d51e5c1c9aaf769955c325598ca4daa615051ca18.jpg", "img_caption": ["Figure 3: Illustration of image reconstruction (a) and generation (b) with the TiTok framework (c). TiTok contains an encoder Enc, a quantizer Quant, and a decoder Dec. Image patches, along with a few (e.g., 32) latent tokens, are passed through the Vision Transformer (ViT) encoder. The latent tokens are then vector-quantized. The quantized tokens, along with the mask tokens [15, 24], are fed to the ViT decoder to reconstruct the image. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Preliminary Background on VQ-VAE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The image tokenizer plays a pivotal role in facilitating the generative models by providing a compact image representation at latent space. For the scope of our discussion, we primarily focus on the VectorQuantized (VQ) tokenizer [64, 19], given its broad applicability across various domains, including but not limited to image and video generation [19, 9, 58, 71], large-scale pretraining [12, 5, 49, 3, 18] and multi-modal models [20, 73]. ", "page_idx": 3}, {"type": "text", "text": "A typical VQ model contains three key components: an encoder $E n c$ , a vector quantizer Quant, and a decoder $D e c$ . Given an input image $\\mathbf{I}\\in\\mathbb{R}^{H\\times W\\times3}$ , where $H$ and $W$ denote the image\u2019s height and width, the image is initially processed by the encoder $E n c$ and converted to latent embeddings ${\\bf{Z}}_{2D}=E n c({\\bf{I}})$ , where $\\mathbf{Z}_{2D}\\in\\mathbb{R}^{\\frac{H}{f}\\times\\frac{W}{f}\\times D}$ , which downsamples the spatial dimensions by a factor of $f$ . Subsequently, each embedding $z\\in\\mathbb{R}^{D}$ is mapped (via the vector quantizer Quant) to the nearest code $c_{i}\\in\\mathbb{R}^{D}$ in a learnable codebook $\\mathbb{C}\\in\\mathbb{R}^{N\\times\\\"D}$ , comprising $N$ codes. Formally, we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ u a n t(z)=c_{i},\\;\\mathrm{where}\\;i=\\operatorname*{argmin}_{j\\in\\{1,2,\\ldots,N\\}}\\|z-c_{j}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "During de-tokenization, the reconstructed image $\\hat{\\bf I}$ is obtained via the decoder $D e c$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{I}}=D e c(Q u a n t(\\mathbf{Z}_{2D})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Despite the numerous improvements over VQ-VAE [64] (e.g., loss function [19], model architecture [69], and quantization/codebook strategies [80, 37, 72]), the fundamental workflow (e.g., the 2D grid-based latent representations) has largely remained unchanged. ", "page_idx": 3}, {"type": "text", "text": "3.2 TiTok: From 2D to 1D Tokenization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "While existing VQ models have demonstrated significant achievements, a notable limitation within the standard workflow exists: the latent representation $\\mathbf{Z}_{2D}$ is often envisioned as a static 2D grid. Such a configuration inherently assumes a strict one-to-one mapping between the latent grids and the original image patches. This assumption limits the VQ model\u2019s ability to fully exploit the redundancies present in images, such as similarities among adjacent patches. Additionally, this approach constrains the flexibility in selecting the latent size, with the most prevalent configurations being $f=4$ , $f=8$ , or $f=16$ [58], resulting in 4096, 1024, or 256 tokens for an image of dimensions $256\\times256\\times3$ . Inspired by the success of 1D sequence representations in addressing a broad spectrum of computer vision problems [8, 1, 41], we propose to use a 1D sequence, without the fixed correspondence between latent representation and image patches in 2D tokenization, as an efficient and effective latent representation for image reconstruction and generation. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Image Reconstruction with TiTok. To initiate our exploration, we establish a novel framework named Transformer-based 1-Dimensional Tokenizer (TiTok), leveraging Vision Transformer (ViT) [17]1to tokenize images into 1D latent tokens and subsequently reconstruct the original images from these 1D latents. As depicted in Fig. 3, TiTok employs a standard ViT for both the tokenization and de-tokenization processes (i.e., both the encoder $E n c$ and decoder $D e c$ are ViTs). During tokenization, we patchify the image into patches (with a patch embedding layer) P \u2208RfH \u00d7 Wf \u00d7D (with patch size equal to the downsampling factor $f$ and embedding dimension $D$ ) and concatenate them with $K$ latent tokens $\\mathbf{L}\\in\\mathbb{R}^{K\\times D^{\\prime}}$ . They are then fed into the ViT encoder $E n c$ . In the encoder output, we only retain the latent tokens as the image\u2019s latent representation, thereby enabling a more compact latent representation of 1D sequence $\\mathbf{Z}_{1D}$ (with length $K$ ). This adjustment decouples the latent size from image\u2019s resolution and allows more flexibility in design choices. That is, we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf Z}_{1D}=E n c({\\bf P}\\oplus{\\bf L}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\oplus$ denotes concatenation, and we only retain the latent tokens from the encoder output. ", "page_idx": 4}, {"type": "text", "text": "In the de-tokenization phase, drawing inspiration from [15, 5, 24], we incorporate a sequence of mask tokens $\\mathbf{M}\\in\\mathbb{R}^{\\frac{H}{f}\\times\\frac{W}{f}\\times D}$ \u2014obtained by replicating a single mask token \u00d7 Wf times\u2014to the quantized latent tokens $\\mathbf{Z}_{1D}$ . The image is then reconstructed via the ViT decoder $D e c$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{I}}=D e c(Q u a n t(\\mathbf{Z}_{1D})\\oplus\\mathbf{M}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the latent tokens $\\mathbf{Z}_{1D}$ is first vector-quantized by Quant and then concatenated with the mask tokens $\\mathbf{M}$ before feeding to the decoder $D e c$ . ", "page_idx": 4}, {"type": "text", "text": "Despite its simplicity, we emphasize that the concept of compact 1D image tokenization remains underexplored in existing literature. The proposed TiTok thus serves as a foundational platform for exploring the potentials of 1D tokenization and de-tokenization for natural images. It is worth noting that although one may flatten 2D grid latents into a 1D sequence, it significantly differs from the proposed 1D tokenizer, due to the fact that the implicit 2D grid mapping constraints still persist. ", "page_idx": 4}, {"type": "text", "text": "Image Generation with TiTok. Besides the image reconstruction task which the tokenizer is trained for, we also evaluate its effectiveness for image generation, following the typical pipeline [19, 9]. Specifically, we adopt MaskGIT [9] as our generation framework due to its simplicity and effectiveness, allowing us to train a MaskGIT model by simply replacing its VQGAN tokenizer with our TiTok. We do not make any other specific modifications to MaskGIT, but for completeness, we briefly describe its whole generation process with TiTok. ", "page_idx": 4}, {"type": "text", "text": "The image is pre-tokenized into 1D discrete tokens. At each training step, a random ratio of the latent tokens are replaced with mask tokens. Then, a bidirectional transformer takes the masked token sequence as input, and predicts the corresponding discrete token ID of those masked tokens. The inference process consists of multiple sampling steps, where at each step the transformer\u2019s prediction for masked tokens will be sampled based on the prediction confidence, which are then used to update the masked images. In this way, the image is \u201cprogressively generated\u201d from a sequence full of mask tokens to an image with generated tokens, which can later be de-tokenized back into pixel spaces. The MaskGIT framework shows a significant speed-up in the generation process compared to auto-regressive models. We refer readers to [9] for more details. ", "page_idx": 4}, {"type": "text", "text": "3.3 Two-Stage Training of TiTok with Proxy Codes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Existing Training Strategies for VQ Models. Although most VQ models adhere to a straightforward formulation, their training process is notably sensitive, and the model\u2019s performance is heavily influenced by the adoption of more effective training paradigms. For instance, VQGAN [19] achieves a significant improvement in reconstruction FID (rFID) on the ImageNet [14] validation set, when compared to dVAE from DALL-E [55]. This enhancement is attributed to advancements in perceptual loss [33, 79] and adversarial loss [23]. Moreover, MaskGIT\u2019s modern implementation of VQGAN [9] utilizes refined training techniques without architectural improvements to boost the performance further. Notably, most of these improvements are exclusively applied during the training phase (i.e., through auxiliary losses) and significantly affect the models\u2019 efficacy. Given the complexity of the loss functions, extensive tuning of hyper-parameters involved, and, most critically, the missing of a publicly available code-base for reference or reproduction [9, 69, 72], establishing an optimal experimental setup for the proposed TiTok presents a substantial challenge, especially when the target is a compact 1D tokenization which was rarely studied in literature. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Two-Stage Training Comes to the Rescue. Although training TiTok with the typical TamingVQGAN [19] setting is feasible, we introduce a two-stage training paradigm for an improved performance. The two-stage training strategy contains \u201cwarm-up\u201d and \u201cdecoder fine-tuning\u201d stages. Specifically, in the first \u201cwarm-up\u201d stage, instead of directly regressing the RGB values and employing a variety of loss functions (as in existing methods), we propose to train 1D VQ models with the discrete codes generated by an off-the-shelf MaskGIT-VQGAN model, which we refer to as proxy codes. This approach allows us to bypass the intricate loss functions and GAN architectures, thereby concentrating our efforts on optimizing the 1D tokenization settings. Importantly, this modification does not harm the functionality of the tokenizer and quantizer within TiTok, which can still fully function for image tokeniztion and de-tokenization; the main adaptation simply involves the processing of TiTok\u2019s de-tokenizer output. Specifically, this output, comprising a set of proxy codes, is subsequently fed into the same off-the-shelf VQGAN decoder to generate the final RGB outputs. It is noteworthy that the introduction of proxy codes differs from a simple distillation [26]. As verified in our experiments, TiTok yields significantly better generation performance than MaskGIT-VQGAN. ", "page_idx": 5}, {"type": "text", "text": "After the first training stage with proxy codes, we optionally have the second \u201cdecoder fine-tuning\u201d stage, inspired by [10, 53], to improve the reconstruction quality. Specifically, we keep the encoder and quantizer frozen, and only train the decoder towards pixel space with the typical VQGAN training recipe [19]. We observe that such a two-stage training strategy significantly improves the training stability and reconstructed image quality, as shown in the experiments. ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Preliminary Experiments of 1D Tokenization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Building upon TiTok, we explore a range of configurations, including the model size and the number of tokens, to identify the most efficient and effective setup for a 1D image tokenizer. These preliminary experiments serve to provide a thorough evaluation, seeking a practical configuration of TiTok. ", "page_idx": 5}, {"type": "text", "text": "Preliminary Experimental Setup. Unless specified otherwise, we train all models with images of resolution $H\\,=\\,256$ and $W\\,=\\,256$ , using the open-source MaskGIT-VQGAN [9] to supply proxy codes for training. The patch size for both tokenizer and de-tokenizer is established with $f=16$ , and the codebook $\\mathbb{C}$ is configured to have $N=1024$ entries with each entry a vector with 16 channels. For TiTok variants, we primarily investigate three model sizes\u2014small, base, and large (i.e., TiTok-S, TiTok-B, TiTok-L)\u2014comprising $22M$ , $86M$ , and $307M$ parameters for encoder and decoder, respectively. We also assess the impact of varying the number of latent tokens $K$ from 16 to 256. We perform ablation experiments with an efficient setting (e.g., shorter training). ", "page_idx": 5}, {"type": "text", "text": "Evaluation Protocol. Evaluation is conducted across multiple metrics to thoroughly assess the models, including both reconstruction and generation FID metrics (i.e., rFID and gFID) [25] on the ImageNet dataset. We examine training/inference throughput to offer a direct comparison of generative model\u2019s efficiency relative to different latent sizes. Furthermore, given that the 1D VQ model inherently serves as a form of compact image compression, we further investigate the semantic information retained by the model through linear probing following MAE setting [24]. For the complete details of the training and testing protocols (e.g., hyper-parameters, training costs), we refer the reader to the supplementary material Sec. A. ", "page_idx": 5}, {"type": "text", "text": "After the setup, we now summarize the preliminary experimental findings below. ", "page_idx": 5}, {"type": "text", "text": "An Image Can be Represented by 32 Tokens. The redundancy inherent in image representation is well-acknowledged, as evidenced by the practice of masking significant portions of images (e.g., ", "page_idx": 5}, {"type": "image", "img_path": "tOXoQPRzPL/tmp/150ba260f93248badb78ab27ef8888157ae22221ac75742aabf09b88cca5d83e.jpg", "img_caption": ["Figure 4: Preliminary experimental results with different TiTok variants. We provide a comprehensive exploration in (a) ImageNet-1K reconstruction. (b) ImageNet-1K linear probing. (c) ImageNet-1K generation. (d) Training and inference throughput of MaskGIT-ViT as generator and TiTok as tokenizer (evaluated on A100 GPUs, inference includes de-tokenization step with TiTok-B). Detailed numbers can be found in supplementary material Sec. B. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "$75\\%$ in MAE [24]) to expedite the training process without negatively affecting performance. This strategy has been validated across a variety of computer vision tasks that rely on high-level image features [30, 42]. However, the efficacy of such approaches in the context of image reconstruction and generation\u2014where both low-level and high-level details are crucial for creating realistic reconstructed and generated outputs\u2014remains underexplored. Consequently, in this experiment, we aim to determine the minimum number of tokens required to reconstruct and generate high-quality images. As depicted in Fig. 4a, although model performance progressively improves with an increase in the number of latent tokens, significant enhancements are predominantly observed when $K$ ranges from 16 to 128. Beyond this point, increasing the latent space size yields only marginal gains. Intriguingly, we find that with merely 32 latent tokens, TiTok-L achieves performance better than a 2D VQGAN model [19] using 256 tokens. This observation suggests that as few as 32 tokens may suffice as an effective image latent representation, optimizing the utilization of image redundancy. ", "page_idx": 6}, {"type": "text", "text": "Scaling Up Tokenizer Enables More Compact Latent Size. Another intriguing observation from Fig. 4a is that larger tokenizers facilitate more compact representations. Specifically, TiTok-B with 64 latent tokens achieves performance comparable to TiTok-S with 128 latent tokens, while TiTok-L with 32 latent tokens matches the performance of TiTok-B with 64 latent tokens. This pattern indicates that with each incremental increase in TiTok size (e.g., from S to B, or from B to L), it is possible to reduce the size of the latent image representation without compromising performance. This trend underscores the potential benefits of scaling up the tokenizer to achieve even more compact image representations. ", "page_idx": 6}, {"type": "text", "text": "Semantics Emerges with Compact Latent Space. To evaluate the learned image representation, we perform linear probing experiments on the image tokenizer, as shown in Fig. 4b. Specifically, we add a batch normalization layer [31] followed by a linear layer on top of the frozen features from TiTok encoder, with all hyper-parameters strictly following the MAE protocol [24]. We find that as the size of the latent representation decreases, the tokenizer increasingly learns semantically rich representations, as indicated by the improved linear probing accuracy. This suggests that the model learns high-level information in scenarios of constrained representation space. ", "page_idx": 6}, {"type": "text", "text": "Compact Latent Representation Improves Generative Training. In addition to reconstruction capabilities, we assess TiTok\u2019s effectiveness and efficiency in generative downstream tasks, as illustrated in Fig. 4c and Fig. 4d. We note that variants of different tokenizer sizes yield comparable outcomes when the number of latent tokens is sufficiently large (i.e., $K\\geq128)$ . However, within the domain of compact latent sizes (i.e. $.,K\\le64$ ), larger tokenizers notably enhance performance. Furthermore, the adaptability of 1D tokenization in TiTok facilitates more efficient and effective generative model training. For instance, model variants with $K=32$ , despite inferior reconstruction quality, demonstrate significantly better generative performance, underscoring the advantages of employing a more condensed and semantically rich latent space for generative model training. Additionally, the reduction in latent tokens markedly accelerates training and inference, with a $12.8\\times$ increase in training speed (2815.2 vs. 219.7 samples/s/gpu) and a $4.5\\times$ speed up sampling speed (123.1 vs. 27.5 samples/s/gpu), when utilizing $K=32$ as opposed to $K=256$ . ", "page_idx": 6}, {"type": "table", "img_path": "tOXoQPRzPL/tmp/8434e3d0f0f7550d7928e29a50214bd78e44ab1fd8be819b8d152e05f5b387bf.jpg", "table_caption": ["Table 1: ImageNet-1K $256\\times256$ generation results evaluated with ADM [16]. \u2020: Trained on OpenImages [36] \u2021: Trained on OpenImages, LAION-Aesthetics/-Humans [59]. P: generator\u2019s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Main Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Based on the observations above, the proposed TiTok family effectively trades off a larger model size to a more compact latent size. In this section, we majorly focus on ImageNet generation benchmarks against prior arts, and evaluate TiTok as a tokenizer in the generative MaskGIT framework [9]. ", "page_idx": 7}, {"type": "text", "text": "Implementation Details. We primarily investigate the following TiTok variants: TiTok-S-128 (i.e., small model with 128 tokens), TiTok-B-64 (i.e., base model with 64 tokens), and TiTok-L-32 (i.e., large model with 32 tokens), where each variant designed to halve the latent space size while scaling up the model size. For resolution 512, we double the latent size to ensure more details are kept at higher resolution, leading to TiTok-L-64 and TiTok-B-128. In the final setting for TiTok training, the codebook is configured to $N\\,=\\,4096$ , and the training duration is extended to $1M$ iterations (200 epochs). We also adopt the \u201cdecoder fine-tuning\u201d stage to further enhance model performance, where the encoder and quantizer are kept frozen and the decoder is fine-tuned for $500k$ iterations. For the training of generative models, we utilize the MaskGIT [9] framework without any specific modifications, except for the adoption of an arccos masking schedule [6]. All other parameters are the same as previous setups, and all design improvements will be verified in the ablation studies. ", "page_idx": 7}, {"type": "text", "text": "Main Results. We summarize the results on ImageNet-1K generation benchmark of resolution $256\\times256$ and $512\\times512$ in Tab. 1 and Tab. 2, respectively.2 ", "page_idx": 7}, {"type": "text", "text": "For ImageNet $256\\times256$ results in Tab. 1, TiTok can achieve a similar level of reconstruction FID (rFID) with a much smaller number of latent tokens than other VQ models. Specifically, using merely 32 tokens, TiTok-L-32 achieves a rFID of 2.21, comparable to the well trained VQGAN from MaskGIT [9] (rFID 2.28), while using $8\\times$ smaller latent representation size. Furthermore, when using the same generator framework and same sampling steps, TiTok-L-32 improves over MaskGIT by a large margin (from 6.18 to $2.77\\,\\mathrm{gFID})$ , showcasing the benefits of a more effective generator training with compact 1D tokens. When compared to other diffusion-based generative models, TiTok can also achieve a competitive performance while enjoying an over $\\mathbf{100\\times}$ speed-up during the sampling process. Specifically, TiTok-L-32 achieves a better gFID than LDM-4 [58] (2.77 vs. 3.60), while generating images dramatically faster by 254 times (101.6 samples/s vs. 0.4 samples/s). Our best-performing variant TiTok-S-128 outperforms state-of-the-art diffusion method DiT-XL/2 [52] (gFID 1.97 vs. 2.27), with a $13\\times$ speed-up. ", "page_idx": 7}, {"type": "text", "text": "For ImageNet $512\\times512$ results in Tab. 2, the significantly better accuracy-cost trade-off of TiTok persists. TiTok maintains a reasonably good rFID compared to other methods, especially considering that TiTok uses much fewer tokens (i.e., higher compression ratio). For generation, all TiTok variants significantly outperform our baseline MaskGIT [9] by a large margin. When compared with diffusionbased models, TiTok-L-64 shows a superior performance to DiT-XL/2 [52] $(2.74\\;\\nu s.\\;3.04)$ , while running $\\mathbf{410\\times}$ faster. The best-performing variant TiTok-B-128 can significantly outperform DiTXL/2 by a large margin (2.13 vs. 3.04) but also generates high-quality samples $\\mathbf{74\\times}$ faster. We also provide visualization results and analysis in supplementary material Sec. D. ", "page_idx": 7}, {"type": "table", "img_path": "tOXoQPRzPL/tmp/c67b21f528e287f44743a997d0703c843344ca2b5965d9ada4c9f201f2d8f0b4.jpg", "table_caption": ["Table 2: ImageNet-1K $512\\times512$ generation results evaluated with ADM [16]. \u2021: Trained on OpenImages, LAION-Aesthetics and LAION-Humans [59]. P: generator\u2019s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "tOXoQPRzPL/tmp/cab3b3fa94375881bff0332afebcee4c3d5a27a5b5163008fc069069ad5b7325.jpg", "table_caption": ["Table 3: Ablation study improved final models for main experiments. We ablate the tokenizer designs, and generator designs on ImageNet-1k benchmark. The final settings are labeled in gray. Generation results are based on tokenizers without decoder fine-tuning ", "(a) TiTok configuration. Results (b) Masking schedules for genera- (c) Effects of proxy codes reported in accumulation manner tor with TiTok-L-32 rFID\u2193 "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We report the ablation studies regarding our final model designs in Tab. 3. Specifically, in Tab. 3a, we ablate the tokenizer designs on image reconstruction. We begin with our baseline TiTok-L-32 which attains 6.59 rFID. Employing a larger codebook size improves the rFID by 0.74, while further increasing the training iterations (from 100 epochs to 200 epochs) yields another 0.37 improvement of rFID. On top of that, the \u201cdecoder fine-tuning\u201d (our stage-2 training strategy) can substantially improve the overall reconstruction performance to 2.21 rFID. ", "page_idx": 8}, {"type": "text", "text": "In Tab. 3b, we examine the effects of different masking schedules for MaskGIT with TiTok. Interestingly, unlike the original MaskGIT setting [9] which empirically found that the cosine masking schedule significantly outperforms the other schedules, we observe that MaskGIT equipped with TiTok changes the preference to the arccos or linear schedules. Additionally, unlike [9] which reported that the root schedule performs much worse than the others, we observe that TiTok is quite robust to different masking schedules. We attribute the observations to TiTok\u2019s ability to provide a more compact and more semantic meaningful tokens compared to 2D VQGAN, as compared to the cosine masking schedule, linear and arccos schedules have a lower masking ratio in the early steps. This coincides with the observation that masking ratio is usually higher for redundant signals (e.g., $75\\%$ masking ratio in images [24]) while relatively lower for semantic meaningful inputs (e.g., $15\\%$ masking ratio in languages [15]). ", "page_idx": 8}, {"type": "text", "text": "We ablate the effects of training paradigm in Tab. 3c. We begin with the training setting of Taming-VQGAN [19], where TiTok-B-64 obtains 5.15 rFID, outperforming the original 2D TamingVQGAN\u2019s 7.94 rFID under the same training setting. We also show the necessity of 1D tokenization by building a 2D variant of TiTok-B64, where the architecture remains the same except that image patches instead of latent tokens are used as image representation. As a result, we observe that the 2D variant suffers from a much worse performance (15.58 vs. 5.15 rFID), since the fixed correspondences in 2D tokenization limited a reasonable reconstruction under compact latent space. This result demonstrates the effectiveness of the proposed 1D tokenization, especially at a much more compact latent size. Although TiTok can achieve a reasonably well performance under straightforward single-stage training, there exists a performance gap compared to the MaskGIT-VQGAN [9] due to the missing of a strong training recipe, of which no public reference or access exists. Therefore, we adopt the two-stage training with proxy codes, which proves to be effective and can outperform the MaskGIT-VQGAN (1.70 vs. 2.28 rFID). It is noteworthy that the two-stage training is not that crucial to obtain a reasonable 1D tokenizer, and we believe that TiTok, with the simple single-stage Taming-VQGAN\u2019s training setting, could also benefit from training on a lagrer-scale dataset [36] as demonstrated in [58] and we leave it for future work due to the limited compute. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Details on the Two-Stage Training. We provide more technical details on the two-stage training. Specifically: ", "page_idx": 9}, {"type": "text", "text": "\u2022 In the first stage (warm-up stage), we use an off-the-shelf ImageNet-pretrained MaskGIT-VQ tokenizer to tokenize the input image into 256 tokens, which we refer to as proxy codes.   \n\u2022 In the first stage training, instead of regressing the original RGB values, we use the proxy codes as reconstruction targets. Specifically, the workflow is: RGB images are patchified and flattened into a sequence and concatenated with 32 latent tokens, then they are fed into TiTok-Enc (Encoder of TiTok). Later, the latent tokens are kept as token representation and go through the quantizer. The quantized latent tokens are concatenated with 256 mask tokens and go through the TiTok-Dec (Decoder of TiTok). And the final output mask tokens are supervised by proxy codes using cross-entropy loss.   \n\u2022 Afterwards, we freeze both the TiTok-Enc and quantizer, and then only fine-tune the TiTok-Dec (responsible for reconstructing proxy codes) and MaskGIT-Dec (responsible for reconstructing RGB values from proxy codes) end-to-end towards pixel space, where the training losses include L2 loss, perceptual loss, and GAN loss following the common VQGAN paradigm. ", "page_idx": 9}, {"type": "text", "text": "Moreover, we also note that two-stage training is not necessary for TiTok training, and it works fine with the commonly used and publicly available Taming-VQGAN recipe as is shown in Tab. 3c . In this case, the whole workflow is pretty straightforward, where the TiTok-Dec will instead directly reconstruct the images at pixel space. ", "page_idx": 9}, {"type": "text", "text": "However, the Taming-VQGAN recipe (developed more than 3 years ago) leads to an inferior FID score when compared to state-of-the-art tokenizers, putting TiTok at disadvantage when compared against other methods. Therefore we propose the two-stage training to benefit TiTok from the stateof-art MaskGIT-VQGAN tokenizer, which shares a similar architecture to Taming-VQGAN but has a significantly better score (rFID 2.28 v.s. 7.94). ", "page_idx": 9}, {"type": "text", "text": "We also note that TiTok can work well with single-stage recipe, and it is promising to incorporate the recent modern VQGAN recipe from [47, 68] in our preliminary experiments. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have explored a compact 1D tokenization TiTok for reconstructing and generating natural images. Unlike the existing 2D VQ models that consider the image latent space as a 2D grid, we provide a more compact formulation to tokenize an image into a 1D latent sequence. The proposed TiTok can represent an image with 8 to 64 times fewer tokens than the commonly used 2D tokenizers. Moreover, the compact 1D tokens not only significantly improve the generation model\u2019s training and inference throughput, but also achieve a competitive FID on the ImageNet benchmarks. We hope our research can shed some light in the direction towards more efficient image representation and generation models with 1D image tokenization. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. NeurIPS, 2022.   \n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015.   \n[3] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. In CVPR, 2024.   \n[4] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In CVPR, 2023.   \n[5] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2022.   \n[6] Victor Besnier and Mickael Chen. A pytorch reproduction of masked generative image transformer. arXiv preprint arXiv:2310.14400, 2023.   \n[7] Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficient-vqgan: Towards high-resolution image generation with efficient vision transformers. In ICCV, 2023.   \n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.   \n[9] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022.   \n[10] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. In ICML, 2023.   \n[11] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable vision models in the vision-language era. In CVPR, 2024.   \n[12] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020.   \n[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.   \n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.   \n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2018.   \n[16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021.   \n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.   \n[18] Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua M Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models. arXiv preprint arXiv:2401.08541, 2024.   \n[19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021.   \n[20] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In ECCV, 2022.   \n[21] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In ICCV, 2023.   \n[22] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. In ICLR, 2024.   \n[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014.   \n[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022.   \n[25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017.   \n[26] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[27] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786), 2006.   \n[28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[29] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In ICML, 2023.   \n[30] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849, 2020.   \n[31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.   \n[32] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual tokenization. In ICLR, 2024.   \n[33] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016.   \n[34] Dahun Kim, Jun Xie, Huiyu Wang, Siyuan Qiao, Qihang Yu, Hong-Seok Kim, Hartwig Adam, In So Kweon, and Liang-Chieh Chen. Tubeformer-deeplab: Video mask transformer. In CVPR, 2022.   \n[35] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.   \n[36] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020.   \n[37] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022.   \n[38] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and WOOK SHIN HAN. Draft-and-revise: Effective image generation with contextual rq-transformer. NeurIPS, 2022.   \n[39] Jos\u00e9 Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with tokencritic. In ECCV, 2022.   \n[40] Jos\u00e9 Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa. Predictor-corrector sampling for discrete diffusion models. In ICLR, 2023.   \n[41] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023.   \n[42] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. In CVPR, 2023.   \n[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023.   \n[44] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. NeurIPS, 2024.   \n[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.   \n[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[47] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An opensource project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024.   \n[48] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: VQ-VAE made simple. In ICLR, 2024.   \n[49] David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 4m: Massively multimodal masked modeling. NeurIPS, 2023.   \n[50] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. NeurIPS, 2017.   \n[51] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[52] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023.   \n[53] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.   \n[54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.   \n[55] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.   \n[56] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. NeurIPS, 2019.   \n[57] Jason Tyler Rolfe. Discrete variational autoencoders. In ICLR, 2017.   \n[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.   \n[59] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022.   \n[60] stabilityai, 2023. URL https://huggingface.co/stabilityai/sd-vae-ft-ema.   \n[61] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. In ICLR, 2024.   \n[62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[63] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. NeurIPS, 2016.   \n[64] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 2017.   \n[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.   \n[66] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103, 2008.   \n[67] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation with mask transformers. In CVPR, 2021.   \n[68] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. arXiv preprint arXiv:2409.16211, 2024.   \n[69] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR, 2022.   \n[70] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. TMLR, 2022.   \n[71] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00e9 Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In CVPR, 2023.   \n[72] Lijun Yu, Jos\u00e9 Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion\u2013tokenizer is key to visual generation. In ICLR, 2024.   \n[73] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2023.   \n[74] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Cmt-deeplab: Clustering mask transformers for panoptic segmentation. In CVPR, 2022.   \n[75] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. k-means Mask Transformer. In ECCV, 2022.   \n[76] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Convolutions die hard: Openvocabulary segmentation with single frozen convolutional clip. NeurIPS, 2023.   \n[77] Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Towards open-ended visual recognition with large language model. In ECCV, 2024.   \n[78] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.   \n[79] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.   \n[80] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. NeurIPS, 2022.   \n[81] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the supplementary materials, we provide the following additional details: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The comprehensive training and testing hyper-parameters and training costs for TiTok (Sec. A).   \n\u2022 The detailed results of the preliminary experiments reported in main paper\u2019s Fig. 4(Sec. B).   \n\u2022 A more comprehensive comparison with more metrics and baselines (Sec. C).   \n\u2022 Qualitative visualizations (Sec. D).   \n\u2022 Limitation discussion (Sec. E).   \n\u2022 Broader Impacts discussion (Sec. F).   \n\u2022 Dataset Licenses (Sec. G). ", "page_idx": 14}, {"type": "text", "text": "A Training and Testing Protocols ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For image reconstruction (tokenizer) at preliminary experiments, the training augmentation is confined to random cropping and flipping, following [19]. The training regimen spans a short schedule, featuring a batch size of 256 over $500k$ training iterations, which correlates to roughly 100 epochs on the ImageNet dataset. We employ the AdamW optimizer [46] with an initial learning rate of $1\\times10^{-4}$ (with cosine decay) and weight decay $1\\times10^{\\bar{-4}}$ . We only adopt stage-1 training here (i.e., only the \u201cwarm-up\u201d training stage). For the main experiments, we adopt the improvements as shown in the ablation study (Tab. 3 in main paper), including longer training to 200 epochs and decoder fine-tuning, all other hyper-parameters remain the same. We use patch size 16 for all vision transformers at resolution $256\\times256$ and increase it to 32 for resolution $512\\times512$ to ensure a computation efficiency. ", "page_idx": 14}, {"type": "text", "text": "TiTok-L refers to using a ViT-L for TiTok encoder and decoder, and TiTok-B, TiTok-S refers to using ViT-B and ViT-S respectively. Moreover, the tokenizer training takes 64 A100-40G for 74 hours (TiTok-L-32), 32 A100-40G for 41 hours (TiTok-B-64), 32 A100-40G for 50 hours (TiTok-S-128), 32 A100-40G for 70 hours (TiTok-B-128 for resolution 512), and 64 A100-40G for 91 hours (TiTok-L-64 for resolution 512), respectively. ", "page_idx": 14}, {"type": "text", "text": "For image generation (generator) at preliminary experiments, we majorly build the training and testing protocols on top of [9]. Specifically, all images are pre-tokenized using center crop and random flipping augmentation, and then processed by MaskGIT [9] to generate images via the masked image modeling procedure. During inference, a cosine masking schedule is utilized with 8 steps. The generative models are trained with a batch size of 2048 and $500k$ iterations to improve training efficiency. We use AdamW optimizer [46] with learning rate $2\\times10^{-4}$ and weight decay 0.03. The learning rate starts from $2\\times\\dot{1}0^{-4}$ and then decay to $1\\times10^{-5}$ following a cosine decaying schedule. We apply a dropout probability of 0.1 on the class condition. The only differences of main experiments are using an arccos masking schedule as discussed in the ablation study (main paper Tab. 3), all other hyper-parameters remain the same. We follow prior arts [19, 9] to generate $50k$ samples for generation FID evaluation. We also adopt classifier-free guidance [28] following prior arts [10, 72]. ", "page_idx": 14}, {"type": "text", "text": "At ImageNet $256\\times256$ , we use guidance scale 4.5, temperature 9.5 for TiTok-L-32, guidance scale 3.0, temperature 11.0 for TiTok-B-64, guidance scale 2.0, temperature 3.0 for TiTok-S-128. At ImageNet $512\\times512$ , we use guidance scale 2.0, temperature 7.5 for TiTok-L-64, guidance scale 2.5, temperature 6.5 for TiTok-B-128. ", "page_idx": 14}, {"type": "text", "text": "The generator training takes 32 A100-40G for 12 hours (TiTok-L-32), 16 hours (TiTok-B-64), 29 hours (TiTok-S-128), 26 hours (TiTok-B-128 for resolution 512), 18 hours (TiTok-L-64 for resolution 512) respectively. ", "page_idx": 14}, {"type": "text", "text": "B Detailed Results of Preliminary Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We summarize the detailed results for Fig. 4 of main paper in Tab. 4. ", "page_idx": 14}, {"type": "table", "img_path": "tOXoQPRzPL/tmp/e53a09299b8bc4a585e538c34567bded323543b6c5a61dfc90f92a2eb5b1fa77.jpg", "table_caption": ["Table 4: Detailed results of preliminary experiments in main paper. (a) reconstruction FID. (b) generation FID. ", "(c) Linear Probing accuracy. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "#token 16 32 64 96 128 192 256   \nTiTok-S 50.46 48.01 48.40 48.12 46.55 47.05 44.88   \nTiTok-B 57.70 54.79 53.92 53.72 53.59 51.75 52.11   \nTiTok-L 62.10 60.03 58.85 56.12 54.35 53.95 54.36 ", "page_idx": 15}, {"type": "table", "img_path": "tOXoQPRzPL/tmp/9dc647d0134dc92cf58e71de8feceee24b858feb2e49aa33bac4551329ad008f.jpg", "table_caption": ["Table 5: ImageNet-1K $256\\times256$ generation results evaluated with ADM [16]. \u2020: Trained on OpenImages [36] \u2021: Trained on OpenImages, LAION-Aesthetics/-Humans [59]. P: generator\u2019s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision, measured with $w/$ guidance variants if available. \u201cguidance\" refers to classifier-free guidance. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We further report the class-conditional generation results comparison with more metrics and baselines in Tab. 5 and Tab. 6 for ImageNet $256\\times256$ and $512\\times512$ generation benchmarks, respectively. Moreover, we report both results without and with classifier-free guidance [28] under column \u201cw/o guidance\" and \u201cw/ guidance\" respectively. ", "page_idx": 15}, {"type": "text", "text": "As shown in Tab. 5, both TiTok-L-32 and TiTok-B-64 set a new state-of-the-art performance for results without classifier-free guidance (i.e., w/o guidance column), while generating images at a much faster pace. Specifically, TiTok-L-32 achieves $3.15~\\mathrm{gFID}$ , surpassing current state-of-the-art MAGVIT-v2 [72]\u2019s gFID 3.65, while requiring much fewer sampling steps (8 vs. 64) and smaller model size (177M vs. $307M$ ), leading to a substantial sampling speed-up $(92.4\\times$ faster, $101.6~\\nu s$ . 1.1 samples/sec). Additionally, when compared to MaskGIT [9], which uses the exact same generator model (i.e., MaskGIT-ViT) as ours and the only difference is the toeknizer, TiTok-L-32 achieves significantly a better performance (3.15 vs. 6.18). The improvement demonstrates the efficiency and effectiveness of the learned compact 1D latent space for image representation. When it comes to resolution $512\\times512$ in Tab. 6, MaskGIT [9] requires 1024 tokens for image latent representation, while TiTok-L-64 requires $16\\times$ fewer. As a result, when using the same generator (i.e., MaskGITViT), TiTok-L-64, w/o classifier-free guidance, not only significantly outperforms MaskGIT in terms of gFID (3.64 vs. 7.32) but also generates samples much faster. The advantages of TiTok become even more significant when compared to the diffusion models such as DiT-XL/2 [52] with guidance: TiTok-L-64 not only shows a superior performance (2.74 vs. 3.04), but also enjoys a dramatically higher generation throughput $(410\\times)$ . ", "page_idx": 15}, {"type": "table", "img_path": "tOXoQPRzPL/tmp/5a343f49821f2c70e480e1cfeb8c582de60a507c9639ea8571b823920bded025.jpg", "table_caption": ["Table 6: ImageNet-1K $512\\times512$ generation results evaluated with ADM [16]. \u2020: Trained on OpenImages [36] \u2021: Trained on OpenImages, LAION-Aesthetics/-Humans [59]. P: generator\u2019s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision, measured with w/ guidance variants if available. \u201cguidance\" refers to classifier-free guidance. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "An interesting observation is that under w/o guidance case, TiTok-L-32 (for 256 resolution) and TiTok-L-64 (for 512 resolution) can outperform most other methods, including TiTok-S-128 and TiTok-B-128, yet they benefit relatively less from the classifier-free guidance in the w/ guidance column. We note it indicates that the great potential of TiTok at compact latent size is still not fully unleashed yet, and better adaptation of inference time improvements for 1D compact tokens, such as classifier-free guidance, which was designed for methods with much more tokens and steps, could be a promising future direction. ", "page_idx": 16}, {"type": "text", "text": "D Visualizations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide visualization of the generated images using TiTok in Fig. 5 and Fig. 6. Moreover, we visualize the reconstruction results under different numbers of tokens and different model sizes in Fig. 7, where we observe that the model tends to keep the high-level layout or salient objects when the latent representation size is limited. Besides, a larger model size reconstructs an image with more details under a compact latent space size, demonstrating an effective way towards a more compact latent space. ", "page_idx": 16}, {"type": "text", "text": "We also provide more uncurated visualization samples in Fig. 8 and Fig. 9. ", "page_idx": 16}, {"type": "text", "text": "E Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This paper proposes a novel 1D tokenization method designed to eliminate the fixed corresponding constraints of existing 2D tokenization methods. The 1D tokenization model is validated using the Vector Quantization (VQ) tokenizer formulation alongside a Masked Transformer generator framework. Despite the promising results, the proposed 1D tokenization formulation theoretically has the potential to generalize to other tokenizer formulations (e.g., 1D-VAE), other generation frameworks (e.g., Diffusion Models), and beyond the image modality (e.g., video). However, exploring these extensions is beyond the scope of this paper due to limited computational resources, and we leave these as promising directions for future research. ", "page_idx": 16}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Generative models have numerous applications with diverse potential social impacts. While these models significantly enhance human creativity, they can also be misused for misinformation, harassment, and perpetuating social and cultural biases. Similar to other deep learning methods, generative ", "page_idx": 16}, {"type": "image", "img_path": "tOXoQPRzPL/tmp/53cc6163d67b7509b0410a420780c381f4b01fa5e3b4f63b3ab741bc9becd244.jpg", "img_caption": ["Figure 5: Visualization of generated images from TiTok variants with MaskGIT [9]. Corresponding ImageNet class names are shown below the images. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "tOXoQPRzPL/tmp/008ab3b8719d15b51402c0dd538dedb02fcddeef08c1aae988bc1c517d410710.jpg", "img_caption": ["Figure 6: Visualization of generated images from TiTok-L-32 with MaskGIT [9] across random ImageNet classes. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "models can be heavily influenced by dataset biases, leading to the reinforcement of negative social stereotypes and viewpoints. Developing unbiased models that ensure both robustness and fairness is a critical area of research. However, addressing these issues is beyond the scope of this paper. ", "page_idx": 17}, {"type": "image", "img_path": "tOXoQPRzPL/tmp/34c5a080f49eed48e80c15d0733183169df8c817c7225fec368a0e31725531ea.jpg", "img_caption": ["Figure 7: Visual comparison of reconstruction results. Scaling model size enables a better image quality while using a more compact latent space size. It is also observed that TiTok tends to keep the salient regions when latent space is limited. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Considering the potential risks, this paper is limited to class-conditional generation using a fixed, public, and controlled set of classes. ", "page_idx": 18}, {"type": "text", "text": "G Dataset Licenses ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The datasets we used for training and/or testing TiTok are described as follows. ", "page_idx": 18}, {"type": "text", "text": "ImageNet-1K: We train and evaluate TiTok on ImageNet-1K generation benchmark. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and ", "page_idx": 18}, {"type": "image", "img_path": "tOXoQPRzPL/tmp/73bba07b852ef2e9d22cab2a4f93eaf55b9922bba16bca63f96b23c421010cbb.jpg", "img_caption": ["Figure 8: Uncurated $256\\times256$ TiTok-L-32 samples. Class labels (class ids) from left to right and top to down are: \u201cmacaw\" (88), \u201cvolcano\" (980), \u201ccoral reef\" (973), \u201cwhite wolf\" (270). "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "tOXoQPRzPL/tmp/956e29ba9d9d81079a30c41fbf728d6c9d14e4696cdd4092d9fa8af4a09d4a5d.jpg", "img_caption": ["Figure 9: Uncurated $256\\times256$ TiTok-L-32 samples. Class labels (class ids) from left to right and top to down are: \u201ccheeseburger\" (933), \u201ccliff\" (972), \u201cfountain\" (562), \u201cSiberian husky\" (250). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "100,000 test images. We use the training set for our tokenizer and generator training. The validation set is used to compute reconstruction FID for evaluating tokenizers. The generation results are evaluated with generation FID using pre-computed statistics and scripts from ADM [16] 3. ", "page_idx": 19}, {"type": "text", "text": "License: https://image-net.org/accessagreement ", "page_idx": 19}, {"type": "text", "text": "URL: https://www.image-net.org/ ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We propose a novel 1D tokenization method, achieving significant speed-up while still capable of generating high-quality images, as demonstrated in the abstract and introduction, supported by the Sec. 3 and Sec. 4.2 parts. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Please see Supplementary Material Sec. E. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No theoretical result. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Comprehensive details regarding model training and testing have been provided in Main Paper Sec. 4.1 Preliminary Experimental Setup, Sec. 4.2 Implementation Details, and Supplementary Material Sec. A Training and Testing Protocols ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The code and model are available at https://github.com/bytedance/ 1d-tokenizer. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Comprehensive details regarding model training and testing have been provided in Main Paper Sec. 4.1 Preliminary Experimental Setup, Sec. 4.2 Implementation Details, and Supplementary Material Sec. A Training and Testing Protocols ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We did not report error bars following prior arts [19, 9, 72]. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The training costs are reported in Supplementary Material Sec. A Training and Testing Protocols. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The research in the paper conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The broader impacts have been discussed in Supplementary Material Sec. F Broader Impacts. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper poses no such risks. Although our study includes image generation, we focus on class-conditional image generation with a predefined set of classes, which does not pose the same risks as those text-to-image models. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to Supplementary Material Sec. G Dataset Licenses. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]