[{"figure_path": "tOXoQPRzPL/tables/tables_7_1.jpg", "caption": "Table 1: ImageNet-1K 256 \u00d7 256 generation results evaluated with ADM [16]. \u2020: Trained on OpenImages [36] : Trained on OpenImages, LAION-Aesthetics/-Humans [59]. P: generator's parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.", "description": "This table compares the performance of various image generation models on the ImageNet 256x256 dataset, evaluated using the ADM metric.  It includes both diffusion-based and transformer-based models, showing the FID score, model size (parameters), sampling steps, and throughput.  The table highlights the superior performance of the proposed TiTok model in terms of both FID and speed.", "section": "4.2 Main Experiments"}, {"figure_path": "tOXoQPRzPL/tables/tables_8_1.jpg", "caption": "Table 2: ImageNet-1K 512 \u00d7 512 generation results evaluated with ADM [16]. : Trained on OpenImages, LAION-Aesthetics and LAION-Humans [59]. P: generator's parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.", "description": "This table presents a comparison of various image generation models on the ImageNet dataset at 512x512 resolution.  It specifically focuses on the generative fidelity (gFID), model parameters (P), sampling steps (S), and throughput (T). The models are categorized into diffusion-based and transformer-based methods. The table highlights the performance of TiTok variants against state-of-the-art models, showcasing their efficiency and effectiveness.", "section": "4.2 Main Experiments"}, {"figure_path": "tOXoQPRzPL/tables/tables_8_2.jpg", "caption": "Table 3: Ablation study improved final models for main experiments. We ablate the tokenizer designs, and generator designs on ImageNet-1k benchmark. The final settings are labeled in gray. Generation results are based on tokenizers without decoder fine-tuning", "description": "This table presents the ablation study results, showing the impact of different design choices on the performance of the TiTok model for image reconstruction and generation.  It compares different configurations, including tokenizer architecture choices and masking schedules used in the MaskGIT framework. The baseline is TiTok-L-32, and each subsequent row shows the results of a specific modification to this baseline.  The final configurations are highlighted in grey. Note that the generation results are measured without decoder fine-tuning for these experiments.", "section": "4.3 Ablation Studies"}, {"figure_path": "tOXoQPRzPL/tables/tables_15_1.jpg", "caption": "Table 4: Detailed results of preliminary experiments in main paper.", "description": "This table presents a comprehensive overview of the preliminary experimental results obtained using different TiTok variants. It includes three sections, showing the reconstruction FID (a), generation FID (b), and linear probe accuracy (c) for each variant.  The results are broken down by the model size (TiTok-S, TiTok-B, TiTok-L) and the number of latent tokens (16, 32, 64, 96, 128, 192, 256). This provides a detailed analysis of the impact of different hyperparameters on the model's performance across different evaluation metrics.", "section": "4.1 Preliminary Experiments of 1D Tokenization"}, {"figure_path": "tOXoQPRzPL/tables/tables_15_2.jpg", "caption": "Table 1: ImageNet-1K 256 \u00d7 256 generation results evaluated with ADM [16]. \u2020: Trained on OpenImages [36] : Trained on OpenImages, LAION-Aesthetics/-Humans [59]. P: generator's parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.", "description": "This table presents a comparison of various image generation models on the ImageNet-1K dataset at 256x256 resolution, using the ADM evaluation metric.  It shows the reconstruction FID (rFID), generation FID (gFID), Inception Score (IS), model parameters (P), number of sampling steps (S), and throughput (T) for different models, including those based on diffusion and transformer architectures.  The table highlights the performance of the proposed TiTok model against other state-of-the-art models, showing its competitive performance and significant speed advantages.", "section": "4.2 Main Experiments"}, {"figure_path": "tOXoQPRzPL/tables/tables_16_1.jpg", "caption": "Table 2: ImageNet-1K 512 \u00d7 512 generation results evaluated with ADM [16]. : Trained on OpenImages, LAION-Aesthetics and LAION-Humans [59]. P: generator's parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.", "description": "This table presents the results of image generation experiments on the ImageNet-1K dataset at 512x512 resolution.  It compares various models, including TiTok variants, in terms of reconstruction FID (rFID), generation FID (gFID), Inception Score (IS), the number of parameters (P), sampling steps (S), and throughput (T).  The models are evaluated using the ADM [16] framework, trained on OpenImages, LAION-Aesthetics, and LAION-Humans datasets [59], with throughput measured in samples per second on an A100 GPU at float32 precision.  The table highlights the performance of TiTok variants against state-of-the-art diffusion and transformer-based generative models. ", "section": "4.2 Main Experiments"}]