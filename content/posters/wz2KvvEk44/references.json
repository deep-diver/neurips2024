{"references": [{"fullname_first_author": "Rishabh Agarwal", "paper_title": "Contrastive behavioral similarity embeddings for generalization in reinforcement learning", "publication_date": "2021-01-01", "reason": "This paper introduces contrastive behavioral similarity embeddings to improve generalization in reinforcement learning, a technique relevant to the current paper's focus on improving generalization in visual-based RL."}, {"fullname_first_author": "Brandon Amos", "paper_title": "On the model-based stochastic value gradient for continuous reinforcement learning", "publication_date": "2021-01-01", "reason": "This paper explores model-based reinforcement learning methods, which are related to the current paper's use of reconstruction-based auxiliary tasks to enhance generalization."}, {"fullname_first_author": "David Bertoin", "paper_title": "Look where you look! saliency-guided q-networks for generalization in visual reinforcement learning", "publication_date": "2022-01-01", "reason": "This paper addresses generalization in visual reinforcement learning by using saliency maps, a concept similar to the current paper's approach of focusing on task-relevant areas."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Quantifying generalization in reinforcement learning", "publication_date": "2019-01-01", "reason": "This paper provides a framework for quantifying generalization in reinforcement learning, which is directly relevant to evaluating the generalization capabilities of the proposed SMG method."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-01-01", "reason": "Soft Actor-Critic (SAC) is used as the foundational RL algorithm in this paper, making this a highly relevant reference for understanding the baseline method used for comparisons."}]}