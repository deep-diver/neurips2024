[{"figure_path": "stXtBqyTWX/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of MoE Models. (a) MoE module uses a gating function to assign inputs to experts [15]; (b) Dense Transformer Decoder Layer, which consists of Multi-head Attention (MHA) followed by a Feed-Forward Network (FFN); (c) MoE Transformer Layer in which an FFN block is replaced by a set of expert FFNs, which operate in parallel; (d) MoE Transformer with expert parallelism. Each device hosts a subset of experts. Tokens assigned to remote experts are dispatched via all-to-all communication.", "description": "This figure illustrates the architecture of Mixture-of-Experts (MoE) models.  Panel (a) shows a basic MoE module, where a gating network determines which expert networks to activate for a given input. Panels (b), (c), and (d) progressively show how MoE layers are incorporated into a Transformer architecture, increasing complexity and parallelization. Panel (b) shows a standard Transformer decoder layer. Panel (c) replaces the FFN (feed-forward network) with multiple expert FFNs. Finally, Panel (d) shows a distributed MoE Transformer with expert parallelism, where each expert is hosted on a different device, demonstrating the complex communication required in practical MoE deployments.", "section": "1 Introduction"}, {"figure_path": "stXtBqyTWX/figures/figures_4_1.jpg", "caption": "Figure 2: (a) Comparison between the static gating in [2, 8] and our implementation of dynamic gating. We assume E=3, S=6, C=0.5 and top-1 gating in this example. (See Sec. 4.)(b) Illustration of the Expert Buffering mechanism. We move the expert parameters to CPU memory to reduce burden on GPU memory. (See Sec. 5.)", "description": "This figure compares static and dynamic gating methods in MoE models, illustrating the process of token assignment to experts and the impact on computational efficiency.  It also shows the Expert Buffering optimization, which offloads less frequently used expert parameters from GPU memory to CPU memory to improve GPU resource utilization and enable larger batch sizes.", "section": "4 Dynamic Gating Optimization"}, {"figure_path": "stXtBqyTWX/figures/figures_4_2.jpg", "caption": "Figure 2: (a) Comparison between the static gating in [2, 8] and our implementation of dynamic gating. We assume E=3, S=6, C=0.5 and top-1 gating in this example. (See Sec. 4.)(b) Illustration of the Expert Buffering mechanism. We move the expert parameters to CPU memory to reduce burden on GPU memory. (See Sec. 5.)", "description": "This figure illustrates two optimization techniques for Mixture-of-Experts (MoE) inference. (a) compares static and dynamic gating, highlighting how dynamic gating improves efficiency by adapting to varying expert loads.  (b) shows the Expert Buffering mechanism, which moves less frequently used expert parameters from GPU to CPU memory to reduce GPU memory pressure and improve performance.", "section": "4 Dynamic Gating Optimization"}, {"figure_path": "stXtBqyTWX/figures/figures_6_1.jpg", "caption": "Figure 3: Throughput Comparison (Pear). Dynamic gating outperforms static, Tutel, and FastMoE consistently. It outperforms Megablock as batchsize scales. Missing bars represent infeasible combinations of policy and batch size.", "description": "This figure compares the throughput of different gating policies (static, Tutel, Megablock, FastMoE, and dynamic gating) for LM-Small model on the Pear cluster. The x-axis represents different batch sizes, and the y-axis represents the throughput in tokens per second.  The figure shows that dynamic gating consistently outperforms other methods across various batch sizes, particularly showing significant improvement as the batch size increases.  Missing bars indicate that certain combinations of gating policy and batch size were not feasible to test.", "section": "7.1 Impact of Dynamic Gating"}, {"figure_path": "stXtBqyTWX/figures/figures_7_1.jpg", "caption": "Figure 3: Throughput Comparison (Pear). Dynamic gating outperforms static, Tutel, and FastMoE consistently. It outperforms Megablock as batchsize scales. Missing bars represent infeasible combinations of policy and batch size.", "description": "This figure compares the throughput of different gating policies (static, Tutel, FastMoE, Megablock, and dynamic gating) for language modeling (LM) and machine translation (MT) tasks on the Pear cluster.  The x-axis represents the batch size, and the y-axis represents the throughput (tokens per second). Dynamic gating consistently outperforms the other methods across different batch sizes and tasks, especially for larger batch sizes. The missing bars in the chart indicate that some combinations of gating policies and batch sizes were not feasible due to resource constraints.", "section": "7.1 Impact of Dynamic Gating"}, {"figure_path": "stXtBqyTWX/figures/figures_14_1.jpg", "caption": "Figure 6: Average number of inactive experts for (a) LM, (b) MT encoder, (c) MT decoder. LM and MT encoder activate most, if not all, experts. MT decoder exhibits extremely sparse activations.", "description": "This figure shows the average number of inactive experts across different MoE layers for three different tasks: Language Modeling (LM), Machine Translation encoder (MT Encoder), and Machine Translation decoder (MT Decoder).  The key observation is the significant difference in expert activation patterns between the encoder and decoder. LM and the encoder exhibit high activation levels (most experts are active), while the decoder shows extremely sparse activation (a large number of experts remain inactive). This difference highlights a key characteristic of MoE models, particularly for machine translation where some experts may be heavily used while others remain unused across many layers. This is an important finding because it informs optimization strategies for MoE inference.", "section": "3.1 Expert Activation"}, {"figure_path": "stXtBqyTWX/figures/figures_15_1.jpg", "caption": "Figure 7: Inference latency for MoE and Dense models on single node. MoE is slower by 15\u00d7 for LM, 22x for MT encoding, and 3\u00d7 for MT decoding.", "description": "This figure compares the inference latency of Mixture-of-Experts (MoE) models and their corresponding dense model counterparts across three tasks: Language Modeling (LM), Machine Translation Encoder (MT Encoder), and Machine Translation Decoder (MT Decoder). The results reveal that MoE models exhibit significantly higher latency compared to their dense counterparts.  Specifically, MoE models are 15 times slower for the LM task, 22 times slower for MT encoding, and 3 times slower for MT decoding. This highlights a critical performance challenge associated with MoE models during inference.", "section": "3.2 Latency"}, {"figure_path": "stXtBqyTWX/figures/figures_15_2.jpg", "caption": "Figure 8: Inference memory use for MoE and dense models. MoEs use more memory for expanded model capacity (multiple experts) and model activations. Results shown using batch sizes of 8 and 48 for LM and MT, respectively.", "description": "This figure compares the memory usage between MoE models and their dense counterparts for both Language Modeling (LM) and Machine Translation (MT) tasks.  It shows a breakdown of memory consumption into different components: embedding table, attention mechanism, other layers, expert parameters, and dynamic activations.  The key takeaway is that MoE models, while computationally more efficient during training, consume significantly more memory due to the larger number of parameters and the dynamic nature of expert activation during inference.", "section": "3.3 Memory Usage"}, {"figure_path": "stXtBqyTWX/figures/figures_15_3.jpg", "caption": "Figure 9: Inference latency for MoE models. Beyond communication, the gating function and expert execution are significant contributors to latency.", "description": "This figure shows a breakdown of latency for Mixture-of-Experts (MoE) models across different tasks (language modeling and machine translation) and numbers of nodes.  It demonstrates that while inter-node communication contributes to latency, the gating function (which determines which expert handles which token) and the expert execution time itself are major components of the overall latency.  This visualization helps to understand where optimization efforts should focus to improve the inference speed of MoE models.", "section": "Evaluation"}, {"figure_path": "stXtBqyTWX/figures/figures_16_1.jpg", "caption": "Figure 10: Memory trace of baseline MoE implementation on Pear cluster. Gating and reordering briefly allocates a large amount of memory.", "description": "This figure shows the memory usage over time for a baseline Mixture-of-Experts (MoE) model during inference on the Pear cluster. The blue line represents the total memory allocated, while the red dashed line highlights the memory usage specifically for the gating and reordering phase. The purple dashed line highlights the memory usage for the expert execution phase.  The spikes in memory allocation during the gating and reordering phases are significant, indicating inefficiencies in the baseline MoE's memory management.  These spikes demonstrate the large amount of temporary memory required for this stage and then immediately released afterward, suggesting that optimizations in this area could significantly reduce the overall memory footprint of the model.", "section": "3.4 Load Imbalance"}, {"figure_path": "stXtBqyTWX/figures/figures_18_1.jpg", "caption": "Figure 3: Throughput Comparison (Pear). Dynamic gating outperforms static, Tutel, and FastMoE consistently. It outperforms Megablock as batchsize scales. Missing bars represent infeasible combinations of policy and batch size.", "description": "This figure shows the throughput comparison of different gating policies on the Pear cluster for LM-Small.  The x-axis represents the batch size, and the y-axis represents the throughput (tokens/second).  The bars represent the throughput achieved by different gating policies: static, Tutel, FastMoE, Megablock, and dynamic gating.  Dynamic gating consistently outperforms all other methods. The performance advantage is more pronounced at larger batch sizes. Missing bars indicate that some combinations of gating policy and batch size were not feasible.", "section": "7.1 Impact of Dynamic Gating"}, {"figure_path": "stXtBqyTWX/figures/figures_19_1.jpg", "caption": "Figure 12: Cache Miss Rates. For a trace of MT-decoder's expert activations, (a) misses with and without load balancing and (b) misses compared against those from Belady's MIN. Miss rates are further reduced by load balancing.", "description": "This figure shows the cache miss rates for different cache sizes and load balancing strategies in the context of machine translation decoder tasks.  The top graph displays the worst-case cache miss rates for LIFO (Last-In, First-Out) policy, LIFO with anti-correlation, and Belady's MIN (optimal replacement algorithm). The bottom graph compares the miss rate differences of various policies against Belady's MIN, highlighting the effectiveness of the LIFO policy, particularly when combined with anti-correlation, in minimizing cache misses.", "section": "7.2 Impact of Expert Buffering"}, {"figure_path": "stXtBqyTWX/figures/figures_19_2.jpg", "caption": "Figure 13: Cache Sizes vs Latency. For MT-decoder, decreasing the cache size decreases memory usage but increases latency. Cache size (reported in number of experts) per GPU marked on plot.", "description": "This figure shows the relationship between cache size (measured in the number of experts per GPU) and decoder latency and memory usage for machine translation decoder tasks. As expected, decreasing the cache size reduces memory usage. However, this also leads to increased latency because the system has to transfer more frequently accessed experts from CPU memory to GPU memory.  The plot visually represents this trade-off.", "section": "7.2 Impact of Expert Buffering"}, {"figure_path": "stXtBqyTWX/figures/figures_19_3.jpg", "caption": "Figure 14: Load Balancing. Greedy and Anti-correlation algorithms balance load and reduce maximum load across GPUs, reducing risks from out-of-memory errors or poor performance from oversubscribed devices.", "description": "This figure demonstrates the effectiveness of the proposed load balancing techniques (Greedy and Anti-correlation) in mitigating load imbalances across GPUs. It shows the maximum and average maximum load per GPU for various tasks (Language Modeling with three different datasets and Machine Translation into three different languages), comparing the results with and without the load balancing optimizations.  The results indicate that the load balancing techniques successfully reduce the maximum load per GPU, thereby improving performance and reducing the risk of out-of-memory errors or poor performance caused by oversubscribed GPUs.  The Anti-correlation method appears particularly effective for Machine Translation decoding tasks where expert activations are highly correlated.", "section": "7.3 Impact of Load Balancing"}]