[{"figure_path": "RGnjY6l2HT/figures/figures_0_1.jpg", "caption": "Figure 1: Examples edited by UniEdit. Our solution supports both video motion editing in the time axis (i.e., from playing guitar to eating or waving) and various video appearance editing scenarios (i.e., stylization, rigid/non-rigid object replacement, background modification). We encourage the readers to watch the videos on our project page.", "description": "This figure showcases examples of video editing results produced by the UniEdit model.  It demonstrates the model's capability to perform both motion editing (changing the action of a subject in the video) and appearance editing (modifying visual aspects of the video, such as style, object replacement, and background alteration). The caption encourages viewers to visit the project webpage to view the videos for a better understanding of the results.", "section": "Abstract"}, {"figure_path": "RGnjY6l2HT/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of UniEdit. It follows an inversion-then-generation pipeline and consists of a main editing path, an auxiliary reconstruction branch and an auxiliary motion-reference branch. The reconstruction branch produces source features for content preservation, and the motion-reference branch yields text-guided motion features for motion injection. The source features and motion features are injected into the main editing path through spatial self-attention (SA-S) and temporal self-attention (SA-T) modules respectively (Sec. 4.1). We further introduce spatial structure control to retain the coarse structure of the source video (Sec. 4.2).", "description": "This figure illustrates the architecture of UniEdit, a framework for video editing.  It shows three main components: a main editing path which takes the inverted latent representation of the input video and a target prompt as input; an auxiliary reconstruction branch which uses the same latent representation but the source prompt to reconstruct the original video features for content preservation; and an auxiliary motion-reference branch which takes the same latent representation and the target prompt to generate motion features for injection into the main path.  The figure highlights how the source and motion features are integrated into the main editing path via spatial and temporal self-attention modules.  Spatial structure control is also indicated, maintaining the overall structure of the source video.", "section": "4 UniEdit"}, {"figure_path": "RGnjY6l2HT/figures/figures_4_1.jpg", "caption": "Figure 3: Detailed illustration of the relationship between the main editing path, the auxiliary reconstruction branch and the auxiliary motion-reference branch. The content preservation, motion injection and spatial structure control are achieved by the fusion of Q (query), K (key), V (value) features in spatial self-attention (SA-S) and temporal self-attention (SA-T) modules.", "description": "This figure illustrates how UniEdit combines three branches to achieve video editing. The main editing path processes the video based on target prompt. The auxiliary reconstruction branch preserves source video content, and the auxiliary motion-reference branch injects motion aligned with the text prompt. Spatial and temporal self-attention (SA-S and SA-T) modules are used to integrate these features into the main path for content preservation, motion injection, and spatial structure control.", "section": "4 UniEdit"}, {"figure_path": "RGnjY6l2HT/figures/figures_6_1.jpg", "caption": "Figure 1: Examples edited by UniEdit. Our solution supports both video motion editing in the time axis (i.e., from playing guitar to eating or waving) and various video appearance editing scenarios (i.e., stylization, rigid/non-rigid object replacement, background modification). We encourage the readers to watch the videos on our project page.", "description": "This figure showcases examples of video editing results achieved using the UniEdit framework. It demonstrates the capability of UniEdit in handling both motion and appearance editing.  The examples show changes in motion (e.g., a raccoon changing actions from playing guitar to eating or waving) and appearance (e.g., stylization, object replacement, background changes). Viewers are encouraged to view the videos for dynamic demonstration.", "section": "Abstract"}, {"figure_path": "RGnjY6l2HT/figures/figures_7_1.jpg", "caption": "Figure 5: Comparison with state-of-the-art methods for both video appearance and motion editing. It shows that UniEdit achieves better source content preservation, and outperforms baselines in motion editing by a large margin.", "description": "This figure compares UniEdit's performance against several state-of-the-art methods for both video appearance and motion editing.  It showcases four editing tasks: non-rigid object replacement, stylization, motion change (running to waving), and motion change (guitar playing to eating).  The results visually demonstrate UniEdit's superior ability to preserve the original source video content while successfully implementing the desired edits, particularly excelling in motion editing tasks.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "RGnjY6l2HT/figures/figures_8_1.jpg", "caption": "Figure 6: (6a): Visualization of spatial query in SA-S (second row), cross-frame temporal attention maps in SA-T (third row), and the magnitude of optical flow (fourth row). (6b): Visualization of the video output of the main editing path, the reconstruction branch and the motion-reference branch.", "description": "This figure shows the visualization of attention features and the output of each branch in UniEdit.  (a) Visualizes spatial and temporal attention maps alongside the optical flow magnitude, demonstrating the alignment between the attention mechanisms and the inter-frame movement. (b) Shows the video output of the main editing path, the reconstruction branch (content preservation), and the motion-reference branch (motion injection), highlighting how each component contributes to the final edited video.", "section": "4.1 Tuning-Free Video Motion Editing"}, {"figure_path": "RGnjY6l2HT/figures/figures_8_2.jpg", "caption": "Figure 7: Ablation study on hyper-parameters. (a) Ablation study on to in Eq. 2. (b) Ablation study on t\u2081 in Eq. 4.", "description": "This figure shows the results of ablation studies on two hyperparameters: to and t\u2081.  The parameter to controls the number of steps in the main editing path where value features from the reconstruction branch are used for content preservation (Eq. 2).  Different values of to (20, 30, 40, 50) are shown in the left column. The parameter t\u2081 controls the extent to which the spatial structure of the source video is preserved in the appearance editing (Eq. 4). Various values of t\u2081 (0, 15, 25, 50) are shown in the right column. The figure illustrates how different settings of these parameters affect the balance between preserving the source video content and incorporating the changes specified by the target prompt. The results demonstrate that optimal settings for to and t\u2081 are crucial for achieving good results. ", "section": "4.1 Tuning-Free Video Motion Editing"}, {"figure_path": "RGnjY6l2HT/figures/figures_15_1.jpg", "caption": "Figure 4: Examples edited by UniEdit. For each case, the upper frames come from the source videos, and the lower frames indicate the edited results with the target prompt. We encourage the readers to watch the videos and make evaluations.", "description": "This figure shows several examples of video editing results using the UniEdit model. Each example consists of two rows: the top row displays frames from the original source video, and the bottom row presents the corresponding frames after applying the UniEdit model with specific target prompts. The figure illustrates the diversity of the edits accomplished by UniEdit, and it encourages the reader to watch videos for a complete evaluation of the results.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "RGnjY6l2HT/figures/figures_16_1.jpg", "caption": "Figure 9: Visualization of attention maps and masks in mask-guided coordination (Sec. 4.3). The top row are attention maps corresponding to different tokens in CA-S modules, (a) is the final output frame, (b) and (c) are the foreground/background binary mask obtained by employing a threshold on the attention map of 'Man' token and point prompt segmentation with SAM, respectively.", "description": "This figure visualizes the attention maps from the cross-attention module (CA-S) and compares them with segmentation masks generated by a thresholding technique and the Segment Anything Model (SAM).  The top row shows how attention maps relate to different words in the prompt. The bottom row displays the final output frame and two corresponding binary masks; one derived from the CA-S attention map (using a threshold) and another using the SAM model.  This comparison aims to demonstrate that attention maps effectively capture foreground and background information and that they can be used as a proxy for more sophisticated segmentation models.", "section": "4.3 Mask-Guided Coordination (Optional)"}, {"figure_path": "RGnjY6l2HT/figures/figures_17_1.jpg", "caption": "Figure 10: Qualitative editing results across 4 settings: w/o UniEdit (2nd row), UniEdit w/o mask (3rd row), UniEdit with mask from CA-S (4th row), UniEdit with mask from SAM (5th row).", "description": "This figure shows the qualitative results obtained by applying UniEdit with different settings. The first row is the source video. The second row presents the results without applying UniEdit. The third row uses UniEdit without any masks. The fourth row uses UniEdit with masks from CA-S, and the fifth row uses UniEdit with masks from SAM. The results of both rows with masks show better visual consistency with the source video.", "section": "5.2 Ablation Study and Analysis"}, {"figure_path": "RGnjY6l2HT/figures/figures_18_1.jpg", "caption": "Figure 11: Ablation on the proposed feature injection techniques. (11a): comparison of appearance editing without feature replacement (2nd row), with QK replacement (3rd row), with V replacement (4nd row); (11b): comparison of motion editing with and without the designed spatial structure control mechanism.", "description": "This figure demonstrates the ablation study on the proposed feature injection techniques within UniEdit.  The left panel (11a) shows the results of replacing different features (query, key, and value) in the spatial self-attention (SA-S) modules during style transfer. It highlights the impact of these features on spatial structure and texture details. The right panel (11b) shows the results of motion editing with and without spatial structure control.  It illustrates the importance of spatial structure control in maintaining consistency between the edited video and the source video during motion editing.", "section": "B.4 More Observation and Analysis on the Proposed Components"}, {"figure_path": "RGnjY6l2HT/figures/figures_19_1.jpg", "caption": "Figure 12: Visualization of failure cases.", "description": "This figure shows two failure cases of UniEdit. (a) shows that when editing multiple elements simultaneously, the edited video may have relatively large inconsistency with the source video. (b) shows the result when editing video with complex scenes, the model sometimes could not understand the semantics in the target prompt, resulting in incorrect editing. ", "section": "B.6 Failure Cases Visualization"}, {"figure_path": "RGnjY6l2HT/figures/figures_20_1.jpg", "caption": "Figure 5: Comparison with state-of-the-art methods for both video appearance and motion editing. It shows that UniEdit achieves better source content preservation, and outperforms baselines in motion editing by a large margin.", "description": "This figure compares UniEdit with several state-of-the-art methods for video appearance and motion editing.  Two examples are shown, one for appearance editing (non-rigid object replacement and stylization) and one for motion editing.  For each example, it shows the original video frames at the top, followed by the results of each method, including UniEdit. The results demonstrate UniEdit's superior performance in terms of content preservation and achieving better results in motion editing.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "RGnjY6l2HT/figures/figures_20_2.jpg", "caption": "Figure 5: Comparison with state-of-the-art methods for both video appearance and motion editing. It shows that UniEdit achieves better source content preservation, and outperforms baselines in motion editing by a large margin.", "description": "This figure compares UniEdit's performance against several state-of-the-art methods for both video appearance and motion editing.  It demonstrates UniEdit's ability to preserve the original video content better while achieving superior results in motion editing tasks compared to other methods. The results are visually presented for different editing scenarios including non-rigid object replacement, stylization, and motion changes.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "RGnjY6l2HT/figures/figures_21_1.jpg", "caption": "Figure 5: Comparison with state-of-the-art methods for both video appearance and motion editing. It shows that UniEdit achieves better source content preservation, and outperforms baselines in motion editing by a large margin.", "description": "This figure compares UniEdit with several state-of-the-art methods for video appearance and motion editing.  It shows four editing tasks: non-rigid object replacement, stylization, motion change (running), and motion change (waving). For each task, it displays the results from various methods. UniEdit demonstrates better preservation of the original video content and superior performance in motion editing compared to the baselines.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "RGnjY6l2HT/figures/figures_22_1.jpg", "caption": "Figure 16: More appearance editing results of UniEdit.", "description": "This figure showcases several examples of appearance editing achieved using the UniEdit framework.  Each row presents a sequence of frames from a source video, followed by a sequence of edited frames. The edits demonstrate changes in clothing (from a black suit to a red suit and then a superman suit) and artistic style (changing to an oil painting style). The edits are all performed using text prompts as input to the UniEdit system.", "section": "B.8 More Results of UniEdit"}, {"figure_path": "RGnjY6l2HT/figures/figures_22_2.jpg", "caption": "Figure 16: More appearance editing results of UniEdit.", "description": "This figure shows more examples of appearance editing results achieved by UniEdit.  Each row presents a sequence of frames from a source video and its corresponding edited video generated by UniEdit using different text prompts. The edits cover a variety of appearance changes, demonstrating the method's flexibility.", "section": "4.2 Tuning-Free Video Appearance Editing"}, {"figure_path": "RGnjY6l2HT/figures/figures_23_1.jpg", "caption": "Figure 17: More appearance editing results of UniEdit.", "description": "This figure shows several examples of appearance editing results achieved using the UniEdit framework.  Each row shows a source video followed by edits based on various text prompts, illustrating capabilities such as changing the color palette (black and white, at night), stylization (metal robotic, cute panda), and character replacement (cute Iron Man, cute Spiderman). The results highlight the model's ability to modify the appearance of video content while preserving temporal coherence.", "section": "4 UniEdit"}, {"figure_path": "RGnjY6l2HT/figures/figures_24_1.jpg", "caption": "Figure 18: More appearance editing results of UniEdit.", "description": "This figure shows more examples of appearance editing results achieved by UniEdit.  Each row presents a source video clip followed by its edited version using UniEdit, demonstrating the model's ability to transform video appearance according to various textual prompts like changing the style to Van Gogh, Lego, winter scenery or crayon drawing style.  The consistent results across different examples showcase the robustness of the UniEdit framework in appearance editing tasks.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "RGnjY6l2HT/figures/figures_25_1.jpg", "caption": "Figure 19: More appearance editing results of UniEdit.", "description": "This figure shows more examples of appearance editing results produced by the UniEdit model.  Each row displays a source video followed by the video edited according to a target prompt.  The prompts demonstrate diverse appearance changes such as stylization (e.g., pen sketch, oil painting) and object replacement (e.g., a teddy bear changed into Mario or a wolf). The figure highlights the model's ability to manipulate video appearance while preserving temporal consistency.", "section": "4 UniEdit"}, {"figure_path": "RGnjY6l2HT/figures/figures_26_1.jpg", "caption": "Figure 20: More motion editing results of UniEdit.", "description": "This figure shows four examples of video motion editing using UniEdit.  Each example shows a sequence of frames from the original source video at the top, followed by the edited video resulting from applying a specific text prompt below.  The text prompts used here modify the motion of the subjects: jumping, running, turning around, and dancing.", "section": "4.1 Tuning-Free Video Motion Editing"}, {"figure_path": "RGnjY6l2HT/figures/figures_27_1.jpg", "caption": "Figure 1: Examples edited by UniEdit. Our solution supports both video motion editing in the time axis (i.e., from playing guitar to eating or waving) and various video appearance editing scenarios (i.e., stylization, rigid/non-rigid object replacement, background modification). We encourage the readers to watch the videos on our project page.", "description": "This figure showcases example video edits created by the UniEdit framework.  It demonstrates the model's capability to perform both motion and appearance edits. Motion editing examples show changes in the action being performed (e.g., playing guitar to waving), while appearance editing shows changes in style (e.g., Van Gogh style painting), object replacement (e.g., replacing a raccoon with Spiderman), and background modification.", "section": "Abstract"}, {"figure_path": "RGnjY6l2HT/figures/figures_27_2.jpg", "caption": "Figure 2: Overview of UniEdit. It follows an inversion-then-generation pipeline and consists of a main editing path, an auxiliary reconstruction branch and an auxiliary motion-reference branch. The reconstruction branch produces source features for content preservation, and the motion-reference branch yields text-guided motion features for motion injection. The source features and motion features are injected into the main editing path through spatial self-attention (SA-S) and temporal self-attention (SA-T) modules respectively (Sec. 4.1). We further introduce spatial structure control to retain the coarse structure of the source video (Sec. 4.2).", "description": "This figure shows the architecture of UniEdit, which is composed of three main branches: a main editing path, an auxiliary reconstruction branch, and an auxiliary motion-reference branch. The reconstruction branch helps preserve the source video content, while the motion-reference branch injects text-guided motion features.  Both branches feed into the main editing path via self-attention modules, which also incorporate spatial structure control to maintain the source video's overall structure. The entire process follows an inversion-then-generation pipeline.", "section": "4 UniEdit"}, {"figure_path": "RGnjY6l2HT/figures/figures_27_3.jpg", "caption": "Figure 1: Examples edited by UniEdit. Our solution supports both video motion editing in the time axis (i.e., from playing guitar to eating or waving) and various video appearance editing scenarios (i.e., stylization, rigid/non-rigid object replacement, background modification). We encourage the readers to watch the videos on our project page.", "description": "This figure showcases several examples of video editing results achieved using the UniEdit framework.  It highlights the framework's capability to perform both motion editing (changing the action within the video) and appearance editing (modifying the visual style or objects within the video). The examples include changing a raccoon's action from playing guitar to eating an apple or waving, applying artistic stylization (Van Gogh style), replacing objects (Spiderman), and modifying the background. Viewers are encouraged to watch the videos linked on the project page for a complete understanding of the edits.", "section": "Abstract"}, {"figure_path": "RGnjY6l2HT/figures/figures_27_4.jpg", "caption": "Figure 6: (6a): Visualization of spatial query in SA-S (second row), cross-frame temporal attention maps in SA-T (third row), and the magnitude of optical flow (fourth row). (6b): Visualization of the video output of the main editing path, the reconstruction branch and the motion-reference branch.", "description": "This figure visualizes the attention maps from the spatial self-attention (SA-S) and temporal self-attention (SA-T) layers in the UniEdit model, comparing them to the magnitude of optical flow.  It also shows the video outputs from each of UniEdit's three branches: the main editing path, the reconstruction branch, and the motion-reference branch. This helps illustrate how UniEdit uses these different components to achieve its goal of tuning-free video editing by integrating text-guided motion and preserving source video content.", "section": "4.1 Tuning-Free Video Motion Editing"}, {"figure_path": "RGnjY6l2HT/figures/figures_27_5.jpg", "caption": "Figure 5: Comparison with state-of-the-art methods for both video appearance and motion editing. It shows that UniEdit achieves better source content preservation, and outperforms baselines in motion editing by a large margin.", "description": "This figure compares the results of UniEdit with several state-of-the-art methods for both video appearance and motion editing.  It visually demonstrates UniEdit's superior performance in preserving the original content of the source video while achieving the desired edits. Specifically, UniEdit shows significantly better performance in motion editing compared to other methods. The figure showcases examples for different editing tasks, highlighting UniEdit's ability to handle various types of modifications successfully.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "RGnjY6l2HT/figures/figures_27_6.jpg", "caption": "Figure 6: (6a): Visualization of spatial query in SA-S (second row), cross-frame temporal attention maps in SA-T (third row), and the magnitude of optical flow (fourth row). (6b): Visualization of the video output of the main editing path, the reconstruction branch and the motion-reference branch.", "description": "This figure shows the visualization of attention maps in SA-S and SA-T modules and the output of different branches in the UniEdit model. The left part (6a) compares spatial query in SA-S, cross-frame temporal attention maps in SA-T and the magnitude of optical flow. The right part (6b) shows the video output generated by the main editing path, reconstruction branch and motion-reference branch, respectively. This helps to understand how the model performs content preservation and motion injection.", "section": "5.2 Ablation Study and Analysis"}, {"figure_path": "RGnjY6l2HT/figures/figures_27_7.jpg", "caption": "Figure 22: Results of text-image-to-video synthesis in Sec. 4.4.", "description": "This figure demonstrates the results of using UniEdit for text-image-to-video generation.  A source image of a husky in a field is provided. Through UniEdit, a video is generated based on the additional text prompt of \"cartoon style.\" The resulting video shows a cartoon version of the husky in a similar setting, demonstrating the model's ability to synthesize video from both text and image prompts.", "section": "4.4 T2V Models are Zero-Shot TI2V Generators"}, {"figure_path": "RGnjY6l2HT/figures/figures_27_8.jpg", "caption": "Figure 7: Ablation study on hyper-parameters.", "description": "This figure presents an ablation study on two hyper-parameters:  `to` which controls content preservation in the spatial self-attention modules, and `t1` which controls spatial structure in the appearance editing.  The top row shows the effects of varying `to` (the number of denoising steps where value features from the reconstruction branch are injected) on a motion editing task. The bottom row shows the effect of varying `t1` (the number of steps where the query and key from the reconstruction branch are used to maintain spatial consistency) on a style transfer task. The results demonstrate the importance of carefully selecting these hyper-parameters to balance content preservation, motion injection, and stylistic consistency.", "section": "5.2 Ablation Study and Analysis"}]