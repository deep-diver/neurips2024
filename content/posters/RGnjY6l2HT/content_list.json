[{"type": "text", "text": "UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/04c6782be9d470f7e4bce333d5f61d1338470c2e8e45fa46fb42a147719c2c5f.jpg", "img_caption": ["Figure 1: Examples edited by UniEdit. Our solution supports both video motion editing in the time axis (i.e., from playing guitar to eating or waving) and various video appearance editing scenarios (i.e., stylization, rigid/non-rigid object replacement, background modification). We encourage the readers to watch the videos on our project page. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "2   \n3   \n4   \n5   \n6   \n7   \n8   \n9   \n10   \n11   \n12   \n13   \n14   \n15 ", "page_idx": 0}, {"type": "text", "text": "Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization). However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored. In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversionthen-generation framework. To realize motion editing while preserving source video content, based on the insights that temporal and spatial self-attention layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively. The obtained features are then injected into the main editing path via temporal and spatial self-attention layers. Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods. Our code will be 6 publicly available. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "18 The advent of pre-trained diffusion-based [26, 53] text-to-image generators [49, 50, 48] has revo  \n19 lutionized the fields of design and filmmaking, opening new vistas for creative expression. These   \n20 advancements, underpinned by seminal works in text-to-image synthesis, have paved the way for inno  \n21 vative text-guided editing techniques for both images [42, 24, 4, 5] and videos [65, 6, 39, 70, 17, 46].   \n22 Such techniques not only enhance creative workflows but also promise to redefine content creation   \n23 within these industries.   \n24 Video editing, in contrast to image editing, introduces the intricate challenge of ensuring frame-wise   \n25 consistency. Efforts to address this challenge have led to the development of methods that leverage   \n26 shared features and structures with the source video [6, 39, 37, 70, 46, 7, 33, 62, 18] through an   \n27 inversion-then-generation pipeline [42, 53], exemplified by Pix2Video\u2019s approach [6] to consistent   \n28 appearance editing across frames. To transfer the edited appearance from the anchor frame to the   \n29 remaining frames consistently, it employs a pre-trained image generator and extends the self-attention   \n30 layers to cross-frame attention to generate each remaining frame. Despite these advancements in   \n31 performing video appearance editing (e.g., stylization, object appearance replacement, etc.), these   \n32 methodologies fall short in editing video motion (e.g., replacing the movement of playing guitar with   \n33 waving), hampered by a lack of motion priors and limited control over inter-frame dependencies,   \n34 underscoring a critical gap in video editing capabilities.   \n35 Previous attempts [65, 44] at video motion editing through fine-tuning a pre-trained generator on   \n36 the given source video and then editing motion through text guidance. Although effective, they   \n37 necessitate a delicate balance between the generative prowess of the model and the preservation of   \n38 the source video\u2019s content. This compromise often leads to restricted motion diversity and unwanted   \n39 content variations, indicating a pressing need for a more robust solution.   \n40 In response, our work aims to explore a tuning-free framework that adeptly navigates the complexities   \n41 of editing both the motion and appearance of videos. To achieve this, we identify three technical   \n42 challenges: 1) it is non-trivial to incorporate the text-guided motion into the source content, as directly   \n43 applying video appearance editing [46, 18] or image editing [5] schemes leads to undesirable results   \n44 (as shown in Fig. 5); 2) preserving the non-edited content of the source video; 3) inheriting the spatial   \n45 structure of the source video during appearance editing.   \n46 Our solution, UniEdit, harnesses the power of a pre-trained text-to-video generator (e.g., LaVie [63])   \n47 within an inversion-then-generation framework [42], tailored to overcome the identified challenges.   \n48 Particularly, we introduce three key innovations: 1) To inject text-guided motion into the source   \n49 content, we highlight the insight that the temporal self-attention layers of the generator encode   \n50 the inter-frame dependency. Acting in this way, we introduce an auxiliary motion-reference branch   \n51 to generate text-guided motion features, which are then injected into the main editing path via   \n52 temporal self-attention layers. 2) To preserve the non-edited content of the source video, motivated   \n53 by the image editing technique [5], we follow the insight that the spatial self-attention layers of the   \n54 generator encode the intra-frame dependency. Therefore, we introduce an auxiliary reconstruction   \n55 branch, and inject the features obtained from the spatial self-attention layers of the reconstruction   \n56 branch into the main editing path. 3) To retain the spatial structure during the appearance editing, we   \n57 replace the spatial attention maps of the main editing path with those in the reconstruction branch.   \n58 To our best knowledge, UniEdit represents a pioneering leap in text-guided, tuning-free video   \n59 motion editing. In addition, its unified architecture not only facilitates a wide array of video   \n60 appearance editing tasks, as shown in Fig. 1, but also empowers image-to-video generators for   \n61 zero-shot text-image-to-video generation. Through comprehensive experimentation, we demonstrate   \n62 UniEdit\u2019s superior performance relative to existing state-of-the-art methods, highlighting its potential   \n63 to significantly advance the field of video editing. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "64 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "65 2.1 Video Generation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "66 Researchers have achieved video generation with generative adversarial networks [58, 51, 61],   \n67 language models [69, 71], or diffusion models [28, 52, 25, 23, 3, 60, 72, 19, 63, 8, 47]. To make the   \n68 generation more controllable, recent endeavors have also incorporated additional structure guidance   \n69 (e.g., depth map) [16, 10, 74, 11, 20, 64], or conducted customized generation [65, 67, 34, 75, 59, 41].   \n70 These models have generally learned real-world video distribution from large-scale data, and achieved   \n71 promising results on text-to-video or image-to-video generation. Based on their success, we leverage   \n72 the learned prior in the pre-trained model to achieve tuning-free video motion and appearance editing. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "73 2.2 Video Editing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "74 Video editing aims to produce a new video that is aligned with the provided editing instructions   \n75 (e.g., text) while maintaining the other characteristics of the source video. It can be categorized into   \n76 appearance and motion editing.   \n77 For appearance editing [70, 15, 17, 35, 12], like turn a video into the style of Van Gogh, the main   \n78 challenge is to achieve temporal-consistent generation across different frames. Early attempts [6, 37,   \n79 46, 7, 33, 62] leveraged text-to-image models with inter-frame propagation to ensure consistency.   \n80 For instance, Pix2Video [6] replaces the key and value of the current frame with those of the   \n81 first and previous frame. Video-P2P [39] achieved local editing via video-specific fine-tuning and   \n82 unconditional embedding optimization [43]. Follow-up studies [18, 70, 45] also leveraged the edit  \n83 then-propagate framework with neatest-neighbor field [18], estimated optical flow [70], or temporal   \n84 deformation field [45]. Despite the promising results, due to the constraint on the source video   \n85 structure, these approaches are specialized in appearance editing and can not be applied to motion   \n86 editing directly.   \n87 Recent studies have also explored video motion editing with text guidance [65, 44], user-provided   \n88 motion [32, 54, 15], or specific motion representation [55, 36, 22]. For example, Dreamix [44]   \n89 proposed fine-tuning a pre-trained text-to-video model with mixed video-image reconstruction   \n90 objectives for each source video. Then the editing is realized by conditioning the fine-tuned model   \n91 on the given target prompt. MoCA [68] decoupled the video into the first-frame appearance and   \n92 the optical flow, and trained a diffusion model to generate video conditioned on the first frame and   \n93 the text. However, it struggled to preserve the non-edited motion (e.g., background dynamics) as   \n94 it generates the entire motion from the text. Different from the aforementioned approaches that   \n95 require fine-tuning or user-provided motion input, we are the first to achieve tuning-free motion and   \n96 appearance editing with text guidance only. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "97 3 Preliminaries: Video Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "98 Our proposed UniEdit is built upon video diffusion models. Therefore, we first recap the architecture   \n99 that is used in common text-guided video diffusion models [63, 2].   \n100 Overall Architecture Modern text-to-video (T2V) diffusion models typically extend a pre-trained   \n101 text-to-image (T2I) model [49] to the video domain with the following adaptations. 1) Introducing   \n102 additional temporal layers by inflating 2d convolutional layers to 3d form, or adding temporal   \n103 self-attention layers [57] to model the correlation between video frames. 2) Due to the extensive   \n104 computational resources for modeling spatial-temporal joint distribution, these works typically   \n105 first train video generation models on low spatial and temporal resolutions, and then upsampling   \n106 the generated results with cascaded models. 3) Other improvements like efficiency [1], training   \n107 strategy [19], or additional control signals [16], etc. During inference, given standard Gaussian   \n108 distribution $z_{T}\\sim\\mathcal{N}(0,1)$ , the denoising UNet is used to perform $T$ denoising steps to obtain the   \n109 outputs [26, 53]. If the model is trained in latent space [49], a decoder is employed to reconstruct   \n110 videos from the latent domain.   \n111 Attention Mechanisms In particular, for each block of the denoising UNet, there are four basic   \n112 modules: a convolutional module, a spatial self-attention module (SA-S), a spatial cross-attention   \n113 module (CA-S), and a temporal self-attention module (SA-T). Formally, the attention operation [57]   \n114 can be formulated as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{\\sfattn}(Q,K,V)=\\mathrm{\\sfsoftmax}(\\frac{Q K^{T}}{\\sqrt{d}})V,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "115 where $Q$ (query), $K$ (key), $V$ (value) are derived from inputs, and $d$ is the dimension of hidden states. ", "page_idx": 2}, {"type": "text", "text": "116 Intuitively, CA-S is in charge of fusing semantics from the text condition, SA-S models the intra  \n117 frame dependency, SA-T models the inter-frame dependency and ensures the generated results are   \n118 temporally consistent. We leverage these intuitions in our designs as elaborated below. ", "page_idx": 2}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/2d64548686a753a47d3434b74a3ed006dfb5f4eea5b7931589f7ad693ce37828.jpg", "img_caption": ["Figure 2: Overview of UniEdit. It follows an inversion-then-generation pipeline and consists of a main editing path, an auxiliary reconstruction branch and an auxiliary motion-reference branch. The reconstruction branch produces source features for content preservation, and the motion-reference branch yields text-guided motion features for motion injection. The source features and motion features are injected into the main editing path through spatial self-attention (SA-S) and temporal self-attention (SA-T) modules respectively (Sec. 4.1). We further introduce spatial structure control to retain the coarse structure of the source video (Sec. 4.2). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "119 4 UniEdit ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "120 Method Overview. As shown in Fig. 2, our main editing path is based on an inversion-then  \n121 generation pipeline: we use the latent after DDIM inversion [53] as the initial noise $z_{T}\\,^{1}$ , then perform   \n122 denoising process starting from $z_{T}$ with the pre-trained UNet conditioned on the target prompt $P_{t}$ . For   \n123 motion editing, to achieve source content preservation and motion control, we propose to incorporate   \n124 an auxiliary reconstruction branch and an auxiliary motion-reference branch to provide desired source   \n125 and motion features, which are injected into the main editing path to achieve content preservation and   \n126 motion editing (as shown in Fig. 3). We propose the pipeline of motion editing and appearance editing   \n127 in Sec. 4.1 & Sec. 4.2 respectively. To further alleviate the background inconsistency, we introduce   \n128 a mask-guided coordination scheme in Sec. 4.3. We also extend UniEdit to text-image-to-video   \n129 generation (TI2V) in Sec. 4.4. ", "page_idx": 3}, {"type": "text", "text": "130 4.1 Tuning-Free Video Motion Editing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "131 Content Preservation on SA-S Modules. One of the key challenges of editing tasks is to inherit   \n132 the original content (e.g., textures and background) in the source video. To this end, we introduce   \n133 an auxiliary reconstruction branch. The reconstruction path starts from the same inversed latent   \n134 $z_{T}$ similar to the main editing path, and then conducts the denoising process with the pre-trained   \n135 UNet conditioned on the source prompt $P_{s}$ to reconstruct the original frames. As verified in image   \n136 editing [56, 24, 5], the attention features in the denoising model during reconstruction contain the   \n137 content of the source video. Hence, we inject attention features of the reconstruction path into the   \n138 main editing path on spatial self-attention (SA-S) layers for content preservation. At denoising step $t$ ,   \n139 the attention operation of the $l$ -th SA-S module in the main editing path is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{SA}\\!-\\!\\mathrm{S}_{\\mathrm{edit}}^{l}:=\\left\\{\\!\\!\\operatorname{attn}(Q,K,V^{r}),\\quad t<t_{0}\\,\\mathrm{and}\\,l>L,\\!\\!\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "140 where $Q,K,V$ are features in the main editing path, $V^{r}$ refer to the value feature of the corresponding   \n141 SA-S layer in the reconstruction branch, $t_{0}=50$ and $L=10$ are hyper-parameters following previous   \n142 work [5]. By replacing the value of spatial features, the video synthesized by the main editing path   \n143 retains the non-edited characters (e.g., identity and background) of the source video, as exhibited   \n144 in Fig. 7a. Unlike previous video editing works [37, 29] which introduces a cross-frame attention   \n145 mechanism (i.e., using the key and value of the first/last frame), we implement Eq. 2 frame-wisely to   \n146 better tackle source video with large dynamics.   \n147 Motion Injection on SA-T Modules. After implementing the content-preserving technique intro  \n148 duced above, we can obtain an edited video with the same content in the source video. However, it   \n149 is observed that the output video could not follow the text prompt $P_{t}$ properly. A straightforward   \n150 solution is to increase the value of $L$ so that balancing between the impact of injected information and   \n151 the conditioned text prompt. Nevertheless, this could result in a content mismatch with the original   \n152 source video in terms of structures and textures.   \n153 To obtain the desired motion without sacrificing content consistency, we propose to guide the main   \n154 editing path with reference motion. Concretely, an auxiliary motion-reference branch (which also   \n155 starts from the inversed latent $z_{T}$ ) is involved during the denoising process. Different from the   \n156 reconstruction branch, the motion-reference branch is conditioned on the target prompt $P_{t}$ , which   \n157 contains the description of the desired motion. To transfer the motion into the main editing path, our   \n158 core insight here is that temporal layers model the inter-frame dependency of the synthesized video   \n159 clip (as shown in Fig. 6). Motivated by the observations above, we design the attention map injection   \n160 on temporal self-attention layers of the main editing path: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{SA}\\!\\!-\\!\\mathrm{T}_{\\mathrm{edit}}^{l}:={\\sf a t t n}(Q^{m},K^{m},V)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 where $Q^{m}$ and $K^{m}$ refer to the query and key of the motion-reference branch, note that we replace   \n162 the query and key of SA-T modules in the main editing path with those in the motion-reference   \n163 branch on all layers and denoising steps. It\u2019s observed that the injection of temporal attention maps   \n164 can effectively facilitate the main editing path to generate motion aligned with the target prompt.   \n165 To better fuse the motion with the content in the source video, we also implement spatial structure   \n166 control (refer to Sec. 4.2) on the main editing path and motion-reference branch in the early steps. ", "page_idx": 4}, {"type": "text", "text": "167 4.2 Tuning-Free Video Appearance Editing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "168 In Sec. 4.1, we introduce the pipeline of UniEdit   \n169 for video motion editing. In this subsection,   \n170 we aim to perform appearance editing (e.g.,   \n171 style transfer, object replacement, background   \n172 changing) via the same framework. In general,   \n173 there are two main differences between appear  \n174 ance editing and motion editing. Firstly, ap  \n175 pearance editing does not require changing the   \n176 inter-frame relationships. Therefore, we remove   \n177 the motion-reference branch and corresponding   \n178 motion injection mechanism from the motion   \n179 editing pipeline. Secondly, the main challenge   \n180 of appearance editing is to maintain the struc  \n181 tural consistency of the source video. To address   \n182 this, we introduce spatial structure control be  \n183 tween the main editing path and the reconstruc  \n184 tion branch. ", "page_idx": 4}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/89778c1398d26d35d9aa0ed829f643b56028a1b7bb24108f9391b5e6d3efd567.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Detailed illustration of the relationship between the main editing path, the auxiliary reconstruction branch and the auxiliary motionreference branch. The content preservation, motion injection and spatial structure control are achieved by the fusion of $Q$ (query), $K$ (key), $V$ (value) features in spatial self-attention (SA-S) and temporal self-attention (SA-T) modules. ", "page_idx": 4}, {"type": "text", "text": "185 Spatial Structure Control on SA-S Modules. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "186 Previous approaches on video appearance editing [70, 18] mainly realize spatial structure control   \n187 with the assistance of additional network [73]. When the auxiliary control model fails, it may result   \n188 in inferior performance in preserving the structure of the original video. Alternatively, we suggest   \n189 extracting the layout information of the source video from the reconstruction branch. Intuitively,   \n190 the attention maps in spatial self-attention layers encode the structure of the synthesized video, as   \n191 verified in Fig. 6. Hence, we replace the query and key of SA-S module in the main editing path with   \n192 those in the reconstruction branch: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{SA-S}_{\\mathrm{edit}}^{l}:=\\left\\{\\!\\!\\!\\begin{array}{l}{\\mathtt{a t t n}(Q^{r},K^{r},V),\\quad t<t_{1},}\\\\ {\\mathtt{a t t n}(Q,K,V),\\quad}&{\\mathrm{otherwise},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "193 where $Q^{r}$ and $K^{r}$ refer to the query and key of the reconstruction branch, $t_{1}$ is used to control the   \n194 extent of editing. It is worth mentioning that the effect of spatial structure control is distinct from the   \n195 content preservation mechanism in Sec. 4.1. Take stylization as an example, the proposed structure   \n196 control in Eq. 4 only ensures consistency in terms of each frame\u2019s composition, while enabling the   \n197 model to generate the required textures and styles based on the text prompt. On the other hand,   \n198 the content preservation technique inherits the textures and style of the source video. Therefore,   \n199 we use structure control instead of content preservation for appearance editing. In addition, using   \n200 the proposed structure control technique in motion editing can make the layout of the output video   \n201 similar to the source video (shown in Fig. 11b in Appendix). Users have the flexibility to adjust the   \n202 consistency between the edited video and the source video layout based on their specific requirements. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "203 4.3 Mask-Guided Coordination (Optional) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "204 To further improve the editing performance, we suggest leveraging the foreground/background   \n205 segmentation mask $M$ to guide the denoising process [14, 13]. There are two possible ways to obtain   \n206 the mask $M$ : the attention maps of CA-S modules with a threshold [24]; or employing an off-the-shelf   \n207 segmentation model [38] on the source and generated videos. The obtained segmentation masks can   \n208 be leveraged to 1), alleviate the indistinction in foreground and background; 2), improve content   \n209 consistency between edited and source videos. To this end, we leverage mask-guided self-attention in   \n210 the main editing path to coordinate the editing process. Formally, we define: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathsf{m\\mathrm{-}}\\mathsf{a t t n}(Q,K,V;M)=\\mathsf{s o f t m a x}(\\frac{Q K^{T}}{\\sqrt{d}}+M)V.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "211 Then the mask-guided self-attention: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{SA}_{\\mathrm{mask}}:=\\boldsymbol{\\mathbf{m-attn}}(Q,K,V;M^{f})\\odot M_{m}+\\boldsymbol{\\mathbf{m-attn}}(Q,K,V;M^{b})\\odot(1-M_{m}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "212 where $M^{f},M^{b}\\;\\in\\;\\{-\\infty,0\\}$ indicate the foreground and background masks in the editing path   \n213 respectively, $M_{m}\\in\\{0,1\\}$ denotes the foreground mask from the motion-reference branch, and $\\odot$ is   \n214 Hadamard product. In addition, we leverage the mask during the content preservation and motion   \n215 injection for the features obtained from the reconstruction branch and the motion-reference branch   \n216 (e.g., we replace $Q^{m}$ with $M_{m}\\odot Q^{m}+(1-M_{m})\\odot Q)$ . ", "page_idx": 5}, {"type": "text", "text": "217 4.4 T2V Models are Zero-Shot TI2V Generators ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "218 To make our framework more flexible, we further derive a method to incorporate images as input   \n219 and synthesize high-quality video conditioned on both image and text-prompt. Different from some   \n220 image animation techniques [2], our method allows the user to guide the animation process with text   \n221 prompts. Concretely, we first achieve image-to-video (I2V) generation by: 1) transforming input   \n222 images with simulated camera movement to form a pseudo-video clip [44] or 2) leveraging existing   \n223 image animation approaches (e.g., SVD [2], AnimateDiff [21]) to synthesis a video with random   \n224 motion (which may not consistent with the text prompt). Then, we perform text-guided editing with   \n225 UniEdit on the vanilla video to obtain the final output video. ", "page_idx": 5}, {"type": "text", "text": "226 5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "227 5.1 Comparison with State-of-the-Art Methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "228 Implementation Details UniEdit is not limited to specific video diffusion models. In this section,   \n229 we build UniEdit upon LaVie [63] as an instantiation to verify the effectiveness of our method. To   \n230 demonstrate the flexibility of UniEdit across different base models, we also implement the proposed   \n231 method on VideoCrafter2 [9] and exhibit the editing results in Appendix B.1. For each input video,   \n232 we follow the pre-processing step in LaVie to the resolution of $320\\times512$ . Then, the pre-processed   \n233 video is fed into the UniEdit to perform video editing. It takes 1-2 minutes to edit on an NVIDIA   \n234 A100 GPU for each video. More details can be found in Appendix A.   \n235 Baselines. To evaluate the performance of UniEdit, we compare the editing results of UniEdit   \n236 with state-of-the-art motion and appearance editing approaches. For motion editing, due to the   \n237 lack of open-source tuning-free (zero-shot) methods, we adapt the state-of-the-art non-rigid image   \n238 editing technique MasaCtrl [5] to a T2V model [63] (denoted as MasaCtrl\u2217in Fig. 5) and a one-shot   \n239 video editing method Tune-A-Video (TAV) [65] as strong baselines. For appearance editing, we   \n240 use the latest methods with strong performance, including FateZero [46], TokenFlow [18], and   \n241 Rerender-A-Video (Rerender) [70] as baselines.   \n242 Evaluation Set. The evaluation set consists of 100 samples, including: a) 20 randomly sampled   \n243 video clips from the open-source LOVEU-TGVE-2023 [66] dataset, along with their corresponding   \n244 80 text prompts, and b) 20 videos from online sources (www.pexels.com and www.pixabay.com),   \n245 with manually designed prompts, as the baseline methods do not have an open-source evaluation set.   \n246 Qualitative Results. We present editing examples of UniEdit in Fig. 1, Fig. 4 (additional examples   \n247 in Fig. 16-21 of Appendix B.8). Please visit our project page for more videos. UniEdit demonstrates   \n248 the ability to: 1) edit in various scenarios, including motion-changing, object replacement, style   \n249 transfer, and background modification; 2) align with the target prompt; and 3) maintain excellent   \n250 temporal consistency. Additionally, we compare UniEdit with state-of-the-art methods in Fig. 5   \n251 (further comparisons in Fig.13,14,15 of Appendix B.7). For a fair comparison, we also migrated   \n252 all baselines to LaVie [63], using the same base model as our method. The results are presented   \n253 in Fig. 15. For appearance editing, we showcase two scenarios: non-rigid object replacement and   \n254 stylization. In object replacement, our method outperforms baselines in terms of prompt alignment   \n255 and background consistency. In stylization, UniEdit excels in preserving content. For example, the   \n256 grassland retains its original appearance without any additional elements. In motion editing, UniEdit   \n257 surpasses baselines in aligning the video with the target prompt and preserving the source content.   \n258 Quantitative Results. We quantitatively evaluate our method using two approaches: 1) CLIP   \n259 scores and user preference, as employed in previous work [65]; and 2) VBench [31] scores, a recently   \n260 proposed benchmark suite for T2V models. The summarized results are in Tab. 1. Following previous   \n261 work [65], we assess the effectiveness of our method in terms of temporal consistency and alignment   \n262 with the target prompt. Additionally, we conducted a user study involving 10 participants who rated   \n263 the edited videos on a scale of 1 to 5. We also utilize the recently proposed VBench [31] benchmark   \n264 to provide a more comprehensive assessment, which includes \u2018Frame Quality\u2019 metrics and \u2018Temporal   \n265 Quality\u2019 metrics. UniEdit outperforms the baseline methods across all metrics. Furthermore, the   \n266 mask-guided coordination technique introduced in Sec. 4.3 further enhances performance (see   \n267 Appendix B.3). For more detailed quantitative results, please refer to Appendix B.2&B.3&B.5. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/f1aa076c6328f12a979d5c3d4120a85769c030321c4252fdc70b67a23f7365ff.jpg", "img_caption": ["Figure 4: Examples edited by UniEdit. For each case, the upper frames come from the source video, and the lower frames indicate the edited results with the target prompt. We encourage the readers to watch the videos and make evaluations. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "268 5.2 Ablation Study and Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "269 How UniEdit Works? To better understand how UniEdit works and reveal our insight on the   \n270 spatial and temporal self-attention layers, we visualize the features in the SA-S and SA-T modules   \n271 and compare them with the magnitude of optical flow between adjacent frames in Fig. 6a. It is evident   \n272 that, in comparison to the spatial query maps (2nd row), the temporal cross-frame attention maps (3rd   \n273 row) exhibit a notably higher degree of overlap with the optical flow (4th row). This indicates that the   \n274 temporal self-attention layers encode inter-frame dependencies and facilitate motion injection, while   \n275 content preservation and structure control are carried out in the spatial self-attention layers.   \n76 Output Visualization of the Two Auxiliary Branches. Recall that to perform motion editing,   \n77 we propose to transfer the targeted motion from the motion-reference branch and realize content   \n78 preservation via feature injection from the reconstruction branch. To verify the effectiveness, we   \n79 visualized the output of each branch in Fig. 6b. It is observed that the motion-reference branch   \n80 (4th row) generates video with the target motion, and effectively transfers it to the main path (3rd   \n81 row); meanwhile, the main path inherits the content from the reconstruction branch (2nd row), thus   \n82 enhancing the consistency of unedited parts. ", "page_idx": 6}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/50ae7206b4be66496341e9ea745a24c988ace0e9c293d26fe7a80c6d6660b082.jpg", "img_caption": ["Figure 5: Comparison with state-of-the-art methods for both video appearance and motion editing. It shows that UniEdit achieves better source content preservation, and outperforms baselines in motion editing by a large margin. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "RGnjY6l2HT/tmp/7ab447f87b0097116092959490041712d61906278ab416f1b043d9905641816b.jpg", "table_caption": ["Table 1: Quantitative comparison with state-of-the-art video editing techniques. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "The Effectiveness of Each Component. To demonstrate that all the designed feature injection techniques in Sec. 4.1 & 4.2 contribute to the final results, we make a quantitative evaluation on 15 motion editing cases, as we utilize all three components in motion editing. To assess the similarity between the edited video and the source video (e.g., background and identity), we introduce the \u2018Frame ", "page_idx": 7}, {"type": "table", "img_path": "RGnjY6l2HT/tmp/f3fa92ccde64f49c37b02699606edb1f5c33f63d472d19417b97b62f74ab3e23.jpg", "table_caption": ["Table 2: Impact of various components. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Similarity\u2019, which is the average frame cosine similarity between the source frame embedding and the edited frame embedding. As shown in Tab. 2, editing with content preservation results in high frame similarity, suggesting that replacing value features in SA-S modules can effectively retain the content of the source video. The use of motion injection and structure control significantly enhances \u2018Textual Alignment\u2019, indicating successful transfer of the targeted motion to the main editing path. Ultimately, the best results are achieved through the combined use of all components. ", "page_idx": 7}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/49c30e437a8f1329a4b6dff7781c2edd7c1d3bf6524c83e5c57ec9fc4387984c.jpg", "img_caption": ["Figure 6: (6a): Visualization of spatial query in SA-S (second row), cross-frame temporal attention maps in SA-T (third row), and the magnitude of optical flow (fourth row). (6b): Visualization of the video output of the main editing path, the reconstruction branch and the motion-reference branch. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/1f7e2286156d1c3f041a584b981413e4b855e117ff081f74fce99cb069d113a2.jpg", "img_caption": ["Figure 7: Ablation study on hyper-parameters. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "297 Ablation on Hyper-parameters. We utilize content preservation in Eq. 2 to maintain the original   \n298 content from the source video. By varying the feature injection steps in Fig. 7a, we observe that   \n299 replacing the value features at a few steps introduces inconsistencies in the background (footprints   \n300 on the beach). In practice, we adhere to the hyper-parameter selection outlined in [5] (last row).   \n301 Simultaneously, we note that adjusting the blend layers and steps in Eq. 4 can effectively regulate   \n302 the extent to which the edited image adheres to the original image. For instance, in the stylization   \n303 demonstrated in Fig. 7b, injecting the attention map into fewer (15) steps yields a stylized output that   \n304 may not retain the same structure as the input, while injecting into all 50 steps results in videos with   \n305 nearly identical textures but less stylization. Users have the flexibility to adjust the blended steps to   \n306 achieve their preferred balance between stylization and fidelity. ", "page_idx": 8}, {"type": "text", "text": "307 6 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "308 In this paper, we design a novel tuning-free framework UniEdit for both video motion and appearance   \n309 editing. By leveraging a motion-reference branch and a reconstruction branch and injecting features   \n310 into the main editing path, it is capable of performing motion editing and various appearance   \n311 editing. There are nevertheless some limitations. Firstly, we observe performance degradation when   \n312 performing both types of editing simultaneously. Secondly, since our work is based on T2V models,   \n313 the proposed method also inherits some of the shortcomings of the existing models, such as inferior   \n314 performance in understanding complex prompts. We exhibit the failure cases in Appendix B.6. ", "page_idx": 8}, {"type": "text", "text": "315 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "316 [1] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat,   \n317 Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space-time diffusion model for   \n318 video generation. arXiv preprint arXiv:2401.12945, 2024.   \n319 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Do  \n320 minik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion:   \n321 Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.   \n322 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja   \n323 Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent   \n324 diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n325 Recognition, pages 22563\u201322575, 2023.   \n326 [4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow   \n327 image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision   \n328 and Pattern Recognition, pages 18392\u201318402, 2023.   \n329 [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng.   \n330 Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing.   \n331 In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.   \n332 [6] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. Pix2video: Video editing using image   \n333 diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision,   \n334 pages 23206\u201323217, 2023.   \n335 [7] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency  \n336 aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on   \n337 Computer Vision, pages 23040\u201323050, 2023.   \n338 [8] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo   \n339 Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for   \n340 high-quality video generation. arXiv preprint arXiv:2310.19512, 2023.   \n341 [9] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying   \n342 Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models.   \n343 arXiv preprint arXiv:2401.09047, 2024.   \n344 [10] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, and Ming-Hsuan   \n345 Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint   \n346 arXiv:2304.14404, 2023.   \n347 [11] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin.   \n348 Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint   \n349 arXiv:2305.13840, 2023.   \n350 [12] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan  \n351 Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided   \n352 attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023.   \n353 [13] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion  \n354 based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022.   \n355 [14] Paul Couairon, Cl\u00e9ment Rambour, Jean-Emmanuel Haugeard, and Nicolas Thome. Videdit:   \n356 Zero-shot and spatially aware text-driven video editing. arXiv preprint arXiv:2306.08707, 2023.   \n357 [15] Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo:   \n358 Interactive drag-style video editing. arXiv preprint arXiv:2312.02216, 2023.   \n359 [16] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis   \n360 Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings   \n361 of the IEEE/CVF International Conference on Computer Vision, pages 7346\u20137356, 2023.   \n362 [17] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo   \n363 Chen, and Baining Guo. Ccedit: Creative and controllable video editing via diffusion models.   \n364 arXiv preprint arXiv:2309.16496, 2023.   \n365 [18] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion   \n366 features for consistent video editing. In International Conference on Learning Representations   \n367 (ICLR), 2024.   \n368 [19] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh   \n369 Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing   \n370 text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709,   \n371 2023.   \n372 [20] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl:   \n373 Adding sparse controls to text-to-video diffusion models. arXiv preprint arXiv:2311.16933,   \n374 2023.   \n375 [21] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Ani  \n376 matediff: Animate your personalized text-to-image diffusion models without specific tuning.   \n377 arXiv preprint arXiv:2307.04725, 2023.   \n378 [22] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan,   \n379 Chunyu Wang, Han Hu, et al. Gaia: Zero-shot talking avatar generation. In International   \n380 Conference on Learning Representations (ICLR), 2024.   \n381 [23] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video   \n382 diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint   \n383 arXiv:2211.13221, 2022.   \n384 [24] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.   \n385 Prompt-to-prompt image editing with cross attention control. In International Conference on   \n386 Learning Representations (ICLR), 2023.   \n387 [25] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,   \n388 Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High   \n389 definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.   \n390 [26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances   \n391 in neural information processing systems, 33:6840\u20136851, 2020.   \n392 [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint   \n393 arXiv:2207.12598, 2022.   \n394 [28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J   \n395 Fleet. Video diffusion models. arXiv:2204.03458, 2022.   \n396 [29] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, and Sibei Yang. Free  \n397 bloom: Zero-shot text-to-video generator with llm director and ldm animator. arXiv preprint   \n398 arXiv:2309.14494, 2023.   \n399 [30] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao   \n400 Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction  \n401 based image editing with multimodal large language models. arXiv preprint arXiv:2312.06739,   \n402 2023.   \n403 [31] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang,   \n404 Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang,   \n405 Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video   \n406 generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n407 Pattern Recognition, 2024.   \n408 [32] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using   \n409 temporal attention adaption for text-to-video diffusion models. arXiv preprint arXiv:2312.00845,   \n410 2023.   \n411 [33] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-shot grounded video editing using   \n412 text-to-image diffusion models. arXiv preprint arXiv:2310.01107, 2023.   \n413 [34] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change   \n414 Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. arXiv   \n415 preprint arXiv:2312.00777, 2023.   \n416 [35] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M Rehg, and Pinar Yanardag. Rave:   \n417 Randomized noise shuffling for fast and consistent video editing with diffusion models. arXiv   \n418 preprint arXiv:2312.04524, 2023.   \n419 [36] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman.   \n420 Dreampose: Fashion image-to-video synthesis via stable diffusion. arXiv preprint   \n421 arXiv:2304.06025, 2023.   \n422 [37] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang   \n423 Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion   \n424 models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023.   \n425 [38] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,   \n426 Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv   \n427 preprint arXiv:2304.02643, 2023.   \n428 [39] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing   \n429 with cross-attention control. arXiv preprint arXiv:2303.04761, 2023.   \n430 [40] Qi Mao, Lan Chen, Yuchao Gu, Zhen Fang, and Mike Zheng Shou. Mag-edit: Localized   \n431 image editing in complex scenarios via mask-based attention-adjusted guidance. arXiv preprint   \n432 arXiv:2312.11396, 2023.   \n433 [41] Joanna Materzynska, Josef Sivic, Eli Shechtman, Antonio Torralba, Richard Zhang, and   \n434 Bryan Russell. Customizing motion in text-to-video diffusion models. arXiv preprint   \n435 arXiv:2312.04966, 2023.   \n436 [42] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano   \n437 Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In   \n438 International Conference on Learning Representations, 2022.   \n439 [43] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion   \n440 for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF   \n441 Conference on Computer Vision and Pattern Recognition, pages 6038\u20136047, 2023.   \n442 [44] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv   \n443 Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors.   \n444 arXiv preprint arXiv:2302.01329, 2023.   \n445 [45] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei   \n446 Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally   \n447 consistent video processing. arXiv preprint arXiv:2308.07926, 2023.   \n448 [46] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng   \n449 Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the   \n450 IEEE/CVF International Conference on Computer Vision, 2023.   \n451 [47] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei   \n452 Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint   \n453 arXiv:2310.15169, 2023.   \n454 [48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical   \n455 text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3,   \n456 2022.   \n457 [49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High  \n458 resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF   \n459 conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n460 [50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,   \n461 Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.   \n462 Photorealistic text-to-image diffusion models with deep language understanding. Advances in   \n463 Neural Information Processing Systems, 35:36479\u201336494, 2022.   \n464 [51] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with   \n465 singular value clipping. In Proceedings of the IEEE international conference on computer   \n466 vision, pages 2830\u20132839, 2017.   \n467 [52] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,   \n468 Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without   \n469 text-video data. arXiv preprint arXiv:2209.14792, 2022.   \n470 [53] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In   \n471 International Conference on Learning Representations (ICLR), 2021.   \n472 [54] Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, and Xihui Liu. Drag-a-video: Non-rigid   \n473 video editing with point-based interaction. arXiv preprint arXiv:2312.02936, 2023.   \n474 [55] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Mo  \n475 tioneditor: Editing video motion via content-aware diffusion. arXiv preprint arXiv:2311.18830,   \n476 2023.   \n477 [56] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features   \n478 for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on   \n479 Computer Vision and Pattern Recognition, pages 1921\u20131930, 2023.   \n480 [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n481 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information   \n482 processing systems, 30, 2017.   \n483 [58] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.   \n484 Advances in neural information processing systems, 29, 2016.   \n485 [59] Cong Wang, Jiaxi Gu, Panwen Hu, Songcen Xu, Hang Xu, and Xiaodan Liang. Dreamvideo:   \n486 High-fidelity image-to-video generation with image retention and text guidance. arXiv preprint   \n487 arXiv:2312.03018, 2023.   \n488 [60] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.   \n489 Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023.   \n490 [61] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Bryan Catanzaro, and Jan Kautz.   \n491 Few-shot video-to-video synthesis. Advances in Neural Information Processing Systems, 32,   \n492 2019.   \n493 [62] Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and   \n494 Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv   \n495 preprint arXiv:2303.17599, 2023.   \n496 [63] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang,   \n497 Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded   \n498 latent diffusion models. arXiv preprint arXiv:2309.15103, 2023.   \n499 [64] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying   \n500 Shan. Motionctrl: A unified and flexible motion controller for video generation. arXiv preprint   \n501 arXiv:2312.03641, 2023.   \n502 [65] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne   \n503 Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image   \n504 diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International   \n505 Conference on Computer Vision, pages 7623\u20137633, 2023.   \n506 [66] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang,   \n507 Youzeng Li, Zuwei Huang, Yuanxi Sun, Rui He, Feng Hu, Junhua Hu, Hai Huang, Hanyu Zhu,   \n508 Xu Cheng, Jie Tang, Mike Zheng Shou, Kurt Keutzer, and Forrest Iandola. Cvpr 2023 text   \n509 guided video editing competition, 2023.   \n510 [67] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan   \n511 Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, et al. Make-your-video: Customized video   \n512 generation using textual and structural guidance. arXiv preprint arXiv:2306.00943, 2023.   \n513 [68] Wilson Yan, Andrew Brown, Pieter Abbeel, Rohit Girdhar, and Samaneh Azadi. Motion  \n514 conditioned image animation for video editing. arXiv preprint arXiv:2311.18827, 2023.   \n515 [69] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation   \n516 using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.   \n517 [70] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change Loy. Rerender a video: Zero-shot   \n518 text-guided video-to-video translation. In ACM SIGGRAPH Asia 2023 Conference Proceedings,   \n519 2023.   \n520 [71] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00e9 Lezama, Han Zhang, Huiwen Chang, Alexander G   \n521 Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video   \n522 transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n523 Recognition, pages 10459\u201310469, 2023.   \n524 [72] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu,   \n525 Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for   \n526 text-to-video generation. arXiv preprint arXiv:2309.15818, 2023.   \n527 [73] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image   \n528 diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer   \n529 Vision, pages 3836\u20133847, 2023.   \n530 [74] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and   \n531 Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint   \n532 arXiv:2305.13077, 2023.   \n533 [75] Yuxin Zhang, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Weiming Dong, and   \n534 Changsheng Xu. Motioncrafter: One-shot motion customization of diffusion models. arXiv   \n535 preprint arXiv:2312.05288, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Materials ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "536 We organize the Appendix as follows: ", "page_idx": 14}, {"type": "text", "text": "537 \u2022 Appendix A: detailed descriptions of experimental settings.   \n538 \u2022 Appendix B: more experimental results, including:   \n539 \u2022 Editing results on different T2V model (Appendix B.1).   \n540 \u2022 Quantitative ablation on hyper-parameter selection (Appendix B.2).   \n541 \u2022 Ablation study on mask-guided coordination (Appendix B.3).   \n542 \u2022 Observation and analysis on the proposed components (Appendix B.4).   \n543 \u2022 Analysis and comparison on inference time (Appendix B.5).   \n544 \u2022 Failure cases visualization (Appendix B.6).   \n545 \u2022 More Comparisons with baseline methods (Appendix B.7).   \n546 \u2022 More Editing results of UniEdit (Appendix B.8). ", "page_idx": 14}, {"type": "text", "text": "547 \u2022 Appendix C: Broader Impacts. ", "page_idx": 14}, {"type": "text", "text": "548 We encourage the readers to watch the videos on our project page. ", "page_idx": 14}, {"type": "text", "text": "549 A Detailed Experimental Settings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "550 Base T2V Model. We instantiate the proposed method on LaVie [63], which is a pre-trained   \n551 text-to-video generation model that produces consistent and high-quality videos. To achieve a fair   \n552 comparison, we only leverage the base T2V model in LaVie and load the open-source pre-trained   \n553 weights for video editing tasks in the experiments. Note that the edited video clip could further be   \n554 seamlessly fed into the temporal interpolation model and the video super-resolution model to obtain   \n555 video with a longer duration and higher resolution.   \n556 Video Preprocessing. For each input video, we resize it to the resolution of $320\\times512$ , followed by   \n557 normalization, which is consistent with the training configuration of LaVie. Then, the pre-processed   \n558 video is fed into the base model of Lavie to perform video editing. To maximize the generation power   \n559 of LaVie, we set all input videos to 16 frames. For a source video, it takes 1-2 minutes to edit on an   \n560 NVIDIA A100 GPU.   \n561 Configurations. For real source videos, we inverse them with 50 DDIM inversion steps and perform   \n562 DDIM deterministic sampling with 50 steps for generation. For the generated videos, we use the   \n563 same start latent of synthesizing the source video as the initial noise $z_{T}$ for the main editing path and   \n564 two auxiliary branches. We use the commonly used classifier-free guidance technique [27] with a   \n565 scale of 7.5.   \n566 Details of User Study. As a text-guided editing task, in addition to CLIP scores, it is crucial to   \n567 evaluate results through human subjective assessment. To achieve this, we utilized MOS (Mean   \n568 Opinion Score) as our metric and collected feedback from 10 experienced volunteers. We randomly   \n569 selected 20 editing samples and permuted results from different models. Volunteers were then tasked   \n570 to evaluate the results based on two perspectives: frame consistency and textual alignment. They   \n571 provided ratings for these aspects on a scale of 1-5. Specifically, frame consistency measures the   \n572 smoothness of the video, aiming to avoid dramatic jittering and ensure coherence between the content   \n573 of each frame. Textual alignment assesses whether the editing results adhere to the text guidance and   \n574 maintain the content of the source video. In the end, we computed the average user ratings for each   \n575 method as our final results.   \n576 As illustrated in Tab. 1, UniEdit shows the best performance on frame consistency. Regarding textual   \n577 alignment, UniEdit significantly outperforms all other baselines, demonstrating its capacity to support   \n578 diverse editing scenarios.   \n579 Baselines. We implement all baseline methods with their official repositories. For MasaCtrl [5],   \n580 we adapt it to video editing by first setting the base model to a T2V model [63], then performing   \n581 MasaCtrl on all frames of the source video. Moreover, since most baselines use StableDiffusion (SD)   \n582 as the base model, we resize the source video to $512\\times512$ to align with the default configuration of   \n583 SD, then feed it into the denoising model, which can maximize the power of SD. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "584 B Additional Experimental Results and Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "585 B.1 Results on Different T2V Model ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "586 We additionally implement our method on VideoCrafter2 [9], a concurrent work on T2V generation   \n587 to demonstrate the flexibility of UniEdit. The results are shown in Fig. 8. ", "page_idx": 15}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/3a779e9ffd3ace3d27874aecc85b41767409d49e6183e3a71c55ef49fa24c940.jpg", "img_caption": ["Figure 8: Editing results with UniEdit on VideoCrafter2 [9]. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "588 B.2 Quantitative Ablation on Hyper-parameter Selection ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "589 In practice, we empirically found set these values to fixed values, i.e., $t_{0}=50,L=10$ (same as 590 MasaCtrl [5]) and $t_{1}=25$ can achieve satisfying results on most cases, and we further perform a quantitative study when applying different hyper-parameters in Tab. 3&4. ", "page_idx": 15}, {"type": "text", "text": "591 ", "page_idx": 15}, {"type": "table", "img_path": "RGnjY6l2HT/tmp/06704c19edc1db1d567f628fc77535faa2a6a7c0719416cb9ea75859782403da.jpg", "table_caption": ["Table 3: Quantitative comparison on hyper-parameter selection. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "RGnjY6l2HT/tmp/c98518526de7da321edf2e058053d095f682a1b78fe99caaf5439faf9ed923c2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "593 To investigate the impact of mask-guided coordination, we begin by visualizing masks obtained   \n594 from 1) the attention map in CA-S modules; 2) the off-the-shelf segmentation model SAM [38],   \n595 followed by presenting both qualitative and quantitative results of implementing UniEdit with or   \n596 without mask-guided coordination.   \n597 As verified by previous work [24], the attention maps in CA-S modules contain correspondence   \n598 information between text and visual features. The underlying intuition is that the attention maps   \n599 between each word and the spatial features at point $(i,j)$ indicate \u2018how similar this token is to   \n600 the spatial feature at this location\u2019. We visualize the text-image cross attention map alongside the   \n601 synthesized frame in Fig. 9. We observe spatial correspondences that align with the video output from   \n602 the attention map. For instance, areas with higher values of the token \u2018man\u2019 and \u2018NYC\u2019 correspond   \n603 to the foreground and background, respectively. We further employ a fixed threshold (0.4 in practice)   \n604 to derive binary segmentation maps from the attention maps. For comparison, we also display the   \n605 segmentation mask obtained by point prompt on SAM. It\u2019s observed that the cross-attention mask is   \n606 generally accurate and could serve as a reliable proxy in practice when an external segmentor is not   \n607 available.   \n608 We examine the impact of mask-guided coordination through both qualitative and quantitative results   \n609 across 4 settings: {w/o UniEdit, UniEdit w/o mask, UniEdit with mask from CA-S, UniEdit with   \n610 mask from SAM}. Qualitatively, shown in Fig. 10, the implementation of UniEdit significantly   \n611 enhances the consistency between the edited videos and the original video. The application of the   \n612 mask-guided coordination technique further improves the consistency of unedited areas (e.g., color   \n613 and texture). The quantitative results in Tab. 5 align coherently with this analysis.   \n615 Difference Between QK and V Features in SA-S Modules To comprehend why we can have   \n616 inhomogeneous QK and $\\mathrm{v}$ and their differences, we visualized the results of swapping different   \n617 features (QK or V) in SA-S modules during style transfer tasks on the source video in Fig. 11a. As   \n618 can be seen, compared to editing with no feature replacement (2nd row), replacing QK in the 3rd row   \n619 results in the edited video adopting the same spatial structure as the source video. Simultaneously,   \n620 replacing V eradicates the style information in the 4th row, meaning the texture details from the   \n621 source video are utilized to replace the style depicted by the target prompt. To summarize, the query   \n622 and key features (in SA-S modules) dictate the spatial structure of the generated video, while the   \n623 value features tend to influence the texture, including details such as color tones.   \n624 Influence of Spatial Structure Control in Motion Editing We explored the role of spatial control   \n625 in motion editing. The proposed method synthesizes videos with larger modifications when removing   \n626 the spatial control mechanism on both the motion-reference branch and the main editing branch. We   \n627 visualized the results in Fig. 11b. It can be observed that although the motion-reference branch can   \n628 still generate the target motion without the control of spatial structure, the layout deviates significantly,   \n629 for example, the raccoon assumes a different pose and location. We regard this as a suboptimal   \n630 solution because, compared to the results presented in the 3rd row, the results w/o spatial structure   \n631 control modifies the object position of the source video, leading to a decrease in consistency between   \n632 the edited result and the source video. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "RGnjY6l2HT/tmp/80a0c3bcd36e4e24cb6c97bc54765f6011bfb1318d29d991c527a59242838acf.jpg", "table_caption": ["Table 5: Ablation on the proposed mask-guided coordination. "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/783d9c4825ffbf748701a68f5b1cf37a6f9165922fb89eb1c75d58a69a7f2296.jpg", "img_caption": ["Figure 9: Visualization of attention maps and masks in mask-guided coordination (Sec. 4.3). The top row are attention maps corresponding to different tokens in CA-S modules, (a) is the final output frame, (b) and (c) are the foreground/background binary mask obtained by employing a threshold on the attention map of \u2018Man\u2019 token and point prompt segmentation with SAM, respectively. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/6eb665d11c7487377ea54709517874357569c6d0174b164d5b9e0ee13a600abe.jpg", "img_caption": ["Figure 10: Qualitative editing results across 4 settings: w/o UniEdit (2nd row), UniEdit w/o mask (3rd row), UniEdit with mask from CA-S (4th row), UniEdit with mask from SAM (5th row). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/cf6b6ff3ea4636304b0c7c025c785432e140e82ea4c2136bd3f978f362fe72f1.jpg", "img_caption": ["(a) Replacing different features in SA-S modules. (b) Motion editing w/ or w/o structure control. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 11: Ablation on the proposed feature injection techniques. (11a): comparison of appearance editing without feature replacement (2nd row), with QK replacement (3rd row), with V replacement (4nd row); (11b): comparison of motion editing with and without the designed spatial structure control mechanism. ", "page_idx": 18}, {"type": "text", "text": "634 We conduct a theoretical analysis of the additional cost of UniEdit and an empirical comparison with   \n635 baseline methods in terms of inference speed.   \n636 Theoretically, our method primarily involves feature replacement operations in attention modules,   \n637 achieved through forward hook registration and introducing minimal additional computation. There  \n638 fore, the main difference between synthesizing a video from random noise and editing a video   \n639 with UniEdit lies in the batch size of the denoising process (i.e., vanilla generation: batchsize $^{=1}$ ,   \n640 appearance editing: batchsize $^{=2}$ , motion editing: batchsize $_{-3}$ ), and this process could be further   \n641 accelerated through multi-GPU parallel processing techniques. Additionally, we utilize LaVie [63] as   \n642 the base T2V model in the paper, which takes approximately 45 seconds to synthesize a 16-frame   \n643 video. Our method can be even faster when adapted to more efficient base models.   \n644 Empirically, UniEdit demonstrates comparable speed with baseline methods. The comparison of   \n645 inference time on a single 16-frame source video clip with a resolution of 320x512 on 1 NVIDIA   \n646 A100 GPU is as follows: ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "RGnjY6l2HT/tmp/9dd83f4505fb67d6a591e4ed034810caf2e872bcc80864d299b6e011260b64d8.jpg", "table_caption": ["Table 6: Quantitative comparison on inference time of editing a single 16-frame video clip. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "647 B.6 Failure Cases Visualization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "648 We exhibit failure cases in Fig. 12. Fig. 12a showcase when editing multiple elements simultaneously,   \n649 and we observe a relatively large inconsistency with the source video. A naive solution is to perform   \n650 editing with UniEdit multiple times. Fig. 12b visualizes the results when editing video with complex   \n651 scenes, and the model sometimes could not understand the semantics in the target prompt, resulting   \n652 in incorrect editing. This may be caused by the base model\u2019s limited text understanding power,   \n653 as discussed in [30]. It could be alleviated by leveraging the reasoning power of MLLM [30], or   \n654 adapting approaches in complex scenario editing [40]. ", "page_idx": 19}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/8a18c07fd5326ea48dc638d81c5a1c81916ebc7e185557f7be55dd5379f2d2e2.jpg", "img_caption": ["Figure 12: Visualization of failure cases. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "655 B.7 More Comparison with State-of-the-Art Methods ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "656 Please refer to Fig. 13 and Fig. 14 for more comparison with the state-of-the-art methods. For a fair   \n657 comparison, we also migrated all baselines to LaVie [63], using the same base model as our method.   \n658 The results are presented in Fig. 15, and they are found to be inferior compared to those in Fig. 5   \n659 (based on Stable Diffusion). ", "page_idx": 19}, {"type": "text", "text": "660 B.8 More Results of UniEdit ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "661 More edited results of UniEdit are provided in Fig. 16-21. Examples of TI2V generation are provided   \n662 in Fig. 22. ", "page_idx": 19}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/f02bfdc4a950cd0dbd5d9ef91b76d6f6382954ab4789f5311b69f811c03c0b5c.jpg", "img_caption": ["Figure 13: More comparison with state-of-the-art methods. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/a221f528a9a2d86a540cc2bc6b4617296e594544102d191e81e38c6a52bd7a0f.jpg", "img_caption": ["Figure 14: More comparison with state-of-the-art methods. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/355a1c02ecb6cdc1d222ada2f15e372f6cc75d283a71b9181679a11a167ebd8f.jpg", "img_caption": ["Figure 15: More comparison with state-of-the-art methods. We adapt the baseline methods to the text-to-video model LaVie [63] and compare with our method (also based on LaVie). "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/027eff0da6177c4d2f388b089055f9add34d46b04d2244408020b51a2b9bf1a5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/c0082f40b4c7049f8a744399a3eca82a4c814455dc3c78d36a50379969b7ca89.jpg", "img_caption": ["Figure 16: More appearance editing results of UniEdit. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/35a702bd2d14a5970726999f3c7a823de8c2ab5f6b4b555cdf7236e2e2f770c1.jpg", "img_caption": ["Figure 17: More appearance editing results of UniEdit. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/318e450fb90b7a178511e67f5fa1ca236cb33fc082722e9d4e74d19e9eb307d6.jpg", "img_caption": ["Figure 18: More appearance editing results of UniEdit. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/53bab64e953f7c28e0b18778ead89b8872f70b09ca49f7080c501ed489c6ec97.jpg", "img_caption": ["Figure 19: More appearance editing results of UniEdit. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/b3d18e3be20ddd8eefd49f461ac03b47a63d2b772cfa777e11175a1b1c45430e.jpg", "img_caption": ["Figure 20: More motion editing results of UniEdit. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Source Video ", "page_idx": 27}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/7e9c4a2b2eb210f9e696c67cf71bc305759e9bfdc068ca088140cd274890bab9.jpg", "img_caption": ["Target Promp... jumping. . "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/481b6a3f74161238221c305e2af81e82e8f3cc0da3c05dbbf7d1d276ba886a1d.jpg", "img_caption": ["Target Prompt:... lying .. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/e7c3a6d68a0f66255cf6c7c27027cac6860e017f208c082ede03b1bbec20f198.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Source Video ", "page_idx": 27}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/15c4b1c09332bd1984dae3f2a20bd6ba92c4810cfeda3cec49f8f6756f082ae8.jpg", "img_caption": ["Target Prompt: .. standing ... "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/ef426731866a512a7f43ddba2ef016cfe95bec5c838f696361f4de9d5524915e.jpg", "img_caption": ["Target Prompt: .. running "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/50dbcfdc88b3e5e533a28cb2231e67654f96a3a6d173eef702216eaafef1e65e.jpg", "img_caption": ["Figure 21: More motion editing results of UniEdit. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/04f10fab0a650cbdc58023ba10c286277feca64ef361a45d69fc8cc18b58ee47.jpg", "img_caption": ["Video after I2V "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "RGnjY6l2HT/tmp/2c1e677ca889ff41ceb163e2551fa0be3ea7ecdc05e93e3f3c9ffa3030f168ca.jpg", "img_caption": ["Figure 22: Results of text-image-to-video synthesis in Sec. 4.4. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "663 C Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "664 UniEdit is a tuning-free approach and is intended for advancing AI/ML research on video editing.   \n665 We encourage users to use the model responsibly. We discourage users from using the codes to   \n666 generate intentionally deceptive or untrue content or for inauthentic activities. It is suggested to add   \n667 watermarks to prevent misuse. ", "page_idx": 28}, {"type": "text", "text": "668 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "3 Justification: In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained textto-video generator within an inversion-then-generation framework.Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art method. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discussed the potential limitations of the method in Sec. 6 and presented failed cases in Appendix B.6. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "720 3. Theory Assumptions and Proofs ", "page_idx": 29}, {"type": "text", "text": "721 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n722 a complete (and correct) proof?   \n723 Answer: [NA]   \n724 Justification: This paper aims to design a simple-and-effective video editing method named   \n725 UniEdit, without focusing on theoretical results.   \n726 Guidelines:   \n727 \u2022 The answer NA means that the paper does not include theoretical results.   \n728 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n729 referenced.   \n730 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n731 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n732 they appear in the supplemental material, the authors are encouraged to provide a short   \n733 proof sketch to provide intuition.   \n734 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n735 by formal proofs provided in appendix or supplemental material.   \n736 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n737 4. Experimental Result Reproducibility   \n738 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n739 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n740 of the paper (regardless of whether the code and data are provided or not)?   \n741 Answer: [Yes]   \n774423 Jpuasrtaifmiceatteiro sne:l eTchtiiso np,a cpoerm ppruotvaitidoens adl erteasiloeudr ciensf oirn mSaetico. n5  oann tdh eA pmpoednedlisx,  pAa rtao meentseurrs,e  hreyppreor--   \n744 ducibility.   \n745 Guidelines:   \n746 \u2022 The answer NA means that the paper does not include experiments.   \n747 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n748 well by the reviewers: Making the paper reproducible is important, regardless of   \n749 whether the code and data are provided or not.   \n750 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n751 to make their results reproducible or verifiable.   \n752 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n753 For example, if the contribution is a novel architecture, describing the architecture fully   \n754 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n755 be necessary to either make it possible for others to replicate the model with the same   \n756 dataset, or provide access to the model. In general. releasing code and data is often   \n757 one good way to accomplish this, but reproducibility can also be provided via detailed   \n758 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n759 of a large language model), releasing of a model checkpoint, or other means that are   \n760 appropriate to the research performed.   \n761 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n762 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n763 nature of the contribution. For example   \n764 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n765 to reproduce that algorithm.   \n766 (b) If the contribution is primarily a new model architecture, the paper should describe   \n767 the architecture clearly and fully.   \n768 (c) If the contribution is a new model (e.g., a large language model), then there should   \n769 either be a way to access this model for reproducing the results or a way to reproduce   \n770 the model (e.g., with an open-source dataset or instructions for how to construct   \n771 the dataset).   \n772 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n773 authors are welcome to describe the particular way they provide for reproducibility.   \n774 In the case of closed-source models, it may be that access to the model is limited in   \n775 some way (e.g., to registered users), but it should be possible for other researchers   \n776 to have some path to reproducing or verifying the results.   \n777 5. Open access to data and code   \n778 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n779 tions to faithfully reproduce the main experimental results, as described in supplemental   \n780 material?   \n781 Answer: [No]   \n782 Justification: Due to company policy reasons, we are currently unable to upload the code.   \n783 The code will be publicly available after the paper is published.   \n784 Guidelines:   \n785 \u2022 The answer NA means that paper does not include experiments requiring code.   \n786 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n787 public/guides/CodeSubmissionPolicy) for more details.   \n788 \u2022 While we encourage the release of code and data, we understand that this might not be   \n789 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n790 including code, unless this is central to the contribution (e.g., for a new open-source   \n791 benchmark).   \n792 \u2022 The instructions should contain the exact command and environment needed to run to   \n793 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n794 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n795 \u2022 The authors should provide instructions on data access and preparation, including how   \n796 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n797 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n798 proposed method and baselines. If only a subset of experiments are reproducible, they   \n799 should state which ones are omitted from the script and why.   \n800 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n801 versions (if applicable).   \n802 \u2022 Providing as much information as possible in supplemental material (appended to the   \n803 paper) is recommended, but including URLs to data and code is permitted.   \n804 6. Experimental Setting/Details   \n805 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n806 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n807 results?   \n808 Answer: [Yes]   \n809 Justification: This paper provides detailed information on the models, parameters, hyper  \n810 parameter selection, computational resources in Sec. 5 and Appendix A to ensure repro  \n811 ducibility.   \n812 Guidelines:   \n813 \u2022 The answer NA means that the paper does not include experiments.   \n814 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n815 that is necessary to appreciate the results and make sense of them.   \n816 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n817 material.   \n818 7. Experiment Statistical Significance   \n819 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n820 information about the statistical significance of the experiments?   \n821 Answer: [No]   \n822 Justification: The common practice in video editing does not including error bars, and we   \n823 follow the previous papers.   \n824 Guidelines:   \n825 \u2022 The answer NA means that the paper does not include experiments.   \n826 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n827 dence intervals, or statistical significance tests, at least for the experiments that support   \n828 the main claims of the paper.   \n829 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n830 example, train/test split, initialization, random drawing of some parameter, or overall   \n831 run with given experimental conditions).   \n832 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n833 call to a library function, bootstrap, etc.)   \n834 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n835 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n836 of the mean.   \n837 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n838 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n839 of Normality of errors is not verified.   \n840 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n841 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n842 error rates).   \n843 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n844 they were calculated and reference the corresponding figures or tables in the text.   \n845 8. Experiments Compute Resources   \n846 Question: For each experiment, does the paper provide sufficient information on the com  \n847 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n848 the experiments?   \n849 Answer: [Yes]   \n850 Justification: This paper provides detailed information on the computational resources in   \n851 Sec. 5 and Appendix A and inference time comparison in Tab. 6.   \n852 Guidelines:   \n853 \u2022 The answer NA means that the paper does not include experiments.   \n854 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n855 or cloud provider, including relevant memory and storage.   \n856 \u2022 The paper should provide the amount of compute required for each of the individual   \n857 experimental runs as well as estimate the total compute.   \n858 \u2022 The paper should disclose whether the full research project required more compute   \n859 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n860 didn\u2019t make it into the paper).   \n861 9. Code Of Ethics   \n862 Question: Does the research conducted in the paper conform, in every respect, with the   \n863 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n864 Answer: [Yes]   \n865 Justification: The research strictly adheres to the NeurIPS Code of Ethics in every respect.   \n866 Guidelines:   \n867 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n868 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n869 deviation from the Code of Ethics.   \n870 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n871 eration due to laws or regulations in their jurisdiction).   \n872 10. Broader Impacts   \n873 Question: Does the paper discuss both potential positive societal impacts and negative   \n874 societal impacts of the work performed?   \n875 Answer: [Yes]   \nJustification: The broader impacts are discussed in Appendix C   \n877 Guidelines:   \n878 \u2022 The answer NA means that there is no societal impact of the work performed.   \n879 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n880 impact or why the paper does not address societal impact.   \n881 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n882 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n883 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n884 groups), privacy considerations, and security considerations.   \n885 \u2022 The conference expects that many papers will be foundational research and not tied   \n886 to particular applications, let alone deployments. However, if there is a direct path to   \n887 any negative applications, the authors should point it out. For example, it is legitimate   \n888 to point out that an improvement in the quality of generative models could be used to   \n889 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n890 that a generic algorithm for optimizing neural networks could enable people to train   \n891 models that generate Deepfakes faster.   \n892 \u2022 The authors should consider possible harms that could arise when the technology is   \n893 being used as intended and functioning correctly, harms that could arise when the   \n894 technology is being used as intended but gives incorrect results, and harms following   \n895 from (intentional or unintentional) misuse of the technology.   \n896 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n897 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n898 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n899 feedback over time, improving the efficiency and accessibility of ML).   \n900 11. Safeguards   \n901 Question: Does the paper describe safeguards that have been put in place for responsible   \n902 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n903 image generators, or scraped datasets)?   \n904 Answer: [NA]   \n905 Justification: This paper poses no such risks.   \n906 Guidelines:   \n907 \u2022 The answer NA means that the paper poses no such risks.   \n908 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n909 necessary safeguards to allow for controlled use of the model, for example by requiring   \n910 that users adhere to usage guidelines or restrictions to access the model or implementing   \n911 safety filters.   \n912 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n913 should describe how they avoided releasing unsafe images.   \n914 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n915 not require this, but we encourage authors to take this into account and make a best   \n916 faith effort.   \n917 12. Licenses for existing assets   \n918 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n919 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n920 properly respected?   \n921 Answer: [Yes]   \n922 Justification: Yes, the creators or original owners of assets used in the paper are properly   \n923 credited, and the license and terms of use are explicitly mentioned and properly respected.   \n924 Guidelines:   \n925 \u2022 The answer NA means that the paper does not use existing assets.   \n926 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n927 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n928 URL.   \n929 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n930 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n931 service of that source should be provided.   \n932 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n933 package should be provided. For popular datasets, paperswithcode.com/datasets   \n934 has curated licenses for some datasets. Their licensing guide can help determine the   \n935 license of a dataset.   \n936 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n937 the derived asset (if it has changed) should be provided.   \n938 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n939 the asset\u2019s creators.   \n940 13. New Assets   \n941 Question: Are new assets introduced in the paper well documented and is the documentation   \n942 provided alongside the assets?   \n943 Answer: [Yes]   \n944 Justification: We have uploaded the code of this paper to an anonymous repository and   \n945 provided the corresponding link in Appendix. The code will be made publicly available   \n946 after the paper is published.   \n947 Guidelines:   \n948 \u2022 The answer NA means that the paper does not release new assets.   \n949 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n950 submissions via structured templates. This includes details about training, license,   \n951 limitations, etc.   \n952 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n953 asset is used.   \n954 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n955 create an anonymized URL or include an anonymized zip file.   \n956 14. Crowdsourcing and Research with Human Subjects   \n957 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n958 include the full text of instructions given to participants and screenshots, if applicable, as   \n959 well as details about compensation (if any)?   \n960 Answer: [NA]   \n961 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n962 Guidelines:   \n963 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n964 human subjects.   \n965 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n966 tion of the paper involves human subjects, then as much detail as possible should be   \n967 included in the main paper.   \n968 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n969 or other labor should be paid at least the minimum wage in the country of the data   \n970 collector.   \n971 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n972 Subjects   \n973 Question: Does the paper describe potential risks incurred by study participants, whether   \n974 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n975 approvals (or an equivalent approval/review based on the requirements of your country or   \n976 institution) were obtained?   \n977 Answer: [NA]   \n978 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n979 Guidelines:   \n980 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n981 human subjects. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]