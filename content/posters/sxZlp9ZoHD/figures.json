[{"figure_path": "sxZlp9ZoHD/figures/figures_2_1.jpg", "caption": "Figure 1: RetNet has three equivalent computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent representations. Given the same input, three paradigms obtain the same output. \u201cGN\u201d is short for GroupNorm.", "description": "This figure illustrates the three computation paradigms of the Retentive Network (RetNet): parallel, recurrent, and chunkwise recurrent.  It demonstrates that despite using different computational approaches, all three paradigms produce the same output given the same input. The parallel representation is suitable for training, the recurrent representation is for efficient inference (O(1) complexity), and the chunkwise recurrent representation balances both for long sequences.  The diagram shows the flow of data through the layers, highlighting the key components of each paradigm, including the GroupNorm (GN) layer.", "section": "2 Retentive Network"}, {"figure_path": "sxZlp9ZoHD/figures/figures_2_2.jpg", "caption": "Figure 1: RetNet has three equivalent computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent representations. Given the same input, three paradigms obtain the same output. \u201cGN\u201d is short for GroupNorm.", "description": "This figure shows the three equivalent computation paradigms of the Retentive Network (RetNet): parallel, recurrent, and chunkwise recurrent.  It illustrates how the same input (Xn) leads to the same output (On) regardless of which paradigm is used. The parallel representation shows the parallel computation of Q, K, and V, which are then combined with a decay mask and passed through a GroupNorm (GN) layer. The recurrent representation shows the same calculation but shows the recurrent relationship and the use of the previous state (Sn-1). Finally, the chunkwise recurrent representation illustrates a hybrid approach where chunks are processed in parallel while also incorporating information from previous chunks recurrently. This illustrates RetNet's versatility in handling sequence modeling efficiently in different scenarios (training parallelism, inference efficiency).", "section": "2 Retentive Network"}, {"figure_path": "sxZlp9ZoHD/figures/figures_5_1.jpg", "caption": "Figure 3: Validation perplexity (PPL) decreases along with scaling up the model size.", "description": "This figure shows the validation perplexity for language models of different sizes (1.3B, 2.7B, and 6.7B parameters) trained using both the Transformer and RetNet architectures.  The graph demonstrates how perplexity decreases (indicating improved performance) as the model size increases.  It visually compares the scaling behavior of the two architectures, illustrating their relative performance across different model scales.", "section": "3.2 Language Modeling Evaluation with Various Model Sizes"}, {"figure_path": "sxZlp9ZoHD/figures/figures_5_2.jpg", "caption": "Figure 4: Answer perplexity decreases along with longer input documents. Transformer and RetNet+ obtain comparable performance for long-context modeling on the ZeroSCROLLS [41] benchmark.", "description": "This figure compares the performance of Transformer and RetNet+ models on the ZeroSCROLLS benchmark for long-context modeling.  The x-axis represents the context length (length of input document), and the y-axis represents the answer perplexity.  The results show that both models achieve lower perplexity (better performance) as the context length increases, indicating their ability to effectively utilize longer input sequences. RetNet+ shows comparable performance to Transformer across different datasets (GovReport, Qasper, NarrativeQA).", "section": "3.3 Long-Context Evaluation"}, {"figure_path": "sxZlp9ZoHD/figures/figures_6_1.jpg", "caption": "Figure 5: Inference cost of Transformer and RetNet with a model size of 6.7B. RetNet outperforms Transformers in terms of memory consumption, throughput, and latency.", "description": "This figure compares the inference cost (GPU memory, throughput, and latency) of the Transformer and RetNet models with a size of 6.7B.  The leftmost graph shows that RetNet's memory consumption remains constant regardless of the sequence length, unlike the Transformer which increases linearly due to KV caches. The center graph illustrates that RetNet achieves higher and length-invariant throughput during decoding. The rightmost graph demonstrates that RetNet's decoding latency is significantly lower and less sensitive to batch size compared to the Transformer.", "section": "3.4 Inference Cost"}, {"figure_path": "sxZlp9ZoHD/figures/figures_6_2.jpg", "caption": "Figure 6: Training throughput (word per second; wps) of Transformer with FlashAttention-2 [10] and RetNet.", "description": "The figure compares the training throughput of Transformer and RetNet models with varying sequence lengths, using different implementations (FlashAttention-2 for Transformer, Triton and PyTorch for RetNet).  RetNet demonstrates higher training throughput, particularly at longer sequence lengths.", "section": "3.5 Training Throughput"}]