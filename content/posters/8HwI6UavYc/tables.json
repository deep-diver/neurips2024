[{"figure_path": "8HwI6UavYc/tables/tables_8_1.jpg", "caption": "Table 1: We compute a CLIP-based alignment metric, and optical flow-based temporal consistency metric for various datasets and prompts. RAM3D shows the best overall edit prompt alignment and temporal consistency. (Top) GARDEN, (Middle) FACE, (Bottom) FERN.", "description": "This table presents a quantitative comparison of RAM3D against two other methods (GaussianEditor and BlendedNeRF) using two metrics: CLIP Text-Image Direction Similarity (higher is better, measuring how well the generated object matches the text prompt) and warping error (lower is better, measuring the temporal consistency of the generated object across different views).  The results are broken down by prompt and dataset (GARDEN, FACE, and FERN).  The table shows that RAM3D generally outperforms the other methods on both metrics.", "section": "4.2 Quantitative Results"}, {"figure_path": "8HwI6UavYc/tables/tables_14_1.jpg", "caption": "Table 1: We compute a CLIP-based alignment metric, and optical flow-based temporal consistency metric for various datasets and prompts. RAM3D shows the best overall edit prompt alignment and temporal consistency. (Top) GARDEN, (Middle) FACE, (Bottom) FERN.", "description": "This table presents a quantitative comparison of the proposed ReplaceAnything3D (RAM3D) model against existing methods using two metrics: CLIP Text-Image Direction Similarity and Warping Error.  The CLIP-based metric assesses the alignment of the edited objects with the input text prompts. The optical flow-based metric (Warping Error) measures the temporal consistency across multiple views of the modified scene. The results show RAM3D achieving superior performance across both metrics and various scene types (GARDEN, FACE, and FERN).", "section": "4.2 Quantitative Results"}, {"figure_path": "8HwI6UavYc/tables/tables_14_2.jpg", "caption": "Table 1: We compute a CLIP-based alignment metric, and optical flow-based temporal consistency metric for various datasets and prompts. RAM3D shows the best overall edit prompt alignment and temporal consistency. (Top) GARDEN, (Middle) FACE, (Bottom) FERN.", "description": "This table presents a quantitative comparison of RAM3D against two other methods using two metrics: CLIP Text-Image Direction Similarity (measuring how well the generated object matches the text prompt) and Warping Error (measuring the temporal consistency of the generated video).  The results are shown for three different datasets (GARDEN, FACE, and FERN), each with several different prompts. RAM3D consistently outperforms the other methods in terms of both metrics, indicating superior alignment and temporal coherence.", "section": "4.2 Quantitative Results"}, {"figure_path": "8HwI6UavYc/tables/tables_15_1.jpg", "caption": "Table 4: Erase-stage ablation results. We report CLIP Text-Image Direction Similarity scores for all model variants, using the prompt \u201cA white plinth in a park, in front of a path\u201d, on the STATUE scene. Note that our full model performs best.", "description": "This table presents the results of an ablation study on the Erase stage of the RAM3D model.  It compares the performance of three variants: one without halo supervision, one without depth loss, and the full model. The CLIP Text-Image Direction Similarity score, which measures how well the generated image matches the text prompt, is used as the evaluation metric. The results show that the full model achieves significantly higher scores, indicating the importance of both halo supervision and depth loss for effective background inpainting.", "section": "4.3 Ablation studies"}, {"figure_path": "8HwI6UavYc/tables/tables_15_2.jpg", "caption": "Table 5: Replace-stage ablation results. We report CLIP Text-Image Direction Similarity scores for all model variants, using the prompt \u201cA corgi on a white plinth\u201d, on the STATUE scene. Note that our full model performs best.", "description": "This table presents ablation study results for the Replace stage of the RAM3D model.  It compares the performance of different model variants, each lacking a specific component (SDS loss, single-stage training, or background augmentation). The \"CLIP Sim\" metric, which measures the alignment between the generated object and the text prompt, demonstrates that the full model, incorporating all components, yields the highest alignment score (0.232).", "section": "4.2 Quantitative Results"}, {"figure_path": "8HwI6UavYc/tables/tables_17_1.jpg", "caption": "Table 6: CLIP-based metrics for GARDEN and FACE datasets, comparing our method with [3]", "description": "This table presents a quantitative comparison of the proposed method (RAM3D) and InstructNeRF2NeRF [3] using two CLIP-based metrics: CLIP Text-Image Direction Similarity and CLIP Direction Consistency.  The results are shown for five different prompts on the GARDEN and FACE datasets. Higher values in both metrics indicate better performance.  The comparison highlights RAM3D's improved performance, particularly in terms of text-image alignment.", "section": "Additional quantitative comparisons"}]