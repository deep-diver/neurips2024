[{"type": "text", "text": "Generative Modeling of Molecular Dynamics Trajectories ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bowen Jing\u22171 Hannes St\u00e4rk\u22171 Tommi Jaakkola1 Bonnie Berger1 2 ", "page_idx": 0}, {"type": "text", "text": "1CSAIL, Massachusetts Institute of Technology 2Dept. of Mathematics, Massachusetts Institute of Technology {bjing, hstark}@mit.edu, tommi@csail.mit.edu, bab@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show preliminary results on scaling to protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Numerical integration of Newton\u2019s equations of motion at atomic scales, known as molecular dynamics (MD), is a widely-used technique for studying diverse molecular phenomena in chemistry, biology, and other molecular sciences (Alder and Wainwright, 1959; Rahman, 1964; Verlet, 1967; McCammon et al., 1977). While general and versatile, MD is computationally demanding due to the large separation in timescales between integration steps and relevant molecular phenomena. Thus, a vast body of literature spanning several decades aims to accelerate or enhance the sampling efficiency of MD simulation algorithms (Ryckaert et al., 1977; Darden et al., 1993; Sugita and Okamoto, 1999; Laio and Parrinello, 2002; Anderson et al., 2008; Shaw et al., 2009). More recently, learning surrogate models of MD has become an active area of research in deep generative modeling (No\u00e9 et al., 2019; Zheng et al., 2023; Klein et al., 2024; Schreiner et al., 2024; Jing et al., 2024). However, existing training paradigms fail to fully leverage the rich dynamical information in MD training data, restricting their applicability to a limited set of downstream problems. ", "page_idx": 0}, {"type": "text", "text": "In this work, we propose MDGEN, a novel paradigm for fast, general-purpose surrogate modeling of MD based on direct generative modeling of simulated trajectories. Different from previous works, which learn the autoregressive transition density or equilibrium distribution of MD, we formulate end-to-end generative modeling of full trajectories viewed as time-series of 3D molecular structures. Akin to how image generative models were extended to videos (Ho et al., 2022), our framing of the problem augments single-structure generative models with an additional time dimension, opening the door to a larger set of forward and inverse problems to which our model can be applied. When provided (and conditioned on) the initial \u201cframe\" of a given system, such generative models serve as familiar surrogate forward simulators of the reference dynamics. However, by providing other kinds of conditioning, these \u201cmolecular video\" generative models also enable highly flexible applications to a variety of inverse problems not possible with existing surrogate models. In sum, we formulate and showcase the following novel capabilities of MDGEN: ", "page_idx": 0}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/7e6f02c24b25e21c338622103d5c089be6c99d3484a3949583ffb1b0e488d823.jpg", "img_caption": ["Figure 1: (Left) Tasks: generative modeling of MD trajectories addresses several tasks by conditioning on different parts of a trajectory. (Right) Method: We tokenize trajectories of $T$ frames and $L$ residues into an $(T\\times L)$ -array of SE(3)-invariant tokens encoding roto-translation offsets from key frames and torsion angles. Using stochastic interpolants, we generate arrays of such tokens from Gaussian noise. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "\u2022 Forward simulation\u2014given the initial frame of a trajectory, we sample a potential time evolution of the molecular system.   \n\u2022 Interpolation\u2014given the frames at the two endpoints of a trajectory, we sample a plausible path connecting the two. In chemistry, this is known as transition path sampling and is important for studying reactions and conformational transitions.   \n\u2022 Upsampling\u2014given a trajectory with timestep $\\Delta t$ between frames, we upsample the \u201cframerate\" by a factor of $M$ to obtain a trajectory with timestep $\\Delta t/M$ . This infers fast motions from trajectories saved at less frequent intervals.   \n\u2022 Inpainting\u2014given part of a molecule and its trajectory, we generate the rest of the molecule (and its time evolution) to be consistent with the known part of the trajectory. This ability could be applied to design molecules to scaffold desired dynamics. ", "page_idx": 1}, {"type": "text", "text": "These tasks are conceptually illustrated in Figure 1. While the forward simulation task aligns with the typical modeling paradigm of approximating the data-generating process, the others represent novel capabilities on scientifically important inverse problems not straightforward to address even with MD itself. As such, our framework presents a new perspective on how to unlock value from MD simulation with machine learning towards diverse downstream objectives. We highlight further exciting possibilities opened up by our framework in Section 5. ", "page_idx": 1}, {"type": "text", "text": "We demonstrate our framework on MD simulations of tetrapeptides (i.e., length-4 peptides), with preliminary extensions to full-sized protein monomers. To do so, we parameterize molecular trajectories in terms of sidechain torsions and residue offsets with respect to conditioning key frames, obtaining a generative modeling task over a 2D array of $S E(3)$ -invariant tokens rather than residue frames or point clouds. In this parameterization, we can then employ a Scalable Interpolant Transformer (SiT) (Ma et al., 2024) as our flow-based generative backbone, avoiding the more restrictive geometric architectures commonly used for molecular structure. Furthermore, by replacing the time-wise attention in SiT with the long-context architecture Hyena (Poli et al., 2023), we provide proof-of-concept of scaling up to trajectories of $100\\mathbf{k}$ frames, enabling a wide range of timescales and dynamical processes to be captured with a single model generation. ", "page_idx": 1}, {"type": "text", "text": "We evaluate MDGEN on the forward simulation, interpolation, upsampling, and inpainting tasks on tetrapeptides in a transferable setting (i.e., unseen test peptides). Our model accurately reproduces free energy surfaces and dynamical content such as torsional relaxation and Markov state fluxes, provides realistic transition paths between arbitrary pairs of metastable states, and recovers fast dynamical phenomena below the sampling threshold of coarse-timestep trajectories. In preliminary steps toward dynamics-scaffolded design, we show that molecular inpainting with MDGEN obtains much higher sequence recovery than inverse folding methods based on one or two static frames. Finally, we evaluate MDGEN on simulation of proteins and find that it outperforms MSA subsampling with AlphaFold (Del Alamo et al., 2022) in terms of recovering ensemble statistical properties. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Molecular dynamics. At a high level, the aim of molecular dynamics is the integrate the equations of motion $M_{i}\\ddot{\\mathbf{x}}_{i}=-\\nabla_{\\mathbf{x}_{i}}U(\\mathbf{x}_{1}\\ldots\\mathbf{x}_{N})$ for each particle $i$ in a molecular configuration $({\\bf x}_{1}\\ldots{\\bf x}_{N})\\in$ $\\mathbb{R}^{3N}$ , where $M_{i}$ is the mass and $U$ is the potential energy function (or force field) $U:\\mathbb{R}^{3N}\\xrightarrow{}\\mathbb{R}$ However, these equations of motion are often modified to include a thermostat in order to model contact with surroundings at a given temperature. For example, the widely-used Langevin thermostat transforms the equations of motion into a stochastic diffusion process: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\mathbf{x}_{i}=\\mathbf{p}_{i}/M_{i}\\,d t,\\quad d\\mathbf{p}_{i}=-\\nabla_{\\mathbf{x}_{i}}U\\,d t-\\gamma\\mathbf{p}_{i}\\,d t+{\\sqrt{2M_{i}\\gamma k T}}\\,d\\mathbf{w}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{p}_{i}$ are the momenta. By design, this process converges to the Boltzmann distribution of the system $p({\\bf x}_{1}\\ldots{\\bf x}_{N})\\propto e^{-U/\\dot{k}T}$ . To incorporate interactions with solvent molecules\u2014ubiquitous in biochemistry\u2014one includes a box of surrounding solvent molecules as part of the molecular system (explicit solvent) or modifies the force field $U$ to model their effects (implicit solvent). In either case, only the positions $\\mathbf{x}_{i}$ of non-solvent atoms are of interest, and their time evolution constitutes (for our purposes) the MD trajectory. ", "page_idx": 2}, {"type": "text", "text": "Deep learning for MD. An emerging body of work seeks to approximate the distributions over configurations $\\mathbf{X}=(\\mathbf{x}_{1}\\ldots\\mathbf{x}_{N})$ arising from Equation 1 with deep generative models. Fu et al. (2023), Timewarp (Klein et al., 2024), and ITO (Schreiner et al., 2024) learn the transition density $p(\\mathbf{X}_{t+\\Delta t}\\mid\\mathbf{X}_{t})$ and emulate MD trajectories via simulation rollouts of the learned model. On the other hand, Boltzmann generators (No\u00e9 et al., 2019; K\u00f6hler et al., 2021; Garcia Satorras et al., 2021; Midgley et al., 2022, 2024) directly approximate the stationary Boltzmann distribution, forgoing any explicit modeling of dynamics. In particular, Boltzmann-targeting diffusion models trained with frames from MD trajectories have demonstrated promising scalability and generalization to protein systems (Zheng et al., 2023; Jing et al., 2024). However, these works have focused exclusively on forward simulation and have not explored joint modeling of entire trajectories $(\\mathbf{X}_{t}\\ldots...\\mathbf{X}_{t+N\\Delta t})$ or the inverse problems accessible under such a formulation. ", "page_idx": 2}, {"type": "text", "text": "Stochastic interpolants. We build our MD trajectory generative model under the stochastic interpolants framework: Given a continuous distribution $p_{1}\\equiv p_{\\mathrm{data}}$ over $\\mathbb{R}^{n}$ , stochastic interpolants (Albergo and Vanden-Eijnden, 2022; Albergo et al., 2023; Lipman et al., 2022; Liu et al., 2022), provide a method for learning continuous flow-based models $d\\mathbf{x}=v_{\\theta}(\\mathbf{x},t)$ dt transporting a prior distribution $p_{0}$ (e.g., $p_{0}\\equiv\\mathcal{N}(0,\\mathbf{I}))$ to the data $p_{1}$ . To do so, one defines intermediate distributions $\\mathbf{x}_{t}\\sim p_{t},t\\in(0,1)$ via $\\mathbf{x}_{t}=\\alpha_{t}\\mathbf{x}_{1}+\\sigma_{t}\\mathbf{x}_{0}$ where $\\mathbf{x}_{0}\\sim p_{0}$ and $\\mathbf{x}_{1}\\sim p_{1}$ and the interpolation path satisfies $\\alpha_{0}=\\sigma_{1}=0$ and $\\alpha_{1}=\\sigma_{0}=1$ . A neural network $v_{\\theta}:\\mathbb{R}^{n}\\times[0,1]\\to\\mathbb{R}^{n}$ is trained to approximate the time-evolving flow field ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{\\theta}(\\mathbf{x}_{t},t)\\approx v(\\mathbf{x}_{t},t)\\equiv\\mathbb{E}_{\\mathbf{x}_{0},\\mathbf{x}_{1}|\\mathbf{x}_{t}}[\\dot{\\alpha}_{t}\\mathbf{x}_{1}+\\dot{\\sigma}_{t}\\mathbf{x}_{0}]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which satisfies the transport equation $\\partial p_{t}/\\partial t+\\nabla\\cdot(p_{t}{v_{t}})=0$ . Hence, at convergence, noisy samples $\\mathbf{x}_{0}\\sim\\,p_{0}$ can be evolved under $v_{\\theta}$ to obtain data samples $\\mathbf{x}_{1}\\sim\\,p_{1}$ . When parameterized with transformers (Vaswani et al., 2017), stochastic interpolants are state-of-the-art in image generation (Esser et al., 2024). In particular, we adopt the notation, architecture, and training framework of Scalable Interpolant Transformer (SiT) (Ma et al., 2024), to which we refer for further exposition. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Tokenizing Peptide Trajectories ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a chemical specification of a molecular system with $N$ atoms, our aim is to learn a generative model over time-series $\\boldsymbol{\\chi}\\equiv[\\mathbf{X}_{1},\\dots\\mathbf{X}_{T}]$ of corresponding molecular structures $\\mathbf{X}_{i}\\in\\mathbb{R}^{3N}$ for some trajectory length $T$ . In this work, we specialize to MD trajectories of short peptides (Sections $4.1-$ 4.4) or single-chain proteins (4.4). Thus, our chemical specifications are amino acid sequences ", "page_idx": 2}, {"type": "text", "text": "$A=\\{1\\ldots20\\}^{L}$ , and we adopt an $S E(3)$ -based parameterization of peptide structures (Jumper et al., 2021; Yim et al., 2023). In this parameterization, the all-atom coordinates of each amino acid residue are implicitly described by a roto-translation (i.e., element of $S E(3)$ ) corresponding to the rigid body motion of the residue, and seven torsion angles describing its conformation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\chi_{t}^{l}=[g,\\tau_{1},\\dots\\tau_{7}],\\quad g\\in S E(3),\\tau\\in\\ensuremath{\\mathbb{T}},\\quad\\boldsymbol{\\chi}\\in\\left(\\left[S E(3)\\times\\ensuremath{\\mathbb{T}}^{7}\\right]^{L}\\right)^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Throughout, subscripts indicate time and superscripts residue indices. The undefined torsion angles can be randomized and are unsupervised for residues with fewer than seven torsion angles. ", "page_idx": 3}, {"type": "text", "text": "Traditionally, equivariant architectures have been required for geometry-aware processing of polypeptide structures. However, to learn a scalable generative model over this space of roto-translations and torsion angles, we seek to represent each $\\chi_{t}^{l}$ in terms of an $S E(3)$ -invariant feature vector\u2014a token suitable for processing by vanilla transformers. To obtain such a vector, we leverage the fact that we are concerned with conditional trajectory generation\u2014meaning that there always exists at least one frame in the trajectory with un-noised roto-translations, which we do not need to generate and can reference in the modeling process. Inspired by analogy to video compression, we refer to such frames as key frames. We can then obtain $S E(3)$ -invariant tokens by parameterizing the roto-translations of remaining structures relative to the key frames. ", "page_idx": 3}, {"type": "text", "text": "In more detail, given $K$ key frames at times $t_{1}\\ldots t_{K}$ we tokenize residue $j$ in frame $t$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\chi_{t}^{j}=\\left[\\phi\\left([g_{t_{1}}^{j}]^{-1}g_{t}^{j}\\right),\\dots,\\phi\\left([g_{t_{K}}^{j}]^{-1}g_{t}^{j}\\right),\\psi([\\tau_{t}^{j}]_{1}),\\dots\\psi([\\tau_{t}^{j}]_{7})\\right]\\subset\\mathbb{R}^{7K+14}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g_{t}^{j}\\in S E(3)$ represents the roto-translation and $[\\tau_{t}^{j}]_{i}$ the torsion angles of residue $j$ at frame $t$ . Here, $\\phi:S E(3)\\ {\\overset{\\cdot}{\\to}}\\ \\mathbb{R}^{7}$ parameterizes an element of $\\stackrel{\\cdot}{S}\\cal E(3)$ in terms of a unit quaternion and translation vector, and $\\psi:\\mathbb{T}\\overset{\\cdot}{\\to}\\mathbb{R}^{2}$ converts torsion angles to points on the unit circle. We thus obtain a $(7K+14)$ -dimensional array for each residue in every frame. Because the relative roto-translations and torsion angles are both $S E(3)$ -invariant, in this manner we can represent a polypeptide molecular trajectory as an $(T\\times L)$ -array of $S E(3)$ -invariant tokens. ", "page_idx": 3}, {"type": "text", "text": "To untokenize a generated trajectory of tokens to all-atom coordinates $\\mathbf{X}_{t}\\in\\mathbb{R}^{3N}$ , we first convert each predicted quaternion and translation vector to a relative roto-translation and apply it to the key frame(s), obtaining absolute roto-translations. We then read off the torsion angles from the unit circle and assemble the all-atom coordinates as implemented in Jumper et al. (2021), averaging the reconstructions from different key frames if needed. ", "page_idx": 3}, {"type": "text", "text": "3.2 Flow Model Architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our base modeling task is to generate a distribution over $\\mathbb{R}^{T\\times L\\times(7K+14)}$ conditioned on rototranslations of one or more key frames $g_{t_{1}}\\cdot\\cdot\\cdot g_{t_{K}}$ , and (in most settings) amino acid identities $A$ . To do so, we learn a flow-based model via the stochastic interpolant framework described in SiT (Ma et al., 2024) and parameterize a velocity network $v_{\\theta}(\\cdot\\ |\\ g_{t_{1}}\\cdot\\cdot\\cdot g_{t_{K}},A):\\mathbb{R}^{T\\times L\\times(7K+14)}\\times[0,1]\\rightarrow$ RT \u00d7L\u00d7(7K+14). To condition on the key frames and amino acids, we first provide the sequence embedding to several IPA layers (Jumper et al., 2021) that embed the key frame roto-translations; these conditioning representations (which are $S E(3)$ -invariant) are broadcast across the time axis and added to the input embeddings. The main trunk of the network consists of alternating attention blocks across the residue index and across time, with the construction of each block closely resembling DiT (Peebles and Xie, 2023). Sidechain torsions and roto-translation offsets, when available, are directly provided to the model as conditioning tokens. Further details are provided in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "In the molecular inpainting setting where we also generate the amino acid identities, we additionally require a generative framework over these discrete variables. While several formulations of discrete diffusion or flow-matching are available (Hoogeboom et al., 2021; Austin et al., 2021; Campbell et al., 2022, 2024), we select Dirichlet flow matching (Stark et al., 2024) as it is most compatible with the continuous-space, continuous-time stochastic interpolant framework used for the positions. Specifically, we place the amino acid identities on the 20-dimensional probability simplex (one per amino acid), augment the token representations with these variables, and regress against a $T\\times L\\times(7K+14\\bar{+}20)$ -dimensional vector field. Further details are provided in Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "3.3 Conditional Generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We present the precise specifications of the various conditional generation settings in Table 1. Depending on the task, we choose the key frames to be the first frame $g_{1}$ or the first and last frames $g_{1},g_{T}$ . Each conditional generation task is further characterized by providing the ground-truth tokens of known frames or residues as additional inputs to the velocity network. Meanwhile, mask tokens are provided for the unknown frames and residues that the model generates. For example, in the upsampling setting, we provide ground-truth tokens every $M$ frames, while mask tokens are provided for all other frames. We note that in the inpainting setting, the model accesses the roto-translations $g$ of designed residues at the trajectory endpoints via the key frames, constituting a slight departure from the full inpainting setting. However, these residues are not observed for intermediate frames, and their identities are never provided to the model. ", "page_idx": 4}, {"type": "table", "img_path": "yRRCH1OsGW/tmp/edacc84a759261bf4b127c59e42001b38f7669616120cef99e8fcfddd451dfd3.jpg", "table_caption": ["Table 1: Conditional generation settings. $g$ : roto-translations, $\\tau$ : torsions, $A$ : residue identities $M$ : upsampling factor. Superscripts indicate residue index and subscripts indicate frame (time) index. For inpainting, we find that excluding identities and torsions reduces overfitting. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We evaluate MDGEN on its ability to learn from MD simulations of training molecules and then sample trajectories for unseen molecules. We focus on tetrapeptides as our main molecule class for evaluation as they provide nontrivial chemical diversity while remaining small enough to tractably simulate to equilibrium (Klein et al., 2024). Sections 4.1\u20134.3 thoroughly evaluate our model on forward simulations, interpolation / transition path sampling, and trajectory upsampling on test peptides. Section 4.4 provides proof-of-concept and preliminary exploration of additional tasks\u2014 namely, inpainting for dynamics-conditioned design, long trajectories with Hyena (Poli et al., 2023), and scaling to simulations of protein monomers. Separate models are trained for each setting. ", "page_idx": 4}, {"type": "text", "text": "To obtain tetrapeptide MD trajectories for training and evaluation, we run implicit- and explicitsolvent, all-atom simulations of ${\\approx}3000$ training, 100 validation, and 100 test tetrapeptides for $100\\;\\mathrm{ns}$ . For proteins, we use explicit-solvent, all-atom simulations from the ATLAS dataset (Vander Meersche et al., 2024), which provides three 100 ns trajectories for each of 1390 structurally diverse proteins. Unless otherwise specified, models are trained with trajectory timesteps of $\\Delta t\\;=\\;10\\:\\mathrm{ps}$ . Our default baselines consist of replicate MD simulations ranging from 10 ps to 100 ns, with additional comparisons in each section as appropriate. ", "page_idx": 4}, {"type": "text", "text": "Our experiments make extensive use of Markov State Models (MSMs), a widely used coarse-grained representation of molecular dynamics (Prinz et al., 2011; No\u00e9 et al., 2013). We obtain an MSM to represent a system by discretizing its MD trajectory (parameterized with torsion angles) into 10 metastable states and estimating the transition probabilities between them. Appendix B provides further details on constructing MSMs and other experimental settings. Additional results, including structural validations and further comparisons with related methods, can be found in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "4.1 Forward Simulation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the forward simulation setting, we train a model to sample 10 ns trajectories conditioned on the first frame. By chaining together successive model rollouts at inference time, we obtain 100 ns trajectories for each peptide to compare with ground-truth simulations. We evaluate if these sampled trajectories (1) match the structural distribution of trajectories from MD, (2) accurately capture the dynamical content of MD, and (3) traverse the state space in less wall-clock time than MD. ", "page_idx": 4}, {"type": "text", "text": "Distributional similarity. We report the Jensen-Shannon divergence (JSD) between the ground-truth and emulated trajectories along various collective variables shown in Figure 2 and Table 2. The first ", "page_idx": 4}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/440df45580a5c8e99ff54725edf02415596703c6fa74e8033ffea9a9b88297d8.jpg", "img_caption": ["Figure 2: Forward simulation evaluations on test peptides. (A) Torsion angle distributions for the six backbone torsion angles from MD trajectories (orange) and sampled trajectories (blue). (B, C) Free energy surfaces along the top two TICA components computed from backbone and sidechain torsion angles. $\\mathbf{\\eta}(\\mathbf{D})$ Markov State Model occupancies computed from MD trajectories versus sampled trajectories, pooled across all test peptides $n=1000$ states total). $\\mathbf{\\tau}(\\mathbf{E})$ Wall-clock decorrelation times of the first TICA component under MD versus our model rollouts. (F) Relaxation times of torsion angles computed from MD versus sampled trajectories, pooled across all test peptides\u2014508 backbone (blue) and 722 sidechain (orange) torsions in total. $(\\mathbf{G})$ Torsion angles in the tetrapeptide AAAA colored by the decorrelation time computed from MD (top) and from rollout trajectories (bottom). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "set of these are the individual torsion angles (backbone and sidechains) in each tetrapeptide. The second set of variables are the top independent components obtained from time-lagged independent components analysis (TICA), representing the slowest dynamic modes of the peptide. By each of these collective variables, MDGEN demonstrates excellent distributional similarity to the ground truth, approaching the accuracy of replicate 100-ns simulations. To more stringently assess the ability to locate and populate modes in the joint distribution over state space, we build Markov State Models (MSMs) for each test peptide using the MD ", "page_idx": 5}, {"type": "table", "img_path": "yRRCH1OsGW/tmp/e6129d5919c7fd252a4602b40350a45c6ae94cb8817dc222c36393a3b3468f6f.jpg", "table_caption": ["Table 2: JSD between sampled and ground-truth distributions, with replicate simulations as baselines. 100 ns represents oracle performance. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "trajectory, extract the corresponding metastable states, and compare the ground-truth and emulated distributions over metastable states. Our model captures the relative ranking of states reasonably well and rarely misses important states or places high mass on rare states (Figure 2D). ", "page_idx": 5}, {"type": "text", "text": "Dynamical content. We compute the dynamical properties of each tetrapeptide in terms of the decorrelation time of each torsion angle from the MD simulation and from our sampled trajectory. Intuitively, this assesses if our model can discriminate between slow- and fast-relaxing torsional barriers. The correlation between true and predicted relaxation timescales is plotted in Figure 2F, showing excellent agreement for sidechain torsions and reasonable agreement for backbones. To assess coarser but higher-dimensional dynamical content, we compute the flux matrix between all pairs of distinct metastable states using ground-truth and sampled trajectories and find substantial Spearman correlation between their entries (mean $\\rho=0.67\\pm0.01$ ; Figure 8). Thus, our simulation rollouts can accurately identify high-flux transitions in the peptide conformational landscape. ", "page_idx": 5}, {"type": "text", "text": "Sampling speed. Averaged across test peptides, our model samples 100 ns-equivalent trajectories in ${\\approx}60$ GPU-seconds, compared to ${\\approx}3$ GPU-hours for MD. To quantify the speedup more rigorously, we compute the decorrelation wall-clock times along the slowest independent component from TICA, capturing how quickly the simulation traverses the highest barriers in state space. These times are plotted in Figure 2E, showing that our model achieves a speedup of $10\\mathrm{x-}1000\\mathrm{x}$ over the MD simulation for 78 out of 100 peptides (the other 22 peptides did not fully decorrelate). ", "page_idx": 5}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/859f1cf4119caaf9ddceb16acf193435134f5b1516a6b2f5a1f07a5ccc34f053.jpg", "img_caption": ["Figure 3: Transition path sampling results. (Top) Intermediate states of one of the 1-nanosecond interpolated trajectories between two metastable states for the test peptide IPGD. (Bottom Left) The corresponding trajectory on the 2D free energy surface of the top two TICA components (more examples in Figure 9). (Bottom Right) Statistics averaged over 100 test peptides and 1000 paths for each of them. Shown are JSD, fraction of drawn paths that are valid transition paths, and average path likelihood of our discretized transitions under the reference MSM compared to discrete transitions drawn from the reference MSM or alternative MSMs built from replica simulations of varying lengths. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Interpolation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the interpolation or transition path sampling setting, we train a model to sample 1 ns trajectories conditioned on the first and last frames. For evaluation, we identify the two most well-separated states (i.e., with the least flux between them) for each test peptide and sample an ensemble of 1000 transition paths between them. Figure 3 shows an example of such a sampled path, which passes through several intermediate states on the free energy surface to connect the two endpoints. ", "page_idx": 6}, {"type": "text", "text": "To evaluate the accuracy of these sampled transitions, we cannot directly compare with MD trajectories since, in most cases, there are zero or very few 1-ns transitions between the two selected states (by design, the transition is a rare event). Thus, we instead discretize the trajectory over MSM metastable states and evaluate the path likelihood under the transition path distribution from the reference MSM (details in Appendix B.3). We also report the fraction of valid paths (i.e., non-zero probability) and the JSD between the distribution of visited states from our path distribution versus the transition path distribution of the reference MSM. For baselines, we sample transition paths from MSMs constructed from replicate MD simulations of varying lengths and compute the same metrics for these (discrete) path ensembles under the reference MSM. ", "page_idx": 6}, {"type": "text", "text": "As shown in Figure 3, our paths have higher likelihoods than those sampled from any replicate MD MSM shorter than 100ns, which is the length of the reference MD simulation itself. Moreover, MDGEN\u2019s ensembles have the best JSDs to the distribution of visited states of the reference MD MSM and the highest fraction on valid non-zero probability paths. Hence, our model enables zero-shot sampling of trajectories corresponding to arbitrary rare transitions for unseen peptides. ", "page_idx": 6}, {"type": "text", "text": "4.3 Upsampling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Molecular dynamics trajectories are often saved at relatively long time intervals (10s\u2013100s of picoseconds) to reduce disk storage; however, some molecular motions occur at faster timescales and would be missed by downstream analysis of the saved trajectory. In the upsampling setting, we train MDGEN to upsample trajectories saved with timestep 10 ps to a finer timestep of 100 fs, representing a $100\\mathbf{x}$ upsampling factor. To evaluate if the upsampled trajectories accurately capture the fastest dynamics, we compute the autocorrelation function $\\left\\langle\\cos(\\theta_{t}-\\theta_{t+\\Delta t})\\right\\rangle$ of each torsion angle in the test peptides as a function of lag time $\\Delta t$ ranging from 100 fs to 100 ps. ", "page_idx": 6}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/0d3bcea4ed581ac80d889f1e906f4b52c505f7e2a2d9f2c4d9afae80b036f0e7.jpg", "img_caption": ["Figure 4: Recovery of fast dynamics via trajectory upsampling for peptide GTLM. (Left) Autocorrelations of each torsion angle from $(-)$ the original 100 fs-timestep trajectory, $(\\bullet)$ the subsampled 10 ns-timestep trajectory, and $(\\cdots)$ the reconstructed 100 fs-timestep trajectory (all length 100 ns). $(R i g h t)$ Dynamical content as a function of timescale from the upsampled vs. ground truth trajectories, stacked for all torsion angles (same color scheme). The subsampled trajectory contains only the shaded region and our model recovers the unshaded region. Further examples in Figure 10. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "yRRCH1OsGW/tmp/e89034c9a65924242ffa755c436a39e362829cc64e80b963b8b91915417fe73a.jpg", "table_caption": ["Table 3: Sequence recovery for the inner two peptides when conditioning on the partial trajectory (MDGEN), the two terminal frames (DynMPNN), or a single frame (S-MPNN). "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/743e8181313ee8e198600a1e61ec69b4af54b4ede7aae2661227c0c1d7a35622.jpg", "img_caption": ["Figure 5: Autocorrelation functions of MDGEN sidechain torsion angles computed from a 10-ns MD trajectory (left) versus a single $\\mathbf{100k}$ -frame model sample with Hyena (right), capturing dynamics spanning four orders of magnitude. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Representative examples of ground truth, subsampled, and reconstructed autocorrelation functions for two test peptides are shown in Figure 4 (further examples in Figure 10). We further compute the dynamical content as the negative derivative of the autocorrelation with respect to log-timescale, which captures the extent of dynamic relaxations occurring at that timescale (Shaw et al., 2009). These visualizations highlight the significant dynamical information absent from the subsampled trajectory and which are accurately recovered by our model. In particular, our model distinctly recovers the oscillations of certain torsion angles as seen in the non-monotonicity of the autocorrelation function at sub-picosecond timescales; these features are completely missed at the original sampling frequency. ", "page_idx": 7}, {"type": "text", "text": "4.4 Additional Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Inpainting Design. We aim to sample trajectories conditioned on the dynamics of the two flanking residues of the tetrapeptide; in particular, the model determines the identities and dynamics of the two inner residues. We focus on dynamics scaffolding as one possible higher-level objective of inpainting: given the conformational transition of the observed residues, we hope to design peptides that support flux between the corresponding Markov states. Thus, for each test peptide, we select a 100-ps transition between the two most well-connected Markov states, mask out the inner residue identities and dynamics, and inpaint them with our model. To evaluate the designs, we compute the fraction of generated residue types that are identical to the tetrapeptide in which the target transition is known to occur. We compare MDGEN with a bespoke inverse folding baseline that is provided the two terminal states (i.e., two fully observed MD frames), and thus designs peptides that support the two modes (rather than additionally a partially-observed transition between them). We call this baseline DYNMPNN, and it otherwise has the same architecture and settings as MDGEN (more details in Appendix B.3). We find (Table 3) that MDGEN recovers the ground-truth peptide substantially more often than DynMPNN when conditioned on a high-flux path or (as a sanity check) a random path from the reference simulatiom. ", "page_idx": 7}, {"type": "table", "img_path": "yRRCH1OsGW/tmp/284f603e65f09ca80a1a73e29b3ce4b8aac358b05549c0275abb008dd792ae26.jpg", "table_caption": ["Table 4: Median results on test protein ensembles $n=82$ ); evaluations from Jing et al. (2024). Runtimes are reported per sample structure or frame. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/28f7fa17f1a3da3370fa9e363c8b2b6cfe8a7171d19a2e91938d36a052450eb5.jpg", "img_caption": ["Figure 6: MD vs generated ensembles for 6uof_A, with $\\mathrm{C}\\alpha$ RMSFs plotted by residue index (Pearson $r=0.74_{\\cdot}$ ). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Scaling to Long Trajectories. Although Section 4.1 showed that our model can emulate long trajectories, this was limited to rollouts of 1000 frames at a time with coarse 10 ps timesteps, potentially missing faster dynamics or disrupting slower dynamics. Thus, we investigate generating extremely long consistent trajectories that capture timescales spanning several orders of magnitude within a single model sample. To do so, we replace the time attention in our baseline SiT architecture with a non-causal Hyena operator (Poli et al., 2023), which has $O(N\\log N)$ rather than $O(N^{2})$ overhead. We overfit on $100\\mathbf{k}$ -frame, 10-ns trajectories of the pentapeptide MDGEN and compare the torsional autocorrelation functions computed from a single generated trajectory with a single ground truth trajectory (Figure 5). Although not yet comparable to the main set of forward simulation experiments due to data availability and architectural expressivity reasons, these results demonstrate proof-of-concept for longer context lengths in future work. ", "page_idx": 8}, {"type": "text", "text": "Protein Simulation. To demonstrate the applicability of our method for larger systems such as proteins, we train a model to emulate all-atom simulations of proteins from the ATLAS dataset (Vander Meersche et al., 2024) conditioned on the first frame (i.e., forward simulation). We follow the same splits as Jing et al. (2024). Due to the much larger number of residues, we generate samples with 250 frames and 400 ps timestep, such that a single sample emulates the 100 ns ATLAS reference trajectory. The difficulty of running fully equilibrated trajectories for proteins prevents the construction of Markov state models used in our main evaluations. Instead, we compare statistical properties of forward simulation ensembles following Jing et al. (2024). Our ensembles successfully emulate the ground-truth ensembles at a level of accuracy between AlphaFlow and MSA subsampling while being orders of magnitude faster per generated structure than either (Table 4; Figure 6). ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Limitations. Our experiments have validated the model and architecture for peptide simulations; however, a few limitations provide opportunities for future improvement. Due to the reliance on key frames, the model is not capable of unconditional generation or inpainting of residue rototranslations. The weaker performance on protein monomers relative to peptides suggests that scaling to larger systems will likely require additional data or methodological innovations. Fine-tuning of single structure models for co-generation of the key frames and trajectory tokens, similar to the content-frame decomposition of video diffusion models (Yu et al., 2024), may provide improvement. Since our tokenization scheme is specific to polypeptides, alternative strategies will be needed to model all-atom trajectories of more general systems, such as organic ligands, materials, or explicit solvent. More ambitious applications (see below) may require the ability to model trajectories not of a predefined set of atoms but over a region of space in which atoms may enter and exit. As such, we anticipate advancements in tokenization and architecture to be a fruitful direction of future work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Opportunities. Similar to the foundational role of video generative models for understanding the macroscopic world (Yang et al., 2024), MD trajectory generation could serve as a multitask, unifying paradigm for deep learning over the microscopic world. Interpolation can be more broadly framed as hypothesis generation for mechanisms of arbitrary molecular phenomena, especially when only partial information about the end states is supplied. Molecular inpainting could be a general technique to design molecular machinery by scaffolding more fine-grained and complex dynamics, for example, redesigning proteins to enhance rare transitions observed only once in a simulation or (with ab initio trajectories) de novo design of enzymatic mechanisms and motifs. Other types of conditioning not explored in this work may lead to further applications, such as conditioning over textual or experimental descriptors of the trajectory. Future availability of significantly more ground truth MD trajectory data for diverse chemical systems could be a chief enabler of such work. Lastly, considerations unique to molecular trajectories, such as equilibrium vs non-equilibrium processes, Markovianity, and the reversibility of the microscopic world contrasted with the macroscopic world (e.g., the missing arrow of time), could provide ripe areas for theoretical exploration. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Felix Faltings, Jason Yim, Mateo Reveiz, Gabriele Corso, and anonymous NeurIPS reviewers for helpful feedback and discussions. ", "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Institute of General Medical Sciences of the National Institutes of Health under award number 1R35GM141861; the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Department of Energy Computational Science Graduate Fellowship under Award Number DESC0022158; the National Science Foundation under Grant No. 1918839; the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium; the Abdul Latif Jameel Clinic for Machine Learning in Health; the DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE) threats program; and the DARPA Accelerated Molecular Discovery program. This research used resources of the National Energy Research Scientific Computing Center (NERSC), a Department of Energy Office of Science User Facility using NERSC awards ASCR-ERCAP0027302 and ASCRERCAP0027818 ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Michael S Albergo, Nicholas M Boff,i and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. ", "page_idx": 9}, {"type": "text", "text": "Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2022.   \nBerni J Alder and Thomas Everett Wainwright. Studies in molecular dynamics. i. general method. The Journal of Chemical Physics, 31(2):459\u2013466, 1959.   \nJoshua A Anderson, Chris D Lorenz, and Alex Travesset. General purpose molecular dynamics simulations fully implemented on graphics processing units. Journal of computational physics, 227(10):5342\u20135359, 2008.   \nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021.   \nAndrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:28266\u201328279, 2022.   \nAndrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024.   \nJohn D Chodera and Frank No\u00e9. Markov state models of biomolecular conformational dynamics. Current opinion in structural biology, 25:135\u2013144, 2014.   \nTom Darden, Darrin York, and Lee Pedersen. Particle mesh ewald: An n log (n) method for ewald sums in large systems. The Journal of chemical physics, 98(12):10089\u201310092, 1993.   \nDiego Del Alamo, Davide Sala, Hassane S Mchaourab, and Jens Meiler. Sampling alternative conformational states of transporters and receptors with alphafold2. Elife, 11:e75751, 2022.   \nPeter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A Beauchamp, Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al. Openmm 7: Rapid development of high performance algorithms for molecular dynamics. PLoS computational biology, 13(7):e1005659, 2017.   \nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024.   \nXiang Fu, Tian Xie, Nathan J Rebello, Bradley Olsen, and Tommi S Jaakkola. Simulate timeintegrated coarse-grained molecular dynamics with multi-scale graph networks. Transactions on Machine Learning Research, 2023.   \nVictor Garcia Satorras, Emiel Hoogeboom, Fabian Fuchs, Ingmar Posner, and Max Welling. E (n) equivariant normalizing flows. Advances in Neural Information Processing Systems, 34:4181\u20134192, 2021.   \nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633\u20138646, 2022.   \nEmiel Hoogeboom, Alexey A Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. arXiv preprint arXiv:2110.02037, 2021.   \nBrooke E Husic and Vijay S Pande. Markov state models: From an art to a science. Journal of the American Chemical Society, 140(7):2386\u20132396, 2018.   \nBowen Jing, Bonnie Berger, and Tommi Jaakkola. Alphafold meets flow matching for generating protein ensembles. arXiv preprint arXiv:2402.04845, 2024.   \nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.   \nLeon Klein, Andrew Foong, Tor Fjelde, Bruno Mlodozeniec, Marc Brockschmidt, Sebastian Nowozin, Frank No\u00e9, and Ryota Tomioka. Timewarp: Transferable acceleration of molecular dynamics by learning time-coarsened dynamics. Advances in Neural Information Processing Systems, 36, 2024.   \nJonas K\u00f6hler, Andreas Kr\u00e4mer, and Frank No\u00e9. Smooth normalizing flows. Advances in Neural Information Processing Systems, 34:2796\u20132809, 2021.   \nAlessandro Laio and Michele Parrinello. Escaping free-energy minima. Proceedings of the national academy of sciences, 99(20):12562\u201312566, 2002.   \nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \nXingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.   \nNanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boff,i Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024.   \nJ Andrew McCammon, Bruce R Gelin, and Martin Karplus. Dynamics of folded proteins. nature, 267(5612):585\u2013590, 1977.   \nLaurence Midgley, Vincent Stimper, Javier Antor\u00e1n, Emile Mathieu, Bernhard Sch\u00f6lkopf, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Se (3) equivariant augmented coupling flows. Advances in Neural Information Processing Systems, 36, 2024.   \nLaurence Illing Midgley, Vincent Stimper, Gregor NC Simm, Bernhard Sch\u00f6lkopf, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Flow annealed importance sampling bootstrap. arXiv preprint arXiv:2208.01893, 2022.   \nFrank No\u00e9, Simon Olsson, Jonas K\u00f6hler, and Hao Wu. Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. Science, 365(6457):eaaw1147, 2019.   \nFrank No\u00e9, Hao Wu, Jan-Hendrik Prinz, and Nuria Plattner. Projected and hidden Markov models for calculating kinetics and metastable states of complex molecules. The Journal of Chemical Physics, 139(18):184114, 11 2013.   \nVijay S Pande, Kyle Beauchamp, and Gregory R Bowman. Everything you wanted to know about markov state models but were afraid to ask. Methods, 52(1):99\u2013105, 2010.   \nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \nGuillermo P\u00e9rez-Hern\u00e1ndez, Fabian Paul, Toni Giorgino, Gianni De Fabritiis, and Frank No\u00e9. Identification of slow molecular order parameters for markov model construction. The Journal of chemical physics, 139(1), 2013.   \nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pages 28043\u201328078. PMLR, 2023.   \nJan-Hendrik Prinz, Hao Wu, Marco Sarich, Bettina Keller, Martin Senne, Martin Held, John D. Chodera, Christof Sch\u00fctte, and Frank No\u00e9. Markov models of molecular kinetics: Generation and validation. The Journal of Chemical Physics, 134(17):174105, 2011.   \nAneesur Rahman. Correlations in the motion of atoms in liquid argon. Physical review, 136(2A): A405, 1964.   \nSusanna R\u00f6blitz and Marcus Weber. Fuzzy spectral clustering by pcca+: application to markov state models and data classification. Advances in Data Analysis and Classification, Jun 2013.   \nJean-Paul Ryckaert, Giovanni Ciccotti, and Herman JC Berendsen. Numerical integration of the cartesian equations of motion of a system with constraints: molecular dynamics of n-alkanes. Journal of computational physics, 23(3):327\u2013341, 1977.   \nMartin K. Scherer, Benjamin Trendelkamp-Schroer, Fabian Paul, Guillermo P\u00e9rez-Hern\u00e1ndez, Moritz Hoffmann, Nuria Plattner, Christoph Wehmeyer, Jan-Hendrik Prinz, and Frank No\u00e9. PyEMMA 2: A Software Package for Estimation, Validation, and Analysis of Markov Models. Journal of Chemical Theory and Computation, 11:5525\u20135542, 2015.   \nMathias Schreiner, Ole Winther, and Simon Olsson. Implicit transfer operator learning: Multiple time-resolution models for molecular dynamics. Advances in Neural Information Processing Systems, 36, 2024.   \nDavid E Shaw, Ron O Dror, John K Salmon, JP Grossman, Kenneth M Mackenzie, Joseph A Bank, Cliff Young, Martin M Deneroff, Brannon Batson, Kevin J Bowers, et al. Millisecond-scale molecular dynamics simulations on anton. In Proceedings of the conference on high performance computing networking, storage and analysis, pages 1\u201311, 2009.   \nHannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. arXiv preprint arXiv:2402.05841, 2024.   \nYuji Sugita and Yuko Okamoto. Replica-exchange molecular dynamics method for protein folding. Chemical physics letters, 314(1-2):141\u2013151, 1999.   \nYann Vander Meersche, Gabriel Cretin, Aria Gheeraert, Jean-Christophe Gelly, and Tatiana Galochkina. Atlas: protein flexibility description from atomistic molecular dynamics simulations. Nucleic Acids Research, 52(D1):D384\u2013D392, 2024.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nLoup Verlet. Computer\" experiments\" on classical fluids. i. thermodynamical properties of lennardjones molecules. Physical review, 159(1):98, 1967.   \nChristoph Wehmeyer, Martin K Scherer, Tim Hempel, Brooke E Husic, Simon Olsson, and Frank No\u00e9. Introduction to markov state modeling with the pyemma software\u2014v0. 3.   \nSherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139, 2024.   \nJason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. arXiv preprint arXiv:2302.02277, 2023.   \nSihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, and Anima Anandkumar. Efficient video diffusion models via content-frame motion-latent decomposition. arXiv preprint arXiv:2403.14148, 2024.   \nShuxin Zheng, Jiyan He, Chang Liu, Yu Shi, Ziheng Lu, Weitao Feng, Fusong Ju, Jiaxi Wang, Jianwei Zhu, Yaosen Min, et al. Towards predicting equilibrium distributions for molecular systems with deep learning. arXiv preprint arXiv:2306.05445, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Method Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Flow Model Architecture ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Notation. Here, as in the main text, we use the following notation: ", "page_idx": 13}, {"type": "text", "text": "\u2022 $T$ : number of trajectory frames \u2022 $L$ : number of amino acids \u2022 $K$ : number of key frames, with indicies $t_{1}\\ldots t_{K}$ ", "page_idx": 13}, {"type": "text", "text": "In Algorithms 1\u20133 below, we modify the architectures of DiffusionTransformerAttentionLayer and DiffusionTransformerFinalLayer from DiT (Peebles and Xie, 2023). Elements from these layers are also then incorporated into our custom InvariantPointAttentionLayer. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1: Velocity network ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: noisy tokens $\\boldsymbol{\\chi}\\in\\mathbb{R}^{T\\times L\\times(7K+14)}$ , conditioning tokens $\\boldsymbol{\\chi}_{\\mathrm{cond}}\\in\\mathbb{R}^{T\\times L\\times(7K+14)}$ , key frame roto-translations $g_{t_{1}}\\,.\\,.\\,.\\,g_{t_{K}}\\,\\in\\,\\bigl(S E(3)^{L}\\bigr)^{K}$ , flow matching time $t$ , amino acid identities $A\\in\\{1,\\ldots20\\}^{L}$ , conditioning mask $\\mathbf{m}\\in\\{0,1\\}^{T\\times L\\times(7K+14)}$ Output: flow velocity $v\\in\\mathbb{R}^{T\\times L\\times(7K+14)}$   \n1 $t\\gets\\mathrm{Embed}(t)$ ;   \n2 for $k\\gets1$ to $K$ do   \n3 $\\begin{array}{r}{\\mathbf{x}_{k}=\\mathrm{Embed}(A)+\\sum_{k^{\\prime}}\\mathrm{Linear}(g_{t_{k}}^{-1}g_{t_{k^{\\prime}}})\\;;}\\end{array}$ ;   \n4 for $l\\gets1$ to num_ipa_layers do   \n5 $\\boxed{\\begin{array}{r l}&{\\ L\\ \\mathbf{x}_{k}=\\mathrm{InvariantPointAttentionLayer}(\\mathbf{x},g_{k},t)}\\end{array}}$   \n6 $\\begin{array}{r}{\\mathbf{x}=\\sum_{k}\\mathbf{x}_{k}+\\mathrm{Linear}(\\boldsymbol{\\chi})+\\mathrm{Linear}(\\chi_{\\mathrm{cond}}\\odot\\mathbf{m})+\\mathrm{Embed}(\\mathbf{m});}\\end{array}$   \n7 for $l\\gets1$ to num_transformer_layers do   \n8 $\\mathbf{x}=$ DiffusionTransformerAttentionLayer(x, t)   \n9 return DiffusionTransformerFinalLayer(x, t) ", "page_idx": 13}, {"type": "text", "text": "Algorithm 2: DiffusionTransformerAttentionLayer ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: $\\mathbf{x}\\in\\mathbb{R}^{T\\times L\\times C}$ , time conditioning $t$   \n1 $(\\alpha,\\beta,\\gamma)_{t,\\ell,f}=\\mathrm{Linear}(t)$ ;   \n2 $\\mathbf{x}+=g_{\\ell}\\odot$ $\\begin{array}{r}{\\mathcal{I}_{\\ell}\\odot\\mathrm{AttentionWithRoPE}(\\gamma_{\\ell}\\odot\\mathrm{LayerNorm}(\\mathbf{x})+\\beta_{\\ell},\\mathrm{dim}=1)}\\end{array}$ ; $\\mathbf{x}+=g_{t}\\odot$ $\\mathrm{AttentionWithRoPE}(\\gamma_{t}\\odot\\mathrm{LayerNorm}(\\mathbf{x})+\\beta_{t}.$ , dim = 0); $\\mathbf{1}\\,\\textbf{x}\\!+\\!\\!=g_{m}\\odot\\mathrm{MLP}(\\gamma_{m}\\odot\\mathrm{LayerNorm}(\\mathbf{x})+\\beta_{m})$ ;   \n5 return x ", "page_idx": 13}, {"type": "text", "text": "Algorithm 3: InvariantPointAttentionLayer ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: $\\mathbf{x}\\in\\mathbb{R}^{L\\times C}$ , time conditioning $t$ , roto-translations $g\\in S E(3)^{L}$ 1 $(\\alpha,\\beta,\\gamma)\\ell,f=\\mathrm{Linear}(t)$ ;   \n2 ${\\bf x}+=$ InvariantPointAttention $(\\mathrm{LayerNorm}(\\mathbf{x}),g)$ ;   \n$\\mathbf{x}+=g_{\\ell}\\odot$ AttentionWithRoPE(\u03b3\u2113\u2299LayerNorm(x) + \u03b2\u2113); $\\begin{array}{r}{4\\ \\mathbf{x}+=g_{m}\\odot\\mathrm{MLP}(\\gamma_{m}\\odot\\mathrm{LayerNorm}(\\mathbf{x})+\\beta_{m})}\\end{array}$ ;   \n5 return x ", "page_idx": 13}, {"type": "text", "text": "A.2 Integrating Dirichlet Flow Matching ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To additionally generate amino acid identities along with the trajectory dynamics, we integrate our SiT flow matching framework with Dirichlet flow matching (Stark et al., 2024). Specifically, we now parameterize a velocity network v\u03b8 : R7K \u2295R20 T \u00d7L $\\begin{array}{r}{v_{\\theta}\\,:\\,\\left(\\mathbb{R}^{7K}\\oplus\\mathbb{R}^{20}\\right)^{T\\times L}\\,\\times\\,[0,1]\\,\\rightarrow\\,\\left(\\mathbb{R}^{7K}\\oplus\\mathbb{\\bar{R}}^{20}\\right)^{T\\times L}}\\end{array}$ . No architecture modifications are necessary other than augmenting the tokens with one-hot tokens of residue identity, broadcasted across time. At training time, we sample from the Dirichlet probability path (rather than the Gaussian path) for those token elements. However, the parameterization is subtle as Dirichlet FM trains with cross-entropy loss, contrary to the standard flow-matching MSE loss. Thus, during training time we minimize the loss ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathbb{E}\\left[\\Vert v_{\\theta}[\\,.\\,.\\,.\\,,\\,;-20]-u_{t}(\\chi_{t}\\mid\\chi_{1})\\Vert^{2}+\\mathrm{CrossEntropy}\\big(\\mathrm{Softmax}\\big(v_{\\theta}\\,[\\,.\\,.\\,.\\,,20\\,:\\,]\\big),A\\big)\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "That is, we interpret the last 20 outputs in the channel dimension as logits over the 20 residue types. At inference time, on the other hand, we convert these logits to the Dirichlet FM flow field: ", "page_idx": 14}, {"type": "equation", "text": "$$\nv_{\\theta}^{\\prime}=\\operatorname{Concat}\\left(v_{\\theta}\\left[\\,.\\,.\\,.\\,,\\,;-20]\\,,\\sum_{i}\\mathrm{Softmax}(v_{\\theta}\\left[\\,.\\,.\\,.\\,,-20\\,;\\,\\right]\\,)_{i}\\cdot u_{\\mathrm{DFM}}(\\cdot\\mid x_{1}=i)\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $u_{\\mathrm{DFM}}$ is the appropriate Dirichlet vector field from Stark et al. (2024). ", "page_idx": 14}, {"type": "text", "text": "A.3 Conditional Generation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We control the conditional generation settings by simply setting appropriate entries of the conditioning mask m in Algorithm 1 to 1 or 0. Specifically, ", "page_idx": 14}, {"type": "text", "text": "\u2022 For the forward simulation setting, $\\mathbf{m}[t,\\ell,c]=\\left\\{{1\\atop0}\\begin{array}{r l}{t=1}\\\\ {t\\neq1}\\end{array}}\\right.$   \n\u2022 For the inpainting setting, $\\mathbf{m}[t,\\ell,c]=\\left\\{1\\right.\\ \\ t\\in\\{1,T\\}}$   \n\u2022 For the upsampling setting, $\\mathbf{m}[t,\\ell,c]=\\left\\{1\\begin{array}{r l}{\\phantom{-}t\\,\\%\\,M=1}\\\\ {0}&{{t\\,\\%\\,M\\neq1}}\\end{array}\\right.$ where $M$ is the upsampling factor. \u2022 For the inpainting setting, $\\mathbf{m}[t,\\ell,c]=\\left\\{1\\!\\!\\begin{array}{r l}{\\ell\\in S_{\\mathrm{known}}}\\\\ {0}&{\\ell\\notin S_{\\mathrm{known}}}\\end{array}\\right.$ \u2113\u2208Sknown where Sknown is the set of residues in the known part of the trajectory. ", "page_idx": 14}, {"type": "text", "text": "We use 1 indexing to be consistent with the main text. In practice, in the inpainting setting we also mask out all torsion angles and withhold the amino acid identities for all residues. Further, we do not train the model to generate the torsions as all, such that the tokenization yields $\\boldsymbol{\\chi}\\in\\mathbb{R}^{T\\times L\\times7K}$ . These interventions were observed to be necessary to prevent overfitting. ", "page_idx": 14}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Markov State Models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Markov State Model (MSM) is a representation of a system\u2019s dynamics discretized into $r$ states $s\\in$ $\\{1\\ldots r\\}$ and a discrete timesteps separated by time lag $\\tau$ such that the dynamics are approximately Markovian (Husic and Pande, 2018; Chodera and No\u00e9, 2014; Pande et al., 2010). An MSM is parameterized with a vector $\\pi$ that assigns each state a stationary probability and a matrix $T$ containing the probabilities for transitioning from state $s_{t}$ to $s_{t+1}$ after one timestep, i.e., $T_{i,j}\\,=$ $p(s_{t+1}=\\dot{\\textit{j}}|\\ \\dot{s_{t}}=\\dot{\\textit{i}})$ . ", "page_idx": 14}, {"type": "text", "text": "To build a Markov state model, we use PyEMMA (Scherer et al., 2015; Wehmeyer et al.) and its accompanying tutorials. Briefly, we first featurize molecular trajectories with all torsion angles as points on the unit circle, obtaining a $2m$ -dimensional invariant trajectory where $m$ is the number of torsion angles. We run TICA on these trajectories with kinetic scaling and then run $k$ -means clustering with $k=100$ over the first few (5\u201310 chosen by PyEMMA) TICA coordinates. We then estimate an MSM over these 100 states and use $\\mathrm{PCCA+}$ spectral clustering (R\u00f6blitz and Weber, 2013) to further group these into 10 metastable states. Our final MSM is built from the discrete trajectory over these 10 metastable states. In all cases we use timelag $\\tau=100$ ps. ", "page_idx": 14}, {"type": "text", "text": "Unconditionally sampling an MSM. To unconditionally sample a trajectory of length $N$ from an MSM, we first sample the start state from the stationary distribution, i.e., $s_{1}\\sim\\pi$ . We then iteratively sample each subsequent state as $s_{t+1}\\sim T_{s_{t},:}$ . ", "page_idx": 14}, {"type": "text", "text": "Sampling an MSM conditioned on a start state. To sample a trajectory of length $N$ conditioned on a starting state $s_{1}$ , we iteratively sample each subsequent state as $s_{t+1}\\sim T_{s_{t},}$ :. ", "page_idx": 15}, {"type": "text", "text": "Sampling an MSM conditioned on a start and end state. For our transition path sampling evaluations in Section 4.2, we employ replica transition paths sampled from an MSM by conditioning on a start state $s_{1}$ and end state $s_{N}$ . To do so, we iteratively sample each state between the conditioning states by utilizing the probability ", "page_idx": 15}, {"type": "equation", "text": "$$\np(s_{t+1}=j\\mid s_{t}=i,s_{N}=k)={\\frac{p(s_{N}=k\\mid s_{t+1}=j,s_{t}=i)p(s_{t+1}=j\\mid s_{t}=i)}{p(s_{N}=k\\mid s_{t}=i)}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Firstly, the term $p(s_{t+1}=j\\mid s_{t}=i)$ is available in out transition matrix as $T_{i,j}$ . Secondly, we obtain $p(s_{N}\\;=\\;k\\;\\mid\\;s_{t}\\;=\\;i)$ as an entry of the $(N-t)t h$ matrix exponential of the transition matrix. Specifically $p(s_{N}~=~k~\\mid~s_{t}~=~i)~=~T_{i,k}^{(N-t)}$ T i(,kN\u2212t) where the superscript denotes a matrix exponential. Lastly, we obtain the term $p(s_{N}\\,=\\,k\\,\\mid\\,s_{t+1}\\,=\\,j,s_{t}\\,=\\,i)$ by realizing that under the Markov assumption $p(s_{N}\\;=\\;k\\;\\;|\\;\\;s_{t+1}\\;=\\;j,s_{t}\\;=\\;i)\\;=\\;p(s_{N}\\;=\\;k\\;\\;|\\;\\;s_{t+1}\\;=\\;j)$ . Further, $p(s_{N}=k\\mid s_{t+1}=j)=T_{j,k}^{(N-t)-1}$ . ", "page_idx": 15}, {"type": "text", "text": "Replacing the terms in Equation 7 results in ", "page_idx": 15}, {"type": "equation", "text": "$$\np(s_{t+1}=j\\mid s_{t}=i,s_{N}=k)=\\frac{T_{j,k}^{(N-t-1)}T_{i,j}}{T_{i,k}^{(N-t)}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we sample states $s_{2}\\ldots s_{N-1}$ iteratively as ", "page_idx": 15}, {"type": "equation", "text": "$$\ns_{t+1}\\sim\\frac{T_{:,s_{N}}^{(N-t-1)}T_{s_{t},:}}{T_{s_{t},s_{N}}^{(N-t)}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.2 Tetrapeptide Molecular Dynamics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We run all-atom molecular dynamics simulations in OpenMM (Eastman et al., 2017) using the amber14 force field parameters with gbn2 implicit solvent or tip3pfb water model. Initial structures are generated with PyMOL, prepared with pdbfixer, and protonated at neutral $\\mathrm{pH}$ . For explicit solvent, we prepare a solvent box with $10\\,\\mathrm{\\bar{A}}$ padding and neutralize the system with sodium or chloride ions. All simulations are integrated with Langevin thermostat at 350K with hydrogen bond constraints, timestep 2 fs, and friction coefficient $\\bar{0.3}\\,\\mathsf{p s}^{-1}$ (explicit) or $0.1\\,\\mathrm{ps^{-1}}$ (implicit). For explicit solvent, nonbonded interactions are cut off at $10\\,\\mathrm{\\bar{A}}$ with long-range particle-mesh Ewald. We first minimize the energy with L-BFGS and then equilibrate the system in the NVT ensemble for 20 ps. We then run 100 ns of production simulation in the NVT ensemble (implicit) or NPT ensemble with Monte Carlo barostat at 1 bar (explicit). We write heavy atom positions every 100 fs. ", "page_idx": 15}, {"type": "text", "text": "For explicit-solvent settings (forward simulation, interpolation, inpainting), we run simulations for 3109 training, 100 validation, and 100 test peptides. For implicit-solvent settings (upsampling), we run simulations for 2646 training, 100 validation, and 100 test peptides. All peptides are randomly chosen and split. Additionally, 5195 training and 100 validation implicit solvent simulations are run for the pentapeptide MDGEN. ", "page_idx": 15}, {"type": "text", "text": "B.3 Evaluation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Trajectory Featurization We featurize trajectories by selecting the sine and cosine of all torsion angles as the collective variables. Specifically, we featurize $\\psi,\\phi$ backbone angles and all $\\chi$ sidechain torsion angles for each peptide. We then reduce dimensionality with Time-lagged Independent Components Analysis (TICA) (P\u00e9rez-Hern\u00e1ndez et al., 2013) in PyEMMA (Scherer et al., 2015). ", "page_idx": 15}, {"type": "text", "text": "Jensen-Shannon Divergence We compute the JSD as implemented in scipy, i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{D(p\\mid m)+D(q\\mid m)}{2}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $m=(p+q)/2$ . For the 1-dimenional JSD over torsion angles, we discretize the range $[-\\pi,\\pi]$ into 100 bins. For the 1-dimensional JSD over TIC-0, we discretize the range spanning the maximum and minimum values into 100 bins. For the 2-dimensional JSD over TIC-0,1 we discretize the space into $50\\times50$ bins. ", "page_idx": 16}, {"type": "text", "text": "Autocorrelation The autocorrelation of torsion angle $\\theta$ at time lag $\\Delta t$ is defined as $\\langle\\cos(\\theta_{t}-$ $\\theta_{t+\\Delta t})\\rangle$ , corresponding to the inner product of $\\theta_{t},\\theta_{t+\\Delta t}$ on the unit circle. To compute the decorrelation time of a torsion angle, we subtract the baseline inner product $\\langle\\cos\\theta\\rangle^{2}+\\dot{\\langle}\\sin\\theta\\rangle^{2}$ , this is analogous to removing the mean of a real-valued time series before computing the autocorrelation. The decorrelation time is then defined as the time required for the autocorrelation to fall below $1/e$ of its initial value (which is always unity), with the subtracted baseline computed from the reference trajectory. In a small number of cases (21 torsions), the MDGEN trajectory did not decorrelate within 1000 frames (10 ns), and we exclude the angle from Figure 2F. ", "page_idx": 16}, {"type": "text", "text": "To compute the decorrelation time for TIC-0, we now define the autocorrelation as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[(y_{t}-\\mu)(y_{t+\\Delta t}-\\mu)]/\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mu,\\sigma$ are computed from the reference trajectory. Hence, when computed for a sampled MDGEN trajectory, the autocorrelation may not start at unity and may not decay to zero. We report a decorrelation time if starts above and falls below 0.5 within 1000 frames (10 ns), which happens in 74 out of 100 cases as shown in Figure 2E. ", "page_idx": 16}, {"type": "text", "text": "Interpolation In our interpolation or transition path sampling experiments, we sample 1000 trajectories of length 1ns for each of our 100 test tetrapeptides. We first select a start state $s_{1}$ and an end state $s_{N}$ that exhibits non-trivial transitions. To do so, we consider a reference MD simulation of 100 ns for the tetrapeptide and obtain an MSM as described in Appendix B.1. From the MSM\u2019s transition matrix $T$ and stationary distribution $\\pi$ , we compute the flux matrix $\\begin{array}{r}{F=T\\odot P i}\\end{array}$ where $P i$ is the square matrix with $\\pi$ in each column. The chosen start and end state is the row and column index of the smallest non-zero entry in $F$ . ", "page_idx": 16}, {"type": "text", "text": "With the start state $s_{1}$ and end state $s_{N}$ selected, we sample 1000 start frames $\\mathbf{x}_{1}$ and end frames $\\mathbf{x}_{N}$ from the states. The 1000 start frames are sampled from all frames in the reference MD simulation that belong to state $s_{1}$ . Analogously, the end frames are sampled from all frames belonging to state $s_{N}$ . Using the 1000 pairs of start and end frames, we condition MDGEN on them and generate trajectories of 100 frames (1 ns). For evaluation, we discretize these trajectories under the 10-state clustering determined by the MSM of the reference MD simulation as described in Appendix B.1. Note that with the MSM lag time of $100\\,\\mathrm{ps}$ , these discrete trajectories are of length 10. ", "page_idx": 16}, {"type": "text", "text": "MD baselines. To sample transition paths of 1 ns between our selected start and end states, we employ MSMs built from replica MD simulations of varying lengths. For instance, for a replica MD simulation of 100ns, we first discretize its trajectory with the cluster assignments of the reference MD simulation (the same cluster assignments as we use to discretize the MDGEN ensemble and that we use for evaluation). Next, we estimate an MSM from the discretized trajectory. We then proceed to sample 1000 transition paths from the MSM as described in B.1 where the path is conditioned on an end and start state. In the event that the replica MSM has zero transition probability for transitioning out of the start state or zero probability for transitioning into the end state (this occurs if the replica MD simulation never visited the start or end state), we treat all 1000 paths of the replica MD as having zero probability for our evaluation metrics which are further detailed in the following. ", "page_idx": 16}, {"type": "text", "text": "Computing TPS metrics. As described above, we obtain ensembles of 1000 discretized 1ns paths of 100 frames for both MDGEN and the replica MD simulations. For these, in Figure 3, we show a JSD, the rate of valid paths, and the average path probability. These metrics are computed with respect to the MSM of the reference MD simulation of length $100\\,\\mathrm{ns}$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 To compute the JSD, we draw 1000 discrete transition paths from the reference MD simulation and compute the probability of visiting each state from the frequency with which each state is visited. We do the same for the transition path ensemble of MDGEN (or the baseline) and compute the JSD between the categorical distributions as described above. \u2022 The average path probability for an ensemble is the average of its paths\u2019 probabilities for transitioning from the start to the end state under the reference MSM. This probability can be computed as described in Appendix B.1. ", "page_idx": 16}, {"type": "text", "text": "Inpainting In our inpainting experiments, we set out to design tetrapeptides that transition between two states. Considering the residue indices 1, 2, 3 and 4, we call the residues 1, 4 the flanking residues which we condition on and 2, 3 the inner residues which we aim to design. Specifically, we condition MDGEN on the trajectory of the flanking residues\u2019 backbone coordinates and generate the residue identities of the inner two residues. To carry out this design for a single tetrapeptide, we draw 1000 samples from MDGEN to estimate the mode of its joint distribution over the inner two residues. ", "page_idx": 17}, {"type": "text", "text": "The conditioning information (the trajectories of the outer two residues\u2019 backbone coordinates) is different for the two evaluation settings of designing transitions with high flux or for designing arbitrary transitions. However, for both of them, the start and end frames are provided as conditioning information via the key frames. In the high flux setting, the conditioning information is obtained by sampling 1000 paths from the reference MD simulation of length 10 ps with 100 frames that start and end in the desired states. These states are determined as those with the maximum flux between them (see the paths about interpolation above for a description of flux). When designing residues that give rise to arbitrary random paths, the trajectories are randomly sampled from the reference simulation. ", "page_idx": 17}, {"type": "text", "text": "After sampling 1000 pairs of residues for the inner two residues, we select the most frequently occurring pair as the final design. For this design, we report the sequence recovery (the fraction of residues that match the original sequence of the MD simulations from which the conditioning information was sampled). ", "page_idx": 17}, {"type": "text", "text": "Inpainting Baselines. We aim to assess the benefit that is obtained by the trajectory-based inference of MDGEN over a baseline that only takes the start frame or the start and end frame as input for designing residues that transition between two states. Thus, we construct DYNMPNN and S-MPNN. These baselines use the same architecture as MDGEN in the inpainting setting, but DYNMPNN only obtains the start and end frames as key frames and via their roto-translation offsets for the first and last frames. S-MPNN is the analog with only the first frame. ", "page_idx": 17}, {"type": "text", "text": "Notably, in the inpainting setting, MDGen and the baselines do not treat torsion angles, and all torsion angle entries of the SE(3)-invariant tokens are set to 0. Furthermore, the model does not take the amino acids of the flanking residues as input. We make this choice of withholding all information about amino acid identities since otherwise, the models overfit on the arbitrary identities of the flanking residues and do not generalize to the test set. ", "page_idx": 17}, {"type": "text", "text": "Protein Simulations For training and evaluation on proteins, we use trajectories from the ATLAS dataset (Vander Meersche et al., 2024), which includes 3 replicates of 100 ns explicit-solvent, all-atom simulations for each of 1390 non-membrane protein monomers. The proteins are chosen from the PDB as representatives of all available ECOD domains and are thus structurally non-redundant. We split the dataset into 1265 training, 39 validation, and 82 test proteins by PDB release date following Jing et al. (2024). At training time, we randomly select a protein, select one of the three replicates, subsample every 40 frames, obtaining a training target with 250 frames. We train with random crops of up to 256 residues, but draw samples for the full protein at inference time. To compute statistical similarity of the MDGEN ensembles with the ground truth MD ensembles, we compare the 250 frames with 30k pooled frames from all three trajectories. Baseline metrics and runtimes for AlphaFlow and MSA subsampling are taken directly from Jing et al. (2024). Analysis and visualization code for Table 4 and Figure 6 are provided courtesy of Jing et al. (2024). ", "page_idx": 17}, {"type": "text", "text": "Runtime MD runtimes in Table 2 are tabulated on a NVIDIA T4 GPU. All MDGEN experiments are carried out on NVIDIA A6000 GPUs. AlphaFlow and MSA subsampling runtimes in Table 4 are tabulated on NVIDIA A100 GPUs by Jing et al. (2024). ", "page_idx": 17}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/a3ad16c1b6df16c95bb027b9c3e4a77ad79830ec7405aa52ef0d678b121b07ea.jpg", "img_caption": ["C.1 Forward Simulation ", "Figure 7: Additional backbone torsion angle distributions (orange from MD, blue from samples) and free energy surfaces along the top two TICA components for 10 randomly chosen test peptides. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/d2626a83246b43bfe0befc84910308bd1e6f9784cf5395284576f28bd9a6697b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 8: Flux matrices between MSM metastable states computed from reference MD trajectories (upper right) and MDGEN trajectories (bottom left) for 10 random test peptides (the matrices are symmetric). Cells are colored by the square root of the flux, with darker indicating high flux. The Spearman correlation between the entries is shown. ", "page_idx": 18}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/5719d03650831c5ede42ff8e5e4e44ec9433d24cbf2d01e69f8d065102fc933c.jpg", "img_caption": ["Figure 9: Four of 1000 transition paths of MDGEN for several tetrapeptides in the test set. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/db175f85fe7c5a56320183e65adb8c1b3f8a739b917699ed171b8fbaa3391312.jpg", "img_caption": ["Figure 10: Recovery of fast dynamics via trajectory upsampling for random test peptides. $(L e f t)$ Autocorrelations of each torsion angle from ( ) the original 100 fs-timestep trajectory, $(\\bullet)$ the subsampled $10~\\mathrm{{ns}}$ -timestep trajectory, and $(\\cdots)$ the reconstructed 100 fs-timestep trajectory (all length $100\\;\\mathrm{ns}$ ). (Right) Dynamical content as a function of timescale from the upsampled vs. ground truth trajectories, stacked for all torsion angles (same color scheme). The subsampled trajectory contains only the shaded region and our model recovers the unshaded region. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/3a04dec605224febd6ac70d60d707ffc41ded031cd8d7baa30448ddbbcbfe049.jpg", "img_caption": ["Figure 11: For six tetrapeptides, we show the states that we chose in our design experiments when designing transitions between the highest flux states. Column 1 shows the flux matrix with zeros on the diagonal. Column 2, the free energy surface of a 100 ns simulation and the selected start and end states based on the highest flux in the flux matrix. Column 3, the MSM that was built from the MD simulation. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.5 Structural Validation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In addition to distributional similarity and dynamical content, we also assess the frequency of clashes or high-energy structures in MDGEN forward simulation rollouts. Specifically, we compute the distributions of: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The closest distance between any pair of nonbonded atoms   \n\u2022 Nonbonded energy (Coulomb $^+$ Lennard-Jones)   \n\u2022 Torsional energy   \n\u2022 Heavy atom bond lengths   \n\u2022 Radius of gyration ", "page_idx": 22}, {"type": "text", "text": "These distributions are shown and compared to the ground truth in Figure 12. We find that the vast majority of MDGEN structures are of high quality (i.e., clashes are rare) and adhere closely to the ground truth distributions. ", "page_idx": 22}, {"type": "image", "img_path": "yRRCH1OsGW/tmp/7c9f4e2eefba43dcb47e6ba67edbad4ac8a3b70d969a87d02bb5a7032f63b36b.jpg", "img_caption": ["Figure 12: Histograms of various structural validation metrics between MDGEN forward simulation and reference trajectories, pooled across all test tetrapeptides. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.6 Additional Comparisons ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we compare MDGEN to Timewarp (Klein et al., 2024) and ITO (Schreiner et al., 2024), generative models for autoregressively rolling out surrogate MD trajectories. Note that these comparisons are limited to the forward simulation task as Timewarp and ITO are not capable of solving the other tasks. ", "page_idx": 22}, {"type": "text", "text": "\u2022 For Timewarp (Klein et al., 2024), we use the 4AA model with weights from the authors and sample 100 ns trajectories by running 2000 inference steps with timestep 50 ps. We do not use MH acceptance steps as the authors found exploration of the energy landscape to be much more effective without them.   \n\u2022 For ITO (Schreiner et al., 2024), transferable models across tetrapeptides are not available. We therefore re-train ITO on our tetrapeptide dataset with timesteps of 500 ps. We then run 200 inference steps to sample 100 ns trajectories. ", "page_idx": 22}, {"type": "text", "text": "For both methods, we observe that trajectories are unstable without further intervention. To bolster the baselines, we run OpenMM (Eastman et al., 2017) relaxation steps between each timestep to proceed with further analysis. We note that the Timewarp authors, in lieu of relaxation, rejected steps with an energy increase of $300\\,\\mathrm{kJ}\\,/\\,\\mathrm{mol}$ (Klein et al., 2024); however, we found that this strategy would reject the majority of proposed steps on generic tetrapeptides. In Figure 13, we visualize the free energy surfaces and torsion angle distributions for several peptides from Timewarp and ITO compared with MDGen. Table 5 shows the Jensen-Shannon divergences from the forward simulation rollouts across all test peptides (c.f. Table 2). Qualitatively and quantitatively, our model obtains better consistency with the ground-truth free energy surfaces. ", "page_idx": 22}, {"type": "table", "img_path": "yRRCH1OsGW/tmp/79b2398ec7306f08fc06b0c55707376ff34b28d305fe664203336cb686399e86.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 5: Comparison of MDGEN, Timewarp, and ITO in terms of the JSD between sampled and ground-truth distributions along various collective variables in the forward simulation setting. ", "page_idx": 23}, {"type": "table", "img_path": "yRRCH1OsGW/tmp/56cf6def8c18890a403c84c7afee6247ae213170e623362e5d3d067c394baad5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We outline all contributions of the paper, emphasizing the ones with robust support and qualifying the ones for which the support is preliminary. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: See Section 5 for a discussion of limitations. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide details sufficient to reproduce our model and experiments in Appendix A and Appendix B. In particular, the model architecture is detailed in Algorithm 1. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We have released our code and data under MIT license. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Training and test details are provided throughout the main text, figure and table captions, and Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our results are reported across a large test set for the task $(n=100)$ ) and focus more on the new qualitative capabilities that we introduce rather than quantitative improvements on benchmarks. Thus, we did not feel it was necessary to report error bars but would be happy to provide them upon request. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our models are small by modern standards and we did not feel it was necessary to explicitly highlight the compute time required for experiments. We provide some information on computational resources in Appendix B. Further, the runtimes reported in Tables 2 and 4 provide an idea of the efficiency of our models. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All authors have read and adhered to, in every respect, the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of AI for Science. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release data or models that have a high risk for misuse. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the original citations of major open-source software packages and datasets used in our work. We did not feel it was necessary to explicitly mention their open-source licenses as all are publicly available. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Model code and weights for reproducibility are provided via a public repository with instructions for replicating training and evaluation. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]