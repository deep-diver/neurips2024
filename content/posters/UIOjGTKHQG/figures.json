[{"figure_path": "UIOjGTKHQG/figures/figures_4_1.jpg", "caption": "Figure 1: The framework the proposed D-LLMs. The inference paradigm of dynamic decisions for transformer layers is shown in Fig. 1a. The design of dynamic execution decision modules is shown in Fig. 1b. The mask in multi-head self-attention with eviction strategy is shown in Fig. 1c.", "description": "This figure illustrates the framework of D-LLMs, a dynamic inference mechanism for large language models.  It shows three parts: (a) overall architecture with dynamic decision modules before each transformer layer, (b) the structure of a dynamic decision module, and (c) how the KV-cache eviction mask works with the causal mask in the self-attention module. The dynamic decision module determines whether a transformer layer should be executed or skipped based on input features.  The KV-cache eviction strategy optimizes memory usage by excluding skipped layers from subsequent attention calculations.", "section": "3 Methods"}, {"figure_path": "UIOjGTKHQG/figures/figures_5_1.jpg", "caption": "Figure 2: The performance against computational cost of D-LLMs on three datasets. The figures show that reducing around 40% to 60% computational cost achieves the best trade-off.", "description": "This figure compares the performance (accuracy or perplexity) against computational cost (FLOPs) for three different datasets (MaWPS, OBQA, and SAMSum) using D-LLMs and other methods (LoRA, Shortened-LLaMA, Ada-Infer, MoD). It demonstrates that D-LLMs achieve a good balance between performance and reduced computational cost, showing superior performance with around 40-60% less FLOPs compared to the baseline.", "section": "3.4 KV-Cache Eviction Strategy"}, {"figure_path": "UIOjGTKHQG/figures/figures_8_1.jpg", "caption": "Figure 3: Execution ratios on different layers of three grammatical terms. Blue dots refer to number and math symbols, e.g., '1, +, \u00d7'. Red dots refer to subject terms, e.g., 'She, He, They'. Green dots refer to modal verbs, e.g., \u2018may, should, might'. The distance of a dot to the center represents executing ratio. The red circle is probability of 100% and the blue circle is the average ratios.", "description": "This figure visualizes the execution ratios of different grammatical terms across various layers of the D-LLM model.  Each subplot represents a different layer, showing the distribution of execution ratios for number/math symbols (blue), subject terms (red), and modal verbs (green). The distance from the center of each circle indicates the execution ratio, with points closer to the edge having a higher execution ratio.  The overall distribution of execution ratios for each grammatical category provides insights into the role of different layers in processing various linguistic elements.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "UIOjGTKHQG/figures/figures_8_2.jpg", "caption": "Figure 4: The execution ratios of different layers visualizations on Benchmarks and Questions. Fig. 4a shows the execution ratios of different benchmarks on respective layers. A deeper color refers to a higher execution ratios. Fig. 4b shows six standard questions' execution ratios over layers from MMLU. The Y-axis is the execution ratios and X-axis is the layer index.", "description": "This figure visualizes the execution ratios of different layers in the D-LLM model for various tasks and questions.  Panel (a) shows a heatmap illustrating the average execution ratios across layers for different benchmark datasets.  Warmer colors indicate higher execution ratios, suggesting that certain layers are used more frequently for specific tasks. Panel (b) presents line graphs showing the execution ratios across layers for six example questions from the MMLU benchmark. The lines reveal how the utilization of different layers varies depending on the specific question, reflecting task complexity and information content.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "UIOjGTKHQG/figures/figures_8_3.jpg", "caption": "Figure 4: The execution ratios of different layers visualizations on Benchmarks and Questions. Fig. 4a shows the execution ratios of different benchmarks on respective layers. A deeper color refers to a higher execution ratios. Fig. 4b shows six standard questions' execution ratios over layers from MMLU. The Y-axis is the execution ratios and X-axis is the layer index.", "description": "This figure visualizes the execution ratios of different layers in the D-LLM model for various tasks and questions.  Panel (a) shows the execution ratios across layers for different benchmark datasets, revealing that different types of tasks (e.g., Q&A, summarization, common sense reasoning) utilize different layers to varying degrees. Panel (b) further illustrates this point by showing the execution ratios for individual questions from the MMLU benchmark, highlighting that similar questions tend to have similar layer usage patterns. This demonstrates the adaptive nature of the D-LLM's resource allocation strategy.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "UIOjGTKHQG/figures/figures_13_1.jpg", "caption": "Figure 5: The transformer layers to be executed for tasks of different difficulties in D-LLMs. fig. 5a is the execution decisions, when D-LLM answers \"How can I develop my critical thinking skills?\". fig. 5b is the execution decisions, when D-LLM answers \"Can you explain Fermat's Last Theorem?\". The second question is more difficulty than the first one, therefore, utilizing more transformer layers. A filled block refers to a token executing the corresponding layer, while an empty block refers to a token skipping the corresponding layer.", "description": "This figure visualizes the difference in layer execution patterns between simple and complex tasks using the D-LLM model.  The top half shows the execution pattern for the question, \"How can I develop my critical thinking skills?\", while the bottom half shows the pattern for the more complex question, \"Can you explain Fermat's Last Theorem?\" The figure uses a heatmap to represent layer execution decisions for each token, where filled blocks indicate that a layer was executed for that token, and empty blocks indicate that it was skipped.  The difference in execution patterns highlights the adaptive nature of D-LLM, showing how it utilizes more layers for more complex tasks.", "section": "A.1 Visualization on Tasks of Different Difficulties"}, {"figure_path": "UIOjGTKHQG/figures/figures_14_1.jpg", "caption": "Figure 6: Execution Decisions over Layers under different ratios on MaWPS and OBQA. The vertical coordinate is the computational cost. The horizontal coordinate is the indices of layers. A dark color refers to most tokens execute the layer, while a light color refers to most tokens skip the layer.", "description": "This figure visualizes the execution ratios of layers under different computational costs for the MaWPS and OBQA datasets.  The vertical axis represents the computational cost (ranging from 27% to 80%), and the horizontal axis represents the layer indices (from 2 to 30). Each cell in the heatmap displays the execution ratio of a specific layer at a given computational cost. Darker colors indicate that a higher proportion of tokens execute that layer at that cost, while lighter colors indicate that a smaller proportion of tokens execute that layer.", "section": "A.2 Execution Ratios under Different Cost"}]