[{"type": "text", "text": "Soft Tensor Product Representations for Fully Continuous, Compositional Visual Representations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bethia Sun Maurice Pagnucco Yang Song UNSW, Sydney UNSW, Sydney UNSW, Sydney bethia.sun@unsw.edu.au morri@unsw.edu.au yang.song1@unsw.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Since the inception of the classicalist vs. connectionist debate, it has been argued that the ability to systematically combine symbol-like entities into compositional representations is crucial for human intelligence. In connectionist systems, the field of disentanglement has emerged to address this need by producing representations with explicitly separated factors of variation (FoV). By treating the overall representation as a string-like concatenation of the inferred FoVs, however, disentanglement provides a fundamentally symbolic treatment of compositional structure, one inherently at odds with the underlying continuity of deep learning vector spaces. We hypothesise that this symbolic-continuous mismatch produces broadly suboptimal performance in deep learning models that learn or use such representations. To fully align compositional representations with continuous vector spaces, we extend Smolensky\u2019s Tensor Product Representation (TPR) and propose a new type of inherently continuous compositional representation, Soft $T P R$ , along with a theoretically-principled architecture, Soft TPR Autoencoder, designed specifically for learning Soft TPRs. In the visual representation learning domain, our Soft TPR confers broad benefits over symbolic compositional representations: state-of-the-art disentanglement and improved representation learner convergence, along with enhanced sample efficiency and superior low-sample regime performance for downstream models, empirically affirming the value of our inherently continuous compositional representation learning framework. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Compositional structure, capturing the property of being decomposable into a set of constituent parts, is ubiquitous in our surroundings \u2013 from the recursive application of syntax in language, to the parsing of richly complex visual scenes into their constituent parts. Given the central role such structure plays in our understanding of the world, it is highly intuitive that deep learning representations also embody compositional structure. Indeed, empirical evidence highlights the usefulness of explicitly compositional representations, showcasing a multitude of beneftis, including increased interpretability [10, 12], reduced sample complexity [30, 33], increased fairness [20, 25, 41], and improved performance in out-of-distribution generalisation [33, 48, 50]. ", "page_idx": 0}, {"type": "text", "text": "We consider the following, intuitive notion of compositional representations. A representation of compositionally-structured data is a compositional representation if it has a structure that faithfully reflects the compositional structure of the represented data [49]. In the visual representation learning domain, data is clearly compositionally-structured, as images can be decomposed into a set of constituent factors of variation (FoVs), e.g., {magenta floor,orange wall, aqua object colour, oblong object shape $\\}$ for the image in Figure 1. ", "page_idx": 0}, {"type": "text", "text": "A widely explored representation learning framework producing explicitly compositional representations is that of disentanglement. We adopt the conventional [5, 25, 33, 43], intuitive definition of a disentangled representation, which states that a representation, $\\psi(x)$ , is disentangled if each of the underlying FoVs can be cleanly separated into a distinct dimension (or contiguous subset of dimensions) of $\\psi(x)$ , or, in other words, if each FoV has a 1-1 correspondence with a distinct part of the representation [43]. Framed in this way, it is apparent that disentangled representations are explicitly compositional by nature. The majority of state-of-the-art disentanglement approaches use a variational autoencoder backbone, and rely on weak supervision [13, 22, 31, 33, 35], or a penalisation of the aggregate posterior $\\begin{array}{r}{\\int q(z|x)p(x)d\\bar{x}}\\end{array}$ [10, 14, 17, 24, 26, 32] to promote disentanglement. More recent approaches depart from the restrictive assumptions of a variational framework, and instead use standard autoencoding [47], or energy-function based optimisation [36], with additional inductive biases to encourage disentanglement. Despite the diversity of methods characterising existing work, we make a crucial observation which unifies them together: by enforcing the 1-1 correspondence between FoVs and distinct parts of the representation, existing approaches [10, 13, 14, 17, 22, 24, 26, 31, 32, 33, 35, 36, 47] essentially produce compositional representations corresponding to a concatenation of scalar-valued or vector-valued FoV tokens, as illustrated in Figure 1a. This concatenative approach enforces a rigid, slot-based representational structure that constraints how information can be represented and combined, and mirrors symbolic systems, where distinct symbols occupy discrete slots within a representation. We argue that this fundamentally symbolic approach creates a deep incompatibility with the inherent continuity of the vector spaces underlying deep learning for the following reasons: ", "page_idx": 1}, {"type": "text", "text": "1. Misalignment with Gradient-Based Learning: The symbolic approach\u2019s introduction of discrete representational slots for each FoV introduces non-differentiable boundaries between FoV slots, challenging the efficacy of gradient-based learning, which thrives on smooth and continuous transformations. For example, when modifying a given FoV, this symbolic structure restricts gradient propagation to the dimensions associated solely to the corresponding representational slot, inhibiting the smooth permeation of gradients across the entire vector space. Managing these discrete, slot-based boundaries thus fragments gradient flow across the vector space, producing abrupt, discontinuous transitions that may potentially complicate learning. ", "page_idx": 1}, {"type": "text", "text": "2. Restrictive / Incompatible Structure: By allocating distinct representational slots for each FoV, the symbolic approach imposes a rigid representational structure that prevents the representation from exploiting the expressivity inherent in continuous vector spaces. More concretely, this slot-based framework prevents the encoding of FoVs as flexible combinations of basis vectors that span the entire representational space \u2013 an approach that is not only more intuitive, but also critical for capturing rich interactions and complex dependencies among FoVs. By failing to allow for this flexibility, the symbolic approach prevents the representation from leveraging the full expressivity of its underlying vector space. ", "page_idx": 1}, {"type": "text", "text": "Critically, we hypothesise that the fundamental incompatibility between the symbolic treatment of compositional structure provided by disentanglement, and the continuous vector spaces of deep learning produces suboptimal behaviour in the models that learn or use these representations. This hypothesis prompts the following question: can we instead represent compositional structure in an inherently continuous manner? A continuous compositional representation would yield the representation, $\\psi(x)$ , by continuously combining the FoVs, rather than maintaining a discrete, slotbased separation, as in the symbolic approach. The continuous approach to representing compositional structure is thus, a more mathematically intuitive framework in the context of deep learning. ", "page_idx": 1}, {"type": "text", "text": "Pioneered by Smolensky, the Tensor Product Representation [3] is a specific representational form that encodes compositional structure in an inherently continuous manner. At the crux of it, TPRs are formed by continuously blending the FoVs together into the overall representation, in a manner analogous to superimposing multiple waves together to produce a complex waveform, as illustrated in Figure 1b. For a representation to qualify as a TPR, it must adhere to a highly specific mathematical form, which confers upon the TPR valuable theoretical properties (elaborated on in Section 3.2), but also imposes two major limitations (see B.1 for further details). First, as depicted by the stars in Figure 1c, only a discrete subset of points in the underlying representational space, V\u02dc , satisfies the stringent mathematical criteria to qualify as TPRs. Consequently, to learn TPRs, representation learners must map from the data manifold onto this discrete subset, which constitutes a highly constrained and inherently challenging learning task. Second, the TPR specification enforces a strict, algebraic definition of compositional structure, limiting the TPR\u2019s ability to faithfully represent real-world data which is often quasi-compositional, only approximately adhering to a rigid, formal definition of compositionality. Historically, these limitations have confined TPR learning to formal domains characterised by explicit, algebraic structure \u2013 as evidenced by the near exclusive deployment of TPRs in language [19, 23, 28, 34, 52] \u2013 and, to contexts where strong supervision from highly structured downstream tasks is available to steer the representation learning process [23, 28, 38, 51]. To negate these drawbacks and extend continuous compositional representations to weakly supervised, non-formal domains, we propose Soft TPR, a new, inherently continuous compositional representation that can be thought of as a continuous relaxation of the traditional TPR, as illlustrated by the translucent circular regions in Figure 1c. At its core, the Soft TPR is designed to promote representational flexibility and ease of learning while simultaneously preserving the structural and mathematical integrity of the traditional TPR. We additionally introduce Soft TPR Autoencoder, a theoretically-principled weakly-supervised architecture for learning Soft TPRs, which we use to operationalise the Soft TPR framework in the visual representation learning domain. ", "page_idx": 1}, {"type": "image", "img_path": "oEVsxVdush/tmp/240971aabffad30d842b4e6d717024134b3ef535c6adf2e1ebfac0f419a9286c.jpg", "img_caption": ["Figure 1: (a) Disentangled representations can be conceptualised as a concatenation of FoV tokens (coloured blocks), effectively enforcing a string-like, symbolic compositional structure, where each FoV is allocated to a discrete slot in the representation. We instead, consider a continuous representation of compositional structure, (b), where the FoVs (first 6 waves) are continuously superimposed together to produce the overall representation, $\\psi(x)$ (in red). (c) Only a subset of points (stars) in the underlying representational space (rainbow manifold) satisfy the TPR specification. The Soft TPR relaxes this, capturing larger, continuous regions of the underlying representational space (the translucent circles), while preserving the TPR\u2019s key properties. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our main contributions are threefold: i) We propose a novel compositional representation learning framework, introducing the inherently continuous Soft TPR compositional form, alongside a dedicated, weakly-supervised architecture, Soft TPR Autoencoder, for learning this form. ii) Our framework is the first to learn continuous compositional representations in the non-formal domain of vision. iii) We empirically affirm the far-reaching beneftis of enhanced vector space alignment produced by the Soft TPR framework, demonstrating that Soft TPRs achieve state-of-the-art disentanglement, accelerate representation learner convergence, and provide downstream models with enhanced sample efficiency and superior low-sample regime performance. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Disentanglement: In aiming to produce explicitly compositional representations without strong supervision, our work shares the same objective as disentangled representation learning. Prior to the highly influential work of [25], which proved the impossibility of learning disentangled representations without supervision or other inductive biases, disentangled representations were learnt in a completely unsupervised fashion [8, 10, 14, 17, 24, 26, 37]. Our use of weak supervision is inspired by the work [13, 22, 31, 33, 35] relating to this highly influential impossibility result. In particular, we leverage the type of weak supervision termed \u2018match pairing\u2019 [35], where pairs, $(x,x^{\\prime})$ , differing in values for a subset of known FoVs are presented to the model, to incentivise disentanglement. Our work, however, fundamentally diverges from all disentanglement work we are aware of, by adopting an inherently continuous representation of compositional structure, which contrasts with the symbolic representations of compositional structure characterising existing work. ", "page_idx": 2}, {"type": "text", "text": "TPR-based Work: Existing TPR-based approaches generate continuous representations of compositional structure by producing an element with the explicit mathematical form of a TPR. To learn this highly specific form, these approaches rely on the algebraic characterisation of compositionality present in formal domains, such as mathematics [38], or language [19, 23, 28, 34, 52] in addition to strong supervision signals from highly structured downstream tasks, such as part-of-speech tagging [23], and answering structured language [28, 51] or mathematics questions [38]. In contrast, our Soft ", "page_idx": 2}, {"type": "text", "text": "TPR eases these stringent constraints by offering a relaxed specification of inherently continuous compositional structure. This allows our approach to extend continuous representations of compositional structure to an orthogonal and less structured domain, that of visual representation learning, while also reducing reliance on annotated data by instead using weak supervision to learn this relaxed representational form. ", "page_idx": 3}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 A Formal Framework for Compositional Representations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We adopt a generalised, non-generative version of the definition of compositional representations from [49]. Data $x\\in X$ is compositionally-structured if there exists a decomposition function $\\beta:$ $X\\rightarrow A_{1}\\times...\\times A_{n}$ decomposing $x$ into constituent parts, i.e. $\\bar{\\beta}(x)=\\{a_{1},\\ldots,a_{n}\\}$ , where $a_{i}\\in A_{i}$ . A map $\\psi:X\\rightarrow V_{F}$ produces a compositional representation if $\\psi(x)=C(\\psi_{1}(a_{1}),\\dots,\\psi_{n}(a_{n}))$ where $\\psi_{i}\\;:\\;A_{i}\\;\\rightarrow\\;V_{i}$ denote component functions that independently embed the parts of $x$ into vector spaces, and $C:V_{1}\\times...\\times V_{n}\\to V_{F}$ denotes a composition function that combines the embedded parts of $x$ together to form the overall representation. Intuitively, this definition enforces a faithful structural correspondence between the constituency structure of the data, $x$ (i.e., the parts, $\\{a_{1},\\ldots,a_{n}\\})$ and the constituency structure of the representation, $\\psi(x)$ (i.e., the embedded parts, $\\{\\psi_{1}(a_{1}),\\...\\,,\\psi_{n}(a_{n})\\})$ (assuming $C$ is invertible). ", "page_idx": 3}, {"type": "text", "text": "We formalise a symbolic compositional representation, $\\psi_{s}(x)$ , as a compositional representation where $C$ is a concatenation operation. Thus, $\\psi_{s}(x)=\\left(\\psi_{1}(a_{1})^{T},\\dots,\\psi_{n}(a_{n})^{T}\\right)^{T}$ for any symbolic compositional representation, $\\psi_{s}(x)$ . Clearly, the aforementioned disentanglement methods [8, 10, 13, 14, 17, 22, 24, 26, 31, 32, 33, 35, 36, 37, 47] all fit this framework. While we observe that the fundamentally symbolic definition of $C$ as concatenation produces an inherent misalignment with the continuous vector spaces of deep learning, it has one notable benefti: the embedded FoVs, $\\{\\psi_{i}(a_{i})\\}$ , can be easily recovered from the representation, $\\psi_{s}(x)$ , by simply partitioning $\\psi_{s}(x)$ . ", "page_idx": 3}, {"type": "text", "text": "It is highly intuitive that for any compositional representation, $\\psi(x)$ , to be broadly useful, the FoVs, $\\{a_{i}\\}$ , should be easily recoverable from $\\psi(x)$ (here we assume the $\\psi_{i}$ \u2019s are invertible, and so, that this property corresponds to being able to recover the representational components, $\\{\\psi_{i}(a_{i})\\}$ from $\\psi(x))$ . We thus explore whether an alternative $C$ exists that simultaneously 1) combines the embedded FoVs into the overall representation in an inherently continuous manner and 2) preserves the direct recoverability of the embedded FoVs. ", "page_idx": 3}, {"type": "text", "text": "3.2 The TPR Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "TPR [3] is a specific type of representation that is compositional, continuous, and under certain conditions, ensures the direct recoverability of the embedded parts $\\{\\psi_{i}(a_{i})\\}$ from the overall representation. We briefly introduce essential aspects of the framework, deferring further details and formal proofs to Appendix A. The TPR framework views compositionally-structured objects as possessing a number of (potentially infinite) roles1, where each role is bound to a corresponding fliler. It thus defines the constituent parts $\\{a_{i}\\}$ of any compositionally-structured object as a set of role-fliler bindings. This role-fliler binding formalism has predominantly been applied in the natural language domain [19, 28, 34, 52], with flilers often corresponding to words and roles to grammatical categories (e.g., the word cat as a filler, and the the category noun as a role). We translate this formalism into the domain of visual representation learning by informally equating roles as FoV types, and flilers as FoV values, e.g., {floor colour, wall colour, object colour, object size, object shape, orientation $\\}$ and $\\{$ blue, magenta, orange, green, small, medium, large, oblong, cube, . $\\cdot\\cdot\\cdot\\}$ respectively for the Shapes3D domain of Figure 1. The binding of a filler, $f$ , from a set $F$ of $N_{F}$ fillers, to a role, $r$ , from a set $R$ of $N_{R}$ roles, such as the filler magenta to the role object colour conveys a sort of filler-specific, role-modulated semantic content, and is denoted by $f/r$ . The compositional structure of the image, $x$ , in Figure 1 would thus correspond to the following set of role-filler bindings: $\\beta(x)\\!=\\!\\left\\{\\begin{array}{l l}\\end{array}\\right.$ {magenta/floor colour, orange/wall colour, aqua/object colour, large/object size, oblong/object shape}. ", "page_idx": 3}, {"type": "text", "text": "To produce the TPR, the roles and flilers for each binding in $x$ are independently embedded using role and fliler embedding functions, $\\xi_{R}:R\\rightarrow V_{R},\\xi_{F}:F\\stackrel{\\textstyle.}{\\rightarrow}V_{F}$ respectively. To produce an embedding of the binding $f/r$ , a tensor product, which we denote by $\\otimes$ , is then taken over the embedded role, $\\xi_{R}(r)$ , and embedded fliler, $\\xi_{F}(f)$ , comprising the binding. Finally, a summation is performed over all embedded bindings in $x$ to produce the overall representation. More concretely, the TPR, $\\psi_{t p r}(x)$ , is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{t p r}(x):=\\sum_{i}\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $m:\\{1,\\dots,N_{R}\\}\\rightarrow\\{1,\\dots,N_{F}\\}$ is a matching function associating each of the roles to the unique filler it binds to in the decomposition of $x^{\\,2}$ . ", "page_idx": 4}, {"type": "text", "text": "We now place the TPR in the formal framework of Section 3.1, by observing that the TPR defines the component functions as $\\psi_{i}(f_{m(i)},r_{i}):=\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i})$ , and the composition function, $C$ , as ordinary vector space addition. As the TPR does not concatenate, but rather, additively superimposes all representational components $\\{\\psi_{i}(f_{m(i)},r_{i})\\}$ together to produce the overall representation, it instantiates an inherently continuous representation of compositional structure. Defining $C$ as ordinary addition, however, prompts the question of whether the summed up representational components $\\{\\psi_{i}(f_{m(i)},r_{i})\\}$ can be recovered from the representation, $\\psi_{t p r}(x)$ . Remarkably, due to the special form of the TPR, each representational component $\\psi_{i}(f_{m(i)},r_{i})$ corresponding to an embedded role-filler binding, $\\xi_{F}\\big(f_{m(i)}\\big)\\otimes\\xi_{R}\\big(r_{i}\\big)$ can be faithfully recovered from the TPR through a process referred to as unbinding (see A.2 for more details). More concretely, provided that all role embedding vectors $\\{\\xi_{R}(r_{i})\\}$ are linearly independent, the embedded fliler bound to the $i$ -th role can be unbound from the representation, $\\psi_{t p r}(x)$ , by taking a (tensor) inner product between $\\psi_{t p r}(x)$ and the $i$ -th unbinding vector, $u_{i}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{t p r}(x)u_{i}=\\left(\\sum_{i}\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i})\\right)u_{i}=\\xi_{F}(f_{m(i)}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $u_{i}$ is a vector corresponding to the $i$ -th column of $U^{T}$ , the (left) inverse of the matrix formed by taking all (linearly independent) role embeddings as columns. By repeating the unbinding procedure using each of the $N_{R}$ unbinding vectors, the embedded fillers bound to each role, and hence, the embeddings $\\big(\\xi_{F}\\big(f_{m(i)}\\big),\\xi_{R}\\big(r_{i}\\big)\\big)$ comprising each binding embedding, $\\xi_{F}\\big(f_{m(i)}\\big)\\otimes\\xi_{R}\\big(r_{i}\\big)$ , can be recovered from the overall representation, $\\psi_{t p r}(x)$ . Thus, provided that the linear independence condition is satisfied, the TPR represents a continuous compositional representation that retains the key benefti of symbolic compositional representations: the direct recoverability of the representational parts, $\\{\\psi_{i}(f_{m(i)},r_{i})\\}$ . ", "page_idx": 4}, {"type": "text", "text": "4 Methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Soft TPR: An Extension to the TPR Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The TPR\u2019s highly specific representational form, $\\begin{array}{r}{\\psi_{t p r}(x):=\\sum_{i}\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i})}\\end{array}$ , can only be satisfied by a discrete subset of points in the underlying representational space, $V_{F}\\otimes V_{R}$ . This imposes an arduous learning task on representation learners: to parameterise the highly constrained map from the data manifold to a discrete subset of points. This representational form additionally assumes a strict algebraic definition of compositionality that corresponds to a set of bindings, where each binding comprises a single role and a single fliler, precluding the TPR from representing quasicompositional objects that only approximately satisfy this strict, algebraic definition of compositional structure (e.g., French liaison consonants, where a weighted sum of multiple fillers, rather than a single filler, bind to a role [9]). Our primary insight is that both these drawbacks can be mitigated by continuously relaxing the TPR specification (see B.1). This relaxation allows for a wider variety of mappings within a \u2018cloud\u2019 around each TPR (represented by the translucent circular regions in Figure 1c), which eases the difficulty of representation learning. Furthermore, it relaxes the rigid role-filler based specification of compositional structure into a softer, less rigid notion, enabling the representation of nuanced, quasi-compositional data. We thus introduce the Soft TPR, a continuously relaxed, less stringently defined variant of the explicit TPR that simultaneously retains the TPR\u2019s key properties of 1) continuous compositional structure, and 2) direct recoverability of representational parts $\\{\\big(\\xi_{F}(f_{m(i)}),\\xi_{R}(r_{i})\\big)\\}$ from the overall representation. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Consider an element $z$ in $V_{F}\\otimes V_{R}$ , the vector space underlying TPRs produced by an arbitrary role embedding function $\\xi_{R}:R\\to V_{R}$ and an arbitrary filler embedding function $\\xi_{F}:F\\to V_{F}$ . If $z$ is sufficiently \u2018close\u2019 to some TPR, $\\psi_{t p r}{^3}$ , quantified by $||z-\\psi_{t p r}||_{F}<\\epsilon$ , where $||A||_{F}$ denotes the Frobenius norm of the rank-2 tensor, $A$ , and $\\epsilon$ is some small, scalar valued quantity, then this distance metric induces the approximation $z\\approx\\psi_{t p r}$ . By performing unbinding of the $i$ -th fliler to both sides of the approximation, we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z u_{i}\\approx\\psi_{t p r}u_{i}=\\displaystyle\\left(\\sum_{i}\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i})\\right)u_{i}=\\xi_{F}(f_{m(i)}),}\\\\ &{\\phantom{z a_{i}\\xi_{F}(f_{m(i)})}=\\xi_{F}(f_{m(i)})+\\epsilon_{i}=:\\tilde{f}_{i},\\mathrm{~where~}\\epsilon_{i}=z u_{i}-\\xi_{F}(f_{m(i)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, performing unbinding on an element $z$ in $V_{F}\\otimes V_{R}$ where the sufficient closeness condition holds recovers a soft filler embedding, $\\tilde{f}_{i}$ , that approximates the true filler embedding, $\\xi_{F}{\\left(f_{m(i)}\\right)}$ , of the filler bound to role $r_{i}$ for $\\psi_{t p r}$ with approximation error $\\epsilon_{i}$ . We define such elements as Soft $T P R s$ , noting that these elements both 1) softly approximate the continuous compositional structure captured by some explicit TPR, $\\psi_{t p r}$ , as $z$ is sufficiently \u2018close\u2019 to $\\psi_{t p r}$ based on the chosen distance metric, and 2) approximately preserve the recoverability of the representational components $\\{(\\xi_{F}(f_{m(i)}),\\xi_{R}(r_{i}))\\}$ of the explicit TPR, $\\psi_{t p r}$ , they approximate, with the only difference being that soft filler embeddings, $\\tilde{f}_{i}$ are returned in place of the actual filler embeddings, $\\xi_{F}{\\left(f_{m(i)}\\right)}$ . Defining Soft $T P R s$ in this way 1) provides a less restrictive representational specification that can be satisfied by any arbitrary element from $V_{F}\\otimes V_{R}$ provided that, for some explicit TPR, $\\psi_{t p r}$ , the sufficient closeness requirement, $||z-\\psi_{t p r}||_{F}<\\epsilon$ , holds, and 2) allows learned representations to embody a more flexible, relaxed notion of compositional structure. ", "page_idx": 5}, {"type": "text", "text": "4.2 Soft TPR Autoencoder: A Concrete Implementation of Learning Soft TPRs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We define our vector spaces of interest over the reals as $V_{F}:=\\mathbb{R}^{D_{F}}$ and $V_{R}:=\\mathbb{R}^{D_{R}}$ where $D_{F},D_{R}$ denote the dimensionality of the filler and role embedding spaces. The main insight underlying our method is that, as the Soft TPR is effectively any arbitrary element from a vector space4 $\\bar{\\mathbb{R}}^{D_{F}\\cdot D_{R}}$ that is sufficiently close to some explicit TPR, any $(D_{F}\\cdot D_{R})$ -dimensional vector produced by an encoder in a standard autoencoding framework can be treated as a Soft TPR candidate. This suggests that a simple autoencoding framework only needs to be slightly modified to produce Soft TPRs. Briefly speaking, our Soft TPR Autoencoder contains a standard encoder, $E$ , the TPR decoder, and a standard decoder, $D$ , where the encoder output, $z$ , corresponds to the Soft TPR. At a high level, our framework aims to ensure two properties: 1) representational form, and 2) representational content. Representational form requires that the encoder output, $z$ , has the desired Soft TPR form (i.e. that $||z-\\psi_{t p r}||_{F}<\\epsilon$ for some TPR, $\\psi_{t p r},$ ). However, having a Soft TPR form alone is insufficient; the representation produced by the autoencoder must also reflect the true role-filler content of the data to be a good representation, as required by the aim of representational content. These 2 properties are (mostly) respectively achieved using the unsupervised and weakly supervised components of our method. ", "page_idx": 5}, {"type": "text", "text": "Representational Form: to encourage the autoencoder to produce elements that are Soft TPRs, we penalise the Euclidean distance $||z-\\psi_{t p r}^{*}||_{2}$ between the encoder output, $z$ , and the explicit TPR, $\\psi_{t p r}^{*}$ , that $z$ best approximates. To obtain $\\psi_{t p r}^{*}$ , needed to penalise the above distance, we derive an explicit analytical form for $\\psi_{t p r}^{*}$ and construct elements satisfying this analytical form using a role embedding matrix, $M_{\\xi_{R}}$ containing $N_{R}\\,D_{R}$ -dimensional role embedding vectors, $\\{\\xi_{R}(r_{i})\\}$ , and a filler embedding matrix, $M_{\\xi_{F}}$ containing $N_{F}$ $D_{F}$ -dimensional filler embedding vectors, $\\{\\dot{\\xi}_{F}(f_{i})\\}$ . To define an explicit analytical expression for $\\psi_{t p r}^{*}$ , we are guided by the intuition that the TPR that $z$ best approximates should have an explicit dependency on the soft fliler bindings of $z$ , and so, elect to ", "page_idx": 5}, {"type": "image", "img_path": "oEVsxVdush/tmp/5c4fd09e9c202544780b7d23751a575f4574ff2bb17e1c09d8c9e9de430c682c.jpg", "img_caption": ["Figure 2: Diagram illustrating the Soft TPR Autoencoder. We encourage the encoder $E$ \u2019s output, $z$ , to have the form of a Soft TPR by penalising its distance with the greedily defined, explicit TPR, $\\psi_{t p r}^{*}$ of Equation 5 that $z$ best approximates. $\\psi_{t p r}^{*}$ is recovered using a 3 step process performed by our TPR decoder (center rectangle): 1) unbinding, 2) quantisation, and 3) TPR construction. The decoder, $D$ , reconstructs the input image using $\\psi_{t p r}^{*}$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "use the following, greedily optimal definition: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\psi_{t p r}^{*}:=\\sum_{i}\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i}),\\mathrm{where~}m(i):=\\arg\\operatorname*{min}_{j}||\\tilde{f}_{k}-\\xi_{F}(f_{j})||_{2},\\mathrm{and~}\\tilde{f}_{k}:=z u_{i}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "That is, we define $\\psi_{t p r}^{*}$ as the TPR constructed from explicit filler embeddings $\\xi_{F}(f_{j})$ with the smallest Euclidean distance to the soft fliler embeddings $\\tilde{f}_{k}$ of $z$ . To construct elements satisfying (5), we use a 3-step process carried out by the novel TPR decoder we introduce, visible in Figure 2: 1) Unbinding: The unbinding module consists of a fixed semi-orthogonal role embedding matrix $M_{\\xi_{R}}$ and recovers the soft filler embeddings $\\{\\tilde{f}_{i}\\}$ associated with each encoder output, $z$ , by performing the TPR unbinding operation. We elaborate on our theoretically-informed reason for this choice of role embedding matrix, how the unbinding vectors are obtained, and for not backpropagating gradient to $M_{\\xi_{R}}$ in B.3.1. 2) Quantisation: The quantisation module, containing a learnable filler embedding matrix $M_{\\xi_{F}}$ , employs the VQ-VAE vector quantisation algorithm [11] to both 1) learn the explicit fliler embeddings, and 2) quantise the soft fliler embeddings $\\{\\tilde{f}_{1},\\dotsc,\\tilde{f}_{N_{R}}\\}$ produced by the unbinding module into the explicit filler embeddings $\\{\\xi_{F}(f_{1}),\\allowbreak...,\\allowbreak\\dot{\\xi_{F}}(f_{N_{R}})\\}$ with the smallest Euclidean distances. 3) TPR Construction: The TPR construction module recovers $\\psi_{t p r}^{*}$ by simply performing the TPR operation $\\begin{array}{r}{\\sum_{i}\\xi_{F}\\big(f_{m(i)}\\big)\\!\\otimes\\!\\xi_{R}\\big(r_{i}\\big)}\\end{array}$ over the fixed, semi-orthogonal role embedding vectors with the corresponding explicit filler embeddings produced by the quantisation module. To ensure the quantised filler embeddings $\\left\\{\\xi_{F}{\\big(}f_{m(i)}{\\big)}\\right\\}$ depend explicitly on the reconstructed image, we pass the output of the TPR decoder, $\\psi_{t p r}^{*}$ to the image decoder, $D$ , for reconstruction. The overall unsupervised loss, $\\mathcal{L}_{u}$ , is thus given by the following, where s denotes the stop-gradient operator, $\\beta$ a hyperparameter, and $\\mathcal{L}_{r}$ any suitable image-based reconstruction loss (we use $L_{2}$ ): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{\\Sigma}_{u}:=\\underbrace{||z-\\psi_{t p r}^{*}||_{2}^{2}}_{\\mathrm{Soft~TPR~penall}}+\\underbrace{\\mathcal{L}_{r}(x,D(\\psi_{t p r}^{*}))+\\sum_{i}\\frac{1}{N_{R}}\\left(||\\mathsf{s}[\\xi_{F}(f_{m(i)})]-\\tilde{f}_{i}||_{2}^{2}+\\beta||\\xi_{F}(f_{m(i)})-\\mathsf{s}[\\tilde{f}_{i}]||_{2}^{2}\\right)}_{\\mathrm{reconsmrion~loss}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Representational Content: While the unsupervised loss $\\mathcal{L}_{u}$ encourages the autoencoder to produce encodings $z$ with the desired Soft TPR form, it may not ensure that the content of $z$ accurately reflects the true role-filler semantics of the represented data. To address this, we introduce a weakly supervised loss to ensure that the explicit TPR, $z_{t p r}^{*}$ , which $z$ best approximates, reflects the groundtruth semantics of the image. We employ a match-pairing context similar to [13, 22, 33, 35], where image pairs $(x,x^{\\prime})$ share the same role-fliler bindings for all but one of the roles, $r_{i}$ , and the identity of $r_{i}$ is known, but not any of the flilers, or role-fliler bindings. Our intuition is that, for the role-fliler binding embeddings of $z$ to reflect the semantics of the represented image, the Euclidean distance between the quantised flilers of $x$ and $x^{\\prime}$ bound to role $r_{i}$ should be maximal, relative to the distances between the pairs of filler embeddings for all other roles $r_{j},j\\neq i$ . To encourage this, we apply the cross entropy loss corresponding to the 3rd term in Eq 7, where $\\Delta q$ denotes the $N_{R}$ -dimensional vector with each dimension $(\\Delta q)_{k}$ populated by the Euclidean distance between the quantised flilers of $x$ and $x^{\\prime}$ for role $r_{k}$ , and $l$ denotes the one-hot vector of dimension $N_{R}$ with the index for $r_{i}$ set to 1. Additionally, we apply a reconstruction loss using the TPRs, $\\psi_{t p r}^{s}(x)$ and $\\psi_{t p r}^{s}(x^{\\prime})$ , which are constructed by swapping the quantised fliler embeddings of $x$ and $x^{\\prime}$ bound to role $r_{i}$ , to reconstruct $x^{\\prime}$ and $x$ respectively. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Our final loss, $\\mathcal{L}$ , is a weighted sum over the unsupervised and weakly supervised loss components, where $\\lambda_{1}$ and $\\lambda_{2}$ are hyperparameters: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}:=\\mathcal{L}_{u}+\\lambda_{1}\\left(\\frac{1}{2}\\mathcal{L}_{r}(x,D(\\psi_{t p r}^{s}(x^{\\prime})))+\\frac{1}{2}\\mathcal{L}_{r}(x^{\\prime},D(\\psi_{t p r}^{s}(x)))\\right)+\\lambda_{2}\\mathrm{CE}(\\Delta q,l),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To assess the compositional representations produced by our Soft TPR framework, we perform evaluation along three dimensions: 1) Compositional Structure / Disentanglement: What is the degree to which Soft TPR representations achieve explicitly compositional structure? 2) Representation Learner Convergence Rate: Can representation learners learn the inherently continuous compositional structure embodied in the Soft TPR faster than symbolic alternatives? 3) Downstream Models: Does the enhanced vector space alignment produced by the Soft TPR facilitate beneftis for downstream models using compositional representations? ", "page_idx": 7}, {"type": "text", "text": "We benchmark against a suite of weakly supervised disentanglement baselines: Ada-GVAE [33], GVAE [22], ML-VAE [13], SlowVAE [39], and the GAN-based model of [35], which we henceforth refer to as \u2018Shu\u2019. These models all produce symbolic compositional representations corresponding to a concatenation of scalar-valued FoV tokens. Like our model, Ada-GVAE, GVAE, ML-VAE, and Shu are trained with paired samples $(x,x^{\\prime})$ sharing values for all but a subset of FoVs types (roles), $I$ . ML-VAE, GVAE, and Shu assume access to $I$ , the FoV types (roles) that differ between $x$ and $x^{\\prime}$ , matching our model\u2019s level of supervision, while Ada-GVAE does not. We thus modify Ada-GVAE (method detailed in Appendix C.2.2) for more direct comparability, denoting our modification by Ada-GVAE-k. In contrast, SlowVAE is trained with pairs of samples where all underlying FoV values change, and also assumes that this change can be characterised as a sample from a Laplacian. We additionally benchmark against 2 baselines producing vector-tokened compositional representations: COMET [36], and Visual Concept Tokeniser (VCT) [47]. These vector-tokened models cannot be directly compared to our model as they are fully unsupervised, however, we include them for completeness. We train 5 instances of each representation learning model using 5 random seeds for 200,000 iterations across all datasets, and report results averaged over the 5 random runs. ", "page_idx": 7}, {"type": "text", "text": "5.1 Compositional Structure / Disentanglement ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To evaluate the degree to which Soft TPRs achieve explicitly compositional structure, we quantify representational disentanglement using standard disentanglement datasets Cars3D [4], MPI3D [21], and Shapes3D [24] (see C.4.1 for further details). As can be seen in Table 1, our model achieves state-of-the-art disentanglement for all datasets, with notable DCI metric increases of $29\\%$ and $74\\%$ on the 2 more challenging datasets of Cars3D and MPI3D respectively. To rule out the possibility that the improvement in disentanglement produced by our model is due to a slight increase of 13,568 (Cars3D), 1,824 (Shapes3D), 1,600 (MPI3D) learnable parameters produced by the addition of the filler embedding matrix, $M_{\\xi_{F}}$ , to a standard (variational) encoder-decoder framework, we modify models with fewer parameters to have an identical number of parameters as ours. We note that the modifications are applicable only for the scalar-tokened baselines, as our model has tens of millions less parameters than COMET and VCT. In line with [40], we observe that the performance of scalar-tokened disentanglement models remains fairly consistent, or even deteriorates, when the number of learnable parameters increases, so we defer control experiment results, and our associated method to Appendix C.2.4. ", "page_idx": 7}, {"type": "text", "text": "Key Implication: The Soft TPR\u2019s superior level of explicit compositionality (as quantified by the disentanglement metrics) in a controlled, parameter-count environment, suggests that its fundamentally continuous representational form is potentially easier for deep learning models to learn compared to symbolic representational forms characterising existing disentanglement work. ", "page_idx": 7}, {"type": "text", "text": "5.2 Representation Learner Convergence Rate ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate whether the inherently continuous compositional form of the Soft TPR can be learned more quickly than symbolic alternatives, we consider representations produced at 100, 1,000, 10,000, 100,000 and 200,000 iterations of training, and evaluate 1) their disentanglement, and 2) their utility, as quantified by the performance of downstream models using these representations. For downstream model performance, we consider two commonly used [30, 46, 33] tasks: the classification-based abstract visual reasoning task of [30] and a regression task involving the prediction of continuous FoV values for the disentanglement datasets. Downstream models are evaluated on a fixed, held-out test set for both tasks. While our framework does not promote faster disentanglement convergence (results in Appendix C.4.1), it interestingly, promotes accelerated learning of useful representations for both downstream tasks compared to baselines. The downstream performance improvements are particularly pronounced in the low iteration regime of 100 iterations of representation learner training, as demonstrated by the improvements of $10\\bar{\\%}$ , $10\\%$ , and $31\\%$ in Table 2 and the $27\\%$ improvement in Table 3. To ensure fair comparison, we embed baseline representations of both higher, and lower dimensionality into the same space as our model. For each baseline model, we take the best result from either the original, or the modified model (denoted by $\\dagger$ ), and present the full suite of results, and details of our embedding method and downstream model setup in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "Key Implication: The representation learning convergence results suggests that while our model may not learn the explicit compositional structure captured by disentanglement metrics more quickly than baselines (though in the limit, the greatest possible disentanglement is higher), it learns useful information for downstream tasks more quickly than baselines, where that useful information is encoded in the relaxed compositional structure of the Soft TPR. ", "page_idx": 8}, {"type": "table", "img_path": "oEVsxVdush/tmp/e80b0592749db031269ba6dd687cb01758efe4e3012476c1939e29a996bf4932.jpg", "table_caption": ["Table 1: FactorVAE and DCI scores. Additional results in Section C.3.3 "], "table_footnote": ["urs 0.999 \u00b1 0.001 0.863 \u00b1 0.027 0.984 \u00b1 0.012 0.926 \u00b1 0.028 0.949 \u00b1 0.032 0.828 \u00b1 0.015 "], "page_idx": 8}, {"type": "table", "img_path": "oEVsxVdush/tmp/08e1ee1b4c021ab59e993bc11028876ad3038697ca842d14fbc20c05793e9ee6.jpg", "table_caption": ["Table 2: FoV regression $R^{2}$ scores (100 iterations of representation learner training). "], "table_footnote": ["Ours 0.531 \u00b1 0.054 0.981 \u00b1 0.003 0.732 \u00b1 0.012 "], "page_idx": 8}, {"type": "table", "img_path": "oEVsxVdush/tmp/629fd631014578e26c85ef9f8006865a0544491e375e3692ee4d394c477dd4b8.jpg", "table_caption": ["Table 3: Abstract visual reasoning accuracy (100 iterations of representation learner training). "], "table_footnote": ["Ours 0.804 \u00b1 0.016 "], "page_idx": 8}, {"type": "text", "text": "5.3 Downstream Models ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate whether the enhanced alignment between compositional representations and continuous vector spaces produced by our Soft TPR benefits downstream models, we examine downstream 1) sample efficiency, and 2) raw performance in the low sample regime. We use the previously mentioned tasks of abstract visual reasoning and FoV regression. To quantify sample efficiency, in line with [25], we use a ratio-based metric obtained by dividing the performance of the downstream model when trained using a restricted number of samples (100, 250, 500, 1,000 and 10,000 samples), by its performance when trained using all samples (between 19,104-1,036,800 samples depending on the task). As illustrated in Table 4, our model has superior sample efficiencies compared to baselines, especially in the most restrictive case where downstream models have access to only 100 samples produced by representation learners, achieving a $93\\%$ improvement. The Soft TPR representations produced by our model additionally produce substantial raw performance increases in the low sample regime, as evidenced in Table 4, where its performance in the low-sample regimes of 100 and 200 samples constitutes a respective $138\\%$ and $\\bar{1}68\\%$ improvement, and in Table 5, a $30\\%$ improvement. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "oEVsxVdush/tmp/ce416549a26d0ec0ac0a37a1c8a7a1f01324d76933c9ef1f5a1ba2e909353a51.jpg", "table_caption": ["Table 4: Downstream FoV $R^{2}$ scores (odd columns) and sample efficiencies (even columns) on the MPI3D dataset. "], "table_footnote": ["Ours 0.490 \u00b1 0.068 0.556 \u00b1 0.078 0.594 \u00b1 0.056 0.665 \u00b1 0.067 "], "page_idx": 9}, {"type": "table", "img_path": "oEVsxVdush/tmp/001efcc840f0c65d31dcb82233dea8bfd5d7e6ad03ea5572afc1621936b8d216.jpg", "table_caption": ["Table 5: Abstract visual reasoning accuracy in the low-sample regime of 500 samples. "], "table_footnote": ["Ours 0.360 \u00b1 0.033 "], "page_idx": 9}, {"type": "table", "img_path": "oEVsxVdush/tmp/d3d14faa62710687b257dbbaec215d9946761cb268d441eeaec109763fd922cb.jpg", "table_caption": ["Table 6: Sample efficiencies for FoV regression. "], "table_footnote": ["Soft TPR (250/all samples) 0.889 \u00b1 0.042 0.730 \u00b1 0.038 0.665 \u00b1 0.067 "], "page_idx": 9}, {"type": "table", "img_path": "oEVsxVdush/tmp/a58105bc30d0c6be1ca5e28ac0074d4b0748a4dc9bd1fa18fdda8ea2bd0476ea.jpg", "table_caption": ["Table 7: Effect of model properties on disentanglement performance (MPI3D dataset). "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Key Implication: The consistent performance improvement we observe in generic downstream models with no a priori knowledge of the Soft TPR\u2019s representational form, suggests the relaxed, inherently continuous representation of compositional structure embodied by our Soft TPR can be more efficiently leveraged by downstream models compared to symbolic compositional representations, benefiting both sample efficiency, and raw performance in the low sample regime. ", "page_idx": 9}, {"type": "text", "text": "5.4 More Ablation Studies ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We additionally repeat our full suite of experiments using the explicit TPR, $\\psi_{t p r}^{*}$ , produced by the TPR decoder, in place of our Soft TPR, $z$ . These experiments, a subset of which is presented in Table 6, empirically demonstrate that the Soft TPR\u2019s continuously relaxed specification of compositional structure confers exclusive benefits for both the representation learner and the downstream models not captured by the traditional TPR (see Appendix C.6.1). Note for MPI3D, the explicit TPR has a lower raw $R^{2}$ score when fully trained $(0.785\\pm0.019$ vs $0.882\\pm0.016)$ ), contributing to its higher sample efficiency in Table 6. We additionally examine the importance of the following properties of our model in producing explicitly compositional Soft TPR representations: 1) the presence of weak supervision, by setting $\\lambda_{1}=\\lambda_{2}=0$ in Equation 7, 2) the explicit dependency between the quantised filler embeddings and the decoder output, by instead using the Soft TPR to reconstruct the input image, and 3) the semi-orthogonality of the role embedding matrix, $M_{\\xi_{R}}$ by removing this constraint in the random initialisation of $M_{\\xi_{R}}$ , with the results of these ablations illustrated in Table 7. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we address a longstanding issue in the connectionist approach to compositionality: the fundamental mismatch between disentangled representations and the inherently continuous nature of deep learning vector spaces. To overcome this, we introduce Soft TPR, a novel, inherently continuous compositional representational form that extends Smolensky\u2019s Tensor Product Representation, together with the Soft TPR Autocoder, a theoretically-principled architecture designed for learning Soft TPRs. Our flexible, continuous framework yields substantial improvements in the visual domain, enhancing compositional structure, accelerating convergence in representation learners, and boosting efficiency in downstream models. These wide-ranging empirical beneftis underscore the importance of rethinking compositional representations to honour deep learning\u2019s continuous foundations. Future work will extend our continuous framework to hierarchical forms of compositionality, enabling bound fillers themselves to decompose into role-filler bindings for enhanced representational expressivity. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research has been supported by an UNSW University Postgraduate Award given to B.S. We thank the anonymous reviewers for their valuable feedback, and B. Sphear for insightful discussions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Noam Chomsky. Syntactic Structures. The Hague: Mouton, 1957. [2] Jerry A. Fodor. The Language of Thought: A Theory of Mental Representation. Cambridge, MA: Harvard University Press, 1975. [3] Paul Smolensky. \u201cTensor product variable binding and the representation of symbolic structures in connectionist systems\u201d. In: Artificial Intelligence 46.1 (1990), pp. 159\u2013216. [4] Sanja Fidler, Sven Dickinson, and Raquel Urtasun. \u201c3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model\u201d. In: Advances in Neural Information Processing Systems. 2012. [5] Yoshua Bengio. Deep Learning of Representations: Looking Forward. 2013. arXiv: 1305 . 0445 [cs.LG]. [6] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. \u201cNeural Module Networks\u201d. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015), pp. 39\u201348. URL: https://api.semanticscholar.org/CorpusID:5276660. [7] Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merri\u00ebnboer, Armand Joulin, and Tomas Mikolov. Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks. 2015. arXiv: 1502.05698 [cs.AI]. URL: https://arxiv.org/abs/1502.05698.   \n[8] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. \u201cInfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\u201d. In: Advances in Neural Information Processing Systems. Ed. by D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett. Vol. 29. Curran Associates, Inc., 2016.   \n[9] Paul Smolensky and Matthew A. Goldrick. \u201cGradient Symbolic Representations in Grammar: The case of French Liaison\u201d. In: 2016. URL: https://api.semanticscholar.org/CorpusID:36953611.   \n[10] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. \u201cbeta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\u201d. In: International Conference on Learning Representations. 2017.   \n[11] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu koray. \u201cNeural Discrete Representation Learning\u201d. In: Advances in Neural Information Processing Systems. Ed. by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc., 2017.   \n[12] Tameem Adel, Zoubin Ghahramani, and Adrian Weller. \u201cDiscovering Interpretable Representations for Both Deep Generative and Discriminative Models\u201d. In: Proceedings of the 35th International Conference on Machine Learning. Vol. 80. Proceedings of Machine Learning Research. PMLR, 2018, pp. 50\u201359.   \n[13] Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. \u201cMulti-Level Variational Autoencoder: Learning Disentangled Representations From Grouped Observations\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence 32.1 (2018).   \n[14] Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in $\\beta$ -VAE. 2018. arXiv: 1804.03599 [stat.ML].   \n[15] Ricky T. Q. Chen, Xuechen Li, Roger Grosse, and David Duvenaud. \u201cIsolating Sources of Disentanglement in Variational Autoencoders\u201d. In: Advances in Neural Information Processing Systems. 2018.   \n[16] Cian Eastwood and Christopher K. I. Williams. \u201cA Framework for the Quantitative Evaluation of Disentangled Representations\u201d. In: International Conference on Learning Representations. 2018.   \n[17] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. \u201cVariational Inference of Disentangled Latent Concepts from Unlabeled Observations\u201d. In: 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. 2018.   \n[18] Adam Santoro, Felix Hill, David Barrett, Ari Morcos, and Timothy Lillicrap. \u201cMeasuring abstract reasoning in neural networks\u201d. In: International conference on machine learning. 2018, pp. 4477\u20134486.   \n[19] Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul Smolensky, Kenneth D. Forbus, and Jianfeng Gao. \u201cNatural- to formal-language generation using Tensor Product Representations\u201d. In: CoRR abs/1910.02339 (2019). arXiv: 1910.02339. URL: http://arxiv.org/abs/1910.02339.   \n[20] Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard Zemel. \u201cFlexibly Fair Representation Learning by Disentanglement\u201d. In: Proceedings of the 36th International Conference on Machine Learning. 2019.   \n[21] Muhammad Waleed Gondal, Manuel W\u00fcthrich, undefinedor d\u00afe Miladinovic\u00b4, Francesco Locatello, Martin Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Sch\u00f6lkopf, and Stefan Bauer. \u201cOn the transfer of inductive bias from simulation to the real world: a new disentanglement dataset\u201d. In: Proceedings of the 33rd International Conference on Neural Information Processing Systems. 2019.   \n[22] Haruo Hosoya. \u201cGroup-based learning of disentangled representations with generalizability for novel contents\u201d. In: Proceedings of the 28th International Joint Conference on Artificial Intelligence. 2019, pp. 2506\u20132513.   \n[23] Qiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu, and Xiaodong He. \u201cAttentive Tensor Product Learning\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence 33.01 (2019), pp. 1344\u2013 1351.   \n[24] Hyunjik Kim and Andriy Mnih. Disentangling by Factorising. 2019. arXiv: 1802.05983 [stat.ML].   \n[25] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, and Olivier Bachem. \u201cChallenging Common Assumptions in the Unsupervised Learning of Disentangled Representations\u201d. In: Proceedings of the 36th International Conference on Machine Learning. Vol. 97. Proceedings of Machine Learning Research. PMLR, 2019, pp. 4114\u20134124.   \n[26] Emile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. \u201cDisentangling Disentanglement in Variational Autoencoders\u201d. In: Proceedings of the 36th International Conference on Machine Learning. 2019.   \n[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. \u201cPyTorch: An Imperative Style, High-Performance Deep Learning Library\u201d. In: Advances in Neural Information Processing Systems 32. Curran Associates, Inc., 2019, pp. 8024\u2013 8035. URL: http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-highperformance-deep-learning-library.pdf.   \n[28] Imanol Schlag and J\u00fcrgen Schmidhuber. \u201cLearning to Reason with Third-Order Tensor Products\u201d. In: Advances in Neural Processing Information Systems. 2019.   \n[29] Imanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, J\u00fcrgen Schmidhuber, and Jianfeng Gao. \u201cEnhancing the Transformer with Explicit Relational Encoding for Math Problem Solving\u201d. In: ArXiv abs/1910.06611 (2019). URL: https://api.semanticscholar.org/CorpusID:204575948.   \n[30] Sjoerd van Steenkiste, Francesco Locatello, J\u00fcrgen Schmidhuber, and Olivier Bachem. \u201cAre disentangled representations helpful for abstract visual reasoning?\u201d In: Proceedings of the 33rd International Conference on Neural Information Processing Systems. 2019.   \n[31] Junxiang Chen and Kayhan Batmanghelich. \u201cWeakly Supervised Disentanglement by Pairwise Similarities\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence 34.04 (2020), pp. 3495\u2013 3502.   \n[32] Zheng Ding, Yifan Xu, Weijian Xu, Gaurav Parmar, Yang Yang, Max Welling, and Zhuowen Tu. \u201cGuided Variational Autoencoder for Disentanglement Learning\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020.   \n[33] F. Locatello, B. Poole, G. R\u00e4tsch, B. Sch\u00f6lkopf, O. Bachem, and M. Tschannen. \u201cWeakly-Supervised Disentanglement Without Compromises\u201d. In: Proceedings of the 37th International Conference on Machine Learning (ICML). Vol. 119. Proceedings of Machine Learning Research. PMLR, 2020, pp. 6348\u2013 6359.   \n[34] R. Thomas McCoy, Tal Linzen, Ewan Dunbar, and Paul Smolensky. \u201cTensor Product Decomposition Networks: Uncovering Representations of Structure Learned by Neural Networks\u201d. In: Proceedings of the Society for Computation in Linguistics 2020. Association for Computational Linguistics, 2020, pp. 277\u2013278. URL: https://aclanthology.org/2020.scil-1.34.   \n[35] Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. \u201cWeakly Supervised Disentanglement with Guarantees\u201d. In: International Conference on Learning Representations. 2020.   \n[36] Yilun Du, Shuang Li, Yash Sharma, B. Joshua Tenenbaum, and Igor Mordatch. \u201cUnsupervised Learning of Compositional Energy Concepts\u201d. In: Advances in Neural Information Processing Systems. 2021.   \n[37] Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, and Gunhee Kim. \u201cIB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence 35.9 (May 2021), pp. 7926\u20137934. DOI: 10.1609/aaai. v35i9.16967. URL: https://ojs.aaai.org/index.php/AAAI/article/view/16967.   \n[38] Yichen Jiang, Asli Celikyilmaz, Paul Smolensky, Paul Soulos, Sudha Rao, Hamid Palangi, Roland Fernandez, Caitlin Smith, Mohit Bansal, and Jianfeng Gao. \u201cEnriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization\u201d. In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Online: Association for Computational Linguistics, 2021, pp. 4780\u20134793. DOI: 10.18653/ v1/2021.naacl-main.381. URL: https://aclanthology.org/2021.naacl-main.381.   \n[39] David A. Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan Paiton. \u201cTowards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding\u201d. In: International Conference on Learning Representations. 2021. URL: https://openreview.net/ forum?id=EbIDjBynYJ8.   \n[40] Milton Llera Montero, Casimir JH Ludwig, Rui Ponte Costa, Gaurav Malhotra, and Jeffrey Bowers. \u201cThe role of Disentanglement in Generalisation\u201d. In: International Conference on Learning Representations. 2021. URL: https://openreview.net/forum?id=qbH974jKUVy.   \n[41] Sungho Park, Sunhee Hwang, Dohyung Kim, and Hyeran Byun. \u201cLearning Disentangled Representation for Fair Facial Attribute Classification via Fairness-aware Information Alignment\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence 35 (2021), pp. 2403\u20132411. DOI: 10.1609/aaai.v35i3. 16341. URL: https://ojs.aaai.org/index.php/AAAI/article/view/16341.   \n[42] L. Schott, J. von K\u00fcgelgen, F. Tr\u00e4uble, P. Gehler, C. Russell, M. Bethge, B. Sch\u00f6lkopf, F. Locatello, and W. Brendel. \u201cVisual Representation Learning Does Not Generalize Strongly Within the Same Domain\u201d. In: ICLR 2021 - Workshop on Generalization beyond the training distribution in brains and machines. 2021.   \n[43] Frederik Tr\u00e4uble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bernhard Sch\u00f6lkopf, and Stefan Bauer. \u201cOn Disentangled Representations Learned from Correlated Data\u201d. In: Proceedings of the 38th International Conference on Machine Learning. Vol. 139. Proceedings of Machine Learning Research. 2021, pp. 10401\u201310412.   \n[44] Milton Montero, Jeffrey Bowers, Rui Ponte Costa, Casimir Ludwig, and Gaurav Malhotra. \u201cLost in Latent Space: Examining failures of disentangled models at combinatorial generalisation\u201d. In: Advances in Neural Information Processing Systems. Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., 2022, pp. 10136\u201310149.   \n[45] Zolt\u00e1n Gendler Szab\u00f3. \u201cCompositionality\u201d. In: The Stanford Encyclopedia of Philosophy. Ed. by Edward N. Zalta and Uri Nodelman. Fall 2022. Metaphysics Research Lab, Stanford University, 2022.   \n[46] Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. \u201cTowards Building A Group-based Unsupervised Representation Disentanglement Framework\u201d. In: International Conference on Learning Representations. 2022. URL: https://openreview.net/forum?id=YgPqNctmyd.   \n[47] Tao Yang, Yuwang Wang, Yan Lu, and Nanning Zheng. \u201cVisual Concepts Tokenization\u201d. In: Advances in Neural Information Processing Systems. 2022.   \n[48] H. Zhang, Y.-F. Zhang, W. Liu, A. Weller, B. Sch\u00f6lkopf, and E. Xing. \u201cTowards Principled Disentanglement for Domain Generalization\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022, pp. 8024\u20138034. URL: https://openaccess.thecvf.com/ content/CVPR2022/papers/Zhang_Towards_Principled_Disentanglement_for_Domain_ Generalization_CVPR_2022_paper.pdf.   \n[49] Thadd\u00e4us Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, and Wieland Brendel. \u201cCompositional Generalization from First Principles\u201d. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023. URL: https://openreview.net/forum?id=LqOQ1uJmSx.   \n[50] Haoyang Li, Xin Wang, Zeyang Zhang, Haibo Chen, Ziwei Zhang, and Wenwu Zhu. \u201cDisentangled Graph Self-supervised Learning for Out-of-Distribution Generalization\u201d. In: Forty-first International Conference on Machine Learning. 2024. URL: https://openreview.net/forum?id $\\equiv$ OS0szhkPmF.   \n[51] Taewon Park, Inchul Choi, and Minho Lee. \u201cAttention-based Iterative Decomposition for Tensor Product Representation\u201d. In: The Twelfth International Conference on Learning Representations. 2024. URL: https://openreview.net/forum?id=FDb2JQZsFH.   \n[52] Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, and Dapeng Wu. \u201cTensor Product Generation Networks for Deep NLP Modeling\u201d. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A TPR Framework 14 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Additional Details . 14   \nA.2 Formal Proofs . 15 ", "page_idx": 13}, {"type": "text", "text": "B Soft TPR Framework 16 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Shortcomings of the TPR and How Soft TPR Helps 16   \nB.2 Alternative Formulations . . . 18   \nB.3 Soft TPR Autoencoder . . 18   \nB.4 Model Hyperparameters and Hyperparameter Tuning 22 ", "page_idx": 13}, {"type": "text", "text": "C Results 23 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Datasets 23   \nC.2 Baseline Implementations and Experimental Settings 23   \nC.3 Disentanglement . 26   \nC.4 Representation Learning Convergence 27   \nC.5 Downstream Performance . . 46   \nC.6 More Ablation Experiments 65 ", "page_idx": 13}, {"type": "text", "text": "D Limitations and Future Work 76 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 Extension to Linguistic Domains 76   \nD.2 Need for Weak Supervision . . . 77   \nD.3 Downstream Utility . . 77   \nD.4 Dimensionality . . 78   \nD.5 Computational Cost . . 78 ", "page_idx": 13}, {"type": "text", "text": "A TPR Framework ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide additional details regarding Smolensky\u2019s TPR framework [3], as well as formal proofs of presented results. We refer interested readers to [3] for a more comprehensive dive into this fascinating framework. ", "page_idx": 13}, {"type": "text", "text": "A.1 Additional Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "By defining the constituent components of any compositional object as a set of role-filler bindings, the TPR defines the decomposition function, $\\beta$ , of Section 3.1 that maps from a set of compositional objects, $X$ , to a set of parts, more explicitly as follows [3]: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\beta:X\\to2^{F\\times R};x\\to\\{(f,r)|f/r\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $F$ denotes a set of fillers, and $R$ denotes a set of roles. Note that in contrast to the formal definition of $\\beta$ we use in Section 3.1, which assumes each $x\\,\\in\\,X$ is decomposable into a set of $n$ parts, the above decomposition allows objects to be decomposed into a variably-sized set of role-filler bindings, with this set corresponding to an element in the powerset of $F\\times R$ . For the visual representation learning domain we consider, all considered disentanglement datasets clearly have the property of being decomposable into a fixed size set of role-fliler bindings, as all images in these datasets contain the same number of FoV types and each FoV type is bound to a FoV token. Due to this property, we can take $\\beta$ as a special subcase of the generalised definition in Equation 8: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\beta:X\\to A:x\\to\\{(f_{m(i)},r_{i})|f_{m(i)}/r_{i}\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $m:\\{1,\\dots,N_{R}\\}\\rightarrow\\{1,\\dots,N_{F}\\}$ denotes a matching function that associates each role $r_{i}$ with the fliler it binds to in $\\beta(x)$ (we again drop the dependence of $m$ on $x$ for ease of notation), and $\\boldsymbol{\\mathcal{A}}$ denotes the set of all possible bindings produced by binding a filler to each of the $N_{R}$ roles, with size $\\binom{N_{F}+N_{R}-1}{N_{R}}$ (we assume the same filler can bind to multiple roles). ", "page_idx": 14}, {"type": "text", "text": "A.2 Formal Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We now formally prove that the TPR with $\\beta$ defined in 9 has form $\\psi_{t p r}(x)=C(\\psi_{1}(a_{1}),\\dots,\\psi_{n}(a_{n}))$ and thus corresponds to the definition of a compositional representation in Section 3.1. ", "page_idx": 14}, {"type": "text", "text": "Proof. We denote the role embedding and fliler embedding functions as $\\xi_{R}:R\\to V_{R},\\xi_{F}:F\\to V_{F}$ respectively. ", "page_idx": 14}, {"type": "text", "text": "By definition of the TPR in $\\operatorname{Eq}1$ , we have that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\psi_{t p r}(x):=\\sum_{i}\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and hence, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\psi_{t p r}({\\boldsymbol{x}})=\\sum_{i}\\psi_{i}(f_{m(i)},r_{i}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{(f_{m(i)},r_{i})\\in\\beta(x),\\psi_{i}:F\\times R\\rightarrow V_{F}\\otimes V_{R};(f_{m(i)},r_{i})\\rightarrow\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i}),}\\end{array}$ and $C$ is ordinary vector space addition. Hence, almost trivially, $\\psi_{t p r}(x)$ clearly has the required form to be a compositional representation. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Now, we prove the recoverability of the embedded components $\\{\\psi_{i}(f_{m(i)},r_{i})\\}$ from the TPR, $\\psi_{t p r}(x)$ , provided that the set of all role embedding vectors, $\\{\\xi_{R}(r_{i})\\}$ , are linearly independent. Similar variants of this proof can be found in [3], [19]. ", "page_idx": 14}, {"type": "text", "text": "Proof. Assume the set of all role embedding vectors $\\{\\xi_{R}(r_{i})\\}$ are linearly independent. Then, the role embedding matrix, $M_{\\scriptscriptstyle R}:=(\\xi_{R}(r_{1})\\ldots\\bar{\\xi}_{R}(r_{N_{R}}))$ formed by taking the role embedding vectors as columns, has a left inverse, $U$ , such that: ", "page_idx": 14}, {"type": "equation", "text": "$$\nU M_{{\\xi}_{R}}=I_{N_{R}\\times N_{R}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, we have that $(U M_{\\xi_{R}})_{i j}=U_{i:}M_{\\xi_{R}:j}=I_{i j}$ . ", "page_idx": 14}, {"type": "text", "text": "For ease of notation, let $u_{i}$ denote the $i$ -th column of $U^{T}$ , and note that $\\xi_{R}(r_{j})$ clearly corresponds to $M_{\\xi_{R}:j}$ . So, $U_{i:}M_{\\xi_{R}:j}=(U_{:i}^{T})^{T}M_{\\xi_{R}:j}=u_{i}^{T}\\xi_{R}(r_{j})=I_{i j}$ . ", "page_idx": 14}, {"type": "text", "text": "Hence, we have that: ", "page_idx": 14}, {"type": "equation", "text": "$$\nu_{i}^{T}\\xi_{R}(r_{j})={\\delta}_{i j}=\\left\\{\\begin{array}{l l}{1}&{i=j}\\\\ {0}&{\\mathrm{otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the definition of $\\psi_{t p r}(x)$ $\\begin{array}{r}{\\mathbf{\\Theta}_{\\!\\;\\!r}(x),\\,\\psi_{t p r}(x)\\,=\\,\\sum_{i}\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i})\\,}\\end{array}$ , we apply the (tensor) inner product of $\\psi_{t p r}(x)$ with $u_{i}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{t p r}(x)=\\left(\\displaystyle\\sum_{i}\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i})\\right)u_{i}}\\\\ &{\\qquad\\quad=\\left(\\displaystyle\\sum_{i}\\xi_{F}(f_{m(i)})\\xi_{R}(r_{i})^{T}\\right)u_{i}}\\\\ &{\\qquad\\quad=\\displaystyle\\sum_{i}\\xi_{F}(f_{m(i)})\\delta_{j i}}\\\\ &{\\qquad\\quad=\\xi_{F}(f_{m(i)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the embedding of the filler, $f_{m(i)}$ , bound to each role, $r_{i}$ , can be recovered through use of a tensor inner product with the unbinding vector, $u_{i}$ , corresponding to the $i$ -th column of $U^{T}$ . Note that the representational components of $\\psi_{t p r}(x)$ , i.e., the embedded bindings, $\\psi_{i}(f_{m(i)},r_{i})$ are fully determined by the embedding of the role, $\\xi_{R}(\\boldsymbol{r}_{i})$ , and filler, $\\xi_{F}{\\left(f_{m\\left(i\\right)}\\right)}$ , comprising the binding, as $\\xi_{F}{\\left(f_{m(i)},r_{i}\\right)}$ simply corresponds to their tensor product. Thus, recovering $(\\xi(f_{m(i)}),\\xi_{R}(r_{i}))$ for each binding in $\\beta(x)$ corresponds to recovering the representational component, $\\psi_{i}(f_{m(i)},r_{i})$ . So, provided the set of role embeddings are linearly independent, and they can be obtained (e.g. through a look-up table of role embeddings), all representational components, $\\psi_{i}(f_{m(i)},r_{i})$ can be directly recovered from the overall TPR representation, $\\psi_{t p r}(x)$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B Soft TPR Framework ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide additional information regarding our Soft TPR framework, more extensively detailing our theoretically-informed rationale behind certain design decisions, as well as providing all necessary model architecture information for our Soft TPR Autoencoder to replicate our results (note that code is available at https://github.com/gomb0c/soft_tpr/). ", "page_idx": 15}, {"type": "text", "text": "B.1 Shortcomings of the TPR and How Soft TPR Helps ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this subsection, we provide a more detailed explanation regarding the fundamental limitations produced by the TPR\u2019s stringent specification $(\\mathrm{Eq\\1})$ . For concreteness, we use an example. We consider the following set of roles and fillers, $\\bar{R^{\\prime}}=\\{\\mathrm{shape,colour}\\},F\\,=\\,\\{\\mathrm{red,blue,square}\\}$ . Additionally, we define the role $\\xi_{R}:R\\to\\mathbb{R}^{2}$ and filler $\\xi_{F}:F\\to\\mathbb{R}^{3}$ embedding functions as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\xi_{R}(\\mathrm{shape})=\\left[\\!\\!\\begin{array}{l}{1}\\\\ {0}\\end{array}\\!\\!\\right],}\\\\ {\\xi_{R}(\\mathrm{colow})=\\left[\\!\\!\\begin{array}{l}{1}\\\\ {1}\\end{array}\\!\\!\\right],}\\\\ {\\xi_{F}(\\mathrm{red})=\\left[\\!\\!\\begin{array}{l}{1}\\\\ {0}\\end{array}\\!\\!\\right],\\ }\\\\ {\\xi_{F}(\\mathrm{blue})=\\left[\\!\\!\\begin{array}{l}{2}\\\\ {0}\\end{array}\\!\\!\\right],}\\\\ {\\xi_{F}(\\mathrm{st})=\\left[\\!\\!\\begin{array}{l}{0}\\\\ {1}\\end{array}\\!\\!\\right],}\\\\ {\\xi_{F}(\\mathrm{square})=\\left[\\!\\!\\begin{array}{l}{0}\\\\ {0}\\end{array}\\!\\!\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": ". Discrete Mapping: Consider the set of possible TPRs, $T$ , that can be produced by $R,F,\\xi_{R},\\xi_{F}$ as defined above. We have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{t r e}\\left(x_{\\mathrm{red},\\,\\mathrm{square}}\\right)=\\xi_{F}(\\mathrm{red})\\otimes\\xi_{R}(\\mathrm{colour})+\\xi_{F}(\\mathrm{square})\\otimes\\xi_{R}(\\mathrm{square})}\\\\ &{=\\left[\\!\\frac{1}{3}\\!\\right]\\otimes\\left[\\!1\\!\\right]+\\left[\\!\\frac{0}{1\\!}\\right]\\otimes\\left[\\!\\frac{1}{0}\\right]\\otimes\\left[\\!\\frac{1}{3}\\!\\right]+\\left[\\!\\frac{0}{1\\!}\\right]}\\\\ &{=\\left[\\!\\frac{1}{4}\\!\\right]}\\\\ &{=\\left[\\!\\frac{1}{12}\\!\\right]}\\\\ &{\\left[\\!\\frac{2}{3}\\!\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{t r p r}(x_{\\mathrm{blue~square}})=\\xi_{F}(\\mathrm{blue})\\otimes\\xi_{R}(\\mathrm{colour})+\\xi_{F}(\\mathrm{square})\\otimes\\xi_{R}(\\mathrm{\\bf~a})}\\\\ &{=\\left[\\!\\!\\left[\\!\\frac{2}{3}\\!\\!\\right]\\otimes\\left[\\!\\!\\left[\\frac{1}{1}\\!\\!\\right]+\\left[\\!\\!\\left[\\!\\frac{0}{1}\\!\\!\\right]\\otimes\\left[\\!\\!\\left[\\frac{1}{0}\\right]\\!\\!\\right]\\otimes\\left[\\!\\!\\left[\\frac{1}{2}\\!\\!\\right]\\!\\right]+\\left[\\!\\!\\left[\\!\\frac{0}{1}\\!\\!\\right]\\!\\right]}\\\\ &{=\\left[\\!\\!\\left[\\!\\frac{2}{2}\\!\\!\\right]\\!\\right]}\\\\ &{=\\left[\\!\\!\\left[\\!\\frac{2}{2}\\!\\!\\right]\\!\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "These 2 possible TPRs form a discrete, 2 element subset of the underlying representational space, $\\mathbb{R}^{\\bar{6}}$ , $T=\\{(2\\,2\\,4\\,2\\,2\\,3)^{T},(1\\,2\\,4\\,1\\,2\\,3)^{T}\\}$ . Relaxing the TPR specification to the Soft $T P R$ allows any point, $z$ , in the underlying representational space, $\\bar{\\mathbb{R}}^{6}$ to qualify as a Soft TPR provided that for some explicit TPR, $\\psi_{t p r}\\in T$ , the sufficient closeness requirement holds: $||z-\\psi_{t p r}||_{2}<\\epsilon$ . So, if we consider the set of possible Soft TPRs, $T_{S}$ , we have $=$ $T_{S}:=\\{(2\\,2\\,4\\,2\\,2\\,3)^{T}+\\alpha,(1\\,2\\,4\\,1\\,2\\,3)^{T}+\\alpha:|\\alpha|<\\epsilon\\}$ . Hence, $T_{S}$ clearly has strictly more points than $T$ , so there should be more functions parameterising the map from the observed data to $T_{S}$ compared to $T$ . Furthermore, in contrast to $T$ , which contains discrete (singular) points scattered around in $\\mathbb{R}^{6}$ , $T_{S}$ contains continuous cloud-like regions centered at each $\\psi_{t p r}\\in T$ . Both these factors should make the Soft TPR representation potentially easier to learn and extract information from, compared to the TPR. This is reflected in our empirical results in Section C.6.1, where, compared to the traditional TPR, the Soft TPR demonstrates 1) greater representation learner convergence, 2) superior sample efficiency for downstream tasks, and 3) superior raw downstream performance in the low sample regime. ", "page_idx": 16}, {"type": "text", "text": "2. Quasi-Compositional Structure: The traditional TPR enforces a strict algebraic definition of compositionality (i.e. $\\begin{array}{r}{\\sum_{i}\\xi_{F}\\big(f_{m(i)}\\big)\\otimes\\xi_{R}\\big(r_{i}\\big))}\\end{array}$ . Even explicitly algebraically-structured domains such as natura l language, do not always adhere to this rigid specification of compositional structure (e.g. French liaison consonants, which consist of a weighted sum of fillers bound to a single role [9]). The Soft TPR\u2019s continuous relaxation of this constraint allows it to represent structures that only approximately satisfy the TPR\u2019s strict definition of compositionality. A more relevant example could be the construction of compositional representations of coloured squares where the colour is a weighted combination of flilers blue and red (i.e., varying shades of purple). In this case, even if purple has not been seen by the TPR Autoencoder previously (i.e., there is no quantised fliler representing $\\xi_{F}(\\mathrm{purple}))$ , it may be possible for the TPR Autoencoder to produce a Soft TPR corresponding to purple square if there is a suitable $\\epsilon$ -level relaxation of any of the explicit TPRs (i.e., red square, blue square) that represents a purple square. ", "page_idx": 16}, {"type": "text", "text": "3. Serial Construction: Building explicit TPRs requires that the embedded fillers and roles comprising a binding (i.e. $\\bar{(\\xi_{F}(f_{m(i)}),\\xi_{R}(r_{i})))}$ are first tokened before the entire compositional representation, $\\psi_{t p r}(x)$ can be produced [19, 23, 28, 34, 38, 51, 52]. This sort of sequential approach, where constituents must be tokened before the compositional representation can be formed, is a key characteristic of the symbolic representation of compositional structure [45]. In contrast, the Soft TPR allows the Encoder to produce any arbitrary element of $V_{F}\\otimes V_{R}$ (in this case $\\cong\\mathbb{R}^{6}$ ), provided that the sufficient closeness requirement holds. Thus, once the Encoder is trained, it is theoretically possible for the Soft TPR Autoencoder to exploit vector space continuity to generate approximately compositional representations by mapping directly from the data to a Soft TPR, without needing to token any representational constituents. ", "page_idx": 17}, {"type": "text", "text": "B.2 Alternative Formulations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In contrast to the greedily optimal analytic form of $\\psi_{t p r}^{*}$ that we derive in Equation 5, it is possible to instead derive a globally optimal TPR that the $(\\bar{D_{R}}\\cdot D_{F})$ -dimensional vector, $z$ , produced by the encoder, $E$ , best approximates with reference to the Frobenius norm metric of $||z-\\psi_{t p r}||_{F}$ We fix the set of fillers, $F$ , and roles, $R$ , as well as the embedding functions, $\\xi_{F}:F\\to\\mathbb{R}^{D_{F}}$ and $\\xi_{R}:R\\to\\mathbb{R}^{D_{R}}$ to be arbitrary sets and embedding functions respectively. Note that with $F,R$ , $\\xi_{F}$ , and $\\xi_{R}$ fixed, the only degree of freedom in defining the space of possible TPRs is given by defining $\\mathcal{M}^{\\ast}$ , the set of role-filler matching functions that define the permissible role-filler binding decompositions of $x\\in X$ . The globally optimal TPR that $z$ best approximates, $\\psi_{t p r}^{o p t}$ , is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\psi_{t p r}^{o p t}:=\\underset{\\psi_{t p r}\\in\\mathcal{T}}{\\arg\\operatorname*{min}}\\,||z-\\psi_{t p r}||_{F},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{T}:=\\{\\sum_{i}\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i})\\}_{m\\in\\mathcal{M}^{*}}}\\end{array}$ denotes the set of all possible TPRs given the choices of $F,\\,R,\\,\\xi_{F}$ , $\\xi_{R}$ and $\\mathcal{M}^{\\ast}$ . Again, we drop the dependency of both $\\psi_{t p r}$ and $m$ on $x$ for ease of notation. ", "page_idx": 17}, {"type": "text", "text": "If $\\mathcal{M}^{\\ast}$ is a subset of the entire set of matching functions, $\\mathcal{M}$ , i.e., if there are some constraints to what flilers can be bound to what roles, then, solving for 10 is NP-complete. If $\\mathcal{M}^{\\ast}$ however corresponds to the entire set of possible matching functions, i.e., $\\mathcal{M}^{\\ast}=\\mathcal{M}$ , then, 10 can be solved by simply using any method that solves the (one-to-many) assignment problem. Noting the poor worst case time complexity of $O(n^{3})$ for such solutions, and guided by our intuition that there should be a more explicit dependency between $\\psi_{t p r}^{*}$ and the structure of $z$ , we thus propose the greedily optimal definition of $\\psi_{t p r}^{*}$ in 5, which creates an explicit dependency between the unbound soft fillers of $z$ $\\{\\tilde{f_{i}}\\}$ , and $\\psi_{t p r}^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "B.3 Soft TPR Autoencoder ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Note that defining $m$ as $m(i)\\ :=\\ \\arg\\operatorname*{min}_{j}||\\tilde{f}_{k}\\ -\\ \\xi_{F}(f_{j})||_{2}$ in Equation 5 effectively permits any filler, $f_{j}$ , to bind to a role, $r_{i}$ . This is somewhat inevitable given the weakly supervised framework we are situated in, as without knowledge of the ground-truth FoV type-token bindings, it is impossible to define $\\mathcal{M}^{\\ast}$ as a subset of all possible matching functions, $\\mathcal{M}$ , that produces the ground-truth set of role-filler binding decompositions, i.e. we cannot know  m \u2208M|{(\u03beF (fm(i)), \u03beR(ri))} = \u03b2(x) x\u2208X. We observe, however, that the size of $\\mathcal{M}$ is combinatorially large, i.e., $\\begin{array}{r l r}{|\\mathcal{M}|}&{{}\\!=\\!}&{{\\binom{N_{F}+N_{R}-1}{N_{R}}}}\\end{array}$ NF +NNR\u22121 , and contains role-filler binding decompositions that are clearly misaligned with the ground-truth semantics of images, e.g., {cube/object colour, purple/object shape, green/orientation, blue/wall colour, $2\\pi/\\mathrm{floor\\colour}\\}$ for the image in Figure 2. We thus, decide to design our Soft TPR Autoencoder with 2 orthogonal and separately achieved aims: 1) representational form, to ensure the produced representation has the form of a Soft TPR, and 2) representational content, to ensure that $m$ produces sensible role-filler bindings, and hence, that the Soft TPR, $z$ , which best approximates $\\psi_{T P R}^{*}$ reflects the ground-truth FoV type-token bindings of the image. ", "page_idx": 17}, {"type": "text", "text": "B.3.1 Representational Form ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We now provide additional details on how our method achieves the representational form property. Recall from Section 4.2, that we penalise the Euclidean distance between the encoder, $E$ \u2019s output, $z$ , and the explicit TPR $\\psi_{t p r}^{*}$ that $z$ best approximates with the analytic form of Equation 5. Such a penalisation should ideally encourage the Soft TPR Autoencoder to produce encodings of inputted images that have the form of a Soft TPR, as all of $E$ \u2019s outputs are directly penalised to satisfy the sufficient closeness property of $||z-\\psi_{t p r}||_{2}^{5}$ for the TPR, $\\psi_{t p r}=\\psi_{t p r}^{*}$ . To recover $\\psi_{t p r}^{*}$ , we use our TPR decoder. As specified in Section 4.2, the TPR decoder consists of 3 main modules, 1) Unbinding, where the soft fillers of $z$ , $\\{\\tilde{f_{i}}\\}$ are unbound from $z$ , 2) Quantisation, where the explicit TPR filler embeddings $\\{\\xi_{F}\\big(f_{m(i)}\\big)\\}$ with the closest Euclidean distances to each soft filler of $z$ are obtained, and 3) TPR Construction, where the required operations are applied to the embedding vectors of the role-filler bindings produced from previous modules, to construct the explicit TPR, $\\begin{array}{r}{\\sum_{i}\\xi_{F}\\big(f_{m(i)}\\big)\\otimes\\xi_{R}\\big(r_{i}\\big)}\\end{array}$ with the analytic form of 5. We now provide additional details on these 3 modules. ", "page_idx": 18}, {"type": "text", "text": "1) Unbinding: While FoV types (roles) can be bound to the same FoV value (fillers), e.g. object colour/magenta, floor colour/magenta, each FoV type (role) clearly represents an independent visual concept type, and so, it is reasonable to use linearly independent embedding vectors for the FoV types. Thus, within our considered domain of visual representation learning, we can reasonably satisfy the linear independence requirement that ensures recoverability of the embedded representational components $(\\bar{\\xi_{F}}\\bar{(f_{m(i)})},\\bar{\\xi_{R}(\\bar{r}_{i})})$ from the TPR representation. Note that any randomly initialised role embedding matrix $M_{\\xi_{R}}\\in\\mathbb{R}^{D_{R}\\times N_{R}}$ is likely (though not guaranteed) to consist of $N_{R}$ linearly independent role embedding vectors provided $D_{R}>>N_{R}$ , and furthermore, that we can encourage that the linear independence property to be preserved during all stages of training by adding orthogonality regularisation, $||M_{\\xi_{R}}^{T}\\bar{M_{\\xi_{R}}}-\\dot{\\bar{I}}_{N_{R}\\times N_{R}}||_{F}$ . There is one pertinent problem, however, if we simply let the role embedding matrix, $M_{\\xi_{R}}$ , consist of any set of arbitrary, linearly independent role embedding vectors, $\\{\\xi_{R}(r_{i})\\}$ . That is, for any choices of $N_{R},D_{R}$ , where these values are large, it is computationally expensive to obtain $U$ , the (left) inverse of $M_{\\xi_{R}}$ required to produce the unbinding vectors, $\\{u_{i}\\}$ , needed for unbinding. We thus, decide to leverage the properties of (left-invertible) semi-orthogonal matrices, $A\\in\\mathbb{R}^{d\\times n}$ , for which $A^{T}A\\,=\\,I_{n\\times n}$ . In this case, $U^{T}=M_{\\xi_{R}}$ , and so, the $i$ -th unbinding vector, $u_{i}$ simply corresponds to the $i$ -the role embedding vector, $\\bar{\\xi}_{R}\\bar{(\\boldsymbol{r}_{i})}$ . Thus, we can simply unbind the soft filler embedding from $z$ \u2019bound\u2019 to role $r_{i}$ by taking the (tensor) inner product between $z$ and $\\xi_{R}(\\boldsymbol{r}_{i})$ , the $i^{\\th}$ -th column of $M_{\\xi_{R}}$ . ", "page_idx": 18}, {"type": "text", "text": "To accomplish this in practice, our unbinding module randomly initialises the role embedding matrix, $M_{\\xi_{R}}$ as a semi-orthogonal matrix, through use of torch.nn.utils.parameterizations.orthogonal, and fixes $M_{\\xi_{R}}$ during all stages of training (i.e., gradient is not backpropagated to $M_{\\xi_{R}}$ ), to ensure that the semi-orthogonal property is preserved during all stages of training. Fixing $M_{\\xi_{R}}$ may prompt concerns that that role embedding vectors do not capture semantically informative content. However, for the visual representation learning domain we consider, it is intuitive that role embeddings themselves do not need to convey much semantic content, as the majority of an image\u2019s semantic content can be conveyed through fliler embeddings and the bindings of flilers to roles. More concretely, fliler embeddings, being learnable, can convey semantic information by being close (in some norm) to one another, e.g., the fliler embeddings for $\\{$ {green, magenta, blue $\\}$ may be close in Euclidean distance to one another, and thus convey the semantic information that they correspond to values for the same (colour) role. Additionally, as the decoder, $D$ , receives information from all embedded bindings in the form of the TPR, $\\psi_{t p r}^{*}$ , to perform image reconstruction, $D$ should intuitively learn through the reconstruction loss term of Equation 6, the semantics associated with a role. For example, given the embedding for binding (oblong/object shape), $D$ should learn from the reconstruction loss, the mapping between the randomly initialised role embedding vector for object colour and the semantics of the corresponding $\\mathrm{FoV}$ type (i.e., that this role changes the colour of the object in the image). ", "page_idx": 18}, {"type": "text", "text": "Now, we proceed to formally prove the aforementioned semi-orthogonality properties. ", "page_idx": 18}, {"type": "text", "text": "We first prove that for a (left-invertible) semi-orthogonal matrix, $A\\in\\mathbb{R}^{d\\times n}$ , $A^{T}A=I_{n\\times n}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $A$ denote a (left-invertible) semi-orthogonal matrix of dimension $\\mathbb{R}^{d\\times n}$ . Writing $A$ out using its columns, $\\{a_{1},\\ldots,a_{n}\\}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\nA^{T}A={\\binom{a_{1}^{T}}{\\vdots}}\\left(a_{1}\\quad\\cdot\\cdot\\quad a_{n}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As the columns $\\{a_{1},\\ldots,a_{n}\\}$ are orthonormal, we have that: $a_{i}\\cdot a_{j}=\\delta_{i j}$ , where $\\cdot$ denotes the dot product, and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta_{i j}={\\binom{1}{0}}_{\\mathrm{otherwise,}}^{\\mathrm{~}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "from which the result clearly follows. ", "page_idx": 19}, {"type": "text", "text": "For completeness, we also prove that $u_{i}$ corresponds to the $i$ -th role embedding vector, though this follows rather trivially from the properties of semi-orthogonal matrices, and the definition of $u_{i}$ . ", "page_idx": 19}, {"type": "text", "text": "orthogonal matrices proved earlier, we have that Proof. Let $M_{\\xi_{R}}$ be a (left-invertible) semi-orthogonal matrix. Then, from the properties of semi- $M_{\\xi_{R}}^{T}M_{\\xi_{R}}=I$ , and so, $U^{T}=\\left(M_{\\xi_{R}}^{T}\\right)^{T}=M_{\\xi_{R}}$ . Hence, the $i$ -th column of $U^{T}$ , which corresponds to the unbinding vector, $u_{i}$ , by definition, is simply $(M_{\\xi_{R}})_{:i}=\\xi_{R}(r_{i})$ , the $i$ -th role embedding vector. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "2) Quantisation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The quantisation module relies on the VQ-VAE vector quantisation algorithm [11] to learn the fliler embedding vectors of the filler embedding matrix, $M_{\\xi_{F}}$ , and to quantise the soft filler embeddings $\\{\\tilde{f_{i}}\\}$ into the explicit filler embeddings $\\{\\xi_{F}(f_{i})\\}$ with the closest Euclidean distances. Briefly speaking, to perform vector quantisation, the VQ-VAE algorithm simply quantises each soft filler $\\bar{\\tilde{f}}_{i}$ into the embedding vector from $M_{\\xi_{F}}$ with the smallest Euclidean distance [11]. This clearly corresponds with the definition of $m$ in Equation 5 (i.e. for each soft filler embedding, $\\tilde{f}_{i}$ , $m$ matches an explicit fliler embedding with the smallest Euclidean distance to $\\tilde{f_{i}}$ to produce $\\psi_{t p r}^{*}$ ). As this quantisation operation corresponds to an argmax and is thus non-differentiable, the VQ-VAE algorithm uses a simple $L_{2}$ loss, the codebook loss, to move the embedding vectors $\\xi_{R}(\\boldsymbol{r}_{i})$ towards the soft fliler embeddings, as captured by the first term of the VQ-VAE quantisation loss term in Equation 6. To prevent the embedding space from growing arbitrarily, a commitment loss, corresponding to the final term of the VQ-VAE quantisation loss in 6 is added, to ensure the encoder commits to an embedding. ", "page_idx": 19}, {"type": "text", "text": "We refer interested readers to [11] for more details on how the quantisation algorithm works. ", "page_idx": 19}, {"type": "text", "text": "3) TPR Construction ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The TPR construction module contains no learnable parameters, and simply deterministically applies the operations required to produce a TPR from the quantised filler embeddings, $\\bar{\\{\\xi_{F}\\big(f_{m(i)}\\big)\\}}$ , with the corresponding role embeddings, $\\{\\xi_{R}(r_{i})\\}$ , from the role embedding matrix, $M_{\\xi_{R}}$ . ", "page_idx": 19}, {"type": "text", "text": "B.3.2 Representational Content ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide an explicit form for the \u2018swapped\u2019 TPRs, $\\psi_{t p r}^{s}(x)$ and $\\psi_{t p r}^{s}(x^{\\prime})$ , used in our weakly supervised reconstruction loss corresponding to the second term of 7. ", "page_idx": 19}, {"type": "text", "text": "Given a pair of images, $(x,x^{\\prime})$ where $x$ and $x^{\\prime}$ share the same role-fliler bindings for all roles but $r_{i}$ , and the identity of $r_{i}$ , but not any of the role-filler bindings are known, we construct the \u2018swapped\u2019 TPRs for $x$ and $x^{\\prime}$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\psi_{t p r}^{s}(x):=\\sum_{j\\neq i}\\xi_{F}(f_{m(j)})\\otimes\\xi_{R}(r_{j})+\\xi_{F}(f_{m^{\\prime}(i)})\\otimes\\xi_{R}(r_{i})}\\\\ {\\displaystyle\\psi_{t p r}^{s}(x^{\\prime}):=\\sum_{j\\neq i}\\xi_{F}(f_{m^{\\prime}(j)})\\otimes\\xi_{R}(r_{j})+\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $m$ and $m^{\\prime}$ denote the matching functions for $x$ and $x^{\\prime}$ respectively. That is, we construct \u2018swapped\u2019 TPRs by simply swapping the quantised filler embedding associated with role $r_{i}$ in constructing new TPR representations for $x$ and $x^{\\prime}$ . Note that in contrast to a similar operation that might be applied to scalar-tokened or vector-tokened compositional representations, swapping the quantised filler between the representations for $x$ and $x^{\\prime}$ in this case produces a global (not local) effect on the resulting representation. ", "page_idx": 20}, {"type": "text", "text": "B.3.3 Model Architecture ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We now provide concrete details of our model architecture. Our model consists of a standard Convbased encoder, $E$ , (Table 8) our TPR decoder, (Table 9) and a standard Conv-transpose based decoder, $D$ (Table 10). The only learnable component of the TPR decoder corresponds to the fliler embedding matrix, $M_{\\xi_{F}}$ , and so, our Soft TPR Autoencoder only adds an additional $D_{F}\\cdot N_{F}$ parameters to a standard (variational) autoencoding framework consisting of the encoder and decoder, where $D_{F}$ and $N_{F}$ correspond to the dimensionality of the fliler embedding space, and number of fliler embedding vectors respectively. ", "page_idx": 20}, {"type": "text", "text": "For all our experiments, we use the same general architecture for the encoder, $E$ , the TPR decoder, and the decoder, $D$ . ", "page_idx": 20}, {"type": "table", "img_path": "oEVsxVdush/tmp/9ea049e9c4d9e9d2f4377e7c866da54539648d5cb5de1dbf8f5372398004186b.jpg", "table_caption": ["Table 8: Encoder $(E)$ architecture. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "oEVsxVdush/tmp/e6f818182383460bfa5a2ec865b37475e3b1eb3f1631d84b0ebf1780e0d75f87.jpg", "table_caption": ["Table 9: TPR Decoder architecture. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "oEVsxVdush/tmp/92ac98c4387a8982f01a701ce3f55dc5ec187ab01178bccfb7ecc8ae1555cefd.jpg", "table_caption": ["Table 10: Decoder $(D)$ architecture. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "B.4 Model Hyperparameters and Hyperparameter Tuning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The tunable hyperparameters of the Soft TPR Autoencoder are the following following: 1) architectural hyperparameters $N_{R},N_{F},D_{R}$ , and $D_{F}$ corresponding to the number of role (respectively fliler) embedding vectors and the dimensionalities of their respective embedding spaces, and 2) loss function hyperparameters $\\beta$ (Equation 6), $\\lambda_{1}$ (Equation 7) and $\\lambda_{2}$ (Equation 7). ", "page_idx": 21}, {"type": "text", "text": "In line with VQ-VAE [11], we set $\\beta$ , the coefficient for the VQ-VAE commitment loss to 0.5. As $N_{R}$ corresponds to the number of FoV types, to ensure fair comparisons with scalar-tokened generative baselines, and COMET, which all assume that the number of FoVs equals 10, (VCT assumes 20), we fix $N_{R}$ to be 10. We tune remaining hyperparameters by running hyperparameter optimisation using the open-source hyperparameter sweep framework of Weights and Biases (WandB). We set the search method as Bayesian search, and the optimisation criterion to be the fully unsupervised MSE reconstruction loss corresponding to the second term in Equation 6. During hyperparameter optimisation, we train all models for between 50,000-100,000 iterations. The obtained hyperparameter values for the Soft TPR Autoencoder are listed in Table 11. See Section C.6.2 for ablation experiments demonstrating our model\u2019s robustness to hyperparameter configurations. ", "page_idx": 21}, {"type": "table", "img_path": "oEVsxVdush/tmp/2f029bee67a1963b9321c0ab5895c4981a9e2d91ad15fb9bc704a6dc199fa003.jpg", "table_caption": ["Table 11: Hyperparameter values. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "In practice, we find that the Soft TPR penalty of Equation 6 is negligible, most likely because the codebook loss of VQ-VAE, which pushes the quantised fillers $\\bar{\\{\\xi_{F}\\bar{(f_{m(i)})\\}}}$ and the soft filler embeddings $\\{\\widetilde{f}_{i}\\}$ of $z$ together, is already sufficient to push $z$ to $\\psi_{t p r}^{*}$ . To prevent redundantly calculating this term, we remove it from the overall loss in the implementation of our model. ", "page_idx": 21}, {"type": "text", "text": "Our model is implemented in Pytorch [27] and trained using the Adam optimiser on the loss corresponding to 7. We use a learning rate of $\\mathrm{1e-4}$ , and the default setting of $(\\bar{\\beta_{1}},\\beta_{2})=(0.9,0.999)$ across all instances of model training. ", "page_idx": 21}, {"type": "text", "text": "C Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide additional details about our experiments, and present additional results. ", "page_idx": 22}, {"type": "text", "text": "C.1 Datasets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For disentanglement, we consider the standard disentanglement datasets: Cars3D [4], Shapes3D [24], and MPI3D [21], where for MPI3D, we consider the \u2018real\u2019 variant of the dataset, (i.e., not the \u2018realistic\u2019, \u2018toy\u2019, or \u2018complex\u2019 variants of the dataset). These datasets contain 3, 6, and 7 ground-truth FoVs respectively, and are procedurally generated by taking the Cartesian product of all possible FoV values for each FoV type. We provide metadata in Tables 12, 13 and 14 and denote all continuousvalued FoV types by the symbol \u2021. As the colours in the Shapes3D dataset correspond to 10 linearly spaced values with a natural ordering (e.g. red, with value 0, is perceptually more similar to orange, with value 1, compared to aqua, with value 5), we treat all FoV colour types in the Shapes3D dataset as continuous-valued variables, in line with [42]. ", "page_idx": 22}, {"type": "table", "img_path": "oEVsxVdush/tmp/f1acd18316276f4ef55d21f4c7630157d284d38309b3b8a46c1ccb6f3b245c92.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "For the downstream task of regression, we simply use the above disentanglement datasets, regressing on each dataset\u2019s continuous-valued FoVs, as indicated by the \u2021 symbol in Tables 12, 13 and 14. For the downstream task of abstract visual reasoning [30], we use the dataset from [30]. Each sample from this dataset consists of a Raven\u2019s Progressive Matrix (RPM) style question (samples of the dataset can be found in [30]). Each RPM-style question has a set of 8 context panels, and 6 answer panels, where each panel corresponds to an image from the Shapes3D dataset. For the model to select the correct answer out of a selection of 6 possible answer panels, it must correctly infer the abstract visual relation that each row of the context panels share. ", "page_idx": 22}, {"type": "text", "text": "C.2 Baseline Implementations and Experimental Settings ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.2.1 Experiment Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For the generative weakly supervised, scalar-valued baselines (i.e., AdaGVAE-k, GVAE, MLVAE, SlowVAE, the Shu model), and our model, we perform model training on a single Nvidia RTX4090 GPU. We also perform all associated experiments for these models (i.e., downstream model training, downstream model evaluation, disentanglement evaluation, etc.) on this single Nvidia RTX4090 GPU. Our model takes approximately 1.5 hours to fully train (i.e., run 200,000 iterations) on the Cars3D and Shapes dataset, and approximately 4.0 hours to fully train on the MPI3D dataset. ", "page_idx": 22}, {"type": "text", "text": "For the vector-valued baselines of COMET and VCT, we perform model training, and all associated experiments on a single Nvidia V100 GPU. ", "page_idx": 22}, {"type": "text", "text": "C.2.2 Disentanglement Models ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For Ada-GVAE, GVAE, MLVAE and SlowVAE, use the Pytorch-based, open-source implementation of [42], which was verified by authors to reproduce official reported results for each model. For COMET, and VCT, we use official open-source implementations published by the authors of the corresponding models [36, 47]. For the GAN-based model of Shu, we convert the official Tensorflowbased implementation of [35] to Pytorch and verify that our implementation reproduces official results. For all baseline models, we use suggested hyperparameters where dataset-specific hyperparameters are given, otherwise, we perform hyperparameter tuning using an identical WandB hyperparameter sweep setup as our model. ", "page_idx": 22}, {"type": "text", "text": "As mentioned in the first paragraph of Section 5, for the weakly supervised baselines of Ada-GVAE, GVAE, MLVAE, SlowVAE and Shu, all models with exception to Ada-GVAE assume access to $I$ , the FoV that differ between each pair in an observed sample $(x,x^{\\prime})$ , and thus have identical levels of weak supervision as our model. Ada-GVAE, however, does not assume access to $I$ , so we make the following modification to make Ada-GVAE more comparable with our model. Ada-GVAE adaptively estimates $I$ using a method that relies on the estimation of $k:=|I|$ , i.e., the number of FoVs that have changed in each observed sample. Thus, we simply amend Ada-GVAE by providing it access to the ground-truth value for $k$ (note that if we instead provide Ada-GVAE with knowledge of the FoVs comprising $I$ itself, it becomes identical to GVAE). We empirically verify that our modification, which we denote by Ada-GVAE- $\\cdot\\mathbf{k}$ , produces superior or in worst case, comparable results to the original Ada-GVAE model. We train all models, with exception to SlowVAE, which assumes all FoV values in observed pairs $(x,x^{\\prime})$ change, with $k=1$ (i.e., only $1\\,\\mathrm{FoV}$ changes between $x$ and $x^{\\prime}$ ). This ensures all models are trained in an identical setting as our framework. ", "page_idx": 23}, {"type": "text", "text": "For the selected hyperparameter configuration associated with each model, including our own, we obtain 5 trained models using 5 random seeds, and collate results over these 5 seeds. All models are trained for 200,000 iterations on all datasets. ", "page_idx": 23}, {"type": "text", "text": "C.2.3 Downstream Models ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For the task of $\\mathrm{FoV}$ regression, in line with [42], we use a simple, generic MLP model, with the architecture listed in Table 15. For each disentanglement dataset, the MLP regression model receives representations of images produced by a representation learner (i.e., a disentanglement model), and is trained to predict the corresponding ground-truth FoV values in a supervised fashion. We evaluate regression performance by computing the $R^{2}$ score on a held out, randomly selected test set of 1,000 samples. ", "page_idx": 23}, {"type": "text", "text": "For each of the 5 instances of a representation learning model produced by the 5 random seeds, we obtain 2 MLPs, resulting in a total of $10\\,\\mathrm{MLP}$ models for each representation learner. We obtain each MLP by uniformly sampling the number of output nodes in the first, second, and fifth layers of the MLP, as indicated in 15. All reported results are averaged over these 10 MLP models. Note that all MLP regression models have no a priori knowledge of the representational form (i.e. scalar-tokened symbolic compositional representation, vector-tokened symbolic compositional representation, fully continuous compositional representation) that they will receive during training as we do not provide any inductive biases to the MLP models or optimise them in any representation-learner specific way (apart from through using the supervised, MSE-based training loss). ", "page_idx": 23}, {"type": "text", "text": "For each MLP model, we use the Adam optimiser on the MSE loss between predicted and ground-truth FoV values, with a learning rate of 1e\u22124, and the default setting of $(\\beta_{1},\\beta_{2})=(0.9,0.999)$ . ", "page_idx": 23}, {"type": "table", "img_path": "oEVsxVdush/tmp/a0ebef0475194b4e2863a2632456d24dfd5671c9497dc585ed0ac296c8fbd7f5.jpg", "table_caption": ["Table 15: MLP architecture "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "For the abstract visual reasoning task, in line with [30], we use the Wild Relation Network (WReN) model [18] on representations obtained by all representation learners to predict the ground-truth answer for each RPM matrix. For each of the 5 instances of a model produced by the 5 random seeds, we randomly sample 2 possible configurations of the WReN model, producing 10 WReN models for each representation learning model. In line with [30], for the edge MLP, $g$ , in the $\\mathrm{WReN}$ model, we uniformly sample either 256 or 512 hidden units. Similarly, we uniformly sample either 128 or 256 hidden units for the graph MLP, $f$ . We however, fix the number of hidden layers in $g$ to 2, and $f$ in 1, representing the smallest possible number of hidden layers, to constrain the capacity of the WReN model. We refer readers to [18] for further details on the WReN architecture. All WReN models are trained using Adam optimisation on the BCE loss between the predicted logits and ground-truth labels, with a learning rate of 1e\u22125 and the default setting of $(\\beta_{1},\\beta_{2})=(0.9,0.999)$ . ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "C.2.4 Experimental Controls ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Recall that our main hypothesis is that neural networks can both learn explicit compositional structure, and leverage it more easily when that structure is instantiated in a fully continuous way, e.g., when embodied by our Soft TPRs. To ensure that the experimental results indeed provide empiricial support for this hypothesis, we apply a series of controls to rule out the contribution of any confounding variables to the empirical results. We detail these controls below. ", "page_idx": 24}, {"type": "text", "text": "Disentanglement Controls ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To ensure that any performance boosts on the disentanglement metrics (shown in Tables 16 and 17) are not attributable to the slight increase in learnable parameters of our model (note that our model only adds one learnable component, i.e., the filler embedding matrix, $M_{\\xi_{F}}$ on top of a standard (variational) autoencoding framework, with this corresponding to 13,568, 1,824, and 1,600 additional parameters for the Cars3D, Shapes3D, and MPI3D datasets respectively), we perform a control to fix the number of learnable parameters in baseline models with fewer learnable parameters compared to our model. The baseline models with fewer learnable parameters than our model are SlowVAE, Ada-GVAE-k, GVAE, ML-VAE, and the Shu model. VCT and COMET are substantially more parameter hungry ( $^{10+}$ million) than our model, so we do not perform the controls on these models. We increase the number of parameters by increasing the number of fliters in the convolution (and transpose-convolution) layers of the generative baseline models, and/or increasing the number of convolution (and transpose-convolution) layers until the modified model has at least as many parameters as our model, repeating this process for each of the disentanglement datasets. We denote these parameter-controlled models in Tables 16 and 17 with the symbol \u2217. In line with [40] and as shown in Tables 16 and 17, we do not observe any increase in disentanglement performance conferred by increasing the number of learnable parameters in these generative, scalar-tokened baselines, so we apply representation learning convergence experiments, and downstream model experiments using the original models, and not their parameter-controlled variants. ", "page_idx": 24}, {"type": "text", "text": "Downstream Task Controls ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To ensure that any performance boosts on the downstream tasks of FoV regression and abstract visual reasoning are not attributable to our representation\u2019s increased dimensionality, we postprocess representations produced by all models (including models producing higher-dimensional representations) to match the dimensionality of our representation. Note that all generative, weakly supervised models producing scalar-tokened representations produce representations with a dimension of 10 for all datasets, which contrasts with the representational dimensions of 1536 (Cars3D), 512 (Shapes3D), and 320 (MPI3D) of our Soft TPRs, highlighting the necessity of such a control. ", "page_idx": 24}, {"type": "text", "text": "To perform this control, we apply separate methods for the scalar-tokened and vector-tokened models. For scalar-tokened models, we multiply each dimension $k$ of the latent vector, $l_{k}$ with an random embedding vector $e_{k}\\in\\mathbb{R}^{d}$ from an fixed, randomly initialised, semi-orthogonal embedding matrix $E\\in\\mathbb{R}^{d\\times\\overline{{1}}0}$ , and subsequently concatenate all multiplied random embedding vectors together to form the dimensionality-controlled representation, $(l_{1}\\dot{e}_{k},\\dots,l_{10}e_{10})\\in\\mathbb{R}^{10d}$ . $d$ is a dataset specific integer that ensures that the size of the dimensionality-controlled representation is at least as small as the dimensionality of the Soft TPR representation for the given dataset, i.e., $d:=\\lceil\\dim(z)\\rceil/10$ , where $z$ is the Soft TPR representation produced by a given dataset. Note that we choose a semiorthogonal embedding matrix with the intuition that this ensures the maximal distinguishability of each continguous subset of dimensions corresponding to $\\boldsymbol{l}_{i}\\boldsymbol{e}_{i}$ . ", "page_idx": 24}, {"type": "text", "text": "For COMET and VCT, which both produce vector-tokened representations, we consider alternative methods of performing dimensionality-control. COMET\u2019s representations have a dimensionality of 640 for all datasets, and so, the model produces lower-dimensional representations than ours for Shapes3D and Cars3D, but not MPI3D. VCT, on the other hand, produces representations with dimension 5120, and so, produces higher-dimensional representations than ours for all datasets. For any vector-tokened representation with higher dimensionality than our representation, we apply PCAbased postprocessing to the model representation, reducing the dimensionality of each vector-tokened value in the representation to the required dimensionality, $d$ , where $d:=\\lceil\\dim(z)\\rceil/\\dim(z_{b a s e l i n e})$ , before concatenating all PCA-reduced vectors together to produce the modified representation. For any vector-tokened representation produced by COMET with lower dimensionality than ours, we apply a simple matrix multiplication between the representation, and a randomly initialised matrix of required dimensionality to embed the representation into the same-dimensional space as ours. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "We denote the results produced by all such postprocessing by \u2020 in relevant tables, and indicate such postprocessing in the captions of relevant plots. ", "page_idx": 25}, {"type": "text", "text": "C.3 Disentanglement ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In reporting the disentanglement metric results for baseline models, we use published results where applicable, i.e., we use the results published in [47] for VCT and COMET and the published results for SlowVAE [39]. For GVAE, MLVAE, and Ada-GVAE-K, we evaluate disentanglement using the Pytorch-based implementation of disentanglement metrics, [42], which corresponds to a Pytorchbased implementation of the official disentanglement lib of [25]. We also use this implementation to evaluate the disentanglement of representations produced by all models. ", "page_idx": 25}, {"type": "text", "text": "C.3.1 Disentanglement Metrics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In line with [47], we consider the following 4 disentanglement metrics: the FactorVAE score [24], the DCI Disentanglement score [16] (we refer to DCI Disentanglement as DCI), the BetaVAE score [10], and the MIG score [15]. We provide a brief overview of all 4 disentanglement metrics, and refer interested readers to the papers these metrics are introduced in for further details, as well as the Appendix of [25] for more details on how the metrics are implemented in [42] and [25]. ", "page_idx": 25}, {"type": "text", "text": "FactorVAE Metric: A randomly selected FoV of the dataset is fixed, and a mini-batch of observations is subsequently randomly sampled. The representation learner produces representations for the samples. Disentanglement is quantified using the accuracy of a majority vote classifier that predicts the index of the ground-truth fixed FoV based on the index of the representation vector with the smallest variance. ", "page_idx": 25}, {"type": "text", "text": "DCI Metric: A Gradient Boosted Tree (GBT) is trained to predict the ground-truth FoV values from representations produced by a representation learner. The predictive importance of the dimensions of a representation is obtained using the model\u2019s feature importances. For each sample, a score is computed that corresponds to one minus the entropy of the probability that a dimension of the learned representation is useful for FoV prediction, weighted by the relative entropy of the corresponding dimension. An average of these scores over the mini-batch of samples is taken to produce the final score. ", "page_idx": 25}, {"type": "text", "text": "BetaVAE Metric: This metric quantifies disentanglement by predicting the index of a fixed FoV from representations produced by a representation learner, similar to the FactorVAE metric. In contrast to the FactorVAE metric, however, the BetaVAE metric uses a linear classifier on difference vectors to predict the index of the fixed FoV. Each difference vector is produced by taking the difference between representations produced for a pair of samples $(x,x^{\\prime})$ with one underlying fixed FoV. ", "page_idx": 25}, {"type": "text", "text": "MIG Metric: For each FoV, the MIG metric computes the mutual information between each dimension in the representation, and the corresponding FoV. The score is obtained by computing the average, normalised difference between the highest and second highest mutual information of each FoV with the dimensions of the representation. ", "page_idx": 25}, {"type": "text", "text": "C.3.2 Evaluating the Disentanglement of Soft TPRs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Since disentanglement metrics are typically computed under the assumption that the representation corresponds to a concatenation of scalar-valued FoV tokens, we now detail how we compute the above disentanglement metrics on the Soft TPR, a continuous compositional representation. Similar amendments have been made in COMET and VCT, so that the disentanglement of their representations, corresponding to a concatenation of vector-valued FoV tokens, can be computed. ", "page_idx": 25}, {"type": "text", "text": "FactorVAE metric: To compute the FactorVAE score of our Soft TPR, we produce a $N_{R}$ -dimensional vector, $v$ , for each Soft TPR, where $N_{R}$ corresponds to the number of roles, i.e. the FoV types. We produce $v$ by simply populating each dimension, $v_{i}$ , with the index of the quantised fliler that role $r_{i}$ is bound to, i.e. we set $v_{i}$ to $m(i)$ . We use the resulting $v$ \u2019s to compute the variances required by the FactorVAE metric, noting that if the FoV type corresponding to role $i$ is fixed, and this is reflected in our representation, dimension $i$ of the $v$ vectors produced in a mini-batch should clearly have the smallest variance, as all fillers, $f_{m(i)}$ , that role $r_{i}$ binds to, will have the same identity across the mini-batch. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "DCI metric: As the DCI relies on computing the ground-truth FoV values from $N_{R}$ -dimensional representations produced by the representation learner, we again, follow a similar procedure as the above, in converting our $(D_{F}\\cdot D_{R})$ -dimensional Soft TPR representation to a $N_{R}$ -dimensional representation. That is, we simply consider a $N_{R}$ -dimensional vector, $v$ , where each dimension, $v_{i}$ , is populated by the index of the quantised filler, $m(i)$ that role $r_{i}$ is bound to, and use this vector to compute the corresponding DCI result. ", "page_idx": 26}, {"type": "text", "text": "BetaVAE metric: For the BetaVAE metric, as each sample used to train the linear classifier consists of a $N_{R}$ -dimensional difference vector obtained by computing the difference between the $N_{R}$ - dimensional scalar-tokened compositional representations, we obtain the following difference vector, $d$ , for our Soft TPR representations. For each role $i\\in\\{1,\\ldots,N_{R}\\}$ , we obtain the corresponding quantised filler embeddings for each sample $x,x^{\\prime}$ in the pair, i.e., we obtain $\\xi_{F}(f_{m(i)}),\\xi_{F}(f_{m^{\\prime}(i)})$ . For each pair of quantised filler embeddings, we obtain a scalar distance measure corresponding to the cosine similarity between the the pair. The $i$ -th dimension of $d$ is populated using this value. We use difference vectors obtained in this way to compute the BetaVAE metric. ", "page_idx": 26}, {"type": "text", "text": "MIG metric: For the MIG metric, which relies on a discretisation of the values in each dimension $i$ of the $N_{R}$ -dimensional scalar-tokened compositional representations to compute the discrete mutual information, we apply the same postprocessing technique as in the FactorVAE, and DCI metric, to evaluate MIG on our Soft TPRs. That is, we produce the same $N_{R}$ -dimensional vector, $v$ , noting that this choice of postprocessing also performs the discretisation required by the MIG computation. ", "page_idx": 26}, {"type": "text", "text": "C.3.3 Full Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We now present unabridged results for all considered disentanglement metrics, denoting the learnable parameter control modification of each relevant baseline with the symbol \u2217. As can be seen in Tables 16 and 17, our Soft TPR representations are explicitly more compositional (as quantified by the disentanglement metric scores) compared to all considered baselines, with especially notable performance increases on the more challenging datasets of Cars3D and MPI3D. ", "page_idx": 26}, {"type": "table", "img_path": "oEVsxVdush/tmp/8410f501f8ed2b5a5c019256f5c02fc9c266ee3de2f3a0f48799b92503c510fc.jpg", "table_caption": ["Table 16: FactorVAE and DCI scores "], "table_footnote": [""], "page_idx": 26}, {"type": "text", "text": "C.4 Representation Learning Convergence ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We additionally examine representation learning convergence by evaluating the representations produced at 100, 1,000, 10,000, 100,000, and 200,000 iterations of model training, where the latter stage of 200,000 iterations corresponds to fully trained models. To quantify representation learning convergence, we evaluate both 1) the explicit compositionality of representations produced at these stages of training (as quantified by disentanglement metric performance), and 2) the usefulness of these representations for the downstream tasks of FoV regression and abstract visual reasoning. ", "page_idx": 26}, {"type": "table", "img_path": "oEVsxVdush/tmp/f839f1946df4f2dfb5e7f0902c6f236b8bb0b3623e32ca32e2c363d567671d7c.jpg", "table_caption": ["Table 17: BetaVAE and MIG scores "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "As mentioned in Section 5.1, the representation learning convergence of our model as measured by disentanglement performance is comparable with baselines, however, our model consistently converges faster than baselines in producing representations that can be effectively leveraged for both downstream tasks, as indicated by higher downstream model performance across the majority of representation learner training stages. We now present the full suite of unabridged results, first presenting disentanglement results, and subsequently downstream results. ", "page_idx": 27}, {"type": "text", "text": "In all line plots, we plot the mean, and indicate the standard deviation by the shaded regions. We use the same legend for all plots, where 0 (grey) denotes SlowVAE, 1 (orange) denotes AdaGVAE-k, 2 (green) denotes GVAE, 3 (red) denotes MLVAE, 4 (purple) denotes Shu, 5 (pink) denotes VCT, 6 (brown) denotes COMET, and 7 (blue) denotes our model, Soft TPR Autoencoder. ", "page_idx": 27}, {"type": "text", "text": "C.4.1 Disentanglement ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We first present line plots of representation learner convergence for each of the four considered disentanglement metrics (i.e., the FactorVAE, DCI, BetaVAE and MIG scores) for all three disentanglement datasets (Cars3D, Shapes3D, MPI3D). A series of tables containing the values associated with these line plots is presented following the plots. As the disentanglement results produced by our learnable parameter controls for the models of Ada-GVAE-k, GVAE, ML-VAE and Shu, do not achieve superior disentanglement results compared to the original models, we only present disentanglement convergence results for the original variants of all baseline models. ", "page_idx": 27}, {"type": "image", "img_path": "oEVsxVdush/tmp/6d83a0df8456cc617fb37149a83ad4e9f0be711c4d08a970a6f3549a9087932e.jpg", "img_caption": ["Figure 3: Factor score convergence on the Cars3D dataset "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "oEVsxVdush/tmp/08e93f6a655b90145109d7b14819595e3073cc40aba26af4488c5da2b429a350.jpg", "img_caption": ["Figure 4: DCI score convergence on the Cars3D dataset "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "oEVsxVdush/tmp/029c69857d72afc192c8b52d6c91671d4a59de6394faa17f651fe13dee174a9e.jpg", "img_caption": ["Figure 5: BetaVAE score convergence on the Cars3D dataset "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "oEVsxVdush/tmp/22df4ff6baeb8f507e0f8c793c0d341f312b348e2bc76195f8056470785fe355.jpg", "img_caption": ["Figure 6: MIG score convergence on the Cars3D dataset "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "oEVsxVdush/tmp/86c07e2ddd5c95e0ba80fcaea9c93c29bcd311d631f3b3eaec9a7e584fcb97c6.jpg", "img_caption": ["Figure 7: Factor score convergence on the Shapes3D dataset "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "oEVsxVdush/tmp/2ece23147e9a85568a5abeee6cf343d300a83ed77a5d3ef6b7bdb960fe6dcb84.jpg", "img_caption": ["Figure 8: DCI score convergence on the Shapes3D dataset "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "oEVsxVdush/tmp/f03a8e7f5a482e0a9ea8212223665f51e430ccde7ced85598fbc3055b1247d5a.jpg", "img_caption": ["Figure 9: BetaVAE score convergence on the Shapes3D dataset "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "oEVsxVdush/tmp/07cc6ee849a3ffd3730605a6c4f0c5caccc5a8ba44f08057234ce3b3053140bc.jpg", "img_caption": ["Figure 10: MIG score convergence on the Shapes3D dataset "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "oEVsxVdush/tmp/558766c66a111ab12bfa1453484e384adb652c57171728e3ca09f179864da523.jpg", "img_caption": ["Figure 11: Factor score convergence on the MPI3D dataset "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "oEVsxVdush/tmp/18cbb514b0e7975564557314d72b859be11e2e2b096e2277e8f483289fdd7f8b.jpg", "img_caption": ["Figure 12: DCI score convergence on the MPI3D dataset "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "oEVsxVdush/tmp/5e6876d78dea84d3482ba44276a0883b46a78b7c53fcb8bc2b69fb43ae86d5ce.jpg", "img_caption": ["Figure 13: BetaVAE score convergence on the MPI3D dataset "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "oEVsxVdush/tmp/d2ad10eb72ae52254fa3b0ccc0719a16b99ed2a0002cac6da9437dcfbd590852.jpg", "img_caption": ["Figure 14: MIG score convergence on the MPI3D dataset "], "img_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "oEVsxVdush/tmp/44fb563a65caa0c40c28909d3dd6eebebb3ad1446f41cc6b825f8222390d8ad6.jpg", "table_caption": ["Table 18: Representation learner convergence on the Cars3D dataset (Factor score) "], "table_footnote": ["Ours 0.639 \u00b1 0.045 0.898 \u00b1 0.038 0.999 \u00b1 0.001 0.999 \u00b1 0.001 0.999 \u00b1 0.001 "], "page_idx": 34}, {"type": "table", "img_path": "oEVsxVdush/tmp/44228971646459332d7772d1902b54ce289fa7ea97b0854e32cb94c285752f0b.jpg", "table_caption": ["Table 19: Representation learner convergence on the Cars3D dataset (DCI score) "], "table_footnote": ["Ours 0.379 \u00b1 0.117 0.381 \u00b1 0.026 0.812 \u00b1 0.033 0.843 \u00b1 0.024 0.832 \u00b1 0.048 "], "page_idx": 34}, {"type": "table", "img_path": "oEVsxVdush/tmp/4e738b169adc7010eb22df65e29e6970f0ed9c873893a4205786329e83c77b7f.jpg", "table_caption": ["Table 20: Representation learner convergence on the Cars3D dataset (BetaVAE score) "], "table_footnote": ["Ours 0.953 \u00b1 0.042 1.000 \u00b1 0.000 (=) 1.000 \u00b1 0.000 (=) 1.000 \u00b1 0.000 (=) 1.000 \u00b1 0.000 "], "page_idx": 34}, {"type": "table", "img_path": "oEVsxVdush/tmp/4d0434ef9eb5105f8242585792f130d7e7fa658e6f50f3f9cf592264e2a244b9.jpg", "table_caption": ["Table 21: Representation learner convergence on the Cars3D dataset (MIG score) "], "table_footnote": ["Ours 0.016 \u00b1 0.010 0.136 \u00b1 0.009 0.344 \u00b1 0.011 0.349 \u00b1 0.010 0.349 \u00b1 0.010 "], "page_idx": 35}, {"type": "table", "img_path": "oEVsxVdush/tmp/b8ec91aade3c5e5846fde192e8dae3ddbf8ca7021762b2671dab3f9c40b18dce.jpg", "table_caption": ["Table 22: Representation learner convergence on the Shapes3D dataset (Factor score) "], "table_footnote": ["Ours 0.356 \u00b1 0.011 0.435 \u00b1 0.048 0.975 \u00b1 0.040 0.995 \u00b1 0.005 0.960 \u00b1 0.053 "], "page_idx": 35}, {"type": "table", "img_path": "oEVsxVdush/tmp/124c274d5ba43a3457bd10ac86eb843bbaea2328d5f46690b64aaf3d5aba4086.jpg", "table_caption": ["Table 23: Representation learner convergence on the Shapes3D dataset (DCI score) "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "oEVsxVdush/tmp/5e389eaa73731f945e13245b9dfd68da9be507e5120842e52cd68ae52ebcf280.jpg", "table_caption": ["Table 24: Representation learner convergence on the Shapes3D dataset (BetaVAE score) "], "table_footnote": [""], "page_idx": 36}, {"type": "table", "img_path": "oEVsxVdush/tmp/106d1008ee3e642dc79f9e14ca9515b2722afa919e9f71c111f01e8b33c93c10.jpg", "table_caption": ["Table 25: Representation learner convergence on the Shapes3D dataset (MIG score) "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "oEVsxVdush/tmp/c7cac222bbf09518f1a1ae5060d001f4d0991aa2995f646b312a1f52431de274.jpg", "table_caption": ["Table 26: Representation learner convergence on the MPI3D dataset (Factor score) "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "oEVsxVdush/tmp/d9c779d3d38833bf9aab71f74fc23f203764eeb2ebeea238b452923b3d755ba9.jpg", "table_caption": ["Table 27: Representation learner convergence on the MPI3D dataset (DCI score) "], "table_footnote": ["Ours 0.172 \u00b1 0.051 0.174 \u00b1 0.037 0.670 \u00b1 0.060 0.813 \u00b1 0.010 0.799 \u00b1 0.078 "], "page_idx": 37}, {"type": "table", "img_path": "oEVsxVdush/tmp/51faab360fb8c8e8e0208205c22d52a7d452a14f443fb6d62de2711395baf85f.jpg", "table_caption": ["Table 28: Representation learner convergence on the MPI3D dataset (BetaVAE score) "], "table_footnote": ["Ours 0.302 \u00b1 0.011 0.577 \u00b1 0.021 0.973 \u00b1 0.048 0.980 \u00b1 0.022 0.976 \u00b1 0.024 "], "page_idx": 37}, {"type": "table", "img_path": "oEVsxVdush/tmp/2c9d23708de7f598b3e0acf1e3306e196b5ec405471d0d74f022cdf4399e0684.jpg", "table_caption": ["Table 29: Representation learner convergence on the MPI3D dataset (MIG score) "], "table_footnote": ["Ours 0.037 \u00b1 0.020 0.046 \u00b1 0.018 0.393 \u00b1 0.063 0.612 \u00b1 0.068 0.514 \u00b1 0.202 "], "page_idx": 37}, {"type": "text", "text": "C.4.2 Downstream Performance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We additionally evaluate the representation learning convergence by examining the usefulness of representations produced at different stages of training (i.e., 100, 250, 500, 1,000, 10,000, 100,000 and 200,000 iterations of training). To quantify \u2018usefulness\u2019, we consider performance of downstream models on the tasks of FoV regression, and abstract visual reasoning when trained using representations produced by each stage of training. For each task, we present line plots, and additionally tables with values corresponding to each of the plots. We use the same legend as in the previous section, where 0 (grey) denotes SlowVAE, 1 (orange) denotes AdaGVAE-k, 2 (green) denotes GVAE, 3 (red) denotes MLVAE, 4 (purple) denotes Shu, 5 (pink) denotes VCT, 6 (brown) denotes COMET, and 7 (blue) denotes our model, Soft TPR Autoencoder. We additionally provide all results for the dimensionality-control setting, where the dimensionality of representations produced by all representation learners is held constant following the approach detailed in C.2.4, denoting this clearly in plot captions, and by the symbol \u2020 in the tables. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "FoV Regression ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "As clearly visible in the tables and plots, generic regression models are able to more effectively use representations produced by our Soft TPR Autoencoder produced across almost all stages of training for all disentanglement datasets. Improvements are most notable in the low-iteration regime of $\\mathrm{10^{\\overline{{2}}}}$ iterations, and across most stages of training for the more challenging task of FoV regression on the MPI3D dataset (Figures 19 and 20). ", "page_idx": 38}, {"type": "image", "img_path": "oEVsxVdush/tmp/89be323a3d9f89a49b88c208ea6e009cf09510558baf79a1f9b7bd61aea6269e.jpg", "img_caption": ["Figure 15: Convergence of representation learners as measured by FoV regression on the Cars3D dataset (original setting) "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "oEVsxVdush/tmp/3f2c609c4cad5f3c5d37fdadbe725fb58642fad2bc442b162dfddf8a858ff92d.jpg", "img_caption": ["Figure 16: Convergence of representation learners as measured by FoV regression on the Cars3D dataset (dimensionality-controlled setting) "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "oEVsxVdush/tmp/f06a6fe63b9f3ea540b60d3daffabcb729ee5854be9c601fa988d5b9b511edc0.jpg", "img_caption": ["Figure 17: Convergence of representation learners as measured by FoV regression on the Shapes3D dataset (original setting) "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "oEVsxVdush/tmp/71b9d7ce3523c751deeb1bdff288ebff0e2092440b805dfed5ae97f6879d3ced.jpg", "img_caption": ["Figure 18: Convergence of representation learners as measured by FoV regression on the Shapes3D dataset (dimensionality-controlled setting) "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "oEVsxVdush/tmp/bee6c8f9503d9238f9ff2bb80ec3f6fbb8f0d700dd7c55868c2fe3c5e81da15c.jpg", "img_caption": ["Figure 19: Convergence of representation learners as measured by FoV regression on the MPI3D dataset (original setting) "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "oEVsxVdush/tmp/1d4f76a77b038d6d33d182627e3cc1e44f3579efd8ad134c4c57943fbe88743a.jpg", "img_caption": ["Figure 20: Convergence of representation learners as measured by FoV regression on the MPI3D dataset (dimensionality-controlled setting) "], "img_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "oEVsxVdush/tmp/f572ae57e1f9bcac9a25a4be0a4dc16b9174850e9de4ffc296674ba12e5fbc5f.jpg", "table_caption": ["Table 30: Convergence of representation learners as measured by FoV regression on the Cars3D dataset "], "table_footnote": [], "page_idx": 42}, {"type": "table", "img_path": "oEVsxVdush/tmp/0f2211c9c1e23e3ca3c463fb29aa0e695e88bc9238e290cfdb88b88919632dc1.jpg", "table_caption": ["Table 31: Convergence of representation learners as measured by FoV regression on the Shapes3D dataset "], "table_footnote": ["Ours 0.981 \u00b1 0.003 0.989 \u00b1 0.007 1.000 \u00b1 0.000 (=) 1.000 \u00b1 0.000 (=) 1.000 \u00b1 0.000 (=) "], "page_idx": 42}, {"type": "table", "img_path": "oEVsxVdush/tmp/451c59b7c08fce993fd067f8e18ea3c2840913c3341856913b5a1034baea98db.jpg", "table_caption": ["Table 32: Convergence of representation learners as measured by FoV regression on the MPI3D dataset "], "table_footnote": ["Ours 0.741 \u00b1 0.058 0.914 \u00b1 0.012 0.891 \u00b1 0.011 0.882 \u00b1 0.016 "], "page_idx": 43}, {"type": "text", "text": "Abstract Visual Reasoning ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We present now present our full suite of results for the downstream task of abstract visual reasoning. As demonstrated in Table 33 and the corresponding Figures 21 and 22, representations produced by our model at only $10^{2}$ iterations of training are able to be leveraged by downstream models to achieve a $80.04\\%$ accuracy for the challenging abstract visual reasoning task, in contrast to the value of $63.1\\%$ obtained by the best performing baseline, representing a $2\\bar{6}.78\\%$ performance increase. This again provides strong empirical support for our hypothesis that the approximate, continuouslyinstantiated compositional structure embodied by our Soft TPR can be learnt more quickly by representation learners than alternative, symbolic representations of compositional structure, thereby allowing downstream models to effectively leverage these representations despite a small number of representation learner training iterations. ", "page_idx": 43}, {"type": "image", "img_path": "oEVsxVdush/tmp/9eb09117345ed2ef13dde5ddc5502085dbda029d26d19f2cfe08113b9b093781.jpg", "img_caption": ["Figure 21: Convergence of representation learners as measured by classification performance on the abstract visual reasoning dataset (original setting) "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "oEVsxVdush/tmp/ea30951a3df62dd7c222a83c5d60fc066f48076aa5d6cb1e1bd8c9e1ec9344e2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "Figure 22: Convergence of representation learners as measured by classification performance on the abstract visual reasoning dataset (dimensionality-controlled setting) ", "page_idx": 44}, {"type": "table", "img_path": "oEVsxVdush/tmp/ab36a7ad114f0960a3abb336797a1f345bf0b2ffe3f9cf4fd69bbfe527f3773a.jpg", "table_caption": ["Table 33: Convergence of representation learners as measured by classification performance on the abstract visual reasoning dataset "], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "C.5 Downstream Performance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "We present our full suite of experimental results that empirically demonstrate the utility of our Soft TPR representation from the perspective of downstream models, by considering the sample efficiency, and low-sample regime performance of downstream models on the tasks of FoV regression, and abstract visual reasoning. ", "page_idx": 45}, {"type": "text", "text": "C.5.1 Sample Efficiency Results ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "For sample efficiency, as mentioned in Section 5.2, and in line with [25], we compute a ratio-based metric obtained by dividing the performance of the downstream model when trained using a restricted number of 100, 250, 500, 1,000 and 10,000 samples, by its performance when trained using all samples. The total number of samples corresponds to 19,104, 480,000, 1,036,800 and 100,000 for the tasks of regression on the Cars3D, Shapes3D, MPI3D datasets, and the abstract visual reasoning task respectively. As this metric is dependent on the performance of downstream models when trained using all samples, we do not compute this metric for representation learners where the corresponding downstream models achieve an $\\dot{R}^{2}$ score of less than 0.5 for regression, as this may produce sample efficiency scores with very little semantic meaning (e.g. a model that achieves a sample efficiency score of 0.9 when its final $R^{2}$ score is 0.1). This corresponds to removing the Shu model from Shapes3D sample efficiency calculations, and COMET and Shu from the Cars3D sample efficiency calculations. ", "page_idx": 45}, {"type": "text", "text": "As many models for the abstract visual reasoning task have low classification accuracies on the held-out test set following training with the maximal number of 100,000 samples, we do not compute sample efficiencies for this task, and instead refer readers to results in Section C.5.2 for the raw classification accuracies associated with each model. ", "page_idx": 45}, {"type": "text", "text": "Note that for all box plots, we follow standard convention, and display the median in each box with a solid line, where the box shows the quartiles of the corresponding values, and the whiskers extend to 1.5 times the interquartile range. We again, use the same legend, where grey denotes SlowVAE, orange denotes AdaGVAE-k, green denotes GVAE, red denotes MLVAE, purple denotes Shu, pink denotes VCT, brown denotes COMET, and blue denotes our model, Soft TPR Autoencoder. ", "page_idx": 45}, {"type": "image", "img_path": "oEVsxVdush/tmp/9132be8ea05ddc33b378bbe1e6ba8cc326a1ae3eb4f62e393d9827534f2fab9d.jpg", "img_caption": ["Figure 23: Downstream regression model sample efficiency on the Cars3D dataset (original setting). "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "oEVsxVdush/tmp/bdbcfaad1442a10cd11eeafc53b7f933e0f394670e9c70d606adf7c116ce7065.jpg", "img_caption": ["Figure 24: Downstream regression model sample efficiency on the Cars3D dataset (original setting). "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "oEVsxVdush/tmp/9449279bcff391097bcf7dcc7760f3f8f0033167f1d3283e7e850551178e53e1.jpg", "img_caption": ["Figure 25: Downstream regression model sample efficiency on the Cars3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "oEVsxVdush/tmp/75fa94553fa40581b1a3781869a3bf926ccb623325f8168fc692d1a236ffe6cb.jpg", "img_caption": ["Figure 26: Downstream regression model sample efficiency on the Cars3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "oEVsxVdush/tmp/cc73f2c1d4f2d9d71545d5d018fcf4614d56548dce2856ebd7873a3a98273c63.jpg", "img_caption": ["Figure 27: Downstream regression model sample efficiency on the Shapes3D dataset (original setting). "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "oEVsxVdush/tmp/229eb652399edfc218cbd533eb8ffaef1de146c9885fbb2cad5795a4b420e1f4.jpg", "img_caption": ["Figure 28: Downstream regression model sample efficiency on the Shapes3D dataset (original setting). "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "oEVsxVdush/tmp/619a1f392cc357c86ade41c2603f35bcb958a6b0f65e354e58b27dba4d6ed911.jpg", "img_caption": ["Figure 29: Downstream regression model sample efficiency on the Shapes3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "oEVsxVdush/tmp/33aa921af01a741db7208473e86b5626f46820f81598805662a71ff4eba23001.jpg", "img_caption": ["Figure 30: Downstream regression model sample efficiency on the Shapes3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "oEVsxVdush/tmp/ad9e79dd83fb1ac2abc2f4458e2f3f879853e7e271cba9ae1553e7350ce3b057.jpg", "img_caption": ["Figure 31: Downstream regression model sample efficiency on the MPI3D dataset (original setting). "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "oEVsxVdush/tmp/9761536556c42896dfdcc50bc13dcd16168ef53f1231fe1faa9a4aff644e2bc0.jpg", "img_caption": ["Figure 32: Downstream regression model sample efficiency on the MPI3D dataset (original setting). "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "oEVsxVdush/tmp/36fba0d07f0e0df4bd0f7b841f0df03a8de08b0e6c617eceb797d10eec1e018e.jpg", "img_caption": ["Figure 33: Downstream regression model sample efficiency on the MPI3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 51}, {"type": "image", "img_path": "oEVsxVdush/tmp/280877fd1acf6943071a8784da8f3eebe8f89a3f0172b6e190663b159f6d2914.jpg", "img_caption": ["Figure 34: Downstream regression model sample efficiency on the MPI3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 51}, {"type": "table", "img_path": "oEVsxVdush/tmp/ab084130ae3e6823519763f7828c5a020a8380e5102b64479ab34f0c1c21df87.jpg", "table_caption": ["Table 34: Downstream regression model sample efficiency on the Cars3D dataset "], "table_footnote": [], "page_idx": 52}, {"type": "table", "img_path": "oEVsxVdush/tmp/a53d3d38140b4ea50f84ee268d26e7bf9b294977ca9ffb05bb098171cfa67347.jpg", "table_caption": ["Table 35: Downstream regression model sample efficiency on the Shapes3D dataset "], "table_footnote": [""], "page_idx": 52}, {"type": "table", "img_path": "oEVsxVdush/tmp/35242e26818884e8241560021515a1c2ea534c79ec948d2bee04897ff9a7dbe9.jpg", "table_caption": ["Table 36: Downstream regression model sample efficiency on the MPI3D dataset "], "table_footnote": [""], "page_idx": 53}, {"type": "text", "text": "C.5.2 Low Sample Regime Results ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "To evaluate the utility of our Soft TPR representation from the perspective of downstream models, we additionally evaluate the raw performance of downstream models as a function of the number of samples they have been trained on (again considering 100, 250, 500, 1,000, 10,000 and all samples). We find that our Soft TPR representation contributes to a substantial performance boost in the downstream model\u2019s performance in a low-sample regime where the downstream model has been trained with 100, 250, 500, and 1,000 samples. We present our full suite of experimental results, and highlight the particular performance differentials conferred by our representational form in the low-sample regime. ", "page_idx": 53}, {"type": "image", "img_path": "oEVsxVdush/tmp/749e245ecf974a9f73193f188106155c49906c5908febc9c124bcf461697bab2.jpg", "img_caption": ["Figure 35: Downstream regression model $R^{2}$ scores on the Cars3D dataset (original setting). "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "oEVsxVdush/tmp/2dd8fc5b616254c85b208f076867723a758d21c679418407c79438a8d7755212.jpg", "img_caption": ["Figure 36: Downstream regression model $R^{2}$ scores on the Cars3D dataset (original setting). "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "oEVsxVdush/tmp/c0f0041d11ab78c156b1b5d698bd442771da6238ec13301e716b3832ec4a8f15.jpg", "img_caption": ["Figure 37: Downstream regression model $R^{2}$ scores on the Cars3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "oEVsxVdush/tmp/aa41f96f6ca74504b67e678e4429dbf9ed4df64ffa6707c7e2fa08704f34abee.jpg", "img_caption": ["Figure 38: Downstream regression model $R^{2}$ scores on the Cars3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "oEVsxVdush/tmp/3ea5740b34b15ef7102bf5099558a988f01f87f58ccd7793eda33f869ce1512a.jpg", "img_caption": ["Figure 39: Downstream regression model $R^{2}$ scores on the Shapes3D dataset (original setting). "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "oEVsxVdush/tmp/fd3b8a68404e448d682e77fbbb6fb38ca580244b46948a067475a326d82d4386.jpg", "img_caption": ["Figure 40: Downstream regression model $R^{2}$ scores on the Shapes3D dataset (original setting). "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "oEVsxVdush/tmp/08dc7ee7de7059b49559ddeed004f4905c5a6a54532f74bb89c30448b022e0f1.jpg", "img_caption": ["Figure 41: Downstream regression model $R^{2}$ scores on the Shapes3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "oEVsxVdush/tmp/84aa863100b5a0b24a38c9313933b5c38f775c35fa24355af99b9d8a7dcd49d1.jpg", "img_caption": ["Figure 42: Downstream regression model $R^{2}$ scores on the Shapes3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "oEVsxVdush/tmp/470972c74dcb74f7976077db0c5bb591b7a9b87bf9758593c558f239b2697a43.jpg", "img_caption": ["Figure 43: Downstream regression model $R^{2}$ scores on the MPI3D dataset (original setting). "], "img_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "oEVsxVdush/tmp/a8cf34beffac45e8e1469816d154fcfb53fb2b83a1328a6f094db1f1f1d90092.jpg", "img_caption": ["Figure 44: Downstream regression model $R^{2}$ scores on the MPI3D dataset (original setting). "], "img_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "oEVsxVdush/tmp/cc256c775b4ef29dfabe2e90d7db70ff6ff87dd35d7338d596a35d5336a31c27.jpg", "img_caption": ["Figure 45: Downstream regression model $R^{2}$ scores on the MPI3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 59}, {"type": "image", "img_path": "oEVsxVdush/tmp/0487d93d487c5f9e3a4cfcd4baa2ae505ea982b1bf3448cb95cb044ee27feb8e.jpg", "img_caption": ["Figure 46: Downstream regression model $R^{2}$ scores on the MPI3D dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 59}, {"type": "table", "img_path": "oEVsxVdush/tmp/5829ef0cbd98e620adb2209e0a8ebd1b7af7aa14ab5fe35b205b988a0e3a146c.jpg", "table_caption": ["Table 37: Downstream regression model performance on the Cars3D dataset "], "table_footnote": [], "page_idx": 60}, {"type": "table", "img_path": "oEVsxVdush/tmp/3663fea0bf9e373f1b53d31600f8efd8d905a388ece0f71d20dc9421c31e5a9e.jpg", "table_caption": ["Table 38: Downstream regression model performance on the Shapes3D dataset "], "table_footnote": [""], "page_idx": 60}, {"type": "table", "img_path": "oEVsxVdush/tmp/5f9991ebbf328f5905983afcbc43b887bfa2016998bcbccdc2ac5c358d5f2b0f.jpg", "table_caption": ["Table 39: Downstream regression model performance on the MPI3D dataset "], "table_footnote": [], "page_idx": 61}, {"type": "image", "img_path": "oEVsxVdush/tmp/e4153ee0ee9282ebd742d313c32501e59d703526eb8d6bbdea74f86f963ac61e.jpg", "img_caption": ["Figure 47: Downstream WReN model classification accuracy on the abstract visual reasoning dataset (original setting). "], "img_footnote": [], "page_idx": 62}, {"type": "image", "img_path": "oEVsxVdush/tmp/b8060323be6c86f6eefde099393dda51524953e7c1a684dd703f79208993948d.jpg", "img_caption": ["Figure 48: Downstream WReN model classification accuracy on the abstract visual reasoning dataset (original setting). "], "img_footnote": [], "page_idx": 62}, {"type": "image", "img_path": "oEVsxVdush/tmp/98217d1eb4bf2ddd8a2e44eb082ae06442d5d8cf63b489af7b674677f3c93281.jpg", "img_caption": ["Figure 49: Downstream WReN model classification accuracy on the abstract visual reasoning dataset (dimensionality-controlled setting). "], "img_footnote": [], "page_idx": 63}, {"type": "image", "img_path": "oEVsxVdush/tmp/2b60e594b1e4d24dccf20902c9875d1073e74a29b15c40714995f9f3088c0476.jpg", "img_caption": [], "img_footnote": [], "page_idx": 63}, {"type": "text", "text": "Figure 50: Downstream WReN model classification accuracy on the abstract visual reasoning dataset (dimensionality-controlled setting). ", "page_idx": 63}, {"type": "table", "img_path": "oEVsxVdush/tmp/1b60ae970d3541ec4cff2eecd7a1684539c00e78d89c796d5a34cb6351d13d04.jpg", "table_caption": ["Table 40: Downstream WReN model performance on the abstract visual reasoning dataset "], "table_footnote": ["Ours 0.273 \u00b1 0.033 0.312 \u00b1 0.027 0.360 \u00b1 0.033 0.412 \u00b1 0.066 0.560 \u00b1 0.103 0.869 \u00b1 0.024 "], "page_idx": 64}, {"type": "text", "text": "C.6 More Ablation Experiments ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "C.6.1 Soft TPR vs TPR ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "To verify that our Soft TPR confers exclusive benefits compared to TPRs, we repeat all our experiments using the explicit TPR that our TPR decoder produces, which represents the Soft TPR\u2019s greedily optimal explicit TPR counterpart, $\\psi_{t p r}^{*}$ . In all plots, we use the same legend, denoting the explicit TPR as yellow (0) and the Soft TPR as blue (1). Across all considered cases: i.e., 1) convergence rate of representation learning (as measured by the downstream model\u2019s ability to effectively leverage representations produced at different stages of training), 2) sample efficiency of downstream models, and 3) raw performance of downstream models in the low sample regime, the Soft TPR confers differential performance boosts compared to the explicit TPR. This offers strong empirical evidence for our hypothesis that embodying a more approximate form of explicitly compositional structure helps representation learners by alleviating the strigent requirement of having to produce explicit TPRs, and additionally provides representation learners with more knowledge of the entire manifold underlying compositional structure, which TPRs cannot fully capture either due to representation learner deficits, or the inherent quasi-compositional structure of represented objects. ", "page_idx": 64}, {"type": "text", "text": "Convergence Rate of Representation Learning ", "text_level": 1, "page_idx": 64}, {"type": "image", "img_path": "oEVsxVdush/tmp/11e5f0085897b32423b323588cb895383ca5db70600b651a06e9f7334e71867e.jpg", "img_caption": ["Figure 51: Convergence of Soft TPR (0) vs TPR (1) as measured by FoV regression on the Cars3D dataset "], "img_footnote": [], "page_idx": 65}, {"type": "image", "img_path": "oEVsxVdush/tmp/3de0a8042db197c6062ca97b36b4bbf2a77835aaeb21e74c3adaf7d7d0b27a28.jpg", "img_caption": ["Figure 52: Convergence of Soft TPR (0) vs TPR (1) as measured by FoV regression on the Shapes3D dataset "], "img_footnote": [], "page_idx": 65}, {"type": "image", "img_path": "oEVsxVdush/tmp/4df609141dc2b8ee7446ec6c02b719e9877f87c321e116569bb865fdec80f023.jpg", "img_caption": ["Figure 53: Convergence of Soft TPR (0) vs TPR (1) as measured by FoV regression on the MPI3D dataset "], "img_footnote": [], "page_idx": 66}, {"type": "image", "img_path": "oEVsxVdush/tmp/c31ecee0a23b340b6fba37c9bc1d6c912db66d2ed0e946a85eb00423be0ddb04.jpg", "img_caption": [], "img_footnote": [], "page_idx": 66}, {"type": "text", "text": "Figure 54: Convergence of Soft TPR (0) vs TPR (1) as measured by classification performance on the abstract visual reasoning dataset ", "page_idx": 66}, {"type": "text", "text": "Sample Efficiency of Downstream Models ", "text_level": 1, "page_idx": 66}, {"type": "image", "img_path": "oEVsxVdush/tmp/46a3946ad802b68a37ce5277c32d6ad2b4c3e8515545ac74f6b9ec063ef0c5d6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 67}, {"type": "text", "text": "Figure 55: Downstream regression model sample efficiency on the Cars3D dataset using TPR representations (0) vs Soft TPR representations (1) ", "page_idx": 67}, {"type": "image", "img_path": "oEVsxVdush/tmp/86b4ebe2247cf4737e774d1f2cabf0a11a13a481856c8b578143b32bc970b043.jpg", "img_caption": [], "img_footnote": [], "page_idx": 67}, {"type": "text", "text": "Figure 56: Downstream regression model sample efficiency on the Cars3D dataset using TPR representations (0) vs Soft TPR representations (1) ", "page_idx": 67}, {"type": "image", "img_path": "oEVsxVdush/tmp/81fe008cb65b55c2f5724659bd9347f9731a9d9df8b4aa5023a5110f9690d13a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 68}, {"type": "text", "text": "Figure 57: Downstream regression model sample efficiency on the Shapes3D dataset using TPR representations (0) vs Soft TPR representations (1) ", "page_idx": 68}, {"type": "image", "img_path": "oEVsxVdush/tmp/b414c730c8f5dccf363f5784cdcfe3b3d008cfb0a1f7f5570ab8a1d5be5cb8c9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 68}, {"type": "text", "text": "Figure 58: Downstream regression model sample efficiency on the Shapes3D dataset using TPR representations (0) vs Soft TPR representations (1) ", "page_idx": 68}, {"type": "image", "img_path": "oEVsxVdush/tmp/6678dc419dee715f0a583cc1c5b67c6e76649ef8fdafde787564564fe0bfd08d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 69}, {"type": "text", "text": "Figure 59: Downstream regression model sample efficiency on the MPI3D dataset using TPR representations (0) vs Soft TPR representations (1) ", "page_idx": 69}, {"type": "image", "img_path": "oEVsxVdush/tmp/f965b9a5114f562ebb9ec7db2ddc16b4e0e86b8f23b48a643005b259829dd9ad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 69}, {"type": "text", "text": "Figure 60: Downstream regression model sample efficiency on the MPI3D dataset using TPR representations (0) vs Soft TPR representations (1) ", "page_idx": 69}, {"type": "image", "img_path": "oEVsxVdush/tmp/b80ed859375052d319b2a65ceb27a0d526f4b646d305d7df6b43b1e804169471.jpg", "img_caption": [], "img_footnote": [], "page_idx": 70}, {"type": "text", "text": "Figure 61: Downstream WReN model sample efficiency on the abstract visual reasoning dataset using TPR representations (0) vs Soft TPR representations (1) ", "page_idx": 70}, {"type": "image", "img_path": "oEVsxVdush/tmp/fee5cc367c39ff9c141e307daf599798c199401c2c34ff520852e5aeb4a9d1f9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 70}, {"type": "text", "text": "Figure 62: Downstream WReN model sample efficiency on the abstract visual reasoning dataset using TPR representations (0) vs Soft TPR representations (1) ", "page_idx": 70}, {"type": "text", "text": "Low Sample-Regime Performance of Downstream Models ", "page_idx": 70}, {"type": "image", "img_path": "oEVsxVdush/tmp/e00ae4251f38dd2de9504001fa5b44ef12cffbbad677f2e63194900765a90b12.jpg", "img_caption": ["Figure 63: Downstream regression model $R^{2}$ scores on the Cars3D dataset using TPR representations (0) vs Soft TPR representations (1) "], "img_footnote": [], "page_idx": 71}, {"type": "image", "img_path": "oEVsxVdush/tmp/1f98728f29dcf7f9b557f088bc87f0d148d0cb1b25e94454b50ddd4b0cc62197.jpg", "img_caption": ["Figure 64: Downstream regression model $R^{2}$ scores on the Cars3D dataset using TPR representations (0) vs Soft TPR representations (1) "], "img_footnote": [], "page_idx": 71}, {"type": "image", "img_path": "oEVsxVdush/tmp/99a942b20434bd6e4144c724544ff2db6976fbe306b4fd295e4cfaee600ad33b.jpg", "img_caption": ["Figure 65: Downstream regression model $R^{2}$ scores on the Shapes3D dataset using TPR representations (0) vs Soft TPR representations (1) "], "img_footnote": [], "page_idx": 72}, {"type": "image", "img_path": "oEVsxVdush/tmp/e4b82ec7c8f94493e95ffcfba71640795f07f8792fd1730c6a6946b70a4b1876.jpg", "img_caption": [], "img_footnote": [], "page_idx": 72}, {"type": "text", "text": "Figure 66: Downstream regression model $R^{2}$ scores on the Shapes3D dataset using TPR representations (0) vs Soft TPR representations (1) ", "page_idx": 72}, {"type": "image", "img_path": "oEVsxVdush/tmp/cb36d92154553adc0452291a244857145f9def49752d3dbb067146c5de0c707e.jpg", "img_caption": ["Figure 67: Downstream regression model $R^{2}$ scores on the MPI3D dataset using TPR representations (0) vs Soft TPR representations (1) "], "img_footnote": [], "page_idx": 73}, {"type": "image", "img_path": "oEVsxVdush/tmp/2e240a22aef947c67f2d98dab2c6be78eb0bfb3f541fe3557a1354b450bfdc95.jpg", "img_caption": [], "img_footnote": [], "page_idx": 73}, {"type": "text", "text": "Figure 68: Downstream regression model $R^{2}$ scores on the MPI3D dataset using TPR representations (0) vs Soft TPR representations (1) ", "page_idx": 73}, {"type": "image", "img_path": "oEVsxVdush/tmp/2e56bfadd2aac37bc5fa0b63fbd1345ea195a303486033b1f1126db315bfb965.jpg", "img_caption": ["Figure 69: Downstream WReN model classification accuracy on the abstract visual reasoning dataset using TPR representations (0) vs Soft TPR representations (1) "], "img_footnote": [], "page_idx": 74}, {"type": "image", "img_path": "oEVsxVdush/tmp/175f6abb49b03a373176d1443c5707ada83d2350a4f6a7e717f735a50718e328.jpg", "img_caption": ["Figure 70: Downstream WReN model classification accuracy on the abstract visual reasoning dataset using TPR representations (0) vs Soft TPR representations (1) "], "img_footnote": [], "page_idx": 74}, {"type": "text", "text": "C.6.2 Robustness to Hyperparameter Choices ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "We perform an additional experiment to empiricially verify that our model is robust to different hyperparameter choices. For the MPI3D dataset, which disentanglement models experience the greatest difficulty in producing disentangled representations for (see Table 1), we randomly pick another set of hyperparameters, shown in Table 41, from the top 5 models based on the MSE loss criterion. For this randomly chosen hyperparameter configuration, we evaluate the disentanglement of the representations on the MPI3D dataset produced by the resulting model. ", "page_idx": 74}, {"type": "table", "img_path": "oEVsxVdush/tmp/1a1de473797cbbbc619251b8e7c6a34c70b7c38653df6a91db66249a0fd0ac22.jpg", "table_caption": ["Table 41: Hyperparameter values of ablation setting "], "table_footnote": [], "page_idx": 75}, {"type": "table", "img_path": "oEVsxVdush/tmp/6b4dd8f9e4301e11125b5918dbebe5449d90b6eba1f0adefab1f26612c840ef1.jpg", "table_caption": ["Table 42: Disentanglement metric scores on the MPI3D dataset "], "table_footnote": [], "page_idx": 75}, {"type": "text", "text": "D Limitations and Future Work ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "D.1 Extension to Linguistic Domains ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Applying the Soft TPR to the TPR\u2019s typical domain of language is an intriguing future direction, especially as language can deviate from strict algebraic compositionality \u2013 for instance, idiomatic expressions such as \u2018spill the beans\u2019 cannot be understood as a function of their constituents alone. Soft TPR\u2019s more flexible specification allows it to capture approximate forms of compositionality precluded from the TPR\u2019s strict algebraic definition (Eq 1), thereby potentially providing the Soft TPR the ability to better handle the nuance and complexity of language. ", "page_idx": 75}, {"type": "text", "text": "To adapt our framework to language, we replace our Conv/Deconv encoder/decoders with simple RNNs, retrain our TPR decoder, and remove the semi-supervised loss, using Eq 6 as the full loss. Preliminary results in Table 43 on the BaBI [7] dataset are compared with TPR baselines from AID [51]. Our Soft TPR Autoencoder does not presently surpass AID, but notable points include: ", "page_idx": 75}, {"type": "text", "text": "1. Our use of simpler RRN-based encoders and an MLP-based downstream network, unlike the more sophisticated architectures of [51].   \n2. Soft TPR retains its performance improvement above the corresponding explicit TPR it can be quantised into.   \n3. The smaller gap between systematic vs non-systematic dataset splits in our model compared to TPR-RNN $\\mathrm{(+AID)}$ and FWM.   \n4. We train our representation learner using self-supervision (reconstruction loss) alone, only employing supervision on the downstream prediction network, while the baselines employ strong supervision and end-to-end training to produce representations. ", "page_idx": 75}, {"type": "table", "img_path": "oEVsxVdush/tmp/2c00e71f6d0c1ae85886202e5a8d35a926ffaf06500696eaa8bb6230985d8d29.jpg", "table_caption": ["Table 43: Mean word error rate $[\\%]$ on the sys-bAbI task [51] "], "table_footnote": [], "page_idx": 75}, {"type": "text", "text": "D.2 Need for Weak Supervision ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "To produce a compositional representation, $\\psi(x)=C(\\psi_{1}(a_{1}),\\dots,\\psi_{n}(a_{n}))$ (as in Section 3.1), each representational constituent, $\\bar{\\psi}_{i}(a_{i})$ , must map 1-1 to a data constituent, $a_{i}$ . Without supervision (i.e., access only to observational data, $\\{x_{i}\\})$ , this is challenging, because the data constituents $\\{a_{i}\\}$ underlying each object, $x$ , are unknown and cannot be identified. ", "page_idx": 76}, {"type": "text", "text": "We can frame the above intuition in a more mathematically rigorous way, using the generative framework. Essentially, as formally proved in [25], it is impossible to identify the true distribution for the data constituents (generative factors), $p(a)$ , using observational data, $\\{x_{i}\\}$ , alone as there are infinitely many bijective functions $f:\\operatorname{supp}(a)\\to a$ such that: 1) $a$ and $f(a)$ are completely entangled (i.e. non-diagonal Jacobian) and 2) the marginal distributions of $a$ and $f(a)$ are identical (meaning the marginal distributions of the observations are also identical, i.e., $\\begin{array}{r l r}{\\lefteqn{\\int p(\\boldsymbol{x}|\\boldsymbol{a})p(\\boldsymbol{a})d\\boldsymbol{a}=}}\\end{array}$ $\\begin{array}{r}{\\int p(x|f(a))p(f(a))d a)}\\end{array}$ . Thus, without inductive biases, it is impossible to infer the data constituents $\\{a_{i}\\}$ of any observation, $x$ , from observational data $\\{x_{i}\\}$ alone. ", "page_idx": 76}, {"type": "text", "text": "To combat this non-identifiability result, in line with [13, 22, 31, 33, 35], we use weak supervision, presenting the model with data pairs $(x,x^{\\prime})$ where $x$ and $x^{\\prime}$ differ in a subset of FoVs, e.g. $\\beta(x)=$ {shape/cube, colour/purple, size/large}, \u03b2(x\u2032) = {shape/cube, colour/cyan, size/large}, where the FoV types corresponding to the different FoV values are known to the model. Note that this weak supervision is minimal, only providing the model to access to the differing FoV types in an index set, $I$ , (i.e., $I:=\\{\\mathrm{colour}\\}$ ) and not any of the FoV values (i.e., $\\{$ cube, purple, cyan, large} are all not known by the model). ", "page_idx": 76}, {"type": "text", "text": "Some possible future extensions to reduce this level of weak supervision, or alternative forms of weak supervision include: ", "page_idx": 76}, {"type": "text", "text": "Embodied Learning: In the visual domain, some roles, e.g. object position correspond to affordances. Embodied agents may be able to reduce the need for explicit supervision by collecting $(x,x^{\\prime})$ and $I$ through interaction with their environment.   \n2. Pretrained Filler Embeddings: Initialising the filler embedding matrix, $M_{\\xi_{F}}$ with embeddings learnt by a pre-trained vision model could impart knowledge of domain-agnostic fillers (e.g., colours, shapes), reducing the need to explicitly provide $(x,x^{\\prime})$ and $I$ to the model.   \n3. Segmentation Masks: Segmentation masks for each object may potentially reduce the need to explicitly provide $(x,x^{\\bar{\\prime}})$ and $I$ to the model. ", "page_idx": 76}, {"type": "text", "text": "D.3 Downstream Utility ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Our investigation of downstream utility centers on two selected tasks \u2013 FoV regression/classification and abstract visual reasoning, which aligns with the standard framework for assessing the quality and downstream utility of compositional representations [26, 30, 33, 40, 44, 46]. While existing work [30, 33] demonstrates that explicitly compositional representations enhance downstream sample efficiency compared to non-compositional representations, a result we improve upon (C.5.1), the broader utility of compositional representations remains a topic of ongoing exploration [21, 40, 42, 44]. ", "page_idx": 76}, {"type": "text", "text": "Theoretical perspectives [1, 2] argue that explicitly compositional representations are fundamental in the production of productive, systematic, and inferentially coherent thought \u2013 3 key properties characterising human cognition. Investigating how explicitly compositional representations can yield empirical benefits across these dimensions represents an essential avenue for future research. Although preliminary studies [40, 42, 44] do not find strong evidence that explicitly compositional representations improve compositional generalisation (a key aspect of systematicity), [44] suggests that this finding is because compositional representations are necessary, but not sufficient to induce systematicity; an explicitly compositional processing approach [6] is also required. ", "page_idx": 76}, {"type": "text", "text": "Future work could extend our theoretical framework with the hope of producing empirical results consistent with the theoretical arguments of [1, 2]. In particular, as our unbinding module is designed to provably and efficiently recover structured role-filler constituents from the Soft TPR, it may be possible to exploit this module to systematically reconfigure roles and fillers from existing representations to create representations of novel combinations of role-filler bindings (i.e., novel compositional data). This type of ability could prove extremely beneficial in areas such as concept learning and compositional generalisation. ", "page_idx": 76}, {"type": "text", "text": "", "page_idx": 77}, {"type": "text", "text": "D.4 Dimensionality ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "In this subsection, we denote the dimensions of the role and fliler embedding spaces as $D_{F}$ and $D_{R}$ respectively. We also denote the number of fillers as $N_{F}$ and the number of roles as $N_{R}$ . ", "page_idx": 77}, {"type": "text", "text": "The Soft TPR belongs to $V_{F}\\otimes V_{R}$ , which is a $D_{F}\\times D_{R}$ dimensional space, which grows multiplicatively in $D_{F}$ , $D_{R}$ . Several factors, however, mitigate scalability concerns in light of this fact: ", "page_idx": 77}, {"type": "text", "text": "1. Independence of Embedding Space Dimensionality: We note that the dimensionality of the role and filler embedding spaces $(D_{R},D_{F}$ respectively) are properties of the corresponding embedding functions $(\\xi_{R}:R\\to V_{R}$ and $\\xi_{F}:F\\to V_{F})$ ) and thus, can be fixed independently of the number of roles, $N_{R}$ , the number of fillers, $N_{F}$ , or the number of total role-filler bindings (which we denote by $n$ ) within a domain. Thus, it is possible to fix the Soft TPR\u2019s dimensionality $(D_{F}\\times D_{R})$ to be smaller than $N_{F}\\times N_{R}$ (the number of roles/FoV types multipled by the number of fillers/FoV tokens) or $n$ (the number of bindings), which all may be large in complex visual domains. As illustrated in Table 44, the dimensionality of the TPR is smaller than $N_{F}\\times N_{R}$ in both the Shapes3D and MPI3D domains. ", "page_idx": 77}, {"type": "text", "text": "2. Relaxing Orthogonality: While $D_{F}$ can be set a priori with no regard to $N_{R},N_{F}$ or $n$ , we require $D_{R}\\geq N_{R}$ for semi-orthogonality of the role-embedding matrix $M_{\\xi_{R}}$ , which guarantees faithful (see proof 2 of A.2) and computationally efficient (see \u2018Unbinding\u2019 heading of B.3.1) recoverability of constituents. It is, however, possible to relax this constraint (i.e., to have $D_{R}<N_{R})$ ) to further reduce dimensionality. In this case, semiorthogonality of $M_{\\xi_{R}}$ is impossible and hence the recoverability of constituents cannot be guaranteed, however, there are some less stringent guarantees on the outcome of unbinding that can still be derived (see p291 of [3] for more details). ", "page_idx": 77}, {"type": "text", "text": "We also more explicitly compare the dimensionality of the Soft TPR with baselines in Table 45. Scalar-tokened symbolic representations have a low dimensionality of 10 $(N_{R})$ at the expense of representational expressivity (each representational constituent $\\psi_{i}(a_{i})$ is scalar-valued). In contrast, Soft TPR has vector-valued representational constituents (i.e. $\\approx\\xi_{F}(f_{m(i)})\\otimes\\xi_{R}(r_{i}))$ , similar to VCT and COMET. When compared to these models, the Soft TPR has significantly lower dimensionality compared to VCT and is comparable with COMET. ", "page_idx": 77}, {"type": "table", "img_path": "oEVsxVdush/tmp/163457f8ccf636225782fc1e62a99176efc0bb31862d257ddfa8f80d7bf45754.jpg", "table_caption": ["Table 44: Comparison of multiplicative dimensionality "], "table_footnote": [], "page_idx": 77}, {"type": "table", "img_path": "oEVsxVdush/tmp/8ebffac47a96f657e4c9cbbbcf31209b1405fcd5d31219a5650e327cd4ba0184.jpg", "table_caption": ["Table 45: Comparison of dimensionality of representations "], "table_footnote": [], "page_idx": 77}, {"type": "text", "text": "D.5 Computational Cost ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "In the Soft TPR Autoencoder, the expensive tensor product operation is employed to generate $\\psi_{t p r}^{*}$ . Given the computational cost of the tensor product, we more concretely examine the computational cost of training the Soft TPR Autoencoder by computing the FLOPs for a single forward pass on a batch size of 16 using the open-source implementation of fvcore https://github.com/ facebookresearch/fvcore/tree/main/docs, visible in Table 46. This data demonstrates that, despite the tensor product\u2019s computational cost, the mathematically-informed derivation of our model allows it to obtain compositional representations with vector-valued representational constituents at a significantly lower cost compared to relevant, vector-tokened baselines (2 orders of magnitude less than VCT, and 4 orders of magnitude less than COMET). ", "page_idx": 77}, {"type": "text", "text": "", "page_idx": 78}, {"type": "text", "text": "Future work could explore the use of tensor contraction techniques to reduce computational expense. For instance [29] uses a Hadamard product based tensor product compression technique. This reduces computational cost from $n^{2}$ (tensor product of 2 vectors) to $n$ (Hadamard product), but comprises the theoretical guarantees on constituent recoverability. We believe developing tensor contraction techniques within the TPR framework is an important direction for future research, to ensure efficient TPR-based representations with provable recoverability of constituents. ", "page_idx": 78}, {"type": "table", "img_path": "oEVsxVdush/tmp/25dce3c9b1a01f542d7745b48a3c69619e46d8b1d0110da460a3c4bdcaa966d5.jpg", "table_caption": ["Table 46: Comparison of FLOPs required for a forward pass of batch size 16 "], "table_footnote": [], "page_idx": 78}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 79}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 79}, {"type": "text", "text": "Justification: All claims accurately reflect the paper\u2019s contributions and scope. We provide theoretical proofs in the Appendix, as well as our complete suite of experimental results. ", "page_idx": 79}, {"type": "text", "text": "Guidelines: ", "page_idx": 79}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 79}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 79}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Justification: We explicitly state in the last paragraph of the paper that future work is to be done in developing hierarchical Soft TRP representations. ", "page_idx": 79}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u2019Limitations\u2019 section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 79}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 79}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 80}, {"type": "text", "text": "Justification: We include complete proofs for all theoretical results in the Appendix. Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 80}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 80}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 80}, {"type": "text", "text": "Justification: We provide all information (specification of model architecture, computing resources, hyperparameters) to replicate experimental results in the Appendix. ", "page_idx": 80}, {"type": "text", "text": "Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 80}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 81}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 81}, {"type": "text", "text": "Justification: All datasets are open-access. We provide sufficient instructions in our Appendix to reproduce experimental results. Furthermore, we provide the main code for our Soft TPR model architecture with this submission. We plan to release a streamlined version of our entire code base used to conduct all experiments if the paper is accepted. ", "page_idx": 81}, {"type": "text", "text": "Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 81}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 81}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Justification: We provide these details in the Appendix, and additionally, in the code. Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 81}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 81}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Justification: We present results with standard deviations, as well as the IQR and whiskers extending to 1.5 times the IQR for all box plots. ", "page_idx": 81}, {"type": "text", "text": "Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \u2019Yes\u2019 if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 81}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 82}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 82}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 82}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 82}, {"type": "text", "text": "Justification: We specify the specific GPUs we use for all experiments in the Appendix, as well as the average amount of time it takes to train the Soft TPR Autoencoder for each dataset. ", "page_idx": 82}, {"type": "text", "text": "Guidelines: ", "page_idx": 82}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 82}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 82}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 82}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 82}, {"type": "text", "text": "Justification: Our research conforms to the NeurIPS Code of Ethics. ", "page_idx": 82}, {"type": "text", "text": "Guidelines: ", "page_idx": 82}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 82}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 82}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 82}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 82}, {"type": "text", "text": "Justification: As the central aim of our work relates to the representational form of compositional representations, our research is theoretical in nature and has no immediate societal impact. ", "page_idx": 82}, {"type": "text", "text": "Guidelines: ", "page_idx": 83}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 83}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 83}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 83}, {"type": "text", "text": "Justification: Our work poses no such risks. ", "page_idx": 83}, {"type": "text", "text": "Guidelines: ", "page_idx": 83}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 83}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 83}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 83}, {"type": "text", "text": "Justification: In the main paper, as well as the Appendix, we cite the papers associated with each dataset we use. We additionally make explicit references in the Appendix to all externally created code that we use in our experiments. Furthermore, if the paper is to be accepted, we will clearly provide licenses and attribution to the original authors where applicable in the code files. ", "page_idx": 83}, {"type": "text", "text": "Guidelines: ", "page_idx": 83}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 83}, {"type": "text", "text": "", "page_idx": 84}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 84}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 84}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 84}, {"type": "text", "text": "Justification: The code we submit in our submission corresponds with our Soft TPR Autoencoder architecture, and is thus reasonably straightforward to understand, especially when supplemented by this paper. If this paper is accepted, however, we will include more extensive written documentation for the official, more extensive codebase we release. ", "page_idx": 84}, {"type": "text", "text": "Guidelines: ", "page_idx": 84}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 84}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 84}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 84}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 84}, {"type": "text", "text": "Justification: Our research does not involve crowdsourcing nor human subjects. ", "page_idx": 84}, {"type": "text", "text": "Guidelines: ", "page_idx": 84}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 84}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 84}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 84}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 84}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 85}, {"type": "text", "text": "Guidelines: ", "page_idx": 85}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 85}]