[{"figure_path": "YCS0xGFrb4/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the return-conditioned generation of Decision Diffuser [5] in hopper-medium-expert task. Existing return-conditional diffusion models fail to align generated trajectories with the return condition, while the red line indicates the desired relationship between the return conditions and true returns of generated trajectories [14].", "description": "This figure illustrates the shortcomings of existing return-conditioned diffusion models in aligning generated trajectories with the return condition.  The Decision Diffuser model's performance on the hopper-medium-expert task is shown. The blue line with error bars represents the actual true returns of generated trajectories given different return conditions, while the red line shows the ideal relationship \u2013 a strong positive correlation between the return condition and the true return of the generated trajectory. The large discrepancy between the blue line and red line highlights the model's failure to effectively align generated trajectories with the specified return condition.", "section": "1 Introduction"}, {"figure_path": "YCS0xGFrb4/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of the representation space of trajectories in multi-task preference data. For each task i, the positive samples \u03c4+ consist of preferred trajectories \u03c4i+ from task i, while negative samples \u03c4\u00af include less preferred \u03c4i\u2212 from the same task, as well as \u03c4j from other tasks. Trajectories from diverse tasks are expected to be differentiated in the representation space, and {wi}i\u2208[m] attempts to characterize the best trajectories for each task.", "description": "This figure illustrates how the model learns to represent trajectories in a multi-task setting.  The input is multi-task preference data, which includes preferred (\u03c4+) and less preferred (\u03c4-) trajectories from multiple tasks. The encoder, f\u03c8, maps these trajectories to a representation space.  Importantly, the model aims to separate preferred trajectories within each task and distinguish trajectories across different tasks.  The \u2018optimal\u2019 representations, wi, represent the best trajectories within each task and are also learned.", "section": "3.1 Versatile Representation for Multi-Task Preferences"}, {"figure_path": "YCS0xGFrb4/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of our method. (1) We learn preference representations w = f\u03c8(\u03c4) and the optimal one w\u2217 from trajectory segments \u03c4, which comprise positive samples \u03c4+ and negative samples \u03c4\u2212. (2) We augment the diffusion model with an auxiliary mutual information term I(\u03c40; w) to ensure the alignment between \u03c40 and w. (3) During the inference process, the diffusion model conditioned on w can generate desired trajectories aligned with preferences.", "description": "This figure illustrates the three main steps of the proposed method, CAMP.  First, preference representations (w) are learned using a trajectory encoder (f\u03c8) and a triplet loss, differentiating between preferred and less preferred trajectories.  These representations also include an 'optimal' representation (w*) for each task.  Second, a mutual information regularized diffusion model is trained to align generated trajectories (\u03c40) with the learned representations (w).  This is accomplished by maximizing the mutual information between the generated trajectories and the conditions. Third, during inference, the optimal representation (w*) for a specific task is used as a condition for the diffusion model to generate trajectories aligned with that task's preferences.", "section": "3 Method"}, {"figure_path": "YCS0xGFrb4/figures/figures_7_1.jpg", "caption": "Figure 4: Average success rates in MT-10 benchmarks trained with different datasets. Orange bars are reward-based methods, while green bars represent preference-based methods. Detailed comparisons for each task can be found in \u00a7D.", "description": "This figure compares the average success rates of different methods on the MT-10 benchmark using two different datasets: near-optimal and sub-optimal.  It shows the performance of both reward-based (orange bars) and preference-based (green bars) methods.  The results indicate that the proposed method (Ours) outperforms many others, particularly in the sub-optimal dataset setting.", "section": "5.2 Performance Comparison"}, {"figure_path": "YCS0xGFrb4/figures/figures_8_1.jpg", "caption": "Figure 5: Left: Brighter dots indicate trajectories with higher returns. Red dots represent each dimension of w*. Black triangles in (b) mark trajectories with the highest return. f\u03c8 can separate trajectories from different tasks and with different returns. w* aligns with the optimal trajectories for each task. Right: Guided by w*, diffusion models can generate trajectories \u03c40 that mainly lie around w* (shown as black circles), which represents better trajectories in offline data \u03c40.", "description": "The figure demonstrates the visualization of trajectory representations and generated trajectories. The left panel shows how the learned representation (f\u03c8) effectively separates trajectories with different returns and from different tasks, aligning the optimal representations (w*) with high-return trajectories. The right panel showcases how the generated trajectories, guided by w*, closely align with the optimal trajectories in the offline dataset. This visualization supports the effectiveness of the proposed method in learning meaningful representations and generating aligned trajectories.", "section": "5.3 Visualization"}, {"figure_path": "YCS0xGFrb4/figures/figures_13_1.jpg", "caption": "Figure 3: Overview of our method. (1) We learn preference representations w = fy(\u03c4) and the optimal one w* from trajectory segments \u03c4, which comprise positive samples \u03c4+ and negative samples \u03c4\u00af. (2) We augment the diffusion model with an auxiliary mutual information term I(\u03c4\u03bf; w) to ensure the alignment between \u03c4\u03bf and w. (3) During the inference process, the diffusion model conditioned on w* can generate desired trajectories aligned with preferences.", "description": "This figure illustrates the overall framework of the proposed method, CAMP. It consists of three main stages: 1) Learning preference representations by using a triplet loss and KL divergence to differentiate trajectories with different preferences. 2) Augmenting the diffusion model with mutual information regularization to improve alignment between generated trajectories and preference representations. 3) Generating trajectories conditioned on optimal preference representations to align with desired preferences during inference. ", "section": "3 Method"}, {"figure_path": "YCS0xGFrb4/figures/figures_15_1.jpg", "caption": "Figure 6: MetaWold MT-10 tasks. The goal is to learn a policy that can succeed on a diverse set of tasks.", "description": "This figure shows ten different tasks from the MetaWorld MT-10 benchmark.  Each task involves a Sawyer robot manipulating various objects in a simulated environment.  The tasks illustrate the diversity and complexity of the multi-task setting; the goal is to train a single agent capable of performing all of these different manipulation tasks.", "section": "B.1 Datasets"}, {"figure_path": "YCS0xGFrb4/figures/figures_18_1.jpg", "caption": "Figure 7: Ablation on the dimension of w.", "description": "This ablation study investigates the impact of the dimension of the preference representation (w) on the performance of the proposed method.  The study varies the dimension of w (|w|) and evaluates its effect on the average success rates across multiple tasks in the MT-10 benchmark.  The results show that an intermediate dimension of w performs best, while both low-dimensional and high-dimensional representations result in performance degradation.", "section": "5.5 Ablation Study"}]