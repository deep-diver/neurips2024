{"references": [{"fullname_first_author": "Michael Janner", "paper_title": "Offline reinforcement learning as one big sequence modeling problem", "publication_date": "2021-12-01", "reason": "This paper frames offline reinforcement learning as a sequence modeling problem, a core concept that influences the paper's approach to multi-task preference alignment using diffusion models."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduces denoising diffusion probabilistic models, the foundation of the generative modeling approach used in the paper for sequential decision-making."}, {"fullname_first_author": "Ilya Kostrikov", "paper_title": "Offline reinforcement learning with implicit q-learning", "publication_date": "2022-05-01", "reason": "This paper introduces implicit Q-learning, which is related to the paper's approach to learning preference representations and solving the offline reinforcement learning problem."}, {"fullname_first_author": "Anurag Ajay", "paper_title": "Is conditional generative modeling all you need for decision making?", "publication_date": "2023-05-01", "reason": "This paper explores the use of conditional generative models for decision making and highlights the challenges of aligning generated trajectories with conditions, a problem directly addressed in the current paper."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-04-14", "reason": "This paper introduces D4RL, a benchmark dataset used in the paper's experiments, making it a crucial reference for evaluating the method's performance."}]}