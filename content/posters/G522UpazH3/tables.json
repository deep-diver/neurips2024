[{"figure_path": "G522UpazH3/tables/tables_6_1.jpg", "caption": "Table 1: The attack success rates (%) of different attacks on normal models. Best results are in bold.", "description": "This table presents the attack success rates achieved by different attack methods against various normal models (ResNet50, DenseNet121, EfficientNet, Inception V3, MobileNetV2, SqueezeNet, ShuffleNetV2, ConvNet, RegNet, MNASNet, WideResNet50, VGG19, ViT, and Swin).  The results show the percentage of successful attacks for each method against each model.  The best-performing method for each model is highlighted in bold.", "section": "5.2 Attack Results"}, {"figure_path": "G522UpazH3/tables/tables_7_1.jpg", "caption": "Table 2: The attack success rates (%) of different methods on secured models. Three different robust training methods are considered: adversarial training with L2 perturbation (L2 - {0.03 ~ 5}) [31] and L\u221e perturbation (AdvIncV3 and EnsAdvIncResV2) [34], robust training with Styled ImageNet (SIN) and the mixture of Styled and natural ImageNet (SIN-IN) [16]. The best results are in bold.", "description": "This table presents the attack success rates of various methods against secured models.  It compares the performance of different attack methods on models that have undergone three different types of robust training: adversarial training with L2 and L\u221e perturbations, and robust training with Styled ImageNet and a mix of styled and natural ImageNet.  The best result for each model/method combination is highlighted in bold.", "section": "5.2 Attack Results"}, {"figure_path": "G522UpazH3/tables/tables_7_2.jpg", "caption": "Table 3: The targeted attack success rates of different methods. The proxy model is ResNet50.", "description": "This table presents the success rates of various targeted attacks against different models.  The attacks were designed to force misclassification to a specific target label, making this a more challenging scenario than untargeted attacks. ResNet50 is used as the proxy model for generating adversarial examples, which are then tested on other models. The results showcase the relative effectiveness of different attack methods under these conditions. ", "section": "5.2 Attack Results"}, {"figure_path": "G522UpazH3/tables/tables_8_1.jpg", "caption": "Table 4: The scoring for the effectiveness of adversarial examples against real-world applications. We randomly extract 100 samples from ImageNet and generate adversarial examples for them using TPA and ResNet50. We enlist a volunteer to assess the consistence between the image contents with the predictions made by applications. A lower rating reflects a higher effectiveness of the attack.", "description": "This table presents the results of evaluating the effectiveness of adversarial examples generated using the TPA method against various real-world applications.  A total of 100 samples were randomly chosen from ImageNet, and adversarial examples were created using TPA and ResNet50.  A volunteer assessed the consistency between the image content and application predictions, rating them on a scale of 1 (completely incorrect) to 5 (completely correct). Lower scores indicate higher attack effectiveness. The applications evaluated include image classification, object detection, and several search engines (Google, Bing, Yandex, Baidu).  The table also shows the performance against large language models (GPT-4 and Claude3).", "section": "6 Evaluation in Real World Applications"}, {"figure_path": "G522UpazH3/tables/tables_16_1.jpg", "caption": "Table 5: The attack success rates (%) of attacks against various defenses. We use ResNet50 as the proxy model.", "description": "This table presents the attack success rates of different attack methods against various defense mechanisms.  The ResNet50 model is used as the proxy model for all attacks. The table compares the performance of four attacks: VT, SSA, RAP, and the proposed 'Ours' attack against six different defenses: R&P, NIPS-R3, FD, ComDefend, RS, and NRP.  Higher values indicate better attack performance (higher success rate).  The results show that the proposed 'Ours' attack consistently outperforms the baselines against all defenses.", "section": "Supplementary Experiments"}, {"figure_path": "G522UpazH3/tables/tables_18_1.jpg", "caption": "Table 5: The attack success rates (%) of attacks against various defenses. We use ResNet50 as the proxy model.", "description": "This table shows the attack success rates of different attack methods against various defense mechanisms.  The proxy model used is ResNet50.  The results show the effectiveness of different attacks in bypassing defenses, such as R&P, NIPS-R3, FD, ComDefend, RS, and NRP.", "section": "B Supplementary Experiments"}]