[{"figure_path": "p2PO2PUPFY/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Results of DiffPose [22] and Di\u00b2Pose in Human3.6M [72] dataset (with MPJPE metric), across varying proportions of training samples. (b) Prediction results of two methods under occlusion.", "description": "This figure compares the performance of DiffPose and the proposed Di\u00b2Pose method.  (a) shows a plot illustrating how the MPJPE (mean per joint position error) changes as the amount of training data varies for both models. The plot demonstrates Di\u00b2Pose's superior performance, particularly when training data is scarce. (b) provides a visual comparison of the 3D pose estimation results of both methods under occlusion scenarios. The images clearly show that Di\u00b2Pose generates more realistic and accurate 3D poses in the presence of occlusions compared to DiffPose.", "section": "1 Introduction"}, {"figure_path": "p2PO2PUPFY/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our two-stage Di\u00b2Pose framework. In the stage 1, we train a pose quantization step that transforms a 3D pose P into multiple discrete tokens k, each token representing the indices of implied codebook C. In the stage 2, we model k in the discrete space by discrete diffusion process. In the forward process, each token is probabilistically occluded with Occ token or replaced with another available token. In the reverse process, the model leverages an independent image encoder and a pose denoiser to reconstruct all the tokens based on the condition 2D image. These reconstructed tokens are finally decoded by the pose decoder, resulting in the recovered 3D pose. Notably, we only update the parameters of pose denoiser, pose decoder and image encoder are frozen.", "description": "This figure illustrates the two-stage Di\u00b2Pose framework. Stage 1 involves a pose quantization step that converts a 3D pose into discrete tokens. Stage 2 uses a discrete diffusion process to model these tokens, handling occlusions probabilistically in the forward process and reconstructing them from a 2D image in the reverse process. The final output is a recovered 3D pose.", "section": "3 Di\u00b2Pose"}, {"figure_path": "p2PO2PUPFY/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of our two-stage Di\u00b2Pose framework. In the stage 1, we train a pose quantization step that transforms a 3D pose P into multiple discrete tokens k, each token representing the indices of implied codebook C. In the stage 2, we model k in the discrete space by discrete diffusion process. In the forward process, each token is probabilistically occluded with Occ token or replaced with another available token. In the reverse process, the model leverages an independent image encoder and a pose denoiser to reconstruct all the tokens based on the condition 2D image. These reconstructed tokens are finally decoded by the pose decoder, resulting in the recovered 3D pose. Notably, we only update the parameters of pose denoiser, pose decoder and image encoder are frozen.", "description": "This figure illustrates the Di\u00b2Pose framework, which consists of two stages: pose quantization and discrete diffusion.  The pose quantization stage converts a 3D pose into discrete tokens, representing sub-structures of the pose. The discrete diffusion stage then models these tokens using a conditional diffusion model, simulating a transition from occluded to recovered poses. The model uses an image encoder and a pose denoiser to handle occlusions and reconstruct the pose.", "section": "3 Di\u00b2Pose"}, {"figure_path": "p2PO2PUPFY/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative results on two datasets. The black lines represent the ground truth poses and the blue lines are prediction results.", "description": "This figure shows a qualitative comparison of the 3D human pose estimation results between DiffPose and the proposed Di\u00b2Pose method on the Human3.6M and 3DPW datasets.  For each dataset, two example images are shown, along with the ground truth poses (black lines) and the estimated poses from both DiffPose and Di\u00b2Pose (blue lines). The red circles highlight areas where there are noticeable differences in the pose estimation results between the two methods, suggesting that Di\u00b2Pose may be more accurate in these particular scenarios, especially when occlusions are present.", "section": "4 Experiments"}, {"figure_path": "p2PO2PUPFY/figures/figures_9_1.jpg", "caption": "Figure 4: Qualitative results on two datasets. The black lines represent the ground truth poses and the blue lines are prediction results.", "description": "This figure shows a qualitative comparison of the proposed Di2Pose model's performance against the DiffPose model on Human3.6M and 3DPW datasets.  The images show example input images, ground truth 3D poses (black lines), and the poses estimated by DiffPose (blue lines) and Di2Pose (red lines). The comparison highlights the ability of Di2Pose to improve accuracy of 3D pose estimation, especially in challenging situations such as occlusion.  It visually demonstrates the effectiveness of the proposed discrete diffusion model for handling occluded 3D human pose estimation.", "section": "4 Experiments"}, {"figure_path": "p2PO2PUPFY/figures/figures_18_1.jpg", "caption": "Figure 6: Qualitative results on two datasets. Joints on the right side are marked in green, while other joints are highlighted in blue.", "description": "This figure showcases qualitative results comparing the ground truth 3D human poses (GT) with the predictions of the proposed Di2Pose method. The results are presented for both the Human3.6M and 3DPW datasets.  Each row in the figure displays a sequence of poses. The images on the left represent frames from the dataset, while the pose visualizations are provided next to the images.  The color-coding helps distinguish between joints: joints that are correctly predicted are shown in blue, and those incorrectly predicted are highlighted in green. This visual representation allows for a direct comparison between the GT and Di2Pose predictions, highlighting the model's performance on different poses and datasets, including challenging scenarios with occlusions. The figure demonstrates that the model's predictions show a high degree of agreement with the ground truth data, but there are also cases where there are prediction errors, especially on those challenging scenarios involving occlusions.", "section": "4.3 Comparsion with State-of-the-Arts"}]