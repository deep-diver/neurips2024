[{"type": "text", "text": "Di2Pose: Discrete Diffusion Model for Occluded 3D Human Pose Estimation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weiquan Wang1, Jun Xiao1, Chunping Wang2, Wei $\\mathbf{Liu^{3}}$ , Zhao Wang1, Long Chen4\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Zhejiang University 2Finvolution Group 3Tencent 4Hong Kong University of Science and Technology {wqwangcs, junx}@zju.edu.cn, wangchunping02@xinye.com, wl2223@columbia.edu, zhao_wang@zju.edu.cn, longchen@ust.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have demonstrated their effectiveness in addressing the inherent uncertainty and indeterminacy in monocular 3D human pose estimation (HPE). Despite their strengths, the need for large search spaces and the corresponding demand for substantial training data make these models prone to generating biomechanically unrealistic poses. This challenge is particularly noticeable in occlusion scenarios, where the complexity of inferring 3D structures from 2D images intensifies. In response to these limitations, we introduce the Discrete Diffusion Pose $(\\mathbf{D}\\mathbf{i}^{2}\\mathbf{P_{0Se}})$ , a novel framework designed for occluded 3D HPE that capitalizes on the benefits of a discrete diffusion model. Specifically, $\\mathrm{Di^{2}}$ Pose employs a two-stage process: it first converts 3D poses into a discrete representation through a pose quantization step, which is subsequently modeled in latent space through a discrete diffusion process. This methodological innovation restrictively confines the search space towards physically viable configurations and enhances the model\u2019s capability to comprehend how occlusions affect human pose within the latent space. Extensive evaluations conducted on various benchmarks (e.g., Human3.6M, 3DPW, and 3DPW-Occ) have demonstrated its effectiveness. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D Human Pose Estimation (HPE) from monocular images remains a challenging yet pivotal research in the realm of computer vision, boasting a wide range of applications including human-machine interaction, autonomous driving, and animations [57, 81, 5, 70]. Generally, the mainstream approaches, including Direct Estimation [68, 43, 47] and 2D-to-3D Lifting [87, 51, 86], aim to perform 3D HPE by either directly predicting 3D poses from 2D images or lifting detected 2D poses into 3D space. These approaches aim to address the inherent 2D-3D ambiguity in 3D HPE tasks by learning mapping from training data. Despite significant advancements, accurately estimating 3D poses from monocular images remains a formidable challenge, particularly when humans are partially occluded [39]. Such occlusions introduce considerable uncertainty and indeterminacy into the estimation process. ", "page_idx": 0}, {"type": "text", "text": "Existing 3D HPE methods try to handle the occlusion challenges with pose priors/constraints [58, 62] or data augmentation strategies (e.g., annotations augmentation [61], pose transformation [35], and differentiable operations [82]). However, due to the inherent discreteness of 3D poses (primarily defined by discrete anatomical landmarks), these methods tend to represent poses using coordinate vectors or heatmap embeddings, treating joints as independent units and overlooking the interdependencies among body joints. Recent research [21] has introduced a compositional pose representation that captures the dependencies among joints by converting a pose into multiple tokens, enabling the use of mutual context between joints. This approach, which learns from real pose datasets, results in each learned token corresponding to a physically realistic prototype. Nevertheless, Geng et al. [21] casts HPE to a classification task, where the system simply classifies tokens based on prototype poses. Unfortunately, such scheme does not account for the effects of occlusions in the estimation process, potentially leading to inaccuracies due to unresolved uncertainty and indeterminacy. ", "page_idx": 0}, {"type": "image", "img_path": "p2PO2PUPFY/tmp/bf81519f59163ff4b4aba00121ec28f1b7d49dd3675bc21b859157191db194db.jpg", "img_caption": ["Figure 1: (a) Results of DiffPose [22] and $\\mathrm{Di^{2}}$ Pose in Human3.6M [72] dataset (with MPJPE metric), across varying proportions of training samples. (b) Prediction results of two methods under occlusion. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recent studies have shown marked progress in 3D HPE via generative models [2, 84, 19, 27]. Notably, diffusion models [25] have demonstrated effectiveness in handling complex and uncertain data distributions, making them suitable for handling uncertainty and indeterminacy in 3D HPE tasks [19, 91, 66, 27, 37]. They excel at generating samples that conform to a target data distribution by iteratively removing noise through a series of diffusion steps, ultimately predicting more accurate 3D poses. However, these diffusion-based 3D HPE methods initialize the 3D pose from random noise at the begining of the diffusion process, where each joint can be sampled from the continuous 3D space. Since the continuous 3D space has an infinite number of points, training such diffusion-based models requires a large amount of 3D pose data to achieve optimal outcomes [23, 75, 3]. This demand implies a substantial need for training data, presenting a stark contradiction to the limited availability of 3D human pose datasets. As illustrated in Figure 1(a), the predictive performance of DiffPose [22] declines more rapidly as the proportion of training data decreases. Given the scarcity of 3D pose training data, previous diffusion models may generate physically implausible configurations that do not adhere to human biomechanics, leading to inaccurate human pose estimations, particularly in occluded scenes $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!c.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!c.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ , Figure 1(b) with DiffPose). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a novel framework for 3D HPE with occlusions: Discrete Diffusion Pose $\\bf(D i^{2}P o s e)$ , drawing on compositional pose representation and diffusion model to achieve the best of two worlds. Specifically, $\\mathrm{Di^{2}}$ Pose employs a two-stage approach: a pose quantization step followed by a discrete diffusion process. The pose quantization step leverages the discrete nature of 3D poses and represents them as quantized tokens by capturing the local interactions between joints. This step effectively confines the search space to physically plausible configurations by learning from real 3D human poses. Subsequently, the discrete diffusion process models the quantized pose tokens in the latent space through a conditional diffusion model. By integrating the forward and reverse processes, our framework adeptly simulates the transition of a 3D pose from occluded to recovered. By modeling occlusion implicitly within the latent space, $\\mathrm{Di^{2}}$ Pose enhances its understanding of how occlusions affect human poses, providing valuable insights during the training phase. ", "page_idx": 1}, {"type": "text", "text": "For the pose quantization step, we devise a pose quantization step inspired by VQ-VAE [71], consisting of a pose encoder, a quantization process, and a pose decoder. To effectively capture local interactions between 3D joints, we introduce the Local-MLP block for both pose encoder and decoder. Within each Local-MLP block, a simple Joint Shift operation integrates information from different joints. The pose encoder utilizes several Local-MLP blocks to convert a 3D pose into multiple rich token features, each representing a sub-structure of the overall pose. These tokens are quantized using a shared codebook, yielding corresponding discrete indices. Additionally, we implement the finite scalar quantization (FSQ) [49] to address the codebook collapse issue observed in traditional VQ-VAE methods [59, 89, 56, 42]. This strategy ensures that the generated codewords are meaningful, a crucial aspect for the subsequent success of the discrete diffusion process. ", "page_idx": 1}, {"type": "text", "text": "For the discrete diffusion process, during the training phase, we introduce occlude and replace strategies to model the quantized pose tokens, enabling the discrete diffusion model to predict occluded tokens and update potential tokens. The occluded token represents the occlusion of the corresponding sub-structure of the 3D pose. The token replacement mechanism is designed to enhance the diversity of potential sub-structures, reflecting the indeterminacy in occluded parts. During the inference phase, pose tokens are either occluded or initialized randomly. The denoising diffusion process estimates the probability density of pose tokens step-by-step based on the input 2D image until the tokens are completely reconstructed. Each step leverages contextual information from all tokens of the entire pose as predicted in the previous step, facilitating the estimation of a new probability density distribution and the prediction of the current step\u2019s tokens. This sequential approach ensures a detailed and accurate reconstruction of 3D poses from occluded scenes. ", "page_idx": 2}, {"type": "text", "text": "We extensively evaluate our approach in 3D HPE on three challenging benchmarks (Human3.6M [34], 3DPW [72] and 3DPW-Occ). $\\mathrm{\\bar{Di}^{2}}$ Pose consistently yields lower errors compared to state-of-the-art methods. In particular, it achieves significantly better results when evaluated on occluded scenarios, verifying its advantages of occlusion-handling capability. Our contributions are threefold: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose the $\\mathrm{Di^{2}P o s e}$ framework, which integrates the inherent discreteness of 3D pose data into the diffusion model, offering a new paradigm for addressing 3D HPE under occlusions. \u2022 The designed pose quantization step represents 3D poses in a compositional manner, effectively capturing local correlations between joints and confining search space to reasonable configurations. \u2022 The constructed discrete diffusion process simulates the complete process of a 3D pose transitioning from occluded to recovered, which introduces the impact of occlusions into pose estimation process. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Monocular 3D HPE. Existing approaches can generally be classified into frame-based and videobased methodologies. Frame-based methods predict the 3D pose from a single RGB image, employing different networks in various studies [18, 20, 52, 54, 55] to directly output the human pose from the 2D image. Alternatively, a significant number of studies [48, 77, 85, 88] initially determine the 2D pose, which subsequently forms the foundation for inferring the 3D pose. In contrast, video-based methods leverage temporal relationships across video frames. Such methods predominantly [9, 11, 15, 31, 65, 73, 76] commence with the extraction of 2D pose sequences using a 2D pose detector from the video clips, aiming to harness the essential spatio-temporal data for 3D pose estimation. To validate the efficacy of our approach, we evaluate our $\\mathrm{Di^{2}}$ Pose on the more challenging frame-based setting, wherein the 3D human pose is directly inferred from a 2D image. ", "page_idx": 2}, {"type": "text", "text": "Occluded 3D HPE. Occlusions significantly challenge 3D HPE. As evidenced by research [76, 58, 62], pose priors and constraints have been proven crucial for mitigating such issue. Approaches typically involve statistical models to deduce occluded parts from visible cues [76, 41, 44] or pre-defined rules to constraint poses [61, 1]. Moreover, due to the scarcity of 3D pose data, data augmentation, including synthetic occlusions [7, 38, 60, 64, 13] and pose transformations [35, 82], remains vital for enhancing model robustness. Diverging from these aforementioned methods, our method innovatively introduces occlusion in the latent space without extra priors or explicit augmentations, providing a deeper feature-based understanding of occlusion\u2019s effects on pose estimation. ", "page_idx": 2}, {"type": "text", "text": "Diffusion Models for 3D HPE. Recent advancements have shown that diffusion models are capable of managing complex and uncertain data distributions [25, 26, 17, 4, 32, 8, 10, 80, 40, 36, 74, 78], which is particularly beneficial for 3D HPE. Typically, these models predict 3D poses by progressively refining the pose distribution from high to low uncertainty [22, 14, 19, 91, 63]. Other approaches use diffusion models to generate multiple pose hypotheses from a single 2D observation [66, 27]. These 3D pose estimators effectively reduce uncertainty and indeterminacy throughout the estimation process. Moreover, discrete diffusion models have also gained attention in various domains [30, 40, 24, 33]. Inspired by these advancements, our work introduces a discrete diffusion model for occluded 3D HPE, which aligns more closely with the inherent discreteness of 3D pose data and effectively incorporates occlusion into the estimation process, providing a novel perspective in the field. ", "page_idx": 2}, {"type": "image", "img_path": "p2PO2PUPFY/tmp/51c9c9205bb5864fe0c00852d6a0d74ca244c63d062cf1d3060cc836ba8ea3e9.jpg", "img_caption": ["Figure 2: Overview of our two-stage $\\mathrm{Di^{2}}$ Pose framework. In the stage 1, we train a pose quantization step that transforms a 3D pose $\\mathbf{P}$ into multiple discrete tokens $\\mathbf{k}$ , each token representing the indices of implied codebook $\\mathcal{C}$ . In the stage 2, we model $\\mathbf{k}$ in the discrete space by discrete diffusion process. In the forward process, each token is probabilistically occluded with Occ token or replaced with another available token. In the reverse process, the model leverages an independent image encoder and a pose denoiser to reconstruct all the tokens based on the condition 2D image. These reconstructed tokens are finally decoded by the pose decoder, resulting in the recovered 3D pose. Notably, we only update the parameters of pose denoiser, pose decoder and image encoder are frozen. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Di2Pose ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given an 2D image $I\\in\\mathbb{R}^{H\\times W\\times3}$ , the goal of 3D HPE is to predict $\\hat{\\mathbf{P}}\\in\\mathbb{R}^{J\\times3}$ , which represents the 3D coordinates of all the $J$ joints of the human body. In this paper, we construct occluded 3D HPE task as a two-stage framework including the pose quantization step and the discrete diffusion process. ", "page_idx": 3}, {"type": "text", "text": "As shown in Figure 2, in the training phase, Stage $^{\\,I}$ learns a pose quantization step by a VQ-VAE like structure (Sec. 3.1), which is able to encode a 3D pose into multiple quantized tokens. Stage 2 models quantized pose tokens in the latent space by the forward and reverse process of a conditional diffusion model (Sec. 3.2). In the inference phase, we only use the reverse process of Stage 2 and the pre-trained pose decoder of Stage 1 to recover 3D pose from the 2D image. Notably, pose tokens are either occluded or initialized randomly at the beginning of the inference phase. The model reconstructs all the tokens based on the condition 2D image step-by-step. These reconstructed tokens are finally decoded by the pre-trained pose decoder, resulting in the recovered 3D pose. ", "page_idx": 3}, {"type": "text", "text": "3.1 Pose Quantization Step ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As depicted in Figure 2, a pose quantization step comprises a pose encoder, the quantization process, and a pose decoder. Initially, for a real 3D pose $\\mathbf{P}\\in\\dot{\\mathbb{R}}^{J\\times3}$ , the pose encoder $\\bar{f_{P E}}(\\cdot)$ converts $\\mathbf{P}$ to token features $\\mathbf{F}$ . During the quantization process, we utilize FSQ to quantize $\\mathbf{F}=(\\mathbf{f}_{1},\\mathbf{f}_{2},\\ldots,\\mathbf{f}_{N})$ $(\\mathbf{f}_{i}\\in\\mathbb{R}^{D})$ into tokens $\\mathbf{T}=(\\mathbf{t}_{1},\\mathbf{t}_{2},\\cdots\\,,\\mathbf{t}_{N})$ $(\\mathbf{t}_{i}\\in\\mathbb{R}^{D})$ . Finally, the quantized tokens $\\mathbf{T}$ are decoded by the pose decoder $f_{P D}(\\cdot)$ to reconstruct 3D pose $\\hat{\\mathbf{P}}$ . ", "page_idx": 3}, {"type": "text", "text": "Pose Encoder. Considering the interdependencies among human body joints, our goal is to represent 3D poses in a compositional manner, moving away from reliance on coordinates vectors or heatmap embeddings. The VQ-VAE architecture, incorporating MLP-Mixer blocks [69] within its encoder and decoder, has been proven effective in decomposing a pose into multiple token features, each corresponding to a sub-structure of the pose [21]. However, the MLP-Mixer block is designed to extract global information across all joints, which can not adequately capture the local relationships between joints within individual sub-structure. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "In response to aforementioned limitation, we design Local-MLP block to capture the local interactions between 3D joints. The pose encoder $f_{P E}(\\cdot)$ , comprising several Local-MLP blocks, converts $\\mathbf{P}$ to $N$ token features: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{F}=(\\mathbf{f}_{1},\\mathbf{f}_{2},\\cdots,\\mathbf{f}_{N})=f_{P E}(f_{e m b}(\\mathbf{P})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{e m b}(\\cdot)$ embeds $\\mathbf{P}$ to $\\mathbf{P_{emb}}\\in\\mathbb{R}^{J\\times D}$ by a linear layer. ", "page_idx": 4}, {"type": "text", "text": "As shown in Figure 3(a), a Local-MLP block is composed of a Layer Normalization layer, a Joint Shift block (JS-Block), a Channel MLP, and a residual connection. The JS-Block is specifically designed to capture local interactions among $X$ joints. It extracts features by linear projection, and the Joint Shift operation enables feature translation along joint connection directions. As shown in Figure $3({\\mathsf{b}})$ , with the input $\\mathbf{P_{emb}^{\\top}}\\in\\mathbb{R}^{D\\times J}$ , the feature is evenly divided into $X$ segments $X=3$ in the example), each segment being shifted incrementally by units from $-\\lfloor X/2\\rfloor$ to $\\lfloor X/2\\rfloor$ . The central segment remains stationary, while the segments to the left and right are symmetrically shifted away from the center by up to $\\pm\\lfloor X/2\\rfloor$ units. Zero padding is used to maintain dimensionality. Features highlighted within the dashed box are selected for further linear projection. Finally, the Channel MLP processes these features channelwise to facilitate information integration. ", "page_idx": 4}, {"type": "image", "img_path": "p2PO2PUPFY/tmp/dd965a37679adc0315a45adcc74a2827c81efa8da1ea97c076129bd66c8cf1eb.jpg", "img_caption": ["Figure 3: (a) depicts the structure of the LocalMLP block; (b) shows the Joint Shift operation, where the arrows indicate the steps, and different subscript numbers represent the features of different joints. The gray blocks indicate zero padding. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Quantization Process. During this process, we ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "exploit FSQ [49] to enhance the utilization of codewords. FSQ quantizes token features $\\mathbf{F}$ as corresponding token indices: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{k}=(k_{1},k_{2},\\cdot\\cdot\\cdot\\mathbf{\\delta},k_{N})=\\mathrm{FSQ}(f_{p r o j}(\\mathbf{F})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{p r o j}(\\cdot)$ projects each $\\mathbf{f}_{i}\\,\\in\\,\\mathbb{R}^{D}$ of $\\mathbf{F}$ to $\\mathbf{q}_{i}\\,\\in\\,\\mathbb{R}^{d}$ , and each $k_{i}$ of $\\mathbf{k}$ denotes the entries of implied codebook $\\mathcal{C}$ . For each $\\mathbf{q}_{i}$ , FSQ employs a bounding function $f_{b n d}:\\mathbf{q}_{i}\\mapsto\\lfloor L/2\\rfloor\\cdot\\operatorname{tanh}(\\mathbf{q}_{i})$ to constrain each channel of $d$ . As a result, each channel in $\\hat{\\mathbf{q}}_{i}=\\operatorname{round}\\mathopen{}\\mathclose\\bgroup\\left(f_{b n d}\\mathopen{}\\mathclose\\bgroup\\left(\\mathbf{q}_{i}\\aftergroup\\egroup\\right)\\aftergroup\\egroup\\right)$ takes one of $L$ unique values. This procedure yields $\\hat{\\mathbf{q}_{i}}\\in\\mathcal{C}$ , where the total number of unique codebook entries is $\\begin{array}{r}{|{\\mathcal{C}}|=\\prod_{i=1}^{d}L_{i}}\\end{array}$ (mapping the $i$ -th channel to $L_{i}$ values). The vectors in $\\mathcal{C}$ can be enumerated, establish ing a bijective mapping from any $\\hat{\\bf q}_{i}$ to an integer within $\\{1,\\ldots,|{\\mathcal{C}}|\\}$ . In addition, the corresponding codeword of $k_{i}$ , which is denoted as $\\mathbf{t}_{i}\\in\\mathbf{\\overline{{R}}}^{D}$ , represents the quantized result of $\\mathbf{f}_{i}$ . Thereby, using FSQ, the token features $\\mathbf{F}$ are quantized as $\\mathbf{T}=(\\mathbf{t}_{1},\\mathbf{t}_{2},\\cdots\\,,\\mathbf{t}_{N})$ . ", "page_idx": 4}, {"type": "text", "text": "Pose Decoder. The pose decoder $f_{P D}(\\cdot)$ is designed to recover 3D pose $\\hat{\\mathbf{P}}$ from $\\mathbf{T}$ . $f_{P D}(\\cdot)$ adopts a structure similar to the pose encoder but in reverse, utilizing a reduced number of Local-MLP blocks. ", "page_idx": 4}, {"type": "text", "text": "Loss. The pose quantization step, including the pose encoder, quantization process, and pose decoder, is jointly optimized by minimizing L1 loss $\\mathcal{L}_{P Q}=||P-\\hat{P}||_{1}$ across the training dataset. ", "page_idx": 4}, {"type": "text", "text": "3.2 Discrete Diffusion Process ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After training the pose quantization step, we can acquire $N$ quantized tokens $\\mathbf{k}$ from the original 3D pose P. The next step in $\\mathrm{Di^{2}}$ Pose pipeline is to model $\\mathbf{k}$ in the latent space by the discrete diffusion process. In the following, we first briefly introduce the diffusion models and clarify the basic principles of the discrete diffusion model. Then we explain the details of discrete diffusion process, including the designed transition matrix and loss function. Eventually, we illustrate the architecture and training and inference process. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Discrete Diffusion Model. Our discrete diffusion model is characterized by two distinct processes: 1) Forward process: It progresses through discrete steps $s\\in\\{0,1,2,...,S\\}$ , gradually transforming the initial tokens $\\mathbf{k}_{\\mathrm{0}}$ (the quantized token $\\mathbf{k}$ ) into a noise-infused latent representation $\\mathbf{k}_{S}$ . 2) Reverse process: It is tasked with reconstructing the original data $\\mathbf{k}_{\\mathrm{0}}$ from the latent $\\mathbf{k}_{S}$ , following a reverse temporal sequence $s\\in\\{S,S-1,...,1,0\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Followed previous studies [67, 3, 28], we use a transition probability matrix $[{\\bf M}_{s}]_{i j}\\;=\\;q({\\bf k}_{s}\\;=\\;$ $i|\\mathbf{k}_{s-1}=j)\\in\\mathbb{R}^{|\\mathcal{C}|\\times|\\mathcal{C}|}$ elucidate the likelihood of transitioning from $\\mathbf{k}_{s-1}$ to $\\mathbf{k}_{s}$ . Then the forward process for the entire sequence of tokens is expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nq(\\mathbf{k}_{s}|\\mathbf{k}_{s-1})=c^{\\top}(\\mathbf{k}_{s})\\mathbf{M}_{s}c(\\mathbf{k}_{s-1}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c(\\cdot)$ symbolizes a function capable of converting a scalar into a one-hot column vector. The distribution of ${\\bf k}_{s}$ follows a categorical distribution, determined by the vector $\\mathbf{M}_{s}c(\\mathbf{k}_{s-1})$ . Leveraging the Markov chain property, it is feasible to bypass intermediate stages, directly computing the probability of ${\\bf k}_{s}$ from $\\mathbf{k}_{0}$ for any given step as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nq(\\mathbf{k}_{s}|\\mathbf{k}_{0})=c^{\\top}(\\mathbf{k}_{s})\\overline{{\\mathbf{M}}}_{s}c(\\mathbf{k}_{0}),\\mathrm{with}\\;\\overline{{\\mathbf{M}}}_{s}=\\mathbf{M}_{s}\\ldots\\mathbf{M}_{1}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, the posterior of the reverse process, $q(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\mathbf{k}_{0})$ , can be ascertained as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nq(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\mathbf{k}_{0})=\\frac{q(\\mathbf{k}_{s}|\\mathbf{k}_{s-1},\\mathbf{k}_{0})q(\\mathbf{k}_{s-1}|\\mathbf{k}_{0})}{q(\\mathbf{k}_{s}|\\mathbf{k}_{0})}=\\frac{\\left(c^{\\top}(\\mathbf{k}_{s})\\mathbf{M}_{s}c(\\mathbf{k}_{s-1})\\right)\\left(c^{\\top}(\\mathbf{k}_{s-1})\\overline{{\\mathbf{M}}}_{s-1}c(\\mathbf{k}_{0})\\right)}{c^{\\top}(\\mathbf{k}_{s})\\overline{{\\mathbf{M}}}_{s}c(\\mathbf{k}_{0})}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Occlude and Replace Transition Matrix. Notably, a suitable design for transition probability matrix $\\mathbf{M}_{s}$ is significant to train the discrete diffusion process. As illustrated in Section 3.1, through pretrained pose encoder and FSQ, $\\mathbf{P}$ can be converted to $\\mathbf{k}=\\left(k_{1},k_{2},\\cdot\\cdot\\cdot,k_{N}\\right)$ , each $k_{i}$ corresponding to a sub-structure of the overall pose. With this foundation, we specifically devise the occlude and replace scheme, which is inspired by [24], for tackling the challenges of occluded 3D HPE. In occlusion scenes, the human body is always occluded in various situations (self-occlusions, object or people-to-person occlusions), and the typical manifestation is that some sub-structures of the pose are invisible. Consequently, we design the occlude scheme simulating the occlusion of corresponding joints, which introduces occlusion impact in the training process. Additionally, recognizing the inherent uncertainty in occlusion scenarios where a single occluded region may correspond to multiple potential 3D human poses, we develop the replace strategy to update certain token with another available token. ", "page_idx": 5}, {"type": "text", "text": "In practice, each quantized token $k_{i}$ has a probability of $\\gamma_{s}$ to transition to the $\\bigcirc\\!\\bigcirc\\!\\bigcirc$ token. Moreover, $k_{i}$ is also subject to a probability of $|\\mathcal{C}|\\beta_{s}$ to be uniformly resampled across all $|{\\mathcal{C}}|$ categories. Furthermore, $k_{i}$ retains a probability of $\\alpha_{s}\\,=\\,1\\,-\\,|\\mathcal{C}|\\beta_{s}\\,-\\,\\gamma_{s}$ to remain unchanged. Then, the transition matrix $\\mathbf{M}_{s}\\in\\mathbb{R}^{(\\bar{|\\mathcal{C}|}+1)\\times(|\\mathcal{C}|+1)}$ is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{M}_{s}=\\left[\\begin{array}{c c c c}{\\alpha_{s}+\\beta_{s}}&{\\beta_{s}}&{\\cdot\\cdot}&{0}\\\\ {\\beta_{s}}&{\\alpha_{s}+\\beta_{s}}&{\\cdot\\cdot}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\gamma_{s}}&{\\gamma_{s}}&{\\cdot\\cdot}&{1}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha_{s}$ , $\\beta_{s}\\in[0,1]$ . The prior distribution of step $S$ can be derived as: $p(\\mathbf{k}_{S})=[\\overline{{\\beta}}_{S},\\overline{{\\beta}}_{S},\\cdot\\cdot\\cdot,\\overline{{\\gamma}}_{S}]$ , where $\\textstyle{\\overline{{\\alpha}}}_{S}=\\prod_{i=1}^{S}\\alpha_{i}$ , $\\begin{array}{r}{\\overline{\\gamma}_{S}=1-\\prod_{i=1}^{S}(1-\\gamma_{i})}\\end{array}$ and $\\overline{{\\beta}}_{S}=(1-\\overline{{\\alpha}}_{S}-\\overline{{\\gamma}}_{S})/\\vert\\mathcal{C}\\vert$ . In this study, we adapt the line ar schedule [25] as noise schedule to pre-define the value of transition matrices $(\\overline{{\\alpha}}_{S}$ , $\\overline{{\\beta}}_{S}$ , and $\\overline{\\gamma}_{S}$ ). Subsequently, we can calculate $q(\\mathbf{k}_{s}|\\mathbf{k}_{0})$ according to Eq. (4). However, when the number of categories $|{\\mathcal{C}}|$ and time step $S$ is too large, it can quickly become impractical to store all of the transition matrices ${{\\bf{M}}_{s}}$ in memory, as the memory usage grows like $O(|\\bar{C}|^{2}S)$ . Actually, it is unnecessary to store all of the transition matrices. Instead we only store all of $\\overline{{\\alpha}}_{s}$ and $\\overline{{\\beta}}_{s}$ in advance, since we can calculate $q(\\mathbf{k}_{s}|\\mathbf{k}_{0})$ according to following formula (refer to Appendix for proofs): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{M}}}_{s}c(\\mathbf{k}_{0})=\\overline{{\\alpha}}_{s}c(\\mathbf{k}_{0})+(\\overline{{\\gamma}}_{s}-\\overline{{\\beta}}_{s})c(|\\mathcal{C}|+1)+\\overline{{\\beta}}_{s}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Training Objectives. We train a network $f_{\\theta}(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\pmb{y})$ to estimate $q(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\mathbf{k}_{0})$ in the reverse process. The network is trained to minimize the variational lower bound (VLB): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{v l b}=D_{K L}(q(\\mathbf{k}_{S}|\\mathbf{k}_{0})||p(\\mathbf{k}_{S}))+\\sum_{s=1}^{S-1}\\big\\{D_{K L}[q(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\mathbf{k}_{0})||f_{\\theta}(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},y)]\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In addition, we follow [50, 24] to utilize the reparameterization trick, which lets $\\mathrm{Di^{2}P o s e}$ predict the noiseless token distribution $f_{\\theta}(\\hat{\\mathbf{k}}_{0}|\\mathbf{k}_{s},y)$ at each reverse step, and then compute $f_{\\theta}(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\pmb{y})$ as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{\\theta}(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\pmb{y})=\\sum_{\\hat{\\mathbf{k}}_{0}=1}^{H}\\!q(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\hat{\\mathbf{k}}_{0})f_{\\theta}(\\hat{\\mathbf{k}}_{0}|\\mathbf{k}_{s},\\pmb{y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Based on the Eq. (9), an auxiliary denoising objective loss is introduced, which encourages the network to predict $f_{\\theta}(\\hat{\\mathbf{k}}_{0}|\\mathbf{k}_{s},y)$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k_{0}}=-\\log f_{\\theta}(\\hat{\\mathbf{k}}_{0}|\\mathbf{k}_{s},\\pmb{y}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Our final loss function is defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\lambda\\mathcal{L}_{\\mathbf{k}_{0}}+\\mathcal{L}_{v l b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda$ is a hyper-parameter to control the weight of the auxiliary loss $\\mathcal{L}_{\\mathbf{k}_{0}}$ ", "page_idx": 6}, {"type": "text", "text": "Diffusion Architecture. As depicted in Figure 2, our discrete diffusion model consists of three main components: an image encoder, a pose denoiser, and a pose decoder. The pre-trained image encoder processes the 2D image to produce a conditional feature sequence. The pose denoiser, receiving the quantized pose tokens ${\\bf k}_{s}$ and and step $S$ , predicts the distribution of noiseless tokens $f_{\\theta}(\\hat{\\mathbf{k}}_{0}|\\mathbf{k}_{s},y)$ . This component is equipped with several transformer blocks, each featuring an AdaLNorm operator [6], multi-head attention blocks that combine the image feature information with ${\\bf k}_{s}$ , and layer normalization and linear layers. At the end of the reverse process, all recovered tokens are obtained, and the final prediction of 3D pose is decoded by the well-trained pose decoder. ", "page_idx": 6}, {"type": "text", "text": "Training and Inference Process. In the training process, as for step $s$ , we sample $\\mathbf{k}_{s}$ from $q(\\mathbf{k}_{s}|\\mathbf{k}_{0})$ based on Eq. (7) in the forward process. We then estimate $f_{\\theta}(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\pmb{y})$ in the reverse process. The final loss will be calculated according to Eq. (11). In the inference process, all pose tokens are either masked or initialized randomly. Subsequently, we predict $f_{\\theta}(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\pmb{y})$ step by step until the tokens are completely recovered. Finally, reconstructed tokens are decoded by the pose decoder, resulting in the recovered 3D pose. The complete algorithms are summarized in Appendix. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Datasets and Evaluation Metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. Human3.6M [34] is the most extensive benchmark for 3D HPE, consisting of 3.6 million images. We follow [22] with same protocol, which involves training on subjects S1, S5, S6, S7, and S8, and testing on subjects S9 and S11. 3DPW [72] is the first dataset in the wild that includes video footage taken from a moving phone camera. We also evaluate our method on this dataset to measure the robustness and generalization. Additionally, to further verify the occlusion-robustness, we evaluate $\\mathrm{Di^{2}}$ Pose on the 3DPW-Occ [83], which is a subset of the 3DPW. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. For Human3.6M and 3DPW, we follow the standard protocols. Mean per joint position error (MPJPE) calculates the mean Euclidean distance between the root-aligned reconstructed poses and ground truth joint coordinates. PA-MPJPE employs a Procrustes alignment between the poses before calculating the MPJPE. In addition, to further evaluate the effectiveness of our method under occlusion scenes, we devise an adversarial protocol, termed 3DPW-AdvOcc, following the previous research [84]. We apply occlusion patches to the input image to identify the most challenging predictions. This process involves assessing the relative performance degradation on the visible joints. Similar to [84], we utilize textured patches generated by randomly cropping texture maps from the DTD [16]. We employ two square patch sizes: 40 and 80 relative to a $256\\,\\times$ 192 image, denoted as $\\operatorname{Occ}(\\!\\alpha\\!\\,\\!40\\$ and Occ $@80$ respectively, with a stride of 10. ", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Pose Quantization Step. The pose encoder is constructed with four Local-MLP blocks, while the pose decoder incorporates a single block. Within these Local-MLP blocks, the embedding dimensions $D$ for the pose encoder and decoder are configured to 2048 and 512, respectively. For the quantization process, the projected vector $\\mathbf{q}_{i}$ features the channel $d\\,=\\,5$ . The levels per channel, denoted as $\\left[L_{1},\\cdot\\cdot\\cdot,L_{d}\\right]$ , are specified as [7, 5, 5, 5, 5]. The number of quantized tokens $N$ is set to 100. ", "page_idx": 6}, {"type": "table", "img_path": "p2PO2PUPFY/tmp/e0244950b570af0415e825357dd7ef18ccb7c0657c58f7cd8b49cf8cd0265bfd.jpg", "table_caption": ["Table 1: Results on Human3.6M in millimeters under MPJPE. The best results are in bold, and the second-best ones are underlined. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "p2PO2PUPFY/tmp/fb3b2d418c243b0f503eda95e94bb21508a56f98338737e240c1dc1e627edfc4.jpg", "table_caption": ["Table 2: Evaluation on 3DPW, 3DPW-Occ, and 3DPW-AdvOcc. The number 40 and 80 after 3DPWAdvOcc denote the occluder size. \\* denotes the results from our implementation. The best results are in bold, and the second-best ones are underlined. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Discrete Diffusion Process. For the occlude and replace transition matrix, we linearly increase $\\overline{{\\beta}}_{s}$ and $\\overline{\\gamma}_{s}$ from 0 to 0.1 and 0.9, respectively, and decrease $\\overline{{\\alpha}}_{s}$ from 1 to 0. For the discrete diffusion model, we use off-the-shelf image encoder [79] to extract feature sequence of conditional 2D image. As for the pose denoiser, we build a 21-layer 16-head transformer with the dimension of 1024. We set steps $S$ as 100 and loss weight $\\lambda$ is set to 5e-4. Please refer to Appendix for more details. ", "page_idx": 7}, {"type": "text", "text": "4.3 Comparsion with State-of-the-Arts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Human3.6M. To explore the effectiveness of $\\mathrm{Di^{2}P o s e}$ , we evaluate its performance in the challenging context of frame-based 3D pose estimation. Specifically, within the discrete diffusion process, context information is extracted from a single input frame using the image encoder. As shown in Table 1, we benchmark $\\mathrm{Di^{2}}$ Pose against SOTA 3D HPE methods on the Human3.6M. Our $\\mathrm{Di^{2}}$ Pose achieves $49.2\\mathrm{mm}$ in average MPJPE, surpassing the performance of the SOTA diffusion model [22] by $0.5\\mathrm{mm}$ , which indicates that $\\mathrm{Di^{2}P o s e}$ is able to enhance monocular 3D HPE in indoor scenes. ", "page_idx": 7}, {"type": "text", "text": "3DPW. Beyond indoor settings, we evaluate the performance of $\\mathrm{Di^{2}P o s e}$ on the in-the-wild 3DPW dataset. As Table 2 shows, $\\mathrm{Di^{2}P o s e}$ achieves the SOTA performance, and outperforms the SOTA method [22] by $3.4\\mathrm{mm}$ in MPJPE and $3.7\\mathrm{mm}$ in PA-MPJPE. On the occlusion-centric 3DPW-Occ, $\\mathrm{Di^{2}}$ Pose maintains its superiority. When assessed under the 3DPW-AdvOcc protocol, all methods exhibit performance drops\u2014MPJPE surges by up to $129\\%$ and PA-MPJPE by up to $72\\%$ . Despite this, $\\mathrm{Di}^{\\bar{2}}$ Pose remains markedly robust, leading the SOTA by significant margins in both MPJPE and PA-MPJPE, underscoring its effectiveness in handling occlusions. ", "page_idx": 7}, {"type": "text", "text": "Qualitative Results. Figure 4 presents the qualitative results of DiffPose [22] in comparison with our $\\mathrm{Di^{2}}$ Pose across two datasets. It can be observed that our method yields more accurate predictions than compared diffusion model (DiffPose), especially under various occlusion scenarios (self-occlusion and object occlusion). This demonstrates the superior occlusion-robustness of our $\\mathrm{Di^{2}P o s e}$ . ", "page_idx": 7}, {"type": "image", "img_path": "p2PO2PUPFY/tmp/2f77b9005ee37872df1758d410279f4cd9abb76e56b41ee93693e8ba02fc78e9.jpg", "img_caption": ["Figure 4: Qualitative results on two datasets. The black lines represent the ground truth poses and the blue lines are prediction results. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "p2PO2PUPFY/tmp/6af94022645f04b919ba98d0af2a2b788f6a1a15103ea89f8298a4c0def36d3f.jpg", "table_caption": ["Table 3: Ablations on Human3.6M. P-1 and P-2 represent MPJPE and PA-MPJPE, respectively. "], "table_footnote": ["(a) Different local joint (b) Different levels per (c) Different final occlude (d) Different number of number $X$ of Joint Shift channel $\\left[L_{1},\\cdot\\cdot\\cdot,L_{d}\\right]$ of rate $\\overline{\\gamma}_{S}$ for the occlude and training and inference operations in JS-Block. quantization process FSQ. replace transition matrix. steps $S$ . P-1 are reported. "], "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effectiveness of Pose Quantization Step. Our pose quantization step, which consists of Local-MLP blocks, is designed for representing 3D human pose by capturing the local interactions between 3D joints. Table 4 displays the MPJPE metrics comparing the original 3D poses with those reconstructed via various methods. The results show that our pose quantization step reconstructs 3D poses with lower errors compared to previous method [21], which uses an MLP-Mixer for global joint information extraction. It indicates that our model learns a more accurate representation of 3D poses. In addition, we conducted other ablation studies to investigate different local joint numbers $X$ and levels per channel $\\left[L_{1},\\cdot\\cdot\\cdot,L_{d}\\right]$ within pose quantization step, as shown in Table 3a and Table 3b. As to different $X$ , note that when $X=1$ , we only extract feature of individual joint, and when $X>1$ , JS-Block is able to capture local interactions of different joints. Experimental results indicate that $X=3$ reaches lowest reconstruct error. As for $[L_{1},\\cdot\\cdot\\cdot\\ ,L_{d}]$ , the best level of FSQ for pose quantization is [7, 5, 5, 5, 5]. ", "page_idx": 8}, {"type": "table", "img_path": "p2PO2PUPFY/tmp/b07f41d6179c40443476c8521e7820aa7aedbae8b9eebe166da7847439c99167.jpg", "table_caption": ["Table 4: Different representation methods for 3D HPE. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Impact of Different Transition Matrices. To demonstrate the effectiveness of the specifically designed occlude and replace transition matrix, we constructed three transition matrices for discrete diffusion process: occlude transition matrix, replace transition matrix, and occlude and replace transition matrix. Table 5 illustrates that the optimal results are achieved when utilizing the occlude and replace transition matrix. The suboptimal performance observed when exclusively employing the other two transition matrices can be attributed to the following reasons: Utilizing solely the replace transition matrix introduces the challenge of random, irrelevant sub-structures, complicating the learning of the reverse process; Conversely, relying exclusively on the occlude transition matrix causes the model to overly focus on the occluded portions, neglecting the contextual information from other visible parts. These clarifications can be verified in Table 3c, where we investigate the impact of different $\\overline{\\gamma}_{S}$ . When $\\overline{\\gamma}_{S}=0$ , the occlude and replace transition matrix can be seen as the replace transition matrix, and when $\\overline{\\gamma}_{S}=1$ , the occlude and replace transition matrix can be seen as the occlude transition matrix. The best performance is obtained when $\\overline{{\\gamma}}_{S}=0.9$ . ", "page_idx": 8}, {"type": "table", "img_path": "p2PO2PUPFY/tmp/a433ef42ffb21c1dabc7cf3a5682d6fe62144032f1a5ffd795b2ac28740a5d1b.jpg", "table_caption": ["Table 5: Different transition matrices for discrete diffusion model. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "p2PO2PUPFY/tmp/4991274ea2f55b78e0b6be7160c0f52cf8f7ac98f6b3cbc9717de394f69e376e.jpg", "img_caption": ["Figure 5: Failure cases of our $\\mathrm{Di^{2}}$ Pose for 3D HPE. These instances primarily occur in scenarios with severe occlusions, as compared against ground truth (GT) poses. The content encircled by the dashed line indicates the parts where differences exist. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "In addition, we conducted an ablation study to investigate the impact of $S$ on the training and inference processes, as shown in Table 3d. We observed that using larger numbers of steps during both training and inference stages improves performance but also increases time complexity. Moreover, the results indicate that performance remains satisfactory even when the number of inference steps is reduced by $75\\%$ (e.g., from 100 steps during training to 25 steps during inference). This finding suggests a viable strategy for enhancing generation speed without significantly compromising quality. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Figure 5 illustrates several results of 3D human pose estimation. When substantial occlusions cover the human body\u2014obscuring the exact pose to the extent that it confounds even human observers\u2014the predictions made by $\\mathrm{Di^{2}}$ Pose may deviate from GT 3D pose. This deviation primarily stems from the inherent limitation of inferring 3D poses directly from 2D images, which lack critical spatial depth information. Such limitations introduce uncertainty and indeterminacy in the predictions. ", "page_idx": 9}, {"type": "text", "text": "Despite these challenges, $\\mathrm{Di^{2}}$ Pose manages occlusions effectively by producing physically plausible outcomes. This capability is attributed to the integration of a pose quantization step within $\\mathrm{\\dot{Di}^{2}P o s e}$ , which constrains the model\u2019s search space to physically reasonable configurations. Note that the pose quantization step is trained on real 3D human pose data, enhancing its reliability under severe occlusions. ", "page_idx": 9}, {"type": "text", "text": "Currently, $\\mathrm{Di^{2}P o s e}$ is primarily designed for frame-based 3D HPE and does not utilize interframe data from videos. Future enhancements will focus on incorporating interframe information to refine the accuracy of 3D pose predictions further within the $\\mathrm{Di^{2}}$ Pose framework. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents $\\mathrm{Di^{2}P o s e}$ , a novel diffusion-based framework that tackles occluded 3D HPE in discrete space. $\\mathrm{Di^{2}}$ Pose first captures the local interactions of joints and represents a 3D pose by multiple quantized tokens. Then, the discrete diffusion process models the discrete tokens in latent space through a conditional diffusion model, which implicitly introduces occlusion into the modeling process for more reliable 3D HPE with occlusions. Experimental results show that our method surpasses the state-of-the-art approaches on three widely used benchmarks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (62337001) and the Fundamental Research Funds for the Central Universities (226-2024-00058). Long Chen is supported by HKUST Special Support for Young Faculty (F0927) and HKUST Sports Science and Technology Research Grant (SSTRG24EG04). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] I. Akhter and M. J. Black. Pose-conditioned joint angle limits for 3d human pose reconstruction. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1446\u20131455, 2015.   \n[2] T. Alldieck, H. Xu, and C. Sminchisescu. imghum: Implicit generative models of 3d human shape and articulated pose. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5461\u20135470, 2021.   \n[3] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021.   \n[4] O. Avrahami, D. Lischinski, and O. Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18208\u2013 18218, 2022.   \n[5] S. Azadi, A. Shah, T. Hayes, D. Parikh, and S. Gupta. Make-an-animation: Large-scale text-conditional 3d human motion generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 15039\u201315048, 2023.   \n[6] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \n[7] B. Biggs, D. Novotny, S. Ehrhardt, H. Joo, B. Graham, and A. Vedaldi. 3d multi-bodies: Fitting sets of plausible 3d human models to ambiguous image data. Advances in neural information processing systems, 33:20496\u201320507, 2020.   \n[8] E. A. Brempong, S. Kornblith, T. Chen, N. Parmar, M. Minderer, and M. Norouzi. Denoising pretraining for semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4175\u20134186, 2022.   \n[9] Y. Cai, L. Ge, J. Liu, J. Cai, T.-J. Cham, J. Yuan, and N. M. Thalmann. Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2272\u20132281, 2019.   \n[10] S. Chen, P. Sun, Y. Song, and P. Luo. Diffusiondet: Diffusion model for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19830\u201319843, 2023.   \n[11] T. Chen, C. Fang, X. Shen, Y. Zhu, Z. Chen, and J. Luo. Anatomy-aware 3d human pose estimation with bone-based pose decomposition. IEEE Transactions on Circuits and Systems for Video Technology, 32(1):198\u2013209, 2021.   \n[12] Y. Cheng, B. Wang, B. Yang, and R. T. Tan. Graph and temporal convolutional networks for 3d multi-person pose estimation in monocular videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1157\u20131165, 2021.   \n[13] H.-g. Chi, S. Chi, S. Chan, and K. Ramani. Pose relation transformer refine occlusions for human pose estimation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 6138\u20136145. IEEE, 2023.   \n[14] J. Choi, D. Shim, and H. J. Kim. Diffupose: Monocular 3d human pose estimation via denoising diffusion probabilistic model. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3773\u20133780. IEEE, 2023.   \n[15] H. Ci, C. Wang, X. Ma, and Y. Wang. Optimizing network structure for 3d human pose estimation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2262\u20132271, 2019.   \n[16] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3606\u20133613, 2014.   \n[17] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[18] Z. Fan, J. Liu, and Y. Wang. Motion adaptive pose estimation from compressed videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11719\u201311728, 2021.   \n[19] R. Feng, Y. Gao, T. H. E. Tse, X. Ma, and H. J. Chang. Diffpose: Spatiotemporal diffusion model for video-based human pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14861\u201314872, 2023.   \n[20] L. G. Foo, J. Gong, Z. Fan, and J. Liu. System-status-aware adaptive network for online streaming video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10514\u201310523, 2023.   \n[21] Z. Geng, C. Wang, Y. Wei, Z. Liu, H. Li, and H. Hu. Human pose as compositional tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 660\u2013671, 2023.   \n[22] J. Gong, L. G. Foo, Z. Fan, Q. Ke, H. Rahmani, and J. Liu. Diffpose: Toward more reliable 3d pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13041\u201313051, 2023.   \n[23] S. Gong, M. Li, J. Feng, Z. Wu, and L. Kong. Diffuseq-v2: Bridging discrete and continuous text spaces for accelerated seq2seq diffusion models. arXiv preprint arXiv:2310.05793, 2023.   \n[24] S. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.   \n[25] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[26] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):1\u201333, 2022.   \n[27] K. Holmquist and B. Wandt. Diffpose: Multi-hypothesis human pose estimation using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15977\u201315987, 2023.   \n[28] E. Hoogeboom, D. Nielsen, P. Jaini, P. Forr\u00e9, and M. Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454\u201312465, 2021.   \n[29] M. R. I. Hossain and J. J. Little. Exploiting temporal information for 3d human pose estimation. In Proceedings of the European conference on computer vision (ECCV), pages 68\u201384, 2018.   \n[30] M. Hu, Y. Wang, T.-J. Cham, J. Yang, and P. N. Suganthan. Global context with discrete diffusion in vector quantised modelling for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11502\u201311511, 2022.   \n[31] W. Hu, C. Zhang, F. Zhan, L. Zhang, and T.-T. Wong. Conditional directed graph convolution for 3d human pose estimation. In Proceedings of the 29th ACM International Conference on Multimedia, pages 602\u2013611, 2021.   \n[32] R. Huang, Z. Zhao, H. Liu, J. Liu, C. Cui, and Y. Ren. Prodiff: Progressive fast diffusion model for high-quality text-to-speech. In Proceedings of the 30th ACM International Conference on Multimedia, pages 2595\u20132605, 2022.   \n[33] N. Inoue, K. Kikuchi, E. Simo-Serra, M. Otani, and K. Yamaguchi. Layoutdm: Discrete diffusion model for controllable layout generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10167\u201310176, 2023.   \n[34] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence, 36(7):1325\u20131339, 2013.   \n[35] W. Jiang, S. Jin, W. Liu, C. Qian, P. Luo, and S. Liu. Posetrans: A simple yet effective pose transformation augmentation for human pose estimation. In European Conference on Computer Vision, pages 643\u2013659. Springer, 2022.   \n[36] Z. Jiang, Z. Wang, and L. Chen. Combing text-based and drag-based editing for precise and flexible image editing. arXiv preprint arXiv:2410.03097, 2024.   \n[37] Z. Jiang, Z. Zhou, L. Li, W. Chai, C.-Y. Yang, and J.-N. Hwang. Back to optimization: Diffusion-based zero-shot 3d human pose estimation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6142\u20136152, 2024.   \n[38] H. Joo, N. Neverova, and A. Vedaldi. Exemplar fine-tuning for 3d human model ftiting towards in-the-wild 3d human pose estimation. In 2021 International Conference on 3D Vision (3DV), pages 42\u201352. IEEE, 2021.   \n[39] M. Kocabas, C.-H. P. Huang, O. Hilliges, and M. J. Black. Pare: Part attention regressor for 3d human body estimation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 11127\u201311137, 2021.   \n[40] H. Kong, K. Gong, D. Lian, M. B. Mi, and X. Wang. Priority-centric human motion generation in discrete latent space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14806\u201314816, 2023.   \n[41] J. N. Kundu, S. Seth, M. Rahul, M. Rakesh, V. B. Radhakrishnan, and A. Chakraborty. Kinematicstructure-preserved representation for unsupervised 3d human pose estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11312\u201311319, 2020.   \n[42] D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11523\u201311532, 2022.   \n[43] J. Li, S. Bian, A. Zeng, C. Wang, B. Pang, W. Liu, and C. Lu. Human pose regression with residual log-likelihood estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11025\u201311034, October 2021.   \n[44] J. Li, C. Xu, Z. Chen, S. Bian, L. Yang, and C. Lu. Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3383\u20133393, 2021.   \n[45] K. Liu, R. Ding, Z. Zou, L. Wang, and W. Tang. A comprehensive study of weight sharing in graph networks for 3d human pose estimation. In Proceedings of the European conference on computer vision (ECCV), pages 318\u2013334. Springer, 2020.   \n[46] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[47] W. Mao, Y. Ge, C. Shen, Z. Tian, X. Wang, Z. Wang, and A. v. den Hengel. Poseur: Direct human pose regression with transformers. In Proceedings of the European conference on computer vision (ECCV), pages 72\u201388. Springer, 2022.   \n[48] J. Martinez, R. Hossain, J. Romero, and J. J. Little. A simple yet effective baseline for 3d human pose estimation. In Proceedings of the IEEE international conference on computer vision, pages 2640\u20132649, 2017.   \n[49] F. Mentzer, D. Minnen, E. Agustsson, and M. Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023.   \n[50] A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162\u20138171. PMLR, 2021.   \n[51] Q. Nie, Z. Liu, and Y. Liu. Lifting 2d human pose to 3d with domain adapted 3d body concept. International Journal of Computer Vision, 131(5):1250\u20131268, 2023.   \n[52] S. Park, J. Hwang, and N. Kwak. 3d human pose estimation using convolutional neural networks with 2d pose information. In Computer Vision\u2013ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14, pages 156\u2013169. Springer, 2016.   \n[53] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[54] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis. Coarse-to-fine volumetric prediction for singleimage 3d human pose. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7025\u20137034, 2017.   \n[55] D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli. 3d human pose estimation in video with temporal convolutions and semi-supervised training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7753\u20137762, 2019.   \n[56] J. Peng, D. Liu, S. Xu, and H. Li. Generating diverse structure for image inpainting with hierarchical vq-vae. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10775\u201310784, 2021.   \n[57] I. A. Petrov, R. Marin, J. Chibane, and G. Pons-Moll. Object pop-up: Can we infer 3d objects and their poses from human interactions alone? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4726\u20134736, 2023.   \n[58] I. Radwan, A. Dhall, and R. Goecke. Monocular image 3d human pose estimation under self-occlusion. In Proceedings of the IEEE International Conference on Computer Vision, pages 1888\u20131895, 2013.   \n[59] A. Razavi, A. Van den Oord, and O. Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019.   \n[60] C. Rockwell and D. F. Fouhey. Full-body awareness from partial observations. In European Conference on Computer Vision, pages 522\u2013539. Springer, 2020.   \n[61] G. Rogez and C. Schmid. Mocap-guided data augmentation for 3d pose estimation in the wild. Advances in neural information processing systems, 29, 2016.   \n[62] G. Rogez, P. Weinzaepfel, and C. Schmid. Lcr-net: Localization-classification-regression for human pose. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3433\u20133441, 2017.   \n[63] C. Rommel, E. Valle, M. Chen, S. Khalfaoui, R. Marlet, M. Cord, and P. P\u00e9rez. Diffhpe: Robust, coherent 3d human pose lifting with diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3220\u20133229, 2023.   \n[64] I. S\u00e1r\u00e1ndi, T. Linder, K. O. Arras, and B. Leibe. How robust is 3d human pose estimation to occlusion? arXiv preprint arXiv:1808.09316, 2018.   \n[65] W. Shan, Z. Liu, X. Zhang, S. Wang, S. Ma, and W. Gao. P-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation. In European Conference on Computer Vision, pages 461\u2013478. Springer, 2022.   \n[66] W. Shan, Z. Liu, X. Zhang, Z. Wang, K. Han, S. Wang, S. Ma, and W. Gao. Diffusion-based 3d human pose estimation with multi-hypothesis aggregation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14761\u201314771, 2023.   \n[67] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[68] X. Sun, B. Xiao, F. Wei, S. Liang, and Y. Wei. Integral human pose regression. In Proceedings of the European conference on computer vision (ECCV), pages 529\u2013545, 2018.   \n[69] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261\u201324272, 2021.   \n[70] S. Tripathi, L. M\u00fcller, C.-H. P. Huang, O. Taheri, M. J. Black, and D. Tzionas. 3d human pose estimation via intuitive physics. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 4713\u20134725, 2023.   \n[71] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.   \n[72] T. Von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and G. Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In Proceedings of the European conference on computer vision (ECCV), pages 601\u2013617, 2018.   \n[73] J. Wang, S. Yan, Y. Xiong, and D. Lin. Motion guided 3d pose estimation from videos. In European conference on computer vision, pages 764\u2013780. Springer, 2020.   \n[74] Z. Wang, Y. Jiang, D. Zheng, J. Xiao, and L. Chen. Event-customized image generation. arXiv preprint arXiv:2410.02483, 2024.   \n[75] E. Xie, L. Yao, H. Shi, Z. Liu, D. Zhou, Z. Liu, J. Li, and Z. Li. Difffti: Unlocking transferability of large diffusion models via simple parameter-efficient fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4230\u20134239, 2023.   \n[76] J. Xu, Z. Yu, B. Ni, J. Yang, X. Yang, and W. Zhang. Deep kinematics analysis for monocular 3d human pose estimation. In Proceedings of the IEEE/CVF Conference on computer vision and Pattern recognition, pages 899\u2013908, 2020.   \n[77] T. Xu and W. Takano. Graph stacked hourglass networks for 3d human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16105\u201316114, 2021.   \n[78] Y. Xu, Z. Wang, J. Xiao, W. Liu, and L. Chen. Freetuner: Any subject in any style with training-free diffusion. arXiv preprint arXiv:2405.14201, 2024.   \n[79] Y. Xu, J. Zhang, Q. Zhang, and D. Tao. Vitpose: Simple vision transformer baselines for human pose estimation. Advances in Neural Information Processing Systems, 35:38571\u201338584, 2022.   \n[80] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu. Diffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.   \n[81] A. Zanfir, M. Zanfir, A. Gorban, J. Ji, Y. Zhou, D. Anguelov, and C. Sminchisescu. Hum3dil: Semisupervised multi-modal 3d humanpose estimation for autonomous driving. In Conference on Robot Learning, pages 1114\u20131124. PMLR, 2023.   \n[82] J. Zhang, K. Gong, X. Wang, and J. Feng. Learning to augment poses for 3d human pose estimation in images and videos. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[83] T. Zhang, B. Huang, and Y. Wang. Object-occluded human shape and pose estimation from a single color image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7376\u20137385, 2020.   \n[84] Y. Zhang, P. Ji, A. Wang, J. Mei, A. Kortylewski, and A. Yuille. 3d-aware neural body ftiting for occlusion robust 3d human pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9399\u20139410, 2023.   \n[85] L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. N. Metaxas. Semantic graph convolutional networks for 3d human pose regression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3425\u20133435, 2019.   \n[86] Q. Zhao, C. Zheng, M. Liu, and C. Chen. A single 2d pose with context is worth hundreds for 3d human pose estimation. Advances in Neural Information Processing Systems, 36, 2024.   \n[87] Q. Zhao, C. Zheng, M. Liu, P. Wang, and C. Chen. Poseformerv2: Exploring frequency domain for efficient and robust 3d human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8877\u20138886, 2023.   \n[88] W. Zhao, W. Wang, and Y. Tian. Graformer: Graph-oriented transformer for 3d pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20438\u2013 20447, 2022.   \n[89] C. Zheng and A. Vedaldi. Online clustered codebook. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22798\u201322807, 2023.   \n[90] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, and Z. Ding. 3d human pose estimation with spatial and temporal transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 11656\u201311665, 2021.   \n[91] J. Zhou, T. Zhang, Z. Hayder, L. Petersson, and M. Harandi. Diff3dhpe: A diffusion model for 3d human pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2092\u20132102, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this Appendix, we provide relevant preliminary knowledge, mathematical proofs, complete training and inference algorithms, additional experimental results, more implementation details about our $\\mathrm{Di^{2}P o s e}$ and broader impacts. ", "page_idx": 14}, {"type": "text", "text": "A Preliminary: Continuous Diffusion Model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The continuous diffusion model consists of two primary processes: the forward process and the reverse process. The forward process methodically corrupts the original data $\\scriptstyle x_{0}$ into a noisy latent variable $\\pmb{x}_{S}$ , which converges to a stationary distribution (e.g., a Gaussian distribution). Conversely, the reverse process aims to reconstruct the original data $\\scriptstyle{x_{0}}$ from $\\pmb{x}_{S}$ , utilizing learned parameters. ", "page_idx": 14}, {"type": "text", "text": "Forward Process Starting with $\\scriptstyle x_{0}$ drawn from the distribution $q(x_{0})$ , the forward process incrementally corrupts $\\pmb{x}_{0}$ through a sequence of latent variables $\\pmb{x}_{1:S}=(\\pmb{x}_{1},\\pmb{x}_{2},\\dots,\\pmb{x}_{S})$ , where each $\\pmb{x}_{s}$ retains the same dimensionality as $\\pmb{x}_{0}$ . This transformation is modeled as a fixed Markov chain: ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\pmb{x}_{1:S}|\\pmb{x}_{0})=\\prod_{s=1}^{S}q(\\pmb{x}_{s}|\\pmb{x}_{s-1}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where each transition $q(\\pmb{x}_{s}|\\pmb{x}_{s-1})$ is defined by a Gaussian distribution: ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\pmb{x}_{s}|\\pmb{x}_{s-1})=\\mathcal{N}(\\pmb{x}_{s};\\sqrt{1-\\eta_{s}}\\pmb{x}_{s-1},\\eta_{s}\\pmb{I})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $\\eta_{s}$ is a small positive constant that follows a predefined schedule $(\\eta_{1},\\eta_{2},\\dots,\\eta_{S})$ , allowing the data to progressively approach an isotropic Gaussian distribution, $\\mathcal{N}(\\mathbf{0},\\boldsymbol{I})$ , as $s$ increases. The overall transition from $\\scriptstyle x_{0}$ to $\\pmb{x}_{s}$ can thus be expressed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathbf{\\boldsymbol{x}}_{s}|\\mathbf{\\boldsymbol{x}}_{0})=\\mathcal{N}(\\mathbf{\\boldsymbol{x}}_{s};\\sqrt{\\bar{\\zeta}_{s}}\\mathbf{\\boldsymbol{x}}_{0},(1-\\bar{\\zeta}_{s})I)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\zeta_{s}=1-\\eta_{s}$ and $\\textstyle\\overline{{\\zeta}}_{s}=\\prod_{i=1}^{s}\\zeta_{i}$ . ", "page_idx": 14}, {"type": "text", "text": "Reverse Process In the reverse process, the model aims to convert the latent variable $\\pmb{x}_{S}$ , which is assumed to follow the distribution $\\mathcal{N}(\\mathbf{0},\\boldsymbol{I})$ , back into the original data $\\pmb{x}_{0}$ . The joint probability distribution is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{\\boldsymbol{x}}_{0:S})=p(\\mathbf{\\boldsymbol{x}}_{S})\\prod_{s=1}^{S}p_{\\theta}(\\mathbf{\\boldsymbol{x}}_{s-1}|\\mathbf{\\boldsymbol{x}}_{s})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The conditional distributions involved are inferred using Bayes rule as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\pmb{x}_{s-1}|\\pmb{x}_{s},\\pmb{x}_{0})=\\frac{q(\\pmb{x}_{s}|\\pmb{x}_{s-1},\\pmb{x}_{0})q(\\pmb{x}_{s-1}|\\pmb{x}_{0})}{q(\\pmb{x}_{s}|\\pmb{x}_{0})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To optimize the generative model $p_{\\theta}(\\mathbf{\\boldsymbol{x}}_{0})$ for fitting the data distribution $q(x_{0})$ , we minimize a variational upper bound on the negative log-likelihood: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{v b}=\\mathbb{E}_{q({\\pmb{x}}_{0})}\\Big[D_{K L}\\big[q({\\pmb{x}}_{S}|{\\pmb{x}}_{0})\\big||p({\\pmb{x}}_{S})\\big]+\\sum_{s=1}^{S}\\mathbb{E}_{q({\\pmb{x}}_{s}|{\\pmb{x}}_{0})}\\big[D_{K L}\\big[q({\\pmb{x}}_{s-1}|{\\pmb{x}}_{s},{\\pmb{x}}_{0})\\big||p_{\\theta}({\\pmb{x}}_{s-1}|{\\pmb{x}}_{s})\\big]\\big]\\Big].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "However, continuous diffusion models are not applicable in discrete spaces, such as quantized token indices $\\mathbf{k}=\\left(k_{1},k_{2},\\ldots,k_{N}\\right)$ where each $k_{i}$ assumes one of $|{\\mathcal{C}}|$ discrete values. This limitation arises because Gaussian noise cannot corrupt discrete elements in a meaningful way. Thus, modeling in discrete spaces necessitates the development of discrete diffusion processes. ", "page_idx": 14}, {"type": "text", "text": "B Mathematical Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide a detailed mathematical proofs for Eq. (6), which can quickly calculate $q(\\mathbf{k}_{s}|\\mathbf{k}_{0})$ according to Eq. (2). ", "page_idx": 14}, {"type": "text", "text": "Concretely, we use mathematical induction to prove Eq. (6). At first, we have following conditional information: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{s},\\beta_{s}\\in[0,1],\\alpha_{s}=1-|\\mathcal{C}|\\beta_{s}-\\gamma_{s},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{\\alpha}}_{s}=\\prod_{i=1}^{s}\\alpha_{s},\\overline{{\\gamma}}_{s}=1-\\prod_{i=1}^{s}(1-\\gamma_{s}),\\overline{{\\beta}}_{s}=(1-\\overline{{\\alpha}}_{s}-\\overline{{\\gamma}}_{s})/|\\mathcal{C}|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we want to prove that $\\overline{{\\mathbf{M}}}_{s}c(\\mathbf{k}_{0})=\\overline{{\\alpha}}_{s}c(\\mathbf{k}_{0})+(\\overline{{\\gamma}}_{s}-\\overline{{\\beta}}_{s})c(|\\mathcal{C}|+1)+\\overline{{\\beta}}_{s}$ . Firstly, when $s=1$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathbf{M}}}_{1}c(\\mathbf{k}_{0})=\\left\\{\\begin{array}{l l}{\\overline{{\\alpha}}_{1}+\\overline{{\\beta}}_{1},}&{\\mathbf{k}=\\mathbf{k}_{0}}\\\\ {\\overline{{\\beta}}_{1},}&{\\mathbf{k}\\neq\\mathbf{k}_{0}\\;\\mathrm{and}\\;\\mathbf{k}\\neq|\\mathcal{C}|+1}\\\\ {\\overline{{\\gamma}}_{1},}&{\\mathbf{k}=|\\mathcal{C}|+1}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is clearly hold. Suppose the Eq. (6) holds at step $s$ , then for $s=s+1$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{M}}}_{s+1}\\mathbf{c}(\\mathbf{k}_{0})=\\mathbf{M}_{\\mathbf{k}+1}\\overline{{\\mathbf{M}}}_{t}\\mathbf{c}(\\mathbf{k}_{0}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we consider three conditions: (1) when $\\mathbf{k}=\\mathbf{k}_{0}$ in step $s+1$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{M}_{s+1}c(\\mathbf{k}_{0})_{(\\mathbf{k})}=\\overline{{\\beta}}_{s}\\beta_{s+1}(|\\mathcal{C}|-1)+\\big(\\alpha_{s+1}+\\beta_{s+1}\\big)(\\overline{{\\alpha}}_{s}+\\overline{{\\beta}}_{s})}}\\\\ &{=\\overline{{\\beta}}_{s}\\big(|\\mathcal{C}|\\beta_{s+1}+\\alpha_{s+1}\\big)+\\overline{{\\alpha}}_{s}\\big(\\alpha_{s+1}+\\beta_{s+1}\\big)}\\\\ &{=\\frac{1}{|\\mathcal{C}|}\\big(\\overline{{\\beta}}_{s}(1-\\gamma_{s+1})+\\overline{{\\alpha}}_{s}\\beta_{s+1}-\\overline{{\\beta}}_{s+1}\\big)*|\\mathcal{C}|+\\overline{{\\alpha}}_{s+1}+\\overline{{\\beta}}_{s+1}}\\\\ &{=\\frac{1}{|\\mathcal{C}|}\\big[(1-\\overline{{\\alpha}}_{s}-\\overline{{\\gamma}}_{s})(1-\\gamma_{s+1})+|\\mathcal{C}|\\overline{{\\alpha}}_{s}\\beta_{s+1}-\\big(1-\\overline{{\\alpha}}_{s+1}-\\overline{{\\gamma}}_{s+1}\\big)\\big]+\\overline{{\\alpha}}_{s+1}+\\overline{{\\beta}}_{s+1}}\\\\ &{=\\frac{1}{|\\mathcal{C}|}\\big[(1-\\overline{{\\gamma}}_{s+1})-\\overline{{\\alpha}}_{s}\\big(1-\\gamma_{s+1}-K\\beta_{s+1}\\big)-\\big(1-\\overline{{\\gamma}}_{s+1}\\big)+\\overline{{\\alpha}}_{s+1}\\big]+\\overline{{\\alpha}}_{s+1}+\\overline{{\\beta}}_{s+1}}\\\\ &{=\\overline{{\\alpha}}_{s+1}+\\overline{{\\beta}}_{s+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(2) when $\\mathbf{k}=|\\mathcal{C}|+1$ in step $s+1$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{M}_{s+1}c(\\mathbf{k}_{0})_{(\\mathbf{k})}=\\overline{{\\gamma}}_{s}+(1-\\overline{{\\gamma}}_{s})\\gamma_{s+1}=1-(1-\\overline{{\\gamma}}_{s+1})=\\overline{{\\gamma}}_{s+1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(3) when $\\mathbf{k}\\neq\\mathbf{k}_{0}$ and $\\mathbf{k}\\neq|\\mathcal{C}|+1$ in step $s+1$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbf{M}_{s+1}\\mathbf{c}(\\mathbf{k}_{0})_{(\\mathbf{k})}=\\overline{{\\beta}}_{s}(\\alpha_{s+1}+\\beta_{s+1})+\\overline{{\\beta}}_{s}\\beta_{s+1}(|\\mathcal{C}|-1)+\\overline{{\\alpha}}_{s}\\beta_{s+1}}}\\\\ &{\\hphantom{=\\overline{{\\beta}}_{s}(\\alpha_{s+1}+|\\mathcal{C}|\\beta_{s+1})+\\overline{{\\alpha}}_{s}\\beta_{s+1}}}\\\\ &{=\\frac{1-\\overline{{\\alpha}}_{s}-\\overline{{\\gamma}}_{s}}{|\\mathcal{C}|}\\ast(1-\\gamma_{s+1})+\\overline{{\\alpha}}_{s}\\beta_{s+1}}\\\\ &{=\\frac{1}{|\\mathcal{C}|}(1-\\overline{{\\gamma}}_{s+1})+\\overline{{\\alpha}}_{s}(\\beta_{s+1}-\\frac{1-\\gamma_{s+1}}{|\\mathcal{C}|})}\\\\ &{=\\overline{{\\beta}}_{s+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The proof of Eq. (6) is completed. Notably, according to Eq. (6), the computation cost of $q(\\mathbf{k}_{s}|\\mathbf{k}_{0})$ can be reduced from $O(|\\mathcal{C}|^{2}S)$ to $O(|\\mathcal{C}|)$ . ", "page_idx": 15}, {"type": "text", "text": "C Algorithms for Discrete Diffusion Process ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide complete training and inference algorithms for discrete diffusion process. ", "page_idx": 15}, {"type": "text", "text": "C.1 Training Procedure ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The discrete diffusion process aims to model quantized 3D pose tokens in a discrete space. This involves utilizing a 2D image $I$ and its corresponding 3D human pose $\\mathbf{P}$ as inputs. The image $I$ serves as a contextual condition, while $\\mathbf{P}$ is converted into discrete tokens for modeling. ", "page_idx": 15}, {"type": "text", "text": "Require: A transition matrix $\\mathbf{M}_{s}$ , the number of steps $S$ , parameters of pose denoiser $\\theta$ , training epoch $T$ , pose dataset $_{D}$ (including 2D image $I$ and 3D human pose $\\mathbf{P}$ ), and the well-learned pose encoder $f_{P E}(\\cdot)$ .   \n1: for $i=1$ to $T$ do   \n2: for $(I,{\\bf P})$ in $_{D}$ do   \n3: $\\mathbf{k}_{0}=\\mathrm{FSO}(f_{P E}(\\mathbf{P})),$ , $\\pmb{y}=$ ImageEncoder $(I)$ ;   \n4: sample $s$ from Uniform $\\{1,2,...,S-1,S\\}$ ;   \n5: calculate $q(\\mathbf{k}_{s}|\\mathbf{k}_{0})$ based on Eq. (6);   \n6: estimate $f_{\\theta}(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\pmb{y})$ ;   \n7: calculate loss according to Eq. (10);   \n8: update $\\theta$ ;   \n9: end for   \n10: end for   \n11: return $\\theta$ . ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 Inference Algorithm for the discrete diffusion process. ", "page_idx": 16}, {"type": "text", "text": "Require: The number of steps $S$ , input 2D image $I$ , the pose decoder $f_{P D}(\\cdot)$ , parameters of pose denoiser $\\theta$ , stationary distribution $p(\\mathbf{k}_{S})$ ;   \n1: $s=S$ , $_y=$ ImageEncoder $(I)$ ;   \n2: sample ${\\bf k}_{s}$ from $p(\\mathbf{k}_{S})$ ;   \n3: while $s>0$ do   \n4: ${\\bf k}_{s}\\gets$ sample from $p_{\\theta}(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\pmb{y})$   \n5: $s\\gets(s-1)$   \n6: end while   \n7: return $f_{P D}(\\mathbf{k}_{s})$ . ", "page_idx": 16}, {"type": "text", "text": "Firstly, the 3D human pose $\\mathbf{P}$ is encoded by $f_{P E}(\\cdot)$ and subsequently quantized using the FSQ technique, resulting in multiple discrete tokens. Concurrently, a pre-trained Image Encoder extracts contextual features from $I$ , producing a conditional feature sequence $\\textit{\\textbf{y}}$ . During the forward process, we sample $s$ from a uniform distribution $\\{1,2,...,S-1,S\\}$ and compute $q(\\mathbf{k}_{s}|\\mathbf{k}_{0})$ based on Eq. (6). In the reverse process, the pose denoiser $f_{\\theta}(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\pmb{y})$ is trained to estimate $q(\\mathbf{\\dot{k}}_{s-1}|\\mathbf{k}_{s},\\mathbf{k}_{0})$ . Finally, the overall loss is calculated according to Eq. (10), and the parameters of the pose denoiser $\\theta$ are updated accordingly. ", "page_idx": 16}, {"type": "text", "text": "The complete training algorithm for the discrete diffusion process is presented in Algorithm 1. ", "page_idx": 16}, {"type": "text", "text": "C.2 Inference Procedure ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the inference process, our objective is to recover the 3D human pose $\\hat{\\mathbf{P}}$ from an input 2D image and discrete tokens. ", "page_idx": 16}, {"type": "text", "text": "Initially, all pose tokens are either masked or initialized randomly, which is achieved by sampling from the stationary distribution $p(\\mathbf{k}_{S})$ . The 2D image $I$ is encoded using the pre-trained Image Encoder. Subsequently, we predict $\\bar{f}_{\\theta}(\\mathbf{k}_{s-1}|\\mathbf{k}_{s},\\pmb{y})$ step by step until the pose tokens are fully recovered. Finally, the reconstructed tokens are decoded using the pose decoder $f_{P E}(\\cdot)$ , yielding the recovered 3D pose P\u02c6. ", "page_idx": 16}, {"type": "text", "text": "The complete inference algorithm for the discrete diffusion process is presented in Algorithm 2. ", "page_idx": 16}, {"type": "text", "text": "D Additional Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All experiments are carried out on one NVIDIA A100 PCIe GPU. The proposed $\\mathrm{Di^{2}}$ Pose is completely implemented in PyTorch [53]. In this section, we provide the detailed training settings for the pose quantization step and the discrete diffusion process. ", "page_idx": 16}, {"type": "table", "img_path": "p2PO2PUPFY/tmp/7c6533f7ddf2da99f80655d0de6f6b519171f558a97b1f7df2151f02062905e2.jpg", "table_caption": ["Table 6: Results on Human3.6M in millimeters under PA-MPJPE. The best results are in bold, and the second-best ones are underlined. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "For the pose quantization step, we employ the AdamW [46] optimizer with $\\beta_{1}=0.9$ and $\\beta_{2}=0.999$ , adhering to a base learning rate of 1e-3 and a weight decay parameter of 0.15. The training process is configured with a batch size of 256 across a total of 20 epochs. ", "page_idx": 17}, {"type": "text", "text": "For the discrete diffusion process, we still utilize the the AdamW optimizer with $\\beta_{1}\\,=\\,0.9$ and $\\beta_{2}=0.96$ , adhering to a base learning rate of 5.5e-4 and a weight decay parameter of 4.5e-2. The training process is configured with a batch size of 64 across a total of 50 epochs. ", "page_idx": 17}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We exhibit more experimental results to verify the effectiveness of our $\\mathrm{Di^{2}P o s e}$ . ", "page_idx": 17}, {"type": "text", "text": "E.1 Quantitative Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As shown in Table 6, we benchmark $\\mathrm{Di^{2}}$ Pose against SOTA 3D HPE methods on the Human3.6M under PA-MPJPE protocol. Our $\\mathrm{Di^{2}}$ Pose achieves $39.0\\mathrm{mm}$ in average PA-MPJPE, surpassing the performance of the compared SOTA 3D HPE methods, which indicates that $\\mathrm{Di^{2}P o s e}$ is able to enhance monocular 3D HPE in indoor scenes. ", "page_idx": 17}, {"type": "text", "text": "E.2 Qualitative Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this part, we present additional qualitative results on the Human3.6M and 3DPW datasets. As illustrated in Figure 6, our $\\mathrm{Di^{2}}$ Pose model demonstrates the ability to accurately recover 3D human poses in both indoor and in-the-wild scenarios. Particularly noteworthy is its performance under various occlusion conditions, including self-occlusion and object occlusion. Even in these challenging situations, $\\mathrm{Di^{2}}$ Pose consistently produces reasonable 3D pose estimations, highlighting its robustness to occlusions. ", "page_idx": 17}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This research focuses on estimating physically valid 3D human poses from monocular frames, especially under occlusion scenes. Such a method can be positively used for sports analysis, surveillance, healthcare, autonomous driving, etc. where clear, unobstructed views of the subject may not always be available. It can also lead to malicious use cases, such as illegal surveillance and video synthesis. Thus, it is essential to deploy these algorithms with care and make sure that the extracted human poses are with consent and not misused. Moreover, the diffusion-based model has a longer runtime compared to other CNN or GCN-based methods, causing more computational resources and energy consumption. ", "page_idx": 17}, {"type": "image", "img_path": "p2PO2PUPFY/tmp/7e792c61e5a0a9832ae758d70435f662106b2fbfaf7d6859def37857e058f62b.jpg", "img_caption": ["Figure 6: Qualitative results on two datasets. Joints on the right side are marked in green, while other joints are highlighted in blue. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Refer to the Abstract and Sec.1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Refer to Sec.5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Refer to Sec.3 and Sec.B. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Refer to Sec.4.1, Sec.4.2 and Sec.D. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We will release code upon paper acceptance. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Refer to Sec.4.2 and Sec.D. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Refer to Sec.4. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Refer to Sec.D in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We make sure to conduct this paper conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Refer to Sec.F in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 22}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All used datasets and models in this paper are explicitly mentioned and properly cited. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve this issue. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We use publicly available 3D human pose datasets in this paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve this issue. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]