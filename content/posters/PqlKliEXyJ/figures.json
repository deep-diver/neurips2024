[{"figure_path": "PqlKliEXyJ/figures/figures_1_1.jpg", "caption": "Figure 1: In this paper, we propose LoD-Loc to tackle visual localization w.r.t a scene represented by a LoD 3D map, characterized by its ease of acquisition, lightweight nature, and built-in privacy-preserving capabilities. Given a query image and its coarse sensor pose, our method utilizes the wireframe alignment of LoD models to recover the camera pose.", "description": "The figure shows a schematic of the LoD-Loc method.  A drone takes a query image of a scene. The scene is represented by a low-detail (LoD) 3D map which is easier to acquire and maintain than high-detail maps.  LoD-Loc uses the wireframe (edges of buildings etc.) from the LoD map to estimate the position and orientation (pose) of the drone.  The sensor provides an initial, coarse estimate of the pose.  The LoD-Loc method refines this estimate by aligning the predicted wireframe from a neural network to the wireframe projected from the LoD map.", "section": "1 Introduction"}, {"figure_path": "PqlKliEXyJ/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of datasets. The left side shows the LoD models of the released data. The LoD2.0 model from Swiss-EPFL includes building height and roof information, while the LoD3.0 model from UAVD4L-LoD contains more detailed structural information such as building height, roof, and side pillars. The right side illustrates samples of query images, which consist of images captured by drones in various scenes.", "description": "This figure provides an overview of the two datasets used in the paper: UAVD4L-LoD and Swiss-EPFL.  The left side shows the Level of Detail (LoD) 3D models for each dataset, highlighting the difference in detail levels between LoD 2.0 and LoD 3.0.  The right side displays example query images from each dataset, showcasing the variety of aerial scenes captured by drones.", "section": "Datasets"}, {"figure_path": "PqlKliEXyJ/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of LoD-Loc. 1. LoD-Loc employs a CNN to extract multi-level features F for the query image I (Sec. 3.1). 2. A cost volume C\u012b is built for various pose hypotheses sampled around the coarse sensor pose \u00a7, to select the pose \u0123\u012b with the highest probability, based on the projected wireframe of the 3D LoD model (Sec. 3.2). 3. A differentiable Gauss-Newton method is used to refine the final selected pose \u00a73, to obtain a more accurate pose \u00a3* (Sec. 3.3).", "description": "This figure provides a visual overview of the LoD-Loc method, showing its three main stages.  First, a CNN extracts multi-level features from the query image. Second, a cost volume is created to identify the pose with the highest probability, using projected wireframes from the 3D LoD model.  Finally, a Gauss-Newton method refines this pose for higher accuracy.  The process is hierarchical, refining the pose estimate progressively.", "section": "3 Method"}, {"figure_path": "PqlKliEXyJ/figures/figures_9_1.jpg", "caption": "Figure 5: Visualization of feature maps from different levels. The feature maps of different levels reflect different fineness of wireframe extraction.", "description": "This figure visualizes the feature maps generated by the multi-scale feature extractor at different levels (Level 1, Level 2, Level 3) for both LoD 3.0 and LoD 2.0 datasets.  Each level's feature map shows a progressively finer representation of the wireframe structure extracted from the query image. The \"Refine\" column shows the final refined feature map after post-processing.  The varying degrees of fineness illustrate the effectiveness of the multi-scale approach in capturing wireframe details at different levels of granularity.", "section": "4.3 Ablation Studies"}, {"figure_path": "PqlKliEXyJ/figures/figures_15_1.jpg", "caption": "Figure 6: Flight trajectories of query images in the UAVD4L-LoD dataset. We present the flight trajectories of the registered in-Traj. and out-of-Traj. query images. The in-Traj. images follow a predetermined flight path, primarily covering the left half of the map. In contrast, the out-of-Traj. images navigate arbitrarily without a fixed route, randomly covering the entire map.", "description": "This figure shows the flight paths of the UAV during data collection for the UAVD4L-LoD dataset.  The left panel displays the planned flight path ('in-Traj'), which is a structured, zig-zag pattern covering roughly half the map area. The right panel shows the less structured, free-form flight path ('out-of-Traj') which covered a wider area of the map more randomly.", "section": "A.2 Query Image Collection"}, {"figure_path": "PqlKliEXyJ/figures/figures_16_1.jpg", "caption": "Figure 3: Overview of LoD-Loc. 1. LoD-Loc employs a CNN to extract multi-level features F for the query image I (Sec. 3.1). 2. A cost volume C\u012b is built for various pose hypotheses sampled around the coarse sensor pose \u00a7, to select the pose \u0123\u012b with the highest probability, based on the projected wireframe of the 3D LoD model (Sec. 3.2). 3. A differentiable Gauss-Newton method is used to refine the final selected pose \u00a73, to obtain a more accurate pose \u00a3* (Sec. 3.3).", "description": "This figure provides a high-level overview of the LoD-Loc method.  It shows the three main stages of the process: feature extraction using a convolutional neural network, pose selection from a cost volume based on the alignment of projected and predicted wireframes, and pose refinement using a differentiable Gauss-Newton method.  The figure illustrates the flow of data through each stage and highlights the key components involved in achieving accurate 6-DoF pose estimation.", "section": "3 Method"}, {"figure_path": "PqlKliEXyJ/figures/figures_17_1.jpg", "caption": "Figure 8: 3D wireframe projection over UAVD4L-LoD dataset. We visualize the projected wireframe on query images based on sensor and GT poses to demonstrate their accuracy.", "description": "This figure shows a comparison of the projected wireframes onto query images using both sensor-estimated poses (Priors) and ground-truth poses (GT).  It visually demonstrates the accuracy of the pose estimation by showing how well the projected wireframes align with the actual building edges in the images. The left side displays the in-Traj. (trajectory-based) results, while the right shows out-of-Traj. (free-flight) results, allowing for a comparison between different flight scenarios.", "section": "Details on Method"}, {"figure_path": "PqlKliEXyJ/figures/figures_18_1.jpg", "caption": "Figure 9: Samples of mislabeled and selected query images over Swiss-EPFL dataset. We eliminate mislabeled query images by manually identifying the alignment between the projected 2D wireframe and the corresponding RGB image.", "description": "This figure shows examples of query images from the Swiss-EPFL dataset that were either mislabeled (top row) or selected (bottom row) after manual inspection.  The mislabeled images show a poor alignment between the projected wireframe and the actual building structures in the RGB image. This indicates that the ground truth pose information associated with these images is inaccurate. In contrast, the selected images demonstrate good alignment, suggesting accurate ground truth pose data. The manual selection process helps to improve the quality and reliability of the dataset by removing images with erroneous pose labels.", "section": "A Details on Dataset Collection"}, {"figure_path": "PqlKliEXyJ/figures/figures_19_1.jpg", "caption": "Figure 10: Visualization of reference RGB and depth maps. RGB and depth maps are rendered using a textured mesh model or a 3D LoD map.", "description": "This figure shows a comparison of RGB and depth maps generated from two different types of 3D models: mesh-based and LoD-based.  The mesh-based models provide detailed textures and depth information, while the LoD-based models offer simplified wireframe representations with less detailed depth information. This visual comparison highlights the key difference in the level of detail between these two map types, which is a crucial aspect of the paper's approach to aerial visual localization.", "section": "E.2 Reference Image Details"}, {"figure_path": "PqlKliEXyJ/figures/figures_20_1.jpg", "caption": "Figure 2: Overview of datasets. The left side shows the LoD models of the released data. The LoD2.0 model from Swiss-EPFL includes building height and roof information, while the LoD3.0 model from UAVD4L-LoD contains more detailed structural information such as building height, roof, and side pillars. The right side illustrates samples of query images, which consist of images captured by drones in various scenes.", "description": "This figure provides a visual overview of the two datasets used in the LoD-Loc research. The left panel displays the Level of Detail (LoD) 3D models used for localization, showcasing the difference in detail between LoD2.0 (from Swiss-EPFL, showing building height and roof) and LoD3.0 (from UAVD4L-LoD, including additional structural elements like side pillars).  The right panel shows examples of query images captured by drones, offering a visual representation of the dataset's scene variety.", "section": "Datasets"}, {"figure_path": "PqlKliEXyJ/figures/figures_24_1.jpg", "caption": "Figure 2: Overview of datasets. The left side shows the LoD models of the released data. The LoD2.0 model from Swiss-EPFL includes building height and roof information, while the LoD3.0 model from UAVD4L-LoD contains more detailed structural information such as building height, roof, and side pillars. The right side illustrates samples of query images, which consist of images captured by drones in various scenes.", "description": "This figure provides a visual overview of the two datasets used in the paper: UAVD4L-LoD and Swiss-EPFL. The left side displays the LoD models (Level of Detail 3D city maps), showcasing the different levels of detail available in each dataset.  The LoD2.0 model (from Swiss-EPFL) shows basic building outlines and roof information, while the LoD3.0 model (from UAVD4L-LoD) includes more detailed information such as building height, roof structure, and side pillars. The right side shows example query images (captured by drones), illustrating the variety of scenes and perspectives included in both datasets.", "section": "Datasets"}, {"figure_path": "PqlKliEXyJ/figures/figures_25_1.jpg", "caption": "Figure 13: Failure retrieval cases of baselines. Even with narrowed searching scopes, the retrieval phase still suffers from issues such as repetitive textures and cross-modal challenges.", "description": "This figure shows examples where baseline methods (UAVD4L and CadLoc) failed to retrieve relevant images even with a narrowed search scope. The failures are attributed to challenges in handling repetitive textures and cross-modal inconsistencies between the query images and the reference images in the database.", "section": "E.1 Sensor-guided Image Retrieval"}, {"figure_path": "PqlKliEXyJ/figures/figures_26_1.jpg", "caption": "Figure 14: Failure matching cases of baselines. The differences in viewpoint and modality influence the results for image matching.", "description": "This figure shows examples where baselines (CadLoc and UAVD4L) fail to match images.  The failures highlight the challenges posed by variations in viewpoint and differences in data modality (e.g., textured meshes versus wireframes) when trying to establish accurate correspondences for localization.", "section": "E.1 Sensor-guided Image Retrieval"}, {"figure_path": "PqlKliEXyJ/figures/figures_27_1.jpg", "caption": "Figure 2: Overview of datasets. The left side shows the LoD models of the released data. The LoD2.0 model from Swiss-EPFL includes building height and roof information, while the LoD3.0 model from UAVD4L-LoD contains more detailed structural information such as building height, roof, and side pillars. The right side illustrates samples of query images, which consist of images captured by drones in various scenes.", "description": "This figure provides an overview of the two datasets used in the paper: UAVD4L-LoD and Swiss-EPFL. The left side displays the different levels of detail (LOD) in the 3D models used for each dataset, showing that the LoD3.0 model (UAVD4L-LoD) provides more detailed building information than the LoD2.0 model (Swiss-EPFL).  The right side shows example query images captured by drones in various scenes, illustrating the types of aerial imagery used for localization in the study.  This highlights the different levels of detail in the maps and the variety of scenarios represented in the datasets.", "section": "Datasets"}, {"figure_path": "PqlKliEXyJ/figures/figures_27_2.jpg", "caption": "Figure 16: Region of training and testing. We use boxes with different colors and symbols to delineate different regions.", "description": "This figure shows the regions used for training and testing in the UAVD4L-LoD and Swiss-EPFL datasets.  The UAVD4L-LoD dataset is divided into two regions (A1 and A2), represented by yellow and green boxes respectively. The Swiss-EPFL dataset is also divided into two regions (B1 and B2), represented by light blue and purple boxes respectively. The different colors and symbols help to visually distinguish the different regions used for training and evaluating the model's performance.", "section": "Experiment"}, {"figure_path": "PqlKliEXyJ/figures/figures_28_1.jpg", "caption": "Figure 5: Visualization of feature maps from different levels. The feature maps of different levels reflect different fineness of wireframe extraction.", "description": "This figure visualizes the feature maps generated by the multi-scale feature extractor at different levels (Level 1, Level 2, Level 3) of the LoD-Loc model. The query images are shown for comparison. The feature maps are single-channel images where pixel intensity represents the likelihood of a wireframe.  The figure illustrates how the network progressively refines its wireframe extraction, with finer details captured at deeper levels. This showcases the hierarchical nature of the feature extraction process in LoD-Loc, enhancing its accuracy and efficiency in wireframe detection.", "section": "4.3 Ablation Studies"}, {"figure_path": "PqlKliEXyJ/figures/figures_29_1.jpg", "caption": "Figure 18: Visualization of predictions at different levels. Based on the predicted poses at each stage, we can obtain 2D projected wireframe and overlay them on the query image to check the accuracy of the poses. It can be observed that as the levels progress, the projected wireframes gradually align with the edges of the buildings. Please zoom in to see the details of the alignment.", "description": "This figure shows the visualization of predictions at different levels of the LoD-Loc method.  It displays how the projected wireframes, generated using predicted poses at each level (Priors, Level 1, Level 2, Level 3, Refine), align more accurately with the edges of buildings in the query images as the process progresses. The results are shown separately for the UAVD4L-LoD and Swiss-EPFL datasets.", "section": "Visualization of Results"}]