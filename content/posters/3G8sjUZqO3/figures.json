[{"figure_path": "3G8sjUZqO3/figures/figures_8_1.jpg", "caption": "Figure 2: Convergence in terms of the NCut objective value for two UCI datasets.", "description": "This figure shows the convergence of the FPC algorithm in terms of the NCut objective value for two UCI datasets: \"Balloons\" and \"Shuttle\". The x-axis represents the iteration number, and the y-axis represents the NCut objective value.  The plots show that the FPC algorithm converges to the global optimum within a few iterations for both datasets.  This demonstrates the algorithm's efficiency and effectiveness in minimizing the NCut objective.", "section": "5 Experiments"}, {"figure_path": "3G8sjUZqO3/figures/figures_9_1.jpg", "caption": "Figure 3: Image segmentation by the different algorithms.", "description": "This figure shows the image segmentation results of different algorithms including the original image, SC, FINC, FCD and FPC on four different images. Each row represents a different image, and each column represents a different algorithm. It visually compares the performance of each algorithm in segmenting various images with complex backgrounds and shapes.", "section": "5 Experiments"}, {"figure_path": "3G8sjUZqO3/figures/figures_9_2.jpg", "caption": "Figure 4: Running time of the different algorithms when applied to the different datasets.", "description": "This figure shows the running time of four different graph clustering algorithms (SC, FINC, FCD, and FPC) across eight different datasets.  The datasets vary in size and complexity, allowing for a comparison of algorithm efficiency under different conditions.  The y-axis represents the time taken in seconds, and the x-axis displays the names of the datasets. The graph highlights the relative performance of each algorithm across these datasets.", "section": "5 Experiments"}, {"figure_path": "3G8sjUZqO3/figures/figures_12_1.jpg", "caption": "Figure 5: The original objective value f(x) is nondecreasing after each iteration of the MM method.", "description": "This figure illustrates the monotonic convergence property of the MM (Minorization-Maximization) method.  The MM method iteratively approximates a difficult optimization problem by maximizing a surrogate function. The surrogate function, g(x|x^(t-1)), is designed to be less complex than the original objective function, f(x), and always below or equal to it. Each iteration finds the maximum of the surrogate function given the previous iteration's solution. The figure shows that the original objective function value, f(x), is non-decreasing after each iteration of the MM method.", "section": "A.1 Proof of Proposition 4"}]