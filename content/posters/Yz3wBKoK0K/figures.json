[{"figure_path": "Yz3wBKoK0K/figures/figures_1_1.jpg", "caption": "Figure 1: Aggregate-and-adapt the textual knowledge in natural language prompts for downstream tasks. (a) For classification of object-centric images, we query GPT-3 to obtain a list of prompts for each class, e.g., the car model of \"Jeep Compass SUV 2012\". Note how redundant the reference prompts can be (e.g., the first two), and how they can be irrelevant to the image (e.g., the last prompt). Alternatively, for complex tasks like VQA, we use human-generated image captions to depict multi-object images. For all tasks, we first learn to aggregate the reference prompts into an image-aligned \"summary\" (prompt embedding) based on CLIP reward. Then a prompt generator is jointly trained to generate Aggregate-and-Adapted Prompt Embedding (AAPE), such that the distance between AAPE and the aggregated summary is minimized and the task loss is minimized too for adaptation purpose. (b) At test time, we only keep the prompt generator with the prompt aggregator discarded. Our AAPE is applicable to different vision-language tasks with strong generalization performance.", "description": "This figure illustrates the Aggregate-and-Adapt Prompt Embedding (AAPE) method.  The (a) part shows the prompt learning stage where a prompt aggregator summarizes multiple natural language prompts (either human-generated or from GPT-3) into a single image-aligned prompt embedding.  A prompt generator then learns to create an AAPE embedding that's close to the summary while minimizing the task loss. (b) shows the inference stage, where only the prompt generator is used, making the process efficient at test time. AAPE is shown to work well for various downstream vision-language tasks.", "section": "3 Method"}, {"figure_path": "Yz3wBKoK0K/figures/figures_3_1.jpg", "caption": "Figure 1: Aggregate-and-adapt the textual knowledge in natural language prompts for downstream tasks. (a) For classification of object-centric images, we query GPT-3 to obtain a list of prompts for each class, e.g., the car model of \"Jeep Compass SUV 2012\". Note how redundant the reference prompts can be (e.g., the first two), and how they can be irrelevant to the image (e.g., the last prompt). Alternatively, for complex tasks like VQA, we use human-generated image captions to depict multi-object images. For all tasks, we first learn to aggregate the reference prompts into an image-aligned \"summary\" (prompt embedding) based on CLIP reward. Then a prompt generator is jointly trained to generate Aggregate-and-Adapted Prompt Embedding (AAPE), such that the distance between AAPE and the aggregated summary is minimized and the task loss is minimized too for adaptation purpose. (b) At test time, we only keep the prompt generator with the prompt aggregator discarded. Our AAPE is applicable to different vision-language tasks with strong generalization performance.", "description": "This figure illustrates the Aggregate-and-Adapt Prompt Embedding (AAPE) method.  Panel (a) shows the prompt learning stage, where natural language prompts (either from GPT-3 or human-generated) are aggregated into an image-aligned summary using a learned prompt aggregator. A prompt generator then creates an AAPE embedding that balances closeness to the summary with minimization of task loss.  Panel (b) shows the inference stage, where only the trained prompt generator is used, removing the need for LLMs at test time.  AAPE is applied to various downstream vision-language tasks.", "section": "3 Method"}, {"figure_path": "Yz3wBKoK0K/figures/figures_4_1.jpg", "caption": "Figure 3: (a) Input-adapted prompt aggregator which aggregates the embeddings of reference prompts P into an image-aligned, condensed prompt embedding p based on CLIP reward. (b) Instantiation of our prompt learning approach for image classification. The CLIP model is kept frozen.", "description": "Figure 3 shows the architecture of the input-adapted prompt aggregator and the overall prompt learning framework for image classification.  The aggregator uses an attention mechanism to combine reference prompts into a single, image-aligned embedding (p ). This is then used to supervise a prompt generator (h) that creates the final Aggregate-and-Adapted Prompt Embedding (AAPE). The CLIP model itself remains frozen during this learning process.  The classification process involves combining the AAPE with a class template, and then projecting to obtain final classification weights.", "section": "3 Method"}, {"figure_path": "Yz3wBKoK0K/figures/figures_8_1.jpg", "caption": "Figure 4: Quantifying the role of LLM knowledge (distilled with Ldistill) in prompt learning. Ldistill consistently improves the base and new class accuracies on 11 classification datasets.", "description": "This figure shows two bar charts visualizing the absolute gain in accuracy achieved by using the distillation loss (Ldistill) in the AAPE (Aggregate-and-Adapt Prompt Embedding) method compared to not using it. The left chart displays the results for base classes, while the right chart shows the results for new classes across 11 different datasets used in the few-shot image classification task. The results consistently demonstrate that incorporating LLM knowledge through distillation significantly boosts performance for both base and new classes in various datasets.", "section": "5 Results"}, {"figure_path": "Yz3wBKoK0K/figures/figures_8_2.jpg", "caption": "Figure 5: AAPE helps disambiguate the classification task. To highlight the textual knowledge encoded in AAPE, we show some reference prompts generated by GPT-3. For both the prompt template and AAPE (before concatenation and projection), we measure their Cosine similarity score with the image. Note the similarity score can be small when using a basic prompt template to match the \"altar\" class instance on ImageNet. Indeed, in this non-canonical image view, the altar is small and the whole scene can be classified as the easily confused class of \"church\". Whereas AAPE is able to eliminate confusion by providing additional cues like altar \u201cis a raised table\" often at the location of \"church\". This results in increased image-text similarity. Similarly, the textual cues from AAPE are helpful for the OOD examples in special domains of DTD and EuroSAT.", "description": "This figure demonstrates how AAPE leverages textual knowledge from LLM-generated prompts to improve classification accuracy, especially in cases with non-canonical views or out-of-distribution (OOD) examples.  It highlights how AAPE's additional textual cues help disambiguate similar classes and improve image-text similarity scores.", "section": "5 Results"}, {"figure_path": "Yz3wBKoK0K/figures/figures_9_1.jpg", "caption": "Figure 6: AAPE scales better with data (a-b) and LLM size (c) than alternatives. Experiments are conducted under the base-to-new generalization setting for few-shot classification. We measure the Harmonic mean (H) of base and new class accuracies. To adjust the total number of reference prompts per class to supervise AAPE learning, we vary the number of prompts generated by each LLM-prompt template. Four models of GPT-3 are considered: Ada, Babbage, Curie and Davinci.", "description": "This figure shows the scaling performance of AAPE compared to other methods in few-shot image classification.  It demonstrates that AAPE performs better with more data and larger language models. The experiments use the base-to-new class generalization setting, and the harmonic mean of base and new class accuracies is used as the performance metric.  The number of shots per class, the number of prompts per LLM prompt and the size of the LLM are varied to show the effect of these factors on the performance.", "section": "5 Results"}, {"figure_path": "Yz3wBKoK0K/figures/figures_15_1.jpg", "caption": "Figure 1: Aggregate-and-adapt the textual knowledge in natural language prompts for downstream tasks. (a) For classification of object-centric images, we query GPT-3 to obtain a list of prompts for each class, e.g., the car model of \"Jeep Compass SUV 2012\". Note how redundant the reference prompts can be (e.g., the first two), and how they can be irrelevant to the image (e.g., the last prompt). Alternatively, for complex tasks like VQA, we use human-generated image captions to depict multi-object images. For all tasks, we first learn to aggregate the reference prompts into an image-aligned \"summary\" (prompt embedding) based on CLIP reward. Then a prompt generator is jointly trained to generate Aggregate-and-Adapted Prompt Embedding (AAPE), such that the distance between AAPE and the aggregated summary is minimized and the task loss is minimized too for adaptation purpose. (b) At test time, we only keep the prompt generator with the prompt aggregator discarded. Our AAPE is applicable to different vision-language tasks with strong generalization performance.", "description": "This figure illustrates the proposed Aggregate-and-Adapt Prompt Embedding (AAPE) method.  Panel (a) shows the prompt learning stage, where natural language prompts (either from GPT-3 or human-generated) are aggregated into a concise summary embedding for each image using a learned prompt aggregator. A prompt generator is then trained to produce an AAPE embedding that is close to the summary while minimizing the task loss. Panel (b) depicts the inference stage where only the trained prompt generator is used to generate AAPE embeddings for downstream tasks, eliminating the need for LLM inference at test time. The method adapts to various vision-language tasks.", "section": "3 Method"}, {"figure_path": "Yz3wBKoK0K/figures/figures_18_1.jpg", "caption": "Figure 4: Quantifying the role of LLM knowledge (distilled with Ldistill) in prompt learning. Ldistill consistently improves the base and new class accuracies on 11 classification datasets.", "description": "This figure shows two bar charts visualizing the absolute gain in accuracy achieved by using the distillation loss (Ldistill) compared to not using it, for both base classes and new classes across 11 image classification datasets.  The results demonstrate a consistent improvement in accuracy when Ldistill is included in the training process, highlighting the importance of distilling knowledge from large language models (LLMs) for improved prompt learning and downstream generalization.", "section": "5 Results"}, {"figure_path": "Yz3wBKoK0K/figures/figures_20_1.jpg", "caption": "Figure 9: Captioning results on NoCaps dataset: LiMBeR vs. LiMBeR+AAPE (zero-shot).", "description": "This figure shows a comparison of image captioning results on the NoCaps dataset between the LiMBeR model and the LiMBeR model enhanced with AAPE (Aggregate-and-Adapted Prompt Embedding).  For each of six example images, the ground truth caption is provided, along with captions generated by LiMBeR and LiMBeR+AAPE.  The examples highlight how adding AAPE to LiMBeR improves the quality and accuracy of the generated captions, especially in cases where the visual information is ambiguous or complex.  The improved results are especially noticeable in images where visual cues are difficult to interpret; this demonstrates AAPE's capability in improving caption quality when faced with difficult or ambiguous images. ", "section": "5.3 Image Captioning & VQA"}, {"figure_path": "Yz3wBKoK0K/figures/figures_20_2.jpg", "caption": "Figure 2: LLM-generated image prompts for ImageNet categories, and the hand-constructed image captions on COCO and Flickr30k datasets. Note ImageNet mainly contains object-centric images with relatively clean background, and the LLM-generated image prompts can describe distinct characteristics of the given classes. While COCO and Flickr30k contain multi-object images with cluttered background, and the hand-constructed captions can represent varying object relations.", "description": "This figure shows examples of LLM-generated image prompts for ImageNet, compared to human-written captions for COCO and Flickr30k.  It highlights the difference in image complexity and prompt style. ImageNet prompts focus on object-centric descriptions, while COCO and Flickr30k captions describe multi-object scenes with more complex relationships.", "section": "3.1 Generating Natural Language Prompts"}, {"figure_path": "Yz3wBKoK0K/figures/figures_20_3.jpg", "caption": "Figure 2: LLM-generated image prompts for ImageNet categories, and the hand-constructed image captions on COCO and Flickr30k datasets. Note ImageNet mainly contains object-centric images with relatively clean background, and the LLM-generated image prompts can describe distinct characteristics of the given classes. While COCO and Flickr30k contain multi-object images with cluttered background, and the hand-constructed captions can represent varying object relations.", "description": "This figure shows examples of LLM-generated image prompts for ImageNet, and hand-constructed image captions for COCO and Flickr30k datasets.  It highlights the difference in image characteristics between the datasets. ImageNet images are mainly object-centric with clean backgrounds, allowing for concise, descriptive prompts focusing on object features.  In contrast, COCO and Flickr30k images feature multiple objects and cluttered backgrounds, necessitating more complex captions that capture the relationships between objects.", "section": "3.1 Generating Natural Language Prompts"}, {"figure_path": "Yz3wBKoK0K/figures/figures_20_4.jpg", "caption": "Figure 1: Aggregate-and-adapt the textual knowledge in natural language prompts for downstream tasks. (a) For classification of object-centric images, we query GPT-3 to obtain a list of prompts for each class, e.g., the car model of \"Jeep Compass SUV 2012\". Note how redundant the reference prompts can be (e.g., the first two), and how they can be irrelevant to the image (e.g., the last prompt). Alternatively, for complex tasks like VQA, we use human-generated image captions to depict multi-object images. For all tasks, we first learn to aggregate the reference prompts into an image-aligned \"summary\" (prompt embedding) based on CLIP reward. Then a prompt generator is jointly trained to generate Aggregate-and-Adapted Prompt Embedding (AAPE), such that the distance between AAPE and the aggregated summary is minimized and the task loss is minimized too for adaptation purpose. (b) At test time, we only keep the prompt generator with the prompt aggregator discarded. Our AAPE is applicable to different vision-language tasks with strong generalization performance.", "description": "This figure illustrates the Aggregate-and-Adapt Prompt Embedding (AAPE) method.  Panel (a) shows the prompt learning stage, where GPT-3 or human-generated captions provide multiple prompts per image. These prompts are aggregated into a single, image-aligned summary embedding using CLIP reward. A prompt generator then creates the AAPE, aiming to minimize both the distance to the aggregated summary and the task loss. Panel (b) shows the inference stage, where only the prompt generator is used, making the process efficient by eliminating LLM inference costs at test time. AAPE is applied to various vision-language tasks, demonstrating strong generalization.", "section": "Method"}, {"figure_path": "Yz3wBKoK0K/figures/figures_20_5.jpg", "caption": "Figure 2: LLM-generated image prompts for ImageNet categories, and the hand-constructed image captions on COCO and Flickr30k datasets. Note ImageNet mainly contains object-centric images with relatively clean background, and the LLM-generated image prompts can describe distinct characteristics of the given classes. While COCO and Flickr30k contain multi-object images with cluttered background, and the hand-constructed captions can represent varying object relations.", "description": "This figure shows example prompts generated by LLMs (Large Language Models) for ImageNet, and hand-constructed captions for COCO and Flickr30k datasets.  It highlights the differences in image characteristics and caption styles across these datasets. ImageNet images are primarily object-centric with clean backgrounds, allowing for concise, object-focused prompts. In contrast, COCO and Flickr30k contain multi-object images with cluttered backgrounds, necessitating more detailed and context-rich captions that describe relationships between objects.", "section": "3.1 Generating Natural Language Prompts"}, {"figure_path": "Yz3wBKoK0K/figures/figures_20_6.jpg", "caption": "Figure 2: LLM-generated image prompts for ImageNet categories, and the hand-constructed image captions on COCO and Flickr30k datasets. Note ImageNet mainly contains object-centric images with relatively clean background, and the LLM-generated image prompts can describe distinct characteristics of the given classes. While COCO and Flickr30k contain multi-object images with cluttered background, and the hand-constructed captions can represent varying object relations.", "description": "This figure shows example prompts generated by LLMs for ImageNet, and hand-constructed captions for COCO and Flickr30k datasets.  It highlights the difference in image characteristics and caption styles. ImageNet images are mostly object-centric with clean backgrounds, and LLMs generate prompts focusing on single object characteristics. In contrast, COCO and Flickr30k images are more complex, containing multiple objects and cluttered backgrounds, with captions describing object relationships.", "section": "3.1 Generating Natural Language Prompts"}, {"figure_path": "Yz3wBKoK0K/figures/figures_20_7.jpg", "caption": "Figure 1: Aggregate-and-adapt the textual knowledge in natural language prompts for downstream tasks. (a) For classification of object-centric images, we query GPT-3 to obtain a list of prompts for each class, e.g., the car model of \"Jeep Compass SUV 2012\". Note how redundant the reference prompts can be (e.g., the first two), and how they can be irrelevant to the image (e.g., the last prompt). Alternatively, for complex tasks like VQA, we use human-generated image captions to depict multi-object images. For all tasks, we first learn to aggregate the reference prompts into an image-aligned \"summary\" (prompt embedding) based on CLIP reward. Then a prompt generator is jointly trained to generate Aggregate-and-Adapted Prompt Embedding (AAPE), such that the distance between AAPE and the aggregated summary is minimized and the task loss is minimized too for adaptation purpose. (b) At test time, we only keep the prompt generator with the prompt aggregator discarded. Our AAPE is applicable to different vision-language tasks with strong generalization performance.", "description": "This figure illustrates the Aggregate-and-Adapt method for prompt learning.  Panel (a) shows the prompt learning stage, where a prompt aggregator summarizes natural language prompts (either from GPT-3 or human-generated) into an image-aligned summary (prompt embedding). A prompt generator then creates an Aggregate-and-Adapted Prompt Embedding (AAPE) that balances closeness to the summary with minimization of task loss. Panel (b) details the inference stage, where only the prompt generator is used, making it efficient at test time.  The AAPE is applied across various vision-language tasks.", "section": "3 Method"}]