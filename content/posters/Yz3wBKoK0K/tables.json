[{"figure_path": "Yz3wBKoK0K/tables/tables_6_1.jpg", "caption": "Table 1: Image-to-Text Retrieval: zero-shot and finetuned results (Recall@K) on Flickr30k dataset. Note our AAPE is learned on the COCO dataset.", "description": "This table presents the results of image-to-text retrieval experiments using the Aggregate-and-Adapt Prompt Embedding (AAPE) method.  The model was first trained on the COCO dataset and then evaluated in zero-shot and finetuned settings on the Flickr30k dataset.  The results are reported in terms of Recall@K, a metric that measures the percentage of times the correct text caption is retrieved among the top K results. The table compares the performance of AAPE against several state-of-the-art methods in the zero-shot and finetuned settings.", "section": "5.1 Image-to-Text Retrieval"}, {"figure_path": "Yz3wBKoK0K/tables/tables_7_1.jpg", "caption": "Table 2: Few-shot classification in the base-to-new class generalization setting. OGEN denotes the OGEN+PromptSRC variant. Our AAPE follows CuPL to query an LLM to obtain natural language prompts, but further learns from those prompts. H: Harmonic mean of base and new class accuracies.", "description": "This table compares the performance of AAPE with other prompt learning methods in few-shot image classification.  It specifically focuses on the base-to-new class generalization setting, where the model is trained on a subset of classes and tested on both seen (base) and unseen (new) classes.  The table shows the average accuracy across 11 datasets, highlighting the harmonic mean of base and new class accuracies (H) as a performance metric. The comparison includes methods that utilize LLMs and those that don't, showcasing AAPE's ability to leverage LLM-generated prompts effectively.", "section": "5.2 Few-shot Image Classification"}, {"figure_path": "Yz3wBKoK0K/tables/tables_9_1.jpg", "caption": "Table 3: Image captioning and VQA performance. Note our AAPE is learned on COCO dataset, and we show both its zero-shot and finetuned results on the two tasks with different testing datasets.", "description": "This table presents the results of image captioning and visual question answering (VQA) tasks.  The AAPE model, trained on the COCO dataset, is evaluated on both tasks using zero-shot and fine-tuned approaches.  It shows performance metrics such as CIDEr-D, CLIP Score, and Ref-CLIP Score for image captioning and accuracy for VQA with varying numbers of training shots. The results highlight AAPE's ability to generalize across different vision-language tasks and data distributions.", "section": "5 Results"}, {"figure_path": "Yz3wBKoK0K/tables/tables_14_1.jpg", "caption": "Table 4: Ablation study on our prompt aggregator. We experiment under the base-to-new class generalization setting for few-shot classification. CLIP performance (zero-shot) is listed as a baseline. H: Harmonic mean of base and new class accuracies.", "description": "This table presents an ablation study on the prompt aggregator used in the Aggregate-and-Adapt Prompt Embedding (AAPE) method.  It compares the performance of four different prompt aggregation methods: random sampling, simple averaging, MLP-based aggregation, and attention-based aggregation (the default method).  The results are reported as the harmonic mean (H) of base and new class accuracies across 11 datasets, with zero-shot CLIP results serving as a baseline for comparison. The table shows that the attention-based aggregation method significantly outperforms other methods.", "section": "More Ablations and Analyses"}, {"figure_path": "Yz3wBKoK0K/tables/tables_15_1.jpg", "caption": "Table 2: Few-shot classification in the base-to-new class generalization setting. OGEN denotes the OGEN+PromptSRC variant. Our AAPE follows CuPL to query an LLM to obtain natural language prompts, but further learns from those prompts. H: Harmonic mean of base and new class accuracies.", "description": "This table compares the performance of AAPE with other prompt learning methods on 11 datasets in a few-shot image classification setting.  It contrasts methods that learn prompt vectors without using textual knowledge (CoOp, CoCoOp, MaPLe, CLIPood, PromptSRC, OGEN) against those that align learned prompts with hand-written ones (ProGrad, KgCoOp, LASP-V, CuPL) and AAPE, which distills textual knowledge from LLMs.  The harmonic mean (H) of base and new class accuracies is reported as a key performance metric, showing AAPE's superior generalization performance.  The table highlights that AAPE effectively learns from natural language prompts obtained from LLMs, outperforming other methods, especially in generalizing to new classes.", "section": "5.2 Few-shot Image Classification"}, {"figure_path": "Yz3wBKoK0K/tables/tables_16_1.jpg", "caption": "Table 6: Sensitivity analysis of the distillation loss weight \u03bb. We report the few-shot classification results (base-to-new class generalization setting) averaged across 11 datasets in terms of H, the Harmonic mean of base and new class accuracies.", "description": "This table shows the results of a sensitivity analysis performed to determine the optimal value for the hyperparameter \u03bb (lambda), which controls the weight of the distillation loss in the model's training process.  The analysis was conducted across 11 different datasets using a base-to-new class generalization setting for few-shot classification. The table presents the harmonic mean (H) of base and new class accuracies for different values of \u03bb.  The results show the robustness of the model's performance across a range of \u03bb values.", "section": "5.2 Few-shot Image Classification"}, {"figure_path": "Yz3wBKoK0K/tables/tables_16_2.jpg", "caption": "Table 7: Inference cost for few-shot classification and 3 other tasks beyond classification. For few-shot classification, we report accuracy averaged over 11 datasets (base-to-new setting). We implement CoCoOp\u2020 to have a bigger prompt prediction network than CoCoOp, such that CoCoOp\u2020 has matching parameter count with AAPE.", "description": "This table compares the inference cost and few-shot accuracy of different prompt learning methods for image classification and other vision-language tasks.  It shows the number of parameters, GFLOPS, frames per second (FPS), and accuracy (base, new, and harmonic mean) for each method.  The table highlights the efficiency of AAPE compared to other methods, especially those that use large language models.", "section": "4.2 Vision-Language Understanding and Generation Tasks"}, {"figure_path": "Yz3wBKoK0K/tables/tables_17_1.jpg", "caption": "Table 8: Few-shot classification in the domain generalization setting. Note our AAPE follows CuPL to query an LLM to obtain natural language prompts, but further learns from those prompts.", "description": "This table presents the results of few-shot image classification experiments conducted in a domain generalization setting.  The \"source\" dataset is ImageNet, and the target datasets are ImageNetV2, ImageNet-Sketch, ImageNet-A, and ImageNet-R, which represent different types of domain shifts from the source. The table compares the performance of AAPE against various baseline methods including CLIP, MaPLe, CoCoOp, PromptSRC, CoOp, CLIPood, RPO, UPT, TaskRes, LASP, KgCoOp, ProGrad, and CuPL.  The results are reported as accuracy percentages for each target dataset.  AAPE utilizes natural language prompts obtained using an LLM, which are aggregated and adapted during prompt learning.", "section": "4.1 Few-shot Image Classification"}]