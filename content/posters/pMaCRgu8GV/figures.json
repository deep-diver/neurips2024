[{"figure_path": "pMaCRgu8GV/figures/figures_1_1.jpg", "caption": "Figure 1: Left: A flow chart describing our RL model of cultural accumulation. Right: An annotated, illustrative plot demonstrating in-context accumulation as observed in our results.", "description": "The figure consists of two parts. The left panel is a flowchart illustrating the process of cultural accumulation in reinforcement learning. It shows how a new generation of agents is initialized, trained using RL or in-context learning while observing the previous generation (social learning), and how their final parameters are set as the basis for the next generation.  The right panel is a plot showing the performance of agents over multiple generations. Each line represents a generation; the later generations achieve higher returns (performance) than the earlier ones, showcasing the improvement through cultural accumulation.  This demonstrates that the agents learn and improve cumulatively across generations, exceeding the performance of single-lifetime learning agents.", "section": "3 Problem Statement"}, {"figure_path": "pMaCRgu8GV/figures/figures_3_1.jpg", "caption": "Figure 2: Left: A visualization of the Goal Sequence Environment. Right: Routes travelled get shorter across generations in the TSP environment. Visualisation implementation is based on Jumanji [Bonnet et al., 2024].", "description": "The figure demonstrates the Goal Sequence environment (left) and the improvement in the solutions of the Traveling Salesperson Problem (TSP) across generations (right).  The Goal Sequence environment shows a grid with goals of different colors, where agents need to learn the correct sequence to visit the goals.  The TSP visualization shows how the route length decreases over generations, illustrating the accumulation of cultural knowledge improving the efficiency of solutions across generations.", "section": "3.1 Environments"}, {"figure_path": "pMaCRgu8GV/figures/figures_4_1.jpg", "caption": "Figure 1: Left: A flow chart describing our RL model of cultural accumulation. Right: An annotated, illustrative plot demonstrating in-context accumulation as observed in our results.", "description": "The figure on the left is a flowchart that illustrates the process of cultural accumulation in reinforcement learning.  It shows how a new generation of agents is initialized, observes the previous generation, trains using either reinforcement learning or in-context learning, and ultimately improves upon the previous generation. The observation of the previous generation implicitly enables social learning. The process iterates, leading to improvements over multiple generations.  The right panel shows a plot visualizing the performance of agents across multiple generations demonstrating that the performance improves over time (generations) showcasing in-context cultural accumulation.", "section": "3 Problem Statement"}, {"figure_path": "pMaCRgu8GV/figures/figures_6_1.jpg", "caption": "Figure 3: Left: In-context accumulation during evaluation on Memory Sequence. Right: Evaluation results following training with different oracle accuracies.", "description": "This figure presents the results of in-context accumulation experiments conducted on the Memory Sequence task. The left panel shows the performance of agents trained with in-context accumulation compared to single-lifetime baselines (RL2 trained for 4 and 32 trials). The results indicate that agents with in-context accumulation outperform baselines, achieving higher returns across trials and generations. The right panel shows the results of experiments with varying oracle accuracies (0.33, 0.67, and 0.86). This panel illustrates the impact of oracle reliability on the performance of in-context learning agents. Agents trained with less noisy oracles tend to perform better.", "section": "5.1 In-Context Results"}, {"figure_path": "pMaCRgu8GV/figures/figures_7_1.jpg", "caption": "Figure 4: Left: In-context accumulation during evaluation on Goal Sequence. Right: In-context accumulation during evaluation on TSP.", "description": "This figure visualizes the results of in-context accumulation experiments on two different tasks: Goal Sequence and Traveling Salesperson Problem (TSP). The left panel shows the performance of agents across seven generations (0 to 6) on the Goal Sequence task, compared to single-lifetime baselines (RL2 with 4 and 28 trials). A noisy oracle is also included as a reference.  The right panel displays similar results for the TSP task, showing performance across eight generations (0 to 7). Both panels illustrate how in-context learning leads to sustained performance gains across generations.", "section": "5.1 In-Context Results"}, {"figure_path": "pMaCRgu8GV/figures/figures_7_2.jpg", "caption": "Figure 5: Left: In-weights accumulation on Memory Sequence. Right: In-weights accumulation compounds with resetting. Error bars represent 95% confidence intervals.", "description": "This figure presents the results of in-weights accumulation experiments on the Memory Sequence task.  The left panel shows the cumulative return over training steps for four generations (0-4) compared to a single-lifetime agent.  It demonstrates that the in-weights accumulation method leads to a significant improvement over the single-lifetime baseline in terms of cumulative returns, showcasing the advantages of cultural accumulation where learning is spread across multiple generations. The right panel illustrates the performance increase when the in-weights accumulation method is combined with layer resetting during training. This combination provides an additional boost to performance, indicating that resetting network layers can enhance the benefits of cultural accumulation. Error bars in both plots provide confidence intervals around the mean, indicating the reliability of results.", "section": "5.2 In-Weights Results"}, {"figure_path": "pMaCRgu8GV/figures/figures_8_1.jpg", "caption": "Figure 5: Left: In-weights accumulation on Memory Sequence. Right: In-weights accumulation compounds with resetting. Error bars represent 95% confidence intervals.", "description": "This figure presents the results of in-weights accumulation experiments on the Memory Sequence task.  The left panel shows the learning curves for multiple generations of agents trained using the in-weights method. It demonstrates that performance improves over generations, exceeding the performance of a single-lifetime agent trained for an equivalent total number of steps.  The right panel further explores the benefit of combining in-weights accumulation with resetting of the agent's network parameters. This panel shows that resetting enhances the accumulation effect, leading to even greater performance improvements compared to single-lifetime training and in-weights accumulation alone. Error bars indicate the 95% confidence intervals.", "section": "5.2 In-Weights Results"}, {"figure_path": "pMaCRgu8GV/figures/figures_14_1.jpg", "caption": "Figure 7: In-weights accumulation on TSP.", "description": "This figure shows the results of in-weights accumulation on the Traveling Salesperson Problem (TSP).  It compares the performance of agents trained across multiple generations (generations 0 and 4 shown) against a single-lifetime baseline.  The x-axis represents the training step or update, and the y-axis represents the accumulated return.  The plot demonstrates that after two generations, the performance of the in-weights accumulation model surpasses that of a single-lifetime trained agent, indicating the effectiveness of the cultural accumulation approach. However, improvements beyond two generations are less pronounced. ", "section": "In-Weights Accumulation in TSP"}]