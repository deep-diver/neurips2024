[{"type": "text", "text": "Transformation-Invariant Learning and Theoretical Guarantees for OOD Generalization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Omar Montasser Han Shao Emmanuel Abbe Yale University Harvard University EPFL and Apple omar.montasser@yale.edu han@ttic.edu emmanuel.abbe@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning with identical train and test distributions has been extensively investigated both practically and theoretically. Much remains to be understood, however, in statistical learning under distribution shifts. This paper focuses on a distribution shift setting where train and test distributions can be related by classes of (data) transformation maps. We initiate a theoretical study for this framework, investigating learning scenarios where the target class of transformations is either known or unknown. We establish learning rules and algorithmic reductions to Empirical Risk Minimization (ERM), accompanied with learning guarantees. We obtain upper bounds on the sample complexity in terms of the VC dimension of the class composing predictors with transformations, which we show in many cases is not much larger than the VC dimension of the class of predictors. We highlight that the learning rules we derive offer a game-theoretic viewpoint on distribution shift: a learner searching for predictors and an adversary searching for transformation maps to respectively minimize and maximize the worst-case loss. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "It is desirable to train machine learning predictors that are robust to distribution shifts. In particular when data distributions vary based on the environment, or when part of the domain is not sampled at training such as in reasoning tasks. How can we train predictors that generalize beyond the distribution from which the training examples are drawn from? A common challenge that arises when tackling out-of-distribution generalization is capturing the structure of distribution shifts. A common approach is to mathematically describe such shifts through distance or divergence measures, as in prior work on domain adaptation theory [e.g., Redko et al., 2020] and distributionally robust optimization [e.g., Duchi and Namkoong, 2021]. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we put forward a new formulation for out-of-distribution generalization. Our formulation offers a conceptually different perspective from prior work, where we describe the structure of distribution shifts through data transformations. We consider an unknown distribution $\\mathcal{D}$ over $\\mathcal X\\times\\mathcal X$ which can be thought of as the \u201ctraining\u201d or \u201csource\u201d distribution from which training examples are drawn, and a collection of data transformation maps $\\mathcal{T}=\\{T:\\mathcal{X}\\to\\mathcal{X}\\}$ which can be thought of as encoding \u201ctarget\u201d distribution shifts, hence denoted as $\\{T(\\mathcal{D})\\}_{T\\in\\mathcal{T}}$ . We consider a covariate shift setting where labels are not altered or changed under transformations $T\\in\\mathcal T$ , and we write $T(\\mathcal{D})$ for notational convenience. Our goal, which will be formalized further shortly, is to learn a single predictor $\\hat{h}$ that performs well uniformly across all distributions $\\{T(\\mathcal{D})\\}_{T\\in\\mathcal{T}}$ . ", "page_idx": 0}, {"type": "text", "text": "We view this formulation as enabling a different way to describe distribution shifts through transformations $\\mathcal{T}=\\{T:\\mathcal{X}\\to\\mathcal{X}\\}$ . The collection of transformations $\\tau$ can be viewed as either: (a) given to the learning algorithm as part of the problem, or (b) chosen by the learning algorithm.View (a) represents scenarios where the target distribution shifts are known and specified by some downstream application (e.g., learning a classifier that is invariant to image rotations and translations). View (b) ", "page_idx": 0}, {"type": "text", "text": "represents scenarios where there is uncertainty or there are no pre-specified target distribution shifts and we would like to perform maximally well relative to an expressive collection of transformations. We highlight next several problems of interest that can be captured by this formulation. We refer the reader to Section 7 for a more detailed discussion in the context of prior work. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Covariate Shift & Domain Adaptation. By Brenier\u2019s theorem [Brenier, 1991], when $\\mathcal{X}=\\mathbb{R}^{d}$ , then under mild assumptions, for any source distribution $P$ over $\\mathcal{X}$ and target distribution $Q$ over $\\mathcal{X}$ , there exists a transformation $T:\\mathcal{X}\\rightarrow\\mathcal{X}$ such that $Q=T(P)$ . Thus, by choosing an expressive collection of transformations $\\tau$ , we can address arbitrary covariate shifts.   \n\u2022 Transformation-Invariant Learning. In many applications, it is desirable to train predictors that are invariant to transformations or data preprocessing procedures representing different \u201cenvironments\u201d (e.g., an image classifier deployed in different hospitals, or a self-driving car operating in different cities).   \n\u2022 Representative Sampling. In many applications, there may be challenges in collecting \u201crepresentative\u201d training data. For instace, in learning Logic or Arithmetic tasks [Abbe et al., 2023], the combinatorial nature of the data makes it not possible to cover well all parts of the domain. E.g., there is always a limit to the length of the problem considered at training, or features may not be homogeneously represented at training (bias towards certain digits etc.). Choosing a suitable collection of transformations $\\tau$ under which the target function is invariant can help to model in such cases.   \n\u2022 Adversarial Attacks. Test-time adversarial attacks such as adversarial patches in vision tasks [Brown et al., 2017, Karmon et al., 2018], attack prompts in large language models [Zou et al., 2023], and \u201cuniversal attacks\u201d [Moosavi-Dezfooli et al., 2017] can all be viewed as instantiations constructing specific transformations $\\tau$ . ", "page_idx": 1}, {"type": "text", "text": "Our Contributions. Let $\\mathcal{X}$ be the instance space and $y=\\{\\pm1\\}$ the label space. Let $\\mathcal{H}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ be a hypothesis class, and denote by $\\operatorname{vc}({\\mathcal{H}})$ its VC dimension. Consider a collection of transformations $\\mathcal{T}=\\{T:\\mathcal{X}\\to\\mathcal{X}\\}$ , and some unknown distribution $\\mathcal{D}$ over $\\mathcal X\\times\\mathcal X$ . Let $\\mathrm{err}(h,T({\\mathcal{D}}))=$ $\\operatorname*{Pr}_{(x,y)\\sim{\\cal D}}\\left[h(T(x))\\neq y\\right]$ be the error of predictor $h$ on transformed distribution $T(\\mathcal{D})$ . ", "page_idx": 1}, {"type": "text", "text": "Given a training sample $S\\,=\\,\\{(x_{1},y_{1}),\\dots,(x_{m},y_{m})\\}\\,\\sim\\,{\\mathcal{D}}^{m}$ , we are interested in learning a predictor $\\hat{h}$ with uniformly small risk across all transformations $T\\in\\mathcal T$ . Formally, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{T\\in\\mathcal{T}}\\mathrm{err}(\\hat{h},T(\\mathcal{D}))\\leq\\mathsf{O P T}_{\\infty}+\\varepsilon,\\mathrm{~where~OPT_\\infty:}=\\operatorname*{inf}_{h^{\\star}\\in\\mathcal{H}}\\operatorname*{sup}_{T\\in\\mathcal{T}}\\left\\{\\mathrm{err}(h^{\\star},T(\\mathcal{D}))\\right\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This objective is similar to that considered in prior work on distributionally robust optimization [Duchi and Namkoong, 2021] and multi-distribution learning [Haghtalab et al., 2022]. The main difference is that in this work we are describing the collection of \u201ctarget\u201d distributions $\\{T(\\mathcal{D})\\}_{T\\in\\mathcal{T}}$ as transformations of the \u201csource\u201d distribution $\\mathcal{D}$ . This allows us to obtain new upper bounds on the sample complexity of learning under distribution shifts based on the VC dimension of the composition of $\\mathcal{H}$ with $\\tau$ , denoted $\\operatorname{vc}({\\mathcal{H}}\\circ{\\mathcal{T}})$ (see Equation (3)). We describe next our results (informally): ", "page_idx": 1}, {"type": "text", "text": "1. In Section 2 (Theorem 2.1), we show that, given the knowledge of any hypothesis class $\\mathcal{H}$ and any collection of transformations $\\tau$ , by minimizing the empirical worst case risk, we can solve Objective 1 with sample complexity bounded by $\\operatorname{vc}({\\mathcal{H}}\\circ T)$ . Furthermore, in Theorem 2.2, we show that the sample complexity of any proper learning rule is bounded from below by $\\Omega(\\mathrm{vc}({\\mathcal{H}}\\circ{\\mathcal{T}}))$ .   \n2. In Section 3 (Theorem 3.1), we consider a more challenging scenario in which $\\mathcal{H}$ is unknown. Instead, we are only given an ERM oracle for $\\mathcal{H}$ . We then present a generic algorithmic reduction (Algorithm 1) solving Objective 1 using only an ERM oracle for $\\mathcal{H}$ , when the collection $\\tau$ is finite. This is established by solving a zero-sum game where the $\\mathcal{H}$ -player runs ERM and the $\\tau$ -player runs Multiplicative Weights [Freund and Schapire, 1997].   \n3. In Section 4 (Theorem 4.1), we consider situations where we do not know which transformations are relevant (or important) for the learning task at hand, and so we pick an expressive collection $\\tau$ and aim to perform well on as many transformations as possible. We then present a different generic learning rule (Equation (4)) that learns a predictor \u02c6h achieving low error (say $\\varepsilon$ ) on as many target distributions in $\\{T(\\mathcal{D})\\}_{T\\in\\mathcal{T}}$ as possible.   \n4. In Section 5 (Theorems 5.1 & E.1), we extend our learning guarantees to a slightly different objective, Objective 7, that can be favorable to Objective 1 when there is heterogeneity in the noise ", "page_idx": 1}, {"type": "text", "text": "across different transformations. This is inspired by Agarwal and Zhang [2022] who introduced this objective. ", "page_idx": 2}, {"type": "text", "text": "2 Minimizing Worst-Case Risk ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "If we have access to, or know, the hypothesis class $\\mathcal{H}$ and the collection of transformations $\\tau$ , then the most direct and intuitive way of solving Objective 1 is minimizing the empirical worst-case risk. Specifically, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{h}\\in\\underset{h\\in\\mathcal{H}}{\\operatorname{argmin}}\\operatorname*{max}_{T\\in\\mathcal{T}}\\left\\{\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{1}\\left[h(T(x_{i}))\\neq y_{i}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We highlight that this learning rule offers a game-theoretic perspective on distribution shift, where the $\\mathcal{H}$ -player searches for a predictor $h\\in\\mathcal H$ to minimize the worst-case error while the $\\tau$ -player searches for a transformation $T\\in\\mathcal T$ to maximize the worst-case error. For instance, both predictors $\\mathcal{H}$ and transformations $\\tau$ can be parameterized by neural network architectures, which is an interesting direction to explore further. We note that similar min-max optimization problems have appeared before in the literature on adversarial examples and generative adversarial networks [e.g., Madry et al., 2018, Goodfellow et al., 2020]. ", "page_idx": 2}, {"type": "text", "text": "We present next a PAC-style learning guarantee for this learning rule which offers the interpretation that solving the min-max optimization problem in Equation (1) yields a predictor $\\hat{h}\\,\\in\\,\\mathcal{H}$ that generalizes to the collection of transformations $\\tau$ . We show that the sample complexity of this learning rule is bounded by the VC dimension of the composition of $\\mathcal{H}$ with $\\tau$ , where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{H}\\circ\\mathcal{T}:=\\left\\{h\\circ T:h\\in\\mathcal{H},T\\in\\mathcal{T}\\right\\},\\ \\mathrm{where}\\left(h\\circ T\\right)(x)=h(T(x))\\ \\forall x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1. For any class $\\mathcal{H}$ , any collection of transformations $\\tau$ , any $\\varepsilon,\\delta\\in(0,{^1\\mathord{/}{\\vphantom{^12}}2})$ , any distribution D, with probability at least 1 \u2212\u03b4 over S \u223cDm(\u03b5,\u03b4) where m(\u03b5, \u03b4) = O vc(H\u25e6T )\u03b5+2log(1/\u03b4) , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{T\\in\\mathcal{T}}(\\hat{h},T(D))\\leq\\mathsf{O P T}_{\\infty}+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Proof. The proof follows from invoking uniform convergence guarantees with respect to the composition $\\mathcal{H}\\circ\\mathcal{T}$ (see Proposition A.1 in Appendix A) and the definition of $\\hat{h}$ described in Equation (2). Let $h^{\\star}\\in\\mathcal{H}$ be an a-priori fixed predictor (independent of sample $S$ ) attaining ${\\mathsf{O P T}}_{\\infty}=$ $\\begin{array}{r}{\\operatorname*{inf}_{h\\in\\mathcal{H}}\\operatorname*{sup}_{T\\in\\mathcal{T}}\\operatorname{err}(h,T(\\mathcal{D}))}\\end{array}$ (or $\\varepsilon$ -close to it). By setting $\\begin{array}{r}{m(\\varepsilon,\\delta)\\,=\\,O\\left(\\frac{\\mathrm{vc}(\\mathcal{H}\\circ T)+\\log\\left(1/\\delta\\right)}{\\varepsilon^{2}}\\right)}\\end{array}$ and invoking Proposition A.1, we have the guarantee that with probability at least $1-\\delta$ over $S\\sim{\\cal D}^{m(\\varepsilon,\\delta)}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n(\\forall h\\in\\mathcal{H})\\left(\\forall T\\in\\mathcal{T}\\right):\\left|\\mathrm{err}(h,T(S))-\\mathrm{err}(h,T(\\mathcal{D}))\\right|\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Since $\\hat{h},h^{\\star}\\in\\mathcal{H}$ , the inequality above implies that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall T\\in\\mathcal{T}:\\ \\mathrm{err}(\\hat{h},T(\\mathcal{D}))\\leq\\mathrm{err}(\\hat{h},T(S))+\\varepsilon.}\\\\ &{\\forall T\\in\\mathcal{T}:\\ \\mathrm{err}(h^{\\star},T(S))\\leq\\mathrm{err}(h^{\\star},T(\\mathcal{D}))+\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Furthermore, by definition, since $\\hat{h}$ minimizes the empirical objective, it holds that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{T\\in{\\mathcal{T}}}\\operatorname{err}({\\hat{h}},T(S))\\leq\\operatorname*{sup}_{T\\in{\\mathcal{T}}}\\operatorname{err}(h^{\\star},T(S)).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By combining the above, we get ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{T\\in T}\\operatorname{err}(\\hat{h},T(\\mathcal{D}))\\leq\\operatorname*{sup}_{T\\in T}\\operatorname{err}(\\hat{h},T(S))+\\varepsilon\\leq\\operatorname*{sup}_{T\\in T}\\operatorname{err}(h^{\\star},T(S))+\\varepsilon\\leq\\mathsf{O P T}_{\\infty}+2\\varepsilon.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We show next that $\\operatorname{vc}({\\mathcal{H}}\\circ{\\mathcal{T}})$ can be much higher than $\\operatorname{vc}({\\mathcal{H}})$ and the dependency on $\\operatorname{vc}({\\mathcal{H}}\\circ{\\mathcal{T}})$ is tight for all proper learning rules, which includes the learning rule described in Equation (2) and more generally any learning rule that is restricted to outputting a classifier in $\\mathcal{H}$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.2. $\\forall k\\in\\mathbb{N},\\exists\\mathcal{X},\\mathcal{H},\\mathcal{T}$ such that $\\operatorname{vc}(\\mathcal{H})=1$ but $\\operatorname{vc}({\\mathcal{H}}\\circ T)\\geq k,$ , and the sample complexity of any proper learning rule $\\mathbb{A}:({\\mathcal{X}}\\times{\\mathcal{Y}})^{*}\\to{\\mathcal{H}}$ solving Objective $^{\\,l}$ is at least $\\Omega(\\mathrm{vc}({\\mathcal{H}}\\circ{\\mathcal{T}}))$ . ", "page_idx": 2}, {"type": "text", "text": "A proof is deferred to Appendix C. We remark that the sample complexity cannot be improved by proper learning rules and this leaves open the possibility of improving the sample complexity with improper learning rules. There are many examples in the literature where there are sample complexity gaps between proper and improper learning [e.g., Angluin, 1987, Daniely and Shalev-Shwartz, 2014, Foster et al., 2018, Montasser et al., 2019, Alon et al., 2021]. In particular, it appears that we encounter in this work a phenomena similar to what occurs in adversarially robust learning [Montasser et al., 2019]. Nonetheless, even at the expense of (potentially) higher sample complexity, we believe that there is value in the simplicity of the learning rule described in Equation (2), and exploring ways of implementing it is an interesting direction beyond the scope of this work. ", "page_idx": 3}, {"type": "text", "text": "2.1 Examples and Instantiations of Guarantees ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To demonstrate the utility of our generic result in Theorem 2.1, we discuss next a few general cases where we can bound the VC dimension of $\\mathcal{H}$ composed with $\\tau$ , $\\operatorname{vc}({\\mathcal{H}}\\circ{\\mathcal{T}})$ . This allows us to obtain new learning guarantees with respect to classes of distribution shifts that are not covered by prior work, to the best of our knowledge. ", "page_idx": 3}, {"type": "text", "text": "Linear Transformations. Consider $\\tau$ being a (potentially infinite) collection of linear transformations. For example, in vision tasks, this includes many transformations that have been widely studied in practice such as rotations, translations, maskings, adding random noise (or any fixed a-priori arbitrary noise), and their compositions [Engstrom et al., 2019, Hendrycks and Dietterich, 2019]. ", "page_idx": 3}, {"type": "text", "text": "Interestingly, for a broad range of hypothesis classes $\\mathcal{H}$ , we can show that $\\operatorname{vc}(\\mathcal{H}\\circ\\mathcal{T})\\leq\\operatorname{vc}(\\mathcal{H})$ without incurring any dependence on the complexity of $\\tau$ . Specifically, the result applies to any function class $\\mathcal{H}$ that consists of a linear mapping followed by an arbitrary mapping. This includes feed-forward neural networks with any activation function, and modern neural network architectures (e.g., CNNs, ResNets, Transformers). We find the implication of this bound to be interesting, because it suggests (along with Theorem 2.1) that the learning rule in Equation (2) can generalize to linear transformations with sample complexity that is not greater than the sample complexity of standard PAC learning. We formally present the lemma below, and defer the proof to Appendix B. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.3. For any collection of linear transformations $\\tau$ and any hypothesis class of the form $\\mathcal{H}=\\big\\{f\\circ W:\\mathbb{R}^{d}\\overset{\\cdot}{\\to}\\mathcal{V}\\ |\\ W\\in\\mathring{\\mathbb{R}}^{k\\times d}\\wedge f:\\mathbb{R}^{\\tilde{k}}\\to\\mathcal{V}\\big\\}$ , it holds that $\\mathrm{vc}(\\mathcal{H}\\circ\\mathcal{T})\\leq\\mathrm{vc}(\\mathcal{H})$ . ", "page_idx": 3}, {"type": "text", "text": "Non-Linear Transformations. Consider $\\tau$ being a (potentially infinite) collection of non-linear transformations parameterized by a feed-forward neural network architecture, where each $T=$ $W_{L}\\circ\\phi\\circ\\cdots\\phi\\circ W_{2}\\circ\\phi\\circ W_{1}$ and $\\phi(\\cdot)=\\operatorname*{max}\\left\\lbrace0,\\cdot\\right\\rbrace$ is the ReLU activation function. Similarly, consider a hypothesis class $\\mathcal{H}$ that is parameterized by a (different) feed-forward neural network architecture, where each $h=\\mathrm{sign}\\,\\circ\\tilde{W_{H}}\\circ\\phi\\circ\\cdots\\phi\\circ\\tilde{W}_{2}\\circ\\phi\\circ\\tilde{W}_{1}$ . Observe that the composition $\\mathcal{H}\\circ\\mathcal{T}$ consists of (deeper) feed-forward neural networks, where $h\\circ T=\\operatorname{sign}\\circ{\\tilde{W}}_{H}\\circ\\phi\\circ\\cdot\\cdot\\cdot\\phi\\circ{\\tilde{W}}_{2}\\circ$ $\\phi\\circ\\tilde{W}_{1}\\circ W_{L}\\circ\\phi\\circ\\cdot\\cdot\\cdot\\phi\\circ W_{2}\\circ\\phi\\circ W_{1}$ . Thus, we can bound $\\operatorname{vc}({\\mathcal{H}}\\circ T)$ by appealing to classical results bounding the VC dimension of feed-forward neural networks. For example, according to Bartlett et al. [2019], it holds that $\\mathrm{vc}(\\mathcal{H}\\circ\\mathcal{T})\\leq O\\left((H+L)P_{\\mathcal{H}\\circ\\mathcal{T}}\\log(P_{\\mathcal{H}\\circ\\mathcal{T}})\\right)$ , where $H+L$ is the depth of the networks in $\\mathcal{H}\\circ\\mathcal{T}$ and $P_{\\mathcal{H}\\circ\\mathcal{T}}$ is the number of parameters of the networks in $\\mathcal{H}\\circ\\mathcal{T}$ (which is $P_{\\mathcal{H}}+P_{\\mathcal{T}})$ ). In this context, Theorem 2.1 and Equation (2) present a new learning guarantee against distribution shifts parameterized by non-linear transformations induced with feed-forward neural networks. ", "page_idx": 3}, {"type": "text", "text": "Transformations on the Boolean hypercube. The Boolean hypercube has also received attention recently as a case-study for distribution shifts in Logic or Arithmetic tasks[Abbe et al., 2023]. $\\mathcal{H}\\circ\\mathcal{T}$ e afwrc ohn siVps aCac  fed $\\mathcal{X}\\,=\\,\\left\\{0,1\\right\\}^{d}$ $\\mathcal{H}$ uwalteni ndcg at fnhr eob moV urCne sddt ritihmcetei nnVsgi Ctor nadsni smoffoe $\\{\\mathcal{T}_{i}\\}_{i=1}^{d}$ $\\mathcal{T}_{i}=\\{x\\mapsto T(x)_{i}:T\\in\\mathcal{T}\\}$ $T:\\left\\{0,1\\right\\}^{d}\\rightarrow\\left\\{0,1\\right\\}^{d}\\in{\\mathcal{T}}$ to output only the $i^{\\mathrm{th}}$ bit. The proof is deferred to Appendix B ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.4. When $\\mathcal{X}=\\left\\{0,1\\right\\}^{d}$ , for any hypothesis class $\\mathcal{H}$ and any collection of transformations $\\begin{array}{r}{\\mathcal{T},\\,\\mathrm{vc}(\\mathcal{H}\\circ\\mathcal{T})\\leq O(\\log d)(\\mathrm{vc}(\\mathcal{H})+\\sum_{i=1}^{d}\\mathrm{vc}(\\mathcal{T}_{i}))}\\end{array}$ , where each $\\mathcal{T}_{i}=\\{x\\mapsto T(x)_{i}:T\\in\\mathcal{T}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "In this context, Theorem 2.1 and Equation (2) present a new learning guarantee against arbitrary distribution shifts parameterized by transformations on the Boolean hypercube, where the sample complexity (potentially) grows with the complexity of the transformations as measured by the VC dimension. We note however that this learning guarantee does not address the problem of length generalization, since we restrict to transformations that preserve domain length. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Adversarial Attacks. In adversarially robust learning, a test-time attacker is typically modeled as a pertubation function $\\mathcal{U}:\\mathcal{X}\\xrightarrow{}2^{\\mathcal{X}}$ , which specifies for each test-time example $x$ a set of possible adversarial attacks $\\mathcal{U}(x)\\subseteq\\mathcal{X}$ that the attacker can choose from at test-time [Montasser et al., 2019]. The robust risk of a predictor $\\hat{h}$ is then defined as: $\\begin{array}{r}{\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\left[\\operatorname*{sup}_{z\\in\\mathcal{U}(x)}\\mathbb{1}\\left[\\hat{h}(z)\\neq y\\right]\\right]}\\end{array}$ . On the other hand, the framework we consider in this paper can be viewed as restricting a test-time attacker to commit to a set of attacks ${\\mathcal T}\\,=\\,\\{T:{\\mathcal X}\\,\\dot{\\rightarrow}\\,\\dot{x}\\}$ without knowledge of the test-time samples, and the risk of a predictor $\\hat{h}$ is then defined as: $\\begin{array}{r}{\\operatorname*{sup}_{T\\in\\mathcal{T}}\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\mathbb{1}\\left[\\hat{h}(T(x))\\neq y\\right]}\\end{array}$ . While less general, our framework still captures several interesting adversarial attacks in practice which are constructed before seeing test-time examples, such as adversarial patches in vision tasks [Brown et al., 2017, Karmon et al., 2018] and attack prompts for large language models [Zou et al., 2023] can be represented with linear transformations. ", "page_idx": 4}, {"type": "text", "text": "3 Unknown Hypothesis Class: Algorithmic Reductions to ERM ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Implementing the learning rule in Equation (2) crucially requires knowing the base hypothesis class $\\mathcal{H}$ and the transformations $\\tau$ , which may not be feasible in many scenarios. Moreover, in many applications we only have black-box access to an off-the-shelve supervised learning method such as an ERM for $\\mathcal{H}$ . Hence, in this section, we study the following question: ", "page_idx": 4}, {"type": "text", "text": "Can we solve Objective 1 using only an ERM oracle for $\\mathcal{H}$ ? ", "page_idx": 4}, {"type": "text", "text": "We prove yes, and we present next generic oracle-efficient reductions solving Objective 1 using only an ERM oracle for $\\mathcal{H}$ . We consider two cases, ", "page_idx": 4}, {"type": "text", "text": "Realizable Case. When ${\\mathsf{O P T}}_{\\infty}\\,=\\,0$ , i.e., $\\exists h^{\\star}\\,\\in\\,{\\mathcal{H}}$ such that $\\forall T\\,\\in\\,{\\mathcal{T}}:\\,\\mathrm{err}(h^{\\star},T({\\mathcal{D}}))\\,=\\,0,$ there is a simple reduction to solve Objective 1 using a single call to an ERM oracle for $\\mathcal{H}$ . The idea is to inflate the training dataset $S$ to include all possible transformations ${\\mathcal{T}}(S)\\ =$ $\\{(T(x),y):(x,y)\\in S\\land T\\in T\\}$ (similar to data augmentation), and then run ERM on $\\tau(S)$ . Formal guarantee and proof are deferred to Appendix D. It is also possible, via a fairly standard boosting argument, to achieve a similar learning guarantee using multiple ERM calls (specifically, $O(\\log|{\\bar{\\mathcal{T}}}({\\bar{S}})|)\\leq O(\\log(|S|\\,|{\\mathcal{T}}|)))$ , where each ERM call is on a sample of size $O\\big(\\mathrm{vc}(\\mathcal{H})\\big)$ . So, we get a tradeoff between the size of a dataset given to ERM on a single call, and the total number of calls to ERM. ", "page_idx": 4}, {"type": "text", "text": "Agnostic Case.When $\\mathsf{O P T}_{\\infty}>0$ , the simple reduction above no longer works. Specifically, the issue is that running a single ERM on the inflation $\\tau(S)$ effectively minimizes average error over transformations $T\\in\\mathcal T$ as opposed to minimizing maximum error over transformations $T\\in\\mathcal T$ . So, $\\mathsf{O P T}_{\\infty}>0$ , by definition, implies there is no predictor $h\\in\\mathcal H$ that is consistent (i.e., zero error) on every transformation $T(S),T\\in{\\mathcal{T}}$ , thus minimizing average error over transformations can be bad. To overcome this limitation, we present a different reduction (Algorithm 1) that minimizes Objective 1 by solving a zero-sum game where the $\\mathcal{H}$ -player runs ERM and the $\\tau$ -player runs Multiplicative Weights [Freund and Schapire, 1997]. This can be viewed as solving a sequence of weighted-ERM problems (with weights over transformations), where Multiplicative Weights determines the weight of each transformation. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. For any class $\\mathcal{H}$ , collection of transformations $\\tau$ , distribution $\\mathcal{D}$ and any $\\varepsilon$ $,\\delta\\in(0,{^1\\mathord{/}{\\vphantom{^12}}2})$ , with probability at least $1-\\delta$ over $S\\sim\\mathcal{D}^{m(\\varepsilon,\\delta)}$ , where $\\begin{array}{r}{m(\\varepsilon,\\delta)\\leq O(\\frac{\\mathrm{vc}(\\mathcal{H}\\circ\\mathcal{T})+\\log\\left(1/\\delta\\right)}{\\varepsilon^{2}})}\\end{array}$ , running Algorithm $^{\\,l}$ on $S$ for $R\\geq\\frac{8\\ln\\vert\\tau\\vert}{\\varepsilon^{2}}$ rounds produces $\\begin{array}{r}{\\bar{h}=\\frac{1}{R}\\sum_{r=1}^{R}h_{r}}\\end{array}$ satisfying ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall T\\in\\mathcal{T}:\\operatorname*{Pr}_{(x,y)\\sim D\\atop r\\sim\\mathrm{Unif}\\{1,\\ldots,R\\}}\\left[h_{r}(T(x))\\neq y\\right]\\leq0\\mathsf{P T}_{\\infty}+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 3.2. When $\\tau$ is a finite collection of transformations, we can bound $\\operatorname{vc}({\\mathcal{H}}\\circ{\\mathcal{T}})$ from above by $O(\\mathrm{vc}(\\mathcal{H})+\\log|\\mathcal{T}|)$ using the Sauer-Shelah-Perels Lemma [Sauer, 1972]. See Lemma B.1 and proof in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Input: Black-box $\\mathtt{E R M}_{\\mathcal{H}}$ , dataset $\\overline{{S=\\{(x_{1},y_{1}),\\dots,(x_{m},y_{m})\\}}}$ , and transformations $\\tau$ .   \nFor each T \u2208T , set Q1(T) =|T1 |. ", "page_idx": 5}, {"type": "text", "text": "2 Set number of rounds $\\begin{array}{r}{R=\\frac{8\\ln|\\mathcal{T}|}{\\varepsilon^{2}}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "3 for 1 \u2264r \u2264R do ", "page_idx": 5}, {"type": "text", "text": "4 Run $\\mathtt{E R M}_{\\mathcal{H}}$ on $m_{\\mathrm{ERM}}$ i.i.d. samples drawn from the distribution induced by $Q_{r}$ over $\\tau$ and $\\mathrm{Unif}(S)$ , and let $h_{r}$ denote its output.   \n5 For each $T\\in\\mathcal T$ , update $\\begin{array}{r}{Q_{r+1}(T)=\\frac{Q_{r}(T)\\exp(-\\eta(1-\\mathrm{err}(h_{r},T(S))))}{Z_{r}}}\\end{array}$ , where $Z_{r}$ is a normalization factor such that $Q_{r+1}$ is a distribution. ", "page_idx": 5}, {"type": "text", "text": "Proof of Theorem 3.1. Let $S=\\{(x_{1},y_{1}),\\dots,(x_{m},y_{m})\\}$ be an arbitrary dataset. By setting $R\\geq$ $\\frac{8\\ln|{\\mathcal{T}}|}{\\varepsilon^{2}}$ and invoking Lemma D.2, which is a helpful lemma (statement and proof in Appendix D) that instantiates the regret guarantee of Multiplicative Weights in our context, we are guaranteed that Algorithm 1 produces a sequence of distributions $Q_{1},\\dots,Q_{R}$ over $\\tau$ that satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{T\\in\\mathcal{T}}\\ \\frac{1}{R}\\sum_{r=1}^{R}\\operatorname{err}\\left(h_{r},T(S)\\right)\\leq\\frac{1}{R}\\sum_{r=1}^{R}\\underset{T\\sim Q_{r}}{\\mathbb{E}}\\operatorname{err}(h_{r},T(S))+\\frac{\\varepsilon}{4}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "At each round $r$ , observe that Step 4 in Algorithm 1 draws an i.i.d. sample from a distribution $P_{r}$ over $\\mathcal X\\times\\mathcal X$ that is defined by $Q_{r}$ over $\\tau$ and $\\bar{\\mathrm{Unif}}(S)$ , and since $\\mathtt{E R M}_{\\mathcal{H}}$ is an $(\\varepsilon,\\delta)$ -agnostic-PAC-learner for $\\mathcal{H}$ , Step 5 guarantees that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathop{\\mathbb{E}}_{\\sim Q_{r}}\\operatorname{err}(h_{r},T(S))=\\mathop{\\mathbb{E}}_{T\\sim Q_{r}}\\frac{1}{m}\\sum_{i=1}^{m}\\mathop{\\mathbb{I}}\\left\\{h_{r}(T(x_{i}))\\neq y_{i}\\right\\}\\le\\operatorname*{min}_{h\\in\\mathcal{H}\\,T\\sim Q_{r}}\\operatorname{err}(h,T(S))+\\frac{\\varepsilon}{4}\\le\\operatorname*{min}_{h^{*}\\in\\mathcal{H}\\,T\\in\\mathcal{T}}(h^{*})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Combining the above inequalities implies that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{T\\in\\mathcal{T}}\\ \\frac{1}{R}\\sum_{r=1}^{R}\\operatorname{err}\\left(h_{r},T(S)\\right)\\leq\\operatorname*{min}_{h^{\\star}\\in\\mathcal{H}}\\operatorname*{max}_{T\\in\\mathcal{T}}\\operatorname{err}(h^{\\star},T(S))+\\frac{\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, by appealing to uniform convergence over $\\mathcal{H}\\circ\\mathcal{T}$ (Proposition A.1), with probability at least $1-\\delta$ over $S\\sim\\mathcal{D}^{m}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle}&{\\displaystyle\\operatorname*{max}_{\\forall\\in\\mathcal{T}}\\frac{1}{R}\\displaystyle\\sum_{r=1}^{R}\\operatorname{err}\\left(h_{r},T(\\mathcal{D})\\right)\\leq\\displaystyle\\operatorname*{max}_{T\\in\\mathcal{T}}\\frac{1}{R}\\displaystyle\\sum_{r=1}^{R}\\operatorname{err}\\left(h_{r},T(S)\\right)+\\frac{\\varepsilon}{4}\\leq\\displaystyle\\operatorname*{min}_{h^{*}\\in\\mathcal{H}}\\operatorname*{max}_{T\\in\\mathcal{T}}(h^{\\star},T(S))+\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{4}}\\\\ &{\\displaystyle\\leq\\operatorname*{min}_{h^{*}\\in\\mathcal{H}}\\operatorname*{max}_{T\\in\\mathcal{T}}(h^{\\star},T(\\mathcal{D}))+\\frac{\\varepsilon}{2}+2\\frac{\\varepsilon}{4}=0\\mathsf{P T}_{\\infty}+\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "On finiteness of $\\tau$ . We argue informally that requiring $\\tau$ to be finite is necessary in general when only an ERM oracle for $\\mathcal{H}$ is allowed. For example, consider a distribution supported on a single point $(x,-)$ on the real line where $x=5$ , and transformations $T_{i}(x)=x+i$ for all $i\\geq1$ induced by some collection $\\{T_{i}\\}_{i\\in\\mathbb{N}}$ . Calling ERM on a finite subset of these transformations $T_{i_{1}},\\ldots,T_{i_{k}}$ could return a predictor that labels $x,x+i_{1},x+i_{2},\\ldots,x+i_{k}$ with a label \u2212and labels $x+i_{k}+1,\\ldots$ with $^+$ (e.g., if $\\mathcal{H}$ is thresholds) which fails to satisfy Objective 1. But it would be interesting to explore additional structural conditions that would enable handling infinite $\\tau$ , and leave this to future work. ", "page_idx": 5}, {"type": "text", "text": "4 Unknown Invariant Transformations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "When we have a large collection of transformations $\\tau$ and there is uncertainty about which transformations $T\\in\\mathcal T$ under-which we can simultaneously achieve low error using a base class $\\mathcal{H}$ , the learning rule presented in Section 2 (Equation 1) can perform badly. We illustrate this with the following concrete example: ", "page_idx": 5}, {"type": "text", "text": "Example 1. Consider a class $\\mathcal{H}=\\{h_{1},h_{2},h_{3}\\}$ , a collection of transformations $\\mathcal{T}=\\{T_{1},T_{2},T_{3}\\}$ , and a distribution $\\mathcal{D}$ with risks (errors) as reported in the table. ", "page_idx": 5}, {"type": "table", "img_path": "u2gzfXRLaN/tmp/c0f6fd0e7e2df575fe9bdd13d574966b8501f07b877641ffec7b8c7963b48a15.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Then, solving Objective 1 may return predictor $h_{3}$ where $\\forall T\\in{T}:\\operatorname{err}(h_{3},T({\\mathcal{D}}))=49\\%$ , since we only need to compete with the worst-case risk $\\mathsf{O P T}_{\\infty}=49\\%$ . However, predictor $h_{1}$ is arguably better since it achieves a low error of $1\\%$ on at least two out of the three transformations. ", "page_idx": 6}, {"type": "text", "text": "To address this limitation, we switch to a different learning goal\u2014achieving low error under as many transformations as possible. We present next a different generic learning rule for any class $\\mathcal{H}$ and any collection of transformations $\\tau$ , that enjoys a different guarantee from the learning rule presented in Section 2. In particular, it can be thought of as greedy since it maximizes the number of transformations under which low empirical error is possible, but also conservative since it ignores transformations under which low empirical error is not possible. Specifically, given a training dataset $S$ , the learning rule searches for a predictor $\\hat{h}\\in\\mathcal{H}$ that achieves low empirical error on as many transformations $T\\in\\mathcal T$ as possible, say $\\mathrm{err}(\\hat{h},T(S))\\leq\\varepsilon$ . ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{h}\\in\\underset{h\\in\\mathcal{H}}{\\mathrm{argmax}}\\sum_{T\\in\\mathcal{T}}\\mathbb{1}\\left[\\mathrm{err}(h,T(S))\\leq\\varepsilon\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Another way of thinking about this learning rule is that it provides us with more flexibility in choosing the collection of transformations $\\tau$ , since the learning rule is not stringent on achieving low error on all transformations but instead attempts to achieve low error on as many transformations as allowed by the base class $\\mathcal{H}$ . Thus, this is useful in situations where there is uncertainty in choosing the collection of transformations. We present next the formal learning guarantee for this learning rule, ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. For any class $\\mathcal{H}$ , any countable collection of transformations $\\tau$ , any distribution $\\mathcal{D}$ and any $\\varepsilon,\\delta\\ \\stackrel{.}{\\in}\\ (0,1)$ , with probability at least $1\\,-\\,\\delta$ over $S~\\sim~{\\cal D}^{m}$ , where $m\\ =$ $\\begin{array}{r}{O\\left(\\frac{\\mathrm{vc}(\\mathcal{H}\\circ\\mathcal{T})\\log(1/\\varepsilon)+\\log(1/\\delta)}{\\varepsilon}\\right)}\\end{array}$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{T\\in{\\mathcal{T}}}\\mathbb{1}\\left[\\operatorname{err}({\\hat{h}},T({\\mathcal{D}}))\\leq3\\varepsilon\\right]\\geq\\operatorname*{max}_{h^{\\star}\\in{\\mathcal{H}}}\\sum_{T\\in{\\mathcal{T}}}\\mathbb{1}\\left[\\operatorname{err}(h^{\\star},T({\\mathcal{D}}))\\leq{\\frac{\\varepsilon}{3}}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Furthermore, it holds that $\\forall T\\in\\mathcal{T}$ : $\\begin{array}{r}{\\mathrm{err}(\\hat{h},T(\\mathcal{D}))\\leq\\mathrm{err}(\\hat{h},T(S))+\\sqrt{\\mathrm{err}(\\hat{h},T(S))\\frac{\\varepsilon}{3}}+\\frac{\\varepsilon}{3}.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Remark 4.2. We can generalize the result above to any prior over the transformations $\\tau$ , encoded as a weight function $w\\,:\\,\\tau\\,\\rightarrow\\,[0,1]$ such that $\\begin{array}{r}{\\dot{\\sum_{T\\in{\\mathcal T}}w(T)}\\,\\leq\\,1}\\end{array}$ . By maximizing the weighted version of Equation (4) according to w, it hol ds that T \u2208T w(T)1[err(\u02c6h,T (D))\u2264\u03b5/3] \u2265 $\\begin{array}{r}{\\operatorname*{max}_{h^{\\star}\\in\\mathcal{H}}\\sum_{T\\in\\mathcal{T}}w(T)\\mathbb{1}_{[\\mathrm{err}(h^{\\star},T(\\mathcal{D}))\\leq3\\varepsilon]}}\\end{array}$ with high probability. ", "page_idx": 6}, {"type": "text", "text": "Proof. The proof follows from the definition of $\\hat{h}$ and using optimistic generalization bounds (Proposition A.2). By setting m(\u03b5, \u03b4) = O vc(H\u25e6T ) log(\u03b51/\u03b5)+log(1/\u03b4) and invoking Proposition A.2, we have the guarantee that with probability at least $1-\\delta$ over $S\\sim{\\cal D}^{m(\\varepsilon,\\delta)}$ , $(\\forall h\\in\\mathcal{H})(\\forall T\\in\\mathcal{T})$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{err}(h,T(\\mathcal{D}))\\leq\\mathrm{err}(h,T(S))+\\sqrt{\\mathrm{err}(h,T(S))\\frac{\\varepsilon}{3}}+\\frac{\\varepsilon}{3},}\\\\ &{\\mathrm{err}(h,T(S))\\leq\\mathrm{err}(h,T(\\mathcal{D}))+\\sqrt{\\mathrm{err}(h,T(\\mathcal{D}))\\frac{\\varepsilon}{3}}+\\frac{\\varepsilon}{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since $\\hat{h}\\in\\mathcal{H}$ , inequality (4) above implies that $\\forall T\\in\\mathcal{T}$ if $\\mathrm{err}(\\hat{h},T(S))\\leq\\varepsilon$ then $\\mathrm{err}(\\hat{h},T({\\mathcal D}))\\leq3\\varepsilon$ Thus, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{T\\in T}\\mathbb{1}\\left[\\mathrm{err}(\\hat{h},T(\\mathcal{D}))\\leq3\\varepsilon\\right]\\geq\\sum_{T\\in\\mathcal{T}}\\mathbb{1}\\left[\\mathrm{err}(\\hat{h},T(S))\\leq\\varepsilon\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Furthermore, by definition, since $\\hat{h}$ maximizes the empirical objective, it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{T\\in T}\\mathbb{1}\\left[\\mathrm{err}(\\hat{h},T(S))\\leq\\varepsilon\\right]\\geq\\sum_{T\\in T}\\mathbb{1}\\left[\\mathrm{err}(h^{\\star},T(S))\\leq\\varepsilon\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since $\\begin{array}{r l r}{h^{\\star}}&{{}\\in}&{\\mathcal{H}}\\end{array}$ , inequality (5) above implies that $\\forall T\\ \\in\\ {\\mathcal{T}}$ if $\\mathrm{err}(h^{\\star},T({\\mathcal{D}}))\\ \\leq\\ \\varepsilon/3$ then $\\mathrm{err}(h^{\\star},T(S))\\leq\\varepsilon$ . Thus, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{T\\in T}\\mathbb{1}\\left[\\mathrm{err}(h^{\\star},T(S))\\leq\\varepsilon\\right]\\geq\\sum_{T\\in T}\\mathbb{1}\\left[\\mathrm{err}(h^{\\star},T(\\mathcal{D}))\\leq\\varepsilon/3\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "By combining the above three inequalities, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{T\\in{\\mathcal{T}}}\\mathbb{1}\\left[\\operatorname{err}({\\hat{h}},T({\\mathcal{D}}))\\leq3\\varepsilon\\right]\\geq\\sum_{T\\in{\\mathcal{T}}}\\mathbb{1}\\left[\\operatorname{err}(h^{\\star},T({\\mathcal{D}}))\\leq\\varepsilon/3\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "5 Extension to Minimizing Worst-Case Regret ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "When there is heterogeneity in the noise across the different distributions, Agarwal and Zhang [2022] argue that, in the context of distributionally robust optimization, solving Objective 1 may not be advantageous. Additionally, they introduced a different objective (see Objective 7) which can be favorable to minimize. In this section, we extend our guarantees for transformation-invariant learning to this new objective which we describe next. ", "page_idx": 7}, {"type": "text", "text": "For each $\\textit{T}\\in\\textit{\\mathcal{T}}$ , let $\\mathsf{O P T}_{T}\\;\\;=\\;\\;\\mathrm{inf}_{h_{T}^{\\star}\\in\\mathcal{H}}\\,\\mathrm{err}(h_{T}^{\\star},T(\\mathcal{D}))$ be the smallest achievable error on transformed distribution $T(\\mathcal{D})$ with hypothesis class $\\mathcal{H}$ . Given a training sample $\\textit{S}=$ $\\left\\{(x_{1},y_{1}),\\dotsc,(x_{m},y_{m})\\right\\}\\sim{\\mathcal{D}}^{m}$ , we would like to learn a predictor $\\hat{h}:\\mathcal{X}\\to\\mathcal{Y}$ with uniformly small regret across all transformations $T$ in $\\tau$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{T\\in\\mathcal{T}}(\\hat{h},T(\\mathcal{D}))-\\mathsf{O P T}_{T}\\leq\\operatorname*{inf}_{h^{\\star}\\in\\mathcal{H}}\\operatorname*{sup}_{T\\in\\mathcal{T}}\\left\\{\\mathsf{e r r}(h^{\\star},T(\\mathcal{D}))-\\mathsf{O P T}_{T}\\right\\}+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We illustrate with a concrete example below how solving Objective 7 can be favorable to Objective 1. Example 2 (Risk vs. Regret). Consider a class $\\mathcal{H}\\;=\\;\\{h_{1},h_{2}\\}$ , a collection of transformations $\\mathcal{T}=\\{T_{1},T_{2},T_{3},T_{4}\\}$ , and a distribution $\\mathcal{D}$ such that with errors as reported in the table: ", "page_idx": 7}, {"type": "table", "img_path": "u2gzfXRLaN/tmp/36dbd638230a8d085c92858138b1009d9b1491cc8e6976291f35dd70599c79f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Thus, solving Objective 1 may return predictor $h_{2}$ where $\\forall T\\in{T}:\\mathrm{err}(h_{2},T({\\mathcal{D}}))=1/2$ , since we only need to compete with the worst-case risk $\\mathsf{O P\\bar{T}}_{\\infty}=\\frac{1}{2}$ . However, solving Objective 7 will return predictor $h_{1}$ where $\\forall T:T:\\mathrm{err}(h_{1},T(\\mathcal{D}))\\leq0\\mathsf{P T}_{T}$ . ", "page_idx": 7}, {"type": "text", "text": "More generally, as highlighted by Agarwal and Zhang [2022], whenever there exists $h^{\\star}\\ \\in\\ {\\mathcal{H}}$ satisfying $\\forall T\\,\\in\\,T:\\,\\mathsf{e r r}(\\bar{h}^{\\star},T(\\bar{D_{\\big}}))\\,-\\,\\mathsf{O P T}_{T}\\,\\leq\\,\\varepsilon$ , solving Objective 7 is favorable. We present next a generic learning rule solving Objective 7 for any hypothesis class $\\mathcal{H}$ and any collection of transformations $\\tau$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{h}\\in\\underset{h\\in\\mathcal{H}}{\\operatorname{argmin}}\\operatorname*{max}_{T\\in\\mathcal{T}}\\left\\{\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{1}\\left[h(T(x_{i}))\\neq y_{i}\\right]-0\\hat{\\mathsf{P}}\\mathsf{T}_{T}\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We present next a PAC-style learning guarantee for this learning rule with sample complexity bounded by the VC dimension of the composition of $\\mathcal{H}$ with $\\tau$ . The proof is deferred to Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1. For any class $\\mathcal{H}$ , any collection of transformations $\\tau$ , any $\\varepsilon,\\delta\\in(0,{^1\\mathord{/}{\\vphantom{^12}}2})$ , any distribution D, with probability at least 1 \u2212\u03b4 over S \u223cDm(\u03b5,\u03b4) where m(\u03b5, \u03b4) = O vc(H\u25e6T )\u03b5+2log(1/\u03b4) , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{T\\in\\mathcal{T}}\\Big\\{\\mathrm{err}(\\hat{h},T(D))-\\mathsf{O P T}_{T}\\Big\\}\\leq\\operatorname*{inf}_{h^{\\star}\\in\\mathcal{H}}\\operatorname*{sup}_{T\\in\\mathcal{T}}\\big\\{\\mathrm{err}(h^{\\star},T(\\mathcal{D}))-\\mathsf{O P T}_{T}\\big\\}+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Algorithmic Reduction to ERM. Using ideas and techniques similar to those used in Section 3, we develop a generic oracle-efficient reduction solving Objective 7 using only an ERM oracle for $\\mathcal{H}$ . Theorem, proof, and algorithm are deferred to Appendix $\\boldsymbol{\\mathrm E}$ . ", "page_idx": 7}, {"type": "image", "img_path": "u2gzfXRLaN/tmp/379261823b568ebb20d717d6c400365ca828f2148d2d19256a23bb9ee75143df.jpg", "img_caption": ["Figure 1: Left plot is for learning $f_{1}^{\\star}$ , the full parity function in dimension 18, with a train set size of 7000. Transformations are sampled from $\\tau_{1}$ : the set of all permutations. Right plot is for learning $f_{2}^{\\star}$ , a majority-of-subparities function in dimension 21, with a train set size of 5000. Transformations are sampled from $\\mathcal{T}_{2}$ : permutations on which $f_{2}^{\\star}$ is invariant. In each case, the test set size is 1000. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Basic Experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present results for a basic experiment on learning Boolean functions on the hypercube $\\{\\pm1\\}^{d}$ .   \nWe consider a uniform distribution $D$ over $\\{\\pm1\\}^{d}$ and two target functions: (1) $f_{1}^{\\star}(x)=\\Pi_{i=1}^{d}x_{i}$ , the parity function, and (2) $\\begin{array}{r}{f_{2}^{\\star}(x)=\\mathrm{sign}(\\sum_{j=0}^{2}(\\Pi_{i=1}^{d/3}x_{j(d/3)+i}))}\\end{array}$ , a majority-of-subparities function.   \nWe consider transformations $\\tau_{1}$ , $\\mathcal{T}_{2}$ under which $f_{1}^{\\star},f_{2}^{\\star}$ are invariant, respectively (see Section 2).   \nSince $D$ is uniform, note that for any $\\hat{h}$ : $\\mathrm{sup}_{T\\in\\mathcal{T}}\\mathrm{err}(\\bar{h},\\bar{T}(D_{f^{\\star}}))=\\mathrm{err}(\\hat{h},D_{f^{\\star}})$ . ", "page_idx": 8}, {"type": "text", "text": "Algorithms. We use a two-layer feed-forward neural network architecture with 512 hidden units as our hypothesis class $\\mathcal{H}$ . We use the squared loss and consider two training algorithms. First, the baseline is running standard mini-batch SGD on training examples. Second, as a heuristic to implement Equation (2), we run mini-batch SGD on training examples and permutations of them. Specifically, in each step we replace correctly classified training examples in a mini-batch with random permutations of them (drawn from $\\tau$ ), and then perform an SGD update on this modified mini-batch. We set the mini-batch size to 1 and the learning rate to 0.01. Results are averaged over 5 runs with different seeds and are reported in Figure 1. We ran experiments on freely available Google CoLab T4 GPUs, and used Python and PyTorch to implement code. ", "page_idx": 8}, {"type": "text", "text": "7 Related Work and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Covariate Shift, Domain Adaptation, Transfer Learning. There is substantial literature studying theoretical guarantees for learning when there is a \u201csource\u201d distribution $P$ and a \u201ctarget\u201d distribution $Q$ [see e.g., survey by Redko et al., 2020, Quinonero-Candela et al., 2008]. Many of these works explore structural relationships between $P$ and $Q$ using various divergence measures (e.g., total variation distance or KL divergence), sometimes incorporating the structure of the hypothesis class $\\mathcal{H}$ [e.g., Ben-David et al., 2010, Hanneke and Kpotufe, 2019]. Sometimes access to unlabeled (or few labeled) samples from $Q$ is assumed. Our work differs from this line of work by expressing the structural relationship between $P$ and $Q$ in terms of a transformation $T$ where $Q=T(P)$ . ", "page_idx": 8}, {"type": "text", "text": "Distributionally Robust Optimization. With roots in optimization literature [see e.g., Ben-Tal et al., 2009, Shapiro, 2017], this framework has been further studied recently in the machine learning literature [see e.g., Duchi and Namkoong, 2021]. The goal is to learn a predictor $\\hat{h}$ that minimizes the worst-case error $\\operatorname*{sup}_{Q\\in\\mathcal{P}}\\operatorname{err}(\\hat{h},Q)$ , where $\\mathcal{P}$ is a collection of distributions. Most prior work adopting this framework has focused on distributions $\\mathcal{P}$ that are close to a \u201csource\u201d distribution $\\mathcal{D}$ in some divergence measure [e.g., $f$ -divergences Namkoong and Duchi, 2016]. Instead of relying on divergence measures, our work describes the collection $\\mathcal{P}$ through data transformations $\\tau$ of $\\mathcal{D}$ : $\\left\\{T(\\mathcal{D})\\right\\}_{T\\in\\mathcal{T}}$ which may be operationally simpler. ", "page_idx": 8}, {"type": "text", "text": "Multi-Distribution Learning. This line of work focuses on the setting where there are $k$ arbitrary distributions $\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{k}$ to be learned uniformly well, where sample access to each distribution $\\mathcal{D}_{i}$ is provided [see e.g., Haghtalab et al., 2022]. In contrast, our setting involves access to a single distribution $\\mathcal{D}$ and transformations $T_{1},\\ldots,T_{k}$ , that together describe the target distributions: $T_{1}({\\bar{D}}),\\dots,T_{k}(D)$ . From a sample complexity standpoint, multi-distribution learning requires sample complexity scaling linearly in $k$ while in our case it is possible to learn with sample complexity scaling logarithmically in $k$ (see Theorem 3.1 and Lemma B.1). The lower sample complexity in our approach is primarily due to the assumption that the transformations $T_{1},\\ldots,T_{k}$ are known in advance, allowing the learner to generate $k$ samples from a single draw of $\\mathcal{D}$ . In contrast, in multi-distribution learning, the learner pays for $k$ samples in order to see one sample from each of $\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{k}$ . Therefore, while the sample complexity is lower in our setting, this advantage arises from the additional information/structure provided rather than an inherent improvement over the more general setting of multi-distribution learning. From an algorithmic standpoint, our reduction algorithms employ similar techniques based on regret minimization and solving zero-sum games [Freund and Schapire, 1997]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Invariant Risk Minimization (IRM). This is another formulation addressing domain generalization or learning a predictor that performs well across different environments [Arjovsky et al., 2019]. One main difference from our work is that in the IRM framework training examples from different environments are observed and no explicit description of the transformations is provided. Furthermore, to argue about generalization on environments unseen during training, a structural causal model is considered. Recent works have highlighted some drawbacks of IRM [Rosenfeld et al., 2021, Kamath et al., 2021]. For example, how in some cases ERM outperforms IRM on out-of-distribution generalization, and the sensitivity of IRM to finite empirical samples vs. infinite population samples. ", "page_idx": 9}, {"type": "text", "text": "Data Augmentation. A commonly used technique in learning under invariant transformations is data augmentation, which involves adding transformed data into the training set and training a model with the augmented data. Theoretical guarantees of data augmentation have received significant attention recently [see e.g., Dao et al., 2019, Chen et al., 2020, Lyle et al., 2020, Shao et al., 2022, Shen et al., 2022]. In this line of research, it is common to assume that the transformations form a group, and the learning goal is to achieve good performance under the \u201csource\u201d distribution by leveraging knowledge of the invariant transformations structure. In contrast, our work does not make the group assumption over transformations, and our goal is to learn a model with low loss under all possible \u201ctarget\u201d distributions parameterized by transformations of the \u201csource\u201d distribution. ", "page_idx": 9}, {"type": "text", "text": "Multi-Task Learning. Ben-David and Borbely [2008] studied conditions underwhich a set of transformations $\\tau$ can help with multi-task learning, assuming that $\\tau$ forms a group and that $\\mathcal{H}$ is closed under $\\tau$ . Our work does not make such assumptions, and studies a different learning objective. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Suriya Gunasekar for insightful discussions at early stages of this work. This work was done in part under the NSF-Simons Collaboration on the Theoretical Foundations of Deep Learning. OM was supported by a FODSI-Simons postdoctoral fellowship at UC Berkeley. HS was supported in part by Harvard CMSA. This work was conducted primarily while HS was at TTIC and supported in part by the National Science Foundation under grants CCF-2212968 and ECCS-2216899, and by the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003. The views expressed in this work do not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "E. Abbe, S. Bengio, A. Lotf,i and K. Rizk. Generalization on the unseen, logic reasoning and degree curriculum. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 31\u201360. PMLR, 2023. URL https://proceedings.mlr.press/v202/abbe23a.html. A. Agarwal and T. Zhang. Minimax regret optimization for robust machine learning under distribution shift. In P.-L. Loh and M. Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 2704\u20132729. PMLR, 02\u201305 Jul 2022. URL https://proceedings.mlr.press/v178/agarwal22b.html. ", "page_idx": 9}, {"type": "text", "text": "N. Alon, S. Hanneke, R. Holzman, and S. Moran. A theory of PAC learnability of partial concept classes. In 62nd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2021, Denver, CO, USA, February 7-10, 2022, pages 658\u2013671. IEEE, 2021. doi: 10.1109/FOCS52979. 2021.00070. URL https://doi.org/10.1109/FOCS52979.2021.00070.   \nN. Alon, A. Gonen, E. Hazan, and S. Moran. Boosting simple learners. TheoretiCS, 2, 2023. doi: 10.46298/THEORETICS.23.8. URL https://doi.org/10.46298/theoretics.23.8.   \nD. Angluin. Queries and concept learning. Mach. Learn., 2(4):319\u2013342, 1987. doi: 10.1007/ BF00116828. URL https://doi.org/10.1007/BF00116828.   \nM. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. CoRR, abs/1907.02893, 2019. URL http://arxiv.org/abs/1907.02893.   \nP. L. Bartlett, N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks. J. Mach. Learn. Res., 20:63:1\u201363:17, 2019. URL http://jmlr.org/papers/v20/17-612.html.   \nS. Ben-David and R. S. Borbely. A notion of task relatedness yielding provable multiple-task learning guarantees. Machine learning, 73:273\u2013287, 2008.   \nS. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different domains. Mach. Learn., 79(1-2):151\u2013175, 2010. doi: 10.1007/S10994-009-5152-4. URL https://doi.org/10.1007/s10994-009-5152-4.   \nA. Ben-Tal, L. E. Ghaoui, and A. Nemirovski. Robust Optimization, volume 28 of Princeton Series in Applied Mathematics. Princeton University Press, 2009. ISBN 978-1-4008-3105-0. doi: 10.1515/9781400831050. URL https://doi.org/10.1515/9781400831050.   \nA. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. Journal of the Association for Computing Machinery, 36(4):929\u2013965, 1989a.   \nA. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the vapnikchervonenkis dimension. J. ACM, 36(4):929\u2013965, 1989b. doi: 10.1145/76359.76371. URL https://doi.org/10.1145/76359.76371.   \nY. Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Communications on pure and applied mathematics, 44(4):375\u2013417, 1991.   \nT. B. Brown, D. Man\u00e9, A. Roy, M. Abadi, and J. Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.   \nN. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006. ISBN 978-0-521-84108-5. doi: 10.1017/CBO9780511546921. URL https://doi.org/ 10.1017/CBO9780511546921.   \nS. Chen, E. Dobriban, and J. H. Lee. A group-theoretic framework for data augmentation. Journal of Machine Learning Research, 21:1\u201371, 2020.   \nC. Cortes, S. Greenberg, and M. Mohri. Relative deviation learning bounds and generalization with unbounded loss functions. Ann. Math. Artif. Intell., 85(1):45\u201370, 2019. doi: 10.1007/ S10472-018-9613-Y. URL https://doi.org/10.1007/s10472-018-9613-y.   \nA. Daniely and S. Shalev-Shwartz. Optimal learners for multiclass problems. In M. Balcan, V. Feldman, and C. Szepesv\u00e1ri, editors, Proceedings of The 27th Conference on Learning Theory, COLT 2014, Barcelona, Spain, June 13-15, 2014, volume 35 of JMLR Workshop and Conference Proceedings, pages 287\u2013316. JMLR.org, 2014. URL http://proceedings.mlr.press/v35/ daniely14b.html.   \nT. Dao, A. Gu, A. Ratner, V. Smith, C. De Sa, and C. R\u00e9. A kernel theory of modern data augmentation. In International Conference on Machine Learning, pages 1528\u20131537. PMLR, 2019.   \nJ. C. Duchi and H. Namkoong. Learning models with uniform performance via distributionally robust optimization. The Annals of Statistics, 49(3):1378\u20131406, 2021.   \nA. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. A general lower bound on the number of examples needed for learning. Information and Computation, 82(3):247\u2013261, 1989.   \nL. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry. Exploring the landscape of spatial robustness. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 1802\u20131811. PMLR, 2019. URL http://proceedings.mlr.press/v97/engstrom19a.html.   \nD. J. Foster, S. Kale, H. Luo, M. Mohri, and K. Sridharan. Logistic regression: The importance of being improper. In S. Bubeck, V. Perchet, and P. Rigollet, editors, Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018, volume 75 of Proceedings of Machine Learning Research, pages 167\u2013208. PMLR, 2018. URL http://proceedings.mlr.press/ v75/foster18a.html.   \nY. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 55(1):119\u2013139, 1997. doi: 10.1006/JCSS.1997.1504. URL https://doi.org/10.1006/jcss.1997.1504.   \nI. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Bengio. Generative adversarial networks. Commun. ACM, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622.   \nN. Haghtalab, M. I. Jordan, and E. Zhao. On-demand sampling: Learning optimally from multiple distributions. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 02917acec264a52a729b99d9bc857909-Abstract-Conference.html.   \nS. Hanneke and S. Kpotufe. On the value of target data in transfer learning. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 9867\u20139877, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/b91f4f4d36fa98a94ac5584af95594a0-Abstract.html.   \nD. Hendrycks and T. G. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/ forum?id $\\cdot^{=}$ HJz6tiCqYm.   \nP. Kamath, A. Tangella, D. J. Sutherland, and N. Srebro. Does invariant risk minimization capture invariance? In A. Banerjee and K. Fukumizu, editors, The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research, pages 4069\u20134077. PMLR, 2021. URL http://proceedings.mlr.press/v130/kamath21a.html.   \nD. Karmon, D. Zoran, and Y. Goldberg. Lavan: Localized and visible adversarial noise. In International Conference on Machine Learning, pages 2507\u20132515. PMLR, 2018.   \nC. Lyle, M. van der Wilk, M. Kwiatkowska, Y. Gal, and B. Bloem-Reddy. On the benefits of invariance in neural networks. arXiv preprint arXiv:2005.00178, 2020.   \nA. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.   \nO. Montasser, S. Hanneke, and N. Srebro. Vc classes are adversarially robustly learnable, but only improperly. In A. Beygelzimer and D. Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Research, pages 2512\u20132530, Phoenix, USA, 25\u201328 Jun 2019. PMLR.   \nS. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial perturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 86\u201394. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.17. URL https://doi.org/10.1109/CVPR.2017.17.   \nH. Namkoong and J. C. Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 2208\u20132216, 2016. URL https://proceedings.neurips.cc/paper/2016/ hash/4588e674d3f0faf985047d4c3f13ed0d-Abstract.html.   \nJ. Quinonero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset shift in machine learning. Mit Press, 2008.   \nI. Redko, E. Morvant, A. Habrard, M. Sebban, and Y. Bennani. A survey on domain adaptation theory. CoRR, abs/2004.11829, 2020. URL https://arxiv.org/abs/2004.11829.   \nE. Rosenfeld, P. K. Ravikumar, and A. Risteski. The risks of invariant risk minimization. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id $\\cdot$ BbNIbVPJ-42.   \nN. Sauer. On the density of families of sets. J. Comb. Theory, Ser. A, 13(1):145\u2013147, 1972. doi: 10.1016/0097-3165(72)90019-2. URL https://doi.org/10.1016/0097-3165(72)90019-2.   \nH. Shao, O. Montasser, and A. Blum. A theory of pac learnability under transformation invariances. Advances in Neural Information Processing Systems, 35:13989\u201314001, 2022.   \nA. Shapiro. Distributionally robust stochastic programming. SIAM J. Optim., 27(4):2258\u20132275, 2017. doi: 10.1137/16M1058297. URL https://doi.org/10.1137/16M1058297.   \nR. Shen, S. Bubeck, and S. Gunasekar. Data augmentation as feature manipulation. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 19773\u201319808. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr. press/v162/shen22a.html.   \nV. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16(2):264\u2013280, 1971.   \nV. Vapnik and A. Chervonenkis. Theory of Pattern Recognition. Nauka, Moscow, 1974.   \nA. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson. Universal and transferable adversarial attacks on aligned language models. CoRR, abs/2307.15043, 2023. doi: 10.48550/ARXIV.2307.15043. URL https://doi.org/10.48550/arXiv.2307.15043. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Uniform Convergence ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We can use tools from VC theory [Vapnik and Chervonenkis, 1971, 1974] to obtain uniform convergence guarantees that will allow us to establish our learning guarantees and sample complexity bounds in the remainder of the paper. The starting point is a simple but key observation concerning the hypothesis class $\\mathcal{H}$ and the collection of transformations $\\tau$ . Specifically, consider the composition of $\\mathcal{H}$ with $\\tau$ defined as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{H}\\circ\\mathcal{T}:=\\left\\{h\\circ T:h\\in\\mathcal{H},T\\in\\mathcal{T}\\right\\},\\ \\mathrm{where}\\left(h\\circ T\\right)(x)=h(T(x))\\ \\forall x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We next apply VC theory to the class $\\mathcal{H}\\circ\\mathcal{T}$ to obtain our uniform convergence guarantees. Formally, Proposition A.1. For any class $\\mathcal{H}$ , any collection of transformations $\\tau$ , any distribution $\\mathcal{D}$ over $\\mathcal X\\times\\mathcal X$ , and any $m\\in\\mathbb{N}$ , with probability at least $1-\\delta$ over $S\\sim\\mathcal{D}^{m};\\,\\forall h\\in\\mathcal{H},\\forall T\\in T_{*}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\mathrm{err}(h,T(S))-\\mathrm{err}(h,T(\\mathcal{D}))|\\leq c\\sqrt{\\frac{\\mathrm{vc}(\\mathcal{H}\\circ\\mathcal{T})+\\log(1/\\delta)}{m}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Since the composition $\\mathcal{H}\\circ\\mathcal{T}$ is a hypothesis class consisting of functions $h\\circ T$ where $h\\in{\\mathcal{H}},T\\in{\\mathcal{T}}$ , the claim follows from the definition of VC dimension and uniform convergence guarantees for VC classes [Vapnik and Chervonenkis, 1971, 1974]. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Proposition A.2 (Optimistic Rate). For any class $\\mathcal{H}$ , any collection of transformations $\\tau$ , any distribution $\\mathcal{D}$ , and any $m\\,\\in\\,\\mathbb{N}$ , letting $\\begin{array}{r}{B(m,\\delta)\\,:=\\,\\frac{1}{m}\\left(\\mathrm{vc}(\\mathcal{H}\\circ\\mathcal{T})\\log\\left(\\frac{2e m}{\\mathrm{vc}(\\mathcal{H}\\circ\\mathcal{T})}\\right)+\\log(4/\\delta)\\right)}\\end{array}$ with probability at least $1-\\delta$ over $S\\sim\\mathcal{D}^{m};\\,\\forall h\\in\\mathcal{H},\\forall T\\in\\mathcal{T},$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{err}(h,T(\\mathcal{D}))\\leq\\mathrm{err}(h,T(S))+2\\sqrt{\\mathrm{err}(h,T(S))B(m,\\delta)}+4B(m,\\delta),}\\\\ &{\\mathrm{err}(h,T(S))\\leq\\mathrm{err}(h,T(\\mathcal{D}))+2\\sqrt{\\mathrm{err}(h,T(\\mathcal{D}))B(m,\\delta)}+4B(m,\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. The claim follows from applying relative deviation bounds, or optimistic rates, for the composition class $\\mathcal{H}\\circ\\mathcal{T}$ [see e.g., Corollary 7 in Cortes et al., 2019]. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "B Bounding the VC dimension of $\\mathcal{H}$ composed with $\\tau$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Given the relevance of $\\operatorname{vc}({\\mathcal{H}}\\circ T)$ in our theoretical study, in this section we explore the relationship between $\\operatorname{vc}(\\mathcal{H}\\circ\\mathcal{T})$ and $\\operatorname{vc}({\\mathcal{H}})$ which we believe can be helpful in interpreting our results and comparing them with the sample complexity of standard PAC learning [which is controlled by $\\operatorname{vc}({\\mathcal{H}})$ Blumer et al., 1989a, Ehrenfeucht et al., 1989]. To this end, we consider below a few general cases and prove bounds on $\\operatorname{vc}({\\mathcal{H}}\\circ T)$ in terms of $\\operatorname{vc}({\\mathcal{H}})$ and some form of capacity control for $\\tau$ . These results may be of independent interest. ", "page_idx": 13}, {"type": "text", "text": "Finitely many transformations. When $\\tau$ is a finite collection of transformations, we can bound $\\operatorname{vc}({\\mathcal{H}}\\circ T)$ from above by $O(\\mathrm{vc}(\\mathcal{H})+\\log|\\mathcal{T}|)$ using the Sauer-Shelah-Perels Lemma [Sauer, 1972], ", "page_idx": 13}, {"type": "text", "text": "Lemma B.1. For any class $\\mathcal{H}$ and any finite collection $\\tau$ $\\mathrm{,~vc}(\\mathcal{H}\\circ\\mathcal{T})\\leq O(\\mathrm{vc}(\\mathcal{H})+\\log|\\mathcal{T}|).$ ", "page_idx": 13}, {"type": "text", "text": "Proof. Consider an arbitrary set of points $P=\\{x_{1},\\ldots,x_{m}\\}\\subseteq{\\mathcal{X}}$ . To bound $\\operatorname{vc}(\\mathcal{H}\\circ\\mathcal{T})$ from above, it suffices to bound the number of behaviors when projecting the function class $\\mathcal{H}\\circ\\mathcal{T}$ on $P$ , defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{H}\\circ\\mathcal{T}}(P):=\\left\\{(h(T(x_{1})),\\ldots,h(T(x_{m}))):h\\in\\mathcal{H},T\\in\\mathcal{T}\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Observe that ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\Pi_{\\mathcal{H}\\circ\\mathcal{T}}(P)|\\leq\\sum_{T\\in\\mathcal{T}}|\\{(h(T(x_{1})),\\dots,h(T(x_{m}))):h\\in\\mathcal{H}\\}|\\leq|\\mathcal{T}|\\left(\\frac{e m}{\\mathrm{vc}(\\mathcal{H})}\\right)^{\\mathrm{vc}(\\mathcal{H})},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality follows from the definition of $\\Pi_{\\mathcal{H}\\circ\\mathcal{T}}(P)$ and the second inequality follows from the Sauer-Shelah-Perels Lemma [Sauer, 1972]. Solving for $m$ such that the above bound is less than $2^{m}$ , implies that $\\operatorname{vc}(\\mathcal{H}\\circ\\mathcal{T})\\leq O(\\operatorname{vc}(\\mathcal{H})+\\log|\\mathcal{T}|)$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Linear transformations. Consider $\\tau$ being a (potentially infinite) collection of linear transformations. For example, in vision tasks, this includes transforming images through rotations, translations, maskings, adding random noise (or any fixed a-priori arbitrary noise), and their compositions. Surprisingly, for a broad range of hypothesis classes $\\mathcal{H}$ (including linear predictors and neural networks), we can show that $\\mathrm{vc}(\\mathcal{H}\\circ T)\\leq\\mathrm{vc}(\\mathcal{H})$ without incurring any dependence on the complexity of $\\tau$ . Specifically, the result applies to any function class $\\mathcal{H}$ that consists of a linear mapping followed by an arbitrary mapping. This includes feed-forward neural networks with any activation function, and modern neural network architectures (e.g., CNNs, ResNets, Transformers). ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2. For any collection of linear transformations $\\tau$ and any hypothesis class of the form $\\mathcal{H}=\\left\\{f\\circ W:\\mathbb{R}^{d}\\overset{}{\\to}\\mathcal{V}\\ |\\ W\\in\\mathbb{R}^{k\\times d}\\wedge f:\\mathbb{R}^{\\bar{k}}\\to\\mathcal{V}\\right\\}$ , it holds that $\\mathrm{vc}(\\mathcal{H}\\circ\\mathcal{T})\\leq\\mathrm{vc}(\\mathcal{H})$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. By definition of the VC dimension, it suffices to show that $\\mathcal{H}\\circ\\mathcal{T}\\subseteq\\mathcal{H}$ . To this end, consider an arbitrary $h=f\\circ W\\in\\mathcal{H}$ where $W:\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}^{k}$ is a linear map and $f:\\mathbb{R}^{k}\\rightarrow\\mathcal{V}$ is an arbitrary map (see definition of $\\mathcal{H}$ in the lemma statement), and consider an arbitrary linear transformation $T\\in\\mathcal T$ . Then, observe that for each $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n(h\\circ T)(x)=(f\\circ W)(T(x))=f(W(T(x)))=f(T^{*}(W)(x)),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last equality follows from Riesz Representation theorem and $T^{*}$ is the adjoint transformation of $T$ . Thus, we have shown that there exists $\\tilde{W}=T^{*}(W)$ such that $(f\\circ W)(T({\\bar{x}}))=(f\\circ{\\tilde{W}})(x)$ for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . Therefore, $\\mathcal{H}\\circ\\mathcal{T}\\subseteq\\mathcal{H}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Transformations on the Boolean hypercube. When the instance space $\\mathcal{X}=\\left\\{0,1\\right\\}^{d}$ is the Boolean hypercube, we can bound the VC dimension of $\\mathcal{H}\\circ\\mathcal{T}$ from above by the sum of the VC dimension of $\\mathcal{H}$ and the sum of the VC dimensions of $\\{{\\mathcal{T}}_{i}\\}_{i=1}^{d}$ where each $\\overline{{J_{i}}}=\\{x\\mapsto T(x)_{i}:T\\in\\mathcal{T}\\}$ is a function class resulting from restricting transformations $T:\\left\\{0,1\\right\\}^{d}\\rightarrow\\left\\{0,1\\right\\}^{d}\\in{\\mathcal{T}}$ to output only the $i$ th bit. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.3. When $\\mathcal{X}=\\left\\{0,1\\right\\}^{d}$ , for any hypothesis class $\\mathcal{H}$ and any collection of transformations $\\begin{array}{r}{T,\\;\\mathrm{vc}(\\mathcal{H}\\circ T)\\leq O(\\log d)(\\mathrm{vc}(\\mathcal{H})+\\sum_{i=1}^{d}\\mathrm{vc}(\\mathcal{T}_{i}))}\\end{array}$ , where each $\\mathcal{T}_{i}=\\{x\\mapsto T(x)_{i}:T\\in\\mathcal{T}\\}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Every function $h\\circ T\\in\\mathcal{H}\\circ T$ can be viewed as $x\\,\\mapsto\\,h(T(x)_{1},\\ldots,T(x)_{d})$ , which is a composition of $h$ with $d$ Boolean functions $T(\\cdot)_{1},\\cdot\\cdot\\cdot,T(\\cdot)_{d}:\\mathscr{X}\\rightarrow\\{0,1\\}$ where each $T(\\cdot)_{i}$ is the restriction of transformation $T$ to the $i$ th coordinate. The claim then follows from a direct application of Proposition 4.9 in Alon et al. [2023], which itself generalizes a classical result due to Blumer et al. [1989b] bounding the VC dimension of composed function classes. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "C Proofs for Section 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 2.2. We note that the proof follows a standard no-free-lunch argument for VC classes, where the game will be to guess the support of a distribution. ", "page_idx": 14}, {"type": "text", "text": "Let $x_{1},\\ldots,x_{3k}$ be arbitrary points. For each $P\\,\\subseteq\\,[3k]$ where $|P|=k$ , define a transformation $T_{P}$ that maps $x_{1},\\ldots,x_{3k}$ to some new and unique points $T_{P}(x_{1}),\\dots,T_{P}(x_{3k})$ . Then, define classifier $h_{P}$ to be positive everywhere, except on the points $\\{T_{P}(x_{i})\\}_{i\\in P}$ which are labeled negative. Let $\\begin{array}{r}{\\mathcal{X}\\,=\\,\\{{x_{1},\\dots,x_{3k}}\\}\\bigcup_{P}\\left\\{{T_{P}(x_{1}),\\dots,T_{P}(x_{3k})}\\right\\}}\\end{array}$ , $\\mathcal{\\tilde{H}}\\,=\\,\\{h_{P}:\\bar{P^{\\}}\\subseteq[3k],|P|=k\\}$ , and $\\tau=$ $\\{T_{P}:P\\subseteq[3k],|P|=k\\}$ . ", "page_idx": 14}, {"type": "text", "text": "It is easy to see that $\\operatorname{vc}(\\mathcal{H})=1$ , since classifiers in $\\mathcal{H}$ operate in different parts of $\\mathcal{X}$ . Furthermore, $\\operatorname{vc}(\\mathcal{H}\\circ\\mathcal{T})\\ \\geq\\ k$ where we can shatter $x_{1},\\ldots,x_{k}$ with $\\mathcal{H}\\circ\\mathcal{T}$ as follows: for each $y_{1},\\ldots,y_{k}$ , let $I\\;=\\;\\{i\\in[k]:y_{i}=-1\\}$ and $P\\,=\\,I\\cup\\{j:k+1\\leq j\\leq2k-|I|\\}$ , then $(h_{P}\\circ T_{P})(x_{i})\\ =$ $h_{P}(T_{P}(x_{i}))=\\dot{y_{i}}$ for all $i\\in[k]$ . ", "page_idx": 14}, {"type": "text", "text": "Consider now a family of distributions $\\{\\mathcal{D}_{P}:P\\subseteq[3k],|P|=k\\}$ over $\\mathcal X\\times\\mathcal Y$ where each $\\mathcal{D}_{P}$ is uniform over $2k$ points $\\{(x_{i},+1)\\}_{i\\notin P}$ . For each $P\\subseteq[3k]$ where $|P|=k$ , observe that by definitions of $\\ensuremath{\\mathcal{D}_{P}},\\ensuremath{\\mathcal{H}_{\\!\\:}},\\ensuremath{\\mathcal{T}}$ $\\begin{array}{r}{\\mathcal{H},\\mathcal{T},\\operatorname*{sup}_{T\\in\\mathcal{T}}\\operatorname{err}(h_{P},T(\\mathcal{\\bar{D}}_{P}))=0}\\end{array}$ since $h_{P}$ only labels the points $\\{T_{P}(x_{i})\\}_{i\\in P}$ negative and $\\{x_{i}\\}_{i\\in P}$ are not in the support of $\\mathcal{D}_{P}$ . That is to say, our lower bound holds in the realizable setting where ${\\mathsf{O P T}}_{\\infty}=0$ (see Equation (1)). ", "page_idx": 14}, {"type": "text", "text": "Next, consider an arbitrary proper learning rule $\\mathbb{A}:({\\mathcal{X}}\\times{\\mathcal{Y}})^{*}\\to{\\mathcal{H}}$ . For a distribution $\\mathcal{D}_{P}$ chosen uniformly at random and upon receiving a random sample $S\\sim\\mathcal{D}_{P}^{k}$ , A needs to correctly guess which points from $\\{x_{1},\\ldots,x_{3k}\\}\\setminus S$ lie in the support of $\\mathcal{D}_{P}$ in order to choose an appropriate $h\\in\\mathcal H$ with small error. However, since the support is chosen uniformly at random, A will most likely incorrectly guess a constant fraction of the support, leading to a constant error. This is a standard argument [see e.g., Montasser et al., 2019, Alon et al., 2021], but we repeat it below for completeness. ", "page_idx": 15}, {"type": "text", "text": "Fix an arbitrary sequence $S\\;\\in\\;\\{(x_{1},+1),\\ldots,(x_{3k},+1)\\}^{k}$ . Denote by $E_{S}$ the event that $\\textit{S}\\in$ $\\operatorname{supp}(\\mathcal{D}_{P})$ for a distribution $\\mathcal{D}_{P}$ that is picked uniformly at random. Next, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{P}{\\mathbb{E}}\\left[\\underset{T\\in\\mathcal{T}}{\\operatorname*{sup}}\\exp\\left(\\mathbb{A}(S),T(\\mathcal{D}_{P})\\right)\\vert E_{S}\\right]\\geq\\underset{P}{\\mathbb{E}}\\left[\\mathrm{err}(\\mathbb{A}(S),T_{P}(\\mathcal{D}_{P}))\\vert E_{S}\\right]}&{}\\\\ &{\\geq\\underset{P}{\\mathbb{E}}\\left[\\frac{1}{2k}\\underset{i\\neq P}{\\sum}1[\\mathbb{A}(S)(T_{P}(x_{i}))\\neq+1]\\vert E_{S}\\right]}\\\\ &{\\geq\\frac{1}{2}\\underset{P}{\\mathbb{E}}\\left[\\frac{1}{k}\\underset{i\\neq P\\wedge(x_{i},+1)\\notin S}{\\sum}1[\\mathbb{A}(S)(T_{P}(x_{i}))\\neq+1]\\vert E_{S}\\right]}\\\\ &{\\geq\\frac{1}{4},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality follows from the fact that $\\mathbb{A}(S)\\in{\\mathcal{H}}$ and that the remaining (at least $k$ ) points that are not in $S$ but in $\\operatorname{supp}(\\mathcal{D}_{P})$ are chosen uniformly at random, because $\\mathcal{D}_{P}$ is chosen randomly. From the above, by law of total expectation, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\;S\\sim\\mathcal{D}_{P}^{k}}\\left[\\operatorname*{sup}_{T\\in\\mathcal{T}}\\mathrm{err}(\\mathbb{A}(S),T(\\mathcal{D}_{P}))\\right]\\geq\\frac{1}{4}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the probabilistic method, this means there exists $P^{*}$ such that $\\begin{array}{r l r}{\\mathbb{E}_{S\\sim\\mathcal{D}_{P^{*}}^{k}}\\left[\\operatorname*{sup}_{T\\in\\mathcal{T}}\\operatorname{err}(\\mathbb{A}(S),T(\\mathcal{D}_{P}))\\right]}&{\\geq}&{\\frac{1}{4}}\\end{array}$ . Using a variant of Markov\u2019s inequality, this implies that $\\mathrm{Pr}_{S\\sim\\mathcal{D}_{P^{*}}^{k}}$ $\\begin{array}{r}{\\left[\\operatorname*{sup}_{T\\in\\mathcal{T}}\\operatorname{err}(\\mathbb{A}(S),T(\\mathcal{D}_{P}))>\\frac{1}{8}\\right]\\geq\\frac{1}{7}.}\\end{array}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "D Proofs for Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition D.1. For any class $\\mathcal{H}$ , any ERM for $\\mathcal{H}$ , any collection of transformations $\\tau$ , any distribution $\\mathcal{D}$ such that ${\\mathsf{O P T}}_{\\infty}=0,$ , and any $\\varepsilon,\\delta\\,\\in\\,(0,{^1\\mathord{/}{\\vphantom{^12}}2})$ , with probability at least $1-\\delta$ over S \u223cDm(\u03b5,\u03b4), where m(\u03b5, \u03b4) = O vc(H\u25e6T ) log(\u03b51/\u03b5)+log(1/\u03b4) , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall T\\in T:\\mathrm{err}(\\hat{h},T(\\mathcal{D}))\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\hat{h}$ is the output predictor of running ERM on the inflated dataset $\\begin{array}{r l}{\\boldsymbol{\\mathcal{T}}(\\boldsymbol{S})}&{{}=}\\end{array}$ $\\{(T(x),y):(x,y)\\in S\\land T\\in T\\}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition $D.I$ . Since ${\\mathsf{O P T}}_{\\infty}\\;=\\;0$ , i.e., there exists $h^{\\star}\\ \\in\\ {\\mathcal{H}}$ such that $\\forall T\\;\\in\\;\\tau:$ $\\mathrm{err}(h^{\\star},T({\\mathcal D}))\\,=\\,0$ , and since $\\hat{h}$ is the output predictor of running ERM on the inflated dataset ${\\mathcal{T}}(S)=\\{(T(x),y):(x,y)\\in S\\land T\\in{\\mathcal{T}}\\}$ , it follows that $\\forall T\\in{\\mathcal{T}}:\\operatorname{err}({\\hat{h}},T(S))=0$ . Thus, by invoking the optimistic generalization guarantee (Proposition A.2), with probability at least $1-\\delta$ over $S\\sim{\\cal D}^{m(\\varepsilon,\\delta)}$ : $(\\forall h\\in{\\mathcal{H}})(\\forall T\\in{\\mathcal{T}}):\\operatorname{err}(h,T(S))\\Rightarrow\\operatorname{err}(h,T({\\mathcal{D}}))\\leq\\varepsilon.$ . Since $\\hat{h}\\in\\mathcal{H}$ , it follows that $\\forall T\\in{\\mathcal{T}}:\\operatorname{err}({\\hat{h}},T({\\mathcal{D}}))\\leq\\varepsilon$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma D.2. Let $S=\\{(x_{1},y_{1}),\\dots,(x_{m},y_{m})\\}$ be an arbitrary dataset. For any distribution $Q$ over $\\tau$ and any predictor $h$ , define the loss function $\\ell_{S}(h,Q)=1-\\mathbb{E}_{T\\sim Q}\\operatorname{err}(h,T(S))$ . Then for any sequence of predictors $h_{1},\\ldots,h_{R}$ , running Multiplicative Weights with $\\eta=\\sqrt{8\\ln\\left|{\\mathcal{T}}\\right|/R}\\left(s e\\right.$ e Algorithm $^{\\,l}$ ) produces a sequence of distributions $Q_{1},\\dots,Q_{R}$ over $\\tau$ that satisfy ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{R}\\sum_{r=1}^{R}\\ell_{S}(h_{r},Q_{r})\\leq\\operatorname*{min}_{T\\in{\\mathcal T}}\\frac{1}{R}\\sum_{r=1}^{R}\\ell_{S}(h_{r},T)+\\sqrt{\\frac{\\ln|{\\mathcal T}|}{2R}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma $D.2$ . The proof follows directly from considering a two-player game, where the $\\tau$ -player plays mixed strategies (distributions over $\\tau$ ) $Q_{1},\\ldots,Q_{R}$ against predictors $h_{1},\\ldots,h_{R}$ played by an arbitrary learning algorithm A, and in each round the $\\tau$ -player incurs loss $\\ell_{S}(h_{r},Q_{r})=$ $\\tilde{1}-\\mathbb{E}_{T\\sim Q_{r}}\\operatorname{err}(h_{r},\\dot{T}(S))$ . Then, the regret guarantee of Multiplicative Weights [see e.g., Theorem 2.2 in Cesa-Bianchi and Lugosi, 2006] implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{R}\\sum_{r=1}^{R}\\ell_{S}(h_{r},Q_{r})\\leq\\operatorname*{min}_{T\\in\\mathcal{T}}\\frac{1}{R}\\sum_{r=1}^{R}\\ell_{S}(h_{r},T)+\\sqrt{\\frac{\\ln|\\mathcal{T}|}{2R}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "E Proofs for Section 5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 5.1. The proof follows from the definition of \u02c6h (Equation (8)) and using uniform convergence bounds (Proposition A.1). Let $h^{\\star}\\in\\mathcal{H}$ be an a-priori fixed predictor (independent of sample $S$ ) attaining ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h^{\\star}\\in\\mathcal{H}}\\operatorname*{sup}_{T\\in\\mathcal{T}}\\left\\{\\mathrm{err}(h^{\\star},T(\\mathcal{D}))-0\\mathsf{P}\\mathsf{T}_{T}\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "or is \u03b5-close to it. By setting m(\u03b5, \u03b4) = m(\u03b5, \u03b4) = O vc(H\u25e6T )\u03b5+2log(1/\u03b4) and invoking Proposition A.1, we have the guarantee that with probability at least $1-\\delta$ over $S\\sim{\\cal D}^{m(\\varepsilon,\\delta)}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\forall h\\in\\mathcal{H})\\left(\\forall T\\in\\mathcal{T}\\right):\\left|\\mathrm{err}(h,T(S))-\\mathrm{err}(h,T(\\mathcal{D}))\\right|\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\hat{h},h^{\\star}\\in\\mathcal{H}$ , the inequality above implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall T\\in\\mathcal{T}:\\ \\mathrm{err}(\\hat{h},T(\\mathcal{D}))\\leq\\mathrm{err}(\\hat{h},T(S))+\\varepsilon.}\\\\ &{\\forall T\\in\\mathcal{T}:\\ \\mathrm{err}(h^{\\star},T(S))\\leq\\mathrm{err}(h^{\\star},T(\\mathcal{D}))+\\varepsilon.}\\\\ &{\\forall T\\in\\mathcal{T}:\\ \\left|0\\mathsf{P T}_{T}-0\\hat{\\mathsf{P T}}_{T}\\right|\\leq\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, by definition, since $\\hat{h}$ minimizes the empirical objective, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{T\\in{\\cal T}}\\mathrm{err}(\\hat{h},T(S))-\\mathsf{O}\\hat{\\mathsf{P}}\\mathsf{T}_{T}\\le\\operatorname*{sup}_{T\\in{\\cal T}}\\mathrm{err}(h^{\\star},T(S))-\\mathsf{O}\\hat{\\mathsf{P}}\\mathsf{T}_{T}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By combining the above, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall T\\in\\mathcal{T}:\\mathrm{err}(\\hat{h},T(\\mathcal{D}))-\\mathsf{O P T}_{T}\\leq\\underset{T\\in\\mathcal{T}}{\\operatorname*{sup}}\\mathrm{err}(\\hat{h},T(S))-\\mathsf{O P T}_{T}+2\\varepsilon}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underset{T\\in\\mathcal{T}}{\\operatorname*{sup}}\\mathrm{err}(h^{\\star},T(S))-\\mathsf{O P T}_{T}+2\\varepsilon}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underset{T\\in\\mathcal{T}}{\\operatorname*{sup}}\\mathrm{err}(h^{\\star},T(S))-\\mathsf{O P T}_{T}+3\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This concludes the proof by definition of $h^{\\star}$ . ", "page_idx": 16}, {"type": "text", "text": "Similar to Section 3, we develop in this section a generic oracle-efficient reduction solving Objective 7 using only an ERM oracle for $\\mathcal{H}$ . This reduction may be favorable in applications where we only have black-box access to an off-the-shelve supervised learning method. The techniques used are similar to those used in Section 3, and Agarwal and Zhang [2022] who developed a similar reduction when having access to a collection of importance weights instead of a collection of transformations (which is the view we propose in this work). ", "page_idx": 16}, {"type": "text", "text": "Theorem E.1. For any class $\\mathcal{H}$ , collection of transformations $\\tau$ , distribution $\\mathcal{D}$ and any $\\varepsilon,\\delta~\\in$ (0, 1/2), with probability at least 1 \u2212\u03b4 over S \u223cDm(\u03b5,\u03b4), where m(\u03b5, \u03b4) \u2264O vc(H\u25e6T )\u03b5+2log(1/\u03b4) , running Algorithm 2 on $S$ for $R\\geq\\frac{8\\ln\\vert\\tau\\vert}{\\varepsilon^{2}}$ rounds produces $\\begin{array}{r}{\\bar{h}=\\frac{1}{R}\\sum_{r=1}^{R}h_{r}}\\end{array}$ s.t. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall T\\in\\mathcal{T}:\\ \\mathrm{err}(\\bar{h},T(D))-\\mathsf{O P T}_{T}\\leq\\operatorname*{inf}_{h^{\\star}\\in\\mathcal{H}}\\operatorname*{sup}_{T\\in\\mathcal{T}}\\left\\{\\mathrm{err}(h^{\\star},T(\\mathcal{D}))-\\mathsf{O P T}_{T}\\right\\}+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Input: Black-box learner $\\mathtt{E R M}_{\\mathcal{H}}$ , dataset $S=\\{(x_{1},y_{1}),\\dots,(x_{m},y_{m})\\}$ , and transformations $\\tau$ . 1 For each $T\\in\\mathcal T$ , run learner $\\mathtt{E R M}_{\\mathcal{H}}$ on $T(S)$ and denote its output by $\\hat{h}_{T}$ . 2 For each T \u2208T , set Q1(T) =|T1 |. ", "page_idx": 17}, {"type": "text", "text": "3 Set R = 8 ln|2T |.   \n4 for $1\\leq r\\leq R$ do   \n5 Draw $m_{\\mathrm{ERM}}$ i.i.d. samples $(X_{1},Y_{1}),\\dots,(X_{m_{\\mathtt{E R M}}},Y_{m_{\\mathtt{E R M}}})$ , where each $(X_{i},Y_{i})$ is drawn by randomly drawing a transformation $T$ according to $Q_{t}$ and randomly drawing $(X,Y)$ from $\\mathrm{Unif}(S)$ , and letting $(X_{i},Y_{i})=(T(X),Y)$ .   \n6 Run learner $\\mathtt{E R M}_{\\mathcal{H}}$ on $(X_{1},Y_{1}),\\dots,(X_{m_{\\mathtt{E R M}}},Y_{m_{\\mathtt{E R M}}})$ , and let $h_{r}$ denote its output.   \n7 For each $T\\in\\mathcal T$ , update $\\begin{array}{r}{Q_{r+1}(T)=\\frac{Q_{r}(T)\\exp\\big(\\eta(\\exp(h_{r},T(S))-\\mathrm{err}(\\hat{h}_{T},T(S)))\\big)}{Z_{r}}}\\end{array}$ , where $Z_{r}$ is a normalization factor such that $Q_{r+1}$ is a distribution. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem E.1. Let $S=\\{(x_{1},y_{1}),\\dots,(x_{m},y_{m})\\}$ be an arbitrary dataset, and let A be an (\u03b5, \u03b4)-agnostic-PAC-learner for H. Then, by setting R \u2265 8 l\u03b5n|2T |a nd invoking Lemma D.2, we are guaranteed that Algorithm 2 produces a sequence of distributions $Q_{1},\\dots,Q_{T}$ over $\\tau$ that satisfy ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{r\\in T}\\frac{1}{R}\\sum_{r=1}^{R}\\operatorname{err}(h_{r},T(S))-\\operatorname{err}(\\hat{h}_{T},T(S))\\leq\\frac{1}{R}\\sum_{r=1}^{R}\\mathbb{E}_{\\sim Q_{r}}\\left[\\operatorname{err}(h_{r},T(S))-\\operatorname{err}(\\hat{h}_{T},T(S))\\right]+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "At each round $r$ , observe that Step 3 in Algorithm 1 draws an i.i.d. sample from a distribution $P_{r}$ over $\\mathcal X\\times\\mathcal X$ that is defined by $Q_{r}$ over $\\tau$ and ${\\mathrm{Unif}}(S)$ , and since $\\mathtt{E R M}_{\\mathcal{H}}$ is an $(\\varepsilon,\\delta)$ -agnostic-PAC-learner for $\\mathcal{H}$ , Steps 5-6 guarantee that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathop{\\mathbb{L}}_{r\\sim Q_{r}}^{\\mathbb{L}}\\operatorname{err}(h_{r},T(S))=\\mathop{\\mathbb{L}}_{T\\sim Q_{r}}^{\\mathbb{L}}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{1}\\left\\{h_{r}(T(x_{i}))\\neq y_{i}\\right\\}\\le\\operatorname*{inf}_{h\\in\\mathcal{H}\\,T\\sim Q_{r}}\\operatorname{err}(h,T(S))+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining the above inequalities implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{T\\in T}\\mathrm{err}(\\bar{h},T(S))-\\mathrm{err}(\\hat{h}_{T},T(S))\\leq\\frac{1}{R}\\sum_{r=1}^{R}\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}_{\\sim Q_{r}}\\left[\\mathrm{err}(h,T(S))-\\mathrm{err}(\\hat{h}_{T},T(S))\\right]+\\varepsilon+\\varepsilon\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, by appealing to uniform convergence over $\\mathcal{H}$ and $\\tau$ , with probability at least $1-\\delta$ over $S\\sim\\mathcal{D}^{m}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall T\\in\\mathcal{T}:}&{\\mathrm{err}(\\hat{h}_{T},T(S))\\leq\\mathrm{err}(h_{T}^{\\star},T(S))\\leq\\mathsf{O P T}_{T}+\\varepsilon,}\\\\ &{\\mathsf{O P T}_{T}\\leq\\mathrm{err}(\\hat{h}_{T},T(\\mathcal{D}))\\leq\\mathrm{err}(\\hat{h}_{T},T(S))+\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall T\\in{\\mathcal T}:\\ \\mathrm{err}(\\bar{h},T({\\mathcal D}))-\\mathsf{O P T}_{T}\\leq\\mathrm{err}(\\bar{h},T(S))-\\mathrm{err}(\\hat{h}_{T},T(S))+2\\varepsilon}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underset{h\\in{\\mathcal H}}{\\operatorname*{inf}}\\underset{T\\in{\\mathcal T}}{\\operatorname*{sup}}\\left[\\mathrm{err}(h,T(S))-\\mathrm{err}(\\hat{h}_{T},T(S))\\right]+4\\varepsilon}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{h\\in{\\mathcal H}}{\\operatorname*{inf}}\\ \\underset{T\\in{\\mathcal T}}{\\operatorname*{sup}}\\left[\\mathrm{err}(h,T({\\mathcal D}))-\\mathsf{O P T}_{T}\\right]+6\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The problem setup and the main theoretical results of the paper are described in the introduction, under \u201cOur Contributions\u201d. They match the scope and the claims made in the abstract and introduction. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the limitations of our results immediately after the statements or at the beginning of the next section. E.g., for the limitations of the algorithm proposed in Section 2, we state its limitation immediately after Theorem 2.2 (i.e., the sample complexity cannot be improved by such proper learning rules) and at the beginning of Section 3 (i.e., the algorithm requires knowledge of $\\mathcal{H}$ and $\\tau$ ). We also mention the limitation of Algorithm 1in the introduction, i.e., the algorithm is limited to finite transformations. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All the theoretical results clearly state the required assumptions. Full proofs are provided either in the main text or the supplemental material. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide a full description of the simple experiment that we perfomed in Section 6. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 19}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The data used in our experiment is synthetic and we provide a full description of how to generate it in Section 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All is specified in Section 6. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Reported in Section 6 and Figure 1. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Section 6. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We see no violation of the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper contributes theoretical/foundational results addressing the problem of statistical learning under distribution shifts. Our work will potentially lead to further theoretical study, as well as practical methodological development with direct impact on downstream applications. There are many potential societal consequences of our work, none of which we think must be specifically highlighted here. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]