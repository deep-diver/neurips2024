[{"figure_path": "I6tRENM5Ya/tables/tables_7_1.jpg", "caption": "Table 1: Classification performance (i.e., Macro-F1 and Micro-F1) on heterogeneous graph datasets.", "description": "This table presents the Macro-F1 and Micro-F1 scores achieved by various methods (including the proposed SCHOOL method) on four different heterogeneous graph datasets (ACM, Yelp, DBLP, and Aminer) for a node classification task.  It allows for a comparison of the performance of the proposed method against existing state-of-the-art techniques in heterogeneous graph learning.", "section": "3.2 Results Analysis"}, {"figure_path": "I6tRENM5Ya/tables/tables_8_1.jpg", "caption": "Table 2: Classification performance (i.e., Macro-F1 and Micro-F1) of each component in the objective function I on all heterogeneous graph datasets.", "description": "This table presents the performance of node classification using different combinations of the three components of the objective function (Lsp, Lnc, Lcc) across four heterogeneous graph datasets (ACM, Yelp, DBLP, Aminer).  It shows how each component contributes to the overall performance and highlights the importance of including all three components for optimal results.  The Macro-F1 and Micro-F1 scores are used as evaluation metrics.", "section": "3.2 Results Analysis"}, {"figure_path": "I6tRENM5Ya/tables/tables_23_1.jpg", "caption": "Table 3: Statistics of all datasets.", "description": "This table presents a summary of the characteristics of six datasets used in the paper's experiments.  For each dataset, it shows whether it is heterogeneous or homogeneous, the number of nodes and node types, the number of edges and edge types, the target node type for the downstream tasks, and the number of nodes used for training and testing.", "section": "3.1 Experimental Setup"}, {"figure_path": "I6tRENM5Ya/tables/tables_24_1.jpg", "caption": "Table 1: Classification performance (i.e., Macro-F1 and Micro-F1) on heterogeneous graph datasets.", "description": "This table presents the Macro-F1 and Micro-F1 scores achieved by different methods on four heterogeneous graph datasets (ACM, Yelp, DBLP, and Aminer) for a node classification task.  It compares the performance of the proposed SCHOOL method against various baselines, including both traditional and self-supervised graph learning techniques.  The scores indicate the accuracy of each method in classifying nodes within the heterogeneous graphs.", "section": "3.2 Results Analysis"}, {"figure_path": "I6tRENM5Ya/tables/tables_25_1.jpg", "caption": "Table 5: Settings for the dimensions of encoders (i.e., g\u00a2 \u2208 Rf\u00d7d1 and fo \u2208 Rf\u00d7d1) and projection heads (i.e., pp \u2208 Rd1\u00d7c and qy \u2208 Rd1\u00d7d2) on all datasets.", "description": "This table shows the hyperparameters used in the proposed SCHOOL model for different datasets.  The table includes the dimension of node features (f), the dimension of the semantic representation (d1), the dimension of the projected representation (d2), and the number of classes (c) for each dataset (ACM, Yelp, DBLP, Aminer, Photo, Computers). These settings are crucial for the model's performance and are dataset-specific.", "section": "3.1 Experimental Setup"}, {"figure_path": "I6tRENM5Ya/tables/tables_25_2.jpg", "caption": "Table 6: Clustering performance (i.e., NMI and ARI) of all methods on heterogeneous graph datasets.", "description": "This table presents the performance of various node clustering methods on four heterogeneous graph datasets.  The performance is measured using two metrics: Normalized Mutual Information (NMI) and Adjusted Rand Index (ARI).  Higher values for both metrics indicate better clustering performance. The table allows for a comparison of the proposed SCHOOL method against existing state-of-the-art methods in a node clustering task.", "section": "3.2 Results Analysis"}, {"figure_path": "I6tRENM5Ya/tables/tables_26_1.jpg", "caption": "Table 7: Classification performance (i.e., Macro-F1 and Micro-F1) on homogeneous graph datasets.", "description": "This table presents the Macro-F1 and Micro-F1 scores achieved by different methods on two homogeneous graph datasets (Photo and Computers) for node classification.  The results show the performance of various methods, including DeepWalk, GCN, GAT, and several self-supervised methods, along with the proposed SCHOOL method.  Macro-F1 and Micro-F1 are metrics evaluating the model's classification performance, considering both the majority and minority classes.", "section": "3.2 Results Analysis"}, {"figure_path": "I6tRENM5Ya/tables/tables_27_1.jpg", "caption": "Table 8: Classification performance (i.e., Macro-F1 and Micro-F1) of variant methods with the affinity matrix, cosine similarity and, self-attention mechanisms on heterogeneous graph datasets.", "description": "This table compares the performance of three different methods for creating the affinity matrix used in the node classification task.  The methods compared are using a cosine similarity matrix, a self-attention mechanism, and the rank-constrained affinity matrix proposed in the paper. The performance is measured using Macro-F1 and Micro-F1 scores across four different heterogeneous graph datasets (ACM, Yelp, DBLP, and Aminer). The results show that the proposed rank-constrained affinity matrix significantly outperforms the other two methods.", "section": "3.2 Results Analysis"}, {"figure_path": "I6tRENM5Ya/tables/tables_27_2.jpg", "caption": "Table 9: Classification performance (i.e., Macro-F1 and Micro-F1) of the affinity matrix and self-attention mechanisms on heterogeneous graph datasets.", "description": "This table compares the classification performance (Macro-F1 and Micro-F1 scores) of two methods on four different heterogeneous graph datasets. The first method uses the InfoNCE loss, while the second method is the proposed approach using an affinity matrix.  The results demonstrate the superior performance of the proposed method.", "section": "3.2 Results Analysis"}]