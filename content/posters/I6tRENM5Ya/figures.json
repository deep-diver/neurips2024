[{"figure_path": "I6tRENM5Ya/figures/figures_1_1.jpg", "caption": "Figure 1: The flowchart of SCHOOL, which first employs the Multi-Layer Perception g\u03b8 to derive semantic representations H, followed by obtaining orthogonal cluster assignment matrix Y and orthogonal H. Subsequently, SCHOOL filters noisy connections by deriving the rank-constrained affinity matrix S, which is further used to multiply with H and then obtain node representations Z. Meanwhile, SCHOOL employs a heterogeneous encoder f\u03c9 to aggregate information across node types, yielding heterogeneous representations \u017d. Finally, SCHOOL incorporates spectral loss Lsp to optimize Y to fit eigenvectors of the Laplacian matrix of S. Moreover, SCHOOL designs node-level (i.e., Lnc) and cluster-level (i.e., Lcc) consistency constraints on projected representations (i.e., Q and Q) and cluster representations Q to capture the invariant and clustering information, respectively.", "description": "This figure shows the framework of the proposed model, SCHOOL.  It illustrates the process of generating semantic representations, refining the affinity matrix to remove noise, aggregating heterogeneous information, and incorporating spectral and consistency constraints to learn effective representations.", "section": "2 Method"}, {"figure_path": "I6tRENM5Ya/figures/figures_9_1.jpg", "caption": "Figure 2: Visualization of the affinity matrix S and t-SNE on DBLP and Aminer datasets.", "description": "This figure visualizes the learned affinity matrix (S) and the resulting node representations using t-distributed Stochastic Neighbor Embedding (t-SNE) for both the DBLP and Aminer datasets.  The heatmaps of the affinity matrices (S) clearly show a block diagonal structure indicating that the algorithm effectively groups nodes of the same class together. The t-SNE plots further demonstrate this clustering by visualizing the low-dimensional embedding of the node representations, where nodes of the same class are positioned closer together in the embedding space. This visualization supports the claim that the method effectively learns a clustered representation of the data.", "section": "3.2.3 Visualization"}, {"figure_path": "I6tRENM5Ya/figures/figures_27_1.jpg", "caption": "Figure 4: Classification performance (i.e., Macro-F1) of the proposed method under different clusters.", "description": "This figure shows the performance of the proposed method on node classification tasks across four different heterogeneous graph datasets (ACM, Yelp, DBLP, and Aminer) when varying the number of clusters. The x-axis represents the number of clusters, and the y-axis represents the Macro-F1 score. The results indicate that the optimal performance is achieved when the number of clusters is equal to the actual number of classes (indicated by 'c' on the x-axis) in each dataset. As the number of clusters deviates from the optimal value, the performance degrades.", "section": "3.2 Results Analysis"}, {"figure_path": "I6tRENM5Ya/figures/figures_28_1.jpg", "caption": "Figure 2: Visualization of the affinity matrix S and t-SNE on DBLP and Aminer datasets.", "description": "This figure visualizes the affinity matrix S learned by the proposed method and the t-SNE representation of the node features. The visualization helps to verify the effectiveness of the proposed method in mitigating noisy connections and improving the quality of node representations.  The heatmaps show the affinity matrix for DBLP and Aminer, with darker colors indicating stronger connections. The t-SNE plots show the 2-D embeddings of the node representations, demonstrating that the proposed method effectively separates nodes into distinct clusters.", "section": "3.2.3 Visualization"}]