[{"type": "text", "text": "SMART: Scalable Multi-agent Real-time Simulation via Next-token Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei Wu\u2217   \nTsinghua University   \nSenseTime Research   \nwuwei@senseauto.com ", "page_idx": 0}, {"type": "text", "text": "Xiaoxin Feng\u2217 SenseTime Research fengxiaoxin@senseauto.com ", "page_idx": 0}, {"type": "text", "text": "Ziyan Gao\u2217 SenseTime Research gaoziyan@senseauto.com ", "page_idx": 0}, {"type": "text", "text": "Yuheng Kan SenseTime Research kanyuheng@senseauto.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data-driven autonomous driving motion generation tasks are frequently impacted by the limitations of dataset size and the domain gap between datasets, which precludes their extensive application in real-world scenarios. To address this issue, we introduce SMART, a novel autonomous driving motion generation paradigm that models vectorized map and agent trajectory data into discrete sequence tokens. These tokens are then processed through a decoder-only transformer architecture to train for the next token prediction task across spatial-temporal series. This GPT-style method allows the model to learn the motion distribution in real driving scenarios. SMART achieves state-of-the-art performance across most of the metrics on the generative Sim Agents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset (WOMD), demonstrating remarkable inference speed. Moreover, SMART represents the generative model in the autonomous driving motion domain, exhibiting zero-shot generalization capabilities: Using only the NuPlan dataset for training and WOMD for validation, SMART achieved a competitive score of 0.72 on the Sim Agents challenge. Lastly, we have collected over 1 billion motion tokens from multiple datasets, validating the model\u2019s scalability. These results suggest that SMART has initially emulated two important properties: scalability and zero-shot generalization, and preliminarily meets the needs of large-scale real-time simulation applications. We have released all the code to promote the exploration of models for motion generation in the autonomous driving field. The source code is available at https://github.com/rainmaker22/SMART. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the context of autonomous driving, leveraging vectorized maps and vehicle trajectory data facilitates various motion generation tasks, including motion planning [17, 6, 19, 18, 7], motion prediction [47, 11, 38], and Sim Agents [14]. Previous research [9, 5, 27] has predominantly employed encoder networks to represent driving scenes and decoder networks to generate multi-modal motions. These generated motions are then directly regressed to continuous trajectory distributions using Gaussian [4] or Laplace [53] mixture loss functions. While this framework demonstrates strong performance in prediction tasks that prioritize regression accuracy, it often underperforms in motion generative tasks that emphasize the safety and reasonableness of driving behavior, such as planning [3] or Sim Agents [26]. The primary reasons for this underperformance are as follows: First, the framework does not represent future interactions between the motions of different agents, leading to inconsistent scenelevel forecasting. Second, the model generates multi-modal motion by initializing multiple intention queries in the decoder, which is typically limited by GPU memory, resulting in a fixed number of motion modalities. Consequently, it is uncertain whether the generated modalities sufficiently represent the diversity of future behaviors. Thirdly, these models struggle to generalize across different datasets, requiring new data collection for training in new urban environments or maps. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The advent of autoregressive large language models (LLMs) [12, 42] has ushered in a new era in artificial intelligence. Drawing inspiration from this, some studies in the driving motion generation domain[30, 34], have tokenized agent trajectories into discrete motion tokens and employed a Next Token Prediction (NTP) task based on cross-entropy loss for autoregression. These models continue to utilize an encoder-decoder architecture, encoding continuous vectorized map and historical trajectory data with an encoder, and decoding discrete tokens solely in the decoder module. Compared to continuous distribution regression methods, the autoregressive paradigm of NTP has the following advantages: the model adopts a step-by-step next token prediction, allowing it to model interactions between agents\u2019 motions at each time step, and the number of modalities is not limited, leading to better diversity in generative tasks. ", "page_idx": 1}, {"type": "text", "text": "However, existing NTP-based motion models still fail to address the aforementioned issues of generalizability and scalability, which have a critical impact on industrial applications. Generalizability means achieving satisfactory results across diverse datasets through zero-shot and few-shot learning, while scalability involves improving model performance as dataset size or model parameters increase, following scaling laws defined by [16]. This shortfall is due to two main factors: First, current model architectures lack generalizability under the constraints of limited data scale. Due to the high cost of acquiring extensive driving data, open-source datasets typically cover only a few hundred hours of driving in specific urban areas, with significant domain gaps caused by perceptual and regional differences. Second, unlike tasks involving the serialization of a single dimension, motion generation requires the serialization of both the temporal dimension of trajectories and the spatial interactions between maps and agents. To tackle these challenges, this paper introduces the SMART model: Scalable Multi-Agent Real-Time Motion Generation via Next-token Prediction. The model incorporates a tokenizer for map data and proposes an autoregressive prediction task for the next road token prediction to enhance the model\u2019s spatial comprehension. Subsequently, a GPT-style approach is adopted, tokenizing agent trajectories across the entire time series to establish a decoder-only transformer model. The decoder-only transformer allows SMART to compute the next token for the upcoming frame at the current moment during inference, eliminating the need to re-encode historical motion tokens with each inference, which significantly improves inference efficiency for real-time interactive autonomous driving simulation. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions to the community include: (1) We propose a novel framework for motion generation, incorporating a tokenization scheme for both vectorized road and agent trajectories and utilizing a decoder-only transformer for training on the next token prediction task. This approach offers new insights into the design of motion generation algorithms for autonomous driving. (2) In the field of driving motion generation, we have pioneered a focus on the model\u2019s zero-shot generalizability across different datasets. Notably, the model trained solely on the NuPlan dataset performed well on the WOMD test dataset, despite the lack of overlap between the map areas of these two datasets. An empirical validation of SMART models\u2019 scalability emulates the appealing properties of large fundamental models. (3) SMART achieves state-of-the-art performance across most metrics in the generative Sim Agents challenge, ranking $\\mathbf{1^{th}}$ on the WOMD leaderboards2. Furthermore, SMART\u2019s single-frame inference time is within $15\\mathrm{ms}$ , meeting the real-time requirements for interactive simulation in autonomous driving. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Properties of auto-regressive large models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Scalability and zero-shot generalization Power-law scaling laws [22, 12, 31] mathematically describe the relationship between the growth of model parameters, dataset sizes, computational resources, and the performance improvements of machine learning models, providing several distinct benefits. Firstly, they enable the extrapolation of a larger model\u2019s performance by scaling up model size, data size, and computational cost. Secondly, the scaling laws have demonstrated a consistent and non-saturating increase in performance, corroborating their sustained advantage in enhancing model capabilities. Zero-shot generation refers to the ability of models to generate predicted motions for time series from unseen datasets. Previous work [29, 21] on zero-shot generation typically involves training on a single time series dataset and testing on a different dataset. In this study, we utilize the NuPlan dataset for training SMART models and the WOMD validation dataset for testing. Existing methods in the autonomous driving field [37, 40] often rely on LLMs or VLMs to assist in decision-making and planning to enhance generalizability and interpretability. However, no studies have attempted to directly construct a foundational model for the driving motion field to validate scalability and zero-shot generalizability. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Tokenizer in continuous domains ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Language models [42, 43] rely on Byte Pair Encoding or WordPiece algorithms for text tokenization. Visual generation models[49, 48] based on language models also necessitate the encoding of 2D images into 1D token sequences. Early endeavors VQVAE [44] have demonstrated the ability to represent images as discrete tokens, although the reconstruction quality was relatively moderate. In the driving motion domain, MotionLM[34] used a simple uniform quantization of axis-aligned deltas between consecutive waypoints of agent trajectories. ", "page_idx": 2}, {"type": "text", "text": "2.3 Driving motion generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our work builds heavily on recent advancements in driving motion generation. A comprehensive range of generative models has been applied to this problem, including continuous motion distribution regression [33, 1, 39], diffusion models [50, 20], and discrete autoregressive models [30, 34]. MotionDiffuser [20] is a diffusion-based representation method for modeling the joint distribution of future trajectories across multiple agents, leveraging a simple predictor design and PCA compression for efficient, top-performing multi-agent motion prediction. While these diffusion-based models produce multi-modal future trajectories of individual agents, they only capture the marginal distributions of possible agent movements and do not model interactions among agents\u2019 future motions. Typical distribution regression models use parametric continuous distributions such as Gaussian [36] or Laplace [53] to model the future motion distribution. A limitation of these models is the uncertainty of whether the Gaussian or Laplace mixture distribution is flexible enough to represent the distribution over future states. Additionally, to generate multi-modal future motions, these models often need to incorporate motion goal candidates [13] or learnable latent embeddings [45] as multi-modal queries in the decoder module, resulting in significant memory usage and increased inference time. MotionLM [34] treats multi-agent motion prediction in autonomous vehicles as a language modeling task, generating interactive trajectories through a simplified autoregressive process without requiring complex optimizations and latent anchor embeddings. On this basis, Trajeglish [30] targets multi-agent offilne closed-loop simulation. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce SMART, an autoregressive generative model for dynamic driving scenarios. While both language and agent motions are sequential, they differ in their representation\u2014natural language consists of words from a finite vocabulary, whereas agent motions are continuous real-valued data. This distinction necessitates the unique design outlined in Sec. 3.1 for agent motion and road vector tokenizer, including the construction of vocabulary and the tokenization of motion sequences. Sec. 3.2 provides a comprehensive description of the model\u2019s architecture. Sec. 3.3 elaborates on the training tasks designed for the proposed model to learn the distribution of the motion token within the temporal sequence and the distribution of the road token within the spatial sequence. ", "page_idx": 2}, {"type": "text", "text": "3.1 Tokenization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Agent Motion tokenization To apply discrete sequence modeling in continuous domains, prior works typically follow one of two approaches: either use a pre-trained tokenizer, such as VQVAE [44] or VQGAN [10], to encode continuous features into discrete tokens, or normalize continuous features and divide continuous values into discrete slots at equal intervals [2, 34]. For the former approach, establishing a latent vocabulary often requires a large amount of raw data to train the tokenizer; otherwise, the tokenizer itself will be biased towards the pre-training dataset. Since our work aims to enable the model to generalize effectively when trained on a small number of data samples, SMART opts to discretize explicit trajectory and map features. Specifically, similar to [30], we segment the continuous trajectories of all agents in the dataset into trajectory sets by fixed time intervals $t=0.5s$ . Then, we cluster the trajectory sets using the $\\mathbf{k}\\cdot$ -disks algorithm. As shown in Figure 1(b), the sampled trajectories serve as our final agent motion token vocabulary $V_{a}$ . ", "page_idx": 2}, {"type": "image", "img_path": "2uy3LZHNIG/tmp/4193acb5eabc06c8477bee543b8518cd5233f8c54f4271e94fac5636e0a1d15d.jpg", "img_caption": ["Figure 1: (a) At time $\\scriptstyle\\mathrm{t=0s}$ , the current vehicle state is used as the reference to select the token closest to the ground truth bounding box within the token set. At time $\\scriptstyle{\\mathrm{t=}}0.5\\mathrm{s}$ , the matched token from the previous step is used to select the next predicted token. At time $\\scriptstyle\\mathrm{t=1.0s}$ , a noised token serves as the reference to determine the token for $\\scriptstyle\\mathrm{t}=1.5\\mathrm{s}$ . This iterative process continues. (b) Motion token vocabulary with time granularity equal to 0.5s.(c) The original road vector features are represented as continuous sequences of map points. We divide the original map into multiple segments, each within 5 meters in length, and then perform matching with discrete tokens. The final map is composed of road vector tokens represented by different colored segments. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "As shown in Figure 1(a), the blue box represents the tokens obtained after discretizing the ground truth trajectory. At every 0.5-second interval, a search is conducted within the token vocabulary for candidate tokens, from which an appropriate(closest) token is selected to represent the current moment. Note that to prevent matching errors that may occur during the tokenization process of the agent motion sequence, we implement a rolling matching approach for the entire continuous motion sentence in a given period $T$ . This implies that the token for the next time step is matched by referring to the position of the token currently matched, rather than relying on the actual correct position. However, due to the transformer decoder must perform sequential inference step by step, this approach inevitably leads to out-of-distribution issues due to compounding errors[32]. Especially, in the field of autonomous driving, these accumulated errors may result in collisions and off-map events[51]. To address this issue, we introduce noise into the tokenization process to enable the model to simulate distribution shifts during training. Specifically, we perturb the currently matched token by selecting one from the top- $\\cdot\\mathbf{k}$ tokens closest to the ground truth token in the vocabulary. Then, in the next time step, we match the motion token based on the perturbed vehicle state. This data augmentation method allows the model to effectively handle issues such as distribution shifts and accumulated errors, thereby enhancing robustness in generative tasks. Finally, the agent motion token is represented as $A\\in\\mathbb{R}^{\\tilde{N}_{A}\\times N_{T}\\times F_{A}^{\\bullet}}$ , where $N_{A}$ denotes the total number of agents, and $N_{T}$ represents the number of time steps, with a feature size of $F_{A}$ , containing coordinates, heading, and shapes. ", "page_idx": 3}, {"type": "text", "text": "Road vector tokenization To enhance the model\u2019s generalization capabilities, we have applied a similar tokenization process to road vectors as we did with agent motion. Each road vector is a directed lane segment with features including start and end positions, length, turn direction, and other semantics from the dataset. To obtain fine-grained inputs for the road network, all road vectors are segmented into tokens spanning no longer than 5 meters in length. Unlike the motion sequence, the tokenization process of the road sentence does not have a time-series dependency. As shown in Figure 1(c), the tokenization of the road sentence is performed in parallel, directly tokenizing all the original road vector segments. The road vector token is represented as $\\boldsymbol{R}\\in\\mathbb{R}^{\\upharpoonright N_{R}\\times\\boldsymbol{F}_{R}}$ , where $N_{R}$ denotes the total number of road vectors, and $F_{A}$ represents the token features. ", "page_idx": 3}, {"type": "image", "img_path": "2uy3LZHNIG/tmp/9d310c61e83875e1dda89d654c0c63f2a9417ee7508a05a92b05652ea20684b1.jpg", "img_caption": ["Figure 2: The architecture of SMART framework (a) We train a decoder-only transformer that predicts the motion tokens of multi-agents conditional on previous motion tokens, interactive agent motion tokens, and encoding road tokens. The model is trained to predict the next motion token. (b) Illustration for our proposed road spatial understanding training task. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Model Architecture ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Figure 2 illustrates the simple but expressive model architecture of SMART. The model comprises an encoder for road map encoding and a motion decoder that predicts a category distribution based on motion token embeddings. ", "page_idx": 4}, {"type": "text", "text": "RoadNet: road token encoder We employ multi-head self-attention (MHSA) to model the relationships among road tokens, after which the updated road token encodings will assist motion token decoding. For the $i^{t h}$ road token, we derive a query from its embedding $r_{i}$ and let it attend to the neighboring tokens $r_{j}\\in R_{i}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{i^{\\prime}}=\\mathrm{MHSA}\\left(q(r_{i}),\\ \\ k(r_{j},\\mathrm{RPE}_{i j}),v(r_{j},\\mathrm{RPE}_{i j})\\,,\\ \\ \\ j\\in R_{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $R_{i}$ denotes the neighbor set of the road tokens. To incorporate spatial awareness for map encoding, we generate the $\\bar{j}^{t h}$ key/value vector from the concatenation of $r_{j}$ and the relative positional embedding $\\mathrm{RPE}_{i j}$ [8]. ", "page_idx": 4}, {"type": "text", "text": "MotionNet: factorized agent motion decoder Prevailing methods for encoding agents prioritize capturing the temporal dynamics of an agent\u2019s movements, followed by the integration of agent-map and agent-agent interactions, as highlighted by [35]. Factorized attention effectively captures detailed agent-map interactions across temporal scales [28]. In our work, we leverage a factorized Transformer architecture with multi-head cross-attention (MHCA) to decode complex road-agent and agent-agent relationships along the time series. Akin to query-centric methodologies [52], we utilize relative positional embeddings to differentiate between agents\u2019 local coordinate frames, enabling symmetric encoding. Take the $i^{{\\check{t}}h}$ agent at time step $t$ as an example. Denoted as Eq.2a, given the query derived from the agent motion token\u2019s embedding $e_{i}^{t}$ , we employ temporal attention by computing the key and value based on which are the $i^{t h}$ agent\u2019s token embeddings from time step $t-\\tau$ to time step $t-1$ and the corresponding relative positional embeddings. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{i^{\\prime}}=\\mathrm{MHSA}\\left(q(e_{i}^{t}),k(e_{i}^{t-\\tau},\\mathrm{RPE}_{i}^{t,t-\\tau}),v(e_{i}^{t-\\tau},\\mathrm{RPE}_{i}^{t,t-\\tau})\\right),\\quad0<\\tau<t}\\\\ &{\\qquad e_{i^{\\prime}}=\\mathrm{MHCA}\\left(q(e_{i}^{t}),k(r_{j},\\mathrm{RPE}_{i j}),v(r_{j},\\mathrm{RPE}_{i j})\\right),\\quad j\\in N_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Likewise, in $\\operatorname{Eq}.2\\mathfrak{b}$ and $\\operatorname{Eq.}2\\mathbf{c}$ , the key and value for agent-map and agent-agent attention are derived from road token $r_{j},j\\in N_{i}$ and agents\u2019 motion token $e_{j}^{t},j\\in N_{i}$ in the neighborhood respectively, where the neighbor set $N_{i}$ is determined by a distance threshold of 50 meters. We stack the temporal, the agent-agent, and the agent-map attention sequentially as one fusion block and repeat such blocks $K$ times. ", "page_idx": 5}, {"type": "text", "text": "3.3 Spatial-temporal next token prediction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the training stage, we train SMART to understand the temporal and spatial relationships in the traffic scene. This is achieved with two next token prediction tasks on RoadNet and MotionNet, the model is optimized with the summation of the two tasks\u2019 objectives. ", "page_idx": 5}, {"type": "text", "text": "Road vector next token prediction As shown in Figure 2(b), the road vector NTP task targets RoadNet to learn the spatial structure of road vector inputs. Unlike agent motions, road vectors form a graph rather than a sequence, making it challenging to apply next token prediction tasks directly. To address this issue, we extract the original topological information of roads and model the road vector tokens with sequential relationships based on their predecessor-successor connections. As depicted in Figure 2(b), in the pre-training NTP task, the subsequent road vector token is predicted using the preceding road token based on the road topology. This approach requires RoadNet to understand the connectivity and continuity among unordered road vectors. The loss function for a single tokenized road vector sequence is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\cos(\\gamma)=-\\sum_{j=1}^{J}\\sum_{i=1}^{V_{r}}(\\mathbf{r}_{i}^{j+1}==\\mathbf{r}_{i\\circ}^{j+1})\\log(p_{\\gamma}(\\mathbf{r}_{i}^{j+1}|\\mathbf{r}_{i}^{1:j}))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p_{\\gamma}(\\mathbf{r}_{i}^{j+1}|\\mathbf{r}_{i}^{1:j})$ denotes the categorical distribution predicted by the RoadNet parameterized by $\\gamma$ , $J$ represents a complete polyline that has not yet been split into road vector tokens, $\\mathbf{r}^{1:j}$ Representing the road token embedding of the predecessor, and ${\\bf r}_{i}^{j+1}$ is the next predicted road vector token. This loss function ensures that RoadNet learns to predict the correct next road vector token given the preceding tokens, thereby capturing the spatial continuity and connectivity within the road network. ", "page_idx": 5}, {"type": "text", "text": "Motion next token prediction Motion NTP task targets MotionNet to understand not only the temporal dependencies in agents\u2019 motions but also the spatial dependencies between agent-map and agent-agent. SMART is trained to minimize the cross entropy between the distribution of the ground truth token label and the predicted distribution. Formally, the loss function for a single tokenized motion sentence is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathit{l o s s}(\\theta)=-\\sum_{t=1}^{T}\\sum_{i=1}^{V_{a}}(a_{i}^{t+1}==a_{i\\ell}^{t+1})\\mathit{l o g}(p_{\\theta}(a_{i}^{t+1}|e_{i}^{1:t},r_{j}))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p_{\\theta}(a_{i}^{t+1}|e_{i}^{1:t},m_{j})$ denotes the categorical distribution predicted by the model parameterized by $\\theta$ , $e_{1:t}$ is the historical tokenized agent motion embeddings, $a_{i}^{t+1}\\in A$ is the next predicted agent motion token and $r_{j}$ is the tokenized nearby road vector series. Note that SMART performs autoregression via classification[41]. Opting for a categorical output distribution offers a key advantage: it imposes no restrictions on the structure of the output distribution, allowing the model to learn arbitrary distributions, including multimodal ones. This flexibility is especially valuable for a fundamental model, as agent and road tokens from diverse datasets may follow distinct output distribution patterns. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To validate the generalizability and scalability of the SMART model, we conducted extensive experiments and trained models across various scales. On the official WOMD Sim Agents Challenge (WOSAC), we employed the SMART 7 Million parameters (7M) model, which was exclusively trained on the WOMD dataset. Concurrently, the SMART 7M model was also utilized for generalization experiments and ablation studies. In the scale law experiments, we integrated additional datasets and trained on models of multiple scales. For all experiments, the testing datasets employed the split validation dataset from WOMD. Detailed hyperparameters for the SMART architecture can be found in Section A.1. In the following sections, Section 4.1 presents the results of rollouts generated by SMART on the WOSAC benchmark [26]. Evaluations of SMART\u2019s generalizability and scalability are detailed in Sections 4.2 and 4.3, respectively. Finally, an ablation analysis of our design methods is conducted in Section 4.4. ", "page_idx": 5}, {"type": "table", "img_path": "2uy3LZHNIG/tmp/9dad99f4758b53449404bb4e1ef3f477765aff9f64d1438e579a4377ca261c43.jpg", "table_caption": ["Table 1: Comparison with state-of-the-art models on WOMD 2023 Sim Agents benchmark "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.1 Comparison for motion generation task ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Performance comparison We compare proposed SMART with existing motion generation approaches including diffusion models[15], continuous distribution regression models [46, 36], and next token autoregressive model[30]. Because the Sim Agents challenge metrics were changed twice, to compare it more broadly with the previous methodology, we test the performance of our model using both the WOMD Sim Agents 2023 and 2024 Benchmark[26]. As shown in Table 1 and Table 2, SMART achieves not only the best Realism Meta metric but also a high prediction precision. SMART\u2019s modeling approach for maps and motion enables it to learn the behavioral distribution within the data more effectively than prior work. Notably, SMART-zeroshot represents a model trained solely on the NuPlan dataset and directly inferred on the Waymo test set. As shown in Table 2, it achieves performance close to that of MVTE. For further detailed comparisons, please refer to A.2. ", "page_idx": 6}, {"type": "table", "img_path": "2uy3LZHNIG/tmp/4be92f35bb66f8163548850a76a542ada050c33241c781cb49ecb9148acaeaaa.jpg", "table_caption": ["Table 2: Comparison with state-of-the-art models on WOMD 2024 Sim Agents benchmark "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Efficiency comparison SMART also demonstrates remarkable speed in multi-agent motion generation. Previous encoder-decoder models [34, 36] suffer from high computational costs, as the model requires multiple query embeddings in the decoder module to generate multi-modal motions. Benefiting from the advantages of the decoder-only transformer architecture, SMART only needs to compute the next token for the upcoming frame at the current moment during inference, without the need to re-encode historical motion tokens. By reusing the token embeddings computed in previous observation time horizons, the complexity of the agent motion decoder is reduced to $O(\\dot{N}_{A}N_{T})+O(N_{A}N_{R})+O(N_{A}^{2})$ . In contrast, for encoder-decoder models like [24], besides the computational load of the encoder module, additional computations of $O(N_{A}^{2}N_{M})+O(N_{A}N_{M}N_{R})$ are required for generating multi-modalities of trajectories, where $N_{M}$ represents the number of modalities. The average single-step inference time of SMART is influenced by the number of map tokens and agent motion tokens, fluctuating between 5 to $20~\\mathrm{ms}$ , and averaging under $10\\;\\mathrm{ms}$ . Thus, it significantly meets the current needs of interactive real-time online simulation in autonomous driving. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Generalization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Zero-shot generalization on different dataset Zero-shot generation is the ability of models to generate motions for time series from different datasets. In this work, we use the training data from NuPlan dataset to train SMART models and the test data from WOMD validation dataset. As shown in Table 3, SMART\\* still achieves good performance in the overall metrics. Due to significant differences in the accuracy of the calibrated ground truth values for agent position and heading between different datasets, there may be a larger gap in the agent kinematic metrics, resulting in lower scores. However, SMART\\* demonstrated excellent generalization in the metrics of agent interaction and drivable map. It is worth mentioning that the size of the two datasets does not differ greatly, so the SMART model can have good generalization ability based on a small number of data training. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Zero-shot generalization on different datasets. SMART denotes a model trained on WOMD only. SMART\\* denotes a model trained on NuPlan dataset only. SMART\\*\\* denotes a model after 1 epoch of finetuning with an initial learning rate of 0.0001 on WOMD based on SMART\\* model. ", "page_idx": 7}, {"type": "table", "img_path": "2uy3LZHNIG/tmp/62fb219e608cc141420d47be1d530141213b08f25c83cf0a1dfb9b6f60634bda.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Zero-shot generalization on unseen scenarios Multiple map scenarios as shown in Figure 3 are present only in the WOMD but not in the NuPlan dataset. Without modifications to the network architecture or tuning parameters, SMART trained only on NuPlan has achieved decent results in these scenarios, substantiating the generalization ability of SMART. ", "page_idx": 7}, {"type": "text", "text": "4.3 Scalability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Prior research [22, 42] have established that scaling up large language models (LLMs) leads to a predictable decrease in test loss $L$ . This trend correlates with parameter counts $N$ , training tokens $T$ , following a power-law: ", "page_idx": 7}, {"type": "equation", "text": "$$\nl o g(L)=\\beta l o g(X)+\\alpha\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $X$ can be any of $N,T$ . The exponent $\\alpha$ reflects the smoothness of power-law, and $L$ denotes the reducible loss normalized by irreducible loss. The data sources for validating scaling laws are detailed in the A.3. Overall, we trained models across four sizes, ranging from 1M to 100M parameters, on a training set containing 2.2M scenarios (or 1B motion tokens under 0.5s agent motion tokenization). ", "page_idx": 7}, {"type": "text", "text": "Scaling laws with model parameters We investigate the test loss trend as the model size increases. We assessed the final test cross-entropy loss $L$ on the validation set of 100,000 traffic scenarios. The results are plotted in Figure 4, where we observed a clear power-law scaling trend for Loss $L$ as a function of model size $N$ . The power-law scaling laws can be expressed as: ", "page_idx": 7}, {"type": "equation", "text": "$$\nl o g(L)=-0.157l o g(X)+1.52\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "These results verify the strong scalability of SMART, providing valuable insights into how model performance scales with dataset size. ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this study, we aim to verify the effectiveness of each component of SMART. Results are reported in Table 4. The initial model, denoted as $M1$ , is constructed on the architecture depicted in Sec.3.2, employing solely agent tokenization. The introduction of the road vector tokenization in $M2$ , which tokenized the road vector states into discrete tokens, results in marked improvements over $M1$ in ", "page_idx": 7}, {"type": "image", "img_path": "2uy3LZHNIG/tmp/b1121fbb89808f3fe5a9d92acdfa3d117b5e876d21c205836337220f79bd5944.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Qualitative results of closed-loop planning for two representative scenarios from the test set. Each scenario (every row) lasts 8 seconds and we take 4 snapshots with a 2-second interval. SMART controls all the agents in the scenario. The first row depicts a parking lot area. The red vehicle in the picture effectively completed a detour around a stationary vehicle ahead in the parking lot. The second row shows a scene of a large curvature U-turn in a ramp zone, where the traffic flow in the right lane of the ramp has completed the behavior of ramp exit under the control of SMART. It is recommended to refer to supplementary materials for more videos ", "page_idx": 8}, {"type": "image", "img_path": "2uy3LZHNIG/tmp/2c7a244ae70d32b9d544576b7a8201a85da0d2bf25b0fcf2460c8205515f74e8.jpg", "img_caption": ["Figure 4: Due to limitations in dataset size, we trained models at multiple scales ranging from 1M to 101M on a total of 1 billion tokens. (a) Training loss of different models (b) Axes are all on a logarithmic scale. The power-law scaling law can be expressed as a solid line. Exponents $\\beta=-0.157$ suggest a smooth decline in test loss $L$ when scaling up SMART models. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "the generalization capability. Comparing models $M1$ and $M2$ reveals that when trained solely on the WOMD dataset, the tokenization of road vectors results in a certain reduction in overall metrics. We speculate that discretized map tokens may lose some fine-grained geometric information about roads. $M4$ incorporates noised agent motion tokenization, designed to address cumulative errors and distributional shifts during inference. This modification leads to enhancements in both the interaction metric and the map-based metric. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we have introduced SMART, a novel paradigm for autonomous driving motion generation that leverages vectorized map and agent trajectory data, processed through a decoder-only transformer architecture in a GPT-style framework. We have observed that SMART emulates two critical properties: scalability and zero-shot generalization, which are essential for advancing large models. We believe that our findings and the release of all codes will encourage further exploration and development of models for motion generation in the autonomous driving field, ultimately contributing to more reliable autonomous driving systems. ", "page_idx": 8}, {"type": "table", "img_path": "2uy3LZHNIG/tmp/5d60da6a7ed60537eb1ded36395cdde3ade999b0fde882a728efd7d0998f37af.jpg", "table_caption": ["Table 4: Ablation study on each component of SMART. Experimental results are based on the WOMD validation set. \"RVT\" indicates road vector tokenization, \"RVNTP\" indicates road vector next token prediction, \"NAT\" indicates noised agent tokenization, \"NRVT\" indicates noised road vector tokenization "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations In this work, we primarily focus on the design of the learning paradigm and maintain a relatively simple design for the discrete token vocabulary. We believe that iterating SMART with an advanced tokenizer[25] or sampling technique can further improve the performance. Although we have collected training data from multiple datasets, we are still limited by the dataset size when validating the model\u2019s scalability, restricting us to models with a maximum scale of 100 million parameters. Given the focus of this work on generalization and scaling laws, a large number of hyperparameter ablation experiments remain to be verified, including the time granularity of agent motion tokens and the size of the token vocabulary. As a motion generation model, the ability of SMART to migrate to planning and prediction tasks still needs to be verified, and this is our top priority for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement We thank the anonymous reviewers, area chairs, and program committee members for their valuable suggestions, which have greatly improved the quality of our work. We also appreciate the thoughtful discussions with Yue Gong and Shuxiang Lu. Authors Wei Wu, Xiaoxin, and Ziyan contributed equally to this work. Wei Wu led the project and provided funding support, while Xiaoxin and Ziyan focused on algorithm design, implementation, model training, and manuscript writing. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Elmira Amirloo, Amir Rasouli, Peter Lakner, Mohsen Rohani, and Jun Luo. Latentformer: Multi-agent transformer-based interaction modeling and trajectory prediction. arXiv preprint arXiv:2203.01880, 2022.   \n[2] Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, et al. Chronos: Learning the language of time series. arXiv preprint arXiv:2403.07815, 2024.   \n[3] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nuplan: A closed-loop ml-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810, 2021.   \n[4] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. arXiv preprint arXiv:1910.05449, 2019.   \n[5] Yongli Chen, Shen Li, Xiaolin Tang, Kai Yang, Dongpu Cao, and Xianke Lin. Interaction-aware decision making for autonomous vehicles. IEEE Transactions on Transportation Electrification, 2023.   \n[6] Jie Cheng, Yingbing Chen, Xiaodong Mei, Bowen Yang, Bo Li, and Ming Liu. Rethinking imitation-based planner for autonomous driving. arXiv preprint arXiv:2309.10443, 2023.   \n[7] Alexander Cui, Sergio Casas, Abbas Sadat, Renjie Liao, and Raquel Urtasun. Lookout: Diverse multi-future prediction and planning for self-driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16107\u201316116, 2021.   \n[8] Alexander Cui, Sergio Casas, Kelvin Wong, Simon Suo, and Raquel Urtasun. Gorela: Go relative for viewpoint-invariant motion forecasting. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 7801\u20137807. IEEE, 2023.   \n[9] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schneider, and Nemanja Djuric. Multimodal trajectory predictions for autonomous driving using deep convolutional networks. In 2019 International Conference on Robotics and Automation (ICRA), pages 2090\u20132096. IEEE, 2019.   \n[10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.   \n[11] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R Qi, Yin Zhou, et al. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9710\u20139719, 2021.   \n[12] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30:681\u2013694, 2020.   \n[13] Junru Gu, Chen Sun, and Hang Zhao. Densetnt: End-to-end trajectory prediction from dense goal sets. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15303\u201315312, 2021.   \n[14] Cole Gulino, Justin Fu, Wenjie Luo, George Tucker, Eli Bronstein, Yiren Lu, Jean Harb, Xinlei Pan, Yan Wang, Xiangyu Chen, et al. Waymax: An accelerated, data-driven simulator for large-scale autonomous driving research. Advances in Neural Information Processing Systems, 36, 2024.   \n[15] Zhiming Guo, Xing Gao, Jianlan Zhou, Xinyu Cai, and Botian Shi. Scenedm: Scene-level multiagent trajectory generation with consistent diffusion models. arXiv preprint arXiv:2311.15736, 2023.   \n[16] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[17] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853\u201317862, 2023.   \n[18] Zhiyu Huang, Peter Karkus, Boris Ivanovic, Yuxiao Chen, Marco Pavone, and Chen Lv. Dtpp: Differentiable joint conditional prediction and cost evaluation for tree policy planning in autonomous driving. arXiv preprint arXiv:2310.05885, 2023.   \n[19] Zhiyu Huang, Haochen Liu, and Chen Lv. Gameformer: Game-theoretic modeling and learning of transformer-based interactive prediction and planning for autonomous driving. arXiv preprint arXiv:2303.05760, 2023.   \n[20] Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin Sapp, Yin Zhou, Dragomir Anguelov, et al. Motiondiffuser: Controllable multi-agent motion prediction using diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9644\u20139653, 2023.   \n[21] Xiaoyong Jin, Youngsuk Park, Danielle Maddix, Hao Wang, and Yuyang Wang. Domain adaptation for time series forecasting via attention sharing. In International Conference on Machine Learning, pages 10280\u201310297. PMLR, 2022.   \n[22] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[24] Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox. Overcoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7144\u20137153, 2019.   \n[25] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023.   \n[26] Nico Montali, John Lambert, Paul Mougin, Alex Kuefler, Nicholas Rhinehart, Michelle Li, Cole Gulino, Tristan Emrich, Zoey Yang, Shimon Whiteson, et al. The waymo open sim agents challenge. Advances in Neural Information Processing Systems, 36, 2024.   \n[27] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat, and Benjamin Sapp. Wayformer: Motion forecasting via simple & efficient attention networks. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 2980\u20132987. IEEE, 2023.   \n[28] Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zhengdong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, et al. Scene transformer: A unified architecture for predicting multiple agent trajectories. arXiv preprint arXiv:2106.08417, 2021.   \n[29] Bernardo P\u00e9rez Orozco and Stephen J Roberts. Zero-shot and few-shot time series forecasting with ordinal regression recurrent neural networks. arXiv preprint arXiv:2003.12162, 2020.   \n[30] Jonah Philion, Xue Bin Peng, and Sanja Fidler. Trajeglish: Learning the language of driving scenarios. arXiv preprint arXiv:2312.04535, 2023.   \n[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[32] Vipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation models, 2023.   \n[33] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron $^{++}$ : Dynamically-feasible trajectory forecasting with heterogeneous data. In Computer Vision\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVIII 16, pages 683\u2013700. Springer, 2020.   \n[34] Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S Refaat, Rami Al-Rfou, and Benjamin Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8579\u20138590, 2023.   \n[35] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Motion transformer with global intention localization and local movement refinement. Advances in Neural Information Processing Systems, 35:6531\u20136543, 2022.   \n[36] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. $\\mathrm{Mtr}++$ : Multi-agent motion prediction with symmetric scene modeling and guided intention querying. arXiv preprint arXiv:2306.17770, 2023.   \n[37] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. arXiv preprint arXiv:2312.14150, 2023.   \n[38] Haoran Song, Wenchao Ding, Yuxuan Chen, Shaojie Shen, Michael Yu Wang, and Qifeng Chen. Pip: Planning-informed trajectory prediction for autonomous driving. In Computer Vision\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXI 16, pages 598\u2013614. Springer, 2020.   \n[39] Simon Suo, Sebastian Regalado, Sergio Casas, and Raquel Urtasun. Trafficsim: Learning to simulate realistic multi-agent behaviors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10400\u201310409, 2021.   \n[40] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024.   \n[41] Lu\u00eds Torgo and Jo\u00e3o Gama. Regression by classification. In Advances in Artificial Intelligence: 13th Brazilian Symposium on Artificial Intelligence, SBIA\u201996 Curitiba, Brazil, October 23\u201325, 1996 Proceedings 13, pages 51\u201360. Springer, 1996.   \n[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[44] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.   \n[45] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivastava, Khaled S Refaat, Nigamaa Nayakanti, Andre Cornman, Kan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir Anguelov, et al. Multipath $^{++}$ : Efficient information fusion and trajectory aggregation for behavior prediction. In 2022 International Conference on Robotics and Automation (ICRA), pages 7814\u20137821. IEEE, 2022.   \n[46] Yu Wang, Tiebiao Zhao, and Fan Yi. Multiverse transformer: 1st place solution for waymo open sim agents challenge 2023. arXiv preprint arXiv:2306.11868, 2023.   \n[47] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, et al. Argoverse 2: Next generation datasets for self-driving perception and forecasting. arXiv preprint arXiv:2301.00493, 2023.   \n[48] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.   \n[49] Lijun Yu, Jos\u00e9 Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, and Lu Jiang. Language model beats diffusion \u2013 tokenizer is key to visual generation, 2024.   \n[50] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 3560\u20133566. IEEE, 2023.   \n[51] Jinyun Zhou, Rui Wang, Xu Liu, Yifei Jiang, Shu Jiang, Jiaming Tao, Jinghao Miao, and Shiyu Song. Exploring imitation learning for autonomous driving with feedback synthesizer and differentiable rasterization. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1450\u20131457, 2021.   \n[52] Zikang Zhou, Jianping Wang, Yung-Hui Li, and Yu-Kai Huang. Query-centric trajectory prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17863\u201317873, 2023.   \n[53] Zikang Zhou, Zihao Wen, Jianping Wang, Yung-Hui Li, and Yu-Kai Huang. Qcnext: A next-generation framework for joint multi-agent trajectory prediction. arXiv preprint arXiv:2306.10508, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Implementation and Simulation Inference ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Architecture details Table 5 summarizes the hyperparameters of the different models used in our implementation. We train a single model to generate the future motion of all three categories (i.e., Vehicle, Pedestrian, Cyclist), with each category having its own motion token vocabulary. The input road token feature contains three types of information: the position of each road token point, the road token direction at each point, and the type of each road token. For the prediction head in each decoder layer, we use a three-layer MLP, and the model weights are not shared across different decoder layers. ", "page_idx": 13}, {"type": "table", "img_path": "2uy3LZHNIG/tmp/7710e2fcbd9fdf1950e887a060e5c861510206ea033b7f70f2a5b68477839bfe.jpg", "table_caption": ["Table 5: Hyperparameters of different SMART models "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Training details The simulation model is trained end-to-end for all three agent types using the AdamW optimizer [23]. Both the dropout rate and the weight decay rate are set to 0.1. The learning rate is decayed from 0.0002 to 0 using a cosine annealing scheduler. Training includes all vehicles within a scene. The batch size is set to 4, with a maximum GPU memory usage of 30GB. ", "page_idx": 13}, {"type": "text", "text": "Inference for WOSAC The test set comprises 44,920 scenes, and each scene requires running the model inference $32\\times T$ times to generate the 32 simulations for a group of agents. During model inference, each simulation step produces the classified distribution of next tokens. There are two options for next token sampling: selecting the maximum-likelihood token or sampling among the top- $\\cdot\\mathbf{k}$ motion tokens with the redistributed probability. The first approach, while accurate, tends to yield less varied generations. Conversely, opting for the top- $\\cdot\\mathbf{k}$ motion tokens encourages diversity but can compound errors, generating trajectories with unrealistic kinematic motions or even drift. To balance realism and diversity, we use top-5 sampling at every step during the simulation. Videos of rollouts can be found on our project page or supplementary materials. For each scenario, the SMART model directly controls all agents within the scene. Due to the focus of this article on the generalization and scalability of the model, we have achieved good results in specific scene generation without extensive exploration of detailed tricks. ", "page_idx": 13}, {"type": "text", "text": "A.2 Detailed comparison in the WOSAC leaderboard ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 6: Per-component metric results on the test split of WOMD, representing likelihoods. Due to updates in the calculation of WOSC evaluation metrics, methods are ranked by the composite metric on the 2023 Leaderboard for a broader comparison. For latest WOSC, please refer directly to the updated 2024 Leaderboard for detailed comparisons. ", "page_idx": 13}, {"type": "table", "img_path": "2uy3LZHNIG/tmp/6d5e589a15e0075237f54170d91168e372bfc3eec3a7236dcec2b88c4b57584d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "The Waymo Open Sim Agents Challenge (WOSAC) is a significant initiative aimed at advancing the development and evaluation of simulation agents for autonomous vehicles. This challenge leverages the Waymo Open Motion Dataset (WOMD) to provide high-fidelity object behaviors and shapes produced by a state-of-the-art offboard perception system. Participants are required to simulate scenarios involving up to 128 agents, focusing on closed-loop evaluation to ensure realism in agent behaviors and interactions. The evaluation framework employs various metrics, including kinematic features, interaction-based features, and map-based features, to assess the performance of simulation agents in generating realistic and diverse behaviors that match real-world driving data. WOSAC computes three metrics over nine measurements: kinematic metrics (linear speed, linear acceleration, angular speed, angular acceleration magnitude), object interaction metrics (distance to nearest object, collisions, time-to-collision), and map-based metrics (distance to road edge, road departures). ", "page_idx": 14}, {"type": "text", "text": "In the benchmark comparisons presented in Table 6, the SMART 7M method, developed by our team, demonstrates superior performance across multiple metrics, particularly excelling in interactive and safety-related indicators. Notably, SMART 7M achieved the highest scores in angular acceleration, distance to nearest object, collision avoidance, and off-road metrics, underscoring its effectiveness in complex driving scenarios. These results highlight the robustness of SMART 7M in ensuring safety and reliability, indicating its advanced capability in managing dynamic and potentially hazardous traffic conditions more effectively than other evaluated methods. This performance also suggests the potential of the SMART model to be applied to planning tasks. ", "page_idx": 14}, {"type": "text", "text": "A.3 Additional ablation studies ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Comparison of the scalability and generalization of different architectures This section presents experiments comparing the architecture proposed in this paper with the MVTE model. The MVTE model3, derived from MTR, represents continuous distribution regression models. The experimental results shown in Figure 5 indicate that distribution regression-based models have poor generalization capabilities across different datasets. Models trained with incremental data from other datasets performed worse overall than models trained solely on the WOMD. An interesting phenomenon is that although our private dataset contains more data than the NuPlan dataset, the performance of MVTE trained on it was inferior to that on the NuPlan dataset. This suggests that the distribution regression-based paradigm is likely to cause the model to overfit to a dataset. From the SMART w/o results, it can be seen that the model\u2019s generalization performance is poor, but the effect of incremental data can improve the performance compared to a single training dataset. Based on the above experiments, it can be inferred that discrete tokenization is a very effective way to eliminate the dataset gap. Moreover, autoregressive models based on cross-entropy classification loss are key to the scalability of trajectory generation models, which aligns with why large language models (LLMs) have significant scaling capabilities. ", "page_idx": 14}, {"type": "image", "img_path": "2uy3LZHNIG/tmp/92f82bd20dbae8ab2a30755fa2f08fe42191ca8bd9987316285d50e16d0c9c16.jpg", "img_caption": ["Figure 5: SMART w/o refers to SMART model without the road vector tokenization and noise tricks proposed in this paper. To ensure the fairness of the experiments, all model parameters were adjusted to the 90-100M. Models were trained on various datasets and validated only on the WOMD validation dataset "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Comparison of different tokenizer In Trajeglish[30], a detailed comparison of various discretized tokenizers is conducted. As introduced in Sec. 3.1 of this paper, we ultimately adopted the $\\mathbf{k}\\cdot$ -disks approach for token vocabulary construction. Prior to our work, no studies had attempted to construct a vocabulary for agent and road motion tokens using latent tokenizer methods [44]. Therefore, we drew on the visual domain\u2019s VQ-VAE approach to perform latent autoencoding of motion tokens and provided a comparison of this tokenizer with the method selected in this paper. ", "page_idx": 15}, {"type": "table", "img_path": "2uy3LZHNIG/tmp/982f5210ad7872c40729d02e5a0f6087d41a6538bddefdffaa88c45adfec1bcf.jpg", "table_caption": ["Table 7: Comparison of different tokenizer. Experimental results are based on the SMART 7M "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "From the results in Table 7, it is evident that VQ-VAE performs better on a single dataset compared to $\\mathbf{k}$ -disks. Specifically, both methods achieve similar results in interactive and map-based metrics, but VQ-VAE outperforms $\\boldsymbol{\\mathrm{k}}$ -disks in kinematic metrics. The $\\mathbf{k}$ -disks approach loses fine-grained trajectory information during discretization, whereas VQ-VAE better fits the true distribution of the dataset when reconstructing trajectories. However, when comparing the two methods\u2019 performance in zero-shot generalization, $\\mathbf{k}$ -disks significantly outperform VQ-VAE. We speculate that during the training of the VQ-VAE tokenizer to construct motion and road token vocabularies, the tokenizer may have already memorized or overfitted to the training dataset. Therefore, to achieve better generalization performance using the VQ-VAE approach, it is essential to pre-train the VQ-VAE tokenizer on a large-scale dataset. ", "page_idx": 15}, {"type": "text", "text": "Comparison of SMART models with different scales For language models, large and diverse datasets are relatively easy to obtain. In contrast, the autonomous driving motion domain lacks a data source of comparable size and diversity. To validate scaling laws on a larger dataset, we integrated data from Waymo, Nuplan, and our proprietary dataset. We introduced our proprietary dataset solely for validating scaling laws. For the WOSC leaderboard evaluation, we exclusively used the Waymo dataset. For generalization and other ablation experiments, we utilized both Nuplan and Waymo open-source datasets to facilitate reproducibility of the experiments by providing access to widely available datasets. Table 8 below summarizes the scenario count, duration, and total motion token count for each dataset. ", "page_idx": 15}, {"type": "text", "text": "The results in Table 9 highlight the performance of SMART models with different parameter scales across various metrics. As the model scale increases from SMART 1M to SMART 101M, there is a significant improvement in both the interactive metrics and the map-based metrics. This indicates that larger models are better at capturing interactions and understanding map-based context, leading to enhanced performance in these areas. However, the kinematic metrics show minimal variation. ", "page_idx": 15}, {"type": "table", "img_path": "2uy3LZHNIG/tmp/e0e8377b9048f54a5b529ccba93040c99cadad9aadc60cb58ff0a2798c752737.jpg", "table_caption": ["Table 8: Data sources "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "2uy3LZHNIG/tmp/deb61a6c0b6cb4ca05931b46fea997abfeac639927e82939a942666767964278.jpg", "table_caption": ["Table 9: Comparison of SMART models with different scales. Training time refers to the duration required for the model to converge using the entire dataset. Inference time refers to the time taken by the model to predict the next token for a single frame. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Additionally, the training time and average inference time increase substantially with larger models, reflecting the trade-off between model performance and computational cost. Validation is conducted every 50,000 train steps. The model is considered to have converged if there is no significant loss reduction or metric improvement after five consecutive validations. The training and inference time is measured on 32 NVIDIA TESLA V100 GPUs. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The abstract and introduction clearly outline the contributions of the SMART framework, emphasizing its novel autonomous driving motion generation paradigm, stateof-the-art performance, scalability, and zero-shot generalization capabilities. These claims are substantiated in the subsequent sections of the paper. Specifically, Sections 3 and 4 provide a detailed explanation of the methodology and experimental results that support these claims. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: In section 5, we discussed the potential limitations of this study and possible directions for future iterations of the model. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper does not propose new theorems; In Section4.3, it references the scale power-law and demonstrates through experiments that the proposed model conforms to this scale law. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper provides detailed instructions for replicating the experimental results. This includes methodology 3, data preprocessing and tokenization details3.1, model architecture3.2 and parameter settings3.3, training procedures4 and A.2, and any datasets used A.3. The training code has been made open-source to ensure the reproducibility of all results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 18}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The code and dataset used in this paper are all in the code base. This paper introduces private datasets in the experimental verification of scale laws, which are not open-source. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In section 4, this paper briefly introduces the training and validation test design, and in A.2, all modules and training hyperparameters are introduced in detail. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: In Section 4, we outline the composition of the training and testing datasets used. In Appendix A.1, we describe the sampling method employed for scenario generation. Due to WOSAC\u2019s evaluation requirement for the model to randomly generate multiple scenarios, there may be slight deviations in the final metrics compared to those reported in the paper. However, given the large scale of the evaluation dataset, consisting of over 20,000 samples, such error is negligible. Therefore, the experimental results in this paper can be accurately reproduced using the provided open-source code. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In appendix A.3, we have recorded the experimental testing environment, training time, and model inference time. All models in this paper were trained using 32 V100 GPUs. The training process requires a GPU memory of at least 25GB, while model inference typically requires only 10GB of memory. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In the introduction, we summarize this paper\u2019s contributions to autonomous driving applications, particularly highlighting the model\u2019s application in multi-agent simulation for autonomous driving. In the conclusion, we provide an outlook on the model\u2019s application in planning. We have released the largest model and codes to promote the exploration of autoregressive models for drive motion generation. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our proposed generative model can generate a large number of autonomous driving simulation scenarios or synthesis test data. Although safety is a critical aspect of autonomous driving, the current primary application is in simulation. Therefore, the model does not pose significant risks or potential for misuse at this stage. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 21}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The training and evaluation datasets used in this study are cited within this paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 22}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our study involves simulation-based autonomous driving scenarios without the direct involvement of human subjects or crowdsourcing, negating the need for IRB approval ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]