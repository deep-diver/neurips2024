[{"figure_path": "gDpWYpocE1/figures/figures_1_1.jpg", "caption": "Figure 1: Our attacker has no access to the model details of the LVLM. Meanwhile, we design a universal noise that is adversarial to multiple LVLM-driven tasks.", "description": "This figure illustrates the differences between existing attack methods and the proposed universal attack method.  Existing methods, such as white-box and gray/black-box attackers, rely on detailed model knowledge (white-box) or at least access to the vision encoder (gray/black-box) to generate task-specific adversarial perturbations. In contrast, the proposed method only has access to the input and output of the large vision-language model (LVLM) and aims to generate a universal noise (patch) that is adversarial against multiple downstream tasks, regardless of the specific task or prompt used.", "section": "1 Introduction"}, {"figure_path": "gDpWYpocE1/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the proposed universal adversarial attack against real-world LVLM models. To make the perturbation universally adversarial to multiple LVLM downstream tasks, we design a special patch-wise perturbation pattern by first initializing it on a fixed location of various image inputs and then optimizing it against images of different tasks. To update the adversarial patch by solely querying the LVLM, we introduce a language-based judge model to evaluate the LVLM output and design a novel importance-aware gradient approximation strategy to adaptively estimate gradients and adjust weights on gradient directions for optimizing the perturbations on input samples.", "description": "This figure illustrates the proposed universal adversarial attack against real-world Large Vision-Language Models (LVLMs).  It shows a three-stage process: patch initialization, gradient approximation, and iterative updates.  The attack uses a universal adversarial patch applied to various image inputs across different tasks, leveraging only the model's input and output.  A key component is an importance-aware gradient approximation to efficiently optimize the patch without access to internal model details.", "section": "3 Method"}, {"figure_path": "gDpWYpocE1/figures/figures_7_1.jpg", "caption": "Figure 3: Analysis on the attack \u201cUniversality\u201d on LLaVa model and DALLE-3 dataset.", "description": "The figure demonstrates the effectiveness of the proposed universal attack. It shows that the attack's effectiveness is not limited to a specific target text (\"Unknown\") but extends to various other target texts (e.g., \"I cannot answer\", \"I am sorry\", \"I hate people\"). The experiment results are shown in terms of averaged similarity scores on three tasks (Image Classification, Image Caption, VQA) for both the proposed universal attack and a baseline without universality. The results show the superior performance of the universal attack across all three tasks and various target texts.", "section": "4 Experiments"}, {"figure_path": "gDpWYpocE1/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization results on the targeted universal adversarial attack.", "description": "This figure visualizes the results of the targeted universal adversarial attack.  It shows several images, each with a clean version and four altered versions. The altered images each include an adversarial patch designed to cause the vision-language model to produce a specific target label (\u201cUnknown\u201d, \u201cI cannot answer\u201d, \u201cI am sorry\u201d, \u201cI hate people\u201d). The results demonstrate that the adversarial patch successfully fools the model, regardless of the specific target label selected, highlighting the universality of the attack. The uniformity of the patch's location across images also emphasizes the approach's task-agnostic nature.", "section": "4.2 Main Results"}, {"figure_path": "gDpWYpocE1/figures/figures_8_2.jpg", "caption": "Figure 2: Overview of the proposed universal adversarial attack against real-world LVLM models. To make the perturbation universally adversarial to multiple LVLM downstream tasks, we design a special patch-wise perturbation pattern by first initializing it on a fixed location of various image inputs and then optimizing it against images of different tasks. To update the adversarial patch by solely querying the LVLM, we introduce a language-based judge model to evaluate the LVLM output and design a novel importance-aware gradient approximation strategy to adaptively estimate gradients and adjust weights on gradient directions for optimizing the perturbations on input samples.", "description": "This figure illustrates the proposed universal adversarial attack against real-world Large Vision-Language Models (LVLMs).  It details a three-stage process: (1) Patch Initialization, where a patch is randomly placed and patterned; (2) Gradient Approximation, where a language-based judge model assesses the LVLM output, and an importance-aware gradient approximation strategy refines the patch; and (3) Patch Optimization, iteratively updating the patch to make it universally adversarial across multiple downstream tasks.", "section": "3 Method"}, {"figure_path": "gDpWYpocE1/figures/figures_18_1.jpg", "caption": "Figure 2: Overview of the proposed universal adversarial attack against real-world LVLM models. To make the perturbation universally adversarial to multiple LVLM downstream tasks, we design a special patch-wise perturbation pattern by first initializing it on a fixed location of various image inputs and then optimizing it against images of different tasks. To update the adversarial patch by solely querying the LVLM, we introduce a language-based judge model to evaluate the LVLM output and design a novel importance-aware gradient approximation strategy to adaptively estimate gradients and adjust weights on gradient directions for optimizing the perturbations on input samples.", "description": "This figure illustrates the proposed universal adversarial attack against real-world Large Vision-Language Models (LVLMs).  The process involves creating a universal adversarial patch that can be applied to various image inputs and fool different downstream tasks. The patch is optimized using a gradient approximation method that only queries the LVLM's input and output, without needing any model details. A language-based judge model is used to evaluate the LVLM's output and guide the optimization process.", "section": "3 Method"}, {"figure_path": "gDpWYpocE1/figures/figures_19_1.jpg", "caption": "Figure 2: Overview of the proposed universal adversarial attack against real-world LVLM models. To make the perturbation universally adversarial to multiple LVLM downstream tasks, we design a special patch-wise perturbation pattern by first initializing it on a fixed location of various image inputs and then optimizing it against images of different tasks. To update the adversarial patch by solely querying the LVLM, we introduce a language-based judge model to evaluate the LVLM output and design a novel importance-aware gradient approximation strategy to adaptively estimate gradients and adjust weights on gradient directions for optimizing the perturbations on input samples.", "description": "This figure illustrates the workflow of a universal adversarial attack against Large Vision-Language Models (LVLMs).  It highlights the process of creating a task-agnostic adversarial patch that can fool the model across multiple tasks by only querying the model's input and output. The key components are patch initialization, gradient approximation using an importance-aware strategy, and a judge function to evaluate the model's output based on the target label. The method iteratively refines the patch to enhance its effectiveness across tasks.", "section": "3 Method"}, {"figure_path": "gDpWYpocE1/figures/figures_22_1.jpg", "caption": "Figure 2: Overview of the proposed universal adversarial attack against real-world LVLM models. To make the perturbation universally adversarial to multiple LVLM downstream tasks, we design a special patch-wise perturbation pattern by first initializing it on a fixed location of various image inputs and then optimizing it against images of different tasks. To update the adversarial patch by solely querying the LVLM, we introduce a language-based judge model to evaluate the LVLM output and design a novel importance-aware gradient approximation strategy to adaptively estimate gradients and adjust weights on gradient directions for optimizing the perturbations on input samples.", "description": "This figure illustrates the proposed universal adversarial attack against real-world Large Vision-Language Models (LVLMs).  It details a three-stage process:  1) Patch Initialization, where a patch is randomly placed on images; 2) Gradient Approximation, leveraging a judge function and importance-aware gradient computation to estimate the gradient without model details; and 3) Patch Optimization, iteratively refining the patch to enhance its adversarial effect across multiple tasks.  The attacker only has access to inputs and outputs of the LVLM, highlighting the practical nature of this attack.", "section": "3 Method"}]