[{"figure_path": "gDpWYpocE1/tables/tables_6_1.jpg", "caption": "Table 1: Attack performance on different LVLM models across different datasets. We report the semantic similarity scores between the LVLM's output and the attackers' chosen label \"Unknown\". \"w/o importance\" denotes our full model without using importance weights in gradient approximation.", "description": "This table presents the results of the proposed universal adversarial attack against four different Large Vision-Language Models (LLaVAs) across three different datasets (MS-COCO, DALLE-3, VQAv2).  The \"semantic similarity scores\" represent how similar the model's output is to the target label (\"Unknown\") when the attack is applied.  Higher scores indicate a more successful attack.  The table compares the performance of the full attack method against a baseline (\"w/o importance\"), demonstrating the effectiveness of the proposed \"importance-aware gradient approximation\" strategy.  Each model and dataset is evaluated using three downstream vision-language tasks (Image Classification, Image Captioning, and VQA).", "section": "4.2 Main Results"}, {"figure_path": "gDpWYpocE1/tables/tables_7_1.jpg", "caption": "Table 2: Attack performance on LLaVA model and DALLE-3 dataset with different target labels.", "description": "This table presents the results of an adversarial attack experiment on the LLaVA model using the DALLE-3 dataset. The experiment aims to evaluate the effectiveness of the attack against different target labels. The attack method is evaluated with and without importance weights in the gradient approximation step. The table shows the semantic similarity scores between the LVLMs output and the attacker's chosen label for four different target labels: \"Unknown\", \"I cannot answer\", \"I am sorry\", and \"I hate people\".  The scores are provided for three downstream tasks: Image Classification, Image Captioning, and VQA (Visual Question Answering), and also the overall average.", "section": "4.2 Main Results"}, {"figure_path": "gDpWYpocE1/tables/tables_7_2.jpg", "caption": "Table 3: Comparison with existing LVLM attack: MF-Attack [33]. For a fair comparison, experiments are conducted on the same ImageNet-1k dataset [83] in the VQA task.", "description": "This table compares the performance of the proposed universal and practical attack against the MF-Attack [33], a transfer-based black-box attack.  The comparison uses the ImageNet-1k dataset [83] and focuses on the VQA task to ensure a fair evaluation.  The results highlight the superior performance of the proposed method.", "section": "4.2 Main Results"}, {"figure_path": "gDpWYpocE1/tables/tables_7_3.jpg", "caption": "Table 4: Comparison with existing LVLM attack: CroPA [31]. For a fair comparison, we follow CroPA to evaluate the same ASR metric on the same OpenFlamingo model and MS-COCO dataset.", "description": "This table compares the performance of the proposed universal and practical attack against the white-box attack method CroPA [31] on the OpenFlamingo model and MS-COCO dataset. The comparison uses the ASR metric and shows that the proposed attack outperforms CroPA in image classification, image captioning, and VQA tasks, demonstrating its effectiveness.", "section": "4.2 Main Results"}, {"figure_path": "gDpWYpocE1/tables/tables_8_1.jpg", "caption": "Table 5: Ablation study on the patch size on LLaVA model and DALLE-3 dataset.", "description": "This table presents the results of an ablation study conducted to determine the optimal patch size for a universal adversarial attack against the LLaVA and DALLE-3 models.  The study varied the patch size (S<sub>p</sub>) and measured the attack's performance across three image-to-text tasks: Image Classification, Image Captioning, and Visual Question Answering (VQA). The results show how the performance (measured as semantic similarity scores) changes as the patch size is increased, indicating the optimal size for achieving the best attack effectiveness.", "section": "4.3 Ablation"}, {"figure_path": "gDpWYpocE1/tables/tables_9_1.jpg", "caption": "Table 7: Attack performance against black-box defense strategies.", "description": "This table shows the attack success rate (ASR) of the proposed universal attack against four different black-box defense strategies.  The ASR is calculated across three image-to-text tasks (Image Classification, Image Captioning, and VQA). The results indicate the robustness of the proposed attack against various defense mechanisms, highlighting its effectiveness even when the model's internal details are not accessible to the attacker.", "section": "4.4 Robustness to Defense Strategy"}, {"figure_path": "gDpWYpocE1/tables/tables_9_2.jpg", "caption": "Table 1: Attack performance on different LVLM models across different datasets. We report the semantic similarity scores between the LVLM's output and the attackers' chosen label \"Unknown\". \"w/o importance\" denotes our full model without using importance weights in gradient approximation.", "description": "This table presents the results of the proposed universal adversarial attack against four different large vision-language models (LLaVA, MiniGPT-4, Flamingo, and BLIP-2) across three different datasets (MS-COCO, DALLE-3, and VQAv2).  The performance is measured using semantic similarity scores between the model's output and the target label \"Unknown.\"  The table compares the full attack model to a variant (\"w/o importance\") that omits the importance-aware weights during gradient approximation, showcasing the impact of this key component on attack effectiveness.  Each model and dataset is tested with three different image-text tasks: Image Classification, Image Captioning, and Visual Question Answering (VQA).", "section": "4.2 Main Results"}, {"figure_path": "gDpWYpocE1/tables/tables_17_1.jpg", "caption": "Table 1: Attack performance on different LVLM models across different datasets. We report the semantic similarity scores between the LVLM's output and the attackers' chosen label \"Unknown\". \"w/o importance\" denotes our full model without using importance weights in gradient approximation.", "description": "This table presents the results of the proposed universal adversarial attack against four different large vision-language models (LLaVA, MiniGPT-4, Flamingo, and BLIP-2) across three datasets (MS-COCO, DALLE-3, and VQAv2).  The performance is measured using semantic similarity scores between the model's output and the target label \"Unknown.\"  For each model and dataset, the table shows the similarity scores for three different conditions: clean images (no attack), an attack without importance weighting, and the full attack with importance weighting. The results demonstrate the effectiveness of the proposed attack, with the full attack consistently achieving the highest similarity scores.", "section": "4.2 Main Results"}, {"figure_path": "gDpWYpocE1/tables/tables_17_2.jpg", "caption": "Table 10: Ablation on the task number during the model querying, tested on the LLaVA model and DALLE-3 dataset.", "description": "This table presents the results of an ablation study investigating the impact of the number of tasks used during the model querying phase of the universal adversarial attack. It compares the attack performance with and without the importance-aware weights, across three different numbers of tasks (1, 2, and 3), and for three downstream tasks (Image Classification, Image Caption, and VQA). The overall performance is also presented.", "section": "4.3 Ablation"}, {"figure_path": "gDpWYpocE1/tables/tables_18_1.jpg", "caption": "Table 11: Ablation on the image number during the patch generation, tested on the LLaVA model and DALLE-3 dataset.", "description": "This table presents the results of an ablation study on the proposed universal adversarial attack. The experiment varied the number of images used during the patch generation process, while keeping the number of tasks constant at three.  The results show how the attack performance, measured as semantic similarity scores (ImageClassification, ImageCaption, VQA, and Overall), changes with different numbers of images (100, 300, and 500).  The table also compares the performance with and without incorporating importance-aware weights ('w/o importance' vs. 'Full attack').  The study aimed to determine the impact of image diversity on the effectiveness of the universal patch.", "section": "4.3 Ablation"}, {"figure_path": "gDpWYpocE1/tables/tables_20_1.jpg", "caption": "Table 1: Attack performance on different LVLM models across different datasets. We report the semantic similarity scores between the LVLM's output and the attackers' chosen label \"Unknown\". \"w/o importance\" denotes our full model without using importance weights in gradient approximation.", "description": "This table presents the results of the universal adversarial attack against four different large vision-language models (LLaVA, MiniGPT-4, Flamingo, and BLIP-2) across three datasets (MS-COCO, DALLE-3, and VQAv2).  The performance metric is the semantic similarity score between the model's output and the target label \"Unknown\", which measures how well the model's response aligns with the intended adversarial output.  The table also shows a comparison with a version of the attack that does not employ importance-aware weights in its gradient approximation, highlighting the impact of this technique on the attack's effectiveness.", "section": "4.2 Main Results"}, {"figure_path": "gDpWYpocE1/tables/tables_20_2.jpg", "caption": "Table 13: Attack performance of our Full Attack on LLaVA model and DALLE-3 dataset when target labels are set to long and special target texts.", "description": "This table presents the results of the \"Full Attack\" method from the paper, applied to the LLaVA model and DALLE-3 dataset.  Instead of using standard, short target labels, the experiment used longer and more descriptive labels. The table shows the semantic similarity scores between the model's output and the target text for different tasks (Image Classification, Image Caption, and VQA) and overall.", "section": "4.2 Main Results"}, {"figure_path": "gDpWYpocE1/tables/tables_21_1.jpg", "caption": "Table 1: Attack performance on different LVLM models across different datasets. We report the semantic similarity scores between the LVLM's output and the attackers' chosen label \"Unknown\". \"w/o importance\" denotes our full model without using importance weights in gradient approximation.", "description": "This table presents the results of the proposed universal adversarial attack against four different Large Vision-Language Models (LVLMs): LLaVA, MiniGPT-4, Flamingo, and BLIP-2.  The attack's performance is evaluated across three different datasets: MS-COCO, DALLE-3, and VQAv2. For each LVLMs and dataset combination, the table shows the semantic similarity scores (using a cosine similarity metric) between the model's output when attacked and the target label \"Unknown\".  The table also includes a comparison with a version of the attack that does not utilize importance weights, which helps assess the contribution of this technique to the attack's success. Higher scores indicate better attack performance (closer to the target label).", "section": "4.2 Main Results"}]