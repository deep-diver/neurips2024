{"references": [{"fullname_first_author": "Dosovitskiy, A.", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This paper introduces the Vision Transformer (ViT), a foundational model for many image-related tasks, and is heavily referenced in the current paper's methodology."}, {"fullname_first_author": "Raffel, C.", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-00-00", "reason": "This paper introduces the Text-to-Text Transformer (T5), a crucial model for processing text data, used extensively in the current paper's multimodal approach."}, {"fullname_first_author": "Mizrahi, D.", "paper_title": "4M: Massively multimodal masked modeling", "publication_date": "2023-00-00", "reason": "This paper introduces the 4M model, which serves as the foundation for the current paper's approach, significantly influencing its architecture and training methodology."}, {"fullname_first_author": "van den Oord, A.", "paper_title": "Neural discrete representation learning", "publication_date": "2017-00-00", "reason": "This paper introduces Vector Quantized Variational Autoencoders (VQ-VAEs), a key technique used for tokenization in the current paper's multimodal model."}, {"fullname_first_author": "He, K.", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-00-00", "reason": "This paper introduces Masked Autoencoders (MAEs), a significant model for self-supervised visual learning, which is referenced for its influence on the current paper's training approach."}]}