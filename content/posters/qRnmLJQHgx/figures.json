[{"figure_path": "qRnmLJQHgx/figures/figures_0_1.jpg", "caption": "Figure 1: We demonstrate training a single model on tens of highly diverse modalities without a loss in performance compared to specialized single/few task models. The modalities are mapped to discrete tokens using modality-specific tokenizers. The model can generate any of the modalities from any subset of them.", "description": "This figure illustrates the architecture of the 4M-21 model, highlighting its ability to handle a wide variety of input modalities (images, text, depth maps, etc.) and generate corresponding outputs in any of those modalities. The model uses modality-specific tokenizers to convert the diverse inputs into a common representation that can be processed by a transformer-based encoder and decoder. This any-to-any capability is a key feature of the 4M-21 model, allowing for flexible and efficient multimodal processing.", "section": "Abstract"}, {"figure_path": "qRnmLJQHgx/figures/figures_2_1.jpg", "caption": "Figure 2: One-to-all generation. 4M-21 can generate all modalities from any given input modality and can benefit from chained generation [65]. Notice the high consistency among the predictions of all modalities for one input. Each row starts from a different modality coming from the same scene. Highlighted in green are new input/output pairs that 4M [65] cannot predict nor accept as input. Note that, while this figure shows predictions from a single input, 4M-21 can generate any modality from any subset of all modalities.", "description": "This figure demonstrates the any-to-any generation capabilities of the 4M-21 model.  Starting from a single input modality (e.g., RGB image, depth map, caption), the model can generate all other modalities. The consistency across predictions for different modalities from the same scene highlights the model's robustness and coherence. New capabilities beyond the previous 4M model are highlighted in green.", "section": "Method"}, {"figure_path": "qRnmLJQHgx/figures/figures_4_1.jpg", "caption": "Figure 3: Tokenization overview. We employ suitable tokenization schemes for different modalities based on their format and performance. For image-like modalities and feature maps, we use spatial VQ-VAEs [66] with optional diffusion decoders for detail rich modalities like RGB. For non-spatial modalities like global tokens or parameterized poses, we compress them to a fixed number of discrete tokens using Memcodes [62] with MLP encoders and decoders. All sequence modalities are encoded as text using WordPiece [24]. The shown examples are real tokenizer reconstructions. Notice the low reconstruction error. See appendix D for more details and Fig. 13 for visualizations.", "description": "This figure illustrates the different tokenization methods used for various modalities in the 4M-21 model.  Image-like data is tokenized using spatial VQ-VAEs, with diffusion decoders optionally used for high-detail modalities. Non-spatial data is tokenized using MLPs and Memcodes, while sequence data is handled with WordPiece tokenization.  The low reconstruction error highlights the effectiveness of these methods.", "section": "Method"}, {"figure_path": "qRnmLJQHgx/figures/figures_5_1.jpg", "caption": "Figure 2: One-to-all generation. 4M-21 can generate all modalities from any given input modality and can benefit from chained generation [65]. Notice the high consistency among the predictions of all modalities for one input. Each row starts from a different modality coming from the same scene. Highlighted in green are new input/output pairs that 4M [65] cannot predict nor accept as input. Note that, while this figure shows predictions from a single input, 4M-21 can generate any modality from any subset of all modalities.", "description": "This figure demonstrates the any-to-all generation capability of the 4M-21 model.  It shows that the model can generate all 21 modalities starting from any single input modality.  The consistency of the results across modalities, even when chaining generation (using an output modality as input for subsequent generation), is highlighted.  New capabilities of 4M-21 compared to the previous 4M model are shown in green. The figure emphasizes that, while only one example input is shown per row, the model's capability extends to generating any modality from any combination of input modalities.", "section": "Method"}, {"figure_path": "qRnmLJQHgx/figures/figures_6_1.jpg", "caption": "Figure 5: Different modes of multimodal retrieval. We perform multimodal retrievals by predicting global embeddings (here shown for DINOv2) from a given input (of any modality) using 4M-21 and comparing the cosine distances between the query and retrieval set embeddings. Left: Retrieving RGB images from distinctly different query modalities (here RGB, segmentation map, edges, depth map, color palette, and caption). Middle: Retrieving any modality using any other modality as the query input. Each query modality constrains the retrievals differently, e.g. here the RGB image and caption queries always yield Neuschwanstein castle retrievals. In contrast, for depth and semantic queries, the scene is more ambiguous, thus they retrieve other buildings with similar characteristics. Right: We can also combine any subset of modalities to define the query input, e.g. surface normals and a color palette, to better control the retrieval. See appendix B.2 for more results.", "description": "This figure demonstrates the multimodal retrieval capabilities of the 4M-21 model.  It shows three scenarios: retrieving RGB images from various input modalities (left), retrieving any modality from any other modality (middle), and retrieving using a combination of modalities (right). The results highlight the model's ability to perform cross-modal retrieval effectively and control the retrieval process through input modality selection.", "section": "3.2 Multimodal retrieval"}, {"figure_path": "qRnmLJQHgx/figures/figures_7_1.jpg", "caption": "Figure 6: Out-of-the-box vision tasks. Given an RGB image, 4M-21 can predict all tasks successfully, as can be seen from their high consistency with the pseudo labels. See fig. 7 for more results.", "description": "This figure demonstrates the model's ability to perform several vision tasks (caption, bounding boxes, semantic segmentation, depth, CLIP, surface normals, human poses, DINOV2, ImageBind, metadata, Canny edges, SAM edges, SAM instances, and color palette) given only an RGB image as input.  The predictions are compared against pseudo labels (predictions from specialist models), showcasing the model's strong zero-shot capabilities.", "section": "3 Evaluating out-of-the-box capabilities"}, {"figure_path": "qRnmLJQHgx/figures/figures_17_1.jpg", "caption": "Figure 6: Out-of-the-box vision tasks. Given an RGB image, 4M-21 can predict all tasks successfully, as can be seen from their high consistency with the pseudo labels. See fig. 7 for more results.", "description": "This figure demonstrates the model's ability to perform various vision tasks (surface normals, depth estimation, semantic segmentation, instance segmentation,  keypoints detection, and caption generation) directly from an RGB image input. The high consistency between the model's predictions and the pseudo labels from task-specific experts suggests the model's strong performance on these tasks, even without any specific task training.  Additional results illustrating more tasks are shown in Figure 7.", "section": "3 Evaluating out-of-the-box capabilities"}, {"figure_path": "qRnmLJQHgx/figures/figures_18_1.jpg", "caption": "Figure 8: Steerable multimodal generation using metadata. This is an extension of fig. 4 and shows our model's capability of generating multimodal data by conditioning on a wide set of controls. The common caption for all examples is \"a painting of a bridge in a lush forest\".", "description": "This figure demonstrates the model's ability to generate diverse modalities using metadata as control inputs.  It shows how various image attributes (brightness, saturation, colorfulness, etc.), geometric properties (complexity, occlusion), and semantic information (walkability, diversity, objectness, instance counts, etc.) can be used to steer the generation process, resulting in different outputs for the same caption. This showcases fine-grained control over multimodal generation beyond simple text prompts.", "section": "B Multimodal Capabilities"}, {"figure_path": "qRnmLJQHgx/figures/figures_19_1.jpg", "caption": "Figure 9: Probing with grounded generation. This is an extension of fig. 4 and further shows our model's capability on performing generation by conditioning on multimodal input. The top row varies SAM instances and combines them with a fixed caption and color palette input. The bottom row fixes the normals and caption inputs and varies the color palette.", "description": "This figure shows the model's ability to generate images conditioned on various inputs. The top half demonstrates the effect of changing the shape of SAM instances while keeping the caption and color palette consistent. The bottom half shows how changing the color palette impacts the generated image while keeping the caption and normal map consistent.", "section": "B Multimodal Capabilities"}, {"figure_path": "qRnmLJQHgx/figures/figures_19_2.jpg", "caption": "Figure 9: Probing with grounded generation. This is an extension of fig. 4 and further shows our model's capability on performing generation by conditioning on multimodal input. The top row varies SAM instances and combines them with a fixed caption and color palette input. The bottom row fixes the normals and caption inputs and varies the color palette.", "description": "This figure demonstrates the model's ability to generate images based on different combinations of input modalities.  The top half shows how varying SAM (Segment Anything Model) instances impacts the generated image while keeping the caption and color palette constant. The bottom half shows the effect of varying color palettes while keeping the caption and normals constant.  This highlights the model's fine-grained control over image generation through multimodal conditioning.", "section": "B Multimodal Capabilities"}, {"figure_path": "qRnmLJQHgx/figures/figures_20_1.jpg", "caption": "Figure 2: One-to-all generation. 4M-21 can generate all modalities from any given input modality and can benefit from chained generation [65]. Notice the high consistency among the predictions of all modalities for one input. Each row starts from a different modality coming from the same scene. Highlighted in green are new input/output pairs that 4M [65] cannot predict nor accept as input. Note that, while this figure shows predictions from a single input, 4M-21 can generate any modality from any subset of all modalities.", "description": "This figure demonstrates the any-to-any generation capability of the 4M-21 model.  Starting with a single input modality (e.g., RGB image, depth map, caption), the model generates all other modalities in a consistent manner.  The high consistency highlights the model's ability to understand the underlying scene regardless of the input modality.  Green highlights show new modality pairs that previous models like 4M couldn't handle.", "section": "Method"}, {"figure_path": "qRnmLJQHgx/figures/figures_21_1.jpg", "caption": "Figure 5: Different modes of multimodal retrieval. We perform multimodal retrievals by predicting global embeddings (here shown for DINOv2) from a given input (of any modality) using 4M-21 and comparing the cosine distances between the query and retrieval set embeddings. Left: Retrieving RGB images from distinctly different query modalities (here RGB, segmentation map, edges, depth map, color palette, and caption). Middle: Retrieving any modality using any other modality as the query input. Each query modality constrains the retrievals differently, e.g. here the RGB image and caption queries always yield Neuschwanstein castle retrievals. In contrast, for depth and semantic queries, the scene is more ambiguous, thus they retrieve other buildings with similar characteristics. Right: We can also combine any subset of modalities to define the query input, e.g. surface normals and a color palette, to better control the retrieval. See appendix B.2 for more results.", "description": "This figure demonstrates the multimodal retrieval capabilities of the 4M-21 model.  It shows three different retrieval scenarios: retrieving RGB images from various input modalities, retrieving any modality from any other modality, and combining multiple input modalities to refine retrieval results.  Each scenario highlights how different input modalities influence the retrieval process, showcasing the model's ability to handle diverse input types and perform cross-modal retrieval effectively.", "section": "3 Multimodal capabilities"}, {"figure_path": "qRnmLJQHgx/figures/figures_22_1.jpg", "caption": "Figure 5: Different modes of multimodal retrieval. We perform multimodal retrievals by predicting global embeddings (here shown for DINOv2) from a given input (of any modality) using 4M-21 and comparing the cosine distances between the query and retrieval set embeddings. Left: Retrieving RGB images from distinctly different query modalities (here RGB, segmentation map, edges, depth map, color palette, and caption). Middle: Retrieving any modality using any other modality as the query input. Each query modality constrains the retrievals differently, e.g. here the RGB image and caption queries always yield Neuschwanstein castle retrievals. In contrast, for depth and semantic queries, the scene is more ambiguous, thus they retrieve other buildings with similar characteristics. Right: We can also combine any subset of modalities to define the query input, e.g. surface normals and a color palette, to better control the retrieval. See appendix B.2 for more results.", "description": "This figure demonstrates the multimodal retrieval capabilities of the 4M-21 model.  It shows three different retrieval scenarios:\n\n1.  **Any-to-RGB:** Retrieving RGB images using various input modalities (RGB image, segmentation, edges, depth, normals, caption). The results highlight that different input modalities lead to varying retrieval results. \n2. **Any-to-Any:** Retrieving any modality using any other modality as input, demonstrating the model's flexibility to handle different input-output modality pairs.  \n3. **Multimodal Retrieval:** Combining multiple modalities to define the query input, which results in more focused and controlled retrieval. ", "section": "3.2 Multimodal retrieval"}, {"figure_path": "qRnmLJQHgx/figures/figures_25_1.jpg", "caption": "Figure 13: Tokenizer reconstruction quality. Our multimodal tokenizers can reconstruct the ground truth well. Here we show sample reconstructions of RGB, depth, surface normals, semantic segmentation, SAM instances, and canny edges on (pseudo) labels from the CC12M validation set at 224 \u00d7 224 resolution. Quantitative evaluations are provided in table 1 for different tasks and datasets (last row, Tokenizer bound), confirming the reconstruction quality.", "description": "This figure demonstrates the quality of the tokenization process for various input modalities. It shows several examples of input modalities (RGB, Depth, Surface Normals, Semantic Segmentation, SAM Instances, and Canny Edges) and their corresponding reconstruction after applying the modality-specific tokenizers. The results demonstrate that the tokenizers effectively capture the essential information of the input modalities, highlighting their capacity to convert diverse data into discrete tokens for the model's processing.", "section": "D Multimodal Dataset & Tokenization Details"}, {"figure_path": "qRnmLJQHgx/figures/figures_27_1.jpg", "caption": "Figure 2: One-to-all generation. 4M-21 can generate all modalities from any given input modality and can benefit from chained generation [65]. Notice the high consistency among the predictions of all modalities for one input. Each row starts from a different modality coming from the same scene. Highlighted in green are new input/output pairs that 4M [65] cannot predict nor accept as input. Note that, while this figure shows predictions from a single input, 4M-21 can generate any modality from any subset of all modalities.", "description": "This figure demonstrates the any-to-any generation capability of the 4M-21 model.  It shows that the model can generate any of the 21 modalities from a single input modality, and that this generation remains consistent across multiple modalities.  Highlighed in green are results that the previous 4M model could not generate.  The figure highlights the model's ability to chain generations, using the output of one modality prediction as the input for another.", "section": "Method"}, {"figure_path": "qRnmLJQHgx/figures/figures_30_1.jpg", "caption": "Figure 13: Tokenizer reconstruction quality. Our multimodal tokenizers can reconstruct the ground truth well. Here we show sample reconstructions of RGB, depth, surface normals, semantic segmentation, SAM instances, and canny edges on (pseudo) labels from the CC12M validation set at 224 \u00d7 224 resolution. Quantitative evaluations are provided in table 1 for different tasks and datasets (last row, Tokenizer bound), confirming the reconstruction quality.", "description": "This figure showcases the reconstruction quality of different modality tokenizers used in the 4M-21 model.  It shows examples of reconstructed images and other modalities (depth, surface normals, semantic segmentations, SAM instances, and Canny edges) compared to their ground truth versions. The quality is generally high, demonstrating effective tokenization.", "section": "D Multimodal Dataset & Tokenization Details"}, {"figure_path": "qRnmLJQHgx/figures/figures_31_1.jpg", "caption": "Figure 13: Tokenizer reconstruction quality. Our multimodal tokenizers can reconstruct the ground truth well. Here we show sample reconstructions of RGB, depth, surface normals, semantic segmentation, SAM instances, and canny edges on (pseudo) labels from the CC12M validation set at 224 \u00d7 224 resolution. Quantitative evaluations are provided in table 1 for different tasks and datasets (last row, Tokenizer bound), confirming the reconstruction quality.", "description": "This figure shows the reconstruction quality of different tokenizers used in the 4M-21 model. It provides visual examples of how well the tokenizers reconstruct various modalities (RGB images, depth maps, surface normals, semantic segmentations, SAM instances, and Canny edges) from their corresponding token representations.  The results demonstrate that the tokenizers effectively capture the essential information of each modality.", "section": "D Multimodal Dataset & Tokenization Details"}]