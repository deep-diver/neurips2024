{"references": [{"fullname_first_author": "Diederik P. Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2015-00-00", "reason": "This paper introduces the Adam optimizer, a widely used algorithm in deep learning that the current paper improves upon."}, {"fullname_first_author": "Sashank J. Reddi", "paper_title": "On the convergence of Adam and beyond", "publication_date": "2018-00-00", "reason": "This paper reveals convergence issues with the Adam optimizer, motivating the need for improved optimization strategies as explored in the current work."}, {"fullname_first_author": "Greg Yang", "paper_title": "Tuning large neural networks via zero-shot hyperparameter transfer", "publication_date": "2021-00-00", "reason": "This paper introduces a method for transferring hyperparameters across different network scales, which the current paper generalizes and improves upon."}, {"fullname_first_author": "Greg Yang", "paper_title": "Tensor programs VI: Feature learning in infinite depth neural networks", "publication_date": "2024-00-00", "reason": "This paper provides a theoretical framework for understanding feature learning in deep networks, which the current paper builds upon to develop a new norm for weight updates."}, {"fullname_first_author": "Blake Bordelon", "paper_title": "Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit", "publication_date": "2024-00-00", "reason": "This paper provides another theoretical foundation related to hyperparameter transfer in deep networks, enriching the context of the current paper's contributions."}]}