[{"figure_path": "SFxAjB7UXx/figures/figures_1_1.jpg", "caption": "Figure 1: Learning rate transfer in the modular norm. We train GPT with context length 128 for 10k steps on OpenWebText. Left: Learning rate sweeps for normed Adam (Adam with updates normalized in the modular norm) with three transformer blocks and varying width. The optimal learning rate (marked by red dots) transfers well across scales. Mid-left: The same, but varying the number of blocks at width 128. Mid-right: Comparing normed versus unnormed Adam and SGD at fixed learning rate and varying width. For each method, we tune the learning rate at the scale marked by the dotted line. The normed methods scale better. Right: The same, but scaling number of blocks.", "description": "This figure demonstrates the effectiveness of the modular norm in enabling learning rate transfer across different network scales.  The plots show training loss curves for various optimizers (Adam and SGD, both with and without modular norm normalization) while varying the width and depth of a GPT model trained on OpenWebText. The consistent optimal learning rates across scales highlight the modular norm's ability to improve the scalability and stability of training.", "section": "2 Descent in Normed Spaces"}, {"figure_path": "SFxAjB7UXx/figures/figures_3_1.jpg", "caption": "Figure 2: Modules and trees of modules. A module is an object that maps an input and a weight vector to an output. Left: In addition to the standard forward function, our modules are endowed with two numbers\u2014a mass and sensitivity\u2014and a norm. Middle: New compound modules are built via the binary operations of composition and concatenation. We provide rules for composing and concatenating all module attributes. Right: Compound modules are binary trees, where the leaves are modules and the internal nodes compose and concatenate their children. Here we illustrate a sum of modules, which leverages a special utility module Add\u2014see Table 1 for more on this.", "description": "This figure illustrates the concept of modules and how they are combined to form more complex neural networks.  The left panel shows a single module with its attributes (forward function, mass, sensitivity, and norm). The middle panel demonstrates how new modules are created through composition and concatenation of existing modules.  The right panel depicts a compound module, represented as a binary tree where leaf nodes are individual modules and internal nodes represent the combination operations (composition, concatenation, and addition).", "section": "3 Constructing the Modular Norm"}, {"figure_path": "SFxAjB7UXx/figures/figures_6_1.jpg", "caption": "Figure 3: Exploring mass allocation. We tune the total mass of the hidden layers, training with normed Adam. Left group: Learning rate sweeps for ResMLP on CIFAR-10, for varying depth and mass. The bottom right subplot reports the best train loss at each mass and depth. Mass 0.5 was best at all depths. Right group: Learning rate sweeps for GPT on OpenWebText, for varying mass. Both optimal mass and learning rate transferred from the small model (top) to the large model (bottom).", "description": "This figure shows the effect of mass allocation on the learning rate and training loss for two different models: ResMLP on CIFAR-10 and GPT on OpenWebText.  The left side demonstrates how the optimal mass (controlling the proportion of learning for different layers) affects training loss at various depths for ResMLP. The right side shows how the optimal mass changes the learning rate and training loss for GPT across different scales, demonstrating transferability of the optimal hyperparameters.", "section": "Mass allocation in compound modules"}, {"figure_path": "SFxAjB7UXx/figures/figures_8_1.jpg", "caption": "Figure 4: Learning rate transfer on CIFAR-10. We tune the learning rate on a small model\u2014at the scale marked by the dotted line\u2014and test the performance on models of increasing width and depth at this fixed learning rate. We find that normed Adam and SGD scale better than their unnormed counterparts on both ResMLPs and ResNets. See Figure 1 for the same experiment on GPT.", "description": "This figure shows the results of an experiment to test the scalability of training with normed Adam and SGD optimizers.  The learning rate was tuned on a small model, and then the performance of models with increasing width and depth was tested using that same fixed learning rate. The results demonstrate that the normed Adam and SGD optimizers scale better than their unnormed counterparts on both ResMLP and ResNet models.", "section": "4 Experiments"}, {"figure_path": "SFxAjB7UXx/figures/figures_26_1.jpg", "caption": "Figure 1: Learning rate transfer in the modular norm. We train GPT with context length 128 for 10k steps on OpenWebText. Left: Learning rate sweeps for normed Adam (Adam with updates normalized in the modular norm) with three transformer blocks and varying width. The optimal learning rate (marked by red dots) transfers well across scales. Mid-left: The same, but varying the number of blocks at width 128. Mid-right: Comparing normed versus unnormed Adam and SGD at fixed learning rate and varying width. For each method, we tune the learning rate at the scale marked by the dotted line. The normed methods scale better. Right: The same, but scaling number of blocks.", "description": "This figure demonstrates the learning rate transferability in the modular norm across different network scales.  It shows that normalizing weight updates using the modular norm allows for consistent optimal learning rates across varying network width and depth, unlike standard optimizers such as Adam and SGD. The leftmost panel shows learning rate sweeps for normed Adam with a fixed number of blocks and varying width. The red dots show that the optimal learning rates are consistent across widths.  The next panel shows the same experiment, but with varying numbers of blocks. The next panel compares normed versus unnormed Adam and SGD.  The final panel shows scaling by number of blocks. In all cases, the modular norm improves the transferability of the learning rate.", "section": "2 Descent in Normed Spaces"}, {"figure_path": "SFxAjB7UXx/figures/figures_27_1.jpg", "caption": "Figure 3: Exploring mass allocation. We tune the total mass of the hidden layers, training with normed Adam. Left group: Learning rate sweeps for ResMLP on CIFAR-10, for varying depth and mass. The bottom right subplot reports the best train loss at each mass and depth. Mass 0.5 was best at all depths. Right group: Learning rate sweeps for GPT on OpenWebText, for varying mass. Both optimal mass and learning rate transferred from the small model (top) to the large model (bottom).", "description": "This figure shows the effect of mass allocation on the learning rate in two different models, ResMLP (on CIFAR-10 dataset) and GPT (on OpenWebText dataset).  The left side displays learning rate sweeps for ResMLP with varying depth and mass, indicating that a mass of 0.5 consistently yielded the best results across different depths. The right side shows learning rate sweeps for GPT with varying mass; the optimal mass and learning rate are transferable from smaller to larger models, highlighting the transferability aspect of the modular norm.", "section": "3 Mass allocation in compound modules"}, {"figure_path": "SFxAjB7UXx/figures/figures_28_1.jpg", "caption": "Figure 7: Mass and learning rate sweeps across datasets of increasing difficulty. A small GPT architecture of width 128 and 3 transformer blocks was trained on the Shakespeare, TinyStores and OpenWebText datasets. We varied the learning rate as well as the total mass of the blocks. Optimal mass and learning rate seem to transfer reasonably well from TinyStories to OpenWebText, and less well from the much smaller Shakespeare dataset.", "description": "This figure displays the results of training a small GPT model (width 128, 3 blocks) on three datasets of increasing size and complexity: Shakespeare, TinyStories, and OpenWebText.  The experiment swept across different learning rates and total masses assigned to the model's blocks. The results show that optimal learning rates and masses transfer well between TinyStories and OpenWebText (similar in size and complexity), but less so when transferring from the much smaller Shakespeare dataset.", "section": "D.6 Mass allocation"}, {"figure_path": "SFxAjB7UXx/figures/figures_29_1.jpg", "caption": "Figure 8: Context length transfer. We trained GPTs of various context lengths using normed Adam. As can be seen, learning rate transferred quite well across context length.", "description": "This figure shows the results of training GPT models with different context lengths using the normed Adam optimizer.  The x-axis represents the learning rate used during training, and the y-axis represents the test loss achieved. Each line represents a different context length, ranging from 32 to 1024.  The figure demonstrates that the optimal learning rate remains relatively consistent across different context lengths, supporting the claim that the modular norm allows for better learning rate transfer across various scales.", "section": "D.7 Context length"}, {"figure_path": "SFxAjB7UXx/figures/figures_30_1.jpg", "caption": "Figure 1: Learning rate transfer in the modular norm. We train GPT with context length 128 for 10k steps on OpenWebText. Left: Learning rate sweeps for normed Adam (Adam with updates normalized in the modular norm) with three transformer blocks and varying width. The optimal learning rate (marked by red dots) transfers well across scales. Mid-left: The same, but varying the number of blocks at width 128. Mid-right: Comparing normed versus unnormed Adam and SGD at fixed learning rate and varying width. For each method, we tune the learning rate at the scale marked by the dotted line. The normed methods scale better. Right: The same, but scaling number of blocks.", "description": "This figure demonstrates the effectiveness of the modular norm in normalizing optimizer updates.  It shows that using the modular norm (normed Adam and SGD) allows for consistent optimal learning rates across varying network widths and depths, unlike the unnormalized versions (Adam and SGD).  The optimal learning rate, indicated by red dots, remains relatively stable when scaling either width or depth, highlighting the improved transferability of learning rates achieved through modular norm normalization.", "section": "Descent in Normed Spaces"}, {"figure_path": "SFxAjB7UXx/figures/figures_31_1.jpg", "caption": "Figure 1: Learning rate transfer in the modular norm. We train GPT with context length 128 for 10k steps on OpenWebText. Left: Learning rate sweeps for normed Adam (Adam with updates normalized in the modular norm) with three transformer blocks and varying width. The optimal learning rate (marked by red dots) transfers well across scales. Mid-left: The same, but varying the number of blocks at width 128. Mid-right: Comparing normed versus unnormed Adam and SGD at fixed learning rate and varying width. For each method, we tune the learning rate at the scale marked by the dotted line. The normed methods scale better. Right: The same, but scaling number of blocks.", "description": "This figure shows the impact of using the modular norm to normalize optimizer updates during the training of a GPT model.  Four plots demonstrate that the optimal learning rate remains consistent when varying the width or depth of the model, provided the updates are normalized using the modular norm. This demonstrates the transferability of the learning rate across different model scales and suggests that the modular norm improves the scalability of training compared to traditional optimization methods.", "section": "2 Descent in Normed Spaces"}, {"figure_path": "SFxAjB7UXx/figures/figures_32_1.jpg", "caption": "Figure 4: Learning rate transfer on CIFAR-10. We tune the learning rate on a small model\u2014at the scale marked by the dotted line\u2014and test the performance on models of increasing width and depth at this fixed learning rate. We find that normed Adam and SGD scale better than their unnormed counterparts on both ResMLPs and ResNets. See Figure 1 for the same experiment on GPT.", "description": "This figure demonstrates the impact of using the modular norm to normalize Adam and SGD optimizers on the transferability of learning rates across different network scales (width and depth).  The experiment tunes the learning rate on a smaller model and then evaluates performance on larger models with the same learning rate. Results show that using the modular norm improves the scalability of both Adam and SGD, as compared to their unnormalized counterparts.", "section": "4 Experiments"}, {"figure_path": "SFxAjB7UXx/figures/figures_33_1.jpg", "caption": "Figure 4: Learning rate transfer on CIFAR-10. We tune the learning rate on a small model\u2014at the scale marked by the dotted line\u2014and test the performance on models of increasing width and depth at this fixed learning rate. We find that normed Adam and SGD scale better than their unnormed counterparts on both ResMLPs and ResNets. See Figure 1 for the same experiment on GPT.", "description": "This figure demonstrates the effectiveness of the modular norm in improving the scalability of Adam and SGD optimizers.  The learning rate is tuned on smaller models (indicated by a dotted line), and then the performance of those optimizers is tested on models with increasing width and depth, using the same tuned learning rate.  The results show that when using the modular norm (i.e., \"normed\" versions), both optimizers significantly outperform their un-normalized counterparts, demonstrating the transferability of the learning rate across different model scales.", "section": "4 Experiments"}]