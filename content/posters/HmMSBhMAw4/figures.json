[{"figure_path": "HmMSBhMAw4/figures/figures_5_1.jpg", "caption": "Figure 2: The model for Ex. 1, where states which have the same color give the same observation; the green edges give a reward of +1 and blue edges give a reward of +0.5.", "description": "This figure shows two diagrams illustrating the dynamics of a POMDP under two different actions (0 and 1).  The states are represented as nodes, and the transitions between states are represented by edges. The colors of the nodes indicate the observations associated with each state (white for 0 and grey for 1). The colors of the edges represent rewards: green for +1 and blue for +0.5.  Each diagram shows the probability of transitioning to another state given the current state and the action taken. This is a simplified representation of the POMDP that helps in demonstrating the behavior and performance of different reinforcement learning algorithms.", "section": "3 Numerical experiments"}, {"figure_path": "HmMSBhMAw4/figures/figures_6_1.jpg", "caption": "Figure 3: PASQL iterates for different behavioral policies (in blue) and the limit predicted by Thm. 1 (in red).", "description": "This figure shows the results of a numerical experiment on the convergence of PASQL with different behavioral policies.  The plots show the evolution of the Q-values for different state-action pairs over time for three different behavioral policies (\u03bc1, \u03bc2, and \u03bc3). The blue lines represent the actual Q-values learned by PASQL, while the red lines represent the theoretical limits predicted by Theorem 1. The experiment demonstrates the convergence of PASQL to its theoretical limit and the effect of different behavioral policies on the learning process.", "section": "3 Numerical experiments"}, {"figure_path": "HmMSBhMAw4/figures/figures_8_1.jpg", "caption": "Figure 4: A T-shaped grid world for Ex. 4. In state s, the agent learns about the goal state. In states {1,2,..., 2n}, the agent simply knows that it is in the gray corridor, but does not know which cell it is in. In state T, it knows that it has reached the end of corridor and must decide whether to go up or down. The agent gets a reward of +1 for reaching the correct goal state and a reward of -1 for reaching the wrong goal state.", "description": "This figure shows a T-shaped grid world used as an example in the paper.  The agent starts at 's' and must learn which goal state (G1 or G2) is the correct one. The agent can only observe its location in the corridor (gray cells) or its location at either T, G1, or G2.  The agent receives a reward of +1 for reaching the correct goal state and -1 for the incorrect one. The example highlights the difference between memory and periodic policies in solving the problem.", "section": "Numerical experiments"}, {"figure_path": "HmMSBhMAw4/figures/figures_15_1.jpg", "caption": "Figure 3: PASQL iterates for different behavioral policies (in blue) and the limit predicted by Thm. 1 (in red).", "description": "This figure shows the results of the PASQL algorithm for three different behavioral policies (\u03bc1, \u03bc2, \u03bc3). The blue lines represent the actual iterates of the PASQL algorithm, while the red lines represent the theoretical limit predicted by Theorem 1.  The plots show the convergence of the Q-function for each action-state pair ((0,0), (0,1), (1,0), (1,1)) across iterations.  The figure illustrates that PASQL converges to the theoretical limit, and that the limiting value depends on the behavioral policy used.", "section": "3 Numerical experiments"}, {"figure_path": "HmMSBhMAw4/figures/figures_16_1.jpg", "caption": "Figure 2: The model for Ex. 1, where states which have the same color give the same observation; the green edges give a reward of +1 and blue edges give a reward of +0.5.", "description": "This figure shows a POMDP model used in numerical experiments.  The model has six states (0-5), two actions (0 and 1), and two observations (0 and 1). States 0, 1, and 2 have observation 0, and states 3, 4, and 5 have observation 1. Transitions are represented by edges, with green edges indicating a reward of +1, blue edges +0.5, and other transitions having no reward.  The figure is divided into two parts: (a) dynamics under action 0 and (b) dynamics under action 1, showing how the state transitions differ based on the chosen action.", "section": "3 Numerical experiments"}, {"figure_path": "HmMSBhMAw4/figures/figures_17_1.jpg", "caption": "Figure 7: Performance of stationary stochastic policies \u03c0p for p \u2208 [0, 1] for Ex. 3.", "description": "The figure shows the performance of stationary stochastic policies for different values of p, which is the probability of choosing action 1.  The x-axis represents p, and the y-axis represents the performance J\u03c0p.  The curve shows that the optimal performance is achieved by a stochastic policy with p \u2248 0.39.", "section": "A.3 Ex. 3: stochastic policies can outperform deterministic policies"}, {"figure_path": "HmMSBhMAw4/figures/figures_17_2.jpg", "caption": "Figure 4: A T-shaped grid world for Ex. 4. In state s, the agent learns about the goal state. In states {1,2,..., 2n}, the agent simply knows that it is in the gray corridor, but does not know which cell it is in. In state T, it knows that it has reached the end of corridor and must decide whether to go up or down. The agent gets a reward of +1 for reaching the correct goal state and a reward of -1 for reaching the wrong goal state.", "description": "This figure illustrates a T-shaped grid world environment used in Example 4 of the paper to demonstrate the difference between stationary policies with memory augmentation and periodic policies. The agent starts at state 's', must navigate a long corridor of 2n cells to reach a decision point 'T'. At 'T', the agent chooses to go up to G1 or down to G2. The reward is +1 for the correct goal and -1 otherwise.  The observation space is limited, making the problem partially observable. This example highlights the limitations of policies with limited memory and how periodic policies can achieve optimal performance with a small number of parameters.", "section": "Numerical experiments"}]