[{"figure_path": "wZ5kEOCTce/tables/tables_6_1.jpg", "caption": "Table 1: ImageNet-1K classification accuracy. CrossMAE performs on par or better than MAE. All experiments are run with 800 epochs. The best results are in bold while the second best results are underlined.", "description": "This table presents the ImageNet-1K classification accuracy results for different self-supervised learning methods, including MAE and CrossMAE. The accuracy is evaluated using various Vision Transformer (ViT) models (ViT-S, ViT-B, ViT-L, ViT-H).  The table highlights that CrossMAE achieves comparable or superior performance to the traditional MAE across different model sizes, demonstrating the effectiveness of the proposed method.", "section": "4.1 ImageNet Classification"}, {"figure_path": "wZ5kEOCTce/tables/tables_6_2.jpg", "caption": "Table 2: COCO instance segmentation. Compared to previous masked visual pretraining works, CrossMAE performs favorably on object detection and instance segmentation tasks.", "description": "This table shows the performance of CrossMAE on COCO instance segmentation.  It compares CrossMAE to several other masked image modeling methods (MoCo v3, BEIT, MixedAE, and MAE). The results are presented for both ViT-B and ViT-L models, demonstrating CrossMAE's competitive performance on downstream tasks.", "section": "4.2 Object Detection and Instance Segmentation"}, {"figure_path": "wZ5kEOCTce/tables/tables_7_1.jpg", "caption": "Table 3: Ablations on CrossMAE. We report fine-tuning performance on ImageNet-1K classification with 400 epochs (i.e., half of the full experiments) with ViT-B/16. MAE performance is reproduced using the official MAE code. Underline indicates the default setting for CrossMAE. Bold indicates the best hyperparameter among the tested ones. 1 feature map fused (row 1, Table 3(d)) indicates using only the feature from the last encoder block. We use 25% prediction ratio for both settings in Table 3(f) to accelerate training.", "description": "This table presents ablation studies on the CrossMAE model, evaluating the impact of different design choices on ImageNet-1K classification accuracy.  It compares CrossMAE's performance against MAE (reproduced from the official code), varying parameters such as attention type in decoder blocks, mask ratio, prediction ratio, decoder depth, number of feature maps fused (inter-block attention), and input resolution.  The results highlight CrossMAE's robustness and efficiency compared to MAE, showing consistent high performance across different settings.", "section": "4.3 Ablations"}, {"figure_path": "wZ5kEOCTce/tables/tables_13_1.jpg", "caption": "Table 1: ImageNet-1K classification accuracy. CrossMAE performs on par or better than MAE. All experiments are run with 800 epochs. The best results are in bold while the second best results are underlined.", "description": "This table presents the ImageNet-1K classification accuracy for various models, including MAE and CrossMAE.  The results demonstrate that CrossMAE achieves comparable or superior performance to traditional MAE, highlighting the effectiveness of the proposed approach.  The table shows accuracy for different vision transformer models (ViT-S, ViT-B, ViT-L, ViT-H), with the best results for each model shown in bold and the second-best results underlined.  The consistent performance of CrossMAE across these different model sizes is a key finding.", "section": "4.1 ImageNet Classification"}, {"figure_path": "wZ5kEOCTce/tables/tables_14_1.jpg", "caption": "Table 4: For MAE, inter-block attention has very small differences in terms of finetuning performance, potentially due to the fact that MAE's decoder only takes in one set of features.", "description": "This table shows the results of an ablation study comparing the performance of MAE and CrossMAE with and without inter-block attention (IBA). The results demonstrate that adding IBA to MAE does not significantly improve performance, likely because the MAE decoder only uses one set of features. In contrast, CrossMAE shows improved performance, especially with a higher prediction ratio. This suggests that the effectiveness of IBA is dependent on the decoder architecture.", "section": "A.3 Ablation that Adds Self-Attention"}, {"figure_path": "wZ5kEOCTce/tables/tables_14_2.jpg", "caption": "Table 5: Improving inter-block attention by adding linear projections to the input features. The performance gain indicates that it is possible to design variants of readout functions to improve CrossMAE.", "description": "This table presents the ablation study on the impact of adding linear projections to the input features of the inter-block attention mechanism in CrossMAE.  It compares the ImageNet-1k classification accuracy of the standard CrossMAE model against a variant incorporating linear projections (CrossMAE + LP). The results show that CrossMAE + LP achieves a slightly higher accuracy, demonstrating the potential for further improvement in CrossMAE's performance by refining the design of the readout functions.", "section": "3.4 Inter-block Attention"}, {"figure_path": "wZ5kEOCTce/tables/tables_15_1.jpg", "caption": "Table 6: Pretraining Hyperparameters", "description": "This table lists the hyperparameters used during the pre-training phase of the CrossMAE model.  It includes settings for the optimizer (AdamW), base learning rate, learning rate schedule (cosine decay), batch size, weight decay, optimizer momentum, warm-up epoch, total epochs, and data augmentation techniques (RandomResizedCrop and RandomHorizontalFlip).  These settings are crucial for the efficient and effective pre-training of the model's visual representation learning capabilities.", "section": "A.5 Hyperparameters"}, {"figure_path": "wZ5kEOCTce/tables/tables_15_2.jpg", "caption": "Table 7: Finetuning Hyperparameters", "description": "This table shows the hyperparameters used for the finetuning stage of the CrossMAE model.  It includes settings for the optimizer (AdamW), base learning rate, learning rate schedule (cosine decay), batch size, weight decay, optimizer momentum, warm-up epochs, total epochs, augmentation (RandAug), label smoothing, mixup, cutmix, and drop path.  These settings were used to fine-tune the model on downstream tasks after pre-training.", "section": "A.5 Hyperparameters"}, {"figure_path": "wZ5kEOCTce/tables/tables_16_1.jpg", "caption": "Table 8: Linear probe experiments of CrossMAE.", "description": "This table presents the results of linear probe experiments comparing MAE and CrossMAE on ViT-S and ViT-B models.  Linear probing is a method used to evaluate the quality of learned representations. The table shows that CrossMAE achieves slightly better linear probe performance than vanilla MAE, indicating that CrossMAE learns better representations.", "section": "B.1 Linear Probe"}, {"figure_path": "wZ5kEOCTce/tables/tables_16_2.jpg", "caption": "Table 9: Ablation of masking strategies.", "description": "This table presents the ablation study results comparing two different masking strategies: Grid Masking and Random Masking.  The results show that randomly masking image patches during pre-training yields slightly better downstream performance in image classification than using a grid-based masking strategy.  This suggests that the random masking strategy is more effective for learning robust image representations.", "section": "B.2 Masking Strategy"}, {"figure_path": "wZ5kEOCTce/tables/tables_16_3.jpg", "caption": "Table 10: CrossMAE greatly improves the training throughput and reduces the memory requirements, lowering the barrier for masked pretraining. Statistics are measured on 2 NVIDIA A100 80GB GPUs. Please refer to Appendix C for comparison details. *: MAE\u2019s default batch size exceeds the capacity of 4 GPUs, requiring gradient accumulation for runtime measurement.", "description": "This table compares the training throughput, GPU memory usage and ImageNet-1k classification accuracy between MAE and CrossMAE. CrossMAE shows significant improvements in training speed and memory efficiency while maintaining comparable accuracy to MAE.", "section": "C Runtime and GPU Memory Comparisons with MAE"}]