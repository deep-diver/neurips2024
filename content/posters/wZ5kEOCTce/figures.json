[{"figure_path": "wZ5kEOCTce/figures/figures_1_1.jpg", "caption": "Figure 1: Method Overview. (A) Masked autoencoder (MAE) starts by masking random patches of the input image. (B) To reconstruct a mask token (marked by the blue star), MAE attends to both the masked tokens (B.Left) and the visible tokens (B.Right). A quantitative comparison over the ImageNet validation set shows that the masked tokens in MAE disproportionally attend to the visible tokens (\u03bc=1.42 vs \u03bc=0.39), questioning the necessity of attention within mask tokens. (C) We propose CrossMAE, the masked patches are reconstructed from only the cross attention between the masked tokens and the visible tokens. Surprisingly, CrossMAE attains the same or better performance than MAE on ImageNet classification and COCO instance segmentation.", "description": "This figure provides a high-level overview of the proposed CrossMAE method in comparison to the original MAE method.  Panel A shows the basic masking process in both methods. Panel B illustrates the attention mechanism used in MAE, highlighting the disproportionate attention given to visible tokens compared to masked tokens. Panel C shows the reconstruction comparison, demonstrating that CrossMAE achieves comparable or even better results despite using only cross-attention.", "section": "1 Introduction"}, {"figure_path": "wZ5kEOCTce/figures/figures_1_2.jpg", "caption": "Figure 2: Example reconstructions of ImageNet validation images. For each set of 5 images, from left to right, are the original image, masked image with a mask ratio of 75%, MAE [30], CrossMAE (trained to reconstruct 25% of image tokens, or 1/3 of the mask tokens), and CrossMAE (trained to reconstruct all masked tokens). Since CrossMAE does not reconstruct them, all model outputs have the visible patches overlaid. Intriguingly, CrossMAE, when trained for partial reconstruction, can decode all mask tokens in one forward pass (shown above), indicating that the encoder rather than the decoder effectively captures global image information in its output tokens. Its comparable reconstruction quality to full-image-trained models suggests that full-image reconstruction might not be essential for effective representation learning.", "description": "This figure shows example image reconstruction results from the ImageNet validation set, comparing MAE and CrossMAE.  It demonstrates that CrossMAE, even when trained to reconstruct only a fraction of masked tokens, can still reconstruct the entire image, highlighting the importance of the encoder in capturing global image information.", "section": "1 Introduction"}, {"figure_path": "wZ5kEOCTce/figures/figures_4_1.jpg", "caption": "Figure 4: Overview of CrossMAE. (a) The vanilla version of CrossMAE uses the output of the last encoder block as the keys and values for cross-attention. The first decoder block takes the sum of mask tokens and their corresponding positional embeddings as queries, and subsequent layers use the output of the previous decoder block as queries to reconstruct the masked patches. (b) Unlike the decoder block in [56], the cross-attention decoder block does not contain self-attention, decoupling the generation of different masked patches. (c) CrossMAE's decoder blocks can leverage low-level features for reconstruction via inter-block attention. It weighs the intermediate feature maps, and the weighted sum of feature maps is used as the key and value for each decoder block.", "description": "This figure illustrates the architecture of CrossMAE, highlighting three key aspects: (a) Vanilla CrossMAE uses the last encoder block's output for cross-attention, with subsequent decoder blocks using the previous block's output as queries; (b) The cross-attention decoder lacks self-attention, allowing independent reconstruction of masked patches; and (c) Inter-block attention allows decoder blocks to use a weighted sum of features from multiple encoder blocks for reconstruction, leveraging both high-level and low-level features.", "section": "3 CrossMAE"}, {"figure_path": "wZ5kEOCTce/figures/figures_7_1.jpg", "caption": "Figure 1: Method Overview. (A) Masked autoencoder (MAE) starts by masking random patches of the input image. (B) To reconstruct a mask token (marked by the blue star), MAE attends to both the masked tokens (B.Left) and the visible tokens (B.Right). A quantitative comparison over the ImageNet validation set shows that the masked tokens in MAE disproportionally attend to the visible tokens (1.42 vs 0.39), questioning the necessity of attention within mask tokens. (C) We propose CrossMAE, the masked patches are reconstructed from only the cross attention between the masked tokens and the visible tokens. Surprisingly, CrossMAE attains the same or better performance than MAE on ImageNet classification and COCO instance segmentation.", "description": "This figure provides a high-level overview of the proposed method, CrossMAE, in comparison to the existing MAE method. It illustrates the key differences in their decoding mechanisms and shows that CrossMAE achieves comparable or better performance than MAE on image classification and instance segmentation tasks.", "section": "1 Introduction"}, {"figure_path": "wZ5kEOCTce/figures/figures_7_2.jpg", "caption": "Figure 1: Method Overview. (A) Masked autoencoder (MAE) starts by masking random patches of the input image. (B) To reconstruct a mask token (marked by the blue star), MAE attends to both the masked tokens (B.Left) and the visible tokens (B.Right). A quantitative comparison over the ImageNet validation set shows that the masked tokens in MAE disproportionally attend to the visible tokens (\u03bc = 1.42 vs \u03bc = 0.39), questioning the necessity of attention within mask tokens. (C) We propose CrossMAE, the masked patches are reconstructed from only the cross-attention between the masked tokens and the visible tokens. Surprisingly, CrossMAE attains the same or better performance than MAE on ImageNet classification and COCO instance segmentation.", "description": "This figure provides a high-level overview of the Masked Autoencoder (MAE) method and the proposed CrossMAE method.  It highlights the differences in how masked tokens are reconstructed. MAE utilizes both self-attention and cross-attention, while CrossMAE uses only cross-attention. A quantitative analysis demonstrates that CrossMAE achieves comparable or better performance.", "section": "1 Introduction"}, {"figure_path": "wZ5kEOCTce/figures/figures_8_1.jpg", "caption": "Figure 5: We visualize the output of each decoder block. (a-b) Different decoder blocks play different roles in the reconstruction, with most details emerging at later decoder blocks, which confirms the motivation for inter-block attention. (c) Visualizations of inter-block attention shows that different decoder blocks indeed attend to feature from different encoder blocks, with later blocks focusing on earlier encoder features to achieve reconstruction. The reconstructions are unnormalized w.r.t ground truth mean and std for each patch.", "description": "This figure visualizes the output of each decoder block in the CrossMAE model.  It shows that different decoder blocks contribute differently to the reconstruction, with later blocks adding finer details.  The inter-block attention maps (c) demonstrate how later decoder blocks attend more to earlier encoder features while earlier decoder blocks focus on later features, highlighting the effectiveness of the model's inter-block attention mechanism. ", "section": "4.5 Visualizations"}, {"figure_path": "wZ5kEOCTce/figures/figures_14_1.jpg", "caption": "Figure 4: Overview of CrossMAE. (a) The vanilla version of CrossMAE uses the output of the last encoder block as the keys and values for cross-attention. The first decoder block takes the sum of mask tokens and their corresponding positional embeddings as queries, and subsequent layers use the output of the previous decoder block as queries to reconstruct the masked patches. (b) Unlike the decoder block in [56], the cross-attention decoder block does not contain self-attention, decoupling the generation of different masked patches. (c) CrossMAE's decoder blocks can leverage low-level features for reconstruction via inter-block attention. It weighs the intermediate feature maps, and the weighted sum of feature maps is used as the key and value for each decoder block.", "description": "This figure illustrates the architecture of CrossMAE, highlighting three key aspects: (a) vanilla CrossMAE architecture using cross-attention in decoder blocks; (b) the decoupling of mask patch generation in the decoder using only cross-attention; and (c) the use of inter-block attention to leverage low-level features for reconstruction.", "section": "3 CrossMAE"}, {"figure_path": "wZ5kEOCTce/figures/figures_16_1.jpg", "caption": "Figure 7: We compare ViT-B which is pre-trained for 800 epochs with different variants of Cross-MAE v.s. MAE. For CrossMAE, we vary the prediction ratio p and number of decoder blocks n, and we denote each as (p, n). While all experiments are run with inter-block attention, Cross-MAE has lower decoder FLOPS than MAE [30] and performs on par or better.", "description": "This figure compares the performance of CrossMAE against MAE in terms of ImageNet accuracy and decoder FLOPS.  Different variants of CrossMAE are tested by varying the prediction ratio (percentage of masked tokens reconstructed) and the number of decoder blocks. The results show that CrossMAE achieves comparable or better accuracy than MAE while using significantly fewer FLOPS, highlighting its efficiency.", "section": "C Runtime and GPU Memory Comparisons with MAE"}]