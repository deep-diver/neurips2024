[{"type": "text", "text": "One-to-Multiple: A Progressive Style Transfer Unsupervised Domain-Adaptive Framework for Kidney Tumor Segmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kai Hu Jinhao Li Yuan Zhang\u2217 Xiangtan University Xiangtan University Xiangtan University kaihu@xtu.edu.cn jhli@smail.xtu.edu.cn yuanz@xtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Xiongjun Ye Cancer Hospital, Chinese Academy of Medical Sciences yexiongjun@cicams.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Xieping Gao\u2217 Hunan Normal University xpgao@hunnu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In multi-sequence Magnetic Resonance Imaging (MRI), the accurate segmentation of the kidney and tumor based on traditional supervised methods typically necessitates detailed annotation for each sequence, which is both time-consuming and labor-intensive. Unsupervised Domain Adaptation (UDA) methods can effectively mitigate inter-domain differences by aligning cross-modal features, thereby reducing the annotation burden. However, most existing UDA methods are limited to one-to-one domain adaptation, which tends to be inefficient and resourceintensive when faced with multi-target domain transfer tasks. To address this challenge, we propose a novel and efficient One-to-Multiple Progressive Style Transfer Unsupervised Domain-Adaptive (PSTUDA) framework for kidney and tumor segmentation in multi-sequence MRI. Specifically, we develop a multi-level style dictionary to explicitly store the style information of each target domain at various stages, which alleviates the burden of a single generator in a multi-target transfer task and enables effective decoupling of content and style. Concurrently, we employ multiple cascading style fusion modules that utilize point-wise instance normalization to progressively recombine content and style features, which enhances cross-modal alignment and structural consistency. Experiments conducted on the private MSKT and public KiTS19 datasets demonstrate the superiority of the proposed PSTUDA over comparative methods in multi-sequence kidney and tumor segmentation. The average Dice Similarity Coefficients are increased by at least $1.8\\%$ and $3.9\\%$ , respectively. Impressively, our PSTUDA not only significantly reduces the floating-point computation by approximately $72\\%$ but also reduces the number of model parameters by about $50\\%$ , bringing higher efficiency and feasibility to practical clinical applications. 2 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kidney tumor segmentation is a crucial task in medical image analysis, playing an essential role in the diagnosis, staging, and treatment of kidney cancer [1, 2, 3]. Previous studies have primarily focused on computed tomography (CT) images [4, 5, 6, 7, 8, 9], but in recent years, magnetic resonance imaging (MRI) has emerged as a safer alternative due to its non-radiative nature [10, 11, 12]. Compared to CT, ", "page_idx": 0}, {"type": "text", "text": "MRI offers superior contrast for characterizing kidney tissue and tumors, enabling a clearer distinction between normal and pathological tissues. Furthermore, the multi-parameter imaging capability of MRI generates diverse sequences [13, 14, 15] that enhances the comprehensive description of pathological features. These advantages have established MRI as the preferred modality for clinicians in diagnosing kidney disease, particularly kidney tumors, and have facilitated the widespread use of multi-sequence MRI in clinical practice for kidney and tumor segmentation. ", "page_idx": 1}, {"type": "text", "text": "In existing studies, traditional supervised segmentation methods typically rely on a large amount of annotated data, and performing fine-grained annotations for a single sequence is already a timeconsuming and labor-intensive task, let alone for every sequence. Unsupervised domain adaptation (UDA) methods [16, 17, 18, 19, 20] have been proposed as a promising solution to address these challenges. Current UDA methods mainly focus on one-to-one domain adaptation, involving a single source domain and a target domain. Although these methods perform well in certain scenarios, they lack scalability, meaning that for multi-target domain tasks, training paired generators for each sourcetarget domain pair results in significant computational costs and resource consumption. Furthermore, one-to-one domain adaptation methods can only learn fixed mappings between two domains, thereby failing to capture the potential commonalities and connections among multiple sequences. ", "page_idx": 1}, {"type": "text", "text": "As an initial attempt to apply multi-domain adaptation technology to medical imaging, Xu et al. [21] proposed OMUDA for abdominal organ segmentation using multi-sequence MRI. However, the architecture of OMUDA is derived from StarGAN v2 [22], which was originally designed for style transfer in natural images and has not been specifically tailored and optimized for the needs of domain adaptation in medical imaging. Although it shows improvement in resolving domain confusion across multiple domains, this limitation leads to suboptimal generated images with regards to organ structural consistency and detail preservation. Consequently, the task of one-to-multiple domain adaptation in medical imaging remains a significant challenge, necessitating more refined and specialized approaches for further advancement. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a One-to-Multiple Progressive Style Transfer Unsupervised DomainAdaptive (PSTUDA) framework, which explicitly stores multi-level style features from different domains in designated multi-level style dictionaries, thus alleviating the burden on the generator and achieving the decoupling of content features from style features. Our PSTUDA employs multiple cascaded style fusion modules to recombine multi-level style features from different domains with content features layer by layer using Point-wise Instance Normalization (PIN), thereby ensuring that the generated images across multiple target domains have high-quality style and structure. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of our work are summarised as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We explore a novel and efficient One-to-Multiple Progressive Style Transfer Unsupervised Domain-Adaptive (PSTUDA) framework, which is capable of simultaneously transferring a single annotated source domain to multiple unannotated target domains, significantly reducing the need for tedious domain adaptation work for each target domain. \u2022 We introduce a multi-level style dictionary that stores style information for each domain at different stages of style transfer, alleviating the burden on the generator to perform multiple tasks and effectively decoupling content features from style features. \u2022 We propose a progressive style transfer paradigm and a Point-wise Instance Normalization (PIN) method. The former comprises multiple cascading style fusion modules, each recombining content features with corresponding style features through PIN, thereby achieving better cross-modal alignment and structural consistency. \u2022 We construct a multi-sequence kidney tumor MRI dataset called MSKT to facilitate research on kidney tumor analysis. Quantitative and qualitative results on the MSKT and the public dataset KiTS19 show that our PSTUDA framework outperforms the state-of-the-art methods and significantly improves segmentation performance and training efficiency. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Kidney Tumor Segmentation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Computer-aided diagnostic methods for kidney tumor segmentation play a crucial role in clinical practice [23, 24, 25, 26, 27]. The significant variability in the size, shape, and location of kidneys and tumors presents a considerable challenge to accurate segmentation. Yu et al. [28] developed CrossbarNet to capture global and local features of kidney tumors through crossbar patches and focused on difficult-to-segment regions through a cascade training strategy. Myronenko et al. [29] designed a dedicated boundary branch supervised by an edge-aware loss term to enhance the consideration of organ and tumor edge information. These approaches are centered on improving the ability of the model to recognize the complex morphology of kidney tumors. To further advance this effort, Pandey et al. [30] integrated active contouring with 3D-U-Net to achieve precise delineation of kidney tumor shapes, showcasing the potential of merging deep learning with traditional image processing techniques. However, these approaches typically rely on fully supervised learning with extensive pixel-level annotations. To address this limitation, researchers are exploring alternative solutions in the fields of semi-supervised, self-supervised, and unsupervised learning. Ma et al. [31] introduced an Affinity Network that learns from limited data using k-nearest neighbors attention pooling layers. Similarly, Ciga et al. [32] and Faust et al. [33] developed methods that enhance feature learning and guide tumor analysis through self-supervised and unsupervised techniques, respectively. By reducing the reliance on annotated data, these methods not only lower costs but also improve the model\u2019s generalization capabilities. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Unsupervised Domain Adaptation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "UDA is one of the important methods for addressing domain difference. It aims to transfer a model from an annotated source domain to an unannotated target domain. Existing works have mostly focused on one-to-one domain adaptation [34, 35, 36, 37, 38, 39], yielding impressive outcomes. For example, CycleGAN [34] used cycle consistency constraints to transform unpaired images from one domain to another. CyCADA [40] enforced cycle consistency by combining methods of image space alignment and latent representation space alignment. MUNIT [41] decomposed image representations into content and style codes, enabling multimodal image translation. For medical image domain adaptation, SIFA [35] achieved domain alignment from both image and feature perspectives, enabling the segmentation network to adapt to the unannotated target domain. Thereafter, DEPL [39] further improved the segmentation accuracy by employing multi-source domain extension and selective pseudo-labelling strategies. ", "page_idx": 2}, {"type": "text", "text": "However, these one-to-one domain adaptation methods lack scalability when handling multiple domain transfer tasks, as they can only learn the relationships between two different domains at a time. StarGAN [42] performed image-to-image translation across multiple domains using a single model, and then the improved version, StarGAN v2 [22] further enhanced diversity in generated images by introducing style codes specific to each domain. Additionally, Sharma and Hamarneh [43] proposed a multi-modal generative adversarial network that leveraged redundant information from available sequences to synthesize missing MRI pulse sequences in patient scans. Gholami et al. [44] developed an information-theoretic approach that aimed to find shared latent spaces for domain adaptation across multiple target domains. It should be noted that these methods predominantly focus on the diversity of the generated images, while paying less attention to the structural consistency before and after image translation. ", "page_idx": 2}, {"type": "text", "text": "2.3 Normalization of Image Translation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In UDA tasks, image normalization is a key step that can help models to learn and transfer features effectively [45, 46, 47, 48, 49, 50]. Instance Normalization (IN) [51] normalizes the features of each sample to improve the quality and realism of generated images. Adaptive Instance Normalization (AdaIN) [52] dynamically adjusts the relationship between style and content features of input images to enable a rapid transformation across arbitrary styles. Batch-Instance Normalization (BIN) [53] explicitly normalizes unnecessary style variations in images while preserving useful styles. AdaAttN [54] introduces adaptive attention normalization to optimize the effects of arbitrary neural style transfer. GramLIN [55] continuously measures the proximity of the current stylized output to the target style to achieve progressive stain transfer. In medical image domain adaptation tasks, it is crucial to maintain structural consistency during image translation, in addition to changing image styles. Therefore, we propose a novel normalization method called PIN, which progressively fuses content and style features at each pixel by considering image details and local style differences. The goal is to ensure well-structured images after transfer. ", "page_idx": 2}, {"type": "image", "img_path": "cMwSoXLCVi/tmp/6da42867b0471e37f24de63c177e8796530b4da87cc26350b3b9dd035ca6bf12.jpg", "img_caption": ["(b)Ilustration of theprogressive style transferprocess "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: (a) The overall architecture of the proposed One-to-Multiple Progressive Style Transfer Unsupervised Domain-Adaptive framework, which includes a shared generator and a discriminator. The generator is composed of an encoder, a decoder, and multiple style fusion modules. (b) shows the progressive style transfer process, achieved through cascaded style fusion modules. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Overview of the proposed One-to-Multiple Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Formally, for a one-to-multiple domain adaptation task, we have one source domain $S$ and multiple target domains $T_{i}$ . The source domain $S$ is annotated, dNenoted as ${\\cal{S}}=\\{(x_{j},y_{j})\\}_{j=1}^{N_{s}}$ , while the target domains are unannotated, denoted as $T_{i}=\\{x_{j}^{T_{i}}\\}_{j=1}^{N_{T_{i}}}$ . Each domain is assigned a domain label, for example, representing the source domain 0, target domain 1, etc., up to target domain $N.$ . Figure 1(a) illustrates our PSTUDA framework, which mainly consists of a shared generator (including an encoder, a decoder, and multiple style fusion modules) and a multiscale discriminator. $X_{s r c}$ , $X_{t r g s}$ , and $X_{r e c o n}$ represent the input image from the source domain, the generated pseudo-target domain images corresponding to the input image, and the image reconstructed back to the source domain from the pseudo-target domain images, respectively. Our task involves two stages. The goal of the first stage is to train a single generator $G$ such that given a source domain image $X_{s r c}\\in S$ and any target domain label $l_{i}$ , it can generate a pseudo-target domain image $X_{t r g_{-}i}$ corresponding to the image $X_{s r c}$ , i.e., $G(X_{s r c},l_{i})\\Rightarrow\\mathbf{\\bar{{X}}}_{t r g_{-}i}$ . The objective of the second stage is to utilize the generated pseudo-target domain images $X_{t r g s}$ and source domain annotation $y$ to train a segmentation network to achieve accurate segmentation of the kidney and tumor. ", "page_idx": 3}, {"type": "text", "text": "3.2 Multi-level Style Dictionary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our design is inspired by the StarGAN v2 multi-domain image generation framework, which employs a Mapping Network and a Style Encoder to obtain style encodings. The Mapping Network transforms random Gaussian noise into diverse style encodings, while the Style Encoder extracts style encodings from reference images. These techniques are essential for transforming natural image styles and enhancing artistic variety. However, in medical image domain adaptation, they encounter challenges in extracting comprehensive style features. Style encodings from a single image may not capture the full range of styles within a domain, and deriving styles from Gaussian noise, while introducing diversity, can result in instability. In medical image analysis, such instability could potentially compromise feature recognition and segmentation accuracy. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To address these challenges, we propose to define a set of learnable multi-level style dictionaries for each domain, where the style information at each level closely corresponds to the respective phase of style fusion. This multi-level structure allows for starting with basic style features and exploring more complex layers of style, thereby providing a progressively refined path for the style fusion process. During the generative adversarial process, the continuous updating and learning of the style dictionary can adapt to the ever-changing style demands. The early-stage stylized features will provide feedback for subsequent style feature learning, thus guiding them to update in a dynamic and targeted manner. By learning style information from the whole domain, the style dictionary becomes more representative, which effectively overcomes the limitation of extracting style encoding from a single image. Moreover, the iterative updates of style dictionaries enhance stability and reduce uncertainty in extracting style encodings from random noise. Storing style encoding information in a multi-level dictionary helps alleviate burden on the generator and achieves effective decoupling of content features and style features. This allows it to focus on capturing domain-invariant features such as structure and shape. In this way, the multi-levels style dictionary refines complex styles within target domains at each level, ensuring that each can timely reflect latest progress in style transfer process more precise and coordinated. ", "page_idx": 4}, {"type": "text", "text": "3.3 Progressive Style Transfer Paradigm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In PSTUDA, multiple cascaded style fusion modules and decoders constitute the core components of progressive style transfer. Figure 1(b) illustrates the main processes of the first and last (decoding phase) style transfer stages, with similar style transfer processes in the intermediate stages. In the first style transfer stage, the style fusion module requires two inputs: the content feature $\\bar{X_{1}^{\\breve{1}}}\\in\\mathbb{R}^{H\\times W\\times C}$ obtained by downsampling from the encoder, and the first-level style encoding $V_{1}\\in\\mathbb{R}^{S\\times S\\times1}$ from the target domain, matched with the current style fusion module. We utilize the domain label $L$ multiplied by the first style dictionary to select the target style encoding $V_{1}$ . First, the content feature $X_{1}$ passes through a convolutional layer to obtain the content feature $\\hat{X}_{1}$ , and the target style encoding $V_{1}$ undergoes two consecutive $1\\times1$ convolutions for channel transformation to obtain the style feature V\u02c61 \u2208RH\u00d7W \u00d7C. ", "page_idx": 4}, {"type": "text", "text": "After obtaining the content feature $\\hat{X}_{1}$ and style feature $\\hat{V}_{1}$ , we propose a novel style fusion normalization method, Point-wise Instance Normalization (PIN), to effectively combine the two for more subtle and accurate pixel-level style transfer. The PIN can be defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nP I N(\\hat{X}_{l},\\hat{V}_{l})=\\gamma_{c h w}(\\hat{V}_{l})\\cdot N o r m(\\hat{X}_{l})+\\beta_{c h w}(\\hat{V}_{l}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma_{c h w}(\\hat{V}_{l}),\\beta_{c h w}(\\hat{V}_{l})=C h u n k(h_{(2c)h w}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\nh_{(2c)h w}=C o n v B l o c k(\\hat{V}_{l}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N o r m(\\cdot)$ denotes the Instance Normalization of the content feature $\\hat{X}_{l}\\in\\mathbb{R}^{H\\times W\\times C}$ at layer $l$ . The parameters $\\gamma_{c h w}$ and $\\beta_{c h w}$ are scaling and shifting parameters specific to the target domain. These parameters are derived by applying ConvBlock and Chunk operations to the style feature $\\hat{V}_{l}\\,\\in\\,\\bar{\\mathbb{R}}^{H\\times W\\times C}$ and are adjusted to match the spatial dimensions of the content feature $\\hat{X_{l}}$ , thus enabling independent style transfer at each pixel. By rescaling the feature map using these parameters, style information specific to the target domain is integrated into the feature map for style transfer. PIN provides unique style statistics for each spatial point of the content feature, allowing for customized style fusion based on different regions and features of the image content. This facilitates finer local style variations and richer style details, making it particularly suitable for applications requiring precise style adjustments, such as medical image segmentation. ", "page_idx": 4}, {"type": "text", "text": "In the final stage of style transfer, the decoder takes the stylized content feature Xn \u2208RH\u2032\u00d7W \u2032\u00d7C\u2032 from the previous layer and the last-level style encoding $\\check{V_{n}}\\in\\mathbb{R}^{S\\times S\\times1}$ as input. To spatially match the content feature $X_{n}$ , the style encoding $V_{n}$ first undergoes upsampling through a deconvolutional layer for scale transformation, followed by two consecutive $1\\times1$ convolutions for channel transformation, resulting in the style feature $\\dot{V_{n}}\\,\\in\\,\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times C^{\\prime}}$ . The other style fusion operations are similar to those in the first stage. There are two considerations for performing style transfer during the decoding phase. Firstly, the decoding phase is the process of image reconstruction, where integrating style features can effectively incorporate high-level style information from the target domain into the image. This helps to better express the target style while preserving content information, especially in terms of details and textures. Secondly, as the decoder is responsible for upsampling, it has the opportunity to refine and restore image details while enlarging the feature maps. Introducing style transfer at this stage ensures that these details not only conform to the content structure but also match the characteristics of the target style, thus achieving effective fusion of content and style at different scales. ", "page_idx": 4}, {"type": "image", "img_path": "cMwSoXLCVi/tmp/425022c68507951771871e9d453f80bd4a359ea89e6cd0874861cc268e4d8ce8.jpg", "img_caption": ["Figure 2: Architecture of the Multi-Scale Discriminator, composed of multiple residual blocks. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.4 Generator and Discriminator Architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Generator As previously mentioned, the generated pseudo-target domain images will be used in conjunction with the corresponding segmentation annotations from the source domain to train the subsequent segmentation networks. Therefore, it is crucial to maintain structural consistency during the image translation process for the success of downstream applications. The generator in StarGAN v2, due to its multiple downsampling steps, unfortunately leads to a loss of spatial information, which poses challenges in maintaining structural consistency during image translation. In light of CycleGAN\u2019s outstanding performance in one-to-one medical image translation, we adopt the approach of OMUDA by integrating the generator architecture of CycleGAN into our encoder and decoder. To facilitate a single generator to handle one-to-multiple transfer tasks, we replace the IN layers in the decoder with PIN layers. The complete generator comprises an encoder, a decoder, and a series of cascaded style fusion modules. Among them, the encoder, equipped with two downsampling layers and multiple IN layers, is tasked with extracting domain-invariant features from the input image of the source domain. The cascaded style fusion modules are responsible for integrating the style features of the target domain with the domain-invariant features of the source domain. The decoder is composed of two upsampling layers embedded with PIN and is responsible for style transfer and image reconstruction during the decoding phase. ", "page_idx": 5}, {"type": "text", "text": "Discriminator Inspired by Wang et al.\u2019s work [56], we enhance the original discriminator architecture of StarGAN v2 with a multi-scale mechanism. As depicted in Fig. 2, the discriminator is composed of multiple independent discrimination branches, each containing four residual blocks [57], with each residual block having $N$ output branches serving $N$ specific target domains. The input to the discriminator includes not only the original image but also images processed through different scales of downsampling. The multi-branch output of the discriminator first multiplies with the target domain label $L$ to select the discrimination output for the corresponding domain before proceeding to authenticity determination. By evaluating the generated images at various scales, the multi-scale discriminator can more comprehensively judge the realism of the images, thereby encouraging the generator to produce images with higher quality. More detailed information on the generator and discriminator architectures can be found in Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "3.5 Loss Function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For convenience, we give the following symbol definition. $x_{s}\\in X$ denotes a source domain image, with its corresponding source domain label $l_{s}\\in L$ . $l_{t}\\in L$ represents any target domain label. $G$ and $D$ stand for the generator and discriminator, respectively. The loss functions in the generator include adversarial loss $\\mathcal{L}_{a d v}$ , cycle consistency loss $\\mathcal{L}_{c y c}$ , and identity loss $\\mathcal{L}_{i d t}$ [58]. The loss in the discriminator includes adversarial loss $\\mathcal{L}_{a d v}$ . Among them, the adversarial loss is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a d v}=E_{\\boldsymbol{x}_{s},l_{t}}\\left[l o g(1-D(G(\\boldsymbol{x}_{s},l_{t}),l_{t}))\\right]+E_{\\boldsymbol{x}_{s}}\\left[l o g D(\\boldsymbol{x}_{s},l_{s})\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which encourages the generator to generate images that are indistinguishable from the target domain images by the discriminator. The cycle consistency loss is computed as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c y c}=E_{x_{s},l_{s},l_{t}}\\left[\\left\\|G(G(x_{s},l_{t}),l_{s})-x_{s}\\right\\|\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which ensures that an image translated from the source domain to the target domain can be translated back to the original image, thus preserving the structural consistency of the image throughout the translation process. The identity loss can be defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i d t}=E_{x_{s},l_{s}}\\left[\\left\\Vert\\left(G(x_{s},l_{s})-x_{s}\\right\\Vert\\right],\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which is used to preserve the content of the source domain images when the generator is applied to them. The complete training loss can be summarized as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{G}\\operatorname*{max}_{D}\\mathcal{L}_{a d v}+\\lambda_{c y c}\\mathcal{L}_{c y c}+\\lambda_{i d t}\\mathcal{L}_{i d t},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda_{c y c}$ and $\\lambda_{i d t}$ denote the weights for the corresponding loss terms and they are set to 10 and 1 in our experiments, respectively. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this study, we utilize two datasets for the performance evaluation of the method. The first one is a private dataset named MSKT, which comprises 104 cases, each including four sequences: T1c, FS T2W, T2W, and DWI. The second is the publicly available KiTS19 [59] dataset. We conduct our first set of comparative experiments using the MSKT dataset. Specifically, the annotated T1c data is served as the source domain, while the unannotated FS T2W, T2W, and DWI data are considered as the target domain. For our second set of comparative experiments, we combine KiTS19 with the MSKT dataset. In this case, all training data from KiTS19 are used as the source domain, and all four sequences of the MSKT are considered as the target domain. We ensure that there is no overlap between the source domain, target domain, and test set to prevent any potential information leakage. More details about the dataset and evaluation metrics are provided in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "4.2 Comparative Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To comprehensively evaluate the performance of the proposed method, we compare PSTUDA with five other state-of-the-art UDA methods on the MSKT and KiTS19 datasets: CycleGAN [34], MUNIT [41], SIFA [35], DEPL [39], and StarGAN v2 [22]. Except for SIFA and DEPL, the others are limited to cross-domain image transfer. Therefore, we train a dedicated U-Net [60] from scratch based on the pseudo-target domain images generated by these methods for multiple sequences. For a fair comparison, all hyperparameters in the U-Net remain consistent. Due to the one-to-one image transfer characteristics of SIFA, DEPL, CycleGAN, and MUNIT, we train a separate model for each sequence using these methods to handle multi-sequence MR image transformations. ", "page_idx": 6}, {"type": "text", "text": "Due to the unpaired nature of multi-sequence MR images, it is challenging to quantitatively assess the style effects and structural consistency of the generated images. Experience suggests that if anatomical structures are distorted during the image transfer process, the pseudo-target domain images generated will not correspond with the annotations of the source domain, thereby impairing segmentation performance. Moreover, if there is a significant difference between the style of the generated images and the real target domain images, this discrepancy in data distribution may also detrimentally affect the segmentation results. Hence, the quality of the generated images is indirectly indicated by the segmentation results. ", "page_idx": 6}, {"type": "text", "text": "Results on MSKT Table 1 presents the Dice Similarity Coefficient $(D S C)$ and $95\\%$ Hausdorff Distance $(H D_{95})$ results for all methods across different MR sequences. The average DSC indicate that our PSTUDA outperforms other methods in segmentation performance on all MRI sequences. We also observe that PSTUDA significantly surpasses MUNIT, SIFA, DEPL, and StarGAN v2 from both $D S C$ and $H D_{95}$ . Although SIFA and DEPL are more efficient end-to-end domain adaptation segmentation methods, they do not perform well in our task. The images generated by these methods exhibit structural distortions and lack naturalness ( $\\mathrm{4}^{t h}$ and $5^{t h}$ columns in Fig. 3). Particularly, DEPL shows severe structural distortion of kidneys and tumors in the T2W sequence, which has greater inter-domain differences. MUNIT and StarGAN v2, while excelling in the diversity of generated images, fall short in maintaining cross-domain structural consistency, leading to distorted structures in the generated images $3^{r d}$ and $6^{t h}$ columns in Fig. 3). The performance of CycleGAN is close to that of PSTUDA on FS T2W and DWI sequences, but it underperforms on the T2W sequence, as the translated images lose many important structural details, resulting in poor image alignment. In contrast, our PSTUDA prioritizes structural consistency in the cross-domain image translation process by separating content from style and progressively recombining content and style features in a point-wise manner. This approach produces images with higher fidelity, as demonstrated in the last column of Fig. 3, thereby significantly enhancing segmentation performance. ", "page_idx": 6}, {"type": "image", "img_path": "cMwSoXLCVi/tmp/e220d49e7dd5ca1126d31fd5eb19d96f1881feee16948411dac9e417c164be80.jpg", "img_caption": ["Figure 3: Qualitative results for T1c $\\rightarrow$ FS T2W, T2W, and DWI on the MSKT dataset. Blue and red bounding boxes indicate the annotated boundaries of the kidney and tumor, respectively (Same below). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/88a68732c6e98ddf48cac09ca4b3edb45b2d1fa218b0343a28708c769c923e2e.jpg", "table_caption": ["Table 1: Quantitative segmentation results of different comparative methods on the MSKT dataset. "], "table_footnote": ["$^{\\dagger}$ implies that we report the results of our own runs according to the official code. \u2217implies that the method is based on our implementation. "], "page_idx": 7}, {"type": "text", "text": "Results on KiTS19 and MSKT This experiment utilizes CT images from KiTS19 as the source domain and four MRI sequences from MSKT as the target domain. Given the different imaging technologies employed by CT and MRI, the domain difference between them is significantly greater than that between the various MRI sequences. As shown in Table 2, most comparative methods show a reduction in kidney and tumor segmentation performance on FS T2W, T2W, and DWI when compared to the first set of experiments. Although the average $D S C$ of CycleGAN on the DWI is higher than that of PSTUDA (by about $1.9\\%$ ), its performance is lower than that of PSTUDA by approximately $7.5\\%$ and $9.1\\%$ on FS T2W and T2W, respectively. Our PSTUDA almost achieves the best average $D S C$ results on all sequences. Compared to the first set of experiments, the performance of CycleGAN has decreased by about $6.8\\%$ and $4.4\\%$ on FS T2W and T2W, respectively, while the performance of the proposed PSTUDA has decreased by $0.5\\%$ and increased by $1.2\\%$ on these two sequences. These results underscore PSTUDA\u2019s exceptional robustness in tasks with larger domain differences. As shown in Fig. 4 in Appendix C, our PSTUDA outperforms other comparative methods in maintaining structural consistency in translated images. Moreover, thanks to the one-to-multiple features, our method also shows a significant advantage in training efficiency, which will be detailed in the subsequent section. In summary, the highlight of PSTUDA lies in its dual improvement of segmentation performance and training efficiency. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/ae806b7321be0234b3bc98cec99aac412a72bedffc3a5506d145e1c38ba2369e.jpg", "table_caption": ["Table 2: Quantitative segmentation results of different comparative methods on the KiTS19 and MSKT datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Training efficiency In this section, we evaluate the performance of PSTUDA in comparison with CycleGAN, MUNIT, and StarGAN v2 in terms of model parameters and FLOPs. As shown in Table 9 in Appendix D, the FLOPs for CycleGAN and MUNIT represent the cumulative results of the image translation tasks for the three domains, i.e., T1c $\\mathrm{\\rightarrowFS}$ T2W, $\\mathrm{Tlc}{\\rightarrow}\\mathrm{T}2\\mathrm{W}.$ , and $\\mathrm{Tlc}{\\rightarrow}\\mathrm{DWI}$ . For the transfer of T1c to the other three MR sequences using CycleGAN, MUNIT, and StarGAN v2, the parameters that need to be optimized are 2, 3.3, and 1.9 times that of PSTUDA, respectively. Regarding FLOPs, PSTUDA demonstrates a reduction of $72\\%$ and $79\\%$ compared to CycleGAN and MUNIT, respectively, but shows a $17\\%$ increase compared to StarGAN v2. The increased computational demand is mainly due to the PIN module in PSTUDA, which requires per-pixel fusion of style and content features. It is worth noting that the training efficiency of PSTUDA becomes more advantageous when more target domains are included in a one-to-multiple domain adaptation task. Overall, PSTUDA substantially decreases both model parameters and FLOPs while maintaining high segmentation accuracy, implying lower memory requirements and faster inference speed. In summary, these results highlight the significant advantages of PSTUDA in enhancing the efficiency and feasibility of clinical applications. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effectiveness of PST and MS_D To investigate the effectiveness of the proposed Progressive Style Transfer (PST) paradigm, we conduct ablation studies on it with the Multi-Scale Discriminator (MS_D) on the MSKT dataset. As shown in Table 3 , the introduction of PST significantly improves the $D S C$ and $H D_{95}$ metrics compared to the Baseline. This reflects the ability of the PST to enhance the realism of translated images while maintaining structural consistency. Furthermore, the multiscale discriminator assesses the pseudo-target domain images at different scales, thereby further enhancing the capabilities of the generator. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of M_SD and PIN We also conduct ablation studies on the internal sub-modules within PST, namely the Multi-level Style Dictionary (M_SD) and the Point-wise Instance Normalization (PIN). As illustrated in Table 4 , replacing the Mapping Network and Style Encoder in the ", "page_idx": 8}, {"type": "text", "text": "Baseline with the M_SD results in a significant performance improvement. This can be attributed to the stability and representativeness of the learned style encodings. Moreover, the integration of the M_SD with PIN fully considers the details and local style variations of images, thereby yielding exceptional results. ", "page_idx": 9}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/b902491983400289a8d5357b27c0c22fb082d33de577db4686e65d515b3db936.jpg", "table_caption": ["Table 3: Ablation study of Progressive Style Transfer paradigm (PST) and Multi-Scale Discriminator (MS_D) on the MSKT dataset. The Baseline model is StarGAN v2. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/41af32107e9e6bbee842c8cde813d8aa3de316c5ccc0d994884f0d09111b235d.jpg", "table_caption": ["Table 4: Ablation study of Multi-level Style Dictionary (M_SD) and Point-wise Instance Normalization (PIN) in PST on the MSKT Dataset. The Baseline model is StarGAN v2. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Ablation of PIN and other normalization methods We compare PIN with AdaIN [52] and BIN [53], and as shown in Table 5, PIN outperforms the others. We attribute this superior performance to PIN\u2019s ability to account for local style differences, which is particularly advantageous for finegrained segmentation tasks (e.g., kidney tumors, as abnormal pathological tissues, exhibit significant style differences). This capability enables PSTUDA to generate synthetic images that align more closely with the data distribution of the target domain. ", "page_idx": 9}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/91709f1f032e19f6052cdbbf7e7c91fca4497dfb830cb5564776a3759fe3e10a.jpg", "table_caption": ["Table 5: Ablation study of PIN with AdaIN and BIN on the MSKT dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a novel and efficient One-to-Multiple PSTUDA framework that utilizes a multi-level style dictionary to decouple and store style information. By employing multiple cascaded style fusion modules, our framework progressively recombines content and style features, thereby achieving superior cross-modal alignment and consistency of medical tissue structures. Experimental validation on both a private dataset and a public dataset demonstrates the significant advantages of our method in improving training efficiency for one-to-multiple domain adaptation tasks and enhancing the accuracy of multi-sequence kidney tumor segmentation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China under Grants 62272404, 62076007, and 62372170, in part by the Natural Science Foundation of Hunan Province of China under Grants 2022JJ30571 and 2023JJ40638, and in part by the Research Foundation of Education Department of Hunan Province of China under Grant 23A0146. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dinggang Shen, Guorong Wu, and Heung-Il Suk. Deep learning in medical image analysis. Annual Review of Biomedical Engineering, 19:221\u2013248, 2017.   \n[2] Huiyan Jiang, Zhaoshuo Diao, and Yu-Dong Yao. Deep learning techniques for tumor segmentation: a review. The Journal of Supercomputing, 78(2):1807\u20131851, 2022.   \n[3] Matteo Ferro, Felice Crocetto, Biagio Barone, Francesco Del Giudice, Martina Maggi, Giuseppe Lucarelli, Gian Maria Busetto, Riccardo Autorino, Michele Marchioni, Francesco Cantiello, et al. Artificial intelligence and radiomics in evaluation of kidney lesions: a comprehensive literature review. Therapeutic Advances in Urology, 15:17562872231164803, 2023.   \n[4] Guanyu Yang, Jinjin Gu, Yang Chen, Wangyan Liu, Lijun Tang, Huazhong Shu, and Christine Toumoulin. Automatic kidney segmentation in CT images based on multi-atlas image registration. In 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, pages 5538\u20135541. IEEE, 2014.   \n[5] HeiShun Yu, Jonathan Scalera, Maria Khalid, Anne-Sophie Touret, Nicolas Bloch, Baojun Li, Muhammad M Qureshi, Jorge A Soto, and Stephan W Anderson. Texture analysis as a radiomic marker for differentiating renal tumors. Abdominal Radiology, 42:2470\u20132478, 2017.   \n[6] Xue-Ying Sun, Qiu-Xia Feng, Xun Xu, Jing Zhang, Fei-Peng Zhu, Yan-Hao Yang, and YuDong Zhang. Radiologic-radiomic machine learning models for differentiation of benign and malignant solid renal masses: comparison with expert-level radiologists. American Journal of Roentgenology, 214(1):W44\u2013W54, 2020.   \n[7] Nicholas Heller, Sean McSweeney, Matthew Thomas Peterson, Sarah Peterson, Jack Rickman, Bethany Stai, Resha Tejpaul, Makinna Oestreich, Paul Blake, Joel Rosenberg, et al. An international challenge to use artificial intelligence to define the state-of-the-art in kidney and kidney tumor segmentation in ct imaging. Journal of Clinical Oncology, 38(Suppl):626, 2020.   \n[8] Nicholas Heller, Fabian Isensee, Klaus H Maier-Hein, Xiaoshuai Hou, Chunmei Xie, Fengyi Li, Yang Nan, Guangrui Mu, Zhiyong Lin, Miofei Han, et al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 challenge. Medical Image Analysis, 67:101821, 2021.   \n[9] Jinhao Li, Huying Li, Yuan Zhang, Zhiqiang Wang, Sheng Zhu, Xuanya Li, Kai Hu, and Xieping Gao. MCNet: A multi-level context-aware network for the segmentation of adrenal gland in CT images. Neural Networks, 170:136\u2013148, 2024.   \n[10] Lawrence P Panych and Bruno Madore. The physics of MRI safety. Journal of Magnetic Resonance Imaging, 47(1):28\u201343, 2018.   \n[11] Subhashis Banerjee, Sushmita Mitra, Francesco Masulli, and Stefano Rovetta. Deep radiomics for brain tumor detection and classification from multi-sequence MRI. arXiv preprint arXiv:1903.09240, 2019.   \n[12] Xinyu Zhou, Xuanya Li, Kai Hu, Yuan Zhang, Zhineng Chen, and Xieping Gao. ERV-Net: An efficient 3d residual neural network for brain tumor segmentation. Expert Systems with Applications, 170:114566, 2021.   \n[13] JJ Nikken and GP Krestin. MRI of the kidney\u2014state of the art. European Radiology, 17:2780\u2013 2793, 2007.   \n[14] Fabian Isensee, Marianne Schell, Irada Pflueger, Gianluca Brugnara, David Bonekamp, Ulf Neuberger, Antje Wick, Heinz-Peter Schlemmer, Sabine Heiland, Wolfgang Wick, et al. Automated brain extraction of multisequence MRI using artificial neural networks. Human Brain Mapping, 40(17):4952\u20134964, 2019.   \n[15] Daniela Said, Stefanie J Hectors, Eric Wilck, Ally Rosen, Daniel Stocker, Octavia Bane, Alp Tuna Beksa\u00e7, Sara Lewis, Ketan Badani, and Bachir Taouli. Characterization of solid renal neoplasms using MRI-based quantitative radiomics features. Abdominal Radiology, 45:2840\u20132850, 2020.   \n[16] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning, pages 1180\u20131189. PMLR, 2015.   \n[17] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4893\u20134902, 2019.   \n[18] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):4396\u20134415, 2022.   \n[19] Pengfei Wei, Lingdong Kong, Xinghua Qu, Yi Ren, Zhiqiang Xu, Jing Jiang, and Xiang Yin. Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective. Advances in Neural Information Processing Systems, 36, 2024.   \n[20] Zhongqi Yue, Qianru Sun, and Hanwang Zhang. Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation. Advances in Neural Information Processing Systems, 36, 2024.   \n[21] Xiaowei Xu, Yinan Chen, Jianghao Wu, Jiangshan Lu, Yuxiang Ye, Yechong Huang, Xin Dou, Kang Li, Guotai Wang, Shaoting Zhang, et al. A novel one-to-multiple unsupervised domain adaptation framework for abdominal organ segmentation. Medical Image Analysis, 88:102873, 2023.   \n[22] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8188\u20138197, 2020.   \n[23] Xiaoshuai Hou, Chunmei Xie, Fengyi Li, Jiaping Wang, Chuanfeng Lv, Guotong Xie, and Yang Nan. A triple-stage self-guided network for kidney tumor segmentation. In 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), pages 341\u2013344. IEEE, 2020.   \n[24] Mingjun Ma, Haiying Xia, Yumei Tan, Haisheng Li, and Shuxiang Song. HT-Net: hierarchical context-attention transformer network for medical ct image segmentation. Applied Intelligence, pages 1\u201314, 2022.   \n[25] Luana Batista da Cruz, Domingos Alves Dias J\u00fanior, Jo\u00e3o Ot\u00e1vio Bandeira Diniz, Arist\u00f3- fanes Corr\u00eaa Silva, Jo\u00e3o Dallyson Sousa de Almeida, Anselmo Cardoso de Paiva, and Marcelo Gattass. Kidney tumor segmentation from computed tomography images using deeplabv $3+2.5$ d model. Expert Systems with Applications, 192:116270, 2022.   \n[26] Xiuzhen Xie, Lei Li, Sheng Lian, Shaohao Chen, and Zhiming Luo. SERU: A cascaded SEResNeXT U-Net for kidney and tumor segmentation. Concurrency and Computation: Practice and Experience, 32(14):e5738, 2020.   \n[27] Yasmeen George. A coarse-to-fine 3D U-Net network for semantic segmentation of kidney CT scans. In International Challenge on Kidney and Kidney Tumor Segmentation, pages 137\u2013142. Springer, 2021.   \n[28] Qian Yu, Yinghuan Shi, Jinquan Sun, Yang Gao, Jianbing Zhu, and Yakang Dai. Crossbar-net: A novel convolutional neural network for kidney tumor segmentation in ct images. IEEE Transactions on Image Processing, 28(8):4060\u20134074, 2019.   \n[29] Andriy Myronenko and Ali Hatamizadeh. 3d kidneys and kidney tumor semantic segmentation using boundary-aware networks. arXiv preprint arXiv:1909.06684, 2019.   \n[30] Mohit Pandey and Abhishek Gupta. Tumorous kidney segmentation in abdominal CT images using active contour and 3D-UNet. Irish Journal of Medical Science (1971-), 192(3):1401\u20131409, 2023.   \n[31] Tianle Ma and Aidong Zhang. Affinitynet: semi-supervised few-shot learning for disease type prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 1069\u20131076, 2019.   \n[32] Ozan Ciga, Tony Xu, and Anne Louise Martel. Self supervised contrastive learning for digital histopathology. Machine Learning with Applications, 7:100198, 2022.   \n[33] Kevin Faust, Adil Roohi, Alberto J Leon, Emeline Leroux, Anglin Dent, Andrew J Evans, Trevor J Pugh, Sangeetha N Kalimuthu, Ugljesa Djuric, and Phedias Diamandis. Unsupervised resolution of histomorphologic heterogeneity in renal cell carcinoma using a brain tumor\u2013 educated neural network. JCO Clinical Cancer Informatics, 4:811\u2013821, 2020.   \n[34] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 2223\u20132232, 2017.   \n[35] Cheng Chen, Qi Dou, Hao Chen, Jing Qin, and Pheng Ann Heng. Unsupervised bidirectional cross-modality adaptation via deeply synergistic image and feature alignment for medical image segmentation. IEEE Transactions on Medical Imaging, 39(7):2494\u20132505, 2020.   \n[36] Chenhao Pei, Fuping Wu, Liqin Huang, and Xiahai Zhuang. Disentangle domain features for cross-modality cardiac image segmentation. Medical Image Analysis, 71:102078, 2021.   \n[37] Devavrat Tomar, Manana Lortkipanidze, Guillaume Vray, Behzad Bozorgtabar, and JeanPhilippe Thiran. Self-attentive spatial adaptive normalization for cross-modality domain adaptation. IEEE Transactions on Medical Imaging, 40(10):2926\u20132938, 2021.   \n[38] Qingsong Xie, Yuexiang Li, Nanjun He, Munan Ning, Kai Ma, Guoxing Wang, Yong Lian, and Yefeng Zheng. Unsupervised domain adaptation for medical image segmentation by disentanglement learning and self-training. IEEE Transactions on Medical Imaging, 43(1):4\u201314, 2022.   \n[39] Xiaokang Liu, Zhiqiang Wang, Kai Hu, and Xieping Gao. Pseudo Multi-Source Domain Extension and Selective Pseudo-Labeling for Unsupervised Domain Adaptive Medical Image Segmentation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.   \n[40] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International Conference on Machine Learning, pages 1989\u20131998. Pmlr, 2018.   \n[41] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-toimage translation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 172\u2013189, 2018.   \n[42] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8789\u20138797, 2018.   \n[43] Anmol Sharma and Ghassan Hamarneh. Missing MRI pulse sequence synthesis using multimodal generative adversarial network. IEEE Transactions on Medical Imaging, 39(4):1170\u2013 1183, 2019.   \n[44] Behnam Gholami, Pritish Sahu, Ognjen Rudovic, Konstantinos Bousmalis, and Vladimir Pavlovic. Unsupervised multi-target domain adaptation: An information theoretic approach. IEEE Transactions on Image Processing, 29:3993\u20134002, 2020.   \n[45] Yoonsik Kim, Jae Woong Soh, Gu Yong Park, and Nam Ik Cho. Transfer learning from synthetic to real-noise denoising with adaptive instance normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3482\u20133492, 2020.   \n[46] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Chengpeng Chen. Hinet: Half instance normalization network for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 182\u2013192, 2021.   \n[47] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \n[48] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. Advances in Neural Information Processing Systems, 32, 2019.   \n[49] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524\u201310533. PMLR, 2020.   \n[50] Junho Kim, Minjae Kim, Hyeonwoo Kang, and Kwanghee Lee. U-gat-it: Unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation. arXiv preprint arXiv:1907.10830, 2019.   \n[51] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.   \n[52] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE International Conference on Computer Vision, pages 1501\u20131510, 2017.   \n[53] Hyeonseob Nam and Hyo-Eun Kim. Batch-instance normalization for adaptively style-invariant neural networks. Advances in Neural Information Processing Systems, 31, 2018.   \n[54] Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Meiling Wang, Xin Li, Zhengxing Sun, Qian Li, and Errui Ding. Adaattn: Revisit attention mechanism in arbitrary neural style transfer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6649\u20136658, 2021.   \n[55] Xianchao Guan, Yifeng Wang, Yiyang Lin, Xi Li, and Yongbing Zhang. Unsupervised MultiDomain Progressive Stain Transfer Guided by Style Encoding Dictionary. IEEE Transactions on Image Processing, 2024.   \n[56] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8798\u20138807, 2018.   \n[57] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \n[58] Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. arXiv preprint arXiv:1611.02200, 2016.   \n[59] Nicholas Heller, Niranjan Sathianathen, Arveen Kalapara, Edward Walczak, Keenan Moore, Heather Kaluzniak, Joel Rosenberg, Paul Blake, Zachary Rengel, Makinna Oestreich, et al. The kits19 challenge data: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgical outcomes. arXiv preprint arXiv:1904.00445, 2019.   \n[60] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[61] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. ", "page_idx": 14}, {"type": "text", "text": "[62] A Emre Kavur, N Sinem Gezer, Mustafa Bar\u0131\u00b8s, Sinem Aslan, Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee, Philipp Ernst, Sava\u00b8s \u00d6zkan, et al. CHAOS challenge-combined (CT-MR) healthy abdominal organ segmentation. Medical Image Analysis, 69:101950, 2021. ", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the appendix of this paper, Section A provides a detailed introduction to the experimental datasets and evaluation metrics. Subsequently, Section B offers the specific implementation details of the experiments. Section C presents a visual comparison of the experimental results from different methods on the KiTS19 and MSKT datasets. Section D shows a comparison of training efficiency among different methods. Sections E and $\\boldsymbol{\\mathrm F}$ present two sets of supplementary comparative experiments and two sets of supplementary ablation experiments, respectively. Finally, Section G discusses the potential limitations and the broad impacts of our method. ", "page_idx": 15}, {"type": "text", "text": "A Details of the Experimental Datasets and Evaluation Metrics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "MSKT The MSKT dataset contains 104 cases of multi-sequence kidney and kidney tumor MRI data. Each case includes four MRI sequences: T1c, FS T2W, T2W, and DWI. The data are stored in NRRD format and scanned in axial view. The MR images have a resolution of $512\\times512\\times N.$ where $N$ represents the number of slices, ranging from 11 to 96. The slice thickness varies from 2 to 6 millimeters. Annotations were created by experienced clinicians, with normal kidney tissue regions labeled as 1, kidney tumor regions as 2, and other areas labeled as 0, considered as the background. In the first set of comparative experiments (Section 4.2), we randomly divide all cases into source domain training sets, target domain training sets, and target domain test sets in a 4.5:4.5:1 ratio. The annotated T1c data, comprising 46 cases, serve as the source domain, while the unannotated FS T2W, T2W, and DWI data, each comprising 46 cases, constitute the target domain. The remaining 12 cases are used as the test set. This division ensures that there is no intersection between the source domain, target domain, and test set to prevent information leakage. The number of samples and slices included in each sequence is detailed in Table 6. ", "page_idx": 15}, {"type": "text", "text": "To protect patient privacy, all data were anonymized during collection and processing, removing any information that could identify the patients. The use of this dataset has been reviewed and approved by the Institutional Review Board (IRB) to ensure that the research complies with ethical standards and safeguards participant rights. We strictly adhere to relevant laws and regulations regarding data use and protection to ensure the legal and compliant use of the data. ", "page_idx": 15}, {"type": "text", "text": "KiTS19 The KiTS19 dataset, established for the 2019 kidney and kidney tumor segmentation challenge, comprises multi-phase CT scans along with corresponding segmentation annotations for 300 patients. A total of 210 cases are designated for model training, and the remaining 90 are used to evaluate model performance. The CT images have a resolution of $512\\,\\times\\,512\\,\\times\\,N$ , where $N$ ranges from 29 to 1,059 slices. For our second set of comparative experiments (Section 4.2), we combine the KiTS19 dataset with the MSKT dataset. Specifically, as the KiTS19 challenge initially provides only the training set, we use all cases from this set as our source domain training set, which includes $45{,}424\\,\\mathrm{CT}$ slices, with 16,336 slices containing targets. All four sequences of the MSKT are considered as the target domain, and we merge the 92 cases identified as source and target domains in Table 6 to form the target domain training set, while the test set remains unchanged. Please note that in all of our experiments, the slices used for training all contain the kidney or tumor targets, while the slices used for testing are all the slices in each case. ", "page_idx": 15}, {"type": "text", "text": "Evaluation metrics Following [21], the Dice Similarity Coefficient $(D S C)$ and the $95\\%$ Hausdorff Distance $(H D_{95})$ are utilized for quantitative comparisons. It is important to note that if the predicted results and the true annotations do not contain the same categories, directly calculating $H D_{95}$ may lead to errors. To address this issue, we set the $H D_{95}$ value to 100 in cases of potential calculation errors, indicating no overlap between the predicted results and the true annotations. ", "page_idx": 15}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All experiments are conducted on an NVIDIA A800 GPU utilizing the PyTorch framework. CT and MR data are uniformly processed to a resolution of $256\\!\\times\\!256$ . In the one-to-multiple domain adaptation stage, the number of domains is set to 4 and 5 for the two sets of experiments, respectively (including source and target domains). The length of the multi-level style dictionary is set to 4,096. The number of style fusion modules in the generator is 4. The weights of adversarial loss, cycle consistency loss, and identity loss are set to 1, 10, and 1, respectively. Network optimization employs the Adam optimizer [61] with an initial learning rate of 1e-4, and the batch size is 8. Other hyperparameters follow the configuration in StarGAN v2. In the segmentation stage, U-Net is chosen as the network architecture, paired with the Adam optimizer starting with a learning rate of 1e-4. The batch size is set to 16, with a total training duration of 50 epochs. In the above setup, it takes about 18 hours for our method to fully execute once. ", "page_idx": 15}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/7cccc7cb18b6daa678f2d89346a7ceae0c4dc999d71d4c76245dbb3dfcad3370.jpg", "table_caption": ["Table 6: Data partitioning for each sequence in the MSKT dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B.1 Network Architecture ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Generator As shown in Table 7, the generator within PSTUDA comprises an encoder, four style fusion modules, and a decoder. The encoder includes three sets of convolutional blocks and four Resnet blocks, while the decoder is composed of two upsampling layers with transposed convolution operations and one convolutional block. Instance Normalization is applied during the encoding phase, and Point-wise Instance Normalization is utilized within the style fusion modules and the decoder. ", "page_idx": 16}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/8eedc44bb1b2924295ce382f6e9318e4807ee2bbf40b3f3af6aab29bf8ff28e5.jpg", "table_caption": ["Table 7: Architecture of the generator. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Discriminator As shown in Fig. 2 in Section 3.4, we utilize four independent and identical discrimination branches in the multi-scale discriminator. The detailed architecture of the first discrimination branch, which takes the original size image as input, is presented in Table 8. ", "page_idx": 16}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/f1adacd5f9c6f8750921266acfe239fcf4ec685b7b5ac460caeea65ddc1f7d13.jpg", "table_caption": ["Table 8: Architecture of the discriminator. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Implementation of Comparative Methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "CycleGAN This method is derived from the GitHub repository: https://github.com/junyanz/pytorchCycleGAN-and-pix2pix. In our experiments, ResNet_9block and PatchGAN with 3 convolutional layers are selected as the generator and discriminator for CycleGAN, respectively. The weights for adversarial loss (MSE), cycle consistency loss (L1), and domain-invariant perceptual loss (L1) are set to 1, 10, and 5, respectively. Other hyperparameters are consistent with the open-source code. During the segmentation phase, all hyperparameters for U-Net are the same as those in our PSTUDA. ", "page_idx": 17}, {"type": "text", "text": "MUNIT This method is from the GitHub repository: https://github.com/NVlabs/MUNIT. The architecture of the MUNIT network remains the same as the open-source code. The weights for adversarial loss (MSE), image reconstruction loss (L1), style reconstruction loss (L1), and content reconstruction loss (L1) are set to 1, 10, 1, and 1, respectively. In the segmentation stage, all U-Net hyperparameters are identical to those in our PSTUDA. ", "page_idx": 17}, {"type": "text", "text": "SIFA This method is available from the GitHub repository: https://github.com/JianghaoWu/SIFApytorch. The official version of this method is implemented in TensorFlow, available at https://github.com/cchen-cc/SIFA. In OMUDA, they have re-implemented SIFA in PyTorch, and we have used the PyTorch version of SIFA to ensure all methods use the same framework for a fair comparison. All settings, hyperparameters, and losses used in our experiments are the same as those in the open-source code. ", "page_idx": 17}, {"type": "text", "text": "DEPL This method is based on our own implementation. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "StarGAN v2 This method is from the GitHub repository: https://github.com/clovaai/stargan-v2. In our experiments, the network architecture is consistent with the official code. The weights for R1 regression loss, adversarial loss (CE), cycle consistency loss (L1), style reconstruction loss (L1), and diversity-sensitive loss (L1) are all set to 1. High-pass filtering is not used in this comparative experiment. During the segmentation phase, all U-Net hyperparameters are the same as those in PSTUDA. ", "page_idx": 17}, {"type": "text", "text": "C Visualization of the KiTS19 and MSKT Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present the visualized results of the second set of comparative experiments conducted on the KiT19 and MSKT datasets, and these visualizations are discussed in Section 4.2. ", "page_idx": 17}, {"type": "image", "img_path": "cMwSoXLCVi/tmp/f1198f42ccd323dce0380b7a30c0dc93dfa32a1e8df344587a0012ec7540bbff.jpg", "img_caption": ["Figure 4: Qualitative results for CT $\\rightarrow$ T1c, FS T2W, T2W, and DWI on the KiTS19 and MSKT datasets. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Training Efficiency ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section displays the comparative outcomes in terms of model parameters and FLOPs among different cross-domain image translation methods. As illustrated in Table 9, we compare PSTUDA with CycleGAN, MUNIT, and StarGAN v2, because these methods do not involve a segmentation stage. It is worth note that the style dictionary itself does not contribute to the computation of FLOPs. All models are trained using FP32 precision. The detailed analysis of these comparative results is provided in Section 4.2. ", "page_idx": 18}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/f8d2858ec9266938da974f9eadb69b9b22aeabb31786e44508db72090e37e5c0.jpg", "table_caption": ["Table 9: Model parameters and FLOPs of different methods. "], "table_footnote": ["$G$ : Generator, $D$ : Discriminator, $N_{M}$ : Mapping Network, $E_{S}$ : Style Encoder, $D_{S}$ : Style Dictionary. "], "page_idx": 18}, {"type": "text", "text": "E Supplementary Comparative Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To further validate the generalization capability of PSTUDA, we conduct bidirectional cross-modal validation experiments on a publicly available abdominal multi-organ dataset [62] and perform reverse validation experiments from MR to CT on the MSKT and KiTS19 datasets. The results, as shown in Tables 10 and 11, indicate that our method significantly outperforms StarGAN v2 (baseline) in both experimental groups. ", "page_idx": 18}, {"type": "text", "text": "F Supplementary Ablation Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The multi-level style dictionary and style fusion module are the core components of PSTUDA, and these two modules include the following hyperparameters: the depth of the style dictionary, the number of levels in the style dictionary, and the number of style fusion modules. It is important to note that the number of levels in the style dictionary is equal to the number of style fusion modules. To investigate the sensitivity of model performance to various hyperparameters, we conduct extensive ablation studies on the depth of the multi-level style dictionary (M_SD) and the number of style fusion modules (SFM). As presented in Tables 12 and 13, under the original settings (dictionary depth of 4,096 and module number of 4), PSTUDA performs optimally in most experiments. Notably, the segmentation performance on the T2W sequence improves when the dictionary depth is increased to 16,384. ", "page_idx": 18}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/045ec97d3465ba453dd25af20dcb1fb3eb8f0df54bae22fd35b53487199ebdda.jpg", "table_caption": ["Table 10: Quantitative segmentation results for bidirectional cross-modal experiments on the abdominal multi-organ dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/fb6c9638a301a2dff508ae9eb0942eaae6ffa7e89467c6dd528a5289d82349e1.jpg", "table_caption": ["Table 11: Quantitative segmentation results from MR to CT on the MSKT and KiTS19 datasets. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/4038fb0826b2e8c0e417c28f84aa261c81ee51b7e877d660fcba1d4651b0150b.jpg", "table_caption": ["Table 12: Ablation study on the depth of the Multi-level Style Dictionary (M_SD) on the MSKT dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "cMwSoXLCVi/tmp/b693de96c888c0e4872afdeb7750fff49f44f6201fabf1dc91278256aef5649d.jpg", "table_caption": ["Table 13: Ablation study on the number of Style Fusion Modules (SFM) on the MSKT dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "G Limitations and Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Limitations Our work presents a domain adaptation method for multi-target domain image translation. However, the field of one-to-multiple domain adaptation is not extensively explored, and our study may have some limitations that should be acknowledged. Firstly, the tasks we focus on are confined to domain adaptation of multi-sequence MR images and the segmentation of kidneys and tumors as presented in this work. In future research, we aim to broaden our scope to include domain adaptation for other medical and natural images, as well as other downstream tasks such as object detection. Secondly, compared to the ideal one-to-multiple model, our current model inherently operates within a broader multi-domain to multi-domain adaptation framework. Therefore, in our future studies, we aspire to design a more specialized one-to-multiple model, which is expected to further improve model performance and reduce model complexity. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Broader impacts In fact, the impact of one-to-multiple domain adaptation technology is profound, especially when facing numerous target domains and limited annotated data. Our PSTUDA framework can efficiently extend an annotated source domain to adapt to multiple unannotated new domains, significantly reducing the time and resource investment required for multi-domain transfer tasks. This capability is crucial in complex medical data environments, as it not only greatly alleviates the workload of medical professionals but also holds promise for improving the diagnosis and treatment processes for patients. Overall, our research enhances the flexibility and efficiency of domain adaptation technology, which advances the field of machine learning and brings innovative solutions to critical industries such as healthcare. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our main claims stated in the abstract and introduction accurately reflect the contributions and scope of the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Appendix G. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We introduce our method in detail in Section 3. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We disclose all the information needed to reproduce the main experimental results of the paper in Appendix B. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: We open-source the code and a portion of the data. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide detailed information about the experimental setup in Appendix B. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: Error bars are not reported because they are too computationally expensive for one-to-one domain adaptive methods. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide detailed information about the compute resources needed for the experiments in Appendices B and D. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research conducted in our paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the potential broader impacts of our work in Appendix G. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our work does not pose such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have cited all the relevant assets used in our paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We used our private dataset in our work, which has good documentation. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide an explanation about this issue in Appendix A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Our private dataset has been reviewed and approved by the IRB, and we provide an explanation about this issue in Appendix A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]