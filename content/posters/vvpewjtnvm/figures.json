[{"figure_path": "vvpewjtnvm/figures/figures_7_1.jpg", "caption": "Figure 1: Accuracy and loss of FedAvg with full precision (origin), our method with precision levels of 8 bit and 6 bit. We set \u03b1 = 0.01 on all the four datasets. Our method exhibits an effective reduction in fluctuation variance and improves the stability of training. The reason is that compared with the full precision training, our low precision local training can prevent the client models to drift further away from each other and overfit the local datasets, making the aggregation stable.", "description": "This figure compares the accuracy and loss curves of the FedAvg algorithm using full precision and the proposed low-precision method with 8-bit and 6-bit precision. The results are shown for four datasets: FashionMNIST, CIFAR10, CINIC10, and CIFAR100. The low-precision method demonstrates improved stability and reduced fluctuation compared to the full-precision method, highlighting its effectiveness in mitigating model drift and overfitting issues in federated learning.", "section": "6.1 Results on FedAvg"}, {"figure_path": "vvpewjtnvm/figures/figures_7_2.jpg", "caption": "Figure 2: Effectiveness of our method in relieving the over-fitting issue. We present the averaged local training loss and the global test loss over training. We select a part of the training procedure (iteration 1000 to 1400) for display, and enlarge a part of the picture in the upper right corner to show more details. We can observe that the local training loss of FedAvg (full precision) is significantly lower than our method, however its global test loss is much higher than us and fluctuates dramatically.", "description": "This figure compares the training and testing loss curves for FedAvg (full precision) and the proposed low-precision method.  It shows that while the low-precision method has higher local training loss, it significantly reduces the global testing loss and exhibits greater stability, indicating that it effectively mitigates overfitting.", "section": "6.2 Quantization Relieves Overfitting"}, {"figure_path": "vvpewjtnvm/figures/figures_9_1.jpg", "caption": "Figure 3: Results on CIFAR10 with \u03b1 = 0.01. () denotes the percentage of models on the clients. We use the number of weights, activation, and gradients of local training to approximate the training cost (MB/client) and communication cost (MB/round).", "description": "This figure compares the training cost, communication cost, and accuracy of different model compression and low-precision training methods on the CIFAR10 dataset.  The methods include HeteroFL, SplitMix, and the proposed low-bit moving average method. The varying percentages shown in parentheses represent the fraction of the global model used in client training. The figure showcases the trade-offs between these factors, illustrating how the proposed method achieves high accuracy while reducing resource consumption.", "section": "6 Experiments"}, {"figure_path": "vvpewjtnvm/figures/figures_25_1.jpg", "caption": "Figure 1: Accuracy and loss of FedAvg with full precision (origin), our method with precsion levels of 8 bit and 6 bit. We set a = 0.01 on all the four datasets. Our method exhibits an effective reduction in fluctuation variance and improves the stability of training. The reason is that compared with the full precision training, our low precision local training can prevent the client models to drift further away from each other and overfit the local datasets, making the aggregation stable.", "description": "This figure compares the accuracy and loss curves for FedAvg with full precision and the proposed low-precision method using 8-bit and 6-bit precision.  The results show that the low-precision method achieves comparable accuracy while demonstrating improved stability and reduced fluctuation. This is attributed to the low-precision local training preventing client models from diverging and overfitting their local datasets, thereby leading to more stable aggregation.", "section": "6.1 Results on FedAvg"}]