{"importance": "This paper is crucial for researchers in federated learning due to its significant contribution in enhancing efficiency and addressing critical challenges. By proposing and validating a low-precision local training paradigm, it reduces communication and computation costs substantially. This is especially relevant given the resource-constrained nature of edge devices in federated learning scenarios. The theoretical analysis and empirical validation demonstrate the effectiveness of the method, addressing overfitting issues commonly encountered in heterogeneous data settings. Furthermore, its flexibility to integrate with existing algorithms and achieve comparable performance with 8-bit precision opens exciting avenues for future work, such as exploring hardware-specific optimizations for further efficiency improvements.", "summary": "Low-precision local training, surprisingly, is sufficient for accurate federated learning, significantly reducing communication and computation costs.", "takeaways": ["Low-precision local training is sufficient for accurate federated learning model aggregation.", "The proposed method significantly reduces communication and computation costs in federated learning.", "Low-precision training alleviates overfitting in heterogeneous data settings."], "tldr": "Federated learning (FL) faces challenges in communication efficiency and computation costs due to resource-constrained edge devices and the need for privacy-preserving data handling. Existing methods often focus on improving global model aggregation or employing regularization to reduce local model drift, but they might not fully address the computational burden at the client-side. Heterogeneous data distribution across clients further complicates the training process, causing overfitting and model aggregation failures. This paper presents a novel FL paradigm to tackle these issues. \nThe proposed method utilizes low-precision computation during local model training and communication, while reserving high-precision for model aggregation on the server-side. This significantly reduces both communication and computation costs. The method leverages the stability of moving average for aggregation, theoretically demonstrating its convergence to the optimal solution, even with low-precision local training. Experiments validate the effectiveness of the paradigm, showing comparable performance with 8-bit precision to that of full-precision methods. The reduced model expressiveness from low-precision training also alleviates overfitting, preventing local model drift and ensuring effective aggregation in the heterogeneous data scenario.", "affiliation": "Fudan University", "categories": {"main_category": "Machine Learning", "sub_category": "Federated Learning"}, "podcast_path": "vvpewjtnvm/podcast.wav"}