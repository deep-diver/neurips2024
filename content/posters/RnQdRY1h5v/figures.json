[{"figure_path": "RnQdRY1h5v/figures/figures_4_1.jpg", "caption": "Figure 1: B'MOJO's memory management. (Left) Illustration of the B'MOJO layer. (Right) B'MOJO's Realization. B\u2019MOJO's fading memory is computed by a SSM that represents long-range dependencies through its state (a fixed-dimensional representation) which is later aggregated along with with the most recent past. B'MOJO's eidetic memory stores tokens selected from the past using an innovation test on the SSM's state and appends them to the current sliding window. The innovation test measures how difficult it is to predict the next token using the SSM's state. If a token is difficult to predict, we store it in the eidetic memory and pass it to the attention module together with the state, a compressed summary of the past, and the most recent tokens.", "description": "The figure illustrates B'MOJO's memory management system.  The left panel shows a single B'MOJO layer, highlighting the interaction between eidetic memory (short-term, lossless storage), fading memory (long-term, lossy state representation from an SSM), and the sliding window attention mechanism. The right panel details B'MOJO's realization, showcasing how long-range dependencies are captured by the SSM and how the innovation selection process adds informative past tokens to the eidetic memory based on their unpredictability.", "section": "3 B'MOJO"}, {"figure_path": "RnQdRY1h5v/figures/figures_7_1.jpg", "caption": "Figure 3: B\u2019MOJO language modeling scaling laws. We plot the perplexity reached by models at different scales against the number of parameters and the wall-clock training time. B\u2019MOJO is faster than Mamba and Mistral at training time while achieving better perplexity than Mamba and comparable perplexity with Mistral. The plot also exhibits a non-saturating scaling law, showing that increasing the amount of resources leads to increasingly better B\u2019MOJO models.", "description": "This figure shows the scaling laws of B\u2019MOJO and other language models, comparing perplexity and training time as a function of the number of parameters.  B\u2019MOJO demonstrates faster training and comparable or better perplexity than other models, indicating efficient use of resources and a potential for continued improvement with scale.", "section": "4.2 Language Modeling Scaling laws"}, {"figure_path": "RnQdRY1h5v/figures/figures_9_1.jpg", "caption": "Figure 4: Time in ms to process 2k sequences. B\u2019MOJO is faster than other efficient implementations of Mamba [19] and Transformers [19] at all scales.", "description": "This figure compares the inference time of B\u2019MOJO with other state-of-the-art models such as Mamba and Mistral across different scales (model sizes). The results demonstrate that B\u2019MOJO is significantly faster for processing 2k sequences compared to these other models, showcasing its efficiency in terms of inference speed. The speed advantage is consistent across various model sizes.", "section": "4 Experimental Results"}, {"figure_path": "RnQdRY1h5v/figures/figures_9_2.jpg", "caption": "Figure 2: (Panels 1-3) B\u2019MOJO has high memory efficiency on Associative Recall Tasks (sequence length is 256 and attention window 32). For various models, we plot accuracy on the Multi-Query Associative Recall (MQAR) task as a function of the model dimension (totaling the SSM state, eidetic memory and KV cache where applicable). The transformer paragon attains 100% accuracy because it operates on the full context. While all models benefit strongly from increased memory, B\u2019MOJO and B\u2019MOJO-F consistently achieve the best accuracies for a given memory budget. Panels 1-3 report MQAR tasks of increasing difficulty, on which the performance gap between B\u2019MOJO and other models increases, showcasing the value of eidetic memory. (Panel 4) Increases in eidetic memory size corresponds to gains in recall. We ablate the effects of eidetic memory by growing the number of eidetic memory tokens in B\u2019MOJO. Each added token contributes to an increase in recall accuracy until performance is saturated.", "description": "The figure shows the results of an experiment comparing different models' performance on the Multi-Query Associative Recall (MQAR) task.  The experiment varied the model's memory capacity (SSM state, eidetic memory, KV cache) and assessed accuracy. B'MOJO and B'MOJO-F consistently outperformed other models, demonstrating the effectiveness of their combined eidetic and fading memory.  An ablation study also confirmed that increasing eidetic memory size improved recall until saturation.", "section": "4.1 Synthetic Tasks"}, {"figure_path": "RnQdRY1h5v/figures/figures_9_3.jpg", "caption": "Figure 5: Length generalization. (Left) We pre-train B\u2019MOJO 1.4B and Mamba 1.4B on 2k context lengths and a 1.4B Transformer baseline on 1k. (Right) We pre-train B\u2019MOJO 790M and Mamba 790M on 8k context length and compare models evaluating perplexity on longer sequences up to 32K tokens on PG-19. Transformers cannot length generalize (a known failure mode), on the other hand B\u2019MOJO preserves/improves in perplexity better than Mamba even on longer sequences.", "description": "This figure shows the results of evaluating the length generalization capabilities of B\u2019MOJO and Mamba models.  The left panel shows the pre-training setup, while the right panel demonstrates the perplexity results on the PG-19 dataset as the evaluated context size increases, up to 32K tokens.  The key finding is B\u2019MOJO's ability to maintain or improve perplexity as the context length extends beyond the training context length, unlike Transformers which exhibit a known failure mode in this regard, and unlike Mamba, which performs worse on longer sequences.", "section": "4.4 Length Generalization"}, {"figure_path": "RnQdRY1h5v/figures/figures_15_1.jpg", "caption": "Figure 1: B'MOJO's memory management. (Left) Illustration of the B'MOJO layer. (Right) B'MOJO's Realization. B\u2019MOJO's fading memory is computed by a SSM that represents long-range dependencies through its state (a fixed-dimensional representation) which is later aggregated along with with the most recent past. B'MOJO's eidetic memory stores tokens selected from the past using an innovation test on the SSM's state and appends them to the current sliding window. The innovation test measures how difficult it is to predict the next token using the SSM's state. If a token is difficult to predict, we store it in the eidetic memory and pass it to the attention module together with the state, a compressed summary of the past, and the most recent tokens.", "description": "This figure illustrates the memory management mechanisms of the B'MOJO model. The left panel shows a high-level illustration of the B'MOJO layer, highlighting its components: sliding window attention, eidetic memory, and fading memory. The right panel provides a detailed breakdown of B'MOJO's realization, explaining how the fading memory (computed by a state-space model) and eidetic memory (tokens selected based on an innovation test) are combined to perform inference. The innovation test helps identify tokens that are difficult to predict and appends them to the eidetic memory.", "section": "3 B'MOJO"}, {"figure_path": "RnQdRY1h5v/figures/figures_15_2.jpg", "caption": "Figure 1: B'MOJO's memory management. (Left) Illustration of the B'MOJO layer. (Right) B'MOJO's Realization. B\u2019MOJO's fading memory is computed by a SSM that represents long-range dependencies through its state (a fixed-dimensional representation) which is later aggregated along with with the most recent past. B'MOJO's eidetic memory stores tokens selected from the past using an innovation test on the SSM's state and appends them to the current sliding window. The innovation test measures how difficult it is to predict the next token using the SSM's state. If a token is difficult to predict, we store it in the eidetic memory and pass it to the attention module together with the state, a compressed summary of the past, and the most recent tokens.", "description": "This figure illustrates B'MOJO's memory management system. The left panel shows a single B'MOJO layer, highlighting the interaction between the SSM (red), eidetic memory (blue), fading memory (orange), and sliding window attention (yellow).  The right panel provides a broader view of the overall B'MOJO realization, emphasizing the innovation selection process for determining which tokens are stored in eidetic memory based on their predictability.", "section": "3 B'MOJO"}, {"figure_path": "RnQdRY1h5v/figures/figures_17_1.jpg", "caption": "Figure 8: B'MOJO's long context training using BPTT Appendix B.2. We train B'MOJO with two different context sizes, 2k and 16k (BPTT) respectively and evaluate on long context task (PG-19). We show that our model trained with 2k context size is able to extrapolate for context size upto 65536 (with marginal increase in the perplexity), while model trained with 16k context size can handle long context much more effectively which can be seen by the lower perplexity values as the context size increases. Remarkably, as previously observed in [10], B'MOJO models trained on long contexts underperform models trained on shorter ones if evaluated on fewer tokens (i.e., when the inference context size is much smaller than the training context size).", "description": "This figure shows the perplexity of B\u2019MOJO models trained with different context lengths (2k and 16k) when evaluated on various context lengths (up to 65536 tokens). It demonstrates the model's ability to generalize to longer context lengths than it was trained on (length generalization), particularly with the model trained on 16k context.", "section": "4.4 Length Generalization"}, {"figure_path": "RnQdRY1h5v/figures/figures_21_1.jpg", "caption": "Figure 1: B'MOJO's memory management. (Left) Illustration of the B'MOJO layer. (Right) B'MOJO's Realization. B\u2019MOJO's fading memory is computed by a SSM that represents long-range dependencies through its state (a fixed-dimensional representation) which is later aggregated along with with the most recent past. B'MOJO's eidetic memory stores tokens selected from the past using an innovation test on the SSM's state and appends them to the current sliding window. The innovation test measures how difficult it is to predict the next token using the SSM's state. If a token is difficult to predict, we store it in the eidetic memory and pass it to the attention module together with the state, a compressed summary of the past, and the most recent tokens.", "description": "This figure illustrates B'MOJO's memory management. The left panel shows the architecture of a single B'MOJO layer, highlighting the interaction between the sliding window attention, eidetic memory (for storing important tokens), and fading memory (represented by the SSM's state).  The right panel provides a schematic of the entire B'MOJO realization, demonstrating how the short-term eidetic memory, long-term fading memory, and asynchronous retrieval work together to improve the model's ability to access information from both the recent and distant past.  The innovation selection mechanism is key to deciding which tokens should be stored in the eidetic memory for later retrieval.", "section": "3 B'MOJO"}, {"figure_path": "RnQdRY1h5v/figures/figures_22_1.jpg", "caption": "Figure 1: B'MOJO's memory management. (Left) Illustration of the B'MOJO layer. (Right) B'MOJO's Realization. B\u2019MOJO's fading memory is computed by a SSM that represents long-range dependencies through its state (a fixed-dimensional representation) which is later aggregated along with with the most recent past. B'MOJO's eidetic memory stores tokens selected from the past using an innovation test on the SSM's state and appends them to the current sliding window. The innovation test measures how difficult it is to predict the next token using the SSM's state. If a token is difficult to predict, we store it in the eidetic memory and pass it to the attention module together with the state, a compressed summary of the past, and the most recent tokens.", "description": "This figure illustrates the memory management mechanism of the B'MOJO model. The left panel shows a high-level overview of the B'MOJO layer, which combines eidetic and fading memory using a sliding window attention mechanism. The right panel provides a more detailed breakdown of B'MOJO's realization, explaining how the fading and eidetic memory components work together to achieve long-range dependency and high recall capabilities.", "section": "3 B'MOJO"}]