[{"figure_path": "GcZgo9ffGt/tables/tables_4_1.jpg", "caption": "Table 1: Performance comparisons using 7 instruction tuning datasets with the LLAMA-2-7B on 6 categories of 18 traditional NLP tasks and 3 open-ended benchmarks with LLM as judgements. \u201cIT\u201d refers to INSTRUCTION TUNING. \"IM\" refers to INSTRUCTION MODELLING. Green and red arrows indicate performance changes against the baseline (IT).", "description": "This table presents a comparison of the performance of Instruction Tuning (IT) and Instruction Modeling (IM) on 21 NLP datasets using the LLAMA-2-7B language model.  The datasets are categorized into 6 groups of traditional NLP tasks and 3 open-ended benchmarks.  Performance is measured using various metrics for each task category, and the table shows the mean performance across all tasks and for each specific task type. Green and red arrows indicate whether IM improved or worsened performance compared to the baseline (IT).", "section": "4.2 Main Results"}, {"figure_path": "GcZgo9ffGt/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparisons using 7 instruction tuning datasets with the LLAMA-2-7B on 6 categories of 18 traditional NLP tasks and 3 open-ended benchmarks with LLM as judgements. \u201cIT\u201d refers to INSTRUCTION TUNING. \"IM\" refers to INSTRUCTION MODELLING. Green and red arrows indicate performance changes against the baseline (IT).", "description": "This table presents a detailed comparison of the performance of INSTRUCTION TUNING (IT) and INSTRUCTION MODELLING (IM) on various NLP tasks and open-ended generation benchmarks. It uses seven different instruction tuning datasets and the LLAMA-2-7B language model.  The results are broken down into six categories of traditional NLP tasks and three open-ended benchmarks, with LLM-based evaluation metrics.  Green and red arrows highlight the performance improvements and decreases, respectively, compared to the baseline (IT).", "section": "4.2 Main Results"}, {"figure_path": "GcZgo9ffGt/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparisons using 7 instruction tuning datasets with the LLAMA-2-7B on 6 categories of 18 traditional NLP tasks and 3 open-ended benchmarks with LLM as judgements. \u201cIT\u201d refers to INSTRUCTION TUNING. \"IM\" refers to INSTRUCTION MODELLING. Green and red arrows indicate performance changes against the baseline (IT).", "description": "This table presents a comparison of the performance of INSTRUCTION TUNING (IT) and INSTRUCTION MODELLING (IM) on 21 NLP tasks using the LLAMA-2-7B model.  It shows the performance difference for each method across 7 instruction tuning datasets, with 18 traditional NLP tasks categorized into 6 groups and 3 open-ended benchmarks evaluated with LLMs. Green and red arrows indicate whether IM improved or worsened performance compared to IT. ", "section": "4.2 Main Results"}, {"figure_path": "GcZgo9ffGt/tables/tables_8_1.jpg", "caption": "Table 1: Performance comparisons using 7 instruction tuning datasets with the LLAMA-2-7B on 6 categories of 18 traditional NLP tasks and 3 open-ended benchmarks with LLM as judgements. \u201cIT\u201d refers to INSTRUCTION TUNING. \"IM\" refers to INSTRUCTION MODELLING. Green and red arrows indicate performance changes against the baseline (IT).", "description": "This table presents a comparison of the performance of INSTRUCTION TUNING (IT) and INSTRUCTION MODELLING (IM) using the LLAMA-2-7B model.  The comparison is done across 7 different instruction tuning datasets and 21 NLP benchmarks (categorized into 6 groups). The table shows the mean performance scores for each method across various NLP tasks and open-ended generation benchmarks (AlpacaEval 1.0 and 2.0, MT-Bench). Green and red arrows indicate whether IM outperforms or underperforms IT, respectively.", "section": "4.2 Main Results"}, {"figure_path": "GcZgo9ffGt/tables/tables_16_1.jpg", "caption": "Table 1: Performance comparisons using 7 instruction tuning datasets with the LLAMA-2-7B on 6 categories of 18 traditional NLP tasks and 3 open-ended benchmarks with LLM as judgements. \u201cIT\u201d refers to INSTRUCTION TUNING. \"IM\" refers to INSTRUCTION MODELLING. Green and red arrows indicate performance changes against the baseline (IT).", "description": "This table presents a detailed comparison of the performance of INSTRUCTION TUNING (IT) and INSTRUCTION MODELLING (IM) on various NLP tasks and open-ended generation benchmarks.  It uses the LLAMA-2-7B language model and seven different instruction tuning datasets.  The results are categorized into six groups of traditional NLP tasks and three open-ended benchmarks, with performance changes (improvements or reductions) compared to the baseline (IT) indicated by green and red arrows respectively.  The table allows for a comprehensive evaluation of the effectiveness of IM compared to the established IT method across diverse language tasks.", "section": "4.2 Main Results"}, {"figure_path": "GcZgo9ffGt/tables/tables_19_1.jpg", "caption": "Table 1: Performance comparisons using 7 instruction tuning datasets with the LLAMA-2-7B on 6 categories of 18 traditional NLP tasks and 3 open-ended benchmarks with LLM as judgements. \u201cIT\u201d refers to INSTRUCTION TUNING. \"IM\" refers to INSTRUCTION MODELLING. Green and red arrows indicate performance changes against the baseline (IT).", "description": "This table presents a detailed comparison of the performance of INSTRUCTION TUNING (IT) and INSTRUCTION MODELLING (IM) on various NLP tasks and open-ended generation benchmarks. Seven instruction tuning datasets were used with the LLAMA-2-7B language model.  The table is organized into six categories of traditional NLP tasks and three open-ended benchmarks, with the performance of both methods displayed for each dataset and benchmark, indicated by green and red arrows representing improvements and declines compared to IT, respectively. The table provides a comprehensive evaluation of the two methods across a diverse range of tasks.", "section": "4.2 Main Results"}, {"figure_path": "GcZgo9ffGt/tables/tables_21_1.jpg", "caption": "Table 1: Performance comparisons using 7 instruction tuning datasets with the LLAMA-2-7B on 6 categories of 18 traditional NLP tasks and 3 open-ended benchmarks with LLM as judgements. \u201cIT\u201d refers to INSTRUCTION TUNING. \"IM\" refers to INSTRUCTION MODELLING. Green and red arrows indicate performance changes against the baseline (IT).", "description": "This table presents a comparison of the performance of INSTRUCTION TUNING (IT) and INSTRUCTION MODELLING (IM) on 21 NLP benchmarks using the LLAMA-2-7B language model.  It shows the performance differences for each method across six categories of 18 traditional NLP tasks and three open-ended generation benchmarks.  The results are broken down by the seven instruction-tuning datasets used for training, indicating the mean performance and LLM-based evaluation metrics. Green and red arrows highlight performance improvements or decreases compared to the baseline IT method.", "section": "4.2 Main Results"}, {"figure_path": "GcZgo9ffGt/tables/tables_22_1.jpg", "caption": "Table 1: Performance comparisons using 7 instruction tuning datasets with the LLAMA-2-7B on 6 categories of 18 traditional NLP tasks and 3 open-ended benchmarks with LLM as judgements. \u201cIT\u201d refers to INSTRUCTION TUNING. \"IM\" refers to INSTRUCTION MODELLING. Green and red arrows indicate performance changes against the baseline (IT).", "description": "This table presents a comparison of the performance of Instruction Tuning (IT) and Instruction Modelling (IM) on various NLP tasks and benchmarks using the LLAMA-2-7B language model.  Seven different instruction tuning datasets were used for training, and the results are categorized into six groups of traditional NLP tasks and three open-ended benchmarks. Green arrows indicate improvements achieved by IM over IT, while red arrows indicate where IT outperformed IM. The table allows for a comprehensive assessment of the relative effectiveness of IM versus IT across diverse tasks and datasets.", "section": "4.2 Main Results"}]