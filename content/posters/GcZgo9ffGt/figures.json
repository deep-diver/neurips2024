[{"figure_path": "GcZgo9ffGt/figures/figures_0_1.jpg", "caption": "Figure 1: Performance differences between INSTRUCTION TUNING (IT) and our proposed method INSTRUCTION MODELLING (IM) trained on 7 instruction tuning datasets. These datasets contain prompts and responses but do not contain preference pairs. Specifically, we use the Less datasets [68] and Alpagasus datasets [11], which are subsets of Flan V2 [14], Dolly [18], and Stanford Alpaca [61] to ensure good performance. We also report the results on the LIMA dataset. (Left) The mean performance across 18 traditional NLP tasks (see \u00a74.1 for details). (Right) The win rate on the AlpacaEval 1.0 benchmark [37]. Please refer to \u00a74.2 for details.", "description": "This figure compares the performance of Instruction Tuning (IT) and Instruction Modelling (IM) across various instruction tuning datasets.  The left panel shows the mean performance difference between IT and IM across 18 traditional NLP tasks. The right panel shows the win rate difference between IT and IM on the AlpacaEval 1.0 benchmark.  The figure highlights that IM generally outperforms IT, especially when trained on datasets with lengthy instructions and short outputs.", "section": "Experiments and Results"}, {"figure_path": "GcZgo9ffGt/figures/figures_1_1.jpg", "caption": "Figure 2: (Left) Performance improvement, achieved by our approach INSTRUCTION MODELLING (IM) compared to INSTRUCTION TUNING (IT) on the AlpacaEval 1.0, against the ratio between average instruction length and average output length in instruction tuning datasets (training size noted in parentheses). We highlight several representative instruction tuning datasets in yellow. Our analysis suggests that IM is especially beneficial for datasets characterized by lengthy instructions or prompts paired with comparably brief outputs, such as Code Alpaca [10] and Less MMLU Chat [68]. (Right) Performance improvement achieved by our approach IM over IT on the AlpacaEval 1.0 against the number of training examples in instruction tuning datasets. Here we maintain a fixed ratio between instruction and output length of 10. This analysis suggests that IM is particularly effective under the low-resource setting or Superficial Alignment Hypothesis. Please refer to \u00a74.2 for details.", "description": "This figure shows the performance improvement of INSTRUCTION MODELLING (IM) over INSTRUCTION TUNING (IT) on the AlpacaEval 1.0 benchmark. The left panel shows the relationship between performance improvement and the ratio of average instruction length to average output length in the training dataset.  The right panel shows the relationship between performance improvement and the number of training examples, while maintaining a fixed instruction-to-output length ratio of 10.  The results suggest that IM is particularly effective for datasets with lengthy instructions and short outputs, and in low-resource scenarios.", "section": "4.2 Main Results"}, {"figure_path": "GcZgo9ffGt/figures/figures_6_1.jpg", "caption": "Figure 3: (Left) Training loss distribution for each example between our approach INSTRUCTION MODELLING (IM) and INSTRUCTION TUNING (IT) on the LIMA dataset. (Right) Test loss distribution for each example between IM and IT on the Tulu V2 dataset, using a 10% randomly sampled data for efficacy. Mean losses are marked by dashed lines. For both IM and IT, here we only compute the loss over the output part. IM has a higher train loss with lower test loss, suggesting that IM effectively mitigates the overfitting issues compared to IT. See Appendix \u00a7D for more examples.", "description": "This figure compares the training and testing loss distributions of the INSTRUCTION MODELLING (IM) and INSTRUCTION TUNING (IT) methods.  The left panel shows the training loss distribution on the LIMA dataset, illustrating that IM has a slightly higher mean training loss than IT. The right panel presents the test loss distribution on a 10% subset of the Tulu V2 dataset, revealing that IM achieves a lower mean test loss than IT. This difference in train and test loss suggests that IM effectively mitigates overfitting during instruction tuning.", "section": "4.3 Instruction Modelling Mitigates Overfitting of Instruction Tuning"}, {"figure_path": "GcZgo9ffGt/figures/figures_6_2.jpg", "caption": "Figure 1: Performance differences between INSTRUCTION TUNING (IT) and our proposed method INSTRUCTION MODELLING (IM) trained on 7 instruction tuning datasets. These datasets contain prompts and responses but do not contain preference pairs. Specifically, we use the Less datasets [68] and Alpagasus datasets [11], which are subsets of Flan V2 [14], Dolly [18], and Stanford Alpaca [61] to ensure good performance. We also report the results on the LIMA dataset. (Left) The mean performance across 18 traditional NLP tasks (see \u00a74.1 for details). (Right) The win rate on the AlpacaEval 1.0 benchmark [37]. Please refer to \u00a74.2 for details.", "description": "This figure compares the performance of Instruction Tuning (IT) and Instruction Modeling (IM) across seven different instruction tuning datasets.  The left panel shows the average performance across 18 standard NLP tasks, while the right panel displays the win rate on the AlpacaEval 1.0 benchmark.  The datasets used are subsets of Flan V2, Dolly, and Stanford Alpaca, along with the LIMA dataset.  The figure highlights that IM outperforms IT in many scenarios.", "section": "4 Experiments and Results"}, {"figure_path": "GcZgo9ffGt/figures/figures_7_1.jpg", "caption": "Figure 1: Performance differences between INSTRUCTION TUNING (IT) and our proposed method INSTRUCTION MODELLING (IM) trained on 7 instruction tuning datasets. These datasets contain prompts and responses but do not contain preference pairs. Specifically, we use the Less datasets [68] and Alpagasus datasets [11], which are subsets of Flan V2 [14], Dolly [18], and Stanford Alpaca [61] to ensure good performance. We also report the results on the LIMA dataset. (Left) The mean performance across 18 traditional NLP tasks (see \u00a74.1 for details). (Right) The win rate on the AlpacaEval 1.0 benchmark [37]. Please refer to \u00a74.2 for details.", "description": "This figure compares the performance of Instruction Tuning (IT) and Instruction Modelling (IM) across various instruction tuning datasets.  The left panel shows the average performance improvement across 18 NLP tasks, while the right panel illustrates the win rate on the AlpacaEval 1.0 benchmark. The datasets used include subsets of Flan V2, Dolly, and Stanford Alpaca, along with the LIMA dataset.", "section": "4 Experiments and Results"}, {"figure_path": "GcZgo9ffGt/figures/figures_8_1.jpg", "caption": "Figure 2: (Left) Performance improvement, achieved by our approach INSTRUCTION MODELLING (IM) compared to INSTRUCTION TUNING (IT) on the AlpacaEval 1.0, against the ratio between average instruction length and average output length in instruction tuning datasets (training size noted in parentheses). We highlight several representative instruction tuning datasets in yellow. Our analysis suggests that IM is especially beneficial for datasets characterized by lengthy instructions or prompts paired with comparably brief outputs, such as Code Alpaca [10] and Less MMLU Chat [68]. (Right) Performance improvement achieved by our approach IM over IT on the AlpacaEval 1.0 against the number of training examples in instruction tuning datasets. Here we maintain a fixed ratio between instruction and output length of 10. This analysis suggests that IM is particularly effective under the low-resource setting or Superficial Alignment Hypothesis. Please refer to \u00a74.2 for details.", "description": "This figure shows the performance improvement of INSTRUCTION MODELLING (IM) over INSTRUCTION TUNING (IT) on the AlpacaEval 1.0 benchmark.  The left panel plots the improvement against the ratio of average instruction length to average output length in the training datasets, showing that IM is particularly beneficial for datasets with long instructions and short outputs. The right panel plots the improvement against the number of training examples, showing that IM is more effective in low-resource settings or under the Superficial Alignment Hypothesis.", "section": "4.2 Main Results"}, {"figure_path": "GcZgo9ffGt/figures/figures_17_1.jpg", "caption": "Figure 1: Performance differences between INSTRUCTION TUNING (IT) and our proposed method INSTRUCTION MODELLING (IM) trained on 7 instruction tuning datasets. These datasets contain prompts and responses but do not contain preference pairs. Specifically, we use the Less datasets [68] and Alpagasus datasets [11], which are subsets of Flan V2 [14], Dolly [18], and Stanford Alpaca [61] to ensure good performance. We also report the results on the LIMA dataset. (Left) The mean performance across 18 traditional NLP tasks (see \u00a74.1 for details). (Right) The win rate on the AlpacaEval 1.0 benchmark [37]. Please refer to \u00a74.2 for details.", "description": "This figure compares the performance of Instruction Tuning (IT) and Instruction Modeling (IM) across 18 NLP tasks and AlpacaEval 1.0.  Seven instruction tuning datasets (Less and Alpagasus subsets of Flan V2, Dolly, and Stanford Alpaca, plus LIMA) were used, with the left panel showing mean performance on traditional NLP tasks and the right showing the win rate on AlpacaEval 1.0.  IM shows improvements in many scenarios, particularly on AlpacaEval 1.0.", "section": "4 Experiments and Results"}, {"figure_path": "GcZgo9ffGt/figures/figures_20_1.jpg", "caption": "Figure 3: (Left) Training loss distribution for each example between our approach INSTRUCTION MODELLING (IM) and INSTRUCTION TUNING (IT) on the LIMA dataset. (Right) Test loss distribution for each example between IM and IT on the Tulu V2 dataset, using a 10% randomly sampled data for efficacy. Mean losses are marked by dashed lines. For both IM and IT, here we only compute the loss over the output part. IM has a higher train loss with lower test loss, suggesting that IM effectively mitigates the overfitting issues compared to IT. See Appendix \u00a7D for more examples.", "description": "This figure shows the training and testing loss distributions for both INSTRUCTION TUNING (IT) and INSTRUCTION MODELLING (IM).  The left panel displays the training loss distribution for the LIMA dataset, while the right panel shows the test loss distribution for a 10% sample of the Tulu V2 dataset.  The results demonstrate that IM, while exhibiting higher training loss, achieves lower testing loss, suggesting better generalization and reduced overfitting compared to IT.", "section": "4.3 Instruction Modelling Mitigates Overfitting of Instruction Tuning"}, {"figure_path": "GcZgo9ffGt/figures/figures_20_2.jpg", "caption": "Figure 3: (Left) Training loss distribution for each example between our approach INSTRUCTION MODELLING (IM) and INSTRUCTION TUNING (IT) on the LIMA dataset. (Right) Test loss distribution for each example between IM and IT on the Tulu V2 dataset, using a 10% randomly sampled data for efficacy. Mean losses are marked by dashed lines. For both IM and IT, here we only compute the loss over the output part. IM has a higher train loss with lower test loss, suggesting that IM effectively mitigates the overfitting issues compared to IT. See Appendix \u00a7D for more examples.", "description": "This figure shows the training and testing loss distributions for both INSTRUCTION TUNING (IT) and INSTRUCTION MODELLING (IM).  The left panel displays the training loss distribution on the LIMA dataset, illustrating that IM has a higher mean training loss (1.45) than IT (1.37). This suggests IM is less prone to overfitting during training. The right panel shows the test loss distribution on a 10% sample of the Tulu V2 dataset, revealing that IM achieves a lower mean test loss (1.17) compared to IT (1.32). This demonstrates that IM generalizes better to unseen data, further highlighting its effectiveness in mitigating overfitting.", "section": "4.3 Instruction Modelling Mitigates Overfitting of Instruction Tuning"}, {"figure_path": "GcZgo9ffGt/figures/figures_21_1.jpg", "caption": "Figure 1: Performance differences between INSTRUCTION TUNING (IT) and our proposed method INSTRUCTION MODELLING (IM) trained on 7 instruction tuning datasets. These datasets contain prompts and responses but do not contain preference pairs. Specifically, we use the Less datasets [68] and Alpagasus datasets [11], which are subsets of Flan V2 [14], Dolly [18], and Stanford Alpaca [61] to ensure good performance. We also report the results on the LIMA dataset. (Left) The mean performance across 18 traditional NLP tasks (see \u00a74.1 for details). (Right) The win rate on the AlpacaEval 1.0 benchmark [37]. Please refer to \u00a74.2 for details.", "description": "This figure compares the performance differences between Instruction Tuning (IT) and Instruction Modelling (IM) across 18 NLP tasks and AlpacaEval 1.0.  The left panel shows the average performance improvement of IM over IT across 18 traditional NLP tasks using 7 different instruction tuning datasets, while the right panel illustrates the win rate (the percentage of times IM outperforms IT) on the AlpacaEval 1.0 benchmark using the same datasets.  The datasets used are subsets of several popular instruction tuning datasets including Flan V2, Dolly, and Stanford Alpaca, chosen to ensure good performance.", "section": "4 Experiments and Results"}]