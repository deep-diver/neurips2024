[{"figure_path": "FjssnGuHih/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of our UniAR model. UniAR is a multimodal model that takes an image (could be a natural image, screenshot of a webpage, graphic design, or UI) along with a text prompt as input, and outputs heatmaps of human attention/interaction, scanpath or sequence of viewing/interaction, and subjective preference/likes. Example inputs and corresponding outputs for saliency, scanpath, and rating are shown on the left side, and the detailed model architecture is shown on the right side. To the best of our knowledge, a unified approach is still missing to modeling human visual behavior, ranging from implicit, early-perceptual behavior of what draws human attention, to explicit, later-stage decision-making on subjective preferences or likes.", "description": "The figure provides a high-level overview of the UniAR model, a multimodal model designed to predict human attention and responses to visual content. The left side shows example inputs (images, web pages, etc.) and their corresponding outputs (saliency heatmaps, scanpaths, and ratings).  The right side details the model architecture, illustrating how image and text inputs are processed through a transformer encoder to generate the various outputs. The caption highlights the model's unique ability to unify the prediction of both implicit (early-stage perceptual) and explicit (later-stage decision-making) human behavior, a significant advancement in the field.", "section": "3 Unifying Human Attention and Responses"}, {"figure_path": "FjssnGuHih/figures/figures_2_1.jpg", "caption": "Figure 2: Examples of UniAR's predictions across different tasks/domains. Images in green border are ground-truth, while images in orange border are UniAR's predictions. First row: attention/saliency heatmap prediction on natural images (Salicon) and webpages (WS-Saliency). Second row: importance heatmap on graphic designs (Imp1k), and saliency heatmap on Mobile UI. Third row: scanpath-sequence during free-viewing of webpages (WS-Scanpath) and object-searching within images (COCO-Search18). Fourth row: preference/rating prediction for natural images (Koniq-10k) and webpages (Web Aesthetics).", "description": "This figure showcases examples of UniAR's predictions across various tasks and data domains.  Each row demonstrates a different task: saliency heatmap prediction, importance heatmap prediction, scanpath prediction, and rating prediction.  Within each row, the left image shows the ground truth and the right image shows UniAR's prediction for the same input image. This highlights the model's ability to generalize across different types of visual content and prediction tasks.", "section": "Main contributions of this work are summarized below:"}, {"figure_path": "FjssnGuHih/figures/figures_15_1.jpg", "caption": "Figure 3: Another set of visualizations on UniAR's predictions. Images in green border are ground-truth, while images in orange border are UniAR's predictions. First row: saliency heatmap on Salicon and WS-Saliency. Second row: importance heatmap on Imp1k, and saliency heatmap on Mobile UI. Third row: free-viewing scanpath on WS-Scanpath and object-searching scanpath on COCO-Search18. Fourth row: rating prediction on Koniq-10k and Web Aesthetics datasets.", "description": "This figure shows more examples of UniAR's predictions across different tasks and datasets.  It visually compares the model's output (orange border) against ground truth (green border) for various visual content types. The rows represent different tasks: saliency heatmaps, importance heatmaps, scanpaths (sequences of eye movements), and rating predictions. Each column shows results for a specific dataset, indicating the model's performance across various scenarios.", "section": "Full Results and More Visualizations"}]