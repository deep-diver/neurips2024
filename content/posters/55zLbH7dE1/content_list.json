[{"type": "text", "text": "Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cong Xu\u2020 Jun Wang\u2020 Jianyong Wang \u00a7 Wei Zhang\u2020\u2217 ", "page_idx": 0}, {"type": "text", "text": "\u2020East China Normal University \u00a7Tsinghua University \u2020{congxueric, wongjun, zhangwei.thu2011}@gmail.com \u00a7jianyong@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Embedding plays a key role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision-making models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate module, SEvo is able to directly inject graph structural information into embedding with minimal computational overhead during training. The convergence properties of SEvo along with its potential variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. Particularly SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively utilize graph structural information beyond explicit GNN modules. Our code is available at https://github.com/MTandHJ/SEvo. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Surfing Internet leaves footprints such as clicks [15], browsing [5], and shopping histories [58]. For a modern recommender system [6, 12], the entities involved (e.g., goods, movies) are typically embedded into a latent space based on these interaction data. As the embedding takes the most to construct and is the foundation to subsequent decision-making models, its modeling quality directly determines the final performance of the entire system. According to the homophily assumption [31, 59], it is natural to expect that related entities have closer representations in the latent space. Note that the similarity between two entities refers specifically to those extracted from interaction data [47] or prior knowledge [37]. For example, goods selected consecutively by the same user or movies of the same genre are often perceived as more relevant. Graph neural networks (GNNs) [2, 10, 14] are a widely adopted technique to exploit such structural information, in concert with a weighted adjacency matrix wherein each entry characterizes how closely two nodes are related. Rather than directly injecting structural information into embedding, GNN typically serves as an intermediate module in the recommender system. However, designing a versatile GNN module suitable for various recommendation scenarios is challenging. This is especially true for sequential recommendation [8, 51], which needs to take into account both structural and sequential information at the same time. Moreover, the post-processing fashion inevitably increases the overhead of training and inference, limiting the scalability for real-time recommendation. ", "page_idx": 0}, {"type": "text", "text": "In this work, we aim to directly inject graph structural information into embedding through a novel embedding update mechanism. Figure 1 (a) illustrates a normal embedding evolution process, in which the embedding $\\mathbf{E}$ is updated at step $t$ as follows: ", "page_idx": 0}, {"type": "image", "img_path": "55zLbH7dE1/tmp/3084479211e5baaed97aff7d942a8d0540188accdab384c5ec0d538ef1419bc8.jpg", "img_caption": ["Figure 1: Overview of SEvo. (a) Normal embedding evolution. (b) (Section 2) Structure-aware embedding evolution. (c) (Section 2.2) Geometric visualization of the variation from $\\Delta\\mathbf{E}$ to $\\psi^{*}(\\Delta\\mathbf{E})$ . The gray ellipse represents the region with proper smoothness. (d) (Section 2.3) The $L$ -layer approximation with a faster convergence guarantee. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{E}_{t}\\leftarrow\\mathbf{E}_{t-1}+\\Delta\\mathbf{E}_{t-1}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Note that the variation $\\Delta\\mathbf{E}$ is primarily determined by the (anti-)gradient. It points to a region able to decrease a loss function concerning recommendation performance [35], but lacks an explicit mechanism to ensure that the variations between related nodes are similar. The embeddings thus cannot be expected to capture model pairwise relations while minimizing the recommendation loss. ", "page_idx": 1}, {"type": "text", "text": "Conversely, structural information can be effectively learned if related nodes evolve similarly at each update. The structure-aware embedding evolution (SEvo), depicted in Figure 1 (b), is developed for this goal. A special transformation is applied so as to meet both smoothness and convergence [57]. Given that these two criteria inherently conflict to some extent, we resort to a graph regularization framework [57, 7] to balance them. While analogous frameworks have been used to understand feature smoothing and modify message passing mechanisms for GNNs [28, 60], applying this to variations is not as straightforward as to features [24] or labels [19]. Previous efforts are capable of smoothing, but cannot account for strict convergence. The specific transformation form must be chosen carefully; subtle differences may slow down or even kill training. Through comprehensive theoretical analysis, we develop an applicable solution with a provable convergence guarantee. ", "page_idx": 1}, {"type": "text", "text": "Apart from the anti-gradient, the variation $\\Delta\\mathbf{E}$ can also be derived from the moment estimates. Therefore, existing optimizers, such as SGD [41] and Adam [23], can benefit from SEvo readily. In contrast to Adam, AdamW [27] decouples the weight decay from the optimization step, making it more compatible with SEvo as it is unnecessary to smooth the weight decay as well. Furthermore, we recorrect the moment estimates of SEvo-enhanced AdamW when encountering sparse gradients. This modification enhances its robustness across a variety of recommendation models and scenarios. Extensive experiments over six public datasets have demonstrated that it can effectively inject structural information to boost recommendation performance. It is important to note that SEvo does not alter the inference logic of the model, so the inference time is exactly the same and very little computational overhead is required during training. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this paper can be summarized as follows. 1) The graph regularization framework [57, 7] has been widely used for feature/label smoothing. To the best of our knowledge, we are the first to apply it to variations as an alternative to explicit GNN modules for recommendation. 2) The final formulation of SEvo is non-trivial (previous iterative [57] or Neumann series [39] approximation methods proved to be incompatible in this case) and comes from comprehensive theoretical analyses. 3) We further present SEvo-enhanced AdamW by integrating SEvo and recorrecting the original moment estimates. These modifications demonstrate consistent performance, yielding average $9\\%{\\sim}23\\%$ improvements across a spectrum of models. For larger-scale datasets containing millions of nodes, the performance gains can be as high as $40\\%{\\sim}139\\%$ . 4) Beyond interaction data, we preliminarily explore the pairwise similarity estimation based on other prior knowledge: node categories to promote intra-class representation proximity, and knowledge distillation [18] to encourage a light-weight student to mimic the embedding behaviors of a teacher model. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Structure-aware Embedding Evolution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first introduce some necessary terminology and concepts, in particular smoothness. SEvo and its theoretical analyses will be detailed in Section 2.2 and 2.3. The proofs hereafter are deferred to Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations and terminology. Let $\\mathcal{V}=\\{v_{1},\\ldots,v_{n}\\}$ denote a set of nodes and ${\\bf A}=[w_{i j}]\\in\\mathbb{R}^{n\\times n}$ a symmetric adjacency matrix, where each entry $w_{i j}=w_{j i}$ characterizes how closely $v_{i}$ and $v_{j}$ are related. They jointly constitute the graph $\\mathcal{G}=(\\mathcal{V},\\bar{\\mathbf{A}})$ . For example, $\\nu$ could be a set of movies, and $w_{i j}$ is the frequency of $v_{i}$ and $v_{j}$ being liked by the same user. Denoted by $\\mathbf{D}\\in\\mathbb{R}^{n\\times n}$ the diagonal degree matrix of A, the normalized adjacency matrix and the corresponding Laplacian matrix are defined as $\\tilde{\\mathbf{A}}=\\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}$ and $\\mathbf{\\tilde{L}}=\\mathbf{I}-\\mathbf{\\tilde{A}}$ , respectively. For ease of notation, we use $\\langle\\cdot,\\cdot\\rangle$ to denote the inner product, $\\langle\\mathbf{x},\\mathbf{y}\\rangle=\\mathbf{x}^{T}\\mathbf{y}$ for vectors and $\\langle{\\bf X},{\\bf Y}\\rangle=\\mathrm{Tr}({\\bf X}^{T}{\\bf Y})$ for matrices. Here, $\\operatorname{Tr}(\\cdot)$ denotes the trace of a given matrix. ", "page_idx": 2}, {"type": "text", "text": "Smoothness. Before delving into the details of SEvo, it is necessary to present a metric to measure the smoothness [57, 7] of node features $\\mathbf{X}$ as a whole. Denoted by $\\mathbf{x}_{i},\\mathbf{x}_{j}$ the row vectors of $\\mathbf{X}$ and $\\textstyle d_{i}=\\sum_{j}w_{i j}$ , we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}_{s m o o t h n e s s}(\\mathbf{X};\\mathcal{G}):=\\mathrm{Tr}(\\mathbf{X}^{T}\\tilde{\\mathbf{L}}\\mathbf{X})=\\sum_{i,j\\in\\mathcal{V}}w_{i j}\\biggl\\|\\frac{\\mathbf{x}_{i}}{\\sqrt{d_{i}}}-\\frac{\\mathbf{x}_{j}}{\\sqrt{d_{j}}}\\biggr\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This term is often used as graph regularization for feature/label smoothing [24, 19]. A lower $\\mathcal{I}_{s m o o t h n e s s}$ indicates smaller difference between closely related pairs of nodes, and in this case $\\mathbf{X}$ is considered smoother. However, smoothness alone is not sufficient from a performance perspective. Over-emphasizing this indicator instead leads to the well-known over-smoothing issue [26]. How to balance variation smoothness and convergence is the main challenge to be addressed below. ", "page_idx": 2}, {"type": "text", "text": "2.2 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The entities involved in a recommender system are typically embedded into a latent space [6, 12], and the embeddings in $\\mathbf{E}\\in\\mathbb{R}^{n\\times d}$ are expected to be smooth so that related nodes are close to each other. As discussed above, $\\mathbf{E}$ is learnable and updated at step $t$ by Eq. (1), where the variation $\\Delta\\mathbf{E}$ is mainly determined by the (anti-)gradient. For example, $\\Delta\\mathbf{E}=-\\eta\\nabla_{\\mathbf{E}}\\mathcal{L}$ when gradient descent with a learning rate of $\\eta$ is used to minimize a loss function $\\mathcal{L}$ . However, the final embeddings based on this evolution process may be far from sufficient smoothness because: 1) The variation $\\Delta\\mathbf{E}$ points to the region able to decrease the loss function concerning recommendation performance, but lacks an explicit smoothness guarantee. 2) As numerous item embeddings (millions of nodes in practice) to be trained together for a recommender system, the variations of two related nodes may be quite different due to the randomness from initialization and mini-batch sampling. ", "page_idx": 2}, {"type": "text", "text": "We are to design a special transformation $\\psi(\\cdot)$ to smooth the variation so that the evolution deduced from the following update formula is structure-aware, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{E}_{t}\\leftarrow\\mathbf{E}_{t-1}+\\psi(\\Delta\\mathbf{E}_{t-1}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Recall that, in this paper, the similarity is confined to quantifiable values in the adjacency matrix A, in which more related pairs are weighted higher. Therefore, this transformation should encourage pairs of nodes connected with higher weights to evolve more similarly than those connected with lower weights. This can be boiled down to structure-aware transformation as defined below. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Structure-aware transformation). The transformation $\\psi(\\cdot)$ is structure-aware if ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{T}_{s m o o t h n e s s}(\\psi(\\Delta\\mathbf{E}))\\leq\\mathcal{T}_{s m o o t h n e s s}(\\Delta\\mathbf{E}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "On the other hand, the transformation must ensure convergence throughout the evolution process, which means that the transformed variation should not differ too much from the original. For the sake of theoretical analysis, the ability to maintain the update direction will be used to qualitatively depict this desirable property below, though a quantitative squared error will be employed later. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Direction-aware transformation). The transformation $\\psi(\\cdot)$ is direction-aware if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\langle\\psi(\\Delta\\mathbf{E}),\\Delta\\mathbf{E}\\right\\rangle>0,\\quad\\forall\\,\\Delta\\mathbf{E}\\neq\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "These two criteria inherently conflict to some extent. We resort to a hyperparameter $\\beta\\in[0,1)$ to make a trade-off and the desired transformation is the corresponding minimum; that is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\psi^{*}(\\Delta\\mathbf{E};\\beta)=\\underset{\\Delta}{\\mathrm{argmin}}\\left(1-\\beta\\right)\\|\\Delta-\\Delta\\mathbf{E}\\|_{F}^{2}+\\beta\\,\\mathrm{Tr}(\\Delta^{T}\\tilde{\\mathbf{L}}\\Delta).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A larger $\\beta$ indicates a stronger smoothness constraint and $\\psi^{*}(\\Delta\\mathbf{E})$ reduces to $\\Delta\\mathbf{E}$ when $\\beta\\to0$ . Geometrically, as shown in Figure 1 (c), $\\psi^{*}(\\Delta\\mathbf{E})$ can be interpreted as a projection of $\\Delta\\mathbf{E}$ onto the region with proper smoothness. Taking the gradient to zero could give a closed-form solution, but it requires prohibitive arithmetic operations and memory overhead, which is particularly timeconsuming in recommendation due to the large number of nodes. Zhou et al. [57] suggested a $L$ -layer iterative approximation to circumvent this problem (with $\\hat{\\psi}_{0}(\\Delta\\mathbf{E})=\\Delta\\mathbf{E})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\psi}_{i t e r}(\\Delta\\mathbf{E}):=\\hat{\\psi}_{L}(\\Delta\\mathbf{E}),\\quad\\hat{\\psi}_{l}(\\Delta\\mathbf{E})=\\beta\\hat{\\mathbf{A}}\\hat{\\psi}_{l-1}(\\Delta\\mathbf{E})+(1-\\beta)\\Delta\\mathbf{E}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The resulting transformation is essentially a momentum update that aggregates higher-order information layer by layer. Analogous message-passing mechanisms have been used in previous GNNs such as APPNP [24] and C&S [19]. However, this commonly used approximate solution is incompatible with SEvo; sometimes, variations after the transformation may be opposite to the original direction, resulting in a failure to converge. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. The iterative approximation is direction-aware for all possible normalized adjacency matrices and $L\\ \\geq\\ 0,$ , if and only if $\\beta\\ <\\ 1/2$ . In contrast, the Neumann series approximation $\\begin{array}{r}{\\hat{\\psi}_{n s a}(\\Delta\\mathbf{E})=\\left(1-\\beta\\right)\\sum_{l=0}^{L}\\beta^{l}\\tilde{\\mathbf{A}}^{l}\\Delta\\mathbf{E}}\\end{array}$ is structure-aware and direction-aware for any $\\beta\\in[0,1)$ . ", "page_idx": 3}, {"type": "text", "text": "As suggested in Theorem 1, a feasible compromise for $\\hat{\\psi}_{i t e r}$ is to restrict $\\beta$ to $[0,1/2)$ , but this may cause a lack of smoothness. The Neumann series approximation [39] appears to be a viable alternative as it qualitatively satisfies both desirable properties. Nonetheless, this transformation can be further improved for a faster convergence rate based on the analysis presented next. ", "page_idx": 3}, {"type": "text", "text": "2.3 Convergence Analysis for Further Modification ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In general, the recommender system has some additional parameters $\\pmb\\theta\\in\\mathbb{R}^{m}$ except for embedding to be trained. Therefore, we analyze the convergence rate of the following gradient descent strategy: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}_{t+1}\\leftarrow\\mathbf{E}_{t}-\\eta\\hat{\\psi}_{n s a}\\big(\\nabla_{\\mathbf{E}}\\mathcal{L}(\\mathbf{E}_{t},\\pmb{\\theta}_{t})\\big),\\quad\\pmb{\\theta}_{t+1}\\leftarrow\\pmb{\\theta}_{t}-\\eta^{\\prime}\\nabla_{\\pmb{\\theta}}\\mathcal{L}(\\mathbf{E}_{t},\\pmb{\\theta}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "wherein SEvo is performed on the embedding and a normal gradient descent is applied to $\\pmb{\\theta}$ . To make the analysis feasible, some mild assumptions on the loss function should be given: $\\mathcal{L}\\,:\\,\\mathbb{R}^{n\\times d}\\,\\times\\,\\mathbb{R}^{m}\\,\\dot{\\,}\\rightarrow\\,\\mathbb{R}$ is a twice continuously differentiable function whose first derivative is Lipschitz continuous for some constant $C$ . Then, we obtain the following properties. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2 (Informal). If $\\eta\\,=\\,\\eta^{\\prime}\\,=\\,1/C$ , the convergence rate after $T$ updates is ${\\mathcal{O}}(C/(1-$ $\\beta)^{2}T)$ ). If we adopt a modified learning rate of $\\begin{array}{r}{\\eta\\,=\\,\\frac{1}{(1-\\beta^{L+1})C}}\\end{array}$ for embedding, the convergence rate could be improved to $\\mathcal{O}(C/((1-\\beta)T))$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 1. Our main interest is to justify the designs of SEvo rather than to pursue a particular convergence rate, so some mild assumptions suggested in [32] are adopted here. By introducing the steepest descent for quadratic norms $I I J,$ better convergence can be obtained with stronger assumptions. ", "page_idx": 3}, {"type": "text", "text": "Two conclusions can be drawn from Theorem 2. 1) The theoretical upper bound becomes worse when $\\beta\\rightarrow1$ . This makes sense since $\\hat{\\psi}_{n s a}(\\nabla_{\\mathbf{E}}\\mathcal{L})$ is getting smoother and further away from the original descent direction. 2) A modified learning rate for embedding can significantly improve the convergence rate. This phenomenon can be understood readily if we notice the fact that ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\psi}_{n s a}(\\Delta\\mathbf{E})\\|_{F}\\leq(1-\\beta^{L+1})\\|\\Delta\\mathbf{E}\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thus, the modified learning rate is indeed to offset the scaling effect induced by SEvo. In view of this, we directly incorporate this factor into SEvo to avoid getting stuck in the learning rate search, yielding the final desired transformation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\psi}(\\Delta\\mathbf{E};\\beta)=\\frac{1-\\beta}{1-\\beta^{L+1}}\\sum_{l=0}^{L}\\beta^{l}\\tilde{\\mathbf{A}}^{l}\\Delta\\mathbf{E}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It can be shown that $\\hat{\\psi}$ is structure-aware and direction-aware, and converges to $\\psi$ as $L$ increases. ", "page_idx": 4}, {"type": "text", "text": "2.4 Integrating SEvo into Existing Optimizers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Algorithm 1: SEvo-enhanced AdamW. Differences from the original AdamW are colored in blue. The matrix operation below are element-wise. ", "page_idx": 4}, {"type": "text", "text": "Input: embedding matrix $\\mathbf{E}$ , learning rate $\\eta$ , momentum factors $\\beta_{1},\\beta_{2},\\beta\\in[0,1)$ , weight decay $\\lambda$ . ", "page_idx": 4}, {"type": "text", "text": "foreach step $t$ do ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "$\\mathbf G_{t}\\gets\\nabla_{\\mathbf E}\\mathcal L$ ; ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "// Get gradients w.r.t E ", "page_idx": 4}, {"type": "text", "text": "Update first/second moment estimates for each node $i$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{M}_{t}[i]\\leftarrow\\left\\{\\begin{array}{l l}{\\beta_{1}\\mathbf{M}_{t-1}[i]+(1-\\beta_{1})\\mathbf{G}_{t}[i]}&{\\mathrm{if~}\\mathbf{G}_{t}[i]\\neq\\mathbf{0}}\\\\ {\\beta_{1}\\mathbf{M}_{t-1}[i]+\\frac{(1-\\beta_{1})}{1-\\beta_{1}^{t-1}}\\mathbf{M}_{t-1}[i]}&{\\mathrm{otherwise}}\\end{array}\\right.,}\\\\ &{\\mathbf{V}_{t}[i]\\leftarrow\\left\\{\\begin{array}{l l}{\\beta_{2}\\mathbf{V}_{t-1}[i]+(1-\\beta_{2})\\mathbf{G}_{t}^{2}[i]}&{\\mathrm{if~}\\mathbf{G}_{t}[i]\\neq\\mathbf{0}}\\\\ {\\beta_{2}\\mathbf{V}_{t-1}[i]+\\frac{(1-\\beta_{2})}{1-\\beta_{2}^{t-1}}\\mathbf{V}_{t-1}[i]}&{\\mathrm{otherwise}}\\end{array}\\right.;}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Compute bias-corrected first/second moment estimates: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{M}}_{t}\\leftarrow\\mathbf{M}_{t}/(1-\\beta_{1}^{t}),\\quad\\hat{\\mathbf{V}}_{t}\\leftarrow\\mathbf{V}_{t}/(1-\\beta_{2}^{t});\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Update via SEvo: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{E}_{t}\\gets\\mathbf{E}_{t-1}-\\eta\\,\\hat{\\psi}\\bigg(\\hat{\\mathbf{M}}_{t}/\\sqrt{\\hat{\\mathbf{V}}_{t}+\\boldsymbol{\\epsilon}};\\beta\\bigg)-\\eta\\lambda\\mathbf{E}_{t-1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Output: optimized embeddings $\\mathbf{E}$ . ", "page_idx": 4}, {"type": "text", "text": "SEvo can be seamlessly integrated into existing optimizers since the variation involved in Eq. (3) can be extended beyond the (anti-)gradient. For SGD with momentum, the variation becomes the first moment estimate, and for Adam this is jointly determined by the first/second moment estimates. AdamW is also widely adopted for training recommenders. Unlike Adam whose moment estimate is a mixture of gradient and weight decay, AdamW decouples the weight decay from the optimization step, which is preferable since it makes no sense to require the weight decay to be smooth as well. However, in very rare cases, SEvo-enhanced AdamW fails to work very well. We next try to ascertain the causes and then improve the robustness of SEvo-enhanced AdamW. ", "page_idx": 4}, {"type": "text", "text": "Denoted by $\\mathbf{g}:=\\nabla_{\\mathbf{e}}\\mathcal{L}\\in\\mathbb{R}^{d}$ the gradient for a node embedding $\\mathbf{e}$ and $\\mathbf{g}^{2}:=\\mathbf{g}\\odot\\mathbf{g}$ the element-wise square, AdamW estimates the first and second moments at step $t$ using the following formulas ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{m}_{t}=\\beta_{1}\\mathbf{m}_{t-1}+(1-\\beta_{1})\\mathbf{g}_{t-1},\\quad\\mathbf{v}_{t}=\\beta_{2}\\mathbf{v}_{t-1}+(1-\\beta_{2})\\mathbf{g}_{t-1}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta_{1},\\beta_{2}$ are two momentum factors. Then the original AdamW updates embeddings by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{e}_{t}=\\mathbf{e}_{t-1}-\\eta\\cdot\\Delta\\mathbf{e}_{t-1},\\quad\\Delta\\mathbf{e}_{t-1}:=\\hat{\\mathbf{m}}_{t}/\\sqrt{\\hat{\\mathbf{v}}_{t}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the bias-corrected estimates $\\hat{\\mathbf{m}}_{t}\\,=\\,\\mathbf{m}_{t}/(1\\,-\\,\\beta_{1}^{t})$ and $\\hat{\\mathbf{v}}_{t}\\,=\\,\\mathbf{v}_{t}/(1-\\beta_{2}^{t})$ are employed for numerical stability [23]. In practice, only a fraction of nodes are sampled for training in a minibatch, so the remaining embeddings have zero gradients. In this case, the sparse gradient problem may introduce some unexpected \u2018biases\u2019 as depicted below. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. If a node is no longer sampled in subsequent $p$ batches after step $t_{\\perp}$ , we have $\\begin{array}{r}{\\Delta\\mathbf{e}_{t+p-1}=\\boldsymbol{\\kappa}\\cdot\\frac{\\beta_{1}^{p}}{\\sqrt{\\beta_{2}^{p}}}\\Delta\\mathbf{e}_{t-1}}\\end{array}$ , and the coefficient of $\\kappa$ is mainly determined by $t$ . ", "page_idx": 5}, {"type": "text", "text": "Considering a common case $\\beta_{2}\\approx\\beta_{1}$ , the right-hand side approaches $\\mathcal{O}(\\beta_{1}^{p/2})$ . The step size for inactive nodes then gets slower and slower during idle periods. This seems reasonable as their moment estimates are becoming outdated; however, this effect sometimes prevents the variation from being smoothed by SEvo. We hypothesize that this is because SEvo itself tends to assign more energy to active nodes and less energy to inactive nodes. So this auto-attenuation effect of the original AdamW is somewhat redundant. Fortunately, there is a feasible modification to make SEvo-enhanced AdamW more robust: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Under the same assumptions as in Proposition 1, $\\Delta\\mathbf{e}_{t+p-1}=\\Delta\\mathbf{e}_{t-1}$ if the moment estimates are updated in the following manner when $\\mathbf{g}_{t}=\\mathbf{0}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{m}_{t}=\\beta_{1}\\mathbf{m}_{t-1}+(1-\\beta_{1})\\frac{1}{1-\\beta_{1}^{t-1}}\\mathbf{m}_{t-1},\\quad\\mathbf{v}_{t}=\\beta_{2}\\mathbf{v}_{t-1}+(1-\\beta_{2})\\frac{1}{1-\\beta_{2}^{t-1}}\\mathbf{v}_{t-1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As can be seen, when sparse gradients are encountered, the approach in Theorem 3 is actually to estimate the current gradient from previous moments. The coefficients $1/(1-\\beta_{1}^{t-1})$ and $1/(1-$ $\\beta_{1}^{t-1})$ are used here for unbiasedness (refer to Appendix A.3 for detailed discussion and proofs). We summarize the SEvo-enhanced AdamW in Algorithm 1 and the modifications for Adam and SGD in Appendix B.1, with an empirical comparison presented in Section 3.3. ", "page_idx": 5}, {"type": "text", "text": "The previous discussion lays the technical basis for injecting graph structural information, but the final recommendation performance is determined by how \u2018accurate\u2019 the similarity estimation is. Following other GNN-based sequence models [48, 49], the number of consecutive occurrences across all sequences will be used as the pairwise similarity $w_{i j}$ . In other words, items $v_{i}$ and $v_{j}$ that appear consecutively more frequently are assumed more related. Notably, we would like to emphasize that SEvo can readily inject other types of knowledge beyond interaction data. We have made some preliminary efforts in Appendix C and observed some promising results. ", "page_idx": 5}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we comprehensively verify the superiority of SEvo. We focus on sequential recommendation for two reasons: 1) This is the most common scenario in practice; 2) Utilizing both sequential and structural information is beneficial yet challenging. We showcase that SEvo is a promising way to achieve this goal. It is worth noting that although technically SEvo can be applied to general graph embedding learning [4], we contend SEvo-AdamW is especially useful for mitigating the inconsistent embedding evolution caused by data sparsity, while effectively injecting structural information in conjunction with other types of information. ", "page_idx": 5}, {"type": "text", "text": "Due to space constraints, this section presents only the primary results concerning accuracy, efficiency, and some empirical evidence that supports the aforementioned claims. We begin by introducing the datasets, evaluation metrics, baselines, and implementation details. ", "page_idx": 5}, {"type": "table", "img_path": "55zLbH7dE1/tmp/0b279906cf8a19d02facd6d4b5cc4821aeb7e71d53aff3b27dc7ed4436e6ebe8.jpg", "table_caption": ["Table 1: Dataset statistics "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Datasets. Six benchmark datasets are considered in this paper. The first four datasets including Beauty, Toys, Tools, and MovieLens-1M are commonly employed in previous studies for empirical comparisons. Additionally, two larger-scale datasets, Clothing and Electronics, are used to assess SEvo\u2019s scalability in scenarios involving millions of nodes. Following [22, 13], we filter out users and items with less than 5 interactions, and the validation set and test set are split in a leave-one-out fashion, namely the last interaction for testing and the penultimate one for validation. This splitting allows for fair comparisons, either for sequential recommendation or collaborative filtering. The dataset statistics are presented in Table 1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Evaluation metrics. For each user, the predicted scores over all items will be sorted in descending order to generate top-N candidate lists. We consider two widely-used evaluation metrics, $\\operatorname{HR}\\mathcal{O}\\mathrm{N}$ (Hit Rate) and $\\mathrm{NDCG}@\\mathrm{N}$ (Normalized Discounted Cumulative Gain). The former measures the rate of successful hits among the top-N recommended candidates, while the latter takes into account the ranking positions and assigns higher scores in order of priority. ", "page_idx": 5}, {"type": "table", "img_path": "55zLbH7dE1/tmp/ced47d85f2b75103bfbcf0e177730bb881b354f17a67debdd96420241341ac5f.jpg", "table_caption": ["Table 2: Overall performance comparison. The best baselines and ours are marked in underline and bold, respectively. Symbol $\\pmb{\\triangle}\\%$ stands for the relative gap between them. Paired t-test is performed over 5 independent runs for evaluating $p$ -value $\\zeta~0.05$ indicates statistical significance). \u2018Avg. Improv.\u2019 for each backbone depicts average relative improvements against the baseline. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Baselines. We select four GNN-based models (LightGCN [16], SR-GNN [48], LESSR [8], and MAERec [51]) as performance and efficiency benchmarks. Since this study is not to develop a new model, four classic sequence models (GRU4Rec [17], SASRec [22], BERT4Rec [40], and STOSA [13]) are utilized as backbones to validate the effectiveness of SEvo. Besides, MF-BPR [35] is also considered here as a backbone without sequence modeling. We carefully tune the hyperparameters according to their open-source code and experimental settings. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. Since the purpose is to study the effectiveness of SEvo, only hyperparameters concerning optimization are retuned, including learning rate ([1e-4, 5e-3]), weight decay ([0, 0.1]) and dropout rate ([0, 0.7]). Other hyperparameters in terms of the architecture are consistent with the corresponding baseline. For a fair comparison, the number of layers $L$ is fixed to 3 as in other GNN-based recommenders. As for the hyperparameter in terms of the degree of smoothness, we found $\\beta=0.99$ performs quite well in practice. The loss functions follow the suggestions in the respective papers, given that SEvo can be applied to any of them. ", "page_idx": 6}, {"type": "text", "text": "3.1 Overall Comparison ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we are to verify the effectiveness of SEvo in boosting recommendation performance. Table 2 compares the overall performance and efficiency over four widely used datasets, and Table 3 further provides the results on two large-scale datasets with millions of nodes. ", "page_idx": 6}, {"type": "text", "text": "Firstly, GNN-based models seem to over-emphasize structural information but lack full exploitation of sequential information. Their performance is only on par with GRU4Rec. On the Tools dataset, SR-GNN and LESSR are even inferior to LightGCN, a collaborative filtering model with no access to sequential information. MAERec makes a slightly better attempt to combine the two by learning structural information through graph-based reconstruction tasks. It employs a SASRec backbone for recommendation to allow for a better utilization of sequential information. Despite the identical recommendation backbone, SASRec trained with SEvo-enhanced AdamW enjoys significantly better performance. Not to mention the consistent gains on other state-of-the-art methods such as BERT4Rec and STOSA, from $2\\%$ to $30\\%$ according to the last two columns of Table 2. Overall, the promising improvements from the SEvo enhancement suggest a different route to exploit graph structural information, especially in conjunction with sequence modeling. ", "page_idx": 6}, {"type": "image", "img_path": "55zLbH7dE1/tmp/eec575c15d35b772b773d8adbc4024d498c4d109826a9e385b54d7148b6d9bca.jpg", "img_caption": ["Figure 2: Empirical illustrations of convergence and smoothness. The top and bottom panels respectively depict the results for Beauty and MovieLens-1M. (a) SASRec enhanced by SEvo with or without rescaling. (b) Smoothness of (I) the original variation; (II) the smoothed variation; (III) the optimized embedding. A lower $\\mathcal{I}_{s m o o t h n e s s}$ indicates stronger smoothness. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Secondly, either dynamic graph construction in SR-GNN and LESSR, or path sampling in MAERec, requires heavy computational overhead, which directly causes the training failures on large-scale datasets like Electronics and Clothing. Even worse, these high costs associated with SR-GNN and LESSR are inevitable during inference. In contrast, SEvo does not alter the model inference logic at all, thereby maintaining consistent inference time. The computational overhead required in training is also negligible compared to previous graph-enhanced models that employ GNNs as intermediate modules. For example, SASRec with SEvo consumes only 10 minutes compared to the hours of training time required for MAERec. When millions of nodes are encountered in Table 3, each epoch demands just a few more seconds. SEvo is arguably superior to these cumbersome GNN modules in real-world applications. Combining Table 2 and Table 3, it can be inferred that the performance gain increases as the number of items increases. This can be explained by the fact that the randomness of sampling leads to a much more inconsistent evolution when more and more nodes are encountered [56]. SEvo thus plays an increasingly important role as it is capable of imposing direct consistency constraints on embeddings. ", "page_idx": 7}, {"type": "table", "img_path": "55zLbH7dE1/tmp/20e8a5fede9b650d0fbb0ed17cbcc5c7f4aba2b640a834a4d5c4eea2aab06bc4.jpg", "table_caption": ["Table 3: SEvo on large-scale datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Since SASRec is a pioneer in the field of sequential recommendation, it will serve as the default backbone for subsequent studies. ", "page_idx": 7}, {"type": "text", "text": "3.2 Empirical Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Convergence comparison. In Section 2.3, we theoretically verified the necessity of rescaling the original Neumann series approximation for faster convergence. Figure 2a shows the loss curves of SASRec trained with AdamW under identical settings other than the form of SEvo. Without rescaling, SASRec exhibits significantly slower convergence, consistent with the conclusion in Theorem 2. While the theoretical worst-case convergence rate of the corrected SEvo is only $1\\%$ of the normal gradient descent when $\\beta=0.99$ , its practical performance is much better. SASRec trained with SEvo-enhanced AdamW initially converges marginally slower and catches up in the final stage. ", "page_idx": 7}, {"type": "image", "img_path": "55zLbH7dE1/tmp/29ec38fa33998c0a16da4b06caf9c37246b613cea586a9394eb33c6ad4bd4346.jpg", "img_caption": ["Figure 3: SEvo ablation experiments. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Smoothness evaluation. Figure 2b demonstrates the variation\u2019s smoothness throughout the evolution process and the eventual embedding differences from $\\beta\\,=\\,0$ to $\\beta\\,=\\,0.999$ . $(\\mathbf{I})\\rightarrow(\\mathbf{II})$ : The original variations exhibit a similar degree of smoothness, but after transformation, they are quite different\u2014\u2013smoother as $\\beta$ increases. $(\\mathbf{II})\\rightarrow(\\mathbf{III})$ : Consequently, the embedding trained with a stronger smoothness constraint becomes smoother as well. The structure-aware embedding evolution successfully makes related nodes closer in the latent space. Although smoothness is not the sole quality measure of embedding, combined with the analyses above, we can conclude that SEvo injects appropriate structural information under the default setting of $\\beta=0.99$ . ", "page_idx": 8}, {"type": "text", "text": "3.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "SEvo for various optimizers. It is of interest to study whether SEvo can be extended to other commonly used optimizers such as SGD and Adam. Figure 3a compares $\\mathrm{NDCG}@10$ performance on Beauty and MovieLens-1M. For a fair comparison, the hyperparameters are tuned independently. It is evident that the performance of SGD, Adam, and AdamW improves significantly after integrating SEvo, with AdamW achieving the best as it does not need to smooth the weight decay. ", "page_idx": 8}, {"type": "text", "text": "Neumann series approximation versus iterative approximation. Theorem 1 suggests that the Neumann series approximation is preferable to the commonly used iterative approximation because the latter is not always direction-aware and thus a conservative hyperparameter of $\\beta$ is needed to ensure convergence. This conclusion can also be drawn from Figure 3b. When only a little smoothness is required, their performance is comparable as both approximations differ only at the last term. The iterative approximation however fails to ensure convergence once $\\beta\\,>\\,0.7$ on the Beauty dataset, potentially resulting in a lack of smoothness. ", "page_idx": 8}, {"type": "text", "text": "Moment estimate correction for AdamW. We compare SEvo-enhanced AdamW with or without moment estimate correction in Figure 3c, in which average relative improvements against the baseline are presented for each recommender. Overall, the two variants of SEvo-enhanced AdamW perform comparably, significantly surpassing the baseline. However, in some cases (e.g., GRU4Rec and STOSA), the moment estimate correction as suggested in Theorem 3 is particularly useful to improve performance. Recall that BERT4Rec is trained using the output softmax from a separate fully-connected layer that is fully updated at each step. This may explain why the correction has little effect on BERTRec. In conclusion, the results underscore the importance of the proposed modification in alleviating bias in moment estimates. ", "page_idx": 8}, {"type": "text", "text": "3.4 Applications of SEvo Beyond Interaction Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further explore the potential of applying SEvo to other types of prior knowledge. On the one hand, the category smoothness constraint can also be fulfilled through SEvo (see Appendix C.2), leading to progressively stronger clustering effects as $\\beta$ increases. This provides compelling visual evidence of why SEvo is inherently structure-aware. On the other hand, SEvo is arguably an efficient tool for transferring embedding knowledge (see Appendix C.3). Notice that the learning of other modules cannot be guided in the same way, so SEvo alone is still inferior to state-of-the-art knowledge distillation methods [21, 55]. Fortunately, SEvo and other methods can work together to further boost the recommendation performance. ", "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Recommender systems are developed to enable users to quickly and accurately find relevant items in diverse applications, such as e-commerce [58], online news [15] and social media [5]. Typically, the entities involved are embedded into a latent space [6, 12, 54], and then decision models are built on top of the embedding for tasks like collaborative filtering [16] and context/knowledge-aware recommendation [46, 44]. Sequential recommendation [36, 25] focuses on capturing users\u2019 dynamic interests from their historical interaction sequences. Early approaches adapted recurrent neural networks (RNNs) [17] and convolutional filters [42] for sequence modeling. Recently, Transformer [45, 11] has become a popular architecture for sequence modeling due to its parallel efficiency and superior performance. SASRec [22] and BERT4Rec [40] use unidirectional and bidirectional selfattention, respectively. Fan et al. [13] proposed a novel stochastic self-attention (STOSA) to model the uncertainty of sequential behaviors. ", "page_idx": 9}, {"type": "text", "text": "Graph neural networks [2, 10] are a type of neural network designed to operate on graph-structured data, in concert with a weighted adjacency matrix to characterize the pairwise relations between nodes. GNN equipped with this adjacency matrix can be used for message passing between nodes. The most relevant work is the optimization framework proposed in [57] for solving semi-supervised learning problems via a smoothness constraint. This graph regularization approach has recently inspired a series of work [7, 28, 60]. As opposed to applying it to smooth node representations [24] or labels [19], it is employed here primarily to balance smoothness and convergence on the variation. ", "page_idx": 9}, {"type": "text", "text": "Structural information in recommendation is typically learned through GNN as well, with specific modifications made to cope with like data sparsity [52, 29]. LightGCN [16] is a pioneering collaborative filtering work on modeling user-item relations, which removes nonlinearities for easier training. To further utilize sequential information, previous efforts focus on equipping sequence models with complex GNN modules, but this inevitably increases the computational cost of training and inference, making it unappealing for practical recommendation. For example, SR-GNN [48] and LESSR [8] need dynamically construct adjacency matrices for each batch of sequences. Differently, MAERec [51] proposes an adaptive data augmentation to boost a novel graph masked autoencoder, which learns to sample less noisy paths from semantic similarity graph for subsequent reconstruction tasks. The resulting strong self-supervision signals help the model capture more useful information. ", "page_idx": 9}, {"type": "text", "text": "5 Broader Impact and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Utilizing both sequential and structural information is beneficial yet challenging, and SEvo proposed in this paper suggests a novel and effective technical route for this purpose. Compared to other explicit GNN modules, SEvo is light-weight and easy-to-use in practice. These insights may inspire future research efforts regarding structure-aware optimization. However, there are still some limitations. Firstly, the training-free nature of SEvo makes it versatile, but also limits the expressive power. For a specific task, a sophisticated GNN module may be more desirable for achieving higher recommendation accuracy. Secondly, it might not be so straightforward to apply SEvo to the scenario involving multiple types of prior knowledge. Some efforts [43, 33] in the field of multiple graph learning have proposed some technically feasible solutions. However, these approaches still encounter challenges in terms of efficiency, particularly in the context of recommendation systems. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have proposed a novel update mechanism for injecting graph structural information into embedding. Theoretical analyses of the convergence properties motivate some necessary modifications to the proposed method. SEvo can be seamlessly integrated into existing optimizers. For AdamW, we recorrect the moment estimates to make it more robust. Besides, an interesting direction for future work is extending SEvo to multiplex heterogeneous graphs [3], as real-world entities often participate in various relation networks. Furthermore, we believe that SEvo holds potential for application to dynamic graph structures and incremental updates [53]. Two challenges may be encountered in practice: the computational overhead associated with the ongoing adjacency matrix normalization, and how to adaptively weaken the outdated historical information. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[2] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In International Conference on Learning Representations (ICLR), 2014.   \n[3] Yukuo Cen, Xu Zou, Jianwei Zhang, Hongxia Yang, Jingren Zhou, and Jie Tang. Representation learning for attributed multiplex heterogeneous network. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), pages 1358\u20131368, 2019.   \n[4] Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christopher R\u00e9. Lowdimensional hyperbolic knowledge graph embeddings. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 6901\u20136914. ACL, 2020.   \n[5] Jiajia Chen, Xin Xin, Xianfeng Liang, Xiangnan He, and Jun Liu. Gdsrec: Graph-based decentralizedcollaborative filtering for socialrecommendation. IEEE Transactions on Knowledge and Data Engineering (TKDE), 2022.   \n[6] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-dimensional Sparse Data, pages 1\u20134, 2019.   \n[7] Siheng Chen, Aliaksei Sandryhaila, Jos\u00e9 MF Moura, and Jelena Kovacevic. Signal denoising on graphs via graph filtering. In IEEE Global Conference on Signal and Information Processing (GlobalSIP), pages 872\u2013876. IEEE, 2014.   \n[8] Tianwen Chen and Raymond Chi-Wing Wong. Handling information loss of graph neural networks for session-based recommendation. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), pages 1172\u20131180, 2020. [9] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. In Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103\u2013111. ACL, 2014.   \n[10] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems (NeurIPS), volume 29, 2016.   \n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 4171\u20134186. ACL, 2019.   \n[12] Ahmed El-Kishky, Thomas Markovich, Serim Park, Chetan Verma, Baekjin Kim, Ramy Eskander, Yury Malkov, Frank Portman, Sof\u00eda Samaniego, Ying Xiao, et al. Twhin: Embedding the twitter heterogeneous information network for personalized recommendation. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), pages 2842\u20132850, 2022.   \n[13] Ziwei Fan, Zhiwei Liu, Yu Wang, Alice Wang, Zahra Nazari, Lei Zheng, Hao Peng, and Philip S Yu. Sequential recommendation via stochastic self-attention. In ACM Web Conference (WWW), pages 2036\u20132047, 2022.   \n[14] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning (ICML), pages 1263\u20131272. PMLR, 2017.   \n[15] Shansan Gong and Kenny Q Zhu. Positive, negative and neutral: Modeling implicit feedback in session-based news recommendation. In International ACM Conference on Research and Development in Information Retrieval (SIGIR), 2022.   \n[16] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In International ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 639\u2013 648, 2020.   \n[17] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. In International Conference on Learning Representations (ICLR), 2016.   \n[18] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015.   \n[19] Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R. Benson. Combining label propagation and simple models out-performs graph neural networks. In International Conference on Learning Representations (ICLR), 2021.   \n[20] Tony Jebara, Jun Wang, and Shih-Fu Chang. Graph construction and b-matching for semisupervised learning. In International Conference on Machine Learning (ICML), pages 441\u2013 448, 2009.   \n[21] SeongKu Kang, Junyoung Hwang, Wonbin Kweon, and Hwanjo Yu. Topology distillation for recommender system. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 829\u2013839. ACM, 2021.   \n[22] Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In IEEE International Conference on Data Mining (ICDM), pages 197\u2013206. IEEE, 2018.   \n[23] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.   \n[24] Johannes Klicpera, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In International Conference on Learning Representations (ICLR), 2019.   \n[25] Jiacheng Li, Yujie Wang, and Julian McAuley. Time interval aware self-attention for sequential recommendation. In International Conference on Web Search and Data Mining (WSDM), pages 322\u2013330, 2020.   \n[26] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Conference on Artificial Intelligence (AAAI), 2018.   \n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2018.   \n[28] Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A unified view on graph neural networks as graph signal denoising. In ACM International Conference on Information and Knowledge Management (CIKM), pages 1202\u20131211, 2021.   \n[29] Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He. UltraGCN: Ultra simplification of graph convolutional networks for recommendation. In International Conference on Information and Knowledge Management (CIKM), 2021.   \n[30] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.   \n[31] Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social networks. Annual Review of Sociology, pages 415\u2013444, 2001.   \n[32] Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course. Lecture notes, 3(4):5, 1998.   \n[33] Feiping Nie, Jing Li, and Xuelong Li. Self-weighted multiview clustering with multiple graphs. In International Joint Conference on Artificial Intelligence (IJCAI), pages 2564\u20132570, 2017.   \n[34] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3967\u20133976. IEEE, 2019.   \n[35] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Schmidt-Thieme Lars. Bpr: Bayesian personalized ranking from implicit feedback. In Conference on Uncertainty in Artificial Intelligence (UAI), 2009.   \n[36] Guy Shani, David Heckerman, Ronen I Brafman, and Craig Boutilier. An mdp-based recommender system. Journal of Machine Learning Research (JMLR), 6(9), 2005.   \n[37] Kartik Sharma, Yeon-Chang Lee, Sivagami Nambi, Aditya Salian, Shlok Shah, Sang-Wook Kim, and Srijan Kumar. A survey of graph neural networks for social recommender systems. arXiv preprint arXiv:2212.04481, 2022.   \n[38] Daniel A Spielman. Spectral graph theory and its applications. In Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201907), pages 29\u201338. IEEE, 2007.   \n[39] Gilbert W Stewart. Matrix algorithms: volume 1: basic decompositions. SIAM, 1998.   \n[40] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In ACM International Conference on Information and Knowledge Management (CIKM), pages 1441\u20131450, 2019.   \n[41] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International Conference on Machine Learning (ICML), pages 1139\u20131147. PMLR, 2013.   \n[42] Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional sequence embedding. In ACM International Conference on Web Search and Data Mining (WSDM), pages 565\u2013573. ACM, 2018.   \n[43] Wei Tang, Zhengdong Lu, and Inderjit S Dhillon. Clustering with multiple graphs. In IEEE International Conference on Data Mining (ICDM), pages 1016\u20131021. IEEE, 2009.   \n[44] Zhen Tian, Ting Bai, Wayne Xin Zhao, Ji-Rong Wen, and Zhao Cao. Eulernet: Adaptive feature interaction learning via euler\u2019s formula for CTR prediction. In International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 1376\u20131385. ACM, 2023.   \n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017.   \n[46] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. Kgat: Knowledge graph attention network for recommendation. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), pages 950\u2013958, 2019.   \n[47] Yu Wang, Yuying Zhao, Yi Zhang, and Tyler Derr. Collaboration-aware graph convolutional network for recommender systems. In ACM Web Conference (WWW), pages 91\u2013101, 2023.   \n[48] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-based recommendation with graph neural networks. In Conference on Artificial Intelligence (AAAI), volume 33, pages 346\u2013353, 2019.   \n[49] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Lizhen Cui, and Xiangliang Zhang. Selfsupervised hypergraph convolutional networks for session-based recommendation. In Conference on Artificial Intelligence (AAAI), volume 35, pages 4503\u20134511, 2021.   \n[50] Cong Xu, Jun Wang, and Wei Zhang. Stablegcn: Decoupling and reconciling information propagation for collaborative filtering. IEEE Transactions on Knowledge and Data Engineering (TKDE), 2023.   \n[51] Yaowen Ye, Lianghao Xia, and Chao Huang. Graph masked autoencoder for sequential recommendation. In International ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 321\u2013330. ACM, 2023.   \n[52] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), pages 974\u2013983, 2018.   \n[53] Jiaxuan You, Tianyu Du, and Jure Leskovec. Roland: graph learning framework for dynamic graphs. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), pages 2358\u20132366, 2022.   \n[54] Shengyu Zhang, Fuli Feng, Kun Kuang, Wenqiao Zhang, Zhou Zhao, Hongxia Yang, TatSeng Chua, and Fei Wu. Personalized latent structure learning for recommendation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023.   \n[55] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distillation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11943\u201311952. IEEE, 2022.   \n[56] Wayne Xin Zhao, Zihan Lin, Zhichao Feng, Pengfei Wang, and Ji-Rong Wen. A revisiting study of appropriate offline evaluation for top-n recommendation algorithms. ACM Transactions on Information Systems (TOIS), 41(2):1\u201341, 2022.   \n[57] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Sch\u00f6lkopf. Learning with local and global consistency. Advances in Neural Information Processing Systems (NeurIPS), 16, 2003.   \n[58] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. Deep interest network for click-through rate prediction. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), pages 1059\u20131068, 2018.   \n[59] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. 33:7793\u2013 7804, 2020.   \n[60] Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph neural networks with an optimization framework. In ACM Web Conference (WWW), pages 1215\u20131226, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1 Introduction 1 ", "page_idx": 14}, {"type": "text", "text": "2 Structure-aware Embedding Evolution 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "2.1 Preliminaries 3   \n2.2 Methodology 3   \n2.3 Convergence Analysis for Further Modification 4   \n2.4 Integrating SEvo into Existing Optimizers 5   \n3 Experiments 6   \n3.1 Overall Comparison 7   \n3.2 Empirical Analysis 8   \n3.3 Ablation Study 9   \n3.4 Applications of SEvo Beyond Interaction Data 9 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "4 Related Work 10 ", "page_idx": 14}, {"type": "text", "text": "5 Broader Impact and Limitations 10 ", "page_idx": 14}, {"type": "text", "text": "6 Conclusion and Future Work 10 ", "page_idx": 14}, {"type": "text", "text": "A Proofs 16   \nA.1 Proof of Theorem 1 16   \nA.2 Proof of Theorem 2 19   \nA.3 Proofs of Proposition 1 and Theorem 3 21   \nA.4 Connection between LightGCN and SEvo-enhanced MF-BPR 23   \nB Detailed Settings 23   \nB.1 Algorithms 23   \nB.2 Datasets 23   \nB.3 Baselines 24 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "C Applications of SEvo beyond Interaction Data 25 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Pairwise Similarity Estimation Factors 25   \nC.2 SEvo for Intra-class Representation Proximity 26   \nC.3 SEvo for Knowledge Distillation 27   \nD Additional Experimental Results 28   \nD.1 SEvo for GNN-based models 28   \nD.2 Jsmoothness as a Regularization Term 29   \nD.3 $L$ -layer Approximation . . 29   \nD.4 Training and Inference Times 29 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this part, we are to prove the structure-aware/direction-aware properties of the $L$ -layer iterative approximation [57]: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\psi}_{i t e r}(\\Delta\\mathbf{E}):=\\hat{\\psi}_{L}(\\Delta\\mathbf{E})=\\underbrace{\\Bigl\\{(1-\\beta)\\sum_{l=0}^{L-1}\\beta^{l}\\tilde{\\mathbf{A}}^{l}+\\beta^{L}\\tilde{\\mathbf{A}}^{L}\\Bigr\\}}_{=:\\mathbf{P}^{\\prime}}\\Delta\\mathbf{E},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the $L$ -layer Neumann series approximation [39]: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\psi}_{n s a}(\\Delta\\mathbf{E})=\\underbrace{(1-\\beta)\\sum_{l=0}^{L}\\beta^{l}\\tilde{\\mathbf{A}}^{l}}_{=:\\mathbf{P}}\\Delta\\mathbf{E}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Fact 1. If L is odd, the geometric series $\\begin{array}{r}{S(x)=\\sum_{k=0}^{L}x^{l}}\\end{array}$ is monotonically increasing when $x\\le0$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. It is easy to show that ", "page_idx": 15}, {"type": "equation", "text": "$$\nS(x)=\\frac{1-x^{L+1}}{1-x},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the derivative w.r.t $x\\ne1$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\nS^{\\prime}(x)=\\frac{L x^{L+1}-(L+1)x^{L}+1}{(1-x)^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $L$ is odd, $S^{\\prime}(x)$ is positive when $x\\le0$ and in this case $S(x)$ is monotonically increasing. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. Given a normalized adjacency matrix $\\tilde{\\mathbf{A}}\\,\\in\\,\\mathbb{R}^{n\\times n}$ , let the symmetric matrix deduced from the Neumann series approximation be ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{P}=\\left(1-\\beta\\right)\\sum_{l=0}^{L}\\beta^{l}\\tilde{\\mathbf{A}}^{l}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Denoted by $\\lambda_{\\operatorname*{min}}(\\mathbf{P}),\\lambda_{\\operatorname*{max}}(\\mathbf{P})$ the smallest and largest eigenvalues of $\\mathbf{P}$ , respectively, then we have $\\forall\\beta\\in[0,1)$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L})\\leq\\lambda_{\\operatorname*{min}}(\\mathbf{P})\\leq\\lambda_{\\operatorname*{max}}(\\mathbf{P})=1-\\beta^{L+1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. It is easy to shown that the eigenvalues of $\\mathbf{P}$ are in the form of ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{i}=(1-\\beta)\\sum_{l=0}^{L}\\beta^{l}\\lambda_{i}^{l},\\;i=1,2,\\ldots,n,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\lambda_{1}\\,\\leq\\,\\lambda_{2}\\cdot\\cdot\\,\\leq\\,\\lambda_{n}$ denote the eigenvalues of $\\tilde{\\mathbf A}$ . Recall that these eigenvalues all fall into $[-1,1]$ and $\\lambda_{n}=1$ can be achieved exactly [38]. Hence, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{n}=(1-\\beta)\\sum_{l=0}^{L}\\beta^{l}=1-\\beta^{L+1}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "is the largest eigenvalue of $\\mathbf{P}$ . ", "page_idx": 15}, {"type": "text", "text": "In addition, notice that the last term $(1-\\beta)\\beta^{L}\\lambda^{L}$ is non-negative when $L$ is even. Then, we can get a lower bound no matter $L$ is odd or even: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}=(1-\\beta)\\sum_{l=0}^{L}\\beta^{l}\\lambda^{l}\\geq(1-\\beta)\\underbrace{\\sum_{l=0}^{2\\lfloor(L-1)/2\\rfloor+1}\\beta^{l}\\lambda^{l}}_{=:S(\\lambda)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The minimum of $S(\\lambda)$ must be achieved in $[-1,0]$ because $S(-\\lambda)\\le S(\\lambda)$ for any $\\lambda>0$ . In fact, in view of Fact 1, we know that $S(\\lambda)\\geq S({-}1)$ . Hence, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\lambda}\\ge(1-\\beta)S(-1)=(1-\\beta)\\sum_{l=0}^{2\\lfloor(L-1)/2\\rfloor+1}\\beta^{l}(-1)^{l}}\\\\ &{\\quad\\quad\\quad\\quad\\bigcup_{l=0}^{\\lfloor(L-1)/2\\rfloor}\\left(\\beta^{2l}-\\beta^{2l+1}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\bigcup_{l=0}^{\\lfloor(L-1)/2\\rfloor}\\left(\\beta^{2l}-\\beta^{2l+1}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\beta^{2}\\displaystyle\\sum_{l=0}^{\\lfloor(L-1)/2\\rfloor}\\beta^{2l}=(1-\\beta)^{2}\\displaystyle\\frac{(1-\\beta^{2\\lfloor(L-1)/2\\rfloor+2})}{1-\\beta^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\bigcup_{l=0}^{1-\\beta}(1-\\beta^{2\\lfloor(L-1)/2\\rfloor+2})\\ge\\displaystyle\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The last inequality holds because $2\\lfloor(L-1)/2\\rfloor+2\\geq L$ . Therefore, the smallest eigenvalue of $\\mathbf{P}$ must be greater than ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proposition 2. The Neumann series approximation is structure-aware and direction-aware for any $\\beta\\in[0,1)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. In view of Lemma 1, we know ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(\\mathbf{P})\\geq\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L})>0,\\quad\\forall\\beta\\in[0,1),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "so $\\mathbf{P}$ is positive definite and thus $\\hat{\\psi}_{n s a}(\\cdot)$ is direction-aware. Also, notice that $\\mathbf{P}$ has the same eigenvectors as $\\tilde{\\mathbf A}$ , and so does $\\tilde{\\bf L}$ . Hence, it is also structure-aware: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\hat{\\psi}_{n s a}(\\mathbf{x}),\\tilde{\\mathbf{L}}\\hat{\\psi}_{n s a}(\\mathbf{x})\\rangle=\\langle\\hat{\\psi}_{n s a}(\\mathbf{x}),\\tilde{\\mathbf{L}}\\hat{\\psi}_{n s a}(\\mathbf{x})\\rangle=\\langle\\mathbf{Px},\\tilde{\\mathbf{L}}\\mathbf{Px}\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\langle\\mathbf{x},\\mathbf{P}^{T}\\tilde{\\mathbf{L}}\\mathbf{Px}\\rangle\\leq\\langle\\mathbf{x},\\tilde{\\mathbf{L}}\\mathbf{x}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The last inequality follows from the fact $\\lambda_{\\operatorname*{max}}(\\mathbf{P})\\leq1$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 2. Given a normalized adjacency matrix $\\tilde{\\mathbf{A}}\\,\\in\\,\\mathbb{R}^{n\\times n}$ , let the symmetric matrix deduced from the iterative approximation be ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{P}^{\\prime}=(1-\\beta)\\sum_{l=0}^{L-1}\\beta^{l}\\tilde{\\mathbf{A}}^{l}+\\beta^{L}\\tilde{\\mathbf{A}}^{L}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have $\\lambda_{\\operatorname*{min}}(\\mathbf{P}^{\\prime})>0,\\,\\forall\\beta<1/2.$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. This conclusion is trivial for the case of $L\\leq1$ . Let us assume that $L\\geq2$ . Firstly, rewrite Eq. (17) as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{P}^{\\prime}=\\mathbf{P}+\\beta^{L}\\tilde{\\mathbf{A}}^{L},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{P}:=\\left(1-\\beta\\right)\\sum_{l=0}^{L-1}\\beta^{l}\\tilde{\\mathbf{A}}^{l}}\\end{array}$ . $\\mathbf{P}$ is positive definite in view of Lemma 1 and $\\beta^{L}\\tilde{\\mathbf{A}}^{L}$ is positive semidefinite when $L$ is even. Therefore, only the case of $L\\geq3$ needs to be proved. For a vector , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}^{T}\\mathbf{P}^{\\prime}\\mathbf{x}=\\mathbf{x}^{T}\\mathbf{P}\\mathbf{x}+\\beta^{L}\\mathbf{x}^{T}\\hat{\\mathbf{A}}^{L}\\mathbf{x}\\geq\\lambda_{\\operatorname*{min}}(\\mathbf{P})\\|\\mathbf{x}\\|_{2}^{2}+\\beta^{L}\\lambda_{\\operatorname*{min}}(\\tilde{\\mathbf{A}})^{L}\\|\\mathbf{x}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\geq\\Big(\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L-1})+\\beta^{L}\\lambda_{\\operatorname*{min}}(\\tilde{\\mathbf{A}})^{L}\\Big)\\|\\mathbf{x}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\geq\\Big(\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L-1})-\\beta^{L}\\Big)\\|\\mathbf{x}\\|_{2}^{2}=\\frac{1-\\beta-\\beta^{L-1}-\\beta^{L+1}}{1+\\beta}\\|\\mathbf{x}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\geq\\frac{1-\\beta-\\beta^{2}-\\beta^{4}}{1+\\beta}\\|\\mathbf{x}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first two inequalities follow from Lemma 1. The last inequality holds by noting the fact that, for $L\\geq3$ and $\\beta<1/2$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\beta+\\beta^{L-1}+\\beta^{L+1}\\le\\beta+\\beta^{2}+\\beta^{4}<13/16.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda_{\\mathrm{min}}(\\mathbf{P}^{\\prime})=\\operatorname*{min}_{\\mathbf{x}}\\frac{\\mathbf{x}^{T}\\mathbf{P}^{\\prime}\\mathbf{x}}{\\|\\mathbf{x}\\|^{2}}\\geq\\frac{1-\\beta-\\beta^{2}-\\beta^{4}}{1+\\beta}>0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition 3. The iterative approximation is direction-aware for all possible normalized adjacency matrices and $L\\geq0,$ , if and only if $\\dot{\\beta}<1/2$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. If $\\beta<1/2$ , we have $\\lambda_{\\operatorname*{min}}(\\mathbf{P}^{\\prime})>0$ according to Lemma 2, and thus $\\hat{\\psi}$ is direction-aware. Conversely, if $\\beta\\geq1/2$ , we can construct an adjacency matrix $\\tilde{\\mathbf A}$ such that $\\lambda_{\\operatorname*{min}}(\\mathbf{P}^{\\prime})\\leq0$ for some $L$ . Let us assume that $L=1$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{A}}:=\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{0}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In this case, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{P}^{\\prime}=(1-\\beta)\\mathbf{I}+\\beta\\tilde{\\mathbf{A}}=\\left[\\begin{array}{c c}{1-\\beta}&{\\beta}\\\\ {\\beta}&{1-\\beta}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "whose eigenvalues are 1 and $1-2\\beta$ . The latter is non-positive for any $\\beta\\geq1/2$ . ", "page_idx": 17}, {"type": "text", "text": "Remark 2. The construction of $\\tilde{\\mathbf A}$ in Eq. (22) is not unique. In fact, any bipartite graph can be used as a counterexample. ", "page_idx": 17}, {"type": "text", "text": "Corollary 1 (The proof of Theorem 1). The iterative approximation is direction-aware for all possible normalized adjacency matrices and $L\\geq0$ , if and only if $\\beta<1/2$ . In contrast, the Neumann series approximation ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\psi}_{n s a}(\\Delta\\mathbf{E})=(1-\\beta)\\sum_{l=0}^{L}\\beta^{l}\\tilde{\\mathbf{A}}^{l}\\Delta\\mathbf{E},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is structure-aware and direction-aware for any $\\beta\\in[0,1)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. This is a corollary of Proposition 2 and Proposition 3. ", "page_idx": 17}, {"type": "text", "text": "Proposition 4. The rescaled Neumann series approximation $\\hat{\\psi}$ is structure-aware and directionaware, and converges to the optimal solution as $L$ increases. ", "page_idx": 17}, {"type": "text", "text": "Proof. The convergence to the optimal solution is obvious by noting that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{lim}_{L\\to+\\infty}\\hat{\\psi}(\\Delta\\mathbf{E})}&{=\\operatorname*{lim}_{L\\to+\\infty}\\frac{1}{1-\\beta^{L+1}}\\cdot\\operatorname*{lim}_{L\\to+\\infty}\\hat{\\psi}_{n s a}(\\Delta\\mathbf{E})}\\\\ &{=1\\cdot\\psi^{*}(\\Delta\\mathbf{E})=\\psi^{*}(\\Delta\\mathbf{E}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similar to Proposition 2, it can be proved that $\\begin{array}{r}{\\frac{1}{1-\\beta^{L+1}}\\mathbf{P}}\\end{array}$ is positive definite with a largest eigenvalue $\\leq1$ . Therefore, the rescaled transformation is also structure-aware and direction-aware. ", "page_idx": 17}, {"type": "text", "text": "A.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Before delving into the proof of the convergence, we would like to claim that the lemmas below are well known and can be found in most textbooks [32, 1] on convex optimization. For the sake of completeness, we provide here these proofs. Hereinafter, we use $||\\mathbf{X}||_{2}$ to denote the spectral norm which returns the largest singular value of the matrix $\\mathbf{X}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 3. For a twice continuously differentiable function $f\\,:\\,\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ with $\\|\\nabla^{2}f(\\mathbf{x})\\|_{2}\\ \\leq$ $C$ , $\\forall\\mathbf{x}\\in\\mathbf{dom}(f)$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\mathbf{y})\\leq f(\\mathbf{x})+\\langle\\nabla f(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle+\\frac{C}{2}\\|\\mathbf{y}-\\mathbf{x}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. For a Taylor expansion of $f(\\mathbf{x})$ , there exists a $\\mathbf{z}=\\tau\\mathbf{x}+(1-\\tau)\\mathbf{y}$ for some $\\tau\\in[0,1]$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal f}({\\bf y})=f({\\bf x})+\\langle\\nabla f({\\bf x}),({\\bf y}-{\\bf x})\\rangle+\\frac{({\\bf y}-{\\bf x})^{T}\\nabla^{2}f({\\bf z})({\\bf y}-{\\bf x})}{2}}\\ ~}\\\\ {{\\displaystyle~~~~~~\\leq f({\\bf x})+\\langle\\nabla f({\\bf x})^{T}({\\bf y}-{\\bf x})\\rangle+\\frac{C}{2}\\|{\\bf y}-{\\bf x}\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma 4. For a positive definite matrix $\\mathbf{P}$ , let $\\|\\mathbf{x}\\|_{\\mathbf{P}}:=(\\mathbf{x}^{T}\\mathbf{Px})^{1/2}$ be the quadratic norm induced from P. If the eigenvalues of $\\mathbf{P}$ fall into $[a,b],$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{Px}\\|_{2}^{2}\\leq b\\|\\mathbf{x}\\|_{\\mathbf{P}}^{2},\\quad a\\|\\mathbf{x}\\|_{2}^{2}\\leq\\|\\mathbf{x}\\|_{\\mathbf{P}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Firstly, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{P}\\mathbf{x}\\|_{2}=\\|\\mathbf{P}^{1/2}\\mathbf{P}^{1/2}\\mathbf{x}\\|_{2}\\le\\|\\mathbf{P}^{1/2}\\|_{2}\\|\\mathbf{x}\\|_{\\mathbf{P}}\\le\\sqrt{b}\\|\\mathbf{x}\\|_{\\mathbf{P}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Secondly, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}\\|_{2}=\\|\\mathbf{P}^{-1/2}\\mathbf{P}^{1/2}\\mathbf{x}\\|_{2}\\leq\\|\\mathbf{P}^{-1/2}\\|_{2}\\|\\mathbf{x}\\|_{\\mathbf{P}}\\leq{\\frac{1}{\\sqrt{a}}}\\|\\mathbf{x}\\|_{\\mathbf{P}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Theorem 4. Let $f:\\mathbb{R}^{n}\\times\\mathbb{R}^{m}\\rightarrow\\mathbb{R}$ be a twice continuously differentiable function bounded below, and its Hessian matrix satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\nabla^{2}f(\\mathbf{x},\\mathbf{y})\\|_{2}\\leq C,\\,\\forall(\\mathbf{x},\\mathbf{y})\\in\\mathbf{dom}(f)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some constant $C$ . The following gradient descent scheme is used to train $\\mathbf x,\\mathbf y$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t+1}\\leftarrow\\mathbf{x}_{t}-\\eta\\mathbf{P}\\nabla_{\\mathbf{x}}f(\\mathbf{x}_{t},\\mathbf{y}_{t}),\\quad\\mathbf{y}_{t+1}\\leftarrow\\mathbf{y}_{t}-\\eta^{\\prime}\\nabla_{\\mathbf{y}}f(\\mathbf{x}_{t},\\mathbf{y}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{P}$ is deduced from the $L$ -layer Neumann series approximation. Then, after $T$ updates, we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\le T}\\|\\nabla f(\\mathbf x,\\mathbf y)\\|_{2}^{2}\\le\\frac{2C}{\\gamma(T+1)}\\epsilon,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\epsilon=f(\\mathbf{x},\\mathbf{y})-f(\\mathbf{x}^{*},\\mathbf{y}^{*})$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma=\\left\\{\\begin{array}{l l}{\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L})(1+\\beta^{L+1}),}&{i f\\eta=\\eta^{\\prime}=\\frac{1}{C}}\\\\ {\\frac{1-\\beta}{1+\\beta}\\frac{1-\\beta^{L}}{1-\\beta^{L+1}},}&{o t h e r w i s e}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The update formula (29) can be unified into ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{t+1}:=\\left[\\begin{array}{l}{\\mathbf{x}_{t+1}}\\\\ {\\mathbf{y}_{t+1}}\\end{array}\\right]=\\left[\\begin{array}{l}{\\mathbf{x}_{t}}\\\\ {\\mathbf{y}_{t}}\\end{array}\\right]-\\underbrace{\\left[\\begin{array}{c c}{\\eta\\mathbf{P}}&{0}\\\\ {0}&{\\eta^{\\prime}\\mathbf{I}}\\end{array}\\right]}_{=:\\tilde{\\mathbf{P}}}\\nabla f(\\mathbf{x}_{t},\\mathbf{y}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is easy to show that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a:=\\operatorname*{min}(\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L})\\eta,\\eta^{\\prime})\\leq\\lambda_{\\operatorname*{min}}(\\tilde{\\mathbf{P}})}\\\\ &{\\leq\\lambda_{\\operatorname*{max}}(\\tilde{\\mathbf{P}})=\\operatorname*{max}((1-\\beta^{L+1})\\eta,\\eta^{\\prime})=:b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Specifically, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a=\\left\\{\\begin{array}{l l}{\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L})\\frac{1}{C},}&{\\mathrm{~if~}\\eta=\\eta^{\\prime}=\\frac{1}{C}}\\\\ {\\frac{1-\\beta}{1+\\beta}\\frac{1-\\beta^{L}}{1-\\beta^{L+1}}\\frac{1}{C},}&{\\mathrm{~otherwise~}}\\end{array}\\right.,}\\\\ &{b=\\left\\{\\begin{array}{l l}{(1-\\beta^{L+1})\\frac{1}{C},}&{\\mathrm{~if~}\\eta=\\eta^{\\prime}=\\frac{1}{C}}\\\\ {\\frac{1}{C},}&{\\mathrm{~otherwise~}}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In view of Lemma 3 and Lemma 4, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{z}_{t+1})\\leq f(\\mathbf{z}_{t})-\\langle\\nabla f(\\mathbf{z}_{t}),\\tilde{\\mathbf{P}}\\nabla f(\\mathbf{z}_{t})\\rangle+\\displaystyle\\frac{C}{2}\\|\\tilde{\\mathbf{P}}\\nabla f(\\mathbf{z}_{t})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=f(\\mathbf{z}_{t})-\\|\\nabla f(\\mathbf{z}_{t})\\|_{\\tilde{\\mathbf{P}}}^{2}+\\displaystyle\\frac{C}{2}\\|\\tilde{\\mathbf{P}}\\nabla f(\\mathbf{z}_{t})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\leq f(\\mathbf{z}_{t})-\\|\\nabla f(\\mathbf{z}_{t})\\|_{\\tilde{\\mathbf{P}}}^{2}+\\displaystyle\\frac{b C}{2}\\|\\nabla f(\\mathbf{z}_{t})\\|_{\\tilde{\\mathbf{P}}}^{2}}\\\\ &{\\quad\\quad\\quad=f(\\mathbf{z}_{t})-(1-\\frac{b C}{2})\\|\\nabla f(\\mathbf{z}_{t})\\|_{\\tilde{\\mathbf{P}}}^{2}}\\\\ &{\\quad\\quad\\quad\\leq f(\\mathbf{z}_{t})-a(1-\\frac{b C}{2})\\|\\nabla f(\\mathbf{z}_{t})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Denoted by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma}&{:=a C(2-b C)}\\\\ &{=\\left\\{\\begin{array}{l l}{\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L})(1+\\beta^{L+1}),}&{\\mathrm{~if~}\\eta=\\eta^{\\prime}=\\frac{1}{C}}\\\\ {\\frac{1-\\beta}{1+\\beta}\\frac{1-\\beta^{L}}{1-\\beta^{L+1}},}&{\\mathrm{~otherwise}}\\end{array}\\right.,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{0\\leq t\\leq T}\\|\\nabla f(\\mathbf{x}_{t},\\mathbf{y}_{t})\\|_{2}^{2}\\leq\\displaystyle\\frac{1}{T+1}\\displaystyle\\sum_{t=0}^{T}\\|\\nabla f(\\mathbf{x}_{t},\\mathbf{y}_{t})\\|_{2}^{2}}\\\\ &{\\le\\displaystyle\\frac{1}{T+1}\\displaystyle\\sum_{t=0}^{T}\\frac{2C}{\\gamma}(f(\\mathbf{x}_{t},\\mathbf{y}_{t})-f(\\mathbf{x}_{t+1},\\mathbf{y}_{t+1}))}\\\\ &{=\\displaystyle\\frac{2C}{\\gamma(T+1)}(f(\\mathbf{x}_{0},\\mathbf{y}_{0})-f(\\mathbf{x}_{T+1},\\mathbf{y}_{T+1}))}\\\\ &{\\le\\displaystyle\\frac{2C}{\\gamma(T+1)}\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem 5 (The proof of Theorem 2). If $\\eta=\\eta^{\\prime}=1/C$ , after $T$ updates, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\leq T}\\|\\nabla\\mathcal{L}(\\mathbf{E}_{t},\\pmb{\\theta}_{t})\\|_{2}^{2}=\\mathcal{O}\\big(C/((1-\\beta)^{2}T)\\big).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If we adopt a modified learning rate for embedding: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{(1-\\beta^{L+1})C},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "the convergence rate could be improved to $\\mathcal{O}\\big(C/((1-\\beta)T)\\big)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. This is true for $L=0$ , since in this case the update mechanism becomes a normal gradient descent regardless of $\\eta=1/C$ or $\\begin{array}{r}{\\eta=\\frac{1}{(1-\\beta)C}}\\end{array}$ . Let us prove a general case for $L\\geq1$ next. ", "page_idx": 19}, {"type": "text", "text": "According to Theorem 4, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\leq T}\\|\\nabla\\mathcal{L}(\\mathbf{E}_{t},\\theta_{t})\\|_{2}^{2}\\leq\\frac{2C}{\\gamma(T+1)}\\epsilon,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\epsilon=\\mathcal{L}(\\mathbf{E},\\pmb{\\theta})-\\mathcal{L}(\\mathbf{E}^{*},\\pmb{\\theta}^{*})$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma=\\left\\{\\begin{array}{l l}{\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L})(1+\\beta^{L+1}),}&{\\mathrm{if}\\,\\,\\eta=\\eta^{\\prime}=\\frac{1}{C}}\\\\ {\\frac{1-\\beta}{1+\\beta}\\frac{1-\\beta^{L}}{1-\\beta^{L+1}},}&{\\mathrm{otherwise}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Notice that, for $\\eta=\\eta^{\\prime}=1/C$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{lim}_{\\beta\\to1}\\frac{1/\\gamma}{1/(1-\\beta)^{2}}}&{=\\operatorname*{lim}_{\\beta\\to1}\\frac{\\frac{1-\\beta}{\\frac{1-\\beta}{1+\\beta}(1-\\beta^{L})(1+\\beta^{L+1})}}{\\frac{1}{(1-\\beta)^{2}}}}\\\\ &{=\\operatorname*{lim}_{\\beta\\to1}\\frac{1-\\beta}{1-\\beta^{L}}=\\frac{1}{L},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and for $\\begin{array}{r}{\\eta=\\frac{1}{(1-\\beta^{L+1})C},\\eta^{\\prime}=\\frac{1}{C}}\\end{array}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{lim}_{\\beta\\to1}\\frac{1/\\gamma}{1/(1-\\beta)}}&{=\\operatorname*{lim}_{\\beta\\to1}\\frac{\\frac{1}{\\frac{1-\\beta}{1+\\beta}\\frac{(1-\\beta^{L})}{(1-\\beta^{L+1})}}}{\\frac{1}{1-\\beta}}}\\\\ &{=2\\cdot\\operatorname*{lim}_{\\beta\\to1}\\frac{1-\\beta^{L+1}}{1-\\beta^{L}}=\\frac{2(L+1)}{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{\\gamma}=\\left\\{\\begin{array}{l l}{\\mathcal{O}(1/(1-\\beta)^{2}),}&{\\mathrm{~if~}\\eta=\\eta^{\\prime}=\\frac{1}{C}}\\\\ {\\mathcal{O}(1/(1-\\beta)),}&{\\mathrm{~if~}\\eta=\\frac{1}{(1-\\beta^{K+1})C},\\eta^{\\prime}=\\frac{1}{C}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The remainder of the proof is straightforward. ", "page_idx": 20}, {"type": "text", "text": "A.3 Proofs of Proposition $\\mathbf{1}$ and Theorem 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Adam(W) [23] uses the bias-corrected moment estimates for updating because they are unbiased when the actual moments are stationary throughout the training. Below, Lemma 5 formally elaborates on this, and Theorem 6 extends Theorem 3 with a proof of unbiasedness. ", "page_idx": 20}, {"type": "text", "text": "Lemma 5 ([23]). Denoted by $\\hat{\\mathbf{m}}_{t}=\\mathbf{m}_{t}/(1\\!-\\!\\beta_{1}^{t})$ and $\\hat{\\mathbf{v}}_{t}=\\mathbf{v}_{t}/(1\\!-\\!\\beta_{2}^{t})$ the bias-corrected estimates, if the first and second moments are stationary, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{g}_{t}]=\\mathbb{E}[\\mathbf{g}],\\quad\\mathbb{E}[\\mathbf{g}_{t}^{2}]=\\mathbb{E}[\\mathbf{g}^{2}],\\quad\\forall t=1,2,\\ldots,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then these bias-corrected estimates are unbiased: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\hat{\\mathbf{m}}_{t}]=\\mathbb{E}[\\mathbf{g}],\\quad\\mathbb{E}[\\hat{\\mathbf{v}}_{t}]=\\mathbb{E}[\\mathbf{g}^{2}],\\quad\\forall t=1,2,\\ldots.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proposition 5. If a node is no longer sampled in subsequent $p$ batches after step $t$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Delta\\mathbf{e}_{t+p-1}=\\kappa\\cdot\\frac{\\beta_{1}^{p}}{\\sqrt{\\beta_{2}^{p}}}\\Delta\\mathbf{e}_{t-1},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the coefficient of $\\kappa$ is mainly determined by $t$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. In this case, the iterative formula becomes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{m}_{t+j}=\\beta_{1}^{j}\\mathbf{m}_{t}+\\mathbf{0},\\quad\\mathbf{v}_{t+j}=\\beta_{2}^{j}\\mathbf{v}_{t}+\\mathbf{0},\\quad\\forall j=0,1,\\ldots,p.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta\\mathbf{e}_{t+p-1}=\\frac{\\hat{\\mathbf{m}}_{t+p}}{\\sqrt{\\hat{\\mathbf{v}}_{t+p}}}=\\frac{\\sqrt{1-\\beta_{2}^{t+p}}}{1-\\beta_{1}^{t+p}}\\frac{\\mathbf{m}_{t+p}}{\\sqrt{\\mathbf{v}_{t+p}}}}&{}\\\\ {=\\frac{\\sqrt{1-\\beta_{2}^{t+p}}}{1-\\beta_{1}^{t+p}}\\frac{\\beta_{1}^{t}}{\\sqrt{\\beta_{2}^{t}}}\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}}=\\frac{\\beta_{1}^{t}\\sqrt{1-\\beta_{2}^{t+p}}}{\\sqrt{\\beta_{2}^{t}}(1-\\beta_{1}^{t+p})}\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}}}&{}\\\\ {=\\frac{\\beta_{1}^{t}(1-\\beta_{1}^{t})}{\\sqrt{\\beta_{2}^{t}}(1-\\beta_{1}^{t+p})\\sqrt{1-\\beta_{2}^{t+p}}}\\frac{\\hat{\\mathbf{m}}_{t}}{\\sqrt{\\mathbf{v}_{t}}}}&{}\\\\ {=\\frac{\\beta_{1}^{t}}{\\sqrt{\\beta_{2}^{t}}}\\frac{\\left(1-\\beta_{1}^{t+p}\\right)\\sqrt{1-\\beta_{2}^{t+p}}}{\\left(1-\\beta_{1}^{t+p}\\right)\\sqrt{1-\\beta_{2}^{t+p}}}\\frac{\\Delta\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}}}&{}\\\\ {=\\frac{\\beta_{1}^{t}}{\\sqrt{\\beta_{2}^{t}}}\\frac{\\left(1-\\beta_{1}^{t+p}\\right)\\sqrt{1-\\beta_{2}^{t+p}}}{\\left(1-\\beta_{1}^{t+p}\\right)\\sqrt{1-\\beta_{2}^{t}}}\\Delta\\mathbf{e}_{t-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It is easy to show that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow+\\infty}\\kappa(t,p)=1,\\quad\\forall\\beta_{1},\\beta_{2}\\in[0,1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Theorem 6 (The proof of Theorem 3). Under the same assumptions as in Lemma $^{5}$ and Proposition $^{\\,l}$ , the bias-corrected estimates are unbiased and $\\Delta\\mathbf{e}_{t+p-1}\\,=\\,\\Delta\\mathbf{e}_{t-1}$ if the estimates are updated in the following manner when $\\mathbf{g}_{t}=\\mathbf{0}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{m}_{t}=\\beta_{1}\\mathbf{m}_{t-1}+(1-\\beta_{1})\\frac{1}{1-\\beta_{1}^{t-1}}\\mathbf{m}_{t-1},\\quad\\mathbf{v}_{t}=\\beta_{2}\\mathbf{v}_{t-1}+(1-\\beta_{2})\\frac{1}{1-\\beta_{2}^{t-1}}\\mathbf{v}_{t-1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We first show the unbiasedness of ${\\bf m}_{t}$ and the proof for $\\mathbf{v}_{t}$ is completely analogous. It remains only to show that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{m}_{t}]=(1-\\beta_{1}^{t})\\mathbb{E}[\\mathbf{g}].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This is trivial for $t\\,=\\,0$ . Assuming that this also holds in the case of $t-1$ , it can be proved by induction. ", "page_idx": 21}, {"type": "text", "text": "If $\\mathbf g_{t}\\neq\\mathbf0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbf{m}_{t}]=\\beta_{1}\\mathbb{E}[\\mathbf{m}_{t-1}]+(1-\\beta_{1})\\mathbb{E}[\\mathbf{g}_{t}]}\\\\ &{\\quad\\quad\\quad=\\beta_{1}(1-\\beta_{1}^{t-1})\\mathbb{E}[\\mathbf{g}]+(1-\\beta_{1})\\mathbb{E}[\\mathbf{g}]}\\\\ &{\\quad\\quad\\quad=(1-\\beta_{1}^{t})\\mathbb{E}[\\mathbf{g}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If $\\mathbf{g}_{t}=\\mathbf{0}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbf{m}_{t}]=\\beta_{1}\\mathbb{E}[\\mathbf{m}_{t-1}]+(1-\\beta_{1})\\frac{1}{1-\\beta_{1}^{t-1}}\\mathbb{E}[\\mathbf{m}_{t-1}]}\\\\ &{\\quad\\quad=\\frac{1-\\beta_{1}^{t}}{1-\\beta_{1}^{t-1}}\\mathbb{E}[\\mathbf{m}_{t-1}]=(1-\\beta_{1}^{t})\\mathbb{E}[\\mathbf{g}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If the node is no longer sampled in subsequent $p$ batches after step $t$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf m}_{t+j}=\\beta_{1}{\\bf m}_{t+j-1}+(1-\\beta_{1})\\frac{1}{1-\\beta_{1}^{t+j-1}}{\\bf m}_{t+j-1}}\\ ~}\\\\ {{\\displaystyle~~~~~=\\frac{1-\\beta_{1}^{t+j}}{1-\\beta_{1}^{t+j-1}}{\\bf m}_{t+j-1}=\\frac{1-\\beta_{1}^{t+j}}{1-\\beta_{1}^{t+j-1}}\\frac{1-\\beta_{1}^{t+j-1}}{1-\\beta_{1}^{t+j-2}}{\\bf m}_{t+j-2}}}\\\\ {{\\displaystyle~~~~~=\\dots=\\frac{1-\\beta_{1}^{t+j}}{1-\\beta_{1}^{t}}{\\bf m}_{t},~~~\\forall j=1,2,\\dots,p.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Analogously, we have, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t+j}=\\frac{1-\\beta_{2}^{t+j}}{1-\\beta_{2}^{t}}\\mathbf{v}_{t},\\quad\\forall j=1,2,\\ldots,p.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the conclusion can be deduced from ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{m}}_{t+p}=\\frac{1}{1-\\beta_{1}^{t+j}}\\mathbf{m}_{t+p}=\\frac{1}{1-\\beta_{1}^{t}}\\mathbf{m}_{t}=\\hat{\\mathbf{m}}_{t},}\\\\ {\\hat{\\mathbf{v}}_{t+p}=\\frac{1}{1-\\beta_{2}^{t+j}}\\mathbf{v}_{t+p}=\\frac{1}{1-\\beta_{2}^{t}}\\mathbf{v}_{t}=\\hat{\\mathbf{v}}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "A.4 Connection between LightGCN and SEvo-enhanced MF-BPR ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For a $L$ -layer LightGCN, it can be formulated as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{F}=\\psi(\\mathbf{E}):=\\sum_{l=0}^{L}\\alpha_{l}\\tilde{\\mathbf{A}}^{l}\\mathbf{E},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\alpha_{l},l\\,=\\,0,\\dots,L$ represent the layer weights. According to the linear nature of the gradient operator, it can be obtained that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\bf E}\\mathcal{L}=\\psi(\\nabla_{\\bf F}\\mathcal{L}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, denoted by $\\zeta(\\cdot)$ the gradient processing procedure of an optimizer, we can establish that LightGCN is identical to the following system: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{F}(t)=\\psi(\\mathbf{E}(t))}\\\\ {=\\psi(\\mathbf{E}(t-1)-\\eta\\Delta\\mathbf{E}(t-1))}\\\\ {=\\psi(\\mathbf{E}(t-1))-\\eta\\psi(\\Delta\\mathbf{E}(t-1))}\\\\ {=\\mathbf{F}(t-1)-\\eta\\psi\\circ\\zeta\\circ\\psi(\\nabla_{\\mathbf{F}}\\mathcal{L}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When $\\zeta(\\cdot)$ is an identity mapping (i.e., standard gradient descent), LightGCN is equivalent to MFBPR with SEvo being applied twice at each update. However, when $\\zeta(\\cdot)$ is not an identity mapping (e.g., an optimizer with momentum or weight decay is integrated), they cannot be unified into a single system. Compared to explicit GNNs, SEvo is easy-to-use and has minimal impact on the forward pass, making it more suitable for assisting recommenders in simultaneously utilizing multiple types of information. These connections in part justify why SEvo can inject structural information directly. ", "page_idx": 22}, {"type": "text", "text": "B Detailed Settings ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "B.1 Algorithms ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We present the algorithms of SEvo-enhanced Adam and SGD in 2 and Algorithm 3, respectively. ", "page_idx": 22}, {"type": "text", "text": "B.2 Datasets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this study, we perform experiments on six public datasets. Specifically, the Beauty, Toys, and Tools datasets are extracted from Amazon reviews published in $2\\bar{0}14^{2}$ , while Electronics and Clothing are sourced from Amazon reviews published in $2018^{3}$ . Additionally, the MovieLens-1M dataset is made available by GroupLens4. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 2: Adam enhanced by SEvo. Differences from the original Adam are colored in blue. The matrix operation below are element-wise. ", "page_idx": 23}, {"type": "text", "text": "Input: embedding matrix $\\mathbf{E}$ , learning rate $\\eta$ , momentum factors $\\beta_{1},\\beta_{2},\\beta\\in[0,1)$ , weight decay $\\lambda$ .   \nforeach step $t$ do $\\mathbf{G}_{t}\\gets\\nabla_{\\mathbf{E}}\\mathcal{L}+\\lambda\\mathbf{E}_{t-1}$ ; // Get gradients Update first/second moment estimates: $\\begin{array}{r}{\\mathbf{M}_{t}\\leftarrow\\beta_{1}\\mathbf{M}_{t-1}+(1-\\beta_{1})\\mathbf{G}_{t},}\\\\ {\\mathbf{V}_{t}\\leftarrow\\beta_{1}\\mathbf{V}_{t-1}+(1-\\beta_{2})\\mathbf{G}_{t}^{2};}\\end{array}$ Compute bias-corrected first/second moment estimates: $\\begin{array}{r}{\\hat{\\mathbf{M}}_{t}\\leftarrow\\mathbf{M}_{t}/(1-\\beta_{1}^{t}),}\\\\ {\\hat{\\mathbf{V}}_{t}\\leftarrow\\mathbf{V}_{t}/(1-\\beta_{2}^{t});}\\end{array}$ Update via SEvo: ${\\bf E}_{t}\\leftarrow{\\bf E}_{t-1}-\\eta\\,\\hat{\\psi}\\bigg(\\hat{\\bf M}_{t}/\\sqrt{\\hat{\\bf V}_{t}+\\epsilon};\\beta\\bigg).$ $\\mathbf{E}$ ", "page_idx": 23}, {"type": "text", "text": "Output: optimized embeddings . ", "page_idx": 23}, {"type": "text", "text": "Algorithm 3: SGD with momentum enhanced by SEvo. Differences from the original SGD are colored in blue. The matrix operation below are element-wise. ", "page_idx": 23}, {"type": "image", "img_path": "55zLbH7dE1/tmp/9a63dd812415b151e8c18eb74f4750abf3551a78c68d507904ce252d751f59d6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Output: optimized embeddings $\\mathbf{E}$ . ", "page_idx": 23}, {"type": "text", "text": "B.3 Baselines ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Four GNN-based baselines for performance and efficiency benchmarks: ", "page_idx": 23}, {"type": "text", "text": "\u2022 LightGCN [16] is a pioneering collaborative filtering model that simplifies graph convolutional networks (GCNs) by removing nonlinearities for easier training. It uses only graph structural information and has no access to sequential information. ", "page_idx": 23}, {"type": "text", "text": "\u2022 SR-GNN [48] and LESSR [8] are two baselines dynamically constructing session graph. The former employs a gated graph neural network to obtain the final node vectors, while the latter utilizes edge-order preserving multigraph and a shortcut graph to address the lossy session encoding and ineffective long-range dependency capturing problems, respectively. ", "page_idx": 23}, {"type": "text", "text": "\u2022 MAERec [51] learns to sample less noisy paths from a semantic similarity graph for subsequent reconstruction tasks. However, we found that the official implementation treats the recommendation loss and reconstruction loss equally, leading to poor performance here. Therefore, an additional weight is attached to the reconstruction loss and a grid search is performed in the range of [0, 1]. Almost all hyperparameters are tuned for competitive performance. ", "page_idx": 23}, {"type": "text", "text": "Four sequence backbones to validate the effectiveness of SEvo: ", "page_idx": 23}, {"type": "text", "text": "\u2022 GRU4Rec [17] applies RNN [9] to recommendation with specific modifications made to cope with data sparsity. In addition to the learning rate in {1e-4, 5e-4, 1e-3, 5e-3} and weight decay in [0, 0.1], we also tune the dropout rate for node features in the range of [0, 0.7]. ", "page_idx": 24}, {"type": "text", "text": "\u2022 SASRec [22] and BERT4Rec [40] are two pioneering works on sequential recommendation equipped with unidirectional and bidirectional self-attention, respectively. For BERT4Rec which employs a separate fully-connected layer for scoring, the weight matrix therein will also be smoothed by SEvo. In addition to some basic hyperparameters, the mask ratio is also researched for BERT4Rec. ", "page_idx": 24}, {"type": "text", "text": "\u2022 STOSA [13] is one of the state-of-the-art models. It aims to capture the uncertainty of sequential behaviors by modeling each item as a Gaussian distribution. The hyperparameters involved are tuned similarly to SASRec. ", "page_idx": 24}, {"type": "text", "text": "Four knowledge distillation methods used in Appendix C.3: ", "page_idx": 24}, {"type": "text", "text": "\u2022 KD [18] and DKD [55] are two logits-based approaches to transfer knowledge. DKD decomposes the classical KD loss into target class knowledge distillation loss and non-target class knowledge distillation loss. ", "page_idx": 24}, {"type": "text", "text": "\u2022 RKD [34] and HTD [21] are two ranking-based approaches. The former focuses the distillation of relational knowledge through distance-wise and angle-wise alignments, while the latter emphasizes the distillation of hierarchical topology by dividing nodes into multiple groups and requiring intra-group and inter-group alignments. ", "page_idx": 24}, {"type": "text", "text": "C Applications of SEvo beyond Interaction Data ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here, we preliminarily explore the exploitation of more types of knowledge besides consecutive occurrences. We first investigate some elementary factors for interaction data, and then introduce the applications of SEvo to node categories and knowledge distillation. ", "page_idx": 24}, {"type": "text", "text": "C.1 Pairwise Similarity Estimation Factors ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "55zLbH7dE1/tmp/7ac35cfb2401d45d30fcc2d612bb2bb7fca2469ede48f77fbc66f34859845dff.jpg", "img_caption": ["Figure 4: Illustrations of different pairwise similarity estimation methods based on interaction data. (a) The default is to adopt the co-occurrence frequency within the last $K$ items. (b) Using only the first $K$ items. (c) Allowing a maximum walk length of $H$ beyond 1. (d) Frequency-based similarity versus distance-based similarity. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Recent GNN-based sequence models [48, 49], as well as the SEvo-enhanced models reported in Section 3, estimate the pairwise similarity $w_{i j}$ between items $v_{i}$ and $v_{j}$ based on their co-occurrence frequency in sequences. In other words, items that appear consecutively more frequently are assumed more related. Yet there are some factors that deserve a closer look: (1) The maximum sequence length $K$ for construction to investigate the number of interactions required for accurate estimation; (2) Using only the first $K$ versus last $K$ interactions in each sequence to compare the utility of early and recent preferences; (3) Allowing related items to be connected by a walk of length $\\leq H$ rather than strict consecutive occurrences; (4) Frequency-based similarity versus distance-based similarity. The former weights all related pairs equally, while the latter weights inversely to their walk length. For example, given a sequence $v_{2}\\,\\rightarrow\\,v_{1}\\,\\rightarrow\\,v_{3}$ with a maximum walk length of $H\\,=\\,2$ , ", "page_idx": 24}, {"type": "image", "img_path": "55zLbH7dE1/tmp/f293b71b58f3c073983ba71d29835b65991838fdaf0aed3d83143fe41a08d207.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 5: Comparison of similarity estimation across four potential factors. $\\Xi^{,}$ indicates the default way applied to SEvo-enhanced sequence models in Section 3.1. (a) Using only the first/last $K$ items for pairwise similarity estimation. (b) Frequency- and distance-based similarity with a maximum walk length of $H$ . ", "page_idx": 25}, {"type": "text", "text": "the frequency-based similarity of $(v_{2},v_{3})$ gives 1, while the distance-based similarity is $1/2$ (as the walk length from $v_{2}$ to $v_{3}$ is 2). ", "page_idx": 25}, {"type": "text", "text": "Figure 4 illustrates these four variants and Algorithm 4 details a step-by-step process. We further compare these four potential factors in Figure 5: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Figure 5a shows the effect of confining the maximum sequence length to the first/last $K$ items, so only the early/recent preferences will be considered. In constrast to early interactions, recent ones imply more precise pairwise relations for future prediction, even for small $K$ . With the increase of the maximum sequence length, the recommendation performance on Beauty improves steadily, but not the case for MovieLens-1M. This suggests that shopping relations may be more consistent than movie preferences.   \n\u2022 Figure 5b explores the relations beyond strict consecutive occurrences; that is, two items are considered related once they co-occur within a path of length $\\leq H$ . For the shopping and movie datasets, estimating similarity beyond co-occurrence frequency appears less reliable overall. We also compare frequency-based similarity with distance-based similarity that decreases weights for more distant pairs. It is clear that the distance-based approach performs more stably as the maximum walk length $H$ increases. ", "page_idx": 25}, {"type": "text", "text": "C.2 SEvo for Intra-class Representation Proximity ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Sometimes embeddings are expected to be smooth w.r.t. a prior knowledge. For example, in addition to the interaction data, each movie in the MovieLens-1M dataset is associated with at least one genre. ", "page_idx": 25}, {"type": "image", "img_path": "55zLbH7dE1/tmp/f16dda740a9c1a6bd274edfa8118f24037ef1a86728ab7f267b50905de2095d4.jpg", "img_caption": ["Figure 6: UMAP [30] visualization of movies based on their embeddings. For ease of differentiation, we group the 18 genres into 6 categories and colored them individually: Thriller/Crime/Action/Adventure; Horror/Mystery/Film-Noir; War/Drama/Romance; Comedy/Musical/Children\u2019s/Animation; Fantasy/Sci-Fi; Western/Documentary. "], "img_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "55zLbH7dE1/tmp/b2c058012e2c6bf3d08d0b88f902497ab9558e783ba8b19da6b0352de4ece0e1.jpg", "table_caption": ["Table 4: Pairwise similarity estimation based on interaction data versus node categories (movie genres). "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "It is natural to assume that movies of the same genre are related to each other. Heuristically, we can define the similarity $w_{i j}$ to be 1 if $v_{i}$ and $v_{j}$ belong to the same genre and 0 otherwise. ", "page_idx": 26}, {"type": "text", "text": "As can be seen in Figure 6, such smoothness constraint can also be fulfilled through SEvo, leading to progressively stronger clustering effects as $\\beta$ increases. However, the resulting performance gains are slightly less than those based on interaction data (see Table 4). One possible reason is that the movie genres are too coarse to provide particularly useful information. In conclusion, while smoothness is an appealing inductive bias, its utility depends on how well the imposed structural information agrees with the performance metrics of interest. ", "page_idx": 26}, {"type": "text", "text": "C.3 SEvo for Knowledge Distillation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In addition to the affinity matrix extracted from interaction data or relation data, the pairwise similarity can also be estimated from a heavy-weight teacher model. Recall that Knowledge Distillation (KD) [18] encourages a light-weight student model to mimic the behaviors (e.g., output distribution) of the teacher model, so the learned student model achieves both accuracy and efficiency. In general, higher-dimensional embeddings are capable of better fitting the underlying distribution between entities. The pairwise similarities extracted from a teacher model, needless to say, can be used to guide the embedding evolution of a student model. Unlike interaction or relation data, the deduced graph is dense if only the distance function is applied. Therefore, some graph construction steps including sparsification and reweighting should be involved as well. We attempt to use the widely used KNN graph here, and leave a more comprehensive study of graph construction [20] as a future work. ", "page_idx": 26}, {"type": "text", "text": "Table 5: Knowedge distillation from Teacher (SASRec with a embedding size of 200) to Student (SASRec with a embedding size of 20). The results are averaged over 5 independent runs on the Beauty dataset. 10-nearest neighbors (i.e., $K=10)$ ) are selected for each node. ", "page_idx": 27}, {"type": "table", "img_path": "55zLbH7dE1/tmp/359f332adfc37e70d69d03e17e1af6658f60a48a317f1fe77cf992b9aff4573a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Specifically, the distance between each pair $v_{i},v_{j}$ is estimated using a cosine similarity distance function: ", "page_idx": 27}, {"type": "equation", "text": "$$\nd_{i j}=2-2{\\frac{\\mathbf{e}_{i}^{T}\\mathbf{e}_{j}}{\\|\\mathbf{e}_{i}\\|_{2}\\|\\mathbf{e}_{j}\\|_{2}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, $K$ -nearest neighbors are selected for each node; that is ", "page_idx": 27}, {"type": "equation", "text": "$$\n(i,j)\\in\\mathcal{E},\\:i\\neq j\\mathrm{~iff~}|\\{k\\neq i:d_{i k}\\leq d_{i j}\\}|\\leq K.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This sparsification is neccessary for several reasons: 1) SEvo with a dense adjacency matrix is computationally prohibitive to conduct; 2) Generally, only the top-ranked neighbors are reliable for next distillation. Finally, the adjacency matrix is obtained through reweighting and symmetrizing: ", "page_idx": 27}, {"type": "equation", "text": "$$\nw_{i j}=\\hat{w}_{i j}+\\hat{w}_{j i},\\quad\\hat{w}_{i j}:=\\exp(-d_{i j}/\\tau),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\tau>0$ is the kernel bandwidth parameter. ", "page_idx": 27}, {"type": "text", "text": "Table 5 reports the results of the SASRec backbone with different embedding sizes (200 versus 20). Although a student equipped with SEvo can only derive guidance from the teacher in terms of embedding modeling, it has surpassed RKD and HTD that focus on feature/output alignments. Recall that SEvo only needs to access the teacher model once for adjacency matrix construction, whereas other knowledge distillation approaches require accessing the teacher model for each update. SEvo is arguably an efficient tool for transferring embedding knowledge. Nevertheless, SEvo alone cannot be expected to facilitate the learning of the other modules, which consequently is still inferior to state-of-the-art methods such as DKD. Fortunately, SEvo and DKD can work together to further boost the recommendation performance. ", "page_idx": 27}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "D.1 SEvo for GNN-based models ", "text_level": 1, "page_idx": 27}, {"type": "table", "img_path": "55zLbH7dE1/tmp/a047bdca5f997ee6b10b94841a3ee5134d946a12fd842d62784b9059105135a8.jpg", "table_caption": ["Table 6: Beauty recommendation performance comparison. SEvo-enhanced AdamW is applied to LESSR and MAERec. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "In Section 3 we have comprehensively validated the effectiveness of SEvo for classic sequence models. It is also of interest to explore the impact on GNN-based models that have learned certain structural information. Table 6 reports the results on LESSR and MAERec: SEvo not only facilitates the learning of LESSR, but also helps MAERec that already utilizes global graph information. This implies that previous efforts fail to fully exploit structural information, while SEvo demonstrates superior performance in this regard. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "D.2 Jsmoothness as a Regularization Term ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "55zLbH7dE1/tmp/79a6524c5515424b8c3bb1124ba8cc6b7fb0b5912b93d598e64241f7b768c7b5.jpg", "img_caption": ["Figure 7: Smoothness constraints through a regularization term. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Structural information may be injected by imposing $\\mathcal{I}_{s m o o t h n e s s}$ as a regularization term; that is, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{E},\\theta}\\quad\\mathcal{L}(\\mathbf{E},\\theta)+\\lambda\\mathcal{J}_{s m o o t h n e s s}(\\mathbf{E};\\mathcal{G}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\lambda\\geq0$ is a hyperparameter governing the degree of smoothness. We conduct this ablation study in Figure 7 with a $\\lambda$ from $10^{\\bar{-}6}$ to 0.01. As can be seen, incorporating a smoothness regularization term could slightly improve the recommendation performance, but it is not optimal. SEvo performs better because the gradient of the regularization term may be in conflict with the primary loss function. ", "page_idx": 28}, {"type": "text", "text": "D.3 $L$ -layer Approximation ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "55zLbH7dE1/tmp/b07defca966579dd7115649cf1832d072bc033432a58977590a01bbbc4e3fba5.jpg", "table_caption": ["Table 7: SEvo using different approximation layers $L$ . "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "As $L$ increases, SEvo gets closer to the exact solution while accessing higher-order neighborhood information. Table 7 lists the performance of different layers, which reaches its peak around $L=3$ and starts to decrease then. A possible reason is that the higher-order information is over-smoothed and thus not as reliable and easy to use as the lower order information. Similar phenomena have been found in previous works [50, 8] on applying GNNs to recommendation. ", "page_idx": 28}, {"type": "text", "text": "D.4 Training and Inference Times ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The time complexity of SEvo is mainly determined by the arithmetic operations of $\\tilde{\\mathbf{A}}^{l}\\Delta\\mathbf{E},l\\ =$ $1,2,\\dots,L$ . Assuming that the number of non-zero entries of $\\tilde{\\mathbf A}$ is $S$ , the complexity required is about $\\mathcal{O}(L S d)$ . Because the recommendation datasets are known for high sparsity (i.e., $S$ is very small), the actual computational overhead can be reduced to a very low level, almost negligible. Table 8 provides the actual training and inference times. ", "page_idx": 28}, {"type": "text", "text": "Table 8: Training and inference times. The wall time (seconds) here is evaluated on an Intel Xeon E5-2620 v4 platform and a single GTX 1080Ti GPU, while the results in Table 3 are tested on an Intel Xeon CPU E5-2680 v4 platform and a single RTX 3090 GPU. ", "page_idx": 29}, {"type": "table", "img_path": "55zLbH7dE1/tmp/5728ade9da58c99b5275ef77b4703f203e12de250562c9166d769931cdd396d6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The paper\u2019s contributions are summarized in the introduction point by point. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Please refer to Section 5. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All assumptions and proofs are detailed in Appendix A. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Please refer to the settings introduced in Section 3. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All experiments are conducted on public datasets and the code can be found in the supplementary material. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The implementation details of baselines and ours can be found in Appendix B.3 and Section 3, respectively. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: 1) Paired t-test is performed over 5 independent runs (see Table 2). 2) 1-sigma error bars are marked in most figures and tables. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Please refer to Appendix D.4. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The broader impact has been discussed in Section 5. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This work poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The source of datasets are credited in Appendix B.2, and all baselines have been introduced in Appendix B.3. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]