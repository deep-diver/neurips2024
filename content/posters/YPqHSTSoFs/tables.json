[{"figure_path": "YPqHSTSoFs/tables/tables_5_1.jpg", "caption": "Table 1: Instruction tuning results on AlapcaEval (Win %). In cross-model control, all base models incorporate the same delta model, which is trained using the LLAMA2-7B as the template model.", "description": "This table presents the results of instruction tuning experiments using the AlpacaEval benchmark.  It compares the performance (Win %) of four large language models (LLMs): LLAMA2-7B, LLAMA2-13B, LLAMA2-70B, and MISTRAL-7B.  The results are shown for three different methods: a vanilla base model, LORA (a parameter-efficient fine-tuning technique), and the proposed Cross-model Control (CMC) method.  CMC uses a shared, smaller \"delta model\" trained on LLAMA2-7B to improve multiple LLMs simultaneously. The numbers in parentheses show the improvement achieved by each method over the Vanilla Base Model.", "section": "4.1 Experiment on Instruction Tuning"}, {"figure_path": "YPqHSTSoFs/tables/tables_6_1.jpg", "caption": "Table 2: Unlearning results on TOFU benchmark. All chat models incorporate the same delta model, which is trained using the LLAMA2-7B-TOFU as the template model. Better scores are bolded.", "description": "This table presents the results of the unlearning experiments using the TOFU benchmark.  It compares various methods (vanilla model, LoRA, 8-UNLEARNING, and the proposed CMC method) across three LLMs (LLAMA2-7B, LLAMA2-13B, and MISTRAL-7B) in their ability to forget information from a forget set while retaining information from a retain set.  The performance is measured using ROUGE-L (recall-oriented understanding for gisting evaluation), Probability (likelihood of correct answers), and Truth Ratio (ratio of correct to incorrect answers).  The results show how effectively each method prevents the model from outputting information from the forget set while maintaining accuracy on the retain set and other knowledge domains.  Bold values indicate better performance.", "section": "4.2 Experiment on Unlearning"}, {"figure_path": "YPqHSTSoFs/tables/tables_7_1.jpg", "caption": "Table 3: Different delta model size on first 50 data points of AlpacaEval (win %).", "description": "This table presents the results of instruction tuning experiments using different sizes of delta models (15M, 42M, and 110M parameters).  The win rate (in percentage) on the first 50 data points of the AlpacaEval benchmark is shown for four different LLMs (LLAMA2-7B, LLAMA2-13B, LLAMA2-70B, and MISTRAL-7B).  The results demonstrate the impact of the delta model's size on the performance of instruction tuning across various LLMs.", "section": "4.3 Impact of Parameter Size of Delta Model on Performance"}, {"figure_path": "YPqHSTSoFs/tables/tables_8_1.jpg", "caption": "Table 4: Ablation study", "description": "This table presents the results of ablation studies conducted to evaluate the impact of removing LogSoftmax and prefix matching from the Cross-model Control (CMC) method.  The AlpacaEval (Win %) metric is used to measure the performance of three different LLMs (LLAMA2-7B, LLAMA2-13B, and MISTRAL-7B) under different conditions: with both LogSoftmax and prefix matching (baseline), without LogSoftmax, and without prefix matching.  The results show the performance degradation when either or both of these components are removed from CMC, highlighting their importance to the method's effectiveness.", "section": "4.5 Ablation Study"}, {"figure_path": "YPqHSTSoFs/tables/tables_12_1.jpg", "caption": "Table 5: Average Distance between logits shifts. Smaller distances mean more similarities", "description": "This table presents a quantitative analysis of the similarity in fine-tuning effects across different LLMs. It shows the average Sinkhorn divergence between the logits shifts of various model pairs.  The divergence is calculated both when all models are fine-tuned on the same dataset (GPT4-Alpaca) and when one model is fine-tuned on GPT4-Alpaca and another on a different dataset (GSM8k). Lower divergence values indicate higher similarity in fine-tuning effects.", "section": "4.3 Impact of Parameter Size of Delta Model on Performance"}]