[{"figure_path": "YPqHSTSoFs/figures/figures_1_1.jpg", "caption": "Figure 1: Cross-model control could apply the fine-tuning outcomes of one model to other models", "description": "This figure illustrates the core idea of Cross-model Control (CMC). A single, small portable tiny language model is trained alongside a larger template language model.  After training, the tiny model's ability to modify the output logits is shared with other user LLMs, regardless of their parameter scales or vocabularies.  This allows efficient improvement of multiple models in a single training process.", "section": "3 Cross Model Control"}, {"figure_path": "YPqHSTSoFs/figures/figures_2_1.jpg", "caption": "Figure 2: Logits shifs on different models exhibit a high degree of similarity.", "description": "This figure shows heatmaps visualizing the logit shifts on different LLMs (Llama2-7b, Llama2-13b, and Mistral-7B) before and after fine-tuning. The high similarity across models despite differences in parameter scales and vocabularies supports the paper's claim that fine-tuning effects are remarkably similar across different LLMs.  The heatmaps represent the change in logit values for each token after fine-tuning compared to before fine-tuning, providing visual evidence of the consistent patterns across different models.", "section": "2 Preliminaries"}, {"figure_path": "YPqHSTSoFs/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of Cross-model Control", "description": "This figure illustrates the architecture and workflow of the Cross-model Control (CMC) method.  Panel (a) shows the training stage, where a frozen template LLM and a tunable tiny language model (delta model) are trained together. The delta model learns to adjust the logits of the template LLM to achieve desired outcomes (e.g., instruction following or unlearning).  Panel (b) shows the inference stage, where the trained delta model interacts with other user LLMs to modify their logits output.  Panel (c) details the token mapping strategy (PM-MinED) that handles the differences in vocabulary between the delta model and user LLMs, focusing on finding the closest semantic match for improved accuracy.", "section": "3 Cross Model Control"}, {"figure_path": "YPqHSTSoFs/figures/figures_7_1.jpg", "caption": "Figure 4: Impact of strength coefficient \u03b1 on performance", "description": "This figure shows the impact of the strength coefficient \u03b1 on the performance of the model in instruction tuning and unlearning tasks.  The left subplot (a) displays the AlpacaEval win rate for instruction tuning across different epochs (2, 4, and 8) at varying \u03b1 values. The right subplot (b) presents the ROUGE scores for the unlearning task, broken down by dataset subset (Real Authors, Real World, Retain, and Forget) with varying \u03b1 values.  The plots illustrate how adjusting \u03b1 affects the balance between overfitting and underfitting during training and impacts the model's performance in unlearning sensitive information.", "section": "4.4 Impact of Strength Coefficient on Performance"}]