{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper provides a comprehensive technical report on the capabilities and limitations of GPT-4, a large language model that is highly relevant to the current study's topic of improving multiple large language models."}, {"fullname_first_author": "Edward J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2022-04-25", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning method that is used as a baseline in the current study, making it an important reference for comparison."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces Llama, the open-source language model which serves as a basis for the portable tiny language model used in the current study."}, {"fullname_first_author": "Samuel Gehman", "paper_title": "Real-toxicityprompts: Evaluating neural toxic degeneration in language models", "publication_date": "2020-11-16", "reason": "This paper discusses the issue of harmful outputs from language models, a problem that the current study addresses through unlearning techniques."}, {"fullname_first_author": "Jason Wei", "paper_title": "Finetuned language models are zero-shot learners", "publication_date": "2022-04-25", "reason": "This paper demonstrates that fine-tuned language models can perform zero-shot learning, which is a relevant finding for the current study's focus on improving model capabilities."}]}