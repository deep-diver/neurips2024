{"importance": "This paper is highly important because it presents a novel and efficient method, CMC, for improving multiple large language models (LLMs) simultaneously.  This addresses the significant cost and resource constraints often faced in fine-tuning LLMs, offering a practical solution to a prevalent challenge in the field.  The introduction of a portable tiny language model and token mapping strategy (PM-MinED) offers new avenues for cross-model optimization and parameter-efficient model enhancement.  The findings contribute directly to ongoing research on LLM optimization, instruction tuning, and unlearning, potentially leading to more efficient and effective LLMs across diverse applications.", "summary": "One-time training improves multiple LLMs using a tiny portable model, drastically reducing costs and resource needs for model enhancement.", "takeaways": ["Cross-model Control (CMC) improves multiple LLMs using a single, portable tiny language model during one-time training.", "The PM-MinED token mapping strategy enables CMC to work effectively with LLMs possessing different vocabularies.", "Extensive experiments on instruction tuning and unlearning tasks validate CMC's effectiveness, showing significant performance gains with minimal parameter increases."], "tldr": "The sheer number of large language models (LLMs) with diverse parameter scales and vocabularies presents significant challenges, particularly concerning cost-effective optimization for specific applications (like instruction following or removing sensitive information).  Existing methods address each model individually, increasing training costs. This research tackles this problem.\nThis paper proposes Cross-model Control (CMC), a method to improve multiple LLMs using a single, small, portable model trained alongside a frozen template LLM.  This approach leverages the similarity of logit shifts before and after fine-tuning across models, enabling the small model to effectively alter the output logits of other LLMs.  A novel token mapping strategy (PM-MinED) further extends the method's applicability to models with different vocabularies. **CMC demonstrates significant performance improvements in instruction tuning and unlearning tasks, achieving remarkable efficiency gains and reduced computational requirements.**", "affiliation": "East China Normal University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "YPqHSTSoFs/podcast.wav"}