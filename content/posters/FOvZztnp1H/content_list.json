[{"type": "text", "text": "AutoTimes: Autoregressive Time Series Forecasters via Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yong Liu,\u2217 Guo Qin,\u2217 Xiangdong Huang, Jianmin Wang, Mingsheng Long ", "page_idx": 0}, {"type": "text", "text": "School of Software, BNRist, Tsinghua University, China {liuyong21,qinguo24}@mails.tsinghua.edu.cn, {huangxdong,jimwang,mingsheng}@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Foundation models of time series have not been fully developed due to the limited availability of time series corpora and the underexploration of scalable pre-training. Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities. To fully revitalize the general-purpose token transition and multi-step generation capability of large language models, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which projects time series into the embedding space of language tokens and autoregressively generates future predictions with arbitrary lengths. Compatible with any decoder-only LLMs, the consequent forecaster exhibits the flexibility of the lookback length and scalability with larger LLMs. Further, we formulate time series as prompts, extending the context for prediction beyond the lookback window, termed in-context forecasting. By introducing LLM-embedded textual timestamps, AutoTimes can utilize chronological information to align multivariate time series. Empirically, AutoTimes achieves state-of-the-art with $0.1\\%$ trainable parameters and over $5\\times$ training/inference speedup compared to advanced LLM-based forecasters. Code is available at this repository: https://github.com/thuml/AutoTimes. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series forecasting is of crucial demand in real-world applications, covering various domains including climate, economics, energy, operations, etc. [22, 43]. The growing challenges of generalpurpose forecasting, where one model is versatile to handle variable-length scenarios [24, 41] and the prediction is necessarily instructed by auxiliary information in other modalities [40, 44], underscore the demand for foundation models [3] of time series, which are aimed to exhibit enhanced capabilities, including multi-step generation, zero-shot generalization [49, 13], in-context learning and multimodal utilization [15], thereby expanding the scope of time series forecasting to a wider range of situations. ", "page_idx": 0}, {"type": "text", "text": "Nevertheless, the development of time series foundation models has been hampered by the limited availability of large-scale pre-training datasets and the technical uncertainty of scalable backbones. In contrast, rapid progress is witnessed in large language models (LLM), facilitated by extensive text corpora [50], available pre-trained models [36], and well-established adaptation techniques [14]. Notably, language and time series share basic commonalities in sequence modeling and generation by learned token transitions, presenting opportunities to adopt off-the-shelf LLMs for time series. ", "page_idx": 0}, {"type": "image", "img_path": "FOvZztnp1H/tmp/91ebfe3a81784a7345aa921703e9dfdcdf1cf8d4d20aba5d45393f4a9e5a2de0.jpg", "img_caption": ["Figure 1: (a) Prevalent LLM4TS methods non-autoregressively generate predictions with the globally flattened representation of lookback series, while large language models inherently predict the next tokens by autoregression [47]. (b) Previous methods adopt language prompts that may lead to the modality disparity, while we find time series can be self-prompted, termed in-context forecasting. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Despite recent studies on large language models for time series (LLM4TS) achieving performance breakthroughs in current forecasting benchmarks [15], the mechanism by which LLMs are aligned to the time series modality still remains obscure. The pilot work, FPT [49] leverages LLMs as generic sequential representation extractors for time series, influencing subsequent LLM4TS methodologies. As depicted in Figure 1 (a), the non-autoregressive approach, where time series are segmented into tokens, flattens and projects all lookback tokens for the prediction in a single step. However, it causes inconsistencies in both model structure and generative approach of LLMs: decoder-only models for autoregressive generation are converted to encoder-only and non-autoregressive forecasters. ", "page_idx": 1}, {"type": "text", "text": "Given that prior studies [9, 38] reveal that generalization performance of LLMs is largely derived from the decoder-only structure trained autoregressively, talents of LLMs may not be fully exhibited. It is also supported by the recent rethinking of previous LLM4TS methods [35], which generally lack the maintenance of autoregression, the essential characteristic of both large language models and statistical forecasters [5, 39]. Therefore, autoregressive LLM4TS methods are underexplored, which can potentially unlock multi-step generation like LLMs, presenting one model for arbitrary lengths. ", "page_idx": 1}, {"type": "text", "text": "Motivated by the reflections, we propose AutoTimes to adapt LLMs as time series forecasters, which retrieves the consistency of autoregression with revitalized LLM capabilities to produce foundation models for time series forecasting. Technically, we independently embed time series segments into the latent space of language models by the consistent training objective: next token prediction [2]. To fully leverage the inherent token transitions of LLMs and reduce the training cost, we freeze the LLM and establish token embedding and projection for time series, which only account for up to $0.1\\%$ total parameters. The consequent forecaster adopts autoregressive inference like LLMs, which is no longer constrained to specific lookback/forecast lengths. Going beyond conventional time series forecasting, we propose in-context forecasting as shown in Figure 1, where time series can be self-prompted by relevant contexts. We further adopt LLM-embedded timestamps as the position embedding to utilize chronological information and align multiple variates. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 By refining the inconsistency of non-autoregressive LLM4TS methods, we propose to inherit the autoregressive property of LLMs, which frees our method from training respectively on the lookback length and allows arbitrary-length predictions with chronological awareness. \u2022 We present AutoTimes, a simple but effective approach to acquire LLM-based forecasters by lightweight adaptation, which utilizes the inherent token transition as the future extrapolation of time series. Further, we propose in-context forecasting, which renovates the conventional paradigm by introducing relevant time series prompts to enhance forecasting. \u2022 Compared with state-of-the-art methods, our repurposed forecaster achieves superior performance while saving over $80\\%$ training and inference time, and further exhibits zero-shot generalizability, in-context forecasting, and scaling behavior empowered by LLMs. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Autoregressive Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Autoregression is an essential concept of both language modeling and time series forecasting. Despite prevalent deep forecasters [10, 26, 42, 48] adopt a non-autoregressive approach without the requirement of iterative forecasting, autoregression, the absent exploration in deep forecasters, serves as the fundamental principle of statistical methods, which enables variable-length predictions. The most well-known model, ARIMA [4] is developed by incorporating differencing on AR and MA models, which are both autoregressive models with learned time-invariant transition from the past to the future. Incorporated with decomposition and pre-defined transitions, exponential smoothing [39] and state space models (SSM) [12, 23] also take the same autoregressive formulation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Autoregressive language models [27, 31] are trained with fine-grained supervision, where the generated token of each position is independently supervised. Consequently, they are not constrained by specific input/output lengths and excel at multi-step generation. Furthermore, existing LLMs are inherently autoregressive models [47], which demonstrate advanced abilities that are not present in small models, such as the generalization [38], scalability [6], and task generality [31, 32]. Therefore, it is imperative to adapt off-the-shelf LLMs as autoregressive forecasters, which keeps the consistency to fully revitalize the model capacity and general-purpose token transitions. ", "page_idx": 2}, {"type": "text", "text": "2.2 Large Language Models for Time Series ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "With the immense advancement of large language model infrastructure, LLM4TS methods have been experiencing significant development in recent years. PromptCast [44] reformulates time series as text pairs and accomplishes forecasting as a sentence-to-sentence task. LLMTime [13] regards time series as numerical tokens, demonstrating the zero-shot generalizability in time series forecasting. FPT [49] fine-tunes parameters of the LLM to adapt it as a general representation extractor serving for multiple time series analysis tasks. UniTime [21] adapts a language model across diverse time series for a unified forecaster of multiple domains. Based on thriving prompting techniques, deft language prompts [15, 21] and soft prompting [7] for time series are further investigated. ", "page_idx": 2}, {"type": "text", "text": "LLM4TS methods have achieved performance breakthroughs in time series forecasting, but the cost of training and inference can sometimes be resource-consuming due to the immensity of LLMs. Recent revisiting of LLM4TS methods has revealed the inefficacy of LLMs adapted in the non-autoregressive approach [35]. By contrast, AutoTimes frozen LLMs, transfers the general-purpose token transition, and introduces minimal parameters to realize autoregressive next token prediction, thereby achieving better model efficiency and consistent utilization of large models. We further provide Table 1 that categorizes prevalent LLM4TS methods by several essential aspects. ", "page_idx": 2}, {"type": "text", "text": "2.3 Multimodal Language Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multimodal models have been well-developed upon LLMs, among which vision language models (VLM) have experienced rapid growth [1, 27]. The booming pre-trained vision backbones [11, 30], together with the instruction tuning paradigm, has revealed the potential of LLMs for vision tasks, where visual tokens and language tokens are concatenated as the input of the LLM [19, 20]. Inspired by this, previous LLM4TS methods utilize instructive language tokens as prefix-prompts for time series analysis [15, 34, 44]. Unlike previous works, our proposed method regards time series itself as the instructive prompt. It avoids the modality gap caused by concatenating time series and language tokens directly. We incorporate chronological information, the textual timestamp of time series, such that the language model can effectively perceive date and periodicity as the position embedding, and align simultaneous events from different time series [22] for multivariate forecasting. ", "page_idx": 2}, {"type": "text", "text": "Table 1: Comparison of LLM4TS methods: Autoregressive categories LLM-based forecasters by whether to conduct autoregression. Freeze LLM enables quick adaptation, which would otherwise require significant resources for fine-tuning. Multimodal refers to the utilization of information from other modalities. Prior to AutoTimes, none of the LLM4TS methods achieved all three. ", "page_idx": 2}, {"type": "table", "img_path": "FOvZztnp1H/tmp/6a8332c301ebbc9499e7b18a480467b0766e4e653481c304dc1311973a82aee9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The proposed AutoTimes adapts large language models for multivariate time series forecasting. Given lookback observations $\\mathbf{x}_{1:L}\\overset{\\cdot}{=}\\{\\mathbf{x}_{1},\\ldots,\\bar{\\mathbf{x}_{L}}\\}\\overset{\\circ}{\\in}\\mathbb{R}^{L\\times C}$ with $L$ time steps and $C$ variates, the objective is to predict the future $F$ time steps $\\mathbf{x}_{L+1:L+F}=\\{\\mathbf{x}_{L+1},\\dots,\\mathbf{x}_{L+F}\\}\\in\\mathbb{R}^{F\\times C}$ . Besides, the textual timestamp ${\\mathbf a}_{t}$ (e.g. $2016/07/05\\ 00{:}00{:}00)$ , as the most common covariate, is adopted for prediction, which is aligned with time points $\\mathbf{x}_{t}\\in\\mathbb{R}^{C}$ at time $t$ . The task is to train an LLM-based forecaster $f$ that is able to predict with the (varying) lookback length $L$ for the (arbitrary) forecast length $F$ as: ", "page_idx": 2}, {"type": "image", "img_path": "FOvZztnp1H/tmp/5e688291e1ce5cbe6997b1e8c908c89493bb3960f1b8be9329fa05e1a607539c.jpg", "img_caption": ["Figure 2: An example to illustrate how AutoTimes adapts language models for time series forecasting. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nf:({\\bf x}_{1:L},{\\bf a}_{1:L+F})\\mapsto\\hat{\\bf x}_{L+1:L+F}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.1 Modality Alignment ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Time series token To empower the forecaster with the capability to predict time series for arbitrary lengths, we repurpose autoregressive LLMs as time series forecasters as depicted in Figure 2. Prior to this, we define time series token as the consecutive and non-overlapping segment of a single variate. It is regarded as the common token of the LLM-based forecaster, which encompasses series variations and mitigates excessively long autoregression. To focus on modeling temporal variations, our forecaster predicts each variate independently. Beyond Channel Independence [26] that implicitly captures the multivariate correlation [22] by shared parameters, AutoTimes converts timestamps into position embeddings and explicitly aligns simultaneous segment tokens, which is detailed in the next paragraph. Therefore, we simplify $\\mathbf{x}_{t}$ as the time point of specific variate $x_{t}\\in\\mathbb{R}$ . Given a single-variate time series of context length $N S$ , the $i$ -th segment of length $S$ is denoted as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{s}_{i}=\\{x_{(i-1)S+1},\\ldots,x_{i S}\\}\\in\\mathbb{R}^{S},\\;i=1,\\ldots,N.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Considering the general-purpose token transition, we freeze the parameters of large language models. To realize the token-wise alignment between time series tokens and language tokens, we establish SegmentEmbeddi $\\mathrm{ng}(\\cdot):\\mathbb{R}^{\\breve{S}}\\mapsto\\mathbb{R}^{D}$ that independently embeds segments into the latent space: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{S}\\mathbf{E}_{i}=\\mathrm{SegmentEmbedding}(\\mathbf{s}_{i}),\\ i=1,\\ldots,N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $D$ is consistent with the dimension of the LLM. ", "page_idx": 3}, {"type": "text", "text": "Position embedding Timestamp, an essential covariate indicating the chronological information, is generally utilized as an extra embedding in previous deep forecasters [43, 48]. However, increasing models [10, 26, 45] have discarded the embedding and found the performance will not be greatly affected, implying the improper encoding of timestamps. In contrast, textual timestamps have been demonstrated as an enhancement in LLM4TS methods, which are always formulated into prefixprompts [15, 21]. Nevertheless, it also leads to excessive context length, impeding LLMs from paying sufficient attention to time series tokens and inducing time-consuming feed-forwarding. Inspired by the functionality of position embedding, which incorporates information about the relative or absolute position of the tokens [37]. We adopt LLM-embedded timestamps as position embeddings to utilize temporal information and align simultaneous events (segments) from different varieties. ", "page_idx": 3}, {"type": "text", "text": "Technically, we formulate the starting and end timestamps of corresponding segments by the template demonstrated in Figure 3. Experimentally, we observe that the simple template without deft design can consistently boost the forecasting performance in Appendix D.5, aiding the LLM-based forecaster to comprehend the date and align different variates based on Channel Independence. Since all the previous language tokens are visible to the special ending token $\\tt{<E O S>}$ of a sentence, we adopt the embedding of $\\tt{<}\\tt{E O S}\\mathrm{>}$ as $\\mathbf{TE}_{i}\\in\\mathbb{R}^{D}$ as the position embedding from textual timestamps: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{TE}_{i}=\\mathrm{SelectLast}\\left(\\mathrm{LLM}(\\mathrm{TimestampTemplate}(\\mathbf{s}_{i}))\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notably, $\\mathbf{TE}_{i}$ is pre-computed by LLMs such that runtime forwarding for language tokens is not required during training. Given that the latent space of the LLM locates both time series tokens and ", "page_idx": 3}, {"type": "image", "img_path": "FOvZztnp1H/tmp/dfe49ddf1ece754a15afa8768ee3e6a33864ac942cb4db961bc8eb9b31baec4a.jpg", "img_caption": ["Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2) textual timestamps are converted into the position embeddings by the LLM; (3) time series segments are embedded and projected by next token prediction, where intermediate layers of LLM are frozen. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "language tokens, the position embedding can be integrated with the corresponding time span without increasing the context length. Concretely, the token embedding $\\mathbf{E}_{i}\\in\\mathbb{R}^{D}$ is obtained by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{E}_{i}=\\mathbf{S}\\mathbf{E}_{i}+\\mathbf{T}\\mathbf{E}_{i}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2 Next Token Prediction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As shown in Figure 3, prevalent LLMs [6, 36] are endowed with the capability of predicting the next token $\\mathbf{s}_{i}$ based on the preceding tokens $\\mathbf{s}_{<i}$ . We reutilize LLMs in a fully consistent approach and generate prediction of arbitrary lengths iteratively. Given a time series of context length $N S$ , the input series is segmented and embedded into $N$ token embeddings $\\{{\\bf E}_{1},\\ldots,{\\bf E}_{N}\\}$ . The training objective is to independently generate the next tokens $\\{\\hat{s}_{2},\\dots,\\hat{s}_{N+1}\\}$ . We feed the token embeddings $\\mathbf{E}_{i}$ into the intermediate layers of the LLM, which inherently parameterize token transitions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\{\\hat{\\bf E}_{2},\\ldots,\\hat{\\bf E}_{N+1}\\}=\\mathrm{LLMLayers}(\\{{\\bf E}_{1},\\ldots,{\\bf E}_{N}\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We adopt SegmentProjectio $\\mathbf{\\boldsymbol{\\mathsf{\\rho}}}_{1}(\\cdot):\\mathbb{R}^{D}\\mapsto\\mathbb{R}^{S}$ to independently projects embeddings to segments: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{s}}_{i}=\\mathrm{SegmentProjection}(\\hat{\\mathbf{E}}_{i}),\\;i=2,\\ldots,N+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, each predicted segment is supervised by the token-wise ground truth to optimize the parameters of embedding and projection layers, which are simply implemented by multi-layer perceptrons: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{MSE}}=\\frac{1}{N S}\\sum||\\mathbf{s}_{i}-\\hat{\\mathbf{s}}_{i}||_{2}^{2},\\;i=2,\\ldots,N.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notably, the context length $N S$ is decided during training, representing the maximum input length during inference. Therefore, one consequent forecaster is suitable for different input lengths like the LLM, validated in Appendix D.4. Moreover, AutoTimes can generate predictions of arbitrary lengths by iterative multi-step forecasting, proven to overcome error accumulation better than state-of-the-art forecasters in Section 4.1, since autoregressive LLMs inherently excel at multi-step generation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{s}}_{i}=\\mathrm{LLMForecaster}(\\mathbf{s}_{<i}),\\ i=1,\\dots,\\frac{F}{S}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Instead of respectively training models on different lookback/forecast lengths, AutoTimes handles all the scenarios by one model. Surprisingly, with the consistency of autoregression, it also inherits notable generalizability and scaling behavior of LLMs, which is demonstrated in Sections 4.2 and 4.4. ", "page_idx": 4}, {"type": "text", "text": "3.3 In-Context Forecasting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Large language models are capable of generating expected outputs based on provided task demonstrations from downstream datasets without gradient updating, known as the in-context learning ability. The task demonstrations are generally constituted by paired questions and answers [47]. Formally, the context $\\mathcal{C}=\\{g(x^{(1)},y^{(1)}\\bar{)},\\ldots,\\bar{g(x^{(m)},y^{(m)})}\\}$ represents a set of demonstrations with $m$ pairs, where $g(\\cdot)$ is the template that transforms each question and answer into natural language. ", "page_idx": 5}, {"type": "text", "text": "In terms of time series forecasting, we propose to constitute the pair by lookback-forecast windows, which are exactly represented as successive time points from earlier historical observations. Hence, we use time series in target datasets as prompts, extending the context for prediction beyond consecutive lookback series. We denote the extended context as $\\mathcal{C}$ , which contains $m$ time series prompts $\\mathrm{tsp}^{(j)}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal C=\\{\\mathrm{tsp}^{(j)}=\\mathbf x_{\\le t_{j}}|\\;\\mathrm{earlier~historical~time~series}\\},\\;j=1,\\ldots,m,\\;t_{j}\\le L.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "During training, we first obtain an LLM-based forecaster on a source dataset and select time series prompts from the downstream target dataset based on a unified strategy. During inference, we ensure all the prompts appear before the window to be predicted, such that there is no data leakage from future information. As shown in Figure 4, we concatenate time series prompts with lookback series and feed them as the context of the forecaster, termed in-context forecasting: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf:(\\mathcal{C},\\mathbf{x}_{1:L},\\mathbf{a}_{1:L+F})\\mapsto\\hat{\\mathbf{x}}_{L+1:L+F}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct thorough evaluations of the performance of AutoTimes, including time series forecasting, zero-shot forecasting, and the proposed in-context forecasting. Additional analyses are included to evaluate the generality, scaling behavior, and adaptation cost of large language models. Detailed code implementation for reproduction is provided in our public code repository. ", "page_idx": 5}, {"type": "text", "text": "4.1 Time Series Forecasting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Benchmarks For long-term time series forecasting, we extensively include real-world datasets, including ETTh1, ECL, Traffic, Weather [43], and Solar-Energy [22]. For short-term forecasting, we adopt the well-acknowledged M4 competition [25]. Detailed descriptions are provided in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Baselines We compare AutoTimes with state-of-the-art models, including advanced LLM4TS methods: TimeLLM [15], UniTime [21], and FPT [49]; well-acknowledged deep forecasters: iTransformer [22], DLinear [45], PatchTST [26], and TimesNet [42]. For the challenging short-term forecasting, we further include competitive baselines: Koopa [23], N-HiTS [8] and N-BEATS [28]. All baselines are officially implemented or reported. We adopt LLaMA-7B [36] as our base LLM. Detailed implementations, error bars, and hyperparameter analysis are provided in Appendix B and C. ", "page_idx": 5}, {"type": "text", "text": "Setups For short-term forecasting, we follow the well-acknowledged TimesNet [42], which assesses the fundamental ability of forecasters in modeling temporal variations. For long-term forecasting, we establish a novel one-for-all benchmark: a single forecaster is trained on one dataset and subsequently utilized for all prediction lengths. We highlight that this approach evaluates the basic versatility as foundation models of time series, which aims to break the prevailing practice of extensive training across diverse real-world scenarios. To be specific, we evaluate all methods by rolling forecasting: a model is trained with predetermined input/output lengths, and the predicted values are integrated as part of the input in subsequent iterations until reaching the desired forecast length. Therefore, the key to success in this task lies in mitigating multi-step error accumulation. Still, the conventional one-for-one approach that trains forecasters respectively on each length is also provided in Table 12. ", "page_idx": 5}, {"type": "text", "text": "Results The average results are presented in Table 2- 3, with the best results in bold and the second best underlined. AutoTimes consistently outperforms all counterparts of short-term forecasting in Table 2, demonstrating the basic ability of LLM-based forecasters to capture diverse series variations. Further, in the one-for-all long-term scenarios, AutoTimes surpasses other LLM4TS methods and deep forecasters in $80\\%$ datasets in Table 3, outperforming previous state-of-the-art TimeLLM by ", "page_idx": 5}, {"type": "table", "img_path": "FOvZztnp1H/tmp/453de5fa16695c784a88069edf63c4e477119d58165c2cf5f4599b264f6200a9.jpg", "table_caption": ["Table 2: Average short-term forecasting results on the M4 [25]. Full results are provided in Table 11. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "FOvZztnp1H/tmp/eb85424b6dbf1e49103d96480312bbcb67164e28b4d681d4d9da0b7766f6c072.jpg", "table_caption": [], "table_footnote": ["Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in $\\{96,\\bar{192},336,72\\bar{0}\\}$ . AutoTimes adapt LLMs with the context length $C=672$ . We set the input length $L=672$ and output length $F=96$ in other methods. All results are averaged. Full results is provided in Table 10. "], "page_idx": 6}, {"type": "text", "text": "$9.12\\%$ in average. Compared with other forecasters trained in the one-for-one scenario in Table 12, AutoTimes still achieved state-of-the-art performance in $70\\%$ of settings without respective training. ", "page_idx": 6}, {"type": "text", "text": "By diving into the proposed one-for-all and the traditional one-for-one benchmarks in Table 3 and 12, it is notable that prevalent deep forecasters, such as Transformer-based forecasters and DLinear, can achieve competitive and even better results under rolling forecasting. Nevertheless, the performance of non-autoregressive LLM4TS methods can degenerate a lot without respective training. Therefore, it highlights our persistent utilization of autoregression and thorough leveraging of inherent token transitions of LLMs, thereby mitigating error accumulation during multi-step rolling forecasting. ", "page_idx": 6}, {"type": "text", "text": "4.2 Zero-Shot Forecasting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setups Large language models have exhibited remarkable zero-shot generalization capability [6]. To verify whether our LLM-based forecaster inherits this ability, where no training sample of the target domain is available, we assess the performance of zero-shot forecasting. Concretely, we adhere to the benchmark established by FPT [49], where the forecaster is initially trained on a source domain and subsequently evaluated on an unseen target domain. We conduct the transfer learning between the M3 and M4 competitions, both of which encompass abundant temporal variation patterns but follow different data distributions. We compare AutoTimes with deep forecasters and FPT as the only LLM4TS method, given that only FPT has exhibited zero-shot generalization in this benchmark. ", "page_idx": 6}, {"type": "text", "text": "Table 4: Zero-shot forecasting results in averaged SMAPE. $\\mathbf{M}4\\rightarrow\\mathbf{M}3$ trains forecasters on the datasets of M4 and evaluates on M3, and vice versa. Detailed results are provided in Appendix D.2 ", "page_idx": 6}, {"type": "table", "img_path": "FOvZztnp1H/tmp/f8179132f4ab608ad2ec4e849c63b68c3d53f9a1352a41ed4ca73db7c3e58cbe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Results The comprehensive results of zero-shot forecasting are presented in Table 4. AutoTimes demonstrates superior performance compared to deep forecasters and FPT in both $\\mathbf{M}4\\rightarrow\\mathbf{M}3$ and $\\mathbf{M}3\\rightarrow\\mathbf{M}4$ scenarios. It is evident that LLM4TS methods generally achieve improved performance in this task due to the enhanced model capacity, leading to a $15\\%$ SMAPE reduction compared with the efficient forecaster DLinear. Despite sharing the same Transformer backbone, LLM4TS methods still outperform PatchTST due to the transferable knowledge pre-trained on large corpora of sequences. This underscores the advantage of leveraging LLMs for time series forecasting. Moreover, AutoTimes inherits general-purpose token transitions, surpassing FPT without tuning intermediate LLM layers. ", "page_idx": 6}, {"type": "image", "img_path": "FOvZztnp1H/tmp/9b6758fb35df198c104460be1f38513f8ee6c021da77c18e7acd00939acbc507.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Demonstration of in-context forecasting and results compared with zero-shot. We uniformly select the foremost time points from the target domain as prompts and concatenate them with lookback to obtain the prediction. AutoTimes adapts LLMs on the source domain with a larger context length to place the additional time series prompt. Supplementary showcases are provided in Figure 12. ", "page_idx": 7}, {"type": "text", "text": "4.3 In-Context Forecasting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setups We conduct in-context forecasting on AutoTimes, which is depicted in Figure 4. Similar to zero-shot forecasting, the task is to apply a forecaster, trained on a source dataset, to an unseen target dataset. Additionally, several task demonstrations from the target domain, referred to as time series prompts in Equation 10, are available during inference. Specifically, we concatenate these prompts with the lookback window to form the context for prediction. ", "page_idx": 7}, {"type": "text", "text": "We adopt the aforementioned $\\mathrm{M}4\\rightarrow\\mathrm{M}3$ scenario. Since the samples of the M3 dataset are univariate time series with different lengths, we always predict the last $F$ time points of each sample during inference. We set the lookback length $L=F$ , and thus the length of time series prompts is $2F$ . We set the number of prompts $m=1$ . Therefore, we initially train an LLM-based forecaster on the source M4 dataset with the context length of $3F$ . We adopt an intuitive strategy to select the prompt: uniformly adopting the first $2F$ time points of the time series as the corresponding prompt. Supposing the lookback series starts after time $t\\left(\\geq F\\right)$ , in-context forecasting is formulated as: ", "page_idx": 7}, {"type": "equation", "text": "$$\nf:\\left(\\left\\{x_{1:2F}\\right\\},x_{t+1:t+F},\\mathbf{a}_{t+1:t+2F}\\right)\\mapsto\\hat{x}_{t+F+1,t+2F}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To prevent data leakage of future information, too short samples are discarded to prevent the overlap between the prompt and the future prediction. The implementation details of in-context forecasting are provided in Appendix D.6. We further investigate different prompt retrieval strategies. Insightful results are provided to reveal the influence of using time series prompts for interactive prediction and take-away instructions of prompt engineering in the time series modality. ", "page_idx": 7}, {"type": "text", "text": "Results The quantitative results of in-context forecasting are provided on the right of Figure 4. The results of zero-shot forecasting, where no downstream demonstration is available, are compared as the baseline. Beneftiing from the time series prompts of the target domain, our LLM-based forecaster with the proposed in-context forecasting paradigm achieves consistent promotions on all M3 subsets and the averaged $13.3\\%$ SMAPE reduction compared with zero-shot forecasting. In contrast to previous LLM4TS methods that rely on deft language prompts, LLMs adopted by AutoTimes can be instructed by time series itself with our intuitive prompting engineering. From the perspective of the forecasting paradigm, we extend the prediction context beyond the lookback window. To inherit token transitions of language models parameterized by intermediate layers, AutoTimes takes a crucial step by establishing a mapping between time series segments and the latent space of language tokens, which is however absent in non-autoregressive LLM4TS methods. Therefore, ensuring autoregression consistency enhances the effective utilization of LLMs as foundation models. ", "page_idx": 7}, {"type": "text", "text": "4.4 Method Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Generality Previous LLM4TS [15, 49] methods focus on applying their approach to specific LLMs. We demonstrate that AutoTimes is compatible with any decoder-only LLMs. By extensively training LLM-based forecasters by AutoTimes based on prevalent LLMs, including GPT-2 [31], OPT [46], and LLaMA [36], we present the results in Table 5, highlighting the generality of AutoTimes. ", "page_idx": 7}, {"type": "table", "img_path": "FOvZztnp1H/tmp/d5a2ea29a509958adb515ede35dfde54f60dc5558d7894aa6ec26a3b7e028c0f.jpg", "table_caption": ["Table 5: Averaged results of alternative language models. Full results are provided in Table 18. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Scaling behavior Scalability is an essential characteristic that emerges from small models to large foundation models. By investigating the results presented in Table 5, we observe that the prediction accuracy of the forecaster generally improves with the increase in LLM parameters. This scaling behavior of LLM-based forecasters introduces a trade-off between performance and adaptation cost. To provide a comprehensive assessment, we evaluate each adapted forecaster from three perspectives: performance, training speed, and parameters, as presented in Figure 5. We observe that the largest LLaMA-7B consistently delivers optimal forecasting performance. As a relatively small language model, OPT-1.3B exhibits good parameter efficiency as an out-of-the-box forecaster. ", "page_idx": 8}, {"type": "image", "img_path": "FOvZztnp1H/tmp/dca47c47e288d8423be5b4d274951008641521eb159a1f7c2d54aa4ac617b8c1.jpg", "img_caption": ["Figure 5: Efficiency comparison of alternative LLMs, evaluated by the same configuration of Table 5. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Adaptation cost To mitigate the substantial cost of adapting large language models, AutoTimes introduces minimal parameters with all intermediate layers of LLM frozen. Additionally, we seamlessly integrate the language tokens (e.g. textual timestamps) without excessive context length and runtime overhead for training, thereby significantly reducing the adaptation cost. Figure 6 presents a comprehensive efficiency analysis with advanced LLM4TS methods: FPT is applicable on GPT-2 and TimeLLM is applicable on LLaMA-7B. Not only does AutoTime achieve better results in Table 3, but its training and reasoning time is also greatly reduced, bringing over $5\\times$ speedup on average. In terms of parameter efficiency, AutoTimes focuses on establishing the embedding for time series segments, which is simply implemented by the MLP $(0.79M)$ account for $0.1\\%$ parameters of the LLM $(7B)$ . Therefore, the results affirm the effectiveness of reutilizing the inherent token transition. ", "page_idx": 8}, {"type": "image", "img_path": "FOvZztnp1H/tmp/c6c04e8f3aa8fd5a61190ba4a40bbceaf3ee9c9a21c7bb0b122164674d9b3fe7.jpg", "img_caption": ["Figure 6: Comparison of AutoTimes and other LLM4TS methods in terms of training/inference time and tunable parameters with the same batch size (224) on the ETTh1 dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Ablation study Recent research has raised doubts about the validity of previous LLM4TS methods [35], which predominantly adopt non-autoregression, that is, treating the LLM as a BERT-style pre-trained backbone and utilize a globally flattening projector on all lookback tokens. Here, we provide a thorough ablation study to examine our proposed AutoTimes in Table 6. The results underscore that our method maintains the consistency of the decoder-only architecture and autoregressive inference, effectively leveraging LLMs and addressing the concerns regarding performance improvement and adaptation cost. Further, we provide a comparison by substituting our token-wise segment projection (consistent with LLMs) with the flatten linear head [26] (common in non-autoregressive forecasters). Results of Table 21 in the Appendix reveal that the performance of non-autoregressive generation is consistently inferior to that of our autoregressive AutoTimes approach. ", "page_idx": 9}, {"type": "text", "text": "Table 6: We follow the protocol of LLM4TS ablation studies [35] to verify whether the LLM is truly useful in our AutoTimes: (1) w/o LLM replaces the language model entirely and passing input tokens directly to the last layer; (2) LLM2Attn replaces the language model with a single multi-head attention layer; (3) LLM2Trsf replaces the language model with a single transformer block. ", "page_idx": 9}, {"type": "table", "img_path": "FOvZztnp1H/tmp/9022419814595d683cb9ead2b7fe492a7852bab2390a7abc0c1118cad4b28077.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "LoRA adaptation By incorporating low-rank adaptation technique [14] on the intermediate LLM layers, the token transition of the large language model can be further fine-tuned to align the future extrapolation of time series. Table 7 provides the performance comparing the incorporation of LoRA, which consistently improves the performance of the LLM-based forecaster adapted by AutoTimes. ", "page_idx": 9}, {"type": "table", "img_path": "FOvZztnp1H/tmp/67888f158f6f990984c6f04fd13321eb22d1c7292f8cffc233d0a9c238c2b927.jpg", "table_caption": ["Table 7: Full long-term forecasting results of AutoTimes and AutoTimes equipped with LoRA [14]. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper aims to develop foundation models for time series forecasting. We utilize off-the-shelf LLMs as autoregressive forecasters by transferring the general-purpose and multi-step generation ability. Different from prior methods, we notice prevalent non-autoregressive LLM4TS methods may contradict the decoder-only structure and lead to insufficient utilization of LLMs. Experimentally, the proposed method achieves state-of-the-art performance with remarkable model efficiency. Further analysis reveals that our forecaster effectively inherits advanced capabilities such as zero-shot and in-context forecasting, and is able to utilize both instructive times series and timestamps. In the future, we will further incorporate advanced low-rank adaptation and utilize booming language backbones. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the Ministry of Industry and Information Technology of China. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022. [2] Yoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in neural information processing systems, 13, 2000. [3] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[4] George Box. Box and jenkins: time series analysis, forecasting and control. In A Very British Affair: Six Britons and the Development of Time Series Analysis During the 20th Century, pages 161\u2013215. Springer, 2013.   \n[5] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis: forecasting and control. John Wiley & Sons, 2015. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. [7] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: Promptbased generative pre-trained transformer for time series forecasting. arXiv preprint arXiv:2310.04948, 2023. [8] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, and Artur Dubrawski. Nhits: Neural hierarchical interpolation for time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 6989\u20136997, 2023. [9] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022.   \n[10] Abhimanyu Das, Weihao Kong, Andrew Leach, Rajat Sen, and Rose Yu. Long-term forecasting with tide: Time-series dense encoder. arXiv preprint arXiv:2304.08424, 2023.   \n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[12] James Durbin and Siem Jan Koopman. Time series analysis by state space methods, volume 38. OUP Oxford, 2012.   \n[13] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. arXiv preprint arXiv:2310.07820, 2023.   \n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[15] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023.   \n[16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[18] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pages 95\u2013104, 2018.   \n[19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.   \n[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.   \n[21] Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, and Roger Zimmermann. Unitime: A language-empowered unified model for cross-domain time series forecasting. arXiv preprint arXiv:2310.09751, 2023.   \n[22] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023.   \n[23] Yong Liu, Chenyu Li, Jianmin Wang, and Mingsheng Long. Koopa: Learning non-stationary time series dynamics with koopman predictors. arXiv preprint arXiv:2305.18803, 2023.   \n[24] Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Timer: Transformers for time series analysis at scale. arXiv preprint arXiv:2402.02368, 2024.   \n[25] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: 100,000 time series and 61 forecasting methods. International Journal of Forecasting, 36(1):54\u201374, 2020.   \n[26] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.   \n[27] R OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13, 2023.   \n[28] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019.   \n[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[33] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \n[34] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test: Text prototype aligned embedding to activate llm\u2019s ability for time series. arXiv preprint arXiv:2308.08241, 2023.   \n[35] Mingtian Tan, Mike A Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen. Are language models actually useful for time series forecasting? arXiv preprint arXiv:2406.16964, 2024.   \n[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[38] Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective works best for zero-shot generalization? In International Conference on Machine Learning, pages 22964\u201322984. PMLR, 2022.   \n[39] Peter R Winters. Forecasting sales by exponentially weighted moving averages. Management science, 6(3):324\u2013342, 1960.   \n[40] Gerald Woo, Chenghao Liu, Akshat Kumar, and Doyen Sahoo. Pushing the limits of pre-training for time series forecasting in the cloudops domain. arXiv preprint arXiv:2310.05063, 2023.   \n[41] Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. Unified training of universal time series forecasting transformers. arXiv preprint arXiv:2402.02592, 2024.   \n[42] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. arXiv preprint arXiv:2210.02186, 2022.   \n[43] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419\u201322430, 2021.   \n[44] Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. IEEE Transactions on Knowledge and Data Engineering, 2023.   \n[45] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121\u201311128, 2023.   \n[46] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n[47] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.   \n[48] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106\u201311115, 2021.   \n[49] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained lm. arXiv preprint arXiv:2302.11939, 2023.   \n[50] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19\u201327, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Dataset Descriptions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We conduct experiments to evaluate the performance of the proposed AutoTimes on seven real-world datasets spanning diverse domains: (1) ETTh1 [48] spans from July 2016 to July 2018 and consists of seven factors related to electricity transformers. (2) Weather [43] encompasses 21 meteorological factors collected every 10 minutes in 2020 from the Weather Station of the Max Planck Biogeochemistry Institute. (3) ECL [43] captures hourly electricity consumption data from 321 clients. (4) Traffic [43] gathers hourly road occupancy rates from 862 sensors on San Francisco Bay area freeways, covering the period from January 2015 to December 2016. (5) Solar-Energy [18] records solar power production from 137 PV plants in 2006, sampled every 10 minutes. (6) M4 is a competition dataset encompassing various time series across different frequencies and domains such as business and economics. (7) M3, albeit smaller than M4, also contains diverse time series from various domains. ", "page_idx": 13}, {"type": "text", "text": "We follow the same data processing and train-validation-test set split protocol used in TimesNet [43], where the train, validation, and test datasets are strictly divided according to chronological order to ensure no data leakage. As for long-term forecasting settings, we fix the context length of AutoTimes and the lookback length of other compared methods as 672 in ETT, ECL, Traffic, Weather, and Solar-Energy, and the forecast length varies in {96, 192, 336, 720}. For the short-term forecasting on M4 and M3 datasets, the input length is generally set to twice the output length according to the official implementation of TimesNet. The details are provided in Table 8. ", "page_idx": 13}, {"type": "text", "text": "Table 8: Detailed dataset descriptions. Dim denotes the variate number. Dataset Size denotes the total number of time points in (Train, Validation, Test) splits respectively. Forecast Length denotes the future time points to be predicted. Frequency denotes the sampling interval of time points. ", "page_idx": 13}, {"type": "table", "img_path": "FOvZztnp1H/tmp/66a4df1d553be7fe76da0204af3bacd25000ac6eee6ec68136eda0200de91a82.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "AutoTimes processes timestamps in textual form rather than numerical encoding, potentially enabling to handle other textual data such as news or logs. We utilize LLM to obtain embedding for the special token $<\\tt E O S>$ to capture embedding for the entire sentence. Pseudo-code for this process is depicted in Algorithm 1. It is worth noting that in the context of multivariate time series forecasting, timestamps are shared across variates. Thus, timestamps can implicitly express relationships between variates even with channel independence. Further, assuming there are $C$ variates since the number of timestamps is $\\textstyle{\\frac{1}{C}}$ of the total time point count, these embeddings can be efficiently pre-computed by large language models. ", "page_idx": 13}, {"type": "text", "text": "After obtaining embedding for the timestamps, we repurpose LLM for time series forecasting using Algorithm 2. At this stage, only the parameters of SegmentEmbedding and SegmentProjection are updated, while the parameters of LLMs remain entirely frozen. During inference, AutoTimes utilizes the last token generated as its prediction and then employs this output to create subsequent predictions autoregressively. This approach enables AutoTimes to predict sequences of variable lengths with just one model dynamically. Such capability is crucial in real-world application scenarios. The pseudo-code in Algorithm 3-4 illustrates this process. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "All the experiments are conducted using PyTorch [29] on NVIDIA A100 GPUs. We employ Adam [17] with an initial learning rate in $\\{10^{-3},5\\times1\\tilde{0}^{-4},10^{-4}\\}$ and MSE loss for model optimization. We adopt Channel Independence [26] for multivariate time series and utilize our position embeddings of timestamps to explicitly align them. The batch size is chosen from $\\{256,1024,2048\\}$ , and we set the number of training epochs as 10. As for SegmentEmbedding and SegmentProjection, we implement them by either a linear layer or MLP. Results of deep forecaster are based on the benchmark provided by the TimesNet [43] repository, which is fairly built on the same configurations provided by the original paper. LLM4TS methods [15, 21, 49] are implemented by their official and open-source repository. Unless otherwise specified, we use LLaMA-7B [36] as the default base LLM. We also present the standard deviation of AutoTimes forecasting performance with three random seeds in Table 9, demonstrating that the performance of AutoTimes is stable. ", "page_idx": 14}, {"type": "table", "img_path": "FOvZztnp1H/tmp/b7aaf42c6e71bc49dbc2cbc81e675672aa6c9a19c647468c8c8e0cfe9e7940fb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "FOvZztnp1H/tmp/59b96160a013441f019d1000ba9409d8b5361ab0cab382399d8e93dc7676cfdf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Algorithm 3 AutoTimes - LLM Forecasting ", "page_idx": 14}, {"type": "text", "text": "Require: Input time series $\\{x_{1},\\dots,x_{N_{1}\\times S}\\}$ ; text embeddings {TE1, . . . , TEN1} \u25b7Lookback   \ntoken number $N_{1}\\leq N$   \n1: for $i$ in $\\left\\{1,\\ldots,N_{1}\\right\\}$ :   \n2: r $\\mathbf{s}_{i}=\\{x_{(i-1)S+1},\\ldots,x_{i S}\\}$ $\\begin{array}{r}{\\triangleright\\mathbf{s}_{i}\\in\\mathbb{R}^{S}}\\\\ {\\triangleright\\mathbf{S}\\mathbf{E}_{i}\\in\\mathbb{R}^{D}}\\\\ {\\triangleright\\mathbf{E}_{i}\\in\\mathbb{R}^{D}}\\\\ {\\triangleright\\hat{\\mathbf{E}}_{i}\\in\\mathbb{R}^{D}}\\end{array}$   \n3: $\\mathbf{S}\\mathbf{E}_{i}=\\mathrm{SegmentEmbedding}(\\mathbf{s}_{i})$   \n4: $\\mathbf{E}_{i}=\\mathbf{S}\\mathbf{E}_{i}+\\mathbf{T}\\mathbf{E}_{i}$   \n5: $\\{\\hat{\\mathbf{E}}_{2},\\dotsc,\\hat{\\mathbf{E}}_{N_{1}+1}\\}=\\mathrm{LLMLayers}(\\{\\mathbf{E}_{1},\\dotsc,\\mathbf{E}_{N_{1}}\\})$   \n6: for $i$ in $\\{2,\\ldots,N+1\\}$ :   \n7: for $\\hat{\\mathbf{s}}_{i}=\\mathrm{SegmentProjection}(\\hat{\\mathbf{E}}_{i})$ $\\mathsf{\\Delta}\\mathsf{\\hat{s}}_{i}\\in\\mathbb{R}^{S}$   \n8: Return $\\hat{\\bf s}_{N+1}$ \u25b7Return last token $\\hat{\\mathbf{s}}_{N+1}\\in\\mathbb{R}^{S}$ as the prediction ", "page_idx": 14}, {"type": "text", "text": "Algorithm 4 AutoTimes - Autoregressive Generation ", "page_idx": 15}, {"type": "text", "text": "Require: Input time series $\\{x_{1},\\dots,x_{N_{1}\\times S}\\}$ ; textual embeddings $\\left\\{\\mathbf{TE}_{1},\\dots,\\mathbf{TE}_{N_{2}}\\right\\}$ \u25b7Forecast token number $N_{2}-N_{1}$   \n1: $\\mathbf{x}=\\{x_{1},\\dots,x_{N_{1}\\times S}\\}$   \n2: prediction $=\\{\\}$   \n3: for $i$ in $\\{1,\\dots,N_{2}-N_{1}\\}$ :   \n4: for $\\hat{\\mathbf{y}}=\\mathrm{LLMForecaster}(\\mathbf{x},\\mathbf{T}\\mathbf{E}_{:N_{1}+i-1})$ \u25b7Details in Algorithm 3 5: for $\\mathbf x\\leftarrow\\{\\mathbf x,\\hat{\\mathbf y}\\}$ \u25b7Concatenate for the input for next iteration 6: fo prediction \u2190{prediction, y\u02c6} $\\triangleright$ Record prediction results 7: Return prediction \u25b7Return result \u2208R(N2\u2212N1)\u00d7S ", "page_idx": 15}, {"type": "table", "img_path": "FOvZztnp1H/tmp/d165196da88c8059816cc3764a98d53a76933dfdc52ad6b1553f9015fea4d29d.jpg", "table_caption": ["Table 9: Performance and standard deviations of AutoTimes. Results come from three random seeds. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Hyperparameter Sensitivity ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "SegmentEmbedding and SegmentProjection are uniformly implemented by MLP. The number of layers is fixed as 2 and the hidden dimension is selected from $\\{256,512,1024\\}$ according to the validation loss. The segment length is set as $S\\,=\\,96$ in multivariate datasets and is set as the prediction length $S\\,=\\,F$ in M3 and M4. We verify the robustness of AutoTimes of hyperparameters as follows: the layer number and hidden dimension of SegmentEmbedding and SegmentProjection, context length, and segment length. We observe that AutoTimes is insensitive to the configurations of embedding and projection layers. Besides, performance can be improved by increasing the context length. For long prediction lengths, a larger segment length is favored. ", "page_idx": 15}, {"type": "image", "img_path": "FOvZztnp1H/tmp/1d78a11463543eef6d0e843cafb45a7c4251a56195c6e840a416b5dacb4672b0.jpg", "img_caption": ["Figure 7: Hyperparameter sensitivity of AutoTimes. Each curve presents a specific forecast length. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Supplementary Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Time Series Forecasting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We compare the performance of AutoTimes with state-of-the-art LLM4TS methods and well-acknowledged deep forecasters. Table 11 shows detailed short-term forecast results on M4. Table 10 presents results of the one-for-all forecasting benchmark across ETTh1, ECL, Traffic, Weather, and Solar-Energy datasets. We evaluate all methods by rolling forecasting: each model is trained with input length 672 and output length 96, and the predicted values are integrated as part of the input in the next iteration until reaching the desired forecast horizon. ", "page_idx": 16}, {"type": "text", "text": "Furthermore, the traditional one-for-one approach, where forecasters are trained individually for each prediction length, is also presented in Table 12. The results are reproduced using their corresponding official code. For the sake of rigor, we also provide our reproduced results with the officially reported results in Table 13. ", "page_idx": 16}, {"type": "text", "text": "Additionally, we evaluate AutoTimes along with other baseline models on recent benchmarks [24]. Results are presented in Table 15. We also look forward to evaluating on more diverse benchmarks in the future. ", "page_idx": 16}, {"type": "text", "text": "Table 10: Full long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in $\\{96,1\\bar{9}2,336,72\\bar{0}\\}$ . AutoTimes adapt LLMs with the context length $C=672$ . We set the input length $L=672$ and output length $F=96$ in other methods, which are all implemented by their official code. ", "page_idx": 16}, {"type": "text", "text": "Method AutoTimes TimeLLM [15] UniTime [21] FPT [49] iTrans. [22] DLinear [45] PatchTST [26] TimesNet [42]   \nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE   \n96 0.360 0.400 0.380 0.412 0.386 0.409 0.377 0.404 0.387 0.418 0.369 0.400 0.374 0.401 0.452 0.463   \n192 0.388 0.419 0.408 0.431 0.639 0.577 0.412 0.426 0.416 0.437 0.405 0.422 0.405 0.422 0.474 0.477   \n336 0.401 0.429 0.425 0.443 0.814 0.680 0.440 .434 0.450 0.435 0.445 0.423 0.435 0.493 0.489   \n720 0.406 0.440 0.434 0.463 0.891 0.719 0.488 0.483 0.447 0.473 0.493 0.508 0.434 0.460 0.560 0.534   \nAvg 0.389 0.422 0.412 0.437 0.683 0.596 0.429 0.439 0.421 0.445 0.426 0.444 0.409 0.430 0.495 0.491   \n96 0.129 0.225 0.137 0.244 0.171 0.266 0.137 0.236 0.133 0.229 0.138 0.238 0.132 0.232 0.184 0.288   \n192 0.147 0.241 0.158 0.266 0.293 0.378 0 0.258 0.151 0.245 0.152 0.251 0.151 0.250 0.192 0.295   \n336 0.162 0.258 0.18 0.292 0.379 0. 2 0.167 0.268 0.171 0.272 0.200 0.303   \nE   \n720 0.199 0.288 0.247 0.348 0.455 0.502 0.258 0.355 0.205 0.294 0.203 0.302 0.222 0.318 0.228 0.325   \nAvg 0.159 0.253 0.181 0.288 0.325 0.399 0.184 0.284 0.164 0.258 0.165 0.265 0.169 0.268 0.201 0.303   \n96 0.153 0.203 0.149 0.200 0.180 0.223 0.154 0.205 0.174 0.225 0.169 0.229 0.149 0.202 0.169 0.228   \n192 0.201 0.250 0.193 0.243 0.450 0.451 0.19 7 0.268 0.211 0.268 0.194 0.245 0.222 0.269   \n336 0.256 0.2 .243 0.284 0.594 0.570 0 0 0.309 0.258 0.306 0.244 0.285 0.290 0.310   \n720 0.331 0.345 0.315 0.336 0.618 0.593 0.318 0.335 0.374 0.360 0.320 0.362 0.317 0.338 0.376 0.364   \nAvg 0.235 0.273 0.225 0.266 0.461 0.459 0.228 0.266 0.266 0.291 0.239 0.291 0.226 0.268 0.264 0.293   \n96 0.343 0.248 0.376 0.280 0.438 0.291 0.395 0.283 0.353 0.259 0.399 0.285 0.359 0.255 0.593 0.315   \n192 0.362 0.257 0.397 0.294 0.538 0.353 0.425 0.302 0.373 0.267 0.409 0.290 0.377 0.265 0.596 0.317   \n336 0.379 0.266 0.420 0.311 0.621 0.389 0.463 0.328 0.386 0.275 0.422 0.297 0.393 0.276 0.600 0.319   \n720 0.413 0.284 0.448 0.326 0.737 0.435 0.560 0.392 0.425 0.296 0.461 0.319 0.436 0.305 0.619 0.335   \nAvg 0.374 0.264 0.410 0.303 0.584 0.367 0.461 0.326 0.384 0.274 0.423 0.298 0.391 0.275 0.602 0.322   \n96 0.171 0.221 0.224 0.289 0.223 0.274 0.196 0.261 0.183 0.265 0.193 0.258 0.168 0.237 0.180 0.272   \n192 0.190 0.236 0.248 0.315 0.373 0.431 0.219 0.284 0.205 0.283 0.214 0.274 0.189 0.257 0.199 0.286   \n336 0.203 0.248 0.269 0.338 0.445 0.536 0.245 0.311 0.224 0.299 0.233 0.291 0.212 0.277 0.220 0.301   \n720 0.222 0.262 0.310 0.396 0.526 0.608 0.285 0.354 0.239 0.316 0.246 0.307 0.240 0.305 0.251 0.321   \nAvg 0.197 0.242 0.263 0.335 0.392 0.462 0.236 0.303 0.213 0.291 0.222 0.283 0.202 0.269 0.213 0.295 ", "page_idx": 16}, {"type": "table", "img_path": "FOvZztnp1H/tmp/fa573696a82d5c79f62dcfe6021deb3a85b46dec4b393dacda1a7e9f29e7e778.jpg", "table_caption": ["Table 11: Full results of short-term forecasting. We follow the same protocol of TimesNet [42]. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 12: Long-term forecasting results of one-for-one: AutoTimes trains one LLM-based forecaster to handle all prediction lengths by autoregression, whereas other models are trained respectively on each prediction length. AutoTimes adapts LLMs with the context length $C=672$ . The lookback length is set as $L=672$ in others. All results are averaged. Full results is provided in Table 14. ", "page_idx": 17}, {"type": "text", "text": "Models AutoTimes TimeLLM [15] UniTime [21] FPT [49] iTrans. [22] DLinear [45] PatchTST [26] TimesNet [42] Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1 0.389 0.422 0.409 0.432 0.438 0.445 0.426 0.438 0.438 0.450 0.423 0.437 0.413 0.431 0.458 0.450 ECL 0.159 0.253 0.170 0.275 0.194 0.287 0.167 0.264 0.161 0.256 0.177 0.274 0.159 0.253 0.192 0.295 Weather 0.235 0.273 0.227 0.266 0.260 0.283 0.231 0.269 0.238 0.272 0.240 0.300 0.226 0.264 0.259 0.287 Traffic 0.374 0.264 0.402 0.294 0.460 0.301 0.416 0.295 0.379 0.272 0.434 0.295 0.391 0.264 0.620 0.336 Solar. 0.197 0.242 0.234 0.293 0.254 0.291 0.229 0.296 0.202 0.269 0.217 0.278 0.189 0.257 0.200 0.268 ", "page_idx": 17}, {"type": "text", "text": "Table 13: Results of LLM4TS methods from the original paper and our reproduction by official code. ", "page_idx": 17}, {"type": "table", "img_path": "FOvZztnp1H/tmp/7e1b048d8770e59d28badf9a5ab8f3c7d8b8fcf956ead728844408b5facec425.jpg", "table_caption": [], "table_footnote": ["1 Methods with \u2217means results from the original paper; without \u2217means the reproduction. 2 \u201c-\u201d indicates that results are not reported in the original paper. "], "page_idx": 17}, {"type": "text", "text": "Table 14: Full long-term forecasting results of one-for-one: AutoTimes trains one LLM-based forecaster to handle all prediction lengths by autoregression, whereas other models are trained respectively on each prediction length. AutoTimes adapt LLMs with the context length $C=672$ . The lookback length is set as $L=672$ in others, which are all implemented by their official code. ", "page_idx": 18}, {"type": "table", "img_path": "FOvZztnp1H/tmp/0c82b8113aa219241605d923850a00568043cf3bd2399adcf19db072f06c649e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "FOvZztnp1H/tmp/405c3f8cfdd3b7faeabbb67cbbfb2cc9f9a0aeb68c7746053e14a07cfac8d45f.jpg", "table_caption": ["Table 15: Forecasting results on additional benchmark datasets [24] (672-pred-96). "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.2 Zero-Shot Forecasting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Following the zero-shot forecasting of FPT [15], each experiment comprises the source and target datasets. We train a model on the source dataset and apply the model on the target dataset for predictions directly. ", "page_idx": 19}, {"type": "text", "text": "Notably, the zero-shot scenarios are conducted respectively on the subsets (e.g. M4 Monthly $\\rightarrow\\mathbf{M}3$ Monthly) and the subsets are divided by the sampling frequency but follow different distributions [25]. ", "page_idx": 19}, {"type": "text", "text": "For $\\mathbf{M4}\\rightarrow\\mathbf{M}3$ , which means training on M4 and testing on M3, we directly utilize the same model in the short-term forecasting experiments reported in Table 11. Considering different horizons in subsets, for M3 Yearly, M3 Quarterly, and M3 Monthly, we directly employ models trained on corresponding subsets of M4 for testing. As for M3 Others, we test using the model trained on M4 Quarterly to keep the same horizon. ", "page_idx": 19}, {"type": "text", "text": "For $\\mathbf{M}3\\rightarrow\\mathbf{M}4$ , similarly, for M4 Yearly, M4 Quarterly, and M4 Monthly, we directly employ models trained on corresponding subsets of M3 for testing. For the remaining subsets, M4 Weekly, M4 Daily, and M4 Hourly, we perform inference using the model trained on M3 Monthly. Table 16 shows the detailed result. ", "page_idx": 19}, {"type": "text", "text": "Table 16: Results of zero-shot forecasting. We adopt the same protocol as FPT [49]. $\\mathrm{M}4\\rightarrow\\mathrm{M}3$ means training forecasters on M4 datasets and evaluating the performance on M3, and vice versa. Results of compared baselines are reported from FPT [49]. Lower SMAPE indicates better performance. ", "page_idx": 19}, {"type": "table", "img_path": "FOvZztnp1H/tmp/33be9776912613b5f48f57e82163a97540800dd3ee2788bc9597e7d89133c20b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D.3 Method Generality ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Mainstream LLMs predominantly adopt the decoder-only architecture, and AutoTimes can utilize any decoderonly LLM. We conduct experiments on various types and sizes of LLMs, including GPT-2 [31], multiple sizes of OPT [46], and LLaMA [36]. Detailed configurations and results are shown in Table 17 and 18, demonstrating a general trend where performance improves as the model size increases, consistent with the scaling law [16]. ", "page_idx": 19}, {"type": "table", "img_path": "FOvZztnp1H/tmp/2b55281fbe11f24758da47cdcbf0c9db289b9207f4232188c2864fbe64cb935c.jpg", "table_caption": ["Table 17: Detailed method configurations of AutoTimes for alternative language models. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D.4 Variable Lookback Length ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the conventional forecasting paradigm, deep forecasters are trained respectively on lookback/forecast lengths, limiting their applicability to a single lookback length. In contrast, LLMs have the versatility to handle various input lengths. This capability is derived from Rotary Position Embedding [33] and the next toke prediction objective, where LLMs are trained with token-wise supervision in Equation 8, that is, the generated token at each position is supervised. While non-autoregressive LLM4TS methods are typically constrained to a fixed lookback setting, AutoTimes with a consistent training objective has the flexibility to deal with different lookback lengths. We present the results in Figure 8, where we adapt the LLM by AutoTimes with the context length of $C=672$ , and evaluate the performance with different lookback lengths, which demonstrates the inherited versatility of LLM-based forecasters. Moreover, the performance is generally improving with increased available lookback observations, leading to an averaged $9.3\\%$ promotion from 384 to 672. By contrast, several works have observed that the performance of respectively trained deep forecasters does not necessarily improve with the increasing of lookback length [22, 26, 45]. ", "page_idx": 19}, {"type": "table", "img_path": "FOvZztnp1H/tmp/b1859ec873fd05691218153f19e2f0c36a2d9af304e8132c435b33697ab987e5.jpg", "table_caption": ["Table 18: Full Results of alternative LLMs, which are adapted with the context length $C=672$ . "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "FOvZztnp1H/tmp/ef8bd4f3ae553960d5c4aed4c3070d8f51a2f80d7a27229aba70290d7a150b61.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 8: Performance of LLM-based forecasters on the pred-96 scenario, which are adapted by AutoTimes by the context length $C=672$ and directly applied on different lookback lengths. ", "page_idx": 20}, {"type": "text", "text": "D.5 Timestamps as Position Embeddings ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We conduct an ablation study on the proposed position embeddings that integrate timestamps, a prevalent textual covariate in real-world applications. As shown in Figure 9, the forecasting performance is consistently promoted across all multivariate datasets and prediction lengths. The steady improvement can be attributed to the fact that timestamps demote the absolute position of time series segments on the timeline, explicitly aligning different variates in multivariate scenarios. The increasing promotion with longer prediction length also implies that chronological information, such as date and periodicity, yields significant benefits for long-term forecasting. ", "page_idx": 20}, {"type": "image", "img_path": "FOvZztnp1H/tmp/293f6ddbb5935e99d09d77f494b91f9d86c7c3f1132dd0ac45be9843ab29c36f.jpg", "img_caption": ["Figure 9: Ablation on whether to utilize textual timestamps as the position embedding. Results of different prediction lengths are provided, where the embedding leads to consistent performance promotion across all datasets, and the promotion can increase with a longer prediction length. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.6 In-Context Forecasting ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For in-context forecasting, similar to zero-shot forecasting in Appendix D.2, we train our model using the source dataset and directly evaluate it on the target dataset. In this task, we first choose M4 as the source dataset and M3 as the target dataset. It is important to note that the structure of the M3 and M4 datasets differs from typical datasets used for long-term forecasting. They consist of multiple univariate time sequences of different lengths. The final part of each sequence serves as the test set, while the preceding part is used for training. ", "page_idx": 21}, {"type": "text", "text": "Implementation In zero-shot scenarios, we use a sequence of length $F$ preceding and consecutive to the test set as input, referred to as the lookback window, where $F$ is the forecast length of each subset. During in-context forecasting, we concatenate the first $2F$ time points that belong to the same sequence with the lookback window as input. We aim to enhance prediction performance by incorporating more contextual information. Too short sequences $(\\leq4F)$ are discarded to prevent overlap between the prompt and the lookback window. For a fair comparison, both zero-shot and in-context forecasting performance are reported on the same remaining sequences. Figure 12 provides showcases of zero-shot and in-context forecasting. ", "page_idx": 21}, {"type": "text", "text": "Prompt engineering Regarding in-context learning, we further delve into the effect of different strategies to retrieve time series as prompts, which is provided in Table 19. P.1 and P.2 correspond to the zero-shot and in-context forecasting evaluated in Section 4.3. The prompt of P.3 contains the last $2F$ time points preceding the beginning of the lookback window. Another retrieval of prompts as P.4, adopts time series that come from another uncorrelated time series (out-of-series). We can obtain the following observations: ", "page_idx": 21}, {"type": "text", "text": "\u2022 P.1 v.s. P.4 indicates that the selected prompt is not suitable, since the prompt does not come from the earlier observations of the same series to be predicted. Although the context window becomes larger, the averaged performance will deteriorate because of irrelevant prompts.   \n\u2022 P.2 and P.3 indicates that in most cases, selecting the relevant $2F$ time series from the same series can provide better contextual information. ", "page_idx": 21}, {"type": "text", "text": "This highlights the prompt engineering for in-context forecasting. An intuitive suggestion is to utilize consecutive, inter-periodic, and multiple prompts. To verify this idea, we analyze the periodic effect of time series prompts. ", "page_idx": 21}, {"type": "table", "img_path": "FOvZztnp1H/tmp/7e9704054c098680d272e9f3e7637487da3fc7f0e1a1c03be5853deb7ec39cc6.jpg", "table_caption": ["Table 19: Effects of different strategies to retrieve time series as prompts for in-context forecasting. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "In previous experiments, we adopt M3 and M4 datasets, which are consistent with the zero-shot experiment of FPT [49], to present the promotion of our in-context paradigm. To provide more rigorous conclusions, we extend the evaluation to widely recognized datasets. Details of the experiment are as follows: By using a trained model checkpoint on a source domain (Traffic), we conduct forecasting without gradient update on target ETT datasets. We evaluate the pred-96 performance on the last variate (OT). For the zero-shot scenario, the input is length-288 lookback series. For in-context forecasting, the input is (length-384 series prompt $^+$ length-288 lookback series). Considering the dataset periodicity, the prompt is uniformly selected as the Ahead-24 (one-day-ahead) series of the original lookback series. To eliminate the performance boost that comes from extending the input length, we also provide the results of length-672 lookback series in the zero-shot scenario. Moreover, we further delve into the effect of different strategies to select time series prompts: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Ahead-Period: The prompt is uniformly selected as the Ahead-24 series of the original lookback series where 24 is one of the periods (daily period) of ETT.   \n\u2022 Ahead-Random: The prompt is randomly selected as the previous series of the original series.   \n\u2022 Fixed Prompt: The prompt is fixed as the first 384 time points in the variate-OT.   \n\u2022 Other Variate: The prompt is uniformly selected as Ahead-24 series, but comes from other variates. ", "page_idx": 21}, {"type": "text", "text": "Results in Table 20 demonstrate the effectiveness of using suitable time series prompts and highlight the influence of prompt engineering. Using inter-period prompts can outperform simply extending lookback window. ", "page_idx": 21}, {"type": "text", "text": "The benefti of the proposed in-context forecasting is to extend the input context of time series forecasting beyond a continuous lookback window. Since the essence of prompts is to incorporate useful domain-specific knowledge, here is one use case of in-context forecasting: Considering predicting the weather of one day, one approach is to extend the lookback length from days to weekends. However, it can also introduce noisy information since ", "page_idx": 21}, {"type": "table", "img_path": "FOvZztnp1H/tmp/809f951ea202137562bd02db84aec7efc12f6776deb6715a41956a2a86f73c0c.jpg", "table_caption": ["Table 20: Strategies to select time series prompts based on periodicity for in-context forecasting. "], "table_footnote": ["non-stationary meteorological conditions can change with seasons. Another practical way is to consider how the weather changes on the same day in the last year (or years). Although the input is not continuous, the input context becomes more relevant based on prior knowledge about the periodicity (yearly). Therefore, in-context forecasting makes prior knowledge incorporatable and gets performance promotion. "], "page_idx": 22}, {"type": "text", "text": "D.7 Ablation Study ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In addition to the ablation study of whether LLMs are useful in AutoTimes (Table 6), we further delve into the main difference between our method and previous LLM4TS approach and provide a comprehensive ablation study. The results presented in Table 21 demonstrate that the performance of non-autoregression projection is consistently inferior to that of our autoregressive AutoTimes approach. ", "page_idx": 22}, {"type": "text", "text": "Table 21: Ablation study of the autoregression. FlattenHead replaces the segment-wise projection of AutoTimes by flatten and linear head [26], which is prevalent in non-autoregressive forecasters. ", "page_idx": 22}, {"type": "table", "img_path": "FOvZztnp1H/tmp/442564c26e7113719a6a909a05f92576ab3d4af752f76fc0bd92f63192e4965c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E Showcases ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To facilitate a clear comparison among various models, we present additional prediction showcases for long-term forecasting and short-term forecasting. These examples are provided by the following models: TimeLLM [15], FPT [49] and PatchTST [26]. Of all the models, AutoTimes delivers the most accurate future series predictions. Additionally, we provide the showcases of zero-shot and in-context forecasting in Figure 12. ", "page_idx": 22}, {"type": "image", "img_path": "FOvZztnp1H/tmp/7ce6572b6af1d918eb7343356719b82f104dfa4443786b5c818a5dfe20310272.jpg", "img_caption": ["Figure 10: Visualization of input-672-predict-96 results on the Traffic dataset. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "FOvZztnp1H/tmp/41cf1fc71db3220a93fdc0a5e953c04cae82d865b86a7e186f29ee8ca1e7fecf.jpg", "img_caption": ["Figure 11: Visualization of input-36-predict-18 results on the M4 Monthly dataset. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "FOvZztnp1H/tmp/0829f071e0ffd6b1ec1f90c6f9b9c874d231dc43ec40cdd78a132a8fd3ddfa5a.jpg", "img_caption": ["Figure 12: Showcases of zero-shot and in-context forecasting. For in-context forecasting, beyond the lookback window, we uniformly adopt the first $2F$ time points that belong to the same sequence as the prompt and concatenate them as the prediction context, which achieves a more accurate prediction. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "F Broader Impact ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "F.1 Impact on Real-world Applications ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This paper copes with general-purpose time series forecasting, which is faced with increasing challenges such as the versatility to handle variable-length scenarios, good generalizability with scarce samples, utilization of multimodality, and instructive downstream prompts. Since previous studies have demonstrated the feasibility of leveraging large language models for time series, we propose a simple but effective approach as AutoTimes to obtain LLM-based forecasters, which keeps the consistency of autoregression. Our model achieves state-ofthe-art performance on forecasting benchmarks and demonstrates remarkable adaptation speed and parameter efficiency. Besides, advanced capabilities such as multi-step generation and in-context learning are inherited by the repurposed forecaster. Therefore, the proposed method makes it promising to tackle real-world applications, ", "page_idx": 23}, {"type": "text", "text": "which helps our society prevent risks in advance and make better decisions with limited computational budgets.   \nOur paper mainly focuses on scientific research and has no obvious negative social impact. ", "page_idx": 24}, {"type": "text", "text": "F.2 Impact on Future Research ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this paper, we find prevalent non-autoregressive LLM4TS methods have inconsistencies in the model structure and generative approach with LLMs, leading to insufficient utilization of the inherent multi-step token transition. Given that the generalizability and generative ability of LLMs are largely derived from the autoregressive manner, the potentials of LLMs may not be fully exhibited in time series forecasting. Therefore, we propose to adapt LLMs by the consistent training objective, the next token prediction, and accomplish arbitrary-length forecasting by iterative generation. Beyond the conventional forecasting paradigm, we propose in-context forecasting, where the context for prediction is extended, and earlier historical time series can be utilized as advantageous prompts. The compatibility with LLMs and insights from autoregression can be instructive for future LLM4TS research and the development of foundation time series models. ", "page_idx": 24}, {"type": "text", "text": "G Limitation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The proposed method has not supported probabilistic forecasting, since AutoTimes only establishes the mapping between time series segments to latent embeddings of the LLM, instead of discrete language tokens. Advanced low-rank adaptation is under exploration in our work, which can further align suitable token transitions as the future extrapolation of time series. More deftly designed embedding and projection layers are underexplored to support more compatible tokenization for time series. Besides, it is fascinating to apply AutoTimes on real-world multimodal time series datasets (such as news-stocks, and logs-measurements), which leaves our future work. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please refer to Section 1 of the main text, where the claims and contributions are included. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please refer to Section G of Appendix, where we provide several aspects of limitations. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Please refer to Section B in the main text and code in our public repository, including the detailed configurations of experiments and the scripts for reproduction. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Please refer to Section A of Appendix and code in our public repository, including dataset descriptions and how to access them. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please refer to Section C and D of Appendix, where we state how hyperparameters are chosen and the detailed description of experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please refer to Section B and Table 9 of Appendix, where we report standard deviations of results with three random seeds. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please refer to Section B of Appendix, where we provide sufficient information on the computing resource. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We have reviewed and the reasearch conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Please refer to Section F of Appendix, where we discuss societal impacts and influences on future research. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All creators of datasets are properly credited by citations in Section A. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]