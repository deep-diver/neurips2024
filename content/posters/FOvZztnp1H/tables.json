[{"figure_path": "FOvZztnp1H/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of LLM4TS methods: Autoregressive categorizes LLM-based forecasters by whether to conduct autoregression. Freeze LLM enables quick adaptation, which would otherwise require significant resources for fine-tuning. Multimodal refers to the utilization of information from other modalities. Prior to AutoTimes, none of the LLM4TS methods achieved all three.", "description": "This table compares several Large Language Model for Time Series (LLM4TS) methods based on three key aspects: whether they are autoregressive, whether they freeze the pre-trained Large Language Model (LLM), and whether they are multimodal.  Autoregressive models generate predictions sequentially, using previous predictions to inform current ones. Freezing the LLM means that only a small set of parameters are trained, reducing computational cost. Multimodal methods incorporate data from sources other than just the time series, like textual descriptions or images. The table highlights that AutoTimes is unique in combining all three of these characteristics.", "section": "2 Related Work"}, {"figure_path": "FOvZztnp1H/tables/tables_6_1.jpg", "caption": "Table 2: Average short-term forecasting results on the M4 [25]. Full results are provided in Table 11.", "description": "This table presents the average results of short-term time series forecasting on the M4 dataset.  It compares the performance of AutoTimes against several other state-of-the-art methods, including TimeLLM, FPT, Koopa, N-HiTS, DLinear, PatchTST, TimesNet, FiLM, and N-BEATS.  The metrics used for comparison are SMAPE (Symmetric Mean Absolute Percentage Error), MASE (Mean Absolute Scaled Error), and OWA (Overall Weighted Average).  Detailed results for each method on each time series within the M4 dataset are available in Table 11.", "section": "4.1 Time Series Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_6_2.jpg", "caption": "Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in {96, 192, 336, 720}. AutoTimes adapt LLMs with the context length C = 672. We set the input length L = 672 and output length F = 96 in other methods. All results are averaged. Full results is provided in Table 10.", "description": "This table presents the results of long-term time series forecasting experiments using a \"one-for-all\" approach.  A single model is trained on each dataset and then used to generate forecasts of different lengths (96, 192, 336, and 720 time steps).  The AutoTimes model uses a context length of 672 time steps, while other methods use an input length of 672 and an output length of 96. The table shows the average results across all forecast lengths, with more detailed results in Table 10.", "section": "4.1 Time Series Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_6_3.jpg", "caption": "Table 4: Zero-shot forecasting results in averaged SMAPE. M4 \u2192 M3 trains forecasters on the datasets of M4 and evaluates on M3, and vice versa. Detailed results are provided in Appendix D.2", "description": "This table presents the results of zero-shot forecasting experiments.  The models were trained on either the M4 or M3 dataset and then tested on the other dataset, to evaluate their ability to generalize to unseen data.  The results are presented as the average Symmetric Mean Absolute Percentage Error (SMAPE), a common metric for evaluating time series forecasting accuracy. The table includes results for AutoTimes, and several baseline models (FPT, DLinear, PatchTST, TimesNet, NSFormer, FEDFormer, Informer, Reformer).  Appendix D.2 provides more detailed results.", "section": "4.2 Zero-Shot Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_8_1.jpg", "caption": "Table 5: Averaged results of alternative language models. Full results are provided in Table 18.", "description": "This table presents the Mean Squared Error (MSE) and Mean Absolute Error (MAE) achieved by the AutoTimes model when using different large language models (LLMs) as the backbone.  The LLMs tested include GPT-2 (124M), OPT-350M, OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B. The results are averaged across several datasets and prediction horizons.  The full results for each dataset and horizon can be found in Table 18.", "section": "Method Analysis"}, {"figure_path": "FOvZztnp1H/tables/tables_9_1.jpg", "caption": "Table 6: We follow the protocol of LLM4TS ablation studies [35] to verify whether the LLM is truly useful in our AutoTimes: (1) w/o LLM replaces the language model entirely and passing input tokens directly to the last layer; (2) LLM2Attn replaces the language model with a single multi-head attention layer; (3) LLM2Trsf replaces the language model with a single transformer block.", "description": "This table presents the ablation study results to verify the effectiveness of using LLMs in the AutoTimes model.  Three variations of the model are compared against the baseline AutoTimes model: one without the LLM (w/o LLM), one using only a multi-head attention layer (LLM2Attn), and one using a single transformer block (LLM2Trsf). The results, measured by MSE and MAE, are shown for ETTh1 and ECL datasets for four different prediction horizons (Pred-96, Pred-192, Pred-336, Pred-720). This allows for a comparison of the performance impact of using different levels of the LLMs within the AutoTimes architecture.", "section": "4.4 Method Analysis"}, {"figure_path": "FOvZztnp1H/tables/tables_9_2.jpg", "caption": "Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in {96, 192, 336, 720}. AutoTimes adapt LLMs with the context length C = 672. We set the input length L = 672 and output length F = 96 in other methods. All results are averaged. Full results is provided in Table 10.", "description": "This table presents the results of long-term time series forecasting experiments using a one-for-all approach, where a single model is trained on each dataset and used to make predictions for multiple forecast lengths (96, 192, 336, and 720).  The AutoTimes model utilizes LLMs with a context length of 672, while other methods use an input length of 672 and an output length of 96.  The table shows the average results across all forecast lengths, with complete results available in Table 10.", "section": "4.1 Time Series Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_13_1.jpg", "caption": "Table 8: Detailed dataset descriptions. Dim denotes the variate number. Dataset Size denotes the total number of time points in (Train, Validation, Test) splits respectively. Forecast Length denotes the future time points to be predicted. Frequency denotes the sampling interval of time points.", "description": "This table details the characteristics of the datasets used in the paper's experiments.  It lists the name of each dataset, the number of variables (Dim), the forecast lengths considered, the total number of data points in the training, validation, and testing sets, the sampling frequency (e.g., hourly, daily), and a brief description of the data's information.", "section": "4.1 Time Series Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_14_1.jpg", "caption": "Table 18: Full Results of alternative LLMs, which are adapted with the context length C = 672.", "description": "This table presents the complete results of using different LLMs in the AutoTimes model.  The context length is fixed at 672 for all experiments. The table shows the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for different forecast horizons (96, 192, 336, and 720) across various datasets (ECL, ETTh1, Traffic, and Weather).  The results allow for a comparison of model performance across different LLMs of varying sizes.", "section": "D.5 Timestamps as Position Embeddings"}, {"figure_path": "FOvZztnp1H/tables/tables_14_2.jpg", "caption": "Table 9: Performance and standard deviations of AutoTimes. Results come from three random seeds.", "description": "This table shows the performance of the AutoTimes model in terms of Mean Squared Error (MSE) and Mean Absolute Error (MAE) for four different forecasting horizons (96, 192, 336, and 720).  The results are presented for four different datasets (ETTh1, ECL, Weather, and Traffic). For each dataset and horizon, the table provides the average MSE and MAE along with their standard deviations, calculated across three independent runs with different random seeds. This demonstrates the stability and robustness of AutoTimes.", "section": "4 Experiments"}, {"figure_path": "FOvZztnp1H/tables/tables_15_1.jpg", "caption": "Table 9: Performance and standard deviations of AutoTimes. Results come from three random seeds.", "description": "This table presents the mean and standard deviation of the MSE and MAE metrics for the AutoTimes model across different datasets (ETTh1, ECL, Weather, Traffic, Solar-Energy) and forecasting horizons (96, 192, 336, 720).  The results are averaged across three different random seeds to show the model's stability and reliability. Lower MSE and MAE values indicate better forecasting performance.", "section": "4 Experiments"}, {"figure_path": "FOvZztnp1H/tables/tables_17_1.jpg", "caption": "Table 2: Average short-term forecasting results on the M4 [25]. Full results are provided in Table 11.", "description": "This table presents the average results of short-term time series forecasting on the M4 benchmark dataset.  It compares the performance of AutoTimes against several state-of-the-art forecasting methods, including TimeLLM, FPT, Koopa, N-HiTS, DLinear, PatchTST, TimesNet, FiLM, and N-BEATS, across three evaluation metrics: SMAPE, MASE, and OWA. The full, detailed results for each method can be found in Table 11.", "section": "4.1 Time Series Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_17_2.jpg", "caption": "Table 13: Results of LLM4TS methods from the original paper and our reproduction by official code.", "description": "This table compares the results of several Large Language Model for Time Series (LLM4TS) methods reported in their original papers and the results reproduced by the authors of the current paper using the official code of those methods. The goal is to provide a reliable comparison of the performance of different methods on benchmark datasets.", "section": "4.1 Time Series Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_18_1.jpg", "caption": "Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in {96, 192, 336, 720}. AutoTimes adapt LLMs with the context length C = 672. We set the input length L = 672 and output length F = 96 in other methods. All results are averaged. Full results is provided in Table 10.", "description": "This table presents the results of long-term time series forecasting experiments using a one-for-all approach.  A single model is trained on each dataset and then used to predict four different forecast lengths (96, 192, 336, and 720 time steps). AutoTimes uses a context length of 672, while other methods use an input length of 672 and an output length of 96. The table shows the average results across all four forecast lengths, with complete results available in Table 10. This setup tests the model's ability to generalize to different prediction horizons without retraining.", "section": "4.1 Time Series Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_18_2.jpg", "caption": "Table 15: Forecasting results on additional benchmark datasets [24] (672-pred-96).", "description": "This table presents the results of forecasting experiments on three additional benchmark datasets, namely Australian Electricity, Bdg-2 Panther, and Oikolab Weather.  The experiments use a lookback length of 672 time steps and predict the next 96 time steps.  The table compares the performance of AutoTimes against three other state-of-the-art forecasting models: PatchTST, iTransformer, and DLinear. The metrics used for comparison are Mean Squared Error (MSE) and Mean Absolute Error (MAE).", "section": "4 Experiments"}, {"figure_path": "FOvZztnp1H/tables/tables_19_1.jpg", "caption": "Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in {96, 192, 336, 720}. AutoTimes adapt LLMs with the context length C = 672. We set the input length L = 672 and output length F = 96 in other methods. All results are averaged. Full results is provided in Table 10.", "description": "This table presents the results of long-term forecasting experiments using a one-for-all approach.  A single model is trained on each dataset and used to predict four different forecast lengths (96, 192, 336, and 720 time steps). AutoTimes uses a context length of 672, while other methods use an input length of 672 and an output length of 96. The table shows the average results across all forecast lengths, with full details available in Table 10.", "section": "4.1 Time Series Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_19_2.jpg", "caption": "Table 17: Detailed method configurations of AutoTimes for alternative language models.", "description": "This table shows the different configurations used for AutoTimes when using different base LLMs.  It lists the base large language model (LLM) used, the hidden dimension of the model's layers, the type of embedding used (2-layer MLP or Linear), and the number of trainable parameters (in millions) for each configuration. The variations demonstrate the adaptability of AutoTimes to various LLMs.", "section": "D.3 Method Generality"}, {"figure_path": "FOvZztnp1H/tables/tables_20_1.jpg", "caption": "Table 5: Averaged results of alternative language models. Full results are provided in Table 18.", "description": "This table presents the results of experiments using different large language models (LLMs) for time series forecasting.  The models tested include GPT-2 (124M), OPT-350M, OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B. The metrics used to evaluate performance are Mean Squared Error (MSE) and Mean Absolute Error (MAE) for the 96, 192, 336, and 720 time steps ahead.  The table shows average performance across these metrics and time steps.  Full details are available in Table 18.", "section": "Method Analysis"}, {"figure_path": "FOvZztnp1H/tables/tables_21_1.jpg", "caption": "Table 19: Effects of different strategies to retrieve time series as prompts for in-context forecasting.", "description": "This table presents the results of experiments using different strategies for selecting time series prompts in in-context forecasting. Four different strategies are compared: (P.1) using only the lookback window, (P.2) combining the lookback window with a prompt from the first 2F time points of the series, (P.3) combining the lookback window with a prompt from the last 2F time points of the series, and (P.4) combining the lookback window with a prompt from 2F time points of another series. The results are presented in terms of averaged error for four different time series frequencies (Yearly, Quarterly, Monthly, Others). Strategy P.2 and P.3 show improvement over P.1, while P.4 shows negative results as expected.", "section": "4.3 In-Context Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_22_1.jpg", "caption": "Table 20: Strategies to select time series prompts based on periodicity for in-context forecasting.", "description": "This table presents the results of an ablation study on different prompt selection strategies for in-context forecasting.  The study compares the performance of using different methods to select the time series prompts that are concatenated with the lookback window to form the context for prediction. The strategies include using the first 2F time points, the last 2F time points, randomly selected time points, and time points from other uncorrelated time series. The average error for each strategy is reported, illustrating the significant impact of prompt engineering on forecasting performance and highlighting the importance of using relevant and periodic time series prompts.", "section": "D.6 In-Context Forecasting"}, {"figure_path": "FOvZztnp1H/tables/tables_22_2.jpg", "caption": "Table 21: Ablation study of the autoregression. FlattenHead replaces the segment-wise projection of AutoTimes by flatten and linear head [26], which is prevalent in non-autoregressive forecasters.", "description": "This table presents the ablation study comparing the performance of AutoTimes with a variant called \"FlattenHead\".  FlattenHead replaces the segment-wise projection used in AutoTimes with a simpler flatten linear head, a common approach in non-autoregressive forecasting models. The results demonstrate that the performance of the non-autoregressive method (FlattenHead) is consistently inferior to the autoregressive approach (AutoTimes), highlighting the importance of AutoTimes' autoregressive design for better performance.", "section": "D.7 Ablation Study"}]