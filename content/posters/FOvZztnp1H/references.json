{"references": [{"fullname_first_author": "Yoshua Bengio", "paper_title": "A neural probabilistic language model", "publication_date": "2000", "reason": "This paper is foundational to the field of neural language models, which are leveraged in this paper's approach to time series forecasting."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020", "reason": "This paper demonstrates the few-shot learning capabilities of large language models, which are crucial for AutoTimes' efficient adaptation to time series forecasting."}, {"fullname_first_author": "George EP Box", "paper_title": "Time series analysis: forecasting and control", "publication_date": "2015", "reason": "This is a highly influential textbook in time series analysis, providing the foundational concepts and methods that AutoTimes builds upon."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019", "reason": "This paper highlights the multitask learning capabilities of large language models, which allows AutoTimes to effectively adapt to time series forecasting with minimal fine-tuning."}, {"fullname_first_author": "Haixu Wu", "paper_title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting", "publication_date": "2021", "reason": "This paper introduces Autoformer, a state-of-the-art model for long-term time series forecasting that AutoTimes is compared against, demonstrating the effectiveness of the proposed approach."}]}