[{"type": "text", "text": "The Map Equation Goes Neural: Mapping Network Flows with Graph Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Christopher Bl\u00f6cker\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chester Tan ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data Analytics Group Department of Informatics   \nUniversity of Zurich, Switzerland   \nchristopher.bloecker@uzh.ch ", "page_idx": 0}, {"type": "text", "text": "Chair of Machine Learning for Complex Networks Center for Artificial Intelligence and Data Science Julius-Maximilians-Universit\u00e4t W\u00fcrzburg, Germany chester.tan@uni-wuerzburg.de ", "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ingo Scholtes\u2020   \nChair of Machine Learning for Complex Networks   \nCenter for Artificial Intelligence and Data Science   \nJulius-Maximilians-Universit\u00e4t W\u00fcrzburg, Germany ingo.scholtes@uni-wuerzburg.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Community detection is an essential tool for unsupervised data exploration and revealing the organisational structure of networked systems. With a long history in network science, community detection typically relies on objective functions, optimised with custom-tailored search algorithms, but often without leveraging recent advances in deep learning. Recently, first works have started incorporating such objectives into loss functions for deep graph clustering and pooling. We consider the map equation, a popular information-theoretic objective function for unsupervised community detection, and express it in differentiable tensor form for optimisation through gradient descent. Our formulation turns the map equation compatible with any neural network architecture, enables end-to-end learning, incorporates node features, and chooses the optimal number of clusters automatically, all without requiring explicit regularisation. Applied to unsupervised graph clustering tasks, we achieve competitive performance against state-of-the-art deep graph clustering baselines in synthetic and real-world datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many real-world networked systems are organised in communities: groups of nodes that are more similar to each other than to the rest. Communities provide insights into network structure at the mesoscale, revealing sub-systems by analysing link patterns. Motivated by different research questions, several characterisations of what constitutes \u201cgood\u201d communities have been proposed [1, 2], however, neither of them is fundamentally more correct than any other. Moreover, no single community-detection method outperforms all others on any given network [3], motivating the ongoing efforts of research on community detection. Typically, community-detection approaches formulate an objective function that calculates a quality score for a given partition of the network\u2019s nodes into communities. Finding the best partition is an NP-hard search problem and often involves custom heuristic algorithms that attempt to minimise their objective function [4, 5, 6]. ", "page_idx": 0}, {"type": "text", "text": "Graph neural networks (GNNs) have enabled applying deep learning to graph-structured data by utilising the input graph as the neural network\u2019s computational graph [7, 8, 9]. Typical tasks for GNNs include node labelling, graph labelling, and link prediction, all of which involve learning meaningful representations jointly from the graph\u2019s topology, the nodes\u2019 features, and, possibly, the edges\u2019 features. Graph labelling relies on coarse-graining the graph through identifying groups of \u201csimilar\u201d nodes and aggregating their links and features, also referred to as pooling [10, 11, 12], which is related to graph clustering [13], however, these two tasks have different goals. ", "page_idx": 1}, {"type": "text", "text": "While GNNs excel at incorporating node and edge features with graph topology, including this information is also possible but more challenging with traditional network science approaches, typically requiring modelling or adjusting objective functions and their optimisation algorithms. On the other hand, objective functions for community detection provide precise interpretations as to why one partition is considered better than another while deep-learning-based approaches are black boxes. Model selection in deep learning is often done through regularisation techniques or cross-validation; in contrast, objective functions that are based on the minimum description length (MDL) principle naturally implement Occam\u2019s razor, preventing overfitting and enabling principled model selection without requiring extra regularisation or cross-validation [14, 15]. ", "page_idx": 1}, {"type": "text", "text": "Here, we combine the beneftis of traditional community-detection approaches and deep learning and consider the map equation, an information-theoretic objective function for community detection [16]. By adapting the map equation for soft cluster assignments and implementing it in differentiable tensor form, we enable end-to-end optimisation of the map equation as a loss function with gradient descent and GNNs. In analogy to the map equation\u2019s stochastic optimisation algorithm Infomap [6], we call our approach Neuromap and evaluate it against Infomap and several recent GNN-based graph clustering methods. Applied to synthetic and real-world networks, Neuromap demonstrates competitive performance against recent deep graph clustering baselines. ", "page_idx": 1}, {"type": "text", "text": "Our key contributions can be summarised as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We adapt the map equation as a differentiable loss function for end-to-end deep graph clustering and propose Neuromap, a deep-learning-based alternative to the popular Infomap algorithm for unsupervised community detection with the map equation. Neuromap is compatible with any neural network architecture, detects overlapping communities, leverages node features for improved performance on real-world networks, and, by following the minimum description length principle, does not require explicit regularisation.   \n2. We extensively evaluate Neuromap on hundreds of synthetic and ten real datasets against recent baselines paired with various neural network architectures. Neuromap outperforms the baseline on the synthetic networks in most settings and is amongst the best performers in seven out of ten real datasets.   \n3. By choosing a higher maximum number of clusters than previous works, we show empirically that recent baselines tend to overfti and report considerably more than the ground-truth number of communities. Moreover, we find that choosing a small maximum number of communities is often detrimental to graph clustering performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Community detection. Communities, also called clusters or modules, are groups of nodes that are more \u201csimilar\u201d to each other than to the rest, often understood as having more links inside than between groups [1, 2]. However, this rather general characterisation leaves precise details of what constitutes a community open. Modularity compares the observed link densities inside communities against a randomised version of the network [17]. The stochastic block model and its variants assume a latent block structure where the probability that two nodes are connected depends only on their block memberships [18, 19]. The map equation identifies communities as regions where a random walker tends to stay for a relatively long time [16, 20]. Traditional clustering approaches, such as $\\mathbf{k}\\cdot$ -means, group nodes based on their proximity in space, however, here we consider identifying communities from the link patterns in networked systems. For a detailed overview of community detection in complex networks, we refer to [1, 2]. ", "page_idx": 1}, {"type": "text", "text": "Minimum description length principle. The minimum description length principle (MDL) is a model-selection approach that formalises Occam\u2019s razor and frames learning as a compression problem [14, 15]. MDL states that the best model for data $D$ is the one that compresses the data the most. In traditional MDL, the data\u2019s two-part description length $L\\left(D\\right)=\\operatorname*{min}_{M}{\\bar{L}}\\left(M\\right)+L\\left(D\\mid M\\right)$ is the smallest achievable length over all models $M$ , where $L\\left(M\\right)$ is the model\u2019s description length, and $L\\left(D\\mid M\\right)$ is the data\u2019s description length, given the model. MDL has been adopted for a wide range of applications, including regularising neural networks\u2019 weights [21], investigating deep neural networks\u2019 data-compression capabilities [22], analysing the characteristics of datasets [23], and community detection [16, 24]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Deep graph clustering and pooling. Graph clustering has long been a research focus in machine learning [13, 25]. Spectral approaches cluster, for example, the eigenspace of a graph\u2019s Laplacian matrix or identify communities through graph cuts [26, 27]. Methods based on neural embeddings involve learning node representations with, for example, DeepWalk [28] or node2vec [29], followed by applying standard clustering approaches such as $\\boldsymbol{\\mathrm{k}}$ -means, assuming that similar nodes are embedded at similar locations [30, 31]. Other approaches include graph autoencoders [32, 33], contrastive learning [34], and self-expressiveness [35]. Recently, minimum cuts [11, 12], modularity [36, 37], and the Bernoulli-Poisson model [38] have been integrated with GNNs as loss functions for graph pooling and clustering. Such GNN-based approaches can incorporate graph structure as well as node and edge features in end-to-end optimisation of the clustering objective. Inspired by pooling in convolutional neural networks, graph pooling coarse-grains links, node features, and edge features to summarise graphs, enabling GNNs with improved performance on node and graph classification tasks [10, 12]. Consequently, graph pooling has become a research focus for GNNs, emphasising the importance of graph clustering as a primary objective [10, 12, 36]. For recent surveys of deep graph clustering, we refer to [39, 40]. ", "page_idx": 2}, {"type": "text", "text": "3 Background: the map equation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The map equation [16, 20] is an information-theoretic objective function for unsupervised community detection that follows the MDL principle [14], and has demonstrated high performance in synthetic and real networks from across domains [41, 42, 43]. The map equation formulates community detection as a compression problem and uses random walks as a proxy to model dynamic processes on networks, also called flow. The goal is to describe the random walk as efficiently as possible by minimising its expected per-step description length \u2013 also called codelength \u2013 by partitioning the network into groups of nodes, called modules, where the random walker tends to stay for a relatively long time. In practice, however, the map equation does not simulate random walks; instead, the codelength is calculated analytically. ", "page_idx": 2}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/fa185b8780614411782f5af2879d67e503aefc634b13c3e1a935bbe3826b1566.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Coding principles behind the map equation. Colours indicate modules, codewords are shown next to nodes, and the black trace shows a sequence of random-walker steps. Left: All nodes belong to the same module and all codewords are unique. Encoding the random walk sequence requires 60 bits, or 3.72 bits per step in the limit. Right: Partitioning the network enables reusing codewords across modules, reducing the codelength. However, for a unique encoding, we need to introduce codewords for entering and exiting modules, shown next to the arrows pointing into and out of the modules. With this modular coding scheme, we can compress the description to 48 bits, or 3.01 bits per step in the limit. Middle: The two encodings of the random walker\u2019s steps. ", "page_idx": 2}, {"type": "text", "text": "Let $G=(V,E)$ be a graph with nodes $V$ , links $E$ , and let $w_{u v}\\in\\mathbb{R}_{0}^{+}$ denote the non-negative link weight on the link from node $u$ to $v$ . When all nodes are assigned to the same module, the codelength is defined by the Shannon entropy $H$ over the nodes\u2019 visit rates [44], $\\begin{array}{r}{H\\left(P\\right)=-\\sum_{u\\in V}p_{u}\\log_{2}\\bar{p}_{u},}\\end{array}$ , where $p_{u}$ is node $u$ \u2019s visit rate and $P=\\{p_{u}\\,|\\,u\\in V\\}$ is the set of node visit rates. In undirected graphs, we compute visit rates directly as $p_{u}=s_{u}/\\sum_{v\\in V}s_{v}$ , where $\\begin{array}{r}{s_{u}=\\sum_{v\\in V}w_{u v}}\\end{array}$ is node $u^{\\dagger}$ \u2019s strength. In directed graphs, we compute the visit rate s numerically with sma rt teleportation [45] and a power iteration. When we partition the nodes into modules, the codelength becomes a weighted average of the modules\u2019 entropies and the entropy at the so-called index level for switching between modules. Figure 1 illustrates the coding principle behind the map equation using Huffman codes [46]; note, however, that these codewords are only for illustration and we only care about their expected length in the limit to evaluate the map equation. ", "page_idx": 3}, {"type": "text", "text": "Minimising the map equation means balancing between small modules to achieve low module-level entropies and large modules for low index-level entropy. This trade-off between module- and indexlevel entropies prevents trivial solutions where all nodes are assigned to the same module or each node is assigned to a singleton module [14]. The map equation calculates the codelength for a partition $\\mathsf{M}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nL\\left(\\mathsf{M}\\right)=q H(Q)+\\sum_{\\mathsf{m}\\in\\mathsf{M}}p_{\\mathsf{m}}H(P_{\\mathsf{m}})\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\textstyle q=\\sum_{\\mathfrak{m}}q_{\\mathfrak{m}}$ is the random walker\u2019s module entry rate, $\\begin{array}{r}{q_{\\mathsf{m}}=\\sum_{u\\notin\\mathsf{m}}\\sum_{v\\in\\mathsf{m}}p_{u}t_{u v}}\\end{array}$ is module m\u2019s entry  rate, and $Q\\;=\\;\\{q_{\\sf m}/q\\mid{\\sf m}\\in{\\sf M}\\}$ is the set of normalis ed module entry rates; $\\ensuremath{p_{\\mathrm{m}}}=$ $\\begin{array}{r}{\\mathsf{m}_{\\mathrm{exit}}+\\dot{\\sum}_{u\\in\\mathsf{m}}\\,p_{u}}\\end{array}$ is the rate at which the random walker moves in module $\\mathsf{m}$ , including the module exit rat e $\\begin{array}{r}{\\overleftarrow{\\mathbf{m}}_{\\mathrm{exit}}\\;=\\;\\sum_{u\\in\\mathfrak{m}}\\sum_{v\\notin\\mathfrak{m}}p_{u}t_{u v}}\\end{array}$ , and $P_{\\mathfrak{m}}\\;=\\;\\{{\\mathfrak{m}}_{\\mathrm{exit}}/p_{\\mathfrak{m}}\\}\\cup\\,\\{p_{u}/p_{\\mathfrak{m}}\\mid u\\in{\\mathfrak{m}}\\}$ is the set of normalised node v isit and exit rates for module $\\mathsf{m}$ . The random walker\u2019s transition probability from node $u$ to $v$ is $\\begin{array}{r}{t_{u v}=w_{u v}/\\sum_{v\\in V}w_{u v}}\\end{array}$ . We can rewrite the map equation as (see Appendix A) ", "page_idx": 3}, {"type": "equation", "text": "$$\nL\\left(\\mathsf{M}\\right)=q\\log_{2}q-\\sum_{\\mathsf{m}\\in\\mathsf{M}}q_{\\mathsf{m}}\\log_{2}q_{\\mathsf{m}}-\\sum_{\\mathsf{m}\\in\\mathsf{M}}\\mathsf{m}_{\\mathsf{e x i t}}\\log_{2}\\mathsf{m}_{\\mathsf{e x i t}}-\\sum_{\\boldsymbol{u}\\in V}p_{\\boldsymbol{u}}\\log_{2}p_{\\boldsymbol{u}}+\\sum_{\\mathsf{m}\\in\\mathsf{M}}p_{\\mathsf{m}}\\log_{2}p_{\\mathsf{m}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The map equation framework has been extended for overlapping communities through state-space expansions with higher-order network models [47, 48], avoiding over-partitioning in sparse networks using a Bayesian regularisation approach [49], and to deal with sparse constrained structures [50]. Moreover, the map equation framework can incorporate node features through an extension [51] or by preprocessing data [52]. Detecting communities relies on Infomap [6], a greedy stochastic search algorithm that optimises the map equation. However, each of the above extensions requires preprocessing the input data, adjusting the loss function, or adapting the search algorithm. In contrast, adapting the map equation as a loss function for optimisation with gradient descent does not require any custom algorithm, thus enabling flexible experimentation with variations, scalability to GPU clusters, and incorporating it into other loss functions. ", "page_idx": 3}, {"type": "text", "text": "4 The map equation goes neural ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We set out to detect communities by optimising the map equation with GNNs through gradient descent, which essentially means learning coarse-graining node representations in the form of communities (Figure 2). While the standard map equation considers hard clusters where each node is assigned to exactly one module, we introduce a soft cluster assignment matrix $\\mathbf{S}_{n\\times s}$ to make the map equation differentiable and enable overlapping clusters. We optimise $\\mathbf{S}=$ softmax $\\left(\\operatorname{GNN}_{\\theta}\\left(\\mathbf{A},\\mathbf{X}\\right)\\right)$ indirectly by optimising the GNN\u2019s parameters $\\theta$ , that is, its weights, with respect to the codelength $L$ . Here, $\\mathbf{A}_{n\\times n}$ is the graph\u2019s adjacency matrix, $\\mathbf{X}_{n\\times d}$ is the node features matrix, $n=|V|$ is the number of nodes, $s$ is the maximum allowed number of clusters, and $d$ is the node feature dimension. ", "page_idx": 3}, {"type": "text", "text": "Without loss of generality, we assume directed networks. We denote the graph\u2019s total weight as $\\begin{array}{r}{w_{\\mathrm{tot}}=\\sum_{i\\in V}\\breve{\\sum_{j\\in V}w_{i j}}}\\end{array}$ . Let $\\mathbf{T}_{n\\times n}$ be the random walker\u2019s transition matrix and $\\mathbf{d}^{\\mathrm{in}}$ be the vector of weighted node in-degrees with ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{T}_{i j}=\\left\\{\\frac{w_{i j}}{\\sum_{j\\in V}w_{i j}}\\quad\\mathrm{if}\\;\\sum_{j\\in V}w_{i j}>0,\\right.\\qquad\\mathbf{d}_{j}^{\\mathrm{in}}=\\sum_{i\\in V}w_{i j}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To compute the vector $\\mathbf{p}$ of node visit rates, we use smart teleportation [45] and the power iteration method: With probability $\\alpha$ , the random walker teleports to a random node, chosen proportionally to the nodes\u2019 in-degrees, or follows a link with probability $1-\\alpha$ . This approach leads to the iterative update rule $\\begin{array}{r}{\\mathbf{p}^{(t+\\bar{1})}\\leftarrow\\frac{\\alpha}{w_{\\mathrm{tot}}}\\mathbf{d}^{\\mathrm{in}}\\!+\\!\\left(1-\\alpha\\right)\\mathbf{p}^{(t)}\\bar{\\mathbf{T}}}\\end{array}$ , and we set $\\mathbf{p}^{(0)}=\\mathbf{d}^{\\mathrm{in}}$ . The graph\u2019s flow matrix $\\mathbf{F}_{n\\times n}$ encodes the flow between each pair of nodes, where $\\begin{array}{r}{\\mathbf{F}=\\frac{\\alpha}{w_{\\mathrm{tot}}}\\mathbf{A}+\\left(1-\\alpha\\right)\\mathrm{diag}\\left(\\mathbf{p}\\right)\\mathbf{T}}\\end{array}$ . We obtain the flow $\\mathbf{C}_{s\\times s}$ between clusters from $\\mathbf{S}$ and $\\mathbf{F}$ as $\\mathbf{C}=\\mathbf{S}^{\\top}\\mathbf{F}\\mathbf{S}$ . Following Equation (2), we define ", "page_idx": 3}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/da509a65fdf53957cb2190629a15e21a2ccf1fd7a36ca86da027c63f7f9e5905.jpg", "img_caption": ["Figure 2: Illustration of the setup for GNN-based community detection with the map equation. We learn soft cluster assignments S from the graphs adjacency matrix $\\mathbf{A}$ and the node features $\\mathbf{X}$ . Here, we allow up to four clusters. When no node features are available, we set $\\mathbf{X}=\\mathbf{A}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nq=1-\\operatorname{tr}\\left(\\mathbf{C}\\right)\\quad\\mathbf{q}_{\\mathrm{m}}=\\mathbf{C}\\mathbf{1}_{s}-\\operatorname{diag}\\left(\\mathbf{C}\\right)\\quad\\mathbf{m}_{\\mathrm{exit}}=\\left(\\mathbf{1}_{s}^{\\top}\\mathbf{C}\\right)^{\\top}-\\operatorname{diag}\\left(\\mathbf{C}\\right)\\quad\\mathbf{p}_{\\mathrm{m}}=\\mathbf{q}_{\\mathrm{m}}+\\mathbf{1}_{s}^{\\top}\\mathbf{C}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and assemble the map equation ", "page_idx": 4}, {"type": "text", "text": "$L\\left(\\mathbf{A},\\mathbf{S}\\right)=q\\log_{2}q-\\left(\\mathbf{q}_{\\mathsf{m}}\\log_{2}\\mathbf{q}_{\\mathsf{m}}\\right)\\mathbf{1}_{s}-\\left(\\mathbf{m}_{\\mathrm{exit}}\\log_{2}\\mathbf{m}_{\\mathrm{exit}}\\right)\\mathbf{1}_{s}-\\left(\\mathbf{p}\\log_{2}\\mathbf{p}\\right)\\mathbf{1}_{n}+\\left(\\mathbf{p}_{\\mathsf{m}}\\log_{2}\\mathbf{p}_{\\mathsf{m}}\\right)\\mathbf{1}_{s}$ (3) where ${\\bf1}_{k}$ is the $k$ -dimensional vector of ones, and logarithms are applied component-wise. The third term is constant since it only depends on the node visit rates and can be omitted during optimisation. ", "page_idx": 4}, {"type": "text", "text": "The map equation naturally incorporates Occam\u2019s razor by following the MDL principle for balancing between model complexity and fit [14, 15], choosing the optimal number of communities automatically, but at most $s$ . In contrast, recent GNN-based clustering approaches require explicit regularisation to avoid over-partitioning [10, 36, 37, 38], and our results show that they often return the maximum allowed number of communities instead of determining the number of communities in a data-driven fashion (see Section 5). In principle, any neural network architecture, such as a multi-layer perceptron (MLP) or GNN, can be used to learn the soft cluster assignment matrix S. Since the map equation involves logarithms, we add a small constant $\\epsilon$ to each value in the output S before the backpropagation step to ensure differentiability. We refer to the combination of using map equation loss (Equation (3)) together with a (graph) neural network to learn (overlapping) communities as Neuromap. ", "page_idx": 4}, {"type": "text", "text": "Complexity and limitations. The most expensive calculation is the pooling operation $\\mathbf{C}=\\mathbf{S}^{\\top}\\mathbf{F}\\mathbf{S}$ which depends on the network\u2019s density. When $s\\ll n$ and the number of edges is $m=\\mathcal{O}\\left(n\\right)$ , the complexity of Neuromap is linear in $n$ . When the network is dense, $m=\\mathcal{O}\\left(n^{2}\\right)$ , or the maximum number of clusters approaches the number of nodes $s\\,\\approx\\,n$ , we approach quadratic complexity. Therefore, we recommend keeping $s\\ll n$ for scalability. ", "page_idx": 4}, {"type": "text", "text": "We assume connected networks, otherwise, clustering should be run on the individual components. The node features X aid the GNN in learning patterns, however, they do not contribute to the loss. When no node features are available, Neuromap can use, for example, the adjacency matrix as node features; designing expressive low-dimensional node features remains an active research area [53]. ", "page_idx": 4}, {"type": "text", "text": "5 Experimental evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We evaluate Neuromap on synthetic and real-world networks with different neural network architectures: a 2-layer graph convolutional network (GCN) [9], a 2-layer graph isomorphism network (GIN) [54], and a 2-layer SAGE network [55]. To investigate whether GNNs are required for clustering, we also include a fully connected linear layer and a 2-layer MLP. We include a learnable temperature parameter for the softmax operation, which we found speeds up convergence. In all cases, we use the models provided by PyTorch Geometric with SELU activation [56]. Because the specifics between architectures differ, such as message-passing details and aggregation functions, they may be interpreted as using different search algorithms which return different communities. We use the Adam optimiser [57], apply batch normalisation, and for comparability between different methods, set the learning rate for the linear layer to $10^{-1}$ , for MLP to $\\mathrm{i0^{-2}}$ , and for GCN, GIN, and SAGE to $10^{-3}$ . We train all models for up to 10,000 epochs with a patience of 100 epochs and dropout probability 0.5. Because the datasets contain hard clusters, we convert the resulting communities to hard clusters, assigning each node to that cluster where it has its strongest membership. As baselines, we use Infomap [6] and five recent approaches for unsupervised graph clustering with GNNs: DMoN [36], NOCD [38], DiffPool [10], MinCut [11], and Ortho [36]. We base our implementation3on PyTorch [58] and PyTorch Geometric [59] and ran our experiments on a workstation with an Intel i9-11900K $@$ 3.50GHz CPU, 32 GB of RAM, and a GeForce RTX 3090 with 24 GB of memory. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5.1 Synthetic networks with planted communties ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We generate directed and undirected Lancichinetti-Fortunato-Radicchi (LFR) benchmark networks with planted ground-truth communities [60] w\u221aith $n\\:=\\:1000$ nodes, average node degree $k\\ \\in$ $\\{\\ln n,2\\ln n\\}$ , maximum node degree $k_{\\mathrm{max}}=2{\\sqrt{n}}$ , both rounded to the nearest integer, and mixing parameter $\\mu$ between 0.1 and 0.8 with a step size of 0.1. We set the power-law exponents for the node degree distribution to $\\tau_{1}=2$ , and for the community size distribution to $\\tau_{2}=1$ . For each combination of parameters, we generate 10 LFR networks using the implementation provided by the authors,4 resulting in a total of 320 networks. For each parameter combination, there are 10 LFR networks; for each of these LFR networks, we run each model 10 times, measuring its performance as the average adjusted mutual information (AMI) [61] against the ground truth, and plot the average of those AMI values over the 10 networks per parameter combination. To verify that the number of communities is inferred from the data, we set the maximum number of communities to $s=n$ . Since LFR networks do not have node features, we use the adjacency matrix as node features. ", "page_idx": 5}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/e577c32028227bc2e6ae33277ba9ea42b8ed672aa3032761d656a4b82e0f1c2f.jpg", "img_caption": ["Figure 3: Performance for Neuromap using a dense linear layer, MLP, GCN, GIN, and SAGE architectures with two layers and Infomap on directed and undirected LFR networks with planted communities. The results show averages of partition quality measured with AMI, number of detected communities $\\lvert\\mathsf{M}\\rvert$ , and codelength $L$ . The shaded areas show one standard deviation from the mean. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "We find that the detected communities\u2019 quality depends on the choice of neural network architecture (Figure 3). Neuromap achieves the best AMI scores with SAGE. GCN, MLP, and Infomap perform slightly worse, however, with some variation depending on the networks\u2019 properties. The dense linear layer and GIN show weaker performance but still identify relevant communities. In the sparser directed networks, Infomap performs slightly better than SAGE when the mixing $\\mu$ is low. However, the AMI values do not tell the whole story: Infomap and MLP tend to report considerably more communities than are present in the ground truth whereas the dense linear layer and GIN tend to report much fewer communities than the ground truth, especially for higher mixing values. GCN reports more communities than are present in the ground truth in the sparser undirected networks but fewer in the directed networks. SAGE detects close to the true number of communities in all cases. Infomap achieves the lowest codelength across all networks. GCN, MLP, and SAGE achieve close to Infomap\u2019s codelength, whereas the dense linear layer and GIN have slightly higher codelength. ", "page_idx": 6}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/6ea5574378a554c2b71b61fd8f49aa903b9053668f18d4ee23902e50e774fe7d.jpg", "img_caption": ["Figure 4: SAGE-based results for deep learning community-detection methods on synthetic LFR networks with planted communities. We show averages of partition quality measured by AMI and number of detected communities $\\lvert\\mathsf{M}\\rvert$ . The shaded areas show one standard deviation from the mean. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "We compare Neuromap against recent deep-learning-based community detection methods on the same networks by swapping out the loss function while keeping everything else the same, with the exception of using weight decay for NOCD as per the original paper [38]. For DiffPool, Mincut, Ortho, and DMoN, we use the implementation from PyTorch Geometric, for NOCD, we use the implementation provided by the authors.5 Figure 4 shows the results for SAGE; in Appendix B, we also include results for the remaining architectures. Neuromap outperforms the baselines across all architectures, except for NOCD which performs better than Neuromap with GIN and with GCN on directed networks. Different from previous works, we have not limited the maximum number of communities, which allows us to analyse the methods\u2019 overfitting behaviour: While Neuromap reports close to the ground-truth number of communities, the remaining methods often overfit the networks\u2019 structure and report considerably more communities (note the logarithmic scale). MinCut fails to identify meaningful communities on directed networks for mixing values $\\mu>0.3$ . NOCD performs best with the GCN architecture, which was also used in the original paper [38]. Neuromap performs best with SAGE in our experiments. ", "page_idx": 6}, {"type": "text", "text": "5.2 Real-world networks with node features ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We benchmark Neuromap on ten real-world datasets (Table 1) from PyTorch Geometric [59], PyTorch Geometric Signed Directed [62], and Open Graph Benchmark [63], and compare it against the same baselines as before. In contrast to previous works that choose a fixed number of hidden dimensions and set the maximum number of communities to a constant [36] or the \u201cground-truth\u201d number of communities [38], we reflect the networks\u2019 sizes in our choices: We set the number of hidden dimensions to $4\\sqrt{n}$ and the maximum number of communities to $s=\\sqrt{n}$ . Our choices a\u221are based on empirical observations showing that the number of communities typically scales as ${\\mathcal{O}}\\left({\\sqrt{n}}\\right)$ [64]. ", "page_idx": 6}, {"type": "text", "text": "Table 1: Properties of the real-world datasets obtained from PyTorch Geometric (PyG) [59], PyTorch Geometric Signed Directed (PyG-SD) [62], and Open Graph Benchmark (OGB) [63]. $|V|$ is the number of nodes, $|E|$ the number of edges, $|X|$ the node feature dimension, $|Y|$ the number of communities, and $\\mu$ the mixing for the given communities. ", "page_idx": 7}, {"type": "table", "img_path": "aFWx1N84Fe/tmp/91797b71beb9f689b1c1806605d3071ca68a033ae5bec0b1c1c57cc5a65ffae9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "However, a few words of caution are in order: while nodes\u2019 true communities determine the link patterns in synthetic networks, it is generally infeasible to obtain ground truth communities for real networks. Often, metadata labels are used as a drop-in, and the inferred communities\u2019 quality depends on how well the metadata, which is potentially noisy, aligns with the unknown ground\u221a truth [3]. Moreover, determining the number of communities in a network is hard and setting $s=\\sqrt{n}$ should be seen as a simplification rather than an attempt to guess the exact number. ", "page_idx": 7}, {"type": "text", "text": "For each method and architecture, we run 25 trials and show the average achieved AMI in Figure 5; we include a similar plot for the number of detected communities as well as the average AMI and the detected number of communities, both with standard deviations, in tabulated form in Appendix C. When several of the best-performing methods achieve similar AMI, we use an independent twosample t-test to determine whether one of them can be considered to perform better than the other (see Appendix C). In cases where their performances do not differ significantly, we mark both as best. ", "page_idx": 7}, {"type": "text", "text": "Neuromap and NOCD are amongst the best performers in seven cases and DiffPool in two. The GCN architecture performs best in seven cases, MLP and SAGE in four cases, and GIN in two cases. A possible explanation for why the simpler linear layer and MLP architectures perform well in several cases could be that the map equation loss function captures global information in the random walker\u2019s flow patterns, making GNNs superfluous in some cases. All methods tend to detect more communities than are present in the \u201cground truth\u201d, however, this tendency is most pronounced in DMoN, Ortho, and MinCut, which may have gone unnoticed in previous evaluations where the maximum number of communities was set to a much lower, constant value [36], thus artificially preventing overfitting. Infomap is the only baseline that does not utilise node features; instead, it relies solely on topological information, which may explain the large number of detected communities. Comparing Neuromap\u2019s performance against Infomap\u2019s performance suggests that incorporating node features substantially improves the detected communities\u2019 quality in most cases (see Appendix C for significance tests) while drastically reducing the number of detected communities. ", "page_idx": 7}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/9c28ee0018010efcbf9744c71b2da116ab90984ff2c7b203843028908e737452.jpg", "img_caption": ["Figure 5: Average achieved AMI on real-world networks (higher is better) with $s=\\sqrt{n}$ . Colours indicate methods while shapes indicate neural network architectures. DiffPool ran out of memory on the ogb-arxiv dataset. Detailed tabulated results with standard deviations are included in Appendix C. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We repeat the same experiments with 512 hidden features and $s=|Y|$ , that is, the \u201ctrue\u201d number of communities, following [38] (results in Appendix D). Limiting the number of allowed communities often leads to better performance for DMoN, MinCut, and Ortho, however, with a few exceptions, it diminishes the performance of Neuromap, NOCD, and DiffPool across all datasets and neural network architectures. \u221aAppendix E t\u221aabulates the differences in average AMI score between setting the hidden features to $4\\sqrt{n}$ and $s=\\sqrt{n}$ versus using 512 hidden features and $s=|Y|$ . In the case of Neuromap, imposing a lower bound on the number of communities interferes with the MDL principle, limiting what models for the data may be explored. ", "page_idx": 8}, {"type": "text", "text": "5.3 Synthetic networks with overlapping communities ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We apply Neuromap and the baselines to a small synthetic network with overlapping communities [65]. We set the maximum number of communities to $s\\in\\{2,3\\}$ , run each combination of loss function and neural network architecture for 10 trials, and keep the solutions with the lowest loss. Figure 6 shows the results obtained with GCN, Appendix F shows results for the remaining architectures. ", "page_idx": 8}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/15d1c9ea6a984438840eead6bb84a6f7bbcacc5d070ba9f55cdfcaaa0badeb41.jpg", "img_caption": ["Figure 6: Synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualise their community assignments. The top and bottom rows show results for a maximum of $s=2$ and $s=3$ communities, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We find that Neuromap, DMoN, NOCD, and MinCut identify the correct communities for $s=2$ . DiffPool does not detect overlapping communities and Ortho assigns each node to two communities. For $s=3$ , only Neuromap identifies the correct communities. DiffPool returns the same communities as for $s=2$ . All remaining methods return three communities. These results provide further evidence that the baselines suffer from overfitting when they are not provided with the correct number of communities, which, in general, is unknown. Neuromap identifies meaningful communities while inferring the number of communities in a data-driven fashion by following the MDL principle. However, we leave a more rigorous study of overlapping communities for future work. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Network science and deep learning on graphs tackle community detection from different perspectives. Community detection in network science typically relies on custom heuristic optimisation algorithms to optimise objective functions but often does not leverage recent deep learning advances. Recently, deep graph learning methods have started to incorporate methods from network science for deep graph clustering. We contribute to this young field by adapting the map equation, a popular unsupervised information-theoretic community-detection approach, as a differentiable loss function for end-to-end optimisation with (graph) neural networks through gradient descent, and use PyTorch to implement our approach, which we call Neuromap. ", "page_idx": 9}, {"type": "text", "text": "We evaluated Neuromap on various synthetic and real-world datasets, using different neural network architectures to detect communities. Our results show that Neuromap achieves competitive performance and detects close to the ground-truth number of communities across datasets while the baselines tend to overfit and report considerably more communities. Across all tested methods, the achieved performance depends on the used neural network architecture. However, on several realworld benchmarks, Neuromap outperforms several of the the baselines even with simpler, non-GNN, neural network architectures. We hypothesise that this may be because the map equation builds on capturing flow patterns, which contain global information. ", "page_idx": 9}, {"type": "text", "text": "While we have considered first-order networks with two-level community structures, complex realworld networks often involve higher-order dependencies and can have multi-level communities [47, 66], prompting a generalisation of our approach. Furthermore, incorporating our method for graph pooling as well as uncovering the precise connection between the utilised neural network architecture and the achieved community-detection performance requires further empirical and theoretical studies. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Martin Rosvall, Jelena Smiljani\u00b4c, and Daniel Edler for fruitful discussions and helpful feedback. We are indebted to Lisi Qarkaxhija for helping with PyTorch implementations. Christopher Bl\u00f6cker and Ingo Scholtes acknowledge funding from the Swiss National Science Foundation, grant 176938, and the German Federal Ministry of Education and Research, grant 100582863 (TissueNet). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Santo Fortunato. Community detection in graphs. Physics Reports, 486(3):75\u2013174, 2010.   \n[2] Santo Fortunato and Mark EJ Newman. 20 years of network community detection. Nature Physics, 18(8):848\u2013850, 2022.   \n[3] Leto Peel, Daniel B. Larremore, and Aaron Clauset. The ground truth about metadata and community detection in networks. Science Advances, 3(5):e1602548, 2017.   \n[4] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10):P10008, oct 2008.   \n[5] V. A. Traag, L. Waltman, and N. J. van Eck. From louvain to leiden: guaranteeing wellconnected communities. Scientific Reports, 9(1):5233, Mar 2019.   \n[6] Daniel Edler, Ludvig Bohlin, and Martin Rosvall. Mapping higher-order network flows in memory and multilayer networks with infomap. Algorithms, 10(4), 2017.   \n[7] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009.   \n[8] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1263\u20131272. PMLR, 06\u201311 Aug 2017.   \n[9] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.   \n[10] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. Advances in neural information processing systems, 31, 2018.   \n[11] Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph neural networks for graph pooling. In International conference on machine learning, pages 874\u2013883. PMLR, 2020.   \n[12] Filippo Maria Bianchi and Veronica Lachi. The expressive power of pooling in graph neural networks. In Advances in Neural Information Processing Systems, volume 36, pages 71603\u2013 71618. Curran Associates, Inc., 2023.   \n[13] Satu Elisa Schaeffer. Graph clustering. Computer science review, 1(1):27\u201364, 2007.   \n[14] Jorma Rissanen. Modeling by shortest data description. Automatica, 14(5):465\u2013471, 1978.   \n[15] Peter D Gr\u00fcnwald, In Jae Myung, and Mark A Pitt. Advances in minimum description length: Theory and applications. MIT press, 2005.   \n[16] Martin Rosvall and Carl T. Bergstrom. Maps of random walks on complex networks reveal community structure. Proceedings of the National Academy of Sciences, 105(4):1118\u20131123, 2008.   \n[17] M. E. J. Newman. Modularity and community structure in networks. Proceedings of the National Academy of Sciences, 103(23):8577\u20138582, 2006.   \n[18] Paul W. Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5(2):109\u2013137, 1983.   \n[19] Tiago P Peixoto. Efficient monte carlo and greedy heuristic for the inference of stochastic block models. Physical Review E, 89(1):012804, 2014.   \n[20] M. Rosvall, D. Axelsson, and C. T. Bergstrom. The map equation. The European Physical Journal Special Topics, 178(1):13\u201323, Nov 2009.   \n[21] Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pages 5\u201313, 1993.   \n[22] L\u00e9onard Blier and Yann Ollivier. The description length of deep learning models. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[23] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. Rissanen data analysis: Examining dataset characteristics via description length. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8500\u20138513. PMLR, 18\u201324 Jul 2021.   \n[24] Tiago P. Peixoto. Hierarchical block structures and high-resolution model selection in large networks. Phys. Rev. X, 4:011047, Mar 2014.   \n[25] Kai Yu, Shipeng Yu, and Volker Tresp. Soft clustering on graphs. Advances in neural information processing systems, 18, 2005.   \n[26] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and machine intelligence, 22(8):888\u2013905, 2000.   \n[27] Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395\u2013416, Dec 2007.   \n[28] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, page 701\u2013710, New York, NY, USA, 2014. Association for Computing Machinery.   \n[29] Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, page 855\u2013864, New York, NY, USA, 2016. Association for Computing Machinery.   \n[30] Aditya Tandon, Aiiad Albeshri, Vijey Thayananthan, Wadee Alhalabi, Filippo Radicchi, and Santo Fortunato. Community detection in networks using graph embeddings. Physical Review E, 103(2):022316, 2021.   \n[31] Sadamori Kojaku, Filippo Radicchi, Yong-Yeol Ahn, and Santo Fortunato. Network community detection via neural embeddings. arXiv preprint arXiv:2306.13400, 2023.   \n[32] Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. Mgae: Marginalized graph autoencoder for graph clustering. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 889\u2013898, 2017.   \n[33] Nairouz Mrabah, Mohamed Bouguessa, Mohamed Fawzi Touati, and Riadh Ksantini. Rethinking graph auto-encoder models for attributed graph clustering. IEEE Transactions on Knowledge and Data Engineering, 2022.   \n[34] Maedeh Ahmadi, Mehran Safayani, and Abdolreza Mirzaei. Deep graph clustering via mutual information maximization and mixture model. Knowledge and Information Systems, Apr 2024.   \n[35] Sambaran Bandyopadhyay and Vishal Peter. Unsupervised constrained community detection via self-expressive graph neural network. In Uncertainty in Artificial Intelligence, pages 1078\u20131088. PMLR, 2021.   \n[36] Anton Tsitsulin, John Palowitch, Bryan Perozzi, and Emmanuel M\u00fcller. Graph clustering with graph neural networks. Journal of Machine Learning Research, 24(127):1\u201321, 2023.   \n[37] Tsuyoshi Murata and Naveed Afzal. Modularity optimization as a training criterion for graph neural networks. In Complex Networks IX, pages 123\u2013135, Cham, 2018. Springer International Publishing.   \n[38] Oleksandr Shchur and Stephan G\u00fcnnemann. Overlapping community detection with graph neural networks. Deep Learning on Graphs Workshop, KDD, 2019.   \n[39] Liu Yue, Xia Jun, Zhou Sihang, Wang Siwei, Guo Xifeng, Yang Xihong, Liang Ke, Tu Wenxuan, Liu Xin Wang, et al. A survey of deep graph clustering: Taxonomy, challenge, and application. arXiv preprint arXiv:2211.12875, 2022.   \n[40] Su Xing, Xue Shan, Liu Fanzhen, Wu Jia, Yang Jian, Zhou Chuan, Hu Wenbin, Paris Cecile, Nepal Surya, Jin Di, et al. A comprehensive survey on community detection with deep learning. IEEE Trans. Neural Netw. Learn. Syst, 2022.   \n[41] Andrea Lancichinetti and Santo Fortunato. Community detection algorithms: A comparative analysis. Phys. Rev. E, 80:056117, 2009.   \n[42] Rodrigo Aldecoa and Ignacio Mar\u00edn. Exploring the limits of community detection strategies in complex networks. Sci. Rep., 3:2216, 2013.   \n[43] Lovro \u0160ubelj, Nees Jan van Eck, and Ludo Waltman. Clustering scientific publications based on citation relations: A systematic comparison of different methods. PLOS ONE, 11(4):1\u201323, 04 2016.   \n[44] C. E. Shannon. A mathematical theory of communication. Bell Syst. Tech. J., 27:379\u2013423, 1948.   \n[45] R. Lambiotte and M. Rosvall. Ranking and clustering of nodes in networks with smart teleportation. Phys. Rev. E, 85:056107, May 2012.   \n[46] David A. Huffman. A method for the construction of minimum-redundancy codes. Proceedings of the IRE, 40(9):1098\u20131101, 1952.   \n[47] Martin Rosvall, Alcides V. Esquivel, Andrea Lancichinetti, Jevin D. West, and Renaud Lambiotte. Memory in network flows and its effects on spreading dynamics and community detection. Nature Communications, 5(1):4630, Aug 2014.   \n[48] Anton Holmgren, Christopher Bl\u00f6cker, and Martin Rosvall. Mapping biased higher-order walks reveals overlapping communities, 2023.   \n[49] Jelena Smiljani\u00b4c, Christopher Bl\u00f6cker, Daniel Edler, and Martin Rosvall. Mapping flows on weighted and directed networks with incomplete observations. Journal of Complex Networks, 9(6):cnab044, 12 2021.   \n[50] Daniel Edler, Jelena Smiljani\u00b4c, Anton Holmgren, Alexandre Antonelli, and Martin Rosvall. Variable markov dynamics as a multi-focal lens to map multi-scale complex networks, 2022.   \n[51] Scott Emmons and Peter J. Mucha. Map equation with metadata: Varying the role of attributes in community detection. Phys. Rev. E, 100:022301, Aug 2019.   \n[52] Aleix Bassolas, Anton Holmgren, Antoine Marot, Martin Rosvall, and Vincenzo Nicosia. Mapping nonlocal relationships between metadata and network structure with metadata-dependent encoding of random walks. Science Advances, 8(43):eabn7558, 2022.   \n[53] Derek Lim, Joshua David Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie Jegelka. Sign and basis invariant networks for spectral graph representation learning. In The Eleventh International Conference on Learning Representations, 2023.   \n[54] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.   \n[55] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[56] G\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. Advances in neural information processing systems, 30, 2017.   \n[57] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.   \n[58] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32, pages 8024\u20138035. Curran Associates, Inc., 2019.   \n[59] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019.   \n[60] Andrea Lancichinetti, Santo Fortunato, and Filippo Radicchi. Benchmark graphs for testing community detection algorithms. Phys. Rev. E, 78:046110, Oct 2008.   \n[61] Simone Romano, James Bailey, Vinh Nguyen, and Karin Verspoor. Standardized mutual information for clustering comparisons: One step further in adjustment for chance. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1143\u20131151, Bejing, China, 22\u201324 Jun 2014. PMLR.   \n[62] Yixuan He, Xitong Zhang, Junjie Huang, Benedek Rozemberczki, Mihai Cucuringu, and Gesine Reinert. Pytorch geometric signed directed: A software package on graph neural networks for signed and directed graphs. In Learning on Graphs Conference, pages 12\u20131. PMLR, 2024.   \n[63] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.   \n[64] Amir Ghasemian, Homa Hosseinmardi, and Aaron Clauset. Evaluating overfit and underfit in models of network community structure. IEEE Transactions on Knowledge and Data Engineering, 32(9):1722\u20131735, 2020.   \n[65] Alcides Viamontes Esquivel and Martin Rosvall. Compression of flow can reveal overlappingmodule organization in networks. Phys. Rev. X, 1:021025, Dec 2011.   \n[66] M. Rosvall and C. T. Bergstrom. Multilevel Compression of Random Walks on Networks Reveals Hierarchical Organization in Large Integrated Systems. PLoS One, 6:e18209, 2011.   \n[67] David F. Gleich. Pagerank beyond the web. SIAM Review, 57(3):321\u2013363, 2015. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Map equation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $G=(V,E)$ be a graph with nodes $V$ , links $E$ , and let $w_{u v}\\in\\mathbb{R}_{0}^{+}$ denote the non-negative weight on the link from node $u$ to $v$ . Further, let $p_{u}$ be node $u$ \u2019s ergodic visit rate, which can be calculated in closed form as $\\begin{array}{r}{p_{u}\\,=\\,{s_{u}}/\\sum_{v\\in V}s_{v}}\\end{array}$ in undirected graphs, where $\\textstyle s_{u}\\,=\\,\\sum_{v\\in V}w_{u v}$ is node $u$ \u2019s strength. In directed graphs, $p_{u}$ can be calculated numerically with smart te leportation and a power iteration by solving the recursive set of equations $\\begin{array}{r}{p_{v}=\\alpha\\frac{s_{v}^{\\mathrm{out}}}{\\sum_{u\\in V}s_{u}^{\\mathrm{out}}}+(1-\\alpha)\\sum_{u\\in V}p_{u}t_{u v}}\\end{array}$ , where $\\begin{array}{r}{s_{u}^{\\mathrm{out}}=\\sum_{v\\in V}w_{u v}}\\end{array}$ is node \u2019s out-strength, $\\begin{array}{r}{t_{u v}=w_{u v}/\\sum_{v\\in V}w_{u v}}\\end{array}$ is the random walker\u2019s transition probab ility from node $u$ to node $v$ , and $\\alpha$ is a teleportati on parameter, typically set to $\\alpha=0.15$ [45]. Smart teleportation is similar to PageRank [67] but, instead of uniformly teleporting to nodes, the random walker teleports to links proportional to their weight. ", "page_idx": 14}, {"type": "text", "text": "For a given partition $\\mathsf{M}$ of the nodes into modules, the map equation [16] calculates the average per-step description length \u2013 also called codelength \u2013 for describing the position of a random walker on the graph: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL\\left(\\mathsf{M}\\right)=q H(Q)+\\sum_{\\mathsf{m}\\in\\mathsf{M}}p_{\\mathsf{m}}H(P_{\\mathsf{m}})\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $\\textstyle q=\\sum_{\\mathfrak{m}}q_{\\mathfrak{m}}$ is the random walker\u2019s module entry rate, $\\begin{array}{r}{q_{\\mathsf{m}}=\\sum_{u\\notin\\mathsf{m}}\\sum_{v\\in\\mathsf{m}}p_{u}t_{u v}}\\end{array}$ is module m\u2019s entry  rate, and $Q\\;=\\;\\{q_{\\mathsf{m}}/q\\mid{\\mathsf{m}}\\in{\\mathsf{M}}\\}$ is the set of normalis ed module entry rates; $p_{\\mathsf{m}}~=$ $\\begin{array}{r}{\\mathsf{m}_{\\mathrm{exit}}+\\dot{\\sum}_{u\\in\\mathsf{m}}\\,p_{u}}\\end{array}$ is the rate at which the random walker moves in module $\\mathsf{m}$ , including the exit rate $\\begin{array}{r}{\\mathsf{m}_{\\mathrm{exit}}=\\bar{\\sum}_{u\\in\\mathsf{m}_{2}}\\!\\sum_{v\\notin\\mathsf{m}}p_{u}t_{u v}}\\end{array}$ , and $P_{\\mathsf{m}}=\\{\\mathsf{m_{e x i t}}/p_{\\mathsf{m}}\\}\\cup\\{p_{u}/p_{\\mathsf{m}}\\mid u\\in\\mathsf{m}\\}$ is the set of normalised node visit rates and exit rates for module $\\mathsf{m}$ . ", "page_idx": 14}, {"type": "text", "text": "A.1 Rewriting the map equation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The map equation can be rewritten by expanding the entropy terms and cancelling common factors ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}\\left({\\bf M}\\right)=q H(Q)+\\sum_{m\\in{\\bf M}}p_{m}H(P_{m})}\\ ~}\\\\ {{\\displaystyle~~~~=-q\\sum_{m\\in{\\bf M}}\\frac{q_{m}}{\\mathcal{A}}\\log_{2}\\frac{q_{m}}{q}-\\sum_{m\\in{\\bf M}}p_{m}\\left(\\frac{\\mathsf{m}_{\\mathrm{exit}}}{p_{m}}\\log_{2}\\frac{\\mathsf{m}_{\\mathrm{exit}}}{p_{m}}+\\sum_{u\\in{\\bf m}}\\frac{p_{u}}{p_{m}}\\log_{2}\\frac{p_{u}}{p_{m}}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying logarithm rules gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=-\\sum_{m\\in{\\cal M}}q_{m}\\log_{2}q_{m}+\\sum_{m\\in{\\cal M}}q_{m}\\log_{2}q}}\\\\ {{\\displaystyle\\qquad-\\sum_{m\\in{\\cal M}}\\mathsf{m}_{\\mathrm{exit}}\\log_{2}\\mathsf{m}_{\\mathrm{exit}}+\\sum_{m\\in{\\cal M}}\\mathsf{m}_{\\mathrm{exit}}\\log_{2}p_{m}-\\sum_{m\\in{\\cal M}}\\sum_{u\\in{\\cal m}}p_{u}\\log_{2}p_{u}+\\sum_{m\\in{\\cal M}}\\sum_{u\\in{\\sf m}}p_{u}\\log_{2}p_{m},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and further simplification yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n=q\\log_{2}q-\\sum_{\\mathfrak{m}\\in\\mathbb{M}}q_{\\mathfrak{m}}\\log_{2}q_{\\mathfrak{m}}+\\sum_{\\mathfrak{m}\\in\\mathbb{M}}p_{\\mathfrak{m}}\\log_{2}p_{\\mathfrak{m}}-\\sum_{\\mathfrak{m}\\in\\mathbb{M}}\\mathfrak{m}_{\\mathrm{exit}}\\log_{2}\\mathfrak{m}_{\\mathrm{exit}}-\\sum_{\\mathfrak{u}\\in V}p_{\\mathfrak{u}}\\log_{2}p_{\\mathfrak{u}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In undirected networks where $q_{\\mathrm{m}}=\\mathsf{m_{\\mathrm{exit}}}$ , this can be further simplified [20] to ", "page_idx": 14}, {"type": "equation", "text": "$$\n=q\\log_{2}q-2\\sum_{\\mathfrak{m}\\in\\mathbb{M}}q_{\\mathfrak{m}}\\log_{2}q_{\\mathfrak{m}}+\\sum_{\\mathfrak{m}\\in\\mathbb{M}}p_{\\mathfrak{m}}\\log_{2}p_{\\mathfrak{m}}-\\sum_{\\mathfrak{u}\\in V}p_{\\mathfrak{u}}\\log_{2}p_{\\mathfrak{u}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The last term in Equation (10), that is, the nodes\u2019 contribution to the codelength, is constant because it does not depend on the module structure, and can be omitted during optimisation. ", "page_idx": 14}, {"type": "text", "text": "A.2 Node flow with soft cluster assignments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "While the nodes\u2019 codelength contribution is constant with hard clusters (Equation (10)), expressing the map equation with soft cluster assignments allows modelling the flow of nodes that are assigned to more than one module in at least two different ways which we illustrate in Figure 7. ", "page_idx": 14}, {"type": "text", "text": "The first option is to reflect the nodes\u2019 partial cluster assignment for assigning codewords. Consider node $u$ with visit rate $p_{u}$ and partial assignments of $\\frac{1}{2}$ to clusters $\\mathsf{m}_{1}$ whose usage rate is $p_{\\mathsf{m_{1}}}$ and $\\frac{1}{2}$ to cluster $\\mathsf{m}_{2}$ whose usage rate is $p_{\\mathsf{m}_{2}}$ . Then, half of $u$ \u2019s flow, that is $\\textstyle{\\frac{p_{u}}{2}}$ , falls into each of the two clusters and u\u2019s contribution to the ov2erall codelength is \u2212p2u log22ppum12  \u2212 $\\begin{array}{r}{-\\frac{p_{u}}{2}\\log_{2}\\frac{p_{u}}{2p_{m_{1}}}-\\frac{p_{u}}{2}\\log_{2}\\frac{p_{u}}{p_{2m_{2}}}}\\end{array}$ . In general, let node $u$ \u2019s assignment to cluster $\\mathsf{m}_{i}$ be $s_{u i}\\in[0,1]$ with $\\textstyle\\sum_{i}s_{u i}={\\bar{1}}$ , and let $\\mathsf{m}_{i}$ \u2019s usage rate be pmi. Then, node u\u2019s contribution to the overall codelength is  \u2212 i suipu log2supimpiu . Essentially, this approach splits each node with assignments to more than one cluster into its corresponding parts, resulting in potentially many small pieces with low visit rates. ", "page_idx": 14}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/3fc9e6505a4d7d21bcdef73b8685899fe6fc32bf3f5e4471c40cef46b2711423.jpg", "img_caption": ["Figure 7: Modelling node flow with soft cluster assignments. The labels show each node\u2019s contribution to the overall codelength based on their visit rates. (a) The one-level partition where all nodes belong to the same community. (b) With hard communities, the middle node must be assigned to either the blue or orange community. Here, this leads to a higher codelength than for the one-level partition. (c) Reflecting nodes\u2019 partial assignments in the codelength contributions means splitting them into several smaller pieces whose visit rates sum to the original node\u2019s visit rate. Important objects are split into several less important objects, requiring longer codewords because of their lower visit rates. Overall, this tends to increase the codelength and can prevent identifying overlapping modules. (d) Treating nodes\u2019 contribution to the codelength as constant keeps the nodes intact and allows important nodes to retain their higher visit rates, leading to shorter codewords. As the codelength highlights, only this approach would identify communities in this example. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "The second option is to treat the nodes as indivisible when computing their codeword lengths but as above, node reflecting their soft assignments in the contribution to the overall codelength. Using the same example $u$ \u2019s contribution to the overall codelength is $\\begin{array}{r}{-\\frac{p_{u}}{2}\\log_{2}^{\\bullet}\\frac{p_{u}}{p_{\\mathsf{m}_{1}}}\\,-\\,\\frac{\\bar{p}_{u}}{2}\\log_{2}\\frac{p_{u}}{p_{\\mathsf{m}_{2}}}}\\end{array}$ \u2212p2u log2ppmu2 , which is lower because the values inside the logarithms are bigger. In general, $u$ codelength is \u2212 i suipu log2ppmu , where the difference to the previous option is that $s_{u i}$ does not appear inside the logarithm. ", "page_idx": 15}, {"type": "text", "text": "B Results on synthetic networks with different (G)NN architectures ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we report further results on the synthetic LFR networks with planted communities for Neuromap, DMoN, NOCD, DiffPool, MinCut, and Ortho using the following architectures: a dense linear layer, a 2-layer MLP, a 2-layer GCN, and a 2-layer GIN. The setup is as described in the main text. ", "page_idx": 16}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/50d0936547019291a074726252529ea0e53e4fdd32d39c35637f0faee656fc07.jpg", "img_caption": ["B.1 Linear-layer-based results "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 8: Linear-layer-based results for deep-learning community-detection methods on synthetic LFR networks with planted communities. The results show averages of partition quality measured by AMI and number of detected communities $\\lvert\\mathsf{M}\\rvert$ . The shaded areas show one standard deviation from the mean. ", "page_idx": 16}, {"type": "text", "text": "B.2 MLP-based results ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/ed4b57171131758e56b6cff6aa2ab0789efba02a48776499398b49aa8123d4ff.jpg", "img_caption": ["Figure 9: MLP-based results for deep-learning community-detection methods on synthetic LFR networks with planted communities. The results show averages of partition quality measured by AMI and number of detected communities $\\lvert\\mathsf{M}\\rvert$ . The shaded areas show one standard deviation from the mean. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/036b043e2a5813c5a8037b3515a70073577f1f6a2348e19e1c3da67d65dd891d.jpg", "img_caption": ["B.3 GCN-based results "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 10: GCN-based results for deep-learning community-detection methods on synthetic LFR networks with planted communities. The results show averages of partition quality measured by AMI and number of detected communities $\\lvert\\mathsf{M}\\rvert$ . The shaded areas show one standard deviation from the mean. ", "page_idx": 17}, {"type": "text", "text": "B.4 GIN-based results ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/0f40f07e6cc9ab88807184ad9233eed3db219693ab3bb65ac4f5d38c78c8005d.jpg", "img_caption": ["Figure 11: GIN-based results for deep-learning community-detection methods on synthetic LFR networks with planted communities. The results show averages of partition quality measured by AMI and number of detected communities $\\lvert\\mathsf{M}\\rvert$ . The shaded areas show one standard deviation from the mean. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Results on Real-World Networks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "H\u221aere, we report further results on real-world networks for s\u221aetting the number of hidden dimensions to $4\\sqrt{n}$ and the maximum number of communities to $s=\\sqrt{n}$ . Specifically, we visualise the average detected number of communities per loss function and neural network architecture for each dataset. We tabulate the average AMI and average number of detected communities, including standard deviations. Two-sample t-tests show when Neuromap or a baseline performs significantly better than the other. ", "page_idx": 18}, {"type": "text", "text": "C.1 Number of detected communities on real-world networks ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/a06d04a90b597c51b5cbc9fa7b357c818355ecea4dfa6192bb002f63425ff7b4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 12: Average detected number of communities on real-world networks. Colours indicate methods while shapes indicate (G)NN architectures. The dashed horizontal lines show the correct number of communities for each dataset wh\u221aile the solid horizontal lines show the chosen maximum allowed number of communities, here $s=\\sqrt{n}$ . DiffPool ran out of memory on the ogb-arxiv dataset. We omit Infomap in the plot due to the large number of detected communities. ", "page_idx": 18}, {"type": "text", "text": "C.2 Tabulated AMI results on real-world networks ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we report the detailed AMI performance for Neuromap, DMoN, NOCD, DiffPool, M\u221ainCut, Ortho, and Infomap on real-world networks for s\u221aetting the number of hidden dimensions to $4\\sqrt{n}$ and the maximum number of communities to $s=\\sqrt{n}$ . When several of the best-performing methods achieve similar AMI, we use an independent two-sample t-test to determine whether one of them can be considered to perform better than the other. In cases where their performances do not differ significantly, we mark both as best. ", "page_idx": 19}, {"type": "text", "text": "Table 2: Average AMI in $\\%$ (higher is better) and their standard deviations on real-world networks. For each dataset, we highlight the best scores in bold and underline the second-best score. We tested each method with 2-layer MLP, GCN, GIN, and SAGE architectures, except for Infomap, which is not based on deep learning. OOM stands for \u201cout of memory\u201d. ", "page_idx": 19}, {"type": "table", "img_path": "aFWx1N84Fe/tmp/597fe51896a8804abccab7eae7cf9321615a12fa7881ed639a740e3e64477e9a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.3 Significance of best Neuromap results vs. best baseline ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 3: Independ\u221aent two-sample t-test betwee\u221an 25 samples of AMI values for Neuromap vs. the best baseline for $4\\sqrt{n}$ hidden features and $s=\\sqrt{n}$ . The $\\mathfrak{p}$ -values indicate for which datasets the null hypothesis \u201cthe samples have the same mean\u201d can be rejected. Blue p-values highlight cases where Neuromap performs significantly better than the best baseline; red p-values highlight cases where the best baseline performs significantly better than Neuromap. ", "page_idx": 19}, {"type": "table", "img_path": "aFWx1N84Fe/tmp/7609310785c85f1990a544d08c534cdd84099f15e8293f8b0d5a6d2f99a03b23.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 4: Independent two-sample t-test between \u221a25 samples of AMI values fo\u221ar Neuromap with different (G)NN architectures vs. Infomap for $4\\sqrt{n}$ hidden features and $s=\\sqrt{n}$ . The p-values indicate when the null hypothesis \u201cthe samples have the same mean\u201d can be rejected. Blue p-values highlight cases where Neuromap performs significantly better than Infomap; red p-values highlight cases where Infomap performs significantly better than Neuromap. ", "page_idx": 20}, {"type": "table", "img_path": "aFWx1N84Fe/tmp/c2d9a9e8927ef7979f1047c6c0e5fb77297948b714c1bc6a6a82da55c51eb9ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.5 Tabulated number of detected communities on real-world networks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here, we report the average number of detected communities for Neuromap, DMoN, NOCD, DiffPool, M\u221ainCut, Ortho, and Infomap on real-world networks for \u221asetting the number of hidden dimensions to $4\\sqrt{n}$ and the maximum number of communities to $s=\\sqrt{n}$ . ", "page_idx": 20}, {"type": "table", "img_path": "aFWx1N84Fe/tmp/a0b475d879d59fb405fee3d886a7ceb268b0d9ddec82be93fba26944628ed865.jpg", "table_caption": ["Table 5: Average number of detected communities and their standard deviations on real-world networks. We tested each method with 2-layer MLP, GCN, GIN, and SAGE architectures, except for Infomap, which is not based on deep learning. OOM stands for \u201cout of memory\u201d. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D Results on real-world networks with fixed number of hidden channels ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We provide further results on real-world networks with a different configuration of the neural networks: we set the number of hidden channels to 512 and set the maximum number of communities to the \u201cground-truth\u201d number of communities, that is $s=|Y|$ . ", "page_idx": 21}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/cf2f2bc302c9ded5bf7348b43f0a7fdc719893c7e534d53dce0a79fa11a29e45.jpg", "img_caption": ["D.1 Average Achieved AMI ", "Figure 13: Average achieved AMI on real-world networks (higher is better) for $s\\,=\\,|Y|$ , that is, the \u201cground-truth\u201d number of communities. Colours indicate methods while shapes indicate neural network architectures. DiffPool ran out of memory on obg-arxiv. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.2 Average Number of Detected Communities ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/ea87781e536643e2c386804679eee459c2a3ddc864543820a752db8e1b9ce25f.jpg", "img_caption": ["Figure 14: Average detected number of communities on real-world networks. Colours indicate methods while shapes indicate neural network architectures. The dashed horizontal lines show the correct number of communities for each dataset, which is also the maximum allowed number of communities. We omit Infomap in the plot due to the large number of detected communities. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.3 Tabulated AMI results on real-world networks ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here, we report the detailed AMI performance for Neuromap, DMoN, NOCD, DiffPool, MinCut, Ortho, and Infomap on real-world networks for setting the number of hidden dimensions to 512 and the maximum number of communities to $s=|Y|$ . When several of the best-performing methods achieve similar AMI, we use an independent two-sample t-test to determine whether one of them can be considered to perform better than the other. In cases where their performances do not differ significantly, we mark both as best. ", "page_idx": 22}, {"type": "text", "text": "Table 6: Average AMI in $\\%$ (higher is better) and their standard deviations on real-world networks. For each dataset, we highlight the best scores in bold and underline the second-best score. We tested each method with 2-layer MLP, GCN, GIN, and SAGE architectures, except for Infomap, which is not based on deep learning. OOM stands for \u201cout of memory\u201d. ", "page_idx": 22}, {"type": "table", "img_path": "aFWx1N84Fe/tmp/9a900506a4be326c6307c0523aad640a2d85c9dc1d3949c2607c387f553c7a06.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.4 Significance of best Neuromap results vs. best baseline ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Table 7: Independent two-sample t-test between 25 samples of AMI values for Neuromap vs. the best baseline for 512 hidden features and $s=|Y|$ . The p-values indicate for which datasets the null hypothesis \u201cthe samples have the same mean\u201d can be rejected. Red p-values highlight cases where the best baseline performs significantly better than Neuromap. ", "page_idx": 22}, {"type": "table", "img_path": "aFWx1N84Fe/tmp/5b624a3befa55d23d822549e45b9a6745462eca067e8db17bef1f011e415ee31.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.5 Significance of Neuromap results vs. Infomap ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Table 8: Independent two-sample t-test between 25 samples of AMI values for Neuromap with different (G)NN architectures vs. Infomap for 512 hidden features and $s=|Y|$ . The p-values indicate when the null hypothesis \u201cthe samples have the same mean\u201d can be rejected. Blue p-values highlight cases where Neuromap performs significantly better than Infomap; red p-values highlight cases where Infomap performs significantly better than Neuromap. ", "page_idx": 23}, {"type": "table", "img_path": "aFWx1N84Fe/tmp/6b9d0aff88e5f74490932d16b850ee302aad37322612d652be9c9c080bb4d31d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.6 Tabulated number of detected communities on real-world networks ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here, we report the average number of detected communities for Neuromap, DMoN, NOCD, DiffPool, MinCut, Ortho, and Infomap on real-world networks for setting the number of hidden dimensions to 512 and the maximum number of communities to $s=|Y|$ . ", "page_idx": 23}, {"type": "text", "text": "Table 9: Average number of detected communities and their standard deviations on real-world networks. We tested each method with 2-layer MLP, GCN, GIN, and SAGE architectures, except for Infomap, which is not based on deep learning. OOM stands for \u201cout of memory\u201d. ", "page_idx": 23}, {"type": "table", "img_path": "aFWx1N84Fe/tmp/1c599842c35c52f315767ce8a217e416dad999011f17773482a717d669a42c18.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Performance impact of hidden layers and max. number of communities ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here,\u221a we report the performance difference in ter\u221ams of achieved average AMI score between using (i) $4\\sqrt{n}$ hidden layers and a maximum of $s=\\sqrt{n}$ communities, and (ii) 512 hidden layers and a maximum of $s=|Y|$ communities, that is, the \u201cground-truth\u201d number of communities. The tabulated values show much better or worse the methods perform when using setup (ii) as compared to setup (i). That is, a positive value, shown in green, means that setup (ii) gives a better result while a negative value, shown in red, means that setup (i) gives a better result. ", "page_idx": 24}, {"type": "text", "text": "We find that, for Neuromap, NOCD, and DiffPool, setup (i) generally works better. In contrast, DMoN, MinCut, and Ortho often perform better with setup (ii), indicating that they require knowing the correct number of communities to perform well. ", "page_idx": 24}, {"type": "text", "text": "Table 10: Performance difference\u221a in terms of achieved average AMI score between (i) $4\\sqrt{n}$ hidden layers and a maximum of $s\\,=\\,{\\sqrt{n}}$ communities, and (ii) 512 hidden layers and a maximum of $s=|Y|$ communities, that is, the \u201cground-truth\u201d number of communities. OOM stands for \u201cout of memory\u201d. ", "page_idx": 24}, {"type": "table", "img_path": "aFWx1N84Fe/tmp/9a7ccf5171bc526206ce9ea48d2f575eb7e22cfffeeb5a0fd40495fcfc13f5f2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "F Further results on a synthetic network with overlapping communities ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Here we show the detected communities on a synthetic network with overlapping communities with a dense linear layer, a 2-layer MLP, a 2-layer GIN, and a 2-layer SAGE architecture. We use $|V|$ hidden channels and set the maximum number of communities to $s\\in\\{2,3\\}$ . ", "page_idx": 25}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/b04547e26782fd86a76a07ffd9e32f5ccf549757b46fab157c34832d30b237b0.jpg", "img_caption": ["F.1 Linear-layer-based results "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 15: Linear-layer-based results on a synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualise their community assignments. The top and bottom rows show results for a maximum of $s=2$ and $s=3$ communities, respectively. ", "page_idx": 25}, {"type": "text", "text": "F.2 MLP-based results ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/4fa1ce5a120c0bdbea042a5e962c267a7e8565fafdd407d1fa85f2c00f1a3929.jpg", "img_caption": [], "img_footnote": ["Figure 16: MLP-based results on a synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualise their community assignments. The top and bottom rows show results for a maximum of $s=2$ and $s=3$ communities, respectively. "], "page_idx": 25}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/3ebefc4c4b7db6991f7bec7f15b426cfb34c30f78ae4cf7f4e6d2483e021efeb.jpg", "img_caption": ["F.3 GIN-based results "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 17: GIN-based results on a synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualise their community assignments. The top and bottom rows show results for a maximum of $s=2$ and $s=3$ communities, respectively. ", "page_idx": 26}, {"type": "text", "text": "F.4 SAGE-based results ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "aFWx1N84Fe/tmp/74b1565470213ecfc26ac67b458a17166c701c8f7ada5b86820babc8d0f19056.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 18: SAGE-based results on a synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualise their community assignments. The top and bottom rows show results for a maximum of $s=2$ and $s=3$ communities, respectively. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Abstract and introduction introduce the research gap and summarise the developed method and experimentally achieved results on a high level. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss the limitation of our work in section 4. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We focus on adapting the well-established map equation framework for community detection with (graph) neural networks and show its utility empirically, however, our work does not include theoretical results or proofs. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our main contribution is adapting the map equation as a differentiable loss function for optimisation with (graph) neural networks. We present the mathematical formulation of our adaptation in section 4. In section 5, we describe our experimental setup, including hyperparameter settings for our experiments. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We made our code available through a public GitHub repository and the synthetic networks through a Zenodo repository. The used real-world networks are publicly available. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We detail our experimental settings, including hyperparameters, in section 5. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We report the average achieved performance with standard deviations. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] Justification: We describe the utilised computational hardware in section 5. ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have reviewed the code of ethics and find that we conform with it. ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: We consider the task of detecting communities in complex networks. While any algorithm can potentially be abused for malicious purposes, we believe that, in our case, this would be a rather contrived scenario. ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: We find it not applicable to provide safeguards in the context of detecting communities in complex networks to understand their organisational structure. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We cite the original works of the models we use as baselines as well as the sources of the datasets we use. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not introduce new assets. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our work involves neither crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: Our work involves neither crowdsourcing nor research with human subjects. ", "page_idx": 28}]