[{"figure_path": "aFWx1N84Fe/figures/figures_2_1.jpg", "caption": "Figure 1: Coding principles behind the map equation. Colours indicate modules, codewords are shown next to nodes, and the black trace shows a sequence of random-walker steps. Left: All nodes belong to the same module and all codewords are unique. Encoding the random walk sequence requires 60 bits, or 3.72 bits per step in the limit. Right: Partitioning the network enables reusing codewords across modules, reducing the codelength. However, for a unique encoding, we need to introduce codewords for entering and exiting modules, shown next to the arrows pointing into and out of the modules. With this modular coding scheme, we can compress the description to 48 bits, or 3.01 bits per step in the limit. Middle: The two encodings of the random walker's steps.", "description": "This figure illustrates the coding principles behind the map equation used for community detection. It compares two scenarios: one where all nodes belong to a single module (left), requiring longer codewords, and another where nodes are partitioned into modules (right), allowing for shorter codewords by reusing them. The middle part shows the two encodings for comparison. The figure demonstrates how community detection, via the map equation, essentially reduces to a compression problem.", "section": "Background: the map equation"}, {"figure_path": "aFWx1N84Fe/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of the setup for GNN-based community detection with the map equation. We learn soft cluster assignments S from the graphs adjacency matrix A and the node features X. Here, we allow up to four clusters. When no node features are available, we set X = A.", "description": "This figure illustrates the process of community detection using graph neural networks and the map equation.  A graph neural network (GNN) takes the graph's adjacency matrix (A) and node features (X) as input.  The GNN processes this information to produce a soft cluster assignment matrix (S), which represents the probability of each node belonging to each of the s clusters (maximum 4 in this example). This matrix S is then used to compute the codelength, L(A,S), via the map equation. The goal is to minimize this codelength through backpropagation, effectively learning optimal cluster assignments.  When node features are unavailable, the adjacency matrix is used as a substitute for node features.", "section": "4 The map equation goes neural"}, {"figure_path": "aFWx1N84Fe/figures/figures_5_1.jpg", "caption": "Figure 3: Performance for Neuromap using a dense linear layer, MLP, GCN, GIN, and SAGE architectures with two layers and Infomap on directed and undirected LFR networks with planted communities. The results show averages of partition quality measured with AMI, number of detected communities |M|, and codelength L. The shaded areas show one standard deviation from the mean.", "description": "This figure displays the performance of Neuromap (using different neural network architectures) and Infomap on directed and undirected LFR benchmark networks with planted communities.  The performance is evaluated using three metrics: Adjusted Mutual Information (AMI), the number of detected communities (|M|), and the codelength (L).  Higher AMI values indicate better performance. The shaded areas represent one standard deviation from the mean, illustrating variability in the results. The x-axis represents the mixing parameter (\u03bc) which controls the amount of noise or randomness in the network structure.", "section": "5.1 Synthetic networks with planted communities"}, {"figure_path": "aFWx1N84Fe/figures/figures_6_1.jpg", "caption": "Figure 3: Performance for Neuromap using a dense linear layer, MLP, GCN, GIN, and SAGE architectures with two layers and Infomap on directed and undirected LFR networks with planted communities. The results show averages of partition quality measured with AMI, number of detected communities M, and codelength L. The shaded areas show one standard deviation from the mean.", "description": "This figure displays the performance comparison of Neuromap (with different neural network architectures) and Infomap on directed and undirected LFR benchmark networks.  The x-axis represents the mixing parameter (\u03bc), and the y-axis shows three different metrics: Adjusted Mutual Information (AMI), the number of detected communities (M), and the codelength (L).  Each metric's average value and standard deviation are shown for various \u03bc values.  The plot helps to assess Neuromap's performance against a well-established baseline.", "section": "5.1 Synthetic networks with planted communities"}, {"figure_path": "aFWx1N84Fe/figures/figures_7_1.jpg", "caption": "Figure 5: Average achieved AMI on real-world networks (higher is better) with s = \u221an. Colours indicate methods while shapes indicate neural network architectures. DiffPool ran out of memory on the ogb-arxiv dataset.", "description": "This figure compares the performance of Neuromap and several baseline methods on ten real-world datasets, where each node has multiple features. The x-axis represents the datasets, and the y-axis represents the average Adjusted Mutual Information (AMI) score. The AMI score measures the similarity between the detected communities and the ground truth communities. The higher the score, the better.  Different shapes represent various neural network architectures (Linear, MLP, GCN, GIN, and SAGE) and Neuromap. A dashed horizontal line indicates the maximum allowed number of communities, which is set to \u221an. The figure shows that Neuromap achieves comparable or better performance in many datasets compared to the baseline methods and that the performance depends on the specific dataset and neural network architecture used. DiffPool failed on one dataset due to memory constraints.", "section": "5 Experimental evaluation"}, {"figure_path": "aFWx1N84Fe/figures/figures_8_1.jpg", "caption": "Figure 6: Synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualize their community assignments. The top and bottom rows show results for a maximum of s = 2 and s = 3 communities, respectively.", "description": "This figure displays results from various community detection methods on a small synthetic network with overlapping communities.  The leftmost graph represents the ground truth, showing the true community assignments of each node. The remaining graphs illustrate the results obtained from different methods (Neuromap, DMON, NOCD, DiffPool, MinCut, Ortho) for two different maximum numbers of communities (s=2 and s=3). Each node is depicted as a pie chart, with the proportions of the segments representing its assignment to the detected communities. This visualization helps compare how accurately each method identifies and represents overlapping community structures.", "section": "5.3 Synthetic networks with overlapping communities"}, {"figure_path": "aFWx1N84Fe/figures/figures_15_1.jpg", "caption": "Figure 1: Coding principles behind the map equation. Colours indicate modules, codewords are shown next to nodes, and the black trace shows a sequence of random-walker steps. Left: All nodes belong to the same module and all codewords are unique. Encoding the random walk sequence requires 60 bits, or 3.72 bits per step in the limit. Right: Partitioning the network enables reusing codewords across modules, reducing the codelength. However, for a unique encoding, we need to introduce codewords for entering and exiting modules, shown next to the arrows pointing into and out of the modules. With this modular coding scheme, we can compress the description to 48 bits, or 3.01 bits per step in the limit. Middle: The two encodings of the random walker's steps.", "description": "This figure illustrates the coding principles behind the map equation used for community detection.  The left panel shows a network where all nodes belong to a single module, resulting in a longer codeword length (60 bits) to encode random walks. The right panel shows the same network partitioned into modules, allowing for codeword reuse and a shorter codeword length (48 bits). The middle panel highlights the difference in encoding schemes between the two scenarios.", "section": "Background: the map equation"}, {"figure_path": "aFWx1N84Fe/figures/figures_16_1.jpg", "caption": "Figure 3: Performance for Neuromap using a dense linear layer, MLP, GCN, GIN, and SAGE architectures with two layers and Infomap on directed and undirected LFR networks with planted communities. The results show averages of partition quality measured with AMI, number of detected communities M, and codelength L. The shaded areas show one standard deviation from the mean.", "description": "This figure presents the performance comparison of Neuromap (using different neural network architectures: dense linear layer, MLP, GCN, GIN, and SAGE) and Infomap on synthetic LFR networks with planted communities.  The performance is evaluated using three metrics: Adjusted Mutual Information (AMI) to assess the quality of community detection; the number of detected communities (M); and the codelength (L) representing the description length of the random walk.  The shaded regions indicate standard deviations. The x-axis represents the mixing parameter (\u03bc), illustrating the transition between communities.", "section": "5.1 Synthetic networks with planted communities"}, {"figure_path": "aFWx1N84Fe/figures/figures_16_2.jpg", "caption": "Figure 3: Performance for Neuromap using a dense linear layer, MLP, GCN, GIN, and SAGE architectures with two layers and Infomap on directed and undirected LFR networks with planted communities. The results show averages of partition quality measured with AMI, number of detected communities M, and codelength L. The shaded areas show one standard deviation from the mean.", "description": "This figure displays the performance comparison between different neural network architectures (dense linear layer, MLP, GCN, GIN, and SAGE) and Infomap for community detection on synthetic LFR networks.  The evaluation metrics include the Adjusted Mutual Information (AMI) score, the number of detected communities (M), and the codelength (L). Higher AMI values indicate better clustering performance. The shaded regions represent one standard deviation from the mean across multiple trials, indicating the variability of results. The figure illustrates how different architectures handle communities under varying mixing parameter (\u03bc) values and different average degree values.", "section": "5.1 Synthetic networks with planted communties"}, {"figure_path": "aFWx1N84Fe/figures/figures_17_1.jpg", "caption": "Figure 3: Performance for Neuromap using a dense linear layer, MLP, GCN, GIN, and SAGE architectures with two layers and Infomap on directed and undirected LFR networks with planted communities. The results show averages of partition quality measured with AMI, number of detected communities M, and codelength L. The shaded areas show one standard deviation from the mean.", "description": "This figure compares the performance of Neuromap using various neural network architectures (dense linear layer, MLP, GCN, GIN, and SAGE) against Infomap on both directed and undirected LFR benchmark networks.  The x-axis represents the mixing parameter (\u03bc) of the LFR networks, which controls the strength of community structure. The top row shows the Adjusted Mutual Information (AMI), a metric measuring the similarity between the detected and the ground-truth community structures. The bottom row displays the number of communities detected (|M|) and the codelength (L).  The shaded regions indicate standard deviations across multiple runs for each model. The figure aims to demonstrate Neuromap's ability to accurately detect communities in networks with varying levels of community structure, even when using different neural network architectures.", "section": "5.1 Synthetic networks with planted communities"}, {"figure_path": "aFWx1N84Fe/figures/figures_17_2.jpg", "caption": "Figure 3: Performance for Neuromap using a dense linear layer, MLP, GCN, GIN, and SAGE architectures with two layers and Infomap on directed and undirected LFR networks with planted communities. The results show averages of partition quality measured with AMI, number of detected communities M, and codelength L. The shaded areas show one standard deviation from the mean.", "description": "This figure presents the performance comparison of Neuromap (using different neural network architectures) and Infomap on directed and undirected LFR benchmark networks with planted communities.  The performance is evaluated using three metrics: Adjusted Mutual Information (AMI) which measures the accuracy of the community detection, the number of detected communities (M), and the codelength (L) representing the description length of the random walk.  The x-axis represents the mixing parameter (\u03bc) which controls the level of mixing between communities, and different lines represent different methods. The shaded regions indicate standard deviation across multiple runs for each method and parameter.", "section": "5.1 Synthetic networks with planted communities"}, {"figure_path": "aFWx1N84Fe/figures/figures_18_1.jpg", "caption": "Figure 5: Average achieved AMI on real-world networks (higher is better) with s = \u221an. Colours indicate methods while shapes indicate neural network architectures. DiffPool ran out of memory on the ogb-arxiv dataset. Detailed tabulated results with standard deviations are included in Appendix C.", "description": "The figure shows the average achieved AMI (Adjusted Mutual Information) on ten real-world networks, comparing the performance of Neuromap against several baselines across different neural network architectures. The maximum number of communities (s) is set to the square root of the number of nodes (\u221an) for each dataset. The dashed horizontal lines indicate the ground truth number of communities. The plot also visualizes the impact of different neural network architectures on the performance.", "section": "5 Experimental evaluation"}, {"figure_path": "aFWx1N84Fe/figures/figures_21_1.jpg", "caption": "Figure 5: Average achieved AMI on real-world networks (higher is better) with s = \u221an. Colours indicate methods while shapes indicate neural network architectures. DiffPool ran out of memory on the ogb-arxiv dataset.", "description": "The figure shows the performance of different community detection methods on ten real-world datasets.  The AMI (Adjusted Mutual Information) score, a measure of clustering quality, is plotted for each method and network architecture.  The horizontal dashed lines represent the ground truth number of communities for each dataset.  The plot highlights that Neuromap and several other methods achieve comparable or better performance than the baselines.  One method (DiffPool) ran out of memory on the largest dataset, indicating a limitation in scalability.", "section": "5 Experimental evaluation"}, {"figure_path": "aFWx1N84Fe/figures/figures_21_2.jpg", "caption": "Figure 5: Average achieved AMI on real-world networks (higher is better) with s = \u221an. Colours indicate methods while shapes indicate neural network architectures. DiffPool ran out of memory on the ogb-arxiv dataset.", "description": "This figure shows the average achieved AMI (Adjusted Mutual Information) on ten real-world datasets for various community detection methods.  The AMI score quantifies how well the detected communities match the ground truth.  The different colors represent different community detection methods, and the different shapes represent the underlying neural network architecture used by each method. Note that the DiffPool method ran out of memory for the ogb-arxiv dataset. The horizontal lines indicate the maximum number of communities allowed for each dataset, reflecting the chosen maximum number of communities s=\u221an", "section": "5 Experimental evaluation"}, {"figure_path": "aFWx1N84Fe/figures/figures_25_1.jpg", "caption": "Figure 6: Synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualize their community assignments. The top and bottom rows show results for a maximum of s = 2 and s = 3 communities, respectively.", "description": "This figure shows the results of different community detection methods on a small synthetic network with overlapping communities. The network has nodes that belong to multiple communities simultaneously.  The 'True' column displays the ground truth community structure. The remaining columns show the results of Neuromap and several baseline methods (DMON, NOCD, DiffPool, MinCut, Ortho) for allowing either a maximum of two or three communities, respectively.  The visualization of nodes as pie charts helps understand the proportion of each node belonging to different communities, clearly demonstrating the performance of each method in detecting overlapping communities.", "section": "5.3 Synthetic networks with overlapping communities"}, {"figure_path": "aFWx1N84Fe/figures/figures_25_2.jpg", "caption": "Figure 6: Synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualize their community assignments. The top and bottom rows show results for a maximum of s = 2 and s = 3 communities, respectively.", "description": "This figure compares different community detection methods on a small synthetic network with overlapping communities. The true community structure is shown on the leftmost side.  Each subsequent column shows the community assignments detected by a different method: Neuromap, DMON, NOCD, DiffPool, MinCut, and Ortho.  The top and bottom rows represent the results obtained when allowing a maximum of 2 or 3 communities, respectively. The visual representation of nodes as pie charts helps to illustrate the degree of community overlap detected by each method.", "section": "5.3 Synthetic networks with overlapping communities"}, {"figure_path": "aFWx1N84Fe/figures/figures_26_1.jpg", "caption": "Figure 6: Synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualize their community assignments. The top and bottom rows show results for a maximum of s = 2 and s = 3 communities, respectively.", "description": "This figure compares the results of different community detection methods on a small synthetic network with overlapping communities. The true community structure is shown in the leftmost column. The remaining columns show the results obtained by Neuromap, DMON, NOCD, DiffPool, MinCut, and Ortho for a maximum number of communities (s) set to 2 and 3, respectively.  The pie charts within each node represent the community assignment probabilities, visually illustrating the level of overlapping between communities.", "section": "5.3 Synthetic networks with overlapping communities"}, {"figure_path": "aFWx1N84Fe/figures/figures_26_2.jpg", "caption": "Figure 6: Synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualize their community assignments. The top and bottom rows show results for a maximum of s = 2 and s = 3 communities, respectively.", "description": "This figure presents a comparison of community detection methods on a small synthetic network with overlapping communities. The true community structure is shown in the leftmost column. The subsequent columns display the results of different community detection methods (Neuromap, DMON, NOCD, DiffPool, MinCut, Ortho), with each method's results presented for a maximum of 2 and 3 communities, respectively.  The visualization uses pie charts within each node to illustrate the proportion of the node's membership in each community, thereby showcasing the ability (or lack thereof) of each method to identify and handle overlapping communities.", "section": "5.3 Synthetic networks with overlapping communities"}]