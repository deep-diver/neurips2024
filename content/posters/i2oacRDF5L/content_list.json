[{"type": "text", "text": "Belief-State Query Policies for User-Aligned Planning under Partial Observability ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Daniel Bramblett and Siddharth Srivastava ", "page_idx": 0}, {"type": "text", "text": "Autonomous Agents and Intelligent Robots Lab School of Computing and Augmented Intelligence Arizona State University, AZ, USA {drbrambl,siddharths}@asu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Planning in real-world settings often entails addressing partial observability while aligning with users\u2019 requirements. We present a novel framework for expressing users\u2019 constraints and preferences about agent behavior in a partially observable setting using parameterized belief-state query (BSQ) constraints in the setting of goal-oriented partially observable Markov decision processes (gPOMDPs). We present the first formal analysis of such constraints and prove that while the expected cost of a BSQ constraint is not a convex function w.r.t its parameters, it is piecewise constant and yields an implicit discrete parameter search space that is finite for finite horizons. This theoretical result leads to novel algorithms that optimize gPOMDP agent behavior with guaranteed user alignment. Theoretical analysis proves that our algorithms converge to the optimal user-aligned behavior in the limit. Empirical results show that BSQ constraints provide a computationally feasible approach for user-aligned planning in partially observable settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Users of sequential decision-making (SDM) agents in partially observable settings often have requirements and preferences on expected behavior, ranging from safety concerns to high-level knowledge of task completion requirements. However, users are ill-equipped to specify desired behaviors from such agents. For instance, although reward engineering can often encode fully observable preferences [Devidze et al., 2021, Gupta et al., 2023], it requires significant trial-and-error, and can produce unintended behavior even when done by experts working on simple domains [Booth et al., 2023]. These challenges are compounded in partially observable environments, where the agent will not know the full state on which the users\u2019 requirements and preferences are typically defined. For example, defining a reward function on the belief state to align the agent\u2019s behavior with the user can result in wireheading [Everitt and Hutter, 2016] (see Sec. 2 for further discussion on related work). ", "page_idx": 0}, {"type": "text", "text": "Consider a simplified, minimal example designed to illustrate the key principles (Fig. 1(a)). A robot located on a spaceship experiences a communication error with the ship and needs to decide whether to attempt to repair itself or the ship. Importantly, while a robot error is harder to detect, the user would rather risk repairing the robot than repairing the ship, as each repair risks introducing additional failures. In other words, the user may expect the robot to work with the following goals and preferences: The objective is to fix the communication channel. First, if there is a \u201chigh\u201d likelihood that the robot is broken, it should try to repair itself; otherwise, if there is a \u201chigh\u201d likelihood that the ship is broken, it should try to repair that. Such preferences go beyond preferences in fully observable settings: they use queries on the current belief state for expressing users\u2019 requirements while using the conventional paradigm of stating objectives in terms of the true underlying state. Such a formulation avoids wireheading, allowing users to express their constraints and preferences in partially observable settings. Although such constraints on behavior are intuitive and common, they leave a significant amount of uncertainty to be resolved by the agent: it needs to optimize the threshold values of \u201chigh\u201d probability under which each rule would apply while attempting to achieve the objective. ", "page_idx": 0}, {"type": "image", "img_path": "i2oacRDF5L/tmp/a235ee218af267c51ac72bbf8e179370860666d591d654fe6f27808df02a6ec2.jpg", "img_caption": ["Figure 1: (a) Spaceship Repair running example. (b) BSQ constraint for the user preference from the Introduction. (c) The expected cumulative cost function for (b) with a horizon of 12. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We introduce mathematical and algorithmic foundations for addressing these problems by defining constraints on behaviors in terms of properties of the belief state, expressed through belief-state queries (BSQs). We prove the surprising result that although the space of possible threshold values in preferences such as the one listed above is uncountably infinite, only a finite number of evaluations are required for computing optimal, user-aligned policies for finite-horizon problems. We use this result to develop a probabilistically complete algorithm for computing optimal constrained policies. Our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "1. A framework for encoding user requirements and preferences over agent behavior in goaloriented partially observable Markov decision processes (Sec. 3).   \n2. Mathematical analysis proving that the expected cost function of a BSQ constraint w.r.t its parameters is piecewise constant but generally non-convex. (Sec. 4).   \n3. A probabilistically complete algorithm for computing optimal user-aligned policies in goal-oriented POMDPs (Sec. 5).   \n4. Empirical evaluation on a diverse set of problems showing both the efficiency of our algorithm and the quality of the computed user-aligned policies. (Sec. 7). ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Planning over preferences has been well studied in fully observable settings [Baier et al., 2007, Aguas et al., 2016]. Voloshin et al. [2022] present an approach for complying with an LTL specification while carrying out reinforcement learning. Other approaches for using LTL specifications use the grounded state to create a reward function to teach reinforcement learning agents [Toro Icarte et al., 2018, Vaezipoor et al., 2021]. These approaches do not extend to partially observable settings as they consider agents that can access the complete state. ", "page_idx": 1}, {"type": "text", "text": "In partially observable settings, existing approaches for using domain knowledge and preferences require extensive, error-prone reward design and/or do not guarantee compliance. LTL specifications have been incorporated either by designing a reward function that incentivizes actions more likely to adhere to these specifications [Liu et al., 2021, Tuli et al., 2022] or by imposing a compliance threshold [Ahmadi et al., 2020]. In both approaches, the user calibrates rewards for user alignment with those for objective completion; it is difficult to ensure user alignment. We focus on the problem of guaranteeing user alignment without reward engineering. ", "page_idx": 1}, {"type": "text", "text": "Mazzi et al. [2021, 2023] proposed expressing domain control knowledge using belief state probabilities. Mazzi et al. [2021] used expert-provided rule templates and execution traces to construct a shield to prevent irregular actions. Mazzi et al. [2023] used execution traces and domain-specified belief-state queries to learn action preconditions over the belief state. Both approaches use input traces and focus on ensuring a policy is consistent with previously observed behavior. We address the complementary problem of computing user-aligned policies without past traces. ", "page_idx": 1}, {"type": "text", "text": "Belief-state queries have been used to solve POMDPs with uniform parameter sampling [Srivastava et al., 2012] but formal analysis, feasibility of optimizing BSQ constraints, and the existence of provably convergent algorithms have remained open as research questions prior to this work. ", "page_idx": 2}, {"type": "text", "text": "3 Formal Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section formally defines the BSQ framework for expressing user requirements on the agent\u2019s belief. Our framework is built for relational goal-oriented partially observable Markov decision processes. ", "page_idx": 2}, {"type": "text", "text": "3.1 Goal-Oriented Partially Observable Markov Decision Process ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Partially observable Markov decision processes (POMDPs) constitute a standard mathematical framework for modeling SDM problems in partially observable, stochastic settings [Kaelbling et al., 1998, Smallwood and Sondik, 1973]. State-of-the-art POMDP solvers often rely on approximate online approaches [Silver and Veness, 2010, Somani et al., 2013] where recent work addresses the problem of obtaining performance bounds [Barenboim and Indelman, 2023, Lim et al., 2023]. ", "page_idx": 2}, {"type": "text", "text": "We use goal-oriented POMDPs $\\mathrm{\\langlegPOMDPs\\rangle}$ ), where the agent tries to complete one of the tasks/goals. This eliminates the burden of error-prone reward engineering by using a default cost function that associates a constant cost for each timestep before reaching the goal. E.g., the Spaceship Repair problem (Sec. 1) has two objects: the robot and the spaceship. A state is defined using a Boolean function broken $(o)$ representing whether object $o$ needs repair and an integer-valued function rlocationpq representing the robot\u2019s location. These functions are not observable. The agent has two types of actions: try to repair object $)\\left(r e p a i r(o)\\right)$ or wait $(w a i t())$ . A transition function expresses the distribution of rlocationpq depending on the action taken and the robot\u2019s previous location. At each timestep, the robot receives a noisy observation $o b s\\_e r r(o)$ regarding the status of object $o$ . Thus the set of observations can be expressed as $\\{o b s\\_e r r(r o b o t),o\\bar{b}s\\_e r r\\bar{(}s h i p)\\}$ . Due to noisy perception, obs_error $(o)$ may not match broken $.(o)$ . An observation function denotes the probability of each observation conditioned on the (hidden) current state. E.g., ${P r(o b s\\_e r r(r o b o t)=}$ $\\bar{1}|b r o k e n(r o b o t)\\,=\\,1)\\,=\\,0.75.$ The goal is to reach the repair station corresponding to the truly broken component. We define gPOMDPs formally as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 1. $A$ goal-oriented partially observable Markov decision process $\\mathcal{P}$ is defined as $\\langle\\mathcal{C},\\mathcal{F},\\mathcal{A},\\mathcal{O},\\mathcal{T},\\Omega,\\bar{\\mathcal{G}},\\mathrm{Cost},H,b_{0}\\bar{\\rangle}$ where $\\mathcal{C}$ is the finite set of constant symbols and $\\mathcal{F}$ is the finite set of functions. The set of state variables for ${\\mathcal F}_{;}$ , $\\nu_{F}$ , is defined as all instantiations of functions in $\\mathcal{F}$ with objects in $O$ . The set of states $\\boldsymbol{S}$ is the set of all possible valuations for $\\nu_{F}$ ; $\\boldsymbol{\\mathcal{A}}$ is $a$ finite set of actions, $\\scriptscriptstyle\\mathcal{O}$ is a subset of $\\mathcal{F}$ of observation predicates, $\\mathcal{T}:S\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow[0,1]$ is the transition function $T(s,a,s^{\\prime})=P r(s^{\\prime}|a,s),$ ; ${\\mathcal{G}}\\subseteq{\\mathcal{S}}$ is the set of goal states that are also sink states, $\\Omega:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{O}\\rightarrow[0,1]$ is the observation function; $\\Omega(s,\\bar{a},\\bar{o})\\,=\\,P r(o|s,a),$ , $\\mathrm{Cost}(s)\\,=\\,\\{0\\ i f$ $s\\in\\mathcal{G};e l s e\\;1\\}$ is the cost function, $H$ is the horizon, and $b_{0}$ is the initial belief state. $A$ solution for a gPOMDP is a policy that has a non-zero probability of reaching $\\mathcal{G}$ in $H-1$ timesteps. ", "page_idx": 2}, {"type": "text", "text": "3.2 Belief-State Queries and Constraints ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Computing a policy for any $\\mathrm{gPOMDP}$ requires planning around state uncertainty. This is done using the concept of a belief state, which is a probability distribution over the currently possible states. Formally, the belief state constitutes a sufficient statistic for observation-action histories [Astrom et al., 1965]. We express user requirements using queries on the current belief state. ", "page_idx": 2}, {"type": "text", "text": "For any belief state $b$ , when action $a$ is taken and observation $o$ is observed, the updated belief state is computed using $b^{\\prime}(s^{\\prime})=\\alpha\\Omega(s^{\\prime},a,o)\\sum_{s}T(s,a,s^{\\prime})b(s)$ where $\\alpha$ is the normalization factor. We refer to this belief propagation as $b^{\\prime}\\;=\\;b p(b,a,o)$ . We extend the notation to refer to the sequential application of this equation to arbitrary bounded histories as $b p^{*}(b_{0},a_{1},o_{1},...,a_{n},o_{n})=$ $b p\\bar{(}\\cdot\\cdot\\cdot b p(b\\bar{p(}\\bar{b}_{0},a_{1},o_{1}),a_{2},o_{2})\\bar{\\cdot}\\cdot\\cdot.\\bar{)}$ . ", "page_idx": 2}, {"type": "text", "text": "For example, the user preference in the Spaceship Repair problem has the expression \u201ca high likelihood that the robot is broken\u201d. This can be expressed as a query on a belief state $b$ : $P r\\mathbb{[}b r o k e n(r o b o t)\\mathbb{]}_{b}\\ >\\ \\Theta_{r o b}$ where $\\Theta_{r o b}$ is a parameter. If rlocationpq is fully observable, the  expression \u201cth e robot location is smaller than $\\Theta_{l}$ in a belief state $b\\,`$ can be expressed as $P r\\mathbb{[}r l o c a t i o n()<\\Theta_{l}\\mathbb{]}_{b}==1$ . We can combine both queries to express \u201ca high likelihood the robot is broken and its location is lower than $\\Theta_{l}$ \u201d, as: $\\begin{array}{r}{P r\\mathbb[b r o k e n(r o b o t)]_{b}>\\Theta_{r o b}\\wedge P r\\mathbb[r l o c a t i o n()<}\\end{array}$ $\\Theta_{l}\\mathbb{J}_{b}==1$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Formally, BSQs use the vocabulary of the underlying gPOMDP. There are two types of queries we can ask: (1) whether formula $\\varphi$ is true with a probability that satisfies a threshold $\\Theta$ ; (2) whether the fully observable portion of the state satisfies a formula $\\varphi$ containing a threshold $\\Theta$ . These thresholds represent the parameters of a BSQ constraint. The agent must optimize these parameters to achieve the goal while aligning with the user\u2019s requirements. BSQs can be combined using conjunctions or disjunctions to express more complex requirements, which we define as a compound BSQ in Def. 3. We omit subscripts when clear from context. ", "page_idx": 3}, {"type": "text", "text": "Definition 2. $A$ belief-state query $\\lambda_{\\mathcal{P}}(b;\\varphi,\\circ,\\Theta)$ , where $b$ is a belief state, $\\varphi$ is a first-order logic formula composed of functions in gPOMDP $\\mathcal{P}$ , $\\bigcirc$ is any comparison operator, and $\\Theta\\,\\in\\,\\mathbb{R}$ is $a$ parameter, is defined as $\\lambda_{\\mathcal{P}}(b;\\varphi,\\circ,\\Theta)=P r[\\![\\varphi]\\!]_{b}\\circ\\Theta$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3. A compound BSQ $\\Psi(b;\\overline{{\\Theta}})$ , where $b$ is a belief state and $\\overline{{\\Theta}}\\in\\mathbb{R}^{n}$ , is either a conjunction or a disjunction of BSQs that contain $n$ total parameters. ", "page_idx": 3}, {"type": "text", "text": "We use BSQs to formally express user requirements of the form discussed in the introduction by mapping BSQs with variable parameters to actions. Fig. 1(b) illustrates this with a BSQ constraint for the Spaceship Repair problem. Formally, ", "page_idx": 3}, {"type": "text", "text": "Definition 4. Let b be a belief state and $\\overline{{\\Theta}}$ be a tuple of n parameter variables over $\\mathbb{R}$ . An n-parameter Belief-State Query constraint $\\pi(b,{\\overline{{\\Theta}}})$ is a tuple of rules $\\{r_{1},...,r_{m}\\}$ where each $r_{i}\\,=\\,\\Psi_{i}\\,\\to\\,a_{i}$ is composed of a compound BSQ $\\Psi_{i}$ and an action $a_{i}\\in\\mathcal{A}$ . The set $\\{\\Psi_{1},...,\\Psi_{m}\\}$ is mutually exclusive and covers the $n$ -dimensional parameter space $\\mathbb{R}^{n}$ . ", "page_idx": 3}, {"type": "text", "text": "In practice, mutually exclusive coverage is easily achieved using an $i f.$ .. then... else structure, where each condition includes a conjunction of the negation of preceding conditions and the list of rules includes a terminal else with the catchall BSQ True (Fig. 1). Any assignment of values $\\overline{{\\vartheta}}\\in\\mathbb{R}^{n}$ to the parameters $\\overline{{\\Theta}}$ of a BSQ constraint produces an executable policy that maps every possible belief state to an action: ", "page_idx": 3}, {"type": "text", "text": "Definition 5. A BSQ policy $\\pi(b,{\\overline{{\\vartheta}}})$ is a BSQ constraint $\\pi(b,{\\overline{{\\Theta}}})$ with an assignment in $\\mathbb{R}$ to each of the n parameters $\\overline{{\\Theta}}$ . ", "page_idx": 3}, {"type": "text", "text": "Let $P r_{t}^{\\pi}(\\mathcal{G})$ be the probability that an execution of a policy $\\pi$ reaches a state in $\\mathcal{G}$ within $t$ timesteps. A BSQ policy $\\pi(b,{\\overline{{\\vartheta}}})$ is said to be a solution to a $g P O M D P$ with goal $\\mathcal{G}$ and horizon $H$ iff $P r_{H-1}^{\\pi(b,\\overline{{{\\vartheta}}})}(\\mathcal{G})\\,>\\,0$ . The quality of a BSQ policy is defined as its expected cost; due to the uniform cost function in the definition of gPOMDPs, the expected cost of a BSQ policy is the expected time taken to reach a goal state. Formally, the expected cost of a $B S Q$ policy $\\pi(b,{\\overline{{\\vartheta}}})$ is $\\begin{array}{r}{E_{\\pi}(\\overline{{\\vartheta}};H)=\\sum_{t=1}^{H}t\\times P r_{\\mathcal{G},t}[\\pi(b,\\overline{{\\Theta}})]}\\end{array}$ , where $H$ is the horizon and $P r_{\\mathcal{G},t}[\\pi(b,\\overline{{\\Theta}})]$ is the probability of policy $\\pi(b,{\\overline{{\\vartheta}}})$ reaching a goal state for the first time at timestep $t$ . Thus, given a gPOMDP $\\mathcal{P}$ , with goal $\\mathcal{G}$ , and a BSQ constraint $\\pi(b,{\\overline{{\\Theta}}})$ , the objective is to compute: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{\\vartheta}}^{*}=a r g m i n_{\\overline{{\\vartheta}}}\\{E_{\\pi}(\\overline{{\\vartheta}};H):P r_{H-1}^{\\pi(b,\\overline{{\\vartheta}})}(\\mathcal{G})>0\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4 Formal Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our main theoretical result is that the continuous space of policy parameters is, in fact, partitioned into finitely many constant-valued convex sets. This insight allows the development of scalable algorithms for computing low-cost user-aligned policies. We introduce formal concepts and key steps in proving this result here; complete proofs for all results are available in the appendix. We begin with the notion of strategy trees to conceptualize the search process for BSQ policies. ", "page_idx": 3}, {"type": "text", "text": "4.1 Strategy Trees ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Every BSQ constraint $\\pi(b,{\\overline{{\\Theta}}})$ and a gPOMDP $\\mathcal{P}$ defines a strategy tree (e.g., Fig. 2(a)) that captures the possible decisions at each execution step. Intuitively, the tree starts at a belief node representing the initial belief state. Outgoing edges from belief nodes represent rule selection in $\\pi(b,{\\overline{{\\Theta}}})$ , resulting in action nodes. Outgoing edges from action nodes represent possible observations, leading to belief nodes representing the corresponding updated belief. If the tree is truncated at horizon $H$ , each leaf represents the outcome of a unique trajectory of rules from $\\pi(b,{\\overline{{\\Theta}}})$ and observations. ", "page_idx": 3}, {"type": "image", "img_path": "i2oacRDF5L/tmp/06d6ac08ffaf53fb062b0597f0249c64055d6ca2ca830025256c89ab15bee130.jpg", "img_caption": ["Figure 2: (a) Strategy tree created from BSQ constraint in Fig. 1 and Spaceship Repair gPOMDP for a horizon of 2. (b) Complete partitions of parameter space with partitions from two of the braids highlighted. Error detection sensor accuracy for the robot and ship is $60\\%$ and $75\\%$ , respectively. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Each belief node represents a belief state that can be calculated using the rule-observation trajectory leading to that node. A labeling function $l:V_{B}\\cup V_{A}\\rightarrow B\\cup A$ maps the set of belief nodes $V_{B}$ to belief states in $B$ and the set of action nodes $V_{A}$ to actions in $\\boldsymbol{\\mathcal{A}}$ . For ease of notation we define $b_{i}^{*}=l(v_{i})$ for all belief nodes $v_{i}\\in V_{B}$ and $a_{j}^{*}=l(v_{j})$ for all action nodes $v_{j}\\in V_{A}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 6. Let $\\mathcal{P}$ be a gPOMDP, $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint for $\\mathcal{P}$ , and let $b_{0}$ be the initial belief. The strategy tree $\\mathcal{T}_{\\pi}(b_{o})$ is defined as $\\bar{\\mathcal{T}}_{\\pi}(b_{o})=\\langle V,E\\rangle$ where $V=V_{B}\\cup V_{A}$ is a set containing belief nodes $V_{B}$ and action nodes $V_{A}$ , whereas, $E=E_{B}\\cup E_{A}$ is a set containing edges from belief nodes to action nodes $(E_{b}\\subseteq V_{B}\\times V_{A})$ and edges from action nodes to belief nodes $(E_{A}\\subseteq V_{A}\\times V_{B})$ . $E_{B}$ is defined as $\\{(v_{i},r,v_{j})|v_{i}\\in V_{B},v_{j}\\in\\bar{V}_{A},\\dot{r}\\in\\pi(b,\\overline{{\\Theta}})$ , and $\\exists\\Psi:r=\\Psi\\to a_{j}^{*}\\}$ . $E_{a}$ is defined as $\\{(v_{m},o,v_{n})|v_{m}\\in V_{A},v_{n}\\in V_{B},o\\in\\mathcal{O},\\exists(v_{p},r=\\Psi\\rightarrow a,v_{m})\\in E_{b};b_{n}^{*}=b p(\\vec{b}_{p}^{*},a,o)\\}.$ ", "page_idx": 4}, {"type": "text", "text": "Non-convexity of the expected cost function for BSQ constraints Each BSQ constraint permits infinitely many BSQ policies, one for each assignment of real values to its parameters. Unfortunately, the expected cost of BSQ constraints is not a convex function of these parameters. Fig. 1(c) shows this with a counterexample using the BSQ constraint from Fig. 1(b), a horizon of 12, and setting the robot\u2019s initial distance from each repair station to 5. This plot was constructed by sampling the expected cost for a quarter of a million equally-spaced parameter assignments to the BSQ constraint in Fig. 1(a). $E_{\\pi}(\\overline{{\\vartheta}};H)$ is clearly not convex: the expected cost along the line $\\Theta_{2}=\\Theta_{1}-0.25$ has two inflection points at $\\Theta_{1}=0.6$ and $\\Theta_{1}\\,=\\,0.8$ . This creates two local minima: $\\Theta_{1}\\leqslant0.16$ and $\\Theta_{1}\\geqslant0.83\\wedge\\Theta_{2}\\leqslant0.1$ . Intuitively, this is due to the short horizon, which causes the optimal strategy to be selecting a repair station and traversing to it regardless of the observations. This makes it difficult to compute good BSQ policies using existing solvers. However, every possible BSQ policy can be associated with a set of strategy tree leaves that are reachable under that policy. Thus, for a given horizon, there are only finitely many expected costs for BSQ policies for a given problem. ", "page_idx": 4}, {"type": "text", "text": "The main challenge in computing good BSQ policies is that the set of possible BSQ policies with distinct expected costs grows exponentially with the horizon and good BSQ parameters could be distributed arbitrarily in the high-dimensional, continuous space of parameter values. We use strategy trees to define groups of leaves called braids, which we will then use to prove that the space of BSQ policy parameters turns out to be well-structured in terms of the expected cost function. ", "page_idx": 4}, {"type": "text", "text": "Braids We refer to the set of all leaves reachable under a policy $\\pi(b,{\\overline{{\\vartheta}}})$ as the braid of $\\overline{{\\vartheta}}$ . Due to the mutual exclusivity of rules for every assignment of parameter values to a BSQ constraint, at most, one outgoing edge can be taken from each belief node (as these correspond to the rules and actions). However, the stochasticity of dynamics and observations allows for multiple outgoing edges to be possible from action nodes. E.g., in the strategy tree for the Spaceship Repair problem (Fig. 2(a)), leaves $\\ell_{2}$ and $\\ell_{10}$ cannot both be reachable under a BSQ policy because that would require rules $r_{1}$ and $r_{2}$ to be satisfied at the same belief. However, both $\\ell_{1}$ and $\\ell_{5}$ may be reachable under the same BSQ policy since their paths diverge on an action node. Formally, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Definition 7. Let $H$ be the horizon, and let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint for a gPOMDP $\\mathcal{P}$ . The braid of a parameter assignment $\\overline{{\\vartheta}}$ , brai $\\dot{\\cdot}d_{\\pi,H}(\\overline{{\\vartheta}})$ , is the set of all leaves in strategy tree $\\mathcal{T}_{\\pi}(b_{0})$ rooted at the initial belief $b_{0}$ that can be reached while executing $\\pi(b,{\\overline{{\\vartheta}}})$ : $\\begin{array}{r l r}{b r a i d_{\\pi,H}(\\overline{{\\vartheta}})}&{{}=}&{\\{\\ell_{H}\\quad:}\\end{array}$ the path to $\\ell_{H}$ is $(\\boldsymbol{r}_{1},o_{1},...,\\boldsymbol{r}_{H},o_{H});\\forall i$ $\\begin{array}{r c l c r c l}{r_{i}}&{=}&{\\Psi_{i}}&{\\to}&{a}\\end{array}$ i, $b_{i}\\quad=$ $b p^{*}(b_{0},r_{1},o_{1},...,r_{i},o_{i})$ and $\\overline{{\\vartheta}}$ satisfies $\\Psi_{i}$ . ", "page_idx": 5}, {"type": "text", "text": "The unique interval of parameter values where a leaf is reachable can be calculated by taking the intersection of the parameter intervals needed to satisfy each rule on the path to that leaf. This is because for any compound BSQ $\\Psi$ , we can compute the unique interval of parameter values $I(\\Psi)$ under which $b$ will satisfy $I(\\Psi)$ by substituting each BSQ in $\\Psi$ with its corresponding inequality: ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Let $\\Psi(b;\\overline{{\\Theta}})$ be an $n$ -dimensional compound BSQ. There exists a set of intervals $I(\\Psi)\\subseteq$ $\\mathbb{R}^{n}$ s.t. $\\Psi(b;\\overline{{\\Theta}})$ evaluates to true iff ${\\overline{{\\Theta}}}\\in I(\\Psi)$ . ", "page_idx": 5}, {"type": "text", "text": "We can utilize this result to compute the unique interval of parameter values consistent with a braid by taking the intersection of the intervals of each leaf contained in that braid (Def. 8): ", "page_idx": 5}, {"type": "text", "text": "Definition 8. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ be the initial belief state, and $H$ be the horizon. The interval of leaf $\\ell_{i}$ , $I(\\ell)$ , is defined as the intersection of intervals $\\bigcap_{i}I(\\Psi_{i})$ of the conditions of each rule $r_{i}$ that occurs in the path to that leaf. The interval for a s et\u015e of leaves $L$ is defined as $\\textstyle I(L)=\\bigcap_{\\ell_{i}\\in L}I(\\ell_{i})$ . ", "page_idx": 5}, {"type": "text", "text": "Any leaf or braid with an empty parameter interval does not align with the user\u2019s requirements. For example, in Fig. 2, note that $r_{1}$ is the only rule satisfiable if $r_{1}$ is selected from $b_{0}$ and the robot is observed to be broken. Using the Fig. 1(b) policy and assuming the sensor accuracy is $60\\%$ , picking a rule other than $r_{1}$ implies that $50\\%$ likelihood was high enough to fix the robot yet $60\\%$ was not, which is a contradiction. Removing misaligned leaves and braids prunes the tree. ", "page_idx": 5}, {"type": "text", "text": "4.2 BSQ Policies are Piecewise Constant ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now use the concept of braids to prove that the continuous, high-dimensional space of parameter values of a BSQ constraint reduces to a finite set of contiguous, convex partitions with each partition having a constant expected cost. This surprising result implies that although the expected cost of BSQ policies is not a convex function of parameter assignments, optimizing a BSQ constraint requires optimization over a finite set rather than over a continuous space. We first define a notion of similarity over assignments to BSQ constraints that define BSQ policies: ", "page_idx": 5}, {"type": "text", "text": "Definition 9. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, and $H$ be the horizon, and let $\\overline{{\\vartheta}}_{1},\\overline{{\\vartheta}}_{2}$ be assignments to $\\overline{{\\Theta}}$ . $\\overline{{\\vartheta}}_{1}$ is said to be similar to $\\overline{{\\vartheta}}_{2}$ , $\\overline{{\\vartheta}}_{1}\\equiv_{H}\\overline{{\\vartheta}}_{2}$ , iff $b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})=b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})$ . ", "page_idx": 5}, {"type": "text", "text": "It is trivial to $\\operatorname{show}\\equiv_{H}$ is transitive, symmetric, and reflexive, making it an equivalence relation over $\\mathbb{R}^{n}$ . As such, $\\equiv\\!H$ defines a partition over the same space: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ be the initial belief state, and $H$ be the horizon. The operator $\\equiv\\!H$ partitions $\\mathbb{R}^{n}$ . ", "page_idx": 5}, {"type": "text", "text": "However, this result is not sufficient to define the structure of partitions induced in $\\mathbb{R}^{n}$ , which will be required for an efficient optimization algorithm. Based on Sec. 4.1, we know that leaves whose trajectories diverge due to different rules must not be in the same braid. Furthermore, a belief state can only lead to one set of possible observations for an action regardless of the BSQ policy being followed. Intuitively, this prevents braids from being proper subsets of each other, which implies that the parameter intervals for two braids can never have overlapping parameter intervals. This gives us the desired structure for partitions induced in the space of parameter values for BSQ constraints: there are parameter intervals corresponding to distinct braids in the policy tree. In other words, the set of braids partitions the parameter space into contiguous, high-dimensional intervals. This can be proved formally and stated as follows: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $b_{0}$ be the initial belief state, and $H$ be the horizon. Each partition $\\rho$ created by operator $\\equiv\\!H$ partitioning $\\mathbb{R}^{n}$ is the disjoint intervals, $\\rho\\subseteq\\mathbb{R}^{n}$ where $\\bar{\\forall\\vartheta}\\in\\bar{\\rho};$ , $\\begin{array}{r}{\\gamma a i d_{\\pi,H}(\\overline{{\\vartheta}})=L}\\end{array}$ where $L$ is a fixed set of leaves. ", "page_idx": 5}, {"type": "text", "text": "Since each partition corresponds to a braid and each braid corresponds to a fixed set of leaves, which defines the expected cost for all policies corresponding to that braid, all policies defined by a partition of the parameter space have a constant expected cost. As such, the domain of the expected cost function $E_{\\pi}(\\overline{{\\vartheta}};H)$ for $\\operatorname{gPOMDP}$ can be represented as the disjoint intervals of each braid partition. Thus, $E_{\\pi}(\\overline{{\\vartheta}};H)$ is piecewise constant. The following result formalizes this. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ be the initial belief state, and $H$ be the horizon. Each partition created by operator $\\equiv_{H}$ partitioning $\\mathbb{R}^{n}$ has a constant expected cost. ", "page_idx": 6}, {"type": "text", "text": "In some situations, the braids that partition the parameter space can be calculated in closed form (e.g., see the appendix for partitions for the Spaceship Repair problem). The next section uses the theory developed above to develop a general approach for computing the braids and intervals corresponding to a BSQ constraint, for evaluating the expected cost for each such partition, and for optimizing over these partitions. ", "page_idx": 6}, {"type": "text", "text": "5 Partition Refinement Search ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present a novel algorithm for optimizing the parameters for a BSQ constraint using the theory of braids developed above. The Partition Refinement Search (PRS) algorithm (Algo. 1) constructs the set of partitions using hierarchical partition selection and refinement, where a partition is selected to be refined, a leaf that can occur in that partition is sampled and evaluated, and the partitions are refined to isolate the interval of the braid corresponding to the sample. It keeps track of the hypothesized optimal partition $X_{o p t}$ with $X_{o p t}$ being the final result returned after timeout. ", "page_idx": 6}, {"type": "text", "text": "PRS constructs the first parameter space interval as the domain of all possible parameter values (line 3). This is set as the initial hypothesized optimal partition (line 4). In each iteration, a partition $\\rho$ is selected using exploration-exploitation approaches discussed in Sec. 6 (lines 6). A leaf $\\ell$ is sampled from $\\rho$ by uniformly sampling parameter value $\\overline{{\\vartheta}}$ from $\\rho$ \u2019s parameter intervals and ", "page_idx": 6}, {"type": "table", "img_path": "i2oacRDF5L/tmp/287c64b7660e7db9996cdf5e982d43dcf1da71649e63e7233855dbbe23fb9c00.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "performing rollouts from the initial belief state to a reachable leaf using the BSQ policy $\\pi(b,{\\overline{{\\vartheta}}})$ (lines 7 and 8). The sampled leaf $\\ell$ is used to refine partition $\\rho$ using the insight braids cannot overlap (Sec. 4.2). If there exists a subinterval of $\\rho$ where $\\ell$ does not occur, a new partition for this subinterval is constructed containing $\\rho$ \u2019s previous leaves and expected cost (line 9). The remaining portion of $\\rho$ , where $\\ell$ can occur, is used to construct a partition with an updated expected cost representing $\\rho$ \u2019s previous leaves and $\\ell$ (line 10). The hypothesized optimal partition is then updated (line 11). ", "page_idx": 6}, {"type": "text", "text": "PRS converges to the true optimal BSQ policies in the limit: ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ the initial belief state, and $H$ be the horizon. The likelihood of the Partition Refinement Search algorithm returning the optimal parameter interval converges to one in the limit of infinite samples. ", "page_idx": 6}, {"type": "text", "text": "Complexity analysis While the theoretical space and time complexity are linear in the number of leaves, due to PRS grouping leaves from the strategy tree (Def. 6), there is good reason to expect better performance in practice. As discussed in Sec. 4.1, strategy trees can get pruned with the removal of branches and leaves that do not align with the user\u2019s requirements. For example, in the Spaceship Repair problem using the Fig. 1 BSQ constraint, a third of the possible leaves are pruned at a horizon of two, and the pruning becomes even more significant for longer horizons. Additionally, empirical results suggest that rules earlier in rule-observation trajectories are more important in dictating the partitions. Furthermore, selecting and refining partitions can be performed in parallel, further improving performance. ", "page_idx": 6}, {"type": "text", "text": "We explored multiple partition selection approaches with a multiprocessing version of PRS in line 6. Each approach used the same dynamic exploration rate $e_{r}$ that diminished over time. Each thread managed a subset of partitions $X^{\\prime}\\subseteq X$ and updated a global hypothetical optimal partition. Additionally, new partitions were selected for sampling if they had fewer than five samples. In this paper, we focus on three selection approaches and discuss two others in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "Epsilon Greedy (PRS-Epsilon) We explore $e_{r}$ percent of the time by uniformly sampling $s\\sim U_{0}^{1}$ and checking if $s\\leqslant e_{r}$ . If we are exploring, we uniformly at random select a partition from $X^{\\prime}$ . Otherwise, the partition with minimum expected cost, $\\arg\\operatorname*{min}_{\\langle\\rho,\\hat{E}[\\rho]\\rangle\\in X^{\\prime}}\\hat{E}[\\rho]$ , is selected. ", "page_idx": 7}, {"type": "text", "text": "Boltzmann Exploration (PRS-Bolt) Partitions are selected in a weighted random fashion with the probability of selecting partition $\\rho$ as $\\alpha\\times e x p(\\hat{E}[\\rho]/e_{r})$ with $\\alpha$ being the normalization factor. ", "page_idx": 7}, {"type": "text", "text": "Local Thompson Sampling (PRS-Local) Each thread treats the problem as a multi-armed bandit problem where the expected cost for the next sample from each partition is simulated using $\\mathcal{N}(\\mu_{c},\\sigma_{c}\\times$ $e_{r}$ q with $\\mu_{c}$ and $\\sigma_{c}$ being the partition\u2019s mean and standard deviation, respectively. The partition with the lowest estimated expected cost is selected. ", "page_idx": 7}, {"type": "text", "text": "7 Empirical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We created an implementation of PRS and evaluated it on four challenging risk-averse problems. Complete source code is available in the supplementary material. We describe the problems and user preferences here; further details, including BSQ constraint listings, can be found in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "Lane merger (LM) In this problem, an autonomous vehicle driving on a two-lane road must switch lanes safely before reaching a lane merger. However, there is currently a car in the other lane that the agent does not know the location or speed of. Switching lanes too close to this car risks a severe accident. The autonomous vehicle has a noisy detection system that returns whether a vehicle is located in certain areas around the car. The user\u2019s preference is: If there\u2019s a high likelihood of safely switching lanes, do so. If there is a high likelihood of the other car being in close proximity and it is possible to slow down, slow down. Otherwise, keep going. ", "page_idx": 7}, {"type": "text", "text": "Spaceship repair (SR) This is a modified version of the running example with BSQ constraint Fig. 1(b). The robots start 7 steps and 5 steps away from the robot and ship repair stations, respectively. Additionally, the robot\u2019s sensor is $75\\%$ accurate at detecting errors with the robot and only $55\\%$ for the ship. With the short horizon $H=12$ , this results in the parameter space being not convex with multiple local minimums with differing expected costs. ", "page_idx": 7}, {"type": "text", "text": "Graph rock sample (GRS) We modified the classic RockSample $(n,k)$ problem [Smith and Simmons, 2004] by replacing the grid with a graph with waypoints where some waypoints contain rocks. Additionally, we introduced risk by causing the robot to break beyond repair if it samples a rock not worth sampling. We also categorized the rocks into types, and the rover\u2019s goal is to bring a sample of each type to the desired location if a safe rock for that type exists. This goal requires a longer horizon to reach compared to the other problems. The user\u2019s preference is: Evaluating rocks of types not sampled in order $r_{1},...,r_{n}$ , if the rock has a high likelihood of being safe to sample, go and take a sample of it. Else, if the rock has a high likelihood of being safe to sample, get close enough and scan it. Otherwise, move towards the exit if no rocks are worth sampling or scanning. ", "page_idx": 7}, {"type": "text", "text": "Store visit (SV) This problem is based on the partially observable OpenStreetMap problem in Liu et al. [2021]. A robot is located in a city where some locations are unsafe (e.g., construction, traffic), which can terminally damage the robot. The robot is initially uncertain of its location but it can scan its surroundings to determine its general location. The agent traverses the city and can visit the closest building. The goal is to visit a bank and then a store. This problem features a nuanced BSQ constraint: If you are significantly unsure of your current location, scan the area. If you have visited a bank, do the following to visit a store; otherwise, do it to visit a bank. If you are sufficiently sure the current location has the building you are looking for, visit it. Otherwise, move towards where you think that building is while avoiding unsafe locations. If all else fails, scan the current area. ", "page_idx": 7}, {"type": "image", "img_path": "i2oacRDF5L/tmp/f42ef0d70ce5d7adf5c6d47e31477f89449bbf64162c5736ca81fb8e1c6c5474.jpg", "img_caption": ["Figure 3: Empirical results evaluating the hypothesized optimal partition performance tracked. Equally spaced samples across PRS evaluation time are taken while a sample is taken each iteration of Nelder-Mead and Particle Swarm. The error displayed is the standard deviation error. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "7.1 Baselines ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluated PRS against three different types of baselines. ", "page_idx": 8}, {"type": "text", "text": "RCompliant Select random parameter values uniformly at random from the parameter space to produce user-aligned policies. ", "page_idx": 8}, {"type": "text", "text": "Hyperparamter optimization algorithms To measure the beneftis of PRS against existing hyperparameter optimization algorithms, we implemented both Nelder-Mead [Nelder and Mead, 1965] and Particle Swarm [Kennedy and Eberhart, 1995]. The expected cost of parameter space point \u03d1, for BSQ constraint $\\pi$ , was computed by averaging 1,000 parallel runs of the policy $\\pi(b;{\\overline{{\\vartheta}}})$ . For Nelder-Mead optimization, we used a simplex that had vertices numbering one more than the number of parameters in the BSQ constraint being optimized. To prevent poor initialization, the first 100 points are randomly sampled to construct the initial simplex using the best-performing points. For Particle Swarm optimization, 10 particles were used with the location and momentum of each particle clipped to the search space. The coefficients changed based on steps since the last improvement. ", "page_idx": 8}, {"type": "text", "text": "Unconstrained POMDP solvers To measure the differences between BSQ policies and unconstrained POMDP solvers, we implemented variations of our problems into POMDPX and solved them with DESPOT [Somani et al., 2013] and SARSOP [Kurniawati et al., 2009] for 1,000 evaluation runs. To measure whether an action-observation trajectory produced with these solvers aligns with the user\u2019s requirements, we check if there exist parameter values $\\overline{{\\vartheta}}$ where policy $\\pi(b;{\\overline{{\\vartheta}}})$ could produce that trajectory. We use this to evaluate the solutions produced. ", "page_idx": 8}, {"type": "text", "text": "7.2 Analysis of Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For each problem, we evaluated each baseline and PRS variant ten times. For Spaceship Repair, the horizon was 12, with PRS having a 3-minute timeout. For the other problems, the horizon was 100 with a 25-minute timeout. The timeout for Nelder-Mead and Particle Swarm was one hour. Note that the highest expected cost is equal to the horizon due to the default cost function. The performance of each PRS partition selection approach can be found in Figure 4 and the quality of solutions over time compared to the baselines are shown in Figure 3. Additional figures can be found in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "Partition selection approach evaluations PRS partition selection approaches converged to a similar quality policy. The only difference was the time taken with approaches that did not rely on the standard deviation converging faster due to there being a lower standard deviation near the optimal solution, causing selection approaches that used the standard deviation to explore the wrong partition. We use PRS-Epsilon as a representative when comparing against the other baselines. ", "page_idx": 8}, {"type": "text", "text": "PRS solution quality PRS produced a higherquality policy compared to the ones produced by RCompliant. For Spaceship Repair, the simplest problem solved on the shortest horizon, policies produced by PRS-Epsilon had a $17.05\\%$ lower expected cost and $3.41\\%$ higher goal achievement rate. For the other problems, policies produced by RCompliant had more than twice the expected cost and achieved only half the success rate on both Graph Rock Sample and Store Visit. These results demonstrate that optimizing BSQ parameter values has a significant impact on the performance of user-aligned policies. ", "page_idx": 9}, {"type": "image", "img_path": "i2oacRDF5L/tmp/44b220530fe1419d8d95e82bf789b58be821208d96cacc592f3ec34c27c867e0.jpg", "img_caption": ["Figure 4: Results for PRS with different partition selection approaches from Section 6. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Hyperparameter optimization evaluation Compared to traditional hyperparameter optimization algorithms, PRS always found the user-aligned policy with the lowest expected cost with little performance deviation. This is due to Nelder-Mead and Particle Swarm struggling to optimize a nonconvex piecewise-constant function using noisy data, resulting in known problems with local-search algorithms: problems of getting stuck in sub-optimal local minima and exploring the incorrect space. Additionally, PRS converged first since it is more sample-efficient. It is computationally expensive to update the belief state, resulting in poor-quality solutions being more expensive to evaluate due to taking longer to reach the goal. PRS\u2019s exploration only requires a couple of evaluations before spending the computational resources on more promising areas. On the other hand, Nelder-Mead and Particle Swarm require accurate expected costs to figure out where to explore. ", "page_idx": 9}, {"type": "text", "text": "An interesting result is that, in Spaceship Repair, solutions found by Nelder-Mead and Particle Swarm both had a $7.73\\%$ higher expected cost and $18.31\\%$ higher goal achievement rate than the PRS-Epsilon solutions. There is likely a high negative correlation between the expected cost and goal achievement rate. PRS is better at optimizing the stated objective of minimizing the expected cost. ", "page_idx": 9}, {"type": "text", "text": "Unconstrained solver evaluation Without the constraints and preferences encoded in the BSQ constraints, DESPOT and SARSOP struggled with this set of problems. SARSOP failed to converge to a policy within two days due to the long evaluation horizon. DESPOT could not run on Lane Merger, which had the largest state space and branching factor. DESPOT could only achieve the goal $0.5\\%$ of the time on Graph Rock Sample. DESPOT achieved a lower expected cost of $20.0\\%$ and $13.3\\%$ on variations of Spaceship Repair and Store Visit, respectively. However, DESPOT\u2019s policy only aligned with the user\u2019s requirements on Store Visit and only $7.3\\%$ of the time on Spaceship Repair. This indicates that the BSQ framework offers a new approach for expressing both domain knowledge and user requirements. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We presented the BSQ policy framework for expressing users\u2019 requirements over the belief state in partially observable settings for computing user-aligned agent behavior. We performed a formal analysis of these policies, proving that the parameter value space introduced in the BSQ constraints can be partitioned, resulting in BSQ constraints being optimizable through a hierarchical optimization paradigm. We introduced the probabilistically complete Partition Refinement Search algorithm to perform this optimization. Our empirical results show that it converges to the optimal user-aligned policy quicker and more consistently than existing algorithms. Results indicate that BSQ constraints provide a promising approach for solving diverse real-world problems requiring user alignment. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work There are many interesting directions for future work based on the current BSQ policy framework. BSQ representations can be made more expressive by allowing deterministic functions, which would not compromise the presented theoretical results. Furthermore, there exists a natural extension of this work into finite memory controllers that allows temporally extended requirements to be encoded with the same theoretical results. Relaxing the constraints on mapping each belief state to a single action would expand the usability. For more complex problems, a belief-state approximation approach would be required, but the underlying strategy tree discussed in this work would remain mostly unchanged. Another interesting research direction is to develop methods that help users express their requirements in the BSQ framework. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by NSF grant IIS 1942856. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Rati Devidze, Goran Radanovic, Parameswaran Kamalaruban, and Adish Singla. Explicable reward design for reinforcement learning agents. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 20118\u201320131. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/ paper_files/paper/2021/file/a7f0d2b95c60161b3f3c82f764b1d1c9-Paper.pdf. ", "page_idx": 10}, {"type": "text", "text": "Dhawal Gupta, Yash Chandak, Scott Jordan, Philip S. Thomas, and Bruno C. da Silva. Behavior alignment via reward function optimization. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 52759\u201352791. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ a5357781c204d4412e44ed9cbcdb08d5-Paper-Conference.pdf. ", "page_idx": 10}, {"type": "text", "text": "Serena Booth, W. Bradley Knox, Julie Shah, Scott Niekum, Peter Stone, and Alessandro Allievi. The perils of trial-and-error reward design: Misdesign through overftiting and invalid task specifications. Proceedings of the AAAI Conference on Artificial Intelligence, 37(5):5920\u20135929, Jun. 2023. doi: 10.1609/aaai.v37i5.25733. URL https://ojs.aaai.org/index.php/AAAI/article/view/ 25733. ", "page_idx": 10}, {"type": "text", "text": "Tom Everitt and Marcus Hutter. Avoiding wireheading with value reinforcement learning. In Artificial General Intelligence: 9th International Conference, AGI 2016, New York, NY, USA, July 16-19, 2016, Proceedings 9, pages 12\u201322. Springer, 2016. ", "page_idx": 10}, {"type": "text", "text": "Jorge A Baier, Christian Fritz, and Sheila A McIlraith. Exploiting procedural domain control knowledge in state-of-the-art planners. In ICAPS, pages 26\u201333, 2007. ", "page_idx": 10}, {"type": "text", "text": "Javier Segovia Aguas, Sergio Jim\u00e9nez Celorrio, and Anders Jonsson. Generalized planning with procedural domain control knowledge. In Proceedings of the International Conference on Automated Planning and Scheduling, volume 26, pages 285\u2013293, 2016. ", "page_idx": 10}, {"type": "text", "text": "Cameron Voloshin, Hoang Le, Swarat Chaudhuri, and Yisong Yue. Policy optimization with linear temporal logic constraints. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 17690\u201317702. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/70b8505ac79e3e131756f793cd80eb8d-Paper-Conference.pdf. ", "page_idx": 10}, {"type": "text", "text": "Rodrigo Toro Icarte, Toryn Q Klassen, Richard Valenzano, and Sheila A McIlraith. Teaching multiple tasks to an rl agent using ltl. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pages 452\u2013461, 2018. ", "page_idx": 10}, {"type": "text", "text": "Pashootan Vaezipoor, Andrew C Li, Rodrigo A Toro Icarte, and Sheila A. Mcilraith. Ltl2action: Generalizing ltl instructions for multi-task rl. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10497\u201310508. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/vaezipoor21a.html. ", "page_idx": 10}, {"type": "text", "text": "Jason Liu, Eric Rosen, Suchen Zheng, Stefanie Tellex, and George Konidaris. Leveraging temporal structure in safety-critical task specifications for pomdp planning. 2021. ", "page_idx": 10}, {"type": "text", "text": "Mathieu Tuli, Andrew Li, Pashootan Vaezipoor, Toryn Klassen, Scott Sanner, and Sheila McIlraith. Learning to follow instructions in text-based games. Advances in Neural Information Processing Systems, 35:19441\u201319455, 2022. ", "page_idx": 10}, {"type": "text", "text": "Mohamadreza Ahmadi, Rangoli Sharan, and Joel W Burdick. Stochastic finite state control of pomdps with ltl specifications. arXiv preprint arXiv:2001.07679, 2020. ", "page_idx": 10}, {"type": "text", "text": "Giulio Mazzi, Alberto Castellini, and Alessandro Farinelli. Rule-based shielding for partially observable monte-carlo planning. In Proceedings of the International Conference on Automated Planning and Scheduling, volume 31, pages 243\u2013251, 2021.   \nGiulio Mazzi, Daniele Meli, Alberto Castellini, and Alessandro Farinelli. Learning logic specifications for soft policy guidance in pomcp. arXiv preprint arXiv:2303.09172, 2023.   \nSiddharth Srivastava, Xiang Cheng, Stuart Russell, and Avi Pfeffer. First-order open-universe pomdps: Formulation and algorithms. Technical report, Technical report, EECS-2013-243, EECS Department, UC Berkeley, 2012.   \nLeslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99\u2013134, 1998.   \nRichard D Smallwood and Edward J Sondik. The optimal control of partially observable markov processes over a finite horizon. Operations research, 21(5):1071\u20131088, 1973.   \nDavid Silver and Joel Veness. Monte-carlo planning in large pomdps. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc., 2010. URL https://proceedings.neurips.cc/ paper_files/paper/2010/file/edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf.   \nAdhiraj Somani, Nan Ye, David Hsu, and Wee Sun Lee. Despot: Online pomdp planning with regularization. Advances in neural information processing systems, 26, 2013.   \nMoran Barenboim and Vadim Indelman. Online pomdp planning with anytime deterministic guarantees. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 79886\u201379902. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/fc6bd0eef19459655d5b097af783661d-Paper-Conference.pdf.   \nMichael H Lim, Tyler J Becker, Mykel J Kochenderfer, Claire J Tomlin, and Zachary N Sunberg. Optimality guarantees for particle belief approximation of pomdps. Journal of Artificial Intelligence Research, 77:1591\u20131636, 2023.   \nKarl J Astrom et al. Optimal control of markov decision processes with incomplete state estimation. Journal of mathematical analysis and applications, 10(1):174\u2013205, 1965.   \nTrey Smith and Reid Simmons. Heuristic search value iteration for pomdps. In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, UAI \u201904, page 520\u2013527, Arlington, Virginia, USA, 2004. AUAI Press. ISBN 0974903906.   \nJohn A Nelder and Roger Mead. A simplex method for function minimization. The computer journal, 7(4):308\u2013313, 1965.   \nJames Kennedy and Russell Eberhart. Particle swarm optimization. In Proceedings of ICNN\u201995- international conference on neural networks, volume 4, pages 1942\u20131948. ieee, 1995.   \nHanna Kurniawati, David Hsu, and Wee Sun Lee. Sarsop: Efficient point-based pomdp planning by approximating optimally reachable belief spaces. 2009. ", "page_idx": 11}, {"type": "text", "text": "A Appendix Organization ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The appendix is organized as follows. Appendix B contains the proofs for showing the expected cost function is piecewise constant. Appendix C contains the proof that PRS is probabilistically complete. Appendix D discusses the evaluation problems and provides the BSQ constraints used. Appendix E discussed additional implementation details of both Nelder-Mead and Particle Swarm. Appendix F discusses two additional partition selection approaches we tested and provides additional analysis of our results. Appendix G discusses the experimental setup and computational cost of our experiments. Appendix H contains the calculated closed-form solution of the partitions for the Spaceship Repair problem. Appendix I discusses the broader impacts of our work. Finally, Appendix J discusses additional limitations not discussed in the main paper. ", "page_idx": 12}, {"type": "text", "text": "B Lemmas and Proofs From Formal Analysis [Section 3] ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we provide the formal proofs for Lemma 1, Theorem 1, Theorem 2, and Theorem 3 from Section 3, where we proved that braids partition the parameter space resulting in the expected cost function of a BSQ constraint w.r.t its parameter being piecewise constant. We define and prove Lemmas 2, 3, 4, and 5 in this section for building these proofs. ", "page_idx": 12}, {"type": "text", "text": "First, we prove that the similarity operator $\\equiv\\!H$ for braids (Def. 9) has the properties of being reflexive, symmetric, and transitive. As such, $\\equiv\\!H$ defines an equivalence relation over the $\\mathbf{n}$ -dimensional parameter space $\\mathbb{R}^{n}$ , meaning it defines a partition over $\\mathbb{R}^{n}$ . ", "page_idx": 12}, {"type": "text", "text": "Theorem 1. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ be the initial belief state, and $H$ be the horizon. The operator $\\equiv\\!H$ partitions $\\mathbb{R}^{n}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Let $\\overline{{\\vartheta}}\\,\\in\\,\\mathbb{R}^{n}$ be $n$ -parameter values and $H$ be the horizon. By way of contradiction, let\u2019s assume that $\\overline{{\\vartheta}}$ is not similar to itself, $\\overline{{\\vartheta}}\\not\\equiv\\overline{{\\vartheta}}$ . This would mean that $b r a i d_{H,1}(\\overline{{{\\vartheta}}})\\,\\ne\\,b r a i d_{H,2}(\\overline{{{\\vartheta}}})$ . As such, there must exist a leaf $\\ell$ , which is in one but not the other braid. Note that $\\ell$ represents a unique rule-observation trajectory $\\left\\{{{r}_{1}},{{o}_{1}},...,{{r}_{H}},{{o}_{H}}\\right\\}$ . Additionally, for $\\ell$ to be in one of these braids it would need to be true that $\\forall i,r_{i}.\\Psi(b_{i}^{*},\\overline{{\\vartheta}})$ must be satisfied, where $b_{i}^{*}=b p^{*}(b_{0},r_{1},o_{1},...,r_{i},o_{i})$ (Def. 7). However, note that this would hold true for the other braid as well, making it a contradiction for $\\ell$ to be exclusive in either $b r a i d_{H,1}(\\overline{{\\vartheta}})$ or $b r a i d_{H,2}(\\overline{{\\vartheta}})$ . As such, $\\overline{{\\vartheta}}$ must be similar to itself meaning the similarity property holds. ", "page_idx": 12}, {"type": "text", "text": "Let $\\overline{{\\vartheta}}_{1},\\overline{{\\vartheta}}_{2},\\overline{{\\vartheta}}_{3}\\in\\mathbb{R}^{n}$ where $\\overline{{\\vartheta}}_{1}\\equiv_{H}\\overline{{\\vartheta}}_{2}$ and $\\overline{{\\vartheta}}_{2}\\equiv_{H}\\overline{{\\vartheta}}_{3}$ . Therefore, $b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1})\\nonumber\\,=\\,b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})$ and $b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})\\,=\\,b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{3})$ (Def. 7). Using substitution, $b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})\\,=\\,b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{3})$ meaning $\\overline{{\\vartheta}}_{1}\\equiv_{H}\\overline{{\\vartheta}}_{3}$ . As such, the transitive property holds. ", "page_idx": 12}, {"type": "text", "text": "Due to set equality being symmetric, the symmetric property holds. Thus, the operator $\\equiv\\!H$ is an equivalence relation over $\\mathbb{R}^{n}$ causing $\\equiv_{H}$ to define a partition over $\\mathbb{R}^{n}$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "For compound BSQs $\\Psi$ , we now prove that there exist unique intervals of the parameter space where $\\Psi$ is satisfied that we can calculate. ", "page_idx": 12}, {"type": "text", "text": "Lemma 1. Let $\\Psi(b;\\overline{{\\Theta}})$ be an $n$ -dimensional compound BSQ. There exists a set of intervals $I(\\Psi)\\subseteq$ $\\mathbb{R}^{n}$ s.t. $\\Psi(b;\\overline{{\\Theta}})$ evaluates to true iff ${\\overline{{\\Theta}}}\\in I(\\Psi)$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Let $\\mathcal{P}$ be a gPOMDP, $b$ be a belief state, $\\Theta\\in\\mathbb{R}$ be a parameter, $\\bigcirc$ be a comparison operator, and $\\varphi$ be a first-order logic formula composed of functions from $\\mathcal{P}$ . There exist two possible forms for a BSQ (Def. 2). Let $\\lambda_{p}(\\bar{b};\\varphi,\\circ,\\Theta)=\\bar{P r}[\\mathbb{1}\\varphi]_{b}\\circ\\Theta$ . Note that $P r[\\![\\varphi]\\!]_{b}$ evaluates into the probability of $\\varphi$ being satisfied in a belief state $b$ . Theref or e, we can simplify $\\lambda_{p}(b;\\varphi,\\circ,\\Theta)$ to $p\\circ\\Theta$ where $p\\in[0,1]$ , meaning this type of BSQ simplifies to an inequality. Now, let $\\begin{array}{r}{\\dot{\\lambda_{p}}(b;\\varphi,\\circ,\\Theta)=P r[\\![\\varphi]\\!]_{b}==1}\\end{array}$ where $\\varphi$ is composed of $\\Theta$ and fully observable functions in $\\mathcal{P}$ . We assume that $\\Theta$ ca nno t be used as a function parameter, meaning that it must be an operand of a relational operator in $\\varphi$ . Since the functions are fully observable, they can be evaluated for $b$ , leaving the inequalities involving $\\Theta$ to dictate whether $\\varphi$ is satisfied. Thereby, BSQs evaluate to inequalities involving $\\Theta$ . ", "page_idx": 12}, {"type": "text", "text": "A compound BSQ $\\Psi$ comprises conjunctions/disjunctions of BSQs by Definition 3. By substituting each BSQ with its inequalities, we can calculate the interval of $\\Psi,I(\\dot{\\Psi})$ . ", "page_idx": 12}, {"type": "text", "text": "Let us assume that $\\Theta\\in{\\cal{I}}(\\Psi)$ . By way of contradiction, let us assume that $\\Theta$ does not satisfy $\\Psi$ . If $\\Psi$ is a conjunction of BSQs, there exists at least one BSQ that is not satisfied by $\\Theta$ . If $\\Psi$ is a disjunction, all the BSQs are unsatisfied by $\\Theta$ . However, this would mean that $\\Theta$ cannot satisfy the inequalities from these BSQs, so $\\Theta$ cannot be in $I(\\Psi)$ since $I(\\Psi)$ is constructed using the regions of the parameter space that satisfy the necessary BSQs, which is a contradiction. ", "page_idx": 13}, {"type": "text", "text": "Conversely, let us assume that $\\Theta$ satisfies $\\Psi$ . This means one or all the BSQs are satisfied by $\\Theta$ depending on if $\\Psi$ is a conjunction or disjunction. If $\\Theta$ was not in $I(\\Psi)$ , there could not exist a set of BSQs satisfied for $\\Psi$ to be satisfied. ", "page_idx": 13}, {"type": "text", "text": "Thus, for a belief state $b$ , a n-parameter compound BSQ $\\Psi$ has an interval in the parameter space $I(\\Psi)$ s.t. $\\forall\\Theta\\in\\mathbb{R}^{n}$ , $\\Theta\\in{\\cal{I}}(\\Psi)$ iff $\\Theta(b;\\Theta)$ evaluates true. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "As mentioned in Section 3, braids cannot be proper subsets of each other, which we will now prove in Lemma 2. As a high-level intuition, removing a leaf can only occur if a rule along that leaf\u2019s rule-observation trajectory is not satisfied, which would mean another rule must be satisfied since Def. 2 guarantees coverage of the belief state and parameter space. This results in at least one leaf being added to a braid that removes this first leaf, making this new braid not a subset of the other one. ", "page_idx": 13}, {"type": "text", "text": "Lemma 2. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ be the initial belief, and $H$ be the horizon. $\\forall\\overline{{\\vartheta}}_{1},\\overline{{\\vartheta}}_{2}\\in\\mathbb{R}^{n}$ , if b $r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1})\\subseteq b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})$ then brai $d_{\\pi,H}(\\overline{{\\vartheta}}_{1})=b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Assume there exists $\\overline{{\\vartheta}}_{1},\\overline{{\\vartheta}}_{2}\\in\\mathbb{R}^{n}$ s.t. $b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1})\\subset b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})$ implying there exists leaf $\\ell_{2}$ where $\\ell_{2}\\in b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})\\backslash b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1})$ . ", "page_idx": 13}, {"type": "text", "text": "Let $\\ell_{1}\\in b r a i d_{\\pi,H}(\\overline{\\vartheta}_{1})$ be the leaf with the largest rule-observation trajectory $\\tau_{0}$ prefix shared with $\\ell_{2}$ before differing. The trajectory for $\\ell_{1}$ can be expressed as $\\tau_{0}\\tau_{1}$ where $\\tau_{1}$ is the remaining trajectory for reaching $\\ell_{1}$ . Similarly, the trajectory for $\\ell_{2}$ can be expressed as $\\tau_{0}\\tau_{2}$ . Note $\\tau_{0}$ represents the actions executed and observations observed from the initial belief state till right before the diversion resulting in the the belief state $b$ being the same for both leaves up to this point. ", "page_idx": 13}, {"type": "text", "text": "If the first element in $\\tau_{1}$ and $\\tau_{2}$ is a rule, note that $b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})$ must also contain $\\ell_{1}$ . This would imply that $\\pi(b;{\\overline{{\\vartheta}}}_{2})$ is not mutually exclusive since two rules can occur in one element of the strategy tree. This is a contradiction by Def. 4. If the first element in $\\tau_{1}$ and $\\tau_{2}$ is an observation, different observations occurred after executing the last shared action in $\\tau_{0}$ . Due to the observation model and sharing the belief state $b$ at this point, both observations must be possible. This means a leaf in $b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1})$ must have a larger shared trajectory prefix than $\\ell_{1}$ , which is a contradiction. Thus, braids cannot be strict subsets of each other. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Since braids cannot be proper subsets of each other, we can now prove that both braids must contain leaves the other does not have. In turn, this prevents the interval of braids from overlapping. Note that the interval of a braid can be calculated by taking the intersections of the intervals of each leaf contained in that braid (Def. 8): $\\begin{array}{r}{I(b r a i d_{\\pi,H}(\\overline{{\\vartheta}}))=\\bigcap_{\\ell\\in b r a i d_{\\pi,H}(\\overline{{\\vartheta}})}I(\\ell).}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma 3. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be gPOMDP, $b_{0}$ be the initial belief, and $H$ be the horizon. $\\forall\\overline{{\\vartheta}}_{1},\\overline{{\\vartheta}}_{2}\\in\\mathbb{R}^{n}$ , if $b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1})\\cap b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})\\neq\\varnothing$ and brai $d_{\\pi,H}(\\overline{{\\vartheta}}_{1})\\neq b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})$ then $I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1}))\\cap I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2}))=\\mathcal{O}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Let $\\overline{{\\vartheta}}_{1},\\overline{{\\vartheta}}_{2}\\;\\;\\in\\;\\;\\mathbb{R}^{n}$ where $b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})\\,\\cap\\,b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})\\,\\,\\,\\neq\\,\\,\\,\\emptyset$ and $b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1})\\;\\;\\;\\neq$ $b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})$ . Both braids cannot be proper subsets (Lemma 2) meaning both braids must contain leaves that are not in the other braid: $b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})\\backslash b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})\\quad\\neq\\quad\\varnothing$ and $b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})\\backslash b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})\\neq\\varnothing.$ . ", "page_idx": 13}, {"type": "text", "text": "By Definition 8, the interval of a braid is the conjunction of the intervals of each leaf it contains. Using the associative and commutative properties, this can be rewritten as the conjunction of two sets: the interval of leaves shared and the interval of leaves not. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1}))=I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})\\cap b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2}))\\cap I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})\\backslash b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})}}\\\\ {{I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2}))=I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})\\cap b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2}))\\cap I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})\\backslash b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A braid\u2019s interval must exclude these unreachable leaves since a braid is all reachable leaves (Def. 7). As such, $I(b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2}))$ must not overlap with $I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})\\backslash b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})$ and $I(b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1}))$ must not overlap with $I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})\\backslash b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})$ . However, due to the conjunctions of intervals, $I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})\\,\\subseteq\\,I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})\\backslash b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})$ and $I(b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})\\subseteq$ $I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})\\backslash b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})$ . Thus, the intervals of $I(b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1})$ and $I(b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})$ cannot overlap. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "The fact that two braids cannot have overlapping intervals allows us to prove that the sets of parameter values are similar iff they share the same braid interval. ", "page_idx": 14}, {"type": "text", "text": "Lemma 4. $\\forall\\overline{{\\vartheta}}_{1},\\overline{{\\vartheta}}_{2}\\in\\mathbb{R}^{n}$ , $\\bar{\\mathfrak{\\lambda}}_{1}\\equiv_{H}\\overline{{\\vartheta}}_{2}\\,i\\!f\\!f(b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1}))=I(b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})).$ ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $\\overline{{\\vartheta}}_{1}\\equiv_{H}\\overline{{\\vartheta}}_{2}$ , meaning $b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1})=b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2})=L$ where $\\mathrm{L}$ is the set of reachable leaves (Defs. 7 and 9). By Definition 8, the interval of a set of leaves is the intersection of each leaf contained in the set, meaning both braids must have the same interval. ", "page_idx": 14}, {"type": "text", "text": "Let $I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1}))\\ =\\ I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2}))$ . By way of contradiction, assume $b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1})\\;\\;\\neq$ $b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})$ . By Lemma 3, this would mean $I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{1}))\\cap I(b r a i d_{\\pi,H}(\\overline{{{\\vartheta}}}_{2}))=\\mathcal{O}$ , which is a contradiction. Thus, $b r a i d(\\overline{{\\vartheta}}_{1})=b r a i d(\\overline{{\\vartheta}}_{2})$ meaning $\\overline{{\\vartheta}}_{1}\\equiv_{H}\\overline{{\\vartheta}}_{2}$ (Def. 9). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "We can now prove that partitions produced by $\\equiv\\!H$ partitioning the parameter space $\\mathbb{R}^{n}$ each represent a single braid, causing each partition to have a disjoint interval where a constant set of leaves is reachable. ", "page_idx": 14}, {"type": "text", "text": "Theorem 2. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $b_{0}$ be the initial belief state, and $H$ be the horizon. Each partition $\\rho$ created by operator $\\equiv\\!H$ partitioning $\\mathbb{R}^{n}$ is the disjoint intervals, $\\rho\\subseteq\\mathbb{R}^{n}$ where $\\forall\\overline{{\\vartheta}}\\in\\rho,\\,b r a i d_{\\pi,H}(\\overline{{\\vartheta}})=L$ where $L$ is a fixed set of leaves. ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $\\rho$ be a partition produced by $\\equiv_{H}$ partitioning the parameter space $\\mathbb{R}^{n}$ . Note that this means that parameter value sets contained in $\\rho$ must be similar (Def. 9): $\\forall\\overline{{\\vartheta}}_{1},\\overline{{\\vartheta}}_{2}\\in\\rho,\\overline{{\\vartheta}}_{1}\\equiv_{H}\\overline{{\\vartheta}}_{2}$ . As such, all parameter values have the same braid (Def. 7), meaning there exists a set of leaves $L$ that are reachable in $\\rho$ . By Def. 8, this set\u2019s interval must be $\\begin{array}{r}{I(L)=\\bigcap_{\\ell_{H}\\in L}I(\\ell_{H})}\\end{array}$ . By Lemma 3, the interval of other braids cannot overlap with $I(L)$ . Also, there are no  p\u015eroper subsets (Lemma 2), meaning that no other braid can occur in $I(L)$ making it disjoint. ", "page_idx": 14}, {"type": "text", "text": "By Def. 8, $I(L)$ must be contained in $\\rho$ due to all parameter value sets in $I(L)$ having the same braid of $L$ leaves. If $\\rho$ contained parameter value sets not in $I(L)$ , this would imply there exists $\\overline{{\\vartheta}}$ outside of $I(L)$ where just the leaves in $L$ are reachable, which is a contradiction due to $I(L)$ being the only interval space where all the leaves of $L$ are reachable. Meaning the interval of $\\rho$ is actually $I(L)$ . Thus, each partition represents a disjoint interval where only all leaves in $L$ are reachable. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Due to the braid intervals not overlapping, we can prove that parameter value sets contained in that braid\u2019s interval must have a constant expected cost. ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint and $H$ be the horizon. $\\forall\\overline{{\\vartheta}}_{1}\\;\\;\\in\\;\\;\\mathbb{R}^{n},\\;\\;\\forall\\overline{{\\vartheta}}_{2},\\overline{{\\vartheta}}_{3}\\;\\;\\in$   \n$I(b r a i d(\\overline{{\\vartheta}}_{1}))$ , $E_{\\pi}(\\overline{{{\\vartheta}}}_{2};H)=E_{\\pi}(\\overline{{{\\vartheta}}}_{3};H)$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $\\overline{{\\vartheta}}_{2},\\overline{{\\vartheta}}_{3}~\\in~I(b r a i d_{\\pi,H}(\\overline{{\\vartheta_{1}}})$ where $\\overline{{\\vartheta_{1}}}\\ \\in\\ \\mathbb{R}^{n}$ is a tuple of $n$ parameters. Note that $b r a i d(\\overline{{{\\vartheta}}}_{1})\\;=\\;b r a i d(\\overline{{{\\vartheta}}}_{2})\\;=\\;b r a i d(\\overline{{{\\vartheta}}}_{3})$ due there being no strict subsets (Lemma 2) and leaves in $\\overline{{\\vartheta}}_{2}$ and $\\overline{{\\vartheta}}_{3}$ would have to be reachable in $\\overline{{\\vartheta}}_{1}$ . Each braid represents a policy tree (Def. 5), and the expected cost is based on the probability distribution of leaves in the braid. Since both $\\overline{{\\vartheta}}_{2}$ and $\\overline{{\\vartheta}}_{3}$ represent the same policy tree, they must have identical expected cost values. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "It is now trivial to show that each partition represents a disjoint interval of the parameter space where the expected cost is constant. ", "page_idx": 14}, {"type": "text", "text": "Theorem 3. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ be the initial belief state, and $H$ be the horizon. Each partition created by operator $\\equiv_{H}$ partitioning $\\mathbb{R}^{n}$ has a constant expected cost. ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $\\rho$ be a partition created by partitioning $\\mathbb{R}^{n}$ with $\\equiv\\!H$ . By Theorem 2, all parameter value sets in the disjoint interval of $\\rho$ must have the same braid. As such, by Lemma 5, the expected cost is constant for all the parameter sets. Thus, the disjoint interval of each partition must have a constant expected cost. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "C Proofs For Partition Refinement Search [Section 5] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide the formal proof for Theorem 4 proving that the Partition Refinement Search algorithm introduced in Section 5 is probabilistically complete. We define and prove Lemmas 6, 7, and 8 for building this proof. ", "page_idx": 15}, {"type": "text", "text": "When PRS refines a partition $\\rho$ using a leaf $\\ell$ , it can produce up to two possible partitions: a partition for $\\rho$ where $\\ell$ is reachable and a partition for $\\rho$ where $\\ell$ is not (if it exists). We now show that this process prevents empty partitions. ", "page_idx": 15}, {"type": "text", "text": "Lemma 6. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ be the initial belief state, and $H$ be the horizon. For each partition $\\rho$ constructed by Partition Refinement Search, $\\rho\\neq\\mathcal{D}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\rho\\subseteq\\mathbb{R}^{n}$ be a partition constructed by PRS. Since PRS creates partitions based on whether sampled leaves are included or excluded, let $L_{i}$ be the leaves PRS included in partition $\\rho$ and $L_{e}$ be the leaves excluded. Therefore, $\\rho=I(L_{i})\\backslash I(L_{e})$ . ", "page_idx": 15}, {"type": "text", "text": "By way of contradiction, let $\\rho=\\mathcal{Q}$ . There are two cases where this could occur: (1) excluding leaf $\\ell$ caused $\\rho=\\mathcal{Q}$ or (2) including $\\ell$ caused $\\rho=\\mathcal{Q}$ . For case (1), we explicitly do not add partitions if excluding the leaf results in an empty interval, meaning this cannot happen. For case (2), this implies that there exists a previous partition $\\rho_{0}$ where sampling leaf $\\ell_{0}$ resulted in the partition constructed from $\\rho_{0}$ including $\\ell_{0}$ creating $\\rho$ where $\\rho=\\mathcal{Q}$ . Due to $\\ell_{0}$ being uniformly sampled from $\\rho_{0},\\ell_{0}$ must be reachable in $\\rho_{0}$ meaning $\\rho_{0}\\cap I(\\ell_{0})\\neq\\emptyset$ . However, line 10 of Algo. 1 calculates the interval of $\\rho$ as $\\rho_{0}\\cap I(\\ell_{0})$ meaning $\\rho\\neq\\mathcal{Q}$ , which is a contradiction. Thus, all partitions must be not empty. ", "page_idx": 15}, {"type": "text", "text": "A critical property of PRS is that each partition constructed converges to represent a single braid. ", "page_idx": 15}, {"type": "text", "text": "Lemma 7. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ be the initial belief state, and $H$ be the horizon. Let $\\rho$ be a partition constructed by Partition Refinement Search. If all leaves reachable in $\\rho\\subseteq\\mathbb{R}^{n}$ have been sampled, $\\forall\\overline{{\\vartheta}}\\in\\rho,I(b r a i d_{\\pi,H}(\\overline{{\\vartheta}}))=\\rho.$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\rho\\subseteq\\mathbb{R}^{n}$ be a partition constructed by PRS. Let $L_{\\rho}=\\{\\ell_{1},...,\\ell_{n}\\}$ be the n-sampled unique leaves for $\\rho$ . Let all leaves reachable from $\\rho$ be sampled, $\\forall\\ell,\\ell\\in L_{\\rho}\\leftrightarrow[\\exists\\overline{{\\vartheta}}\\in\\rho,\\ell\\in b r a i d_{\\pi,H}(\\overline{{\\vartheta}})]$ . ", "page_idx": 15}, {"type": "text", "text": "Due to $\\rho\\neq\\mathcal{Q}$ (Lemma 6) and BSQ constraints covering $\\mathbb{R}^{n}$ (Def. 4), there must exist a non-empty set of leaves $L$ reachable within $\\rho$ . Since all leaves are sampled, we know that $L\\subseteq L_{\\rho}$ . However, there cannot be proper subsets (Lemma 2) meaning $L=L_{\\rho}$ . This means that the interval of $\\rho$ must also equal the interval of leaves $I(L)$ . Thus, $\\rho$ must represent a braid. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Since partitions are constructed by including/excluding sampled leaves hierarchically, we can prove that this makes each partition represent a unique braid. ", "page_idx": 15}, {"type": "text", "text": "Lemma 8. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ be the initial belief state, and $H$ be the horizon. Let $\\rho_{1},\\rho_{2}\\subseteq\\mathbb{R}^{n}$ be partitions constructed by Partition Refinement Search. If all leaves reachable in $\\rho_{1}$ and $\\rho_{2}$ have been sampled, $\\forall\\overline{{\\vartheta}}_{1}\\in\\rho_{1},\\forall\\overline{{\\vartheta}}_{2}\\in\\rho_{1},b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{1})\\neq b r a i d_{\\pi,H}(\\overline{{\\vartheta}}_{2})$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\rho_{1},\\rho_{2}\\subseteq\\mathbb{R}^{n}$ be two different partitions constructed by PRS. Note that the PRS partitions $\\mathbb{R}^{n}$ by refining one partition using leaf $\\ell$ into two by explicitly including $I(\\ell)$ in one partition and excluding $I(\\ell)$ in the other (Algorithm 1). Meaning $\\rho_{1}$ and $\\rho_{2}$ cannot overlap. ", "page_idx": 15}, {"type": "text", "text": "Since both partitions represent a possible non-empty braid (Lemma 7), there exists a set of leaves reachable in both partitions. However, by the partition construction process, there must exist at least one leaf included in one but excluded in the other. Due to there being no interval overlap between braids, two different braids must be reachable in each partition (Lemma 2). Thus, all partitions must represent a unique braid. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Using the property that each partition in PRS represents a unique braid, we can now prove that PRS is probabilistically complete. ", "page_idx": 16}, {"type": "text", "text": "Theorem 4. Let $\\pi(b,{\\overline{{\\Theta}}})$ be a BSQ constraint, $\\mathcal{P}$ be a gPOMDP, $b_{0}$ the initial belief state, and $H$ be the horizon. The likelihood of the Partition Refinement Search algorithm returning the optimal parameter interval converges to one in the limit of infinite samples. ", "page_idx": 16}, {"type": "text", "text": "Proof. Note that gPOMDPs have a finite set of observations and finite horizon (Def. 1), and BSQ constraints have a finite number of rules (Def. 4). As such, there exists a finite number of unique rule-observation trajectories in the strategy tree (Def. 6). Therefore, there exists a finite number of leaves due to each leaf having a unique rule-observation trajectory. This results in there only being a finite set of braids being all possible combinations of reachable leaves (Def. 7). Since each partition represents a unique braid (Lemma 8), the number of partitions must be finite. ", "page_idx": 16}, {"type": "text", "text": "Let $\\rho\\subseteq\\mathbb{R}^{n}$ be a partition constructed by PRS that is not equivalent to a braid. By Lemma 7, this means there exists a leaf $\\ell$ reachable in $\\rho$ that has not been sampled yet. This also means there must exist a non-empty interval $\\rho\\cap I(\\ell)$ where sampling from $\\rho$ can reach $\\ell$ . Due to uniform sampling selecting parameter values when sampling a leaf for refining the partition (line 7 of Algorithm 1), the probability of selecting a parameter value that could sample $\\ell$ can be calculated as $\\begin{array}{r}{\\frac{\\rho\\cap I(\\ell)}{\\rho}=P r(I(\\ell)|\\rho)}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Note that $\\ell$ represents a unique rule-observation trajectory $\\left\\{{{r}_{1}},{{o}_{1}},...,{{r}_{H}},{{o}_{H}}\\right\\}$ . Note the probability of an observation $o$ being observed in belief state $b$ after action $a$ is executed is $\\bar{P r}(o|b,a)\\stackrel{\\cdot}{=}$ $\\begin{array}{r}{\\sum_{s^{\\prime}}[\\Omega(s^{\\prime},a,o)\\sum_{s}\\mathcal{T}(s,a,\\bar{s^{\\prime}})b(s)]}\\end{array}$ . Meaning that the probability of reaching $\\ell$ during rollout is $\\begin{array}{r}{P r(\\ell)\\,=\\,\\prod_{i}P r(o_{i}|b_{i})}\\end{array}$ where $\\boldsymbol{b_{i}}^{\\intercal}\\!=\\,b p^{*}(b_{0},r_{1},o_{1},...,r_{i},o_{i})$ . Since we know that $\\ell$ is reachable, $P r(\\ell)>0$ .\u015b ", "page_idx": 16}, {"type": "text", "text": "We assume that partition selection approaches discussed in Section 6 have a non-zero probability of refining any partition. Let $P r(\\rho)$ be the probability of $\\rho$ being selected. This means that in any refinement step, the probability of sampling leaf $\\ell$ is $\\dot{P r(\\ell)}\\dot{P r}(I(\\ell)/\\rho)P r(\\rho)$ . Due to each probability being greater than zero, the probability of any non-sampled leaf being sampled must be greater than zero. Therefore, with enough refinement steps, all the leaves will be sampled since there is only a finite number of leaves. Thus, the set of partitions will be refined to the set of braids as the number of samples increases to infinite. ", "page_idx": 16}, {"type": "text", "text": "Note that each partition represents a unique braid (Lemma 8) with a set probability distribution of outcomes based on the reachable leaves. Due to a non-zero probability of refining a partition $P r(\\rho)$ , the sampled expected cost of a partition will converge to the actual expected cost due to the law of large numbers. ", "page_idx": 16}, {"type": "text", "text": "Therefore, within a finite number of samples, the partitions constructed by PRS will accurately represent the set of braids with an accurate representation of their expected costs. Thus, PRS will find the minimal expected cost partition as the number of samples increases to infinite. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "D Evaluation Problem\u2019s Belief-State Query Preferences ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide the BSQ constraints for the Lane Merger, Graph Rock Sample, and Store Visit problems discussed in Section 7. To do this, we first describe the functions that compose each problem\u2019s states and actions. We use loops and quantifiers in the BSQ constraints for clarity that can be unrolled on a problem-by-problem basis. ", "page_idx": 16}, {"type": "text", "text": "D.1 Lane Merger ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The Lane Merger problem is that there are two lanes, and the agent must merge into the other lane within a certain distance. In this other lane, there is another car whose exact location and speed are unknown. Therefore, there exist two objects in the environment: the agent (agent) and the other car (other). For either object $o$ , the location and speed are tracked using the unary integer functions $l o c(o)$ and $s p e e d(o)$ . For actions, the agent can increase their speed $(s p e e d\\_u p())$ , decrease their speed $(s l o w\\_d o w n())$ , remain in their current lane at their current speed $(k e e p\\_s p e e d())$ , or attempt to merge lanes $(m e r g e())$ . Using these functions, the BSQ constraint $\\pi_{l m}(b;\\Theta_{1},\\Theta_{2})$ is formally defined as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{l m}(b;\\Theta_{1},\\Theta_{2}):}\\\\ &{\\quad\\mathrm{If}\\;P r\\big[l o c(a g e n t)>l o c(o t h e r)+s p e e d(o t h e r)+2\\vee}\\\\ &{\\quad\\quad l o c(a g e n t)+s p e e d(a g e n t)+2<l o c(o t h e r)\\big]_{b}>\\Theta_{1}\\rightarrow m e r g e()}\\\\ &{\\mathrm{Else~if}\\;P r\\big[|l o c(a g e n t)-l o c(o t h e r)|\\leqslant1\\big]_{b}>\\Theta_{2}\\wedge}\\\\ &{\\quad\\quad P r\\big[s p e e d(a g e n t)>0\\big]_{b}==1\\rightarrow s l o w_{-}d o w n()}\\\\ &{\\mathrm{Else~}k e e p_{-}s p e e d()}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D.2 Graph Rock Sample ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The Graph Rock Sample problem is that there is a rover with pre-programmed waypoints, where some waypoints contain rocks. These rocks have been categorized into types, and whether it is safe for the rover to sample them is unknown. The objective of the rover is to sample each type with a safe rock before traversing to a dropoff location. The objects are the waypoints, including the rocks $\\{r_{1},...,r_{n}\\}$ and the dropoff location (dropoff). The rover knows if or if it is not located at waypoint $w$ using the unary Boolean function $l o c(w)$ . The rover also knows whether it needs to sample rocks of type $t$ using the unary Boolean function $n e e d e d(t)$ . For any rock $r$ , the distance from the rover, whether the rock is type $t$ , and if the rock is safe to sample are tracked using the unary double function $d i s t a n c e(r)$ and the Boolean functions $t y p e(r,t)$ , and $s a f e(r)$ , respectively. The rover can move to neighboring waypoint $w$ $(m o v e(w))$ , sample rock $r$ at its current waypoint $(s a m p l e(r))$ , and scan any rock $r$ $(s c a n(r))$ . For clarity, we use the function $g o t o(w)$ to specify taking the edge that moves the rover closer to waypoint $w$ . Using these functions, the BSQ constraint $\\bar{\\pi_{g r s}}(b;\\bar{\\Theta_{1}},\\Theta_{2},\\Theta_{3})$ is formally defined as follows. ", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r l}&{\\pi_{g r s}(b;\\Theta_{1},\\Theta_{2},\\Theta_{3}):}\\\\ &{\\quad\\mathrm{For}\\;r_{c}\\in\\{r_{1},...,r_{n}\\}:}\\end{array}$ If Pr $\\begin{array}{r l}&{\\iint_{\\partial t}\\rvert\\lvert t y p e(\\boldsymbol{r_{c}},t)\\wedge n e e d e d(t)\\wedge l o c(\\boldsymbol{r_{c}})\\wedge s a f e(\\boldsymbol{r_{c}})\\rVert_{b}\\geqslant\\Theta_{1}\\rightarrow s a m p l e(\\boldsymbol{r_{c}})}\\\\ &{\\textnormal{f}P r\\big[\\exists t\\rvert t y p e(\\boldsymbol{r_{c}},t)\\wedge n e e d e d(t)\\wedge-l o c(\\boldsymbol{r_{c}})\\wedge s a f e(\\boldsymbol{r_{c}})\\big]_{b}\\geqslant\\Theta_{1}\\rightarrow g o t o(\\boldsymbol{r_{c}})}\\\\ &{\\textnormal{f}P r\\big[\\exists t\\rvert t y p e(\\boldsymbol{r_{c}},t)\\wedge n e e d e d(t)\\wedge s a f e(\\boldsymbol{r_{c}})\\big]_{b}\\geqslant\\Theta_{2}\\wedge}\\\\ &{\\cdot\\big[d i s t a n c e(\\boldsymbol{r_{c}})\\leqslant\\Theta_{3}\\big]_{b}==1\\rightarrow s c a n(\\boldsymbol{r_{c}})}\\\\ &{\\textnormal{f}P r\\big[\\exists t\\rvert t y p e(\\boldsymbol{r_{c}},t)\\wedge n e e d e d(t)\\wedge s a f e(\\boldsymbol{r_{c}})\\big]_{b}\\geqslant\\Theta_{2}\\wedge}\\\\ &{\\cdot\\big[d i s t a n c e(\\boldsymbol{r_{c}})>\\Theta_{3}\\big]_{b}==1\\rightarrow g o t o(\\boldsymbol{r_{c}})}\\end{array}$ Else Else Else   \nElse gotopdropoffq ", "page_idx": 17}, {"type": "text", "text": "D.3 Store Visit ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The Store Visit problem involves an agent in a city with a grid-based layout. Some locations are unsafe, while others contain a bank or a store. The objective is for the agent to visit a bank safely and then a store. The objects are the agent, the set of stores $\\{s_{1},...,s_{n}\\}$ , and the set of banks $\\left\\{b_{1},...,b_{m}\\right\\}$ . Labeling functions $b a n k(o)$ and $s t o r e(o)$ check whether object $o$ is a bank or store, respectively. The ternary Boolean function keeps track of the current $(x,y)$ location of the object $o$ , $\\bar{l o c}(o,x,y)$ . Similarly, whether location $(x,y)$ is safe is tracked by the binary Boolean function $i s\\_s a f e(x,y)$ . Lastly, the state keeps track of whether the agent has visited a bank using the nullary Boolean function $v b a n k()$ . The agent can move left $(l e f t())$ , right $(r i g h t())$ , up $(u p)$ , and down $(d o w n())$ in the grid. The agent can also visit a building in its current location $(v i s i t())$ or scan its surroundings to figure out its location $(s c a n())$ . Using these functions, the BSQ constraint $\\pi_{s v}(b;\\Theta_{1},\\Theta_{2},\\Theta_{3})$ is formally defined as follows. ", "page_idx": 17}, {"type": "text", "text": "$\\pi_{s v}(b;\\Theta_{1},\\Theta_{2},\\Theta_{3}):$ : If @x $\\begin{array}{r l}&{,y|P r[[o c(a g e n t,x,y)]|_{b}<\\Theta_{3}\\to s c a n()}\\\\ &{\\mathrm{if~}P r[\\exists s,x,y|v b a n k()\\land s t o r e(s)\\land l o c(s,x,y)\\land l o c(a g e n t,x,y)]|_{b}\\geqslant\\Theta_{1}\\to v i s i t()}\\end{array}$ Else For $s_{c}\\in\\{s_{1},...,s_{n}\\}$ : Els $\\begin{array}{r l}&{\\mathbf{if}\\:\\operatorname{Pr}\\left\\{\\|\\mathbf{Z}_{T}\\mathbf{\\Phi}_{[1,T]},\\mathbf{g}_{T,\\eta},\\mathbf{g}_{T,\\eta}\\|\\in\\mathrm{band}\\;\\middle\\backslash\\;\\middle\\middle\\alpha~\\middle~\\middle\\middle\\middle~\\middle\\middle\\middle~\\middle\\middle~\\middle\\middle~\\middle\\middle~\\middle\\middle~\\middle\\middle~\\middle~\\middle\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle\\right.~\\middle\\right.}\\times\\middle\\ \u1e0a \\mathbf~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle~\\middle\\ \u1e0a \\mathbf~\\middle~\\middle~\\middle~\\middle\\ \u1e0a \\phi~\\middle~\\middle~\\middle\\middle~\\middle~\\middle\\ \u1e0a \\middle~\\middle\\middle~\\middle~\\middle\\ \u1e0a \\middle~\\middle\\middle~\\middle\\ \u1e0a \\middle~\\middle\\middle~\\middle\\ \u1e0a \\middle~\\middle\\ \u1e0a \\phi~\\middle\\ \u1e0a ~\\middle\\middle~\\middle\\ \u1e0a ~\\middle\\ \u1e0a ~\\middle\\ \u1e0a ~\\middle\\ \u1e0a ~\\ \u1e0a \\phi~ \u1e0c ~ $ Els Els y Els Else i For Else scanpq ", "page_idx": 18}, {"type": "text", "text": "E Hyperparameter Optimization Algorithms Implementation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As a baseline comparison, we implemented Nelder-Mead and Particle Swarm as hyperparameter optimization algorithms to compare solving for the optimal parameter values for a BSQ constraint to minimize the expected cost of the resulting BSQ policy. Both algorithms evaluate points in the parameter space to decide which areas to explore next. For both, we evaluate a parameter point by taking a thousand parallel runs of the BSQ policy with those values to approximate the expected cost. ", "page_idx": 18}, {"type": "text", "text": "Nelder-Mead We used a simplex that has edges numbering one more than the number of parameters in the BSQ constraint being optimized. To start with a better initial simplex, we randomly sampled a hundred points and tracked the points that had lower expected costs and were 0.4 distance away from each of the better-performing points. The closer points were saved but were given a lower priority. Each iteration followed the standard Nelder-Mead steps with the sum quality of all the edges in the simplex calculated. If five iterations pass without an increase in quality, the run is deemed to have converged, and the best quality point of the simplex is returned as the solution. ", "page_idx": 18}, {"type": "text", "text": "Particle Swarm Particle swarm used 10 particles randomly selected from within the parameter space with a random velocity. Let $t$ be the number of iteration steps since the last improvement in the best quality point found. For each iteration, the cognitive coefficient is $1.0-0.1t$ , and the social coefficient is $0.1+0.1t$ , which causes the particles to become more greedy as time since the last improvement increases. The momentum is statically set to 0.6 with the velocity clipped between $\\pm0.5$ . The location of points is also clipped to the parameter search space. If 10 iteration steps pass without seeing an improvement, the run is deemed to have converged, and the best quality point of the swarm is returned as the solution. ", "page_idx": 18}, {"type": "image", "img_path": "i2oacRDF5L/tmp/530dc3de2acb2484a15cbe28216267eaed35937b37cb0ee35c5407057e03f06d.jpg", "img_caption": ["Figure 5: Performance of the hypothesized optimal partition while solving for the Lane Merger, Spaceship Repair, and Store Visit problems. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Additional Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide additional results from the experiments performed. This includes introducing two additional partition selection approaches we evaluated: Global Thompson Sampling and Maximum Confidence. We also provide graphs of the performance of the hypothesized optimal partition for PRS-Epsilon, PRS-Bolt, and PRS-Local. Finally, we provide a results table for all five partition selection approaches and the baseline RCompliant. ", "page_idx": 19}, {"type": "text", "text": "As a reminder, PRS is implemented for multiprocessing by having each process manage a subset of the partitions $X^{\\prime}\\subseteq X$ but share a global hypothesis of the optimal partition. Also, a dynamic exploration rate $e_{r}$ is used that diminishes over the solving time. Using this framework, two additional partition selection approaches were explored. ", "page_idx": 19}, {"type": "text", "text": "Maximum Confidence (PRS-Max) We explore $e_{r}$ percent of the time by uniformly sampling $s\\sim U_{0}^{1}$ and checking if $s\\leqslant e_{r}$ . If exploring, we uniformly at random select a partition from $X^{\\prime}$ . Otherwise, the partition with maximum standard deviation, $\\textstyle\\operatorname{arg\\,min}_{\\langle\\rho,{\\hat{E}}[\\rho]\\rangle\\in X^{\\prime}}\\sigma({\\hat{E}})[\\rho]$ , is selected. ", "page_idx": 19}, {"type": "text", "text": "Global Thompson Sampling (PRS-Global) Unlike the other partition selection approaches, each processor iterates over all partitions it manages before selecting multiple partitions to refine. Partitions are chosen for two reasons: (1) they are below the minimum number of samples, or (2) the partition has the potential of being better than the current global hypothesized optimal partition. This is simulated for each partition using $\\mathcal{N}(\\mu_{c},\\sigma_{c}\\,\\times\\,e_{r})$ with $\\mu_{c}$ and $\\sigma_{c}$ being the mean and standard deviation of that partition, respectively. If the sample taken from this normal distribution has a lower expected cost than the hypothesized optimal partition, this partition is selected for refinement. ", "page_idx": 19}, {"type": "text", "text": "Performance of the hypothesized optimal In Figure 5, we show the results of the two partition selection approaches introduced in the main paper (Section 6) that were not shown in the main paper compared against PRS-Epsilon and RCompliant. The performance supports the idea that PRS converges regardless of the partition refinement approach and that the performance consistently outperforms the baseline RComplient policy. Additionally, the high standard deviation slowed down the convergence of approaches that rely on the standard deviation, like Local Thompson Sampling. Finally, note that Boltzmann Exploration converged prematurely during the Store Visit problem due to the fast cooling temperature. ", "page_idx": 19}, {"type": "text", "text": "Tabulated performance In Table 1, the expected cost and the goal achievement rate have been tabulated, showing the near identical performance of four of the partition refinement approaches. The only exception was Maximum Confidence since selecting the highest standard deviation partition is a form of exploring rather than exploiting. Additionally, as we have mentioned, the solutions for this set of problems have a lower standard deviation, resulting in this partition selection approach exploring the wrong partitions. However, despite this poor partition selection approach, the Partition Refinement ", "page_idx": 19}, {"type": "text", "text": "Search algorithm is robust enough to outperform the baseline RCompliant policy significantly while also outperforming both Nelder-Mead and Particle Swarm. ", "page_idx": 20}, {"type": "text", "text": "G Experimental Setup And Computational Cost ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we go through the empirical setup of the experiments performed in Section 7 and include an estimate of the computation cost for running the experiments for this paper. ", "page_idx": 20}, {"type": "text", "text": "All experiments were performed on an Intel(R) Xeon(R) W-2102 CPU $\\textcircled{a}\\ 2.90\\mathrm{GHz}$ without using a GPU. The Partition Refinement Search algorithm was implemented using a manager-worker design pattern where 8 workers were initialized when solving. The manager maintained the hypothesized optimal partition and current exploration rate. Every 12.5 seconds, the current hypothesized optimal partition was recorded. At exactly 25 minutes, the hypothesized optimal partition was recorded as the solution before closing the workers. ", "page_idx": 20}, {"type": "text", "text": "Both solutions and recorded hypothesized optimal partitions were evaluated using the same random seed to ensure that the same initial states were assessed. This evaluation process was carried out in parallel using a manager-worker design pattern with 16 workers. 25,000 independent runs were conducted for each solution to determine the expected cost and goal achievement rate. Additionally, for each recorded hypothesized optimal partition, 10,000 runs were performed. The average performance and standard deviation error were calculated by averaging the results of ten runs for each combination of problem and solver. A similar approach was used to evaluate the random-parameter user-compliant policy RCompliant. Instead of using solved policies, ten parameter value sets were uniformly selected randomly from the parameter space, and each set was evaluated for 25,000 runs. These results are presented in Figure 3. ", "page_idx": 20}, {"type": "text", "text": "For constructing the Spaceship Repair heatmap (Figure 1), all combinations of parameters $\\Theta_{1}$ and $\\Theta_{2}$ were evaluated with parameter values sampled from 0 to 1 with increments of 0.002. This produced 251,001 equally-spaced parameter values. Parameter values were evaluated on 300 runs with a horizon of 12 to calculate the expected cost. ", "page_idx": 20}, {"type": "text", "text": "Computational cost Running the Partition Refinement Search algorithm for the empirical evaluation section (Section 7) involved nine processes running simultaneously for 25 minutes across ten trials for each of the five partition-selection approaches. This resulted in 21 hours of CPU usage when run in parallel, equivalent to 187.5 hours if executed sequentially. Evaluation complexities were significant, such as the variance in time per run and problem type. For instance, evaluating the solutions and hypothesized optimal partitions for the Lane Merger problem using 17 processes took approximately 48 hours in parallel. The overall CPU usage for the main experiment approximates to 360 hours (15 days) in parallel, translating to about 6,288 hours (262 days) if run sequentially. Additionally, constructing the Spaceship Repair heatmap (Figure,1) required approximately 24 hours of CPU time using 11 processes. These experiments were conducted thrice, culminating in an estimated total computational cost of 2,160 hours (90 days) using an Intel(R) Xeon(R) W-2102 CPU $\\textcircled{a}\\ 2.90\\mathrm{GHz}$ , or 19,656 hours (819 days) if operations were performed sequentially. ", "page_idx": 20}, {"type": "text", "text": "H Spaceship Repair Partitions Closed Form ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we calculate the braids that partition the parameter space for the Spaceship Repair problem with the BSQ constraint from Fig. 1. ", "page_idx": 20}, {"type": "text", "text": "First, we give the exact observation model used. From Section 3.1, the Spaceship Repair state is composed of two functions: broken $(o)$ and $r l o c a t i o n()$ . This means each state is expressed as $\\{b r o k e n(r o b o t),b r o k e n(s h i p),r l o c a$ tionpqu. Additionally, the set of observations can be expressed as $\\{o b s\\_e r r(r o b o t),o b s\\_e r r(s h i p)\\}$ . Let $p_{r}$ and $p_{s}$ be the probability of the observation reflecting the actual state of the robot and spaceship, respectively. The probability of observation $o$ in state $s$ after action $a$ is executed is calculated as follows. ", "page_idx": 20}, {"type": "table", "img_path": "i2oacRDF5L/tmp/a76c2411e453bb6c7a72372a4c4b47018022e0d49ebcf822b2c2730e0f135cf3.jpg", "table_caption": [], "table_footnote": [""], "page_idx": 21}, {"type": "text", "text": "Note observations are independent of the robot\u2019s location and actions. For clarity, we express the states as whether or not the robot and spaceship are broken, $\\{b r o k e n(r o b o t),b r o k e n(s h i\\bar{p})\\}$ . This means there are four possible states depending on whether the robot and ship are broken. For ease of notation, we represent these states as $\\bar{S}=\\left\\{\\bar{s}_{T T},s_{T F},s_{F T},s_{F F}\\right\\}$ , where $s_{T F}$ represents that state where the robot is broken and the spaceship is not. Similar, let the four possible observations be represented as $O=\\left\\{o_{T T},o_{T F},o_{F T},o_{F F}\\right\\}$ . ", "page_idx": 22}, {"type": "text", "text": "The precondition of the first rule of the Spaceship Repair problem BSQ constraint is $[b r o k e n(r o b o t)]_{b}\\leqslant\\Theta_{1}$ (Figure 1). For any belief state $b$ , the probability of the robot being broken i s the probability  of the states where that is true: $\\mathbb{[}b r o k e n(r o b\\bar{o}t)\\mathbb{]}_{b}=b\\bar{(}s_{T T})+b(s_{T F})$ . ", "page_idx": 22}, {"type": "text", "text": "Let $\\left\\{{{a}_{1}},{{o}_{1}},...,{{a}_{t}},{{o}_{t}}\\right\\}$ be an action-observation trajectory for $t$ timesteps where at each timestep an action is executed followed by an observation being observed. We can calculate the probability of the state where the robot and spaceship are broken, $s_{T T}$ , as follows. ", "page_idx": 22}, {"type": "equation", "text": "$$\nb_{t}(s_{T T})=\\alpha P r(o_{t}|s_{T T},a_{t})\\sum_{s}T(s,a_{t},s_{T T})b_{t-1}(s)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that, due to the observations being independent of the robot\u2019s location, the observation and transition functions are independent of the action. For example, there is no action the robot can perform to change whether the robot or spaceship is broken due to the problem being to reach a broken component rather than fixing it. We can simplify Equation 2 significantly as follows. ", "page_idx": 22}, {"type": "equation", "text": "$$\nb_{t}(s_{T T})=\\alpha P r(o_{t}|s_{T T})b_{t-1}(s_{T T})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can now rewrite Equation 3 by unrolling the recursion. Note that $\\alpha$ is the normalization factor meaning we don\u2019t need to calculate $\\alpha$ each timestep because the final normalization will factor in all these changes. Additionally, due to the probability of each initial state being uniform, we don\u2019t need to keep track of the initial belief. Also, note there exist four possible observations. Due to the commutativity of multiplication, we can rearrange to get the following. Let $c_{T T},c_{T F},c_{F T}$ , and $c_{F F}$ be the counts of the number of each observation where $c_{T T}+c_{T F}+c_{F T}+c_{F F}=t$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\nb_{t}(s_{T T})=\\alpha P r(o_{T T}|s_{T T})^{c_{T T}}P r(o_{T F}|s_{T T})^{c_{T F}}P r(o_{F T}|s_{T T})^{c_{F T}}P r(o_{F F}|s_{T T})^{c_{F F}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using Equation 1, the probability of this state can be written in terms of $p_{r}$ and $p_{s}$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\nb_{t}(s_{T T})=\\alpha(p_{r}p_{s})^{c_{T T}}(p_{r}(1-p_{s}))^{c_{T F}}((1-p_{r})p_{s})^{c_{F T}}((1-p_{r})(1-p_{s}))^{c_{F F}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\nb_{t}(s_{T T})=\\alpha p_{r}^{c_{T T}+c_{T F}}p_{s}^{c_{T T}+c_{F T}}(1-p_{r})^{c_{F T}+c_{F F}}(1-p_{s})^{c_{T F}+c_{F F}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This same process can be applied to the other three states to get the equation of their likelihoods. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{t}(s_{T F})=\\alpha p_{r}^{c_{T T}+c_{T F}}p_{s}^{c_{T F}+c_{F F}}(1-p_{r})^{c_{F T}+c_{F F}}(1-p_{s})^{c_{T T}+c_{F T}}}\\\\ &{b_{t}(s_{F T})=\\alpha p_{r}^{c_{F T}+c_{F F}}p_{s}^{c_{T T}+c_{F T}}(1-p_{r})^{c_{T T}+c_{T F}}(1-p_{s})^{c_{T F}+c_{F F}}}\\\\ &{b_{t}(s_{F F})=\\alpha p_{r}^{c_{F T}+c_{F F}}p_{s}^{c_{T F}+c_{F F}}(1-p_{r})^{c_{T T}+c_{T F}}(1-p_{s})^{c_{T T}+c_{F F}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can group the states into two groups depending on whether or not the robot is broken. By factoring we can get the following. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{t}\\big(s_{T T}\\big)+b_{t}\\big(s_{T F}\\big)=}\\\\ &{\\begin{array}{l}{\\alpha p_{r}^{c_{T T}+c_{T F}}\\big(1-p_{r}\\big)^{c_{F T}+c_{F F}}\\big[p_{s}^{c_{T T}+c_{F T}}\\big(1-p_{s}\\big)^{c_{T F}+c_{F F}}+p_{s}^{c_{T F}+c_{F F}}\\big(1-p_{s}\\big)^{c_{T T}+c_{F T}}\\big]}\\\\ {b_{t}\\big(s_{F T}\\big)+b_{t}\\big(s_{F F}\\big)=}\\\\ &{\\alpha p_{r}^{c_{F T}+c_{F F}}\\big(1-p_{r}\\big)^{c_{T T}+c_{T F}}\\big[p_{s}^{c_{T T}+c_{F T}}\\big(1-p_{s}\\big)^{c_{T F}+c_{F F}}+p_{s}^{c_{T F}+c_{F F}}\\big(1-p_{s}\\big)^{c_{T T}+c_{F T}}\\big]}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that in Equations 10 and 11 everything in the brackets is shared, which is due to the individual observations of the spaceship and robot being independent of each other. Also, for normalization, we just divide the sum of Equations 10 and 11, which is equivalent to the sum probability of all states. Additionally, note that Equation 10 is equivalent to the BSQ precondition $\\mathbb{[}b r o k e n(r o b o t)\\mathbb{]}_{b_{t}}$ . Substituting into this BSQ and simplifying we get the following. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|b r o k e n(r o b o t)\\|_{b_{t}}=\\frac{p_{r}^{c_{T T}+c_{T F}}(1-p_{r})^{c_{F T}+c_{F F}}}{p_{r}^{c_{T T}+c_{T F}}(1-p_{r})^{c_{F T}+c_{F F}}+p_{r}^{c_{F T}+c_{F F}}(1-p_{r})^{c_{T T}+c_{T F}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that there are two exponent values: the number of times the robot is observed to be broken and the number it is not. Let $d_{r}=c_{T T}+c_{T F}-c_{F T}-c_{F F}$ be the difference in the number of times that the robot is observed to be broken to not. If $d_{r}>0$ , then the robot has been observed to be broken more often than not. By substituting $d_{r}+c_{F T}c_{F F}\\,=\\,c_{T T}+c_{T F}$ into Equation 12 the equation simplifies down. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{[}b r o k e n(r o b o t)\\mathbb{]}_{b_{t}}=\\frac{p_{r}^{d_{r}}}{p_{r}^{d_{r}}+(1-p_{r})^{d_{r}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Following a similar process, the BSQ from the second rule in the BSQ constraint from Figure 1 can be written similarly. Let $d_{s}$ be the difference in the number of times the spaceship is observed to be or not. If $d_{s}>0$ , then the spaceship has been observed to be broken more often than not. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{[}b r o k e n(s h i p)\\mathbb{]}_{b_{t}}=\\frac{p_{s}^{d_{s}}}{p_{s}^{d_{s}}+(1-p_{s})^{d_{s}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that the observation model used $p_{r}\\,=\\,0.6$ and $p_{s}\\,=\\,0.75$ for the heatmap in Figure 1. The horizontal thresholds can be calculated using Equation 14 and the vertical with Equation 13. ", "page_idx": 23}, {"type": "text", "text": "Therefore, a partition is specific value of $d_{r}\\in\\mathbb{Z}$ and $d_{s}\\in\\mathbb{Z}$ that is equivalent to saying: The objective is to fix the communication channel. If the difference in the number of times the robot has been observed being broken than not is greater than $d_{r}$ , it should try to repair itself; otherwise, if the difference in the number of times the spaceship has been observed being broken than not is greater than $d_{s}$ , it should try to repair that. Formally, this partition represents the parameter space where $\\begin{array}{r}{\\frac{p_{r}^{d_{r}-1}}{p_{r}^{d_{r}-1}+(1-p_{r})^{d_{r}-1}}\\,\\leqslant\\,\\Theta_{1}\\,<\\,\\frac{p_{r}^{d_{r}}}{p_{r}^{d_{r}}+(1-p_{r})^{d_{r}}}}\\end{array}$ and $\\begin{array}{r}{\\frac{\\bar{p}_{s}^{d_{s}-1}}{p_{s}^{d_{s}-1}+(1-p_{s})^{d_{s}-1}}\\leqslant\\Theta_{2}\\,<\\,\\frac{\\bar{p}_{s}^{d_{s}}}{p_{s}^{d_{s}}+(1-p_{s})^{d_{s}}}}\\end{array}$ where all parameter value sets that satisfy both inequalities are similar. Due to $d_{r}$ and $d_{s}$ being the difference between observation counts, the set of possible partitions is finite for finite horizons. ", "page_idx": 23}, {"type": "text", "text": "We explored solving the Spaceship Repair problem directly using these inequalities. The belief state reflects the probability of each outcome, meaning the main challenge is calculating the average number of timesteps to reach the goal. This can be solved by finding the average length of time of the Gambler\u2019s Ruin problem. One possible direction of future work is exploring solving BSQ constraints and gPOMDPs this way. ", "page_idx": 23}, {"type": "text", "text": "I Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The primary positive impact of BSQ constraints is their accessibility to non-experts, allowing them to input their requirements directly into a solver that optimizes the completion of tasks while aligning with the user. Moreover, BSQ constraints enable encoding safety constraints with enforceable guarantees over the belief state. Thus, this paper represents an important step in making AI more usable for non-experts, particularly in encoding constraints and preferences, while addressing safety concerns in real-world applications.\" ", "page_idx": 23}, {"type": "text", "text": "A potential negative impact of making AI more accessible through BSQ constraints is that it could also be exploited by bad actors who might encode harmful preferences. To mitigate this risk, one approach is to design goals such that negative outcomes inherently prevent goal completion, thereby teaching the agent to avoid these outcomes. Additionally, future work can explore methods for prioritizing certain constraints to ensure that the AI does not align with harmful intentions. ", "page_idx": 24}, {"type": "text", "text": "J Additional Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "While we discussed in Section 8 some of the limitations of this work, there is one additional limitation that is an important direction of future work: aligning user and problem objectives. For example, in Graph Rock Sample if the encoded goal for the gPOMDP did not require collecting rocks but the user still wanted to collect one rock of each type using the BSQ constraint in Appendix D.2, PRS would optimize the parameters to make it so no rocks are worth scanning or sampling to exit as fast as possible to minimize the expected cost. While this case is an obvious misalignment between the gPOMDP and BSQ constraint, these misalignments can be more subtle, leading to the optimal policy not behaving as intended. Therefore, future work needs to be done to explore catching misalignments to allow the user to understand and fix them. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The claims made in both the abstract and introduction reflect the paper where we introduced a new framework for user preferences (Section 3), performed a formal analysis of it (Section 4), introduced a piecewise constant algorithm (Section 5), and empirically evaluated this algorithm (Section 7). ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please refer to both Section 8 and Appendix J. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Refer to Appendix B and Appendix C for the formal proofs of the lemmas and theorems defined in the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Both the code has been provided in the supplementary material and the detailed methodology can be found in Appendix G. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The code used for this paper has been provided in the supplementary material. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Refer to Appendix G for a detailed methodology. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: In both Section 7 and Appendix F, all shown results are shown with standard deviation error. Additionally, we make it clear in both sections that we are using standard deviation error. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Refer to Appendix G. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have reviewed and can confirm our research conforms to NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Refer to Appendix I. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper poses no risk of being misused. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Documentation on the code used in this paper is provided with the code. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]