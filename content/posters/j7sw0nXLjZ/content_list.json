[{"type": "text", "text": "Instructor-inspired Machine Learning for Robust Molecular Property Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fang $\\mathbf{W}\\mathbf{u}^{1}\\cdot$ \u2217, Shuting $\\mathbf{Jin}^{2}$ , Siyuan $\\mathbf{Li}^{3}$ , Stan Z. Li3 ", "page_idx": 0}, {"type": "text", "text": "1 Computer Science Department, Stanford University 2 School of Computer Science and Technology, Wuhan University of Science and Technology 3 School of Engineering, Westlake University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning catalyzes a revolution in chemical and biological science. However, its efficacy heavily depends on the availability of labeled data, and annotating biochemical data is extremely laborious. To surmount this data sparsity challenge, we present an instructive learning algorithm named InstructMol to measure pseudolabels\u2019 reliability and help the target model leverage large-scale unlabeled data. InstructMol does not require transferring knowledge between multiple domains, which avoids the potential gap between the pretraining and fine-tuning stages. We demonstrated the high accuracy of InstructMol on several real-world molecular datasets and out-of-distribution (OOD) benchmarks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "An enduring obstacle in applied chemical and biological sciences is identifying and making chemical compounds or materials that have optimal properties for a given purpose [1, 2]. However, the vast majority of progress in these areas is still achieved primarily through time-consuming and costly trial-and-error experiments. Machine learning (ML) has undergone an unparalleled technological advancement, opening up a myriad of applications across various domains. Its potential is particularly notable in expediting the discovery and development of novel materials, pharmaceuticals, and chemical processes [3\u20135]. However, ML models\u2019 efficacy heavily relies on the availability of labeled data and the consistency of prediction targets. Meanwhile, the cost of generating new data labels through wet experiments is prohibitively high. Consequently, the size of labeled data in this field is several orders of magnitude lower than the one that can inspire breakthroughs in other ML fields [6]. This data scarcity severely hampers ML in addressing scientific challenges within this realm, impeding its ability to generalize to new molecules [7]. ", "page_idx": 0}, {"type": "text", "text": "Biochemists have noticed this problem and propose several strategies to overcome the low data limitation (view Fig. 1). Firstly, inspired by the remarkable success of self-supervised learning from NLP [8] and CV [9, 10], researchers apply the pretrain and finetune paradigm [11\u201314] to molecule modeling. They boost the performance of various molecular models by pretraining them on massive unlabeled data. However, this benefit can be negligible when a large gap exists between the sample distributions of pretraining and downstream tasks [15]. An alternative option is to use active learning [16], which iteratively selects new data points to annotate according to the current model\u2019s predictions. However, auxiliary labor is still required to enrich the original database. Thirdly, domain knowledge is incorporated to enhance molecular representations, such as providing more high-quality hand-crafted features [17], constructing motif-based hierarchical molecular graphs [18], and leveraging knowledge graphs [19]. However, domain knowledge can be biased and is difficult to integrate into different training techniques universally. ", "page_idx": 0}, {"type": "image", "img_path": "j7sw0nXLjZ/tmp/41096f0cc3487b854c7ea2f3880b6cbb1d4487d5c9497b7d65f66bd2c6cf038d.jpg", "img_caption": ["Figure 1: Four mainstream paradigms to ameliorate the scarcity of labeled biochemical data. (A) Self-supervised pretraining tasks include masked component modeling, contrastive learning, and auto-encoding. (B) Active learning involves the iterative selection of the most informative data, in which the molecular models are the most uncertain. These samples are then subjected to laboratory testing to determine their labels. This process is repeated with newly labeled data added to the training set. (C) Knowledge graphs are introduced to provide structured relations among multiple drugs and unstructured semantic relations associated with different drug molecules. (D) In SSL, the unlabeled data is used to create a smooth decision boundary between different classes or to estimate the distribution of the input data, while the labeled data is used to provide specific examples of the correct output. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this study, we develop InstructMol, a flexible semi-supervised learning (SSL) approach to excavate the abundant unlabeled biochemical data for robust molecular property prediction. It differs from prior studies in two aspects: (1) it utilizes an additional instructor model to predict the confidence of the predicted label, measure its reliability, and generate pseudo-label information for unannotated data; (2) with the help of pseudo-label information guiding the model, the target molecular model can reliably utilize unlabeled data without the need to transfer knowledge between multiple domains, which perfectly avoids the potential gap between pre-training and fine-tuning stages. We demonstrate that InstructMol outpasses existing SSL approaches and achieves state-of-the-art performance in MoleculeNet and several OOD benchmark datasets. Besides, via InstructMol, we accurately predict the properties of all 9 newly discovered drug molecules in the latest patent (ZA202303678A). Extensive experiments showcase the effectiveness of our model in surmounting the challenge of data scarcity, propelling advancements in the chemistry and biology domains. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "How to exploit large-scale unlabeled molecular data becomes an essential topic in the ML community to alleviate the scarcity of labeled data and improve OOD generalization, where pretraining and SSL are the two major fast-growing lines. The former traditionally employ unsupervised techniques to pretrain ML models, such as autoencoder [20], autoregressive modeling [13, 21], masked component modeling [22, 23], context-prediction [24], contrastive learning [25], and multi-modality [26]. Despite several progress claims, the beneftis of self-supervised pretraining can be negligible in many cases [15]. Recent years have witnessed a rising interest in developing SSL to reduce the amount of required labeled data [27]. Several hypotheses have been discussed in the literature to support specific SSL design decisions [28], such as the smoothness and manifold assumptions. Existing SSL algorithms can be roughly separated into three sorts: consistency regularization, proxy-label methods, and generative models. The consistency regularization is based on the simple concept that randomness within the neural network (NNs) such as dropout) or data augmentation transformations should not modify model predictions given the same input and impose an auxiliary loss. This line of research includes $\\pi$ -model [29], temporal ensembling [29], and mean teachers [30] The proxy-label methods regard proxy labels on unlabeled data as targets and consist of two groups: self-training [31], where the model itself produces the proxy labels, and multi-view learning [32], where proxy labels are produced by models trained on different views of the data. The generative models rely on VAE [33] and GAN [34] to capture the joint distribution more accurately. ", "page_idx": 1}, {"type": "image", "img_path": "j7sw0nXLjZ/tmp/fb25752c8223352470656757b0e617cc907c5b365835fed8b5ec9bae707e77b8.jpg", "img_caption": ["Figure 2: The outline of InstructMol. We utilize a pre-trained target molecular model to forecast the properties of unlabeled examples as pseudo-labels. Then, an instructor model predicts the confidence of those pseudo-annotations, which are leveraged to guide the target molecular model to distribute different attention in inferring different data points. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries and Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Task formulation. Suppose we have access to some labeled molecular data $\\textstyle D=\\{(x_{i},y_{i})\\}_{i=1}^{N}$ . $x_{i}$ can be in any kind of formats such as 1D sequences, 2D graphs, and 3D structures. $y_{i}$ can be any discrete (e.g., toxicity, and drug reaction) or continuous properties (e.g., water solubility, and free energy). Here we consider continuous property for simple illustration, but our approach can be easily extended to binary-class or multi-label circumstances. The target molecular model $f:\\mathcal X\\to\\mathcal Y$ can be any category of architectures, including Transformers, GNNs, and geometric NNs. There are also isnotmo et huen tsreaeinn  daantda $\\mathcal{D}^{t e s t}$ ttioo ne vsaeltusa taes $\\mathcal{D}^{t r a i n}=\\left\\{\\left(x_{i}^{t r a i n},y_{i}^{t r a i n}\\right)\\right\\}_{i=1}^{N_{1}}$ oadnedl. $\\mathcal{D}^{v a l}=\\left\\{\\left(x_{i}^{\\stackrel{.}{v a l}},y_{i}^{v a l}\\right)\\right\\}_{i=1}^{N_{2}}$ $\\mathcal{D}$ Besides, we can obtain some unlabeled data points, denoted as $\\mathcal{D}^{\\star}=\\{x_{i}^{\\star}\\}_{i=1}^{M}$ , where the number of unlabeled data $M$ is usually orders of magnitude larger than that of labeled data $N$ . ", "page_idx": 2}, {"type": "text", "text": "Challenge of proxy-labeling. Labeled data $\\mathcal{D}$ and unlabeled data $\\mathcal{D}^{\\star}$ sometimes follow significantly different data distributions, i.e., $\\mathbb{P}(x_{i},y_{i})\\neq\\mathbb{P}(x_{i}^{\\star},y_{i}^{\\star})$ . However, many SSL algorithms [29, 35, 36] assume no distributional shift between $\\mathcal{D}$ and $\\mathcal{D}^{\\star}$ . They are likely only to reinforce the consistent information in the labeled data $\\mathcal{D}$ to unlabeled examples $\\mathcal{D}^{\\star}$ instead of mining auxiliary information from $\\mathcal{D}^{\\star}$ , without exception for proxy labeling [37]. More importantly, despite the versatility and modality-agnostic of proxy labeling, it achieves relatively poor performance compared to recent SSL approaches [38]. This arises because some pseudo-labels $\\hat{y}_{i}^{\\star}$ can be severely incorrect during training due to the poor generalization ability of classic DL models [39, 40]. If we directly utilize pseudolabels from a previously learned model for subsequent training, the conformance-biased information in the proceeding epochs could increase confidence in erroneous predictions and eventually lead to a vicious circle of error accumulation [41, 42]. The situation can be even worse when labeled data $\\mathcal{D}$ contain noises because of unavoidable experimental errors. Accordingly, it becomes essential to grasp the quality and dependability of pseudo-annotations and intelligently select a subset of them to reduce the hidden noise. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Motivation of InstructMol. Though confidence is crucial for pseudo-label selection and this selection reduces pseudo-label error rates, NNs\u2019 poor calibration renders this solution insufficient. Explicitly, incorrect predictions in poorly calibrated NNs can also have high confidence (i.e., $\\hat{y}_{i}^{\\star}\\ \\to\\ 0$ or $\\hat{y}_{i}^{\\star}\\rightarrow1$ ) [38]. More importantly, prior studies such as UPS [38] resort to the target model\u2019s output $\\hat{y}_{i}$ as the confidence indicator and produce hard labels by $y_{i}^{\\star}=\\mathbb{1}\\left[\\hat{y}_{i}^{\\star}\\geq\\gamma_{1}\\right]$ or $y_{i}^{\\star}=\\mathbb{1}\\left[\\hat{y}_{i}^{\\star}\\leq\\gamma_{2}\\right]$ , where $\\gamma_{1}$ and $\\gamma_{2}$ are pre-defined thresholds. Nonetheless, this selection mechanism becomes inapplicable if $\\boldsymbol{\\wp}$ is a continuous label space, as networks no longer output class probabilities. For regression tasks, $\\hat{y}_{i}^{\\star}$ discloses no confidential information, and numerous biochemical problems are regression-based, including molecular property prediction [43\u201345], 3D structure prediction [46], and binding affinity prediction [47]. This brings a challenge for probabilistic output-based proxy-labeling algorithms [38]. Thus, instead of depending on $\\hat{y}_{i}^{\\star}$ to judge proxy labels\u2019 reliability, we accompany the target molecular model $f$ with an additional instructor model $g$ , which plays the role of a critic and predicts label observability, i.e., whether the label is true or fake. $g$ disentangles the confidence prediction and the property prediction, reducing the noise introduced by the pseudo-labeling process. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Overall Instructive Learning Framework. We separate the integral workflow of InstructMol into two phases. In the first step, we retain pseudo-labels $\\left\\{\\hat{y}_{i}^{\\star}\\right\\}_{i=1}^{M}=f\\left(\\left\\{\\hat{x}_{i}^{\\star}\\right\\}_{i=1}^{M}\\right)$ . There are several approaches to creating proxy labels, such as label propagation via neighborhood graphs [48]. Here, we require the molecular model $f$ to directly annotate samples in the unlabeled dataset $\\mathcal{D}^{\\star}$ . Then, $\\mathcal{D}^{\\prime}=\\mathcal{D}\\cup\\{(x_{i}^{\\star},\\hat{y}_{i}^{\\star})\\}_{i=1}^{M}$ eawn ds eptr. oTceheeds et rtawinoi npgr obcoetdh utrhees  taarreg eitt ermaotilevceluyl arre pmeoadteeld $f$ natnild h ree iancshtreus ctthoer $g$ $f$ optimal performance on the validation set $\\bar{\\mathcal{D}^{v a l}}$ . ", "page_idx": 3}, {"type": "text", "text": "To be specific, the instructor model $g:(\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{H}_{f})\\,\\to\\,\\mathcal{P}$ forecasts the confidence measure $p_{i}$ $[0\\leq p\\leq1]$ ) of whether the given label $y_{i}^{\\prime}$ belongs to the ground-truth label set $\\{y_{i}\\}_{i=1}^{N}$ or the pseudo-label set $\\{y_{i}^{\\star}\\}_{i=1}^{M}$ . It digests three items: the data sample with its label $(x_{i}^{\\prime},y_{i}^{\\prime})\\in\\mathcal{D}^{\\prime}$ and an additional loss term $\\mathcal{\\bar{H}}_{f}(f(x_{i}^{\\prime}),y_{i}^{\\prime})$ , where $\\mathcal{H}_{f}$ is traditionally selected as a root-mean-squared-error (RMSE) loss or a mean-absolute-error (MAE) loss for regression tasks and cross-entropy (CE) loss for classification problems. Here we regard $\\mathcal{H}_{f}(.)$ as the ingredient of $g$ \u2019s input to provide more information about the main molecular property prediction task. At last, the instructor model $g$ is supervised via a binary CE loss (BCE) as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{g}\\left(\\mathcal{D}^{\\prime},\\{\\hat{y}_{i}^{\\prime}\\}_{i=1}^{N+M}\\right)=\\sum_{(x_{i}^{\\prime},y_{i}^{\\prime})\\in\\mathcal{D}^{\\prime}}\\mathbf{BCE}(p_{i}^{\\prime},c_{i})=\\sum_{(x_{i}^{\\prime},y_{i}^{\\prime})\\in\\mathcal{D}^{\\prime}}\\mathbf{BCE}\\Big(g\\big(x_{i}^{\\prime},y_{i}^{\\prime},\\mathcal{H}_{f}\\left(\\hat{y}_{i}^{\\prime},y_{i}^{\\prime}\\right)\\big),c_{i}\\Big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $c_{i}\\,\\in\\,[0,1]$ is an integer and represents the observability mask. It indicates whether $y_{i}$ is pseudo-labeled ( $(c_{i}=0)$ ) or not $(c_{i}=1$ ). However, since the number of unlabeled data $M$ is much larger than the number of labeled data $N$ , it is proper to shift the loss function from BCE to a focal loss [49] (FL) for unbalanced classes as: $\\begin{array}{r l}{\\;\\stackrel{}{\\mathcal{L}}_{g}\\left(\\mathcal{D}^{\\prime},\\{\\hat{y}_{i}^{\\prime}\\}_{i=1}^{N+M}\\right)\\;=\\;\\sum_{(x_{i}^{\\prime},y_{i}^{\\prime})\\in\\mathcal{D}^{\\prime}}\\mathrm{FL}(p_{i}^{\\prime},c_{i})\\;=}&{}\\end{array}$ $\\begin{array}{r}{\\sum_{(x_{i}^{\\prime},y_{i}^{\\prime})\\in\\mathcal{D}^{\\prime}}-(1-p_{i}^{\\prime})^{\\gamma}\\log(p_{i}^{\\prime})}\\end{array}$ , where $\\gamma\\geq0$ is a tunable focusing parameter. ", "page_idx": 3}, {"type": "text", "text": "Meanwhile, the target molecular model $f$ receives the discriminative information }iN=+1Mfrom the instructor model $g$ and uses it to reweight the importance of different samples in backpropagating its gradient. In other words, the instructor model $g$ guides the target model $f$ to deliver different attention to different labels so that correct labels are attached more importance while erroneous labels are ignored. This can be realized by specially designing the loss of the target model $f$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{f}\\left(\\mathcal{D}^{\\prime},\\{p_{i}\\}_{i=1}^{N+M}\\right)=\\alpha\\sum_{(x_{i},y_{i})\\in\\mathcal{D}}\\mathcal{H}_{f}(f(x_{i}),y_{i})+\\sum_{\\left(x_{j}^{\\star},\\hat{y}_{j}^{\\star}\\right)\\in\\mathcal{D}^{\\star}}(2p_{j}-1)\\cdot\\mathcal{H}_{f}\\left(f\\left(x_{j}^{\\star}\\right),\\hat{y}_{j}^{\\star}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $0\\leq\\alpha\\leq1$ is a hyper-parameter to balance the dominance of labeled and unlabeled data sets. $\\mathcal{L}_{f}(.)$ transforms the original main task into a cost-sensitive learning problem [50] by imposing ", "page_idx": 3}, {"type": "text", "text": "1: Input: target model $f$ , instructor model $g$ , labeled data $\\mathcal{D}$ , unlabeled data $\\mathcal{D}^{\\star}$ , pseudo-label   \nupdate frequency $k$ , loss weight $\\alpha$   \n2: Initialize and pretrain a target model $f_{0}$ and an instructor model $g_{0}$   \n3: for epochs $n=0,1,2,\\dots{}$ do   \n4: if $n$ mod $k==0$ then   \n5: With no gradient:   \n6: $\\hat{y}_{i}^{\\star}\\gets\\bar{f}(x_{i}^{\\star})$ , $\\forall x_{i}^{\\star}\\in\\mathcal{D}^{\\star}$ \u25b7Iteratively assign pseudo-labels every $k$ epochs   \n7: $\\begin{array}{r l}&{\\mathbf{end}_{i}^{y_{1}}\\cdots\\mathbf{\\Xi}^{y_{l},y_{l}},\\quad\\cdots\\mathbf{\\Xi}_{\\times\\times\\times\\dots}^{\\times\\dots}\\times\\cdots\\times\\times\\mathbf{\\Xi}_{\\times\\times\\times\\dots}^{\\times\\times\\times\\dots}\\times\\cdots\\times\\times\\cdots\\times\\times\\cdots\\times\\times\\cdots\\times\\times\\cdots\\times\\times\\cdots}\\\\ &{\\mathbf{\\Xi}_{D^{\\prime}}^{D}\\leftarrow D\\cup\\{(x_{i}^{\\star},\\hat{y}_{i}^{\\star})\\}_{i=1}^{M}\\quad\\mathrm{~\\Rightarrow~Construct~the~hybrid~database}}\\\\ &{\\hat{y}_{i}^{\\prime}\\leftarrow f(x_{i}^{\\prime})\\quad\\forall x_{i}^{\\prime}\\in\\mathcal{D}^{\\prime}}\\\\ &{p_{i}\\leftarrow g(x_{i}^{\\prime},y_{i}^{\\prime},\\mathcal{H}_{f}(\\cdot)),\\quad\\forall(x_{i}^{\\prime},y_{i}^{\\prime})\\in\\mathcal{D}^{\\prime}\\quad\\triangleright\\mathrm{Predict~the~confidence~scores}}\\\\ &{\\mathcal{L}_{g}\\left(\\mathcal{D}^{\\prime},\\{\\hat{y}_{i}^{\\prime}\\}_{i=1}^{N+M}\\right)\\leftarrow\\mathrm{Equation~}1}\\\\ &{\\mathcal{L}_{f}\\left(\\mathcal{D}^{\\prime},\\{p_{i}\\}_{i=1}^{N+M}\\right)\\leftarrow\\mathrm{Equation~}2}\\\\ &{\\mathrm{Update~the~parameters~of~}f\\mathrm{~and~}g\\mathrm{~based~on~}\\mathcal{L}_{g}(.)\\mathrm{~and~}\\mathcal{L}_{f}(.)}\\end{array}$   \n8:   \n9:   \n10:   \n11:   \n12:   \n13:   \n14: end for ", "page_idx": 4}, {"type": "text", "text": "a group of soft-labeling weights based on the predicted confidence of data labels. That is, for pseudo-labeled samples (i.e., $\\bar{x}_{j}^{\\prime}\\in\\mathcal{D}^{\\star})$ , the soft-labeling weight becomes $2p_{j}-1$ . ", "page_idx": 4}, {"type": "text", "text": "This loss format in Equation 2 induces different behaviors on the loss $\\mathcal{H}_{f}(.)$ of labeled and pseudolabeled instances, where the judgment {pi}iN=+1Mp roduced by the instructor model $g$ is leveraged to differentiate their informativeness. Notably, if the instructor model $g$ regards a pseudo-label $\\hat{y}_{j}^{\\star}$ to be unreliable (i.e., $0.5>p_{j}>0_{\\cdot}$ ), the loss $\\mathcal{L}_{f}(.)$ chooses to enlarge the gap with the pseudo-label $\\hat{y}_{j}^{\\star}$ . Meanwhile, once the instructor $g$ fully trusts the pseudo-label $\\hat{y}_{j}^{\\star}$ , InstructMol forces the target molecular model $f$ to give more effort to inferring this proxy-labeled sample. Generally, the more likely a pseudo-label $\\hat{y}_{j}^{\\star}$ is considered reliable by the instructor model $g$ (i.e., $p_{j}\\rightarrow1$ ), the stronger it drives the target molecular model $f$ to make further improvement on inferring this label $\\hat{y}_{j}^{\\star}$ . Otherwise, InstructMol will push $f$ to overturn its previous belief if $p_{j}\\to0$ . Noticeably, we can also impose a soft-labeling weight for samples with true labels (i.e., $x_{i}^{\\prime}\\in\\mathcal{D}$ ). For instance, a weight factor of $\\alpha\\geq1$ can navigate $f$ to concentrate more on samples that are more trusted by $g$ . But we practically discover no significant refinement with this design on labeled data and leave it for future exploration. The whole pseudo-code of InstructMol is depicted in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Loss Selection for InstructMol. We compare some relevant methods from the literature under the proxy-labeling SSL algorithms in Appendix Tab. 6, containing the vanilla proxy-labeling (PL), curriculum learning for PL (CPL) [51], UPS, and self-interested coalitional learning (SCL) [52]. It is structured in three main columns that describe the selection for unlabeled samples, loss function, and fitness for regression problems. Except for SCL and InstructMol, all approaches adopt a subset of unannotated instances for training rather than utilizing the entire unlabeled datasets. SCL can take advantage of all data points from $\\mathcal{D}^{\\star}$ , but its main limitation is its improper or even severely wrong loss function design. As the confidence score is close to 1, SCL assigns a strong negative multiplier factor as 1 \u22121\u2212\u03b1pj $\\begin{array}{r}{1-\\frac{\\alpha}{1-p_{j}}\\rightarrow-\\infty}\\end{array}$ and forces the target model $f$ to disregard this label, which is actually reliable. On the contrary, when the instructor model g doubts the reliability of y\u02c6j\u22c6 , the ratio 1 \u22121\u2212\u03b1pj becomes $1-\\alpha$ , driving the target model $f$ to move towards it. ", "page_idx": 4}, {"type": "text", "text": "Guidelines for InstructMol. Before executing InstructMol, it is natural to first obtain a well-trained molecular target model $f_{0}$ through regular supervised learning on $\\mathcal{D}$ and then initialize an instructor $g_{0}$ by discriminating pseudo-labels that are generated by $f_{0}$ . This is empirically proven to achieve higher training stability and robustness. Moreover, pseudo-labels are assigned every $k$ epoch and a proper setting of $k$ is critical to the success of InstructMol. If pseudo-labels are updated too frequently, the training procedure will be volatile at the very beginning. While a too-large $k$ would significantly increase the training complexity. Here we adopt an adaptive decay strategy: with an initial value $k_{0}$ , the update frequency decreases by a factor of 0.5 until it reaches the minimum threshold $k_{\\mathrm{min}}=3$ . ", "page_idx": 4}, {"type": "text", "text": "Analysis of InstructMol. After the curation of $\\mathcal{D}^{\\prime}$ , there are two distinct learning tasks during the second stage of InstructMol. Specifically, the target model follows the regular routine to predict the molecular properties. Meanwhile, the instructor model strives to differentiate whether the label is real. Notably, some prior works embody a similar idea of jointly learning multiple tasks. For instance, the multi-objective optimization (MOO) methods [53] exploit the shared information and the underlying commonalities between two tasks and solve the problem by minimizing an augmented loss. In addition, GAN [54] makes a generator and a discriminator constantly compete against each other. Nevertheless, MOO cannot handle possible contradictions among different tasks in certain settings, where jointly minimizing the augmented loss may impede both tasks from attaining the global optimal [55]. While GAN has been praised for generating high-quality data, it is notoriously difficult to train and requires a large amount of training data. More essentially, making predictions in an adversarial manner for unlabeled data deviates from our primary goal of pseudo-labeling but is a mature technology for domain adaptation [56]. ", "page_idx": 4}, {"type": "table", "img_path": "j7sw0nXLjZ/tmp/eb312d586ae6e39f1492dd83080f74e4f58d6bd696d7533f8f64a5eb02b25820.jpg", "table_caption": ["Table 1: Performance of three distinct ML models with different SSL methods on nine molecular property prediction tasks. For classification tasks, we calculate the ROC-AUC, while for regression tasks, we use RMSE as the evaluation metric. The number in the bracket is the standard deviation of three runs. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "j7sw0nXLjZ/tmp/6f539235411395e33bf975110232246701144c986ad2f6d2d1239f36d6c43984.jpg", "table_caption": ["Table 2: In-domain and OOD performance on the GOOD benchmark All numerical results are averages across 3 random runs. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We carry out a wide scope of experiments in all contexts. Section 5.1 shows the beneftis of InstructMol in predicting molecular properties compared with various SSL learning algorithms. Section 5.2 verifies the superiority of InstructMol in lowering the predictive error over existing OOD generalization algorithms. Section 5.3 investigates the possibility of marring InstructMol with cutting-edge pretraining methods and demonstrates its potency by setting a state-of-the-art performance on MoleculeNet. Section 5.4 analyzes pseudo-labels\u2019 accuracy, the instructor\u2019s behavior, and ablation studies. ", "page_idx": 5}, {"type": "text", "text": "5.1 Molecular Property Prediciton ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Data and Setups. We first investigate the effectiveness of InstructMol on three sorts of backbone including GCN [65], GAT [66], and GIN [67], and report their performance on the standard MoleculeNet [43]. Datasets are divided using scaffold splitting into training, validation, and test sets with a ", "page_idx": 5}, {"type": "table", "img_path": "j7sw0nXLjZ/tmp/bac1380fbc1016b39eab73560dddb88b68758eb3b71f9161a724cca543044bb3.jpg", "table_caption": ["Table 3: Performance on molecular property prediction tasks, where GEM+InstructMol achieves the best result. "], "table_footnote": ["ratio of 8:1:1. We report the mean and standard deviation of the results for three random seeds. More experimental details are put in Appendix A.2. "], "page_idx": 6}, {"type": "text", "text": "Baselines. Several SSL baselines are chosen including consistency regularization, proxy-label methods, and generative models. $\\pi$ -model [29] applies different stochastic transformations (i.e., dropout) to the networks instead of the input graphs. Semi-GAN [34] introduces a discriminator to classify whether the input is labeled or not. Uncertainty-aware pseudo-label selection (UPS) [38] leverages the prediction uncertainty to guide the pseudo-label selection procedure but is merely applicable to classification problems. ", "page_idx": 6}, {"type": "text", "text": "Results. Tab. 1 shows that InstructMol significantly improves various ML architectures and outperforms all SSL baselines. For GIN, GAT, and GCN, it leads to an average increase in ROC-AUC of $8.6\\%$ , $7.1\\%$ , and $6.6\\%$ , respectively, for six classification tasks and an average decrease in RMSE of $9.3\\%$ , $8.0\\%$ , and $7.7\\%$ separately for three regression tasks. These statistics demonstrate our approach effectively boosts existing ML models in low-data circumstances for molecular scaffold property prediction, as most datasets in MoleculeNet have only thousands of labeled samples. Besides that, more up-to-date ML models like GIN enjoy stronger beneftis of our InstructMol than primitive ones like GCN. It is also worth mentioning that InstructMol overcomes UPS\u2019s drawbacks and can be utilized for regression tasks. All evidence clarifies that InstructMol is a more advanced proxy labeling algorithm than the existing SSL mechanisms with stronger virtual screening capacity and broader applications. ", "page_idx": 6}, {"type": "text", "text": "5.2 OOD Generalization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Data and Setsups. Measuring OOD generalization is particularly relevant in molecular property prediction, where distributional shifts can be large and difficult to handle for ML models. Different molecular datasets obtained by distinct pharmaceutical companies and research groups often contain compounds from vastly different areas of chemical space that exhibit high structural heterogeneity. Towards this goal, we leverage the Graph Out-of-Distributio (GOOD) benchmark [77], where GOODHIV is a small-scale dataset for HIV replication inhibition and GOOD-PCBA includes 128 bioassays and forms 128 binary classification tasks. They are divided into a training set, an in-domain (ID) validation set, an ID test set, and OOD test sets by covariate and concept shift splits. ", "page_idx": 6}, {"type": "text", "text": "Baselines. The empirical risk minimization (ERM) and several OOD algorithms are considered. IRM [57] searches for data representations that perform well across all environments by penalizing feature distributions with different optimal linear classifiers for each environment. VREx [58] targets both covariate robustness and the invariant prediction. GroupDRO [59] tackles the problem that the distribution minority lacks sufficient training. DANN [60] adversarially trains the regular classifier and a domain classifier to make features indistinguishable. Deep Coral [61] encourages features in different domains to be similar by minimizing the deviation of covariant matrices from different domains. Q-SAVI [62] encodes explicit prior knowledge of the data-generating process into a prior distribution over functions. DIR [64] and Mixup [63] are two graph-specific OOD methods. ", "page_idx": 6}, {"type": "image", "img_path": "j7sw0nXLjZ/tmp/3a8f005ae4b8fccc93a058682532dfdf7a8f24f13d714ea38bcfae4828dc7639.jpg", "img_caption": ["Figure 3: The scatter plot of the distributions of LogP predictions for unlabeled data with and without InstructMol. The first row includes predictions before instructive learning, and the second row includes predictions after instructive learning. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Results. Tab. 2 documents the statistics, where most OOD algorithms have comparable performance with ERM. The risk interpolation methods like GroupDRO and the risk extrapolation mechanisms like VREx perform favorably against others on multiple shift splits. In contrast, InstructMol significantly exceeds ERM and other OOD baselines in all circumstances. Specifically, InstructMol brings improvements of $2.40\\%$ , $2.05\\%$ , $7.68\\%$ , and $4.69\\%$ for GOOD-HIV and GOOD-PCBA\u2019s different ID splits, and $3.62\\%$ , $6.72\\%$ , $9.83\\%$ , and $7.51\\%$ for GOOD-HIV and GOOD-PCBA\u2019s different OOD splits. It can be seen that gains for OOD are greater than benefits for ID, indicating the promise of pseudo-labeling to tackle OOD generalization. ", "page_idx": 7}, {"type": "text", "text": "5.3 SSL with Self-supervised Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setups and Background. Pretraining and SSL are not mutually exclusive but can collaborate for more robust scientific investigations [78]. So we design a two-step workflow: (1) In the pretraining stages, unlabeled data is first used in a task-agnostic way, and we attain more general molecular representations. Then, those general representations are adapted for a specific task for fine-tuning. (2) In the instructive learning stage, unlabeled data is used again in a task-specific way via InstructMol. We combine GEM [24] and InstructMol and examine their joint effectiveness on MoleculeNet. ", "page_idx": 7}, {"type": "text", "text": "Baselines. Multiple baselines are selected for a thorough comparison. D-MPNN [68], MGCN [70] and AttentiveFP [69] are supervised GNN methods. N-gram [71], PretrainGNN [72], InfoGraph [73], GPT-GNN [22], GROVER [74], 3D-Infomax [75], GraphMVP [26], MolCLR [25], and Uni-Mol [76] are pretraining methods. We adopt the same scaffold splitting strategy as GEM and Uni-Mol with three repeated runs. ", "page_idx": 7}, {"type": "text", "text": "Results. The overall performance of InstructMol based on GEM and other baseline methods is summarized in Tab. 3. InstructMol achieves new SOTA results on all MoleculeNet tasks and brings an average improvement of $4.35\\%$ . Notably, its beneftis are much stronger for regression tasks with a mean decrease of $9.98\\%$ in RMSE. Another set of results on MoleculeNet with a different splitting method is in Appendix Tab. 5, where InstructMol also outperforms all baselines. This highlights the necessity and importance of leveraging unlabeled examples to refine and transfer task-specific knowledge after pretraining through instructive learning. It also implies that InstructMol is not incompatible with existing pre-training ML models but can effectively supplement them with an additional instructor model. ", "page_idx": 7}, {"type": "image", "img_path": "j7sw0nXLjZ/tmp/fafe78adb3848a0976c0976a6c2b32b63b6bf28bbddb35748f07e6bf48704af1.jpg", "img_caption": ["Figure 5: The distributions of confidence scores given by the instructor model during the training process. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Discussion, Ablation, and Other Applications ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Empirical evidence of pseudo-labels\u2019 accuracy. Pseudo-labels\u2019 accuracy is crucial, as it reflects InstructMol\u2019s ability to generalize to unseen molecules. However, no ground-truth annotations exist for unlabeled data across the mentioned tasks. To address this issue, we adopt partition-coefficient values (LogP) as the target. A key predictor of drug-likeness featured in the famous \"Lipinski\u2019s rule of five,\" LogP can quickly screen large libraries of potential therapeutics using computational tools like XLogP. We utilize a public Kaggle dataset of $14.6\\mathbf{k}$ molecules with associated LogP values for training and evaluate the predictions on the much larger ZINC15, whose LogP is computed using RDKit. Furthermore, measurements often number in the tens rather than thousands in the low-data regime of drug discovery. To assess InstructMol\u2019s limits, we significantly reduce the training size and investigate its efficacy with only $0.1\\%$ , $0.5\\%$ , $1\\%$ , $5\\%$ , and $10\\%$ of the entire training samples, resulting in scaffold-split training sets of 14, 73, 146, 730, and 1,326 molecules, respectively. ", "page_idx": 8}, {"type": "text", "text": "Fig. 3 compares the LogP prediction distributions before and after instructive learning. It can be observed that DL models trained solely on labeled data perform poorly in estimating unseen molecules\u2019 LogP with an average RMSE of 5.63. Contrarily, InstructMol enables a pretty accurate prediction of LogP with an average RMSE of 0.63 even with a very limited number of training examples, verifying the robustness of pseudo-labels. ", "page_idx": 8}, {"type": "text", "text": "Ablation Studies. We further investigate the effects of the number of unannotated molecules on the performance of InstructMol. As shown in Figure 4, the enrichment of unlabeled data consistently brings benefits to various downstream tasks. ", "page_idx": 8}, {"type": "text", "text": "Real-world Drug Discovery. We retrieved the latest patent ZA202303678A targeting the 5-HT1A receptor to examine IntructMol in real-world applications. ZA202303678A is published after 2023, and the property test standards of new compounds in ZA202303678A are consistent with those recorded in the CHEMBL214_Ki dataset [79]. All 9 new small molecule drugs are marked as good binders by InstructMol. Since the patent ZA202303678A provides accurate ground truth values of $K_{i}$ through wet experiments, we compare the predicted values with the real ones as shown in Appendix 7. It can be observed that the errors are, at most, within four times, and most predicted results are close to real values. Among them, predicted $K_{i}$ of (S)-5-FPT (Ground truth: $K_{i}=4$ , Prediction: $K_{i}=3.71)$ ), (S)-5-NaT (Ground truth: $K_{i}=64$ , Prediction: $K_{i}=61.86)$ , (S)-5-CPPT (Ground truth: $K_{i}=0.6$ , Prediction: $K_{i}=0.93)$ , and (S)-5-FPyT (Ground truth: $K_{i}=1.3$ , Prediction: $K_{i}=1.29)$ is almost equal to actual $K_{i}$ obtained from biological experiments. ", "page_idx": 8}, {"type": "image", "img_path": "j7sw0nXLjZ/tmp/9f37e0c51046f397779019f5cab11b18904313334053691a29e8996a6541f04c.jpg", "img_caption": ["Figure 4: The influence of unlabeled data size on four tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "j7sw0nXLjZ/tmp/43fdf095e7ce61c22328e4f4c5a7edc688ab82cc3468c4f2b2e26104ef00b5f1.jpg", "img_caption": ["Figure 6: The distribution of the predictions of labeled data and unlabeled data generated by the target model during different training stages. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Instructor model\u2019s Behavior. The instructor estimates the target model\u2019s uncertainty. Therefore, thoroughly evaluating its judgment and contribution is beneficial. Here, we analyze its output value distribution on labeled and unlabeled data in different training stages of an activity cliff estimation task (CHEMBL214_Ki). The plots in Fig. 5 demonstrate the transition of an instructor model. Notably, since we pretrain the instructor before SSL, it performs well in distinguishing fake labels but remains confused with real ones. From the first iterations to 10K iterations, the instructor gradually gains a stronger capacity to discriminate true labels (confidence score $\\rightarrow1.0$ ) and fake ones (confidence score $\\rightarrow0.0$ ). Taking a step further, we draw the distribution of the target model\u2019s output in Fig. 6 and show a significant overlap between predictions of labeled and unlabeled data. This undoubtedly excludes the hypothesis that the instructor discriminates labeled and unlabeled data simply based on distinct distributions of their predictions. Even though predictions of labeled and unlabeled data are highly similar, the instructor still succeeds in comprehending their uncertainty and guides the target model to leverage pseudo-labels more cautiously. The instructor model\u2019s interpretability is a byproduct of our InstructMol and can be useful for many real-world biochemical problems. Here we give the relevant appendix figures cited in the manuscript. ", "page_idx": 9}, {"type": "text", "text": "6 Conlcusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents InstructMol, a novel instructive learning framework, to alleviate the difficulty of experimentally obtaining the ground truth properties of molecular data and to overcome the limitation of the small number of labeled biochemical data points. InstructMol sufficiently cutting-edge pretraining methods for molecular representation, and addresses essential real-world problems. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Despite the great progress of InstructMol in achieving enhanced molecular learning capacity, there are still minor limitations that require future exploration. For instance, our combination of InstructMol with self-supervised mechanisms is based on existing methodologies such as GEM, Uni-Mol, and GROVE. It is interesting to develop a more suitable self-supervised learning algorithm that can be better aligned with InstructMol. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. This work was supported by the National Natural Science Foundation of China (No.62402351), the Hubei Provincial Natural Science Foundation of China (No.2024AFB275), and the Scientific Research Project of Education Department of Hubei Province (No.Q20231109). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] John A Keith, Valentin Vassilev-Galindo, Bingqing Cheng, Stefan Chmiela, Michael Gastegger, Klaus-Robert Muller, and Alexandre Tkatchenko. Combining machine learning and computational chemistry for predictive insights into chemical systems. Chemical reviews, 121(16): 9816\u20139872, 2021. ", "page_idx": 9}, {"type": "text", "text": "[2] Xiangru Tang, Howard Dai, Elizabeth Knight, Fang Wu, Yunyang Li, Tianxiao Li, and Mark Gerstein. A survey of generative ai for de novo drug design: new frontiers in molecule and protein generation. Briefings in Bioinformatics, 25(4):bbae338, 2024.   \n[3] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263\u20131272. PMLR, 2017. [4] Fang Wu and Stan Z Li. Diffmd: a geometric diffusion model for molecular dynamics simulations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 5321\u20135329, 2023.   \n[5] Houtim Lai, Longyue Wang, Ruiyuan Qian, Geyan Ye, Juhong Huang, Fandi Wu, Fang Wu, Xiangxiang Zeng, and Wei Liu. Interformer: An interaction-aware model for protein-ligand docking and affinity prediction. 2024.   \n[6] Pedro Hermosilla and Timo Ropinski. Contrastive representation learning for 3d protein structures. arXiv preprint arXiv:2205.15675, 2022. [7] Fang Wu, Nicolas Courty, Zhang Qiang, Ziqing Li, et al. Metric learning-enhanced optimal transport for biochemical regression domain adaptation. arXiv preprint arXiv:2202.06208, 2022. [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [9] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.   \n[10] Siyuan Li, Weiyang Jin, Zedong Wang, Fang Wu, Zicheng Liu, Cheng Tan, and Stan Z Li. Semireward: A general reward model for semi-supervised learning. arXiv preprint arXiv:2310.03013, 2023.   \n[11] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. PNAS, 2019. doi: 10.1101/622803. URL https://www.biorxiv.org/content/10.1101/622803v4.   \n[12] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 44(10):7112\u20137127, 2021.   \n[13] Fang Wu, Shuting Jin, Yinghui Jiang, Xurui Jin, Bowen Tang, Zhangming Niu, Xiangrong Liu, Qiang Zhang, Xiangxiang Zeng, and Stan Z Li. Pre-training of equivariant graph matching networks with conformation flexibility for drug binding. Advanced Science, 9(33):2203796, 2022.   \n[14] Shengjie Luo, Tianlang Chen, Yixian Xu, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. One transformer can understand both 2d & 3d molecular data. arXiv preprint arXiv:2210.01765, 2022.   \n[15] Ruoxi Sun. Does gnn pretraining help molecular representation? arXiv preprint arXiv:2207.06010, 2022.   \n[16] Justin S Smith, Ben Nebgen, Nicholas Lubbers, Olexandr Isayev, and Adrian E Roitberg. Less is more: Sampling chemical space with active learning. The Journal of chemical physics, 148 (24):241733, 2018.   \n[17] Lauri Himanen, Marc OJ J\u00e4ger, Eiaki V Morooka, Filippo Federici Canova, Yashasvi S Ranawat, David Z Gao, Patrick Rinke, and Adam S Foster. Dscribe: Library of descriptors for machine learning in materials science. Computer Physics Communications, 247:106949, 2020.   \n[18] Fang Wu, Dragomir Radev, and Stan Z Li. Molformer: motif-based transformer on 3d heterogeneous molecular graphs. Rn, 1:1, 2023.   \n[19] Xuan Lin, Zhe Quan, Zhi-Jie Wang, Tengfei Ma, and Xiangxiang Zeng. Kgnn: Knowledge graph neural network for drug-drug interaction prediction. In IJCAI, volume 380, pages 2739\u20132745, 2020.   \n[20] Fang Wu and Stan Z Li. Surface-vqmae: Vector-quantized masked auto-encoders on molecular surfaces. In International Conference on Machine Learning, pages 53619\u201353634. PMLR, 2024.   \n[21] Mengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. Mocl: Data-driven molecular fingerprint via knowledge-aware contrastive learning from molecular graph. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 3585\u20133594, 2021.   \n[22] Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-gnn: Generative pre-training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1857\u20131867, 2020.   \n[23] Fang Wu and Stan Z Li. A hierarchical training paradigm for antibody structure-sequence co-design. Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. Nature Machine Intelligence, 4(2):127\u2013134, 2022.   \n[25] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of representations via graph neural networks. Nature Machine Intelligence, 4(3): 279\u2013287, 2022.   \n[26] Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pre-training molecular graph representation with 3d geometry. arXiv preprint arXiv:2110.07728, 2021.   \n[27] Yassine Ouali, C\u00e9line Hudelot, and Myriam Tami. An overview of deep semi-supervised learning. arXiv preprint arXiv:2006.05278, 2020.   \n[28] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542\u2013542, 2009.   \n[29] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.   \n[30] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.   \n[31] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd annual meeting of the association for computational linguistics, pages 189\u2013196, 1995.   \n[32] Jing Zhao, Xijiong Xie, Xin Xu, and Shiliang Sun. Multi-view learning overview: Recent progress and new challenges. Information Fusion, 38:43\u201354, 2017.   \n[33] M Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel. Infinite variational autoencoder for semi-supervised learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5888\u20135897, 2017.   \n[34] Augustus Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint arXiv:1606.01583, 2016.   \n[35] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semisupervised learning with consistency and confidence. Advances in neural information processing systems, 33:596\u2013608, 2020.   \n[36] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10687\u201310698, 2020.   \n[37] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, page 896, 2013.   \n[38] Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. arXiv preprint arXiv:2101.06329, 2021.   \n[39] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624, 2021.   \n[40] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349, 2021.   \n[41] Xiao Cai, Feiping Nie, Weidong Cai, and Heng Huang. Heterogeneous image features integration via multi-modal semi-supervised learning model. In Proceedings of the IEEE International Conference on Computer Vision, pages 1737\u20131744, 2013.   \n[42] Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudolabeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2020.   \n[43] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513\u2013530, 2018.   \n[44] Fang Wu, Qiang Zhang, Dragomir Radev, Jiyu Cui, Wen Zhang, Huabin Xing, Ningyu Zhang, and Huajun Chen. 3d-transformer: Molecular representation with transformer in 3d space. 2021.   \n[45] Fang Wu. A semi-supervised molecular learning framework for activity cliff estimation.   \n[46] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.   \n[47] Renxiao Wang, Xueliang Fang, Yipin Lu, Chao-Yie Yang, and Shaomeng Wang. The pdbbind database: methodologies and updates. Journal of medicinal chemistry, 48(12):4111\u20134119, 2005.   \n[48] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5070\u20135079, 2019.   \n[49] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.   \n[50] Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate example weighting. In Third IEEE international conference on data mining, pages 435\u2013442. IEEE, 2003.   \n[51] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6912\u20136920, 2021.   \n[52] Huiling Qin, Xianyuan Zhan, Haoran Xu, et al. Enhancing semi-supervised learning via self-interested coalitional learning.   \n[53] Kalyanmoy Deb and Kalyanmoy Deb. Multi-objective optimization. In Search methodologies: Introductory tutorials in optimization and decision support techniques, pages 403\u2013449. Springer, 2013.   \n[54] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020.   \n[55] Huiling Qin, Xianyuan Zhan, Yuanxun Li, Xiaodu Yang, and Yu Zheng. Network-wide traffic states imputation using self-interested coalitional learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1370\u20131378, 2021.   \n[56] Fang Wu, Nicolas Courty, Shuting Jin, and Stan Z Li. Improving molecular representation learning with metric learning-enhanced optimal transport. Patterns, 4(4), 2023.   \n[57] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[58] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning, pages 5815\u20135826. PMLR, 2021.   \n[59] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.   \n[60] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machine learning research, 17(59):1\u201335, 2016.   \n[61] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Computer Vision\u2013ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14, pages 443\u2013450. Springer, 2016.   \n[62] Leo Klarner, Tim GJ Rudner, Michael Reutlinger, Torsten Schindler, Garrett M Morris, Charlotte Deane, and Yee Whye Teh. Drug discovery under covariate shift with domain-informed prior distributions over functions. In International Conference on Machine Learning, pages 17176\u2013 17197. PMLR, 2023.   \n[63] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.   \n[64] Ying-Xin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. arXiv preprint arXiv:2201.12872, 2022.   \n[65] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[66] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \n[67] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.   \n[68] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molecular representations for property prediction. Journal of chemical information and modeling, 59 (8):3370\u20133388, 2019.   \n[69] Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. Journal of medicinal chemistry, 63(16):8749\u20138760, 2019.   \n[70] Chengqiang Lu, Qi Liu, Chao Wang, Zhenya Huang, Peize Lin, and Lixin He. Molecular property prediction: A multilevel quantum interactions modeling perspective. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 1052\u20131060, 2019.   \n[71] Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised representation for graphs, with applications to molecules. Advances in neural information processing systems, 32, 2019.   \n[72] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.   \n[73] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. arXiv preprint arXiv:1908.01000, 2019.   \n[74] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. Advances in Neural Information Processing Systems, 33:12559\u201312571, 2020.   \n[75] Hannes St\u00e4rk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan G\u00fcnnemann, and Pietro Li\u00f2. 3d infomax improves gnns for molecular property prediction. In International Conference on Machine Learning, pages 20479\u201320502. PMLR, 2022.   \n[76] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. 2022.   \n[77] Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. Good: A graph out-of-distribution benchmark. Advances in Neural Information Processing Systems, 35:2059\u20132073, 2022.   \n[78] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:22243\u201322255, 2020.   \n[79] Derek van Tilborg, Alisa Alenicheva, and Francesca Grisoni. Exposing the limitations of molecular machine learning with activity cliffs. Journal of Chemical Information and Modeling, 62(23):5938\u20135951, 2022.   \n[80] Teague Sterling and John J Irwin. Zinc 15\u2013ligand discovery for everyone. Journal of chemical information and modeling, 55(11):2324\u20132337, 2015.   \n[81] Bharath Ramsundar, Peter Eastman, Patrick Walters, Vijay Pande, Karl Leswing, and Zhenqin Wu. Deep Learning for the Life Sciences. O\u2019Reilly Media, 2019. https://www.amazon.com/ Deep-Learning-Life-Sciences-Microscopy/dp/1492039837.   \n[82] Greg Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum, 8, 2013.   \n[83] Anna Gaulton, Louisa J Bellis, A Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, et al. Chembl: a large-scale bioactivity database for drug discovery. Nucleic acids research, 40(D1):D1100\u2013 D1107, 2012.   \n[84] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Appendix A Experimental Setups ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Unlabeled Data ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use the ZINC15 [80] database to collect unlabeled molecular data, which can be downloaded from DeepChem [81]. There are four different data sizes supported by ZINC15: 250K, 1M, 10M, and 270M. For MoleculeNet, we utilize the 1M molecules as the unlabeled dataset. Those unlabeled SMILES are then converted by RDKit [82] into 2D graphs. We expect future work to enrich the unlabeled corpus by leveraging other resources such as ChemBL [83], Chembridge, and Chemdiv. ", "page_idx": 15}, {"type": "text", "text": "A.2 Molecular Property Predictions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Dataset Description. We conduct experiments on the MoleculeNet [43] to examine the efficaciousness of our algorithm for molecular property prediction. It is a widely used benchmark and we include 9 datasets in the main text, which are described as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 BBBP. The blood-brain barrier penetration (BBBP) dataset contains binary labels of bloodbrain barrier penetration (permeability).   \n\u2022 BACE. The BACE dataset provides quantitative $(I C_{50})$ ) and qualitative (binary label) binding results for a set of inhibitors of human $\\beta$ -secretase 1 (BACE-1).   \n\u2022 ClinTox. The ClinTox dataset compares drugs approved by the FDA and those that have failed clinical trials for toxicity reasons.   \n\u2022 Tox21. The \u201cToxicology in the 21st Century\u201d (Tox21) initiative created a public database, which contains qualitative toxicity measurements on 12 biological targets, including nuclear receptors and stress response pathways.   \n\u2022 Toxcast. ToxCast is another data collection (from the same initiative as Tox21) providing toxicology data for a large library of compounds based on in vitro high-throughput screening, including experiments on over 600 tasks.   \n\u2022 SIDER. The Side Effect Resource (SIDER) is a database of marketed drugs and adverse drug reactions (ADR), grouped into 27 system organ classes.   \n\u2022 ESOL. ESOL is a small dataset consisting of water solubility data (log solubility in mols per liter) for common organic small molecules.   \n\u2022 FreeSolv. The Free Solvation Database (FreeSolv) provides experimental and calculated hydration-free energy of small molecules in water.   \n\u2022 Lipo. Lipophilicity is an important feature of drug molecules, which affects both membrane permeability and solubility. This dataset provides experimental results of octanol/water distribution coefficient The Free Solvation Database (FreeSolv) provides(logD at pH 7.4). ", "page_idx": 15}, {"type": "text", "text": "Data Split. In our experiment, we follow the previous work GEM [24] and Uni-Mol [76] and adopt the scaffold splitting to divide different datasets into training, validation, and test sets with a ratio of $80\\%$ , $10\\%$ , and $10\\%$ . It has been widely acknowledged that scaffold splitting is more challenging than random splitting because the scaffold sets of molecules in different subsets do not intersect [76]. This splitting tests the model\u2019s generalization ability and reflects the realistic cases [43], and [76] find that whether or not chirality is considered when generating the scaffold using RDKit has a significant impact on the division results. The performance of different methods in Table 1 all follows the same scaffold splitting, where MolCLR is reproduced. In all experiments, we choose the checkpoint with the best validation loss and document the results on the test set run by that checkpoint. ", "page_idx": 15}, {"type": "text", "text": "Implementation Details. In our experiments for molecular property prediction, we utilize 4 A100 GPUs and an Adam Optimizer [84] with a weight decay of 1e-16 for all GNN models, i.e., GCN, GAT, and GIN. A ReduceLROnPlateau scheduler is employed to automatically adjust the learning rate with a patience of 10 epochs. Before the SSL stage, we first pretrain the target molecular model via supervised learning for 100 epochs and then pretrain the instructor model for 50 epochs, where an early stopping mechanism is utilized with a patience of 5 epochs. Similar to GEM [24], we normalize the property label by subtracting the mean and dividing the standard deviation of the training set. As for the performance of baselines, we copy all available results from GEM [24], Uni-Mol [76] and 3D-Infomax [75]. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "As for the reproduction of three SSL methods, Semi-GAN is modified from https://github.com/ opetrova/SemiSupervisedPytorchGAN. $\\pi$ -model is transformed from a simple Tensorflow-based version at https://github.com/geosada/PI. UPS is directly modified from its official GitHub at https://github.com/nayeemrizve/ups. ", "page_idx": 16}, {"type": "text", "text": "Hyperparameter Search Space. Referring to prior studies, we adopt a grid search to find the best combination of hyperparameters for the molecular property prediction task. To reduce the time cost, we set a smaller search space for the large datasets (e.g., ToxCast). We report the details of the hyperparameter setup of InstructMol in Table 4. ", "page_idx": 16}, {"type": "table", "img_path": "j7sw0nXLjZ/tmp/aa8250a9bbd5b2b2b54af9f29d905a51f02f18acfe4941831c9c796bd0cc2114.jpg", "table_caption": ["Table 4: Hyperparameters setup for InstructMol in molecular property prediction. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.3 LogP Value Prediction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The Kaggle dataset for LogP prediction is downloaded from https://www.kaggle.com/ datasets/matthewmasters/chemical-structure-and-logp?resource $=$ download. ", "page_idx": 16}, {"type": "text", "text": "Appendix B Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Performance for Random Scaffold Splitting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We additionally execute experiments using the random scaffold splitting on the classification datasets following the same experimental setting used in GROVE [74], which is much easier than the standard scaffold splitting used in our main text. As shown in Table 5, our findings notice that GEM blended with InstructMol also achieves stronger results than all baselines. The baseline results are copied from GROVE and GEM. ", "page_idx": 16}, {"type": "text", "text": "Appendix C Comparison among Proxy-labeling Algorithms ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we provide a clear and simple comparison between InstructMol and existing proxy-labeling approaches in Table 6. ", "page_idx": 16}, {"type": "table", "img_path": "j7sw0nXLjZ/tmp/180e961ea5a3ed501ec0568946b10626fc503c4f42a2aba75a494c59f807eeeb.jpg", "table_caption": ["Table 5: Comparison of performance on the molecular property prediction tasks, where a random scaffold splitting is adopted. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "j7sw0nXLjZ/tmp/a2bcba03ea4d676360d4a4f65b143b85c78b7076319a64e8628af406e3ef338c.jpg", "table_caption": ["Table 6: Comparison over different literature under the proxy-labeling framework. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.1 Discovery of Drug-like Molecules ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "It should be noted that these 9 new small molecules are highly similar in structures, but IntructMol successfully and confidently differentiates their bioactivity based on their geometries. In addition to that, most experienced medicinal chemists also believe that the two sets of data within 10 times are manipulable and can be recognized as accurate prediction results. The error range of InstructMol\u2019s results for the prediction of small molecule properties of new compounds further proves the potential of our model in predicting essential properties of activity cliffs molecular and can effectively get aware of subtle changes in the presence of bioactivity cliffs. ", "page_idx": 17}, {"type": "image", "img_path": "j7sw0nXLjZ/tmp/75874d33733c91c94c368f36ac425fae03e9ef263fb6d8bb5ae61f732269b5fb.jpg", "img_caption": ["Figure 7: The $K_{i}$ value prediction results of the 9 newly discovered small molecules of 5-HT1A receptor. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The abstract and introduction have elaborated that our study is located in the molecular representation learning field. The main contribution of our paper is to propose an instructive learning technique to leverage the massive unlabeled molecular data, especially when the labeled points are scarce. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In the Appendix, we claim the potential limitation of our work. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: At the beginning of the Appendix, we have stated clearly the experimental setting, including the datasets used in the main text, the split strategy, and the training details. We also elaborated how we tuned the hyperparameters to achieve the optimal performance. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Notably, all data used in the paper are public datasets, which are easily accessibly from the Internet. Regarding the code for reproducing the results, we are very pleased to release it once our paper is accepted by the conference. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: At the beginning of the Appendix, we have stated clearly the experimental setting, including the datasets used in the main text, the split strategy, and the training details. We also elaborated on how we tuned the hyperparameters to achieve the optimal performance. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In the experiments, we have repeated the trials multiple times and reported the mean and standard deviations at all tables and relevant figures. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: As mentioned in the Appendix, all experiments were implemented on 4 A100 GPUs with a memory of 80G. Notably, it took us around one week to complete the training process. ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have strictly adhered to the NeurIPS Code of Ethics. We promise that we did not reveal any kind of non-anonymous information in the submission. ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have described clearly which version of data that we used for experiments and cited them correctly. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether IRB approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 20}]