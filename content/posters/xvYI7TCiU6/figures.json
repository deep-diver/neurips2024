[{"figure_path": "xvYI7TCiU6/figures/figures_3_1.jpg", "caption": "Figure 1: A three-agent example of traditional sequential updating MARL and our MADPO. The white boxes represent the policies to be updated \u03c0\u00b2, and the orange boxes represent the updated policies \u03c0\u00b2. The white boxes with dashed lines represent the joint policies to be updated \u3160, and the orange ones represent the updated joint policy \u4e93. Compared to the traditional sequential updating MARL, our method takes the intra-agent and inter-agent divergence into account, as shown in the blue boxes. The intra-agent divergence directs agents to explore novel policies based on their former policies, while the inter-agent divergence heterogenizes agents sequentially.", "description": "This figure compares the traditional sequential updating method in multi-agent reinforcement learning (MARL) with the proposed method MADPO.  The traditional method shows a sequential update of agents' individual policies (\u03c0), while the MADPO method incorporates intra-agent and inter-agent policy divergence to enhance exploration and heterogeneity.  The intra-agent divergence encourages an agent to explore new policies based on its previous ones, while the inter-agent divergence ensures that agents learn diverse policies from each other, increasing the heterogeneity of the agents' policies.", "section": "3 Preliminaries"}, {"figure_path": "xvYI7TCiU6/figures/figures_3_2.jpg", "caption": "Figure 1: A three-agent example of traditional sequential updating MARL and our MADPO. The white boxes represent the policies to be updated \u03c0\u00b2, and the orange boxes represent the updated policies \u03c0\u00b2. The white boxes with dashed lines represent the joint policies to be updated \u3160, and the orange ones represent the updated joint policy \u4e93. Compared to the traditional sequential updating MARL, our method takes the intra-agent and inter-agent divergence into account, as shown in the blue boxes. The intra-agent divergence directs agents to explore novel policies based on their former policies, while the inter-agent divergence heterogenizes agents sequentially.", "description": "This figure illustrates the difference between traditional sequential updating MARL and the proposed MADPO method.  The left side shows a standard sequential updating approach where agents update their policies one by one.  The right side depicts MADPO, highlighting the incorporation of intra-agent divergence (measuring policy differences within an agent's episodes) and inter-agent divergence (measuring policy differences between agents) to enhance exploration and heterogeneity.  The colored boxes represent policies and their updates, emphasizing the flow of information and the differences between the two approaches.", "section": "3 Preliminaries"}, {"figure_path": "xvYI7TCiU6/figures/figures_7_1.jpg", "caption": "Figure 2: Performance comparison against baseline methods on Multi-Agent Mujoco. Benefiting from the heterogeneity and exploration enhanced by mutual policy divergence maximization, the proposed MADPO consistently outperforms all baselines.", "description": "This figure compares the performance of MADPO against other baseline methods (A2PO, HAPPO, HATRPO, MAPPO) on various Multi-Agent Mujoco tasks.  The results show that MADPO consistently achieves higher episode rewards across all tasks, demonstrating the effectiveness of its mutual policy divergence maximization strategy in promoting both heterogeneity and exploration. The shaded areas represent 95% confidence intervals.", "section": "5 Experiments"}, {"figure_path": "xvYI7TCiU6/figures/figures_8_1.jpg", "caption": "Figure 1: A three-agent example of traditional sequential updating MARL and our MADPO. The white boxes represent the policies to be updated \u03c0\u00b2, and the orange boxes represent the updated policies \u03c0\u00b2. The white boxes with dashed lines represent the joint policies to be updated \u3160, and the orange ones represent the updated joint policy \u4e93. Compared to the traditional sequential updating MARL, our method takes the intra-agent and inter-agent divergence into account, as shown in the blue boxes. The intra-agent divergence directs agents to explore novel policies based on their former policies, while the inter-agent divergence heterogenizes agents sequentially.", "description": "This figure compares the traditional sequential updating MARL method with the proposed MADPO method.  It illustrates how MADPO incorporates intra-agent (policy divergence between episodes for a single agent) and inter-agent (policy divergence between agents) divergence to enhance exploration and heterogeneity. The diagram uses boxes to visually represent policies and their updates, highlighting the differences in the approaches.", "section": "3 Preliminaries"}, {"figure_path": "xvYI7TCiU6/figures/figures_8_2.jpg", "caption": "Figure 4: IQM performance comparison against baseline methods on 10 tasks of Bi-DexHands and 9 tasks of MA-mujoco.", "description": "This figure presents the results of the Interquartile Mean (IQM) performance comparison across multiple tasks between MADPO and other baseline methods. The IQM is used to measure the sample efficiency, which is the mean of the middle 50% of runs to reduce the bias of outliers.  The figure displays the IQM of episodic rewards for 10 tasks from the Bi-DexHands environment and 9 tasks from the MA-Mujoco environment.  Each line represents a different algorithm, showing the average reward over time, and the shaded area indicates the 95% confidence interval.  The results demonstrate that MADPO achieves higher average rewards across tasks and has higher sample efficiency than state-of-the-art methods.", "section": "5.1 Results on MA-Mujoco and Bi-DexHands"}, {"figure_path": "xvYI7TCiU6/figures/figures_9_1.jpg", "caption": "Figure 5: Performance comparison against other exploration incentives.", "description": "This figure compares the performance of different exploration methods including entropy, KL-divergence, no incentive, and CS-divergence.  Subfigure (a) shows the learning curves for three different multi-agent tasks: HalfCheetah-v2-6x1, ShadowHandDoorOpenInward, and Walker2d-v2-3x2.  Subfigure (b) displays an aggregate IQM comparison of episode rewards across multiple tasks (10 in Dexhands and 10 in Mamujoco). The results demonstrate that the proposed conditional Cauchy-Schwarz (CS) divergence significantly outperforms other methods, providing more stable and effective exploration in multi-agent reinforcement learning.", "section": "5.2 Ablation Study"}, {"figure_path": "xvYI7TCiU6/figures/figures_9_2.jpg", "caption": "Figure 6: Parameter sensitivity studies for MADPO.", "description": "This figure presents the results of parameter sensitivity studies conducted on the MADPO algorithm. It shows how the performance of MADPO varies across different tasks (ShadowHandDoorOpenInward, Walker2d-v2-6x1, and ShadowHandDoorOpenInward) when adjusting the parameters \u03bb and \u03c3. The parameter \u03bb controls the influence of inter-agent and intra-agent policy divergence, while \u03c3 influences the conditional Cauchy-Schwarz policy divergence. The results reveal MADPO's performance is slightly sensitive to \u03c3 and that its performance is significantly better than HAPPO for various settings of \u03bb and \u03c3.", "section": "5 Experiments"}, {"figure_path": "xvYI7TCiU6/figures/figures_15_1.jpg", "caption": "Figure 2: Performance comparison against baseline methods on Multi-Agent Mujoco. Benefiting from the heterogeneity and exploration enhanced by mutual policy divergence maximization, the proposed MADPO consistently outperforms all baselines.", "description": "This figure compares the performance of MADPO against other state-of-the-art multi-agent reinforcement learning algorithms on five different tasks within the Multi-Agent MuJoCo environment. The results are presented as episode rewards plotted over training steps, and error bars show the 95% confidence interval. The results indicate that MADPO consistently outperforms other algorithms, illustrating the benefits of its mutual policy divergence maximization approach in enhancing exploration and heterogeneity, particularly in complex, multi-agent scenarios.", "section": "5 Experiments"}, {"figure_path": "xvYI7TCiU6/figures/figures_16_1.jpg", "caption": "Figure 1: A three-agent example of traditional sequential updating MARL and our MADPO. The white boxes represent the policies to be updated \u03c0\u00b2, and the orange boxes represent the updated policies \u03c0\u00b2. The white boxes with dashed lines represent the joint policies to be updated \u3160, and the orange ones represent the updated joint policy \u4e93. Compared to the traditional sequential updating MARL, our method takes the intra-agent and inter-agent divergence into account, as shown in the blue boxes. The intra-agent divergence directs agents to explore novel policies based on their former policies, while the inter-agent divergence heterogenizes agents sequentially.", "description": "This figure compares the traditional sequential updating MARL approach with the proposed MADPO method. It illustrates how MADPO incorporates intra-agent and inter-agent policy divergence to enhance exploration and heterogeneity, unlike traditional methods which only update policies sequentially without considering divergence.  The diagram visually represents the policy updates in both methods, showing the differences in how policy information is utilized.", "section": "3 Preliminaries"}, {"figure_path": "xvYI7TCiU6/figures/figures_16_2.jpg", "caption": "Figure 7: Performance comparison on against baseline methods on 10 Multi-Agent Mujoco scenarios.", "description": "This figure compares the performance of MADPO against other baseline methods (A2PO, HAPPO, HATRPO, MAPPO) across 10 different Multi-Agent Mujoco scenarios.  The x-axis represents training steps, and the y-axis represents episode reward.  The figure shows MADPO consistently outperforms other methods in most scenarios, highlighting its effectiveness in complex, multi-agent environments. Different colored lines represent different methods, and shaded areas indicate the 95% confidence interval.", "section": "5 Experiments"}, {"figure_path": "xvYI7TCiU6/figures/figures_16_3.jpg", "caption": "Figure 10: Aggregate parameter sensitivity study.", "description": "This figure shows the results of a parameter sensitivity study for MADPO, assessing the impact of parameters \u03bb (influencing the balance between inter-agent and intra-agent policy divergence) and \u03c3 (related to the conditional Cauchy-Schwarz divergence estimator).  The plots display the IQM (Interquartile Mean) episode reward across multiple tasks in both MA-Mujoco and Bi-DexHands environments for different values of \u03bb and \u03c3. The results show that MADPO's performance is moderately sensitive to these parameters, highlighting the importance of careful tuning for optimal results.  Comparing MADPO's performance against HAPPO provides a baseline for evaluating the benefits of MADPO's approach.", "section": "5 Experiments"}]