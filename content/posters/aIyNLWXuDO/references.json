{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is foundational to the current work."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduced the concept of few-shot learning in large language models which is relevant to this paper's exploration of solving arithmetic problems."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-12-01", "reason": "This paper explored the limits of transfer learning in large language models, providing a benchmark for the work on arithmetic tasks."}, {"fullname_first_author": "Hattie Zhou", "paper_title": "What algorithms can transformers learn? a study in length generalization", "publication_date": "2023-10-26", "reason": "This paper is directly related to the research questions and methods of the current paper, providing a direct comparison for the achieved results."}, {"fullname_first_author": "Yongchao Zhou", "paper_title": "Transformers can achieve length generalization but not robustly", "publication_date": "2024-02-15", "reason": "This paper provides a direct comparison and analysis of a very similar study that helps contextualize the improvements of the presented model."}]}