[{"type": "text", "text": "How Does Black-Box Impact the Learning Guarantee of Stochastic Compositional Optimization? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jun Chen College of Informatics, Huazhong Agricultural University, China cj850487243@163.com ", "page_idx": 0}, {"type": "text", "text": "Hong Chen\\* College of Informatics, Huazhong Agricultural University, China Engineering Research Center of Intelligent Technology for Agriculture, China chenh@mail.hzau.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Bin Gu   \nSchool of Artificial Intelligence, Jilin University, China   \nMohamed bin Zayed University of Artificial Intelligence jsgubin@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic compositional optimization (SCO) problem constitutes a class of optimization problems characterized by the objective function with a compositional form, including the tasks with known derivatives, such as AUC maximization, and the derivative-free tasks exemplified by black-box vertical federated learning (VFL). From the learning theory perspective, the learning guarantees of SCO algorithms with known derivatives have been studied in the literature. However, the potential impacts of the derivative-free setting on the learning guarantees of SCO remains unclear and merits further investigation. This paper aims to reveal the impacts by developing a theoretical analysis for two derivative-free algorithms, black-box SCGD and SCSC. Specifically, we first provide the sharper generalization upper bounds of convex SCGD and SCSC based on a new stability analysis framework more effective than prior work under some milder conditions, which is further developed to the non-convex case using the almost co-coercivity property of smooth function. Then, we derive the learning guarantees of three black-box variants of non-convex SCGD and SCSC with additional optimization analysis. Comparing these results, we theoretically uncover the impacts that a better gradient estimation brings a tighter learning guarantee and a larger proportion of unknown gradients may lead to a stronger dependence on the gradient estimation quality. Finally, our analysis is applied to two SCO algorithms, FOO-based vertical VFL and VFL-CZOFO, to build the first learning guarantees for VFL that align with the findings of SCGD and SCSC. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, stochastic compositional optimization (SCO), a class of optimization methods that incorporate the compositional form $f(g(w))$ , has garnered significant attention in the research ", "page_idx": 0}, {"type": "text", "text": "community [1, 2, 3, 4, 5, 6, 7]. It represents a special case of stochastic bilevel optimization [8] ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\boldsymbol{w}\\in\\mathcal{W}\\in\\mathbb{R}^{p}}\\mathbb{E}_{\\bar{z}}\\left[f_{\\bar{z}}\\left(\\boldsymbol{w},\\boldsymbol{v}^{*}(\\boldsymbol{w})\\right)\\right]\\quad\\mathrm{s.t.}\\quad\\boldsymbol{v}^{*}(\\boldsymbol{w})=\\arg\\operatorname*{min}_{\\boldsymbol{v}\\in\\mathbb{R}^{d}}\\mathbb{E}_{z}\\left[h_{z}(\\boldsymbol{w},\\boldsymbol{v})\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbb{E}_{z}[\\cdot]$ represents the expectation with respect to (w.r.t.) the sample $z$ , parameters $w\\in\\mathcal{W}\\in\\mathbb{R}^{p}$ and $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ . If the inner function $h_{z}(w,v)\\,=\\,\\|v-g_{z}(w)\\|^{2}$ and the outer function $f$ is only a function of $v$ , i.e., $f_{\\bar{z}}(w,v)=f_{\\bar{z}}(v)$ , the stochastic bilevel optimization reduces to the SCO: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathcal{W}\\in\\mathbb{R}^{p}}\\left\\{F(w)=f(g(w))=\\mathbb{E}_{\\bar{z}}\\left[f_{\\bar{z}}(\\mathbb{E}_{z}\\left[g_{z}(w)\\right])\\right]\\right\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Where $F(w)$ is the compositional population risk, $f:\\mathbb{R}^{d}\\to\\mathbb{R},g:\\mathbb{R}^{p}\\to\\mathbb{R}^{d},\\,f(v)=\\mathbb{E}_{\\bar{z}}\\left[f_{\\bar{z}}(v)\\right]$ and $g(w)=\\mathbb{E}_{z}\\left[g_{z}(w)\\right]$ ", "page_idx": 1}, {"type": "text", "text": "Many applications adhere to the form of SCO (2) such as risk averse optimization [9], group distributionally robust optimization [10], AUC maximization [11, 12, 13, 14], model-agnostic metalearning [15], and first-order-optimization-based vertical federated learning (FOO-based VFL) [16]. Apart from the above applications with available derivatives, there exist derivative-free scenarios as well, such as reinforcement learning [17] and zeroth-order-optimization-based (ZOO-based) VFL [18, 19]. [2] discussed the extension of stochastic compositional gradient descent (SCGD) to the derivative-free setting, called black-box SCGD, where the zeroth-order information of $f$ Or $g$ is available through sampling. ", "page_idx": 1}, {"type": "text", "text": "From the perspective of statistical learning theory [20], the theoretical guarantees pertaining to generalization and optimization performance for SCO algorithms are worth studying to validate their empirical behaviors. The former assesses the disparity between the empirical performance and the population performance for the trained model. The latter measures the empirical performance gap between the trained model and the empirical optimal model. To our knowledge, there is only one study attempting to provide a generalization guarantee in this area. [21] has pioneered the generalization understanding of two notable SCO algorithms, i.e., SCGD and SCSC [5] via algorithmic stability tool. They have achieved satisfactory excess risk bounds by selecting some specific values of $T$ to balance stability results and optimization errors. However, the intricacy of their analysis framework brings some unnecessary terms in their results leading to these so large $T$ values that it is practically challenging to complete these iterations within a reasonable timeframe. Furthermore, the study has yet to consider a more practical scenario beyond convex and strongly convex cases, specifically, the non-convex case. For the optimization guarantee, plenty of work devotes to studying the convergence behaviors of some first-order SCO algorithms [2, 3, 4, 5] and their extensions [22, 23, 24, 25]. For example, [2] proved that SCGD can converge almost surely to an existing optimal solution with the rate ${\\mathcal{O}}\\left(T^{-1/4}\\right)$ for non-smooth convex problems and the rate ${\\mathcal{O}}\\left(T^{-2/7}\\right)$ for smooth convex problems, where $\\acute{T}$ is the total number of iterations. [5] presented that stochastically corrected stochastic compositional gradient method (SCSC) can achieve the same convergence rate ${\\mathcal{O}}\\left(T^{-1/2}\\right)$ as SGD for non-compositional problems. However, there exists a research gap in the optimization analysis for the derivative-free SCO algorithm as well as in its generalization analysis. ", "page_idx": 1}, {"type": "text", "text": "Considering these problems, this paper leverages algorithmic stability to obtain some similar and even superior results of SCGD and SCSC under the milder parameter selection and the non-convex condition. More importantly, to apply a broader class of stochastic optimization problems, this paper pioneers the theoretical analysis of the black-box SCO algorithms, which uncovers the impacts of black-box on the learning guarantees of SCO algorithms. Our main contributions are listed as follows. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Generalization guarantees under some milder settings. Firstly, we provide the sharper generalization upper bounds of convex SCGD and SCSC based on a new stability analysis framework more effective than prior work [21] with a more practical selection of $T$ .Subsequently, we develop the above convex analysis to the non-convex case by introducing the almost co-coercivity property of smooth function, which yields satisfactory generalization guarantees of SCGD and SCSC under the non-convex condition. \u00b7 Learning guarantees for black-box SCO algorithms. To apply a broader class of stochastic optimization problems, we further consider three black-box variants (outer, inner, and full black-box) of SCGD and SCSC to obtain the generalization and optimization upper bounds similar to the ones of SCGD and SCSC. Comparing the first-order and zeroth-order results, several key insights into the impacts of black-box on the learning guarantees of SCO algorithms are shown: 1) a closer estimation distance brings a better result; 2) more estimation directions lead to a better result; 3) a larger proportion of unknown gradients results in a stronger dependence on the gradient estimation quality. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u00b7 Applications on VFL. Finally, we explore the applications of our analysis framework to two specific SCO algorithms, i.e., FOO-based VFL and VFL-CZOFO, where we build the pioneering stability-based generalization and optimization guarantees for first-order and zeroth-order VFL algorithms that align with the findings of SCGD and SCSC. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section describes the learning paradigm of SCO and introduces two popular SCO algorithms (SCGD and SCSC) and their black-box variants in detail at first. Then, some necessary definitions and assumptions are provided for our theoretical analysis. The explanations for all symbols are shown in Table 3 located in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Considering a stochastic compositional optimization algorithm (2), the distributions of sample $z$ and $\\bar{z}$ are unknown. The training dataset $S=\\bar{S}^{z}\\cup S^{\\bar{z}}=\\{\\bar{z_{1}},...,z_{n}\\}\\cup\\{\\bar{z}_{1},...,\\bar{z}_{m}\\}$ is available to obtain the final output model parameter $A(S)$ via minimizing the following compositional empirical risk ", "page_idx": 2}, {"type": "equation", "text": "$$\nF_{S}(w)=f_{S}(g_{S}(w))=\\frac{1}{m}\\sum_{j=1}^{m}f_{\\bar{z}_{j}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}g_{z_{i}}(w)\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{g_{S}(w)\\,=\\,\\frac{1}{n}\\sum_{i=1}^{n}g_{z_{i}}(w),\\;f_{S}(v)\\,=\\,\\frac{1}{m}\\sum_{j=1}^{m}f_{\\bar{z}_{j}}(v),\\;z_{1},...,z_{n},\\bar{z}_{1},...,\\bar{z}_{m}}\\end{array}$ are independent. Besides, we denote by $w(S),w^{*}$ the empirical optimal model parameter on $S$ and the global optimal model parameter, defined as $w(S)\\overset{\\bullet}{=}\\arg\\operatorname*{min}_{w\\in\\mathcal{W}}F_{S}(w)$ and $w^{*}=\\arg\\operatorname*{min}_{w\\in\\mathcal{W}}F(w)$ . Then, the generalization error, optimization error and excess risk of $A(S)$ are given by $|F(A(S)-F_{S}(A(S))|,$ ${\\dot{F}}_{S}(A(S))\\,-\\,F_{S}(w({\\bar{S}}))$ and $F(A(S))\\,-\\,F(w^{*})$ , respectively. Since $\\mathbb{E}[F(A(S))\\,-\\,F(w^{*})]\\,\\geq\\,0$ and $\\mathbb{E}[F_{S}(w(S))-F(w^{*})]\\leq0$ , the excess risk of $A(S)$ can be decomposed as the summation of generalization error and optimization error as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[F(A(S))-F(w^{*})]\\leq\\mathbb{E}[|F(A(S))-F_{S}(A(S))|]+\\mathbb{E}[F_{S}(A(S))-F_{S}(w(S))],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{E}[\\cdot]$ denotes the expectation w.r.t. all randomness. ", "page_idx": 2}, {"type": "text", "text": "In this work, we primarily investigate the learning guarantees of two prevalent SCO algorithms, i.e., SCGD [2] and SCSC [5], along with their black-box variants. Algorithm 1 presents the detailed parameter update procedures of these algorithms. The difference between SCGD and SCSC lies in the update of the outer model parameter $v_{t}$ . For SCGD, $v_{t+1}$ is the linear combination of $v_{t}$ and $g_{z_{i_{t}}}(w_{t})$ However, this update may lead to a suboptimal convergence rate when the learning rate $\\beta$ of the outer model update is smaller than the learning rate $\\eta_{t}$ utilized for the inner model update. To alleviate this problem, SCSC updates $v_{t+1}$ with the combination of the \"corrected\" $v_{t}$ and $g_{z_{i_{t}}}(w_{t})$ , where $v_{t}$ is corrected by $g_{z_{i_{t}}}(w_{t})-g_{z_{i_{t}}}(w_{t-1})$ so that $v_{t+1}$ approximates $g_{z_{i_{t}}}(w_{t})$ [5]. Besides, [2] discussed the extension of SCGD to the derivative-free setting where only the zeroth-order information of $g$ or $f$ is available through sampling, which potentially applies to a broader class of stochastic optimization problems. Here, we show the first-order gradient estimation of $f$ , which is similar to that of $g$ .The unknown first-order gradient of $f$ is estimated by Equation (4) and then approximated by Taylor expansion (5) in our analysis ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lefteqn{\\widetilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})=\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)}\\quad}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{1}{b}\\sum_{l=1}^{b}\\left(\\left\\langle\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}),u_{t,l}\\right\\rangle u_{t,l}+\\left(\\frac{\\mu}{2}(u_{t,l})^{\\top}\\nabla^{2}f_{\\bar{z}_{j_{t}}}(v)|v=v_{t+1}^{*}u_{t,l}\\right)u_{t,l}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\left\\{u_{t,l}\\right\\}_{l=1}^{b}$ is the set of independent and identically distributed (ii.d.) random direction vectors (obeying the $d$ -dimensional uniform distribution), and $\\mu$ is the distance between two model parameters $(v_{t+1}+\\mu u_{t,l}$ and $v_{t+1}$ ) used to estimate the gradient in the $l$ -th direction. ", "page_idx": 2}, {"type": "text", "text": "Drawing inspiration from the classical non-compositional stability analysis work [26] and the pioneering work [21] investigating the generalization of SCO, we introduce the definition of uniform model stability as follows. ", "page_idx": 2}, {"type": "text", "text": "require: $v_{1},w_{1}$ : initial outer model and inner models; $\\beta,\\eta_{1}$ : initial learning rates   \nfor all $t=1,...,T-1$ do Randomly sample $i_{t}\\in[n]$ , obtain $g_{z_{i_{t}}}(w_{t})$ and $\\nabla g_{z_{i_{t}}}(w_{t})$ (Inner black-box: obtain $\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})$ similar to Equation (4)) SCGD: Update $v_{t+1}=(1-\\beta)v_{t}+\\beta g_{z_{i_{t}}}(w_{t})$ SCSC: Update $v_{t+1}=(1-\\beta)v_{t}+\\beta g_{z_{i_{t}}}(w_{t})+(1-\\beta)(g_{z_{i_{t}}}(w_{t})-g_{z_{i_{t}}}(w_{t-1}))$ Randomly sample $j_{t}\\in[m]$ , obtain $\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})$ (Outer black-box: obtain $\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1}))$ Update $w_{t+1}=w_{t}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})/w_{t+1}=w_{t}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})$ $\\begin{array}{r}{/w_{t+1}=w_{t}-\\eta_{t}\\dot{\\nabla}g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})/w_{t+1}=w_{t}-\\eta_{t}\\dot{\\nabla}g_{z_{i_{t}}}(w_{t})\\dot{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})}\\end{array}$   \nend for ", "page_idx": 3}, {"type": "text", "text": "Definition 1. The randomized algorithm $A$ for SCO problem is uniformly model $(\\epsilon_{z},\\epsilon_{\\bar{z}})$ -stableif ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{A}\\left[\\left\\lVert A(S)-A(S^{i,z})\\right\\rVert\\right]\\leq\\epsilon_{z}\\ \\mathrm{and}\\ \\mathbb{E}_{A}\\left[\\left\\lVert A(S)-A(S^{j,\\bar{z}})\\right\\rVert\\right]\\leq\\epsilon_{\\bar{z}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\parallel\\cdot\\parallel$ is the Euclidean distance ${\\|\\cdot\\|}_{2}$ and $\\begin{array}{l c l}{{S}}&{{=}}&{{\\{z_{1},...,z_{n},\\bar{z}_{1},...,\\bar{z}_{m}\\}\\,,S^{i,z}}}\\end{array}\\,\\,=$ $\\left\\{z_{1},...,z_{i-1},z_{i}^{\\prime},z_{i+1},...,z_{n},{\\bar{z}}_{1},...,{\\bar{z}}_{m}\\right\\},S^{j,{\\bar{z}}}\\,=\\,\\left\\{z_{1},...,z_{n},{\\bar{z}}_{1},...,{\\bar{z}}_{j-1},{\\bar{z}}_{j}^{\\prime},{\\bar{z}}_{j+1},...,{\\bar{z}}_{m}\\right\\}$ for any $i\\in[n],j\\in[m]$ ", "page_idx": 3}, {"type": "text", "text": "According to the foundational concept of algorithmic stability, Definition 1 considers the two datasets obtained from the perturbation of a single sample in $\\{z_{i}\\}_{i+1}^{i}$ and $\\{\\bar{z}_{j}\\}_{j=1}^{m}$ respectively, where the altered sample $z_{i}^{\\prime}$ is i.i.d. to $z_{i}$ , and so does $\\bar{z}_{j}^{\\prime}$ . Prior to filling the relationship gap between the uniformly model stability and the generalization error $\\mathbb{E}\\left[|F(A(S))-F_{S}(A(S))|\\right]$ , it is essential to make some fundamental assumptions, i.e., Lipschitz continuity (bounded first-order gradient) of $g,f$ and bounded variance of $g$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. For any parameters $w,w^{\\prime}\\in\\mathcal{W},v,v^{\\prime}\\in\\mathbb{R}^{d}$ and some $L_{g},L_{f}>0,$ functions $g_{z}(w)$ and $f_{\\bar{z}}(v)$ are Lipschitz. continuous, i.e., $\\|\\nabla g_{z}(w)\\|\\leq L_{g}$ and $\\|\\nabla f_{\\bar{z}}(v)\\|\\leq L_{f}$ ,which alsomean that $\\|g_{z}(w)-g_{z}(w^{\\prime})\\|\\leq L_{g}\\left\\|w-w^{\\prime}\\right\\|$ and $|f_{\\bar{z}}(v)-f_{\\bar{z}}(\\bar{v}^{\\prime})|\\leq L_{f}\\left\\|v-v^{\\prime}\\right\\|$ ", "page_idx": 3}, {"type": "text", "text": "In numerous compositional [2, 5, 6, 7] and non-compositional studies [26, 27], Assumption 1 serves as a general theoretical bridge analyzing the generalization and optimization performance. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. For any $w\\in\\mathcal{W}$ and some $V_{g}>0$ , the variance of function $g_{z}(w)$ is upper bounded $V_{g},$ i.e, $\\mathbb{E}_{z}\\left[\\left\\|g_{z}(w)-g(w)\\right\\|^{2}\\right]\\le V_{g}$ ", "page_idx": 3}, {"type": "text", "text": "The bounded variance is also a classical condition for statistical learning theory [1, 2, 3, 5, 6, 7, 28, 29, 30] which limits the ranges of the variance value of the given functions $g$ .Utilizing these two fundamental assumptions, Theorem 1 builds a rigorous relationship between stability and generalization error, thereby enabling stability to measure the generalization performance in the subsequent analysis. Note that, Theorem 1 was previously proved by [21] (Theorem 2.3), so we omit its detailed proof here for brevity. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. [21] Let Assumptions 1, 2 hold. Assume the randomized algorithms $A$ forSCoproblem isuniformlymodel $(\\epsilon_{z},\\epsilon_{\\bar{z}})$ -stable,then, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[|F(A(S))-F_{S}(A(S))|]\\leq L_{g}L_{f}(4\\epsilon_{z}+\\epsilon_{\\bar{z}})+L_{f}\\sqrt{n^{-1}V_{g}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark 1. As mentioned in [21], Theorem $^{\\,l}$ is the compositional counterpart of Theorem $2.2~i n$ $[26]$ .In other words, the above result is equivalent to $\\bar{\\mathbb{E}}[|F(A(S))-F_{S}(A(S)\\dot{)}|]\\,\\le\\,L_{f}\\epsilon_{\\bar{z}}$ when $g_{z}(w)=w_{z}$ i.e, $F(w)=\\mathbb{E}_{\\bar{z}}\\left[f_{\\bar{z}}(w)\\right]$ and $\\begin{array}{r}{F_{S}(w)=\\frac{1}{m}\\sum_{j=1}^{m}f_{\\bar{z}_{j}}(w)}\\end{array}$ Ifthe order of $\\epsilon_{\\bar{z}}$ is faster than $O\\left(\\epsilon_{z}+n^{-\\frac{1}{2}}\\right)$ , the generalization upper bound of SCO algorithm willbe primarily constrained by the term $4L_{g}L_{f}\\epsilon_{z}+L_{f}\\sqrt{n^{-1}V_{g}}$ atributed to the compositional structure. Otherwise, there is little differencebetweenTheorem $^{\\,I}$ andTheorem $2.2\\,\\,[26]$ ", "page_idx": 3}, {"type": "text", "text": "The subsequent assumptions and definition are required by the stability analysis in Section 3. ", "page_idx": 3}, {"type": "text", "text": "Assumption3.For anyparameters $w,w^{\\prime}\\in\\mathcal{W},v,v^{\\prime}\\in\\mathbb{R}^{d}$ and some $\\alpha_{g},\\alpha_{f},\\alpha>0,$ functions $g_{z}(w)$ $f_{\\bar{z}}(v)$ and $f_{\\bar{z}}(g_{z}(w))$ are smooth, i.e., $\\|\\nabla^{2}g_{z}(w)\\|\\leq\\alpha_{g}$ \uff0c $\\|\\nabla^{2}f_{\\bar{z}}(v)\\|\\leq\\alpha_{f}$ and $\\begin{array}{r}{\\|\\nabla^{2}f_{\\bar{z}}(g_{z}(w))\\|\\leq}\\end{array}$ $\\alpha$ whichalsomeanthat ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla g_{z}(\\boldsymbol{w})-\\nabla g_{z}(\\boldsymbol{w^{\\prime}})\\|\\leq\\alpha_{g}\\left\\|\\boldsymbol{w}-\\boldsymbol{w^{\\prime}}\\right\\|,\\ \\ \\|\\nabla f_{\\bar{z}}(\\boldsymbol{v})-\\nabla f_{\\bar{z}}(\\boldsymbol{v^{\\prime}})\\|\\leq\\alpha_{f}\\left\\|\\boldsymbol{v}-\\boldsymbol{v^{\\prime}}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f_{\\bar{z}}(g_{z}(w))-\\nabla f_{\\bar{z}}(g_{z}(w^{\\prime}))\\|\\leq\\alpha\\,\\|w-w^{\\prime}\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Definition 2. For any parameter $v,v^{\\prime}\\in\\mathbb R^{d}$ $a$ function $f:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is convex if $f(v)\\geq f(v^{\\prime})+$ $\\langle\\nabla f(v^{\\prime}),v-v^{\\prime}\\rangle$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 4. For any $w~\\in~{\\mathcal W},v~\\in~\\mathbb R^{d}$ \uff1adirection vector $u_{i}$ step size $\\mu~>~0$ and some $M_{g},M_{f},M_{g}^{\\prime},M_{f}^{\\prime}>0$ the following inequalities hold ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|g_{z}(w+\\mu u)-g_{z}(w)\\|\\leq M_{g},\\ \\ |f_{\\bar{z}}(v+\\mu u)-f_{\\bar{z}}(v)|\\leq M_{f},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\nabla g_{z}(w+\\mu u)-\\nabla g_{z}(w)\\|\\leq M_{g}^{\\prime},\\;\\;\\|\\nabla f_{\\bar{z}}(v+\\mu u)-\\nabla f_{\\bar{z}}(v)\\|\\leq M_{f}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 2. Assumption 3 is the most important condition for our analysis since there are several key properties (Lemma 4) of smoothness required to measure the algorithmic stability. In addition, our stability analysis framework relies crucially on another key lemma(called co-coercive lemma, Lemma 1) derived from the smoothness and convexity of the function $f(w)$ . Therefore, we provide the definition of convexity in Definition 2. Except for the convex case (Section 3.1), this work mainly considers some non-convex cases (Sections 3.1, 3.2). Although the co-coercive lemma is not available without the convexity condition, a surrogate lemma (called almost co-coercive lemma, Lemma 3) takes a similar rolewithin our analysisframework.Finally,Assumption $^{4}$ gives the upper bounds of the differencebetweentwo adjacent functionvalues for $g,f,\\nabla g,\\nabla f$ .which represents a less stringent assumption compared to thegeneral bounded condition $[26]$ Specifically, Assumption 4 is differentfromtheassumption $|f|\\leq M$ .Assumption $^{4}$ requiresthedistancebetweentwoadjacent function outputs to be bounded, i.e., $||g(w+\\mu u)-g(w)||\\leq M_{g},|f(v+\\mu u)-f(v)|\\leq M_{f}.$ which is milder than $|f|\\leq M$ Besides,it also requires the distance between two adjacent gradient outputs to be bounded, i.e., $||\\nabla g(w+\\mu u)-\\nabla g\\bar{(}w)||\\,\\leq\\,M_{g}^{\\prime},||\\nabla f(v+\\mu u)-\\nabla\\bar{f}(v)||\\,\\overset{\\cdot}{\\leq}\\,M_{f}^{\\prime}$ which is milder than bounded gradient condition $\\begin{array}{r}{\\vert\\vert\\nabla f\\vert\\vert\\leq L\\,[26]}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section presents the learning guarantees of two SCO algorithms (SCGD and SCSC) under several cases. The comparisons among our results and previous work are summarized in Tables 1, 2, and their proofs are provided in Appendices $C,D$ ", "page_idx": 4}, {"type": "text", "text": "3.1 Learning Guarantees for General SCO ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Firstly, we consider the generalization analysis for the general convex SCO algorithm. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let Assumptions 1, $^3$ hold and the function $f(g(w))$ is convex. Assume that the randomized algorithms $A$ (Algorithm $^{\\,l}$ )for SCO problem brings the model sequences $\\{w_{t}\\}_{t=1}^{T}$ and $\\left\\{w_{t}^{i,z}\\right\\}_{t=1}^{T}\\left(\\left\\{w_{t}^{j,\\bar{z}}\\right\\}_{t=1}^{T}\\right)$ ) on S and Si,z (Si=) with the step size sequence {nt}t=1- ", "page_idx": 4}, {"type": "text", "text": "(a) For SCGD with $\\begin{array}{r}{\\eta_{t}\\leq\\frac{2\\beta}{\\alpha t}}\\end{array}$ , the final output $A(S)=w_{T}$ is uniformly model $(\\epsilon_{z},\\epsilon_{\\bar{z}})$ -stable with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon_{z}=\\frac{4L_{g}L_{f}\\beta\\log(e T)}{\\alpha n}\\;\\;\\mathrm{and}\\;\\;\\epsilon_{\\bar{z}}=\\frac{4L_{g}L_{f}\\beta\\log(e T)}{\\alpha m}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(b) For SCSC with $\\begin{array}{r}{\\eta_{t}\\leq\\frac{2}{\\alpha t}}\\end{array}$ , the final output $A(S)=w_{T}$ is uniformly model $(\\epsilon_{z},\\epsilon_{\\bar{z}})$ -stable with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon_{z}=\\frac{4L_{g}L_{f}\\log(e T)}{\\alpha n}\\;\\;\\mathrm{and}\\;\\;\\epsilon_{\\bar{z}}=\\frac{4L_{g}L_{f}\\log(e T)}{\\alpha m}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "4AuEQ1FfUf/tmp/a7eb1a346383634f8c2386a1cd4402c8e0448f75ca367e8c05523ce1b9b65bb2.jpg", "table_caption": ["Table 1: Comparisons among the stability-based generalization guarantees for SCO algorithms and SGD (Thm.-Theorem; Cor.-Corollary; $^*$ -high probability bound; $L,\\alpha,V,M,$ C.-Lipschitz continuity, smoothness, bounded variance, bounded function, and convexity assumptions; $c$ -a positive constant; $\\smash{\\surd}$ -has such a property; $\\times$ -hasn't such a property). "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Remark 3. Based on a new stability analysis framework more effective than prior work $I2I J,$ Theorem 2 states the stability upper bounds $\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)^{\\ast}\\!\\beta\\log T\\right)$ for SCGD and $\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)\\log T\\right)$ for SCsC under the convex condition, which derives a generalization bound $\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)\\log T+n^{-\\frac{1}{2}}\\right)$ by combining with Theorem 1. Previously, [21l provided the stability results $\\mathcal{O}\\left(\\eta T\\left(n^{-1}+m^{-1}\\right)+\\eta\\left(T^{\\frac{1}{2}}+\\beta^{-\\frac{c}{2}}T^{-\\frac{c}{2}+1}+\\beta^{\\frac{1}{2}}T\\right)+\\eta^{2}\\beta^{-1}T\\right)$ for SCGD and $\\mathcal{O}\\left(\\eta T\\left(n^{-1}+m^{-1}\\right)+\\eta\\left(T^{\\frac{1}{2}}+\\beta^{-\\frac{c}{2}}T^{-\\frac{c}{2}+1}+\\beta^{\\frac{1}{2}}T\\right)+\\eta^{2}\\beta^{-\\frac{1}{2}}T\\right)$ for SCSC in the same setting, where $c\\,>\\,0$ is an arbitrary constant. They selected $\\eta\\,=\\,\\mathcal{O}\\left(T^{-\\frac{6}{7}}\\right)$ \uff0c $\\beta\\,=\\,\\mathcal{O}\\left(T^{-\\frac{4}{7}}\\right),c\\,>\\,2,$ $T\\ =\\ {\\mathcal{O}}\\left(\\operatorname*{max}\\left\\{n^{\\frac{7}{2}},m^{\\frac{7}{2}}\\right\\}\\right)$ for SCGD and $\\eta~=~\\mathcal{O}\\left(T^{-\\frac{4}{5}}\\right)$ \uff0c $\\beta~=~\\mathcal{O}\\left(T^{-\\frac{4}{5}}\\right),c~>~4,~T$ = $\\mathcal{O}\\left(\\operatorname*{max}\\left\\{n^{\\frac{5}{2}},m^{\\frac{5}{2}}\\right\\}\\right)$ for SCSC to yield the bounds $\\mathcal{O}\\left(\\operatorname*{max}\\left\\{{\\overset{cdot}{n}}^{-1},m^{-1}\\right\\}\\cdot\\operatorname*{max}\\left\\{n^{\\frac{1}{2}},m^{\\frac{1}{2}}\\right\\}\\right)$ which is slight larger than $\\mathcal{O}\\left(n^{-\\frac{1}{2}}+m^{-\\frac{1}{2}}\\right)$ . Compared with [21], Theorem 2 enjoys not only tighter bounds but also some more practical parameter selections of $\\eta,\\beta,T$ There are some experiments [2, 5, 34, 35] to validate this statement. $^{(I)}$ For $T$ : [21] provided some generalization bounds for convex SCGD and SCSC with some impractical $T$ such as $T=O(\\operatorname*{max}(\\check{n^{7/2}},m^{7/2}))$ in Theorem $^{4}$ While our convex result (Theorem 2) can achieve similar rates even taking $T=O(\\operatorname*{max}(n,m))$ which better matches some empirical observations (Fig. 1, 2 in [5] and Fig. 2 in $l2J)$ . (2) For $\\eta_{t}$ :Theorem 4 in [21] took $\\eta_{t}=T^{-\\hat{6}/7}$ which is too small when $T$ is large. While Theorem 2 takes $\\dot{\\eta}_{t}={\\cal O}(t^{-1})$ closer to some empirical selections $\\prime\\eta_{t}=O(t^{-3/4})$ in $[2,5]$ and $\\eta_{t}=O(t^{-1})$ in [34, 35]). (3) For $\\beta_{t}$ :Theorem $^{4}$ in [21] took $\\beta_{t}=T^{-4/7}$ which is also too small since [2, 5, 35] empirically select $\\bar{\\beta_{t}}=t^{-1/2}$ or $\\beta_{t}=t^{-1}$ . In contrast, Theorem 2 have no special restriction on $\\beta_{t}$ ", "page_idx": 5}, {"type": "text", "text": "Moreover, the bounds of Theorem 2 are similar to some popular stability bounds in the noncompositional literature.For example,the most classical work [26] achieved the uniform stability bound $O\\left(n^{-1}\\log T\\right)$ for convex SGD with $\\eta_{t}\\leq\\mathcal{O}\\left(t^{-1}\\right)$ .[31] showed theuniform stability bound $\\mathcal{O}\\left(n^{-1}T^{\\frac{1}{2}}+n^{-\\frac{1}{2}}\\sqrt{\\log(1/\\delta)}\\right)$ for convex pairwise SGD with $\\eta_{t}=\\mathcal{O}\\left(T^{-\\frac{1}{2}}\\right)$ .The proof of Theorem 2 is provided inAppendix $\\mathbf{C}$ ", "page_idx": 5}, {"type": "text", "text": "To further weaken our assumptions, the generalization analysis of the convex SCO algorithm is developed into the non-convex setting by introducing the almost co-coercivity property of smooth function. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Let Assumptions 1, 3 hold. Assume that the randomized algorithms $A$ (Algorithm $\\left\\{w_{t}\\right\\}_{t=1}^{T}$ $\\left\\{w_{t}^{i,z}\\right\\}_{t=1}^{T}\\left(\\left\\{w_{t}^{j,\\bar{z}}\\right\\}_{t=1}^{T}\\right)$ on $S$ and $S^{i,z}\\left(S^{j,\\bar{z}}\\right)$ withthestep size sequence $\\left\\{\\eta_{t}\\right\\}_{t=1}^{T}$ For SCGD with $\\begin{array}{r}{\\eta_{t}\\,\\le\\,\\frac{1}{2\\rho t},\\rho\\,=\\,\\alpha_{g}L_{f}\\,+\\,\\beta L_{g}^{2}\\alpha_{f}}\\end{array}$ and SCSC with $\\begin{array}{r}{\\eta_{t}\\,\\le\\,\\frac{1}{2\\rho t},\\rho\\,=\\,\\alpha_{g}L_{f}\\,+\\,L_{g}^{2}\\alpha_{f},}\\end{array}$ the fnal output $A(S)\\,=\\,w_{T}$ is uniformly model $(\\epsilon_{z},\\epsilon_{\\bar{z}})$ -stablewith ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\epsilon_{z}=\\frac{L_{g}L_{f}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho n}\\,\\ \\mathrm{and}\\,\\ \\epsilon_{\\bar{z}}=\\frac{L_{g}L_{f}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho m}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 4.Under the non-convex setting, Theorem $^3$ elucidatesasatisfactorystabilitybound $\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T\\right)$ but $T^{\\frac{1}{2}}$ -times larger than Theorem 2. When $T=\\mathcal{O}\\left(\\operatorname*{max}\\{n,m\\}\\right)$ and ignoring logarithmic terms, this bound is equivalent to $\\mathcal{O}\\left(\\operatorname*{max}\\left\\{n^{-1},m^{-1}\\right\\}\\cdot\\operatorname*{max}\\left\\{n^{\\frac{1}{2}},m^{\\frac{1}{2}}\\right\\}\\right)$ Therefore, under the further weakening condition, i.e., non-convexity, Theorem 3 achieves the stability results similar to the ones of the convex SCGD and SCSC in [2l] and some non-compositional, non-convex results [26, 30, 32,33]. The proof of Theorem $^3$ is provided in Appendix $\\mathbf{C}$ ", "page_idx": 6}, {"type": "text", "text": "3.2 Learning Guarantees for Black-box SCO ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The aforementioned results lay the groundwork for elucidating the impacts of black-box on the learning guarantees for the non-convex SCGD and SCSC, including additional optimization analysis. Three black-box cases shown in Algorithm 1 are considered in this part. Prior to the analysis, we provide the following assumption required by the optimization analysis. ", "page_idx": 6}, {"type": "table", "img_path": "4AuEQ1FfUf/tmp/37e2310afcd2ab912ae6201051f19323e1d88d2b786b9099efa2816e65eea074.jpg", "table_caption": ["Table 2: Comparisons among the optimization guarantees for SCO algorithms (Thm.-Theorem; Cor.-Corollary; $L,\\alpha,V,M,\\mathrm{C}$ -Lipschitz continuity, smoothness, bounded variance, bounded function, and convexity assumptions; $d_{2}=\\mathcal{O}(d)$ $\\surd$ -has such a property; $\\times$ -hasn't such a property). "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Assumption 5. For any $w\\ \\in\\ \\mathcal{W}$ and parameter $\\gamma~>~0$ . the empirical risk $F_{S}(w)$ satisfies $\\mathbb{E}\\left[\\|\\nabla F_{S}(w)\\|^{2}\\right]\\geq2\\gamma\\mathbb{E}\\left[F_{S}(w)-F_{S}(w(S))\\right]$ ", "page_idx": 6}, {"type": "text", "text": "In the absence of the convexity assumption, the gradient of empirical risk $\\|\\nabla F_{S}(w)\\|\\,=\\,0$ does not guarantee a global optimal parameter. Assumption 5 postulates that all empirical local optimal parameters are, in fact, empirical global optimal parameters, which prepares for the characterization Oof $\\mathbb{E}[F_{S}(A(S))-F_{S}(w(\\bar{S}))]$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Let Assumptions 1, 2, 3, 4, 5 hold. Assume that the randomized algorithms $A\\;(A l-$ gortim $^{\\,l}$ foCoprobthede qees $\\{w_{t}\\}_{t=1}^{T}$ amd $\\left\\{w_{t}^{i,z}\\right\\}_{t=1}^{T}\\left(\\left\\{w_{t}^{j,\\bar{z}}\\right\\}_{t=1}^{T}\\right)$ on $S$ and $S^{i,z}\\left(S^{j,\\bar{z}}\\right)$ with the step size sequence $\\{\\eta_{t}\\}_{t=1}^{T}$ .For the outer black-box SCGD with $\\begin{array}{r}{\\eta_{t}=\\frac{1}{p\\gamma t},p\\geq\\operatorname*{max}\\left\\{\\sqrt{\\frac{2\\alpha}{\\gamma}},\\frac{2\\left(\\alpha_{g}M_{f}+\\beta L_{g}^{2}M_{f}^{\\prime}\\right)}{\\mu\\gamma}\\right\\}}\\end{array}$ and the outer black-box SCSC with nt = p,p \u2265 $\\operatorname*{max}\\left\\{\\sqrt{\\frac{2\\alpha}{\\gamma}},\\frac{2\\left(\\alpha_{g}M_{f}+L_{g}^{2}M_{f}^{\\prime}\\right)}{\\mu\\gamma}\\right\\}$ $A(S)=w_{T}$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(w_{T})-F(w^{*})\\right]\\leq\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T+n^{-\\frac{1}{2}}+\\mu^{2}+b^{-1}d_{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{d_{2}=d-(2p+1)\\beta+\\left(p+\\frac{1}{2}\\right)^{2}\\beta^{2}}\\end{array}$ for SCGD and $\\begin{array}{r}{d_{2}=d-2p-1+\\left(p+\\frac{1}{2}\\right)^{2}}\\end{array}$ for SCSC ", "page_idx": 7}, {"type": "text", "text": "The proof of Theorem 4 is provided in Appendix $D$ ", "page_idx": 7}, {"type": "text", "text": "Remark 5. Theorem $^{4}$ considers the outer balck-box SCGD and SCSC algorithms where only thegradient of the outer function $f$ is unknown. Itestablishestheexcessrisk bound $\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T+n^{-\\frac{1}{2}}+\\mu^{2}+b^{-1}d_{2}\\right)$ . [21] derived the excess risk bound $\\mathcal{O}\\left(\\operatorname*{max}\\left\\{n^{-1},m^{-1}\\right\\}\\cdot\\operatorname*{max}\\left\\{n^{\\frac{1}{2}},m^{\\frac{1}{2}}\\right\\}\\right)$ for the convex SCGD and SCSC with the parameter selections in Remark 3. Theorem $^{4}$ can derive this bound with the milder condition $T=\\mathcal{O}\\left(\\operatorname*{max}\\{n,m\\}\\right)$ and $\\eta_{t}=\\mathcal{O}\\left(t^{-1}\\right)$ ", "page_idx": 7}, {"type": "text", "text": "For the generalization bound, Theorem $^{4}$ is consistent with Theorem 3. As for the optimization bound, [2] proved the convergence rate $\\mathbb{E}[\\|\\nabla F(w_{T})\\|^{2}]\\leq\\mathcal{O}\\left(T^{-\\frac{1}{4}}\\right)$ of SCGD for non-convex problems. [5] proved the convergence rate $\\begin{array}{r}{T^{-1}\\sum_{t=0}^{T-1}\\mathbb{E}[\\|\\nabla F(w_{t})\\|^{2}]\\leq\\mathcal{O}\\left(T^{-\\frac{1}{2}}\\right)}\\end{array}$ of SCSC for non-convex problems. Compared with [2,5], Theorem $^{4}$ obtains theblack-box-related bound ${\\mathcal{O}}\\left(\\mu^{2}+b^{-1}d_{2}\\right)$ with some smaller learning rates required by our analytical framework. This bound is composed of two dependencies onthe estimation distance $\\mu$ and the number b of estimation directions in Equation (4), which shows the following two key insights into the impacts of black-box on the learning guarantees of SCO algorithms.Firstly,a small $\\mu$ indicates a small distancebetweenthetwofunction values $f_{\\bar{z}}(v+\\mu u)$ and $f_{\\bar{z}}(v)$ selected tomakegradient estimation $\\tilde{\\nabla}f_{\\bar{z}}(v)$ in the direction of the unit vector $u$ .Therefore,a smaller $\\mu_{\\cdot}$ .i.e., a closer estimation distance, brings a better gradient estimation, resulting in a better excess risk bound. Secondly, a large $b$ indicates that plenty of unit vectors $u_{l},l\\,=\\,1,...,b$ with different directions are selected to make gradient estimation $\\tilde{\\nabla}f_{\\bar{z}}(v)$ Then, a larger $b_{!}$ .i.e., more estimation directions, leads to a better gradient estimation, resulting in $a$ better excess risk bound. In summary, the bound of Theorem 4 verifies the fact that a better gradient estimation brings a tighter learning guarantee for the outer black-box SCGD and SCSC. ", "page_idx": 7}, {"type": "text", "text": "Except for the black-box-related term,Theorem4 chooses the learning rates affected by $\\mu,$ which is alsodifferentfromTheorem 4.Although $\\mu$ canbevery small such as $\\stackrel{-4}{10}[l\\bar{9}],$ its negative impact on $\\eta_{t}$ can be eliminated by $M_{f},M_{f}^{\\prime}$ since thetwo inequalities $M_{f}/\\mu\\le L_{f}$ and $M_{f}^{\\prime}/\\mu\\leq\\alpha_{f}$ hold. ", "page_idx": 7}, {"type": "text", "text": "The inner black-box and the full black-box SCO algorithms are studied in the following two corollaries,respectively. ", "page_idx": 7}, {"type": "text", "text": "Corollary 1. Let the assumptions of Theorem 4 hold. For the inner black-box SCGD with $\\eta_{t}\\,=$ $\\begin{array}{r}{\\frac{1}{p\\gamma t},p\\,\\geq\\,\\operatorname*{max}\\left\\{\\sqrt{\\frac{2\\alpha}{\\gamma}},\\frac{2\\left(\\beta L_{g}^{-}\\alpha_{f}M_{g}+M_{g}^{\\prime}L_{f}\\right)}{\\mu\\gamma}\\right\\}}\\end{array}$ and the inner black-box SCSCwith $\\begin{array}{r}{\\eta_{t}\\,=\\,\\frac{1}{p\\gamma t},p\\,\\geq}\\end{array}$ $\\operatorname*{max}{\\left\\{\\sqrt{\\frac{2\\alpha}{\\gamma}},\\frac{2\\left(L_{g}\\alpha_{f}M_{g}+M_{g}^{\\prime}L_{f}\\right)}{\\mu\\gamma}\\right\\}}$ thefinalouput $A(S)=w_{T}$ has the learmninguarantee ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(w_{T})-F(w^{*})\\right]\\leq\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T+n^{-\\frac{1}{2}}+\\mu^{2}+b^{-1}d_{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{d_{2}=d-(2p+1)\\beta+\\left(p+\\frac{1}{2}\\right)^{2}\\beta^{2}}\\end{array}$ for SCGD and $\\begin{array}{r}{d_{2}=d-2p-1+\\left(p+\\frac{1}{2}\\right)^{2}}\\end{array}$ for SCSC ", "page_idx": 7}, {"type": "text", "text": "Corollary 2. Let the assumptions of Theorem $^{4}$ hold. For the full black-box SCGD with $\\eta_{t}~=$ $\\begin{array}{r}{\\frac{1}{p\\gamma t},p\\,\\geq\\,\\operatorname*{max}\\left\\lbrace\\sqrt{\\frac{2\\alpha}{\\gamma}},\\frac{2\\left(\\beta L_{g}^{'}M_{f}^{\\prime}M_{g}+M_{f}M_{g}^{\\prime}\\right)}{\\mu^{2}\\gamma}\\right\\rbrace}\\end{array}$ and the full black-box SCSC with $\\begin{array}{r}{\\eta_{t}\\;=\\;\\frac{1}{p\\gamma t},p\\;\\geq}\\end{array}$ $\\operatorname*{max}\\left\\{\\sqrt{\\frac{2\\alpha}{\\gamma}},\\frac{2\\left(L_{g}M_{f}^{\\prime}M_{g}\\!+\\!M_{f}M_{g}^{\\prime}\\right)}{\\mu^{2}\\gamma}\\right\\}$ ,thefinal output $A(S)=w_{T}$ has the learning guarantee ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(w_{T})-F(w^{*})\\right]\\leq\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T+n^{-\\frac{1}{2}}+\\mu^{4}+b^{-2}d_{2}^{2}+b^{-1}d_{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{d_{2}=d-2\\sqrt{\\left(p+\\frac{1}{2}\\right)\\beta}+\\left(p+\\frac{1}{2}\\right)\\beta\\,f o r\\,S C G D\\,a n d\\,d_{2}=d-2\\sqrt{\\left(p+\\frac{1}{2}\\right)}+p+\\frac{1}{2}\\,f o r\\,S c G D\\,a n d\\,d_{3}=d+\\frac{1}{2},}\\end{array}$ The proofs of Corollaries 1, 2 are provided in Appendix $D$ ", "page_idx": 8}, {"type": "text", "text": "Remark 6. Corollary 1 provides the excess risk bound with the same order as Theorem 4 for the inner black-box SCGD and SCSC. The excess risk bound for the full black-box SCGD and SCSC in Corollary 2 presents the different dependencies on $\\mu$ and $b_{!}$ i.e., $\\mu^{4}+b^{-2}d_{2}^{2}+b^{-1}d_{2}$ . These dependencies also comply with the two key insights uncovered by Theorem 4 and Corollary 1. Besides, when $\\mu^{4}b>d_{2}$ or $\\mu^{2}\\bar{b}<d_{2}$ holds,theterm is dominated by $\\dot{\\mu}^{4}$ or $b^{-2}d_{2}^{2}$ whichdenotesastronger dependence on the gradient estimation quality. Hence, Corollary 2 shows that a larger proportion of unknown gradients may lead to a stronger dependence on the gradient estimation quality. ", "page_idx": 8}, {"type": "text", "text": "4 Applications ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Considering the existing derivative-free cases in VFL, the analysis framework of SCGD and SCSC is herein applied to two VFL algorithms, FOO-based VFL [16] and VFL-CZOFO [19]. As outlined in Algorithm 2 of Appendix A, FOO-based VFL algorithm comprises two components, the $K$ local clients with the model parameters $w^{k},k\\,\\in\\,[K]$ and the central server with the global model $v$ Concerning the data privacy, different clients do not communicate with each other directly, but exchange information indirectly through a server. For this reason, the objective function of the $k$ -th client adopts the same compositional structure $f\\left(g\\left(w^{k}\\right)\\right)$ as SCGD and SCSC. To further ensure data privacy without additional protection techniques, VFL-CZOFO is proposed by introducing the idea of ZOO. Different from the general ZOO-based VFL [18], VFL-CZOFO employs a zeroth-order gradient on the output layer of every client, with other parts utilizing the first-order gradient, which preserves the privacy protection of ZOO while significantly enhancing convergence. ", "page_idx": 8}, {"type": "text", "text": "Before stating our remaining results, it should be noted that there are a few differences between the setting of FOO-based VFL (VFL-CZOFO) and the one of SCGD (SCSC). First of all, we set $S=\\{z_{1},...,z_{n}\\}$ and $S^{i,z}=\\{z_{1},...,z_{i-1},z_{i}^{\\prime},z_{i+1},...,z_{n}\\}$ according to the learning paradigm of VFL. Therefore, Theorem 1 is simplified as Corollary 3. Secondly, the update of the outer model (global model) for FOO-based VFL (VFL-CZOFO) is not based on the simple moving average in SCGD (SCSC). Thirdly, Assumptions 1, 3, 4, 5 hold for every client in all $K$ clients. Without loss of generality, we only study the learning guarantees of FOO-based VFL and VFL-CZOFO for the $k$ -th client. ", "page_idx": 8}, {"type": "text", "text": "Corollary 3.Let Assumption $^{\\,l}$ hold.Assume therandomized VFL algorithms $A$ isuniformlymodel $\\epsilon_{z}$ -stable,then, $\\mathbb{E}[|F(A(S))-F_{S}(A(S))|]\\le L_{g}L_{f}\\epsilon_{z}$ ", "page_idx": 8}, {"type": "text", "text": "Corollary 3 gives the relationship between uniform model stability and generalization error under the setting of VFL. The proof of Corollary 3 is omitted since it can be proved by Equation (15) in the proof of Theorem 2.3 [21] without the decomposition in Equation (14). The last two results study the theoretical performance of FOO-based VFL and VFL-CZOFO. ", "page_idx": 8}, {"type": "text", "text": "Corollary 4. Let Assumptions 1, 3, 4, 5 hold. For the $k$ -thclient $(k\\;\\in\\;[K])$ ,assume that the randomized FOO-based VFL algorithm (Algorithm 2) brings the model sequences $\\left\\{w_{t}^{k}\\right\\}_{t=1}^{T}$ and $\\left\\{w_{t}^{i,z,k}\\right\\}_{t=1}^{T}$ on $S$ and $S^{i,z}$ with the step size sequence $\\begin{array}{r}{\\{\\eta_{t}\\}_{t=1}^{T}\\,,\\eta_{t}\\le\\frac{1}{2\\rho t},\\rho=\\alpha_{g}L_{f}+L_{g}^{2}\\alpha_{f}}\\end{array}$ Then, the final output $A(S)=w_{T}^{k}$ of the $k$ -th client has the generalization guarantee ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[|F(w_{T}^{k})-F_{S}(w_{T}^{k})|\\right]\\leq\\mathcal{O}\\left(n^{-1}T^{\\frac{1}{2}}\\log T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Corollary 5. Let Assumptions 1, 3, 4, 5 hold. For the $k$ -th client $\\langle k\\in[K]\\rangle$ , assume that the randomized VFL-CZOFO algorithm (Algorithm 2) brings the model sequences $\\left\\{w_{t}^{k}\\right\\}_{t=1}^{T}$ and $\\left\\{w_{t}^{i,z,k}\\right\\}_{t=1}^{T}$ m $S$ and $S^{i,z}$ Wwith the tep ie eqguence $\\begin{array}{r}{\\left\\{\\eta_{t}\\right\\}_{t=1}^{T},\\eta_{t}=\\frac{1}{p\\gamma t},p\\geq\\operatorname*{max}\\Bigg\\{\\sqrt{\\frac{2\\alpha}{\\gamma}},\\frac{2\\left(\\alpha_{g}M_{f}+L_{g}^{2}M_{f}^{\\prime}\\right)}{\\mu\\gamma}\\Bigg\\}.}\\end{array}$ Then, the final output $A(S)=w_{T}^{k}$ of the $k$ -th client has the generalization guarantee ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(\\omega_{T}^{k})-F(w^{k*})\\right]\\leq\\mathcal{O}\\left(n^{-1}T^{\\frac{1}{2}}\\log T+\\mu^{2}+b^{-1}d_{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\begin{array}{r}{d_{2}=d-2p-1+\\left(p+\\frac{1}{2}\\right)^{2}}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "The proofs of Corollaries 4, 5 are provided in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "Remark 7. Corollaries 4, 5 both provide the first stability-based generalization bound $\\mathcal{O}\\Big(n^{-1}T^{\\frac{1}{2}}\\log T\\Big)$ Asfor the optimization bound, Corollary 5 gives ${\\mathcal{O}}\\left(\\mu^{2}+b^{-1}d_{2}\\right)$ which originates from the outer black-box setting. ", "page_idx": 9}, {"type": "text", "text": "Apart fromVFL, the generalization guarantee of zeroth-order horizontal federated learning (HFL) was studied with thealgorithmic stability tool.[36]established the systematic theoretical assessments of synchronous federated zeroth-order optimization (FedZO) by developing the on-average model stability analysis. Its generalization bounds and optimization bounds all depend on the two gradient estimation-based parameters p and b. From our perspective, the complicated compositional structure may be a key factor that makes the generalization bound in Corollary 5 unaffected by the impact of theestimatedgradientquality.ThereasonisthattheanalysisframeworkofTheorem2in[36] requires a decomposition (Equation (6)) which is hardly achieved due to the compositional structure inouranalysis. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we provide a novel, more effective theoretical analysis of two SCO algorithms, SCGD and SCSC, and their black-box variants utilizing the uniform model stability tool. The analysis framework is applied to the two VFL algorithms, FOO-based VFL and VFL-CZOFO. Our results not only offer satisfactory learning guarantees but also theoretically validate the impacts of black-box that a better gradient estimation brings a tighter learning guarantee and a larger proportion of unknown gradients leads to a stronger dependence on the gradient estimation quality. We hope our study can facilitate future theoretical analyses of SCO problems and inspire new practical algorithms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China (Nos. 12071166 and 62376104) and the Fundamental Research Funds for the Central Universities of China (No. 2662023LXPY005). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1- 2):267-305, 2016.   \n[2]  Mengdi Wang, Ethan X. Fang, and Han Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2):419-449, 2017.   \n[3]  Mengdi Wang, Ji Liu, and Ethan X. Fang. Accelerating stochastic composition optimization. Journal of Machine Learning Research, 18:105:1-105:23, 2017.   \n[4]  Zhe Zhang and Guanghui Lan. Optimal algorithms for convex nested stochastic composite optimization, 2020.   \n[5]  Tianyi Chen, Yuejiao Sun, and Wotao Yin. Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization. IEEE Transactions on Signal Processing, 69:4937-4948, 2021.   \n[6]  Wei Jiang, Bokun Wang, Yibo Wang, Lijun Zhang, and Tianbao Yang. Optimal algorithms for stochastic multi-level compositional optimization. In International Conference on Machine Learning (ICML), volume 162, pages 10195-10216, 2022.   \n[7]  Bokun Wang and Tianbao Yang. Finite-sum coupled compositional stochastic optimization: Theory and applications. In International Conference on Machine Learning (ICML), volume 162, pages 23292-23317, 2022.   \n[8]  Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. In Advances in Neural Information Processing Systems (NeurIPS), pages 25294-25307, 2021.   \n[9]  Andrzej Ruszczynski and Alexander Shapiro. Optimization of risk measures. Mathematics of Operations Research, 31:433-452, 2006.   \n[10] Qi Qi, Zhishuai Guo, Yi Xu, Rong Jin, and Tianbao Yang. An online method for a class of distributionally robust optimization with non-convex objectives. In Advances in Neural Information Processing Systems (NeurIPS), pages 10067-10080, 2021.   \n[11] Purushottam Kar, Bharath K. Sriperumbudur, Prateek Jain, and Harish Karnick. On the generalization ability of online learning algorithms for pairwise loss functions. In International Conference on Machine Learning (ICML), volume 28, pages 441-449, 2013.   \n[12]  Yiming Ying, Longyin Wen, and Siwei Lyu. Stochastic online AUC maximization. In Advances in Neural Information Processing Systems (NIPS), pages 451-459, 2016.   \n[13]  Yunwen Lei and Yiming Ying. Stochastic proximal AUC maximization. Journal of Machine Learning Research, 22:61:1-61:45, 2021.   \n[14]  Tianbao Yang and Yiming Ying. AUC maximization in the era of big data and AI: A survey. ACM Computing Surveys, 55(8):172:1-172:37, 2023.   \n[15]  Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning (ICML), volume 70, pages 1126-1135, 2017.   \n[16]  Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin. VAFL: a method of vertical asynchronous federated learning, 2020.   \n[17]  Christoph Dann, Gerhard Neumann, and Jan Peters. Policy evaluation with temporal differences: A survey and comparison. Journal of Machine Learning Research, 15(1):809-883, 2014.   \n[18] Qingsong Zhang, Bin Gu, Zhiyuan Dang, Cheng Deng, and Heng Huang. Desirable companion for vertical federated learning: New zeroth-order gradient based algorithm. In ACM International Conference on Information and Knowledge Management (CIKM), pages 2598-2607, 2021.   \n[19] Ganyu Wang, Bin Gu, Qingsong Zhang, Xiang Li, Boyu Wang, and Charles X. Ling. A unified solution for privacy and communication eficiency in vertical federated learning. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[20] V.N. Vapnik. Statistical learning theory. Encyclopedia of the Sciences of Learning, 41(4):3185- 3185, 1998.   \n[21] Ming Yang, Xiyuan Wei, Tianbao Yang, and Yiming Ying. Stability and generalization of stochastic compositional gradient descent algorithms, 2023.   \n[22] Huizhuo Yuan, Xiangru Lian, Chris Junchi Li, Ji Liu, and Wenqing Hu. Efficient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent. In Advances in Neural Information Processing Systems (NeurIPS), pages 6926-6935, 2019.   \n[23]  Adithya M. Devraj and Jianshu Chen. Stochastic variance reduced primal dual algorithms for empirical composition optimization. In Advances in Neural Information Processing Systems (NeurIPS), pages 9878-9888, 2019.   \n[24] Saeed Ghadimi, Andrzej Ruszczynski, and Mengdi Wang. A single timescale stochastic approximation method for nested stochastic optimization. SIAM Journal on Optimization, 30(1):960-979, 2020.   \n[25]  Andrzej Ruszczynski. A stochastic subgradient method for nonsmooth nonconvex multilevel composition optimization. SIAM Journal on Control and Optimization, 59(3):2301-2320, 2021.   \n[26]  Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning (ICML), volume 48, pages 1225-1234, 2016.   \n[27]  Olivier Bousquet and Andr\u00e9 Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2:499-526, 2002.   \n[28] Arkadi Nemirovski, Anatoli B. Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.   \n[29]  L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In International Conference on Computational Statistics (COMPSTAT), pages 177-186, 2010.   \n[30] Yi Zhou, Yingbin Liang, and Huishuai Zhang. Understanding generalization error of SGD in nonconvex optimization. Machine Learning, 111(1):345-375, 2022.   \n[31]  Yunwen Lei, Antoine Ledent, and Marius Kloft. Sharper generalization bounds for pairwise learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 21236- 21246, 2020.   \n[32]  Yunwen Lei, Mingrui Liu, and Yiming Ying. Generalization guarantee of SGD for pairwise learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 21216- 21228, 2021.   \n[33]  Yunwen Lei.  Stability and generalization of stochastic optimization with nonconvex and nonsmooth problems. In Conference on Learning Theory (COLT), pages 191-227, 2023.   \n[34] Zhouyuan Huo, Bin Gu, Ji Liu, and Heng Huang. Accelerated method for stochastic composition optimization with nonsmooth regularization. In Thirty-Second Conference on Artificial Intelligence (AAAI), pages 3287-3294, 2018.   \n[35] Junyu Zhang and Lin Xiao. A stochastic composite gradient method with incremental variance reduction. In Advances in Neural Information Processing Systems (NeurIPS), pages 9075-9085, 2019.   \n[36]  Jun Chen, Hong Chen, Bin Gu, and Hao Deng. Fine-grained theoretical analysis of federated zeroth-order optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[37]  Dominic Richards and Ilja Kuzborskij.  Stability & generalisation of gradient descent for shallow neural networks without the neural tangent kernel. In Advances in Neural Information Processing Systems (NeurIPS), pages 8609-8621, 2021.   \n[38] Yunwen Lei, Rong Jin, and Yiming Ying. Stability and generalization analysis of gradient methods for shallow neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[39]  John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788-2806, 2015. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The main notations of this paper are summarized in Table 3. ", "page_idx": 12}, {"type": "text", "text": "Table 3: Summary of main notations involved in this paper. ", "page_idx": 12}, {"type": "table", "img_path": "4AuEQ1FfUf/tmp/d2a86a653276dbf47ca70c20791310b8758e37fb466495a67296f9f991d6ec1c.jpg", "table_caption": [], "table_footnote": ["The pseudo code of FOO-based VFL and VFL-CZOFO is present in Algorithm 2. "], "page_idx": 12}, {"type": "text", "text": "B Lemmas ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 1. Assume the function $f$ is convex and $\\alpha$ -smooth. Then, for any $w,w^{\\prime}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\langle\\nabla f(w)-\\nabla f(w^{\\prime}),w-w^{\\prime}\\rangle\\geq\\frac{1}{\\alpha}\\|\\nabla f(w)-\\nabla f(w^{\\prime})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma 2. Let e be the base of the natural logarithm. The following inequalities hold: ", "page_idx": 12}, {"type": "text", "text": "(a) if $\\dot{\\boldsymbol{m}}\\in(0,1)$ then $\\sum_{k=1}^{t}k^{-m}\\le t^{1-m}/(1-m);$ (b) ifm = 1, then \u2265 k-m \u2264 log(et); k=1 ", "page_idx": 12}, {"type": "text", "text": "Require: $v_{1},w_{1}^{k}$ : initial global model and $K$ local models; $\\eta_{0},\\eta_{1}$ : initial learning rates for all $t=1,...,T-1$ do for all $k\\in[K]$ in parallel do Randomly select a sample $i_{t}\\in[n]$ , obtain $g_{z_{i_{t}}}(w_{t}^{k})$ and $\\nabla g_{z_{i_{t}}}(w_{t}^{k})$ Send $g_{z_{i_{t}}}(w_{t}^{k})$ to server FOO-based VFL: Receive $\\nabla f(g_{z_{i_{t}}}(w_{t}^{k}))$ Update $\\boldsymbol{w}_{t+1}=\\boldsymbol{w}_{t}-\\eta_{t}\\nabla g_{z_{i_{t}}}(\\boldsymbol{w}_{t}^{k})\\nabla f(g_{z_{i_{t}}}(\\boldsymbol{w}_{t}^{k}))$ VFL-CZOFO: Receive $\\tilde{\\nabla}f(g_{z_{i_{t}}}(w_{t}^{k}))$ Update $w_{t+1}=w_{t}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t}^{k})\\tilde{\\nabla}f(g_{z_{i_{t}}}(w_{t}^{k}))$ end for Server receives $g_{z_{i_{t}}}(w_{t}^{k})$ from $K$ clients FOO-based VFL: Obtain and send $\\nabla f(g_{z_{i_{t}}}(w_{t}^{k}))$ to the $k$ -th client VFL-CZOFO: Compute $\\tilde{\\nabla}f(g_{z_{i_{t}}}(w_{t}^{k}))$ and send it to the $k$ -th client Obtain $\\nabla f(v_{t})$ and update $\\boldsymbol{v}_{t+1}\\dot{=\\boldsymbol{v}_{t}}-\\eta_{0}\\nabla f(\\boldsymbol{v}_{t})$ end for   \nEnsure: $K$ final client models $w_{T}^{1},...,w_{T}^{K}$ ", "page_idx": 13}, {"type": "text", "text": "(c)if $m>1,$ then $\\sum_{k=1}^{t}k^{-m}\\leq\\frac{m}{m-1}.$ $\\begin{array}{r}{i)\\displaystyle\\sum_{k=1}^{t}\\frac{1}{k+k_{0}}\\leq\\log(t+1)}\\end{array}$ ,where $k_{0}\\geq1$ ", "page_idx": 13}, {"type": "text", "text": "Lemma 3. [37, 38] Consider the gradient-based optimization method $\\boldsymbol{w}_{t+1}=\\boldsymbol{w}_{t}-\\eta_{t}\\nabla\\hat{f}(\\boldsymbol{w}_{t})$ For two iteration sequences $\\{w_{t}\\}_{t\\in[T]}$ and $\\{w_{t}^{\\prime}\\}_{t\\in[T]}$ , if the function $\\hat{f}(w_{t})$ is $\\rho$ smooth, $\\eta_{t}\\leq1/(2\\rho)$ and theminimumeigenvalue $\\lambda_{\\operatorname*{min}}\\left(\\nabla^{2}\\hat{f}(w_{t})\\right)\\geq-\\epsilon$ then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\langle w_{t}-w_{t}^{\\prime},\\nabla\\hat{f}(w_{t})-\\nabla\\hat{f}(w_{t}^{\\prime})\\right\\rangle}\\\\ &{\\geq2\\eta_{t}\\left(1-\\displaystyle\\frac{\\eta_{t}\\rho}{2}\\right)\\|\\nabla\\hat{f}(w_{t})-\\nabla\\hat{f}(w_{t}^{\\prime})\\|^{2}-\\epsilon\\|w_{t}-w_{t}^{\\prime}-\\eta_{t}\\nabla\\hat{f}(w_{t})+\\eta_{t}\\nabla\\hat{f}(w_{t}^{\\prime})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 4. If the function $f$ is $\\alpha$ -smooth, then, for any $w,w^{\\prime}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(w)-f(w^{\\prime})\\leq\\langle w-w^{\\prime},\\nabla f(w^{\\prime})\\rangle+\\frac{1}{2}\\alpha\\|w-w^{\\prime}\\|^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{2\\alpha}\\|\\nabla f(w)\\|^{2}\\leq f(w)-\\operatorname*{inf}_{w^{\\prime}}f(w^{\\prime})\\leq f(w)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{2\\alpha}\\|\\nabla F_{S}(w)\\|^{2}\\leq F_{S}(w)-\\operatorname*{inf}_{w^{\\prime}}F_{S}(w^{\\prime})\\leq F_{S}(w).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 5.[39] Assumearandomvector $X\\in\\mathbb{R}^{d}$ is $d$ -dimensionaluniformdistribution.For any $k\\in\\mathbb{N}$ thereholdsIR $\\bar{\\tau}\\left[\\|X\\|^{k}\\right]=d/(d+k)$ ", "page_idx": 13}, {"type": "text", "text": "Lemma 6. [39] Let $u_{l}\\,\\in\\,\\mathbb{R}^{d},l\\,\\in\\,\\{1,2,...,b\\}$ be i.i.d. random vectors satisfying $d$ -dimensional uniform distribution. For every random vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ independent of all $u_{l}$ and $\\beta\\in(0,1)$ ,the following inequality holds ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\frac{1}{b}\\sum_{l=1}^{b}\\langle v,u_{l}\\rangle u_{l}-\\beta v\\right\\Vert\\bigg|v\\right]\\leq\\sqrt{\\frac{d-2\\beta+\\beta^{2}}{b}}\\|v\\|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Table 4: The main diffrences among ourmain results $\\begin{array}{r}{(\\nabla\\hat{f}_{1}(w_{t})=\\nabla g(w_{t})\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}(f(v_{t+1}+\\mu}\\end{array}$ $\\begin{array}{r l r l}&{t_{t,l})\\,-\\,f(v_{t+1})),\\,\\,\\,\\nabla\\hat{f}_{2}(w_{t})}&{=\\,\\,\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\,\\bigl(g(w_{t}+\\mu u_{t,l})-g(w_{t})\\bigr)\\,\\nabla f(v_{t+1}),}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!f(v_{t+1}+\\mu u_{t,l})-f(v_{t+1}))\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\,\\bigl(g(w_{t}+\\mu u_{t,l})-g(w_{t})\\bigr)\\bigr).}&&{}&&\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!$ ", "page_idx": 14}, {"type": "table", "img_path": "4AuEQ1FfUf/tmp/6070cec0238ee77d5b26ecf45567a82dd30bd10497e03d8bc794fdde30f7e30c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Proofs for General SCO ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 2: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "(a) SCGD:  As  Definition  1,  we  define $\\begin{array}{r l r}{S}&{{}=}&{\\left\\{z_{1},...,z_{n},\\bar{z}_{1},...,\\bar{z}_{m}\\right\\},S^{i,z}\\quad\\mathrm{~=~}}\\end{array}$ $\\{z_{1},...,z_{i-1},z_{i}^{\\prime},z_{i+1},...,z_{n},\\bar{z}_{1},...,\\bar{z}_{m}\\}$ and $\\begin{array}{r l r}{S^{j,\\bar{z}}}&{=}&{\\left\\{z_{1},...,z_{n},\\bar{z}_{1},...,\\bar{z}_{j-1},\\bar{z}_{j}^{\\prime},\\bar{z}_{j+1},...,\\bar{z}_{m}\\right\\}}\\end{array}$ for any $i\\;\\in\\;[n],j\\;\\in\\;[m]$ The two terms $\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]$ and $\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\right\\Vert\\right]$ Will be estimated as follows. ", "page_idx": 14}, {"type": "text", "text": "$\\mathbf{1})\\,\\mathbb{E}_{A}\\,\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]$ : There are two cases that need to be considered. Firstly, when $i_{t}\\neq i$ ,there holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|^{2}}\\\\ &{\\quad-\\left.2\\eta_{t}\\left\\langle w_{t}-w_{t}^{i,z},\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Due to two properties of the function $f(g(z))$ , i.e., convexity and smoothness, Lemma 1 implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\langle w_{t}-w_{t}^{i,z},\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\rangle}\\\\ &{=\\!\\frac{1}{\\beta}\\left\\langle w_{t}-w_{t}^{i,z},\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(g_{z_{i_{t}}}(w_{t}))-\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(g_{z_{i_{t}}}(w_{t}^{i,z}))\\right\\rangle}\\\\ &{\\ge\\!\\frac{1}{\\alpha\\beta}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(g_{z_{i_{t}}}(w_{t}))-\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(g_{z_{i_{t}}}(w_{t}^{i,z}))\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\beta\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})=\\nabla f_{\\bar{z}_{j_{t}}}(g_{z_{i_{t}}}(w_{t}))$ based on the update of SCGD. Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2}}\\\\ &{\\le\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert^{2}+\\left(\\frac{\\eta_{t}^{2}}{\\beta^{2}}-\\frac{2\\eta_{t}}{\\alpha\\beta}\\right)\\left\\Vert\\nabla g_{z_{i}}(w_{t})\\nabla f_{\\bar{z}_{t}}(g_{z_{i}}(w_{t}))-\\nabla g_{z_{i}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{t}}(g_{z_{i}}(w_{t}^{i,z}))\\right\\Vert^{2}}\\\\ &{\\le\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second ineqgulit is ased by $\\begin{array}{r}{\\eta_{t}\\,\\leq\\,\\frac{2\\beta}{\\alpha t}\\,\\leq\\,\\frac{2\\beta}{\\alpha}}\\end{array}$ That is $\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|$ Secondly, when $i_{t}=i$ , there holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\|w_{t}-w_{t}^{i,z}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\nabla g_{z_{i_{t}}^{\\prime}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\eta_{t}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}^{\\prime}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+2\\eta_{t}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\right\\|\\left\\|\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+2L_{g}L_{f}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|\\mathbb{I}[i_{t}\\neq i]+\\left(\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+2L_{g}L_{f}\\eta_{t}\\right)\\mathbb{I}[i_{t}=i].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking expectation over $i_{t}$ \uff0c ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t}}\\left[\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert\\right]}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}\\neq i]\\right]+\\left(\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert+2L_{g}L_{f}\\eta_{t}\\right)\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}=i]\\right]}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert+\\frac{2L_{g}L_{f}}{n}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]\\leq\\mathbb{E}_{A}\\left[\\left\\Vert w_{T-1}-w_{T-1}^{i,z}\\right\\Vert\\right]+\\frac{2L_{g}L_{f}}{n}\\eta_{t}}&{}\\\\ {\\leq\\displaystyle\\sum_{t=1}^{T-1}\\frac{2L_{g}L_{f}}{n}\\eta_{t}}&{}\\\\ {=\\!\\frac{4L_{g}L_{f}\\beta}{\\alpha n}\\displaystyle\\sum_{t=1}^{T-1}\\frac{1}{t}}&{}\\\\ {\\leq\\!\\frac{4L_{g}L_{f}\\beta\\log(e T)}{\\alpha n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality is from Lemma 2 (b). ", "page_idx": 15}, {"type": "text", "text": "$\\mathbf{2})\\,\\mathbb{E}_{A}\\,\\Big[\\Big\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\Big\\Vert\\Big]$ : Firstly, when $j_{t}\\ne j$ , there holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{=\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\Vert^{2}}\\\\ &{=\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+\\eta_{t}^{2}\\left\\Vert\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\Vert^{2}}\\\\ &{\\quad-\\left2\\eta_{t}\\left\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Due to two properties of the function $f(g(z))$ , i.e., convexity and smoothness, Lemma 1 implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\rangle}\\\\ &{=\\!\\frac{1}{\\beta}\\left\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(g_{z_{i_{t}}}(w_{t}))-\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}}))\\right\\rangle}\\\\ &{\\ge\\!\\frac{1}{\\alpha\\beta}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(g_{z_{i_{t}}}(w_{t}))-\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}}))\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\beta\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})=\\nabla f_{\\bar{z}_{j_{t}}}(g_{z_{i_{t}}}(w_{t}))$ based on the update of SCGD. Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{\\le\\displaystyle\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+\\left(\\frac{\\eta_{t}^{2}}{\\beta^{2}}-\\frac{2\\eta_{t}}{\\alpha\\beta}\\right)\\left\\Vert\\nabla g_{z_{i}}(w_{t})\\nabla f_{\\bar{z}_{j}}(g_{z_{i}}(w_{t}))-\\nabla g_{z_{i}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j}}(g_{z_{i}}(w_{t}^{j,\\bar{z}}))\\right\\Vert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where te second inequality s caused by $\\begin{array}{r}{\\eta_{t}\\leq\\frac{2\\beta}{\\alpha t}\\leq\\frac{2\\beta}{\\alpha}}\\end{array}$ Thatis $\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|$ Secondly, when $j_{t}=j$ , there holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert}\\\\ &{=\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}^{\\prime}}(v_{t+1}^{j,\\bar{z}})\\right\\Vert}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\eta_{t}\\left\\Vert\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}^{\\prime}}(v_{t+1}^{j,\\bar{z}})\\right\\Vert}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+2\\eta_{t}\\left\\Vert\\nabla g_{z_{i_{t}}}(w_{t})\\right\\Vert\\left\\Vert\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right\\Vert}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+2L_{g}L_{f}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|\\mathbb{I}[j_{t}\\neq j]+\\left(\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+2L_{g}L_{f}\\eta_{t}\\right)\\mathbb{I}[j_{t}=j].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking expectation over $j_{t}$ \uff0c ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{j_{t}}\\left[\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert\\right]}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert\\mathbb{E}_{j_{t}}\\left[\\mathbb{I}[j_{t}\\neq j]\\right]+\\left(\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+2L_{g}L_{f}\\eta_{t}\\right)\\mathbb{E}_{j_{t}}\\left[\\mathbb{I}[j_{t}=j]\\right]}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\frac{2L_{g}L_{f}}{m}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{A}\\left[\\left\\lVert\\boldsymbol{w}_{T}-\\boldsymbol{w}_{T}^{j,\\bar{z}}\\right\\rVert\\right]\\le\\mathbb{E}_{A}\\left[\\left\\lVert\\boldsymbol{w}_{T-1}-\\boldsymbol{w}_{T-1}^{j,\\bar{z}}\\right\\rVert\\right]+\\frac{2L_{g}L_{f}}{m}\\eta_{t}}&{}\\\\ {\\le\\displaystyle\\sum_{t=1}^{T-1}\\frac{2L_{g}L_{f}}{m}\\eta_{t}}&{}\\\\ {=\\!\\frac{4L_{g}L_{f}\\beta}{\\alpha m}\\displaystyle\\sum_{t=1}^{T-1}\\frac{1}{t}}&{}\\\\ {\\le\\!\\frac{4L_{g}L_{f}\\beta\\log(e T)}{\\alpha m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(b) SCSC: Similar to the stability proof of SCGD except for the equation $\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\;\\;=\\;\\;$ $\\nabla f_{\\bar{z}_{j t}}\\big(g_{z_{i_{t}}}\\big(w_{t}\\big)\\big)$ based on the update of SCSC, we have that, for $\\begin{array}{r}{\\eta_{t}=\\frac{\\eta_{1}}{t}\\le\\frac{2}{\\alpha t}\\le\\frac{2}{\\alpha}}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left[\\left\\|w_{T}-w_{T}^{i,z}\\right\\|\\right]\\leq\\frac{4L_{g}L_{f}\\log(e T)}{\\alpha n}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\right\\Vert\\right]\\leq\\frac{4L_{g}L_{f}\\log(e T)}{\\alpha m}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 3: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "SCGD: 1) EA WT - WT $\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]$ : Firstly, when $i_{t}\\neq i$ , there holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}}\\\\ &{=\\Big\\|w_{t}-w_{t}^{i,z}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\Big\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\|w_{t}-w_{t}^{i,z}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|^{2}}\\\\ &{\\quad-\\,2\\eta_{t}\\left\\langle w_{t}-w_{t}^{i,z},\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Without the convexity of the function $f(g(z))$ , Lemma 1 can not hold. An almost co-coercivity of gradient operator (Lemma 3) is introduced to build the relationship between the inner product term $\\left\\langle w_{t}-w_{t}^{i,z},\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\rangle$ and the two squared terms $\\left\\|w_{t}-w_{t}^{i,z}\\right\\|^{2},\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|^{2}$ .With Assumption 3, the terms $\\nabla g(w_{t})$ and $\\nabla f(v_{t+1})$ are both differentiable. Thus, $\\nabla g(w_{t})\\ddot{\\nabla}f(v_{t+1})$ is also differentiable, which means that it is continuous on its domain. As we all know, a continuous function has primitive functions. Then, it is reasonable to assume that there exists a primitive function $\\hat{f}(w_{t})$ at least whose derivative function $\\nabla\\hat{f}(w_{t})=\\nabla g(w_{t})\\nabla f(v_{t+1})$ . For example, $\\textstyle{\\frac{1}{\\beta}}f(v_{t+1})$ is one of the primitive functions $\\hat{f}(w_{t})$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}}\\\\ &{\\quad-\\left2\\eta_{t}\\left\\langle w_{t}-w_{t}^{i,z},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking derivative of $\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ over $w_{t}$ , we get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})=\\nabla^{2}g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\beta\\nabla g_{z_{i_{t}}}(w_{t})\\nabla^{2}f_{\\bar{z}_{j_{t}}}(v_{t+1})\\nabla g_{z_{i_{t}}}^{\\top}(w_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\leq\\alpha_{g}L_{f}+\\beta L_{g}^{2}\\alpha_{f}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\rho=\\alpha_{g}L_{f}+\\beta L_{g}^{2}\\alpha_{f}$ , then $\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ .s $\\rho$ -smooth. Since $\\left\\Vert\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\Vert$ equals to the largest singular value of $\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ , we can know that $\\lambda_{\\operatorname*{min}}\\left(\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right)\\geq-\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\geq$ $-\\rho$ . According to Lemma 3, we can get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\langle w_{t}-w_{t}^{i,z},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\rangle}\\\\ &{\\geq\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}-\\rho\\left\\|w_{t}-w_{t}^{i,z}-\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})+\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}}\\\\ &{=\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}-\\rho\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, plugging the above inequality back into Equation (9) yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert^{2}+\\left(\\eta_{t}^{2}-4\\eta_{t}^{2}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\right)\\left\\Vert\\nabla\\hat{f}_{z_{t_{t}},\\bar{z}_{t_{t}}}(w_{t})\\nabla\\hat{f}_{z_{t_{t}}\\bar{z}_{t_{t}}}(w_{t}^{i,z})\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the seondinequalityis d t $\\begin{array}{r}{\\eta_{t}\\leq\\frac{1}{2\\rho t}\\leq\\frac{3}{2\\rho}}\\end{array}$ The above inequality implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{i,z}\\right\\|,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Secondly, when $i_{t}=i$ , there holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\|w_{t}-w_{t}^{i,z}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\nabla g_{z_{i_{t}}^{\\prime}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\eta_{t}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}^{\\prime}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+2\\eta_{t}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\right\\|\\left\\|\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+2L_{g}L_{f}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{i,z}\\right\\|\\mathbb{I}[i_{t}\\neq i]+\\left(\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+2L_{g}L_{f}\\eta_{t}\\right)\\mathbb{I}[i_{t}=i].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking expectation over $i_{t}$ \uff0c ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t}}\\left[\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}\\neq i]\\right]+\\left(\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert+2L_{g}L_{f}\\eta_{t}\\right)\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}=i]\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert+\\frac{2L_{g}L_{f}}{n}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\lambda}\\left[\\left\\lVert\\mathbf{u}_{T}-\\mathbf{u}_{T^{\\prime}}^{\\prime}\\right\\rVert\\right]\\leq\\frac{1}{\\sqrt{1-\\lambda_{0}}}\\sum_{i=1}^{N}\\mathbb{E}_{\\lambda}\\left[\\left\\lVert\\mathbf{u}_{T-1}-\\mathbf{u}_{T^{\\prime}}^{\\prime}\\right\\rVert\\right]+\\frac{2\\lambda_{0}\\tilde{L}_{T}\\tilde{L}_{T_{0}}}{n}}\\\\ &{\\le\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i}}{\\sqrt{n-1}}\\right)^{\\lambda_{1}}}\\\\ &{\\le\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i}}{\\sqrt{n-1}}\\right)^{\\lambda_{0}}\\frac{1}{n}\\frac{f_{i,T_{0}}}{n}}\\\\ &{\\le\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i}}{\\sqrt{n-1}}\\right)^{\\lambda_{1}}+\\frac{1}{\\gamma}-1\\right)\\frac{f_{i,T_{0}}}{\\rho n}\\frac{f_{i,T_{1}}}{t}}\\\\ &{\\le\\sum_{i=1}^{n}\\left(\\prod_{j=1}^{n}\\sqrt{1+\\frac{1}{\\tau}}\\right)\\frac{f_{i,T_{0}}}{n}\\frac{f_{i,T_{1}}}{t}}\\\\ &{\\le\\sqrt{\\frac{1}{\\left|T-2\\right|}\\left\\langle\\frac{1}{n}\\right\\rangle\\left\\langle\\frac{1}{n}\\right\\rangle^{\\lambda_{0}}\\frac{f_{i,T_{0}}}{t}}\\frac{1}{t}}\\\\ &{\\le\\sqrt{\\left|\\frac{1}{n-1}\\right\\rangle\\left\\langle\\frac{1}{n}\\right\\rangle\\frac{f_{i}}{n}\\frac{f_{i}}{t}\\frac{f_{i}}{n}}}\\\\ &{\\le\\sqrt{\\left|\\exp\\left\\{\\frac{\\sqrt{3}}{\\sqrt{n-1}}\\right\\}\\frac{1}{t}\\right|\\frac{L_{0}}{n}\\frac{f_{i}}{t}\\frac{f_{i}}{n}}}\\\\ &{\\le\\sqrt{\\left|\\exp\\left\\{\\frac{\\sqrt{3}}{\\sqrt{n-1}}\\right\\}\\frac{1}{t}\\right|\\frac{L_{0}}{n}\\sum_{i=1}^{n}\\frac{1}{t}}}\\\\ &{\\le\\frac{\\lambda_{1}L_{t}\\left(t^{2}\\right)^{n}\\left|\\exp\\left\\{\\frac{\\sqrt{3}}{\\sqrt{n-\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the fourth inequality is from $e^{x}\\geq1+x$ ", "page_idx": 18}, {"type": "text", "text": "$\\mathbf{2})\\,\\mathbb{E}_{A}\\,\\Big[\\Big\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\Big\\Vert\\Big]$ : Firstly, when $j_{t}\\ne j$ , there holds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{=\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\Vert^{2}}\\\\ &{=\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+\\eta_{t}^{2}\\left\\Vert\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\Vert^{2}}\\\\ &{\\quad-\\left2\\eta_{t}\\left\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We assume that there exists a primitive function $\\hat{f}(w_{t})$ at least whose derivative function $\\nabla{\\hat{f}}(w_{t})=$ $\\nabla g(w_{t})\\nabla f(v_{t+1})$ .Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\lVert\\boldsymbol{w}_{t+1}-\\boldsymbol{w}_{t+1}^{j,\\bar{z}}\\right\\rVert^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}}\\\\ &{\\quad-\\left.2\\eta_{t}\\left\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taking derivative of $\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ over $w_{t}$ , we get that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})=\\nabla^{2}g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\beta\\nabla g_{z_{i_{t}}}(w_{t})\\nabla^{2}f_{\\bar{z}_{j_{t}}}(v_{t+1})\\nabla g_{z_{i_{t}}}^{\\top}(w_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\leq\\alpha_{g}L_{f}+\\beta L_{g}^{2}\\alpha_{f}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\rho~~=~~\\alpha_{g}L_{f}~+~\\beta L_{g}^{2}\\alpha_{f}$ \uff0c then $\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ is $\\rho$ smooth. And we can know that $\\lambda_{\\operatorname*{min}}\\left(\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right)\\geq-\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\geq-\\rho$ .Accordin to Lemma 3 we can get that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla\\hat{f}_{z_{t_{i}},\\bar{z}_{t_{i}}}(w_{t})-\\nabla\\hat{f}_{z_{t_{i}}\\bar{z}_{t_{i}}}(w_{t}^{j,\\bar{z}})\\rangle}\\\\ &{\\geq\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{t_{i}},\\bar{z}_{t}}(w_{t})-\\nabla\\hat{f}_{z_{t_{i}}\\bar{z}_{t_{i}}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}-\\rho\\left\\|w_{t}-w_{t}^{j,\\bar{z}}-\\nabla\\hat{f}_{z_{t_{i}},\\bar{z}_{t_{i}}}(w_{t})+\\nabla\\hat{f}_{z_{t_{i}}\\bar{z}_{t}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}}\\\\ &{=\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{t_{i}},\\bar{z}_{t}}(w_{t})-\\nabla\\hat{f}_{z_{t_{i}}\\bar{z}_{t}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}-\\rho\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, plugging the above inequality back into Equation (11) yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+\\left(\\eta_{t}^{2}-4\\eta_{t}^{2}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\right)\\left\\Vert\\nabla\\hat{f}_{z_{t_{t}},\\bar{z}_{t_{t}}}(w_{t})\\nabla\\hat{f}_{z_{t_{t}}\\bar{z}_{t_{t}}}(w_{t}^{j,\\bar{z}})\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequalty is de t $\\begin{array}{r}{\\eta_{t}\\leq\\frac{1}{2\\rho t}\\leq\\frac{3}{2\\rho}}\\end{array}$ The above inequality implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Secondly, when $j_{t}=j$ , there holds ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert}\\\\ &{=\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}^{\\prime}}(v_{t+1}^{j,\\bar{z}})\\right\\Vert}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\eta_{t}\\left\\Vert\\nabla g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}^{\\prime}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}^{\\prime}}(v_{t+1}^{j,\\bar{z}})\\right\\Vert}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+2\\eta_{t}\\left\\Vert\\nabla g_{z_{i_{t}}}(w_{t})\\right\\Vert\\left\\Vert\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right\\Vert}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+2L_{g}L_{f}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert\\left\\Vert\\mathcal{I}_{t}\\neq j\\right\\Vert+\\left(\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+2L_{g}L_{f}\\eta_{t}\\right)\\mathbb{I}[j_{t}=j].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taking expectation over $j_{t}$ \uff0c ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t}}\\left[\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert\\mathbb{E}_{j_{t}}\\left[\\mathbb{I}[j_{t}\\ne j]\\right]+\\left(\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+2L_{g}L_{f}\\eta_{t}\\right)\\mathbb{E}_{j_{t}}\\left[\\mathbb{I}[j_{t}=j]\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\frac{2L_{g}L_{f}}{m}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\lambda}\\left[\\left\\lVert\\mathbf{v}_{T}-\\mathbf{v}_{T^{\\prime}}^{\\lambda}\\right\\rVert\\right]\\leq\\frac{1}{\\sqrt{1-\\operatorname*{sup}\\left[\\lambda\\right]}}\\mathbb{E}_{\\lambda}\\left[\\left\\lVert\\mathbf{v}_{T-1}-\\mathbf{v}_{T^{\\prime}}^{\\lambda}\\right\\rVert\\right]+\\frac{2\\lambda_{B}L_{T}}{m}\\mathbb{E}_{\\eta}}\\\\ &{\\le\\sum_{i=1}^{r}\\left(\\frac{\\gamma_{1}}{\\sqrt{r_{1}+1}}\\frac{1}{\\sqrt{1-\\operatorname*{sup}\\left[\\lambda\\right]}}\\right)\\frac{2L_{T}L_{T}}{m}t}\\\\ &{\\le\\sum_{i=1}^{r-1}\\left(\\frac{\\gamma_{1}}{\\sqrt{r_{1}+1}}\\sqrt{1+\\frac{1}{\\mu}}\\right)\\frac{L_{T}L_{T}}{m}\\frac{1}{\\mu}}\\\\ &{\\le\\sum_{i=1}^{r-1}\\left(\\frac{\\gamma_{1}}{\\sqrt{r_{1}+1}}\\sqrt{1+\\frac{1}{\\mu}}\\right)\\frac{L_{T}L_{T}}{m}\\frac{1}{\\mu}}\\\\ &{\\le\\sum_{i=1}^{r-1}\\left(\\frac{\\gamma_{1}}{\\sqrt{r_{1}+2\\lambda_{B}}}\\right)\\frac{\\gamma_{1}}{\\mu}\\frac{L_{T}}{m}t\\frac{1}{\\mu}}\\\\ &{\\le\\sqrt{\\left|\\frac{\\gamma_{1}}{\\sqrt{r_{2}+1}}\\left\\langle\\frac{1}{\\mu}\\right\\rangle\\right|^{2}-\\frac{1}{\\mu}}\\frac{L_{T}L_{T}}{m}t\\frac{1}{\\mu}}\\\\ &{\\le\\sqrt{\\left|\\exp\\left\\{\\sum_{i=1}^{r-1}\\frac{1}{\\mu}\\right\\}\\frac{L_{T}L_{T}}{m}\\frac{\\Gamma-1}{\\mu}}}\\\\ &{\\le\\sqrt{\\left|\\exp\\left\\{\\sum_{i=1}^{r-1}\\frac{1}{\\mu}\\right\\}\\frac{L_{T}L_{T}}{m}\\frac{\\Gamma-1}{m}}}\\\\ &{\\le\\frac{L_{T}L_{T}(\\ell)^{2}\\lVert\\mathbf{v}\\rVert}{\\mu}\\lVert\\mathrm{e}\\rVert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the fourth inequality is from $e^{x}\\geq1+x$ ", "page_idx": 20}, {"type": "text", "text": "SCSC: Similarto the stailityproof of SCGD except for $\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})=\\nabla^{2}g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+$ $\\nabla g_{z_{i_{t}}}(w_{t})\\nabla^{2}f_{\\bar{z}_{j_{t}}}(v_{t+1})\\nabla g_{z_{i_{t}}}^{\\top}(w_{t})$ based on the update of SCSC, we have that, for $\\begin{array}{r}{\\eta_{t}\\,\\leq\\,\\frac{1}{2\\rho t}\\,\\leq\\,}\\end{array}$ $\\textstyle{\\frac{3}{2\\rho}},\\rho=\\alpha_{g}L_{f}+L_{g}^{2}\\alpha_{f}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]\\leq\\frac{L_{g}L_{f}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho n}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\right\\Vert\\right]\\leq\\frac{L_{g}L_{f}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho m}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "DProofs for Black-box SCO ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 4: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "SCGD: 1) $\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]$ : Firstly, when $i_{t}\\neq i$ , there holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}-\\eta_{t}\\nabla g_{z_{t}}(w_{t})\\tilde{\\nabla}f_{z_{t}}(v_{t+1})+\\eta_{t}\\nabla g_{z_{t}}(w_{t}^{i,z})\\tilde{\\nabla}f_{z_{t}}(v_{t+1}^{i,z})\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla g_{z_{t}}(w_{t})\\tilde{\\nabla}f_{z_{t}}(v_{t+1})-\\nabla g_{z_{t}}(w_{t}^{i,z})\\tilde{\\nabla}f_{z_{t}}(v_{t+1}^{i,z})\\right\\|^{2}}\\\\ &{\\quad-2\\eta_{t}\\left\\langle w_{t}-w_{t}^{i,z},\\nabla g_{z_{t}}(w_{t})\\tilde{\\nabla}f_{z_{t}}(v_{t+1})-\\nabla g_{z_{t}}(w_{t}^{i,z})\\tilde{\\nabla}f_{z_{t}}(v_{t+1}^{i,z})\\right\\rangle}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla g_{z_{t}}(w_{t})\\frac{1}{b}\\sum_{t=1}^{B}\\frac{u_{t,i}}{\\mu}\\left(f_{\\tilde{z}_{t}}(v_{t+1}+\\mu u_{t,i})-f_{\\tilde{z}_{t}}(v_{t+1})\\right)-\\nabla g_{z_{t}}(w_{t}^{i,z})\\right\\|}\\\\ &{\\quad\\frac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,i}}{\\mu}\\left(f_{\\tilde{z}_{t}}(v_{t+1}^{i,z}+\\mu u_{t,l})-f_{\\tilde{z}_{t}}(v_{t+1}^{i,z})\\right)\\left\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\left.2\\eta_{t}\\Biggl\\langle w_{t}-w_{t}^{i,z},\\nabla g_{z_{i_{t}}}(w_{t})\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\right.}\\\\ &{\\left.-\\nabla g_{z_{i_{t}}}(w_{t}^{i,z})\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right)\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With Assumption 3, the terms $\\nabla g(w_{t})$ and $f(v_{t+1})$ are both differentiable.  Thus, $\\begin{array}{r}{\\nabla g(w_{t})\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f(v_{t+1}+\\mu u_{t,l})-f(v_{t+1})\\right)}\\end{array}$ is also differeniale It israsonable to assume that there exists a primitive function $\\hat{f}(w_{t})$ at least whose derivative function $\\nabla{\\hat{f}}(w_{t})\\,=$ $\\begin{array}{r}{\\nabla g(w_{t})\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f(v_{t+1}+\\mu u_{t,l})-f(v_{t+1})\\right)}\\end{array}$ Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}}\\\\ &{\\quad-\\ 2\\eta_{t}\\left\\langle w_{t}-w_{t}^{i,z},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking derivative of $\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ over $w_{t}$ , we get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})}\\\\ &{=\\!\\nabla^{2}g_{z_{i_{t}}}(w_{t})\\cfrac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)}\\\\ &{\\quad+\\,\\beta\\nabla g_{z_{i_{t}}}(w_{t})\\cfrac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\nabla g_{z_{i_{t}}}^{\\top}(w_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\nabla^{2}\\hat{f}_{\\lambda_{t},\\varepsilon_{t}}(w)\\right\\|}\\\\ {\\displaystyle=\\left\\|\\nabla^{2}g_{\\varepsilon_{t}}(w)\\frac{1}{b}\\sum_{i=1}^{b}\\frac{u_{t,i}}{\\mu}\\left(f_{\\varepsilon_{t}}(v_{t+1}+\\mu u_{t,i})-f_{\\varepsilon_{t}}(v_{t+1})\\right)\\right.}\\\\ {\\displaystyle\\quad+\\left.\\beta\\nabla g_{\\varepsilon_{t}}(w_{t})\\frac{1}{b}\\sum_{i=1}^{b}\\frac{u_{t,i}}{\\mu}\\left(\\nabla f_{\\varepsilon_{t}}(v_{t+1}+\\mu u_{t,i})-\\nabla f_{\\varepsilon_{t}}(v_{t+1})\\right)\\nabla g_{\\varepsilon_{t}}^{\\top}(w_{t})\\right\\|}\\\\ {\\displaystyle\\leq\\left\\|\\nabla^{2}g_{\\varepsilon_{t}}(w_{t})\\frac{1}{b}\\sum_{i=1}^{b}\\frac{u_{t,i}}{\\mu}\\left(f_{\\varepsilon_{t}}(v_{t+1}+\\mu u_{t,i})-f_{\\varepsilon_{t}}(v_{t+1})\\right)\\right\\|}\\\\ {\\displaystyle\\qquad+\\left\\|\\beta\\nabla g_{\\varepsilon_{t}}(w)\\frac{1}{b}\\sum_{i=1}^{b}\\frac{u_{t,i}}{\\mu}\\left(\\nabla f_{\\varepsilon_{t}}(v_{t+1}+\\mu u_{t,i})-\\nabla f_{\\varepsilon_{t}}(v_{t+1})\\right)\\nabla g_{\\varepsilon_{t}}^{\\top}(w_{t})\\right\\|}\\\\ {\\displaystyle\\leq\\frac{1}{\\mu}\\left(\\alpha_{\\varepsilon}M_{f}+\\beta L_{g}^{\\top}M_{f}^{\\top}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\begin{array}{r l r}{\\rho}&{{}=}&{\\frac{1}{\\mu}\\left(\\alpha_{g}M_{f}+\\beta L_{g}^{2}M_{f}^{\\prime}\\right)}\\end{array}$ \uff0c then $\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ is $\\rho$ -smooth. And we can know that $\\lambda_{\\operatorname*{min}}\\left(\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right)\\geq-\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\geq-\\rho$ According to Lemma 3, we can get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\langle w_{t}-w_{t}^{i,z},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\rangle}\\\\ &{\\geq\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}-\\rho\\left\\|w_{t}-w_{t}^{i,z}-\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})+\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}}\\\\ &{=\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}-\\rho\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, plugging the above inequality back into Equation (15) yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert^{2}+\\left(\\eta_{t}^{2}-4\\eta_{t}^{2}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\right)\\left\\Vert\\nabla\\hat{f}_{z_{i_{t}},\\tilde{z}_{j_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\tilde{z}_{j_{t}}}(w_{t}^{i,z})\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the secnd inequalityis due t $\\begin{array}{r}{\\eta_{t}\\leq\\frac{1}{2\\rho t}\\leq\\frac{3}{2\\rho}}\\end{array}$ The above inequality implies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{i,z}\\right\\|,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Secondly, when $i_{t}=i$ , there holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\nabla g_{\\bar{z}_{i_{t}}^{\\prime}}(w_{t}^{i,z})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\eta_{t}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}^{\\prime}}(w_{t}^{i,z})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+2\\eta_{t}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\right\\|\\left\\|\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\frac{2L_{g}M_{f}}{\\mu}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{i,z}\\right\\|\\mathbb{I}[i_{t}\\neq i]+\\left(\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\frac{2L_{g}M_{f}}{\\mu}\\eta_{t}\\right)\\mathbb{I}[i_{t}=i].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking expectation over $i_{t}$ \uff0c ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t}}\\left[\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert\\right]}\\\\ &{\\leq\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}\\neq i]\\right]+\\left(\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert+\\frac{2L_{g}M_{f}}{\\mu}\\eta_{t}\\right)\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}=i]\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert+\\frac{2L_{g}M_{f}}{\\mu n}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\lambda}\\left[\\left\\lVert\\boldsymbol{w}_{T}-\\boldsymbol{w}_{T}^{i,\\lambda}\\right\\rVert\\right]\\leq\\frac{1}{\\sqrt{1-2\\rho_{0}t}}\\mathbb{E}_{\\lambda}\\left[\\left\\lVert\\boldsymbol{w}_{T-1}-\\boldsymbol{w}_{T-1}^{i,\\lambda}\\right\\rVert\\right]+\\frac{2L_{\\theta}M_{f}}{\\rho\\mu_{0}}}\\\\ &{\\le\\frac{\\sum_{t=1}^{T}}{\\kappa_{1}}\\left(\\frac{\\prod_{s=1}^{T}}{\\epsilon_{1}+1}\\frac{1}{\\sqrt{1-2\\rho_{0}t}}\\right)2\\frac{2L_{\\theta}M_{f}}{\\rho\\mu_{0}}}\\\\ &{\\le\\frac{\\sum_{t=1}^{T}}{\\kappa_{1}}\\binom{T-1}{\\epsilon_{1}+1}\\sqrt{1+\\frac{1}{\\epsilon_{1}-1}}\\right)\\frac{L_{\\theta}M_{f}}{\\rho\\mu_{0}}\\frac{1}{t}}\\\\ &{\\le\\frac{\\sum_{t=1}^{T}}{\\mu_{1}}\\binom{T-1}{\\epsilon_{1}-2}\\sqrt{1+\\frac{1}{\\mu_{1}}}\\frac{1}{\\rho\\mu_{0}}\\frac{L_{\\theta}M_{f}}{t}}\\\\ &{\\le\\sqrt{\\frac{1}{\\|\\boldsymbol{w}_{T}\\|}}\\exp\\left\\{\\frac{1}{\\epsilon_{1}-1}\\right\\}\\sum_{t=1}^{T-1}\\frac{L_{\\theta}M_{f}}{\\rho\\mu_{0}}\\frac{1}{t}}\\\\ &{\\le\\sqrt{\\exp\\left\\{\\frac{\\sum_{t=1}^{T}}{\\epsilon_{1}-1}\\right\\}}\\frac{1}{\\rho\\mu_{0}}\\mathtt{M}_{f}^{T-1}\\frac{1}{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\leq{\\frac{L_{g}M_{f}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho\\mu n}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the fourth inequality is from $e^{x}\\geq1+x$ ", "page_idx": 23}, {"type": "text", "text": "$\\mathbf{2})\\,\\mathbb{E}_{A}\\,\\Big[\\Big\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\Big\\Vert\\Big]$ : Firstly, when $j_{t}\\ne j$ , there holds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{u}_{t+1}-\\mathbf{u}_{t+1}^{t+1}\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{u}_{t}-\\mathbf{u}_{t}^{t,i}-\\boldsymbol{\\eta}\\nabla_{\\lambda_{t}}(\\mathbf{u}_{t})\\hat{\\mathbf{y}}_{t}f_{t_{n}}(v_{t+1})+\\boldsymbol{\\eta}\\nabla_{\\beta_{t}}(\\mathbf{u}_{t}^{t,i})\\hat{\\mathbf{y}}_{t}f_{t_{n}}(v_{t+1}^{t,i})\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{u}_{t}-\\mathbf{u}_{t}^{t,i}\\right\\|^{2}+\\frac{1}{\\eta}\\left\\|\\nabla_{\\lambda_{t}}(\\mathbf{u}_{t})\\hat{\\mathbf{y}}_{t}f_{t_{n}}(v_{t+1})-\\nabla_{\\beta_{t}}(\\mathbf{u}_{t}^{t,i})\\hat{\\mathbf{y}}_{t}f_{t_{n}}(v_{t+1}^{t,i})\\right\\|^{2}}\\\\ &{\\quad-2\\eta_{t}\\left\\langle\\mathbf{u}_{t}-\\mathbf{u}_{t}^{t,i}\\nabla_{\\lambda_{t}}\\mathbf{u}_{t}\\right\\rangle\\hat{\\mathbf{y}}_{t}f_{t_{n}}(v_{t+1})-\\nabla_{\\beta_{t}}(\\mathbf{u}_{t}^{t,i})\\hat{\\mathbf{y}}_{t}f_{t_{n}}(v_{t+1}^{t,i})\\right\\rangle}\\\\ &{=\\left\\|\\mathbf{u}_{t}-\\mathbf{u}_{t}^{t,i}\\right\\|^{2}+\\frac{1}{\\eta}\\left\\|\\nabla_{\\beta_{t}}(\\mathbf{u}_{t})\\right\\|_{\\beta_{t}}\\frac{1}{b}\\frac{\\eta}{b}\\frac{\\eta}{b}\\mu_{t}^{i}\\left(f_{t_{n}}(v_{t+1}+p u_{t})-f_{t_{n}}(v_{t+1})\\right)-\\nabla_{\\beta_{t}}(\\mathbf{u}_{t}^{t,i})}\\\\ &{\\frac{1}{b}\\frac{\\nu}{b}\\frac{\\mathbf{u}_{t}}{b}\\int_{t_{n}}\\left(f_{t_{n}}(v_{t+1}^{t,i}+p u_{t+1})-f_{t_{n}}(v_{t+1}^{t,i})\\right)\\Big\\|^{2}}\\\\ &{\\quad- \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With Assumption 3, the terms $\\nabla g(w_{t})$ and $f(v_{t+1})$ are both differentiable.  Thus, $\\begin{array}{r}{\\nabla g(w_{t})\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f(v_{t+1}+\\mu u_{t,l})-f(v_{t+1})\\right)}\\end{array}$ is also diffrentiable. It is reasonable to assume that there exists a primitive function $\\hat{f}(w_{t})$ at least whose derivative function $\\nabla{\\hat{f}}(w_{t})\\,=$ $\\begin{array}{r}{\\nabla g(w_{t})\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f(v_{t+1}+\\mu u_{t,l})-f(v_{t+1})\\right)}\\end{array}$ Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}}\\\\ &{\\quad-\\,2\\eta_{t}\\left\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking derivative of $\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ over $w_{t}$ , we get that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})}\\\\ &{=\\!\\nabla^{2}g_{z_{i_{t}}}(w_{t})\\displaystyle\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)}\\\\ &{\\quad+\\,\\beta\\nabla g_{z_{i_{t}}}(w_{t})\\displaystyle\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\nabla g_{z_{i_{t}}}^{\\top}(w_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|}\\\\ &{=\\!\\!\\left\\|\\nabla^{2}g_{z_{i_{t}}}(w_{t})\\frac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\right.}\\\\ &{\\quad+\\left.\\beta\\nabla g_{z_{i_{t}}}(w_{t})\\frac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\nabla g_{z_{i_{t}}}^{\\top}(w_{t})\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\left\\|\\nabla^{2}g_{z_{i_{t}}}(w_{t})\\frac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\right\\|}\\\\ &{\\quad+\\left\\|\\beta\\nabla g_{z_{i_{t}}}(w_{t})\\frac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\nabla g_{z_{i_{t}}}^{\\top}(w_{t})\\right\\|}\\\\ &{\\leq\\displaystyle\\frac{1}{\\mu}\\left(\\alpha_{g}M_{f}+\\beta L_{g}^{2}M_{f}^{\\prime}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\begin{array}{r l r}{\\rho}&{{}=}&{\\frac{1}{\\mu}\\left(\\alpha_{g}M_{f}+\\beta L_{g}^{2}M_{f}^{\\prime}\\right)}\\end{array}$ , then $\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ is $\\rho$ smooth. And we can know that $\\lambda_{\\operatorname*{min}}\\left(\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right)\\geq-\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\geq-\\rho$ .Acording to Lemma 3, wecan get that $\\begin{array}{r l}&{\\quad\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\rangle}\\\\ &{\\geq\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}-\\rho\\left\\|w_{t}-w_{t}^{j,\\bar{z}}-\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})+\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}}\\\\ &{=\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}-\\rho\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|^{2}.}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Now, plugging the above inequality back into Equation (17) yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+\\left(\\eta_{t}^{2}-4\\eta_{t}^{2}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\right)\\left\\Vert\\nabla\\hat{f}_{z_{t_{t}},\\bar{z}_{t_{t}}}(w_{t})\\nabla\\hat{f}_{z_{t_{t}}\\bar{z}_{t_{t}}}(w_{t}^{j,\\bar{z}})\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second inequalty is de t $\\begin{array}{r}{\\eta_{t}\\leq\\frac{1}{2\\rho t}\\leq\\frac{3}{2\\rho}}\\end{array}$ The above inequality implies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Secondly, when $j_{t}=j$ , there holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|}\\\\ &{=\\left\\|w_{t}-w_{t}^{j,\\bar{z}}-\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}^{\\prime}}(v_{t+1}^{j,\\bar{z}})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+\\eta_{t}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\nabla g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}^{\\prime}}(v_{t+1}^{j,\\bar{z}})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+2\\eta_{t}\\left\\|\\nabla g_{z_{i_{t}}}(w_{t})\\right\\|\\left\\|\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+\\frac{2L_{g}M_{f}}{\\mu}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|\\mathbb{I}[j_{t}\\neq j]+\\left(\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+\\frac{2L_{g}M_{f}}{\\mu}\\eta_{t}\\right)\\mathbb{I}[j_{t}=j].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking expectation over $i_{t}$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t}}\\left[\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[j_{t}\\neq j]\\right]+\\left(\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\frac{2L_{g}M_{f}}{\\mu}\\eta_{t}\\right)\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[j_{t}=j]\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\frac{2L_{g}M_{f}}{\\mu m}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\lambda}\\left[\\left\\lVert\\boldsymbol{v}_{T}-\\boldsymbol{w}_{T^{\\perp}}^{\\lambda}\\right\\rVert\\right]\\le\\frac{1}{\\sqrt{\\lambda}-2}\\operatorname*{lim}_{\\theta\\in\\Theta}\\mathbb{E}_{\\lambda}\\left[\\left\\lVert\\boldsymbol{v}_{T-1}-\\boldsymbol{w}_{T^{\\perp}}^{\\lambda}\\right\\rVert\\right]+\\frac{2\\lambda_{\\theta}\\mathcal{M}_{I_{0}}}{\\mu\\ m}}\\\\ &{\\le\\sum_{t=1}^{T}\\left(\\frac{\\gamma-1}{\\sqrt{\\lambda_{\\theta}}+1}\\frac{1}{\\sqrt{\\lambda_{\\theta}}}\\right)\\frac{2L_{\\theta}\\mathcal{M}_{I_{0}}}{\\mu\\mu}}\\\\ &{\\le\\sum_{t=1}^{T-1}\\left(\\frac{\\gamma-1}{\\sqrt{\\lambda_{\\theta}}+1}\\sqrt{\\lambda_{\\theta}+1}\\right)\\frac{L_{\\theta}\\mathcal{M}_{I_{0}}}{\\mu\\mu}\\mathrm{\\boldmath~\\hat{\\Pi}~}}\\\\ &{\\le\\sum_{t=1}^{T-1}\\left(\\frac{\\gamma-1}{\\sqrt{\\lambda_{\\theta}}}\\sqrt{1+\\frac{1}{\\ell}}-1\\right)\\frac{L_{\\theta}\\mathcal{M}_{I_{0}}}{\\mu\\mu}\\mathrm{\\boldmath~\\hat{\\Pi}~}}\\\\ &{\\le\\sum_{t=1}^{T-1}\\left(\\frac{\\gamma-1}{\\sqrt{\\lambda_{\\theta}}}\\sqrt{1+\\frac{1}{\\ell}}-1\\right)\\frac{L_{\\theta}\\mathcal{M}_{I_{0}}}{\\mu\\mu}\\mathrm{\\boldmath~\\hat{\\Pi}~}}\\\\ &{\\le\\sqrt{\\frac{1}{\\sqrt{\\lambda_{\\theta}}}\\log\\left(\\frac{1}{\\sqrt{\\lambda_{\\theta}}}\\right)\\frac{1}{\\ell}-\\frac{1}{\\ell}}\\frac{L_{\\theta}\\mathcal{M}_{I_{0}}}{\\mu}\\mathrm{\\boldmath~\\hat{\\Pi}~}}\\\\ &{\\le\\sqrt{\\exp\\left\\{\\frac{\\sqrt{3}}{\\lambda_{\\theta}}\\frac{1}{\\sqrt{\\lambda_{\\theta}}}\\right\\}\\frac{L_{\\theta}\\mathcal{M}_{I_{0}}}{\\mu}\\mathrm{\\boldmath~\\hat{\\Pi}~}}}\\\\ &{\\le\\int_{\\phi_{T}}\\ge\\frac{L_{\\theta}\\mathcal{M}_{I_{0}}}{\\mu}(\\mathcal{M})\\log(T)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the fourth inequality is from $e^{x}\\geq1+x$ ", "page_idx": 25}, {"type": "text", "text": "Next, we will study the optimization bound. According to Equation (6) in Lemma 4, $\\nabla f(w_{t})=$ $\\beta\\nabla g(w_{t})\\nabla f(v_{t+1})$ , second-order Taylor expansion, Lemmas 5 and 6, we provide that, for any $p\\geq{\\sqrt{2}}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\int_{\\mathbb{Z}}|\\nabla_{x}(u_{t+1})-F_{t}(x_{t})|\\right\\}+\\frac{1}{2}\\mathbb{E}\\left|w_{t+1}-w_{t}\\right|^{2}\\Bigg\\}}\\\\ &{\\leq\\mathbb{E}\\Bigg[\\bigg(w_{t+1}-w_{t}\\nabla_{x}\\mathbb{E}\\left(w_{t}\\right)\\Big)+\\frac{1}{2}w_{t}\\left\\{w_{t+1}-w_{t}\\right\\}\\Bigg]}\\\\ &{=\\mathbb{E}\\Bigg[\\bigg(-\\nabla_{x}w_{t+1}\\nabla_{x}\\mathbb{E}\\left(w_{t+1}\\right)\\mathbb{E}\\left(w_{t}\\right)\\Big)+\\frac{1}{2}w\\left\\{\\nabla_{x}w_{t},w_{t}\\right\\}\\nabla f_{t_{\\Delta}(t)}\\times\\bigg|\\Bigg]}\\\\ &{=\\mathbb{E}\\Bigg[\\bigg(-\\nabla_{x}w_{t+1}\\nabla_{x}w_{t}\\left(\\nabla f_{t_{\\Delta}(t)}(\\cdot|w_{t+1})-\\int_{t}^{+1}\\right)\\mathcal{O}(f_{t},(w_{t+1})+\\bigg(\\cdot\\frac{1}{\\epsilon^{2}}\\right)\\bigg)\\times\\mathbb{E}\\mathcal{F}_{t}\\bigg(w_{t+1}\\bigg)\\Bigg)\\Bigg]}\\\\ &{\\qquad\\mathbb{E}\\Bigg[\\int_{\\mathbb{Z}}|w_{t}|\\nabla_{x}(u_{t})\\nabla_{x}(w_{t})\\left(\\nabla f_{t_{\\Delta}(t)}(\\cdot|w_{t+1})-\\mathcal{O}(f_{\\hat{Z}}\\right)_{t}(w_{t+1})+\\mathcal{O}(f_{\\hat{Z}}\\right)_{t}(w_{t+1})\\right)\\Bigg]}\\\\ &{\\leq\\mathbb{E}\\Bigg[-w_{t}\\bigg\\{\\nabla_{x}w_{t}\\left(\\nabla f_{t_{\\Delta}(t)}(\\cdot|w_{t+1})-\\left(\\cdot\\frac{1}{\\epsilon^{2}}\\right)\\mathcal{O}(f_{t},(w_{t+1})\\right)\\bigg),\\nabla F_{t}(w_{t})\\bigg\\}\\Bigg]}\\\\ &{\\qquad-\\left(\\cdot\\left(\\rho+\\frac{1}{2}\\right)w_{t}\\nabla(x_{t+1})\\right)^{2}+w_{t}^{2}\\Bigg[\\nabla_{x}w_{t}\\left(\\nabla f_{t_{\\Delta}(t)}(\\cdot|w_{t+1})-\\mathcal{O}(f_{\\hat{Z}}\\right)_{t}(w_{t \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad_{\\mathbf{S}}(\\mathbf{r},\\mathbf{u})=_{i}\\cdots}\\\\ &{\\phantom{=}+\\omega^{2\\nu}\\left[\\left|\\nabla_{\\mathbf{S}}\\rho_{i}(\\mathbf{r}_{i},\\mathbf{\\hat{r}}_{i}^{\\phantom{\\dagger}})\\frac{\\hat{\\mathbf{l}}_{\\mathbf{u}}^{\\dagger}}{\\sqrt{\\mathbf{r}}_{i}}\\left(\\nabla_{\\mathbf{S}}\\rho_{i}(\\rho_{\\mathbf{u}}(\\mathbf{r}_{i}+\\rho_{\\mathbf{u}})-P_{i}(\\mathbf{r}_{i}(\\mathbf{u}))-\\rho^{\\dagger}\\nabla_{\\mathbf{S}}\\rho_{i}(\\mathbf{r}_{i}(\\mathbf{u}))\\right)\\right|\\right]^{\\mathrm{T}}\\right]}\\\\ &{\\quad+\\left(\\omega^{2\\nu}-\\rho^{2}\\right)\\mathbb{E}\\left[\\left|\\nabla_{\\mathbf{S}}\\rho_{i}(\\mathbf{r}_{i}^{\\dagger})\\right|^{\\mathrm{T}}\\right]}\\\\ &{\\quad_{\\mathbf{J}}\\mathbf{u}^{\\dagger}\\succcurlyeq\\left(\\nabla_{\\mathbf{S}}\\rho_{i}(\\mathbf{r}_{i}^{\\dagger})\\right)}\\\\ &{\\quad-\\frac{1}{2}\\mathbf{{J}}\\times\\left[\\left|\\nabla_{\\mathbf{S}}\\rho_{i}(\\mathbf{r}_{i}^{\\dagger})\\right|^{\\mathrm{T}}\\right]+\\frac{1}{2}\\left(\\nabla_{\\mathbf{S}}\\rho_{i}(\\mathbf{r}_{i},\\mathbf{u}_{i})\\right)\\mathbf{u}_{i}\\cdot\\left(\\frac{\\rho}{\\sqrt{\\mathbf{J}}}\\left(\\mathbf{r}_{i}(\\mathbf{u}_{i})\\nabla^{\\ast}\\rho_{i}(\\mathbf{r}_{i}-\\rho_{\\mathbf{u}})\\right)\\mathbf{u}_{i}\\right)}\\\\ &{\\quad-\\left(\\rho^{\\star}+\\frac{1}{2}\\right)\\rho^{\\mathrm{T}}\\mathbf{E}\\rho_{i}(\\mathbf{r}_{i},\\mathbf{u}_{i})\\right)\\bigg|^{\\mathrm{T}}\\right]+\\frac{1}{2}\\omega^{2\\nu}\\left[\\left|\\nabla_{\\mathbf{S}}\\rho_{i}(\\mathbf{r}_{i}^{\\dagger})\\right|^{\\mathrm{T}}\\left(\\nabla_{\\mathbf{S}}\\rho_{i}(\\mathbf{r}_{i}(\\mathbf{u}_{i}))\\mathbf{u}_{i}\\right)}\\\\ &{\\quad+\\left(\\frac{\\rho^{\\star}}{2}\\right)\\left($ $\\begin{array}{r l}&{\\quad+\\left(\\frac{\\partial}{\\partial\\theta_{i}}\\partial_{j}^{(1)}\\nabla^{2}f_{i,j}(\\theta_{i})\\frac{1}{\\sqrt{\\theta_{i}}}\\right)\\left(\\frac{1}{\\sqrt{\\theta_{i}}}\\right)}\\\\ &{\\le(\\alpha\\partial_{j}^{(1)}\\partial_{j}^{(2)}\\left[\\frac{1}{\\sqrt{\\theta_{j}}}\\left(\\frac{\\partial_{j}^{(1)}}{\\partial\\theta_{i}}\\right)^{\\top}f_{i,j}(\\theta_{i})\\frac{1}{\\sqrt{\\theta_{j}}}\\right]\\right)\\left[\\frac{1}{\\sqrt{\\theta_{i}}}\\right]}\\\\ &{\\quad+\\theta_{i}\\left[\\frac{1}{\\sqrt{\\theta_{i}}}\\left(\\operatorname*{lim}_{j}^{(1)}\\left(\\frac{1}{\\sqrt{\\theta_{i}}}\\right)^{\\top}f_{i,j}(\\theta_{i})\\frac{1}{\\sqrt{\\theta_{j}}}-\\left(\\mu_{j}^{(1)}\\right)^{\\top}\\right)\\theta_{i}\\left[\\frac{1}{\\sqrt{\\theta_{i}}}\\right]}\\\\ &{\\quad+2\\alpha\\frac{1}{\\sqrt{\\theta_{i}}}\\left[\\left[\\nabla^{2}f_{i,j}(\\theta_{i})\\frac{1}{\\sqrt{\\theta_{i}}}\\left(\\nabla f_{i,j}(\\theta_{i})\\frac{1}{\\sqrt{\\theta_{i}}}\\right)\\theta_{i}-\\left(\\mu_{j}^{(1)}\\right)^{\\top}\\right]\\right]}\\\\ &{\\quad+2\\alpha\\frac{1}{\\sqrt{\\theta_{i}}}\\left[\\left[\\frac{1}{\\sqrt{\\theta_{i}}}\\left(\\operatorname*{lim}_{j}^{(1)}\\left(\\nabla f_{i,j}(\\theta_{i})\\frac{1}{\\sqrt{\\theta_{i}}}\\right)\\theta_{i}\\right)-\\left(\\mu_{j}^{(1)}\\right)^{\\top}\\right]\\right]}\\\\ &{\\quad+\\left(\\alpha\\partial_{j}^{(2)}-\\beta\\right)\\frac{1}{\\sqrt{\\theta_{i}}}\\left[\\left[\\frac{1}{\\sqrt{\\theta_{i}}}\\right]\\left(\\frac{1}{\\sqrt{\\theta_{i}}}\\right)^{\\top}f_{i,j}(\\theta_{i})\\frac{1}{\\sqrt{\\theta_{i}}}\\right]\\left[\\frac{1}{\\sqrt{\\theta_{j}}}\\right]}\\\\ &{\\le(\\alpha\\partial_{j}^{(1)}\\partial_{j$ $\\begin{array}{r l}&{\\quad_{1}(\\Delta\\xi)=\\displaystyle_{\\mathcal{P}_{i}}\\mathbb{E}\\Bigg[\\Bigg|\\frac{\\Delta\\xi}{\\Delta x_{i}}\\Bigg|\\mathcal{E}_{p}\\Bigg[\\sigma_{\\Delta\\xi}\\Bigg|^{2}}\\\\ &{\\le(\\Delta\\xi)\\Bigg|^{2}\\Bigg|\\frac{\\Delta\\xi}{\\Delta x_{i}}\\Bigg|\\Bigg|\\frac{\\Delta\\xi}{\\Delta x_{i}}\\Bigg|\\Bigg|\\frac{\\Delta\\xi}{\\Delta x_{i}}\\Bigg|\\Bigg|\\nabla_{\\xi}\\Bigg|\\xi\\Bigg|_{p_{i}}\\Bigg|(\\Delta\\xi)\\Bigg|\\Bigg]}\\\\ &{\\quad+E_{p,\\eta}\\Delta\\xi\\Bigg[\\Bigg|\\frac{\\Delta\\xi}{\\Delta x_{i}}\\Bigg|\\Bigg|\\frac{\\Delta\\xi}{\\Delta x_{i}}\\Bigg|(\\nabla_{\\xi}(\\eta_{\\Delta\\xi}))_{\\alpha,0}\\Bigg|\\Bigg|\\eta_{\\Delta\\xi}\\Bigg|(\\Delta\\xi)\\Bigg|\\Bigg]}\\\\ &{\\quad+2\\Delta x_{i}\\mathbb{E}_{p}^{2}\\Bigg[\\Bigg|\\Bigg|\\frac{\\Delta\\xi}{\\Delta x_{i}}\\Bigg|\\nabla_{\\xi}\\Bigg|\\xi\\Bigg|\\eta_{\\Delta\\xi}\\Bigg|(\\Delta\\eta_{\\Delta\\xi})\\log_{0}\\Bigg|\\eta_{\\Delta\\xi}\\Bigg|\\Bigg]\\Bigg]}\\\\ &{\\quad+(\\alpha\\eta_{\\Delta\\xi}^{2}-\\beta)\\mathbb{E}_{p}[\\eta(\\Delta\\xi)\\Bigg|\\eta_{\\Delta\\xi}\\Bigg|\\eta_{\\Delta\\xi}\\Bigg|\\Bigg]}\\\\ &{\\le(\\Delta\\eta_{\\Delta\\xi}^{2}+\\alpha)\\mathbb{E}_{p}^{2}\\Bigg[\\Bigg|\\frac{\\Delta\\xi}{\\Delta x_{i}}\\Bigg|\\frac{\\Delta\\xi}{\\Delta x_{i}}\\Bigg|(\\xi\\frac{\\Delta\\xi}{\\Delta x_{i}})^{2}\\sqrt{\\eta_{\\Delta\\xi}(\\eta_{\\Delta\\xi})(\\Delta\\xi)}\\log_{0}\\Bigg|\\eta_{\\xi}\\Bigg|\\Bigg]}\\\\ &{\\quad+\\bigg(\\frac{\\Delta\\xi}{\\Delta x_{i}}\\frac{\\Delta\\xi}{\\Delta y}\\Bigg)\\mathbb{E}_{p}[\\eta(\\Delta\\xi)]\\frac{d}{d}\\xi\\exp_{i}(\\Delta\\xi)\\Bigg|\\eta_{\\Delta\\xi}\\Bigg|\\eta_{\\Delta\\xi}\\Bigg|\\eta_{\\Delta\\xi}\\Bigg|}\\\\ &{\\quad+\\bigg(\\alpha\\eta_{\\Delta\\xi}^{2}-\\beta\\alpha\\frac{1}{2}\\beta\\alpha\\frac{1}{2}\\beta\\alpha\\frac{1}{2}\\beta\\alpha\\frac{1} $ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\left(\\alpha\\eta_{t}^{2}-p\\eta_{t}\\right)\\mathbb{E}\\left[\\|\\nabla F_{S}(w_{t})\\|^{2}\\right]+\\frac{d L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}(2\\alpha\\eta_{t}^{2}+\\eta_{t})}\\\\ {\\displaystyle\\quad+\\,\\frac{d-2\\beta+\\beta^{2}}{b}2\\alpha L_{f}^{2}L_{g}^{2}\\eta_{t}^{2}+\\frac{d-\\left(2p+1\\right)\\beta+\\left(p+\\frac{1}{2}\\right)^{2}\\beta^{2}}{b}L_{f}^{2}L_{g}^{2}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $d_{1}=d-2\\beta+\\beta^{2}$ and $\\begin{array}{r}{d_{2}=d-\\left(2p+1\\right)\\beta+\\left(p+\\frac{1}{2}\\right)^{2}\\beta^{2}}\\end{array}$ to get that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left\\{F_{S}(w_{t+1})-F_{S}(w_{t})\\right\\}}\\\\ &{\\leq\\left(\\alpha\\eta_{t}^{2}-p\\eta_{t}\\right)\\mathbb{E}\\left[\\|\\nabla F_{S}(w_{t})\\|^{2}\\right]+\\displaystyle\\frac{d L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}(2\\alpha\\eta_{t}^{2}+\\eta_{t})+\\frac{L_{f}^{2}L_{g}^{2}}{b}\\left(2\\alpha d_{1}\\eta_{t}^{2}+d_{2}\\eta_{t}\\right)}\\\\ &{\\leq-\\displaystyle\\frac{1}{2}p\\eta_{t}\\mathbb{E}\\left[\\|\\nabla F_{S}(w_{t})\\|^{2}\\right]+\\displaystyle\\frac{d L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}(2\\alpha\\eta_{t}^{2}+\\eta_{t})+\\frac{L_{f}^{2}L_{g}^{2}}{b}\\left(2\\alpha d_{1}\\eta_{t}^{2}+d_{2}\\eta_{t}\\right)}\\\\ &{\\leq-p\\gamma\\eta_{t}\\mathbb{E}\\left[F_{S}(w_{t})-F_{S}(w(S))\\right]+\\displaystyle\\frac{d L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}(2\\alpha\\eta_{t}^{2}+\\eta_{t})+\\frac{L_{f}^{2}L_{g}^{2}}{b}\\left(2\\alpha d_{1}\\eta_{t}^{2}+d_{2}\\eta_{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second inequality is due to $\\begin{array}{r}{\\eta_{t}=\\frac{1}{p\\gamma t}\\leq\\frac{p}{2\\alpha t}\\leq\\frac{p}{2\\alpha}}\\end{array}$ when $\\begin{array}{r}{p\\ge\\sqrt{\\frac{2\\alpha}{\\gamma}}}\\end{array}$ , and the last inequality is from Equation 8 in Lemma 4. Then, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[F_{S}(w_{t+1})-F_{S}(w(S))\\right]}\\\\ &{\\le(1-p\\gamma\\eta_{t})\\,\\mathbb{E}\\left[F_{S}(w_{t})-F_{S}(w(S))\\right]+\\frac{d L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}(2\\alpha\\eta_{t}^{2}+\\eta_{t})+\\frac{L_{f}^{2}L_{g}^{2}}{b}\\left(2\\alpha d_{1}\\eta_{t}^{2}+d_{2}\\eta_{t}\\right)}\\\\ &{=\\left(1-\\frac{1}{t}\\right)\\mathbb{E}\\left[F_{S}(w_{t})-F_{S}(w(S))\\right]+\\frac{d L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}\\left(\\frac{2}{p^{2}\\alpha t^{2}}+\\frac{1}{p\\alpha t}\\right)+\\frac{L_{f}^{2}L_{g}^{2}}{b}\\left(\\frac{2d_{1}}{p^{2}\\alpha t^{2}}+\\frac{d_{2}}{p\\alpha t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We multiply both sides of the above inequality by $t$ to get that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad t\\mathbb{E}\\left[F_{S}(w_{t+1})-F_{S}(w(S))\\right]}\\\\ &{\\le\\!(t-1)\\mathbb{E}\\left[F_{S}(w_{t})-F_{S}(w(S))\\right]+\\frac{d L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}\\left(\\frac{2}{p^{2}\\alpha t}+\\frac{1}{p\\alpha}\\right)+\\frac{L_{f}^{2}L_{g}^{2}}{b}\\left(\\frac{2d_{1}}{p^{2}\\alpha t}+\\frac{d_{2}}{p\\alpha}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(T-1)\\mathbb{E}\\left[F_{S}(w_{T})-F_{S}(w(S))\\right]}\\\\ &{\\leq(T-2)\\mathbb{E}\\left[F_{S}(w_{T-1})-F_{S}(w(S))\\right]+\\frac{d L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}\\left(\\frac{2}{p^{2}\\alpha(T-1)}+\\frac{1}{p\\alpha}\\right)+\\frac{L_{f}^{2}L_{g}^{2}}{b}\\left(\\frac{2d_{1}}{p^{2}\\alpha(T-1)}+\\frac{1}{p^{2}\\alpha_{f}^{2}}\\right)}\\\\ &{\\leq\\!\\frac{d L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}\\left(\\sum_{t=1}^{T-1}\\frac{2}{p^{2}\\alpha t}+\\frac{T-1}{p\\alpha}\\right)+\\frac{L_{f}^{2}L_{g}^{2}}{b}\\left(\\sum_{t=1}^{T-1}\\frac{2d_{1}}{p^{2}\\alpha t}+\\frac{d_{2}(T-1)}{p\\alpha}\\right)}\\\\ &{\\leq\\!\\frac{d L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}\\left(\\frac{2\\log(e T)}{p^{2}\\alpha}+\\frac{T-1}{p\\alpha}\\right)+\\frac{L_{f}^{2}L_{g}^{2}}{b}\\left(\\frac{2d_{1}\\log(e T)}{p^{2}\\alpha}+\\frac{d_{2}(T-1)}{p\\alpha}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "That is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[F_{S}(w_{T})-F_{S}(w(S))\\right]}\\\\ &{\\leq\\!\\frac{d\\!L_{g}^{2}\\mu^{2}\\alpha_{f}^{2}}{4(d+6)}\\left(\\frac{2\\log(e T)}{p^{2}\\alpha(T-1)}+\\frac{1}{p\\alpha}\\right)+\\frac{L_{f}^{2}L_{g}^{2}}{b}\\left(\\frac{2d_{1}\\log(e T)}{p^{2}\\alpha(T-1)}+\\frac{d_{2}}{p\\alpha}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining Theorem 1, Equations (16), (18) and (19), we can get that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(w_{T})-F(w^{*})\\right]\\leq\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T+n^{-\\frac{1}{2}}+\\mu^{2}+b^{-1}d_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "SCSC: Similar to the stability proof of SCGD except for ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\nabla^{2}g_{z_{i_{t}}}(w_{t})\\frac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)}\\\\ &{\\quad+\\,\\nabla g_{z_{i_{t}}}(w_{t})\\frac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\nabla g_{z_{i_{t}}}^{\\top}(w_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "based on the update of SCSC, we have that, for $\\begin{array}{r}{\\eta_{t}\\le\\frac{1}{2\\rho t}\\le\\frac{3}{2\\rho},\\rho=\\frac{1}{\\mu}\\,\\Big(\\alpha_{g}M_{f}+L_{g}^{2}M_{f}^{\\prime}\\Big),}\\end{array}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]\\leq\\frac{L_{g}M_{f}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho\\mu n}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\right\\Vert\\right]\\leq\\frac{L_{g}M_{f}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho\\mu m}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Similar to the optimization proof of SCGD, we also have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[F_{S}(w_{T})-F_{S}(w(S))\\right]}\\\\ &{\\le\\!\\frac{d{\\cal L}_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}\\left(\\frac{2\\log(e T)}{p^{2}\\alpha(T-1)}+\\frac{1}{p\\alpha}\\right)+\\frac{{\\cal L}_{g}^{2}{\\cal L}_{f}^{2}}{b}\\left(\\frac{2d_{1}\\log(e T)}{p^{2}\\alpha(T-1)}+\\frac{d_{2}}{p\\alpha}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{d_{1}=d-1,d_{2}=d-(2p+1)+\\left(p+\\frac{1}{2}\\right)^{2}}\\end{array}$ . Combining Theorem 1, Equations (20), (21) and (22), we can get that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(w_{T})-F(w^{*})\\right]\\leq\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T+n^{-\\frac{1}{2}}+\\mu^{2}+b^{-1}d_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Corollary 1: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "SCGD: 1) $\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]$ : Firstly, when $i_{t}\\neq i$ , there holds ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{u}_{t+1}-\\mathbf{u}_{t}^{s,t}\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{u}_{t}-\\mathbf{u}_{t}^{s,t}-\\mathbf{u}_{t}^{s}\\nabla_{\\theta_{k}}(w_{t})\\nabla f_{k_{n}}(w_{t+1})+\\eta\\nabla_{\\theta_{k}}(w_{t}^{s,t})\\nabla f_{k_{n}}(\\mathbf{r}_{t}^{s,t})\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{u}_{t}-\\mathbf{u}_{t}^{s,t}\\right\\|^{2}+\\eta^{2}\\left\\|\\nabla_{\\theta_{k}}(w_{t})\\nabla f_{k_{n}}(w_{t+1})-\\nabla f_{k_{n}}(w_{t}^{s,t})\\nabla f_{k_{n}}(\\mathbf{r}_{t}^{s,t})\\right\\|^{2}}\\\\ &{\\quad-2\\eta_{k}\\left\\langle\\mathbf{r}_{t}-\\mathbf{u}_{t}^{s,t},\\nabla g_{k_{n}}(w_{t})\\nabla f_{k_{n}}(\\mathbf{r}_{t})\\mathbf{r}_{t+1}-\\nabla g_{k_{n}}(w_{t}^{s,t})\\nabla f_{k_{n}}(\\mathbf{r}_{t+1}^{s,t})\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{u}_{t}-\\mathbf{u}_{t}^{s,t}\\right\\|^{2}+\\eta^{2}\\left\\|\\frac{1}{b}\\sum_{l=1}^{M}\\rho\\left(\\mathbf{r}_{t}^{2}(w_{t}+t\\mu_{k}t_{n})-\\nabla g_{k_{n}}(w_{t}^{s})\\nabla f_{l_{n}}(\\mathbf{r}_{t+1}^{s,t})\\right)\\nabla f_{l_{n}}(\\mathbf{r}_{t+1})\\right\\|^{2}}\\\\ &{\\quad-\\frac{1}{b}\\sum_{l=1}^{M}\\rho\\left(g_{l_{n}}(w_{t+1}^{s,t}+\\mu_{l+1})-g_{l_{n}}(\\mathbf{r}_{t}^{(s,t)})\\nabla f_{l_{n}}(\\mathbf{r}_{t+1}^{s,t})\\right)^{2}}\\\\ &{=2\\eta_{k}\\left\\langle\\mathbf{u}_{t}-\\mathbf{u}_{t}^{s,t}\\frac{1}{b}\\sum_{l=1}^{M}\\rho\\left(g_{l_{n \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "With Assumption 3, the terms $\\nabla f(v_{t+1})$ and $g(w_{t})$ are both differentiable.  Thus, $\\begin{array}{r}{\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g(w_{t}+\\mu u_{t,l})-g(w_{t})\\right)\\nabla f(v_{t+1})}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "that there exists a primitive function $\\hat{f}(w_{t})$ at least whose derivative function $\\nabla{\\hat{f}}(w_{t})\\;\\;=\\;\\;$ $\\begin{array}{r}{\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g(w_{t}+\\mu u_{t,l})-g(w_{t})\\right)\\nabla f(v_{t+1})}\\end{array}$ Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}}\\\\ &{\\quad-\\ 2\\eta_{t}\\left\\langle w_{t}-w_{t}^{i,z},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Taking derivative of $\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ over $w_{t}$ , we get that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla^{2}\\widehat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla^{2}\\widehat{f}_{\\mu_{1}},\\widehat{\\tau}_{\\mu_{1}}(w_{t})\\right\\|}\\\\ &{=\\left\\|\\displaystyle\\frac{1}{b}\\displaystyle\\sum_{i=1}^{b}\\frac{u_{t}}{\\mu}\\left(\\nabla g_{\\mu_{1}}\\left(w+\\mu u_{t,i}\\right)-\\nabla g_{\\mu_{1}}\\left(w_{t}\\right)\\right)\\nabla f_{\\widehat{\\mu}_{t}}\\left(v_{t+1}\\right)\\right.}\\\\ &{\\quad\\left.+\\beta\\displaystyle\\frac{1}{b}\\displaystyle\\sum_{i=1}^{b}\\frac{u_{t}}{\\mu}\\left(g_{\\mu_{1}}\\left(w_{t}+\\mu u_{t,i}\\right)-g_{\\mu_{1}}(w_{t})\\right)\\nabla^{2}f_{\\mu_{t}}\\left(v_{t+1}\\right)\\nabla g_{\\mu_{t}}^{\\top}\\left(w_{t}\\right)\\right\\|}\\\\ &{\\leq\\left\\|\\displaystyle\\frac{1}{b}\\displaystyle\\sum_{i=1}^{b}\\frac{u_{t}}{\\mu}\\left(\\nabla g_{\\mu_{1}}\\left(w_{t}+\\mu u_{t,i}\\right)-\\nabla g_{\\mu_{1}}\\left(w_{t}\\right)\\right)\\nabla f_{\\widehat{\\mu}_{t}}\\left(v_{t+1}\\right)\\right\\|}\\\\ &{\\quad+\\left\\|\\displaystyle\\beta\\displaystyle\\sum_{i=1}^{b}\\displaystyle\\sum_{i=1}^{b}\\frac{u_{t}}{\\mu}\\left(g_{z_{i}}\\left(w_{t}+\\mu u_{t,i}\\right)-g_{z_{i}}\\left(w_{t}\\right)\\right)\\nabla^{2}f_{\\mu_{t}}\\left(v_{t+1}\\right)\\nabla g_{z_{i}}^{\\top}\\left(w_{t}\\right)\\right\\|}\\\\ &{\\leq\\frac{1}{\\mu}\\left(\\beta L\\omega_{\\sigma}f_{M_{0}}+M_{\\sigma}^{\\prime}L_{f}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $\\rho~~=~~\\frac{1}{\\mu}\\left(\\beta L_{g}\\alpha_{f}M_{g}+M_{g}^{\\prime}L_{f}\\right)$ \uff0c then $\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ is $\\rho$ smooth. And we can know that $\\lambda_{\\operatorname*{min}}\\left(\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right)\\geq-\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\geq-\\rho$ .According to Lemma 3, we can get that $\\begin{array}{r l}&{\\quad\\left\\langle w_{t}-w_{t}^{i,z},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\rangle}\\\\ &{\\geq\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}-\\rho\\left\\|w_{t}-w_{t}^{i,z}-\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})+\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}}\\\\ &{=\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}-\\rho\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}.}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Now, plugging the above inequality back into Equation (23) yields ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert^{2}+\\left(\\eta_{t}^{2}-4\\eta_{t}^{2}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\right)\\left\\Vert\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{t}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{t}}(w_{t}^{i,z})\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the secnd inequalityis de t $\\begin{array}{r}{\\eta_{t}\\leq\\frac{1}{2\\rho t}\\leq\\frac{3}{2\\rho}}\\end{array}$ The above inequality implies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{i,z}\\right\\|,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Secondly, when $i_{t}=i$ , there holds ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}-\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}^{\\prime}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\eta_{t}\\left\\|\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\tilde{\\nabla}g_{z_{i_{t}}^{\\prime}}(w_{t}^{i,z})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+2\\eta_{t}\\left\\|\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\right\\|\\left\\|\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\frac{2L_{f}M_{g}}{\\mu}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{i,z}\\right\\|\\mathbb{I}[i_{t}\\neq i]+\\left(\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\frac{2L_{f}M_{g}}{\\mu}\\eta_{t}\\right)\\mathbb{I}[i_{t}=i].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taking expectation over $i_{t}$ \uff0c ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t}}\\left[\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert\\right]}\\\\ &{\\leq\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}\\neq i]\\right]+\\left(\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert+\\frac{2L_{f}M_{g}}{\\mu}\\eta_{t}\\right)\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}=i]\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert+\\frac{2L_{f}M_{g}}{\\mu n}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\lambda}\\left[\\left\\lVert\\mathbf{v}_{T}-\\mathbf{u}_{T^{\\perp}}^{\\perp}\\right\\rVert\\right]\\leq\\frac{1}{\\sqrt{1-\\alpha_{T}}}\\frac{1}{\\left(\\alpha_{T}^{3}\\right)}\\mathbb{E}_{\\lambda}\\left[\\left\\lVert\\mathbf{v}_{T-1}-\\mathbf{u}_{T^{\\perp}}^{\\perp}\\right\\rVert\\right]+\\frac{2\\bar{L}_{2}\\mathcal{M}_{\\lambda}}{\\mu_{\\operatorname*{min}}}}\\\\ &{\\le\\sum_{i=1}^{T}\\left(\\frac{\\gamma_{i}}{\\sqrt{1-\\alpha_{i}^{i}}}\\frac{1}{\\sqrt{1-2\\alpha_{i}^{i}\\nu_{i}}}\\right)\\frac{1}{2}\\frac{J_{T}M_{T}}{\\mu_{\\operatorname*{m}}}}\\\\ &{\\le\\sum_{i=1}^{T-1}\\left(\\frac{\\gamma_{i}}{\\sqrt{1-\\alpha_{i}^{i}}}\\sqrt{1+\\frac{1}{\\ell}}\\right)\\frac{L_{T}M_{T}}{\\mu_{\\operatorname*{m}}}\\frac{1}{\\ell}}\\\\ &{\\le\\sum_{i=1}^{T-1}\\left(\\frac{\\gamma_{i}}{\\sqrt{1-\\alpha_{i}^{i}}}\\sqrt{1+\\frac{1}{\\ell}}\\right)\\frac{L_{T}M_{T}}{\\mu_{\\operatorname*{m}}}\\frac{1}{\\ell}}\\\\ &{\\le\\sum_{i=1}^{T-1}\\left(\\frac{\\gamma_{i}}{\\sqrt{1-\\alpha_{i}^{i}}}\\sqrt{1+\\frac{1}{\\ell}}\\right)\\frac{L_{T}M_{T}}{\\mu_{\\operatorname*{m}}}\\frac{1}{\\ell}}\\\\ &{\\le\\sqrt{\\left\\langle\\frac{1}{\\sqrt{1-\\alpha_{i}^{i}}}\\left\\langle\\frac{1}{\\ell}\\right\\rangle\\right\\rangle\\frac{\\zeta_{-}}{\\alpha_{i}}\\frac{L_{T}M_{T}}{\\mu_{\\operatorname*{m}}}}\\frac{1}{\\ell}}\\\\ &{\\le\\sqrt{\\left\\langle\\frac{\\gamma_{i}}{\\sqrt{1-\\alpha_{i}^{i}}}\\frac{1}{\\sqrt{1-\\alpha_{i}^{i}}}\\right\\rangle\\frac{L_{T}M_{T}}{\\mu_{\\operatorname*{m}}}\\frac{\\nu_{-1}^{i-1}}{\\ell}}}\\\\ &{\\le\\sqrt{\\left\\langle\\frac{\\gamma_{i}}{\\sqrt{1-\\alpha_{i}^{i}} \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the fourth inequality is from $e^{x}\\geq1+x$ ", "page_idx": 30}, {"type": "text", "text": "$\\mathbf{2})\\,\\mathbb{E}_{A}\\,\\Big[\\Big\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\Big\\Vert\\Big]$ : Firstly, when $j_{t}\\ne j$ , there holds ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{j,\\bar{z}}-\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-2\\eta_{t}\\left<w_{t}-w_{t}^{i,\\xi},\\overline{{\\nabla}}g_{z_{t}}(w_{t})\\nabla f_{z_{t}}(v_{t+1})-\\overline{{\\nabla}}g_{z_{t}}(w_{t}^{j,\\xi})\\nabla f_{z_{t}}(v_{t+1}^{j,\\xi})\\right>}\\\\ {\\displaystyle=\\left\\lVert w_{t}-w_{t}^{i,\\xi}\\right\\rVert^{2}+\\eta_{t}^{2}\\left\\lVert\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu^{l}}\\left(g_{z_{t}}(w_{t}+\\mu u_{t,l})-g_{z_{t}}(w_{t})\\right)\\nabla f_{z_{t}}(v_{t+1})\\right.}\\\\ {\\displaystyle\\quad-\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g_{z_{t}}(w_{t}^{j,\\xi}+\\mu u_{t,l})-g_{z_{t}}(w_{t}^{j,\\xi})\\right)\\nabla f_{z_{t}}(v_{t+1}^{j,\\xi})\\right\\rVert^{2}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle-\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t}}{\\mu}\\left<g_{z_{t}}(w_{t}^{j,\\xi},\\overline{{b}}\\frac{1}{b-1}\\frac{u_{t,l}}{\\mu}\\left(g_{z_{t}}(w_{t}+\\mu u_{t,l})-g_{z_{t}}(w_{t})\\right)\\nabla f_{z_{t}}(v_{t+1})\\right.}\\\\ {\\displaystyle}\\\\ {\\displaystyle\\left.-\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g_{z_{t}}(w_{t}^{j,\\xi}+\\mu u_{t,l})-g_{z_{t}}(w_{t}^{j,\\xi})\\right)\\nabla f_{z_{t}}(v_{t+1}^{j,\\xi})\\right>}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "With Assumption 3, the terms $\\nabla f(v_{t+1})$ and $g(w_{t})$ are both differentiable.  Thus,   \n$\\begin{array}{r}{\\frac{1}{b}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g(w_{t}+\\mu u_{t,l})-g(w_{t})\\right)\\nabla f(v_{t+1})}\\end{array}$ $\\hat{f}(w_{t})$ $\\nabla{\\hat{f}}(w_{t})\\;\\;=\\;\\;$ =1(g(w + \u03bcut,) - g(wt)f(vt+1). Then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}}\\\\ &{\\quad-\\,2\\eta_{t}\\left\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Taking derivative of $\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ over $w_{t}$ , we get that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla^{2}\\widehat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla^{2}\\widehat{f}_{\\mu_{1}},\\widehat{v}_{\\mu_{1}}(w_{t})\\right\\|}\\\\ &{=\\left\\|\\displaystyle{\\frac{1}{b}\\sum_{i=1}^{b}\\frac{u_{i}}{\\mu}\\left(\\nabla g_{\\mu_{1}}\\left(w+\\mu u_{t,i}\\right)-\\nabla g_{\\mu_{1}}\\left(w_{t}\\right)\\right)\\nabla f_{\\mu_{1}}\\left(w_{t+1}\\right)}\\right.}\\\\ &{\\quad\\left.+\\left.\\beta\\displaystyle{\\frac{1}{b}\\sum_{i=1}^{b}\\frac{u_{i}}{\\mu}\\left(g_{\\mu_{1}}\\left(w_{t}+\\mu u_{t,i}\\right)-g_{\\mu_{1}}\\left(w_{t}\\right)\\right)\\nabla^{2}f_{\\mu_{1}}\\left(w_{t+1}\\right)\\nabla g_{\\mu_{1}}^{T}\\left(w_{t}\\right)}\\right\\|}\\\\ &{\\le\\left\\|\\displaystyle{\\frac{1}{b}\\sum_{i=1}^{b}\\frac{u_{i}}{\\mu}\\left(\\nabla g_{\\mu_{1}}\\left(w_{t}+\\mu u_{t,i}\\right)-\\nabla g_{\\mu_{1}}\\left(w_{t}\\right)\\right)\\nabla f_{\\mu_{1}}\\left(w_{t+1}\\right)}\\right\\|}\\\\ &{\\quad+\\left\\|\\displaystyle{\\beta\\frac{1}{b}\\sum_{i=1}^{b}\\frac{u_{i}}{\\mu}\\left(g_{z_{i}}\\left(w_{t}+\\mu u_{t,i}\\right)-g_{z_{i}}\\left(w_{t}\\right)\\right)\\nabla^{2}f_{\\mu_{1}}\\left(v_{t+1}\\right)\\nabla g_{z_{i}}^{T}\\left(w_{t}\\right)}\\right\\|}\\\\ &{\\le\\left\\|\\left(\\partial_{x}w_{t}\\right)B_{\\mu}+M_{\\mu}^{\\prime}L_{f}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let $\\rho~~=~~\\frac{1}{\\mu}\\left(\\beta L_{g}\\alpha_{f}M_{g}+M_{g}^{\\prime}L_{f}\\right)$ , then $\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ is $\\rho$ smooth. And we can know that $\\lambda_{\\operatorname*{min}}\\left(\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right)\\geq-\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\geq-\\rho$ .Acording to Lemma 3, we can get that $\\left\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\rangle$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geq\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\Vert\\nabla\\hat{f}_{z_{i_{t}},\\xi_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\xi_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\Vert^{2}-\\rho\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}-\\nabla\\hat{f}_{z_{i_{t}},\\xi_{j_{t}}}(w_{t})+\\nabla\\hat{f}_{z_{i_{t}}\\xi_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\Vert^{2}}\\\\ &{=\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\Vert\\nabla\\hat{f}_{z_{i_{t}},\\xi_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\xi_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\Vert^{2}-\\rho\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, plugging the above inequality back into Equation (25) yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+\\left(\\eta_{t}^{2}-4\\eta_{t}^{2}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\right)\\left\\Vert\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second inequality is de to $\\begin{array}{r}{\\eta_{t}\\leq\\frac{1}{2\\rho t}\\leq\\frac{3}{2\\rho}}\\end{array}$ . The above inequality implies ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Secondly, when $j_{t}=j$ , there holds ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|}\\\\ &{=\\left\\|w_{t}-w_{t}^{j,\\bar{z}}-\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+\\eta_{t}\\left\\|\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}^{j,\\bar{z}})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+2\\eta_{t}\\left\\|\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\right\\|\\left\\|\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+\\frac{2L_{f}M_{g}}{\\mu}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|\\mathbb{I}[j_{t}\\neq j]+\\left(\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+\\frac{2L_{f}M_{g}}{\\mu}\\eta_{t}\\right)\\mathbb{I}[j_{t}=j].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Taking expectation over $i_{t}$ \uff0c ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t}}\\left[\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[j_{t}\\neq j]\\right]+\\left(\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\frac{2L_{f}M_{g}}{\\mu}\\eta_{t}\\right)\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[j_{t}=j]\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\frac{2L_{f}M_{g}}{\\mu m}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{A}\\left[\\left\\lVert\\boldsymbol{w}_{T}-\\boldsymbol{w}_{T}^{j,\\varepsilon}\\right\\rVert\\right]\\leq\\frac{1}{\\sqrt{1-2\\rho_{T}}}\\mathbb{E}_{A}\\left[\\left\\lVert\\boldsymbol{w}_{T-1}-\\boldsymbol{w}_{T-1}^{j,\\varepsilon}\\right\\rVert\\right]+\\frac{2L_{f}M_{g}}{\\mu\\rho}_{\\eta}}&{}\\\\ {\\leq\\frac{T-1}{\\sqrt{1-\\tau}}\\left(\\frac{T-1}{\\tau\\sqrt{1+\\sqrt{1-2\\rho_{T}}}\\nu}\\right)\\frac{2L_{f}M_{g}}{\\mu\\rho}_{\\eta}}&{}\\\\ {\\leq\\frac{T-1}{\\sum_{t=1}^{T}}\\left(\\frac{T-1}{\\nu\\tau+1}\\sqrt{1+\\frac{1}{\\mu^{\\tau}}}\\right)\\frac{L_{f}M_{g}}{\\mu\\rho n}\\frac{1}{t}}&{}\\\\ {\\leq\\sum_{t=1}^{T-1}\\left(\\frac{T-1}{\\sqrt{1+\\tau^{2}}\\nu}\\sqrt{1+\\frac{1}{\\mu^{\\tau}}}\\right)\\frac{L_{f}M_{g}}{\\mu\\mu n}\\frac{1}{t}}&{}\\\\ {\\leq\\underset{t=1}{\\overset{T-1}{\\sum}}\\left(\\frac{T-1}{\\nu\\tau^{2}}\\sqrt{1+\\frac{1}{\\mu^{\\tau}}}-1\\right)\\frac{L_{f}M_{g}}{\\mu\\mu n}\\frac{1}{t}}&{}\\\\ {\\leq\\sqrt{\\frac{T-1}{\\sqrt{1-\\tau}}\\exp\\left\\{\\frac{1}{t^{\\tau}}\\right\\}}\\sum_{t=1}^{T-1}\\frac{L_{f}M_{g}}{\\mu\\mu n}\\frac{1}{t}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\sqrt{\\exp\\left\\{\\displaystyle\\sum_{t^{\\prime}=2}^{T-1}\\frac{1}{t^{\\prime}-1}\\right\\}}\\frac{L_{f}M_{g}}{\\rho\\mu m}\\displaystyle\\sum_{t=1}^{T-1}\\frac{1}{t}}\\\\ &{\\leq\\!\\!\\frac{L_{f}M_{g}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho\\mu m},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the fourth inequality is from $e^{x}\\geq1+x$ ", "page_idx": 33}, {"type": "text", "text": "Next, we will study the optimization bound. According to Equation (6) in Lemma 4, $\\nabla f(w_{t})=$ $\\beta\\nabla g(w_{t})\\nabla f(v_{t+1})$ , second-order Taylor expansion, Lemmas 5 and 6, we provide that, for any $p\\geq{\\sqrt{2}}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathcal{E}_{p}(\\mathbf{t}_{i+1})-\\mathcal{E}_{p}(\\mathbf{t}_{i})\\right]}\\\\ &{=\\mathbb{E}\\left[\\Big(\\mathbf{t}_{1^{'}}\\!-\\!\\operatorname*{sup}_{t}\\nabla F_{p}(\\mathbf{t}_{i})\\Big)^{2}\\frac{1}{2}\\pi\\lVert\\mathbf{t}_{1^{'}}\\!-\\!\\mathbf{t}_{2^{'}}\\!\\rVert_{L^{2}}\\right]}\\\\ &{=\\mathbb{E}\\Bigg[\\Big(-\\eta\\nabla_{t}\\Theta_{t_{i}}\\big(\\!\\gamma\\!-\\!\\gamma\\!+\\!\\frac{1}{\\Theta_{t_{i}}}\\big)\\!\\Big)\\!-\\!\\frac{1}{2}\\pi\\lVert\\Theta_{t_{i}}(\\mathbf{t}_{i})\\rangle\\!+\\!\\frac{1}{2}\\pi\\lVert\\Theta_{t_{i}}(\\mathbf{t}_{i+1})\\!\\rVert_{\\mathcal{F}_{p}}^{\\top}\\!\\int_{U_{s}}(\\mathbf{t}_{i+1})\\!\\Big]^{2}}\\\\ &{\\times\\mathbb{E}\\Bigg[\\Big(-\\eta\\nabla_{t}\\Theta_{t_{i}}\\big(\\!\\gamma\\!-\\!\\rho_{t}(\\mathbf{t}_{i})\\big)\\!-\\!\\frac{1}{2}\\Big(\\mathbf{t}_{1^{'}}\\!)\\Big)\\!+\\!\\frac{1}{2}\\pi\\langle\\mathbf{t}_{2^{'}}\\!-\\!\\frac{1}{2}\\rangle\\!\\exp_{t_{i}}(\\mathbf{t}_{i})\\!\\Big)\\!\\Bigg]\\!\\mathcal{F}_{p}\\big[\\mathbf{t}_{i}(\\mathbf{t}_{1:1})\\big]}\\\\ &{\\quad\\times\\mathbb{E}\\big[\\mathcal{E}_{p}(\\mathbf{t}_{i})\\big]+\\frac{1}{2}\\pi\\left[\\Phi_{t_{i}}\\big(\\nabla_{y_{t}}(\\mathbf{t}_{i})\\big)\\!-\\!\\gamma\\!\\nabla_{t}\\Theta_{t_{i}}(\\mathbf{t}_{i})\\big)\\!+\\!\\gamma\\nabla_{t}\\Theta_{t_{i}}(\\mathbf{t}_{i+1})\\right]^{2}\\mathbb{E}_{f_{i}}\\!\\Bigg]\\!+\\!\\frac{1}{2}\\pi\\langle\\mathbf{t}_{i+1}|\\mathbf{t}_{i+1}\\rangle}\\\\ &{\\le\\!\\mathbb{E}\\Bigg[-\\!\\bigg(\\!\\nabla_{t}\\Theta_{t_{i}}(\\mathbf\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\alpha\\left\\lbrace\\Gamma\\left(\\Gamma_{\\mathrm{C}}\\left(\\alpha_{1}\\right)\\Gamma\\left(\\phi_{\\mathrm{r}}\\right)\\right)\\right\\rbrace}\\\\ &{\\leq\\alpha\\left\\lbrace\\overline{{\\Gamma}}\\left[\\frac{1}{2}\\right]\\left\\lbrace\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right)\\left(\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right),\\phi_{\\mathrm{r}}\\right)\\right\\rbrace\\mathcal{F}_{\\mathrm{f},\\{(\\mathrm{r}),(\\mathrm{s})\\}}\\right\\rbrace+\\frac{1}{2}\\frac{1}{2}\\|\\Gamma\\left(\\Gamma_{\\mathrm{C}}\\left(\\phi_{\\mathrm{r}}\\right)\\right)\\|^{2}+\\frac{1}{2}\\frac{1}{2}\\|\\Gamma_{\\mathrm{C}}\\left(\\phi_{\\mathrm{r}}\\right)\\|^{2}}\\\\ &{\\quad-\\left(\\phi_{\\mathrm{r}}^{+}\\right)\\frac{1}{2}\\left\\rbrace\\Gamma\\left(\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right)\\right)+\\alpha\\left\\lbrace\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right)\\left(\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right),-\\phi_{\\mathrm{r}}\\right)\\right\\rbrace\\mathcal{F}_{\\mathrm{f},\\{(\\mathrm{s})\\}}\\right\\rbrace^{2}}\\\\ &{\\quad+\\alpha\\left\\lbrace\\overline{{\\Gamma}}\\left(\\gamma_{1}\\right)\\left\\lbrace\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right)\\right\\rbrace}\\\\ &{\\quad-\\frac{1}{2}\\alpha\\left\\lbrace\\overline{{\\Gamma}}\\left(\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right),-\\left(\\frac{1}{2}\\right)\\right)\\mathcal{F}_{\\mathrm{R},(\\mathrm{r})}\\left(\\omega_{1}\\right)\\right\\rbrace\\mathcal{F}_{\\mathrm{f},(\\mathrm{s})}\\right\\rbrace}\\\\ &{\\quad+\\alpha\\left\\lbrace\\overline{{\\Gamma}}\\left(\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right),-\\overline{{\\Gamma}}\\left(\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right)\\right)\\right)\\mathcal{F}_{\\mathrm{f},(\\mathrm{s})}\\left(\\omega_{1}\\right)\\right\\rbrace^{2}\\left\\rbrace}\\\\ &{\\quad+\\alpha\\left\\lbrace\\overline{{\\Gamma}}\\left(\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right),-\\overline{{\\Gamma}}\\left(\\overline{{\\Gamma}}\\left(\\phi_{\\mathrm{r}}\\right)\\right)\\right\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\left(\\frac{\\partial}{\\partial\\alpha_{i}}\\partial_{j}\\nabla^{2}\\bar{p}_{\\alpha_{j}}(w)\\!-\\!w_{\\alpha_{i}}\\!\\right)\\!\\Bigg|_{\\mathbf{x}=\\mathbf{x}}}\\\\ &{\\quad+(\\alpha_{i}^{2}-p)\\xi[|\\nabla F_{\\alpha}(w_{1})|^{2}]}\\\\ &{\\quad+(\\alpha_{i}^{2}-p)\\xi[|\\nabla F_{\\alpha}(w_{2})|^{2}]}\\\\ &{\\quad+\\left(\\frac{\\partial}{\\partial\\alpha_{i}}\\partial_{j}^{2}\\right)\\!\\Bigg[\\Bigg\\{\\frac{1}{p_{j}}\\Bigg\\}(\\frac{\\partial}{\\partial\\alpha_{j}}\\!\\exp^{2}q_{\\alpha_{i}}(w)\\!-\\!w_{\\alpha_{i}}\\!)\\!\\Bigg|_{\\mathbf{x}=\\mathbf{x}}}\\\\ &{\\quad+E_{\\alpha_{i}}^{2}\\Bigg]\\!\\Bigg[\\Bigg\\{\\frac{1}{p_{j}}\\Bigg\\}(\\frac{\\partial}{\\partial\\alpha_{j}}\\!\\exp^{2}q_{\\alpha_{j}}(w)\\!-\\!\\frac{1}{\\partial\\alpha_{i}}\\Bigg)\\!\\Bigg]\\Bigg|^{2}}\\\\ &{\\quad+\\frac{2}{3}\\alpha_{i}^{2}\\!\\Bigg[\\Bigg\\{\\frac{1}{p_{j}}\\Bigg\\}(\\nabla_{\\alpha_{1}}\\alpha_{1}\\mathrm{,}\\partial_{j}w_{2}\\!-\\!\\left(p\\!+\\!\\frac{1}{2}\\right)\\sigma_{\\alpha_{i}}\\mathrm{,}(w)\\!\\Bigg|_{\\mathbf{x}=\\mathbf{x}}}\\\\ &{\\quad+\\frac{2}{3}\\alpha_{i}\\!\\Bigg[\\frac{1}{p_{j}}\\Bigg]\\!+\\!\\frac{5}{2}\\!\\left(\\nabla_{\\alpha_{1}}\\alpha_{1}\\mathrm{,}\\partial_{j}w_{2}\\!-\\!\\frac{1}{2}\\right)\\!\\Bigg]\\Bigg|^{2}}\\\\ &{\\quad+(\\alpha_{i}^{2}-p)\\xi[|\\nabla F_{\\alpha}(w_{1})|^{2}]}\\\\ &{\\quad\\le\\!\\Bigg(\\frac{\\partial}{\\partial\\alpha_{i}}\\!-\\!\\frac{1}{p_{j}}\\Bigg)\\xi[|\\nabla F_{\\alpha}(w)|^{2}]+\\frac{\\Delta_{i}^{2}\\mu_{i}^{2}}{3}\\alpha_{i}^{2}\\!\\Bigg(\\!\\frac{1}{p_ \n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let $d_{1}=d-2\\beta+\\beta^{2}$ and $\\begin{array}{r}{d_{2}=d-\\left(2p+1\\right)\\beta+\\left(p+\\frac{1}{2}\\right)^{2}\\beta^{2}}\\end{array}$ to get that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[F_{S}(\\boldsymbol{w}_{t+1})-F_{S}(\\boldsymbol{w}_{t})\\right]}\\\\ &{\\le\\left(\\alpha\\eta_{t}^{2}-p\\eta_{t}\\right)\\mathbb{E}\\left[\\|\\nabla F_{S}(\\boldsymbol{w}_{t})\\|^{2}\\right]+\\frac{d L_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}(2\\alpha\\eta_{t}^{2}+\\eta_{t})+\\frac{L_{g}^{2}L_{f}^{2}}{b}\\left(2\\alpha d_{1}\\eta_{t}^{2}+d_{2}\\eta_{t}\\right)}\\\\ &{\\le-\\frac{1}{2}p\\eta_{t}\\mathbb{E}\\left[\\|\\nabla F_{S}(\\boldsymbol{w}_{t})\\|^{2}\\right]+\\frac{d L_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}(2\\alpha\\eta_{t}^{2}+\\eta_{t})+\\frac{L_{g}^{2}L_{f}^{2}}{b}\\left(2\\alpha d_{1}\\eta_{t}^{2}+d_{2}\\eta_{t}\\right)}\\\\ &{\\le-p\\gamma\\eta_{t}\\mathbb{E}\\left[F_{S}(\\boldsymbol{w}_{t})-F_{S}(\\boldsymbol{w}(\\boldsymbol{S}))\\right]+\\frac{d L_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}(2\\alpha\\eta_{t}^{2}+\\eta_{t})+\\frac{L_{g}^{2}L_{f}^{2}}{b}\\left(2\\alpha d_{1}\\eta_{t}^{2}+d_{2}\\eta_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the second inequality is due to $\\begin{array}{r}{\\eta_{t}=\\frac{1}{p\\gamma t}\\,\\leq\\,\\frac{p}{2\\alpha t}\\,\\leq\\,\\frac{p}{2\\alpha}}\\end{array}$ When $\\begin{array}{r}{p\\ge\\sqrt{\\frac{2\\alpha}{\\gamma}}}\\end{array}$ and the last inequality is from Equation 8 in Lemma 4. Then, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[F_{S}(w_{t+1})-F_{S}(w(S))\\right]}\\\\ &{\\le(1-p\\gamma\\eta_{t})\\,\\mathbb{E}\\left[F_{S}(w_{t})-F_{S}(w(S))\\right]+\\frac{d L_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}(2\\alpha\\eta_{t}^{2}+\\eta_{t})+\\frac{L_{g}^{2}L_{f}^{2}}{b}\\left(2\\alpha d_{1}\\eta_{t}^{2}+d_{2}\\eta_{t}\\right)}\\\\ &{=\\left(1-\\frac{1}{t}\\right)\\mathbb{E}\\left[F_{S}(w_{t})-F_{S}(w(S))\\right]+\\frac{d L_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}\\left(\\frac{2}{p^{2}\\alpha t^{2}}+\\frac{1}{p\\alpha t}\\right)+\\frac{L_{g}^{2}L_{f}^{2}}{b}\\left(\\frac{2d_{1}}{p^{2}\\alpha t^{2}}+\\frac{d_{2}}{p\\alpha t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We multiply both sides of the above inequality by $t$ to get that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad t\\mathbb{E}\\left[F_{S}(w_{t+1})-F_{S}(w(S))\\right]}\\\\ &{\\leq\\!(t-1)\\mathbb{E}\\left[F_{S}(w_{t})-F_{S}(w(S))\\right]+\\frac{d L_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}\\left(\\frac{2}{p^{2}\\alpha t}+\\frac{1}{p\\alpha}\\right)+\\frac{L_{g}^{2}L_{f}^{2}}{b}\\left(\\frac{2d_{1}}{p^{2}\\alpha t}+\\frac{d_{2}}{p\\alpha}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then ", "page_idx": 34}, {"type": "equation", "text": "$$\n(T-1)\\mathbb{E}\\left[F_{S}(w_{T})-F_{S}(w(S))\\right]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq(T-2)\\mathbb{E}\\left[F_{S}(w_{T-1})-F_{S}(w(S))\\right]+\\frac{d L_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}\\left(\\frac{2}{p^{2}\\alpha(T-1)}+\\frac{1}{p\\alpha}\\right)+\\frac{L_{g}^{2}L_{f}^{2}}{b}\\left(\\frac{2d_{1}}{p^{2}\\alpha(T-1)}+\\frac{2L_{f}}{p^{2}\\alpha_{g}}\\right)}\\\\ &{\\leq\\!\\frac{d L_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}\\left(\\sum_{t=1}^{T-1}\\frac{2}{p^{2}\\alpha t}+\\frac{T-1}{p\\alpha}\\right)+\\frac{L_{g}^{2}L_{f}^{2}}{b}\\left(\\sum_{t=1}^{T-1}\\frac{2d_{1}}{p^{2}\\alpha t}+\\frac{d_{2}(T-1)}{p\\alpha}\\right)}\\\\ &{\\leq\\!\\frac{d L_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}\\left(\\frac{2\\log(e T)}{p^{2}\\alpha}+\\frac{T-1}{p\\alpha}\\right)+\\frac{L_{g}^{2}L_{f}^{2}}{b}\\left(\\frac{2d_{1}\\log(e T)}{p^{2}\\alpha}+\\frac{d_{2}(T-1)}{p\\alpha}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "That is ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[F_{S}(w_{T})-F_{S}(w(S))\\right]}\\\\ &{\\leq\\!\\frac{d{\\cal L}_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}\\left(\\frac{2\\log(e T)}{p^{2}\\alpha(T-1)}+\\frac{1}{p\\alpha}\\right)+\\frac{{\\cal L}_{g}^{2}{\\cal L}_{f}^{2}}{b}\\left(\\frac{2d_{1}\\log(e T)}{p^{2}\\alpha(T-1)}+\\frac{d_{2}}{p\\alpha}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combining Theorem 1, Equations (24), (26) and (27), we can get that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(w_{T})-F(w^{*})\\right]\\leq\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T+n^{-\\frac{1}{2}}+\\mu^{2}+b^{-1}d_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "SCSC: Similar to the stability proof of SCGD except for ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla^{2}\\widehat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})}\\\\ &{=\\!\\frac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\!\\frac{u_{t,l}}{\\mu}\\left(\\nabla g_{z_{i_{t}}}(w_{t}+\\mu u_{t,l})-\\nabla g_{z_{i_{t}}}(w_{t})\\right)\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})}\\\\ &{\\quad+\\displaystyle\\frac{1}{b}\\displaystyle\\sum_{l=1}^{b}\\!\\frac{u_{t,l}}{\\mu}\\left(g_{z_{i_{t}}}(w_{t}+\\mu u_{t,l})-g_{z_{i_{t}}}(w_{t})\\right)\\nabla^{2}f_{\\bar{z}_{j_{t}}}(v_{t+1})\\nabla g_{z_{i_{t}}}^{\\top}(w_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "based on the update of SCSC, we have that, for $\\begin{array}{r}{\\eta_{t}\\le\\frac{1}{2\\rho t}\\le\\frac{3}{2\\rho},\\rho=\\frac{1}{\\mu}\\;\\big(L_{g}\\alpha_{f}M_{g}+M_{g}^{\\prime}L_{f}\\big),}\\end{array}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]\\leq\\frac{L_{f}M_{g}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho\\mu n}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\right\\Vert\\right]\\leq\\frac{L_{f}M_{g}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho\\mu m}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Similar to the optimization proof of SCGD, we also have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[F_{S}(w_{T})-F_{S}(w(S))\\right]}\\\\ &{\\le\\!\\frac{d{\\cal L}_{f}^{2}\\mu^{2}\\alpha_{g}^{2}}{4(d+6)}\\left(\\frac{2\\log(e T)}{p^{2}\\alpha(T-1)}+\\frac{1}{p\\alpha}\\right)+\\frac{{\\cal L}_{g}^{2}{\\cal L}_{f}^{2}}{b}\\left(\\frac{2d_{1}\\log(e T)}{p^{2}\\alpha(T-1)}+\\frac{d_{2}}{p\\alpha}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\begin{array}{r}{d_{1}=d-1,d_{2}=d-(2p+1)+\\left(p+\\frac{1}{2}\\right)^{2}}\\end{array}$ . Combining Theorem 1, Equations (28), (29) and (30), we can get that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(w_{T})-F(w^{*})\\right]\\leq\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T+n^{-\\frac{1}{2}}+\\mu^{2}+b^{-1}d_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof of Corollary 2: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "SCGD: 1) $\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]$ : Firstly, when $i_{t}\\neq i$ , there holds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}}\\\\ &{=\\Big\\|w_{t}-w_{t}^{i,z}-\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t}^{i,z})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\Big\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\left\\lVert{\\boldsymbol{u}_{\\tau}-\\boldsymbol{u}_{\\tau^{\\prime}}^{\\prime\\prime}}\\right\\rVert^{2}+\\eta_{2}^{2}\\left\\lVert\\overline{{\\boldsymbol{g}}}_{\\alpha_{2}}(w_{t})\\boldsymbol{\\tilde{\\nabla}}f_{T_{\\alpha_{1}}}(v_{t+1})-\\boldsymbol{\\tilde{\\nabla}}g_{\\beta_{2}}(w_{t^{\\prime}}^{\\prime\\prime})\\boldsymbol{\\tilde{\\nabla}}f_{T_{\\alpha_{1}}}(v_{t+1}^{\\prime\\prime})\\right\\rVert^{2}}}\\\\ {{\\displaystyle~~-2\\eta_{\\delta}\\left\\langle{\\boldsymbol{u}_{\\tau^{\\prime}}-\\boldsymbol{u}_{\\tau^{\\prime}}^{\\prime\\prime}}\\right\\rangle\\cdot\\boldsymbol{\\tilde{\\nabla}}g_{\\alpha_{2}}(w_{t})\\boldsymbol{\\tilde{\\nabla}}f_{T_{\\alpha_{1}}}(v_{t+1})-\\boldsymbol{\\tilde{\\nabla}}g_{\\alpha_{2}}(w_{t^{\\prime}}^{\\prime\\prime})\\boldsymbol{\\tilde{\\nabla}}f_{T_{\\alpha_{1}}}(v_{t+1}^{\\prime\\prime})}}\\\\ {{\\displaystyle=\\left\\lVert{\\boldsymbol{u}_{\\tau}-\\boldsymbol{u}_{\\tau^{\\prime}}^{\\prime\\prime}}\\right\\rVert^{2}+\\eta_{1}^{2}\\left\\lVert\\overline{{\\boldsymbol{g}}}_{\\alpha_{2}}^{\\prime}\\boldsymbol{\\tilde{\\nabla}}\\cdot\\overline{{\\boldsymbol{u}}}_{\\tau^{\\prime}}\\right\\rVert^{2}\\left(f_{T_{\\alpha_{1}}}(v_{t+1}+\\mu u_{\\tau_{1}})-f_{T_{\\alpha_{1}}}(v_{t+1})\\right)}\\\\ {{\\displaystyle=\\frac{b}{b}\\frac{1}{b}\\boldsymbol{u}_{\\tau}}\\Big(g_{T_{\\alpha_{1}}}(w_{t}+\\mu u_{\\tau_{1}})-g_{T_{\\alpha_{1}}}(w_{t})-{\\frac{1}{b}}\\frac{b}{L_{T}}\\frac{u_{\\tau_{1}}}{L_{T}}\\Big(f_{T_{\\alpha_{1}}}(v_{t+1}^{\\prime}+\\mu u_{\\tau_{1}})-f_{T_{\\alpha_{1}}}(v_{t+1}^{\\prime\\prime})\\Big)}\\\\ {{\\displaystyle~~\\sum_{l=1}^{b}\\frac{u_{\\tau_{l}}}{\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "With \u03b2 Assumption 3,  the terms $f(v_{t+1})$ and $g(w_{t})$ are both differentiable. Thus, $\\begin{array}{r}{\\frac{1}{b^{2}}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f(v_{t+1}+\\mu u_{t,l})-f(v_{t+1})\\right)\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g(w_{t}+\\mu u_{t,l})-g(w_{t})\\right)}\\end{array}$ I \\*t (wt + ut,) - (rwt) i also dfferentiable. It is reasonable to assume that there exists a primitive function $\\hat{f}(w_{t})$ at least whose derivative function is  \u2265i=1 $\\begin{array}{r}{\\frac{1}{b^{2}}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f(v_{t+1}+\\mu u_{t,l})-f(v_{t+1})\\right)\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g(w_{t}+\\mu u_{t,l})-g(w_{t})\\right)}\\end{array}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}}\\\\ &{\\quad-\\left2\\eta_{t}\\left\\langle w_{t}-w_{t}^{i,z},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Taking derivative of $\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ over $w_{t}$ , we get that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla^{2}\\hat{f}_{z_{i},\\bar{z}_{j_{t}}}(w_{t})}}\\\\ {{\\displaystyle=\\frac{\\beta}{b^{2}}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\nabla g_{z_{i}}(w_{t})\\left(\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g_{z_{i_{t}}}(w_{t}+\\mu u_{t,l})-g_{z_{i_{t}}}(w_{t})\\right)}}\\\\ {{\\displaystyle\\;\\;+\\frac{1}{b^{2}}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(\\nabla g_{z_{i_{t}}}(w_{t}+\\mu u_{t,l})-\\nabla g_{z_{i_{t}}}(w_{t})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla^{2}\\hat{f}_{z_{t},z_{t}},\\hat{w}_{t}(w_{t})\\right\\|}\\\\ &{=\\left\\|\\frac{\\beta}{b^{2}}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\nabla g_{z_{t}}(w_{t})\\left(\\nabla f_{z_{t}}\\left(v_{t+1}+\\mu u_{t,l}\\right)-\\nabla f_{z_{t}}(v_{t+1})\\right)\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g_{z_{t}}\\left(w_{t}+\\mu u_{t,l}\\right)-g_{z_{t}}\\left(w_{t,l}\\right)\\right)\\right\\|}\\\\ &{\\quad+\\displaystyle\\frac{1}{b^{2}}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{z_{t}}\\left(v_{t+1}+\\mu u_{t,l}\\right)-f_{z_{t}}\\left(v_{t+1}\\right)\\right)\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(\\nabla g_{z_{t}}\\left(w_{t}+\\mu u_{t,l}\\right)-\\nabla g_{z_{t}}\\left(w_{t}\\right)\\right)\\right\\|}\\\\ &{\\leq\\left\\|\\frac{\\beta}{b^{2}}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\nabla g_{z_{t}}\\left(w_{t}\\right)\\left(\\nabla f_{z_{t}}\\left(v_{t+1}+\\mu u_{t,l}\\right)-\\nabla f_{z_{t}}\\left(v_{t+1}\\right)\\right)\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g_{z_{t}}\\left(w_{t}+\\mu u_{t,l}\\right)-g_{z_{t}}\\left(w_{t,l}\\right)\\right)\\right\\|}\\\\ &{\\quad+\\left\\|\\displaystyle\\frac{1}{b^{2}}\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{z_{t}}\\left(v_{t+1}+\\mu u_{t,l}\\right)-f_{z_{t}}\\left(v_{t+1}\\right)\\right)\\displaystyle\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(\\nabla g_{z_{t}}\\left(w_{t}+\\mu u_{t,l}\\right)-\\nabla g_{z_{t}}\\left(w_{t}\\right)\\right)\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\leq\\!\\frac{1}{\\mu^{2}}\\left(\\beta L_{g}M_{f}^{\\prime}M_{g}+M_{f}M_{g}^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let $\\rho~=~{\\textstyle{\\frac{1}{\\mu^{2}}}}\\left(\\beta L_{g}M_{f}^{\\prime}M_{g}+M_{f}M_{g}^{\\prime}\\right)$ , then $\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ is $\\rho$ smooth. And we can know that $\\lambda_{\\operatorname*{min}}\\left(\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right)\\geq-\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\geq-\\rho$ According to Lemma 3, we can get that $\\begin{array}{r l}&{\\quad\\left\\langle w_{t}-w_{t}^{i,z},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\rangle}\\\\ &{\\geq\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}-\\rho\\left\\|w_{t}-w_{t}^{i,z}-\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})+\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}}\\\\ &{=\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{i,z})\\right\\|^{2}-\\rho\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|^{2}.}\\end{array}$ ", "page_idx": 37}, {"type": "text", "text": "Now, plugging the above inequality back into Equation (31) yields ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert^{2}+\\left(\\eta_{t}^{2}-4\\eta_{t}^{2}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\right)\\left\\Vert\\nabla\\hat{f}_{z_{i_{t}},\\tilde{z}_{t_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\tilde{z}_{t_{t}}}(w_{t}^{i,z})\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the second inequality is due t $\\begin{array}{r}{\\eta_{t}\\leq\\frac{1}{2\\rho t}\\leq\\frac{3}{2\\rho}}\\end{array}$ The above inequality implies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{i,z}\\right\\|,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Secondly, when $i_{t}=i$ , there holds ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|}\\\\ &{=\\left\\|w_{t}-w_{t}^{i,z}-\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\tilde{\\nabla}g_{\\bar{z}_{i_{t}}^{\\prime}}(w_{t}^{i,z})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\eta_{t}\\left\\|\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\tilde{\\nabla}g_{z_{i_{t}}^{\\prime}}(w_{t}^{i,z})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1}^{i,z})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+2\\eta_{t}\\left\\|\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\right\\|\\left\\|\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\frac{2M_{f}M_{g}}{\\mu^{2}}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{i,z}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{i,z}\\right\\|\\mathbb{I}[i_{t}\\neq i]+\\left(\\left\\|w_{t}-w_{t}^{i,z}\\right\\|+\\frac{2M_{f}M_{g}}{\\mu^{2}}\\eta_{t}\\right)\\mathbb{I}[i_{t}=i].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Taking expectation over $i_{t}$ \uff0c ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t}}\\left[\\left\\Vert w_{t+1}-w_{t+1}^{i,z}\\right\\Vert\\right]}\\\\ &{\\leq\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}\\neq i]\\right]+\\left(\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert+\\frac{2M_{f}M_{g}}{\\mu^{2}}\\eta_{t}\\right)\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}=i]\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{i,z}\\right\\Vert+\\frac{2M_{f}M_{g}}{\\mu^{2}n}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]\\leq\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\mathbb{E}_{A}\\left[\\left\\Vert w_{T-1}-w_{T-1}^{i,z}\\right\\Vert\\right]+\\frac{2M_{f}M_{g}}{\\mu^{2}n}\\eta_{t}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{t=1}^{T-1}\\left(\\prod_{t^{\\prime}=t+1}^{T-1}\\frac{1}{\\sqrt{1-2\\rho\\eta_{t^{\\prime}}}}\\right)\\frac{2M_{f}M_{g}}{\\mu^{2}n}\\eta_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle{\\sum_{t=1}^{T-1}\\left(\\frac{T^{1}-1}{t\\sqrt{t-1}}\\sqrt{1+\\frac{1}{t^{\\prime}-1}}\\right)\\frac{M_{f}M_{g}}{\\rho\\mu^{2}n}\\frac{1}{t}}}\\\\ &{\\le\\displaystyle{\\sum_{t=1}^{T-1}\\left(\\prod_{t=2}^{T-1}\\sqrt{1+\\frac{1}{t^{\\prime}-1}}\\right)\\frac{M_{f}M_{g}}{\\rho\\mu^{2}n}\\frac{1}{t}}}\\\\ &{\\le\\displaystyle{\\sqrt{\\prod_{t=2}^{T-1}\\exp\\left\\{\\frac{1}{t^{\\prime}-1}\\right\\}}\\sum_{t=1}^{T-1}\\frac{M_{f}M_{g}}{\\rho\\mu^{2}n}\\frac{1}{t}}}\\\\ &{\\le\\sqrt{\\exp\\left\\{\\frac{T^{1}-1}{t^{\\prime}-1}\\right\\}}\\frac{M_{f}M_{g}}{\\rho\\mu^{2}n}\\displaystyle{\\sum_{t=1}^{T-1}\\frac{1}{t}}}\\\\ &{\\le\\frac{M_{f}M_{g}(c T)^{\\frac{1}{2}}\\log(c T)}{\\rho\\mu^{2}n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the fourth inequality is from $e^{x}\\geq1+x$ ", "page_idx": 38}, {"type": "text", "text": "$\\mathbf{2})\\,\\mathbb{E}_{A}\\,\\Big[\\Big\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\Big\\Vert\\Big]$ : Firstly, when $j_{t}\\ne j$ , there holds ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad=\\displaystyle\\Big\\lVert\\mathbf{x}_{t}-\\mathbf{u}_{t}^{\\parallel}\\mathbf{x}_{t}\\Big\\rVert^{2}}\\\\ &{=\\displaystyle\\Big\\lVert\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\parallel}\\mathbf{\\bar{d}}^{2}-\\boldsymbol{n}\\nabla_{\\theta_{t}}\\Big(w_{t}\\Big)\\nabla_{t}J_{u_{t}}(v_{t+1})+\\mathbf{u}_{\\theta}\\nabla_{\\theta_{t}}(w_{t}^{\\perp})\\nabla_{t}J_{v_{t}}(v_{t+1}^{\\perp})\\Big\\rVert^{2}}\\\\ &{=\\displaystyle\\Big\\lVert\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\parallel}\\Big\\rVert^{2}+\\eta_{t}^{2}\\Big\\lVert\\nabla_{\\theta_{t}}(w_{t+1})\\nabla_{t}J_{u_{t}}(v_{t+1})-\\nabla_{\\theta_{t}}(w_{t}^{\\perp})\\nabla_{t}J_{v_{t}}(v_{t+1}^{\\perp})\\Big\\rVert^{2}}\\\\ &{\\quad-2\\eta_{t}\\left<w_{t}-w_{t}^{\\parallel}\\nabla_{t}v_{t-1}\\otimes J_{u_{t}}(w_{t}^{\\perp})\\nabla_{t}J_{v_{t}}(v_{t+1})-\\nabla_{\\theta_{t}}(w_{t}^{\\perp})\\nabla_{t}J_{u_{t}}(v_{t+1}^{\\perp})\\right>}\\\\ &{=\\displaystyle\\Big\\lVert\\mathbf{u}_{t}-w_{t}^{\\parallel}\\mathbf{u}_{t}^{\\parallel}\\Big\\rVert^{2}+\\eta_{t}^{2}\\displaystyle\\lVert\\frac{1}{\\beta}\\sum_{t=1}^{M}w_{t}^{\\perp}\\left(J_{u_{t}}(v_{t+1}+m_{t})-J_{u_{t}}(w_{t+1}+1)\\right)}\\\\ &{\\quad\\displaystyle\\sum_{t=1}^{N}w_{t}^{\\perp}\\left(\\theta_{t},(w_{t+1}+m_{t})-J_{v_{t}-1}(w_{t})\\right)-\\frac{1}{\\beta}\\sum_{t=1}^{N}w_{t}^{\\perp}\\left(J_{u_{t}}(v_{t+1}^{\\perp}+m_{t}u_{t})-J_{u_{t}}(v_{t+1}^{\\perp})\\right)}\\\\ &{\\stackrel{\\mathrm{win}}{\\sum_{t=1}^{N}w_{\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "With \u03b2 Assumption 3,  the terms $\\nabla f(v_{t+1})$ and $g(w_{t})$ are both differentiable. Thus, $\\begin{array}{r}{\\frac{1}{b^{2}}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f(v_{t+1}+\\mu u_{t,l})-f(v_{t+1})\\right)\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g(w_{t}+\\mu u_{t,l})-g(w_{t})\\right)}\\end{array}$ It is reasonable to assume that there exists a primitive function $\\hat{f}(w_{t})$ at least whose derivative function is  Zi=1 $\\begin{array}{r}{\\frac{1}{b^{2}}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f(v_{t+1}+\\mu u_{t,l})-f(v_{t+1})\\right)\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g(w_{t}+\\mu u_{t,l})-g(w_{t})\\right)}\\end{array}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}}\\\\ &{\\quad-\\,2\\eta_{t}\\left\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})-\\nabla\\hat{f}_{z_{i_{t}}\\bar{z}_{j_{t}}}(w_{t}^{j,\\bar{z}})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Taking derivative of $\\nabla\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ over $w_{t}$ , we get that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=\\frac{\\beta}{b^{2}}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\nabla g_{z_{i_{t}}}(w_{t})\\left(\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g_{z_{i_{t}}}(w_{t}+\\mu u_{t,l})-g_{z_{i_{t}}}(w_{t})\\right)}\\\\ &{\\displaystyle\\ +\\frac{1}{b^{2}}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(\\nabla g_{z_{i_{t}}}(w_{t}+\\mu u_{t,l})-\\nabla g_{z_{i_{t}}}(w_{t})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\nabla^{2}\\hat{f}_{\\hat{\\varepsilon}_{1},\\hat{\\varepsilon}_{1}}(w_{t})\\right\\|}\\\\ {\\displaystyle=\\left\\|\\frac{\\beta}{\\hat{\\varepsilon}^{2}}\\sum_{l=1}^{\\hat{M}_{l},\\hat{\\varepsilon}_{1}}\\frac{u_{\\hat{\\varepsilon}_{1}}(w_{t})}{\\mu}\\left(\\nabla f_{\\hat{\\varepsilon}_{l}}(v_{t+1}+\\mu u_{t})-\\nabla f_{\\hat{\\varepsilon}_{l}}(v_{t+1})\\right)\\sum_{l=1}^{\\hat{M}_{l}}\\frac{u_{\\hat{\\varepsilon}_{l},l}}{\\mu}\\left(g_{\\varepsilon_{l}}(w_{t}+\\mu u_{t})-g_{\\varepsilon_{l}}\\right)\\right.}\\\\ {\\displaystyle\\quad+\\left.\\frac{1}{b^{2}}\\sum_{l=1}^{\\hat{M}_{l}}\\frac{u_{\\hat{\\varepsilon}_{l},l}}{\\mu}\\left(f_{\\hat{\\varepsilon}_{l}}(v_{t+1}+\\mu u_{t})-f_{\\hat{\\varepsilon}_{l}}(v_{t+1})\\right)\\sum_{l=1}^{\\hat{M}_{l},\\hat{\\varepsilon}_{1}}\\left(\\nabla g_{\\hat{\\varepsilon}_{l}}(w_{t}+\\mu u_{t})-\\nabla g_{\\hat{\\varepsilon}_{l}}(w_{t})\\right)\\right\\|}\\\\ {\\displaystyle\\leq\\left\\|\\frac{\\beta}{b^{2}}\\sum_{l=1}^{\\hat{M}_{l},\\hat{\\varepsilon}_{1}}\\frac{u_{\\hat{\\varepsilon}_{l}}(v_{t})}{\\mu}\\left(\\nabla f_{\\hat{\\varepsilon}_{l}}(v_{t+1}+\\mu u_{t})-\\nabla f_{\\hat{\\varepsilon}_{l}}(v_{t+1})\\right)\\sum_{l=1}^{\\hat{M}_{l},\\hat{\\varepsilon}_{1}}\\frac{u_{\\hat{\\varepsilon}_{l},l}}{\\mu}\\left(g_{\\hat{\\varepsilon}_{l}}(w_{t}+\\mu u_{t})-g_{\\hat{\\varepsilon}_{l}}\\right)\\right.}\\\\ {\\displaystyle\\left.\\quad+\\left\\|\\frac{1}{b^{2}}\\sum_{l=1}^{\\hat{M}_{l}}\\frac{u_{\\hat{\\varepsilon}_{l}}}{\\mu}\\left(f_{\\hat{\\varepsilon}_{l}}(v_{t+1}+\\mu u_{t})-f_{\\hat{\\varepsilon}_{l}}(v \n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let $\\rho~=~{\\textstyle{\\frac{1}{\\mu^{2}}}}\\left(\\beta L_{g}M_{f}^{\\prime}M_{g}+M_{f}M_{g}^{\\prime}\\right)$ , then $\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})$ is $\\rho$ -smooth. And we can know that $\\lambda_{\\operatorname*{min}}\\left(\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right)\\geq-\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\geq-\\rho$ According to Lemma 3, we can get that $\\begin{array}{r l}&{\\quad\\langle w_{t}-w_{t}^{j,\\bar{z}},\\nabla\\hat{f}_{z_{t_{i}},\\bar{z}_{t_{j}}}(w_{t})-\\nabla\\hat{f}_{z_{t_{i}}\\bar{z}_{t_{i}}}(w_{t}^{j,\\bar{z}})\\rangle}\\\\ &{\\geq\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{t_{i}},\\bar{z}_{t}}(w_{t})-\\nabla\\hat{f}_{z_{t_{i}}\\bar{z}_{t}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}-\\rho\\left\\|w_{t}-w_{t}^{j,\\bar{z}}-\\nabla\\hat{f}_{z_{t_{i}},\\bar{z}_{t}}(w_{t})+\\nabla\\hat{f}_{z_{t_{i}}\\bar{z}_{t}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}}\\\\ &{=\\!2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\|\\nabla\\hat{f}_{z_{t_{i}},\\bar{z}_{t}}(w_{t})-\\nabla\\hat{f}_{z_{t_{i}}\\bar{z}_{t}}(w_{t}^{j,\\bar{z}})\\right\\|^{2}-\\rho\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|^{2}.}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Now, plugging the above inequality back into Equation (33) yields ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+\\left(\\eta_{t}^{2}-4\\eta_{t}^{2}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\right)\\left\\Vert\\nabla\\hat{f}_{z_{t_{t}},\\bar{z}_{t_{t}}}(w_{t})\\nabla\\hat{f}_{z_{t_{t}}\\bar{z}_{t_{t}}}(w_{t}^{j,\\bar{z}})\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2}}\\\\ &{\\leq\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the second inequality is due to $\\begin{array}{r}{\\eta_{t}\\leq\\frac{1}{2\\rho t}\\leq\\frac{3}{2\\rho}}\\end{array}$ . The above inequality implies ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Secondly, when $j_{t}=j$ , there holds ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\|}\\\\ &{=\\left\\|w_{t}-w_{t}^{j,\\bar{z}}-\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})+\\eta_{t}\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}^{\\prime}}(v_{t+1}^{j,\\bar{z}})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+\\eta_{t}\\left\\|\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})-\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t}^{j,\\bar{z}})\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}^{\\prime}}(v_{t+1}^{j,\\bar{z}})\\right\\|}\\\\ &{\\leq\\left\\|w_{t}-w_{t}^{j,\\bar{z}}\\right\\|+2\\eta_{t}\\left\\|\\tilde{\\nabla}g_{z_{i_{t}}}(w_{t})\\right\\|\\left\\|\\tilde{\\nabla}f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\leq\\bigg\\|w_{t}-w_{t}^{j,\\bar{z}}\\bigg\\|+\\frac{2M_{f}M_{g}}{\\mu^{2}}\\eta_{t}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert\\mathbb{I}[j_{t}\\neq j]+\\left(\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\frac{2M_{f}M_{g}}{\\mu^{2}}\\eta_{t}\\right)\\mathbb{I}[j_{t}=j].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Taking expectation over $i_{t}$ \uff0c ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t}}\\left[\\left\\Vert w_{t+1}-w_{t+1}^{j,\\bar{z}}\\right\\Vert\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[j_{t}\\neq j]\\right]+\\left(\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\frac{2M_{f}M_{g}}{\\mu^{2}}\\eta_{t}\\right)\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[j_{t}=j]\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\Vert w_{t}-w_{t}^{j,\\bar{z}}\\right\\Vert+\\frac{2M_{f}M_{g}}{\\mu^{2}m}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{4}\\left[\\left\\|w_{T}-w_{T}^{k}\\right\\|^{2}\\right]\\leq\\frac{c_{1}^{m}-1}{\\sqrt{2}}\\rho\\mathbb{E}_{4}\\left[\\left\\|w_{T}-w_{t-1}^{k}\\right\\|^{2}\\right]+\\frac{2(\\ensuremath{\\mathbb{J}}_{2}\\cdot\\ensuremath{\\boldsymbol{h}}_{2})}{\\rho^{2m}\\rho}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T-1}\\left(\\frac{\\gamma-1}{\\sqrt{2}+1}\\frac{1}{\\sqrt{1-2\\rho}}\\right)\\frac{2\\ensuremath{\\mathbb{J}}_{2}\\ensuremath{\\boldsymbol{h}}_{2}}{\\mu^{2m}\\rho}}\\\\ &{\\phantom{2p c}\\geq\\displaystyle\\sum_{t=1}^{T-1}\\left(\\frac{\\gamma-1}{\\sqrt{1-t}}\\sqrt{1+\\frac{1}{t}}\\right)\\frac{\\ensuremath{\\mathbb{J}}_{2}\\ensuremath{\\boldsymbol{h}}_{2}}{\\mu^{2m}\\rho}\\frac{1}{L}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T-1}\\left(\\frac{\\gamma-1}{\\sqrt{1-2}}\\sqrt{1+\\frac{1}{t}}\\right)\\frac{\\ensuremath{\\mathbb{J}}_{2}\\ensuremath{\\boldsymbol{h}}_{2}}{\\mu^{2m}\\rho}\\frac{1}{L}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T-1}\\left(\\frac{\\gamma-1}{\\sqrt{1-2}}\\sqrt{1+\\frac{1}{t}}\\right)\\frac{\\ensuremath{\\mathbb{J}}_{2}\\ensuremath{\\boldsymbol{h}}_{2}}{\\mu^{2m}\\rho}\\frac{1}{L}}\\\\ &{\\leq\\displaystyle\\sqrt{\\prod_{t=1}^{T-1}\\log\\left\\{\\frac{1}{t}\\right\\}}\\sum_{t=1}^{T-1}\\frac{\\ensuremath{\\mathbb{J}}_{2}\\ensuremath{\\boldsymbol{h}}_{2}}{\\mu^{2m}\\rho^{2}\\mu}}\\\\ &{\\leq\\displaystyle\\sqrt{\\exp\\left\\{\\frac{\\gamma-1}{\\sqrt{2}}\\frac{1}{t}\\right\\}}\\frac{1}{\\rho^{2\\mu}\\rho}\\frac{1}{L^{2\\mu}}}\\\\ &{\\leq\\displaystyle\\frac{\\ensuremath{\\mathbb{J}}_{2}\\ensuremath{\\boldsymbol{h}}_{2}}{\\mu^{2}\\rho}\\frac{1}{L^{2\\mu}}\\mathrm{e}^{\\boldsymbol{(T)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the fourth inequality is from $e^{x}\\geq1+x$ ", "page_idx": 40}, {"type": "text", "text": "As for the optimization analysis of the full black-box SCGD, we can combine the proofs of Theorem 4 and Corollary 1 to get that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F_{S}(w_{T})-F_{S}(w(S))\\right]\\le\\mathcal{O}\\left(\\mu^{4}+d_{2}^{2}+b^{-1}d_{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\begin{array}{r}{d_{2}=d-2\\sqrt{\\left(p+\\frac{1}{2}\\right)\\beta}+\\left(p+\\frac{1}{2}\\right)\\beta}\\end{array}$ . Combining Theorem 1, Equations (32),(34) and (35), we can get that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(w_{T})-F(w^{*})\\right]\\leq\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T+n^{-\\frac{1}{2}}+\\mu^{4}+b^{-2}d_{2}^{2}+b^{-1}d_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "SCSC: Similar to the proof of SCGD except for ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla^{2}\\hat{f}_{z_{i_{t}},z_{j_{t}}}(w_{t})}}\\\\ {{\\displaystyle=\\frac{1}{b^{2}}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\nabla g_{z_{i_{t}}}(w_{t})\\left(\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-\\nabla f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(g_{z_{i_{t}}}(w_{t}+\\mu u_{t,l})-g_{z_{i_{t}}}(w_{t})\\right)}}\\\\ {{\\displaystyle\\;\\;+\\frac{1}{b^{2}}\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(f_{\\bar{z}_{j_{t}}}(v_{t+1}+\\mu u_{t,l})-f_{\\bar{z}_{j_{t}}}(v_{t+1})\\right)\\sum_{l=1}^{b}\\frac{u_{t,l}}{\\mu}\\left(\\nabla g_{z_{i_{t}}}(w_{t}+\\mu u_{t,l})-\\nabla g_{z_{i_{t}}}(w_{t})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "based on the update of SCsC, we have that, for $\\begin{array}{r}{\\eta_{t}\\le\\frac{1}{2\\rho t}\\le\\frac{3}{2\\rho},\\rho=\\frac{1}{\\mu^{2}}\\,\\Big(L_{g}M_{f}^{\\prime}M_{g}+M_{f}M_{g}^{\\prime}\\Big),}\\end{array}$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{i,z}\\right\\Vert\\right]\\leq\\frac{M_{f}M_{g}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho\\mu^{2}n}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left[\\left\\Vert w_{T}-w_{T}^{j,\\bar{z}}\\right\\Vert\\right]\\leq\\frac{M_{f}M_{g}(e T)^{\\frac{1}{2}}\\log(e T)}{\\rho\\mu^{2}m}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "As for the optimization analysis of the full black-box SCGD, we can combine the proofs of Theorem 4 and Corollary 1 to get that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F_{S}(w_{T})-F_{S}(w(S))\\right]\\le\\mathcal{O}\\left(\\mu^{4}+b^{-2}d_{2}^{2}+b^{-1}d_{2}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\begin{array}{r}{d_{2}=d-2\\sqrt{\\left(p+\\frac{1}{2}\\right)}+\\left(p+\\frac{1}{2}\\right)}\\end{array}$ . Combining Theorem 1, Equations (36), (37) and (38), we can get that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(w_{T})-F(w^{*})\\right]\\leq\\mathcal{O}\\left(\\left(n^{-1}+m^{-1}\\right)T^{\\frac{1}{2}}\\log T+n^{-\\frac{1}{2}}+\\mu^{4}+b^{-2}d_{2}^{2}+b^{-1}d_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "E Proofs of Applications ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Before stating our remain proofs, it should be noted that there are a few differences between the setting of FOO-based VFL (VFL-CZOFO) and the one of SCGD (SCSC). First of all, we set $S=\\{z_{1},...,z_{n}\\}$ and $S^{i,z}=\\{z_{1},...,z_{i-1},z_{i}^{\\prime},z_{i+1},...,z_{n}\\}$ according to the learning paradigm of VFL. Secondly, the update of the outer model (global model) for FOO-based VFL (VFL-CZOFO) is not based on the simple weighted summation of SCGD (SCSC). Luckily, these differences will not make a difference in our proofs. ", "page_idx": 41}, {"type": "text", "text": "Proof of Corollary 4: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "FOO-based VFL: Considering the independence of all clients, we just prove the corresponding result of the $k$ -th client for some $k\\in[K]$ Firstly, when $i_{t}\\neq i$ thereholds ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}^{k}-w_{t+1}^{i,k}\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}^{k}-w_{t}^{i,k}-\\eta_{t}\\nabla g(w_{t}^{k})\\nabla f(g(w_{t}^{k}))+\\eta_{t}\\nabla g(w_{t}^{i,k})\\nabla f(g(w_{t}^{i,k}))\\right\\|^{2}}\\\\ &{=\\left\\|w_{t}^{k}-w_{t}^{i,k}\\right\\|^{2}+\\eta_{t}^{2}\\left\\|\\nabla g(w_{t}^{k})\\nabla f(g(w_{t}^{k}))-\\nabla g(w_{t}^{i,k})\\nabla f(g(w_{t}^{i,k}))\\right\\|^{2}}\\\\ &{\\quad-\\left.2\\eta_{t}\\left\\langle w_{t}^{k}-w_{t}^{i,k},\\nabla g(w_{t}^{k})\\nabla f(g(w_{t}^{k}))-\\nabla g(w_{t}^{i,k})\\nabla f(g(w_{t}^{i,k}))\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Where $w_{t}^{i,z,k}$ is simplied as $w_{t}^{i,k}$ With ssumption 3 the termns $\\nabla g(w_{t}^{k})$ and $\\nabla f(g(w_{t}^{k}))$ are both differentiable. Thus, $\\nabla g(w_{t}^{k})\\nabla f(g(w_{t}^{k}))$ is also differentiable, which means that it is continuous on its domain. As we all know, a continuous function has primitive functions. Then, it is reasonable to assume that there exists a primitive function $\\hat{f}(w_{t}^{k})$ at least whose derivative function $\\nabla\\hat{f}(w_{t}^{k})=$ $\\nabla g(w_{t}^{k})\\nabla f(g(w_{t}^{k}))$ . For example, considering the independence between $v_{t}$ and $g(w_{t}^{k})$ $f(g(w_{t}^{k}))$ is one of the primitive functions $\\hat{f}(w_{t}^{k})$ . Then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\Vert w_{t+1}^{k}-w_{t+1}^{i,k}\\right\\Vert^{2}}\\\\ &{=\\left\\Vert w_{t}^{k}-w_{t}^{i,k}\\right\\Vert^{2}+\\eta_{t}^{2}\\left\\Vert\\nabla\\hat{f}(w_{t}^{k})\\nabla\\hat{f}(w_{t}^{i,k})\\right\\Vert^{2}-2\\eta_{t}\\left\\langle w_{t}^{k}-w_{t}^{i,k},\\nabla\\hat{f}(w_{t}^{k})-\\nabla\\hat{f}(w_{t}^{i,k})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Taking derivative of $\\nabla\\hat{f}(w_{t}^{k})$ over $w_{t}^{k}$ , we get that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\nabla^{2}\\hat{f}(w_{t}^{k})=\\nabla^{2}g(w_{t}^{k})\\nabla f(g(w_{t}^{k}))+\\left(\\nabla g(w_{t}^{k})\\right)^{2}\\nabla^{2}f(g(w_{t}^{k})).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|\\nabla^{2}\\hat{f}_{z_{i_{t}},\\bar{z}_{j_{t}}}(w_{t})\\right\\|\\le\\alpha_{g}L_{f}+L_{g}^{2}\\alpha_{f}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Let $\\rho\\,=\\,\\alpha_{g}L_{f}\\,+\\,L_{g}^{2}\\alpha_{f}$ , then $\\hat{f}(w_{t}^{k})$ .s $\\rho$ -smooth. And we can know that $\\lambda_{\\operatorname*{min}}\\left(\\nabla^{2}\\hat{f}(w_{t}^{k})\\right)\\;\\geq$ $-\\left\\|\\nabla^{2}\\hat{f}(w_{t}^{k})\\right\\|\\geq-\\rho$ . According to Lemma 3, we can get that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\langle w_{t}^{k}-w_{t}^{i,k},\\nabla\\hat{f}(w_{t}^{k})-\\nabla\\hat{f}(w_{t}^{i,k})\\right\\rangle}\\\\ &{\\geq2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\Vert\\nabla\\hat{f}(w_{t}^{k})-\\nabla\\hat{f}(w_{t}^{i,k})\\right\\Vert^{2}-\\rho\\left\\Vert w_{t}^{k}-w_{t}^{i,k}-\\nabla\\hat{f}(w_{t}^{k})+\\nabla\\hat{f}(w_{t}^{i,k})\\right\\Vert^{2}}\\\\ &{=2\\eta_{t}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\left\\Vert\\nabla\\hat{f}(w_{t}^{k})-\\nabla\\hat{f}(w_{t}^{i,k})\\right\\Vert^{2}-\\rho\\left\\Vert w_{t+1}^{k}-w_{t+1}^{i,k}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Now, plugging the above inequality back into Equation (39) yields ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\Vert w_{t+1}^{k}-w_{t+1}^{i,k}\\right\\Vert^{2}}\\\\ &{\\le\\left\\Vert w_{t}^{k}-w_{t}^{i,k}\\right\\Vert^{2}+\\left(\\eta_{t}^{2}-4\\eta_{t}^{2}\\left(1-\\frac{\\rho\\eta_{t}}{2}\\right)\\right)\\left\\Vert\\nabla\\hat{f}(w_{t}^{k})\\nabla\\hat{f}(w_{t}^{i,k})\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}^{k}-w_{t+1}^{i,k}\\right\\Vert^{2}}\\\\ &{\\le\\left\\Vert w_{t}^{k}-w_{t}^{i,k}\\right\\Vert^{2}+2\\rho\\eta_{t}\\left\\Vert w_{t+1}^{k}-w_{t+1}^{i,k}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the second inequalityis due to $\\begin{array}{r}{\\eta_{t}\\leq\\frac{1}{2\\rho t}\\leq\\frac{3}{2\\rho}}\\end{array}$ . The above inequality implies ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}^{k}-w_{t+1}^{i,k}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}^{k}-w_{t}^{i,k}\\right\\|,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Secondly, when $i_{t}=i$ , there holds ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|w_{t+1}^{k}-w_{t+1}^{i,k}\\right\\|}\\\\ &{=\\left\\|w_{t}^{k}-w_{t}^{i,k}-\\eta_{t}\\nabla g(w_{t}^{k})\\nabla f(g(w_{t}^{k}))+\\eta_{t}\\nabla g(w_{t}^{i,k})\\nabla f(g(w_{t}^{i,k}))\\right\\|}\\\\ &{\\leq\\left\\|w_{t}^{k}-w_{t}^{i,k}\\right\\|+\\eta_{t}\\left\\|\\nabla g(w_{t}^{k})\\nabla f(g(w_{t}^{k}))-\\nabla g(w_{t}^{i,k})\\nabla f(g(w_{t}^{i,k}))\\right\\|}\\\\ &{\\leq\\left\\|w_{t}^{k}-w_{t}^{i,k}\\right\\|+2\\eta_{t}\\left\\|\\nabla g(w_{t}^{k})\\right\\|\\left\\|\\nabla f(g(w_{t}^{k}))\\right\\|}\\\\ &{\\leq\\left\\|w_{t}^{k}-w_{t}^{i,k}\\right\\|+2L_{g}L_{f}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Combining the above two cases, we can get that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|w_{t+1}^{k}-w_{t+1}^{i,k}\\right\\|\\leq\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}^{k}-w_{t}^{i,k}\\right\\|\\mathbb{I}[i_{t}\\neq i]+\\left(\\left\\|w_{t}^{k}-w_{t}^{i,k}\\right\\|+2L_{g}L_{f}\\eta_{t}\\right)\\mathbb{I}[i_{t}=i].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Taking expectation over $i_{t}$ \uff0c ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t}}\\left[\\left\\|w_{t+1}^{k}-w_{t+1}^{i,k}\\right\\|\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}^{k}-w_{t}^{i,k}\\right\\|\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}\\ne i]\\right]+\\left(\\left\\|w_{t}^{k}-w_{t}^{i,k}\\right\\|+2L_{g}L_{f}\\eta_{t}\\right)\\mathbb{E}_{i_{t}}\\left[\\mathbb{I}[i_{t}=i]\\right]}\\\\ &{\\le\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\left\\|w_{t}^{k}-w_{t}^{i,k}\\right\\|+\\frac{2L_{g}L_{f}}{n}\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then, taking expectation over $A$ and taking summation from $t=1$ to $T-1$ to get that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{A}\\left[\\left\\|w_{T}^{k}-w_{T}^{i,k}\\right\\|\\right]\\leq\\!\\!\\frac{1}{\\sqrt{1-2\\rho\\eta_{t}}}\\mathbb{E}_{A}\\left[\\left\\|w_{T-1}^{k}-w_{T-1}^{i,k}\\right\\|\\right]+\\frac{2L_{g}L_{f}}{n}\\eta_{t}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{t=1}^{T-1}\\left(\\prod_{t^{\\prime}=t+1}^{T-1}\\frac{1}{\\sqrt{1-2\\rho\\eta_{t^{\\prime}}}}\\right)\\frac{2L_{g}L_{f}}{n}\\eta_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\underset{t=1}{\\overset{T-1}{\\sum}}\\left(\\underset{\\nu=1}{\\overset{T-1}{\\sum}}\\sqrt{1+\\frac{1}{t^{\\prime}-1}}\\right)\\frac{L_{\\theta}L_{f}}{\\rho n}\\frac{1}{t}}\\\\ &{\\leq\\underset{t=1}{\\overset{T-1}{\\sum}}\\left(\\underset{\\nu=2}{\\overset{T-1}{\\prod}}\\sqrt{1+\\frac{1}{t^{\\prime}-1}}\\right)\\frac{L_{\\theta}L_{f}}{\\rho n}\\frac{1}{t}}\\\\ &{\\leq\\sqrt{\\underset{t=2}{\\overset{T-1}{\\prod}}\\exp\\left\\{\\frac{1}{t^{\\prime}-1}\\right\\}}\\underset{t=1}{\\overset{T-1}{\\sum}}\\frac{L_{\\theta}L_{f}}{\\rho n}\\frac{1}{t}}\\\\ &{\\leq\\sqrt{\\exp\\left\\{\\frac{1}{t-1}\\frac{1}{t^{\\prime}-1}\\right\\}}\\frac{L_{\\theta}L_{f}}{\\rho n}\\underset{t=1}{\\overset{T-1}{\\sum}}\\frac{1}{t}}\\\\ &{\\leq\\frac{L_{\\theta}L_{f}(c T)^{\\frac{1}{\\lambda}}\\log(c T)}{\\rho n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the fourth inequality is from $e^{x}\\geq1+x$ . Combining Theorem 3 and Equation (40), we can getthat ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[|F(w_{T}^{k})-F_{S}(w_{T}^{k})|\\right]\\leq\\mathcal{O}\\left(n^{-1}T^{\\frac{1}{2}}\\log T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof of Corollary 5: Similar to the proofs of Theorem 4 and Corollary 4, we can get the result of Corollary5. ", "page_idx": 43}, {"type": "text", "text": "F  Key Challenges and Technical Tools ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In this section, the key challenges and technical tools of extending the theoretical analysis for SCO problems from white-box cases to black-box cases are listed as follows. ", "page_idx": 43}, {"type": "text", "text": "(1) Generalization: Considering three different types of black-box SCO methods, we apply our new non-convex analysis (Theorem 3) to these cases (Theorem 4, Corollary 1 and 2) in Section 3.2. Due to the difference related to function form, there are some differences related to the upper bounds of first-order and second-order gradients of ${\\tilde{\\nabla}}f$ between Theorem 3 and the generalization part of Theorem 4. The differences among Theorem 3 and Corollary 1, Corollary 2 are the same as Theorem 4. ", "page_idx": 43}, {"type": "text", "text": "(2) Optimization: For optimization, the estimated gradient does introduce several extra terms regarding the accuracy of the gradient estimation, i.e., $\\tilde{\\nabla}f\\mathrm{~-~}(\\boldsymbol{p}+1/2)\\beta\\nabla f$ and $\\tilde{\\nabla}f\\mathrm{~-~}\\beta\\nabla f$ These terms are derived from some special strategies (such as a special decomposition $\\tilde{\\nabla}f\\,=$ $\\tilde{\\nabla}f+(p+1/2)\\beta\\nabla f-(p+1/2)\\beta\\nabla f)$ We propose an extended lemma (Lemma 6) from [39] and combine this lemma with these strategies to limit the expansion of $\\mathbb{E}[F_{S}(w_{t+1})-F_{S}(w(S))]$ during the iterations. Otherwise, these extra terms will lead to the divergence of our result. ", "page_idx": 43}, {"type": "text", "text": "Finally, we want to emphasize our advantages compared with previous work related to the generalization guarantee of SCO [21]. ", "page_idx": 43}, {"type": "text", "text": "(1) Better results: For convex optimization, Theorem 2 leverages the co-coercivity property of convex and smooth function to provide the stability bound $O((\\bar{n^{-1}}+m^{-1})\\beta\\log T)$ under milder parameter selection than [21]. And our proof is more concise since it avoids the intermediate step which measures the distance between $v$ and $g(w)$ in the analysis of [21]. ", "page_idx": 43}, {"type": "text", "text": "(2) Non-convex guarantee: We leverage a special lemma, almost co-coercivity lemma, to develop our proof framework to non-convex case to obtain the first stability bound $\\mathcal{O}((n^{\\frac{.}{-1}}+m^{-1})T^{\\frac{1}{2}}\\log\\bar{T})$ under milder parameter selection than [39]. ", "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: The paper's contributions and scope can be found at the end of the abstract and introduction. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [No] ", "page_idx": 44}, {"type": "text", "text": "Justification: Despite the limitation isn't discussed in a separate \"Limitations\" section, some remarks of results have demonstrated the limitation. For example, Remark 4 shows that Theorem 3 is looser than Theorem 2. Remark 5 shows that Theorem 4 is derived from a more stringent condition, i.e., a smaller learning rate. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: The full set of assumption and a complete (and correct) proof are provided in the Section 2 and Appendices C, D, E. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective. There isn't any data or code. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective. It theoretically explains the impact of black-box on the learning guarantees of SCO algorithms, which may benefit the algorithm designing of SCO algorithm. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective without any data or model being released. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective. The algorithms analyzed in the paper have been cited properly. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective without any new assets being released. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper's contributions are from the theoretical analysis perspective. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 49}]