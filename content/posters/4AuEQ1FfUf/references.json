{"references": [{"fullname_first_author": "Saeed Ghadimi", "paper_title": "Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization", "publication_date": "2016-00-00", "reason": "This paper lays the groundwork for mini-batch stochastic approximation methods for non-convex stochastic composite optimization, which is foundational to the understanding of stochastic compositional optimization (SCO)."}, {"fullname_first_author": "Mengdi Wang", "paper_title": "Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions", "publication_date": "2017-00-00", "reason": "This paper introduces the stochastic compositional gradient descent (SCGD) algorithm, a core algorithm analyzed and extended in the target paper."}, {"fullname_first_author": "Tianyi Chen", "paper_title": "Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization", "publication_date": "2021-00-00", "reason": "This paper introduces the stochastically corrected stochastic compositional gradient method (SCSC), another core algorithm that is analyzed and extended in the target paper."}, {"fullname_first_author": "Moritz Hardt", "paper_title": "Train faster, generalize better: Stability of stochastic gradient descent", "publication_date": "2016-00-00", "reason": "This paper establishes the foundation of using algorithmic stability to analyze the generalization performance of stochastic gradient descent, a technique heavily used in the target paper's analysis."}, {"fullname_first_author": "Ming Yang", "paper_title": "Stability and generalization of stochastic compositional gradient descent algorithms", "publication_date": "2023-00-00", "reason": "This paper is the most closely related work, providing a generalization guarantee for SCGD and SCSC using algorithmic stability, which the target paper builds upon and improves."}]}