[{"figure_path": "pLoX8Og3bH/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Viewpoint diversity of the existing MVSeg dataset [21] and the new MVUAV dataset. (b) & (c) Representative samples from the MVSeg & MVUAV datasets, where RGB videos, thermal videos, and the corresponding semantic annotations are visualized.", "description": "This figure illustrates the difference in viewpoint between the existing MVSeg dataset and the newly proposed MVUAV dataset.  (a) shows a schematic of the data acquisition process, highlighting that MVSeg uses an eye-level perspective while MVUAV uses an oblique bird's-eye view from a UAV. (b) and (c) provide visual examples of data from both datasets, showcasing RGB and thermal video frames alongside their corresponding semantic annotations. The difference in viewpoint and the broader range of scenes captured by MVUAV are visually apparent.", "section": "1 Introduction"}, {"figure_path": "pLoX8Og3bH/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of training examples in semi-supervised MVSS setting, where a limited amount of sparsely labeled RGB-Thermal (RGBT) videos and massive unlabeled ones are utilized.", "description": "This figure illustrates the data used for training the SemiMV model.  It shows two sets of RGB-Thermal video clips. The left side shows a small number of clips with sparse annotations (only the last frame in each clip is annotated).  The right side shows a large number of unlabeled video clips. This semi-supervised learning approach uses the labeled data to guide the learning process for the unlabeled data, improving efficiency and potentially performance.", "section": "2 Related Work"}, {"figure_path": "pLoX8Og3bH/figures/figures_3_1.jpg", "caption": "Figure 3: Illustrations of the proposed MVUAV dataset. (a) Taxonomic system and its histogram distribution showing the number of annotated frames across different categories. (b) Examples of multispectral UAV videos and corresponding annotations in both daytime and nighttime scenarios.", "description": "This figure presents the taxonomic system of the MVUAV dataset, showing the distribution of annotated frames across different object categories and infrastructure types.  The visualization is a circular chart where each slice represents a category and the length corresponds to the number of frames. A second part shows example frames from the dataset, showcasing RGB, thermal and annotation layers, with examples provided for both daytime and nighttime conditions to highlight the dataset's variety. The examples show the oblique bird's-eye view perspective offered by UAV capture and the diversity of real-world scenes included.", "section": "3 The MVUAV Dataset"}, {"figure_path": "pLoX8Og3bH/figures/figures_5_1.jpg", "caption": "Figure 4: Overview of proposed method. For simplicity, the supervised losses are omitted. The C3L loss Lc3l (Eq. 2) aims to learn from unlabeled RGB-Thermal pairs. The DMR is responsible for integrating temporal information from the denoised memory bank to update query features. A dual-C3L loss (Eq. 7) is further applied to regularize updated query features. Finally, a segmentation head predicts the final mask Pfinal. The dotted \\ means stop gradient.", "description": "This figure illustrates the architecture of the proposed SemiMV framework for semi-supervised multispectral video semantic segmentation.  It shows how RGB and thermal image sequences are processed through parallel networks.  The Cross-collaborative Consistency Learning (C3L) module leverages both labeled and unlabeled data to improve segmentation performance. A denoised memory read module incorporates temporal information from previous frames, improving prediction accuracy.  The final segmentation mask is produced by a dual-C3L loss and a segmentation head.", "section": "4.2 Proposed SemiMV Framework"}, {"figure_path": "pLoX8Og3bH/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative results on MVSeg dataset. We highlight the details with the yellow boxes.", "description": "This figure compares the qualitative results of different semantic segmentation methods on the MVSeg dataset.  The top row shows RGB and thermal infrared (TIR) frames of a scene, followed by the segmentation results of several methods (SupOnly, UniMatch, IFR, MVNet, Ours) and the ground truth (GTs). Yellow boxes highlight areas where the methods differ, showcasing the relative performance and capabilities of each model in detail, specifically in low-light conditions.", "section": "5 Experiments"}]