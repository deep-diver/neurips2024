[{"type": "text", "text": "Reciprocal Reward Influence Encourages Cooperation From Self-Interested Agents ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "John L. Zhou Weizhe Hong Jonathan C. Kao University of California, Los Angeles john.ly.zhou@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cooperation between self-interested individuals is a widespread phenomenon in the natural world, but remains elusive in interactions between artificially intelligent agents. Instead, na\u00efve reinforcement learning algorithms typically converge to Pareto-dominated outcomes in even the simplest of social dilemmas. An emerging literature on opponent shaping has demonstrated the ability to reach prosocial outcomes by influencing the learning of other agents. However, such methods differentiate through the learning step of other agents or optimize for meta-game dynamics, which rely on privileged access to opponents\u2019 learning algorithms or exponential sample complexity, respectively. To provide a learning rule-agnostic and sample-efficient alternative, we introduce Reciprocators, reinforcement learning agents which are intrinsically motivated to reciprocate the influence of opponents\u2019 actions on their returns. This approach seeks to modify other agents\u2019 $Q.$ -values by increasing their return following beneficial actions (with respect to the Reciprocator) and decreasing it after detrimental actions, guiding them towards mutually beneficial actions without directly differentiating through a model of their policy. We show that Reciprocators can be used to promote cooperation in temporally extended social dilemmas during simultaneous learning. Our code is available at https://github.com/johnlyzhou/reciprocator/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many species exhibit cooperative behaviors of remarkable variety and complexity. Even among prosocial animals, however, humans are especially notable for their ability to cooperate with unrelated individuals and maintain that cooperation even in highly adversarial environments (Melis & Semmann, 2010). These qualities are often credited with the development of human technology, culture, and advanced cognition (Burkart et al., 2014). As artificially intelligent (AI) agents become more commonplace in human society, it is increasingly important to ensure that they share similar prosocial qualities so that interactions between AI agents, as well as between AI agents and humans, may converge to mutually beneficial outcomes. ", "page_idx": 0}, {"type": "text", "text": "However, state-of-the-art reinforcement learning (RL) methods are typically designed for the single agent setting and are ill-suited for the nonstationarities introduced by multiple agents learning simultaneously. Treating other agents as fixed elements of the environment, referred to as the \u201cna\u00efve\u201d learning (NL) approach, can destabilize training and produce collectively suboptimal outcomes. This is particularly evident in a class of games known as sequential social dilemmas (SSDs), which contain tradeoffs between collective and individual return (Leibo et al., 2017). SSDs present a particularly challenging problem for multi-agent RL (MARL) because of this mixed motivational structure, which precludes the use of common centralized training algorithms designed for cooperative settings (Kraemer & Banerjee, 2016; Sunehag et al., 2017; Gupta et al., 2017; Rashid et al., 2018). ", "page_idx": 0}, {"type": "text", "text": "Previous work has shown that NL agents optimizing only for their individual returns converge to non-cooperative, Pareto-dominated outcomes in even the simplest of SSDs (Foerster et al., 2018). ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose an intrinsic reward that encourages agents called Reciprocators to learn a tit-for-tat-like strategy which reciprocates the influence that others exert on their returns. We first define value influence, a notion of influence that quantifies the effect that one agent\u2019s action has on another\u2019s expected return. Given a pair of agents, a Reciprocator $r c$ and another learning agent $i$ , we track the cumulative influence of agent $i$ \u2019s sequence of actions on $r c$ \u2019s expected return, which we refer to as the influence balance \u201cowed\u201d to $r c$ by agent $i$ . At each time step, the Reciprocator receives an additional intrinsic reciprocal reward proportional to the product of its current influence balance with agent $i$ and the value influence of its action on agent $i$ . This encourages the Reciprocator to take actions whose influence matches the sign of the influence balance: for example, if agent $i$ has produced a net positive influence on $r c$ , then $r c$ will be rewarded for actions that have a reciprocally positive influence on agent $i$ \u2019s expected return. Mechanistically, our method seeks to manipulate the opponent\u2019s $Q$ -values by altering their expected return in particular directions depending on their actions. A reward-maximizing agent $i$ should then be incentivized to take mutually beneficial actions and avoid harmful externalities. ", "page_idx": 1}, {"type": "text", "text": "The contributions of this work are as follows: (1) We formulate a novel intrinsic reward that encourages an agent to incentivize cooperation from other, simultaneously learning agents without modifying the structure of the environment, computing higher-order derivatives, or learning metagame dynamics. (2) Agents trained with this reward achieve state-of-the-art cooperative outcomes in sequential social dilemmas and are able to shape purely self-interested na\u00efve learners into mutually beneficial behavior. (3) Reciprocators demonstrate resistance to exploitation by higher-order baselines, despite using only first-order reinforcement learning algorithms. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In order to improve convergence towards Pareto-optimal solutions among independently learning agents, previous work has modified agents\u2019 reward structures to explicitly consider either group or percapita return in order to promote cooperative behavior, e.g., via inequity aversion (Hughes et al., 2018), \u201cempathic\u201d harm reduction (Bussmann et al., 2019), or \u201caltruistic\u201d gradient adjustments (Li et al., 2024). While agents that abide by such restrictions may seek to reduce their harmful externalities, they have no way of regulating the behavior of other, less magnanimous agents, including purely self-interested \u201cexploiters\u201d in the worst case (Agapiou et al., 2023). Their efficacy in SSDs, as well as in most other types of multi-agent systems, is therefore limited in the absence of strong guarantees over the other agents\u2019 altruistic tendencies. ", "page_idx": 1}, {"type": "text", "text": "2.1 Cooperation through Influence ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A more robust form of cooperation can be achieved by actively exerting influence over other agents in the environment rather than unilaterally adopting a prosocial policy, especially in SSDs where extrinsic rewards incentivize defection. Jaques et al. (2019) provides an intrinsic reward for \u201csocial influence\u201d on other agents, which is computed as the Kullback-Leibler (KL) divergence between an opponent\u2019s policy distribution conditioned on an influence agent\u2019s action $a^{i}$ and a counterfactual distribution which marginalizes out that influence agent. However, as a metric-agnostic quantity, KL divergence is unable to capture key details of how the action distribution changes (Park et al., 2024). In particular, this definition of influence cannot selectively modify opponents\u2019 behavior with respect to extrinsic task returns or state transition dynamics, and therefore has limited utility as a mechanism to incentivize cooperation. ", "page_idx": 1}, {"type": "text", "text": "Other works seek to directly influence opponent returns via the social mechanisms of reward and punishment, which are believed to assist in stabilizing cooperative relationships by controlling freeriding, cheating, and other antisocial behaviors (Melis & Semmann, 2010). Incorporating these mechanisms into groups of agents, whether by providing rewards to incentivize good behavior (Yang et al., 2020), or doling out punishments to discourage bad behavior (Schmid et al., 2021; Yaman et al., 2023), has been shown to encourage cooperation among independent agents in a variety of SSDs. However, these methods require extensions of the original action space that allow agents to directly modify other agents\u2019 rewards. On the other hand, we instead propose to quantify the influence of each naturally available action $a\\in A$ in a given state $s$ on another agent\u2019s expected return $R$ , and use this influence as the medium for reciprocation. As we later show, selectively rewarding or punishing an opponent\u2019s actions can be seen as a form of opponent shaping which seeks to manipulate the $Q$ -value of given state-action pair in order to influence the likelihood of that action in future policies. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Opponent Shaping ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Considering future policies points to a key issue with the canonical reinforcement learning (RL) framework, in that it optimizes only for the expected return within a single episode of the environment. However, taking actions that seek to optimize the long-term behavior of the other agents in the environment will not receive any immediate feedback within an episode, and must wait for one, or possibly many, learning steps. Opponent-shaping methods of this kind address this issue by differentiating across pairs of episodes (Yang et al., 2020), through opponents\u2019 gradient updates (Foerster et al., 2018; Zhao et al., 2022; Willi et al., 2022), or repeated sequences of learning steps organized into \u201cmeta-episodes\u201d (Lu et al., 2022). While these methods have demonstrated convergence to cooperative behavior in simple SSDs, they are impractical or intractable in realistic multi-agent scenarios, requiring additional independent action channels for providing incentives, white-box access to the learning rules and gradients of other agents, or exponential sample complexity (Fung et al., 2023) to learn the dynamics of meta-games, respectively. ", "page_idx": 2}, {"type": "text", "text": "In particular, the Learning with Opponent-Learning Awareness (Foerster et al., 2018, LOLA) class of approaches differentiate through the opponent\u2019s learning update but have only been demonstrated to work with full access to either the opponent\u2019s policy gradients and Hessians or their policy parameters. When using gradient approximations and modeling the opponent\u2019s policy, LOLA with opponent modeling (LOLA-OM) showed significantly worse results, even against vanilla policy gradients. Modeling efforts become even more implausible when faced with modern RL techniques, which employ adaptive optimizers that set different per-weight learning rates (Kingma & Ba, 2017, Adam), randomized experience replay (Schaul et al., 2016), and various auxiliary terms such as policy divergence penalties (Schulman et al., 2017) and entropy exploration bonuses (Williams, 1992; Mnih et al., 2016) to improve learning. Model-Free Opponent Shaping (Lu et al., 2022, MFOS) was introduced as a more general meta-learning approach that requires neither privileged access to nor inconsistent assumptions on opponent learning rules. However, MFOS\u2019s meta-policies are trained across multiple training runs repeated in sequence, each of which are treated as a single \u201cmeta-episode,\u201d in order to learn how to exploit opponent learning dynamics. This requires the ability to freely perform rollouts in the environment against opponents whose policies can be repeatedly reset to initialization, an unrealistic assumption in environments with partially adversarial motivations. ", "page_idx": 2}, {"type": "text", "text": "Our work is conceptually most similar to Learning with Opponent Q-Learning Awareness (Aghajohari et al., 2024, LOQA), which also seeks to shape opponent policies by influencing the $Q$ -values for different actions, under the assumption that opponents are Q-learners. However, LOQA still differentiates through a model of the opponent\u2019s policy and optimizes according to the joint advantage function $A(s_{t},a_{1},a_{2})$ computed with respect to the state-value function $\\bar{V}\\bar{(s_{t})}$ . On the other hand, we use a counterfactual baseline that marginalizes out only the opponent\u2019s action to perform agentspecific credit assignment and use an intrinsic reward instead of gradients over opponent policies to encourage agents to influence the opponent\u2019s $Q$ -values in the correct direction. Most importantly, LOQA focuses on the problem of learning a general end-policy that performs well against a variety of other agents at evaluation and has therefore only demonstrated the ability to shape other LOQA opponents in a controlled self-play scenario, while our method focuses on shaping the policies of diverse agents over multiple episodes of simultaneous learning. ", "page_idx": 2}, {"type": "text", "text": "In the context of these higher-order opponent-shaping approaches, we position our intrinsic reciprocal reward as a form of immediate, within-episode feedback that encourages otherwise na\u00efve learners to implicitly consider the long-term, cross-episode effects of their actions on the policy changes of other agents, demonstrating many of the properties of higher-order opponent-shaping methods while using only first-order reinforcement learning algorithms and standard rollout-based training procedures. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We construct a series of sequential social dilemmas which can each be described as a stochastic game $G$ , defined by a tuple $G=\\langle S,A,P,r,n,\\gamma\\rangle$ . In $G$ , $n$ agents, indexed by $i\\in\\{1,\\ldots,n\\}$ , observe the state of the environment $s\\in S$ and simultaneously choose actions $a^{i}\\in A$ to form a joint action $\\pmb{a}\\in\\pmb{A}\\equiv A^{n}$ . The environment then undergoes a change according to the state transition function $P(s^{\\prime}|s,\\pmb{a}):S\\times\\pmb{A}\\times S\\rightarrow[0,1]$ . Each agent receives an individual reward $r^{i}\\,=\\,r^{i}(s,a)$ , and future rewards are discounted at each time step by a discount factor $\\gamma\\in[0,1]$ . We consider the fully observable setting where agents have access to the full state of the environment $s\\in S$ at every time step, joint action $\\pmb{a}\\in\\pmb{A}$ , and rewards received by each agent. Each agent $i$ conditions a stochastic recurrent policy $\\pi^{i}(a^{i}|\\tau^{i})$ on its action-observation history, which is denoted as $\\tau^{i}\\in T\\equiv(S\\times A)^{*}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 Reciprocal Reward Influence ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 1-Step Value Influence ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In order to compute a general measure of value-based influence, we modify an intrinsic exploration reward proposed by Wang et al. (2020) called Value of Interaction (VoI). VoI measures how much agent $i$ \u2019s actions affect agent $j$ \u2019s expected return by computing the difference between the $Q_{\\mathrm{{}}}$ -value conditioned on the joint state and action $Q_{j}^{\\pi}\\left(s,a\\right)$ and a counterfactual baseline $Q_{j\\mid i}^{\\pi}\\left(s,a^{-i}\\right)$ which marginalizes out the state and action of the influencing agent $i$ to compute a \u201cdefault\u201d expected return [Equation 1]. Here, the bolded $\\pi$ superscript indicates the dependence of these $Q$ -functions on the parameters of all agents\u2019 policies. The VoI can be decomposed into an immediate influence term $r(s,{\\pmb a})\\mathrm{~-~}r(s,{\\pmb a}^{-i})$ and a discounted future influence term computed over changes in state transition probabilities [Equation 2]. Although the original VoI was formulated as an expectation over trajectories $\\tau$ and used as a regularizer, we modify it to compute the one-step Value Influence $(V I)$ where $V o I=\\mathbb{E}_{\\tau}[V I]$ , allowing us to quantify the influence of individual actions ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{V I_{i|j}^{\\pi}\\left(s,\\boldsymbol{a}\\right)=Q_{j}^{\\pi}\\left(s,\\boldsymbol{a}\\right)-Q_{j|i}^{\\pi}\\left(s,\\boldsymbol{a}^{-i}\\right)}}\\\\ &{}&{\\quad=r(s,\\boldsymbol{a})-r(s,\\boldsymbol{a}^{-i})+\\gamma\\displaystyle\\sum_{s^{\\prime}}\\left(1-\\frac{p^{\\pi^{i}}\\left(s^{\\prime}\\mid\\boldsymbol{s},\\boldsymbol{a}^{-i}\\right)}{p\\left(s^{\\prime}\\mid\\boldsymbol{s},\\boldsymbol{a}\\right)}\\right)V(s^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $r(s,\\mathbf{a}^{-i})=\\mathbb{E}_{a^{i}\\sim\\pi^{i}}\\left[r(s,(\\mathbf{a}^{-i},a^{i}))\\right]$ and $p(s^{\\prime}|s,\\mathbf{a}^{-i})=\\mathbb{E}_{a^{i}\\sim\\pi^{i}}\\left[p^{\\pi^{i}}(s^{\\prime}|s,(\\mathbf{a}^{-i},a^{i}))\\right]\\!.$ . This definition of influence relies on the notion of a counterfactual baseline to assign credit to a particular agent\u2019s action while holding all other agents\u2019 actions constant. This counterfactual baseline is so named because it estimates the counterfactual expected return if the agent\u2019s true action is replaced with a \u201cdefault\u201d action, which is computed by marginalizing out the agent\u2019s action to get the expected on-policy return. This baseline return is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ_{j\\mid i}^{\\pi}\\left(s,a^{-i}\\right)=\\mathbb{E}_{a^{i}\\sim\\pi^{i}}\\left[Q_{j}^{\\pi}(s,(a^{-i},a^{i}))\\right]=\\sum_{a_{i}}\\pi^{i}(a_{i}|\\tau^{i})Q_{j}^{\\pi}(s,(a^{-i},a^{i})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In practice, we approximate this by regressing towards the $Q$ -value while masking out $a_{i}$ from the joint state-action input. Marginalizing out an agent\u2019s action to quantify influence is a common paradigm in MARL, having also been used to define 1-step adversarial power (Li & Dennis, 2023), which estimates the maximum reduction in agent $j$ \u2019s expected reward that can be achieved by agent $i$ as the minimum $V I_{i|j}^{\\pi}(s,a)$ over all $a^{i}\\in A$ . Similarly, Counterfactual Multi-Agent policy gradients (Foerster et al., 2017, COMA) marginalizes out a single agent\u2019s action to assess the advantage (i.e., influence) of that agent\u2019s selected action relative to the counterfactual on-policy return ceteris paribus. Note that the COMA advantage function [Equation 4] can be expressed a special case of $V I_{i\\vert j}^{\\bar{\\pi}}\\left(s,\\pmb{a}\\right)$ where $i=j$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{i}(s,{\\mathbf{a}})=Q_{i}(s,\\mathbf{a})-\\sum_{a_{i}}\\pi^{i}(a_{i}|\\tau^{i})Q_{i}^{\\pi}(s,(a^{-i},a^{i})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4.2 Keeping Score with Influence Balances ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The amount of influence that a Reciprocator is able to exert in a single time step is heavily environmentdependent, so that it may not always be possible to immediately reciprocate previous influences. To encourage reciprocation over extended timescales, we continuously accumulate a measure of net ", "page_idx": 3}, {"type": "text", "text": "Initialize agent rc\u2019s policy parameters $\\theta_{\\pi}$ , VI target function parameters $\\phi$ , influence balance vector   \n$B_{r c|i}=\\mathbf{0}$ , policy memory $\\mathcal{M}=\\emptyset$ , and influence memory $\\mathcal{H}=\\emptyset$   \nfor each episode $e$ do Observe initial state $s_{0}$ for $t=1$ to $T$ do Choose action $a_{t}\\sim\\pi_{\\theta}{(s_{t})}$ Observe at, rt, st+1 Store transition tuple $(s_{t},a_{t},r_{t},s_{t+1})$ in $\\mathcal{M}$ and joint transition tuple $\\left({{s_{t}},{a_{t}},{r_{t}},{s_{t+1}}}\\right)$ in $\\mathcal{H}$ end for Update influence target function parameters $\\phi$ with $\\mathcal{H}$ every $k$ episodes Compute reciprocal rewards $r_{r c|i}^{R}(1),...\\,,r_{r c|i}^{R^{}}(T)$ w.r.t. agent $i$ and sum with rewards in $\\mathcal{M}$ Compute advantage estimates $\\hat{A}_{1},\\hdots,\\hat{A}_{T}$ for $K$ epochs do Optimize the surrogate PPO-clip objective w.r.t. $\\theta_{\\pi}$ end for Reset $\\mathcal{M}=\\emptyset$   \nend for ", "page_idx": 4}, {"type": "text", "text": "influence over sequences of actions. We draw inspiration from the \u201cdebit\u201d tally used by approximate Markov tit-for-tat (Lerer & Peysakhovich, 2018, amTFT), which tracks the cumulative advantage gained by an opponent compared to a fully cooperative baseline policy known a priori. ", "page_idx": 4}, {"type": "text", "text": "Extending this idea to our framework, we sum agent $i$ \u2019s influence $V I_{i|r c}^{\\pi}$ on the Reciprocator\u2019s expected return at each timestep rather than its own. However, using influence in only one direction as motivation for reciprocation can lead to continuous punishment or rewarding without a way to settle the score. To mitigate this, we also accumulate the net influence $V I_{r c|i}^{\\pmb{\\pi}}$ in the opposite direction and subtract it from the influence balance at every timestep as a way to \u201cpay off\u201d the balance, limiting the degree to which reciprocation is rewarded. Formally, we define the influence balance $B_{r c|i}(t)$ maintained by a Reciprocator $r c$ with another agent $i$ at time $t$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nB_{r c|i}(t)=B_{r c|i}(t-1)+[V I_{i|r c}^{\\pi}(s_{t},\\pmb{a}_{t})-V I_{r c|i}^{\\pi}(s_{t},\\pmb{a}_{t})].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The influence balance can be thought of as a score of net influence over time between agents, and can be used to motivate reciprocation in the correct direction, i.e., either positive reinforcement of net positive influence or positive punishment of net negative influence. ", "page_idx": 4}, {"type": "text", "text": "4.3 Intrinsic Reciprocal Reward ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "If agent $i$ takes a series of actions that improves the expected return of the Reciprocator over a baseline estimate, i.e., produces a net positive influence balance, then the Reciprocator should be motivated to reinforce this behavior by exerting a reciprocal positive influence on agent $i$ \u2019s expected return $R_{i}$ , in order to encourage a higher likelihood of that behavior during policy updates. Similar logic applies for detrimental deviations, negative influence balance, and subsequent reciprocal punishment. We then define the intrinsic reciprocal reward $r_{r c|i}^{R}(t)$ received by $r c$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{r c|i}^{R}(t)=B_{r c|i}(t-1)\\cdot V I_{r c|i}^{\\pi}\\left(s_{t},\\pmb{a}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Taking the product of existing influence balance and current action\u2019s VI encourages the Reciprocator to take actions that reinforce agent $i$ \u2019s behavior in the correct direction by matching signs, and scales the reward by the magnitude of the outstanding influence balance and the reinforcing influence. The intrinsic reward is then added to the agent\u2019s extrinsic reward to form the total reward used in training. ", "page_idx": 4}, {"type": "text", "text": "4.4 Policy Optimization with Reciprocal Rewards ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use experience replay to periodically update target networks and iteratively update our counterfactual baseline estimates towards these target values (Mnih et al., 2015). This provides two key ", "page_idx": 4}, {"type": "image", "img_path": "Vq2kzpig8v/tmp/7fa456ee4a40ed03e3989c04e62d07f1f304d5f0404f4058682006b1fc84cbf3.jpg", "img_caption": ["(a) Rewards for IPD. ", "(b) Rewards for Coins. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 1: (a) The first number in each cell denotes the reward received by the agent taking the row action, and the second the reward received by the agent taking the column action, where C: cooperate (stay silent) and D: defect (confess). (b) Two agents (red and blue) are tasked with collecting randomly spawning coins. If an agent collects its own coin, it receives a reward of $+1$ (left). If an agent collects another\u2019s coin, then it receives a reward of $+1$ but the other agent receives a punishment of -2. ", "page_idx": 5}, {"type": "text", "text": "beneftis: first, periodically updating these policy-dependent functions stabilizes training and allows us to approximately ignore their gradients, and therefore the gradient of the intrinsic reciprocal reward, with respect to the agents\u2019 policy parameters (Wang et al., 2020). With this assumption, we are able to use standard policy gradient methods to train our agents to jointly optimize the combined extrinsic and intrinsic rewards. ", "page_idx": 5}, {"type": "text", "text": "Second, drawing samples from multiple previous episodes to the train the counterfactual baseline target functions makes the Reciprocator less susceptible to exploitation. If the counterfactual estimators were updated concurrently with the policy using only the most recent on-policy data, then the Reciprocator\u2019s baseline would be immediately adjusted to its opponent\u2019s new policy after each episode. Because assessment of influence hinges on these counterfactual baselines, updating them too frequently would allow adversaries to easily manipulate these estimates of on-policy returns. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct experiments using two commonly used SSDs of varied complexity to demonstrate the shaping abilities of Reciprocators against other types of learning agents. For IPD, we consider memory-1 iterated games as in Foerster et al. (2018) and Lu et al. (2022), following the proof from Press & Dyson (2012) that longer-memory strategies provide no advantage over strategies conditioned on shorter memories. We use two methods to evaluate head-to-head performance in IPD: allowing agents to directly differentiate through the analytic, closed-form solution of the game as originally derived in Foerster et al. (2018), and more standard batched policy rollouts for a fixed episode length. We append \u201canalytic\u201d and \u201crollout\u201d to the game names to denote the evaluation method used. For Coins, we augment the observation given to the critics and value influence estimators with the time remaining in the episode to prevent state aliasing and stabilize learning from experience replay (Pardo et al., 2022), but do not provide them as input to the policy networks. ", "page_idx": 5}, {"type": "text", "text": "5.1 Sequential Social Dilemmas ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Iterated Prisoners\u2019 Dilemma (IPD): The iterated prisoners\u2019 dilemma (IPD) is a temporally extended version of the classical thought experiment, in which two prisoners are given a choice to either stay silent/cooperate (C) or confess/defect (D) with rewards given in Table 1a. The Pareto-optimal strategy is for both agents to cooperate by maintaining their silence, but the only Nash equilibrium (in the non-iterated, single-shot case) is mutual defection. ", "page_idx": 5}, {"type": "text", "text": "Coins: Coins is a temporally extended variant of the IPD introduced by Lerer & Peysakhovich (2018). In this game, two coins spawn randomly in a fixed-size grid, with each coin corresponding to one of the two players (designated by color matching). Each player moves around the grid and receives a reward of 1 for collecting any coin, and a punishment of $-1$ if the other agent collects their coin. If agents collect coins indiscriminately where $P({\\mathrm{collect~own~coin}})\\approx1/n$ , the net expected reward is 0. We show example rewards for various scenarios in Figure 1b. ", "page_idx": 5}, {"type": "table", "img_path": "Vq2kzpig8v/tmp/ee4feb35a3d5fde66c7fcb1d58e117d6321e5f25563757c0cc394d8a5e64f923.jpg", "table_caption": ["Table 1: IPD-Analytic round robin results "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Each entry is the average reward per episode achieved by the row agent against the column agent. Standard error of the mean (SE) is less than 0.01 for all experiments and M-MAML results are averaged across the 10 initial policies provided by Lu et al. (2022). ", "page_idx": 6}, {"type": "image", "img_path": "Vq2kzpig8v/tmp/101e7ab0ffab13f743efa76cc1c0475cc5a9e827971854a41a8f8d3fccc0d992.jpg", "img_caption": ["Figure 2: Representative run of a Reciprocator vs. an NL in IPD-Rollout. Average reciprocal reward per step (left axis) and probability of cooperation (right axis) over the course of an episode. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We select the following baselines because they focus specifically on the problem of shaping other agents\u2019 policies during simultaneous learning and without modifications to the environment. In Coins, we exclude Meta-MAPG and LOLA-DICE as baselines following work by Yu et al. (2022) showing that neither method is able to achieve significant results, even with a simplified shared reward. ", "page_idx": 6}, {"type": "text", "text": "Na\u00efve Learner (NL): As previously defined, NLs optimize their expected return with respect only to their own policy parameters $\\theta$ . In this work, we implement NLs using policy gradient methods (Sutton et al., 1999), which perform updates of the form $\\theta_{t+1}\\,=\\,\\theta_{t}+\\bar{\\alpha\\nabla}_{\\theta}J(\\bar{\\pi_{\\theta}})|_{\\theta_{t}}$ , where $\\alpha$ is the learning rate and $\\nabla_{\\boldsymbol{\\theta}}J(\\pi_{\\boldsymbol{\\theta}})|_{\\boldsymbol{\\theta}_{t}}$ is the gradient of the objective function with respect to the policy parameters $\\theta_{t}$ at step $t$ . ", "page_idx": 6}, {"type": "text", "text": "Learning with Opponent-Learning Awareness (LOLA): LOLA uses either whitebox access to an opponent\u2019s gradients and Hessians or an explicit model of their policy parameters, assuming they are NLs, and differentiates through their learning step using the update rule ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t+1}^{i}=\\theta_{t}^{i}+\\alpha^{i}\\nabla_{\\theta_{t}^{i}}J^{i}\\left(\\theta_{t}^{i},\\theta_{t}^{-i}+\\Delta\\theta_{t}^{-i}\\right)}\\\\ &{\\Delta\\theta_{t}^{-i}=\\alpha^{-i}\\nabla_{\\theta_{t}^{i}}J^{-i}\\left(\\theta_{t}^{i},\\theta_{t}^{-i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Multiagent Model-Agnostic Meta-Learning (M-MAML): M-MAML (Lu et al., 2022) learns initial parameters and then meta-learns over both its own and its opponent\u2019s policy updates, conceptually similar to Meta-Multiagent Policy Gradient (Kim et al., 2021, Meta-MAPG) but modified to differentiate directly through the analytic form of the return in matrix games. ", "page_idx": 6}, {"type": "image", "img_path": "Vq2kzpig8v/tmp/175ee7d0411dd8ff7eeb69eae8bcb6bed7587fa6074905fdb5363b9b1389de62.jpg", "img_caption": ["Figure 3: Shaping an NL in Coins. Proportion of own coins collected by NL during training when facing each opponent (left) and coin counts by type for Reciprocator vs. NL (right). Reciprocator and NL-PPO results are plotted on a scale of single episodes (bottom axis) whereas MFOS results are plotted on a scale of meta-episodes, where one meta-episode contains 16 episodes (top axis). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Model-Free Opponent Shaping (MFOS): As briefly discussed in the introduction, MFOS metalearns over multiple episodes of policy updates in order to accomplish long-horizon opponent shaping. In Lu et al. (2022), MFOS is implemented with inner and outer policies, where the outer policy either directly outputs an inner policy to play in each episode or a conditioning vector which is element-wise multiplied with an inner policy vector, as done in IPD-Analytic and Coins, respectively. ", "page_idx": 7}, {"type": "text", "text": "5.3 Implementation Details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In IPD-Analytic, we differentiate directly through the analytic solution to the matrix game. For rolloutbased experiments, we implement all policy gradient-based agents using actor-critic architectures trained with proximal policy optimization using a clipped surrogate objective (Schulman et al., 2017, PPO-Clip). Results including na\u00efve learners trained with this algorithm are denoted by NL-PPO. Target networks to estimate the $Q$ -values in Equation 1 are updated every $k$ episodes using uniformly sampled experience from a replay buffer (Lin, 1992). Additional hyperparameter values and network architecture details can be found in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "6 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate the Reciprocators\u2019 performance against a variety of baselines in a round-robin tournament for IPD-Analytic. In Coins, we assess opponent-shaping performance against NLs and against another agent of the same type, which we refer to as symmetric Coins. Shaded regions indicate standard error of the mean (SE) over eight random seeds. We do not perform full round-robin experiments in non-analytic environments for the following reasons: the Reciprocators\u2019 complex and stochastic learning rule renders LOLA\u2019s assumptions invalid and the extended time to convergence due to reciprocal behavior makes collecting meta-training episodes for MFOS computationally prohibitive. ", "page_idx": 7}, {"type": "text", "text": "6.1 IPD ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "IPD-Analytic: In the analytic form of IPD, we show that Reciprocators are able to reach cooperative equilibria with all other baselines, resisting exploitation by higher-order methods such as LOLA and MFOS despite being only a first-order method using vanilla gradient descent [Table 1]. Although the Reciprocator is extorted by MFOS to small extent (-1.06 vs. -0.98, respectively), we emphasize that MFOS relies on extensive liberties such as the ability to observe thousands of parallel training runs against their opponents and roll out pre-trained meta-policies against newly initialized agents. ", "page_idx": 7}, {"type": "text", "text": "IPD-Rollout: Stochastic policy rollouts allow the Reciprocator to influence opponent returns differently for different action sequences and provide a stronger learning signal, motivating additional experiments using sampled rollouts. In this setting, we show that Reciprocator is able to consistently shape an NL over the course of a single learning trajectory with limited rollout samples and observe interesting oscillations in the Reciprocator\u2019s rate of cooperation. Figure 2 suggests that these oscillations are driven by the opposing intrinsic and extrinsic rewards, where the derivative of the rate of cooperation corresponds to the reciprocal reward value. ", "page_idx": 7}, {"type": "image", "img_path": "Vq2kzpig8v/tmp/a2ba241072eb1ddfff03eff8c9d6d9cca8f34cd6c02815e3d2707a687ca8cdf6.jpg", "img_caption": ["Figure 4: Head-to-head results in symmetric Coins (two agents of the same kind). Total (extrinsic) reward per episode (left), proportion of own coins collected (right). Again, Reciprocator and NL-PPO results are plotted on a scale of episodes and MFOS results are plotted on a scale of meta-episodes. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We provide the following explanation for this phenomenon: although the Reciprocator initially learns a cooperative tit-for-tat strategy guided by a strong reciprocal reward, as the NL\u2019s policy becomes deterministic and $a^{i}$ becomes predictable, the $V I_{i|r c}^{\\pi}$ component of the influence balance decreases to 0. As the extrinsic reward begins to dominate, the Reciprocator essentially reverts into an $\\mathrm{NL}$ , leading to exploitative defection. However, exploitation of the NL agent $i$ causes the $V I_{r c|i}^{\\pmb{\\pi}}$ term, and subsequently the reciprocal reward, to become negative. Combined with an increase in the NL\u2019s frequency of defection, which leads to an increase in $V I_{i|r c}^{\\pi}$ , the intrinsic reward produces a reversal back to cooperative behavior that is reinforced by positive reciprocal rewards, and the cycle repeats. ", "page_idx": 8}, {"type": "text", "text": "6.2 Coins ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this temporally extended social dilemma, we see that the Reciprocator is able to shape NL-PPO into picking up more of its own coins at rates significantly higher than MFOS. This allows both agents to achieve a positive reward, while needing only a fraction of the samples to converge to mutually beneficial behavior (each meta-episode consists of 16 sequential episodes) [Figure 3]. We show that this change is driven by changes in coin preference rather than in total collection, with both agents collecting at near-optimal pace. This is in contrast to MFOS, which does not shape the NL towards cooperation, but rather uses its pretraining advantage to suppress opponent learning altogether (Khan et al., 2023) by collecting coins faster than the NL can reach them [Appendix B.2]. ", "page_idx": 8}, {"type": "text", "text": "When two Reciprocators are pitted against each other, we see that they quickly learn a cooperative strategy of collecting their own coins [Figure 4], resulting in an average reward of $\\sim8$ per 32 steps without needing self-play. This significantly outperforms MFOS and the reported performance of LOLA with opponent modeling (LOLA-OM), which achieves an average reward of only $\\sim2$ per 32 steps according to Foerster et al. (2018). One concern is that the intrinsic reciprocal reward dominates the extrinsic reward, effectively changing the motivational structure of these SSDs into a cooperative game. To address this, we recorded the cumulative reciprocal reward received per episode [Appendix B.1] to show that it quickly decreases to a negligible fraction of the total reward, similar to the scale of the reciprocal reward in IPD in Figure 2. ", "page_idx": 8}, {"type": "text", "text": "Together, these results demonstrate that Reciprocators are able to robustly shape the behavior of other agents towards prosocial equilibria during simultaneous learning, achieving state-of-the-art results with fewer assumptions and limitations than existing methods. Apart from opponent-shaping properties, we also show that the intrinsic reciprocal reward discourages Reciprocators from exploiting others, showing promise for the development of a more cooperative multi-agent learning framework. ", "page_idx": 8}, {"type": "text", "text": "7 Limitations and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our implementation of Reciprocators using na\u00efve RL algorithms remains limited in that it seeks to maximize its compound return within single episodes rather than across multiple episodes. Using handpicked weights to balance intrinsic and extrinsic rewards opens the door for a suboptimal tradeoff between long-term opponent shaping and short-term return maximization. This is partially mitigated by the counterfactual target baseline updates, which allow the Reciprocator to lower the magnitude of the reciprocal reward in response to opponent policies that remain stationary over multiple episodes [Figure 2]. Future work will focus on methods to evaluate reciprocation efficacy across multiple episodes and dynamically tune the balance between reciprocal and extrinsic rewards. ", "page_idx": 9}, {"type": "text", "text": "In terms of evaluation, Khan et al. (2023) found that Coins does not require history to enable opponent shaping, since the current state is often indicative of past actions. Due to computational limitations, we were unable to assess our method\u2019s performance on the suite of Spatio-Temporal Representations of Matrix Games (STORM) designed to test shaping over longer time horizons. We do note that Reciprocators capture both types of memory necessary to achieve shaping as identified by Khan et al. (2023): the counterfactual baselines trained on replay buffers serve as a way to capture inter-episode context, and the recurrent policies and influence balance capture intra-episode history. Therefore, our method can in theory generalize to STORM environments, although we leave this to future work. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion and Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Emerging interest in cooperative AI has been led by approaches that endow agents with higherorder shaping capabilities. Although these agents exhibit cooperative behaviors when pitted against equivalent opponents, they readily manipulate and exploit agents with simpler learning rules or lower computational capabilities. This is a fundamentally undesirable outcome in partially adversarial interactions between learning agents, with the potential to exacerbate existing computational resource gaps beyond out-of-the-box pretrained performance. ", "page_idx": 9}, {"type": "text", "text": "We presented Reciprocators, agents which seek to influence the behavior of other, simultaneously learning agents towards mutually beneficial outcomes. We showed that Reciprocators can both learn prosocial behaviors and induce them from other agents in a variety of sequential social dilemmas, while remaining resistant to exploitation by higher-order agents. To the best of our knowledge, Reciprocators represent the first class of reinforcement learning algorithms to achieve cooperation during simultaneous learning between two independent agents without needing meta-learning methods, knowledge of other agents\u2019 learning algorithms, or pretraining routines such as self-play or tracing procedures to control opponent selection. We believe that these results show a promising avenue forward for inducing cooperative outcomes from a diverse array of learning opponents. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the following awards to JCK: National Institutes of Health DP2NS122037 and NSF CAREER 1943467. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Agapiou, J. P., Vezhnevets, A. S., Du\u00e9\u00f1ez-Guzm\u00e1n, E. A., Matyas, J., Mao, Y., Sunehag, P., K\u00f6ster, R., Madhushani, U., Kopparapu, K., Comanescu, R., Strouse, D. J., Johanson, M. B., Singh, S., Haas, J., Mordatch, I., Mobbs, D., and Leibo, J. Z. Melting Pot 2.0, October 2023. URL http://arxiv.org/abs/2211.13746. arXiv:2211.13746 [cs]. ", "page_idx": 10}, {"type": "text", "text": "Aghajohari, M., Duque, J. A., Cooijmans, T., and Courville, A. LOQA: LEARNING WITH OPPONENT Q-LEARNING AWARENESS. 2024. ", "page_idx": 10}, {"type": "text", "text": "Burkart, J. M., Allon, O., Amici, F., Fichtel, C., Finkenwirth, C., Heschl, A., Huber, J., Isler, K., Kosonen, Z. K., Martins, E., Meulman, E. J., Richiger, R., Rueth, K., Spillmann, B., Wiesendanger, S., and van Schaik, C. P. The evolutionary origin of human hyper-cooperation. Nature Communications, 5(1):4747, August 2014. ISSN 2041-1723. doi: 10.1038/ncomms5747. URL https://www.nature.com/articles/ncomms5747. Number: 1 Publisher: Nature Publishing Group. ", "page_idx": 10}, {"type": "text", "text": "Bussmann, B., Heinerman, J., and Lehman, J. Towards Empathic Deep Q-Learning, June 2019. URL http://arxiv.org/abs/1906.10918. arXiv:1906.10918 [cs]. ", "page_idx": 10}, {"type": "text", "text": "Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. Counterfactual Multi-Agent Policy Gradients, December 2017. URL http://arxiv.org/abs/1705.08926. arXiv:1705.08926 [cs]. ", "page_idx": 10}, {"type": "text", "text": "Foerster, J. N., Chen, R. Y., Al-Shedivat, M., Whiteson, S., Abbeel, P., and Mordatch, I. Learning with Opponent-Learning Awareness. In Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems, September 2018. URL http://arxiv.org/abs/ 1709.04326. arXiv:1709.04326 [cs]. ", "page_idx": 10}, {"type": "text", "text": "Fung, K., Zhang, Q., Lu, C., Willi, T., and Foerster, J. N. Analyzing the Sample Complexity of ModelFree Opponent Shaping. July 2023. URL https://openreview.net/forum?id=Dm2fbPpU6v. ", "page_idx": 10}, {"type": "text", "text": "Gupta, J. K., Egorov, M., and Kochenderfer, M. Cooperative Multi-agent Control Using Deep Reinforcement Learning. In Sukthankar, G. and Rodriguez-Aguilar, J. A. (eds.), Autonomous Agents and Multiagent Systems, volume 10642, pp. 66\u201383. Springer International Publishing, Cham, 2017. ISBN 978-3-319-71681-7 978-3-319-71682-4. doi: 10.1007/978-3-319-71682-4_5. URL http://link.springer.com/10.1007/978-3-319-71682-4_5. Series Title: Lecture Notes in Computer Science. ", "page_idx": 10}, {"type": "text", "text": "Hughes, E., Leibo, J. Z., Phillips, M., Tuyls, K., Due\u00f1ez-Guzman, E., Garc\u00eda Casta\u00f1eda, A., Dunning, I., Zhu, T., McKee, K., Koster, R., Roff, H., and Graepel, T. Inequity aversion improves cooperation in intertemporal social dilemmas. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/ 2018/hash/7fea637fd6d02b8f0adf6f7dc36aed93-Abstract.html. ", "page_idx": 10}, {"type": "text", "text": "Jaques, N., Lazaridou, A., Hughes, E., Gulcehre, C., Ortega, P., Strouse, D., Leibo, J. Z., and Freitas, N. D. Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning. In Proceedings of the 36th International Conference on Machine Learning, pp. 3040\u2013 3049. PMLR, May 2019. URL https://proceedings.mlr.press/v97/jaques19a.html. ISSN: 2640-3498. ", "page_idx": 10}, {"type": "text", "text": "Khan, A., Willi, T., Kwan, N., Tacchetti, A., Lu, C., Grefenstette, E., Rockt\u00e4schel, T., and Foerster, J. Scaling Opponent Shaping to High Dimensional Games, December 2023. URL http://arxiv. org/abs/2312.12568. arXiv:2312.12568 [cs]. ", "page_idx": 10}, {"type": "text", "text": "Kim, D. K., Liu, M., Riemer, M. D., Sun, C., Abdulhai, M., Habibi, G., Lopez-Cot, S., Tesauro, G., and How, J. A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning. In Proceedings of the 38th International Conference on Machine Learning, pp. 5541\u20135550. PMLR, July 2021. URL https://proceedings.mlr.press/v139/kim21g.html. ISSN: 2640-3498.   \nKingma, D. P. and Ba, J. Adam: A Method for Stochastic Optimization, January 2017. URL http://arxiv.org/abs/1412.6980. arXiv:1412.6980 [cs].   \nKraemer, L. and Banerjee, B. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing, 190:82\u201394, May 2016. ISSN 0925-2312. doi: 10.1016/ j.neucom.2016.01.031. URL https://www.sciencedirect.com/science/article/pii/ S0925231216000783.   \nLeibo, J. Z., Zambaldi, V., Lanctot, M., Marecki, J., and Graepel, T. Multi-agent Reinforcement Learning in Sequential Social Dilemmas, February 2017. URL http://arxiv.org/abs/1702. 03037. arXiv:1702.03037 [cs].   \nLerer, A. and Peysakhovich, A. Maintaining cooperation in complex social dilemmas using deep reinforcement learning, March 2018. URL http://arxiv.org/abs/1707.01068. arXiv:1707.01068 [cs].   \nLi, M. and Dennis, M. The Beneftis of Power Regularization in Cooperative Reinforcement Learning. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, AAMAS \u201923, pp. 457\u2013465, Richland, SC, May 2023. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 978-1-4503-9432-1.   \nLi, Y., Zhang, W., Wang, J., Zhang, S., Du, Y., Wen, Y., and Pan, W. Aligning Individual and Collective Objectives in Multi-Agent Cooperation, February 2024. URL http://arxiv.org/ abs/2402.12416. arXiv:2402.12416 [cs].   \nLin, L.-J. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8(3):293\u2013321, May 1992. ISSN 1573-0565. doi: 10.1007/BF00992699. URL https://doi.org/10.1007/BF00992699.   \nLu, C., Willi, T., Witt, C. A. S. D., and Foerster, J. Model-Free Opponent Shaping. In Proceedings of the 39th International Conference on Machine Learning, pp. 14398\u201314411. PMLR, June 2022. URL https://proceedings.mlr.press/v162/lu22d.html. ISSN: 2640-3498.   \nMelis, A. P. and Semmann, D. How is human cooperation different? Philosophical Transactions of the Royal Society B: Biological Sciences, 365(1553):2663\u20132674, September 2010. ISSN 0962- 8436. doi: 10.1098/rstb.2010.0157. URL https://www.ncbi.nlm.nih.gov/pmc/articles/ PMC2936178/.   \nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, February 2015. ISSN 1476-4687. doi: 10.1038/nature14236. URL https://www.nature.com/articles/nature14236. Number: 7540 Publisher: Nature Publishing Group.   \nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous Methods for Deep Reinforcement Learning. In Proceedings of The 33rd International Conference on Machine Learning, pp. 1928\u20131937. PMLR, June 2016. URL https: //proceedings.mlr.press/v48/mniha16.html. ISSN: 1938-7228.   \nPardo, F., Tavakoli, A., Levdik, V., and Kormushev, P. Time Limits in Reinforcement Learning, January 2022. URL http://arxiv.org/abs/1712.00378. arXiv:1712.00378 [cs].   \nPark, S., Rybkin, O., and Levine, S. METRA: Scalable Unsupervised RL with Metric-Aware Abstraction, March 2024. URL http://arxiv.org/abs/2310.08887. arXiv:2310.08887 [cs].   \nPress, W. H. and Dyson, F. J. Iterated Prisoner\u2019s Dilemma contains strategies that dominate any evolutionary opponent. Proceedings of the National Academy of Sciences, 109(26):10409\u201310413, June 2012. doi: 10.1073/pnas.1206569109. URL https://www.pnas.org/doi/abs/10.1073/ pnas.1206569109. Publisher: Proceedings of the National Academy of Sciences.   \nRashid, T., Samvelyan, M., de Witt, C. S., Farquhar, G., Foerster, J., and Whiteson, S. QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning, June 2018. URL http://arxiv.org/abs/1803.11485. arXiv:1803.11485 [cs, stat].   \nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized Experience Replay, February 2016. URL http://arxiv.org/abs/1511.05952. arXiv:1511.05952 [cs].   \nSchmid, K., Belzner, L., and Linnhoff-Popien, C. Learning to Penalize Other Learning Agents. MIT Press, July 2021. doi: 10.1162/isal_a_00369. URL https://dx.doi.org/10.1162/isal_a_ 00369.   \nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal Policy Optimization Algorithms, August 2017. URL http://arxiv.org/abs/1707.06347. arXiv:1707.06347 [cs].   \nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., and Graepel, T. Value-Decomposition Networks For Cooperative Multi-Agent Learning, June 2017. URL http://arxiv.org/abs/1706.05296. arXiv:1706.05296 [cs].   \nSutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In Advances in Neural Information Processing Systems, volume 12. MIT Press, 1999. URL https://proceedings.neurips.cc/paper/1999/hash/ 464d828b85b0bed98e80ade0a5c43b0f-Abstract.html.   \nWang, T., Wang, J., Wu, Y., and Zhang, C. Influence-Based Multi-Agent Exploration. In Eighth International Conference on Learning Representations, April 2020. URL https://arxiv.org/ abs/1910.05512.   \nWilli, T., Letcher, A. H., Treutlein, J., and Foerster, J. COLA: Consistent Learning with OpponentLearning Awareness. In Proceedings of the 39th International Conference on Machine Learning, pp. 23804\u201323831. PMLR, June 2022. URL https://proceedings.mlr.press/v162/willi22a. html. ISSN: 2640-3498.   \nWilliams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229\u2013256, May 1992. ISSN 1573-0565. doi: 10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696.   \nYaman, A., Leibo, J. Z., Iacca, G., and Wan Lee, S. The emergence of division of labour through decentralized social sanctioning. Proceedings of the Royal Society B: Biological Sciences, 290 (2009):20231716, October 2023. ISSN 0962-8452, 1471-2954. doi: 10.1098/rspb.2023.1716. URL https://royalsocietypublishing.org/doi/10.1098/rspb.2023.1716.   \nYang, J., Li, A., Farajtabar, M., Sunehag, P., Hughes, E., and Zha, H. Learning to Incentivize Other Learning Agents. In Advances in Neural Information Processing Systems, volume 33, pp. 15208\u201315219. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper/2020/hash/ad7ed5d47b9baceb12045a929e7e2f66-Abstract.html.   \nYu, X., Jiang, J., Zhang, W., Jiang, H., and Lu, Z. Model-Based Opponent Modeling. May 2022. URL https://openreview.net/forum?id $=$ A7l8WZIKz3.   \nZhao, S., Lu, C., Grosse, R. B., and Foerster, J. N. Proximal Learning With Opponent-Learning Awareness, October 2022. URL http://arxiv.org/abs/2210.10125. arXiv:2210.10125 [cs]. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Experimental Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "All experiments were run on Nvidia 3070 GPUs with 8 GB of VRAM. Results were averaged over eight random seeds, with a batch size of 8192 for IPD-Analytic and 2048 for IPD-Rollout and Coins. For all experiments involving MFOS agents, we adapted the original code from Lu et al. (2022), leaving all architectural choices and hyperparameters the same. ", "page_idx": 13}, {"type": "text", "text": "The architecture for both the actor and the critic consists of a state encoder (two convolutional layers followed by a linear layer, with ReLU activations between layers), followed by hidden linear layers as detailed in the table below and a final output linear layer. The actor has a softmax activation to output a policy over the action space, whereas the critic simply outputs a scalar value estimate. ", "page_idx": 13}, {"type": "table", "img_path": "Vq2kzpig8v/tmp/0b02f0eb36b7f8bbf1477b5aab1ab387f718b1b85a1ab03f1ec8009f4d97909a.jpg", "table_caption": ["Table 2: General PPO parameters. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "With the exception of a linear layer size of 32 (instead of 16) for Coins, the network architecture and parameters to estimate the various target functions to compute the $V I$ are identical to those described for the PPO components in Table 2. For IPD-Analytic, target estimates for opponent policies in IPD were computed as the frequencies of observed choices for each of the five possible states (start, CC, CD, DC, DD). ", "page_idx": 13}, {"type": "text", "text": "For Coins, we implement the decomposition of $V I$ given in Equation 2, separately predicting the immediate reward and the transition probabilities for each possible next state. We collect one batch of episodes at the beginning of each experiment to initialize the influence estimators. ", "page_idx": 13}, {"type": "table", "img_path": "Vq2kzpig8v/tmp/e050f3a56e9dc5ce5930eb9e28be149f9d1fb189add3ee68924eacdc6c245129.jpg", "table_caption": ["Table 3: Reciprocator-specific parameters. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Replay buffer sizes are in units of episodes, where each episode consists of a batch of 32 steps. A replay buffer of size 4 for Coins corresponds to 32 steps $\\times\\,4$ episodes $\\times\\,2048$ batch size $=262{,}144$ steps of experience. ", "page_idx": 13}, {"type": "text", "text": "B Additional Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Reciprocal Reward in Coins ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A representative run of two Reciprocators in Coins showing the relative magnitude of the intrinsic reciprocal reward and extrinsic environmental rewards over the course of training. ", "page_idx": 13}, {"type": "image", "img_path": "Vq2kzpig8v/tmp/5d4947be6039fcffa387b3647b8f39e6be5db6fef771af9dbca423c1335be559.jpg", "img_caption": ["Figure 5: Mean extrinsic reward (left), mean intrinsic reward (right) for two Reciprocators in Coins. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.2 Coin Counts vs. NL-PPO ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We display the total coin counts of an NL when faced against each other type of agent in order to show that changes in P(Own Coins) when faced by a Reciprocator are driven by changes in coin color preference rather than changes in the total number of coins collected. In contrast, we see that the NL collects far fewer coins when faced with an MFOS agent, providing support for Khan et al. (2023)\u2019s claim that MFOS suppresses rather than shapes learning. Note that this figure is generated from the same experimental data as Figure 3. ", "page_idx": 14}, {"type": "image", "img_path": "Vq2kzpig8v/tmp/4404f3615e7cf6308e46b03b460304a083add4068af0c6319b98f48b2fc236b1.jpg", "img_caption": ["Figure 6: Total number of coins per 32 steps collected by NL-PPO (right) vs. each baseline in Coins. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction do accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We include a Limitations section in the main text. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: This paper focuses on empirical evaluation and does not include theoretical results. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper includes all network parameters, algorithmic details, and hyperparameters necessary to reproduce the main experimental results. A zip file containing the code is also provided in the supplementary material. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Code is linked in the abstract. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Training details are described in the Implementation Details (section 5.3) and hyperparameters are provided in the Appendix. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Number of replicates are provided in the Appendix and error bars (depicting standard error of the mean) are defined in Implementation Details. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Compute resources are described in the Appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Broader impacts of this work are discussed in the Conclusion and Broader Impacts section. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: These models are evaluated on simple game-theoretic environments and do not pose a high risk for misuse. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We use previously released code from the MFOS paper and an adapted opensource implementation of PPO, both of which are credited in the relevant code files and in the Appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: New assets, i.e., code, is well-documented via comments and docstrings and is linked in the abstract. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}]