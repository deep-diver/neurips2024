[{"figure_path": "B5vQ7IQW7d/tables/tables_7_1.jpg", "caption": "Table 1: Task-IL and class-IL overall accuracy on CIFAR10, CIFAR-100 and Tiny-ImageNet, respectively with memory size 500. '-' indicates not applicable/available.", "description": "This table presents the overall accuracy results for various continual learning methods across three benchmark datasets: CIFAR-10, CIFAR-100, and Tiny-ImageNet.  The results are broken down by task-incremental learning (Task-IL) and class-incremental learning (Class-IL) settings,  and a memory size of 500 is used for methods that utilize memory.  The '-' indicates where results are not applicable or not available for a specific method and dataset combination.", "section": "5.2 Results"}, {"figure_path": "B5vQ7IQW7d/tables/tables_8_1.jpg", "caption": "Table 1: Task-IL and class-IL overall accuracy on CIFAR10, CIFAR-100 and Tiny-ImageNet, respectively with memory size 500. '-' indicates not applicable/available.", "description": "This table presents the overall accuracy results for task-incremental learning (Task-IL) and class-incremental learning (Class-IL) on three benchmark datasets: CIFAR-10, CIFAR-100, and Tiny-ImageNet.  The results are reported for various continual learning methods (including the proposed MACL method and several baselines) and compare their performance under different learning scenarios. A memory size of 500 is used across all experiments.", "section": "5.2 Results"}, {"figure_path": "B5vQ7IQW7d/tables/tables_8_2.jpg", "caption": "Table 3: Task-IL and class-IL overall accuracy on CIFAR-100 and Tiny-ImageNet, respectively with memory size 2000.", "description": "This table presents the results of Task-IL and Class-IL experiments conducted on CIFAR-100 and Tiny-ImageNet datasets using a memory size of 2000.  It compares the performance of different continual learning methods (ER, ER+MACL, DER++, DER+++MACL, LODE, and LODE+MACL) in terms of overall accuracy. The results show the impact of integrating the proposed MACL method with existing continual learning approaches.", "section": "5.3 Ablation Study"}, {"figure_path": "B5vQ7IQW7d/tables/tables_9_1.jpg", "caption": "Table 4: Overall accuracy of integrating DER++ with MACL using a memory buffer of 500 and longer task sequence on Tiny-ImageNet.", "description": "This table shows the overall accuracy results of integrating DER++ with MACL on the Tiny-ImageNet dataset.  The experiment was conducted using a memory buffer size of 500, and the number of tasks in the continual learning sequence was varied (10 and 20 tasks).  The results demonstrate the performance improvement achieved by integrating MACL with DER++, even with longer task sequences, highlighting the effectiveness of the method in improving overall accuracy across multiple tasks.", "section": "5 Experiments"}, {"figure_path": "B5vQ7IQW7d/tables/tables_21_1.jpg", "caption": "Table 5: Analysis of hyperparameter \u03b7 on CIFAR100 and Tiny-ImageNet in the setting of task-IL.", "description": "This table presents the results of an ablation study on the hyperparameter \u03b7, which controls the sensitivity of the model to parameter updates.  The study evaluates the effect of different values of \u03b7 on the overall accuracy of the model on two datasets, CIFAR100 and Tiny-ImageNet, when performing task-incremental learning (Task-IL).  The results show how the model's performance changes as the sensitivity to parameter updates is adjusted.", "section": "D.1 Hyperparameter Sensitivity Analysis"}, {"figure_path": "B5vQ7IQW7d/tables/tables_22_1.jpg", "caption": "Table 6: Benefit of MACL-NGD vs. MACL-GD on CIFAR100 and Tiny-ImageNet in the setting of task-IL.", "description": "This table compares the performance of the proposed method MACL using two different optimization methods: Natural Gradient Descent (NGD) and Gradient Descent (GD).  The results show the overall accuracy achieved on two benchmark datasets, CIFAR100 and Tiny-ImageNet, under the task-incremental learning setting. This demonstrates the impact of the optimization method on the model's ability to retain knowledge and achieve good performance on new tasks.  The values show the improvement of using NGD compared to GD for MACL.", "section": "D.2 Benefit of NGD"}, {"figure_path": "B5vQ7IQW7d/tables/tables_22_2.jpg", "caption": "Table 7: Online CL Results on CIFAR100 under the blurry boundary setting", "description": "This table presents the results of online continual learning experiments on the CIFAR100 dataset under blurry boundary conditions.  It compares the performance of the MKD(PCR) method alone with the performance of MKD(PCR) enhanced by the proposed MACL method.  The comparison is made across varying memory sizes (1000, 2000, and 5000). The results demonstrate the improvement in accuracy achieved by integrating MACL into the baseline MKD(PCR) method.", "section": "5.2 Results"}, {"figure_path": "B5vQ7IQW7d/tables/tables_22_3.jpg", "caption": "Table 8: Online CL Results on Tiny-ImageNet under the blurry boundary setting", "description": "This table presents the results of online continual learning experiments on the Tiny-ImageNet dataset under blurry boundary conditions. It compares the performance of the MKD(PCR) method with and without the proposed MACL approach, across three different memory sizes (2000, 5000, and 10000). The results show the improvement achieved by integrating MACL into the MKD(PCR) method for enhancing online continual learning performance under challenging conditions.", "section": "D.3 Online CL results"}, {"figure_path": "B5vQ7IQW7d/tables/tables_22_4.jpg", "caption": "Table 4: Overall accuracy of integrating DER++ with MACL using a memory buffer of 500 and longer task sequence on Tiny-ImageNet.", "description": "This table shows the overall accuracy results of integrating the proposed MACL method with the DER++ baseline method on the Tiny-ImageNet dataset.  The experiment is performed using a memory buffer size of 500 and varies the number of tasks in the sequence to assess the impact of longer task sequences. The results are reported separately for both Class-IL and Task-IL scenarios.  The table highlights that even with longer sequences of tasks, the MACL method improves performance compared to the baseline DER++ method.", "section": "5 Experiments"}, {"figure_path": "B5vQ7IQW7d/tables/tables_22_5.jpg", "caption": "Table 10: Comparison of methods on Class-IL and Task-IL on 5-datasets.", "description": "This table presents a comparison of the performance of various continual learning methods (ER, ER+MACL, DER++, and DER++MACL) on the 5-datasets benchmark, which consists of five distinct datasets (CIFAR-10, MNIST, Fashion-MNIST, SVHN, and notMNIST).  The results are shown for both Class-IL (class-incremental learning) and Task-IL (task-incremental learning) settings.  The table showcases the accuracy achieved by each method, illustrating the effectiveness of MACL when integrated with other continual learning techniques. ", "section": "5.2 Results"}, {"figure_path": "B5vQ7IQW7d/tables/tables_22_6.jpg", "caption": "Table 11: Overall accuracy with ResNet32 using a memory buffer of 2000 by integrating with MEMO.", "description": "This table presents the overall accuracy achieved using the MEMO algorithm alone and when combined with the proposed MACL method.  The experiment uses ResNet32 as the base model and a memory buffer of 2000. The results showcase the improvement in accuracy obtained by integrating MACL with MEMO.", "section": "5.2 Results"}, {"figure_path": "B5vQ7IQW7d/tables/tables_22_7.jpg", "caption": "Table 12: Overall accuracy with ViT using a memory buffer of 500 by integrating DER++ with MACL.", "description": "This table presents the overall accuracy results for Class-IL and Task-IL scenarios on the CIFAR100 dataset using a Vision Transformer (ViT) model.  The results compare the performance of the DER++ method alone against the performance of DER++ integrated with the proposed MACL method.  The memory buffer size is set to 500.  The table shows that incorporating MACL into DER++ improves the overall accuracy for both Class-IL and Task-IL settings.", "section": "5.2 Results"}, {"figure_path": "B5vQ7IQW7d/tables/tables_23_1.jpg", "caption": "Table 13: ImageNet-R Results", "description": "This table presents the results of the ImageNet-R experiments.  It shows the performance (Class-IL and Task-IL accuracy) of four different continual learning methods: DER++, DER++ with the proposed MACL method, LODE, and LODE with MACL.  The memory size used was 500. The results demonstrate the improvement in accuracy achieved by integrating the MACL method with existing state-of-the-art continual learning approaches.", "section": "5.2 Results"}, {"figure_path": "B5vQ7IQW7d/tables/tables_23_2.jpg", "caption": "Table 14: CUB200 Results", "description": "This table presents the results of the CUB200 dataset experiment.  It shows the overall accuracy for class-incremental learning (Class-IL) and task-incremental learning (Task-IL) using different continual learning methods. The methods compared include DER++, DER++ with the proposed MACL method, LODE, and LODE with MACL.  The table highlights the performance gains achieved by integrating the MACL method with existing continual learning techniques.", "section": "5.2 Results"}, {"figure_path": "B5vQ7IQW7d/tables/tables_23_3.jpg", "caption": "Table 15: Running efficiency of MACL on CIFAR100 by training for a single epoch on CIFAR100.", "description": "This table shows the running time (in seconds) for training a single epoch on the CIFAR100 dataset for several continual learning (CL) methods, both with and without the integration of the proposed Model Sensitivity Aware Continual Learning (MACL) method.  The table compares the efficiency gains obtained by incorporating MACL into existing CL techniques.", "section": "D.8 Efficiency Evaluation"}, {"figure_path": "B5vQ7IQW7d/tables/tables_23_4.jpg", "caption": "Table 1: Task-IL and class-IL overall accuracy on CIFAR10, CIFAR-100 and Tiny-ImageNet, respectively with memory size 500. '-' indicates not applicable/available.", "description": "This table presents the overall accuracy results for Task-IL and Class-IL experiments conducted on three datasets: CIFAR10, CIFAR100, and Tiny-ImageNet.  The results are categorized by method and include a comparison with fine-tuning and joint training baselines. The memory size used for the experiments was 500.  A '-' symbol denotes that the result is not applicable or available for that particular method and dataset combination.", "section": "5.2 Results"}]