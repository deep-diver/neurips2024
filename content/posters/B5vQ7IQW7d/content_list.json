[{"type": "text", "text": "Model Sensitivity Aware Continual Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhenyi Wang and Heng Huang Department of Computer Science, Institute of Health Computing University of Maryland College Park College Park, MD, 20742 zwang@umd.edu;heng@umd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual learning (CL) aims to adapt to non-stationary data distributions while retaining previously acquired knowledge. However, CL models typically face a trade-off between preserving old task knowledge and excelling in new task performance. Existing approaches often sacrifice one for the other. To overcome this limitation, orthogonal to existing approaches, we propose a novel perspective that views the CL model ability in preserving old knowledge and performing well in new task as a matter of model sensitivity to parameter updates. Excessive parameter sensitivity can lead to two drawbacks: (1) significant forgetting of previous knowledge; and (2) overftiting to new tasks. To reduce parameter sensitivity, we optimize the model\u2019s performance based on the parameter distribution, which achieves the worst-case CL performance within a distribution neighborhood. This innovative learning paradigm offers dual benefits: (1) reduced forgetting of old knowledge by mitigating drastic changes in model predictions under small parameter updates; and (2) enhanced new task performance by preventing overfitting to new tasks. Consequently, our method achieves superior ability in retaining old knowledge and achieving excellent new task performance simultaneously. Importantly, our approach is compatible with existing CL methodologies, allowing seamless integration while delivering significant improvements in effectiveness, efficiency, and versatility with both theoretical and empirical supports. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual learning (CL) embodies a dynamic approach aimed at adapting to non-stationary data distributions that evolve over time. However, in pursuit of this goal, CL encounters a significant challenge: the trade-off between preserving previously acquired knowledge and effectively learning new knowledge. As the model assimilates new information, it often swiftly erases previously learned knowledge, resulting in catastrophic forgetting (CF) on past tasks [44, 54]. Effectively addressing CF during CL is essential to preserve previously acquired information. On the other hand, effectively learning new information is equally crucial for CL models to adapt to new tasks and environments. ", "page_idx": 0}, {"type": "text", "text": "Existing approaches in CL often face a dilemma: they either prioritize preserving old knowledge or excelling in new task performance, often at the expense of the other. When a CL model prioritizes preserving old knowledge, it focuses on retaining information from previous tasks while minimizing interference or forgetting. However, excessive emphasis on old knowledge can limit the model\u2019s ability to adapt to new tasks. Conversely, when a model prioritizes new task performance, it aims to quickly adapt to new tasks or data distributions. Yet, this emphasis on new tasks can potentially degrade performance on previously learned tasks. ", "page_idx": 0}, {"type": "text", "text": "To overcome the aforementioned limitations, orthogonal to existing approaches, we introduce the concept of model sensitivity and approach the challenge of balancing old knowledge retention and new task performance in CL from the perspective of model parameter sensitivity. When a CL model exhibits high sensitivity to parameter changes, it leads to two significant issues: (1) Increased Forgetting: Excessive sensitivity in model parameters can cause abrupt and substantial changes in model predictions with minor parameter adjustments during CL. This phenomenon results in significant forgetting of previous tasks. (2) Diminished New Task Performance: High sensitivity in model parameters can also result in severe overftiting on new tasks. Overftiting occurs when a model memorizes the training data instead of generalizing patterns that can be applied to unseen data. High parameter sensitivity means that even minor alterations in the training data can induce substantial modifications in the learned model. This renders the model excessively tailored to the training data and reduces its adaptability to new, unseen data, consequently leading to suboptimal performance on new tasks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To reduce the CL model parameter sensitivity under model updates, we aim to ensure that even minor alterations in model parameters do not substantially impair CL model performance. This is accomplished by optimizing the model\u2019s performance based on the worst-case scenario of parameter distributions within a distribution neighborhood. However, finding the optimal worst-case CL model parameter distribution is challenging since the space of all possible distributions within the neighborhood is an infinite-dimensional space [32]. To efficiently solve this problem, we parameterize the optimal worst-case CL model parameter distribution as Gaussian distribution. We propose a natural-gradient descent (NGD) method to efficiently inference the mean and covariance of the Gaussian distribution since NGD incorporates the information geometry of the parameter space by adapting the step size based on the curvature of the cost function. This adaptive approach leads to faster convergence compared to conventional gradient descent methods, particularly in highdimensional spaces where the curvature exhibits notable variations. This is especially beneficial for CL models. However, calculating the natural gradient is computationally expensive due to the explicit calculation of Fisher information matrix (FIM). We thus update the worst-case CL parameters in the expectation parameter space, rather than the traditional natural parameter space, of the Gaussian distribution, thereby eliminating the need for explicit calculation of the FIM. ", "page_idx": 1}, {"type": "text", "text": "Our method offers dual benefits: (1) Reduced Forgetting: By mitigating parameter sensitivity and avoiding drastic changes in model prediction, our approach effectively reduces the loss of previously learned task knowledge. (2) Improved New Task Performance: Through decreased parameter sensitivity, the model becomes less susceptible to overfitting on new task training data. This reduced vulnerability to minor fluctuations fosters the learning of more generalized patterns rather than memorizing specific examples. As a result, the model demonstrates enhanced generalization capabilities on new tasks. Therefore, our method simultaneously achieves superior performance in retaining previously learned knowledge and excelling in new task performance. ", "page_idx": 1}, {"type": "text", "text": "We provide a thorough theoretical analysis for our method. Firstly, the theory illustrates that our approach implicitly reduces the variance of loss against different parameter variations, thereby indicating reduced model parameter sensitivity. Secondly, our method tightens the generalization bound of CL models, suggesting enhanced generalization. Furthermore, our extensive experiments across multiple datasets, compared to various state-of-the-art (SOTA) baseline methods, reveal substantial enhancements in overall performance across all learned tasks, backward transfer, and new task test accuracy. These results indicate significantly enhanced CL model ability in preserving old knowledge and achieving better performance on new task with our method. Additionally, our proposed approach seamlessly integrates with existing CL methodologies, functioning as a versatile plug-in. This demonstrates the effectiveness, efficiency, and versatility of our method. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We tackle the challenge of both retaining old task knowledge and excelling in new task in CL from a novel perspective by mitigating model parameter sensitivity.   \n\u2022 We introduce a novel CL approach aimed at reducing model parameter sensitivity by optimizing CL model performance under the worst-case parameter distribution within a distribution neighborhood. Additionally, we propose an efficient learning algorithm to identify the worst-case parameter distribution.   \n\u2022 We provide comprehensive theoretical analyses that substantiate our method\u2019s ability to decrease model parameter sensitivity and improve model generalization.   \n\u2022 Extensive experiments conducted across multiple datasets demonstrate the efficacy and versatility of our proposed method. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "CL aims to learn non-stationary data distributions without forgetting previously learned knowledge. The CL scenarios can be further categorized into three scenarios: task-incremental learning (Task-IL), domain-incremental learning (Domain-IL) and class-incremental learning (Class-IL) [67]. TaskIL and Class-IL are most representative scenarios in CL, we thus focus on these two scenarios. Existing approaches for CL can be categorized into five classes: (1) regularization-based methods incorporate regularization terms either in model weights or outputs into the loss function to mitigate catastrophic forgetting when learning new tasks, including [28, 62, 84, 55, 11, 1, 22, 10, 39]; (2) memory replay-based methods address the challenge of catastrophic forgetting by explicitly storing and replaying a subset of past experiences (samples from previous tasks) while learning new tasks, including [40, 57, 15, 7, 51, 68, 3, 8, 75, 4, 74, 76, 61, 78, 77, 83, 36, 73, 72]; (3) gradient-projectionbased methods aim to mitigate catastrophic forgetting by projecting gradient updates onto subspaces that minimize interference with previously learned tasks, including [13, 17, 60, 71, 38, 52, 82]; (4) architecture-based methods involve dynamically adapting and modifying the neural network architecture to accommodate new tasks while preserving performance on previously learned tasks, including [41, 63, 34, 23]; (5) Bayesian-based methods leverage principles from Bayesian inference to manage the uncertainty and learning of new tasks while preserving knowledge from previous tasks, including [48, 58, 30, 25, 21, 49, 66, 59]. ", "page_idx": 2}, {"type": "text", "text": "In contrast to existing methods, which often necessitate a trade-off between retaining old knowledge and learning new knowledge, sacrificing one for the other, our approach takes a different path. It sets itself apart from these existing methods by offering an orthogonal solution that preserves old task knowledge while simultaneously enhancing new task performance. This novel perspective is achieved by reducing parameter sensitivity. ", "page_idx": 2}, {"type": "text", "text": "Connection with existing flat-minima/SWAD approaches: (1) Connection and difference with sharpness-aware minimization (SAM) [18, 27, 45] related approach: Our method is fundamentally different from SAM-based CL in two aspects. (i) Deterministic vs. Probabilistic Approach: SAM uses a fixed deterministic neighborhood, which can be restrictive in practice since updates are constrained within a fixed ball. In contrast, our method employs a probabilistic distributional approach, offering two distinct advantages: (a) The distributional neighborhood is more flexible and covers a broader range of parameter variations by sampling from a neighborhood distribution, and (b) Stochastic Gradient Descent (SGD) introduces noise during CL. Our distributional approach accounts for this noise, making it a more realistic model in practice and providing stronger guarantees against parameter sensitivity. (ii) Uniform vs. Parameter-specific sensitivity without explicit calculation of FIM: SAM uniformly updates all parameters, overlooking the varying importance and sensitivity of each parameter in the context of CL. Our method, on the other hand, considers these differences and treats parameters uniquely through the natural gradient without needing to explicitly calculating the FIM. This distinction is crucial for CL, as each parameter has different sensitivity to forgetting\u2014a factor that SAM does not address. (2) Connection and difference with model averaging flatness seeking approach: SWA [24] and SWAD [9], which aim to achieve flatter minima by averaging multiple models during training. However, these approaches are memory-intensive and inefficient for CL, as they require storing multiple sets of model parameters, which compromises memory efficiency. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first present the preliminary in section 3.1 and then present the model sensitivity aware continual learning in section 3.2. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Continual Learning Setup The standard $\\mathrm{CL}$ problem involves learning a sequence of $T$ tasks, represented as $\\mathcal{D}^{t r}\\,\\,\\bar{=}\\,\\,\\{\\mathcal{D}_{1}^{t\\bar{r}},\\mathcal{D}_{2}^{t r},\\cdot\\cdot\\cdot\\,,\\mathcal{D}_{T}^{t r}\\}$ . The training dataset $\\mathcal{D}_{k}^{t r}$ for the $\\bar{k^{t h}}$ task contains a collection of triplets: $(\\mathbf{x}_{i}^{k},y_{i}^{k})_{i=1}^{n_{k}}$ , where $\\pmb{x}_{i}^{k}$ denotes the $i^{t h}$ data example specific to task $k$ , $y_{i}^{k}$ represents the associated data label for $\\pmb{x}_{i}^{k}$ . The primary objective is to train a neural network function, parameterized by $\\pmb{\\theta}$ , denoted as $g_{\\pmb\\theta}(\\pmb x)$ . The goal is to achieve good performance on the test datasets from all the learned tasks, represented as $\\mathcal{D}^{t e^{*}}{=\\{\\mathcal{D}_{1}^{t e},\\mathcal{D}_{2}^{t e},\\cdots\\}}\\mathcal{D}_{T}^{t e}\\bar{\\}$ , while ensuring that knowledge acquired from previous tasks is not forgotten. The CL loss function is defined as the following: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}^{C L}(\\pmb{\\theta}):=\\mathcal{L}_{C E}(\\pmb{x},y;\\pmb{\\theta})+\\zeta\\mathcal{L}_{f}(\\pmb{\\theta})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{L}_{C E}(\\pmb{x},y;\\pmb{\\theta})$ is the current task cross-entropy loss function. $\\mathcal{L}_{f}(\\pmb\\theta)$ is the forgetting-mitigation loss, e.g., memory-replay, weight-regularization and gradient-projection loss, etc. $\\zeta$ is a constant that balances the weight between the loss of the new task and the loss of the previous tasks. ", "page_idx": 3}, {"type": "text", "text": "Exponential Family of Distributions The exponential family distribution [70] is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{\\phi}(\\pmb\\theta):=h(\\pmb\\theta)e x p(\\langle\\phi,\\Omega(\\pmb\\theta)\\rangle-Z(\\phi))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $:=$ denotes a definition. In existing literature [70], $\\phi$ are called the natural parameters for defining the distribution, $P_{\\phi}(\\pmb\\theta).\\ \\ h(\\pmb\\theta)$ is the base measure, $\\Omega(\\theta)$ is the sufficient statistic, $\\begin{array}{r}{Z(\\phi):=\\,\\log\\int h(\\pmb{\\theta})e x p(\\langle\\phi,\\Omega(\\pmb{\\theta})\\rangle)d\\pmb{\\theta}}\\end{array}$ is the log-partition function, $\\langle,\\rangle$ denotes the dot product between two vectors. We denote the expectation parameters as $\\lambda:=\\mathbb{E}_{P_{\\phi}(\\pmb\\theta)}\\Omega(\\pmb\\theta)$ . We can write multivariate Gaussian distribution as canonical form of exponential family as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\theta;\\mu,\\Sigma):=\\frac{1}{(2\\pi)^{\\frac{d}{2}}d e t(\\Sigma)^{\\frac{1}{2}}}e x p\\{-\\frac{1}{2}(\\theta-\\mu)^{T}\\Sigma^{-1}(\\theta-\\mu)\\}}\\\\ &{\\qquad\\qquad=e x p\\{\\theta^{T}\\Sigma^{-1}\\mu-\\frac{1}{2}\\theta^{T}\\Sigma^{-1}\\theta-\\frac{1}{2}[d\\log2\\pi+\\log|\\Sigma|+\\mu^{T}\\Sigma^{-1}\\mu]\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, the correspondence between $f(\\theta;\\mu,\\Sigma)$ and exponential family distribution in Eq.(2) can be expressed as the following: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi:=\\big(\\Sigma^{-1}\\pmb{\\mu},-\\displaystyle\\frac{1}{2}\\Sigma^{-1}\\big),\\quad\\Omega(\\pmb{\\theta}):=(\\pmb{\\theta},\\pmb{\\theta}\\pmb{\\theta}^{T})}\\\\ &{\\pmb{\\lambda}^{1}:=\\mathbb{E}_{f(\\pmb{\\theta};\\pmb{\\mu},\\Sigma)}\\pmb{\\theta}=\\pmb{\\mu},\\quad\\pmb{\\lambda}^{2}:=\\mathbb{E}_{f(\\pmb{\\theta};\\pmb{\\mu},\\Sigma)}\\pmb{\\theta}\\pmb{\\theta}^{T}=\\pmb{\\mu}\\pmb{\\mu}^{T}+\\pmb{\\Sigma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Derivations details of Eq.(6) can be found in Appendix B.1. In the following section, we use exponential family distributions to parameterize the worst-case of CL model parameter distribution since this enables us to efficiently calculate the natural gradient in the expectation parameter space $\\lambda$ without needing to explicitly calculate the Fisher information matrix (FIM) in natural parameter space $\\phi$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Model Sensitivity Aware Continual Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Learning Objective Specifically, we propose the following CL learning objective to reduce the CL parameter sensitivity under model parameter updates: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\pmb{\\mu}}{\\operatorname*{min}}\\,\\underset{\\mathbb{U}\\in\\mathcal{U}}{\\operatorname*{max}}\\,\\mathbb{E}_{\\pmb{\\theta}\\sim\\mathbb{U}(\\pmb{\\theta})}\\mathcal{L}^{C L}(\\pmb{\\theta})}\\\\ &{\\quad\\quad\\mathrm{s.t.}\\;\\mathcal{U}=\\{\\mathbb{U}:D_{\\mathrm{KL}}(\\mathbb{U},\\mathbb{V})\\leq\\epsilon\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{U}}$ denotes the uncertainty set. $D_{\\mathrm{KL}}(\\mathbb{U},\\mathbb{V})$ denotes the KL divergence between the current CL model parameter distribution $\\mathbb{V}$ and the neighbour CL model parameter distribution $\\mathbb{U}$ . $\\epsilon$ is a small constant. $\\operatorname*{max}_{\\mathbb{U}\\in\\mathcal{U}}\\mathbb{E}_{\\pmb{\\theta}\\sim\\mathbb{U}(\\pmb{\\theta})}\\mathcal{L}^{C L}(\\pmb{\\theta})$ aims to find the worst-case CL model parameter distribution within a neighbourhood. We choose probabilistic distributional neighbourhood due to two-fold reasons: (1) the distributional neighbourhood covers more flexible parameter space; and (2) widely used SGD method incurs update noise during $\\mathrm{CL}$ , thereby distributional neighbourhood provides stronger guarantee against parameter sensitivity. It is important to note that the outer minimization is performed with respect to $\\pmb{\\mu}$ , the expectation of $\\pmb{\\theta}$ , since during inference, only $\\pmb{\\mu}$ is used as the model parameter for predictions. ", "page_idx": 3}, {"type": "text", "text": "Objective for Learning the Worst-Case CL Parameter Distribution We convert the constrained inner maximization optimization in Eq. (7) into the following unconstrained optimization to find the worst-case CL model parameter distribution. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\mathbb{U}}[H(\\mathbb{U}):=-\\mathbb{E}_{\\pmb{\\theta}\\sim\\mathbb{U}(\\pmb{\\theta})}\\mathcal{L}^{C L}(\\pmb{\\theta})+\\alpha D_{\\mathrm{KL}}(\\mathbb{U},\\mathbb{V})]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha>0$ is a constant. However, solving Eq. (8) is intractable since the optimization target is in an infinite-dimensional function space [32]. For computation efficiency, we set the current CL model parameter distribution as $\\mathbb{V}(\\pmb\\theta)=\\mathcal{N}(\\pmb\\theta|\\mu_{0},\\pmb\\Sigma_{0})$ , where $\\pmb{\\mu}_{0}$ and $\\Sigma_{0}$ denote the mean vector and covariance matrix, respectively. We set the neighbourhood distribution as $\\mathbb{U}(\\pmb\\theta)=\\mathcal{N}(\\pmb\\theta|\\pmb\\mu,\\pmb\\Sigma)$ , where $\\pmb{\\mu}$ and $\\Sigma$ denote the mean vector and covariance matrix, respectively. To further improve computational efficiency, we constrain the covariance matrix to be diagonal matrix, i.e., $\\Sigma=\\mathrm{diag}(\\sigma^{2})$ and $\\dot{\\Sigma}_{0}=\\mathrm{diag}(\\rho^{2})$ . We denote the density function of $\\mathbb{U}(\\pmb\\theta)$ and $\\mathbb{V}(\\pmb\\theta)$ as $u(\\pmb\\theta)$ and $v(\\theta)$ , respectively. We express the loss function in Eq. (8) as the following: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nH(\\mathbb{U})=\\mathbb{E}_{\\pmb{\\theta}\\sim u(\\pmb\\theta)}[\\mathcal{L}(\\pmb{\\mu},\\pmb{\\Sigma}):=-\\mathcal{L}^{C L}(\\pmb{\\theta})+\\alpha[\\log u(\\pmb\\theta)-\\log v(\\pmb\\theta)]]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By parameterizing the distribution $\\mathbb{U}$ as exponential family distribution in Eq. (4), our goal is to learn the parameters $\\phi$ in Eq. (5) with natural gradient descent (NGD) [42] as the following equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{i+1}=\\phi_{i}-\\eta{\\pmb F}^{-1}\\nabla_{\\phi}\\mathcal{L}(\\phi_{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{F}$ is the FIM. We opt for NGD because it adjusts the step size according to the curvature of the cost function, making convergence faster than traditional gradient descent methods. This is especially advantageous in high-dimensional spaces where the curvature and parameter-wise sensitivity vary significantly, benefiting CL models. However, computing the natural gradient is computationally intensive due to the need to calculate the FIM. To address this, we develop an efficient update method in the dual space, specifically the expectation parameter space $\\lambda$ , rather than the natural parameter space $\\phi$ , eliminating the need for explicit FIM calculation. In the following, we will use ${\\mathcal{L}}(\\phi)$ and $\\mathcal{L}(\\lambda)$ interchangeably, as they represent the same loss function only parameterized in different spaces. We leverage the relation between NGD in natural parameter space and gradient descent in expectation parameter space (in Appendix A.1), NGD can be performed without explicitly computing the FIM. This update in its dual space leads to significantly more efficient parameter updates and promising computational advantages. ", "page_idx": 4}, {"type": "text", "text": "NGD for Efficiently Finding the Worst-Case Gaussian Distribution In the following, we present specific algorithms for updating the $\\pmb{\\mu}$ and $\\Sigma$ with NGD to find the worst-case Gaussian distribution, i.e., $\\mathbb{U}^{*}:=\\arg\\operatorname*{min}_{\\mathbb{U}}H(\\mathbb{U})$ . We can get the following updates for mean $\\pmb{\\mu}$ and diagonal covariance $\\Sigma=\\mathrm{diag}(\\sigma^{2})$ (detailed derivations can be found in Appendix B): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\mu}_{i+1}=\\pmb{\\mu}_{i}+\\eta\\pmb{\\Sigma}_{i+1}[\\nabla_{\\pmb{\\theta}}\\mathcal{L}^{C L}(\\pmb{\\theta})-\\alpha(\\pmb{\\mu}_{i}-\\pmb{\\mu}_{0})\\pmb{\\Sigma}_{0}^{-1}]}\\\\ {\\pmb{\\Sigma}_{i+1}^{-1}=(1-\\eta\\alpha)\\pmb{\\Sigma}_{i}^{-1}+\\eta[-\\nabla_{\\pmb{\\theta}\\pmb{\\theta}}^{2}\\mathcal{L}^{C L}(\\pmb{\\theta})+\\alpha\\pmb{\\Sigma}_{0}^{-1}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By plug-in $\\Sigma=\\mathrm{diag}(\\sigma^{2})$ and $\\Sigma_{0}=\\mathrm{diag}(\\rho^{2})$ into the above equations, we can obtain the following updates: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\mu}_{i+1}=\\pmb{\\mu}_{i}+\\eta\\pmb{\\sigma}_{i+1}^{2}[\\nabla_{\\pmb{\\theta}}\\mathcal{L}^{C L}(\\pmb{\\theta}_{i})-\\alpha(\\pmb{\\mu}_{i}-\\pmb{\\mu}_{0})\\pmb{\\rho}^{-2}]}\\\\ {\\pmb{\\sigma}_{i+1}^{-2}=(1-\\eta\\alpha)\\pmb{\\sigma}_{i}^{-2}+\\eta[-\\nabla_{\\pmb{\\theta}\\pmb{\\theta}}^{2}\\mathcal{L}^{C L}(\\pmb{\\theta}_{i})+\\alpha\\pmb{\\rho}^{-2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In practice, we set $\\alpha=1.0$ to reduce the reliance on hyperparameters. However, computing the diagonal Hessian matrix $\\nabla_{\\theta\\theta}^{2}\\mathcal{L}^{C L}(\\pmb{\\theta})$ in Eq. (14) is a computationally challenging task. Following [42], we efficiently approximate the Hessian as the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\theta^{k}\\theta^{k}}^{2}\\mathcal{L}^{C L}(\\pmb{\\theta})=\\frac{1}{N}\\sum_{j=1}^{j=N}[\\nabla_{\\pmb{\\theta}^{k}}\\mathcal{L}_{j}^{C L}(\\pmb{\\theta})]^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N$ is the number of training data points for the current task, $\\mathcal{L}_{j}^{C L}(\\pmb{\\theta})$ denotes the loss function for the data point $j,\\pmb\\theta^{k}$ denotes the $k^{t h}$ element of the model parameter $\\pmb{\\theta}$ . It is crucial to note that this Hessian approximation is computed only once after learning each task and involves calculating only the diagonal elements, i.e., $\\Sigma\\,\\bar{=}\\,\\mathrm{diag}(\\sigma^{\\bar{2}})$ . As a result, the overall computational cost throughout the continual learning process remains low. Additionally, this update mechanism maintains the same number of learnable parameters as existing methods, ensuring fair comparisons. This is because, during the learning of each task, only the mean parameters of the Gaussian distribution are updated. ", "page_idx": 4}, {"type": "text", "text": "Learning Algorithm We name our method as Model sensitivity Aware Continual Learning (MACL). The detailed algorithm is present in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Model Sensitivity Aware Continual Learning ", "page_idx": 5}, {"type": "text", "text": "1: REQUIRE: model parameters $\\pmb{\\theta}$ , CL model learning rate $\\beta$ , worst-case Gaussian learning rate $\\eta$ , number of CL tasks $T$ , number of CL steps $K$ for each task, distribution neighbourhood regularization strengths $\\alpha=1.0$ . Randomly initialized diagonal covariance matrix, i.e., $\\mathrm{diag}(\\breve{\\sigma}^{2})$ .   \n2: for $n=1$ to $T$ do   \n3: for $i=1$ to $K$ do   \n4: calculate the CL loss function according to Eq. (1)   \n5: update the worst-case Gaussian mean $\\pmb{\\mu}$ (i.e., $\\pmb{\\theta}$ ) by $\\pmb{\\theta}_{i}^{\\prime}=\\pmb{\\theta}_{i}+\\eta\\pmb{\\sigma}_{n}^{2}[\\nabla_{\\pmb{\\theta}}\\mathcal{L}^{C L}(\\pmb{\\theta}_{i})-(\\pmb{\\theta}_{i}-\\pmb{\\theta}_{0})\\pmb{\\rho}^{-2}]$   \n6: sample parameters from the worst-case CL model parameter distribution. ${\\pmb\\theta}^{\\prime}={\\pmb\\theta}_{i}^{\\prime}+{\\pmb\\sigma}_{n}\\zeta$ , where $\\zeta\\sim\\!\\mathcal{N}(\\mathbf{0},I)$   \n7: update CL model parameters $\\pmb{\\theta}_{i+1}=\\pmb{\\theta}^{\\prime}-\\beta\\nabla_{\\pmb{\\theta}}\\mathcal{L}^{C L}(\\pmb{\\theta}^{\\prime})$   \n8: end for   \n9: update the worst-case Gaussian covariance $\\pmb{\\sigma}$ by $\\pmb{\\sigma}_{n+1}^{-2}=(1-\\eta)\\pmb{\\sigma}_{n}^{-2}+\\eta[-\\nabla_{\\theta\\theta}^{2}\\mathcal{L}^{C L}(\\pmb{\\theta})+\\pmb{\\rho}^{-2}]$   \n10: where the Hessian is calculated by $\\begin{array}{r}{\\nabla_{\\theta^{k}\\theta^{k}}^{2}\\mathcal{L}^{C L}(\\theta)=\\frac{1}{N}\\sum_{j=1}^{j=N}[\\nabla_{\\theta^{k}}\\mathcal{L}_{j}^{C L}(\\theta)]^{2}}\\end{array}$ according to Eq. (15) ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we build the theoretical connection between MACL and parameter sensitivity in Theorem 4.2 and the generalization analysis in Theorem 4.3. Due to the space limitations, we provide the theorem proof in Appendix A.2. Let\u2019s first look at the inner maximization problem in Eq. (7). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbb{U}\\in\\mathcal{U}}\\int\\mathcal{L}^{C L}(\\pmb{\\theta})d\\mathbb{U}(\\pmb{\\theta}),\\mathrm{~\\\\s.t.~}\\mathcal{U}=\\{\\mathbb{U}:D_{\\mathrm{KL}}(\\mathbb{U},\\mathbb{V})\\leq\\epsilon\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 4.1. $\\begin{array}{r}{D_{\\mathrm{KL}}(\\mathbb{U},\\mathbb{V})=\\int u(\\pmb{\\theta})\\log(\\frac{u(\\pmb{\\theta})}{v(\\pmb{\\theta})})d\\pmb{\\theta}\\leq\\int\\frac{(u(\\pmb{\\theta})-v(\\pmb{\\theta}))^{2}}{v(\\pmb{\\theta})}d\\pmb{\\theta}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. Assume $\\begin{array}{r}{\\int||\\frac{1}{v(\\pmb\\theta)}||_{\\infty}d\\pmb\\theta\\leq M}\\end{array}$ , we can obtain the following conclusion for Eq. (16): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbb{U}\\in\\mathcal{U}}\\int\\mathcal{L}^{C L}(\\pmb{\\theta})d\\mathbb{U}(\\pmb{\\theta})=\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}+\\sqrt{\\frac{\\epsilon\\mathbb{E}(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})^{2}}{M}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}~:=~\\int\\mathcal{L}^{C L}(\\pmb{\\theta})d\\mathbb{V}(\\pmb{\\theta})}\\end{array}$ . $V a r(\\mathcal{L}^{C L}(\\pmb{\\theta}))$ denotes the variance of $\\mathcal{L}^{C L}(\\pmb{\\theta})$ with respect to different model parameters variations, i.e., $\\begin{array}{r l}{\\phantom{\\sum}}&{{}\\:\\:V a r(\\mathcal{L}^{C L}(\\pmb{\\theta}))\\mathrm{~}=\\mathbb{E}(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})^{2}=}\\end{array}$ $\\begin{array}{r}{\\int(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})^{2}d\\pmb{\\theta}}\\end{array}$ . In this context, $V a r(\\mathcal{L}^{C L}(\\pmb{\\theta}))$ serves as a measure of the CL model\u2019s sensitivity to parameter updates. Essentially, a smaller loss variance indicates lower parameter sensitivity in the CL model. However, directly optimizing the loss variance within the parameter distribution neighborhood is impractical, as it requires computing the loss variation across a large number of different sets of CL model parameters and training data points. In contrast, our method (MACL) offers an efficient and effective alternative. MACL implicitly minimizes the loss variance across different model parameter variations by optimizing CL performance solely on the worst-case CL model parameter distribution. In the following, inspired by UDIL [64], we further provide the following generalization bound for CL: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 (Generalization bound of MACL). Let q be the number of $C L$ model parameters and n be the number of training data points. The $C L$ loss ${\\mathcal{L}}^{C L}(\\pmb{\\theta})\\leq C$ ( $C$ is a constant). With high probability of $1-\\delta_{i}$ , the following bound holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\!N(\\mu,\\Sigma)}\\sum_{i}^{i=T}\\mathcal{L}_{\\mathcal{D}_{i}}^{C L}(\\theta)\\le\\operatorname*{max}_{\\mathbb{U}\\in\\mathcal{U}}\\mathbb{E}_{\\theta\\sim\\mathbb{U}}\\mathcal{L}^{C L}(\\theta)+\\frac{C}{N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}}+\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{\\tau^{2}(\\sqrt{q}+\\sqrt{2\\log(N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}}))^{2}+R+2\\log(\\frac{N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}}{\\delta})}{4(N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}-1)}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Where $\\tau$ is a constant. We denote the number of data examples for task $1,\\cdots,T-1$ in the memory buffer $\\mathcal{M}$ during training on task $T$ as $N_{1},N_{2},\\cdots\\,,N_{T-1}$ when using memory replay based approach or the number of training data points when using regularization based approach. $\\mathcal{L}_{\\mathcal{D}_{i}}^{C L}(\\pmb{\\theta})$ denotes the loss on the data from data distribution $\\mathcal{D}_{i}$ of task (generalization error), i.e., it is defined as: $\\begin{array}{r}{\\mathcal{L}_{\\mathcal{D}_{i}}^{C L}(\\pmb{\\theta}):=\\mathbb{E}_{(\\pmb{x},y)\\sim\\mathcal{D}_{i}}\\mathcal{L}(\\pmb{x},y,\\pmb{\\theta})}\\end{array}$ . $\\mathcal{L}^{C L}(\\pmb{\\theta})$ denotes the empirical $C L$ loss as Eq. $(I)$ . $\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma})$ denotes the $C L$ model parameter posterior distribution parameterized with Gaussian distribution. ", "page_idx": 5}, {"type": "text", "text": "Generalization bound implication: (1) When using a memory-replay approach, the number of samples from new tasks often exceeds the number of samples in the memory buffer, causing data imbalance. This imbalance, where fewer samples from previous tasks are stored, affects the second and third terms in the generalization bound. The bound suggests that as the number of samples in the memory buffer increases (i.e., ii==1T \u22121Ni \u2191), these terms tighten, leading to a tighter generalization upper bound. This is becau se $\\begin{array}{r}{\\operatorname*{lim}_{x\\to\\infty}[h(x):=\\frac{\\log x}{x}]=0}\\end{array}$ , meaning the generalization improves with a larger buffer, aligning with the intuition that more memory buffer data leads to better performance. (2) In the regularization-based approach, \u03b6 ii==1T \u22121N is treated as the effective sample size for previous tasks since the loss is approximated in the absence of earlier data. The parameter $\\zeta$ controls the trade-off between learning the new task and retaining knowledge from past tasks. A larger $\\zeta$ increases regularization, preventing the model from deviating too much from the parameters learned on previous tasks. This leads to higher empirical loss on the new task (first term), but tighter bounds (second and third terms), indicating that knowledge from previous tasks is retained effectively. This prioritizes stability over learning flexibility for the new task. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets We conduct experiments on several datasets, including CIFAR10 (10 classes), CIFAR100 (100 classes) [29], and Tiny-ImageNet (200 classes) [80], to assess the effectiveness of MACL in task incremental learning (Task-IL) and class incremental learning (Class-IL). In addition, we also conduct experiments on 5-dataset [79, 5], CUB200 [69] and ImageNet-R [20] (in Appendix). Following the approach in [7], we split the CIFAR-10 dataset into five tasks, each with two distinct classes. We divided the CIFAR-100 dataset into ten tasks, each containing ten classes. We split the Tiny-ImageNet dataset into ten tasks, each comprising twenty classes. More dataset statistics can be found in Appendix E.1. ", "page_idx": 6}, {"type": "text", "text": "Baselines We compare to the following various SOTA CL methods. (1) Regularization-based methods, including oEWC [62], synaptic intelligence (SI) [84], Learning without Forgetting (LwF) [35], Classifier-Projection Regularization (CPR) [10]. (2) Bayesian-based methods, including NCL [25]. (3) Architecture-based methods, including HAT [63]. (4) Memory-based methods, including ER [15], A-GEM [14], iCaRL[55], GSS [2], HAL [12], $\\mathrm{DER++}$ [7], ER-ACE [8] and LODE [36]. (5) Gradient-projection-based methods: Gradient Projection Memory (GPM) [60]. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details Following [7], we use ResNet18 [19] as the backbone network for all the CL datasets and compared baseline methods. For the baselines that are included in the open-source code of $\\mathrm{DER++}$ [7], we use the same hyperparameters provided in $\\mathrm{DER++}$ [7] for the compared methods. For the baselines not included in the open-source code of $\\mathrm{DER++}$ , e.g., GPM, LODE, etc, we use the open-source code from their original paper for comparisons. For the hyperparameters in our method, we set $\\alpha=1.0$ across all the datasets to minimize the model\u2019s dependence on hyperparameters. For $\\eta$ , we set $\\eta=1e-5$ for CIFAR10 and CIFAR100, and $\\eta=1e-6$ for Tiny-ImageNet. The $\\eta$ is selected from the range of $[1e-4,1e-5,1e-6,1e-7]$ . Following [7, 14], the hyperparameter is determined through the validation sets split from the training sets from the first three tasks. Similar to [7], we train all the CL models using the standard SGD optimizer to update the CL model. The batch size and replay buffer batch size are set to 32. We use a single NVIDIA A5000 GPU with 24GB memory to run the experiments. Each experiment result is averaged for 10 runs with mean and standard deviation. ", "page_idx": 6}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the performance of different CL methods with (1) overall accuracy; (2) new task accuracy;   \nand (3) backward transfer in the following. ", "page_idx": 6}, {"type": "text", "text": "Overall Accuracy (ACC) ACC is the average accuracy across the entire task sequence. We present the results on CIFAR10, CIFAR-100 and Tiny-ImageNet in Table 1. We can observe that our method substantially improve over various SOTA baseline methods up to $3\\%$ to $4\\%$ on CIFAR100, TinyImageNet by integrating MACL with existing CL methods. This overall performance improvement is attributed to the reduced parameter sensitivity. ", "page_idx": 6}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/95e7a19851a10e19a27f2fe95ef804e9a39a1a5987b90a9ba6a7f88827b921aa.jpg", "table_caption": ["Table 1: Task-IL and class-IL overall accuracy on CIFAR10, CIFAR-100 and Tiny-ImageNet, respectively with memory size 500. \u2019\u2014\u2019 indicates not applicable/available. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "New Task Accuracy To evaluate the new task performance of the proposed CL method, we evaluate the new task performance during CL by integrating MACL with $\\mathrm{DER++}$ and GPM in Figure 1. The results show that MACL can significantly improves the new task performance for different CL methods, indicating that reducing the model parameter sensitivity is beneficial to improve new task performance during CL. ", "page_idx": 7}, {"type": "text", "text": "Backward Transfer Backward transfer (BWT) quantifies the degree of forgetting observed on previously learned tasks. When $\\mathrm{BWT}>\\mathrm{~0~}$ , it indicates that learning the current new task positively influences the performance on previously learned tasks. Conversely, when $\\mathrm{BWT}\\leq\\ 0$ , it signals that learning the current new task may result in forgetting previously acquired knowledge. We evaluate BWT in Table 2. We can observe that our method significantly improves BWT by up to $5\\%$ through integrating MACL with existing CL methods. This indicates that reducing parameter sensitivity can substantially reduce forgetting on previously learned knowledge. These empirical ", "page_idx": 7}, {"type": "image", "img_path": "B5vQ7IQW7d/tmp/60be10c0d05c7d22fc0f019edf0dee7d24cc5aac75f2d80cd53c0ef24bf16322.jpg", "img_caption": ["Figure 1: new task performance during CL. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "analysis also verify our theoretical analysis that our method implicitly improves the stability by reducing loss variance. ", "page_idx": 7}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/76cb7ed908121587440ac86fef3c5d97fd983d46d7fe3e3802f329f4f8eace02.jpg", "table_caption": ["Table 2: Backward Transfer of different CL methods with memory size 500. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Hyperparameter Analysis We evaluate the sensitivity of the hyperparameters $\\eta$ in Table 5 in Appendix D.1. Our observations indicate that when parameter sensitivity is not reduced, i.e., $\\eta=0$ , the CL model performs poorly. As we gradually increase the reduction of parameter sensitivity, the CL model\u2019s performance improves. This improvement is because appropriately reducing parameter sensitivity helps mitigate forgetting and enhances learning for new tasks, thus boosting overall CL performance. However, if the reduction in parameter sensitivity is increased excessively, the model\u2019s performance deteriorates. This is because an overly constrained model, while minimizing forgetting, struggles to learn new tasks effectively, resulting in worse performance. ", "page_idx": 8}, {"type": "text", "text": "Effect of Memory Size To assess the impact of varying memory buffer sizes, we present the results in Table 3. The results demonstrate that compared to different baseline methods, our MACL plug-in also enhances the performance of baseline methods with a memory size of 2000. ", "page_idx": 8}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/56addb7f357c3be56317760866f810bc9ea94a5a95113135b1d045b2ec4a6a3e.jpg", "table_caption": ["Table 3: Task-IL and class-IL overall accuracy on CIFAR-100 and Tiny-ImageNet, respectively with memory size 2000. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Benefit of NGD To evaluate the benefits of using NGD over gradient descent (GD) for calculating the worst-case Gaussian distribution, we present comparison results in Table 6 in Appendix D.2. The results show that NGD outperforms GD because NGD better captures parameter importance, which helps preserve old knowledge while effectively adapting to new tasks. ", "page_idx": 8}, {"type": "text", "text": "Efficiency Evaluation To assess the efficiency of our proposed method, we compare the running time of integration of different CL methods with MACL and corresponding CL methods alone on CIFAR100, as shown in Table 15 in Appendix D.8. The results indicate that incorporating MACL increases the computational cost by only $55\\%$ to $61\\%$ compared to the corresponding CL methods alone. This demonstrates the high efficiency of our method, as it introduces only small additional training cost. ", "page_idx": 8}, {"type": "text", "text": "Effect of Different Architectures To evaluate the impact of different architectures, we compared various approaches using both ViT and ResNet32. For the ResNet32 experiments, we followed the setup in [85], integrating MACL with MEMO [86] and comparing it to MEMO alone, using a memory buffer size of 2000 on CIFAR100. Additionally, we conducted experiments with a pre-trained Vision Transformer (ViT) [16], specifically the vit-base-patch16-224 model pre-trained on ImageNet1K. On CIFAR100, we integrated MACL with $\\mathrm{DER++}$ , using a memory size of 500, and demonstrated that using a pre-trained ViT significantly improves CL performance. Moreover, combining MACL with $\\mathrm{DER++}$ further enhances CL performance with the pre-trained ViT. The results are presented in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "Long Task Sequence To assess the effectiveness of the proposed approach across varying task lengths, we conducted experiments by splitting Tiny-ImageNet into sequences of 10 and 20 tasks. The Task-IL and Class-IL results for integrating $\\mathrm{DER++}$ with MACL, using a memory buffer size of 500, are presented in Table 4. These results demonstrate that even with longer task sequences, our method still significantly outperforms $\\mathrm{DER++}$ . ", "page_idx": 9}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/ee8f1ad85be8cb38a7d0b40fec3a2d7dd01f3036fc117102465a98a39f910755.jpg", "table_caption": ["Table 4: Overall accuracy of integrating $\\mathrm{DER++}$ with MACL using a memory buffer of 500 and longer task sequence on Tiny-ImageNet. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Online CL Under the online CL setting, we evaluate the effectiveness of the proposed approach on CIFAR100 and Tiny-ImageNet by comparing with MKD [46] and PCR [37]. The results are put in the Appendix. ", "page_idx": 9}, {"type": "text", "text": "5-datasets results To assess the effectiveness of MACL on the 5-Datasets benchmark [79, 5], which includes CIFAR-10, MNIST [33], Fashion-MNIST [81], SVHN [47], and notMNIST [6], we conducted experiments. This dataset provides a diverse range of CL tasks. We performed experiments on 5-Datasets, using a memory buffer size of 500, with MACL. The detailed results are provided in the Appendix. ", "page_idx": 9}, {"type": "text", "text": "ImageNet-R and CUB200 results We further evaluate the effectiveness of MACL on CUB200 [69] and ImageNet-R [20], the results are shown in the Appendix. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we address the challenge of balancing learning new tasks while preserving knowledge from previous ones in continual learning. We propose a model sensitivity-aware continual learning method that enhances both the model\u2019s ability to retain old knowledge and improve performance on new tasks. Specifically, our goal is to reduce model parameter sensitivity by optimizing CL performance for the worst-case parameter distribution within the neighborhood of the current model\u2019s parameter distribution. This approach improves stability in preserving old knowledge and mitigates overftiting on new tasks. We provide a comprehensive theoretical analysis of the proposed method, and extensive experiments on multiple datasets demonstrate its effectiveness, efficiency, and versatility. ", "page_idx": 9}, {"type": "text", "text": "Limitation Discussion Our method introduces additional training cost compared to existing continual learning approaches. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our work advances continual learning, which is beneficial to develop more adaptable and efficient AI.   \nOur work has no negative societal impacts. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by NSF IIS 2347592, 2347604, 2348159, 2348169, DBI 2405416, CCF 2348306, CNS 2347617. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European conference on computer vision, pages 139\u2013154, 2018.   \n[2] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems 30, 2019.   \n[3] Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: A general continual learning method based on complementary learning system. In International Conference on Learning Representations, 2022.   \n[4] Lorenzo Bonicelli, Matteo Boschini, Angelo Porrello, Concetto Spampinato, and Simone Calderara. On the effectiveness of lipschitz-driven rehearsal in continual learning. Advances in Neural Information Processing Systems, 35:31886\u201331901, 2022.   \n[5] Jorg Bornschein, Alexandre Galashov, Ross Hemsley, Amal Rannen-Triki, Yutian Chen, Arslan Chaudhry, Xu Owen He, Arthur Douillard, Massimo Caccia, Qixuan Feng, et al. Nevis\u2019 22: A stream of 100 tasks sampled from 30 years of computer vision research. Journal of Machine Learning Research, 24(308):1\u201377, 2023.   \n[6] Yaroslav Bulatov. Notmnist dataset. 2011.   \n[7] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920\u201315930, 2020.   \n[8] Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and Eugene Belilovsky. New insights on reducing abrupt representation change in online continual learning. In International Conference on Learning Representations, 2022.   \n[9] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. Advances in Neural Information Processing Systems, 34:22405\u201322418, 2021.   \n[10] Sungmin Cha, Hsiang Hsu, Taebaek Hwang, Flavio Calmon, and Taesup Moon. Cpr: Classifier-projection regularization for continual learning. In International Conference on Learning Representations, 2021.   \n[11] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the European conference on computer vision, pages 532\u2013547, 2018.   \n[12] Arslan Chaudhry, Albert Gordo, Puneet K Dokania, Philip HS Torr, and David Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. Association for the Advancement of Artificial Intelligence, 2021.   \n[13] Arslan Chaudhry, Naeemullah Khan, Puneet Dokania, and Philip Torr. Continual learning in low-rank orthogonal subspaces. Advances in Neural Information Processing Systems, 33:9900\u20139911, 2020.   \n[14] Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. Proceedings of the International Conference on Learning Representations, 2019.   \n[15] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K. Dokania, Philip H. S. Torr, and Marc\u2019Aurelio Ranzato. Continual learning with tiny episodic memories. https://arxiv.org/abs/1902.10486, 2019.   \n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[17] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual learning. In International Conference on Artificial Intelligence and Statistics, pages 3762\u20133773. PMLR, 2020.   \n[18] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021.   \n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[20] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8340\u20138349, 2021.   \n[21] Christian Henning, Maria Cervera, Francesco D\u2019Angelo, Johannes Von Oswald, Regina Traber, Benjamin Ehret, Seijin Kobayashi, Benjamin F Grewe, and Jo\u00e3o Sacramento. Posterior meta-replay for continual learning. Advances in Neural Information Processing Systems, 34:14135\u201314149, 2021.   \n[22] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 831\u2013839, 2019.   \n[23] Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song Chen. Compacting, picking and growing for unforgetting continual learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[24] P Izmailov, AG Wilson, D Podoprikhin, D Vetrov, and T Garipov. Averaging weights leads to wider optima and better generalization. In 34th Conference on Uncertainty in Artificial Intelligence, pages 876\u2013885, 2018.   \n[25] Ta-Chu Kao, Kristopher Jensen, Gido van de Ven, Alberto Bernacchia, and Guillaume Hennequin. Natural continual learning: success is a journey, not (just) a destination. Advances in neural information processing systems, 34:28067\u201328079, 2021.   \n[26] Mohammad Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and scalable bayesian deep learning by weight-perturbation in adam. In International conference on machine learning, pages 2611\u20132620. PMLR, 2018.   \n[27] Minyoung Kim, Da Li, Shell X Hu, and Timothy Hospedales. Fisher sam: Information geometry and sharpness aware minimisation. In International Conference on Machine Learning, pages 11148\u201311161. PMLR, 2022.   \n[28] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.   \n[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[30] Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick Van Der Smagt, and Stephan G\u00fcnnemann. Continual learning with bayesian neural networks for non-stationary data. In International Conference on Learning Representations, 2019.   \n[31] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of statistics, pages 1302\u20131338, 2000.   \n[32] Peter D Lax. Functional analysis, volume 55. John Wiley & Sons, 2002.   \n[33] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[34] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In International Conference on Machine Learning, pages 3925\u20133934. PMLR, 2019.   \n[35] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.   \n[36] Yan-Shuo Liang and Wu-Jun Li. Loss decoupling for task-agnostic continual learning. Advances in Neural Information Processing Systems, 36, 2023.   \n[37] Huiwei Lin, Baoquan Zhang, Shanshan Feng, Xutao Li, and Yunming Ye. Pcr: Proxy-based contrastive replay for online class-incremental continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24246\u201324255, 2023.   \n[38] Sen Lin, Li Yang, Deliang Fan, and Junshan Zhang. Trgp: Trust region gradient projection for continual learning. In International Conference on Learning Representations, 2022.   \n[39] Chenxi Liu, Zhenyi Wang, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng Guo, and Heng Huang. Fewshot class incremental learning with attention-aware self-adaptive prompt. In European Conference on Computer Vision, 2024.   \n[40] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.   \n[41] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 7765\u20137773, 2018.   \n[42] James Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):1\u201376, 2020.   \n[43] David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual conference on Computational learning theory, pages 164\u2013170, 1999.   \n[44] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109\u2013165. Elsevier, 1989.   \n[45] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. An empirical investigation of the role of pre-training in lifelong learning. Journal of Machine Learning Research, 24(214):1\u201350, 2023.   \n[46] Nicolas Michel, Maorong Wang, Ling Xiao, and Toshihiko Yamasaki. Rethinking momentum knowledge distillation in online continual learning. In Forty-first International Conference on Machine Learning, 2024.   \n[47] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 4. Granada, 2011.   \n[48] Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning. In International Conference on Learning Representations, 2018.   \n[49] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard Turner, and Mohammad Emtiyaz E Khan. Continual deep learning by functional regularisation of memorable past. Advances in Neural Information Processing Systems, 33:4453\u20134464, 2020.   \n[50] Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical University of Denmark, 7(15):510, 2008.   \n[51] Quang Pham, Chenghao Liu, and Steven HOI. Dualnet: Continual learning, fast and slow. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \n[52] Jingyang Qiao, Xin Tan, Chengwei Chen, Yanyun Qu, Yong Peng, Yuan Xie, et al. Prompt gradient projection for continual learning. In The Twelfth International Conference on Learning Representations, 2024.   \n[53] Garvesh Raskutti and Sayan Mukherjee. The information geometry of mirror descent. IEEE Transactions on Information Theory, 61(3):1451\u20131457, 2015.   \n[54] Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. Psychological review, 97(2):285, 1990.   \n[55] Sylvestre-Alvise Rebuff,i Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001\u20132010, 2017.   \n[56] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278\u20131286. PMLR, 2014.   \n[57] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In International Conference on Learning Representations, 2019.   \n[58] Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations for overcoming catastrophic forgetting. Advances in Neural Information Processing Systems, 31, 2018.   \n[59] Tim GJ Rudner, Freddie Bickford Smith, Qixuan Feng, Yee Whye Teh, and Yarin Gal. Continual learning via sequential function-space variational inference. In International Conference on Machine Learning, pages 18871\u201318887. PMLR, 2022.   \n[60] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient projection memory for continual learning. ICLR, 2021.   \n[61] Fahad Sarfraz, Elahe Arani, and Bahram Zonooz. Error sensitivity modulation based experience replay: Mitigating abrupt representation drift in continual learning. In The Eleventh International Conference on Learning Representations, 2023.   \n[62] Jonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress and compress: A scalable framework for continual learning. In Proceedings of the International Conference on Machine Learning, 2018.   \n[63] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In International conference on machine learning, pages 4548\u20134557. PMLR, 2018.   \n[64] Haizhou Shi and Hao Wang. A unified approach to domain incremental learning with memory: Theory and algorithm. Advances in Neural Information Processing Systems, 36, 2023.   \n[65] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Continual decomposed attentionbased prompting for rehearsal-free continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11909\u201311919, 2023.   \n[66] Michalis K. Titsias, Jonathan Schwarz, Alexander G. de G. Matthews, Razvan Pascanu, and Yee Whye Teh. Functional regularisation for continual learning with gaussian processes. In International Conference on Learning Representations, 2020.   \n[67] Gido M van de Ven, Tinne Tuytelaars, and Andreas S Tolias. Three types of incremental learning. Nature Machine Intelligence, 4(12):1185\u20131197, 2022.   \n[68] Eli Verwimp, Matthias De Lange, and Tinne Tuytelaars. Rehearsal revealed: The limits and merits of revisiting samples in continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9385\u20139394, 2021.   \n[69] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.   \n[70] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132):1\u2013305, 2008.   \n[71] Shipeng Wang, Xiaorong Li, Jian Sun, and Zongben Xu. Training networks in null space of feature covariance for continual learning. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 184\u2013193, 2021.   \n[72] Zhenyi Wang, Yan Li, Li Shen, and Heng Huang. A unified and general framework for continual learning. In The Twelfth International Conference on Learning Representations, 2024.   \n[73] Zhenyi Wang, Li Shen, Tiehang Duan, Qiuling Suo, Le Fang, Wei Liu, and Mingchen Gao. Distributionally robust memory evolution with generalized divergence for continual learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[74] Zhenyi Wang, Li Shen, Tiehang Duan, Donglin Zhan, Le Fang, and Mingchen Gao. Learning to learn and remember super long multi-domain task sequence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7982\u20137992, June 2022.   \n[75] Zhenyi Wang, Li Shen, Le Fang, Qiuling Suo, Tiehang Duan, and Mingchen Gao. Improving task-free continual learning by distributionally robust memory evolution. In International Conference on Machine Learning, pages 22985\u201322998. PMLR, 2022.   \n[76] Zhenyi Wang, Li Shen, Le Fang, Qiuling Suo, Donglin Zhan, Tiehang Duan, and Mingchen Gao. Metalearning with less forgetting on large-scale non-stationary task distributions. In European Conference on Computer Vision, pages 221\u2013238. Springer, 2022.   \n[77] Zhenyi Wang, Li Shen, Donglin Zhan, Qiuling Suo, Yanjun Zhu, Tiehang Duan, and Mingchen Gao. Metamix: Towards corruption-robust continual learning with temporally self-adaptive data transformation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24521\u2013 24531, 2023.   \n[78] Zifeng Wang, Zheng Zhan, Yifan Gong, Yucai Shao, Stratis Ioannidis, Yanzhi Wang, and Jennifer Dy. Dualhsic: Hsic-bottleneck and alignment for continual learning. In International Conference on Machine Learning, 2023.   \n[79] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 139\u2013149, 2022.   \n[80] Jiayu Wu, Qixiang Zhang, and Guoxi Xu. Tiny imagenet challenge. Technical report, 2017.   \n[81] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[82] Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, and Zhouchen Lin. Hebbian learning based orthogonal projection for continual learning of spiking neural networks. In The Twelfth International Conference on Learning Representations, 2024.   \n[83] Enneng Yang, Li Shen, Zhenyi Wang, Tongliang Liu, and Guibing Guo. An efficient dataset condensation plugin and its application to continual learning. Advances in Neural Information Processing Systems, 36, 2023.   \n[84] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International conference on machine learning, pages 3987\u20133995. PMLR, 2017.   \n[85] Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Class-incremental learning: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[86] Da-Wei Zhou, Qi-Wei Wang, Han-Jia Ye, and De-Chuan Zhan. A model or 603 exemplars: Towards memory-efficient class-incremental learning. In The Eleventh International Conference on Learning Representations, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Theorem Proof ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Duality in Natural Gradient Descent for Exponential Family Distribution ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem A.1. Gradient of the loss $\\mathcal{L}(\\lambda)$ with respect to the expectation parameter $\\lambda$ , i.e., $\\nabla_{\\lambda}\\mathcal{L}(\\lambda)$ , is equal to the natural gradient with respect to natural parameter $\\phi$ , i.e., $F^{-1}\\nabla_{\\phi}\\mathcal{L}(\\phi)$ . This can be expressed as the following: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\lambda}\\mathcal{L}(\\lambda)=F^{-1}\\nabla_{\\phi}\\mathcal{L}(\\phi)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular, NGD in natural parameter space can be equivalently performed through gradient descent with respect to the expectation parameters as the following: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi_{i+1}=\\phi_{i}-\\eta{\\pmb F}^{-1}\\nabla_{\\phi}\\mathcal{L}(\\phi_{i})=\\phi_{i}-\\eta\\nabla_{\\lambda}\\mathcal{L}(\\pmb{\\lambda}_{i})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where \u03b7 is the learning rate and $\\pmb{F}$ is the Fisher information matrix (FIM). ", "page_idx": 15}, {"type": "text", "text": "Proof. The exponential family distribution is defined as the following: ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{\\phi}(\\pmb\\theta)=e x p(\\langle\\phi,\\Omega(\\pmb\\theta)\\rangle-Z(\\phi))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "According to the expectation of the score function is 0, we can obtain the following ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf0=\\mathbb{E}_{P_{\\phi}(\\theta)}\\nabla_{\\phi}\\log P_{\\phi}(\\theta)=\\mathbb{E}_{P_{\\phi}(\\theta)}[\\Omega(\\theta)-\\nabla_{\\phi}Z(\\phi)]=\\lambda-\\nabla_{\\phi}Z(\\phi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda=\\nabla_{\\phi}Z(\\phi)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first equality is due to the fact that the expectation of the score function is zero. We then derive the Fisher information matrix (FIM) as the following: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\phi):=\\mathbb{E}_{P_{\\phi}(\\theta)}[-\\nabla_{\\phi}^{2}\\log P_{\\phi}(\\theta)]}\\\\ &{\\quad\\quad=\\mathbb{E}_{P_{\\phi}(\\theta)}[-\\nabla_{\\phi}(\\nabla_{\\phi}\\log P_{\\phi}(\\theta))]}\\\\ &{\\quad\\quad=\\mathbb{E}_{P_{\\phi}(\\theta)}[-\\nabla_{\\phi}(\\nabla_{\\phi}(\\langle\\phi,\\Omega(\\theta)\\rangle-Z(\\phi))]}\\\\ &{\\quad\\quad=\\mathbb{E}_{P_{\\phi}(\\theta)}[-\\nabla_{\\phi}(\\Omega(\\theta)-\\nabla_{\\phi}Z(\\phi))]}\\\\ &{\\quad\\quad=\\nabla_{\\phi}\\lambda}\\\\ &{\\quad\\quad=\\nabla_{\\phi}\\nabla_{\\phi}Z(\\phi)}\\\\ &{\\quad\\quad=\\nabla_{\\phi}^{2}Z(\\phi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $:=$ denotes defined as. Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}\\lambda=\\nabla_{\\phi}^{2}Z(\\phi)=F\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\lambda}\\mathcal{L}(\\phi)=\\nabla_{\\lambda}\\phi\\nabla_{\\phi}\\mathcal{L}(\\phi)=[\\nabla_{\\phi}\\lambda]^{-1}\\nabla_{\\phi}\\mathcal{L}(\\phi)=F^{-1}\\nabla_{\\phi}\\mathcal{L}(\\phi)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "More general results on manifold can be found in [53]. ", "page_idx": 15}, {"type": "text", "text": "A.2 Theoretical and Generalization Analysis of MACL ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma A.2. DKL(U, V) = u(\u03b8) log( vu((\u03b8\u03b8)))d\u03b8 \u2264  (u(\u03b8)v(\u2212\u03b8v)(\u03b8))2d\u03b8 ", "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}(\\mathbb{U},\\mathbb{V})=\\displaystyle\\int u(\\theta)\\log(\\frac{u(\\theta)}{v(\\theta)})d\\theta}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\log{\\displaystyle\\int\\frac{u(\\theta)^{2}}{v(\\theta)}d\\theta\\ \\ (\\mathrm{by~Jensen}^{\\cdot}\\mathrm{s\\inequality})}}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\int\\frac{u(\\theta)^{2}}{v(\\theta)}-1d\\theta\\ \\ (\\log(1+x)\\leq x)}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\int\\frac{(u(\\theta)-v(\\theta))^{2}}{v(\\theta)}d\\theta}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last equality is because ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int\\frac{(u(\\theta)-v(\\theta))^{2}}{v(\\theta)}d\\theta=\\int\\frac{u(\\theta)^{2}}{v(\\theta)}-2\\int u(\\theta)d\\theta+\\int v(\\theta)d\\theta=\\int\\frac{u(\\theta)^{2}}{v(\\theta)}-1\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\begin{array}{r}{\\int u(\\pmb{\\theta})d\\pmb{\\theta}=\\int v(\\pmb{\\theta})d\\pmb{\\theta}=1}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Theorem A.3. Assume $\\begin{array}{r}{\\int||\\frac{1}{v(\\pmb\\theta)}||_{\\infty}d\\pmb\\theta\\leq M}\\end{array}$ , we can obtain the following conclusion for Eq. (16): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbb{U}\\in\\mathcal{U}}\\int\\mathcal{L}^{C L}(\\pmb{\\theta})d\\mathbb{U}(\\pmb{\\theta})=\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}+\\sqrt{\\frac{\\epsilon\\mathbb{E}(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})^{2}}{M}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}:=\\int\\mathcal{L}^{C L}(\\pmb{\\theta})d\\mathbb{V}(\\pmb{\\theta})}\\end{array}$ . We denote the variance of the random variable $\\mathcal{L}^{C L}(\\pmb{\\theta})$ as $\\begin{array}{r}{V a r(\\mathcal{L}^{C L}(\\pmb{\\theta}))=\\mathbb{E}(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})^{2}=\\int(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})^{2}d\\pmb{\\theta}.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Proof. We define a new distribution $\\mathbb{Z}:=\\mathbb{U}-\\mathbb{V}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg\\int\\mathcal{L}^{C L}(\\pmb{\\theta})d\\mathbb{U}(\\pmb{\\theta})=\\int\\mathcal{L}^{C L}(\\pmb{\\theta})d(\\mathbb{Z}(\\pmb{\\theta})+\\mathbb{V}(\\pmb{\\theta}))=\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}+\\int\\mathcal{L}^{C L}(\\pmb{\\theta})d\\mathbb{Z}(\\pmb{\\theta})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}+\\int(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})d\\mathbb{Z}(\\pmb{\\theta})+\\int\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}d\\mathbb{Z}(\\pmb{\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Lemma 4.1 and H\u00f6lder\u2019s inequality, we can obtain the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}(\\mathbb{U},\\mathbb{V})=\\displaystyle\\int u(\\pmb{\\theta})\\log(\\frac{u(\\pmb{\\theta})}{v(\\pmb{\\theta})})d\\pmb{\\theta}\\leq\\int\\frac{(u(\\pmb{\\theta})-v(\\pmb{\\theta}))^{2}}{v(\\pmb{\\theta})}d\\pmb{\\theta}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\int(u(\\pmb{\\theta})-v(\\pmb{\\theta}))^{2}d\\pmb{\\theta}\\int||\\frac{1}{v(\\pmb{\\theta})}||_{\\infty}d\\pmb{\\theta}}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\int(u(\\pmb{\\theta})-v(\\pmb{\\theta}))^{2}d\\pmb{\\theta}M\\leq\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int(u(\\pmb{\\theta})-v(\\pmb{\\theta}))^{2}d\\pmb{\\theta}\\leq\\frac{\\epsilon}{M}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg\\int(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})d\\mathbb{Z}(\\pmb{\\theta})=\\int(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})(u(\\pmb{\\theta})-v(\\pmb{\\theta}))d\\pmb{\\theta}}&{}\\\\ {\\leq\\sqrt{\\int(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})^{2}d\\pmb{\\theta}\\int(u(\\pmb{\\theta})-v(\\pmb{\\theta}))^{2}d\\pmb{\\theta}}}&{}\\\\ {\\leq\\sqrt{\\frac{\\epsilon\\mathbb{E}(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})^{2}}{M}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The equality holds when the following condition holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\nu(\\pmb{\\theta})-v(\\pmb{\\theta})=a(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $a$ is a constant. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}d\\mathbb{Z}(\\pmb{\\theta})=\\int\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}(u(\\pmb{\\theta})-v(\\pmb{\\theta}))d\\pmb{\\theta}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}\\displaystyle\\int(u(\\pmb{\\theta})-v(\\pmb{\\theta}))d\\pmb{\\theta}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The last equality is because $\\begin{array}{r}{\\int(u(\\pmb{\\theta})-v(\\pmb{\\theta}))d\\pmb{\\theta}=\\int u(\\pmb{\\theta})d\\pmb{\\theta}-\\int v(\\pmb{\\theta})d\\pmb{\\theta}=1-1=0}\\end{array}$ Therefore, we can obtain the following conclusion: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbb{U}\\in\\mathcal{U}}\\int\\mathcal{L}^{C L}(\\pmb{\\theta})d\\mathbb{U}(\\pmb{\\theta})=\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}}+\\sqrt{\\frac{\\epsilon\\mathbb{E}(\\mathcal{L}^{C L}(\\pmb{\\theta})-\\overline{{\\mathcal{L}^{C L}(\\pmb{\\theta})}})^{2}}{M}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In this context, the CL loss variance across various sets of model parameters $V a r(\\mathcal{L}^{C L}(\\pmb{\\theta}))$ serves as a measure of the CL model\u2019s sensitivity to parameter updates. Essentially, a smaller loss variance indicates lower parameter sensitivity in the CL model. However, directly optimizing the loss variance within the parameter distribution neighborhood is impractical, as it requires computing the loss variance across a large number of different sets of CL model parameters and training data points. In contrast, our method (MACL) offers an efficient and effective alternative. MACL implicitly minimizes the loss variance across different model parameter variations by optimizing CL performance solely on the worst-case CL model parameter distribution. ", "page_idx": 17}, {"type": "text", "text": "We denote the prior distribution as $\\mathbb{V}(\\pmb{\\theta})\\;=\\;\\mathcal{N}(\\pmb{\\mu_{p}},\\pmb{\\Sigma_{p}})$ and posterior distribution as $\\mathbb{U}(\\pmb\\theta)\\mathrm{~=~}$ $\\mathcal{N}(\\pmb{\\mu}_{s},\\pmb{\\Sigma}_{s})$ ", "page_idx": 17}, {"type": "text", "text": "Theorem A.4 (Generalization bound of MACL). Let q be the number of $C L$ model parameters and $n$ be the number of training data points. The $C L$ loss ${\\mathcal{L}}^{C L}(\\pmb{\\theta})\\leq C$ ( $C$ is a constant). With high probability of $1-\\delta_{i}$ , the following bound holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\!N(\\mu,\\Sigma)}\\sum_{i}^{i=T}\\mathcal{L}_{\\mathcal{D}_{i}}^{C L}(\\theta)\\le\\operatorname*{max}_{\\mathbb{U}\\in\\mathcal{U}}\\mathbb{E}_{\\theta\\sim\\mathbb{U}}\\mathcal{L}^{C L}(\\theta)+\\frac{C}{N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}}+\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{\\tau^{2}(\\sqrt{q}+\\sqrt{2\\log(N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}}))^{2}+R+2\\log(\\frac{N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}}{\\delta})}{4(N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}-1)}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where $\\tau$ is a constant. We denote the number of data examples for task $1,\\cdots\\,,T\\,-\\,1$ in the memory buffer $\\mathcal{M}$ during training on task $T$ as $N_{1},N_{2},\\cdots,N_{T-1}$ when using memory replay based approach or the number of training data points when using regularization based approach. $\\mathcal{L}_{\\mathcal{D}_{i}}^{C L}(\\pmb{\\theta})$ denotes the $C L$ loss on the data from data distribution $\\mathcal{D}_{i}$ (generalization error), i.e., it is defined as: $\\begin{array}{r}{\\mathcal{L}_{\\mathcal{D}_{i}}^{C L}(\\pmb{\\theta}):=\\mathbb{E}_{(\\pmb{x},y)\\sim\\mathcal{D}_{i}}\\mathcal{L}(\\pmb{x},\\dot{y},\\pmb{\\theta})}\\end{array}$ . $\\mathcal{L}^{C L}(\\pmb{\\theta})$ denotes the empirical $C L$ loss as Eq. (1). $\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma})$ denotes the $C L$ model parameter posterior distribution parameterized with Gaussian distribution. ", "page_idx": 17}, {"type": "text", "text": "Proof. We apply the PAC-Bayes theorem [43] that for any prior distribution, with probability $1-\\delta$ over the CL training dataset $\\tau$ , the following bound holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}[\\mathcal{L}_{\\mathcal{D}}^{C L}(\\pmb{\\theta})]\\leq\\mathbb{E}_{\\pmb{\\theta}\\sim\\mathbb{U}(\\pmb{\\theta})}[\\mathcal{L}_{\\mathcal{T}}^{C L}(\\pmb{\\theta})]+\\sqrt{\\frac{D_{\\mathrm{KL}}(\\mathbb{U}(\\pmb{\\theta})||\\mathbb{V}(\\pmb{\\theta}))+\\log(\\frac{n}{\\delta})}{2(n-1)}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The KL divergence between posterior and prior distribution can be calculated as the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}(\\mathbb{U}(\\theta)||\\mathbb{V}(\\theta))=\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}[\\log(\\mathbb{U}(\\theta))-\\log(\\mathbb{V}(\\theta))]}\\\\ &{=\\frac{1}{2}\\log\\frac{|\\Sigma_{p}|}{|\\Sigma_{s}|}-\\frac{1}{2}\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}(\\theta-\\mu_{s})^{T}\\Sigma_{s}^{-1}(\\theta-\\mu_{s})+\\frac{1}{2}\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}(\\theta-\\mu_{p})^{T}\\Sigma_{p}^{-1}(\\theta-\\mu_{p})}\\\\ &{=\\frac{1}{2}[\\log\\frac{|\\Sigma_{p}|}{|\\Sigma_{s}|}-q+(\\mu_{s}-\\mu_{p})^{T}\\Sigma_{p}^{-1}(\\mu_{s}-\\mu_{p})+T r(\\Sigma_{p}^{-1}\\Sigma_{s})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We assume the following inequality: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\log\\frac{|\\Sigma_{p}|}{|\\Sigma_{s}|}+T r(\\Sigma_{p}^{-1}\\Sigma_{s})\\leq R+q,\\;\\;\\;R\\geq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(\\mathbb{U}(\\pmb{\\theta})||\\mathbb{V}(\\pmb{\\theta}))\\leq\\frac{1}{2}[R+(\\pmb{\\mu}_{s}-\\pmb{\\mu}_{p})^{T}\\pmb{\\Sigma}_{p}^{-1}(\\pmb{\\mu}_{s}-\\pmb{\\mu}_{p})]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "According to [50], we have the following identity: ", "page_idx": 18}, {"type": "text", "text": "For a random variable $\\theta\\sim\\mathcal{N}(\\mu,\\Sigma)$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\mathcal{N}(\\mu,\\Sigma)}(\\theta-\\mu^{\\prime})^{T}A(\\theta-\\mu^{\\prime})=(\\mu-\\mu^{\\prime})^{T}A(\\mu-\\mu^{\\prime})+T r(A\\Sigma)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $T r$ denotes the trace of A matrix. Therefore, according to Eq. (60), we have the following two equations 61 and 62. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}(\\theta-\\mu_{s})^{T}\\Sigma_{s}^{-1}(\\theta-\\mu_{s})=(\\mu_{s}-\\mu_{s})^{T}\\Sigma_{s}^{-1}(\\mu_{s}-\\mu_{s})+T r(\\Sigma_{s}^{-1}\\Sigma_{s})=q}\\\\ {\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}(\\theta-\\mu_{p})^{T}\\Sigma_{p}^{-1}(\\theta-\\mu_{p})=(\\mu_{s}-\\mu_{p})^{T}\\Sigma_{p}^{-1}(\\mu_{s}-\\mu_{p})+T r(\\Sigma_{p}^{-1}\\Sigma_{s})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We set \u03b3 = \u03a3p\u22122(\u00b5s \u2212\u00b5p). Then, ||\u03b3||2 = (\u00b5s \u2212\u00b5p)T \u03a3p\u2212 1(\u00b5s \u2212\u00b5p). ", "page_idx": 18}, {"type": "text", "text": "If $\\gamma\\sim N(\\mathbf{0},\\tau^{2}I)$ , according to [31], we have the following inequality with probability of $\\textstyle1-{\\frac{1}{n}}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n||\\gamma||^{2}\\leq\\tau^{2}(q+2\\sqrt{q\\log n}+2\\log n)\\leq\\tau^{2}(\\sqrt{q}+\\sqrt{2\\log n})^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we partition the space of $\\pmb{\\mu}_{s}$ into two disjoint area that satisfy $({\\pmb{\\mu}}_{s}-{\\pmb{\\mu}}_{p})^{T}{\\pmb{\\Sigma}}_{p}^{-1}({\\pmb{\\mu}}_{s}-{\\pmb{\\mu}}_{p})\\leq$ $2\\epsilon-R$ and $(\\pmb{\\mu}_{s}-\\pmb{\\mu}_{p})^{T}\\pmb{\\Sigma}_{p}^{-1}(\\pmb{\\mu}_{s}-\\pmb{\\mu}_{p})>2\\epsilon-R$ . ", "page_idx": 18}, {"type": "text", "text": "(1) In the case of $(\\mu_{s..}-\\mu_{p})^{T}\\Sigma_{p}^{-1}(\\mu_{s}-\\mu_{p})\\leq2\\epsilon-R$ , we take the maximum loss over $\\pmb{\\mu}_{s}$ , we have the following inequality: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}[\\mathcal{L}_{T}^{C L}(\\theta)]\\leq\\operatorname*{max}_{(\\mu_{s}-\\mu_{p})^{T}\\Sigma_{p}^{-1}(\\mu_{s}-\\mu_{p})\\leq2\\epsilon-R}\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}\\mathcal{L}^{C L}(\\theta)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(2) For the case of $(\\pmb{\\mu}_{s}-\\pmb{\\mu}_{p})^{T}\\pmb{\\Sigma}_{p}^{-1}(\\pmb{\\mu}_{s}-\\pmb{\\mu}_{p})>2\\epsilon-R$ , we have $\\mathcal{L}_{T}^{C L}(\\pmb{\\theta})\\leq C$ ", "page_idx": 18}, {"type": "text", "text": "Combining case (1) and (2), we can obtain the following generalization bound: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}(\\mathbb{U},\\mathbb{V})\\leq\\displaystyle\\frac{1}{2}[(\\mu_{s}-\\mu_{p})^{T}\\Sigma_{p}^{-1}(\\mu_{s}-\\mu_{p})+R+q-q]\\leq\\frac{1}{2}[||\\gamma||^{2}+R]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{2}[\\tau^{2}(\\sqrt{q}+\\sqrt{2\\log n})^{2}+R]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We have the following bound with probability of $\\textstyle1-{\\frac{1}{n}}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}[\\mathcal{L}_{T}^{C L}(\\theta)]\\le(1-\\displaystyle\\frac{1}{n})\\operatorname*{max}_{(\\mu_{s}-\\mu_{p})^{T}\\Sigma_{p}^{-1}(\\mu_{s}-\\mu_{p})\\leq2\\epsilon-R}\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}\\mathcal{L}^{C L}(\\theta)+\\displaystyle\\frac{C}{n}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq(1-\\displaystyle\\frac{1}{n})\\operatorname*{max}_{D\\ K\\ L(\\mathbb{U},\\mathbb{V})\\leq\\epsilon}\\mathbb{E}_{\\theta\\sim\\mathbb{U}(\\theta)}\\mathcal{L}^{C L}(\\theta)+\\displaystyle\\frac{C}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we can obtain the following generalization bound with probability of $\\textstyle1-{\\frac{1}{n}}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\theta\\sim N(\\mu,\\Sigma)}\\displaystyle\\sum_{i}^{i=T}\\mathcal{L}_{D_{i}}^{C L}(\\theta)\\leq\\operatorname*{max}_{\\mathbb{U}\\in\\mathcal{U}}\\mathbb{E}_{\\theta\\sim\\mathbb{U}}\\mathcal{L}^{C L}(\\theta)+\\frac{C}{N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}}+}\\\\ &{\\sqrt{\\frac{\\tau^{2}(\\sqrt{q}+\\sqrt{2\\log(N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}}))^{2}+R+2\\log(\\frac{N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}}{\\delta})}{4(N_{T}+\\zeta\\sum_{i=1}^{i=T-1}N_{i}-1)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In this theorem, we provide the theoretical guarantee for the generalization analysis of our proposed method. This bound indicates by optimizing the MACL loss, our method tighten/reduce the generalization error of the CL method, thus improving the overall performance of our method. ", "page_idx": 19}, {"type": "text", "text": "B Equation Derivation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Exponential Family Distribution Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "According to the definition of expectation, we can obtain the following equation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda^{1}:=\\mathbb{E}_{f(\\pmb{\\theta};\\pmb{\\mu},\\pmb{\\Sigma})}\\pmb{\\theta}=\\pmb{\\mu}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "According to the definition of covariance matrix, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\Sigma}:=\\mathbb{E}[(\\pmb{\\theta}-\\pmb{\\mu})(\\pmb{\\theta}-\\pmb{\\mu})^{T}]}\\\\ &{~~=\\mathbb{E}[\\pmb{\\theta}\\pmb{\\theta}^{T}-2\\pmb{\\mu}\\pmb{\\theta}+\\pmb{\\mu}\\pmb{\\mu}^{T}]}\\\\ &{~~=\\mathbb{E}[\\pmb{\\theta}\\pmb{\\theta}^{T}]-\\pmb{\\mu}\\pmb{\\mu}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By rearranging the above equation, we can obtain the following: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\theta\\theta^{T}]=\\mu\\mu^{T}+\\Sigma\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, the conclusion follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda^{1}:=\\mathbb{E}_{f(\\pmb\\theta;\\pmb\\mu,\\Sigma)}\\pmb\\theta=\\pmb\\mu,\\quad\\lambda^{2}:=\\mathbb{E}_{f(\\pmb\\theta;\\pmb\\mu,\\Sigma)}\\pmb\\theta\\pmb\\theta^{T}=\\pmb\\mu\\pmb\\mu^{T}+\\pmb\\Sigma\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2 Worst-Case Gaussian Distribution NGD Derivations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Gradient of Loss $\\mathcal{L}(\\lambda)$ With Respect to $\\lambda$ Taking gradient with respect to $\\lambda$ as the following: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\lambda^{1}}{\\mathcal{L}}(\\lambda)=\\nabla_{\\mu}{\\mathcal{L}}(\\lambda){\\frac{\\partial\\mu}{\\partial\\lambda^{1}}}+\\nabla_{\\Sigma}{\\mathcal{L}}(\\lambda){\\frac{\\partial\\Sigma}{\\partial\\lambda^{1}}}=\\nabla_{\\mu}{\\mathcal{L}}(\\lambda)-2\\nabla_{\\Sigma}{\\mathcal{L}}(\\lambda)\\mu\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In Eq. (76), the second equality is because the identity: $\\begin{array}{r}{\\frac{\\partial\\pmb{\\mu}}{\\partial\\pmb{\\lambda}^{1}}=\\mathbf{1},\\quad\\frac{\\partial\\pmb{\\Sigma}}{\\partial\\pmb{\\lambda}^{1}}=\\frac{\\partial\\pmb{\\Sigma}}{\\partial\\pmb{\\mu}}=-2\\pmb{\\mu}}\\end{array}$ . (by Eq. (75)) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\lambda^{2}}{\\mathcal{L}}(\\lambda)=\\nabla_{\\mu}{\\mathcal{L}}(\\lambda){\\frac{\\partial\\mu}{\\partial\\lambda^{2}}}+\\nabla_{\\Sigma}{\\mathcal{L}}(\\lambda){\\frac{\\partial\\Sigma}{\\partial\\lambda^{2}}}=\\nabla_{\\Sigma}{\\mathcal{L}}(\\lambda)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In Eq. (77), the second equality is because the identity: $\\begin{array}{r}{\\frac{\\partial\\pmb{\\mu}}{\\partial\\pmb{\\lambda}^{2}}=\\mathbf{0},\\quad\\frac{\\partial\\pmb{\\Sigma}}{\\partial\\pmb{\\lambda}^{2}}=\\mathbf{1}}\\end{array}$ (by Eq. (75)) According to Eq. (5), we set the natural parameters as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\phi^{1}:=\\Sigma^{-1}\\mu,\\ \\ \\phi^{2}:=-\\frac{1}{2}\\Sigma^{-1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u2022 (1) NGD with respect to $\\phi^{2}$ : According to Eq. (20 and 77), NGD with respect to $\\phi^{2}$ can be obtained as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\frac{1}{2}\\Sigma_{i+1}^{-1}=-\\frac{1}{2}\\Sigma_{i}^{-1}-\\eta\\nabla_{\\lambda^{2}}\\mathcal{L}(\\lambda_{i})=-\\frac{1}{2}\\Sigma_{i}^{-1}-\\eta\\nabla_{\\Sigma}\\mathcal{L}(\\lambda_{i})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, obtain the following update: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}_{i+1}^{-1}=\\pmb{\\Sigma}_{i}^{-1}+2\\eta\\nabla_{\\pmb{\\Sigma}}\\mathcal{L}(\\pmb{\\lambda}_{i})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 (2) NGD with respect to $\\phi^{1}$ : Similarly, according to Eq. (20 and 76), NGD with respect to $\\phi^{1}$ can be obtained as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}_{i+1}^{-1}\\pmb{\\mu}_{i+1}=\\pmb{\\Sigma}_{i}^{-1}\\pmb{\\mu}_{i}-\\eta(\\nabla_{\\pmb{\\mu}}\\mathcal{L}(\\pmb{\\lambda}_{i})-2\\nabla_{\\pmb{\\Sigma}}\\mathcal{L}(\\pmb{\\lambda}_{i})\\pmb{\\mu}_{i})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By simplifying and rearranging Eq. (81), the following update for $\\pmb{\\mu}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pmb{\\mu}_{i+1}=\\pmb{\\mu}_{i}-\\eta\\pmb{\\Sigma}_{i+1}\\nabla_{\\mu}\\mathcal{L}(\\lambda_{i})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Mean and Covariance Updates Derivations Following the results in [56, 26], we can obtain the following equation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\mu}\\mathbb{E}_{\\theta\\sim u(\\theta)}\\mathcal{L}(\\mu,\\Sigma)=\\mathbb{E}_{\\theta\\sim u(\\theta)}\\nabla_{\\theta}\\mathcal{L}(\\mu,\\Sigma)}\\\\ {\\nabla_{\\Sigma}\\mathbb{E}_{\\theta\\sim u(\\theta)}\\mathcal{L}(\\mu,\\Sigma)=\\cfrac{1}{2}\\mathbb{E}_{\\theta\\sim u(\\theta)}\\nabla_{\\theta\\theta}^{2}\\mathcal{L}(\\mu,\\Sigma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, we only need to calculate $\\mathbb{E}_{\\theta\\sim u(\\theta)}\\nabla_{\\theta}\\mathcal{L}(\\mu,\\Sigma)$ and $\\mathbb{E}_{\\theta\\sim u(\\theta)}\\nabla_{\\theta\\theta}^{2}\\mathcal{L}(\\mu,\\Sigma)$ . Here, since we assumed a general CL Gaussian distribution for the current CL parameter distribution, i.e., $\\mathbb{V}(\\pmb\\theta)=$ $\\mathcal{N}(\\theta|\\mu_{0},\\bar{\\Sigma_{0}})$ and neighbourhood distribution, i.e., $\\mathbb{U}(\\pmb\\theta)=\\mathcal{N}(\\pmb\\theta|\\pmb\\mu,\\pmb\\Sigma)$ . The detailed derivations for the gradient are present in the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{T}_{\\mu}\\mathbb{E}_{\\theta\\sim u(\\theta)}\\mathcal{L}(\\mu,\\Sigma)=-\\mathbb{E}_{\\theta\\sim u(\\theta)}\\nabla_{\\theta}\\mathcal{L}^{C L}(\\theta)+\\alpha\\mathbb{E}_{\\theta\\sim u(\\theta)}[\\nabla_{\\theta}\\log u(\\theta)-\\nabla_{\\theta}\\log v(\\theta)]}&{{}~~}&{(85)}\\\\ &{=-\\mathbb{E}_{\\theta\\sim u(\\theta)}\\nabla_{\\theta}\\mathcal{L}^{C L}(\\theta)-\\alpha\\mathbb{E}_{\\theta\\sim u(\\theta)}(\\theta-\\mu)\\Sigma^{-1}+\\alpha\\mathbb{E}_{\\theta\\sim u(\\theta)}(\\theta-\\mu_{0})\\Sigma_{0}^{-1}}&{{}}&{(86)}\\\\ &{}&{(86)}\\\\ &{=\\mathbb{E}_{\\theta\\sim u(\\theta)}[-\\nabla_{\\theta}\\mathcal{L}^{C L}(\\theta)+\\alpha(\\mu-\\mu_{0})\\Sigma_{0}^{-1}]}&{{}~~~}&{(87)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nabla_{\\Sigma}\\mathbb{E}_{\\theta\\sim u(\\theta)}\\mathcal{L}(\\mu,\\Sigma)=-\\frac{1}{2}\\mathbb{E}_{\\theta\\sim u(\\theta)}\\nabla_{\\theta\\theta}^{2}\\mathcal{L}^{C L}(\\theta)+\\alpha\\mathbb{E}_{\\theta\\sim u(\\theta)}[\\log u(\\theta)-\\log v(\\theta)]}&{{\\scriptstyle(\\delta)\\theta\\sim u(\\theta)}}\\\\ &{}&{=-\\frac{1}{2}\\mathbb{E}_{\\theta\\sim u(\\theta)}\\nabla_{\\theta\\theta}^{2}\\mathcal{L}^{C L}(\\theta)+\\frac{\\alpha}{2}\\mathbb{E}_{\\theta\\sim u(\\theta)}[\\nabla_{\\theta\\theta}^{2}\\log u(\\theta)-\\nabla_{\\theta\\theta}^{2}\\log v(\\theta)]}\\\\ &{}&{\\quad{\\scriptstyle(\\delta)}}\\\\ &{=-\\frac{1}{2}\\mathbb{E}_{\\theta\\sim u(\\theta)}\\nabla_{\\theta\\theta}^{2}\\mathcal{L}^{C L}(\\theta)+\\frac{\\alpha}{2}\\mathbb{E}_{\\theta\\sim u(\\theta)}[-\\Sigma^{-1}+\\Sigma_{0}^{-1}]}&{{\\scriptstyle(\\delta)}}\\\\ &{}&{=\\frac{1}{2}\\mathbb{E}_{\\theta\\sim u(\\theta)}[-\\nabla_{\\theta\\theta}^{2}\\mathcal{L}^{C L}(\\theta)-\\alpha\\Sigma^{-1}+\\alpha\\Sigma_{0}^{-1}]}&{{\\scriptstyle(\\delta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plug-in the gradient derivation into Eq. (82 and 80), we can obtain the following results: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Sigma_{i+1}^{-1}=\\Sigma_{i}^{-1}+\\eta\\mathbb{E}_{\\theta\\sim u(\\theta)}[-\\nabla_{\\theta\\theta}^{2}\\mathcal{L}^{C L}(\\theta)-\\alpha\\Sigma_{i}^{-1}+\\alpha\\Sigma_{0}^{-1}]}}\\\\ {{=(1-\\eta\\alpha)\\Sigma_{i}^{-1}+\\eta\\mathbb{E}_{\\theta\\sim u(\\theta)}[-\\nabla_{\\theta\\theta}^{2}\\mathcal{L}^{C L}(\\theta)+\\alpha\\Sigma_{0}^{-1}]}}\\\\ {{\\,}}\\\\ {{\\mu_{i+1}=\\mu_{i}-\\eta\\Sigma_{i+1}\\mathbb{E}_{\\theta\\sim u(\\theta)}[-\\nabla_{\\theta}\\mathcal{L}^{C L}(\\theta)+\\alpha(\\mu_{i}-\\mu_{0})\\Sigma_{0}^{-1}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, by using single sample from distribution $\\mathbb{U}$ with density $\\pmb\\theta\\sim\\,u(\\pmb\\theta)$ to approximate the expectation. By plug-in $\\Sigma=\\operatorname{diag}(\\sigma^{2})$ and $\\Sigma_{0}=\\mathrm{diag}(\\rho^{2})$ into the above equations, we can obtain the following updates: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\mu}_{i+1}=\\pmb{\\mu}_{i}+\\eta\\pmb{\\sigma}_{i+1}^{2}[\\nabla_{\\pmb{\\theta}}\\mathcal{L}^{C L}(\\pmb{\\theta}_{i})-\\alpha(\\pmb{\\mu}_{i}-\\pmb{\\mu}_{0})\\pmb{\\rho}^{-2}]}\\\\ {\\pmb{\\sigma}_{i+1}^{-2}=(1-\\eta\\alpha)\\pmb{\\sigma}_{i}^{-2}+\\eta[-\\nabla_{\\pmb{\\theta}\\pmb{\\theta}}^{2}\\mathcal{L}^{C L}(\\pmb{\\theta}_{i})+\\alpha\\pmb{\\rho}^{-2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Baseline Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 EWC [28]: EWC endeavors to alleviate forgetting in continual learning through the utilization of a weighted weight regularization technique based on the Fisher information matrix. ", "page_idx": 21}, {"type": "text", "text": "\u2022 CPR [10]: Drawing on neural networks with wide local minima and principles from information theory, CPR introduces an extra regularization term. This term aims to maximize the entropy of a classifier\u2019s output probabilities, thereby reaching wider local minima to enhance generalization. ", "page_idx": 21}, {"type": "text", "text": "\u2022 GPM [60]: A CL model acquires new skills by adjusting its parameters through gradient steps that move orthogonal to the gradient subspaces considered vital for previous tasks. The Gradient Projection Memory (GPM) establishes these subspaces by analyzing network activations following the completion of each task using Singular Value Decomposition (SVD), then preserves them in memory. ", "page_idx": 21}, {"type": "text", "text": "\u2022 HAT [63]: HAT is a task-driven hard attention mechanism that retains information from prior tasks while ensuring it doesn\u2019t interfere with the current task\u2019s learning process. ", "page_idx": 21}, {"type": "text", "text": "\u2022 A-GEM [14]: AGEM aims to guarantee that, at every training step, the average loss of episodic memory over past tasks does not rise, thus mitigating the risk of forgetting previously acquired knowledge. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Gradient-Based Sample Selection (GSS-Greedy) [2]: The goal is to populate the memory buffer with a diverse set of examples, using the data gradient as a feature for sample selection. For comparison, we opt for the efficient GSS-Greedy version. ", "page_idx": 21}, {"type": "text", "text": "\u2022 ER [15]: This method stores a subset of examples from previous tasks using reservoir sampling [15]. During each iteration, we randomly replay a subset of examples from the memory buffer. ", "page_idx": 21}, {"type": "text", "text": "\u2022 $\\mathbf{DER++}$ [7]: This method combines experience replay with knowledge distillation to further improve the effectiveness of experience replay. ", "page_idx": 21}, {"type": "text", "text": "\u2022 ER-ACE [8]: They discovered that ER causes significant overlap between the representations of newly added classes and previous ones, resulting in highly disruptive parameter updates. From this empirical analysis, they proposed a new method to address this issue by protecting the learned representations from drastic adaptations when incorporating new classes. Their approach uses an asymmetric update rule that pushes new classes to adapt to the older ones, rather than the reverse. This technique is particularly effective at task boundaries, where much of the forgetting typically happens. ", "page_idx": 21}, {"type": "text", "text": "\u2022 LODE [36]: They conducted an in-depth analysis of the impacts of distinguishing between new and old classes, as well as among new classes, finding that these two learning objectives result in varying degrees of forgetting. Consequently, combining these objectives negatively affects the performance of the CL model. To address this, LODE separates the two objectives for new tasks by decoupling the loss associated with them. This approach allows LODE to assign different weights to each objective, leading to better performance compared to methods that use a coupled loss. ", "page_idx": 21}, {"type": "text", "text": "D More Experimental Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1 Hyperparameter Sensitivity Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 5: Analysis of hyperparameter $\\eta$ on CIFAR100 and Tiny-ImageNet in the setting of task-IL. ", "page_idx": 21}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/b170062b493a9ffc620e7a859eb0952e47925aae4fe9d9b6b44b564b17737f69.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.2 Benefit of NGD ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Table 6: Benefti of MACL-NGD vs. MACL-GD on CIFAR100 and Tiny-ImageNet in the setting of task-IL. ", "page_idx": 22}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/7656c733292c4896c6380d0a8d3e45fc5a86c5b6ac9b9c5e3f45c2185968764a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.3 Online CL results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Table 7: Online CL Results on CIFAR100 under the blurry boundary setting ", "page_idx": 22}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/df8d446eb5f2eb4d42e2be61adddab7ff56688e8b2f1b8df0ef18b9c18a9bc70.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 8: Online CL Results on Tiny-ImageNet under the blurry boundary setting ", "page_idx": 22}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/6cf105293a9965d81f3ea4521aa07f3e1b07bc740d93a10b8554c714ce619a26.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.4 Prompt-based CL results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We conducted an experiment integrating MACL with the SOTA prompt-based CL method, CODAPrompt [65]. Our method operates on the parameters of prompt components and corresponding keys/attention vectors. ", "page_idx": 22}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/648c5dc02f77f5da52b7acdfd56c9412dc6a641d717ab1d55a2a4d91a6480055.jpg", "table_caption": ["Table 9: CODA Prompt Results on ImageNet-R "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.5 5-datasets results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Table 10: Comparison of methods on Class-IL and Task-IL on 5-datasets. ", "page_idx": 22}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/fcff418a1b22f6cb500194c24c37adc4e5d0522ac8e5aeffe8d53dda3d1d099a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.6 Effect of Different Architectures ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Table 11: Overall accuracy with ResNet32 using a memory buffer of 2000 by integrating with MEMO. ", "page_idx": 22}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/7a7ede16188fa8ec959ced66a74d726040b94d17e611c5df16cb839103e376b1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 12: Overall accuracy with ViT using a memory buffer of 500 by integrating $\\mathrm{DER++}$ with MACL. ", "page_idx": 22}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/09477394d36230ffdd359812ffc0e55e70ea8b30b78a2d8c31e2cb6afec598d7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.7 ImageNet-R and CUB200 results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We conducted experiment on the recent CL datasets of ImageNet-R and CUB200 with pre-trained Vision Transformer (ViT), i.e., vit-base-patch16-224 as the backbone following the codebase of $\\mathrm{DER++}$ . The results (memory size of 500) are shown in the following table. ", "page_idx": 23}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/36380de10d39554d01295286ba4bc8f1b51958a8778722519436b76a554246d7.jpg", "table_caption": ["Table 13: ImageNet-R Results "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/f420a0496186fde7945266d4e5e1513efc363ddabfd2d7d8998288f082d6647f.jpg", "table_caption": ["Table 14: CUB200 Results "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.8 Efficiency Evaluation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Table 15: Running efficiency of MACL on CIFAR100 by training for a single epoch on CIFAR100. ", "page_idx": 23}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/145733d3ffda2a70629315c3e0e0a2926340c4c938a1f2251dc29e6fa43d334b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Experiment Setup ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Dataset Statistics ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "B5vQ7IQW7d/tmp/c53f0280e5de2978bd57fc7a6913ec95da89e7ad391657de525306e852e4a820.jpg", "table_caption": ["Table 16: Dataset Statistics "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 24}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 24}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 24}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 24}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 24}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Our abstract and introduction summarize the main contributions in our paper. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We dicussed the limitations of our work after conclusion. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide the full set of assumptions and a complete (and correct) proof in Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our contribution is a new continual learning algorithm. We described full implementation details for our proposed algorithm. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Code will be released. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 26}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provided detailed implementation details regarding training and test details, data splits, hyperparameters and type of optimizer. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provided results standard deviation with multiple experiment runs. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provided sufficient information on the computer resources in implementation details. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Our paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discussed the societal impacts afer conclusion. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have cited the code package produced by $\\mathrm{DER++}$ and the dataset used, e.g., CIFAR10, CIFAR100, TinyImageNet. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]