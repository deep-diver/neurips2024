[{"type": "text", "text": "Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yijun Dong\\* Courant Institute New York University yd1319@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Hoang Phan\\* Center of Data Science New York University hvp2011@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Xiang Pan\\* Center of Data Science New York University xiangpan@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Qi Lei Center of Data Science New York University ql518@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\\boldsymbol{S}$ (ii) then the variance is reduced over $\\boldsymbol{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\\boldsymbol{S}$ preserves the fast-rate generalization $O(\\dim(S)/n)$ , independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As the data volume and training cost explode with the unprecedented model performance, the longstanding problem of data selection [1, 2] is getting increasing attention in the modern context of deep learning from various perspectives, including data pruning [3, 4], coreset selection [1, 5, 6, 7, 8, 9], and data filtering [2, 10, 11, 12, 13]. A common goal shared by these perspectives is to train a model from scratch on less data to learn high-quality representations and achieve competitive generalization. However, empirical observations also suggest the limitation of data removal during pre-training: a seemingly inevitable tradeoff between less computation and higher-quality representations [14, 15]. While existing works on data selection have a dominating focus on the training-from-scratch setting, the sensitivity of representation learning to data and the growing availability of powerful pre-trained models calls for attention to a less studied [16] but equally important problem: data selection for finetuning. ", "page_idx": 0}, {"type": "text", "text": "In the simplest finetuning setting\u2014-linear probing on low-dimensional representations\u00b2 , data selection falls in the classical frames of coreset selection for linear regression [17, 18, 19, 20, 21, 22, 23, 24] and optimal experimental design [25, 26, 27, 28, 29] where the generalization gap can be ", "page_idx": 0}, {"type": "image", "img_path": "yAAQWBMGiT/tmp/a60da807d179e49262ecc50a9a54678596b2f7f4d752fd23c82582972bfe820a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Controlling variance-bias tradeoff in data selection for high-dimensional finetuning via gradient sketching $^+$ moment matching (SkMM). Consider a toy dataset with $N$ samples (in blue) whose finetuning gradients lie in a high-dimensional parameter space $\\mathbb{R}^{r}$ (visualized in 3D) with a low intrinsic dimension (e.g., three clusters). The goal is to select $n=n_{1}+n_{2}+n_{3}<r$ samples for finetuning. (a) Bias reduction focuses on minimizing the low-rank approximation error, resulting in uniform selection across clusters regardless of their variance. (b) Variance reduction?places more emphasis on high-variance clusters and could lead to large bias by missing low-variance ones. (c) Gradient sketching efficiently finds a low-dimensional subspace $\\boldsymbol{S}$ (where $\\bar{\\mathrm{dim}}(S)<n)$ with small bias. (d) Moment matching in $\\boldsymbol{S}$ controls the variance within the low-bias subspace, leading to a variance-bias balance with fast-rate generalization $O(\\dim(S)/n)$ ", "page_idx": 1}, {"type": "text", "text": "reduced by selecting data that minimize the associated variance. However, for high-dimensional finetuning, variance minimization alone is insufficient to characterize the generalization due to the overparametrized nature of modern architectures. Even for linear probing, when the parameter dimension $r$ is higher than the sample size $n$ , the selected data necessarily fails to capture a subspace of the parameter space with dimension at least $r\\,-\\,n$ , leading to errors in addition to variance. Nevertheless, the prevailing empirical and theoretical evidence [14, 30] on the ubiquitous intrinsic low-dimensional structures of high-dimensional data/model motivates a natural question: ", "page_idx": 1}, {"type": "text", "text": "Can the low intrinsic dimension be leveraged in data selection for high-dimensional finetuning? ", "page_idx": 1}, {"type": "text", "text": "Low intrinsic dimension leads to variance-bias tradeoff in data selection.  We provide a positive answer to this question through a variance-bias tradeoff perspective. Intuitively, we consider a low-dimensional subspace $\\boldsymbol{S}$ in the finetuning parameter space where the model learns the necessary knowledge for the downstream task. The generalization gap can be controlled by simultaneously reducing the bias (redundant information) by \u201cexploring\u201d the finetuning parameter space to find a suitable $\\boldsymbol{S}$ and the variance by \u201cexploiting\u201d the useful knowledge in $\\boldsymbol{S}$ ", "page_idx": 1}, {"type": "text", "text": "Given the high-dimensional nature of the finetuning parameter space, direct search for such suitable subspace $\\boldsymbol{S}$ is computationally infeasible in general. This leads to a follow-up question: ", "page_idx": 1}, {"type": "text", "text": "How to explore the intrinsic low-dimensional structure effciently for data selection? ", "page_idx": 1}, {"type": "text", "text": "We propose a two-stage solution\u2014Sketchy Moment Matching (SkMM): (i) dimensionality reduction via gradient sketching to efficiently explore the finetuning parameter space, and (i) variance control via moment matching to exploit useful knowledge in the low-dimensional subspace. ", "page_idx": 1}, {"type": "text", "text": "Gradient sketching finds a good low-dimensional subspace fast and provably. First, we construct a low-dimensional parameter subspace $\\boldsymbol{S}$ by sketching the model gradients. Sketching [17, 31] is a well-established dimensionality reduction tool known for affordable and accurate low-rank approximations [32, 33]. In deep learning, sketching recently extends its empirical applications to scalable estimations of influence functions for data selection [16, 34]. We make a first step toward the theoretical guarantee of gradient sketching for data selection: gradient sketching efficiently finds a low-dimensional subspace $\\boldsymbol{S}$ with small bias such that selecting nsamples by reducingvariance over $\\boldsymbol{S}$ issufficienttopreservethefast-rategeneralization $O(\\dim(S)/n)$ ,linearin the lowintrinsic dimension $\\dim(S)$ whileindependent of thehighparameterdimension $r$ ", "page_idx": 1}, {"type": "text", "text": "Moment matching in low dimension selects data that control the variance. Second, we select data that reduce variance in the low-dimensional subspace $\\boldsymbol{S}$ via moment matching. The variance of data selection is characterized by matching between the sketched gradient moments of the original and selected datasets, $\\widetilde{\\Sigma},\\widetilde{\\Sigma}_{S}$ formally $\\mathrm{tr}(\\widetilde{\\Sigma}\\widetilde{\\Sigma}_{S}^{\\dagger})$ . This objective involves optimizing over the inversions of (potentially) ill-conditioned matrices, leading to a challenging discrete optimization problem [28, 35]. Under a common heuristic assumption that $\\widetilde{\\Sigma},\\widetilde{\\Sigma}_{S}$ commute [36, 37], we introduce a continuous relaxation with a quadratic objective and linear constraints that is numerically stable (free of pseudoinverse) and can be efficiently optimized via projected gradient descent. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The contributions of this work are summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u00b7 We provide a rigorous generalization analysis on data selection for finetuning, illustrating the critical role of dimensionality by unveiling the variance-bias tradeoff in high dimensions.   \n\u00b7 We show that gradient sketching provably finds a low-dimensional parameter subspace $\\boldsymbol{S}$ With small bias, reducing variance over which preserves the fast-rate generalization $O\\bar{(}\\dim(S)/n)$ Techniques used in analyzing gradient sketching for data selection are agnostic to the selection method or the finetuning setting and could be of independent interest.   \n\u00b7 We introduce SkMM, a scalable two-stage data selection method for finetuning that simultaneously \u201cexplores\" the high-dimensional parameter space via gradient sketching and \u201cexploits\u201d the information in the low-dimensional subspace via moment matching. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Coreset selection and low-rank approximations. From the variance-bias tradeoff perspective, data selection for high-dimensional finetuning can be viewed as a combination of (i) variance reduction in coreset selection for linear regression [17, 18, 19, 21, 23, 24] with low-dimensional features, and (ii) bias reduction via sample-wise low-rank approximation for high-dimensional matrices [38, 39, 40, 41, 42, 43, 44, 45, 46]. ", "page_idx": 2}, {"type": "text", "text": "Gradient sketching. Gradient sketching [17, 32] based on Johnson-Lindenstrauss transforms (JLTs) [31, 47] has achieved impressive recent successes in efficient data selection [16] and attribution [34]. Despite the empirical success, theoretical understanding of the effect of gradient sketching on generalization remains limited. We make a first step toward this in the context of data selection leveraging existing theories on sketching (vide Remark 3.1 and Appendix C). ", "page_idx": 2}, {"type": "text", "text": "Moment matching and optimal experimental design.  Moment matching is an intuitive idea for selecting low-dimensional data (i.e., overdetermined with coreset size $n$ larger than data/representation dimension $r$ ), bearing various objectives like the A/V-optimality [28, 29] from optimal experimental design (OED) [25, 26, 27]. While classical OED studies the overdetermined scenario with $n\\geq r$ efforts have been made to extend the notion of V-optimality beyond the overdetermined setting [48, 49]. Nevertheless, these works focus on the general overparametrized setting without considering potential special structures in data. In the context of data selection, this can lead to pessimistic sample complexity, especially for learning problems with low-intrinsic dimensions. ", "page_idx": 2}, {"type": "text", "text": "For multimodal contrastive learning, recent works [12, 13] illustrated the effectiveness of moment matching via tailored data selection criteria for CLIP [50]. Distinct from our setting of general finetuning in both low and high dimensions, these works focus on data filtering (with $n>r$ for pretrainingfromscratch. ", "page_idx": 2}, {"type": "text", "text": "(Unsupervised) data selection. In this work, we focus on unsupervised data selection that instead of relying on labels4, leverages the geometry of the feature space and aims to select samples that are spread out, with a broad spectrum of concretizations including herding [51, 52], $\\mathbf{k}\\cdot$ -center greedy [53], leverage score sampling [24, 54, 55], adaptive sampling [44, 56], and volume sampling [39, 41]. ", "page_idx": 2}, {"type": "text", "text": "An inspiring recent work [57] investigates the generalization of weakly supervised data selection via independent sampling in the low-( $\\mathbf{\\chi}_{n}\\to\\infty$ with fixed $r$ \uff09 and high-dimensional $(n,r\\,\\to\\,\\infty$ with $n/r\\ \\rightarrow$ constant) asymptotics. Instead of the asymptotic regime, we consider a realistic setting with finite $n$ and $r$ , without specific assumptions on the data/feature distribution other than the low intrinsic dimension. Along this line, (weakly) supervised data selection commonly make choices based on the uncertainty [58, 59, 60] or sensitivity of the loss to samples (e.g., influence function [4, 61, 62], sensitivity scores [5, 63, 64, 65], and heuristics based on losses and their gradients [66, 67, 68, 69]). ", "page_idx": 2}, {"type": "text", "text": "1.2Notations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given any $n~\\in~\\mathbb{Z}_{+}$ , we denote $[n]\\;=\\;\\{1,\\cdot\\cdot\\;,n\\}$ .Let $\\mathbf{e}_{n}$ be the $n$ -th canonical basis of the conformable dimension; ${\\mathbf{I}}_{n}$ be the $n\\times n$ identity matrix; and $\\mathbf{0}_{n},\\mathbf{1}_{n}\\in\\mathbb{R}^{n}$ being vectors with all entries equal to zero and one, respectively. Let $\\operatorname{\\overline{{S}}}^{n-1}:=\\,\\{\\mathbf{x}\\in\\mathbb{R}^{n}|\\|\\mathbf{x}\\|_{2}=1\\}$ be the unit sphere in $\\mathbb{R}^{n}$ , and $\\Delta_{n}\\,:=\\,\\left\\{\\mathbf{p}\\in[0,1]^{b}\\;\\big|\\;\\|\\mathbf{p}\\|_{1}=1\\right\\}$ be the dimension ${\\mathbf{\\nabla}}n$ probability simplex. We adapt the standard asymptotic notations: for any functions $f,g:\\mathbb{R}_{+}\\,\\to\\,\\mathbb{R}_{+}$ ,we write $f\\,=\\,O\\left(g\\right)$ or $f\\lesssim g$ if there exists some constant $C>0$ such that $f(x)\\leq C g(x)$ for all $x\\in\\mathbb{R}_{+}$ .\uff0c $f\\,=\\,\\backslash\\Omega\\,(g)$ or $f\\gtrsim g$ if $g\\,=\\,O\\left(f\\right)$ $f\\asymp g$ if $f\\,=\\,O\\left(g\\right)$ and $f\\,=\\,\\Omega\\,(g)$ . For any matrix $\\mathbf{A}\\,\\in\\,\\mathbb{R}^{n\\times d}$ , let $s_{1}(\\mathbf{A})\\geq\\cdots\\geq s_{\\mathrm{rank}(\\mathbf{A})}(\\mathbf{A})\\geq0$ be the singular values; and $\\mathbf{A}^{\\dagger}$ be the Moore-Penrose pseudoinverse. Additionally for any $k\\leq$ rank (A), let $\\langle\\mathbf{A}\\rangle_{k}=\\mathrm{argmin}_{\\mathbf{B}}$ $\\mathrm{rank}(\\mathbf{B}){\\leq}k\\ \\|\\mathbf{A}-\\mathbf{B}\\|_{F}$ be the optimal rank$k$ approximation of A (characterized by the rank- $k$ truncated SVD). For any symmetric matrices $\\mathbf{A},\\dot{\\mathbf{B}}\\in\\mathbb{R}^{d\\times d}$ , we write $\\mathbf A\\succcurlyeq\\mathbf B$ or $\\mathbf{A}-\\mathbf{B}\\succcurlyeq0$ $\\mathbf{A}-\\mathbf{B}$ is positive semidefinite. ", "page_idx": 3}, {"type": "text", "text": "2  Data Selection for Finetuning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a data space $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and a label space $\\mathcal{V}\\subseteq\\mathbb{R}$ , let $\\mathcal{D}=\\{(\\mathbf{x}_{i},y_{i})\\in\\mathcal{X}\\times\\mathcal{Y}\\mid i\\in[N]\\}$ be a large dataset, with matrix form $(\\mathbf{X},\\mathbf{y})\\,\\in\\,\\mathbb{R}^{N\\times d}\\,\\times\\,\\mathbb{R}^{N}$ , for some downstream task where the performance is measured by a loss function $\\ell:\\mathcal{V}\\times\\mathcal{V}\\to\\mathbb{R}_{\\ge0}$ ", "page_idx": 3}, {"type": "text", "text": "Finetuning. Let $\\mathcal{F}$ be a class of prediction functions where each $f\\,=\\,h\\circ\\phi\\,\\in\\,\\mathcal{F}$ can be expressed as the composition of an expressive representation function $\\phi$ and a prediction head $h$ We consider a pre-trained model $\\phi$ that yields high-quality representations for some downstream tasks on $\\mathcal{D}$ and denote ${\\mathcal{F}}_{|\\phi}\\subseteq{\\mathcal{F}}$ as the class of finetuned models based on $\\phi$ . Assume that for every $(\\mathbf{x}_{i},y_{i})\\,\\in\\,\\mathcal{D}$ \uff0c $y_{i}\\;\\sim\\;P\\left(y\\mid\\mathbf{x}_{i}\\right)$ i.i.d. such that there exists $f_{*}^{\\phi}\\,\\in\\,\\mathcal{F}_{|\\phi}$ with respect to $\\phi$ satisfying (i) $\\mathbb{E}\\left[y_{i}\\ |\\ \\phi\\left(\\mathbf{x}_{i}\\right)\\right]=f_{*}^{\\phi}\\left(\\mathbf{x}_{i}\\right)$ , and (i) $\\mathbb{V}\\left[y_{i}\\ |\\ \\phi\\left(\\mathbf{x}_{i}\\right)\\right]\\le\\sigma^{2}$ for some $\\sigma>0$ (which will be formalized later in respective settings). ", "page_idx": 3}, {"type": "text", "text": "Data selection.Instead of finetuning on the entire dataset $\\mathcal{D}$ , we aim to select a small coreset $\\mathcal{D}_{S}\\subseteq$ $\\mathcal{D}$ of size $n\\ll N$ where the generalization is close. Precisely, let $\\mathcal{D}_{S}$ be indexed by $S\\subset[N]$ and denoted as $(\\mathbf{X}_{S},\\mathbf{y}_{S})\\in\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n}$ With $\\begin{array}{r}{\\mathcal{L}_{\\mathcal{D}_{S}}\\left(f\\right)=\\frac{1}{n}\\sum_{\\left(\\mathbf{x},y\\right)\\in\\mathcal{D}_{S}}\\ell\\left(f(\\mathbf{x}),y\\right)}\\end{array}$ and aregulaizain $\\mathcal{R}:\\mathcal{F}_{|\\phi}\\to\\mathbb{R}_{\\geq0}$ associated with a hyperparameter $\\alpha\\geq0$ , we want $f_{S}=\\mathrm{argmin}_{f\\in\\mathcal{F}_{|\\phi}}\\,\\mathcal{L}_{\\mathcal{D}_{S}}\\left(f\\right)+$ $\\alpha\\cdot{\\mathcal{R}}\\left(f\\right)$ to provide a low excess risk over $\\begin{array}{r}{\\mathcal{D}\\colon\\mathrm{ER}\\left(f_{S}\\right):=\\frac{1}{N}\\sum_{i=1}^{N}\\ell(f_{S}(\\mathbf{x}_{i}),f_{*}^{\\phi}(\\mathbf{x}_{i}))}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "2.1 Low-dimensional Linear Probing: Variance Minimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Warming up with linear probing, we concretize the general assumption on the ground truth (i.., $\\mathbb{E}\\left[y_{i}\\mid\\dot{\\phi_{\\mathbf{\\Lambda}}}(\\bar{\\mathbf{x_{i}}_{}})\\right]=f_{*}^{\\phi}\\left(\\mathbf{x}_{i}\\right)\\stackrel{\\bar{\\mathbf{\\Lambda}}}{=}\\phi\\left(\\bar{\\mathbf{x_{i}}_{}}\\right)^{\\top}\\pmb{\\theta_{*}}$ and $\\mathbb{V}\\left[y_{i}\\ |\\ \\phi\\left(\\mathbf{x}_{i}\\right)\\right]\\leq\\sigma^{2})$ as follows: ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1 (linear probing ground truth). Assume $\\mathbf{y}=\\phi\\left(\\mathbf{X}\\right)\\pmb{\\theta}_{\\ast}+\\mathbf{z}$ forsome $\\pmb\\theta_{*}\\in\\mathbb{R}^{r}$ where $\\mathbf{z}=\\left[z_{1},\\cdots\\,,z_{N}\\right]^{\\top}\\in\\mathbb{R}^{N}$ consists ofi.i.d. entries with $\\mathbb{E}\\left[\\mathbf{z}\\right]=\\pmb{\\theta}_{N}$ andR $\\mathbf{\\dot{\\bar{\\rho}}}\\left[\\mathbf{z}\\mathbf{z}^{\\mathsf{T}}\\right]\\prec\\sigma^{2}\\mathbf{I}_{N}$ ", "page_idx": 3}, {"type": "text", "text": "Consider the pre-trained representations $\\phi(\\mathbf{X})\\in\\mathbb{R}^{N\\times r}$ and $\\phi(\\mathbf{X}_{S})\\in\\mathbb{R}^{n\\times r}$ with respective moments $\\begin{array}{r}{\\pmb{\\Sigma}^{\\phi}:=\\frac{1}{N}\\phi\\left(\\mathbf{X}\\right)^{\\top}\\phi\\left(\\mathbf{X}\\right)}\\end{array}$ and $\\begin{array}{r}{\\Sigma_{S}^{\\phi}:=\\,\\frac{1}{n}\\phi\\left(\\mathbf{X}_{S}\\right)^{\\top}\\phi\\left(\\mathbf{X}_{S}\\right)}\\end{array}$ For low-dmensional linear probing with $r\\leq n$ (s.t. $\\mathrm{rank}(\\Sigma_{S}^{\\phi})=r)$ , the linear regression $\\begin{array}{r}{\\pmb{\\theta}_{S}=\\operatorname{argmin}_{\\pmb{\\theta}}\\frac{1}{n}\\left\\|\\phi\\left(\\mathbf{X}_{S}\\right)\\pmb{\\theta}-\\mathbf{y}_{S}\\right\\|_{2}^{2}}\\end{array}$ has a unique solution with excessrisk $\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)=\\left\\|\\pmb{\\theta}_{S}-\\pmb{\\theta}_{\\ast}\\right\\|_{\\pmb{\\Sigma}^{\\phi}}^{2}5$ controlled by $\\pmb{\\Sigma}^{\\phi}$ and $\\Sigma_{S}^{\\phi}$ , analogous to the $\\mathrm{V}.$ optimality criterion [28, 29] in optimal experimental design: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\leq\\frac{\\sigma^{2}}{n}\\operatorname{tr}\\!\\left(\\Sigma^{\\phi}(\\Sigma_{S}^{\\phi})^{-1}\\right)\\!,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathcal{D}_{S}$ satisfes $\\Sigma^{\\phi}\\precc{c}_{S}\\Sigma_{S}^{\\phi}$ for some $c_{S}\\geq\\frac{n}{N}$ .then $\\begin{array}{r}{\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\leq c_{S}\\frac{\\sigma^{2}r}{n}}\\end{array}$ (proof in Appendix B.1), where $c_{S}$ characterizes the variance controlled by $\\mathcal{D}_{S}$ , i.e., smaller $c_{S}$ implies lower variance. ", "page_idx": 3}, {"type": "text", "text": "Despite its simplicity, uniform sampling is often observed in practice to serve as a strong baseline for data selection [1], especially when $n$ is large. In the low-dimensional linear probing scenario, (1) provides a theoretical justification for such effectiveness of uniform sampling: ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.1 (Uniform sampling for low-dimensional linear probing (Appendix B.2)). Assume there exists (i) $B_{\\phi}>0$ such that $\\|\\bar{\\phi}(\\mathbf{x})\\|_{2}\\le B_{\\phi}\\,\\forall\\,\\mathbf{x}\\in\\mathcal{D},$ : and (ii) $\\gamma>0$ with $\\boldsymbol{\\Sigma}^{\\phi}\\succcurlyeq\\boldsymbol{\\gamma}\\mathbf{I}_{r}$ . For $S$ sampled uniformly (with replacement) over $\\mathcal{D}$ with probability at least $1-\\delta$ over $S$ $\\Sigma^{\\phi}\\precc{c}_{S}\\Sigma_{S}^{\\phi}$ forary $\\begin{array}{r}{c_{S}>1\\;i f\\,n\\gtrsim\\frac{B_{\\phi}^{4}}{\\gamma^{2}}\\cdot\\frac{r+\\log(1/\\delta)}{(1-1/c_{S})^{2}}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "That is, for linear probing with sufficiently low dimension $r\\ll n$ , under mild regularity assumptions on data, uniform sampling enjoys a near-optimal generalization $O(r/n)$ ", "page_idx": 4}, {"type": "text", "text": "2.2 High-dimension Finetuning with Low Intrinsic Dimension: Variance-Bias Tradeoff ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Extending the analysis to general finetuning, we consider a set of $r$ finetuning parameters $\\pmb{\\theta}\\in\\mathbb{R}^{r6}$ (potentially with $r\\gg n$ ) over a pre-trained model $\\phi$ (e.g., $\\pmb{\\theta}$ can be the parameters of the last layer (i.e., linear probing), last few layers, the entire network, or the LoRA [70] matrices). ", "page_idx": 4}, {"type": "text", "text": "Let ${\\mathcal{F}}_{|\\phi}=\\left\\{f^{\\phi}\\left(\\cdot;\\theta\\right):\\mathcal{X}\\rightarrow\\mathbb{R}\\mid\\theta\\in\\mathbb{R}^{r}\\right\\}$ be the finetuning function class. Without loss of generality, we assume zero initialization of $\\pmb{\\theta}$ such that $f^{\\phi}\\left(\\cdot;\\mathbf{0}_{r}\\right)$ corresponds to the pre-trained model. Analogous to the assumption in [16], under locality constraint on $\\pmb{\\theta}$ (e.g., $\\lVert\\pmb{\\theta}\\rVert_{2}<1)$ , the dynamics of finetuning falls in the kernel regime [71] where $f^{\\phi}$ can be approximated by its first-order Taylor expansion: $f^{\\phi}\\left(\\mathbf{x};\\pmb{\\theta}\\right)\\approx f^{\\phi}\\left(\\mathbf{x};\\mathbf{0}_{r}\\right)+\\nabla_{\\theta}f^{\\phi}\\left(\\mathbf{x};\\mathbf{0}_{r}\\right)^{\\top}\\pmb{\\theta}$ . Then, we formalize the ground truth as follows: ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.2 (Finetuning ground truth). Given the pre-trained $\\phi$ there exists a bounded ground truth $\\pmb\\theta_{\\ast}\\in\\mathbb{R}^{r}$ with $\\lVert\\pmb{\\theta}_{*}\\rVert_{2}<1$ such that for all $(\\mathbf{x},y)\\in\\mathcal{D}_{1}$ $(i)\\,\\mathbb{E}\\left[y\\mid\\phi\\left(\\mathbf{x}\\right)\\right]=f_{*}^{\\phi}\\left(\\mathbf{x}\\right)=f^{\\phi}\\left(\\mathbf{x};\\pmb{\\theta}_{*}\\right),$ and (ii) $\\mathbb{V}\\left[y\\mid\\phi\\left(\\mathbf{x}\\right)\\right]\\le\\sigma^{2}$ for some $\\sigma>0$ ", "page_idx": 4}, {"type": "text", "text": "Intuitively, Assumption 2.2 implies that the pre-trained model $f^{\\phi}\\left(\\cdot;\\mathbf{0}_{r}\\right)$ has a reasonable zero-shot performance. Given any $S\\subset[N]$ with $|S|=n$ , let $f^{\\phi}\\left(\\mathbf{X}_{S};\\pmb{\\theta}\\right)\\in\\mathbb{R}^{n}$ and $\\nabla_{\\pmb{\\theta}}f^{\\phi}\\left(\\mathbf{X}_{S};\\pmb{\\theta}\\right)\\in\\mathbb{R}^{n\\times r}$ be the evaluation of $f^{\\phi}\\left(\\mathbf{x};\\pmb{\\theta}\\right)$ and its Jacobian over $\\mathbf{X}_{S}$ at $\\pmb{\\theta}$ . We observe that with $\\mathbf{z}:=\\mathbf{y}-f^{\\phi}\\left(\\mathbf{X};\\pmb{\\theta}_{\\ast}\\right)$ Assumption 2.2 implies $\\mathbf{y}-f^{\\phi}\\left(\\mathbf{X};\\mathbf{0}_{r}\\right)\\approx\\mathbf{G}\\pmb{\\theta}_{\\ast}+\\mathbf{z}$ where $\\mathbb{E}\\left[\\mathbf{z}\\right]=\\mathbf{0}_{N}$ and E $\\left[\\mathbf{z}\\mathbf{z}^{\\top}\\right]\\preccurlyeq\\sigma^{2}\\mathbf{I}_{N}$ ; while $\\mathbf{G}:=\\nabla_{\\theta}f^{\\phi}\\left(\\mathbf{X};\\mathbf{0}_{r}\\right)\\in\\mathbb{R}^{N\\times r}$ is the Jacobian over $\\mathcal{D}$ at initialization. ", "page_idx": 4}, {"type": "text", "text": "Then in the kernel regime [71],the finetuning objective $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{\\theta}\\in\\mathbb{R}^{r}}\\frac{1}{n}\\left\\|\\boldsymbol{f}^{\\phi}\\left(\\mathbf{X}_{S};\\pmb{\\theta}\\right)-\\mathbf{y}_{S}\\right\\|_{2}^{2}+\\alpha\\left\\|\\pmb{\\theta}\\right\\|_{2}^{2}}\\end{array}$ can be well approximated by a ridge regression problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{S}=\\underset{\\theta\\in\\mathbb{R}^{r}}{\\operatorname{argmin}}\\,\\frac{1}{n}\\left\\|\\nabla_{\\theta}f^{\\phi}\\left(\\mathbf{X}_{S};\\mathbf{0}_{r}\\right)\\theta-\\left(\\mathbf{y}_{S}-f^{\\phi}\\left(\\mathbf{X}_{S};\\mathbf{0}_{r}\\right)\\right)\\right\\|_{2}^{2}+\\alpha\\left\\|\\theta\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Recall $\\mathbf{G}\\,:=\\,\\nabla_{\\theta}f^{\\phi}\\left(\\mathbf{X};\\mathbf{0}_{r}\\right)\\,\\in\\,\\mathbb{R}^{N\\times r}$ and $\\mathbf{G}_{S}\\,:=\\,\\nabla_{\\pmb{\\theta}}f^{\\phi}\\left(\\mathbf{X}_{S};\\mathbf{0}_{r}\\right)\\,\\in\\,\\mathbb{R}^{n\\times r}$ .With the moments $\\begin{array}{r}{\\pmb{\\Sigma}^{\\phi}=\\frac{1}{N}\\mathbf{G}^{\\top}\\mathbf{G}}\\end{array}$ and $\\begin{array}{r}{\\pmb{\\Sigma}_{S}^{\\phi}=\\frac{1}{n}\\mathbf{G}_{S}^{\\top}\\mathbf{G}_{S}}\\end{array}$ , the excess risk $\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)=\\|\\pmb{\\theta}_{S}-\\pmb{\\theta}_{\\ast}\\|_{\\pmb{\\Sigma}^{\\phi}}^{2}$ satisfies?: ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2 (Main result I: variance-bias tradeoff (Appendix B.3). Given $S$ let $\\mathbf{P}_{S}\\in\\mathbb{R}^{r\\times r}$ be an orthogonal projector onto some subspace $\\mathcal{S}\\subseteq\\mathrm{Range}(\\Sigma_{S}^{\\phi})$ and $\\mathbf{P}_{S}^{\\perp}=\\mathbf{I}_{r}-\\mathbf{P}_{S}$ be its orthogonal complement. Under Assumption 2.1, there exists an $\\alpha\\,>\\,0$ such that (2) satisfies $\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\leq$ variance + bias with(i) variance $\\begin{array}{r}{=\\frac{2\\sigma^{2}}{n}\\operatorname{tr}(\\Sigma^{\\phi}(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S})^{\\dagger})}\\end{array}$ and $(i i)\\,b i a s=2\\,\\mathrm{tr}\\left(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\left\\|\\pmb{\\theta}_{*}\\right\\|_{2}^{2}$ Specifically, the variance-bias tradeoff is controlled by the unknown $\\boldsymbol{S}$ : expanding $\\boldsymbol{S}$ leads to higher variance but lower bias. Reducing the generalization gap involves finding a suitable $\\boldsymbol{S}$ in the highdimensional parameter space, a computationally challenging problem addressed in Section 3.1. It is worth highlighting that Theorem 2.2 encapsulates both the low- and high-dimensional finetuning. ", "page_idx": 4}, {"type": "text", "text": "For low-dimensional linear probing, (1) is a special case of Theorem 2.2 (up to constants) with $\\mathbf{P}_{S}=\\mathbf{I}_{r}$ . While in high dimension, an intrinsic low-dimensional structure (e.g., Assumption 2.3) is necessary for the effectiveness of data selection8. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.3 (Low intrinsic dimension). Consider the second moment $\\Sigma^{\\phi}\\;\\succ\\;0$ over $\\mathcal{D}$ with $N$ samples. Let $\\overline{{r}}:=\\operatorname*{min}\\{t\\in[r]\\,\\mid\\,\\mathrm{tr}\\left(\\Sigma^{\\phi}-\\left\\langle\\Sigma^{\\phi}\\right\\rangle_{t}\\right)\\leq\\mathrm{tr}\\left(\\Sigma^{\\phi}\\right)/N\\}$ be the intrinsic dimension. Assume that $\\Sigma^{\\phi}$ has a low intrinsic dimension: $\\bar{r}\\ll\\operatorname*{min}\\left\\{N,r\\right\\}$ ", "page_idx": 5}, {"type": "text", "text": "When the high-dimensional finetuning parameter space has a low intrinsic dimension $\\overline{{r}}\\ \\ll$ $\\operatorname*{min}\\left\\{N,r\\right\\}$ , Theorem 2.2 can be further concretized with suitable $\\mathcal{D}_{S}$ and associated $\\boldsymbol{S}$ ", "page_idx": 5}, {"type": "text", "text": "Corollary 2.3 (Exploitation $^+$ exploration (Appendix B.3)). Under the same setting as Theorem 2.2 and Assumption 2.3, if $S$ satisfies for some subspace $\\mathcal{S}\\subseteq\\mathrm{Range}(\\Sigma_{S}^{\\phi})$ with rank $(\\mathbf{P}_{S})\\asymp\\overline{r}$ and $c_{S}\\geq{\\frac{n}{N}}$ that (i $)\\,\\mathbf{P}_{S}\\big(c_{S}\\pmb{\\Sigma}_{S}^{\\phi}-\\pmb{\\Sigma}^{\\phi}\\big)\\mathbf{P}_{S}\\succcurlyeq0$ and $\\begin{array}{r}{(i i)\\operatorname{tr}(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp})\\leq\\frac{N}{n}\\operatorname{tr}(\\Sigma^{\\phi}-\\langle\\Sigma^{\\phi}\\rangle_{\\overline{{r}}}),}\\end{array}$ .then9 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\leq\\nu a r i a n c e+b i a s\\lesssim\\frac{1}{n}\\left(c_{S}\\sigma^{2}\\overline{{r}}+\\mathrm{tr}\\left(\\Sigma^{\\phi}\\right)\\|\\pmb{\\theta}_{*}\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In particular, with $c_{S},\\sigma\\lesssim1$ $\\lVert\\pmb{\\theta}_{*}\\rVert_{2}^{2}<1$ , and $\\mathrm{tr}(\\Sigma^{\\phi})\\asymp\\bar{r}$ (depending only on the low intrinsic dimension), the generalization achieves a fast rate ${\\cal O}(\\overline{{r}}/n)$ , independent of $r\\gg\\overline{{r}}$ ", "page_idx": 5}, {"type": "text", "text": "In (3), (i) bias is reduced by exploring the parameter space for an $\\boldsymbol{S}$ with small low-rank approximation error $\\begin{array}{r}{\\mathrm{tr}(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp})\\leq\\frac{1}{n}\\operatorname{tr}(\\dot{\\Sigma}^{\\phi})}\\end{array}$ ; while (i) variance is reduced by exploiting information in $\\boldsymbol{S}$ through moment matching, $\\mathbf{P}_{S}\\big(c_{S}\\pmb{\\Sigma}_{S}^{\\phi}-\\pmb{\\Sigma}^{\\phi}\\big)\\mathbf{P}_{S}\\succcurlyeq0$ , where smaller $c_{S}$ means better exploitation. ", "page_idx": 5}, {"type": "text", "text": "3   Sketchy Moment Matching ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A gap between Corollary 2.3 and practice is how to find a suitable $\\boldsymbol{S}$ effciently in thehigh-dimensional parameter space. In this section, we introduce a simple scalable algorithm for constructing $\\boldsymbol{S}$ and $\\mathcal{D}_{S}$ that satisfies the exploration and exploitation conditions in Corollary 2.3. ", "page_idx": 5}, {"type": "text", "text": "3.1  Find Low Intrinsic Dimension via Gradient Sketching ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For high-dimensional finetuning with $r\\gg n$ , a critical limit of Theorem 2.2 and Corollary 2.3 is that the large moment matrices $\\Sigma^{\\phi}$ $\\Sigma_{S}^{\\phi}$ are not invertible, storable, or even directly computable, due to the prohibitive cost. As a remedy, sketching [17, 32] via Johnson-Lindenstrauss transforms [31] is a classical dimensionality reduction strategy that gets increasing recent attention for gradient approximation in large-scale machine learning problems $[16,34]^{10}$ ", "page_idx": 5}, {"type": "text", "text": "Remark 3.1 (Gradient sketching). In the high-dimensional setting with $r\\,\\gg\\,n,$ to reduce the dimensionality of the gradients $\\tilde{\\mathbf{G}}=\\nabla_{\\theta}f^{\\phi}\\left(\\Breve{\\mathbf{X}};\\pmb{\\theta}_{r}\\right)\\in\\mathbb{R}^{N\\times r}$ with a low intrinsic dimension $\\overline{{r}}\\ll$ min $\\{N,r\\}$ (Assumption 2.3), we draw a Johnson-Lindenstrauss transform [31] (JLT, formally in Definition C.1) $\\mathbf{T}\\in\\mathbb{R}^{r\\times m}$ that projects thedimension- $^r$ gradients to a lower dimension $m\\asymp\\overline{r}\\ll r$ $\\widetilde{\\mathbf{G}}\\,=\\,\\mathbf{G}\\mathbf{T}\\,\\in\\,\\mathbb{R}^{N\\times m}$ . One of the most common constructions of JLT is the Gaussian embedding (i.e., a Gaussian random matrix with i.i.d. entries $\\Gamma_{i j}\\sim\\mathcal{N}(0,1/m)$ discussed in Lemma C.3, vide Remark C.1 for a brief overview of various (fast) JLTs and their efficiency). ", "page_idx": 5}, {"type": "text", "text": "While sketching is known for preserving Euclidean distances [31] and providing accurate low-rank approximations [17, 32, 33], whether gradient sketching can convert Theorem 2.2 to an efficiently computable form without compromising the generalization guarantee? We answer this question affirmatively with the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (Main result II: gradient sketching (formally in Theorem C.1)). Under Assumption 2.2 and 2.3 with a low intrinsic dimension $\\bar{r}\\,\\ll\\,\\operatorname*{min}\\left\\{N,r\\right\\}$ , draw a Gaussian embedding $\\textbf{T}\\in\\ \\mathbb{R}^{r\\times m}$ (Lemma C.3) with $m\\ \\geq\\ 11{\\overline{{r}}}$ Let $\\widetilde{\\pmb{\\Sigma}}^{\\phi}\\,:=\\,{\\bf T}^{\\top}{\\pmb{\\Sigma}}^{\\phi}{\\bf T}$ and $\\widetilde{\\pmb{\\Sigma}}_{S}^{\\phi}\\;:=\\;\\mathbf{T}^{\\top}\\pmb{\\Sigma}_{S}^{\\phi}\\mathbf{T}$ be the sketched gradient moments. For any $\\mathcal{D}_{S}$ with $n>m$ samples such that ran $\\operatorname{k}(\\pmb{\\Sigma}_{S}^{\\phi})=n_{\\mathrm{.}}$ and the $\\left\\lceil1.1{\\overline{{r}}}\\right\\rceil$ -th largest eigenvalue $s_{\\lceil1.1\\overline{{r}}\\rceil}(\\widetilde{\\Sigma}_{S}^{\\phi})\\;\\geq\\;\\gamma_{S}$ for some $\\gamma_{S}~>~0$ ,with probability at least 0.9 over $\\mathbf{T}$ there exists $\\alpha\\,>\\,0$ where (2) satisfies I $\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\lesssim$ variance $^+$ sketching error $^+$ bias Wwith (i ariance $\\begin{array}{r l r}{=}&{{}\\frac{\\sigma^{2}}{n}\\,\\mathrm{tr}(\\widetilde\\Sigma^{\\phi}(\\widetilde\\Sigma_{S}^{\\phi})^{\\dagger})}\\end{array}$ i sketching eor $\\begin{array}{r}{=\\;\\frac{\\sigma^{2}}{n}\\frac{1}{m\\gamma_{S}}\\|\\widetilde\\Sigma^{\\phi}(\\widetilde\\Sigma_{S}^{\\phi})^{\\dagger}\\|_{2}\\,\\mathrm{tr}(\\Sigma^{\\phi}),}\\end{array}$ and $\\begin{array}{r}{(i i i)\\,b i a s=\\frac{1}{n}\\|\\widetilde{\\Sigma}^{\\phi}(\\widetilde{\\Sigma}_{S}^{\\phi})^{\\dagger}\\|_{2}\\,\\mathrm{tr}(\\Sigma^{\\phi})\\|\\pmb{\\theta}_{\\ast}\\|_{2}^{2}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "f $S$ further satisfies $\\widetilde{\\Sigma}^{\\phi}\\preccurlyeq c_{S}\\widetilde{\\Sigma}_{S}^{\\phi}$ for some $c_{S}\\geq\\frac{n}{N}$ with $m=\\operatorname*{max}\\{\\sqrt{\\mathrm{tr}\\left(\\Sigma^{\\phi}\\right)/\\gamma_{S}},11\\overline{{r}}\\},$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{ER}\\left(\\theta_{S}\\right)\\right]\\lesssim\\nu a r i a n c e+s k e t c h i n g\\;e r r o r+b i a s\\lesssim\\frac{c_{S}}{n}\\left(\\sigma^{2}m+\\operatorname{tr}\\left(\\Sigma^{\\phi}\\right)\\|\\theta_{*}\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Comparing (4) with (3), we observe that by controlling the variance with $\\widetilde\\Sigma^{\\phi}\\;\\prec\\;c_{S}\\widetilde\\Sigma_{S}^{\\phi}$ in low dimension $m\\asymp\\overline{r}\\ll r$ , gradient sketching preserves the fast-rate generalization $O(m/n)\\stackrel{\\sim}{=}O(\\overline{{r}}/n)$ up to constants. That is, gradient sketching implicitly finds a random subspace $\\mathcal{S}\\subseteq\\mathrm{Range}(\\Sigma_{S}^{\\phi})$ (vide (9)) that satisfies the exploration assumption in Corollary 2.3. Meanwhile, the choice of sketching size $m$ balances the tradeoff between variance and sketching error: a larger $m$ reduces the sketching error at the cost of higher variance. Such tradeoff is optimized at $m=\\sqrt{\\mathrm{tr}\\left(\\Sigma^{\\phi}\\right)/\\gamma_{S}}$ ", "page_idx": 6}, {"type": "text", "text": "3.2  Control Variance via Moment Matching ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given the intrinsic low-dimensional structure with small bias in Section 3.1, Theorem 3.1 connects generalization to the variance controlled by the matching between $\\widetilde{\\Sigma}^{\\phi}$ and $\\widetilde{\\Sigma}_{S}^{\\phi}$ Specically,when the selected data $\\mathcal{D}_{S}$ satisfies $\\widetilde{\\Sigma}^{\\phi}\\preccurlyeq c_{S}\\widetilde{\\Sigma}_{S}^{\\phi}$ for some $c_{S}\\geq\\frac{n}{N}$ , we have $\\mathrm{tr}(\\widetilde{\\Sigma}^{\\phi}(\\widetilde{\\Sigma}_{S}^{\\phi})^{\\dagger})\\leq c_{S}m$ and $\\|\\widetilde\\Sigma^{\\phi}(\\widetilde\\Sigma_{S}^{\\phi})^{\\dagger}\\|_{2}\\leq c_{S}$ upper bounded, leading to the fast-rate generalization in (4). ", "page_idx": 6}, {"type": "text", "text": "Algorithm 3.1 Sketchy Moment Matching (SkMM) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: Input: $f^{\\phi}\\left(\\cdot;\\mathbf{0}_{r}\\right)$ \uff0c $n\\ll N$ $m<n$ \uff0c $c_{S}\\in[\\textstyle{\\frac{n}{N}},1]$ 2: Draw a (fast) Johnson-Lindenstrauss transform $\\mathbf{T}\\in\\mathbb{R}^{r\\times m}$ (Remark 3.1). 3: Compute gradient sketching $\\widetilde{\\mathbf{G}}=\\nabla_{\\theta}f^{\\phi}\\left(\\mathbf{X};\\mathbf{0}_{r}\\right)\\mathbf{I}\\in\\mathbb{R}^{N\\times m}$ (Remark 3.4) 4: Compute the spectral decomposition of $\\begin{array}{r}{\\widetilde\\Sigma^{\\phi}=\\frac{1}{N}\\widetilde{\\mathbf{G}}^{\\top}\\widetilde{\\mathbf{G}}\\succcurlyeq0}\\end{array}$ $\\widetilde{\\pmb{\\Sigma}}^{\\phi}=\\mathbf{V}\\mathbf{A}\\mathbf{V}^{\\top}$ where (a) $\\mathbf{V}=[\\mathbf{v}_{1},\\cdots\\,,\\mathbf{v}_{m}]\\in\\mathbb{R}^{m\\times m}$ consists of the orthonormal eigenvectors, and (b) $\\mathbf{A}=\\mathrm{diag}\\left(\\lambda_{1},\\cdots,\\lambda_{m}\\right)$ contains descending eigenvalues $\\lambda_{1}\\geq\\cdot\\cdot\\geq\\lambda_{m}\\geq0$ 5: Initialize s = [s1,\\*;, SN with s = on $n$ uniformly sampled $i$ 's and $s_{i}=0$ elsewhere. 6: Let $\\mathrm{diag}(\\mathbf{s})\\in\\mathbb{R}^{N\\times N}$ be a diagonal matrix with s on diagonal. Optimizing: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{\\bf s}\\in\\Delta_{N}}{\\operatorname*{min}}\\ \\underset{{\\boldsymbol{\\gamma}}=[\\gamma_{1},\\cdots,\\gamma_{m}]\\in\\mathbb{R}^{m}}{\\operatorname*{min}}\\,\\sum_{j=1}^{m}\\Big({\\bf v}_{j}^{\\top}\\widetilde{\\bf G}^{\\top}\\operatorname{diag}\\,({\\bf s})\\,\\widetilde{\\bf G}{\\bf v}_{j}-\\gamma_{j}\\cdot\\boldsymbol{\\lambda}_{j}\\Big)^{2}}\\\\ &{s.t.\\quad0\\leq s_{i}\\leq1/n\\,\\forall\\,i\\in[N],\\quad\\gamma_{j}\\geq1/c_{S}\\,\\forall\\,j\\in[m].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "7: Output: $S\\subset[N]$ by sampling $n$ data from $\\mathbf{s}\\in\\Delta_{N}$ without replacement. ", "page_idx": 6}, {"type": "text", "text": "While directly minimizing $\\mathrm{tr}\\big(\\widetilde{\\Sigma}^{\\phi}(\\widetilde{\\Sigma}_{S}^{\\phi})^{\\dagger}\\big)$ involves integer programming and pseudoinverse, causing hard and numerically unstable optimization, $\\widetilde{\\Sigma}^{\\phi}\\,\\preccurlyeq\\,c_{S}\\widetilde{\\Sigma}_{S}^{\\phi}$ has a straightforward relaxation (vide Remark 3.2), leading to the simple and stable moment matching objective (5) in Algorithm 3.1. ", "page_idx": 6}, {"type": "text", "text": "Remark 3.2 (Relaxing $\\widetilde{\\Sigma}^{\\phi}\\preccurlyeq c_{S}\\widetilde{\\Sigma}_{S}^{\\phi}$ to(5). Given the spectral decomposition $\\widetilde{\\pmb{\\Sigma}}^{\\phi}=\\mathbf{V}\\mathbf{A}\\mathbf{V}^{\\top}$ \uff0c $\\widetilde{\\Sigma}^{\\phi}\\preccurlyeq$ $c_{S}\\widetilde\\Sigma_{S}^{\\phi}$ can bereritenaes $\\begin{array}{r}{\\mathbf{V}^{\\top}(\\frac{1}{n}\\widetilde{\\mathbf{G}}_{S}^{\\top}\\widetilde{\\mathbf{G}}_{S})\\mathbf{V}\\succcurlyeq\\frac{1}{c_{S}}\\mathbf{A}_{S}}\\end{array}$ and 5) is arelaxation? $(i)$ instead of enforcing $\\widetilde{\\Sigma}^{\\phi}\\preccurlyeq c_{S}\\widetilde{\\Sigma}_{S}^{\\phi}$ strictly, constraints are only imposed on the diagonal : $\\mathbf{v}_{j}^{\\top}({\\textstyle\\frac{1}{n}}\\widetilde{\\mathbf{G}}_{S}^{\\top}\\widetilde{\\mathbf{G}}_{S})\\mathbf{v}_{j}\\geq\\lambda_{j}/c_{S}$ $j\\in[m]$ ; and (ii) the selection of $S$ is relaxed to a weight vector $\\mathbf{s}\\in\\Delta_{N}$ with linear constraints $0\\leq\\dot{s}_{i}\\leq1/n$ .Free of integerconstraints and pseudoinverse,thequadratic data selection objective with linear constraints in (5) can be solved effciently and stably via projected gradient descent. ", "page_idx": 6}, {"type": "text", "text": "Alternative to the moment matching heuristic in Remark 3.2, variance reduction by controlling $\\widetilde{\\Sigma}^{\\phi}(\\widetilde{\\Sigma}_{S}^{\\phi})^{\\dagger}$ in the low-dimensional subspace can be realized via various methods, including leverage score sampling [18, 19, 72, 73, 74] and V-optimal experimental design [28, 29]. We provide brief discussions on these alternatives in Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "Remark 3.3 ( $c_{S}$ controls strength of moment matching). In Algorithm 3.1, smaller $c_{S}$ enforces $\\widetilde{\\Sigma}_{S}^{\\phi}$ to exploit more information in $\\widetilde{\\Sigma}^{\\phi}$ , bringing lower variance and better generalization. While the lower bound $c_{S}\\geq\\frac{n}{N}$ could be tight (vide Remark B.1), in practice, the smallest feasible $c_{S}$ depends on the data distribution and tends to be larger (e.g., $c_{S}\\approx1$ in the experiments). ", "page_idx": 6}, {"type": "text", "text": "Remark 3.4 (Computational efficiency of SkMM). SkMM is efficient in both memory and computation.Consider the two stages in Algorithm 3.1:(i) Gradient sketching can be computed in parallel with input-sparsity time and on the fy without storing the (potentially) high-dimensional gradients (vide Remark C.1). (ii) After gradient sketching, variance reduction via moment matching happensin thelowdimension $m$ ,witha lowmemoryfootprint $O(N m)$ taking $O(m^{3})$ forthespectral decompositionand $O(N m)$ per iteration for optimizing the moment matching objective (5). ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1  Synthetic High-dimensional Linear Probing ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To ground the theoretical insight on variance-bias tradeoff in high-dimensional finetuning, we simulate linear probing with a synthetic underdetermined ridge regression problem12. ", "page_idx": 7}, {"type": "table", "img_path": "yAAQWBMGiT/tmp/17b1af57f0d5c47ad0e388b3f45bf89d83641d61e46b4393acc0f64ef48976bf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Selecting $n=80$ data (colored in red) from the GMM dataset. Intuitively, a coreset $\\mathcal{D}_{S}$ with low bias contains at least one sample per cluster; whereas a low-variance $\\mathcal{D}_{S}$ selectsmoredata from clusters with larger variance. We recall from Theorem 2.2 that the variance-bias balance is essential for good generalization. ", "page_idx": 7}, {"type": "text", "text": "Setup.  We consider a set of $N=2000$ samples with high-dimensional pre-trained representations $\\phi(\\mathbf{X})\\,\\in\\,\\mathbb{R}^{N\\times r}$ \uff0c $r\\,=\\,2400$ modeled by a Gaussian mixture model (GMM) consisting of $\\overline{{r}}\\,=\\,8$ well-separated clusters, each with random sizes and variances (vide Figure 2). Samples within each cluster share the same randomly generated label. We solve the ridge regression problem (2) over the selected coreset of $n$ samples with hyperparameter $\\alpha$ tuning. The empirical risk is evaluated over the full dataset $\\begin{array}{r}{\\mathcal{L}_{\\mathcal{D}}(\\pmb{\\theta}_{S})=\\frac{1}{N}\\left\\|\\phi(\\mathbf{X})\\pmb{\\theta}_{S}-\\mathbf{y}\\right\\|_{2}^{2}}\\end{array}$ (vide Appendix D.1 for implementation details). ", "page_idx": 7}, {"type": "text", "text": "Data selection. For SkMM (Algorithm 3.1), we use a sketching dimension $m=4\\overline{{r}}=32$ and set $c_{S}=0.999$ . We optimize (5) via Adam [75] with constraint projection under learning rate $10^{-7}$ for $10^{4}$ iterations and sample $S$ from $\\mathbf{s}\\in\\Delta_{N}$ with the lowest objective value. ", "page_idx": 7}, {"type": "text", "text": "We compare SkMM to representative unsupervised data selection methods for regression, including uniform, leverage score [18, 19, 72, 73, 74], adaptive sampling [44, 56], herding [51, 52], and $\\mathbf{k}\\cdot$ -center greedy [53]. Specifically, (i) SkMM, truncated leverage score (T-leverage), and ridge leverage score sampling (R-leve rage) can be viewed as different ways of variance-bias balancing; (ii) adaptive sampling (Adapt i ve) and $\\boldsymbol{\\mathrm{k}}$ -centergreedy $({\\mathrm{K}}\\mathrm{-}\\mathsf{c e n t}\\!\\in\\!\\Sigma)$ focus on bias reduction (i.e., providing good low-rank approximation/clustering for $\\phi(\\mathbf{X})$ ); while (ii) Herding and uniform sampling (Un i form) reduce variance (vide Appendix D.2 for baseline details). ", "page_idx": 7}, {"type": "text", "text": "We observe from Figure 2 and Table 1 that balancing the variance-bias tradeoff is crucial for the generalization of data selection in high dimensions. In particular, SkMM achieves the best empirical risk across different coreset sizes $n$ ,especiallywhen $n$ is small.While as $n/N\\,\\rightarrow\\,1$ uniform sampling provides a strong baseline, coinciding with common empirical observations [1]. ", "page_idx": 7}, {"type": "table", "img_path": "yAAQWBMGiT/tmp/219979c07e9d646502026bc7ac7f0617128c3cfec89cefbf3878abd600081952.jpg", "table_caption": ["Table 1: Empirical risk $\\mathcal{L}_{\\mathcal{D}}(\\pmb{\\theta}_{S})$ on the GMM dataset at various $n$ , under the same hyperparameter tuning where ridge regression over the full dataset $\\mathcal{D}$ with $N=2000$ samples achieves $\\bar{\\mathcal{L}}_{\\mathcal{D}}(\\pmb{\\theta}_{[N]})=$ 2.95e-3. For methods involving sampling, results are reported over 8 random seeds. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.2  Experiments on Regression Tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further validate the effectiveness of SkMM on UTKFace [76], a real-world regression dataset for age estimation. We finetune a randomly initialized classification head on top of the feature representation of CLIP [50] with Adam [75] and learning rate $10^{-1}$ .We also retain those baselines from the above synthetic setup in this experiment. ", "page_idx": 8}, {"type": "table", "img_path": "yAAQWBMGiT/tmp/0a2029b7570a6ebd82d6f38fea217c7fc68d97d4f9a0404a2cb27fb487f9cd6c.jpg", "table_caption": ["Table 2: Mean Absolute Error (the lower the better) on UTKFace with a linear regressor trained on top of frozen features from a pre-trained CLIP (ViT-B/32). We use the bold font to indicate the best method for each coreset size. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The results for linear probing are provided in Table 2, where our method remarkably outperforms comparative baselines on UTKFace. For every coreset size, SkMM improves the performance of CLIP compared to uniform sampling. Especially for small coreset size $n=100$ ,200, it achieves a Mean Absolute Error reduction of approximately $50\\%$ ", "page_idx": 8}, {"type": "text", "text": "4.3  Experiments on Image Classification Tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While our analysis focuses on data selection for finetuning regression models, a natural question is whether the idea of SkMM applies to broader scopes. To answer this, we extend our empirical investigation to classification. In particular, we consider an imbalanced classification task: StanfordCars [77] with 196 classes, 8144 training samples, and 8041 testing samples where the classes are highly imbalanced with training sample sizes ranging from 24 to 68. ", "page_idx": 8}, {"type": "text", "text": "Finetuning.  We consider two common ways of finetuning: (i) linear probing (LP) over the last layer and (i) funetuning (FT) over the last few layers, covering both the low- (i.e., $n\\geq r$ for LP) and high-dimensional (i.e., $r>n$ for FT) settings. For LP, we learn the last layer over the embeddings from a CLIP-pretrained ViT-B/32 [50] with a learning rate of $10^{-1}$ . For $\\mathrm{FT}^{13}$ , we finetuning the last two layers of an ImageNet-pretrained ResNet18 [84] with a learning rate of $10^{-2}$ . In both settings, we optimize via Adam for 50 epochs. Due to space limit constraints, detailed results for fine-tuning are deferred to the appendix. ", "page_idx": 8}, {"type": "text", "text": "Data selection.  For SkMM-LP, the gradients (of the last layer) are given by the pretrained features from CLIP. For SkMM-FT, the gradients (of the last two layers) are calculated based on a random classification head. We tune the sketching dimension $m\\in\\{32,64,128,256,512\\}$ and the lower bound for slackness variables $c_{S}\\in\\{0.6,0.7,0.8,0.9\\}$ . Within suitable ranges, smaller $m$ and larger $c_{S}$ lead to better performance in the low data regime. Intuitively, smaller $m$ encourages variance reduction in a more compressed subspace, and larger $c_{s}$ leads to easier optimization. ", "page_idx": 8}, {"type": "table", "img_path": "yAAQWBMGiT/tmp/e6bc9157c27ba2994877bfed215052a7c806142ef3cecfbebe598fef3fcf5778.jpg", "table_caption": ["Table 3: Accuracy and F1 score $(\\%)$ of LP over CLIP on StanfordCars "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "We compare SkMM to various unsupervised and (weakly) supervised data selection methods for classification, including uniform sampling, herding [51], Contextual Diversity [78], Glister [79], GraNd [66], Forgetting [80], DeepFool [81], as well as three uncertainty-based methods, Entropy, Margin, and Least Confidence [82]. ", "page_idx": 9}, {"type": "text", "text": "Observations. We first observe that for both LP (Table 3) and FT (Table 4), SkMM achieves competitive finetuning accuracy on StanfordCars. Since SkMM is an unsupervised process agnostic of true class sizes, the appealing performance of SkMM on the imbalanced StanfordCars dataset echoes the ability of SkMM to handle data selection among clusters of various sizes through variancebias balance (cf. synthetic experiments in Figure 2). Meanwhile, for LP in the low-dimensional setting (Table 3), uniform sampling provides a surprisingly strong baseline. This coincides with the theoretical insight from Proposition 2.1 and the empirical observations in [1]. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion, Limitations, and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We investigated data selection for finetuning in both low and high dimensions from a theoretical perspective. Beyond variance reduction in low dimension, our analysis revealed the variance-bias tradeoff in data selection for high-dimensional finetuningwithlow intrinsic dimension $\\overline{r}$ ,balancing which led to a fast-rate generalization ${\\cal O}(\\overline{{r}}/n)$ . For efficient control of such variance-bias tradeoff in practice, we introduced SkMM that first explores the high-dimensional parameter space via gradient sketching and then exploits the resulting low-dimensional subspace via moment matching. Theoretically, we showed that the low-dimensional subspace from gradient sketching preserves the fast-rate generalization. Moreover, we ground the theoretical insight on balancing the variance-bias tradeoff via synthetic experiments, while demonstrating the effectiveness of SkMM for finetuning real vision tasks. ", "page_idx": 9}, {"type": "text", "text": "In this work, we focus only on moment matching via optimization inspired by the analysis for variance reduction after gradient sketching. Nevertheless, there is a remarkable variety of existing low-dimensional data selection strategies (e.g., via greedy selection or sampling) that could potentially be extended to high dimensions leveraging sketching as an efficient pre-processing step. In linear algebra, sketching has been widely studied for accelerating, as well as stabilizing, large-scale low-rank approximations and linear solvers. However, the intuitions and theories there may or may not be directly applicable to the statistical learning regime. In light of the high-dimensional nature of deep learning where sketching brings an effective remedy, we hope that providing a rigorous generalization analysis for sketching in data selection would make a step toward bridging the classical wisdom of sketching and the analogous challenges in modern learning problems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors wish to thank Yunzhen Feng, Julia Kempe, and Christopher Musco for insightful discussions. QL was partially supported by the NYU Research Catalyst Prize and the Department of Energy under ASCR Award DE-SC0024721. YD was supported by the NYU Courant Instructorship. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Chengcheng Guo, Bo Zhao, and Yanbing Bai. Deepcore: A comprehensive library for coreset selection in deep learning. In International Conference on Database and Expert Systems Applications, pages 181-195. Springer, 2022.   \n[2] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024.   \n[3]  Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523-19536, 2022.   \n[4] Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. Dataset pruning: Reducing training data by examining generalization influence. arXiv preprint arXiv:2205.09329, 2022.   \n[5] Haizhong Zheng, Rui Liu, Fan Lai, and Atul Prakash. Coverage-centric coreset selection for high pruning rates. arXiv preprint arXiv:2210.15809, 2022.   \n[6] Xiaobo Xia, Jiale Liu, Jun Yu, Xu Shen, Bo Han, and Tongliang Liu. Moderate coreset: A universal method of data selection for real-world data-efficient deep learning. In The Eleventh International Conference on Learning Representations, 2022.   \n[7] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-effcient training of machine learning models. In International Conference on Machine Learning, pages 6950-6960. PMLR, 2020.   \n[8]  Zalan Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continuallearning and streaming. Advances in neural information processing systems, 33:14879- 14890, 2020.   \n[9] Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. Retrieve: Coreset selection for efficient and robust semi-supervised learning. Advances in neural information processing systems, 34:14488-14501, 2021.   \n[10] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278-25294, 2022.   \n[11] Samir Yitzhak Gadre, Gabriel Iharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.   \n[12]  Yiping Wang, Yifang Chen, Wendan Yan, Kevin Jamieson, and Simon Shaolei Du. Variance alignment score: A simple but tough-to-beat data selection method for multimodal contrastive learning. arXiv preprint arXiv:2402.02055, 2024.   \n[13] Siddharth Joshi, Arnav Jain, Ali Payani, and Baharan Mirzasoleiman. Data-efficient contrastive language-image pretraining: Prioritizing data quality over quantity. arXiv preprint arXiv:2403.12267, 2024.   \n[14]  Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020.   \n[15] Sachin Goyal, Pratyush Maini, Zachary C Lipton, Aditi Raghunathan, and J Zico Kolter. Scaling laws for data filtering-data curation cannot be compute agnostic. arXiv preprint arXiv:2404.07177, 2024.   \n[16] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting infuential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024.   \n[17]  David P Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and Trends@ in Theoretical Computer Science, 10(1-2):1-157, 2014.   \n[18]  Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In 2006 47th annual IEEE symposium on foundations of computer science (FOCS06), pages 143-152. IEEE, 2006.   \n[19]  Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statistical guarantees. Advances in neural information processing systems, 28, 2015.   \n[20]  Garvesh Raskutti and Michael W Mahoney. A statistical perspective on randomized sketching for ordinary least-squares. Journal of Machine Learning Research, 17(213):1-31, 2016.   \n[21] Xue Chen and Eric Price. Active regression via linear-sample sparsification. In Conference on Learning Theory, pages 663-695. PMLR, 2019.   \n[22] Brett W Larsen and Tamara G Kolda. Sketching matrix least squares via leverage scores estimates. arXiv preprint arXiv:2201.10638, 2022.   \n[23]  Michal Derezinski, Manfred K Warmuth, and Daniel Hsu. Unbiased estimators for random design regression. Journal of Machine Learning Research, 23(167):1-46, 2022.   \n[24]  Atsushi Shimizu, Xiaoou Cheng, Christopher Musco, and Jonathan Weare. Improved active learning via dependent leverage score sampling. arXiv preprint arXiv:2310.04966, 2023.   \n[25]  Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statistical science, pages 273-304, 1995.   \n[26] Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006.   \n[27]  Valeri Vadimovich Fedorov. Theory of optimal experiments. Elsevier, 2013.   \n[28] Yining Wang, Adams Wei Yu, and Aarti Singh. On computationally tractable selection of experiments in measurement-constrained regression models. Journal of Machine Learning Research, 18(143):1-41, 2017.   \n[29]  Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete optimization for experimental design: A regret minimization approach. arXiv preprint arXiv:1711.05174, 2017.   \n[30]  Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? SIAM Journal on Mathematics of Data Science, 1(1):144-160, 2019.   \n[31]  William B Johnson, Extensions of lipshitz mapping into hilbert space. In Conference modern analysis and probability, 1984, pages 189-206, 1984.   \n[32]  Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217-288, 2011.   \n[33]  Per-Gunnar Martinsson and Joel A Tropp. Randomized numerical linear algebra: Foundations and algorithms. Acta Numerica, 29:403-572, 2020.   \n[34]  Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. arXiv preprint arXiv:2303.14186, 2023.   \n[35]  Michal Cermy and Milan Hladik. Two complexity results on c-optimality in experimental design. Computational Optimization and Applications, 51(3):1397-1408, 2012.   \n[36] Helge Blaker. Minimax estimation in linear rgression under restrictions. Journal of statistical planning and inference, 90(1):35-55, 2000.   \n[37] Qi Lei, Wei Hu, and Jason Lee. Near-optimal linear regression under distribution shift. In International Conference on Machine Learning, pages 6164-6174. PMLR, 2021.   \n[38]  Michael W Mahoney and Petros Drineas. Cur matrix decompositions for improved data analysis. Proceedings of the National Academy of Sciences, 106(3):697-702, 2009.   \n[39]  Amit Deshpande, Luis Rademacher, Santosh S Vempala, and Grant Wang. Matrix approximation and projective clustering via volume sampling. Theory of Computing, 2(1):225-247, 2006.   \n[40]  Sergey Voronin and Per-Gunnar Martinsson. Efficient algorithms for cur and interpolative matrix decompositions. Advances in Computational Mathematics, 43:495-516, 2017.   \n[41]  Michat Derezinski and Michael W Mahoney. Determinantal point processes in randomized numerical linear algebra. Notices of the American Mathematical Society, 68(1):34 45, 2021.   \n[42] Michal Derezinski, Rajiv Khanna, and Michael W Mahoney. Improved guarantees and a multiple-descent curve for column subset selection and the nystrom method. Advances in Neural Information Processing Systems, 33:4953-4964, 2020.   \n[43]  Yijun Dong and Per-Gunnar Martinsson. Simpler is better: a comparative study of randomized pivoting algorithms for cur and interpolative decompositions. Advances in Computational Mathematics, 49(4):66, 2023.   \n[44] Yifan Chen, Ethan N Epperly, Joel A Tropp, and Robert J Webber. Randomly pivoted cholesky: Practical approximation of akernel matrix with few entry evaluations. arXiv preprint arXiv:2207.06503, 2022.   \n[45] Yijun Dong, Chao Chen, Per-Gunnar Martinsson, and Katherine Pearce. Robust blockwise random pivoting: Fast and accurate adaptive interpolative decomposition. arXiv preprint arXiv:2309.16002, 2023.   \n[46] Katherine J Pearce, Chao Chen, Yijun Dong, and Per-Gunnar Martinsson. Adaptive parallelizable algorithmsfor interpolative decompositions via partially pivoted lu. arXiv preprint arXiv:2310.09417, 2023.   \n[47] Daniel A Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 563-568, 2008.   \n[48]  Kai Yu, Jinbo Bi, and Volker Tresp. Active learning via transductive experimental design. In Proceedings of the 23rd international conference on Machine learning, pages 1081-1088, 2006.   \n[49] Neta Shoham and Haim Avron. Experimental design for overparameterized learning with application to single shot deep active learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):11766-11777, 2023.   \n[50]  Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnaturallanguage supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.   \n[51]  Max Welling. Herding dynamical weights to learn. In Proceedings of the 26th annual international conference on machine learning, pages 1121-1128, 2009.   \n[52] Yutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. arXiv preprint arXiv:1203.3472, 2012.   \n[53]  Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489, 2017.   \n[54]  Samprit Chatterjee and Ali S Hadi. Influential observations, high leverage points, and outliers in linear regression. Statistical science, pages 379-393, 1986.   \n[55] Petros Drineas, Michael W Mahoney, Shan Muthukrishnan, and Tamas Sarl6s. Faster least squares approximation. Numerische mathematik, 117(2):219-249, 2011.   \n[56]  Amit Deshpande and Santosh Vempala. Adaptive sampling and fast low-rank matrix approximation. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 292-303. Springer, 2006.   \n[57] Germain Kolossov, Andrea Montanari, and Pulkit Tandon. Towards a statistical theory of data selection under weak supervision. arXiv preprint arXiv:2309.14563, 2023.   \n[58] Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics, 27(4):986-1005, 1956.   \n[59] H Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In Proceedings of the fifth annual workshop on Computational learning theory, pages 287-294, 1992.   \n[60]  David D Lewis. A sequential algorithm for training text classifiers: Corrigendum and additional data. In Acm Sigir Forum, volume 29, pages 13-19. ACM New York, NY, USA, 1995.   \n[61]  Daniel Ting and Eric Brochu. Optimal subsampling with infuence functions. Advances in neural information processing systems, 31, 2018.   \n[62] HaiYing Wang, Rong Zhu, and Ping Ma. Optimal subsampling for large sample logistic regression. Journal of the American Statistical Association, 113(522):829-844, 2018.   \n[63]  Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David Woodruff. On coresets for logistic regression. Advances in Neural Information Processing Systems, 31, 2018.   \n[64]  Tung Mai, Cameron Musco, and Anup Rao. Coresets for classification-simplifed and strengthened. Advances in Neural Information Processing Systems, 34:11643-11654, 2021.   \n[65]  Kyriakos Axiotis, Vincent Cohen-Addad, Monika Henzinger, Sammy Jerome, Vahab Mirrokni, David Saulpic, David Woodruff and Michael Wunder. Data-effcient learning via clusteringbased sensitivity sampling: Foundation models and beyond. arXiv preprint arXiv:2402.17327, 2024.   \n[66]  Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important examples early in training. Advances in Neural Information Processing Systems, 34:20596-20607, 2021.   \n[67]  Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. Accelrating deep learning by focusing on the biggest losers. arXiv preprint arXiv: 1910.00762, 2019.   \n[68] Kailas Vodrahalli, Ke Li, and Jitendra Malik. Are alltraining examples created equal? an empirical study. arXiv preprint arXiv: 1811.12569, 2018.   \n[69] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. In International Conference on Learning Representations.   \n[70] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[71]  Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \n[72] Petros Drineas, Michael W Mahoney, and Shan Muthukrishnan. Relative-error cur matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844-881, 2008.   \n[73] Mu Li, Gary L Miller, and Richard Peng. Iterative row sampling. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pages 127-136. IEEE, 2013.   \n[74] Michael B Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1758-1777. SIAM, 2017.   \n[75] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[76] Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5810-5818, 2017.   \n[77] Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting a large-scale dataset of fine-grained cars. 2013.   \n[78] Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan Arora. Contextual diversity for active learning. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI 16, pages 137-153. Springer, 2020.   \n[79] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: Generalization based data subset selection for efficient and robust learning. In Proceedings of the AAAl Conference on Artificial Intelligence, volume 35, pages 8110-8118, 2021.   \n[80]  Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018.   \n[81]  Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2574-2582, 2016.   \n[82]  Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. arXiv preprint arXiv: 1906.11829, 2019.   \n[83] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Finetuning can distort pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054, 2022.   \n[84] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.   \n[85]  Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n[86]  Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019.   \n[87]  Daniel M Kane and Jelani Nelson. Sparser johnson-lindenstrauss transforms. Journal of the ACM (JACM), 61(1):1-23, 2014.   \n[88] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604-613, 1998.   \n[89] Jelani Nelson and Huy L Nguyen. Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings. In 2013 ieee 54th annual symposium on foundations of computer science, pages 117-126. IEEE, 2013.   \n[90] Franco Woolfe, Edo Liberty, Vladimir Rokhlin, and Mark Tygert. A fast randomized algorithm for the approximation of matrices. Applied and Computational Harmonic Analysis, 25(3):335- 366, 2008.   \n[91]  Joel A Tropp. Improved analysis of the subsampled randomized hadamard transform. Advances in Adaptive Data Analysis, 3(01n02):115-126, 2011.   \n[92]  Kazushige Goto and Robert Van De Geijn. High-performance implementation of the level-3 blas. ACM Transactions on Mathematical Software (TOMS), 35(1):1-14, 2008.   \n[93]  Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression. In Proceedings of the forty-fth annual ACM symposium on Theory of computing, pages 91-100, 2013.   \n[94]  Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In International Colloquium on Automata, Languages, and Programming, pages 693-703. Springer, 2002.   \n[95]  Michael B Cohen.  Nearly tight oblivious subspace embeddings by trace inequalities. In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms, pages 278-287. SIAM, 2016.   \n[96]  Joel A Tropp, Alp Yurtsever, Madeleine Udell, and Volkan Cevher. Fixed-rank approximation of a positive-semidefinite matrix from streaming data. Advances in Neural Information Processing Systems, 30, 2017.   \n[97]  Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences,62(12):1707-1739, 2009.   \n[98]  Daniel B Szyld. The many proofs of an identity on the norm of oblique projections. Numerical Algorithms, 42:309-323, 2006. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A  Additional Discussions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Additional Notations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given any matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ , along with indices $i\\in[n],\\,j\\,\\in[d],\\,I\\,\\subseteq\\,[n],$ and $J\\subseteq[d]$ let $[\\mathbf{A}]_{i,j}$ be the $(i,j)$ -th entry of A, $[\\mathbf{A}]_{i}$ be the $i$ -th row (or the $i^{\\th}$ -th entry if $\\mathbf{A}\\in\\mathbb{R}^{n}$ is a vector), and $[\\mathbf{A}]_{:,j}$ be the $j$ -th column; $\\mathbf{A}_{I}=\\left[\\mathbf{A}\\right]_{I,}$ : consists of rows in $\\mathbf{A}$ indexed by $I$ ; and let ${\\bf A}_{I,J}=[{\\bf A}]_{I,J}$ be the submatrix of $\\mathbf{A}$ with rowsindexed by $I$ and columns indexed by $J$ ", "page_idx": 16}, {"type": "text", "text": "A.2 Alternatives to Moment Matching Heuristic in Remark 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In addition to the moment matching heuristic in Remark 3.2, variance in the resulting low-dimensional subspace from gradient sketching can be controlled by $\\widetilde{\\Sigma}^{\\phi}(\\widetilde{\\Sigma}_{S}^{\\phi})^{\\dagger}$ via alternative methods like leverage score sampling and V-optimal experimental design. ", "page_idx": 16}, {"type": "text", "text": "Remark A.1 (Leverage score sampling). Leverage score sampling_ [18, 19, 72, 73, 74] provides arguably one of the most intuitive ways for selecting data based on $\\widetilde{\\mathbf{G}}\\in\\mathbb{R}^{N\\times m}$ In particular, $I I7_{:}$ Theorem $I7J$ implies that for a coreset of size at least $n=\\Omega(m\\log(m/\\delta)\\epsilon^{-2})$ drawn i.i.d. with replacement via leverage score sampling over $\\tilde{\\bf G}$ $c_{S}\\leq(1+\\epsilon)\\frac{m}{\\tau_{S}N}$ with probability at least $1-\\delta$ where $\\tau_{S}\\in[0,1]$ is the minimum leverage score of $\\tilde{\\bf G}$ over the coreset $S$ 14 Such dependence on $\\tau_{S}$ can render the upper bound of $c_{S}$ vacuous when $\\tau_{S}\\to0$ ", "page_idx": 16}, {"type": "text", "text": "Nevertheless, when $\\tau_{S}$ is reasonably large, leverage score sampling based on $\\tilde{\\bf G}$ can be computed more efficiently than SkMM in $O(N m^{2})$ time and can provide good control over $c_{S}$ .While both SkMM and leverage score sampling can facilitate variance reduction in the low-dimensional subspace, SkMM provides better empirical performance (cf. Section 4.1) at a slightly higher cost in the low intrinsicdimension $m$ (vide Remark 3.4) as it is tailored for optimizing moment matching. ", "page_idx": 16}, {"type": "text", "text": "Remark A.2 (V-optimal experimental design). Variance in the low-dimensional subspace can also be controlled by applying the V-optimal experimental design methods [28, 29] on $\\widetilde{\\mathbf{G}}\\in\\mathbb{R}^{N\\times m}$ .For example,[291provides a polynomial-time algorithm tofind a $\\left(1+\\epsilon\\right)$ -estimationoftheV-optimal design for $\\tilde{\\bf G}$ with a coreset of size at least $n=\\Omega(m\\epsilon^{-2})$ :and $c_{S}$ is effectivelycontrolledby the V-optimality criterion $\\mathrm{tr}(\\widetilde{\\Sigma}^{\\phi}(\\widetilde{\\Sigma}_{S}^{\\phi})^{\\dagger})$ ", "page_idx": 16}, {"type": "text", "text": "While such V-optimal design methods can provide good control over cs with nearly optimal sample complexity, they are computationally more expensive than SkMM(or leverage score sampling) and tend to suffer from numerical instability issues in practice. For example, the algorithm in [29] consists of two stages:(i) solving a continuous relaxation of the original discrete optimization problem posed by V-optimality, and (ii) rounding the continuous solution via regret minimization. While the cost of rounding is negligible, solving the continuous relaxation of V-optimality (in contrast to leveraging fast and stable heuristics like the one in SkMM, cf. Remark 3.2) is challenging, both in terms of computationalcomplexityandnumericalstability. ", "page_idx": 16}, {"type": "text", "text": "B Proofs for Section 2.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Proofs of (1) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of (1) and beyond. Under the assumption $\\operatorname{rank}\\left(\\phi\\left(\\mathbf{X}_{S}\\right)\\right)=r$ both $\\phi\\left(\\mathbf{X}_{S}\\right),\\phi\\left(\\mathbf{X}\\right)$ have full column rank. Therefore $\\phi\\left(\\mathbf{X}_{S}\\right)^{\\dagger}\\phi\\left(\\mathbf{X}_{S}\\right)=\\phi\\left(\\mathbf{X}\\right)^{\\dagger}\\phi\\left(\\mathbf{X}\\right)=\\mathbf{I}_{r}$ , and $\\pmb{\\theta}_{S}=\\phi\\left(\\mathbf{X}_{S}\\right)^{\\dagger}\\mathbf{y}_{S}$ . Then, since $\\mathbf{y}=\\phi\\left(\\mathbf{X}\\right)\\pmb{\\theta}_{\\ast}+\\mathbf{z}$ and $\\mathbf{y}_{S}=\\phi\\left(\\mathbf{X}_{S}\\right)\\pmb{\\theta}_{\\ast}+\\mathbf{z}_{S}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{S}-\\theta_{*}=\\phi\\left(\\mathbf{X}_{S}\\right)^{\\dagger}\\mathbf{y}_{S}-\\theta_{*}=\\left(\\phi\\left(\\mathbf{X}_{S}\\right)^{\\dagger}\\phi\\left(\\mathbf{X}_{S}\\right)\\theta_{*}-\\theta_{*}\\right)+\\phi\\left(\\mathbf{X}_{S}\\right)^{\\dagger}\\mathbf{z}_{S}=\\phi\\left(\\mathbf{X}_{S}\\right)^{\\dagger}\\mathbf{z}_{S},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]=\\mathbb{E}\\left[\\frac{1}{N}\\left\\Vert\\phi\\left(\\mathbf{X}\\right)\\left(\\pmb{\\theta}_{S}-\\pmb{\\theta}_{\\ast}\\right)\\right\\Vert_{2}^{2}\\right]}\\\\ &{=\\mathrm{tr}\\left(\\left(\\frac{1}{N}\\phi\\left(\\mathbf{X}\\right)^{\\top}\\phi\\left(\\mathbf{X}\\right)\\right)\\phi\\left(\\mathbf{X}_{S}\\right)^{\\dagger}\\mathbb{E}\\left[\\mathbf{z}_{S}\\mathbf{z}_{S}^{\\top}\\right]\\left(\\phi\\left(\\mathbf{X}_{S}\\right)^{\\dagger}\\right)^{\\top}\\right)}\\\\ &{=\\!\\sigma^{2}\\,\\mathrm{tr}\\left(\\left(\\frac{1}{N}\\phi\\left(\\mathbf{X}\\right)^{\\top}\\phi\\left(\\mathbf{X}\\right)\\right)\\left(\\phi\\left(\\mathbf{X}_{S}\\right)^{\\top}\\phi\\left(\\mathbf{X}_{S}\\right)\\right)^{-1}\\right)}\\\\ &{=\\!\\frac{\\sigma^{2}}{n}\\,\\mathrm{tr}\\left(\\mathbf{\\Sigma}\\!\\Sigma^{\\phi}\\left(\\mathbf{\\Sigma}\\!\\right)^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we explain the necesstyof assuming $c_{S}\\geq n/N$ for $\\Sigma^{\\phi}\\precc{c}_{S}\\Sigma_{S}^{\\phi}$ ", "page_idx": 17}, {"type": "text", "text": "Remark B.1 (Lower bound of $c_{S}$ ). Since $\\phi\\left(\\mathbf{X}\\right)^{\\top}\\phi\\left(\\mathbf{X}\\right)\\;\\succcurlyeq\\;\\phi\\left(\\mathbf{X}_{S}\\right)^{\\top}\\phi\\left(\\mathbf{X}_{S}\\right),$ we observe that $N\\pmb{\\Sigma}^{\\phi}\\succcurlyeq n\\pmb{\\Sigma}_{S}^{\\phi}$ whichimplies $\\begin{array}{r}{\\Sigma^{\\phi}\\succcurlyeq\\frac{n}{N}\\Sigma_{S}^{\\phi}}\\end{array}$ Therefor, $\\Sigma^{\\phi}\\precc{c}_{S}\\Sigma_{S}^{\\phi}$ isonlypossiblewhen $c_{S}\\geq n/N$ Notice that this lower bound of $c_{S}$ is tight, e.g. when $\\tilde{\\bf G}$ consists of $N-n$ rows of zeros. ", "page_idx": 17}, {"type": "text", "text": "Low-dimensional linear probing with moment matching.  Recall from (1) that $\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]=$ ${\\scriptstyle{\\frac{\\sigma^{2}}{n}}}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\left(\\Sigma_{S}^{\\phi}\\right)^{-1}\\right)$ Further assuming a suitable selection of $\\mathcal{D}_{S}$ with $\\Sigma^{\\phi}\\precc{c}_{S}\\Sigma_{S}^{\\phi}$ wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{tr}\\left(\\sum^{\\phi}\\left(\\Sigma_{S}^{\\phi}\\right)^{-1}\\right)\\leq c_{S}\\operatorname{tr}\\left(\\mathbf{I}_{r}\\right)=c_{S}r\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and therefore, $\\begin{array}{r}{\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\leq c_{S}\\frac{\\sigma^{2}r}{n}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "B.2Proof of Proposition 2.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 2.1. Let $\\widehat\\Sigma_{S}^{\\phi}\\,:=\\,\\left(\\Sigma^{\\phi}\\right)^{-1/2}\\Sigma_{S}^{\\phi}\\left(\\Sigma^{\\phi}\\right)^{-1/2}$ . The goal of $\\Sigma^{\\phi}\\,\\prec\\,c_{S}\\Sigma_{S}^{\\phi}$ can be re-expressed as $c_{S}\\widehat{\\pmb{\\Sigma}}_{S}^{\\phi}\\succcurlyeq\\mathbf{I}_{r}$ , or equivalently when $c_{S}>1$ $\\begin{array}{r}{\\left\\|\\widehat{\\pmb{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right\\|_{2}\\leq1-\\frac{1}{c_{S}}}\\end{array}$ With uniform sampling, since ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S}\\left[\\pmb{\\Sigma}_{S}^{\\phi}\\right]=\\mathbb{E}_{S}\\left[\\frac{1}{n}\\sum_{\\mathbf{x}\\in S}\\phi\\left(\\mathbf{X}\\right)\\phi\\left(\\mathbf{X}\\right)^{\\top}\\right]=\\mathbb{E}_{\\mathbf{x}}\\left[\\phi\\left(\\mathbf{X}\\right)\\phi\\left(\\mathbf{X}\\right)^{\\top}\\right]=\\pmb{\\Sigma}^{\\phi},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we have $\\mathbb{E}_{S}\\left[\\widehat{\\pmb{\\Sigma}}_{S}^{\\phi}\\right]=\\mathbf{I}_{r}$ . For any fixed unit vector $\\textbf{z}\\in\\mathbb{S}^{r-1}$ , let $Z_{i}:=\\mathbf{z}^{\\top}\\left(\\Sigma^{\\phi}\\right)^{-1/2}\\phi\\left(\\mathbf{x}_{i}\\right)$ be random variables with randomness on $i\\in[N]$ . Since $\\|\\phi(\\mathbf{x})\\|_{2}\\leq B_{\\phi}\\,\\forall\\,\\mathbf{x}\\in\\mathcal{D}$ and $\\boldsymbol{\\Sigma}^{\\phi}\\succcurlyeq\\boldsymbol{\\gamma}\\mathbf{I}_{r}$ ,we observe that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|Z_{i}\\right|\\leq\\left\\|\\left(\\Sigma^{\\phi}\\right)^{-1/2}\\phi\\left(\\mathbf{x}_{i}\\right)\\right\\|_{2}\\leq\\frac{B_{\\phi}}{\\sqrt{\\gamma}}\\quad\\forall\\,i\\in[N]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is bounded. Therefore, $Z_{i}$ $\\left(\\frac{B_{\\phi}^{2}}{\\gamma}\\right)$ -subGaussian, and $\\left(Z_{i}^{2}-\\mathbb{E}\\left[Z_{i}^{2}\\right]\\right)\\ =\\ \\mathbf{z}^{\\top}\\left(\\widehat{\\Sigma}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\mathbf{z}$ .s   \n$\\left(16\\frac{B_{\\phi}^{2}}{\\gamma}\\right)$   \nany $0<\\epsilon_{1}\\le16B_{\\phi}^{2}/\\gamma$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{P}\\left[\\mathbf{z}^{\\top}\\left(\\widehat{\\mathbf{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\mathbf{z}\\geq\\epsilon_{1}\\right]\\leq\\exp\\left(-\\frac{n}{2}\\cdot\\frac{\\epsilon_{1}^{2}\\gamma^{2}}{16^{2}B_{\\phi}^{4}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By recalling that $\\begin{array}{r}{\\left\\|\\widehat{\\boldsymbol{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right\\|_{2}=\\operatorname*{max}_{\\mathbf{u}\\in\\mathbb{S}^{r-1}}\\mathbf{u}^{\\top}\\left(\\widehat{\\boldsymbol{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\mathbf{u}}\\end{array}$ Equation (6) for a fixed $\\mathbf{z}\\in\\mathbb{S}^{r-1}$ can be extended to the entire unit sphere $\\mathbb{S}^{r-1}$ through an $\\epsilon$ -net argument as follows. Recall that for ", "page_idx": 17}, {"type": "text", "text": "any $\\epsilon_{2}>0$ , there exists an $\\epsilon_{2}$ -net $\\mathcal{U}\\subset\\mathbb{S}^{r-1}$ such that $\\begin{array}{r}{|\\mathcal{U}|\\leq\\left(1+\\frac{2}{\\epsilon_{2}}\\right)^{r}}\\end{array}$ .Then, by the union bound, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}\\left[\\operatorname*{max}_{\\mathbf{u}\\in\\mathcal{U}}\\mathbf{u}^{\\top}\\left(\\widehat{\\mathbf{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\mathbf{u}>\\epsilon_{1}\\right]\\le\\left(1+\\frac{2}{\\epsilon_{2}}\\right)^{r}\\exp\\left(-\\frac{n}{2}\\cdot\\frac{\\epsilon_{1}^{2}\\gamma^{2}}{16^{2}B_{\\phi}^{4}}\\right)}}\\\\ &{}&{=\\exp\\left(r\\log\\left(1+\\frac{2}{\\epsilon_{2}}\\right)-\\frac{n}{2}\\cdot\\frac{\\epsilon_{1}^{2}\\gamma^{2}}{16^{2}B_{\\phi}^{4}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "That is, with probability at least $1-\\delta$ $\\begin{array}{r}{\\operatorname*{max}_{\\mathbf{u}\\in\\mathcal{U}}\\mathbf{u}^{\\top}\\left(\\widehat{\\Sigma}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\mathbf{u}\\leq\\epsilon_{1}}\\end{array}$ when ", "page_idx": 18}, {"type": "equation", "text": "$$\nn\\geq\\frac{512B_{\\phi}^{4}}{\\gamma^{2}\\epsilon_{1}^{2}}\\left(r\\log\\left(1+\\frac{2}{\\epsilon_{2}}\\right)+\\log\\left(\\frac{1}{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the construction of the $\\epsilon_{2}$ -net $\\boldsymbol{\\mathcal{U}}$ for all $\\mathbf{v}\\in\\mathbb{S}^{r-1}$ , there exists $\\mathbf{u}\\in\\mathcal{U}$ such that $\\|\\mathbf{u}-\\mathbf{v}\\|_{2}\\leq\\epsilon_{2}$ Therefore, for any $\\mathbf{v}\\in\\mathbb{S}^{r-1}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{v}^{\\top}\\left(\\widehat{\\mathbf{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\mathbf{v}}\\\\ &{=\\!\\mathbf{u}^{\\top}\\left(\\widehat{\\mathbf{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\mathbf{u}+\\left(\\mathbf{v}-\\mathbf{u}\\right)^{\\top}\\left(\\widehat{\\mathbf{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\left(\\mathbf{v}-\\mathbf{u}\\right)+2\\left(\\mathbf{v}-\\mathbf{u}\\right)^{\\top}\\left(\\widehat{\\mathbf{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\mathbf{u}}\\\\ &{\\leq\\!\\epsilon_{1}+\\left\\|\\widehat{\\mathbf{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right\\|_{2}\\left(\\epsilon_{2}^{2}+2\\epsilon_{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which imples $\\begin{array}{r}{\\left\\|\\widehat{\\boldsymbol{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right\\|_{2}\\leq\\frac{\\epsilon_{1}}{2-(1+\\epsilon_{2})^{2}}}\\end{array}$ By aking $\\epsilon_{2}$ as a smal onsta eg. $\\epsilon_{2}=\\sqrt{3/2}-1)$ \uff0c we have $\\begin{array}{r}{\\left\\|\\widehat{\\pmb{\\Sigma}}_{S}^{\\phi}-\\mathbf{I}_{r}\\right\\|_{2}\\leq1-\\frac{1}{c s}}\\end{array}$ when ", "page_idx": 18}, {"type": "equation", "text": "$$\nn\\gtrsim\\frac{B_{\\phi}^{4}}{\\gamma^{2}}\\cdot\\frac{r+\\log\\left(1/\\delta\\right)}{\\left(1-1/c_{S}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.3Proof of Theorem 2.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 2.2. With $\\begin{array}{r}{\\pmb{\\Sigma}^{\\phi}=\\frac{1}{N}\\mathbf{G}^{\\top}\\mathbf{G}}\\end{array}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]=\\mathbb{E}\\left[\\frac{1}{N}\\left\\|\\mathbf{G}\\left(\\pmb{\\theta}_{S}-\\pmb{\\theta}_{\\ast}\\right)\\right\\|_{2}^{2}\\right]=\\mathbb{E}\\left[\\left\\|\\pmb{\\theta}_{S}-\\pmb{\\theta}_{\\ast}\\right\\|_{\\Sigma^{\\phi}}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Observing that by the optimality of $\\theta_{S}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{2}{n}\\mathbf{G}_{S}^{\\top}\\left(\\mathbf{G}_{S}\\pmb{\\theta}_{S}-\\mathbf{y}_{S}\\right)+2\\alpha\\pmb{\\theta}_{S}=\\mathbf{0}_{r}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Recalling that $\\begin{array}{r}{\\Sigma_{S}^{\\phi}:=\\frac{1}{n}\\mathbf{G}_{S}^{\\top}\\mathbf{G}_{S}}\\end{array}$ this implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\theta}_{S}=\\left(\\frac{1}{n}\\mathbf{G}_{S}^{\\top}\\mathbf{G}_{S}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\frac{1}{n}\\mathbf{G}_{S}^{\\top}\\mathbf{y}_{S}}\\\\ &{\\quad=\\!\\frac{1}{n}\\left(\\pmb{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\mathbf{G}_{S}^{\\top}\\left(\\mathbf{G}_{S}\\pmb{\\theta}_{\\ast}+\\mathbf{z}_{S}\\right)}\\\\ &{\\quad=\\!\\left(\\pmb{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\pmb{\\Sigma}_{S}^{\\phi}\\pmb{\\theta}_{\\ast}+\\frac{1}{n}\\left(\\pmb{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\mathbf{G}_{S}^{\\top}\\mathbf{z}_{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, with $\\mathbb{E}_{\\mathbf{z}}\\left[\\mathbf{z}\\right]=\\mathbf{0}_{N}$ $\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]$ can be decomposed the bias term and variance terms as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\mathbb{E}\\mathbf{R}\\left(\\pmb{\\theta}_{S}\\right)\\right]=\\mathbb{E}\\left[\\left\\|\\pmb{\\theta}_{S}-\\pmb{\\theta}_{*}\\right\\|_{\\Sigma^{\\phi}}^{2}\\right]}\\\\ &{=\\!\\mathbb{E}_{\\mathbf{z}}\\left[\\left\\|\\left(\\left(\\pmb{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\pmb{\\Sigma}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\pmb{\\theta}_{*}+\\frac{1}{n}\\left(\\pmb{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\pmb{\\mathbf{G}}_{S}^{\\top}\\pmb{\\mathbf{z}}_{S}\\right\\|_{\\Sigma^{\\phi}}^{2}\\right]}\\\\ &{=\\!\\left\\|\\left(\\left(\\pmb{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\pmb{\\Sigma}_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\pmb{\\theta}_{*}\\right\\|_{\\Sigma_{S}^{\\phi}}^{2}\\!+\\!\\mathbb{E}_{\\mathbf{z}}\\left[\\left\\|\\frac{1}{n}\\left(\\pmb{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\mathbf{G}_{S}^{\\top}\\mathbf{z}_{S}\\right\\|_{\\Sigma^{\\phi}}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\mathbb{E}_{\\mathbf{z}}\\left[\\mathbf{z}_{S}\\mathbf{z}_{S}^{\\top}\\right]\\prec\\sigma^{2}\\mathbf{I}_{n}$ , the variance term can be bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Variance}\\leq\\!\\frac{\\sigma^{2}}{n}\\operatorname{tr}\\left(\\left(\\pmb{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\pmb{\\Sigma}^{\\phi}\\left(\\pmb{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\pmb{\\Sigma}_{S}^{\\phi}\\right)}\\\\ &{\\quad\\quad\\quad\\leq\\!\\frac{\\sigma^{2}}{n}\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\left(\\pmb{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality follows from the fact that $\\left\\|\\left(\\boldsymbol{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\boldsymbol{\\Sigma}_{S}^{\\phi}\\right\\|_{2}\\leq1.$ ", "page_idx": 19}, {"type": "text", "text": "Recall that $\\mathbf{P}_{\\mathcal{S}}\\,\\in\\,\\mathbb{R}^{r\\times r}$ is an orthogonal projector onto any subspace $\\boldsymbol{S}$ of Range $\\left(\\Sigma_{S}^{\\phi}\\right)$ , and $\\mathbf{P}_{S}^{\\perp}\\,=\\,\\mathbf{I}_{r}\\,-\\,\\mathbf{P}_{S}$ is the orthogonal projector onto its orthogonal complement. By observing that $\\Sigma_{S}^{\\bar{\\phi}}+\\alpha\\mathbf{I}_{r}\\succcurlyeq\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}+\\alpha\\mathbf{P}_{S}^{\\perp}$ since Range (Ps) I Range $\\left(\\mathbf{P}_{S}^{\\perp}\\right)$ ,we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\Sigma_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\preccurlyeq\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}+\\frac{1}{\\alpha}\\mathbf{P}_{S}^{\\perp}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Variance}\\leq\\frac{\\sigma^{2}}{n}\\left(\\mathrm{tr}\\left(\\Sigma^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)+\\frac{1}{\\alpha}\\,\\mathrm{tr}\\left(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the bias part, we first observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{I}_{r}-\\left(\\boldsymbol{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\boldsymbol{\\Sigma}_{S}^{\\phi}=\\alpha\\left(\\boldsymbol{\\Sigma}_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Bias}=\\left\\|\\left(\\left(\\Sigma_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\Sigma_{S}^{\\phi}-\\mathbf{I}_{r}\\right)\\theta_{*}\\right\\|_{\\Sigma^{\\phi}}^{2}=\\left\\|\\alpha\\left(\\Sigma_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\theta_{*}\\right\\|_{\\Sigma^{\\phi}}^{2}}\\\\ &{\\qquad=\\alpha^{2}\\operatorname{tr}\\left(\\left(\\Sigma_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\Sigma^{\\phi}\\left(\\Sigma_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-1}\\theta_{*}\\theta_{*}^{\\top}\\right)}\\\\ &{\\qquad\\leq\\alpha^{2}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\left(\\Sigma_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-2}\\right)\\left\\|\\theta_{*}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\left(\\Sigma_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{2}\\succcurlyeq\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}+\\alpha\\mathbf{I}_{r}\\right)^{2}\\succcurlyeq2\\alpha\\cdot\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}+\\alpha^{2}\\mathbf{P}_{S}^{\\perp}$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\Sigma_{S}^{\\phi}+\\alpha\\mathbf{I}_{r}\\right)^{-2}\\prec\\frac{1}{2\\alpha}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}+\\frac{1}{\\alpha^{2}}\\mathbf{P}_{S}^{\\perp},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{Bias}\\leq\\left({\\frac{\\alpha}{2}}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)+\\operatorname{tr}\\left(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\right)\\left\\|\\pmb{\\theta}_{*}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the bias and variance terms, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\leq\\frac{\\sigma^{2}}{n}\\left(\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)+\\frac{1}{\\alpha}\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad+\\left(\\frac{\\alpha}{2}\\operatorname{tr}\\left(\\ \\mathbf{\\Sigma}^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)+\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\right)\\left\\|\\pmb{\\theta}_{*}\\right\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac{\\sigma^{2}}{n}\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)+\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\left\\|\\pmb{\\theta}_{*}\\right\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{1}{\\alpha}\\cdot\\frac{\\sigma^{2}}{n}\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)+\\alpha\\cdot\\frac{\\left\\|\\pmb{\\theta}_{*}\\right\\|_{2}^{2}}{2}\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{\\alpha_{*}}\\cdot\\frac{\\sigma^{2}}{n}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)+\\alpha_{*}\\cdot\\frac{\\left\\Vert\\theta_{*}\\right\\Vert_{2}^{2}}{2}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)}\\\\ &{\\leq2\\sqrt{\\frac{\\sigma^{2}}{n}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\cdot\\frac{\\left\\Vert\\theta_{*}\\right\\Vert_{2}^{2}}{2}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)}}\\\\ &{\\leq\\!\\frac{1}{\\sqrt{2}}\\left(\\frac{\\sigma^{2}}{n}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)+\\operatorname{tr}\\left(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\left\\Vert\\theta_{*}\\right\\Vert_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore overall, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\leq\\!\\frac{2\\sigma^{2}}{n}\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\left(\\mathbf{P}_{S}\\pmb{\\Sigma}_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)+2\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\mathbf{P}_{S}^{\\bot}\\right)\\left\\|\\pmb{\\theta}_{*}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Corollary 2.3. Given $\\mathbf{P}_{S}\\big(c_{S}\\pmb{\\Sigma}_{S}^{\\phi}-\\pmb{\\Sigma}^{\\phi}\\big)\\mathbf{P}_{S}\\succcurlyeq0$ and rank $(\\mathbf{P}_{S})\\asymp\\overline{r}$ , the variance term is asymptotically upper bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathrm{variance}}={\\frac{2\\sigma^{2}}{n}}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)\\lesssim{\\frac{\\sigma^{2}}{n}}\\cdot c s{\\overline{{r}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Meanwhile, given $\\begin{array}{r}{\\mathrm{tr}\\bigl(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\bigr)\\leq\\frac{N}{n}\\,\\mathrm{tr}\\bigl(\\Sigma^{\\phi}-\\langle\\Sigma^{\\phi}\\rangle_{\\overline{{r}}}\\bigr)}\\end{array}$ and $\\mathrm{tr}(\\Sigma^{\\phi}-\\left\\langle\\Sigma^{\\phi}\\right\\rangle_{\\overline{{r}}})\\leq\\mathrm{tr}\\left(\\Sigma^{\\phi}\\right)/N$ the bias term can be asymptotically upper bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathrm{bias}}=2\\operatorname{tr}\\left(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\left\\|\\pmb{\\theta}_{*}\\right\\|_{2}^{2}\\leq{\\frac{2}{n}}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\right)\\left\\|\\pmb{\\theta}_{*}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The result follows from Theorem 2.2 by combining the variance and bias terms. ", "page_idx": 20}, {"type": "text", "text": "C Proofs for Section 3.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Formal Statement and Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem C.1 (Formal version of Theorem 3.1). Under Assumption 2.2 and 2.3 with a small intrinsic dimension $\\overline{r}\\,\\ll\\,\\operatorname*{min}\\left\\{N,r\\right\\}$ for any $\\delta\\ \\in\\ (0,1)$ drawaGaussianrandommatrix $\\textbf{T}\\in\\~\\mathbb{R}^{r\\times m}$ withi.i.d.entriesfrom ${\\mathcal{N}}\\left(0,1/m\\right)$ where $m\\asymp k/\\delta$ for some $k\\ge1.1\\bar{r}$ Let $\\widetilde{\\Sigma}^{\\phi}:=\\mathbf{T}^{\\top}\\Sigma^{\\phi}\\mathbf{T}$ and $\\widetilde{\\Sigma}_{S}^{\\phi}:=\\mathbf{Gamma}^{\\top}\\Sigma_{S}^{\\phi}\\mathbf{I}$ be the sketched gradient moments. For any $S\\subseteq[N]$ with $n>m$ samples such that $(i)$ $)\\ \\mathrm{rank}(\\Sigma_{S}^{\\phi})=n$ and $(i i)$ the $k$ -th largest eigenvalue $s_{k}(\\widetilde\\Sigma_{S}^{\\phi})\\geq\\gamma_{S}$ for some $\\gamma_{S}>0$ with probabilityatleast $1-\\delta$ over $\\mathbf{T}$ there exists $\\alpha>0$ where (2) satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\lesssim\\!\\!\\frac{\\sigma^{2}}{n}\\operatorname{tr}\\left(\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right)}&{{}}&{\\!\\!\\!\\!\\!(\\nu a r i a n c e)}\\\\ {+\\frac{\\sigma^{2}}{n}\\frac{1}{m\\gamma_{S}}\\left\\|\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right\\|_{2}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\right)}&{{}\\left(s k e t c h i n g\\;e r r o r\\right)}\\\\ {+\\frac{1}{n}\\left\\|\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right\\|_{2}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\right)\\|\\pmb{\\theta}_{*}\\|_{2}^{2}}&{{}\\!\\!\\!\\!\\!(b i a s)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "f\uff1a $S$ further satisfies $\\widetilde{\\Sigma}^{\\phi}\\preccurlyeq c_{S}\\widetilde{\\Sigma}_{S}^{\\phi}$ for some $c_{S}\\geq\\frac{n}{N}$ , taking $m=\\operatorname*{max}\\{\\sqrt{\\mathrm{tr}\\left(\\Sigma^{\\phi}\\right)/\\gamma_{S}},1.1\\overline{{r}}/\\delta\\}$ leads to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{ER}\\left(\\theta_{S}\\right)\\right]\\lesssim\\nu a r i a n c e+s k e t c h i n g\\;e r r o r+b i a s\\lesssim\\frac{c_{S}}{n}\\left(\\sigma^{2}m+\\operatorname{tr}\\left(\\Sigma^{\\phi}\\right)\\|\\theta_{*}\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We start by introducing some helpful notations for the proofs. Let $\\mathbf{G}:=\\nabla_{\\theta}f^{\\phi}\\left(\\mathbf{X};\\mathbf{0}_{r}\\right)\\in\\mathbb{R}^{N\\times r}$ and $\\mathbf{G}_{S}=[\\mathbf{G}]_{S}\\in\\mathbb{R}^{n\\times r}$ be the original gradients of $\\mathcal{D}$ and $\\mathcal{D}_{S}$ , respectively. Recall that $\\begin{array}{r}{\\Sigma^{\\phi}=\\mathbf{G}^{\\top}\\mathbf{G}/N}\\end{array}$ and $\\mathbf{\\Sigma}\\Sigma_{S}^{\\phi}=\\mathbf{G}_{S}^{\\top}\\mathbf{G}_{S}/n$ are the corresponding second moments. ", "page_idx": 20}, {"type": "text", "text": "We consider a Johnson-Lindenstrauss transform (JLT) [31] $\\mathbf{T}\\in\\mathbb{R}^{r\\times m}$ asfollows: ", "page_idx": 20}, {"type": "text", "text": "Definition C.1 (JLT [18] (adapting [17, Definition 3])). For any $\\epsilon>0$ $\\delta\\in(0,1)$ ,and $n\\in\\mathbb N$ $a$ random matrix $\\mathbf{T}\\in\\mathbb{R}^{r\\times m}$ .s $a\\left(\\epsilon,\\delta,\\boldsymbol{k}\\right)$ Johnson-Lindenstrauss transform $[\\epsilon,\\delta,k)$ -JLT) if for any $\\mathbf{U}\\in\\mathbb{R}^{r\\times k}$ consisting of $k$ orthonormal columns in $\\mathbb{R}^{r}$ , with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{I}_{k}-\\mathbf{U}^{\\top}\\mathbf{I}\\mathbf{T}^{\\top}\\mathbf{U}\\right\\|_{2}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Definition C.2 (JL second moment property [87] (adapting [17, Definition 12])). For any $\\epsilon>0$ $\\delta\\in(0,1)$ a randommatrix $\\mathbf{T}\\in\\mathbb{R}^{r\\times m}$ satisfiesthe $(\\epsilon,\\delta)$ -JL second moment property $i f$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\left\\|\\Gamma^{\\top}\\mathbf{u}\\right\\|_{2}^{2}-1\\right)^{2}\\right]\\leq\\epsilon^{2}\\delta\\quad\\forall\\,\\mathbf{u}\\in\\mathbb{S}^{r-1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma C.2 (Approximated matrix-matrix multplication [87] (adapting [17, Theorem 13])). Given $\\epsilon>0$ $\\delta\\in(0,1/2)$ , and a random matrix $\\mathbf{T}\\in\\mathbb{R}^{\\bar{r}\\times m}$ satisfying the $(\\epsilon,\\delta)$ -JL second moment property (Definition C.2),for any matrices A, $\\mathbf{B}$ each with $r$ rows, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left[\\left\\|\\mathbf{A}^{\\top}\\mathbf{I}\\mathbf{T}\\mathbf{T}^{\\top}\\mathbf{B}-\\mathbf{A}^{\\top}\\mathbf{B}\\right\\|_{F}>3\\epsilon\\left\\|\\mathbf{A}\\right\\|_{F}\\left\\|\\mathbf{B}\\right\\|_{F}\\right]\\le\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "One of the most classical constructions of a JLT with JL second moment property is the Gaussian embedding: ", "page_idx": 21}, {"type": "text", "text": "Lemma C.3 (Gaussian embedding [17, Theorem 6]). For any. $\\epsilon\\mathrm{~>~0,~}$ $\\delta~\\in~(0,1)$ , a Gaussian random matrix $\\textbf{T}\\in\\ \\mathbb{R}^{r\\times m}$ with i.i.d. entries $\\Gamma_{i j}\\;\\sim\\;\\tilde{\\mathcal{N}}(0,1/m)$ (i) is a $(\\epsilon,\\dot{\\delta},k){-}J L T~i f$ $m\\gtrsim\\left(k+\\log(1/\\delta)\\right)\\epsilon^{-2}$ ;and(ii)satisfiesthe $(\\epsilon,\\delta)$ -JLsecondmomentpropertyif $m\\gtrsim\\epsilon^{-2}\\delta^{-1}$ ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma C.3. The $(\\epsilon,\\delta,k)$ -JLT condition follows directly from [17, Theorem 6]. ", "page_idx": 21}, {"type": "text", "text": "To show the $(\\epsilon,\\delta)$ -JL second moment property, we observe that for any $\\mathbf{u}\\,\\in\\,\\mathbb{S}^{r-1}$ \uff0c $\\left\\|\\mathbf{r}^{\\top}\\mathbf{u}\\right\\|_{2}^{2}=$ $\\mathbf{u}^{\\top}\\mathbf{T}\\mathbf{T}^{\\top}\\mathbf{u}$ is an average of $m$ independent $\\chi^{2}$ random variables with mean 1 and variance 2, we have $\\mathbb{E}\\left[\\left\\|\\mathbf{r}^{\\top}\\mathbf{u}\\right\\|_{2}^{2}\\right]=1$ and its variance is $\\mathbb{E}\\left[\\left(\\left\\|\\mathbf{r}^{\\top}\\mathbf{u}\\right\\|_{2}^{2}-1\\right)^{2}\\right]=2/m$ Therefore, $m\\gtrsim\\epsilon^{-2}\\delta^{-1}$ leads to the $(\\epsilon,\\delta)$ -JL second moment property. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Remark C.1 ((Fast) Johnson-Lindenstrauss transforms). While we mainly focus on the Gaussian embeddingintheanalysisforsimplicity,thereisarichspectrumofJLTswiththeJLsecondmoment property [88, 89, 90, 91], some of which enjoy remarkably better effciency than the Gaussian embedding without compromising accuracy empirically. We refer interested readers to [17, 32, 33] for in-depth reviews on different JLTs and their applications, while briefly synopsizing two common choices andtheirefficiency asfollows. ", "page_idx": 21}, {"type": "text", "text": "(a)Subgaussian embedding $I88J$ is a randommatrix $\\mathbf{T}\\,\\in\\,\\mathbb{R}^{r\\times m}$ with i.i.d. entries from a zeromeansubgaussiandistributionwithvariance $1/m$ CommonchoicesincludetheRademacher distribution and Gaussian distribution (i.e., Gaussian embedding). ", "page_idx": 21}, {"type": "text", "text": "Applying subgaussian embeddings to an $N\\times r$ matrixAwith $\\mathrm{nnz}(\\mathbf{A})\\leq N r$ nonzeroentries takes $O(\\mathrm{nnz}(\\mathbf{A})m)\\leq O(N r m)$ time,while the involved matrix-matrix multiplication can be computed distributedly in parallel leveraging the efficiency of Level 3 BLAS [92]. In practice, generating and applyingRademacher random matrices tend to beslightlyfaster than Gaussian embeddings due to the simple discrete support. ", "page_idx": 21}, {"type": "text", "text": "(b) Sparse sign matrix [89, 93] is a sparse random matrix $\\begin{array}{r}{\\mathbf{T}=\\sqrt{\\frac{r}{\\xi}}\\left[\\gamma_{1},\\cdots,\\gamma_{r}\\right]^{\\top}\\in\\mathbb{R}^{r\\times m}\\left(\\xi\\in\\mathbb{N}\\right)}\\end{array}$ with i.i.d. rows $\\gamma_{j}\\in\\mathbb{R}^{m}$ each consisting of $\\xi$ non-zero entries at uniformly random coordinates filledwithRademacherrandomvariables.When $\\xi=1$ $\\mathbf{T}$ isknownasCountSketch $[94]$ and requires as many as $m=O(k^{2})$ columns to satisfy the JLT property with constant distortion. Increasing the sparsity slightly, [95] showed that $m\\,=\\,O(k\\log k)$ is sufficient for constantdistortion JLT when $\\xi=O(\\log k)$ .In practice, [96] suggested that a small constant sparsity $\\xi\\geq8$ is usually enough for many applications like low-rank approximations. ", "page_idx": 21}, {"type": "text", "text": "The sparse sign matrix can be applied to an $N\\times r$ matrix A with $\\mathrm{nnz}(\\mathbf{A})$ nonzero entries in $O(\\mathrm{nnz}(\\mathbf{A})\\xi)\\leq O(N r\\xi)$ time, independent of the sketching size $m$ With careful implementation, sketching via sparse sign matrices can be significantly faster than the subgaussian embeddings in practice [33, 43]. ", "page_idx": 21}, {"type": "text", "text": "Let $\\widetilde{\\mathbf{G}}:=\\mathbf{G}\\mathbf{D}\\in\\mathbb{R}^{N\\times m}$ and $\\widetilde{\\mathbf{G}}_{S}=\\mathbf{G}_{S}\\mathbf{\\Gamma}\\in\\mathbb{R}^{n\\times m}$ be the sketched gradients such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{\\ensuremath \u1e0a \\mathbf \u1e0a \\Sigma \u1e0c \u1e0c }^{\\phi}:=\\ensuremath{\\mathbf \u1e0a \\widetilde \u1e0a \\mathbf \u1e0a \\phi \u1e0c \u1e0c }^{\\intercal}\\ensuremath{\\mathbf \u1e0a \\phi \u1e0c }^{\\phi}\\ensuremath{\\mathbf \u1e0a \\widetilde \u1e0a \\mathbf \u1e0a \\phi \u1e0c \u1e0c }=\\widetilde{\\ensuremath \u1e0a \\mathbf \u1e0a \\phi \u1e0c \u1e0c }^{\\intercal}\\widetilde{\\ensuremath \u1e0a \\mathbf \u1e0a \\phi \u1e0c \u1e0c }/N\\in\\mathbb{R}^{m\\times m},\\quad\\widetilde{\\ensuremath \u1e0a \\mathbf \u1e0a \\phi \u1e0c \u1e0c }_{S}^{\\phi}:=\\ensuremath{\\mathbf \u1e0a \\widetilde \u1e0a \\phi \u1e0c }^{\\intercal}\\ensuremath{\\mathbf \u1e0a \\phi \u1e0c }_{S}^{\\phi}\\ensuremath{\\mathbf \u1e0a \\widetilde \u1e0a \\mathbf \u1e0a \\phi \u1e0c \u1e0c }=\\widetilde{\\ensuremath \u1e0a \\mathbf \u1e0a \\widetilde \u1e0a \\mathbf \u1e0a \\phi \u1e0c \u1e0c }}\\widetilde{\\ensuremath \u1e0a \\mathbf \u1e0a \\phi \u1e0c \u1e0c }_{S}/n\\in\\mathbb{R}^{m\\times m}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In particular, for a Gaussian embedding $\\mathbf{T}$ \uff0cwhen $\\operatorname{rank}(\\Sigma_{S}^{\\phi})\\;=\\;\\operatorname{rank}(\\mathbf{G}_{S})\\;=\\;n$ $\\mathrm{rank}(\\widetilde{\\Sigma}_{S}^{\\phi})\\;=\\;$ $\\mathrm{rank}(\\widetilde{\\mathbf{G}}_{S})=m$ almost surely. ", "page_idx": 22}, {"type": "text", "text": "Recall the low intrinsic dimension $\\overline{r}$ from Assumption 2.3. For any $k\\in\\mathbb{N}$ with $1.1{\\overline{{r}}}\\leq k<m$ , let $\\mathbf{P}_{S}\\in\\mathbb{R}^{r\\times r}$ be an orthogonal projector onto a dimension- $k$ subspace $\\mathcal{S}\\subseteq\\mathrm{Range}(\\Sigma_{S}^{\\phi})$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{P}_{\\mathcal{S}}:=(\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{G}_{S})^{\\dagger}(\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{G}_{S})=\\mathbf{G}_{S}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{G}_{S},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $\\mathbf{P}_{S}^{\\perp}=\\mathbf{I}_{r}-\\mathbf{P}_{S}$ be its orthogonal complement. Throughout the proof of Theorem C.1, we assume the following: ", "page_idx": 22}, {"type": "text", "text": "Assumption C.1. $L e t\\operatorname*{min}\\left\\{N,r\\right\\}\\gg n>m>k\\geq1.1\\overline{{r}}$ such that $\\mathrm{rank}(\\Sigma_{S}^{\\phi})=n$ We consider $a$ Gaussian embedding (Lemma C.3) $\\mathbf{T}\\in\\mathbb{R}^{r\\times m}$ with $m\\asymp k$ such that $s_{k}(\\widetilde\\Sigma_{S}^{\\phi})\\geq\\gamma_{S}$ for some $\\gamma_{S}>0$ ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorems 3.1 and C.1. We first recall from Theorem 2.2 that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\leq\\frac{2\\sigma^{2}}{n}\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\left(\\mathbf{P}_{S}\\pmb{\\Sigma}_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)+2\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\left\\Vert\\pmb{\\theta}_{\\ast}\\right\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma C.4 suggests that for $m\\asymp k/\\delta$ , with probability at least $1-\\delta/2$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left(\\mathbf{\\Sigma}_{}^{\\phi}\\left(\\mathbf{P}_{S}\\mathbf{\\Sigma}_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)\\lesssim\\mathrm{tr}\\left(\\widetilde{\\mathbf{\\Sigma}}_{}^{\\phi}\\langle\\widetilde{\\mathbf{\\Sigma}}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right)+\\frac{n}{m\\gamma_{S}}\\,\\mathrm{tr}\\left(\\mathbf{\\Sigma}_{}^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\lesssim\\frac{\\sigma^{2}}{n}\\operatorname{tr}\\left(\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right)+\\left(\\frac{\\sigma^{2}}{m\\gamma_{S}}+\\|\\pmb{\\theta}_{*}\\|_{2}^{2}\\right)\\operatorname{tr}\\left(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, applying Lemma C.7 with the union bound, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left.\\left({\\bf\\Sigma}^{\\phi}{\\bf P}_{S}^{\\perp}\\right)\\lesssim\\frac{1}{n}\\left\\|\\widetilde{\\bf{\\Sigma}}^{\\phi}\\langle\\widetilde{\\bf{\\Sigma}}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right\\|_{2}\\mathrm{tr}\\left({\\bf\\Sigma}^{\\phi}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with probability at least $1-\\delta$ . This implies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\lesssim\\!\\!\\frac{\\sigma^{2}}{n}\\left(\\mathrm{tr}\\left(\\widetilde{\\pmb{\\Sigma}}^{\\phi}\\langle\\widetilde{\\pmb{\\Sigma}}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right)+\\frac{1}{m\\gamma_{S}}\\left\\|\\widetilde{\\pmb{\\Sigma}}^{\\phi}\\langle\\widetilde{\\pmb{\\Sigma}}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right\\|_{2}\\mathrm{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{1}{n}\\left\\|\\widetilde{\\pmb{\\Sigma}}^{\\phi}\\langle\\widetilde{\\pmb{\\Sigma}}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right\\|_{2}\\mathrm{tr}\\left({\\pmb{\\Sigma}}^{\\phi}\\right)\\left\\|\\pmb{\\theta}_{*}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$S$ further satisfies $\\widetilde{\\Sigma}^{\\phi}\\preccurlyeq c_{S}\\widetilde{\\Sigma}_{S}^{\\phi}$ for some $c_{S}\\geq\\frac{n}{N}$ , then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left(\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dag}\\right)\\leq\\mathrm{tr}\\left(\\widetilde{\\Sigma}^{\\phi}(\\widetilde{\\Sigma}_{S}^{\\phi})^{\\dag}\\right)\\leq c_{S}m,\\quad\\left\\lVert\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dag}\\right\\rVert_{2}\\leq\\left\\lVert\\widetilde{\\Sigma}^{\\phi}(\\widetilde{\\Sigma}_{S}^{\\phi})^{\\dag}\\right\\rVert_{2}\\leq c_{S}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, (7) can be further simplified as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{ER}\\left(\\pmb{\\theta}_{S}\\right)\\right]\\lesssim\\frac{c_{S}\\sigma^{2}}{n}\\left(m+\\frac{\\mathrm{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\right)}{m\\gamma_{S}}\\right)+\\frac{c_{S}}{n}\\operatorname{tr}\\left(\\pmb{\\Sigma}^{\\phi}\\right)\\left\\|\\pmb{\\theta}_{*}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "On the right-hand-side, the first (variance) term is minimized at $m=\\sqrt{\\mathrm{tr}\\left(\\Sigma^{\\phi}\\right)/\\gamma_{S}}$ where $m+$ $\\operatorname{tr}\\left(\\Sigma^{\\phi}\\right)/(m\\gamma_{S})\\leq2{\\sqrt{\\operatorname{tr}\\left(\\Sigma^{\\phi}\\right)/\\gamma_{S}}}=2m.$ In addition, incorporting the assumption that $m\\asymp k/\\delta$ for some $k\\ge1.1\\bar{r}$ ,we take $m=\\operatorname*{max}\\{\\sqrt{\\mathrm{tr}\\left(\\Sigma^{\\phi}\\right)/\\gamma_{S}},1.1\\overline{{r}}/\\delta\\}$ and get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{ER}\\left(\\theta_{S}\\right)\\right]\\lesssim\\frac{c_{S}\\sigma^{2}}{n}m+\\frac{c_{S}}{n}\\operatorname{tr}\\left(\\Sigma^{\\phi}\\right)\\left\\lVert\\theta_{*}\\right\\rVert_{2}^{2}=\\frac{c_{S}}{n}\\left(\\sigma^{2}m+\\operatorname{tr}\\left(\\Sigma^{\\phi}\\right)\\left\\lVert\\theta_{*}\\right\\rVert_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Theorem 3.1 is simplified from Theorem C.1 by taking $k=\\lceil1.1{\\overline{{r}}}\\rceil$ and $\\delta=0.1$ ", "page_idx": 22}, {"type": "text", "text": "C.2 Upper Bounding Variance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma C.4. For any $\\delta\\,\\in\\,(0,1)$ let $\\mathbf{T}\\,\\in\\,\\mathbb{R}^{r\\times m}$ be a Gaussian embedding (Lemma C.3) with $m\\asymp k/\\delta$ columns. Then, with probability at least $1-\\delta$ over $\\mathbf{T}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left(\\mathbf{\\Sigma}_{}^{\\phi}\\left(\\mathbf{P}_{S}\\mathbf{\\Sigma}_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)\\lesssim\\mathrm{tr}\\left(\\widetilde{\\mathbf{\\Sigma}}_{}^{\\phi}\\langle\\widetilde{\\mathbf{\\Sigma}}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right)+\\frac{n}{m\\gamma_{S}}\\,\\mathrm{tr}\\left(\\mathbf{\\Sigma}_{}^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma C.4. We first observe that since 1 $\\mathrm{rank}(\\Sigma_{S}^{\\phi})=n$ implies ${\\bf G}_{S}{\\bf G}_{S}^{\\dagger}={\\bf I}_{n}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{G}_{S}\\mathbf{P}_{S}\\mathbf{I}=\\mathbf{G}_{S}\\mathbf{G}_{S}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{G}_{S}\\mathbf{I}=\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\widetilde{\\mathbf{G}}_{S}=\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and therefore, $\\begin{array}{r}{{\\bf G}({\\bf G}_{S}{\\bf P}_{S})^{\\dagger}=\\operatorname*{argmin}_{{\\bf Z}\\in\\mathbb{R}^{N\\times n}}~\\|{\\bf G}-{\\bf Z}{\\bf G}_{S}{\\bf P}_{S}\\|_{F}^{2}+\\|{\\bf Z}\\|_{F}^{2}~{\\bf a}}\\end{array}$ nd ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}=\\underset{\\mathbf{Z}\\in\\mathbb{R}^{N\\times n}}{\\mathrm{argmin}}\\,=\\|(\\mathbf{G}-\\mathbf{Z}\\mathbf{G}_{S}\\mathbf{P}_{S})\\mathbf{T}\\|_{F}^{2}+\\|\\mathbf{Z}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is an approximated solution from a sketched least square problem. ", "page_idx": 23}, {"type": "text", "text": "Accuracy of sketched least square residual. For $m\\,\\asymp\\,k/(\\epsilon^{2}\\delta)$ , Lemma C.3 implies that a Gaussian embedding $\\mathbf{T}$ is a $(1/2,\\delta/2,k)$ -JLT (Definition C.1) with $(\\epsilon/\\sqrt{k},\\delta/2)$ -JL second moment property (Definition C.2). Then, since rank $(\\mathbf{G}_{S}\\mathbf{P}_{S})=k$ , by Lemma C.5, with probability at least $1-\\delta$ over $\\mathbf{T}$ \uff0c ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left({\\mathbf{G}}({\\mathbf{G}}_{S}{\\mathbf{P}}_{S})^{\\dagger}-\\widetilde{{\\mathbf{G}}}\\langle\\widetilde{{\\mathbf{G}}}_{S}\\rangle_{k}^{\\dagger}\\right){\\mathbf{G}}_{S}{\\mathbf{P}}_{S}\\right\\|_{F}^{2}\\leq\\epsilon^{2}\\left\\|{\\mathbf{G}}-{\\mathbf{G}}({\\mathbf{G}}_{S}{\\mathbf{P}}_{S})^{\\dagger}({\\mathbf{G}}_{S}{\\mathbf{P}}_{S})\\right\\|_{2}^{2}.}\\\\ &{{\\mathbb{G}}_{S}{\\mathbf{P}}_{S}={\\mathbf{G}}_{S}{\\mathbf{G}}_{S}^{\\dagger}\\langle\\widetilde{{\\mathbf{G}}}_{S}\\rangle_{k}\\langle\\widetilde{{\\mathbf{G}}}_{S}\\rangle_{k}^{\\dagger}{\\mathbf{G}}_{S}=\\langle\\widetilde{{\\mathbf{G}}}_{S}\\rangle_{k}\\langle\\widetilde{{\\mathbf{G}}}_{S}\\rangle_{k}^{\\dagger}{\\mathbf{G}}_{S},}\\\\ &{\\qquad\\qquad\\ ({\\mathbf{G}}_{S}{\\mathbf{P}}_{S})^{\\dagger}({\\mathbf{G}}_{S}{\\mathbf{P}}_{S})={\\mathbf{G}}_{S}^{\\dagger}\\langle\\widetilde{{\\mathbf{G}}}_{S}\\rangle_{k}\\langle\\widetilde{{\\mathbf{G}}}_{S}\\rangle_{k}^{\\dagger}{\\mathbf{G}}_{S}={\\mathbf{P}}_{S}.}\\end{array}\n$$Since ", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\mathbf{G}(\\mathbf{G}_{S}\\mathbf{P}_{S})^{\\dagger}-\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\right)\\mathbf{G}_{S}\\mathbf{P}_{S}\\right\\|_{F}^{2}\\leq\\epsilon^{2}\\left\\|\\mathbf{G}\\mathbf{P}_{\\mathcal{S}}^{\\perp}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Accuracy of sketched least square solution. To upper bound $\\left\\|\\mathbf{G}(\\mathbf{G}_{S}\\mathbf{P}_{S})^{\\dagger}-\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\right\\|_{F}$ we first observe from (10) that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathbf{G}(\\mathbf{G}_{S}\\mathbf{P}_{S})^{\\dagger}-\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\right\\|_{F}^{2}\\leq\\epsilon^{2}\\left\\|(\\mathbf{G}_{S}\\mathbf{P}_{S})^{\\dagger}\\right\\|_{2}^{2}\\left\\|\\mathbf{G}\\mathbf{P}_{S}^{\\perp}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$\\mathbf{G}_{S}\\mathbf{P}_{S}\\mathbf{G}_{S}^{\\top}=\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dag}\\mathbf{G}_{S}\\mathbf{G}_{S}^{\\top}$ . Since rank $({\\bf G}_{S})=n$ , Lemma C.6 implies that for a Gaussian embedding $\\mathbf{T}$ \uff0c $\\begin{array}{r}{\\mathbf{G}_{S}\\mathbf{G}_{S}^{\\top}\\succcurlyeq O\\left(\\frac{m}{n}\\right)\\mathbf{G}_{S}\\mathbf{I}\\mathbf{T}^{\\top}\\mathbf{G}_{S}^{\\top}}\\end{array}$ with high probability. Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{G}_{S}\\mathbf{P}_{S}\\mathbf{G}_{S}^{\\top}\\succ{\\cal O}\\left(\\frac{m}{n}\\right)\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\widetilde{\\mathbf{G}}_{S}\\widetilde{\\mathbf{G}}_{S}^{\\top}={\\cal O}\\left(\\frac{m}{n}\\right)\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\top}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\langle\\widetilde{\\bf{Sigma}}_{S}^{\\phi}\\rangle_{k}=\\frac{1}{n}\\langle\\widetilde{\\bf{G}}_{S}\\rangle_{k}^{\\top}\\langle\\widetilde{\\bf{G}}_{S}\\rangle_{k}}\\end{array}$ and $s_{k}(\\widetilde\\Sigma_{S}^{\\phi})\\geq\\gamma_{S}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left(\\mathbf{G}_{S}\\mathbf{P}_{S}\\right)^{\\dagger}\\right\\|_{2}^{2}=\\left\\|\\left(\\mathbf{G}_{S}\\mathbf{P}_{S}\\mathbf{G}_{S}^{\\top}\\right)^{\\dagger}\\right\\|_{2}\\leq O\\left(\\frac{n}{m}\\right)\\left\\|\\left(\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\top}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\right)^{\\dagger}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\leq O\\left(\\frac{n}{m}\\right)\\frac{1}{n\\gamma_{S}}=O\\left(\\frac{1}{m\\gamma_{S}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, applying a union bound gives that with probability at least $1-\\delta$ over $\\mathbf{T}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{G}(\\mathbf{G}_{S}\\mathbf{P}_{S})^{\\dagger}-\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\right\\|_{F}^{2}\\leq O\\left(\\frac{\\epsilon^{2}}{m\\gamma_{S}}\\right)\\left\\|\\mathbf{G}\\mathbf{P}_{\\mathcal{S}}^{\\perp}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To upper bound $\\left\\|\\mathbf{G}(\\mathbf{G}_{S}\\mathbf{P}_{S})^{\\dagger}\\right\\|_{F}^{2}$ , we observe that by (11), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\bf G}({\\bf G}_{S}{\\bf P}_{S})^{\\dagger}\\right\\|_{F}^{2}\\leq2\\left\\|\\widetilde{\\bf G}\\langle\\widetilde{\\bf G}_{S}\\rangle_{k}^{\\dagger}\\right\\|_{F}^{2}+2\\left\\|{\\bf G}({\\bf G}_{S}{\\bf P}_{S})^{\\dagger}-\\widetilde{\\bf G}\\langle\\widetilde{\\bf G}_{S}\\rangle_{k}^{\\dagger}\\right\\|_{F}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\lesssim\\left\\|\\widetilde{\\bf G}\\langle\\widetilde{\\bf G}_{S}\\rangle_{k}^{\\dagger}\\right\\|_{F}^{2}+\\frac{\\epsilon^{2}}{m\\gamma_{S}}\\left\\|{\\bf G}{\\bf P}_{\\overline{{S}}}^{\\perp}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, normalizing by multiplying $n/N$ on both sides gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left(\\Sigma^{\\phi}\\left(\\mathbf{P}_{S}\\Sigma_{S}^{\\phi}\\mathbf{P}_{S}\\right)^{\\dagger}\\right)\\lesssim\\mathrm{tr}\\left(\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right)+\\epsilon^{2}\\frac{n}{m\\gamma_{S}}\\,\\mathrm{tr}\\left(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking any small constant $\\epsilon>0$ completes the proof. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.5 (Adapting [17, Theorem 23]). For any $\\epsilon\\,>\\,0$ and $\\delta\\,\\in\\,(0,1)$ let $\\mathbf{r}\\,\\in\\,\\mathbb{R}^{r\\times m}\\textit{b a}$ $(1/2,\\delta/2,k)$ -JLT (Defnition C.1) with $(\\epsilon/\\sqrt{k},\\delta/2){-}J L$ second moment property (Definition C.2). Given $\\mathbf{A}\\in\\mathbb{R}^{r\\times n}$ with $\\operatorname{rank}(\\mathbf{A})=k$ and $\\dot{\\mathbf{B}}\\in\\mathbb{R}^{r\\times N}$ ,let ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{W}}=\\underset{\\mathbf{W}}{\\mathrm{argmin}}\\left\\|\\mathbf{r}^{\\top}(\\mathbf{A}\\mathbf{W}-\\mathbf{B})\\right\\|_{F}^{2}+\\left\\|\\mathbf{W}\\right\\|_{F}^{2},\\quad\\mathbf{W}_{*}=\\underset{\\mathbf{W}}{\\mathrm{argmin}}\\left\\|\\mathbf{A}\\mathbf{W}-\\mathbf{b}\\right\\|_{F}^{2}+\\left\\|\\mathbf{W}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then,with probability at least $1-\\delta$ over $\\mathbf{T}$ $\\left\\|\\mathbf{A}(\\widehat{\\mathbf{W}}-\\mathbf{W}_{*})\\right\\|_{F}\\leq\\epsilon\\left\\|\\mathbf{A}\\mathbf{W}_{*}-\\mathbf{B}\\right\\|_{F}.$ ", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma C.5. Analogous to the proof of [17, Theorem 23], let $\\mathbf{A}=\\mathbf{Q}\\mathbf{R}$ be a reduced QR decomposition of A such that $\\mathbf{Q}\\in\\mathbb{R}^{r\\times k}$ is an orthonormal basis for Range(A), and $\\mathbf{R}\\in\\mathbb{R}^{k\\times n}$ Reparametrizing $\\widehat{\\mathbf{Z}}=\\mathbf{R}\\widehat{\\mathbf{W}}$ and $\\mathbf{Z}_{\\ast}=\\mathbf{R}\\mathbf{W}_{\\ast}$ , up to constant scaling of $\\epsilon$ , it is sufficient to show ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{Q}({\\widehat{\\mathbf{Z}}}-\\mathbf{Z}_{*})\\right\\|_{F}=\\left\\|{\\widehat{\\mathbf{Z}}}-\\mathbf{Z}_{*}\\right\\|_{F}\\leq O(\\epsilon)\\left\\|\\mathbf{Q}\\mathbf{Z}_{*}-\\mathbf{B}\\right\\|_{F}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\mathbf{T}\\in\\mathbb{R}^{r\\times k}$ is an $(1/2,\\delta/2,k)$ JLT, we have $\\left\\|\\mathbf{I}_{k}-\\mathbf{Q}^{\\top}\\mathbf{I}\\mathbf{I}^{\\top}\\mathbf{Q}\\right\\|_{2}\\leq1/2$ with probability at least $1-\\delta/2$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widehat{\\mathbf Z}-\\mathbf Z_{*}\\right\\|_{F}\\leq\\left\\|\\mathbf Q^{\\top}\\mathbf T\\mathbf T^{\\top}\\mathbf Q(\\widehat{\\mathbf Z}-\\mathbf Z_{*})\\right\\|_{F}+\\left\\|\\mathbf Q^{\\top}\\mathbf T\\mathbf T^{\\top}\\mathbf Q(\\widehat{\\mathbf Z}-\\mathbf Z_{*})-(\\widehat{\\mathbf Z}-\\mathbf Z_{*})\\right\\|_{F}}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\mathbf Q^{\\top}\\mathbf T\\mathbf T^{\\top}\\mathbf Q(\\widehat{\\mathbf Z}-\\mathbf Z_{*})\\right\\|_{F}+\\left\\|\\mathbf I_{k}-\\mathbf Q^{\\top}\\mathbf T\\mathbf T^{\\top}\\mathbf Q\\right\\|_{2}\\left\\|\\widehat{\\mathbf Z}-\\mathbf Z_{*}\\right\\|_{F}}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\mathbf Q^{\\top}\\mathbf T\\mathbf T^{\\top}\\mathbf Q(\\widehat{\\mathbf Z}-\\mathbf Z_{*})\\right\\|_{F}+1/2\\left\\|\\widehat{\\mathbf Z}-\\mathbf Z_{*}\\right\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which implies $\\begin{array}{r}{\\left\\|\\widehat{\\mathbf{Z}}-\\mathbf{Z}_{*}\\right\\|_{F}^{2}\\leq2\\left\\|\\mathbf{Q}^{\\top}\\mathbf{I}\\mathbf{T}^{\\top}\\mathbf{Q}(\\widehat{\\mathbf{Z}}-\\mathbf{Z}_{*})\\right\\|_{F}^{2}}\\end{array}$ with probability a least $1-\\delta/2$ ", "page_idx": 24}, {"type": "text", "text": "By the normal equation of the sketched least square problem, $\\mathbf{Q}^{\\top}\\mathbf{I}\\mathbf{I}^{\\top}\\mathbf{Q}\\widehat{\\mathbb{Z}}=\\mathbf{Q}^{\\top}\\mathbf{I}\\mathbf{I}^{\\top}\\mathbf{B}$ Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{\\mathbf{Z}}-\\mathbf{Z}_{*}\\right\\|_{F}^{2}\\leq2\\left\\|\\mathbf{Q}^{\\top}\\mathbf{I}\\mathbf{T}^{\\top}(\\mathbf{Q}\\mathbf{Z}_{*}-\\mathbf{B})\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\mathbf{Q}^{\\top}(\\mathbf{QZ}_{*}-\\mathbf{B})=-\\mathbf{Q}^{\\top}\\left(\\mathbf{I}_{r}-\\mathbf{QQ}^{\\top}\\right)\\mathbf{B}=\\mathbf{0}_{k\\times N}$ and $\\mathbf{T}$ has $(\\epsilon/\\sqrt{k},\\delta/2)$ JL second moment property, Lemma C.2 implies that with probability at least $1-\\delta/2$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{Q}^{\\top}\\mathbf{T}\\mathbf{T}^{\\top}(\\mathbf{Q}\\mathbf{Z}_{*}-\\mathbf{B})\\right\\|_{F}^{2}\\leq O\\left(\\epsilon^{2}/k\\right)\\left\\|\\mathbf{Q}\\right\\|_{F}^{2}\\left\\|\\mathbf{Q}\\mathbf{Z}_{*}-\\mathbf{B}\\right\\|_{F}^{2}=O(\\epsilon^{2})\\left\\|\\mathbf{Q}\\mathbf{Z}_{*}-\\mathbf{B}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, by the union bound, with probability at least $1-\\delta$ over $\\mathbf{T}$ wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{\\mathbf{Z}}-\\mathbf{Z}_{*}\\right\\|_{F}^{2}\\leq2\\left\\|\\mathbf{Q}^{\\top}\\mathbf{\\Gamma}\\mathbf{\\Gamma}^{\\top}(\\mathbf{Q}\\mathbf{Z}_{*}-\\mathbf{B})\\right\\|_{F}^{2}\\leq O(\\epsilon^{2})\\left\\|\\mathbf{Q}\\mathbf{Z}_{*}-\\mathbf{B}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma C.6 ([97]). For a random matrix $\\Omega\\,\\in\\,\\mathbb{R}^{n\\times m}$ $\\left.n\\,>\\,m\\right)$ consisting of i.i.d. subgaussian entries with mean zero and variance one, with high probability, ", "page_idx": 24}, {"type": "equation", "text": "$$\nO\\left(\\left(\\sqrt{n}-\\sqrt{m}\\right)^{2}\\right)\\mathbf{I}_{n}\\preccurlyeq\\Omega\\boldsymbol{\\Omega}^{\\top}\\preccurlyeq O\\left(\\left(\\sqrt{n}+\\sqrt{m}\\right)^{2}\\right)\\mathbf{I}_{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C.3 Upper Bounding Low-rank Approximation Error ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma C.7. Under Assumption 2.3, let $\\mathbf{T}\\in\\mathbb{R}^{r\\times m}$ be a Gaussian embedding (Lemma C.3) such that there exists $m>k\\geq1.1\\bar{r}$ satisfying $s_{k}(\\widetilde\\Sigma_{S}^{\\phi})\\geq\\gamma_{S}$ for some $\\gamma_{S}>0$ . Then, with probability at least $1-\\exp\\left(-\\Omega(\\overline{{r}})\\right)$ \uff0c ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left({\\Sigma}^{\\phi}{\\bf P}_{S}^{\\perp}\\right)\\lesssim\\frac{1}{n}\\left\\|\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right\\|_{2}\\mathrm{tr}\\left({\\Sigma}^{\\phi}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma C.7. Here we follow a similar proof strategy as [43, Theorem 1]. Let $\\mathbf{\\Pi}_{\\Pi_{S}}\\;:=\\;$ $[\\mathbf{I}_{N}]_{S}^{\\ \u3001}\\in\\mathbb{R}^{n\\times N}$ be the selection matrix associated with $S\\subseteq[N]$ We introducethefollowing $N\\times N$ oblique projectors: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{M}_{S}:=\\mathbf{G}\\mathbf{G}_{S}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{I}\\mathbf{I}_{S},\\quad\\widetilde{\\mathbf{M}}_{S}:=\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{I}\\mathbf{I}_{S}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In particular, $\\mathbf{M}_{S}$ and $\\widetilde{\\mathbf{M}}_{S}$ are the oblique projectors since with $\\mathbf{I}_{S}\\mathbf{G}=\\mathbf{G}_{S}$ and ${\\bf G}_{S}{\\bf G}_{S}^{\\dagger}={\\bf I}_{n}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{M}_{S}^{2}=\\mathbf{G}\\mathbf{G}_{S}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{G}_{S}\\mathbf{G}_{S}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{I}\\mathbf{I}_{S}}\\\\ {=\\mathbf{G}\\mathbf{G}_{S}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{I}\\mathbf{I}_{S}=\\mathbf{M}_{S},\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and with $\\mathbf{\\Pi}\\mathbf{I}_{S}\\tilde{\\mathbf{G}}=\\tilde{\\mathbf{G}}_{S}$ \uff0c ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{M}}_{S}^{2}=\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\widetilde{\\mathbf{G}}_{S}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{I}\\mathbf{I}_{S}=\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{I}\\mathbf{I}_{S}=\\widetilde{\\mathbf{M}}_{S}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Recalling $\\mathbf{P}_{\\mathcal{S}}$ from (9), we observe the following identities: ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\bf M}_{S}{\\bf G}={\\bf G}{\\bf G}_{S}^{\\dagger}\\langle\\widetilde{{\\bf G}}_{S}\\rangle_{k}\\langle\\widetilde{{\\bf G}}_{S}\\rangle_{k}^{\\dagger}{\\bf G}_{S}={\\bf G}{\\bf P}_{S};\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "since ${\\bf G}_{S}{\\bf G}_{S}^{\\dagger}={\\bf I}_{n}$ \uff0c", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{M}}_{S}\\mathbf{M}_{S}=\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{G}_{S}\\mathbf{G}_{S}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{I}\\mathbf{I}_{S}=\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{I}\\mathbf{I}_{S}=\\widetilde{\\mathbf{M}}_{S};\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{M}}_{S}\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}=\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\widetilde{\\mathbf{G}}_{S}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}=\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining (12), (13), and (14), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{G}\\mathbf{P}_{S}=\\mathbf{G}-\\mathbf{G}\\mathbf{P}_{S}=\\left(\\mathbf{I}_{N}-\\mathbf{M}_{S}\\right)\\mathbf{G}}&{\\,\\left(\\mathbf{b}\\mathbf{y}\\left(12\\right)\\right)}\\\\ {=\\left(\\mathbf{I}_{N}-\\widetilde{\\mathbf{M}}_{S}\\right)\\left(\\mathbf{I}_{N}-\\mathbf{M}_{S}\\right)\\mathbf{G}}&{\\,\\left(\\mathbf{b}\\mathbf{y}\\left(13\\right)\\right)}\\\\ {=\\left(\\mathbf{I}_{N}-\\widetilde{\\mathbf{M}}_{S}\\right)\\mathbf{G}\\mathbf{P}_{S}^{\\perp}}&{\\,\\left(\\mathbf{b}\\mathbf{y}\\left(12\\right)\\right)}\\\\ {=\\left(\\mathbf{I}_{N}-\\widetilde{\\mathbf{M}}_{S}\\right)\\left(\\mathbf{I}_{N}-\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\widetilde{\\mathbf{G}}^{\\dagger}\\right)\\mathbf{G}\\mathbf{P}_{S}^{\\perp}}&{\\,\\left(\\mathbf{b}\\mathbf{y}\\left(14\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\left\\|\\mathbf{P}_{S}^{\\perp}\\right\\|_{2}^{2}=1$ , this implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}\\left(\\Sigma^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)=\\!\\!\\frac{1}{N}\\left\\|\\mathbf{G}\\mathbf{P}_{S}\\right\\|_{F}^{2}=\\frac{1}{N}\\left\\|\\left(\\mathbf{I}_{N}-\\widetilde{\\mathbf{M}}_{S}\\right)\\left(\\mathbf{I}_{N}-\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\widetilde{\\mathbf{G}}^{\\dagger}\\right)\\mathbf{G}\\mathbf{P}_{S}^{\\dagger}\\right\\|_{F}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\!\\!\\frac{1}{N}\\left\\|\\mathbf{I}_{N}-\\widetilde{\\mathbf{M}}_{S}\\right\\|_{2}^{2}\\left\\|\\left(\\mathbf{I}_{N}-\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\widetilde{\\mathbf{G}}^{\\dagger}\\right)\\mathbf{G}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By the operator norm identity for projectors [98], we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{I}_{N}-\\widetilde{\\mathbf{M}}_{S}\\right\\|_{2}^{2}=\\left\\|\\widetilde{\\mathbf{M}}_{S}\\right\\|_{2}^{2}=\\left\\|\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\mathbf{I}_{S}\\right\\|_{2}^{2}=\\left\\|\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\right\\|_{2}^{2}=\\frac{N}{n}\\left\\|\\widetilde{\\mathbf{\\Sigma}}^{\\phi}\\langle\\widetilde{\\mathbf{\\Sigma}}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right\\|_{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and therefore, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left({\\bf D}^{\\phi}{\\bf P}_{S}^{\\perp}\\right)\\leq\\frac{1}{n}\\left\\|\\widetilde{\\bf D}^{\\phi}\\langle\\widetilde{\\bf D}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right\\|_{2}\\left\\|\\left({\\bf I}_{N}-\\widetilde{\\bf G}\\langle\\widetilde{\\bf G}_{S}\\rangle_{k}^{\\dagger}\\langle\\widetilde{\\bf G}_{S}\\rangle_{k}\\widetilde{\\bf G}^{\\dagger}\\right){\\bf G}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\widetilde{\\mathbf{G}}^{\\dagger}$ is a rank- $k$ orthogonal projector onto ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname{Range}\\left(\\widetilde\\mathbf{G}\\langle\\widetilde\\mathbf{G}_{S}\\rangle_{k}^{\\dagger}\\right)=\\operatorname{Range}\\left(\\mathbf{G}\\left(\\Gamma\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and Gaussian embeddings are rotationally invariant, $\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\widetilde{\\mathbf{G}}^{\\dagger}$ shares the same distribution $(\\mathbf{G}\\Omega)(\\mathbf{G}\\Omega)^{\\dag}$ for a $r\\times k$ Gaussian embedding $\\Omega$ with $\\left[\\Omega\\right]_{i,j}\\sim\\mathcal{N}(0,1/k)$ i.i.d.. Then, we observe that $\\|(\\mathbf{I}_{N}-\\widetilde{\\mathbf{G}}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}^{\\dagger}\\langle\\widetilde{\\mathbf{G}}_{S}\\rangle_{k}\\widetilde{\\mathbf{G}}^{\\dagger})\\mathbf{G}\\|_{F}^{2}$ is the rank- $k$ randomized range-finder error of $\\mathbf{G}$ , which can be controlled according to Lemma C.8: with probability at least $1-\\exp\\left(-\\Omega(\\overline{{r}})\\right)$ \uff0c ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left({\\Sigma}^{\\phi}\\mathbf{P}_{S}^{\\perp}\\right)\\lesssim\\frac{1}{n}\\left\\|\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dag}\\right\\|_{2}\\left\\|\\mathbf{G}-\\langle\\mathbf{G}\\rangle_{\\overline{{r}}}\\right\\|_{F}^{2}=\\frac{N}{n}\\left\\|\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dag}\\right\\|_{2}\\mathrm{tr}\\left({\\Sigma}^{\\phi}-\\langle\\mathbf{D}^{\\phi}\\rangle_{\\overline{{r}}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By the definition of $\\overline{r}$ in Assumption 2.3, $\\mathrm{tr}\\left(\\Sigma^{\\phi}-\\left\\langle\\Sigma^{\\phi}\\right\\rangle_{\\overline{{r}}}\\right)\\leq\\mathrm{tr}\\left(\\Sigma^{\\phi}\\right)/N$ and thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left({\\Sigma}^{\\phi}{\\bf P}_{S}^{\\perp}\\right)\\lesssim\\frac{1}{n}\\left\\|\\widetilde{\\Sigma}^{\\phi}\\langle\\widetilde{\\Sigma}_{S}^{\\phi}\\rangle_{k}^{\\dagger}\\right\\|_{2}\\mathrm{tr}\\left({\\Sigma}^{\\phi}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma C.8 (Randomized range-finder error (simplifying [32, Theorem 10.7])). Let $\\Omega\\in\\mathbb{R}^{r\\times k}$ be a Gaussian embedding with $[\\bar{\\Omega}]_{i,j}\\sim\\mathcal{N}(0,1/k)$ i.i.d.. For any $\\mathbf{G}\\,\\in\\,\\mathbb{R}^{N\\times r}$ and $\\overline{{r}}\\,\\in\\,\\mathbb{N}$ such that $1.1\\overline{{r}}\\leq k\\ll\\operatorname*{min}\\left\\{N,r\\right\\}\\!,$ withprobability at least $1-\\exp\\left(-\\Omega(\\overline{{r}})\\right)$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\mathbf{I}_{N}-(\\mathbf{G}\\boldsymbol{\\Omega})(\\mathbf{G}\\boldsymbol{\\Omega})^{\\dagger}\\right)\\mathbf{G}\\right\\|_{F}\\lesssim\\left\\|\\mathbf{G}-\\langle\\mathbf{G}\\rangle_{\\overline{{r}}}\\right\\|_{F}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D Experiment Details for Section 4.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1  Implementation Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Synthetic data generation. We consider a set of $N\\;=\\;2000$ samples with high-dimensional pre-trained representations $\\phi(\\mathbf{X})~\\in~\\mathbb{R}^{N\\times r}$ where $r~=~2400$ . modeled by a Gaussian mixture model (GMM) consisting of $\\overline{{r}}=8$ well-separated clusters, each with random sizes and variances. Specifically, we generate the GMM dataset as follows: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 Randomly partition the $N$ samples into $\\overline{{r}}=8$ clusters with sizes $\\{N_{j}\\mid j\\in[\\overline{{r}}]\\}$ \u00b7 For each $j\\in[\\overline{{r}}]$ , generate the cluster mean $\\pmb{\\mu}_{j}\\in\\mathbb{R}^{r}$ with $\\pmb{\\mu}_{j}=(Z_{j}\\overline{{r}})\\cdot\\mathbf{e}_{j}$ where $Z_{j}\\sim\\mathrm{Unif}([\\overline{{r}}])$ and variance $\\dot{\\sigma_{j}}=Z_{j}^{\\prime}\\cdot\\sigma_{\\mathrm{max}}$ where $Z_{j}^{\\prime}\\sim\\mathrm{Unif}([0,1])$ and $\\sigma_{\\mathrm{max}}=0.04$ \u00b7 Generate representations $\\left\\{\\phi(\\mathbf{x}_{i})\\sim\\mathcal{N}(\\pmb{\\mu}_{j},\\sigma_{j}^{2}\\mathbf{I}_{r})\\ \\middle|\\ i\\in[N_{j}]\\right\\}$ i.i.d. for each cluster $j\\in[\\overline{{r}}]$ \u00b7 Draw a latent label generator $\\pmb{\\theta}_{g}\\sim\\mathcal{N}(\\mathbf{0}_{r},\\mathbf{I}_{r})$ . For each cluster $j\\,\\in\\,[\\overline{{r}}]$ , assign the same label $y_{i}=\\pmb{\\mu}_{j}^{\\top}\\pmb{\\theta}_{g}$ for all samples $i\\in[N_{j}]$ within the cluster. ", "page_idx": 26}, {"type": "text", "text": "Ridge regression. We solve the ridge regression problem over the selected coreset $\\mathcal{D}_{S}$ of $n$ samples and tune the regularization hyperparameter $\\alpha$ via grid search over 100 linearly spaced values in $[10^{-2},10^{2}]$ with 2-fold cross-validation. ", "page_idx": 26}, {"type": "text", "text": "D.2 Baselines ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We compare SkMM to the following unsupervised data selection methods for regression: ", "page_idx": 26}, {"type": "text", "text": "(a) Uniform sampling (Uni form) selects $n$ samples uniformly at random from the full dataset $\\mathcal{D}$ (b) Herding [51, 52] (He rdi ng) selects data greedily to minimize the distance between the centers of thecoreset $\\mathcal{D}_{S}$ and the original dataset $\\mathcal{D}$ . Notice that although herding aims to reduce the \"bias\"\u2019' of the coreset center, it fails to control our notion of bias in the low-rank approximation sense. Given the construction of the GMM dataset, herding has more emphasis on variance reduction, as illustrated in Figure 2. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "(c) K-center greedy [53] (K-center) provides a greedy heuristic for the minimax facility location problem that aims to minimize the maximum distance between any non-coreset sample and the nearestcoresetsample. ", "page_idx": 26}, {"type": "text", "text": "(d) Adaptive sampling [44, 56] (Adapt ive) iteratively samples data based on their squared norms and adaptively updates the distribution by eliminating the spanning subspace of the selected samples from the dataset. It is proved in the recent work [44] that adaptive sampling achieves nearly optimal sample complexity for low-rank approximations, matching that of volume sampling [39, 41] (with the best know theoretical guarantee) up to a logarithmic factor. ", "page_idx": 26}, {"type": "text", "text": "In practice, adaptive sampling generally achieves comparable accuracy to volume sampling for low-rank approximations, with considerably better efficiency [44, 45]. Due to the prohibitive cost of volume sampling in high dimensions, we choose adaptive sampling in the comparison. ", "page_idx": 27}, {"type": "text", "text": "(e) Truncated [18, 72] and ridge leverage score sampling [19, 73, 74] (T /R-1eve rage) are the extensions of classical leverage score sampling [54] to high dimensions. In particular, leverage score sampling is originally designed for low-dimensional linear regression, while degenerating to uniform sampling in high dimensions. Consider the high-dimensional representations $\\phi(\\mathbf{X})\\in$ $\\mathbb{R}^{N\\times r}\\left(r>N\\right)$ in our setting,for each $i\\in[N]$ ", "page_idx": 27}, {"type": "text", "text": "\u00b7 leverage score: $l_{i}:=\\phi(\\mathbf{x}_{i})^{\\top}(\\phi(\\mathbf{X})^{\\top}\\phi(\\mathbf{X}))^{\\dagger}\\phi(\\mathbf{x}_{i}),$   \n\u00b7 truncated leverage score: $l_{i}^{(m)}:=\\phi(\\ensuremath{\\mathbf{x}}_{i})^{\\top}(\\langle\\phi(\\ensuremath{\\mathbf{X}})\\rangle_{m}^{\\top}\\langle\\phi(\\ensuremath{\\mathbf{X}})\\rangle_{m})^{\\dagger}\\phi(\\ensuremath{\\mathbf{x}}_{i})$ for a given truncation rank $m$ , and   \n\u00b7 ridge leverage score: $l_{i}^{(\\rho)}:=\\phi(\\mathbf{x}_{i})^{\\top}(\\phi(\\mathbf{X})^{\\top}\\phi(\\mathbf{X})+\\rho\\mathbf{I}_{r})^{\\dagger}\\phi(\\mathbf{x}_{i})$ for a given regularization parameter $\\rho>0$ . Larger $\\rho$ brings ridge leverage score sampling closer to uniform sampling.   \nTherefore, both truncated and ridge leverage score sampling balance the variance-bias tradeoff   \nby adjusting the truncation rank $m$ and regularization parameter $\\rho$ , respectively. ", "page_idx": 27}, {"type": "text", "text": "Baseline details. For both Herding and K-center, we adopt the DeepCore implementation [1]. Notice that Herding is a deterministic algorithm. For Adapt ive, we use the implementation from [45]. For T-leverage, we use a rank- $m$ truncated SVD to compute the leverage scores, With $m=4\\overline{{r}}=32$ as in SkMM (i.e., providing both methods approximately the same amount of information and compute). For R-leverage, we choose $\\rho=10^{\\bar{3}}$ ", "page_idx": 27}, {"type": "text", "text": "E  Additional Experiments and Details for Section 4.3 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Table 4: Accuracy and F1 score $(\\%)$ of FT over (the last two layers of) ResNet18 on StanfordCars ", "page_idx": 27}, {"type": "table", "img_path": "yAAQWBMGiT/tmp/01be3a0f6368630692c35ad08ca4dd610a50969b7c0597f5f5e70290d90b485c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Implementation details. For CLIP standard transform, we transform the image size to 224, with normalization mean (0.48145466, 0.4578275, 0.40821073) and std (0.26862954, 0.26130258, 0.27577711). ", "page_idx": 27}, {"type": "text", "text": "Parameter Count.  We show the parameter sizes for the two-layer fine-tuning experiments in ??. Therepresentation dimension $d$ is 512 for ResNet18, the number of classes $K$ is 10for CIFAR-10 and 196 for Stanford Cars. The last layer parameter size is 5130 for CIFAR-10 and 100548 for Stanford Cars. The second but last layer parameter size is 2364426 for CIFAR-10. When we do the last two layers fine-tuning, the total parameter size is 8398858 for CIFAR-10 and 2459844 for Stanford Cars. ", "page_idx": 27}, {"type": "table", "img_path": "yAAQWBMGiT/tmp/53b7aa66e849a1371ea4577e24c1be1911ecd0fa1ce441c0c418990d6487b6a5.jpg", "table_caption": ["Table 5: Classification accuracy $(\\%)$ of LP over CLIP on CIFAR-10. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "yAAQWBMGiT/tmp/63dc0f7cf0dd9f4cccb3472f2360b431c6b074004785a15dedf9e4567d568949.jpg", "table_caption": ["Table 6: Accuracy of FT over ResNet18 on CIFAR-10. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "StanfordCars Baselines. \u03b2 For DeepCore baselines, there are some methods that require warmup training before the finetuning (e.g. Uncertainty-Entropy), we use one-layer training and two-layer training (freezing other layers) in the warmup training for linear probing selection and two-layer finetuning selection. The warmup training is done with Adam optimizer with learning rate 0.01 for 10 epochs. ", "page_idx": 28}, {"type": "text", "text": "Finetuning Details We finetune the model for 50 epochs for both linear probing and finetuning using Adam optimizer with a learning rate 0.01. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We summarized our contributions at the end of the introduction (Section 1). ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We discussed limitations and future directions in Section 5. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n- Ih:la tha authaua miaht faou that aamnlata hanaat, aha.t limitatiaua miaht ha .aad h., uariawa ", "page_idx": 29}, {"type": "text", "text": "as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The assumptions are clearly stated in place with the theoretical results, while the proofs are deferred to the appendices, with clear hyperlink references. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide clear pseudocode and experiment setups in the main text, as well as implementation details in the appendices. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 30}, {"type": "text", "text": "(d)  We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our experiment code for both the synthetic and real data is available at https://anonymous.4open.science/r/data_pruning. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see   the . NeurIPs. code_  and. data. submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results.  See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We specify the experiments setting in the main context (subsection 4.1 ,subsection 4.3) and appendix (subsection D.1, subsection D.2) opensourced our code and scripts at https://anonymous.4open.science/r/data_pruning. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We reported the stand error with 5 seeds. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: All the experiments could be done with A40 or even smaller GPUs. We use 4 workers and 32 GB Memory. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Yes, we follow the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This is a learning theory paper with no societal impact. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This is a learning theory paper with no such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We use the open-sourced dataset (CIFAR10) and models (CLIP), they are open for research usage. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: https://anonymous.4open.science/r/data_pruning. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]