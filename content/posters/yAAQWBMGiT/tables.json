[{"figure_path": "yAAQWBMGiT/tables/tables_7_1.jpg", "caption": "Table 1: Empirical risk LD(\u03b8s) on the GMM dataset at various n, under the same hyperparameter tuning where ridge regression over the full dataset D with N = 2000 samples achieves LD(\u03b8[N]) = 2.95e-3. For methods involving sampling, results are reported over 8 random seeds.", "description": "This table presents the empirical risk (LD(\u03b8s)) achieved by different data selection methods on a Gaussian Mixture Model (GMM) dataset for various sample sizes (n).  It compares the performance of SkMM against baselines like uniform sampling, Herding, K-center, adaptive sampling, and leverage score sampling. The table highlights the impact of different data selection strategies on generalization performance, showing how SkMM balances the variance-bias tradeoff effectively.", "section": "4.1 Synthetic High-dimensional Linear Probing"}, {"figure_path": "yAAQWBMGiT/tables/tables_8_1.jpg", "caption": "Table 1: Empirical risk LD(\u03b8s) on the GMM dataset at various n, under the same hyperparameter tuning where ridge regression over the full dataset D with N = 2000 samples achieves LD(\u03b8[N]) = 2.95e-3. For methods involving sampling, results are reported over 8 random seeds.", "description": "This table presents the empirical risk (LD(\u03b8s)) for different data selection methods on a Gaussian Mixture Model (GMM) dataset with varying sample sizes (n).  The risk is measured using ridge regression, and the results are compared to the risk achieved with the full dataset (LD(\u03b8[N]) = 2.95e-3).  The table shows how different methods, including uniform sampling, k-center, adaptive sampling, leverage score sampling, and the proposed SkMM method, perform in terms of risk reduction at different sample sizes.  The standard deviation across 8 random seeds is reported for the sampling methods.", "section": "4.1 Synthetic High-dimensional Linear Probing"}, {"figure_path": "yAAQWBMGiT/tables/tables_8_2.jpg", "caption": "Table 2: Mean Absolute Error (the lower the better) on UTKFace with a linear regressor trained on top of frozen features from a pre-trained CLIP (ViT-B/32). We use the bold font to indicate the best method for each coreset size.", "description": "This table presents the Mean Absolute Error (MAE) results on the UTKFace dataset for age estimation.  Different data selection methods were compared using a linear regressor trained on top of CLIP's pre-trained features (ViT-B/32).  The MAE is a measure of the average absolute difference between the predicted and actual ages. Lower MAE values indicate better performance. The table shows MAE values for various coreset sizes (number of selected data points), demonstrating the effectiveness of each data selection method at different data scales.", "section": "4.2 Experiments on Regression Tasks"}, {"figure_path": "yAAQWBMGiT/tables/tables_9_1.jpg", "caption": "Table 3: Accuracy and F1 score (%) of LP over CLIP on StanfordCars", "description": "This table presents the accuracy and F1 scores achieved by different data selection methods on the Stanford Cars dataset using linear probing over CLIP features.  The results show the performance of various methods including uniform sampling, herding, contextual diversity, Glister, GraNd, forgetting, DeepFool, entropy, margin, least confidence, and the proposed SkMM method. The performance is evaluated for different subset sizes (n) of the dataset.", "section": "4.3 Experiments on Image Classification Tasks"}, {"figure_path": "yAAQWBMGiT/tables/tables_27_1.jpg", "caption": "Table 3: Accuracy and F1 score (%) of LP over CLIP on StanfordCars", "description": "This table presents the accuracy and F1 scores achieved by different data selection methods for linear probing (LP) on the Stanford Cars dataset using CLIP.  The results are shown for various coreset sizes (n), demonstrating the performance of SkMM in comparison to other methods like Uniform Sampling, Herding, Contextual Diversity, Glister, GraNd, Forgetting, DeepFool, Entropy, Margin, and Least Confidence.", "section": "4.3 Experiments on Image Classification Tasks"}, {"figure_path": "yAAQWBMGiT/tables/tables_28_1.jpg", "caption": "Table 3: Accuracy and F1 score (%) of LP over CLIP on StanfordCars", "description": "This table presents the results of linear probing (LP) experiments on the Stanford Cars dataset using CLIP.  It compares the accuracy and F1 score achieved by SkMM-LP against several baseline data selection methods (Uniform Sampling, Herding, Contextual Diversity, Glister, GraNd, Forgetting, DeepFool, Entropy, Margin, Least Confidence) for different coreset sizes (n).  The results highlight the performance of SkMM-LP, particularly in lower coreset sizes.", "section": "4.3 Experiments on Image Classification Tasks"}, {"figure_path": "yAAQWBMGiT/tables/tables_28_2.jpg", "caption": "Table 2: Mean Absolute Error (the lower the better) on UTKFace with a linear regressor trained on top of frozen features from a pre-trained CLIP (ViT-B/32). We use the bold font to indicate the best method for each coreset size.", "description": "This table presents the Mean Absolute Error (MAE) results for age estimation on the UTKFace dataset using a linear regressor on top of CLIP features.  Different data selection methods are compared at various coreset sizes (the number of data points used for training the regressor). Lower MAE values indicate better performance. The best performing method for each coreset size is highlighted in bold.", "section": "4.2 Experiments on Regression Tasks"}]