[{"figure_path": "dxwIaCVkWU/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Classical PC learns a mean-field approximate posterior with prediction error layers. Right: Divide-and-conquer PC approximates the joint posterior with bottom-up and recurrent errors. Where classical predictive coding has layers communicate through shared error units, divide-and-conquer predictive coding separates recurrent from \u201cbottom-up\u201d error pathways to target complete conditional distributions rather than posterior marginal distributions.", "description": "This figure compares classical predictive coding (PC) with the proposed divide-and-conquer predictive coding (DCPC).  Classical PC uses a mean-field approximation, where each layer's posterior is represented by its mean and variance.  The layers communicate prediction errors through shared units. In contrast, DCPC models the joint posterior with separate recurrent and bottom-up error pathways to provide a more complete representation. This allows DCPC to target complete conditional distributions, rather than just the marginal distributions of individual variables.", "section": "Introduction"}, {"figure_path": "dxwIaCVkWU/figures/figures_6_1.jpg", "caption": "Figure 2: Hierarchical graphical model for DLGM's.", "description": "This figure shows the hierarchical graphical model used for the Deep Latent Gaussian Models (DLGMs) experiments in the paper.  The model consists of two latent variables, z1 and z2, and an observation variable x.  Arrows indicate the directional dependencies in the model, where z1 influences z2, and z2 influences x.  The parameters \u03b8 of the model are also shown, influencing both z2 and x.  This structure highlights the hierarchical dependencies that the Divide-and-Conquer Predictive Coding (DCPC) algorithm is designed to handle effectively.", "section": "Deep latent Gaussian models with predictive coding"}, {"figure_path": "dxwIaCVkWU/figures/figures_6_2.jpg", "caption": "Figure 3: Top: images from validation sets of MNIST (left), EMNIST (middle), and Fashion MNIST (right). Bottom: reconstructions by deep latent Gaussian models trained with DCPC for MNIST (left), EMNIST (middle), and Fashion MNIST (right), averaging over K = 4 particles. DCPC achieves quality reconstructions by inference over z without training an inference network.", "description": "This figure compares the original images from validation sets of MNIST, EMNIST, and Fashion MNIST datasets with their reconstructions generated by deep latent Gaussian models trained using the proposed Divide-and-Conquer Predictive Coding (DCPC) algorithm.  The key takeaway is that DCPC produces high-quality reconstructions using only inference over latent variables (z), without requiring the training of a separate inference network, showcasing its efficiency and effectiveness.", "section": "Experiments"}, {"figure_path": "dxwIaCVkWU/figures/figures_7_1.jpg", "caption": "Figure 4: Left: reconstructions from the CelebA validation set. Right: samples from the generative model. DCPC achieves quality reconstructions by inference over z with K = 16 particles and no inference network, while the learned generative model captures variation in the data.", "description": "This figure shows the results of applying the Divide-and-Conquer Predictive Coding (DCPC) algorithm to the CelebA dataset. The left panel displays reconstructions of images from the validation set, demonstrating the algorithm's ability to accurately reconstruct images from inferred latent variables. The right panel shows samples generated de novo from the learned generative model, showcasing its capacity to capture the diversity and variability within the dataset.  The key finding is that DCPC achieves high-quality reconstruction with only 16 particles and without needing a separate inference network.", "section": "Experiments"}, {"figure_path": "dxwIaCVkWU/figures/figures_15_1.jpg", "caption": "Figure 5: Divide-and-conquer predictive coding provides an algorithmic interpretation for some of the connections mapped in the canonical neocortical microcircuit [Bastos et al., 2012, 2020, Campagnola et al., 2022]: prediction errors (red) arrive through ascending pathways into the central laminar layer 4, which transmits them up to layers 2/3 (green). These layers combine the incoming errors with a present posterior estimate (green L5\u2192L2/3 connection) to generate prediction errors for the next cortical area. Eventually the updated predictions flow back down the cortical hierarchy (blue).", "description": "This figure illustrates how the Divide-and-Conquer Predictive Coding (DCPC) algorithm aligns with the canonical cortical microcircuit model.  It shows how prediction errors are processed in a hierarchical structure, flowing bottom-up (red) and top-down (blue) between cortical layers (L1-L6) to update predictions and refine estimates of latent variables. The green arrows show the interaction between the layers involved in combining predictions and errors.  The algorithm aims to approximate the complete conditional density for each variable by combining local prediction errors from different parts of the network.", "section": "Biological plausibility"}]