[{"figure_path": "1BZKqZphsW/figures/figures_1_1.jpg", "caption": "Figure 1: Environment reward distribution shift, and quantile plot for IMDB-Gen.", "description": "This figure visualizes the changes in reward distribution and quantiles across different model training methods (SFT, RLHF, RA-RLHF) for the IMDB-Gen task.  The leftmost panel shows the distribution of rewards from the input prompts themselves. The next three panels depict the distributions of rewards obtained after the models generate continuations of the input prompts.  The final panel displays a quantile-quantile plot illustrating the average reward generated by each model as a function of the input prompt's toxicity quantile.  This allows for a comparison of the models' performance in avoiding toxic outputs while maintaining effectiveness in generative tasks.", "section": "Experimental Evaluation"}, {"figure_path": "1BZKqZphsW/figures/figures_1_2.jpg", "caption": "Figure 2: Environment reward distribution shift, and quantile plot for Jigsaw-Gen.", "description": "This figure visualizes the shift in reward distribution and quantile plot after fine-tuning a language model using three different methods: supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), and risk-averse reinforcement learning from human feedback (RA-RLHF).  The left four subplots show the distribution of prompt scores (toxicity levels) for each method, illustrating how the distribution changes after fine-tuning. The rightmost subplot shows the average reward obtained at different quantiles of the prompt score distribution. This comparison helps to understand the effectiveness of each method in mitigating the generation of toxic content, especially for prompts with high toxicity levels.", "section": "Experimental Evaluation"}, {"figure_path": "1BZKqZphsW/figures/figures_7_1.jpg", "caption": "Figure 3: Tail sentiment score plotted for one seed.", "description": "This figure shows the tail sentiment scores for different models on the IMDB-Gen task. The tail refers to the most negative movie reviews.  The x-axis represents the different models: GPT-2, Prompted GPT-2, DExperts, SFT, RLHF, Quark, and RA-RLHF. The y-axis shows the average sentiment score for the tail reviews of each model. The goal is to evaluate how effectively each model mitigates negative sentiments, especially in challenging situations. The lower the sentiment score, the more negative the sentiment generated by the model.", "section": "5.1 Results on Risk-Aversion"}, {"figure_path": "1BZKqZphsW/figures/figures_9_1.jpg", "caption": "Figure 4: Average environment rewards, and per batch returns during training for IMDB-Gen and Jigsaw-Gen.", "description": "This figure visualizes the performance of both RA-RLHF and RLHF algorithms across two datasets (IMDB-Gen and Jigsaw-Gen) during the training process.  It presents two sub-figures for each dataset: one showing the average environment reward per training iteration, and the other showing the return over the batch of episodes for each iteration.  The shaded areas likely represent the standard deviation or confidence intervals around the average reward, providing a measure of the variability or stability of the training process. The figure helps illustrate the difference between RA-RLHF (risk-averse RLHF) and standard RLHF approaches, showing how RA-RLHF might achieve different rewards and variability.", "section": "5 Experimental Evaluation"}, {"figure_path": "1BZKqZphsW/figures/figures_9_2.jpg", "caption": "Figure 4: Average environment rewards, and per batch returns during training for IMDB-Gen and Jigsaw-Gen.", "description": "This figure visualizes the average environment rewards and per-batch returns during the training process for two different datasets: IMDB-Gen and Jigsaw-Gen.  Separate plots are shown for each dataset, with one plot displaying the average environment reward over training iterations and the other showing the per-batch returns (rewards obtained for each batch of training episodes). This allows for analysis of the training progress across different metrics and datasets, revealing how the average reward changes over time and how rewards per training batch fluctuates. The lines represent the RLHF and RA-RLHF training methods, and the shaded areas indicate the standard deviation or variability within the training results. The figure provides insight into the stability and effectiveness of each training algorithm.", "section": "5 Experimental Evaluation"}, {"figure_path": "1BZKqZphsW/figures/figures_9_3.jpg", "caption": "Figure 12: IMDB risk schedule analysis", "description": "This figure shows the IMDB risk schedule analysis. The x-axis represents the training iterations, while the y-axis represents the batch size. Different colored lines represent different risk schedules, which are defined by the parameters n, \u03b1, and \u03c1. These parameters control the rate at which the batch size is reduced during training, balancing the exploration of both positive and negative scenarios to achieve risk aversion. The figure illustrates how different choices of these hyperparameters affect the risk schedule, demonstrating the trade-off between risk-aversion and exploration.", "section": "F.1 Risk Scheduler Analysis"}, {"figure_path": "1BZKqZphsW/figures/figures_15_1.jpg", "caption": "Figure 6: Scores for train prompts of size 200 characters (~64 tokens) for IMDB review dataset.", "description": "This figure shows the distribution of environment rewards (sentiment scores) for training prompts in the IMDB dataset.  Panel (a) displays the distribution for all training prompts, while panel (b) focuses specifically on the distribution for prompts with low scores (the tail of the distribution).  The x-axis represents the environment reward (sentiment score), and the y-axis represents the number of prompts. The different colored sections show the number of positive and negative prompts in each bin. This visualization helps to understand the characteristics of the training data, particularly the distribution of difficult prompts (those with negative sentiment).", "section": "D.3.1 Scores (Environment rewards) distribution"}, {"figure_path": "1BZKqZphsW/figures/figures_15_2.jpg", "caption": "Figure 6: Scores for train prompts of size 200 characters (~64 tokens) for IMDB review dataset.", "description": "This figure shows the distribution of environment rewards (sentiment scores) for training prompts of size 200 characters (approximately 64 tokens) in the IMDB review dataset.  It consists of two subfigures: (a) shows the distribution for all training prompts, while (b) focuses on the distribution for prompts in the tail (those with the lowest scores). The distributions for positive and negative sentiment prompts are shown separately in each subfigure. This visualization helps understand the distribution of sentiment in the training data and the characteristics of the more challenging prompts.", "section": "D.3.1 Scores (Environment rewards) distribution"}, {"figure_path": "1BZKqZphsW/figures/figures_16_1.jpg", "caption": "Figure 11: Clustering on Jigsaw test dataset.", "description": "This figure shows the results of a k-means cluster analysis performed on the embeddings of critical prompts from the Jigsaw test dataset.  The embeddings were generated using the EleutherAI/gpt-j-6b model.  The resulting clusters were then projected into two dimensions using PCA for visualization. Each point represents a prompt, and the color indicates the cluster to which it belongs.  The caption is short, so this description provides further context on the methodology and interpretation of the visualization.", "section": "D.4.2 Critical prompt clusters"}, {"figure_path": "1BZKqZphsW/figures/figures_18_1.jpg", "caption": "Figure 6: Scores for train prompts of size 200 characters (~64 tokens) for IMDB review dataset.", "description": "This figure shows the distribution of environment rewards (sentiment scores) for the IMDB dataset training prompts.  Panel (a) displays the distribution of scores for all training prompts, while panel (b) focuses on the distribution of scores for the 'tail' prompts, which are the most negative ones. The x-axis represents the sentiment score, and the y-axis represents the number of prompts with that score. The distribution in (b) helps to highlight the concentration of the most challenging negative prompts.", "section": "D.3.1 Scores (Environment rewards) distribution"}, {"figure_path": "1BZKqZphsW/figures/figures_19_1.jpg", "caption": "Figure 10: Scores for test prompts of size 60 characters (~ 20 tokens) for Jigsaw dataset.", "description": "This figure shows the distribution of environment rewards (scores) for test prompts in the Jigsaw dataset.  Panel (a) displays the distribution for all test prompts, while panel (b) focuses specifically on the \"tail\" test prompts, which represent the most challenging or difficult cases for the model to handle. The x-axis represents the environment score, which reflects the toxicity level of the prompt, with higher scores indicating less toxicity.  The y-axis represents the number of input prompts with that given score. The separation of scores for toxic and non-toxic prompts allows for visualization of the model's performance in handling a range of prompt toxicity levels, from easy to very challenging.", "section": "D.4 Jigsaw"}, {"figure_path": "1BZKqZphsW/figures/figures_19_2.jpg", "caption": "Figure 11: Clustering on Jigsaw test dataset.", "description": "This figure shows the result of k-means clustering performed on the embeddings of critical prompts from the Jigsaw test dataset.  The embeddings were generated using the EleutherAI/gpt-j-6b model. The clusters are then projected into a 2D space using PCA for visualization. Each point represents a prompt, and the color indicates the cluster it belongs to.  Analysis of the prompts within each cluster reveals distinct patterns in terms of their toxicity and the topics they address. For example, some clusters contain prompts exhibiting hate speech targeting specific groups, while others feature more general insults or toxic language. This visualization helps to understand the nature and diversity of toxic prompts encountered in the dataset, and aids in identifying potential areas for improvement in toxicity mitigation strategies.", "section": "D.4.2 Critical prompt clusters"}, {"figure_path": "1BZKqZphsW/figures/figures_23_1.jpg", "caption": "Figure 12: IMDB risk schedule analysis", "description": "This figure shows the different risk schedules used during training with RA-RLHF.  The x-axis represents the number of training iterations, and the y-axis represents the batch size used for training. Different lines represent different hyperparameter settings (n, alpha, rho) which control the rate at which the batch size decreases during training, introducing risk aversion gradually.", "section": "F.1 Risk Scheduler Analysis"}, {"figure_path": "1BZKqZphsW/figures/figures_23_2.jpg", "caption": "Figure 4: Average environment rewards, and per batch returns during training for IMDB-Gen and Jigsaw-Gen.", "description": "This figure displays the training performance of both RLHF and RA-RLHF methods across two datasets: IMDB-Gen and Jigsaw-Gen.  It consists of four subplots. The top row shows the average environment reward (a measure of overall performance) and the average per-batch return (a measure of reward variability during training) for the IMDB-Gen dataset. The bottom row shows the same metrics but for the Jigsaw-Gen dataset.  Each subplot shows the trajectories of both algorithms, allowing visual comparison of their performance. The plots reveal that RA-RLHF generally results in slightly lower average rewards but with reduced variance compared to RLHF, suggesting improved robustness and stability.", "section": "5 Experimental Evaluation"}, {"figure_path": "1BZKqZphsW/figures/figures_23_3.jpg", "caption": "Figure 4: Average environment rewards, and per batch returns during training for IMDB-Gen and Jigsaw-Gen.", "description": "This figure shows the training progress of both RLHF and RA-RLHF methods on two datasets, IMDB-Gen and Jigsaw-Gen.  It displays the average environment reward and the reward per batch across training iterations. The plots visualize the stability and performance of each method, illustrating how the risk-averse approach (RA-RLHF) impacts the training dynamics. By comparing RLHF and RA-RLHF, we can observe the effects of the risk-averse strategy on reward accumulation and training stability.", "section": "Experimental Evaluation"}, {"figure_path": "1BZKqZphsW/figures/figures_24_1.jpg", "caption": "Figure 1: Environment reward distribution shift, and quantile plot for IMDB-Gen.", "description": "This figure presents a comprehensive analysis of reward distribution shifts and quantile plots for the IMDB-Gen task. It visually compares the performance of three different models: Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Risk-Averse RLHF (RA-RLHF).  The leftmost four subplots display histograms showing the distribution of prompt scores for positive and negative reviews across the three models.  These histograms illustrate how the reward distributions shift after fine-tuning using each method. The final subplot is a quantile plot, showing average reward as a function of the quantile of prompt scores. This helps visualize the risk-aversion properties of each model, highlighting how well each model handles rare, high-stakes scenarios (prompts with extreme negative scores). The results indicate that RA-RLHF is more effective in promoting safer, more constructive online discourse by shifting the reward distribution towards more positive outcomes, especially for challenging, high-risk prompts.", "section": "5 Experimental Evaluation"}, {"figure_path": "1BZKqZphsW/figures/figures_25_1.jpg", "caption": "Figure 1: Environment reward distribution shift, and quantile plot for IMDB-Gen.", "description": "This figure visualizes the impact of different fine-tuning methods on the distribution of rewards in the IMDB-Gen environment.  It shows histograms of the reward distribution for prompts and their completions using Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Risk-Averse RLHF (RA-RLHF).  A quantile plot is also included, showing the average reward versus the prompt quantile. The goal is to show how well each method avoids generating negative or toxic content by shifting the reward distribution towards positive outcomes, particularly for the most challenging (riskiest) prompts.", "section": "Experimental Evaluation"}]