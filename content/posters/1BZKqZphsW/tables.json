[{"figure_path": "1BZKqZphsW/tables/tables_1_1.jpg", "caption": "Table 1: Sample generations over tail of prompt reward distribution for IMDB-Gen and Jigsaw-Gen.", "description": "This table provides example outputs generated by three different LLMs (Supervised Fine-Tuning, Reinforcement Learning from Human Feedback, and Risk-Averse Reinforcement Learning from Human Feedback) for both IMDB and Jigsaw datasets.  The examples are chosen from the tail of the prompt reward distribution which represents prompts that were most challenging to generate positive/non-toxic responses to.  The table demonstrates the different models' performance in successfully generating positive or non-toxic responses even when the input prompts were negative or toxic in nature.", "section": "1 Introduction"}, {"figure_path": "1BZKqZphsW/tables/tables_7_1.jpg", "caption": "Table 2: Sentiment score (Senti), perplexity (PP) and diversity evaluation metrics with GPT-2 base model on IMDB-Gen.", "description": "This table presents the quantitative performance results of different LLMs on the IMDB-Gen task. It compares the sentiment scores (Senti), perplexity (PP), and diversity scores (Dist-1, Dist-2, Dist-3) for various models, including the baseline GPT-2,  Prompted GPT-2 (with added prompt for positive sentiment), DExperts, Supervised Fine-tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), Quark, and the proposed Risk-Averse RLHF (RA-RLHF). Higher sentiment scores indicate more positive reviews, lower perplexity suggests better coherence, and higher diversity scores reflect a more varied range of generated text.  The results illustrate the comparative performance of the different methods in balancing positive sentiment generation with text quality and diversity.", "section": "5.1 Results on Risk-Aversion"}, {"figure_path": "1BZKqZphsW/tables/tables_8_1.jpg", "caption": "Table 3: Nagative toxicity score (-Tox) and diversity evaluation metrics for Jigsaw-Gen and RealToxicity Prompts-Gen.", "description": "This table presents the performance of different models on two tasks: Jigsaw-Gen and RealToxicityPrompts-Gen.  The models are evaluated based on negative toxicity scores (-Tox), and diversity metrics (Dist-1, Dist-2, Dist-3).  Higher -Tox scores indicate better performance in mitigating toxicity.  Diversity metrics assess the variety of text generated by each model. The table allows comparison of several baselines (GPT-2, Prompted, DExperts, SFT, RLHF, Quark) with the proposed RA-RLHF method.", "section": "5.1 Results on Risk-Aversion"}, {"figure_path": "1BZKqZphsW/tables/tables_8_2.jpg", "caption": "Table 9: Testing on reward (r), and Perplexity. For average reward calculation, test samples from both positive and negative classes are used. For perplexity calculations, only positive class samples are used. Results are for one seed.", "description": "This table presents the results of the IMDB (GPT-J) experiment. The table shows the average reward and perplexity for different models: GPT2, GPTJ, SFT, RLHF, and RA-RLHF (the proposed method). The average reward indicates how well the model performed on the IMDB sentiment classification task. The perplexity metric shows the model's ability to generate coherent text.", "section": "F.3.1 Results for IMDB-Gen (on GPT-J)"}, {"figure_path": "1BZKqZphsW/tables/tables_16_1.jpg", "caption": "Table 1: Sample generations over tail of prompt reward distribution for IMDB-Gen and Jigsaw-Gen.", "description": "This table displays example outputs generated by different LLMs (Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Risk-Averse RLHF (RA-RLHF)) for both IMDB-Gen and Jigsaw-Gen datasets.  The examples are chosen from the tail of the prompt reward distribution, representing challenging prompts that are more likely to produce negative or toxic outputs. The table shows how RA-RLHF improves the models' ability to avoid generating such outputs while maintaining effectiveness in generative tasks.", "section": "Experimental Evaluation"}, {"figure_path": "1BZKqZphsW/tables/tables_21_1.jpg", "caption": "Table 1: Sample generations over tail of prompt reward distribution for IMDB-Gen and Jigsaw-Gen.", "description": "This table shows example outputs generated by three different LLMs (Supervised Fine-Tuning, Reinforcement Learning from Human Feedback, and Risk-Averse Reinforcement Learning from Human Feedback) in response to prompts with negative sentiment scores.  The goal is to illustrate how the risk-averse model performs better at generating less toxic content in response to difficult prompts, showing its effectiveness in sentiment modification and toxicity mitigation.", "section": "1 Introduction"}, {"figure_path": "1BZKqZphsW/tables/tables_21_2.jpg", "caption": "Table 1: Sample generations over tail of prompt reward distribution for IMDB-Gen and Jigsaw-Gen.", "description": "This table shows example outputs generated by three different models (SFT, RLHF, and RA-RLHF) for a selection of prompts with negative or toxic characteristics.  The goal is to illustrate how the risk-averse fine-tuning method (RA-RLHF) improves the safety and quality of the generated text compared to the other methods.  The table includes the prompt, the generated text for each model, and a score indicating the negativity or toxicity of the prompt. ", "section": "1 Introduction"}, {"figure_path": "1BZKqZphsW/tables/tables_21_3.jpg", "caption": "Table 1: Sample generations over tail of prompt reward distribution for IMDB-Gen and Jigsaw-Gen.", "description": "This table displays example outputs generated by three different LLMs (Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Risk-Averse RLHF (RA-RLHF)) for both IMDB-Gen and Jigsaw-Gen datasets. The prompts used are those with the lowest reward scores (representing the most challenging cases). The table demonstrates how RA-RLHF effectively steers the generated text towards more positive and less toxic outputs compared to the other LLMs, particularly for difficult prompts.", "section": "1 Introduction"}, {"figure_path": "1BZKqZphsW/tables/tables_21_4.jpg", "caption": "Table 1: Sample generations over tail of prompt reward distribution for IMDB-Gen and Jigsaw-Gen.", "description": "This table provides example outputs generated by three different models (SFT, RLHF, and RA-RLHF) for both IMDB-Gen and Jigsaw-Gen datasets.  The examples shown are specifically selected from the tail of the prompt reward distribution, meaning these are prompts that are particularly challenging or likely to elicit negative or toxic responses. Each row shows a prompt, followed by the responses generated by the three models, along with the corresponding reward score for each response.  This highlights the models' performance in handling difficult prompts and illustrates the differences in outputs between the various approaches. The different models represent different approaches to training an LLM, showing how risk-averse fine-tuning can improve the ability of an LLM to avoid generating toxic content.", "section": "1 Introduction"}, {"figure_path": "1BZKqZphsW/tables/tables_22_1.jpg", "caption": "Table 7: RLHF Hyperparameters", "description": "This table lists the hyperparameters used for Proximal Policy Optimization (PPO) training in the RLHF (Reinforcement Learning from Human Feedback) experiments.  It shows the values used for both the IMDB-Gen and Jigsaw-Gen tasks.  Hyperparameters not listed here were set to the default values provided by Hugging Face's PPOConfig object.", "section": "E.3 Training Hyperparameters"}, {"figure_path": "1BZKqZphsW/tables/tables_23_1.jpg", "caption": "Table 8: RA-RLHF Hyperparameters", "description": "This table lists the hyperparameters used in the Risk-Averse Reinforcement Learning from Human Feedback (RA-RLHF) algorithm.  Specifically, it shows the risk level (\u03b1), the warm start (n), and the \u03c1 parameter for both the IMDB-Gen and Jigsaw-Gen datasets. These parameters control the risk-aversion strategy during the fine-tuning process. The risk level determines the quantile of worst-case returns considered during optimization; the warm start determines the initial training phase without risk-aversion; and the \u03c1 parameter affects the rate of risk-aversion introduction.", "section": "F.1 Risk Scheduler Analysis"}, {"figure_path": "1BZKqZphsW/tables/tables_24_1.jpg", "caption": "Table 9: Testing on reward (r), and Perplexity. For average reward calculation, test samples from both positive and negative classes are used. For perplexity calculations, only positive class samples are used. Results are for one seed.", "description": "This table presents the performance of different models on the IMDB dataset using GPT-J.  It shows the average reward, the average reward for the worst-performing prompts (tail), and the perplexity.  The results are based on a single seed.  Higher reward values indicate better performance in terms of generating positive reviews while lower perplexity suggests higher quality of generated text.", "section": "F.3.1 Results for IMDB-Gen (on GPT-J)"}, {"figure_path": "1BZKqZphsW/tables/tables_25_1.jpg", "caption": "Table 10: Testing on reward (r), and Perplexity. For average reward calculation, test samples from both positive and negative classes are used. For perplexity calculations, only positive class samples are used. Results are for one seed.", "description": "This table presents the quantitative performance results for the RealToxicityPrompts-Gen task.  It shows the average reward, the average reward on tail prompts (those with high toxicity), and the perplexity for four different models: the baseline GPT2 model, a model fine-tuned using supervised learning (SFT), a model fine-tuned using standard RLHF, and the proposed RA-RLHF model.  The metrics are used to compare the performance of the models in terms of both the overall reward and the ability to handle toxic prompts effectively.", "section": "F.4 RealToxicityPrompts-Gen"}]