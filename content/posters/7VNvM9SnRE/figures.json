[{"figure_path": "7VNvM9SnRE/figures/figures_8_1.jpg", "caption": "Figure 1: Averaged Regret for m = 5, \u03b8\u03bf = 0.01", "description": "The figure presents a comparison of the averaged regret of three different algorithms (Our Alg-1 (Adaptive pivot), Our Alg-2 (No-Choice pivot), and MNL-UCB) across two different datasets (arith50 and Bad50) for two different objectives (Top-m and Wtd-Top-m). It displays the regret as a function of the number of rounds (T).  The results indicate the superior performance of the algorithm AOA-RBPL-Adaptive.", "section": "5 Experiments"}, {"figure_path": "7VNvM9SnRE/figures/figures_8_2.jpg", "caption": "Figure 2: Comparative performance for varying \u03b8\u2080/\u03b8\u2098\u2090\u2093, m = 5", "description": "This figure shows the comparison of the averaged regret for varying values of the parameter \u03b8\u2080/\u03b8\u2098\u2090\u2093. The averaged regret is calculated for both top-m and weighted top-m objectives. The algorithms compared are our proposed algorithm AOA-RBPL-Adaptive, our other proposed algorithm AOA-RBPL (with no-choice pivot), and MNL-UCB. The figure shows that our proposed AOA-RBPL-Adaptive algorithm consistently outperforms other algorithms across various values of \u03b8\u2080/\u03b8\u2098\u2090\u2093.", "section": "Experiments"}, {"figure_path": "7VNvM9SnRE/figures/figures_8_3.jpg", "caption": "Figure 3: Tradofff: Averaged Regret vs length of the k rank-ordered feedback (k)", "description": "This figure shows the tradeoff between learning rate and the length of rank-ordered feedback. The experiment was conducted on Arith50 dataset with m=30 and k in {1,2,4,8}. The results show an improved regret (for both top-m and weighted top-m objectives) when increasing k, indicating the benefit of using more informative feedback.", "section": "Emperical Analysis"}]