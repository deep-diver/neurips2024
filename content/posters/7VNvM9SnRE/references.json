{"references": [{"fullname_first_author": "Shipra Agrawal", "paper_title": "MNL-bandit: A dynamic learning approach to assortment selection", "publication_date": "2019-00-00", "reason": "This paper is foundational to the field of assortment optimization, introducing a dynamic learning approach and serving as a benchmark for the proposed methods."}, {"fullname_first_author": "Nir Ailon", "paper_title": "Reducing dueling bandits to cardinal bandits", "publication_date": "2014-00-00", "reason": "This paper provides a fundamental connection between dueling bandits and cardinal bandits, which is crucial to the understanding of preference-based feedback mechanisms used in the current work."}, {"fullname_first_author": "Peter Auer", "paper_title": "Using upper confidence bounds for online learning", "publication_date": "2000-00-00", "reason": "This paper introduces the concept of Upper Confidence Bounds (UCB), a core algorithm design technique in the current work, and lays the foundation for regret minimization in online learning."}, {"fullname_first_author": "Viktor Bengs", "paper_title": "Preference-based online learning with dueling bandits: A survey", "publication_date": "2021-00-00", "reason": "This survey provides a comprehensive overview of preference-based online learning and dueling bandits, which forms the background and context for this research."}, {"fullname_first_author": "Gerardo Berbeglia", "paper_title": "Assortment optimisation under a general discrete choice model: A tight analysis of revenue-ordered assortments", "publication_date": "2016-00-00", "reason": "This paper presents a tight analysis of revenue-ordered assortments, which provides insights into the structural properties of the problem under investigation"}]}