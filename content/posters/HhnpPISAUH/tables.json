[{"figure_path": "HhnpPISAUH/tables/tables_5_1.jpg", "caption": "Table 2: The number of communication rounds needed to reach a certain test accuracy in the experiments on FMNIST, CIFAR10, Mini-ImageNet and THUC News. All results are for the concentration parameter setting (2).", "description": "This table shows the number of communication rounds required by different client selection algorithms (random sampling, pow-d, Clustered Sampling, DivFL, FedCor, and HiCS-FL) to achieve a certain test accuracy on four different datasets (FMNIST, CIFAR10, Mini-ImageNet, and THUC News).  The results are specifically for the second concentration parameter setting, demonstrating the efficiency of HiCS-FL in terms of communication rounds needed for convergence.", "section": "4 Experiments"}, {"figure_path": "HhnpPISAUH/tables/tables_8_1.jpg", "caption": "Table 1: Test accuracy (%) for the global model on 3 groups of data partitions of THUC news dataset.", "description": "This table presents the test accuracy results achieved by different client selection schemes on the THUC news dataset. Three different data heterogeneity settings are considered, with varying degrees of class imbalance among clients. The results show how each client selection method performs under different levels of data heterogeneity.  The 'setting' column specifies the level of heterogeneity, with (1) representing more balanced data, and (3) representing more imbalanced data. HiCS-FL consistently outperforms the other methods, demonstrating its effectiveness in handling diverse data heterogeneity scenarios.", "section": "4.1 Comparison on Test Accuracy and Training Loss"}, {"figure_path": "HhnpPISAUH/tables/tables_8_2.jpg", "caption": "Table 2: The number of communication rounds needed to reach a certain test accuracy in the experiments on FMNIST, CIFAR10, Mini-ImageNet and THUC News. All results are for the concentration parameter setting (2).", "description": "This table shows the number of communication rounds required to achieve a certain test accuracy for four different datasets (FMNIST, CIFAR10, Mini-ImageNet, and THUC News) using different client selection methods. The speedup is calculated relative to the random sampling method.  A lower number of rounds indicates faster convergence.", "section": "4.2 Accelerating the Training Convergence"}, {"figure_path": "HhnpPISAUH/tables/tables_9_1.jpg", "caption": "Table 3: Additional experimental results (accuracy in %) on HiCS with the number of clusters M \u2264 K, where K is the number of selected clients each global round.", "description": "This table presents the accuracy results of the HiCS-FL algorithm under different numbers of clusters (M) compared to the number of selected clients per round (K).  It shows how the accuracy changes as the ratio of M to K varies. The results are presented for different datasets and heterogeneity scenarios, indicated by CIFAR10 (1), CIFAR10 (2), CIFAR10 (3), Mini-ImageNet (1), and Mini-ImageNet (2).  These different scenarios likely represent various levels of data heterogeneity across clients.", "section": "4.3 Number of Clustering Groups"}, {"figure_path": "HhnpPISAUH/tables/tables_9_2.jpg", "caption": "Table 4: In experiments on CIFAR10, only 20 out of 50 clients are available in the beginning; additional 10 clients join each 100 global rounds. The initial 20 clients leave the system after 400 global rounds.", "description": "This table presents the test accuracy results (%) on CIFAR10 and Mini-ImageNet datasets under a dynamic client availability setting. In this setting, initially, only 20 out of 50 clients are available for training. Then, every 100 global rounds, 10 more clients join the training process, while the initial 20 clients leave after 400 global rounds. The table compares the performance of HiCS-FL against several baseline methods, showcasing its robustness in scenarios with fluctuating client participation.", "section": "4 Experiments"}, {"figure_path": "HhnpPISAUH/tables/tables_28_1.jpg", "caption": "Table 5: The columns \"Extra Computation\" and \"Extra Communication\" denote the computation and communication complexity of additional operations in each sampling scheme compared to random sampling.", "description": "This table compares the computational and communication complexities of different client sampling methods in federated learning. It shows the additional computational and communication overhead of each method compared to a baseline of random sampling. The complexity is expressed using Big O notation, where |\u03b8t| represents the size of the global model parameters at round t, and C represents the number of classes.  HiCS-FL shows significantly lower computational overhead than other methods.", "section": "A.11 Computational and Communication Complexity"}]