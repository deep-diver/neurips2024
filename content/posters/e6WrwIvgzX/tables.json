[{"figure_path": "e6WrwIvgzX/tables/tables_7_1.jpg", "caption": "Table 1: AIBC values: AutoMix + T and AutoMix + P are variations of our proposed method with thresholding (T) and POMDP-based routers, respectively. Best numbers are bold, and second-best are underlined. AutoMix + POMDP demonstrates robust and consistent AIBC across all datasets, implying judicious utilization of computational resources. Despite domain-specific training and a 0-cost verifier, FrugalGPT and HybridLLM underperform AutoMix in almost all scenarios.", "description": "This table presents the Incremental Benefit per Cost (IBC) values for different model-mixing methods across five datasets and three different smaller language models (SLMs).  It compares AutoMix (with thresholding and POMDP-based routers) against baselines FrugalGPT and HybridLLM.  Higher AIBC values indicate better cost-performance tradeoffs. AutoMix consistently outperforms the baselines, showing improved efficiency in leveraging language models for various tasks.", "section": "5.2 Setup"}, {"figure_path": "e6WrwIvgzX/tables/tables_14_1.jpg", "caption": "Table 1: AIBC values: AutoMix + T and AutoMix + P are variations of our proposed method with thresholding (T) and POMDP-based routers, respectively. Best numbers are bold, and second-best are underlined. AutoMix + POMDP demonstrates robust and consistent AIBC across all datasets, implying judicious utilization of computational resources. Despite domain-specific training and a 0-cost verifier, FrugalGPT and HybridLLM underperform AutoMix in almost all scenarios.", "description": "This table presents the incremental benefit per cost (IBC) values for different model-mixing methods across five datasets.  It compares the performance of AutoMix (with thresholding and POMDP-based routing) against baselines like FrugalGPT and HybridLLM.  Higher AIBC values indicate greater cost-effectiveness in enhancing model performance.", "section": "5.2 Setup"}, {"figure_path": "e6WrwIvgzX/tables/tables_14_2.jpg", "caption": "Table 1: AIBC values: AutoMix + T and AutoMix + P are variations of our proposed method with thresholding (T) and POMDP-based routers, respectively. Best numbers are bold, and second-best are underlined. AutoMix + POMDP demonstrates robust and consistent AIBC across all datasets, implying judicious utilization of computational resources. Despite domain-specific training and a 0-cost verifier, FrugalGPT and HybridLLM underperform AutoMix in almost all scenarios.", "description": "This table presents the Incremental Benefit per Cost (IBC) lift, which compares the efficiency of performance enhancement relative to the additional cost for different model-mixing methods (AutoMix with thresholding and POMDP, FrugalGPT, HybridLLM) across five datasets and three different base models (MISTRAL-7B, LLAMA2-13B, GPT-3.5).  A positive lift indicates cost-effective performance improvements compared to the baseline of always using the large language model (LLM).", "section": "5. Experiments"}, {"figure_path": "e6WrwIvgzX/tables/tables_15_1.jpg", "caption": "Table 1: AIBC values: AutoMix + T and AutoMix + P are variations of our proposed method with thresholding (T) and POMDP-based routers, respectively. Best numbers are bold, and second-best are underlined. AutoMix + POMDP demonstrates robust and consistent AIBC across all datasets, implying judicious utilization of computational resources. Despite domain-specific training and a 0-cost verifier, FrugalGPT and HybridLLM underperform AutoMix in almost all scenarios.", "description": "This table presents the results of comparing AutoMix's performance with two baseline methods (FrugalGPT and HybridLLM) across five different datasets.  The AIBC (Incremental Benefit per Cost) metric is used to assess the cost-effectiveness of each method, showing how much performance is gained relative to the additional cost incurred. AutoMix consistently outperforms the baselines, demonstrating its superior efficiency in using computational resources.", "section": "5.3 Main Results"}, {"figure_path": "e6WrwIvgzX/tables/tables_18_1.jpg", "caption": "Table 1: AIBC values: AutoMix + T and AutoMix + P are variations of our proposed method with thresholding (T) and POMDP-based routers, respectively. Best numbers are bold, and second-best are underlined. AutoMix + POMDP demonstrates robust and consistent AIBC across all datasets, implying judicious utilization of computational resources. Despite domain-specific training and a 0-cost verifier, FrugalGPT and HybridLLM underperform AutoMix in almost all scenarios.", "description": "This table presents the results of the Incremental Benefit per Cost (IBC) metric for different model-mixing methods across five datasets and three language models.  It compares AutoMix (with threshold and POMDP routing) against FrugalGPT and HybridLLM baselines.  The AIBC metric shows how much more efficiently each method improves performance compared to simply using the larger model alone, considering the additional costs involved.", "section": "5.2 Setup"}, {"figure_path": "e6WrwIvgzX/tables/tables_25_1.jpg", "caption": "Table 5: Out-of-domain generalization across various SLMs for different methods. Scores are averaged across five datasets. AutoMix significantly outperforms other methods.", "description": "This table presents the results of an out-of-domain generalization experiment to evaluate the generalizability of AutoMix.  The router was trained on one dataset and then evaluated on the remaining four datasets. This process was repeated for all five datasets. The table shows that AutoMix significantly outperforms both FrugalGPT and HybridLLM across all three language models (Mistral-7b, LLAMA-13b, and GPT-3.5) and five datasets.", "section": "5. Experiments"}]