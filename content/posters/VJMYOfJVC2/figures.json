[{"figure_path": "VJMYOfJVC2/figures/figures_1_1.jpg", "caption": "Figure 1: Metric triangle among reliability, generalization, and locality. ZsRE dataset, number of continual edits T = 100, LLaMA-2-7B. Editing methods based on long-term memory (ROME and FT-EWC) and working memory (DEFER and GRACE) show the impossible triangle in metrics, while our WISE is leading in all three metrics.", "description": "This figure shows the performance of different lifelong model editing methods across three key metrics: reliability, generalization, and locality.  Methods are categorized into those editing long-term memory (directly modifying model parameters) and those using working memory (retrieving and modifying activations). The figure highlights that existing methods struggle to achieve high performance across all three metrics simultaneously, illustrating an \"impossible triangle\".  The proposed WISE method, however, demonstrates superior performance across all three metrics.", "section": "1 Introduction"}, {"figure_path": "VJMYOfJVC2/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of WISE. Side memory (in blue) and main memory (in green) store edited and pretrained knowledge, respectively. Note: during inference, if WISE-Retrieve, the activation routing will retrieve and select one side memory with maximal activation score.", "description": "This figure illustrates the architecture of the WISE model, which uses a dual parametric memory scheme.  The main memory stores pretrained knowledge, while the side memory stores newly edited knowledge.  A routing mechanism decides which memory to use for a given query. For continual editing, knowledge sharding and merging are used to avoid conflicts between edits.  The left panel shows the overall workflow with the knowledge routing mechanism, while the right panel shows the details of knowledge sharding and merging. ", "section": "2.3 WISE: Side Memory with Knowledge Sharding, Merging, and Routing"}, {"figure_path": "VJMYOfJVC2/figures/figures_7_1.jpg", "caption": "Figure 5: Analysis of different mask ratios p and subspaces k for WISE. Left: Avg. performance of Rel., Gen., and Loc.; Right: the subspace overlap probability in Theorem 2.1. ZsRE, LLAMA-2-7B.", "description": "This figure analyzes the impact of two hyperparameters in WISE: the mask ratio (p) and the number of subspaces (k).  The left panel shows the average performance across reliability, generalization, and locality metrics for various combinations of p and k.  The right panel displays the probability of overlap between the subspaces, as calculated by Theorem 2.1 in the paper. The red boxes highlight the optimal parameter settings that balance performance and overlap.  The results suggest an optimal trade-off where sufficient overlap exists to help merge knowledge but not so much that it causes conflicts.", "section": "2.3 WISE: Side Memory with Knowledge Sharding, Merging, and Routing"}, {"figure_path": "VJMYOfJVC2/figures/figures_7_2.jpg", "caption": "Figure 4: Analysis of locating FFN layer of side memory for WISE. ZSRE, LLAMA-2-7B.", "description": "This figure displays the results of an ablation study conducted to determine the optimal layer within the feed-forward network (FFN) of a large language model (LLM) to be used for storing edited knowledge. The study was performed using the ZSRE dataset and the LLAMA-2-7B architecture. The x-axis represents different layers of the FFN, while the y-axis shows the average performance across the metrics of Reliability, Generalization, and Locality. The figure reveals that selecting layers in the middle-to-late range yields the best performance, highlighting the importance of choosing appropriate layers for effective side memory design.  The red line indicates the average performance of GRACE (a baseline method) across these metrics.", "section": "2.3 WISE: Side Memory with Knowledge Sharding, Merging, and Routing"}, {"figure_path": "VJMYOfJVC2/figures/figures_7_3.jpg", "caption": "Figure 2: Overview of WISE. Side memory (in blue) and main memory (in green) store edited and pretrained knowledge, respectively. Note: during inference, if WISE-Retrieve, the activation routing will retrieve and select one side memory with maximal activation score.", "description": "This figure illustrates the architecture of the WISE model, which uses a dual memory system for lifelong model editing. The main memory stores the pretrained knowledge, while the side memory stores the edited knowledge. A routing mechanism is used to decide which memory to use for a given query. The figure also shows the knowledge sharding and merging mechanisms used for continual editing.", "section": "2.3 WISE: Side Memory with Knowledge Sharding, Merging, and Routing"}, {"figure_path": "VJMYOfJVC2/figures/figures_8_1.jpg", "caption": "Figure 6: Inference time of WISE when varying T. ZsRE, LLAMA-2-7B.", "description": "This figure shows the inference time of a single instance for LLAMA after t \u2208 [0,3000] editing steps, measured across 10 trials of each setting.  WISE-Merge incurs a constant inference delay (about 3%) as the editing stream expands. WISE-Retrieve, due to the introduction of retrieval routing, shows an increase in inference time as the number of edits increases, with a time cost increment of about 7% after 3K edits.", "section": "3.3 Further Analysis"}, {"figure_path": "VJMYOfJVC2/figures/figures_18_1.jpg", "caption": "Figure 7: Hallucination length statistics.", "description": "This histogram displays the distribution of the lengths of hallucination examples in the SelfCheckGPT dataset, after tokenization using the LlamaTokenizer.  The x-axis represents the length of the hallucination examples (in tokens), and the y-axis represents the frequency of hallucinations with that length. The red line indicates the threshold (250) used in the paper to filter out excessively long examples which exceeded memory limitations.", "section": "3.1 Experimental Settings and Evaluation Metrics"}, {"figure_path": "VJMYOfJVC2/figures/figures_21_1.jpg", "caption": "Figure 8: Mid-layer MLPs play a crucial mediating role in LLAMA-2-7B and Mistral-7B.", "description": "The figure shows the average indirect effect of the MLP across different layers in LLAMA-2-7B and Mistral-7B. The causal effect of states at the early site with Attn or MLP modules severed shows the importance of mid-layer MLPs in mediating the flow of information, affecting the model's ability to recall factual knowledge and pass information to the final layer.", "section": "Further Analysis"}, {"figure_path": "VJMYOfJVC2/figures/figures_21_2.jpg", "caption": "Figure 1: Metric triangle among reliability, generalization, and locality. ZsRE dataset, number of continual edits T = 100, LLaMA-2-7B. Editing methods based on long-term memory (ROME and FT-EWC) and working memory (DEFER and GRACE) show the impossible triangle in metrics, while our WISE is leading in all three metrics.", "description": "This figure shows the results of different lifelong model editing methods on three key metrics: reliability, generalization, and locality.  Methods that edit long-term memory (direct model parameters) or working memory (retrieval-based activations) struggle to achieve high scores across all three metrics, illustrating an \"impossible triangle.\"  The proposed WISE method significantly outperforms these prior methods, demonstrating high scores in all three metrics simultaneously.", "section": "1 Introduction"}, {"figure_path": "VJMYOfJVC2/figures/figures_23_1.jpg", "caption": "Figure 2: Overview of WISE. Side memory (in blue) and main memory (in green) store edited and pretrained knowledge, respectively. Note: during inference, if WISE-Retrieve, the activation routing will retrieve and select one side memory with maximal activation score.", "description": "This figure shows the architecture of the WISE model, which uses a dual parametric memory mechanism.  The main memory stores the pretrained knowledge, while the side memory stores the edited knowledge. A router decides which memory to use based on the input query.  The figure also illustrates the knowledge sharding and merging mechanism for continual editing.  The edits are placed into distinct subspaces within the side memory, which are subsequently merged to create a comprehensive side memory without conflicts.", "section": "2.3 WISE: Side Memory with Knowledge Sharding, Merging, and Routing"}, {"figure_path": "VJMYOfJVC2/figures/figures_24_1.jpg", "caption": "Figure 11: Comparing editing results of WISE-{Retrieve, Retrieveoracle, Retrieve w. Lmemo} when varying T. (a) shows the simple average of Rel. and Gen. (ES.), while (b) shows retrieval accuracy, i.e., whether the Top-1 Activation routes to the correct MLP (prec@1). X-axis: Num edits. ZsRE. LLAMA-2-7B.", "description": "This figure compares the performance of three variations of WISE (Retrieve, Retrieveoracle, and Retrieve w. Lmemo) across different numbers of edits (T).  The left subfigure (a) displays the average of Reliability and Generalization scores (Edit Success, ES), illustrating the overall editing effectiveness. The right subfigure (b) shows the retrieval accuracy (precision@1), indicating the success rate of the Top-1 Activation in correctly routing to the appropriate Multi-Layer Perceptron (MLP).  The experiment used the ZsRE dataset and the LLaMA-2-7B model.", "section": "Further Analysis"}, {"figure_path": "VJMYOfJVC2/figures/figures_25_1.jpg", "caption": "Figure 12: Ablation studies on Random Prefix Token (PT) of WISE. Light/Dark colors indicate the Editing Sucess w.o./w. PT addition. ZsRE. LLAMA-2-7B", "description": "This ablation study investigates the impact of adding random prefix tokens to the editing process on the editing success rate, which is evaluated using the Reliability and Generalization metrics. The results show that while adding random prefix tokens improves the model's robustness to various contexts, it can also decrease the editing success rate.  The figure shows the performance of the model with and without prefix tokens for different numbers of edits (T).", "section": "B.6 Ablation Study of Random Prefix Token"}, {"figure_path": "VJMYOfJVC2/figures/figures_26_1.jpg", "caption": "Figure 6: Inference time of WISE when varying T. ZsRE, LLAMA-2-7B.", "description": "This figure shows the inference time of a single instance for LLAMA after t \u2208 [0,3000] editing steps, measured across 10 trials of each setting.  WISE-Merge incurs a constant inference delay (about 3%) as the editing stream expands. WISE-Retrieve, due to the introduction of retrieval routing, shows an increase in inference time as the number of edits increases, with a time cost increment of about 7% after 3K edits.", "section": "3 Experiments"}]