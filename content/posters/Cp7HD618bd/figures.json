[{"figure_path": "Cp7HD618bd/figures/figures_1_1.jpg", "caption": "Figure 1: Our proposed nonparametric inference network first internalizes the desired nonparametric Bayesian prior via metalearning a recurrent neural network (RNN) to model its posterior distribution over class assignments. Afterwards, the metalearned RNN, or neural circuit, has captured the corresponding inductive bias and can be used to perform sequential inference over a potentially unbounded number of classes.", "description": "This figure illustrates the proposed method's architecture.  It involves two main stages:  First, a recurrent neural network (RNN) is metalearned using data generated from a Dirichlet Process Mixture Model (DPMM). This metalearning process allows the RNN to learn the inductive bias of the DPMM.  The second stage, 'Attach,' shows how this trained RNN is then used as a component in a larger nonparametric inference network. This network takes sequential inputs and uses the trained RNN to perform inference over a potentially unlimited number of classes, demonstrating the neural circuit's capability for open-set classification.", "section": "1 Introduction"}, {"figure_path": "Cp7HD618bd/figures/figures_5_1.jpg", "caption": "Figure 1: Our proposed nonparametric inference network first internalizes the desired nonparametric Bayesian prior via metalearning a recurrent neural network (RNN) to model its posterior distribution over class assignments. Afterwards, the metalearned RNN, or neural circuit, has captured the corresponding inductive bias and can be used to perform sequential inference over a potentially unbounded number of classes.", "description": "This figure illustrates the architecture of the proposed nonparametric inference network.  The network consists of three main components: a Dirichlet Process Mixture Model (DPMM) to generate data, a recurrent neural network (RNN) to learn the posterior distribution over class assignments via metalearning, and a nonparametric inference network that uses the learned RNN to perform sequential inference on unseen data. The RNN acts as a \"neural circuit,\" capturing the inductive bias from the DPMM and enabling efficient inference over an open-ended set of classes.", "section": "1 Introduction"}, {"figure_path": "Cp7HD618bd/figures/figures_6_1.jpg", "caption": "Figure 1: Our proposed nonparametric inference network first internalizes the desired nonparametric Bayesian prior via metalearning a recurrent neural network (RNN) to model its posterior distribution over class assignments. Afterwards, the metalearned RNN, or neural circuit, has captured the corresponding inductive bias and can be used to perform sequential inference over a potentially unbounded number of classes.", "description": "This figure illustrates the proposed metalearned neural circuit for nonparametric Bayesian inference.  The process begins with data generated from a Dirichlet Process Mixture Model (DPMM), which represents a nonparametric Bayesian prior. This data is used to train a recurrent neural network (RNN).  The trained RNN, referred to as a \"neural circuit,\" then captures the inductive bias from the DPMM.  This allows the neural circuit to perform sequential inference over a potentially unlimited number of classes, effectively handling the open-set classification problem.", "section": "3 Metalearning a Neural Circuit"}, {"figure_path": "Cp7HD618bd/figures/figures_14_1.jpg", "caption": "Figure 1: Our proposed nonparametric inference network first internalizes the desired nonparametric Bayesian prior via metalearning a recurrent neural network (RNN) to model its posterior distribution over class assignments. Afterwards, the metalearned RNN, or neural circuit, has captured the corresponding inductive bias and can be used to perform sequential inference over a potentially unbounded number of classes.", "description": "This figure illustrates the architecture of the proposed nonparametric inference network.  The network consists of three main components: a Dirichlet Process Mixture Model (DPMM) to generate data, a Recurrent Neural Network (RNN) to model the posterior distribution over class assignments, and a nonparametric inference network that performs sequential inference. The RNN is metalearned by simulating data with the DPMM prior, allowing it to capture the inductive bias of the DPMM and perform inference over an unbounded number of classes. The figure showcases how the inductive bias from a nonparametric Bayesian model is transferred to a neural network to facilitate efficient and scalable inference.", "section": "1 Introduction"}]