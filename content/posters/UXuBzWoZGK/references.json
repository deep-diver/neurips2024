{"references": [{"fullname_first_author": "Gao, L.", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2023-00-00", "reason": "This paper introduces the concept of \"overoptimization\" in RLHF, which is central to the current paper's investigation of catastrophic Goodhart."}, {"fullname_first_author": "Ziegler, D. M.", "paper_title": "Fine-tuning language models from human preferences", "publication_date": "2020-00-00", "reason": "This paper is a foundational work on RLHF, which the current paper builds upon and critiques, particularly regarding KL regularization."}, {"fullname_first_author": "Christiano, P. F.", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-00-00", "reason": "This paper is another foundational work on RLHF, providing the initial framework that is being analyzed and extended in the current paper."}, {"fullname_first_author": "Foss, S.", "paper_title": "An Introduction to Heavy-Tailed and Subexponential Distributions", "publication_date": "2013-00-00", "reason": "This book provides the mathematical background on heavy-tailed distributions, crucial to understanding the theoretical results about catastrophic Goodhart."}, {"fullname_first_author": "Schulman, J.", "paper_title": "Trust region policy optimization", "publication_date": "2015-00-00", "reason": "This paper introduces Trust Region Policy Optimization, a key RL algorithm used in RLHF and relevant to understanding KL regularization in the current paper's context."}]}