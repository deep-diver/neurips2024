[{"type": "text", "text": "Video Token Merging for Long-form Video Understanding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Seon-Ho Lee\u2217 Jue Wang Korea University Amazon AGI seonholee@mcl.korea.ac.kr juewangn@amazon.com Zhikang Zhang David Fan\u2020 Xinyu Li Amazon AGI Meta FAIR Amazon AGI zhikang@amazon.com davidfan@meta.com xxnl@amazon.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As the scale of data and models for video understanding rapidly expand, handling long-form video input in transformer-based models presents a practical challenge. Rather than resorting to input sampling or token dropping, which may result in information loss, token merging shows promising results when used in collaboration with transformers. However, the application of token merging for long-form video processing is not trivial. We begin with the premise that token merging should not rely solely on the similarity of video tokens; the saliency of tokens should also be considered. To address this, we explore various video token merging strategies for long-form video classification, starting with a simple extension of image token merging, moving to region-concentrated merging, and finally proposing a learnable video token merging (VTM) algorithm that dynamically merges tokens based on their saliency. Extensive experimental results show that we achieve better or comparable performances on the LVU, COIN, and Breakfast datasets. Moreover, our approach significantly reduces memory costs by ${\\bf{84\\%}}$ and boosts throughput by approximately 6.89 times compared to baseline algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Over the past few years, the Transformer architecture (Vaswani et al., 2017) has risen as a revolutionary paradigm within natural language processing (NLP) (Devlin et al., 2018) and has seamlessly expanded its influence into the domain of computer vision (Wang & Torresani, 2022; Bertasius et al., 2021; Wang et al., 2022; Akbari et al., 2021; Li et al., 2022; Fan et al., 2021). This expansion has been exemplified by remarkable achievements in recent multi-modality foundation models such as Sora (Brooks et al., 2024), GPT4 (Achiam et al., 2023), and Gemini (gem), showcasing its exceptional performance and versatility across diverse applications. ", "page_idx": 0}, {"type": "text", "text": "In contrast to the natural language processing, the visual input has much lower information density and thus tokenizing the raw RGB image as non-overlapped patches becomes the essential operation in vision transformers (Wang & Torresani, 2022; Bertasius et al., 2021; Dosovitskiy et al., 2021). However, the computational cost of transformer exponentially increases with the sequence length, which generates tremendous computation when feeding visual input into the large-scaled transformer models with billions of parameters. Due to the redundancy in video sequence, this phenomenon becomes more severe with video input, especially for long-form videos. This bottleneck impedes the further advancement of foundational models in handling long-form video data. Various attempts (Yin et al., 2021; Wang et al., 2021; Meng et al., 2022; Rao et al., 2021; Liang et al., 2022) have been proposed to improve the efficiency of vision transformer by introducing a token selection module. However, these methods are primarily designed for images and may require non-trivial adaptation to the long-form video scenarios due to the video-level long-term dependencies and motion dynamics. Moreover, tokens dropped by the token selection module cannot be reused in later layers, which may result in the loss of important information. ", "page_idx": 0}, {"type": "image", "img_path": "wduRaBDRBS/tmp/5f6293202d397ccd479410088084217f7ef01b58a998133ce185cac12795fb9a.jpg", "img_caption": ["Figure 1: Comparison of GPU memory footprint and throughput against scene prediction accuracy on the LVU dataset (Wu & Kr\u00e4henb\u00fchl, 2021). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In addition to the token selection, token merging techniques (Bolya et al., 2022; Ren et al., 2023; Bolya & Hoffman, 2023; Li et al., 2024) have been proposed to increase the efficiency and effectiveness of transformer-based networks. Specifically, they reduce the sequence length by merging similar tokens, thereby decreasing the computational cost. In addition to the efficiency, token merging demonstrates huge advantages by increasing the contextual information so that the model can learn from patterns presented across multiple tokens. Previous token merging algorithms in both the image and video domains use manually designed token partitioning methods and merge tokens based on their similarity. Even though the merged tokens would still keep the original information, they may have different granularity after the merging operation. In this paper, it is argued that different regions in the visual data may have different information density. Since the discriminative information of tokens may be degenerated after merging, some visual tokens should not be merged even if they look similar to each other. Rather than relying solely on similarity, we question whether more unmerged tokens should be used to describe salient areas, while merging more tokens for the background. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we explore various video token merging (VTM) methods in long-form video classification task and aim to find the effective token merging method for long-form videos. Previous video token merging method (Li et al., 2024) only decouples the spatial and temporal dimensions, which is unfavorable, especially for long-form videos. As the long-term dependency plays an important role in the long-form video understanding, spatiotemporal visual tokens should be considered jointly. Sequentially merging token along with one dimension after another may generate biased prior. In our work, we first naively extend the image-based token merging (Bolya et al., 2022) to the video domain and then propose region-centralized and motion-based token merging algorithms, which estimate the salient region of video sequences. Finally, we develop a learnable VTM which predicts the saliency score of each token and adaptively merges spatiotemporal visual tokens in data-driven manner. Experimental results demonstrate that the proposed algorithm improves the effectiveness and the efficiency of the transformer-based network and outperforms the conventional long-video understanding methods with better throughput and less memory usage, as also shown in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "We summarize the contributions of this paper as following: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We explore various video token merging methods including the na\u00efve VTM, the regionconcentrated VTM, and the motion-based VTM.   \n\u2022 We propose the learnable video token merging algorithm, which estimates the saliency score of each token and adaptively merges visual tokens based on their scores.   \n\u2022 The proposed algorithm achieves the best or competitive results on various datasets including LVU, Breakfast and COIN. Moreover, we significantly reduce memory costs by $84\\%$ and improve the throughput by 6.89 times via the proposed learnable VTM. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Long-form Video Modeling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Transformers have demonstrated remarkable prowess in capturing long-term dependencies, as evidenced in their success in natural language processing (NLP) tasks (Brown et al., 2020; Dai et al., 2019; Devlin et al., 2018). However, the intensive computational requirements stemming from dense self-attention calculations (Vaswani et al., 2017) pose a significant obstacle not only in NLP but also in the domain of computer vision, especially for the long-form videos. Many recent video transformer works (Wang & Torresani, 2022; Liu et al., 2021; Bertasius et al., 2021) focuses on improving the global attention mechanism. However, they are not designed for dealing with redundant spatial and temporal image tokens that are common in long-form video scenarios. To capture longer temporal information, LF-VILA (Sun et al., 2022) develops a hierarchical architecture to include more frames in the model. Similarly, MeMViT (Wu et al., 2022) utilizes longer temporal information by emerging the previously cached \u201cmemory\" from the past. A novel alternative to transformers is the Structured State-Space Sequence (S4) model proposed by Gu et al. (2021), which models the long-range dependencies by simulating a linear time invariant (LTI) system. Subsequently, ViS4mer (Islam & Bertasius, 2022) and S5 (Wang et al., 2023) extend S4 model to the long-form video classification task. ViS4mer (Islam & Bertasius, 2022) stacks multiple S4 layers with different scales in modeling long-form videos, and S5 (Wang et al., 2023) include an additional selective module to further improve the performance. Unlike these works that focus on the improvement of architecture and attention mechanism, this paper will start from a more basic concept in the transformer, video tokens and how to effectively merge them. Even though our proposed method can theoretically be applied on S4 model, the scope of this paper is on the well established transformer architecture. We will leave the investigation of video token merging on S4 (Gu et al., 2021) in the future work. ", "page_idx": 2}, {"type": "text", "text": "2.2 Adaptive Token Selection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Adaptive token selection is widely used to improve model efficiency by leveraging a light-weight selection module to pick up the \u2018useful\u2019 tokens while dropping the \u2018unuseful\u2019 ones. In vision transformer, STTS (Wang et al., 2021) utilizes a token selection module known as the named scorer network to assign importance scores to each token, subsequently selecting the top-K frames with the highest scores. Building upon this concept, AdaViT (Meng et al., 2022) further extends the approach by developing instance-specific policies. These policies guide the activation of patches, self-attention heads, and transformer blocks, enhancing adaptability and efficiency in processing visual data. STTS, AdaVit and other similar approaches (Wang et al., 2021; Meng et al., 2022; Rao et al., 2021; Liang et al., 2022) drop a significant number of tokens in the early decision stage to save more cost, but the dropped tokens cannot be reused in the later layers, which is easier to degenerate the contextual information in the long-form videos. ", "page_idx": 2}, {"type": "text", "text": "2.3 Token Merging ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Visual token merging is first proposed in (Bolya et al., 2022) which aims at increasing the throughput of existing ViT models without training. Following works (Ren et al., 2023; Bolya & Hoffman, 2023; Li et al., 2024) leverage this idea to save computational cost in different downstream applications, such as diffusion model, video and language understanding, and video editing. Specifically, visual tokens are first partitioned into two sets with equal size; for each of the edge tokens in one set, find the most similar token in another set and merge them by average pooling; finally, concatenate two sets back together. Although the token merging is simple and effective, its applications have mostly remained in the image domain. There is no fundamental research work has been explored for the long-form video token merging strategies, where the spatiotemporal tokens are more redundant and embed complicated dependencies locally and globally. In this work, we argue that visual tokens from the long-form video should be carefully partitioned and merged based on the salient areas in videos. To this end, we ablate various video token merging algorithms and provide extensive expermental results and analysis. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary \u2013 Token Merging ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Token merging (Bolya et al., 2022) aims to reduce the redundancy by merging similar tokens at each transformer block, thereby increasing the effectiveness and efficiency of a transformer-based network. Specifically, token merging has three steps: partitioning, matching, and merging. ", "page_idx": 2}, {"type": "text", "text": "Partitioning: For given a set of $N$ tokens $\\mathcal{X}=\\{x_{1},x_{2},\\ldots,x_{N}\\}$ , token merging first partition $\\mathcal{X}$ into a set of target tokens $\\tau$ and a set of source tokens $\\boldsymbol{S}$ , given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{T}}&{=}&{\\{x_{i}:i\\;\\mathrm{mod}\\;\\gamma=0\\},}\\\\ {\\mathcal{S}}&{=}&{\\{x_{j}:j\\;\\mathrm{mod}\\;\\gamma\\ne0\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\gamma$ is partition factor and mod denotes the modulo operator. Thus, $\\begin{array}{r}{|\\mathcal{T}|=\\frac{|\\mathcal{X}|}{\\gamma}}\\end{array}$ . Also, $\\mathcal{X}=\\mathcal{T}\\cup\\mathcal{S}$ and $\\mathcal{T}\\cap\\mathcal{S}=\\mathcal{O}$ . ", "page_idx": 3}, {"type": "text", "text": "Matching: Then, for each source token in $\\boldsymbol{S}$ , it finds the most similar target token in $\\tau$ . Here, the similarity between tokens $x_{i}$ and $x_{j}$ is defined as the cosine similarity of the corresponding key vectors $k_{i}$ and $k_{j}$ , which are obtained in the most recent self-attention layer. For a source token $x_{j}\\in S$ , the index of its matched target token $m_{j}$ is obtained by, ", "page_idx": 3}, {"type": "equation", "text": "$$\nm_{j}=\\underset{i:\\left\\{i:x_{i}\\in T\\right\\}}{\\mathrm{argmax}}\\ \\frac{k_{i}^{t}k_{j}}{|k_{i}||k_{j}|}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Merging: Lastly, token merging merges the tokens based on the matching results. For each target token $x_{i}\\in\\mathcal{T}$ , it obtains the merged token $y_{i}$ by using average pooling, ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{i}=\\frac{x_{i}+\\sum_{j\\in\\mathcal{M}_{i}}x_{j}}{1+|\\mathcal{M}_{i}|}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{M}_{i}=\\{j:m_{j}=i,\\forall x_{j}\\in S\\}$ is the index set of source tokens which are matched with $x_{i}$ . In this case, the number of tokens is reduced by $|{\\mathcal{S}}|$ after token merging. It can also control the number of reduced tokens by $R$ by reassigning the matching index as ", "page_idx": 3}, {"type": "equation", "text": "$$\nm_{j}=-1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all $x_{j}$ except for the source tokens with the $R$ highest similarity scores. ", "page_idx": 3}, {"type": "text", "text": "3.2 Problem Definition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Suppose that a video with $L$ frames is given, where $L\\,\\geq\\,60$ . To perform the classification or regression on the given video, we can use a simple transformer-based network, which is shown in Figure 2 (a) and (b). It first obtains the tokens $\\dot{X}_{1},X_{2},\\allowbreak\\dots,X_{L}\\in\\mathbb{R}^{H\\times W\\times C}$ by using an encoder. Here, $H,W_{\\times}$ , and $C$ denote the height, the width, and the channel dimension of the token tensor, respectively. Then, it utilizes transformer blocks to update the tokens. As its input, $i$ -th transformer block takes the tokens corresponding to $L_{i}$ frames without overlapping, where $L_{i}\\leq L_{j}\\leq L$ for $i<j$ . The prediction head yields the estimation results. However, it requires the prohibitively large memory and computation costs due to the quadratic complexity of the self-attention $\\mathcal{O}(L^{2}H^{2}\\dot{W}^{2}D^{\\frac{\\nu}{2}})$ , where $D$ is the dimension of key vectors. Hence, our goal is to increase the efficiency of this baseline network by reducing the number of tokens via token merging, while maintaining or even improving the performances of the network by removing the redundant or noisy information in the video. To this end, we explore various token merging methods for long video processing. ", "page_idx": 3}, {"type": "text", "text": "3.3 Video Token Merging \u2013 Exploration ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Na\u00efve video token merging: First, we combine the standard token merging with the baseline network as intact as possible. To this end, we substitute transformer blocks with VTM blocks, in which token merging layer is inserted after the dropout layer, as illustrated in Figure 2 (c). In this na\u00efve VTM, we employ the standard token merging in (1)-(4). At each $i$ -th VTM block, na\u00efve VTM reduces the $R_{i}$ tokens. For example, at $\\gamma=4$ and $R=|S|$ , it gradually removes the $68\\%$ of tokens over the network. Hence, the computation cost of the self-attention is reduced from $\\mathcal{O}(N^{2}D^{2})$ to $\\mathcal{O}((\\frac{N}{\\gamma^{i-1}})^{2}D^{2})$ at $i$ -th VTM block. As shown in Table 1, this na\u00efve VTM shows better scores than the baseline network, because the token merging reduces the information redundancy in the videos. ", "page_idx": 3}, {"type": "text", "text": "Region-concentrated video token merging: Compared to an image, a video contains redundant spatiotemporal tokens. Depending on the tasks, some tokens are more important than others. However, as shown in Figure 3 (a), na\u00efve VTM selects every $\\gamma$ -th token as the target tokens since it exploits uniform token partitioning in (1). Also, for token merging, it purely relies on the similarity between tokens regardless of the semantics, and thus the self-attention can more easily swayed by unnecessary information. Therefore, for better video token merging, it is important to consider the saliency of each token before merging them. ", "page_idx": 3}, {"type": "image", "img_path": "wduRaBDRBS/tmp/e19222f6ee142b847f2d7e7298eb16cb9a7b47c45ef02abefd821caf7367d7b9.jpg", "img_caption": ["Figure 2: The architectures of (a) the baseline network, (b) the transformer block, and (c) the video token merging block. "], "img_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "wduRaBDRBS/tmp/cf2769ee1afca303ac3cd2d6a7320102adacf9aec31aee69e547b3663783b62d.jpg", "table_caption": ["Table 1: Comparison of different VTM methods on the LVU dataset. The best results are boldfaced and the second-best ones are underlined. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To investigate this issue, we explore center-concentrated video token merging and boundarytecono tndirceees cntartirrbagteee t cdte ovnktieednre soa f rtreooa kmae ntnhd  em mceeerrnggtieen rg m.a roerTaeh  tewo ictkhee nntt hfeerr -oscimzo ent hcoeef $\\textstyle{\\frac{H}{2}}\\times{\\frac{W}{2}}$ ,oy .kw eShnpi cemhc ieufrsigceiasnl lgmy , osraweem  upunlsemes $50\\%$ a trootfikt iteohnnes factor of $\\scriptstyle{\\frac{\\gamma}{2}}$ for the center area and $\\textstyle{\\frac{3}{2}}\\gamma$ for the remaining area. On the other side, we implement the opposite operation for the boundary-concentrated video token merging. As shown in Table 1, center-concentrated VTM shows better performances than na\u00efve and boundary-concentrated VTM in general. Since meaningful objects and motions are typically center-concentrated, this suggests more tokens from salient regions should be unmerged while more of the rest tokens should be merged. Figure 3 (b) shows the partitioning results of center-concentrated VTM. ", "page_idx": 4}, {"type": "text", "text": "Motion-based video token merging: Even though center-concentrated VTM has shown better performances than na\u00efve VTM, the meaningful tokens are not always located in the center area. Moreover, the hand-crafted partitioning method forces the center-concentrated VTM to select the same number of target tokens from each frame, which is not flexible enough when applied at scale. Therefore, we explore motion-based video token merging which divides tokens into $\\tau$ and $\\boldsymbol{S}$ based on the motion information since the moving objects carry important cues in general (Fan et al., 2023). ", "page_idx": 4}, {"type": "text", "text": "Let us assume that we have the magnitude of motion vector $v_{i}$ for each token $x_{i}$ . We compute the sampling probability by using softmax ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{i}=\\frac{e^{v_{i}}}{\\sum_{j=1}^{N}e^{v_{j}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "wduRaBDRBS/tmp/56b042d358d762a8ca25948631dfe1757b488e3e85cb4f5e34590732860b2345.jpg", "img_caption": ["Figure 3: Visualizations of target tokens of different VTM methods: (a) na\u00efve VTM, (b) centerconcentrated VTM, (c) motion-based VTM, and (d) learnable VTM. In (d), learnable VTM selects the target tokens around salient objects rather than backgrounds. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Note that the sampling probability is proportional to the motion magnitude. Then, we construct $\\tau$ by sampling $\\frac{N}{\\gamma}$ tokens from $\\mathcal{X}$ with the sampling probability $p_{i}$ for each token $x_{i}$ . ", "page_idx": 5}, {"type": "text", "text": "The goal of VTM is to increase the efficiency of transformer-based network for long video understanding. Therefore, the motion information should be obtained with negligible time and computation costs. Hence, instead of estimating the motion information with an additional module, we use the motion information which is already stored in the video files; most modern video codecs, such as MPEG-4 (Richardson, 2004), H.264 (Richardson, 2004), and HEVC (Wien, 2015), exploit the motion information for efficient compression. The motion decoding takes only 0.3 milliseconds for each frame which is negligible. Figure 3 (c) shows the token partitioning examples of motion-based VTM. ", "page_idx": 5}, {"type": "text", "text": "3.4 Learnable Video Token Merging ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "There are some videos in which unimportant objects have large motions due to various factors such as camera movement. Motion-based VTM may not yield good results on those videos. To maximize the generalizability, we develop learnable video token merging method. Instead of depending on the motion information, learnable VTM estimates the saliency score of each token and samples the target tokens based on the estimated scores. Figure 4 shows the architecture of learnable VTM block. ", "page_idx": 5}, {"type": "text", "text": "Learnable VTM block contains two forward paths: a main path and an auxiliary path. Let us assume that we have a tensor of $N$ tokens $X\\in\\mathbb{R}^{N^{\\star}\\times C}$ . In the main path, we first obtain query $Q$ , key $K$ and value $V$ by ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ=X U_{q},\\qquad K=X U_{k},\\qquad V=X U_{v}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "using learnable projection matrices $U_{q},U_{k},U_{v}\\in\\mathbb{R}^{C\\times D}$ . We perform the standard self-attention on $Q,K$ , and $V$ and yield the updated tokens $X^{\\prime}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\nX^{\\prime}={\\frac{\\operatorname{softmax}(Q K^{\\top})}{\\sqrt{D}}}V.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Also, from $K$ , we estimate the saliency scores $S$ of tokens by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boldsymbol{S}=[s_{1},s_{2},\\ldots,s_{N}]^{\\top}=\\operatorname{tanh}(K U_{s})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $U_{s}\\in\\mathbb{R}^{D\\times1}$ is a learnable matrix. Also, $s_{i}\\in(-1,1)$ for $1\\leq i\\leq N$ . Then, we compute the sampling probability using (6) with $s_{i}$ instead of $v_{i}$ for each token $x_{i}$ and sample $T$ with the sampling probability as in motion-based VTM. After the token partitioning, we match and merge the tokens in $X^{\\prime}$ by using (3) and (4), respectively. ", "page_idx": 5}, {"type": "text", "text": "However, the learnable matrix $U_{s}$ in (9) can not be trained only with the main path since the partitioning process is not differentiable. To handle this issue, we employ the auxiliary path. This path consists of a saliency guided self-attention layer and a merging operation. The auxiliary path takes a tensor of auxiliary tokens $X_{\\mathrm{aux}}\\in\\mathbb{R}^{N\\times C}$ as its input. Similar to the main path, we obtain query $Q_{\\mathrm{aux}}$ , key $K_{\\mathrm{aux}}$ , and value $V_{\\mathrm{aux}}$ . Then, we perform the saliency guided attention to obtain the updated auxiliary tokens $X_{\\mathrm{aux}}^{\\prime}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\nX_{\\mathrm{aux}}^{\\prime}={\\frac{\\mathrm{softmax}(Q_{\\mathrm{aux}}K_{\\mathrm{aux}}^{\\top}+{\\bf1}S^{\\top})}{\\sqrt{C}}}V_{\\mathrm{aux}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "wduRaBDRBS/tmp/80843b8fc697f886f5b867e60be4164794a19715c474138ef9d7d0bb6a8760c5.jpg", "img_caption": ["Figure 4: An overview of the learnable video token merging block. The auxiliary path is used during training only. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "wduRaBDRBS/tmp/86184c494f9bf20dea3b5ba6a73a610b9860c9bac8e9e4afb749700c4905f74a.jpg", "table_caption": ["Table 2: Comparison of the proposed learnable VTM algorithm with conventional algorithms on the LVU dataset. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "where 1 is a $N$ dimensional vector of ones. In the saliency guided attention, the contribution of each token is controlled by its estimated saliency score; if $s_{i}$ is positive, $i$ -th token affects more on $X_{\\mathrm{aux}}^{\\prime}$ whereas if $s_{i}$ is negative it has less influence on $X_{\\mathrm{aux}}^{\\prime}$ . In other words, it increases the influences of the tokens with high saliency scores in the attention process. Therefore, during training, the network is encouraged to assign high saliency scores to the tokens with meaningful information and low saliency scores to the others to obtain the better predictions. At the first VTM block, the auxiliary path employs the same input with the main path. From the second VTM block, it takes the output of auxiliary path in the previous VTM block as its input. ", "page_idx": 6}, {"type": "text", "text": "Also, it is worth pointing out that the auxiliary path is used for the network training only. Compared to other VTM methods, learnable VTM only introduces additional computation of score estimation module, which is fast and light enough, during test. Therefore, it shows almost same inference speed with other VTM methods. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "LVU (Wu & Kr\u00e4henb\u00fchl, 2021): It contains ${\\sim}30\\mathrm{K}$ videos sampled from ${\\sim}3\\mathrm{K}$ movies on the MovieClips (mov) website. Most videos are 1 to 3 minutes long. It provides the labels for 9 long-video understanding tasks which can be grouped into three major categories: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Content understanding: \u2018relationship,\u2019 \u2018speaking style,\u2019 and \u2018scene/place\u2019 \u2022 Metadata prediction: \u2018director,\u2019 \u2018genre,\u2019 \u2018writer,\u2019 and \u2018movie release year\u2019 \u2022 User engagement: \u2018YouTube like ratio,\u2019 and \u2018YouTube popularity ", "page_idx": 6}, {"type": "text", "text": "As the evaluation metrics, we adopt the top 1 classification accuracy for content understanding and metadata prediction tasks and mean-squared error (MSE) for user engagement tasks. ", "page_idx": 6}, {"type": "text", "text": "Breakfast (Kuehne et al., 2014): It provides 1,712 videos with the average length of 2.32 minutes and the total length of 77 hours. The videos contain 52 individuals and 18 different backgrounds in total. Each video belongs to one of 10 complex cooking activities. ", "page_idx": 6}, {"type": "text", "text": "COIN (Tang et al., 2019): It consists of 11,827 videos with the average length of 2.36 minutes, collected from YouTube. Each video belongs to one of 180 distinct procedural tasks. ", "page_idx": 6}, {"type": "table", "img_path": "wduRaBDRBS/tmp/336f0843f4a5c49ae3837e8abebb4d6e4569d5dc58502470134a2ba2b4c328a8.jpg", "table_caption": ["Table 3: Comparison on the Breakfast dataset. PT stands for pretraining. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "wduRaBDRBS/tmp/45e6a0ed71b4480a1c091c3a2471bd8c5c6535c627e2bfa9e53ba31b05660685.jpg", "table_caption": ["Table 4: Comparison with the state-of-the-art methods on the COIN dataset. PT stands for pretraining. Here, $^*$ means the reproduction results with the official codes. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We follow the experimental settings of conventional long-form video understanding algorithms (Islam & Bertasius, 2022; Wang et al., 2023). We employ three transformer blocks in the baseline network. As the encoder, we use ViT-L (Dosovitskiy et al., 2021) pretrained on ImageNet-21K (Ridnik et al., 2021) on the LVU (Wu & Kr\u00e4henb\u00fchl, 2021) dataset and employ Swin-B (Liu et al., 2021) pretrained on Kinetics-600 (Kay et al., 2017) on the Breakfast (Kuehne et al., 2014) and COIN (Tang et al., 2019) datasets. Images are resized to $224\\times224$ for the feature extraction. Hence, $H=W=16$ and $C=1024$ for the LVU dataset and $H=W=7$ and $C=1024$ for the Breakfast and COIN datasets. The size of the length of input video for each dataset is also same with (Islam & Bertasius, 2022; Wang et al., 2023): we use 60 frames for the LVU dataset and 64 frames for the Breakfast and COIN datasets. We use the AdamW (Loshchilov & Hutter, 2017) optimizer with a batch size of 16 and a weight decay of 0.01. We set the learning rate to 0.001. We train the network for 70 epochs by using cosine learning rate scheduler (Gotmare et al., 2018) with 10 epochs warm-up. For experiments, we use 8 Tesla V100 GPUs and PyTorch. ", "page_idx": 7}, {"type": "text", "text": "4.3 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Comparison on LVU: In Table 2, we compare the proposed algorithm with the conventional methods on the LVU dataset. With the smallest memory footprint, the proposed algorithm achieves the best scores in 7 out of 9 tasks on the LVU dataset. Performer (Choromanski et al., 2021) and Orthoformer (Patrick et al., 2021) employ the efficient variants of self-attention to reduce the computation costs. The proposed learnable VTM outperforms these approaches with significant performance margins and less GPU memory usages. It shows the efficiency and effectiveness of our approach. Also, ViS4mer (Islam & Bertasius, 2022) and S5 (Wang et al., 2023) adopt S4 layers instead of self-attention layers to capture the long-term dependencies in videos because of its linear computation complexity to the number of input tokens. The promising results of these methods have suggested that S4 layer can be an efficient replacement of self-attention layer for long-form video inputs. However, the higher scores of the proposed algorithm broaden the potential usages of the self-attention layers for various long-form video tasks. Moreover, S5 utilizes LSMCL, which is a pretraining based on the contrastive learning, to boost its performances. Nevertheless, without the time-consuming pretraining, the proposed algorithm yields better scores on the LVU dataset. ", "page_idx": 7}, {"type": "table", "img_path": "wduRaBDRBS/tmp/23715e9c64ea24a2b1984dbd27bc35be69bb2379975ffbcb7197086c1a1299ef.jpg", "table_caption": ["Table 5: Comparison of long-video understanding results on the LVU dataset according to $\\gamma$ . "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "wduRaBDRBS/tmp/6823adb1b407335c5ffb5fc60cc8507a9c82afcf31b4f2776288b29a97635d15.jpg", "table_caption": ["Table 6: Comparison of long-video understanding results on the LVU dataset according to $(\\bar{L_{1}},L_{2},L_{3})$ . "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "wduRaBDRBS/tmp/c5bdc4bec70b573f433a49c8517d28e29fd717727e6fe51d0f7e88b5a17d33ec.jpg", "img_caption": ["Figure 5: Visualizations of video token merging results on the LVU dataset. Patches with same inner and border color are merged together. The tokens corresponding to the backgrounds are merged together, thereby increasing the influence of salient tokens in the attention process. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Comparison on Breakfast: Table 3 compares the performances of the proposed algorithm and the conventional techniques on the Breakfast dataset. The proposed learnable VTM achieves the best score on this challenging long-range activity classification dataset as well. For pretraining, D-sprv (Lin et al., 2022) leverages the HowTo100M (Miech et al., 2019) dataset which contains much more training samples than our pretraining dataset, Kinetics-600 (Carreira et al., 2018). Nevertheless, we outperform D-sprv (Lin et al., 2022) with the accuracy gap of $1.36\\%$ . ", "page_idx": 8}, {"type": "text", "text": "Comparison on COIN: Table 4 compares the scores of the proposed algorithm and the conventional techniques on the COIN dataset. We note that the COIN dataset consists of the videos on YouTube and more than 1,000 videos are not available anymore. Therefore, ViS4mer, which is one of the state-of-the-art method on the COIN dataset, achieves only $87.11\\%$ accuracy when it is trained on the current version of the COIN dataset. It may be because of many missing training videos. Even though the comparison is not perfectly fair, we report the results on the COIN dataset for reference. The proposed learnable VTM yields better results than ViS4mer with the same training and test data. Also, it shows the comparable score with S5 (Wang et al., 2023). ", "page_idx": 8}, {"type": "text", "text": "4.4 Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Analysis on $\\gamma\\colon$ Table 5 compares the performances of learnable VTM with different $\\gamma$ . Compared to the baseline, at all $\\gamma$ , the proposed algorithm improves the performances. Also, at $\\gamma=10$ , it increase the throughput and the memory efficiency by 7.49 and 6.6 times, respectively. At $\\gamma=6$ , the proposed algorithm shows the best results. ", "page_idx": 8}, {"type": "table", "img_path": "wduRaBDRBS/tmp/c7b14376714cc568a15ceeebe6c4c754995565f5590562cbe2702f8018b934c0.jpg", "table_caption": ["Table 7: Comparison of long-video understanding results of various VTM designs on the LVU dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "wduRaBDRBS/tmp/05588538100bb2334bbeb69b9851e75a4d1b442de3a340ba8c772c1b30e69b10.jpg", "table_caption": ["Table 8: Comparison of throughput and memory footprint of learnable VTM for training and inference. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Analysis on $(L_{1},L_{2},L_{3})$ : Table 6 shows the results of the proposed learnable VTM with different $(L_{1},L_{2},L_{3})$ . Note that $i$ -th VTM block takes the tokens in $L_{i}$ consecutive frames as its input at a time. At $(L_{1},L_{2},L_{3})=(4,20,60)$ , the proposed algorithm yields the lowest scores, since it can not capture the long temporal dependency in the early stage of the network. We see that the proposed algorithm yields the best scores at $(L_{1}^{\\cdot},L_{2},L_{3})=\\bar{(6,30,60)}$ . ", "page_idx": 9}, {"type": "text", "text": "Weighted average pooling: To merge tokens, we use average pooling as the default setting in all VTM methods. However, once tokens are merged, they may represent more than one input patch. Thus, to reflect the token size in merging process, we combine tokens by averaging weighted by their sizes. However, as shown in Table 7, this weighted average pooling decrease the performances of learnable VTM. Thus, we exploit the average pooling to merge tokens. ", "page_idx": 9}, {"type": "text", "text": "Motion weighted average pooling: In motion-based VTM, we can combine tokens by averaging weighted by their motion magnitudes. Table 7 shows the performances of motion-based VTM with the motion weighted average pooling. It yields the similar scores with the standard motion-based VTM with the average pooling. ", "page_idx": 9}, {"type": "text", "text": "Complexity: Table 8 compares the throughput and memory footprint of learnable VTM during training and inference. Since the auxiliary path is additionally employed during training, it requires more computation costs. However, even during training, learnable VTM is still faster than conventional methods including ViS4mer (Islam & Bertasius, 2022) and S5 (Wang et al., 2023) and it also requires less amounts of memory than them. ", "page_idx": 9}, {"type": "text", "text": "Visualizations: In Figure 7, we visualize the tokens merging results at the end of the network over multiple frames of video. We see that tokens with similar semantics are merged together. Also, tokens corresponding to backgrounds or unnecessary informations are merged more than tokens corresponding to salient objects. It is because learnable VTM selects tokens with high saliency scores as the target tokens. More visualization results are available in the supplemental document. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigate the video token merging techniques for long-form video data. Unlike previous algorithms that apply uniform partitioning and merge tokens solely based on the visual similarity, we argue tokens with different saliencies should be treated unequally to avoid undesirable information loss after merging important tokens. To this end, we explore various video token merging methods and receive interesting intuitions from region-concentrated and motion-based token merging results. Lastly, we propose a learnable video token merging scheme that adaptively samples target tokens and learns discriminative representations from the long-form videos. Compared to the baseline, the proposed algorithm achieves substantial improvements in terms of the performance, memory cost and throughput. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. URL https: //storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf. ", "page_idx": 10}, {"type": "text", "text": "MovieClips. URL https://www.movieclips.com/. ", "page_idx": 10}, {"type": "text", "text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv, 2023.   \nHassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. VATT: Transformers for multimodal self-supervised learning from raw video, audio and text. arXiv, 2021.   \nGedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, 2021.   \nDaniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In CVPR Workshop, 2023.   \nDaniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token Merging: Your ViT but faster. In ICLR, 2022.   \nTim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.   \nJoao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. arXiv, 2018.   \nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In ICLR, 2021.   \nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. arXiv, 2019.   \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv, 2018.   \nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.   \nDavid Fan, Jue Wang, Shuai Liao, Yi Zhu, Vimal Bhat, Hector Santos-Villalobos, Rohith MV, and Xinyu Li. Motion-guided masking for spatiotemporal representation learning. In ICCV, 2023.   \nHaoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Int. Conf. Comput. Vis., 2021.   \nAkhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. In arXiv, 2018.   \nAlbert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021.   \nNoureldien Hussein, Efstratios Gavves, and Arnold WM. Smeulders. Timeception for complex action recognition. In CVPR, 2019a.   \nNoureldien Hussein, Efstratios Gavves, and Arnold WM. Smeulders. VideoGraph: Recognizing minutes-long human activities in videos. In ICCV Workshop, 2019b.   \nMd Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In ECCV, 2022.   \nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. In arXiv, 2017.   \nHilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntax and semantics of goal-directed human activities. In CVPR, 2014.   \nXirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. VidToMe: Video token merging for zero-shot video editing. In CVPR, 2024.   \nYanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. MViTv2: Improved multiscale vision transformers for classification and detection. In CVPR, 2022.   \nYouwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. arXiv preprint arXiv:2202.07800, 2022.   \nXudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, and Lorenzo Torresani. Learning to recognize procedural activities with distant supervision. In CVPR, 2022.   \nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.   \nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In arXiv, 2017.   \nLingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. AdaViT: Adaptive vision transformers for efficient image recognition. In CVPR, 2022.   \nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, pp. 2630\u20132640, 2019.   \nMandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Joao F. Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. In NeurIPS, 2021.   \nYongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. DynamicViT: Efficient vision transformers with dynamic token sparsification. In NeurIPS, 2021.   \nShuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, and Lu Hou. TESTA: Temporal-spatial token aggregation for long-form video-language understanding. In EMNLP, 2023.   \nIain E. Richardson. H. 264 and MPEG-4 video compression: video coding for next-generation multimedia. 2004.   \nTal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. ImageNet-21K pretraining for the masses. In NeurIPS, 2021.   \nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. VideoBERT: A joint model for video and language representation learning. In ICCV, 2019.   \nYuchong Sun, Bei Liu, Hongwei Xue, Ruihua Sone, Huan Yang, and Jianlong Fu. Long-form video-language pre-training with multimodal temporal contrastive learning. In NeurIPS, 2022.   \nYansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. COIN: A large-scale dataset for comprehensive instructional video analysis. In CVPR, 2019.   \nYansong Tang, Jiwen Lu, and Jie Zhou. Comprehensive instructional video analysis: The COIN dataset and performance evaluation. IEEE TPAMI, 43(9):3138\u20133153, 2020.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.   \nJue Wang and Lorenzo Torresani. Deformable video transformer. In CVPR, 2022.   \nJue Wang, Gedas Bertasius, Du Tran, and Lorenzo Torresani. Long-short temporal contrastive learning of video transformers. In CVPR, 2022.   \nJue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In CVPR, 2023.   \nJunke Wang, Xitong Yang, Hengduo Li, Zuxuan Wu, and Yu-Gang Jiang. Efficient video transformers with spatial-temporal token selection. arXiv preprint arXiv:2111.11591, 2021.   \nMathias Wien. High efficiency video coding. Coding Tools and Specification, 24, 2015.   \nChao-Yuan Wu and Philipp Kr\u00e4henb\u00fchl. Towards long-form video understanding. In CVPR, 2021.   \nChao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. MemViT: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In CVPR, 2022.   \nHongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. Adavit: Adaptive tokens for efficient vision transformer. arXiv preprint arXiv:2112.07658, 2021.   \nJiaming Zhou, Kun-Yu Lin, Haoxin Li, and Wei-Shi Zheng. Graph-based high-order relation modeling for long-term action recognition. In CVPR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A More Implementation Detail ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Network Architecture ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Figure 6 illustrates the detailed architecture of the proposed learnable VTM. For all datasets, the encoder extracts the tokens with 1024 channel dimension. We note that the linear layer in each VTM block reduces the channel dimension into half of it. Hence, after third VTM block, each token has 256 channel dimension. Also, the auxiliary path is only used for network training. ", "page_idx": 13}, {"type": "image", "img_path": "wduRaBDRBS/tmp/9b52b87ca01be941010f6592a5556feedc2684c5a12d83d8c93bde6b559ac78b.jpg", "img_caption": ["Figure 6: A network architecture of the proposed learnable VTM. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B More Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Analysis on $R$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 9 compares the results of the proposed algorithm at different $R$ . Note that $R$ denotes the number of merged tokens at each VTM block. At $R=|S|$ , we merge all source tokens with target tokens. However, there may exist some source tokens which does not have target tokens with similar semantics. Thus, at $R=|{\\mathcal{S}}|$ , undesirable merging of tokens can happen, thereby decreasing the performances. On the other hand, at $R=0.5|S|$ , only the half of source tokens are merged with target tokens, and thus some source tokens may not be merged even though they have similar target tokens. Therefore, the proposed algorithm shows the best scores at $R=0.8|S|$ . ", "page_idx": 13}, {"type": "text", "text": "Table 9: Comparison of long-video understanding results on the LVU dataset according to $R$ . ", "page_idx": 13}, {"type": "table", "img_path": "wduRaBDRBS/tmp/ea796fafafd702d2cd635b5afac9e8ff2386055681877a07382fe889fbfb9865.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C More Visualizations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 7 visualizes the token merging results of the proposed learnable VTM on the LVU dataset. ", "page_idx": 14}, {"type": "image", "img_path": "wduRaBDRBS/tmp/957ded274cb6b3d265b850a4964e4d92c55a1f6b6d0b720221bf47dddd3e0b4f.jpg", "img_caption": ["Figure 7: Visualizations of video token merging results on the LVU dataset. Patches with same inner and border color are merged together. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: See L65-L71 in Section 1. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: See L280-287 in Section 4.3. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: NA ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: See Section 4.2 and Appendix A. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See Section 4. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Section 4.2 and Appendix ??. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: NA ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 18}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: NA ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should citep the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: NA ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]