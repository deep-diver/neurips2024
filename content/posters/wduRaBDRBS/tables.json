[{"figure_path": "wduRaBDRBS/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of different VTM methods on the LVU dataset. The best results are boldfaced and the second-best ones are underlined.", "description": "This table presents a comparison of various Video Token Merging (VTM) methods on the Large-scale Video Understanding (LVU) dataset.  It shows the performance of different VTM approaches across multiple video understanding tasks, including content understanding (relationship, speaking style, scene), metadata prediction (director, genre, writer, year), and user engagement (like, view). The best results for each task are highlighted in bold, while the second-best results are underlined, providing a clear visual representation of the comparative performance of each VTM method.  The baseline represents the performance without VTM.", "section": "3.3 Video Token Merging \u2013 Exploration"}, {"figure_path": "wduRaBDRBS/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of different VTM methods on the LVU dataset. The best results are boldfaced and the second-best ones are underlined.", "description": "This table compares the performance of different video token merging (VTM) methods on the Large-scale Video Understanding (LVU) dataset.  The methods include a baseline, na\u00efve VTM, boundary-concentrated VTM, center-concentrated VTM, motion-based VTM, and learnable VTM. The table presents the accuracy scores for nine different video understanding tasks, categorized into content understanding, meta-data prediction, and user engagement.  The best results for each task are highlighted in bold, and the second-best results are underlined. This allows for a direct comparison of the effectiveness of each VTM method in improving the accuracy of video understanding across various aspects.", "section": "3.3 Video Token Merging \u2013 Exploration"}, {"figure_path": "wduRaBDRBS/tables/tables_7_1.jpg", "caption": "Table 3: Comparison on the Breakfast dataset. PT stands for pretraining.", "description": "This table compares the performance of different algorithms on the Breakfast dataset, focusing on the accuracy achieved.  It highlights the impact of using different pre-training datasets (PT Dataset) and the number of pre-training samples (#PT Samples) on the final accuracy of the models.  The algorithms listed represent various state-of-the-art methods for video understanding. The Learnable VTM method, proposed in the paper, shows a high accuracy compared to other methods.", "section": "4 Experiments"}, {"figure_path": "wduRaBDRBS/tables/tables_7_2.jpg", "caption": "Table 4: Comparison with the state-of-the-art methods on the COIN dataset. PT stands for pretraining. Here, * means the reproduction results with the official codes.", "description": "This table compares the performance of the proposed 'Learnable VTM' algorithm against other state-of-the-art methods on the COIN dataset for video understanding.  It shows the algorithm used, the pre-training dataset used, the number of pre-training samples, and the achieved accuracy. The asterisk (*) indicates results reproduced using the official code, highlighting potential differences in implementation.", "section": "4 Experiments"}, {"figure_path": "wduRaBDRBS/tables/tables_8_1.jpg", "caption": "Table 5: Comparison of long-video understanding results on the LVU dataset according to \u03b3.", "description": "This table presents the performance comparison of the proposed learnable Video Token Merging (VTM) algorithm with different partition factors (\u03b3) on the LVU dataset.  It shows how the algorithm's accuracy on three specific tasks ('Scene', 'Director', 'Like') and its throughput (samples per second) and GPU memory usage vary as the partition factor is changed from \u03b3=2, 6, and 10. The baseline performance without VTM is also shown for comparison.", "section": "4.3 Experimental Results"}, {"figure_path": "wduRaBDRBS/tables/tables_8_2.jpg", "caption": "Table 6: Comparison of long-video understanding results on the LVU dataset according to (L1, L2, L3).", "description": "This table presents the results of the proposed learnable Video Token Merging (VTM) algorithm on the LVU dataset, showing the impact of varying the number of consecutive frames processed in each transformer block.  It compares the performance (Scene, Director, Like metrics), throughput, and GPU memory usage across three different configurations: (10, 30, 60), (6, 30, 60), and (4, 20, 60).  These numbers represent the number of frames processed sequentially in each of the three transformer blocks in the VTM.  The results show a trade-off between throughput and accuracy as the values of L1, L2, L3 are adjusted.", "section": "4.3 Experimental Results"}, {"figure_path": "wduRaBDRBS/tables/tables_9_1.jpg", "caption": "Table 7: Comparison of long-video understanding results of various VTM designs on the LVU dataset.", "description": "This table compares the performance of different video token merging (VTM) methods on the Long-form Video Understanding (LVU) dataset.  The methods compared include a weighted average approach, motion-based merging, a motion-weighted average, and a learnable VTM.  The results are presented for three specific tasks within the LVU dataset: scene classification, director prediction, and 'like' prediction (likely referring to user engagement metrics). The learnable VTM shows the best performance across all three tasks.", "section": "4.3 Experimental Results"}, {"figure_path": "wduRaBDRBS/tables/tables_9_2.jpg", "caption": "Table 8: Comparison of throughput and memory footprint of learnable VTM for training and inference.", "description": "This table compares the throughput and memory usage of the proposed \"Learnable VTM\" algorithm with two other state-of-the-art algorithms, ViS4mer and S5, during both training and inference phases.  It highlights the efficiency gains achieved by the proposed method in terms of both speed and memory consumption.", "section": "4 Experiments"}, {"figure_path": "wduRaBDRBS/tables/tables_13_1.jpg", "caption": "Table 9: Comparison of long-video understanding results on the LVU dataset according to R.", "description": "This table presents the results of the proposed algorithm with different values of R (the number of merged tokens).  It shows the accuracy scores for 'Scene', 'Director', and 'Like' prediction tasks on the LVU dataset when varying the number of merged tokens. The results indicate that a value of R = 0.8|S| yields the best performance.", "section": "B More Experiments"}]