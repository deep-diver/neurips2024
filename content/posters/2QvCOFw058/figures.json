[{"figure_path": "2QvCOFw058/figures/figures_7_1.jpg", "caption": "Figure 1: Illustration of the gradient norm for GD, PrecGD, and the proposed AGN, with the right subfigure showing a zoomed-in region of the left for iterations from 100 to 500.", "description": "This figure compares the gradient norm of three different algorithms: Gradient Descent (GD), Preconditioned Gradient Descent (PrecGD), and the Approximated Gauss-Newton (AGN) method proposed in the paper. The left subplot shows the overall gradient norm evolution across many iterations, while the right subplot zooms into the iterations 100 to 500 to better visualize the behavior of AGN in the later stage.  The plot demonstrates that AGN achieves significantly faster convergence than the other methods, with its gradient norm decreasing linearly to zero, while PrecGD experiences oscillations before converging, and GD shows a much slower convergence rate.", "section": "4 Saddle point analysis on the population risk"}, {"figure_path": "2QvCOFw058/figures/figures_8_1.jpg", "caption": "Figure 2: Comparison of convergence for PrecGD, GD, and AGN across various condition numbers, with the right subfigure extending the left by iterating from 300 to 1000.", "description": "This figure compares the convergence of three different algorithms (PrecGD, GD, and AGN) for solving the overparameterized non-convex low-rank matrix sensing problem.  The x-axis represents the number of iterations, and the y-axis represents the relative error.  The left panel shows the initial phase of convergence for all three algorithms with condition number (\u03ba) values of 10 and 100. The right panel zooms in on the convergence behavior after 300 iterations, highlighting how AGN significantly outperforms GD and PrecGD in terms of speed and robustness to saddle points. The plots demonstrate that AGN achieves a faster convergence rate compared to both GD and PrecGD, especially in escaping saddle regions where GD and PrecGD struggle.", "section": "Numerical experiments"}, {"figure_path": "2QvCOFw058/figures/figures_9_1.jpg", "caption": "Figure 3: Convergence of AGN under Sym. and Asym. parameterization of symmetric LRMS.", "description": "This figure compares the convergence of the AGN method under two different parameterizations for symmetric low-rank matrix sensing: symmetric and asymmetric.  The y-axis represents the relative error, while the x-axis shows the iteration count. The plot shows that the asymmetric parameterization leads to significantly faster convergence than the symmetric parameterization. This highlights the importance of parameterization choice when using the AGN method for symmetric low-rank matrix sensing problems.", "section": "Numerical experiments"}, {"figure_path": "2QvCOFw058/figures/figures_14_1.jpg", "caption": "Figure 2: Comparison of convergence for PrecGD, GD, and AGN across various condition numbers, with the right subfigure extending the left by iterating from 300 to 1000.", "description": "This figure compares the convergence speed of three different algorithms: PrecGD, GD, and AGN, for solving the overparameterized non-convex low-rank matrix sensing problem. The x-axis represents the number of iterations, and the y-axis represents the relative error. The different lines represent the performance of each algorithm under different condition numbers (\u03ba = 10 and \u03ba = 100). The figure shows that AGN converges much faster than PrecGD and GD, especially in the presence of saddle points, and that its convergence is less affected by the condition number.", "section": "Numerical experiments"}]