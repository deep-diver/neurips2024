[{"type": "text", "text": "Assembly Fuzzy Representation on Hypergraph for Open-Set 3D Object Retrieval ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yang $\\mathbf{X}\\mathbf{u}^{1}$ , Yifan Feng1, Jun Zhang2, Jun-Hai $\\mathbf{Yong^{1}}$ , and Yue $\\mathbf{Gao^{1*}}$ 1BNRist, THUIBCS, KLISS, BLBCI, School of Software, Tsinghua University, China 2Tencent AI Lab {xuyang9610,evanfeng97}@gmail.com, junejzhang@tencent.com, {yongjh,gaoyue}@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The lack of object-level labels presents a significant challenge for 3D object retrieval in the open-set environment. However, part-level shapes of objects often share commonalities across categories but remain underexploited in existing retrieval methods. In this paper, we introduce the Hypergraph-Based Assembly Fuzzy Representation (HAFR) framework, which navigates the intricacies of open-set 3D object retrieval through a bottom-up lens of Part Assembly. To tackle the challenge of assembly isomorphism and unification, we propose the Hypergraph Isomorphism Convolution (HIConv) for smoothing and adopt the Isomorphic Assembly Embedding (IAE) module to generate assembly embeddings with geometric-semantic consistency. To address the challenge of open-set category generalization, our method employs high-order correlations and fuzzy representation to mitigate distribution skew through the Structure Fuzzy Reconstruction (SFR) module, by constructing a leveraged hypergraph based on local certainty and global uncertainty correlations. We construct three open-set retrieval datasets for 3D objects with partlevel annotations: OP-SHNP, OP-INTRA, and OP-COSEG. Extensive experiments and ablation studies on these three benchmarks show our method outperforms current state-of-the-art methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the growing accessibility of 3D data, 3D object retrieval (3DOR) has emerged as an important area of interest in computer vision [19, 12]. The key objective of 3DOR is to establish connections between query and target samples via model training. Despite significant progress in recent years enhancing the development of 3DOR, most existing methods are still based on the closed-set assumption, where all categories encountered in the testing phase have been seen during training [6]. However, training sets can not cover all potential categories in real-world applications [32], which hinders accurate retrieval for unseen categories. Although object-level categories are difficult to cover, the part-level shapes of objects across object-level categories often share commonalities [17] among objects, which may provide sufficient semantic information of the object [29, 33]. However, this part-level method still remains underexploited on assembly-based representation for open-set retrieval. ", "page_idx": 0}, {"type": "text", "text": "3D parts, as essential components in shape representation and analysis, have the capability to represent potential information for both semantic [30] and geometric [25] level tasks. Recently, there has been an increasing application of assembly-based methods in the realm of 3D vision [24, 37]. From a geometric perspective, current methods for 3D part assembly impose strict constraints on the categories or quantities of parts [45], presenting a substantial challenge for implementation in an open-set environment. These methods often overlook the isomorphism and correlations between parts of the same object [20] during semantic embedding. This oversight diminishes the generalization capabilities of the representation model and exacerbates the distribution skew of unseen categories during open-set learning [47, 27]. In this paper, we explore the part-assembly representation method from both semantic and geometric perspectives to mitigate the distribution skew of unseen categories, aiming to enhance the generalization performance for open-set 3D object retrieval. ", "page_idx": 0}, {"type": "image", "img_path": "xOCAURlVM9/tmp/9ab64639b75838226255eee57b4e0ca8a4f27ad0338ad1af1f64ed9a3a0f00dd.jpg", "img_caption": ["Figure 1: Illustration of the assembly-based open-set 3DOR task and proposed HAFR framework. Given 3D objects of unseen categories, our method takes several part features and generates the assembly embedding isomorphically for each object. Then fuzzy embeddings are generated via leverage propagation and fuzzy reconstruction for open-set retrieval with unseen categories generalization. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Distinct from existing 3D shape assembly or open-set learning methods, the assembly-based open-set retrieval task emphasizes global-level semantics in object representation and requires enhanced generalization of geometric assembly. This leads to several challenges for assembly-based open-set retrieval, including: First, the difficulty of achieving assembly isomorphism for part features. While part features can effectively capture the categorical information of objects, direct fusion for multiple parts may lead to inconsistencies arising from geometric factors, including the order of input and the presence of repeated parts. Consequently, there is a strong motivation to achieve assembly isomorphism for part features with geometric-semantic consistency. Second, the difficulty in achieving assembly unification across different parts entails mapping and integrating part embeddings from the local-part space to the global-object space. Third, the difficulty in openset category generalization against distribution skew, which requires spatial propagation and generalization for object embeddings from the seen certainty to unseen uncertainty space. ", "page_idx": 1}, {"type": "text", "text": "Addressing the aforementioned challenges, we explore a method for open-set retrieval tasks through a bottom-up lens of Part Assembly. As shown in Figure 1, we introduce the Hypergraph-based Assembly Fuzzy Representation (HAFR) framework for assembly-based open-set 3DOR. On one hand, to tackle the challenge of assembly isomorphism and unification, we first propose the Hypergraph Isomorphism Convolution (HIConv) layer for feature smoothing, and then we adopt the Isomorphic Assembly Embedding (IAE) module for embedding integration with geometric-semantic consistency. On the other hand, to overcome the difficulty in category generalization, we construct a leverage hypergraph based on the local-certainty and global-uncertainty correlations. This structure captures potential leveraged correlations between seen and unseen categories for propagation. Besides, we adopt the Structure Fuzzy Reconstruction (SFR) module to exploit the fuzzy representation approach for open-set category generalization. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We explore a method to navigate the intricacies of open-set 3D object retrieval through a bottom-up lens of Part Assembly, and we construct three 3D point cloud datasets with multiple part annotations for benchmarking. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose the HAFR framework for assembly-based open-set 3D object retrieval tasks, including the Isomorphic Assembly Embedding (IAE) and the Structured Fuzzy Reconstruction (SFR) modules, which are designed to generate assembly embeddings with geometricsemantic consistency and overcome the distribution skew of unseen categories. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose the Hypergraph Isomorphism Convolution (HIConv) and a leverage hypergraph structure to capture the high-order correlations within and among objects, utilizing them for assembly isomorphism and open-set category generalization. \u2022 Extensive experiments are conducted on the three benchmarks for evaluation, demonstrating the superiority of HAFR over current state-of-the-art 3D object retrieval methods. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 3D Object Retrieval ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Most current 3D object retrieval methods operate under the closed-set assumption, meaning that the training and testing sets exhibit the same distribution of categories. These methods are divided into single-modal and multi-modal types. Single-modal 3D object retrieval focuses on identifying similar objects within a single modality of 3D data. For example, [31] and [35] use a view-based graph model to generate aggregated embeddings from multi-view data. [13] introduces a triplet-center loss to cluster objects of the same category and separate those of different categories. HGNN [9] employs a hypergraph-based structure to capture high-order correlations among objects for improved embeddings. Multi-modal retrieval methods [26, 22, 43, 44, 7, 2] use weighted fusion or feature fusion networks to aggregate embeddings from different modality-specific features. Additionally, CMCL [15] proposes a cross-modal center loss to minimize differences across various 3D modalities using common center embeddings. ", "page_idx": 2}, {"type": "text", "text": "2.2 Open-Set Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Open-set learning focuses on conducting machine learning research in scenarios where key factors are variable [47]. [32] introduces the Semantic Shift Benchmark (SSB) for open-set recognition. [46] proposes a \"none-of-above\" classifier to determine if a sample belongs to known categories. [5] presents an adversarial method to minimize the overlap between known and unknown distributions. Additionally, several open-set recognition methods for 3D object learning have been proposed [3, 16, 1, 48]. However, retrieval tasks in open-set scenarios are more practical than recognition due to the fundamentals of representation. Only a few methods [8, 23, 40, 39, 38] tackle the open-set 3DOR task, focusing on structure learning networks while neglecting the intricacies of the open-set environment. ", "page_idx": 2}, {"type": "text", "text": "2.3 3D Shape Assembly ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Much research in computer graphics has concentrated on assembly representation for the reconstruction [37], analysis [34], and generation [17] of 3D shapes. As for the assembly-related method for retrieval, most existing methods primarily focus on establishing relationships between parts of one object and parts of other objects [10], then utilize these connections for shape analysis [21] or part retrieval [4]. Although these methods have achieved satisfactory results, they rarely concentrate on the higher-order semantic connections from parts to objects, which makes them challenging to apply in 3D object retrieval. Additionally, their geometric constraints also limit their application in open-set environments. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Open-Set 3D Object Retrieval ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given 3D objects from the query set $\\mathcal{D}_{q}$ , the 3D object retrieval (3DOR) task is to find similar samples from the target set $\\mathcal{D}_{t}$ . The core approach for the 3DOR task is to learn the relationship between query samples and target samples from the training set $\\mathcal{D}_{t r n}$ . Each 3D object can be denoted as $\\left({s_{i},y_{i}}\\right)$ , the $y_{i}\\in\\mathcal{Y}=\\bar{\\{c_{j}\\}}_{j=1}^{Y}$ is the category label associated with the 3D object sample $s_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "In the open-set environment for 3DOR, all categories of samples in the testing set have not been learned in the training set, each retrieval sample is from unseen categories for the model, termed as Open-Set Retrieval. Specifically, the open-set settings means that the testing set $\\mathcal{D}_{t e s}=\\{\\mathcal{D}_{q},\\mathcal{D}_{t}\\}$ and the training set $\\mathcal{D}_{t r n}$ are drawn from the different distributions. For the testing set $\\mathcal{D}_{t e s}\\;=\\;$ ", "page_idx": 2}, {"type": "image", "img_path": "xOCAURlVM9/tmp/1b1044dd6d2f148aec13eaad7cce2ab6d84dc69378f5290642a14d27b13037e7.jpg", "img_caption": ["Isomorphic Assemby Embedding (IAE) ", "Structured Fuzzy Reconstruction (SFR) "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: An overview of the Hypergraph-Based Assembly Fuzzy Representation (HAFR) framework for assembly-based open-set 3D object retrieval. Our framework is composed of the Assembled Isomorphism Embedding (IAE) and Structured Fuzzy Reconstruction (SFR) modules, which are designed for geometric-semantic consistent integration and fuzzy-aware generalization, respectively. ", "page_idx": 3}, {"type": "text", "text": "$\\{(s_{i},\\hat{y}_{i})\\}_{i=1}^{T}=\\{{\\mathcal{D}}_{q},{\\mathcal{D}}_{t}\\}$ and the training set $\\boldsymbol{\\mathcal{D}}_{t r n}=\\{(s_{i},y_{i})\\}_{i=1}^{L}$ , the category space of them are not the same indicating $\\hat{y}_{i}\\in\\hat{\\mathcal{Y}}=\\{\\hat{c}_{j}\\}_{j=1}^{\\hat{Y}}$ , $y_{i}\\in\\mathcal{Y}=\\{c_{j}\\}_{j=1}^{Y}$ , and $\\hat{y}\\cap y=\\emptyset$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Assembly Representation for Retrieval ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Typically, each 3D object can be decomposed into multiple parts based on its shape and semantic information, and it can be regarded as a model assembled from these parts, i.e., the doors, roofs, hoods, and wheels of cars, the seats, backs, arms, and legs of the chairs. Face the emergence of the open-set environment with incomplete or missing class labels, the assembly representation based on multiple local parts may provide enough semantic information than global object features for retrieval. We termed this presentation for open-set 3DOR as Assembly Representation: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{i}\\in\\mathcal{S}=\\{\\{p_{i}^{r}\\}_{r=1}^{P}\\}_{i=1}^{N},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\boldsymbol{S}$ denote the set of 3D objects, $s_{i}=\\{p_{i}^{r}\\}_{r=1}^{P}$ denotes a 3D object represented by $P$ semantic parts, $N$ denotes the number of samples in $\\boldsymbol{S}$ . ", "page_idx": 3}, {"type": "text", "text": "Consequently, the assembly-based open-set 3DOR aims to design a method to retrieve similar samples of query in the testing set $\\bar{\\mathcal{D}}_{t e s}=\\{(\\{p_{i}^{r}\\}_{r=1}^{P},\\hat{y}_{i})\\}_{i=1}^{T}=\\{\\mathcal{D}_{q},\\breve{\\mathcal{D}}_{t}\\}$ , based on the data and knowledge in the training set $\\mathcal{D}_{t r n}\\,=\\,\\{(\\{p_{i}^{r}\\}_{r=1}^{P},y_{i})\\}_{i=1}^{L}$ . The assembly-based open-set 3DOR by multiple parts aims to minimize the expected risk: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{*}=a r g m i n\\mathbb{E}_{(D_{i},D_{j})\\sim({\\mathcal D}_{q},{\\mathcal D}_{t})}\\left[\\chi_{\\{\\hat{y}_{i}\\neq\\hat{y}_{j}\\}}e^{-\\|f(\\{p_{i}^{r}\\}_{r=1}^{P})-f(\\{p_{j}^{r}\\}_{r=1}^{P})\\|_{2}}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+\\chi_{\\{\\hat{y}_{i}=\\hat{y}_{j}\\}}(1-e^{-\\|f(\\{p_{i}^{r}\\}_{r=1}^{P})-f(\\{p_{j}^{r}\\}_{r=1}^{P})\\|_{2}})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $D_{i}\\,=\\,(\\{p_{i}^{r}\\}_{r=1}^{P},n_{i},\\hat{y}_{i})$ and $D_{j}\\,=\\,(\\{p_{j}^{r}\\}_{r=1}^{P},n_{j},\\hat{y}_{j})$ are the object samples selected from the query set $\\mathcal{D}_{q}$ and target set $\\mathcal{D}_{t}$ . $\\chi_{\\{\\cdot\\}}$ denotes the indicator function, which evaluates to 1 when the specified condition holds true and 0 otherwise. The embedding function $f:=\\{p_{i}^{r}\\}_{r=1}^{P}\\rightarrow v_{i}$ maps multiple parts $\\{p_{j}^{r}\\}_{r=1}^{P}$ of an object to an assembly representation vector $v_{i}\\in\\mathbb{R}^{d}$ , facilitating similarity-based retrieval. $\\mathcal{H}$ denotes the hypothesis space of the embedding function. The $\\mathcal{L}_{2}$ norm function $\\Vert\\cdot\\Vert_{2}$ is used as a distance metric to measure the Euclidean distance between two vectors. ", "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Overall Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As shown in Figure 2, the proposed Hypergraph-Based Assembly Fuzzy Representation (HAFR) framework is composed of two modules: Isomorphic Assembly Embedding (IAE) and Structured ", "page_idx": 3}, {"type": "text", "text": "Fuzzy Reconstruction $(S F R)$ . The framework takes the basic features of different parts as input. In the IAE stage, the multiple features are assembled isomorphically by the Hypergraph Isomorphism Convolution (HIConv), and the assembly embeddings are generated from multiple parts. Next, in the SFR stage, the leverage hypergraph structure is constructed based on the local-certainty and globaluncertainty correlations. Guided by this structured open-set distribution, hypergraph convolution is adopted for propagation implicitly from seen categories to unseen categories. Finally, the memory bank is adopted for fuzzy-aware reconstruction and fuzzy embedding generation to further overcome openness during open-set retrieval. ", "page_idx": 4}, {"type": "text", "text": "4.2 Isomorphic Assembly Embedding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The IAE module is designed here to obtain assembly embeddings with geometric-semantic consistency from multiple parts. Specifically, the IAE comprises a hypergraph-based isomorphism layer and assembly autoencoders as shown in the left side of Figure 2. We proposed the Hypergraph Isomorphism Convolution (HIConv) in the layer to impart geometric-semantic consistency to part features, overcoming the bias introduced by the order of parts during assembly representation. The assem", "page_idx": 4}, {"type": "image", "img_path": "xOCAURlVM9/tmp/8dad4a4e1fadb0c1cd92bd3c8e9899b3485a86cbf1f14c89ca94561db9ce1ac8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "bly auto-encoders are utilized to get Figure 3: Illustration of input basic part features for HAFR. the assembly embedding from multi  \nple part features. ", "page_idx": 4}, {"type": "text", "text": "The IAE module takes the basic part features $\\{\\{f_{i}^{r}\\}_{r=1}^{P}\\}_{i=1}^{N}$ $(f_{i}^{r}\\in\\mathbb{R}^{N\\times d_{f}})$ of $N$ objects with $P$ parts. In this paper, we use the average point feature of each region extracted by 3D point cloud part segmentation network [28] as shown in Figure 4.1. The hypergraph isomorphism layer constructs an assembly hypergraph structure $\\mathcal{G}_{a}$ and generates isomorphism embeddings $\\boldsymbol{c}_{i}^{r}$ under the guidance of this structure. The assembly auto-encoders $\\mathcal{A}_{a}$ encode the multi-part features of 3D objects and get the assembly embeddings $u_{i}$ by integration function. ", "page_idx": 4}, {"type": "text", "text": "4.2.1 Isomorphism Smoothing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Although part features have the potential to adequately represent the category information of every object, direct fusion for multiple parts may lead to inconsistencies arising from geometric factors, including the order of input and the presence of repeated parts. The HIConv are utilized to impart geometric-semantic consistency during part assembly, aiming to generate isomorphism embeddings from independent-part to correlated-object distribution, by the correlation-based smoothing under the guidance of the assembly hypergraph structure. ", "page_idx": 4}, {"type": "text", "text": "A hypergraph can be represented as $\\mathcal{G}=\\{\\mathcal{V},\\mathcal{E}\\}$ , where $\\nu$ and $\\mathcal{E}$ are the vertex set and the hyperedge set, respectively. In the hypergraph of HIConv, the basic part features $f_{i}^{p}$ are treated as the vertices $\\begin{array}{r}{\\mathbf{X}_{f}=\\bigcup_{d=1}^{D}\\{f_{i}^{d}\\}_{i=1}^{N}}\\end{array}$ . Then the isomorphism hyperedges ${\\mathcal{E}}_{o}$ are constructed as the subset of vertices that are from the same object: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{E}_{o}=\\{\\mathcal{O}_{v}(i)\\mid i\\in\\{1,\\cdots,N\\}\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $O_{v}(i)$ denotes all vertices of the $i$ -th object. In this way, we obtain $N$ hyperedges and $N$ is the number of objects. The assembly hypergraph is constructed as $\\mathcal{G}_{a}=\\{\\mathbf{X}_{f},\\mathcal{E}_{o}\\}$ after getting vertices and hyperedges. ", "page_idx": 4}, {"type": "text", "text": "After the construction of the assembly hypergraph, the HIConv is designed to bridge the geometry and semantic correlations for isomorphic smoothing. For the convenience of convolution, we use the incidence matrix $\\mathbf{H}\\in\\{0,1\\}^{|\\mathcal{V}|\\times|^{2}\\!\\!\\varepsilon|}$ to represent the hypergraph, where the hyperedges are the columns of $\\mathbf{H}$ , and ${\\bf H}(v,e)\\,=\\,1$ if vertex $v$ are contained in hyperedge $e$ . Inspired by [18] and [11], the HIConv is designed to leverage the geometric-semantic collaborative information under the ", "page_idx": 4}, {"type": "text", "text": "guidance of assembly hypergraph: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{X}}_{f}=\\mathbf{M}\\mathbf{L}\\mathbf{P}\\left((1+\\epsilon)\\mathbf{X}_{f}+\\sigma(\\mathbf{D}_{v}^{-\\frac{1}{2}}\\mathbf{H}\\mathbf{D}_{e}^{-1}\\mathbf{H}^{\\top}\\mathbf{D}_{v}^{-\\frac{1}{2}}\\mathbf{X}_{f}\\Theta_{H I C o n v})\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{H}$ denotes the incidence matrix of the hypergraph $\\mathcal{G}_{a}$ , respectively. $\\mathbf{D}_{v}$ and ${\\bf D}_{e}$ are the diagonal degree matrices for vertices and hyperedges, respectively. $\\epsilon$ denotes a learnable parameter and $\\Theta_{H I C o n v}$ is the learnable matrix for HIConv. After the HIConv, the isomorphism embeddings $\\tilde{\\mathbf{X}}_{f}=\\{\\{c_{i}^{r}\\}_{r=1}^{P}\\}_{i=1}^{N}$ are generated from independent-part to correlated-object space. ", "page_idx": 5}, {"type": "text", "text": "4.2.2 Assembly Embedding ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The IAE module first utilizes assembly auto-encoders $\\mathcal{A}_{a}$ to encode the isomorphism embeddings into a latent code space for each part, then employs the isomorphism loss to pull the different part codes together to ensure encoded embeddings from the same object together. Besides, the intra-part and inter-part reconstruction loss are proposed to reduce information loss during compression. ", "page_idx": 5}, {"type": "text", "text": "Specifically for the $\\mathcal{A}_{a}$ , we have $u_{i}^{r}=\\Psi^{r}\\big(c_{i}^{r}\\big)$ and $\\hat{c}_{i}^{r}=\\Phi^{r}(u_{i}^{r})$ , where $c_{i}^{r}\\in\\mathbb{R}^{d_{f}}$ denote isomorphism embeddings of parts, $u_{i}^{r}\\,\\in\\,\\mathbb{R}^{d_{u}}$ denote the unified embeddings compressed from isomorphism embeddings of different parts. The encoder and decoder are defined as $\\Psi^{r}\\;:=\\;\\mathbb{S}_{p}\\;\\to\\;\\mathbb{S}_{a}$ and $\\Phi^{r}:=\\mathbb S_{a}\\to\\mathbb S_{p}$ , which map the representation between local-part space $\\mathbb{S}^{p}$ to global-assembly space $\\mathbb{S}^{a}$ . ", "page_idx": 5}, {"type": "text", "text": "After the generation of unified embeddings $u_{i}^{r}$ for each part, the IAE module employs an integration function $\\bar{\\boldsymbol{B}}(\\cdot)$ to obtain the assembly embeddings integrated from all parts of the same object: ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{i}=B(\\{u_{i}^{r}\\}_{r=1}^{P})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.2.3 Loss Function for the IAE ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For better isomorphism smoothing and assembly embedding, we adopt the Assembly Loss $\\mathcal{L}_{a s}$ and the Cross-Part Loss $\\mathcal{L}_{x p}$ for the IAE module, which is designed to pull the distance and prompt the generalization performance across parts, respectively: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a s}=\\frac{2}{R(R-1)}\\sum_{k=1}^{R}\\sum_{l=k+1}^{R}\\|u_{i}^{k}-u_{i}^{l}\\|_{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{x p}=\\frac{2}{R(R-1)}\\sum_{k=1,l\\neq k}^{D}\\left(\\|c_{i}^{k}-\\hat{c}_{i}^{k}\\|_{2}+\\|c_{i}^{k}-\\Phi^{l}\\left(\\Psi^{k}\\left(c_{i}^{l}\\right)\\right)\\|_{2}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lVert\\cdot\\rVert_{2}$ is the ${\\mathcal{L}}_{2}$ norm, $u_{i}^{k}$ and $u_{i}^{l}$ are both the unified embeddings but from different parts of the same object, $\\Psi^{k}$ is the encoder of $k$ -th part and $\\Phi^{l}$ is the decoder of $l$ -th part. ", "page_idx": 5}, {"type": "text", "text": "The loss function for IAE is constructed by the balanced combination of the two losses: $\\mathcal{L}_{I A E}=$ $\\alpha\\mathcal{L}_{a s}+(1-\\alpha)\\mathcal{L}_{x p}$ , where $\\alpha$ is the hyper-parameter to trade-off between them. ", "page_idx": 5}, {"type": "text", "text": "4.3 Structured Fuzzy Reconstruction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Although the IAE module generated the assembly embeddings from multiple parts, the distribution skew across seen and unseen categories still affects the performance of open-set retrieval. As shown in Figure 2, we proposed the SFR module for generalization. The SFR module first constructs a leverage hypergraph to model the local-certainty and global-uncertainty correlations. Then SFR employs hypergraph convolution for propagation from seen categories to unseen categories based on the implicit leveraged structure. After that, the memory bank is adopted to reconstruct the propagation embeddings into fuzzy space to further overcome openness during open-set retrieval. ", "page_idx": 5}, {"type": "text", "text": "4.3.1 Leverage Propagation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To get the most out of potential correlations from seen categories, the leverage hypergraph is designed here. As shown in Figure 2, the assembly embeddings $u_{i}$ are treated as the vertices $\\bar{\\mathbf{X}_{u}}^{'}=\\{u_{i}\\}_{i=1}^{N}$ in the leverage hypergraph, and the hyperedges are constructed from two perspectives: local-certainty and global-uncertainty correlations. ", "page_idx": 5}, {"type": "text", "text": "The local-certainty hyperedges $\\mathcal{E}_{c}$ are constructed based on the category observability, which is defined as $\\mathcal{E}_{c}=\\{\\mathcal{C}_{v}\\dot{(y)}\\ |\\ y\\ \\in\\mathcal{Y}\\}$ , where $C_{v}(y)$ denotes the subset of vertices that belong to the seen categories $\\boldsymbol{\\wp}$ . For the global-uncertainty hyperedges $\\mathcal{E}_{u}$ , we construct them by linking each vertex and its $K-1$ neighbor vertices: $\\mathcal{E}_{u}\\,=\\,\\{\\bar{K_{\\mathrm{KNN}_{k}}}(v)\\ |\\ v\\in\\mathcal{V}\\}$ , where $K_{\\mathrm{KNN}_{k}}(\\bar{v})$ denotes the top- ${\\cdot k}$ nearest neighbor set of vertex $v$ . In this way, we obtain one local-certainty hyperedge and $N$ global-uncertainty hyperedge. The leverage hypergraph is constructed by $\\mathcal{G}_{l e v}=\\{\\mathbf{X}_{u},\\mathcal{E}_{c}\\cup\\mathcal{E}_{u}\\}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "After the construction of the leverage hypergraph, we utilize the modified hypergraph convolution from [11] for embedding propagation. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{X}}_{u}=\\sigma\\left({\\mathbf{D}_{v}^{-\\frac{1}{2}}\\mathbf{H}\\mathbf{D}_{e}^{-1}\\mathbf{H}^{\\top}\\mathbf{D}_{v}^{-\\frac{1}{2}}\\mathbf{X}_{u}\\boldsymbol\\Theta_{l e v}}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{H}$ denotes the incidence matrix of the leverage hypergraph $\\mathcal{G}_{l e v}$ . $\\mathbf{D}_{v}$ and ${\\bf D}_{e}$ are the diagonal degree matrices for vertices and hyperedges, respectively. $\\Theta_{l e v}$ denotes the learnable matrix for the HGNNConv, $\\tilde{\\mathbf{X}}_{u}$ are the propagation embeddings and $\\tilde{\\mathbf{X}}_{u}=\\{p_{i}\\}_{i=1}^{N}$ . ", "page_idx": 6}, {"type": "text", "text": "4.3.2 Fuzzy Reconstruction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To overcome the distribution skew caused by the open-set environment, the memory bank is adopted here to reconstruct the propagation embeddings to fuzzy space. The memory bank is designed to store a large amount of fuzzy representations with uniform distribution. Specifically, the memory bank $\\mathcal{M}$ is composed of $Z$ invariant memory anchors $m_{j}$ for 3D objects. ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathcal{M}}=\\{m_{j}\\in\\mathbb{R}^{u}\\ |\\ j=1,\\cdots,Z\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given the propagation embedding $\\tilde{u}_{i}$ of the each 3D object, the activation score $s_{i,j}$ are calculated for every memory anchor $m_{j}$ in $\\mathcal{M}$ by $s_{i,j}=\\|\\tilde{u}_{i}-m_{j}\\|_{2}$ , where $\\lVert\\cdot\\rVert_{2}$ is the $\\mathcal{L}_{2}$ norm for distance metric, $s_{i,j}$ denotes the activation score of each anchor. Then we use the normalization of activation scores $s_{i,j}^{\\prime}$ to rebuild the propagation embedding into fuzzy space and get fuzzy embeddings $z_{i}$ by: ", "page_idx": 6}, {"type": "equation", "text": "$$\nz_{i}=\\sum_{j=1}^{Z}s_{i,j}^{\\prime}m_{j}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4.3.3 Loss Function for the SFR ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the SFR stage, we adopt the Cross-Entropy Loss $\\mathcal{L}_{c e}$ and Fuzzy Reconstruction Loss $\\mathcal{L}_{f z}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{f z}=\\left|\\left|\\tilde{u}_{i}-z_{i}\\right|\\right|_{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c e}=-\\sum_{k=1}^{Y}\\left(n_{i,k}\\mathrm{log}(p_{i,k})+n_{i,k}\\mathrm{log}(q_{i,k})\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where pi,k = $\\begin{array}{r}{p_{i,k}=\\frac{e^{\\hat{v}_{i,k}}}{\\sum_{k=1}^{Y}e^{\\tilde{u}_{i,k}}}}\\end{array}$ kYe=1 eu\u02dci,k and qi,k = $\\begin{array}{r}{q_{i,k}=\\frac{e^{z_{i,k}}}{\\sum_{k=1}^{Y}e^{z_{i,k}}}}\\end{array}$ kYe=z1i ,ekzi,k denote the prediction scores of fuzzy embeddings that the image belongs to the $k$ -th category. $n_{i,k}$ is the $k$ -th value of one-hot encoded seen category labels, $Y$ is the number of seen categories. ", "page_idx": 6}, {"type": "text", "text": "The loss function for SFR is constructed by the balanced combination of the two losses: $\\mathcal{L}_{S F R}=$ $\\beta\\mathcal{L}_{f z}+(1-\\beta)\\mathcal{L}_{c e}$ , where $\\beta$ is the hyper-parameter to trade-off between them. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "OpenPart Datasets. We construct three datasets for assembly-based open-set 3D object retrieval (OpenPart datasets), including OP-SHNP, OP-INTRA, OP-COSEG based on the public dataset ShapeNetPart [42], IntrA [41], and COSEG [34]. We sampled the point cloud from the triangular surface for each dataset. As shown in Table 1, the classes of these datasets are split into seen and unseen classes for training and testing, respectively. Each class contains three to five parts. Specifically, the detailed descriptions of the datasets and parts segmentation are shown in Appendix B. ", "page_idx": 6}, {"type": "table", "img_path": "xOCAURlVM9/tmp/fe3b05df6b231c696fadece8ce06ca62c82cf466eca4b5c509f469ba12c15f87.jpg", "table_caption": ["Table 1: The statistics of the three OpenPart datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "xOCAURlVM9/tmp/726827561020d8db18b338e7de4ce9a8b47f5ad2c292693960e439a3a81576a7.jpg", "img_caption": ["Figure 4: The Precision-Recall Curves for comparison on the three datasets, respectively. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Implemental Details. The random seeds are fixed to 2024 in this paper for a fair comparison. The basic features are extracted by PointNet [28] and each part feature is obtained by the average point feature of each region, we use the top four parts for each object in this paper. We set $\\alpha=0.5$ in $\\mathcal{L}_{I A E}$ and $\\beta=0.9$ in $\\mathcal{L}_{S F R}$ . The IAE is trained for 40 epochs on learning rate $l r=0.1$ , and the SFR is trained for 30 epochs on $l r=0.001$ . The detailed implements of the HAFR framework are provided in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "5.2 Retrieval Performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Compared Methods. Since no methods are specifically designed for assembly-based open-set 3DOR, we refine the current state-of-the-art close-set 3DOR methods (MMJN [26], TCL [13], SDML [14], MMSAE [36]), and open-set 3D learning methods (PROSER [46], $\\mathrm{{HGM^{2}R}}$ [8]), then we refine the multi-modal fusion module with multi-part fusion for each method. We provide detailed implements of compared methods in Appendix D ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics. For a fair comparison, we use standard retrieval metrics, including Mean Average Precision (mAP), Normalized Discounted Cumulative Gain (NDCG), Average Normalized Modified Retrieval Rank (ANMRR), and the Precision-Recall Curve (PR-Curve). For the mAP and NDCG metrics, higher scores are better. For the ANMRR metric, a lower score is better. ", "page_idx": 7}, {"type": "text", "text": "Comparsion Analysis. As shown in Table 2, we evaluate the assembly-based open-set retrieval results from HAFR framework and other state-of-the-art compared methods. Quantitative results demonstrate the superiority of our method over the other methods on all three datasets. In particular, on the OP-COSEG dataset, our method achieves $0.7015\\,\\mathrm{\\mAP}$ with about $8.7\\%$ improvements compared with the second-best method $(\\mathrm{HGM^{2}R})$ ). We also compare the Precision-Recall Curves (PR-Curve) which evaluate the trade-off between precision and recall at different thresholds. A larger area between the curve and the axes indicates better performance for retrieval, our method outperforms all other retrieval methods as shown in Figure 4. ", "page_idx": 7}, {"type": "text", "text": "The superior performance in the comparison indicates that by the proposed IAE and SFR modules, the proposed HAFR framework can better utilize the potential semantic information among part features to fully represent 3D objects and generalize them to unseen categories. Notably, for the OP-INTRA dataset with only one category and the least parts in the training set, our method still achieves performance improvements in the retrieval of unseen categories as shown in Table 2 and Figure 4. This indicates that our method has minimal dependence on the performance of classifiers during basic feature extraction. In open-set environments, where training on all potential categories is infeasible, our HAFR method through Part Assembly demonstrates superior open-set adaptability. We provide more visualizations and results in Appendix D. ", "page_idx": 7}, {"type": "table", "img_path": "xOCAURlVM9/tmp/20d90ef176f824e5598e143952355bd3d0b2f3fe11ecd6dff6f2e6bbc909414c.jpg", "table_caption": ["Table 3: Ablation studies of retrieval on the OP-SHNP, OP-INTRA, and OP-COSEG datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "xOCAURlVM9/tmp/a9b836d37bc574af7236906f262e7b874912c097a76057d7a7dca84e649ed830.jpg", "img_caption": ["Figure 5: The Precision-Recall Curves of the ablation studies on the three datasets, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct ablation studies to verify the effectiveness of modules in the proposed framework. For the IAE stage, we first remove the Hypergraph Isomorphism Convolution (HIConv) layer for comparison. Specifically, we replace it with the Graph Isomorphism Convolution $(\\mathrm{HIConv}{\\rightarrow}\\mathrm{GIN})$ ) and MLP $\\scriptstyle(\\mathrm{HIConv}\\to\\mathrm{MLP}$ ). Then we compared it with the method without the Assembly Loss (IAE w/o $\\mathcal{L}_{a s}$ ) or the Cross-Part Loss(IAE w/o $\\mathcal{L}_{x p}$ ). As shown in Table 3 and Figure 5, replacing HIConv or removing parts of the $\\mathcal{L}_{I A E}$ loss significantly degrades the performance of IAE. For the HIConv, this indicates that naive part assembly is dependent on geometric or semantic factors such as order and quantity. Removing isomorphism smoothing results in geometric or semantic inconsistencies, thereby degrading the retrieval performance by the worse assembly embeddings. As for the SFR module, we compare the hypergraph structure without the leveraged structure (SFR w/o $\\mathcal{G}_{l e v}$ ) and we also replace the hypergraph-based structure learning with MLP (MLP-based SFR) and GCN (GCN-based SFR). Besides, we remove the memory bank to verify the effectiveness of fuzzy reconstruction. ", "page_idx": 8}, {"type": "text", "text": "Table 3 and Figure 5 show that the proposed full SFR module outperforms all the other ablative structures, these results show that the proposed leverage propagation and fuzzy reconstruction approach can effectively utilize the high-order correlations between seen and unseen categories for open-set generalization. Besides, we can observe that the complete combination of IAE and SFR yields the best performance. These quantitative and qualitative results indicate that the proposed HAFR framework effectively achieves part-level assembly isomorphism and unification while mitigating distribution skew from seen certainty to unseen uncertainty at the object level. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce the Hypergraph-Based Assembly Fuzzy Representation (HAFR) framework, which navigates the intricacies of open-set 3D object retrieval through a bottom-up lens of Part Assembly. Specifically, we propose the Hypergraph Isomorphism Convolusion (HIConv) and adopt the Isomorphic Assembly Embedding (IAE) module for assembly isomorphism and unification, generating the integration embeddings with geometric-semantic consistency. Besides, we employ the Structure Fuzzy Reconstruction (SFR) approach to exploit high-order correlations among objects and fuzzify representations for open-set category generalization. This module constructs a leveraged hypergraph based on local-certainty and global-uncertainty correlations to mitigate distribution skew. We construct three open-set retrieval datasets for 3D objects with part-level annotations, $i,e.$ , OP-SHNP, OP-INTRA, and OP-COSEG. Extensive experiments and ablation studies on these three benchmarks show our method outperforms current state-of-the-art methods. However, due to data limitations, this paper does not currently consider the assembly fuzzy representation for varying numbers of parts, which will be a focus of our future research. We believe this paper provides a novel perspective for open-set retrieval by exploring from local to global levels. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by Beijing Natural Science Foundation (No. L242167), CCF-Tencent Open Research Fund, and Jiangxi Provincial Natural Science Foundation (20224ACB218002). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Antonio Alliegro, Francesco Cappio Borlino, and Tatiana Tommasi. Towards open set 3d learning: A benchmark on object point clouds. arXiv preprint arXiv:2207.11554, 2022.   \n[2] Song Bai, Feihu Zhang, and Philip HS Torr. Hypergraph convolution and hypergraph attention. Pattern Recognition, 110:107637, 2021.   \n[3] Abhijit Bendale and Terrance E Boult. Towards open set deep networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1563\u20131572, 2016.   \n[4] Siddhartha Chaudhuri, Evangelos Kalogerakis, Leonidas Guibas, and Vladlen Koltun. Probabilistic reasoning for assembly-based 3d modeling. In ACM SIGGRAPH, pages 1\u201310. ACM New York, NY, USA, 2011.   \n[5] Guangyao Chen, Peixi Peng, Xiangqian Wang, and Yonghong Tian. Adversarial reciprocal points learning for open set recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):8065\u2013 8081, 2021.   \n[6] Wei Chen, Yu Liu, Weiping Wang, Erwin M Bakker, Theodoros Georgiou, Paul Fieguth, Li Liu, and Michael S Lew. Deep learning for instance retrieval: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \n[7] Yihe Dong, Will Sawin, and Yoshua Bengio. Hnhn: Hypergraph networks with hyperedge neurons. arXiv preprint arXiv:2006.12278, 2020.   \n[8] Yifan Feng, Shuyi Ji, Yu-Shen Liu, Shaoyi Du, Qionghai Dai, and Yue Gao. Hypergraph-based multimodal representation for open-set 3d object retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(4):2206\u20132223, 2023.   \n[9] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In AAAI Conference on Artificial Intelligence, pages 3558\u20133565, 2019.   \n[10] Thomas Funkhouser, Michael Kazhdan, Philip Shilane, Patrick Min, William Kiefer, Ayellet Tal, Szymon Rusinkiewicz, and David Dobkin. Modeling by example. ACM Transactions on Graphics, 23(3):652\u2013663, 2004.   \n[11] Yue Gao, Yifan Feng, Shuyi Ji, and Rongrong Ji. Hgnn+: General hypergraph neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3181\u20133199, 2022.   \n[12] Yue Gao, Meng Wang, Dacheng Tao, Rongrong Ji, and Qionghai Dai. 3-d object retrieval and recognition with hypergraph analysis. IEEE Transactions on Image Processing, 21(9):4290\u20134303, 2012.   \n[13] Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai. Triplet-center loss for multi-view 3d object retrieval. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1945\u20131954, 2018.   \n[14] Peng Hu, Liangli Zhen, Dezhong Peng, and Pei Liu. Scalable deep multimodal learning for cross-modal retrieval. In Annual Conference of the Association for Computing Machinery Special Interest Group in Information Retrieval, pages 635\u2013644, 2019.   \n[15] Longlong Jing, Elahe Vahdani, Jiaxing Tan, and Yingli Tian. Cross-modal center loss for 3d cross-modal retrieval. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3142\u20133151, 2021.   \n[16] KJ Joseph, Salman Khan, Fahad Shahbaz Khan, and Vineeth N Balasubramanian. Towards open world object detection. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5830\u20135840, 2021.   \n[17] Evangelos Kalogerakis, Siddhartha Chaudhuri, Daphne Koller, and Vladlen Koltun. A probabilistic model for component-based shape synthesis. ACM Transactions on Graphics, 31(4):1\u201311, 2012.   \n[18] Xu Keyulu, Hu Weihua, Leskovec Jure, and Jegelka Stefanie. How powerful are graph neural networks? In International Conference on Learning Representations. OpenReview.net, 2019.   \n[19] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In International Conference on Computer Vision Workshop, pages 554\u2013561, 2013.   \n[20] Jun Li, Chengjie Niu, and Kai Xu. Learning part generation and assembly for structure-aware shape synthesis. In AAAI conference on artificial intelligence, pages 11362\u201311369, 2020.   \n[21] Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao Zhang, and Leonidas Guibas. Grass: Generative recursive autoencoders for shape structures. ACM Transactions on Graphics, 36(4):1\u201314, 2017.   \n[22] Qi Liang, Mengmeng Xiao, and Dan Song. 3d shape recognition based on multi-modal information fusion. Multimedia Tools and Applications, 80:16173\u201316184, 2021.   \n[23] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao Su. Openshape: Scaling up 3d shape representation towards open-world understanding. Annual Conference on Neural Information Processing Systems, 36, 2024.   \n[24] Katia Lupinetti, Jean-Philippe Pernot, Marina Monti, and Franca Giannini. Content-based cad assembly model retrieval: Survey and future challenges. Computer-Aided Design, 113:62\u201381, 2019.   \n[25] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy J Mitra, and Leonidas J Guibas. Structurenet: hierarchical graph networks for 3d shape generation. ACM Transactions on Graphics, 38(6):1\u201319, 2019.   \n[26] Weizhi Nie, Qi Liang, An-An Liu, Zhendong Mao, and Yangyang Li. Mmjn: Multi-modal joint networks for 3d shape recognition. In ACM International Conference on Multimedia, pages 908\u2013916, 2019.   \n[27] Jitendra Parmar, Satyendra Chouhan, Vaskar Raychoudhury, and Santosh Rathore. Open-world machine learning: applications, challenges, and opportunities. ACM Computing Surveys, 55(10):1\u201337, 2023.   \n[28] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 652\u2013660, 2017.   \n[29] Chao-Hui Shen, Hongbo Fu, Kang Chen, and Shi-Min Hu. Structure recovery by part assembly. ACM Transactions on Graphics, 31(6):1\u201311, 2012.   \n[30] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(8):2647\u20132664, 2020.   \n[31] Yuting Su, Yuqian Li, Weizhi Nie, Dan Song, and An-An Liu. Joint heterogeneous feature learning and distribution alignment for 2d image-based 3d object retrieval. IEEE Transactions on Circuits and Systems for Video Technology, 30(10):3765\u20133776, 2019.   \n[32] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need? arXiv preprint arXiv:2110.06207, 2021.   \n[33] Kai Wang, Paul Guerrero, Vladimir G Kim, Siddhartha Chaudhuri, Minhyuk Sung, and Daniel Ritchie. The shape part slot machine: Contact-based reasoning for generating 3d shapes from parts. In European Conference on Computer Vision, pages 610\u2013626. Springer, 2022.   \n[34] Yunhai Wang, Shmulik Asaf,i Oliver Van Kaick, Hao Zhang, Daniel Cohen-Or, and Baoquan Chen. Active co-analysis of a set of shapes. ACM Transactions on Graphics, 31(6):1\u201310, 2012.   \n[35] Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based graph convolutional network for 3d shape analysis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1850\u20131859, 2020.   \n[36] Yiling Wu, Shuhui Wang, and Qingming Huang. Multi-modal semantic autoencoder for cross-modal retrieval. Neurocomputing, 331:165\u2013175, 2019.   \n[37] Xianghao Xu, Paul Guerrero, Matthew Fisher, Siddhartha Chaudhuri, and Daniel Ritchie. Unsupervised 3d shape reconstruction by part retrieval and assembly. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8559\u20138567, 2023.   \n[38] Yang Xu, Yifan Feng, and Lin Bie. Triadic elastic structure representation for open-set incremental 3d object retrieval. In ACM International Conference on Multimedia Retrieval, pages 20\u201328, 2024.   \n[39] Yang Xu, Yifan Feng, and Yue Gao. Negative prompt driven complementary parallel representation for open-world 3d object retrieval. In International Joint Conference on Artificial Intelligence, pages 1498\u20131506, 2024.   \n[40] Yang Xu, Yifan Feng, and Yu Jiang. Structure-aware residual-center representation for self-supervised open-set 3d cross-modal retrieval. In IEEE International Conference on Multimedia and Expo, pages 1\u20136, 2024.   \n[41] Xi Yang, Ding Xia, Taichi Kin, and Takeo Igarashi. Intra: 3d intracranial aneurysm dataset for deep learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2656\u20132666, 2020.   \n[42] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics, 35(6):1\u201312, 2016.   \n[43] Haoxuan You, Yifan Feng, Rongrong Ji, and Yue Gao. Pvnet: A joint convolutional network of point cloud and multi-view for 3d shape recognition. In ACM International Conference on Multimedia, pages 1310\u20131318, 2018.   \n[44] Haoxuan You, Yifan Feng, Xibin Zhao, Changqing Zou, Rongrong Ji, and Yue Gao. Pvrnet: Point-view relation neural network for 3d shape recognition. In AAAI Conference on Artificial Intelligence, pages 9119\u20139126, 2019.   \n[45] Guanqi Zhan, Qingnan Fan, Kaichun Mo, Lin Shao, Baoquan Chen, Leonidas J Guibas, Hao Dong, et al. Generative 3d part assembly via dynamic graph learning. Annual Conference on Neural Information Processing Systems, 33:6315\u20136326, 2020.   \n[46] Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Learning placeholders for open-set recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4401\u20134410, 2021.   \n[47] Zhi-Hua Zhou. Open-environment machine learning. National Science Review, 9(8):nwac123, 2022.   \n[48] Chenming Zhu, Wenwei Zhang, Tai Wang, Xihui Liu, and Kai Chen. Object2scene: Putting objects in context for open-vocabulary 3d detection. arXiv preprint arXiv:2309.09456, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Assembly-Based Open-Set 3DOR ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In practical scenarios, the categories of 3D objects can be labeled from various perspectives such as geometric and semantic, or from different levels of granularity. From a geometric perspective, a 3D object is naturally composed of multiple part-level shapes. For instance, a 3D model of a car can be deconstructed into several part-level shapes, such as wheels, chassis, doors, and windows. Each of these parts represents a distinct geometric part that contributes to the formation of the complete 3D object. This modular decomposition facilitates more efficient manipulation, analysis, and reconstruction, as each part can be independently modified or replaced while preserving the overall structural integrity of the automobile. These parts not only enhance the hierarchical structure and diversity of labels but also play a crucial role in the representation of 3D models. Consequently, they find applications in various fields, such as computer-aided design (CAD), where precise modeling of individual components is essential; virtual reality (VR), which relies on detailed and interactive 3D environments; and ", "page_idx": 11}, {"type": "text", "text": "Table 4: The category splitting on seen and unseen categories for the three datasets. ", "page_idx": 12}, {"type": "table", "img_path": "xOCAURlVM9/tmp/c9bad115fb34fe79c7333203db7f15acc56eac6187678b9ffdd1af062fe9013d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "xOCAURlVM9/tmp/a0373e2830d2de6805e7d25beaecb767e8d4ce327014115deb17ff1a230971b9.jpg", "table_caption": ["Table 5: Detailed statistic of OP-SHNP. "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "xOCAURlVM9/tmp/6946264ca3adf4ba565f68bb9e3fc2aac9f154ae654baa495e32096a9fe878a3.jpg", "table_caption": ["Table 6: Detailed statistic of OP-COSEG. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "medical imaging, where accurate representations of anatomical structures are critical for diagnosis and treatment planning. ", "page_idx": 12}, {"type": "text", "text": "As for the 3D object retrieval (3DOR) task, individual parts and combinations of multiple parts can serve as crucial clues for the representation and analysis of a complete 3D shape. The greater the number of parts, the higher the accuracy and uniqueness of the object representation. This characteristic is particularly advantageous for open-set 3DOR through a bottom-up lens of Part Assembly. Given the incomplete object-level labels in open-set settings, employing a multi-layered representation and analysis of individual and associated parts can significantly enhance the accuracy of open-set 3D object retrieval (3DOR). Furthermore, it may reduce the dependency on extensive training data, addressing a common limitation in open-set environments. ", "page_idx": 12}, {"type": "text", "text": "B OpenPart Dataset Generation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We generate three part-assembly driven open-set 3D object retrieval (OpenPart) datasets, including OP-SHNP, OP-INTRA, OP-COSEG based on the public dataset ShapeNetPart [42], IntrA [41], and COSEG [34]. We sampled the point cloud from the triangular surface for each dataset. Specifically, the point number for point clouds in OP-SHNP, OPINTRA, and OP-COSEG are 2048, 2048, and 1024, respectively. In the OpenPart datasets, we use the part ", "page_idx": 12}, {"type": "table", "img_path": "xOCAURlVM9/tmp/28b9cc5bf001570128dccb661954bab8d1d7a80f83bcbf409c04d5ba0bc3584c.jpg", "table_caption": ["Table 7: Detailed statistic of OP-INTRA. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "segmentation annotations of the original dataset, the classes are split into seen and unseen classes for training and testing as shown in Tab. 4. Besides, we provide more detailed statistics of the three datasets in Table 5, Table 6, and Table 7. ", "page_idx": 12}, {"type": "text", "text": "C Implemental Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The proposed Hypergraph-Based Assembly Fuzzy Representation (HAFR) framework is composed of two modules: Isomorphic Assembly Embedding $(I A E)$ and Structured Fuzzy Reconstruction (SFR). The basic features for input are extracted by PointNet [28] and each part feature is obtained by the average point feature of each region, we use the top four parts for each object in this paper. The IAE utilizes Hypergraph Isomorphism Convolution (HIConv) and assembly auto-encoders $\\mathcal{A}_{a}$ to generate assembly embeddings from the basic part features. The implemented details of the IAE and SFR modules are provided in Algorithm 1 and Algorithm 2, respectively. ", "page_idx": 12}, {"type": "text", "text": "Input: Basic part features $\\{f^{r}\\}_{k=1}^{P}$ .   \nParameter: $\\alpha=0.5$ .   \nOutput: Assembly embeddings $\\{c_{i}\\}_{i=1}^{N}$ . ", "page_idx": 13}, {"type": "text", "text": "1: Let $e p o c h=0$ ; 2: Initialize assembly hypergraph $\\mathcal{G}_{a}=\\{\\mathbf{X}_{h},\\mathcal{E}_{o}\\}$ ; 3: Construct vertices $\\begin{array}{r}{\\mathbf{X}_{f}=\\bigcup_{d=1}^{D}\\{f_{i}^{d}\\}_{i=1}^{N}}\\end{array}$ ; 4: Construct isomorphism h yperedge $\\tilde{\\mathcal{E}_{o}}\\doteq\\{\\mathcal{O}_{v}\\mid i\\in\\{1,\\cdots,N\\}\\}$ ; 5: Calculate diagonal degree matrices $\\mathbf{D}_{v}$ and $\\mathbf{D}_{e}$ ; 6: Calculate incidence matrix $\\mathbf{H}$ of $\\mathcal{G}_{a}$ ; 7: Initialize learnable HIConv parameters $\\Theta_{H I C o n v}$ of $\\mathcal{G}_{a}$ ; 8: Initialize assembly auto-encoder $\\mathcal{A}_{a}=\\{\\Psi,\\Phi\\}$ ;; 9: Initialize aggregation function $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ ; 10: while epoch $\\leq40$ do 11: Isomorphism embeddings $\\tilde{\\mathbf{X}}_{f}=\\mathbf{M}\\mathbf{L}\\mathbf{P}((1+\\epsilon)\\mathbf{X}_{f}+\\sigma(\\mathbf{D}_{v}^{-\\frac{1}{2}}\\mathbf{H}\\mathbf{D}_{e}^{-1}\\mathbf{H}^{\\top}\\mathbf{D}_{v}^{-\\frac{1}{2}}\\mathbf{X}_{f}\\Theta_{H I C o n v}));$ 12: Reshape isomorphism embeddings $\\{\\{c_{i}^{r}\\}_{r=1}^{P}\\}_{i=1}^{N}=\\tilde{\\mathbf{X}}_{f}$ 13: Get unified embeddings of each part $\\{\\{u_{i}^{r}\\}_{r=1}^{P}\\}_{i=1}^{N}=\\Psi^{r}(\\{\\{c_{i}^{r}\\}_{r=1}^{P}\\}_{i=1}^{N})$ . 14: Get assembly embeddings of each object $\\{u_{i}\\}_{i=1}^{N}=\\{\\mathcal{B}(\\{u_{i}^{r}\\}_{r=1}^{P})\\}_{i=1}^{N}$ 1 15: Get mixed embeddings if each part $\\{\\{\\hat{r}_{i}^{r}\\}_{r=1}^{P}\\}_{i=1}^{N}=\\Psi^{r}(\\{\\{c_{i}^{r}\\}_{r=1}^{P}\\}_{i=1}^{N})$ . 16: Calculate the Assembly Loss $\\begin{array}{r}{\\mathcal{L}_{a s}=\\frac{2}{R(R-1)}\\sum_{k=1}^{R}\\sum_{l=k+1}^{R}\\lVert u_{i}^{k}-u_{i}^{l}\\rVert_{2}}\\end{array}$ 17: Calculate the Cross-Part Loss Lxp =R(R2\u22121) $\\begin{array}{r}{\\mathcal{L}_{x p}=\\frac{2}{R(R-1)}\\sum_{k=1,l\\neq k}^{D}\\big(\\|c_{i}^{k}-\\hat{c}_{i}^{k}\\|_{2}+\\|c_{i}^{k}-\\Phi^{l}\\left(\\Psi^{k}\\left(c_{i}^{l}\\right)\\right)\\|_{2}\\big).}\\end{array}$ 18: Calculate loss for the IAE module $\\mathcal{L}_{I A E}=\\alpha\\mathcal{L}_{a s}+(1-\\alpha)\\mathcal{L}_{x p}$ . 19: if $\\mathcal{L}_{I A E}$ does not converges then 20: Update parameters of $\\Theta_{H I C o n v}$ and $\\mathcal{A}_{a}$ by $\\mathcal{L}_{I A E}$ . 21: $e p o c h+=1$ 22: else 23: Break. 24: end if 25: end while 26: return Assembly embeddings $\\{u_{i}\\}_{i=1}^{N}$ ", "page_idx": 13}, {"type": "text", "text": "Our experiments were conducted on a Tesla V100-32G GPU and an Intel(R) Xeon(R) Silver 4210 CPU $@$ 2.20GHz. The hyper-parameters \u201ck\u201d in the SFR module are set to 20, 6, and 40 for OP-SHNP, OP-INTRA, and OP-COSEG, respectively. ", "page_idx": 13}, {"type": "text", "text": "D More Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Since no methods are specifically designed for part-assembly driven open-set 3DOR, we refine the current state-of-theart close-set 3DOR methods (MMJN [26], TCL [13], SDML [14], MMSAE [36]), and open-set 3D learning methods (PROSER [46], $\\mathrm{HGM}^{2}\\mathrm{R}$ [8]), then we refine the multi-modal fusion module with multi-part fusion for each method. Specifically, these compared methods are implemented by: ", "page_idx": 13}, {"type": "text", "text": "MMJN [26]: MMJN is a multi-modal   \njoint network that employs weighted fu  \nsion to integrate features across multiple   \nmodalities for retrieval. Specifically, we   \ngenerate the assembly embeddings by auto-encoders and utilize them in the classification fusion part of the MMJN network. Input: Assembly embeddings $\\{u_{i}\\}_{i=1}^{N}$ .   \nParameter: $\\beta=0.9$ .   \nOutput: Fuzzy embeddings $\\{z_{i}\\}_{i=1}^{N}$ . ", "page_idx": 13}, {"type": "table", "img_path": "xOCAURlVM9/tmp/a7d5f0aa558b9d3185a0377d8bfd7df048db87224d0a78705844a95d2a188e60.jpg", "table_caption": ["Table 8: The hyper-parameters of the HAFR framework. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "1: Let $e p o c h=0$ ; 2: Initialize leverage hypergraph $\\mathcal{G}_{l e v}=\\{\\mathbf{X}_{u},\\mathcal{E}_{l e v}\\}$ ; 3: Construct vertices $\\mathbf{X}_{u}=\\{u_{i}\\}_{i=1}^{N}$ ; 4: Construct local-uncertainty hyperedge ${\\mathcal{E}}_{c}=\\{C_{v}(y)\\ |\\ y\\in\\mathcal{Y}\\}$ ; 5: Construct global-uncertainty hyperedge $\\mathcal{E}_{u}=\\{K_{\\mathrm{KNN}_{k}}(v)\\ |\\ v\\in\\mathcal{V}\\}$ ; 6: Construct hyperedges $\\mathcal{E}_{l e v}=\\mathcal{E}_{c}\\cup\\mathcal{E}_{u}$ ; 7: Calculate diagonal degree matrices $\\mathbf{D}_{v}$ and $\\mathbf{D}_{e}$ ; 8: Calculate incidence matrix $\\mathbf{H}$ of $\\mathcal{G}_{l e v}$ ; 9: Initialize HGNNConv parameters $\\Theta_{l e v}$ of $\\mathcal{G}_{l e v}$ ; 10: Construct memory bank $\\mathcal{M}$ ; 11: while epoch $\\leq120$ do 12: Get propagation embeddings $\\tilde{\\mathbf{X}}_{u}=\\sigma\\left(\\mathbf{D}_{v}^{-\\frac{1}{2}}\\mathbf{H}\\mathbf{D}_{e}^{-1}\\mathbf{H}^{\\top}\\mathbf{D}_{v}^{-\\frac{1}{2}}\\mathbf{X}_{u}\\boldsymbol{\\Theta}_{l e v}\\right);$ ; 13: Reshape propagation embeddings $\\{p_{i}\\}_{i=1}^{N}=\\tilde{\\mathbf{X}}_{u}$ ; 14: Get activation $s_{i,j}=\\|\\tilde{u}_{i}-m_{j}\\|_{2}$ ; 15: Get fuzzy embeddings $\\begin{array}{r}{\\{z_{i}\\}_{i=1}^{N}=\\{\\sum_{j=1}^{L}s_{i,j}^{\\prime}m_{j}\\}_{i=1}^{N}}\\end{array}$ ; 16: Calculate the Cross-Entropy Loss $\\begin{array}{r}{\\mathcal{L}_{c e}=-\\sum_{k=1}^{L}\\Big(n_{i,k}\\mathrm{log}(p_{i,k})+n_{i,k}\\mathrm{log}(q_{i,k})\\Big)}\\end{array}$ ; 17: Calculate the Fuzzy Reconstruction Loss $\\mathcal{L}_{f z}=\\left|\\left|\\tilde{u}_{i}-z_{i}\\right|\\right|_{2}$ ; 18: Calculate loss for the SFR module $\\mathcal{L}_{S F R}=\\beta\\mathcal{L}_{f z}+(1-\\beta)\\mathcal{L}_{c e}$ . 19: if $\\mathcal{L}_{S F R}$ does not converges then 20: Update parameters of $\\Theta_{l e v}$ and $\\mathcal{M}$ by $\\mathcal{L}_{S F R}$ . 21: $e p o c h+=1$ 22: else 23: Break. 24: end if 2256::  erentdu rwnh iFluezzy embeddings $\\{z_{i}\\}_{i=1}^{N}$ . 27: Retrieval by fuzzy embeddings {zi}iN=1. ", "page_idx": 14}, {"type": "text", "text": "TCL [13]: TCL is a method based on metric learning, combining triplet and center loss to get unified fusion embeddings from different modalities. We use the center-based method for object center embedding from different parts ", "page_idx": 14}, {"type": "text", "text": "SDML [14]: SDML is a metric-learning-based method for cross-modal retrieval, which learns projection functions for different modalities independently. We construct the assembly embeddings by auto-encoders and use the joint supervision aligned with its triplet center loss. ", "page_idx": 14}, {"type": "text", "text": "MMSAE [36]: MMSAE is a multi-modal retrieval method using auto-encoders. It trains encoders with a reconstruction loss function to align embeddings from various modalities into a unified latent space. We add auto-encoders to generate assembly embeddings, and we use them for alignment with semantic code vectors for 3D object retrieval. ", "page_idx": 14}, {"type": "text", "text": "PROSER [46]: PROSER is an open-world recognition method that extends the closed-set classifier to determine if a sample belongs to seen categories or not. We construct auto-encoders for assembly embedding, and we mix up the assembly embeddings with the multiple dummy classifiers in the PROSER. ", "page_idx": 14}, {"type": "text", "text": "$\\mathbf{HGM}^{2}\\mathbf{R}$ [8]: $\\mathrm{HGM^{2}R}$ is an open-world 3D multi-modal retrieval method, which retrieves the objects from unseen categories through structure-aware learning. We construct auto-encoders for assembly embedding of 3D parts, and we use the KNN-based hyperedges in the SAIKL module. ", "page_idx": 14}, {"type": "text", "text": "We provide the visualized results of assembly embeddings and fuzzy embeddings in Fig. 6 and Fig. 7. The visualizations indicate that the SFR module is capable of accurately representing open-set categories, which can effectively enhance retrieval performance. ", "page_idx": 14}, {"type": "image", "img_path": "xOCAURlVM9/tmp/bb3266195a29849b7f25ec33d1f3db550253434e8e2df14c70cb3101040b87b9.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 6: The t-SNE visualization of the embeddings from unseen categories in the OP-INTRA and OP-SHNP datasets. ", "page_idx": 15}, {"type": "image", "img_path": "xOCAURlVM9/tmp/c96ec883f090e81cd006b5f2ae6746af1b7b1fa22082069f4d5b913c19c78a42.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 7: The t-SNE visualization of the embeddings from unseen categories in the OP-SHNP dataset. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 15}, {"type": "text", "text": "Justification: We provide comprehensive motivations and detailed introductions of all the contributions in the abstract and introduction. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] Justification: We discussed the current limitations of this work in the conclusion. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provided a formalized, step-by-step explanation of each procedure in the methodology. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide detailed experiment and reproduction settings in the experiments and appendix. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 16}, {"type": "text", "text": "Justification: We are in the preparation of the codes, models, and datasets, and they will be released at the earliest opportunity. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] Justification: We provide all experimental settings and details settings in the experiments and appendix. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We explain the evaluation protocols of the experiments. ", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide the sufficient information on the computer resources in appendix. ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have carefully read and adhered to the NeurIPS Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We discussed the broader impacts of this work in the conclusion. ", "page_idx": 16}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have obtained and adhered to the permissions for the datasets used and have cited them in the paper. ", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We will release the dataset and provide the corresponding documentation immediately upon the paper\u2019s acceptance. ", "page_idx": 17}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 17}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 17}]