[{"figure_path": "hOcsUrOY0D/figures/figures_1_1.jpg", "caption": "Figure 1: Test accuracy (x-axis) of a privately finetuned GPT-2 on SST-2 text sentiment classification dataset (top) and a convolutional neural network on CIFAR-10 image classification dataset (bottom). The DP noise is calibrated to guarantee at most a certain level of privacy attack sensitivity (y-axis) at three possible attack false-positive rates \u03b1 \u2208 {0.01, 0.05, 0.1}. See Section 4 for details.", "description": "This figure compares the test accuracy of two different machine learning models (GPT-2 for text sentiment classification and CNN for image classification) trained with two different noise calibration methods: standard calibration and attack risk calibration.  The x-axis represents the task accuracy achieved by the model, and the y-axis represents the attack risk (sensitivity).  Three different false positive rates (\u03b1) are shown.  The figure demonstrates that directly calibrating noise to attack risk (our method) leads to higher accuracy compared to standard calibration for the same level of risk.", "section": "4 Experiments"}, {"figure_path": "hOcsUrOY0D/figures/figures_6_1.jpg", "caption": "Figure 2: Benefits and pitfalls of advantage calibration.", "description": "This figure compares two methods for calibrating noise in differentially private machine learning models: standard calibration and advantage calibration.  The x-axis represents the attack advantage (\u03b7), a measure of an attacker's success in recovering information. The y-axis represents the noise scale (\u03c3), which is inversely proportional to model utility.  Panel (a) shows that advantage calibration substantially reduces the required noise scale compared to standard calibration, improving utility. However, panel (b) illustrates a potential pitfall: advantage calibration can inadvertently increase the attack power (\u0394\u03b2) in low-FPR regimes.", "section": "3.1 Calibration to Advantage"}, {"figure_path": "hOcsUrOY0D/figures/figures_6_2.jpg", "caption": "Figure 2: Benefits and pitfalls of advantage calibration.", "description": "This figure shows the comparison of standard calibration and advantage calibration in terms of noise scale and attack risk. (a) shows that calibrating noise to attack advantage significantly reduces the required noise scale compared to the standard approach. (b) shows a pitfall of advantage calibration: it allows for higher attack power in the low FPR regime compared to standard calibration.", "section": "3.1 Calibration to Advantage"}, {"figure_path": "hOcsUrOY0D/figures/figures_8_1.jpg", "caption": "Figure 3: Calibration to attack TPR (i.e., 1\u2013FNR) significantly reduces the noise scale in low FPR regimes. Unlike calibration for attack advantage, this approach does not come with a deterioration of privacy for low FPR, as it directly targets this regime.", "description": "This figure shows the results of calibrating noise to the attack True Positive Rate (TPR, which is 1-FNR) at three different False Positive Rate (FPR) levels (0.01, 0.05, and 0.1).  The x-axis represents the attack TPR, and the y-axis represents the noise scale (\u03c3). The figure compares the noise scale required using the standard calibration method (blue line) versus the proposed TPR/FPR calibration method (orange line).  The results demonstrate that the proposed method requires significantly less noise to achieve the same level of privacy risk (specified by the FPR and TPR) compared to the standard calibration. The key finding is that directly calibrating to TPR/FPR avoids the pitfall of advantage calibration, which is a decrease in privacy for the low FPR regime.", "section": "Experiments"}, {"figure_path": "hOcsUrOY0D/figures/figures_8_2.jpg", "caption": "Figure 1: Test accuracy (x-axis) of a privately finetuned GPT-2 on SST-2 text sentiment classification dataset (top) and a convolutional neural network on CIFAR-10 image classification dataset (bottom). The DP noise is calibrated to guarantee at most a certain level of privacy attack sensitivity (y-axis) at three possible attack false-positive rates \u03b1 \u2208 {0.01, 0.05, 0.1}. See Section 4 for details.", "description": "This figure shows the test accuracy achieved by two different models (GPT-2 for text sentiment classification and CNN for image classification) trained with differential privacy.  The x-axis represents the accuracy of the model, while the y-axis represents the sensitivity of a privacy attack. Different lines represent different false positive rates (\u03b1) for the privacy attack.  The figure demonstrates that direct calibration of noise to attack risk (our method) leads to significantly higher accuracy than standard calibration for the same level of attack risk.", "section": "1 Introduction"}, {"figure_path": "hOcsUrOY0D/figures/figures_17_1.jpg", "caption": "Figure 1: Test accuracy (x-axis) of a privately finetuned GPT-2 on SST-2 text sentiment classification dataset (top) and a convolutional neural network on CIFAR-10 image classification dataset (bottom). The DP noise is calibrated to guarantee at most a certain level of privacy attack sensitivity (y-axis) at three possible attack false-positive rates a \u2208 {0.01, 0.05, 0.1}. See Section 4 for details.", "description": "This figure shows the test accuracy achieved by two different machine learning models (GPT-2 for text sentiment classification and CNN for image classification) trained using differential privacy. The x-axis represents the test accuracy, and the y-axis represents the attack sensitivity.  The figure compares the standard calibration method with the proposed attack-aware calibration method.  The results demonstrate that the attack-aware calibration method achieves higher accuracy at the same privacy level, demonstrating that directly calibrating noise to attack risk leads to significantly better model utility.", "section": "4 Experiments"}, {"figure_path": "hOcsUrOY0D/figures/figures_26_1.jpg", "caption": "Figure 5: Trade-off curves of a Gaussian mechanism that satisfies (\u03b5, \u03b4)-DP. Each curve shows a boundary of the feasible region (greyed out) of possible membership inference attack FPR (\u03b1) and FNR (\u03b2) pairs. The solid curve shows the limit of the feasible region guaranteed by DP via Eq. (5), which is a conservative overestimate of attack success rates compared to the exact trade-off curve (dotted). The maximum advantage \u03b7 is achieved with FPR and FNR at the point closest to the origin.", "description": "This figure illustrates the trade-off between the false positive rate (FPR) and the false negative rate (FNR) for membership inference attacks against a Gaussian mechanism satisfying (\u03b5, \u03b4)-differential privacy.  The shaded region represents the area of possible (FPR, FNR) pairs allowed by the (\u03b5, \u03b4)-DP guarantee.  The solid line shows a conservative approximation of this region, while the dotted line provides a more accurate representation of the achievable trade-off.  The point closest to the origin (0,0) corresponds to the maximum advantage an attacker can achieve.", "section": "2.2 Operational Privacy Risks"}, {"figure_path": "hOcsUrOY0D/figures/figures_26_2.jpg", "caption": "Figure 6: The increase in attack sensitivity due to calibration for advantage is less drastic for Gaussian mechanism than for a generic (\u03b5, \u03b4)-DP mechanism.", "description": "This figure compares the attack sensitivity (FNR) for two different calibration methods: standard calibration and advantage calibration.  Both methods are applied to a Gaussian mechanism. The plot shows that while both methods result in a trade-off between attack FPR and FNR, the increase in attack sensitivity when using advantage calibration is less pronounced compared to a generic (\u03b5, \u03b4)-DP mechanism. This suggests that calibrating directly to the desired attack risk (advantage) might be less detrimental to utility for Gaussian mechanisms than for other mechanisms.", "section": "Experiments"}, {"figure_path": "hOcsUrOY0D/figures/figures_26_3.jpg", "caption": "Figure 3: Calibration to attack TPR (i.e., 1\u2013FNR) significantly reduces the noise scale in low FPR regimes. Unlike calibration for attack advantage, this approach does not come with a deterioration of privacy for low FPR, as it directly targets this regime.", "description": "The figure shows the results of calibrating the noise to achieve a target attack TPR (true positive rate), which is 1 minus the FNR (false negative rate), at three different low FPR (false positive rate) levels. The standard calibration method and the proposed attack risk calibration method are compared. The results demonstrate that the attack risk calibration significantly reduces the required noise scale, especially in the low FPR regimes, without compromising privacy.", "section": "Experiments"}]