[{"type": "text", "text": "Invariant subspaces and PCA in nearly matrix multiplication time ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aleksandros Sobczyk Marko Mladenovic Mathieu Luisier IBM Research and ETH Zurich ETH Zurich ETH Zurich obc@zurich.ibm.com mmladenovic@ethz.ch mluisier@iis.ee.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Approximating invariant subspaces of generalized eigenvalue problems (GEPs) is a fundamental computational problem at the core of machine learning and scientific computing. It is, for example, the root of Principal Component Analysis (PCA) for dimensionality reduction, data visualization, and noise filtering, and of Density Functional Theory (DFT), arguably the most popular method to calculate the electronic structure of materials. Given Hermitian $\\mathbf{H},\\mathbf{S}\\in\\mathbb{C}^{n\\times n}$ , where S is positive-definite, let $\\Pi_{k}$ be the true spectral projector on the invariant subspace that is associated with the $k$ smallest (or largest) eigenvalues of the GEP $\\mathbf{H}\\mathbf{C}\\,=\\,\\mathbf{S}\\mathbf{C}\\Lambda$ , for some $k\\,\\in\\,[n]$ . We show that we can compute a matrix $\\tilde{\\Pi}_{k}$ such that $\\|\\boldsymbol{\\Pi}_{k}-\\widetilde{\\boldsymbol{\\Pi}}_{k}\\|_{2}\\leq\\epsilon$ in $O\\left(n^{\\omega+\\eta}\\operatorname{polylog}(n,\\epsilon^{-1},\\kappa(\\mathbf{S}),\\operatorname{gap}_{k}^{-1})\\right)$ bit operations in the floating point model, for some $\\epsilon\\in(0,1)$ , with probability $1-1/n$ Here, $\\eta\\,>\\,0$ is arbitrarily small, $\\omega\\,\\lesssim\\,2.372$ is the matrix multiplication exponent, $\\kappa(\\mathbf{S})=\\|\\mathbf{S}\\|_{2}\\|\\mathbf{S}^{-1}\\|_{2}$ , and $\\mathrm{gap}_{k}$ is the gap between eigenvalues $k$ and $k+1$ To achieve such provable \u201cforward-error\u201d guarantees, our methods rely on a new $O(n^{\\omega+\\eta})$ stability analysis for the Cholesky factorization, and a smoothed analysis for computing spectral gaps, which can be of independent interest. Ultimately, we obtain new matrix multiplication-type bit complexity upper bounds for PCA problems, including classical PCA and (randomized) low-rank approximation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generalized eigenvalue problems (GEPs) arise naturally in a plethora of applications in machine learning, scientific computing, and engineering. Given a pair of matrices $\\mathbf{H}$ and S, often referred to as a matrix pencil, the problem of interest has the following form ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathbf{H}\\mathbf{C}=\\mathbf{S}\\mathbf{C}\\mathbf{A},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mathbf{C}$ and $\\pmb{\\Lambda}$ are the unknown eigenvector and eigenvalue matrices, respectively. Of particular importance are the so-called \u201cHermitian definite\u201d or simply \u201cdefinite\u201d GEPs/pencils, in which case $\\mathbf{H}$ is Hermitian and S is Hermitian and positive-definite. In many important applications, the quantity of interest is an (arbitrarily large) subset of eigenvectors, defining an invariant subspace, rather than the entire $\\mathbf{C}$ and $\\Lambda$ solutions of the GEP. ", "page_idx": 0}, {"type": "text", "text": "In data science and machine learning, invariant subspaces play a central role in many problems, including Spectral Clustering [111, 124], Language Models [73], Image Processing [118, 11], Recommendation Systems [46], Principal Components Analysis (PCA) [45, 79, 122], Support Vector Machines [99], and many others [50, 35, 96, 12]. We particularly focus on PCA applications, which can take the form of a GEP as in Eq. (1) where $\\mathbf{H}$ is the sample covariance and S the identity. In more advanced settings, H and S can be defined over a kernel [122, 20]; See Section 4 and Appendix G for more details. Another closely related application comes from Density Functional ", "page_idx": 0}, {"type": "text", "text": "Theory [88] (DFT), which is not a machine learning problem per se, but it is probably the most commonly used method (it was awarded the Nobel prize in Chemistry in 1998) to compute the electronic and structural properties of materials. In this case, H is the Hamiltonian and S the overlap matrix (cf. Appendix F). The spectral projector on the invariant subspace corresponding to the smallest generalized eigenvalues (occupied energies) directly provides the density matrix and the electron density. Obtaining them often presents a challenge from the computational point of view. ", "page_idx": 1}, {"type": "text", "text": "1.1  Problem definition ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The main focus of this work is the computation of spectral projectors on invariant subspaces that are associated with a subset of the spectrum of Hermitian definite GEPs. As the Abel-Ruffini theorem excludes exact computation, even in exact arithmetic, we seek for approximate computations, as described in the following Problem 1.1. ", "page_idx": 1}, {"type": "text", "text": "Problem 1.1 (Spectral projector). Given a Hermitian definite GEP $\\mathbf{H}\\mathbf{C}\\,=\\,\\mathbf{S}\\mathbf{C}\\Lambda$ of size $n$ .an integer $1\\leq k\\leq n-1$ , and accuracy $\\epsilon\\in(0,1)$ , compute a matrix $\\ensuremath{\\widetilde{\\mathbf{I}}}_{k}\\in\\mathbb{C}^{n\\times n}$ such that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\mathbf{I}}_{k}-\\mathbf{I}\\mathbf{I}_{k}\\right\\|\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\Pi_{k}$ is the true spectral projector on the invariant subspace associated with the $k$ smallestor largest eigenvalues. ", "page_idx": 1}, {"type": "text", "text": "Before proposing algorithms to solve Problem 1.1, we first make some clarifications and define usefulconcepts. ", "page_idx": 1}, {"type": "text", "text": "Type of approximation:  The approximation of the form of Equation (2) is commonly called a \"forward approximation\u201d or \"forward error' in numerical analysis. It quantifies the distance between the true solution of the problem and the one returned by an algorithm. It is a stronger and harder to achieve notion of approximation than the related \u201cbackward error.\" For details see Appendix A.1. ", "page_idx": 1}, {"type": "text", "text": "Model of computation: While many finite precision models of computation exist in the literature, all algorithms in this work are analyzed in the floating point model of computation, which is also the prominent model implemented in existing computers. Each real number $\\alpha$ is rounded to a floating point number $\\mathsf{f l}(\\alpha)=(1+\\theta)\\alpha$ where $\\theta\\in\\mathbb{C}$ satisfies $|\\theta|\\leq\\mathbf{u}\\in\\mathbb{R}_{>0}$ . The machine precision u bounds also the errors introduced by arithmetic operations $\\{+,-,\\times,/\\}$ , and the expression $\\log(1/\\mathbf{u})$ gives the number of bits required to achieve the desired precision. More details can be found in Appendix A.2. ", "page_idx": 1}, {"type": "text", "text": "Bit complexity:  The complexity of numerical algorithms is often measured in terms of the arithmetic operations executed, commonly referred to as arithmetic complexity. A more realistic notion is the bit complexity, which bounds the number of boolean operations. In the foating point model, it is straightforward to translate the arithmetic to the bit complexity if we have an upper bound on the number of bits. For instance, arithmetic operations on $b$ bits can be typically carried out in $O(b^{2})$ bit operations, or even faster by using more advanced algorithms [123, 55, 68]. ", "page_idx": 1}, {"type": "text", "text": "Matrix multiplication time:  In two seminal works [39, 40], it was demonstrated that matrix multiplication and other fundamental problems in Numerical Linear Algebra can be solved in the floating point model with nearly $O(n^{\\bar{\\omega}+\\eta})$ bit-complexity (up to polylogarithmic factors), where $\\eta$ is an arbitrarily small positive number and $\\omega$ is the matrix multiplication exponent, to-date bounded by $\\omega\\lesssim2.372$ [47, 141, 6]. Hereafter, we will use the notation $\\bar{T}_{\\mathsf{M M}}(n)=\\bar{n}^{\\omega+\\eta}$ ", "page_idx": 1}, {"type": "text", "text": "1.2  Existing algorithms ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Here we give a brief overview of existing algorithms. We refer to Appendix A.7 for more details. GEPs in general can be solved using classic eigensolvers and related techniques in $\\widetilde O(n^{3})$ floating operations, e.g., by reducing the matrix (or pencil) to tridiagonal form with similarity transformations and applying the shifted QR algorithm on the tridiagonal matrix, or by using a divide-andconquer method (see [37, 109, 75, 8, 9, 64, 42, 10, 113] and references therein). Significant progresses beyond the $\\widetilde O(n^{3})$ bit complexity barrier have been made [40, 39, 15, 95, 25, 41, 107, 121]. ", "page_idx": 1}, {"type": "text", "text": "Regarding the computation of eigenvalues, two notable examples are the $\\widetilde{O}(T_{\\mathsf{M M}}(n))$ algorithm of [95] for the largest eigenvalye, and the ${\\widetilde{O}}(n^{2})$ algorithm of [107] for the spectral norm. ", "page_idx": 2}, {"type": "text", "text": "The first to have addressed the problem of computing invariant subspaces in nearly $O(T_{\\mathsf{M M}}(n))$ in floating point is [39] (see also [14, 13]). The authors described an iterative algorithm for the Schur decomposition, and showed that each individual step is numerically stable, and it takes $O(T_{\\mathsf{M M}}(n)$ operations. An end-to-end bound on the number of iterations to achieve a backward approximate solution was left open. More recently, the seminal work of [15] extended the analysis to obtain an end-to-end $\\widetilde{O}(T_{\\mathsf{M M}}(n))$ complexity to approximately diagonalize a matrix, and [41, 121] provided a rigorous analysis for the generalized eigenproblem case. In Corollary 1.7 and Proposition 1.1 of [15], it was also outlined how to translate the backward diagonalization error to a forward error for the eigenvectors. The reported bound, however, has two main limitations: it relies on simplicity of the spectrum, which is a strict assumption, and it requires as input an over-estimate on the eigenvector condition number of the problem, which is unknown, and [15] does not describe how to compute it (see Appendix B.1 for more details). In this work we describe how to overcome these limitations and provide a novel, end-to-end, provably accurate analysis (in the sense of Eq. (2)) for arbitrary invariant subspaces of definite GEPs with $\\widetilde{O}(T_{\\mathsf{M M}}(n))$ boolean complexity. ", "page_idx": 2}, {"type": "text", "text": "1.3  Contributions and methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our main contribution, summarized in the following Theorem 1.1 and Algorithm 1, is an end-to-end analysis to solve Problem 1.1 in nearly $O(T_{\\mathsf{M M}}(n))$ time. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.1. Let $(\\mathbf{H},\\mathbf{S})$ be a Hermitian defnite pencil of size n with $\\lVert\\mathbf{H}\\rVert,\\lVert\\mathbf{S}^{-1}\\rVert\\,\\le\\,1,\\,\\lambda_{1}\\,\\le$ $\\lambda_{2}\\leq...\\leq\\lambda_{n}$ its eigenvalues, $\\mathrm{gap}_{k}=\\lambda_{k+1}-\\lambda_{k}$ and $\\kappa(\\mathbf{S})=\\|\\mathbf{S}\\|\\|\\mathbf{S}^{-1}\\|$ .Algorithm $^{\\,l}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{\\Pi}_{k}\\gets\\mathsf{P R O J E C T O R}(\\mathbf{H},\\mathbf{S},k,\\epsilon),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "takes as inputs $\\mathbf{H}$ , S, an integer $k\\in[n-1].$ an error parameter $\\epsilon\\in(0,1)$ and returns a matrix $\\tilde{\\Pi}_{k}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[\\left\\|\\widetilde{\\boldsymbol{\\Pi}}_{k}-\\boldsymbol{\\Pi}_{k}\\right\\|\\leq\\epsilon\\right]\\geq1-1/n,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\bf{\\Pi}}_{{\\bf{I}}{\\bf{k}}}$ is the true spectral projector on the invariant subspace that is associated with the $k$ smallest (or largest) eigenvalues. The algorithm executes ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\mathsf{M M}}(n)\\left(\\log(\\frac{n}{\\mathrm{gap}_{k}})\\log(\\frac{1}{\\mathrm{gap}_{k}})+\\log(n\\kappa(\\mathbf{S}))\\log(\\kappa(\\mathbf{S}))+\\log\\left(\\log(\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\right)\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "foating pointoperations wih $\\begin{array}{r}{O\\left(\\log(n)\\left(\\log^{4}(\\frac{n}{\\mathrm{gap}_{k}})+\\log^{4}(n\\kappa(\\mathbf{S}))+\\log^{3}(\\frac{1}{\\epsilon\\,\\mathrm{gap}_{k}})\\log(\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\right)\\right)}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "bits of precision. Internally, the algorithm needs to generate a total ofat most ${\\widetilde{O}}(n)$ standardnormal floating point numbers using additional $O(\\log(\\log(n)))$ bits. ", "page_idx": 2}, {"type": "text", "text": "To achieve the results of Theorem 1.1, we provide a novel $O(T_{\\mathsf{M M}}(n))$ -type complexity analysis of several problems in numerical linear algebra that can be of independent interest. ", "page_idx": 2}, {"type": "text", "text": "In brief, our methodology is as follows. We first observe that if we can determine reasonable \u201cguesses\" for the spectral gap $\\widetilde{(\\mathrm{gap}_{k})}$ and for the midpoint $(\\widetilde{\\mu}_{k})$ betweenthe $\\lambda_{k}$ and $\\lambda_{k+1}$ eigenvalues then we can efficiently compute the spectral projector by approximating the sign function ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{sgn}({\\widetilde{\\mu}}_{k}-\\mathbf{S}^{-1}\\mathbf{H}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "using the analysis of [15] for the Newton iteration. The matrix $\\begin{array}{r}{\\frac{1}{2}({\\textbf I}+\\mathrm{sgn}(\\widetilde{\\mu}_{k}-{\\textbf S}^{-1}{\\textbf H}))}\\end{array}$ indeed transforms in exact arithmetic all eigenvalues that are smaller than $\\widetilde{\\mu}_{k}$ to $1$ and the ones larger than ${\\widetilde{\\mu}}_{k}$ to zero. As will be proved in Proposition 2.1, in Section 2, this approach is sufficient to provide an accurate spectral projector $\\tilde{\\Pi}_{k}$ in floating point. As a consequence, the problem reduces to approximating the aforementioned midpoint and gap. As a baseline, in Appendix B.1 we prove that this can be done in nearly $O(T_{\\mathsf{M M}}(n))$ with iterative inversion [39] and diagonalization [15] or, similarly, by iteratively calling generalized diagonalization [41]. However, this approach presents two drawbacks: It does not take advantage of the inherent symmetry of the problem, and, at the same time, it performs a full diagonalization when we are only interested in the gap between two specific eigenvalues, which is seemingly redundant. We formally prove this claim by designing a novel approach that achieves better complexity, typically by a factor of ${\\cal O}(\\log(n))$ (cf. Section 3.4). Importantly, no explicit diagonalization is necessary. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "To minimize the complexity of our algorithm, it is crucial to leverage symmetry. To that end we use the Cholesky factorization of S in the spirit of the Cholesky-QR algorithm [37]. We highlight that, while other factorizations have been solved in $O(T_{\\mathsf{M M}}(n))$ in floating point, an end-to-end analysis for Cholesky remains open. In exact arithmetic, for example, the LU of a Hermitian definite matrix directly provides its Cholesky and [39] showed that the LU factorization of non-symmetric matrices can be obtained in $O(T_{\\mathsf{M M}}(n))$ . However, when considering arithmetic errors, the relationship between LU and Cholesky does not hold in floating point, as demonstrated by the counter-example of Appendix C.5. Other fast Cholesky algorithms have been proposed for special classes of matrices, e.g., for matrices with well-defined separators [58, 94, 61] and graph Laplacians [91, 92]. However, they do not generalize to arbitrary dense matrices. Our analysis is the first to improve the classic $O(n^{3})$ floating point Cholesky algorithms [84, 71] for the general case, with provable error bounds. In the following Theorem 1.2 we summarize our new analysis of the Cholesky factorization Algorithm 2 (see also Appendix C). We note that the algorithm itself is not new, only its analysis. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1.2. Given a Hermitian positive-definite matrix $\\mathbf{M}$ there exists an algorithm $\\textbf{L}\\gets$ CHOLESKY(M), listed in Algorithm 2, which requires $O(T_{\\mathsf{M M}}(n))$ arithmetic operations.This algorithm is logarithmically stable, in a sense that, there exist global constants $c_{1}$ $c_{2},\\:c_{3}$ \uff0csuchthat forall $\\epsilon\\in(0,1)$ ,if executed in a floating point machine with precision ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq\\mathbf{u}_{\\mathrm{CHOLESKY}}:=\\epsilon\\frac{1}{c_{1}n^{c_{2}}\\kappa(\\mathbf{M})^{c_{3}\\log n}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "whichtranslates into $O\\left(\\log(n)\\log(\\kappa(\\mathbf{M}))+\\log(\\frac{1}{\\epsilon})\\right)$ required bits of precision, then it does not break down due to arithmetic errors, and the solution returned satisfies $\\lVert\\mathbf{L}\\mathbf{L}^{*}-\\mathbf{M}\\rVert\\leq\\epsilon\\lVert\\mathbf{M}\\rVert$ ", "page_idx": 3}, {"type": "text", "text": "This stand-alone result fulfills the definition of \u201clogarithmic-stability,\u2019 a notion of numerical stability that is commonly used in the related literature [39, 15]. Given this new Cholesky analysis, the following transformation of the GEP to a regular Hermitian eigenvalue problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{H}\\mathbf{C}=\\mathbf{S}\\mathbf{C}\\mathbf{\\Lambda}\\Rightarrow\\mathbf{L}^{*}\\mathbf{H}\\mathbf{L}(\\mathbf{L}^{-1}\\mathbf{C})=(\\mathbf{L}^{-1}\\mathbf{C})\\mathbf{\\Lambda},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "can be carried out accurately in $O(T_{\\mathsf{M M}}(n))$ in floating point, with provable forward-error bounds for all eigenvalues of the transformed problem. Here, $\\mathbf{L}$ is the Cholesky factor of $\\mathbf{S}^{-1}$ instead of S. Specifically, in Proposition C.3 in Appendix C.4, we prove that the corresponding Algorithm 4, $\\widetilde{\\mathbf{H}}\\gets\\mathsf{R E D U C E}(\\mathbf{H},\\mathbf{S},\\epsilon)$ , returns a Hermitian matrix $\\widetilde{\\mathbf{H}}$ such that, for any given accuracy $\\epsilon\\in(0,1)$ \uff0c ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left|\\lambda_{i}(\\widetilde{\\mathbf{H}})-\\lambda_{i}(\\mathbf{S}^{-1}\\mathbf{H})\\right|\\leq\\epsilon,\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The symmetry induced by the Cholesky transformation is crucial to design an efficient algorithm for the spectral gap. As described in Section 3, and analyzed in Appendices $\\mathrm{D}$ and E, any spectral gap or eigenvalue of a Hermitian definite pencil can be approximated by an iterative algorithm that uses only \u201ccounting-queries\", i.e., queries that ask how many eigenvalues are smaller than a given threshold. This way we completely avoid diagonalization, thus leading to a lower complexity. ", "page_idx": 3}, {"type": "text", "text": "To perform the counting queries efficiently, the transformed matrix H must be regularized with small random perturbations, in the spirit of smoothed analysis [128], which has recently drawn attention in the context of matrix algorithms [31, 25, 15, 41, 104] (see Appendix D for the analysis). These aforementioned works typically require a guarantee on the minimum eigenvalue gap of the perturbed matrix, e.g., [104] uses a Minami-type bound [105], while in [15, 41] the entire pseudospectrum of the perturbed matrix must be shattered with respect to a grid. The latter is even more challenging to achieve than a minimum gap and it requires $\\widetilde{O}(n^{2})$ random bits. Our algorithm is significantly less demanding in terms of randomness: All we need is the Wegner estimate [138, 4] for the density-ofstates of random Hermitian operators, and only ${\\widetilde{O}}(n)$ random bits in total. ", "page_idx": 3}, {"type": "text", "text": "Finally, in Section 4, we apply our main results to prove the first matrix multiplication-type upper bounds for the bit complexity of PCA algorithms. Specifically, for the standard PCA formulation, we show that we can first compute the spectral projector and then use deflation to obtain a basis for the desired low-dimensional embedding in nearly matrix multiplication time. We then apply similar arguments to the seminal Block-Krylov PCA algorithm of [106]. ", "page_idx": 3}, {"type": "text", "text": "PROJECTOR Input: Hermitian definite pencil $\\mathbf{H}\\in\\mathbb{H}^{n}$ $\\mathbf{S}\\in\\mathbb{H}_{++}^{n}$ , gap index $k$ , accuracy E. Requires: $\\|\\mathbf{H}\\|\\leq1$ $\\leq1,\\,\\|\\mathbf{S}^{-1}\\|\\leq1,k\\in[n-1]$ Algorithm: $\\widetilde\\Pi\\gets\\mathsf{P R O J E C T O R}(\\mathbf{H},\\mathbf{S},k,\\epsilon)$ $\\begin{array}{r}{:\\widetilde{\\mu}_{k},\\widetilde{\\mathrm{gap}}_{k}\\gets\\mathsf{G A P}(\\mathbf{H},\\mathbf{S},k,\\frac{1}{8},\\frac{1}{2n})}\\end{array}$ 2: $\\widetilde{\\kappa}\\gets\\mathsf{C O N D}(\\mathbf{S},\\frac{1}{4},\\frac{1}{2n})$ \u2265 This is skipped when $\\mathbf{S}=\\mathbf{I}$ 3 $\\mathbf{\\widetilde{\\Pi}}\\cdot\\widetilde{\\mathbf{II}}\\gets\\mathsf{P U R I F Y}(\\mathbf{H},\\mathbf{S},\\widetilde{\\mu}_{k},\\widetilde{\\mathrm{gap}}_{k},\\widetilde{\\kappa},\\epsilon).$ 4: return f1. Output: Approximate projector $\\widetilde{\\bf\\Pi}^{\\prime}$ on the invariant subspace associated with the $k$ smallest eigenvalues. Ensures: $\\lVert\\widetilde{\\mathbf{I}}-\\mathbf{H}\\rVert\\leq\\epsilon$ with probability at least $1-1/n$ ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: PROJECTOR. ", "page_idx": 4}, {"type": "text", "text": "CHOLESKY Input: Matrix $\\mathbf{M}=\\left(\\mathbf{\\begin{array}{c c}{\\mathbf{A}}&{\\mathbf{B}^{*}}\\\\ {\\mathbf{B}}&{\\mathbf{C}}\\end{array}\\right)\\in\\mathbb{H}_{++}^{n}$ Requires: Mis positive-enite, u\u2264 cn(M)cs og(m) f for some constants $c_{1},c_{2},c_{3}$ and $\\epsilon\\in(0,1)$ Algorithm: $\\mathbf{L}\\leftarrow\\mathsf{C H O L E S K Y}(\\mathbf{M})$ 1: if $n=1$ 2: return ${\\bf L}=\\sqrt{{\\bf M}_{1,1}}$ 3: else 4: $\\begin{array}{r l r}&{\\mathbf{L}_{11}\\gets\\mathbf{C}\\mathbf{H}\\mathbf{O}\\mathbf{L}\\mathbf{E}\\mathbf{K}\\mathbf{Y}(\\mathbf{A}).}&{\\forall\\,\\mathbf{L}_{11}\\mathbf{L}_{11}^{*}=\\mathbf{A}+\\mathbf{E}_{\\mathbf{A}}^{\\mathbf{C}\\mathbf{H}}}\\\\ &{\\mathbf{B}\\mathbf{A}i\\gets\\mathbf{M}\\mathbf{M}\\left(\\mathbf{B},\\mathbf{I}\\mathbf{N}\\mathbf{V}\\left(\\mathbf{A}\\right)\\right).}&{\\forall\\,\\mathbf{B}\\mathbf{A}i=\\mathbf{B}\\left(\\mathbf{A}^{-1}+\\mathbf{E}_{1}^{\\mathrm{INV}}\\right)+\\mathbf{E}_{2}^{\\mathrm{MM}}=\\mathbf{B}\\mathbf{A}^{-1}+\\mathbf{E}_{\\mathbf{B}\\mathbf{A}i}.}\\\\ &{\\mathbf{L}_{21}\\gets\\mathbf{M}\\mathbf{M}\\left(\\mathbf{B}\\mathbf{A}i,\\mathbf{L}_{11}\\right).}&{\\forall\\,\\mathbf{\\mu}_{21}=\\left(\\mathbf{B}\\mathbf{A}^{-1}+\\mathbf{E}_{\\mathbf{B}\\mathbf{A}i}\\right)\\mathbf{L}_{11}+\\mathbf{E}_{3}^{\\mathrm{MM}}=\\mathbf{B}\\mathbf{A}^{-1}\\mathbf{L}_{11}+\\mathbf{E}_{\\mathbf{L}_{21}}.}\\\\ &{\\tilde{\\mathbf{S}}\\gets\\mathbf{C}-\\mathbf{H}\\mathbf{E}\\mathbf{R}\\mathbf{M}(\\mathbf{M}\\mathbf{M}(\\mathbf{B}\\mathbf{A}i,\\mathbf{B}^{*})).}&{\\forall\\,\\tilde{\\mathbf{S}}=\\mathbf{C}-\\left((\\mathbf{B}\\mathbf{A}i)\\mathbf{B}^{*}+\\mathbf{E}_{4}^{\\mathrm{MM}}\\right)+\\mathbf{E}_{5}^{\\mathrm{SUB}}=\\mathbf{S}+\\mathbf{E}_{8}\\mathbf{\\mu}_{3}.}\\end{array}$ 5: 6: 7: $\\triangleright$ where $\\mathbf{E_{S}}=\\mathbf{B}\\mathbf{E}_{1}^{\\sf I N V}\\mathbf{B}^{*}+\\mathbf{E}_{2}^{\\sf M M}\\mathbf{B}^{*}+\\mathbf{E}_{4}^{\\sf M M}+\\mathbf{E}_{5}^{\\sf S U B}$ \uff0c 8: $\\begin{array}{r l}&{\\mathbf{L}_{22}\\leftarrow\\mathbf{C}\\mathsf{H O L E S K Y}(\\widetilde{\\mathbf{S}}).}\\\\ &{\\mathbf{return}\\;\\mathbf{L}=\\left(\\mathbf{L}_{11}\\right.}\\\\ &{\\mathbf{L}_{21}\\quad\\mathbf{L}_{22}\\right).}\\end{array}$ D L22L22 = S + EgH. 9: Output: Lower triangular Cholesky factor $\\mathbf{L}\\in\\mathbb{C}^{n\\times n}$ Ensures: $\\lVert\\mathbf{L}\\mathbf{L}^{*}-\\mathbf{M}\\rVert\\leq\\epsilon\\lVert\\mathbf{M}\\rVert$ \uff0c $\\mathbf{L}$ is lower triangular, and $\\mathbf{LL}^{*}$ is Hermitian and positive-definite. ", "page_idx": 4}, {"type": "text", "text": "1.4  Notation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Matrices are denoted by bold capital letters and vectors by bold small letters. For real or complex constants we typically use Greek letters, or the Latin letters $c,C$ . The vector $\\mathbf{e}_{i}$ denotes the $i$ -th column of the standard basis. $\\mathbf{A}^{*}$ is the conjugate transpose of $\\mathbf{A}$ and $\\mathbf{A}^{\\dagger}$ denotes the pseudoinverse. The 2-norm is the default for matrices and vectors. $\\kappa(\\mathbf{A})=\\|\\mathbf{A}\\|\\|\\mathbf{A}^{\\dagger}\\|$ is the two-norm condition number of A. For the error analysis of the various algorithms, we use $\\mathbf{E}_{i}^{\\mathsf{O P}}$ to denote the error matrices that are introduced by the foating point errors of the $i$ -th operation OP. The letters $\\epsilon$ and $\\delta$ typically denote (scalar) error quantities and failure probabilities, respectively. $[n]$ is the set $\\{1,2,...,n\\}$ . We denote by $\\mathbb{H}^{n}\\subset\\mathbb{R}^{n}$ the set of Hermitian matrices of size $n\\times n$ $\\mathbb{H}_{+}^{n}$ the set of Hermitian positive semi-definite matrices and $\\mathbb{H}_{++}^{n}$ the set of Hermitian positive definite matrices. For a matrix A and a scalar $z$ we write $z\\pm\\mathbf{A}$ as a shorthand for $z\\mathbf{I}\\pm\\mathbf{A}$ $\\Lambda(\\mathbf{A})$ and $\\Lambda({\\bf A},{\\bf B})$ denote the spectrum of a matrix $\\mathbf{A}$ and a matrix pencil $(\\mathbf{A},\\mathbf{B})$ , respectively. The eigenvalues and singular values are always sorted in ascending order by default: $\\lambda_{1}\\leq\\lambda_{2}\\leq\\ldots\\leq\\lambda_{n}$ $\\Lambda_{\\epsilon}(\\mathbf{A})$ isthe $\\epsilon$ -pseudospectrum of A (see Definition A.2). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "2   Computing spectral projectors with the sign function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given a Hermitian definite pencil $(\\mathbf{H},\\mathbf{S})$ , our ultimate goal is to compute a forward error approximation of the spectral projector that is associated with the $k$ smallest eigenvalues, as described in Problem 1.1. Algorithm 3 solves this problem provably and efficiently, but it requires that we already have a suitable approximation of the eigenvalue gap that separates the desired subspace from the rest of the eigenspace. The algorithm is called PURIFY since it is inspired by \u201cpurification\" techniques in DFT, referring to the removal of the unoccupied orbitals. The computation of the gap and the midpoint is in fact the bottleneck of our main algorithm, however, we still show that they can be computed efficiently in Section 3, and, importantly, without diagonalizing any matrices. The properties of Algorithm 3 are stated in Proposition 2.1. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2.1. Let $\\mathbf{H}\\in\\mathbb{H}^{n}$ with $\\left\\|\\mathbf{H}\\right\\|\\leq1,\\,\\mathbf{S}\\in\\mathbb{H}_{++}^{n}\\,w i t h\\left\\|\\mathbf{S}^{-1}\\right\\|\\leq1,\\,k\\in[n-1]$ and $\\epsilon\\in(0,1)$ $\\begin{array}{r}{\\mu_{k}=\\frac{\\lambda_{k}+\\lambda_{k+1}}{2}}\\end{array}$ $\\mathrm{gap}_{k}=\\lambda_{k}-\\lambda_{k+1}$ $\\lambda_{1}\\leq...\\leq\\lambda_{n}$ $(\\mathbf{H},\\mathbf{S})$ $\\Pi_{k}$ spectral projector associated with the $k$ smallest eigenvalues. If we have access to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\mu}_{k}\\in\\mu_{k}\\pm\\frac{1}{8}\\operatorname{gap}_{k}\\quad\\widetilde{\\mathrm{gap}}_{k}\\in(1\\pm\\frac{1}{8})\\operatorname{gap}_{k},\\quad\\widetilde{\\kappa}\\in[\\kappa(\\mathbf{S}),C\\kappa(\\mathbf{S})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some constant $C>1$ then Algorithm $^3$ computes $\\widetilde{\\Pi}_{k}\\,\\gets\\,\\mathsf{P U R I F Y}(\\mathbf{H},\\mathbf{S},\\widetilde{\\mu}_{k},\\widetilde{\\mathrm{gap}}_{k},\\widetilde{\\kappa},\\epsilon)$ such That $\\|\\widetilde{\\boldsymbol{\\Pi}}_{k}\\-\\boldsymbol{\\Pi}_{k}\\|\\leq\\epsilon$ $\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\left(\\log(\\frac{1}{\\sf g a p}_{k})+\\log(\\log(\\frac{\\kappa(\\bf S)}{\\epsilon\\operatorname{gap}_{k}}))\\right)\\right)}\\end{array}$ foating pont operaions using $O\\left(\\log(n)\\log^{3}({\\frac{1}{\\operatorname{gap}_{k}}})\\log\\!\\left({\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\operatorname{gap}_{k}}}\\right)\\right)$ bits of precison. ", "page_idx": 5}, {"type": "text", "text": "Proof. The full proof of Proposition 2.1 can be found in Appendix B. We briefly summarize it here. The main idea is to use the sign function algorithm from [15], SGN, to approximate $\\mathrm{sgn}(\\widetilde{\\mu}_{k}\\mathrm{~-~}$ $\\mathbf{S}^{-1}\\mathbf{H})$ . If we already know that $\\widetilde{\\mu}_{k}$ is a reasonable approximation of $\\mu_{k}$ , and that it is located inside the correct eigenvalue gap, then, in exact arithmetic, our problem is equivalent to computing $\\operatorname{sgn}(\\mu_{k}\\,-\\,\\mathbf{S}^{-1}\\mathbf{H})$ . The result can be used to filter the desired spectral projector, often referred as \"purification\" in the context of DFT. The main challenge is to ensure that all propagated numerical errors, success probabilities, and input parameters for all algorithms are well-defined and bounded. To obtain the final forward errors we must rely on matrix similarity arguments, the properties of the pseudospectrum, the eigenvalue bounds of Weyl and Kahan from Fact A.1, and Lemma B.1, which gives explicit bounds on the sign function under small floating point perturbations. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "The rest of the paper is devoted to the analysis of our new algorithm for the spectral gap and the midpoint based on eigenvalue counting queries, described in Theorem 3.1. For comparison purposes, in Appendix B.1 we analyze a diagonalization-based algorithm for the same task (which is a new result itself), specifically, using the state-of-the-art ElG algorithm of [15]. We compare the two algorithms in Section 3.4, demonstrating that our counting-based algorithm is indeed faster. ", "page_idx": 5}, {"type": "text", "text": "3  Fast spectral gaps with counting queries ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our core algorithm efficiently approximates spectral gaps based on \u201ceigenvalue counting queries\" only, thus avoiding an explicit (and expensive) diagonalization. To give some intuition on the main idea, consider the following simplified version of the problem. ", "page_idx": 5}, {"type": "text", "text": "Problem 3.1 (Gap finder). Let $\\lambda_{1}\\leq...\\leq\\lambda_{n}$ in $[-1,1]$ be $n$ (unknown) real values (e.g., they can be the eigenvalues of the original matrix pencil) +>+1, and gapk =\u5165k+1 -\u5165k.for some $k\\in[n-1]$ .Given $k$ and some error parameter $\\epsilon\\in(0,1/\\bar{2})$ as input, we want to approximate $\\mu_{k}$ and $\\mathrm{gap}_{k}$ up to additive $\\epsilon\\operatorname{gap}_{k}$ , i.e., we look for $\\widetilde{\\mu}_{k}=\\mu_{k}\\pm\\epsilon\\,\\mathrm{gap}_{k}$ and $\\widetilde{\\mathrm{gap}}_{k}\\in(1\\pm\\epsilon)\\,\\mathrm{gap}_{k}$ . Only queries of the following form can be performed: We fix a parameter $\\gamma\\in(0,1/2)$ which distorts all $\\lambda_{i}$ to some (unknown) $\\lambda_{i}^{\\prime}\\in[\\lambda_{i}-\\gamma,\\lambda_{i}+\\gamma]$ .We can then choose any value $h\\in[-1-\\gamma,1+\\gamma]$ and ask how many values $\\lambda_{i}^{\\prime}$ are smaller than $h$ For each $\\gamma$ we can query arbitrarily many different values for $h_{i}$ and each $h$ -query costs $q(1/\\gamma)=O(\\mathrm{polylog}(1/\\gamma))$ ", "page_idx": 5}, {"type": "text", "text": "The query cost is arbitrary to avoid trivial solutions by setting $\\gamma=0$ . The following proposition is proved in Appendix E: ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.1. Problem 3.1 can be solved iteratively by executing a total of $O(\\log(\\frac{1}{\\epsilon\\operatorname{gap}_{k}}))$ iterations and $\\Theta(1)$ queries per iteaion, where each query costs at most $q\\big(\\frac{1}{\\epsilon\\,\\mathrm{gap}_{k}}\\big)$ ", "page_idx": 6}, {"type": "text", "text": "3.1  Smoothed analysis of eigenvalue counting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To use the counting query model of Problem 3.1 and Proposition 3.1 to compute the spectral gap of a matrix pencil, we need a \u201cblack-box\" method to count eigenvalues that are_smaller than a threshold. We first describe a straightforward, deterministic algorithm COUNT $(\\widetilde{\\mathbf{X}},h,\\varepsilon)$ for this task (see Lemma E.1), which takes as input a Hermitian matrix $\\widetilde{\\mathbf{X}}$ , a scalar $h$ , and a parameter $\\varepsilon$ with the requirement that $\\sigma_{\\mathrm{min}}(h-\\widetilde{\\mathbf{X}})>\\varepsilon$ . It returns the precise number of eigenvalues of $\\widetilde{\\mathbf{X}}$ that are smaller than $h$ . The runtime of the algorithm depends on $\\log(1/\\varepsilon)$ , and must therefore be minimized. For this we resort to smoothed analysis: We apply a random perturbation to ensure that $\\varepsilon$ is at least polynomial in $1/n$ , up to some other factors detailed in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "To build a random \u201cregularizer,\u2019 in Definition D.1 we introduce a random oracle that samples numbers from a standard normal distribution and returns their floating point representation using a prespecified number of bits. Based on this simple oracle, we can design a foating point algorithm $\\widetilde{\\mathbf{X}}\\gets$ REGULARIZE $(\\mathbf{A},\\gamma,\\delta)$ which has the following properties described in Proposition 3.2: ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.2. Let A with $\\|\\mathbf{A}\\|\\leq1$ be a Hermitian matrix, $\\gamma,\\delta\\in(0,1/4)$ two given parameters, and $\\widetilde{\\mathbf{X}}\\gets\\mathsf{R E G U L A R I Z E}(\\mathbf{A},\\gamma,\\delta)$ . Let $\\mathsf{g}$ be an arbitrary (but fixed) grid of points in $[-2,2]$ with cardinality $|\\mathsf{g}|\\,=\\,T$ For every element $h_{i}\\ \\in{\\mathsf{g}}$ consider the matrices $\\mathbf{M}_{i}\\,=\\,h_{i}\\,-\\,{\\widetilde{\\mathbf{X}}}$ and $\\widetilde{\\mathbf{M}}=$ $h_{i}-\\widetilde{\\mathbf{X}}+\\mathbf{E}_{i}$ where $\\mathbf{E}_{i}$ denote the diagonal floating point error matricesinducedby the shift.All the following hold simultaneously with probability $1-2\\delta$ if we use $O(\\log(\\frac{T n}{\\gamma\\delta}))$ bits of precision: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widetilde{\\mathbf{X}}\\|\\le4/3,\\quad\\Big|\\lambda_{i}(\\widetilde{\\mathbf{X}})-\\lambda_{i}(\\mathbf{A})\\Big|\\le\\frac{9}{16}\\gamma,\\quad\\sigma_{\\operatorname*{min}}(\\widetilde{\\mathbf{M}}_{i})\\ge\\frac{\\gamma\\delta}{4n T\\sqrt{4\\pi\\ln(4n/\\delta)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof. The main result that we use in the proof can be traced back to the Wegner estimate for the density-of-states of Hermitian operators under random diagonal disorder [138]. See Appendix D and in particular D.2 for more details. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "3.2  Computing the gap and the midpoint ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We can now describe the algorithm GAP in Theorem 3.1, that computes the $k$ -th gap and the midpoint of a Hermitian definite pencil. The same methodology can be extended to approximate any singular value, as described in Proposition E.2 in Appendix E.3. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1 (GAP). Let $\\mathbf{H}\\,\\in\\,\\mathbb{H}^{n}$ \uff0c $\\mathbf{S}\\,\\in\\,\\mathbb{H}_{++}^{n}$ and $\\lVert\\bf H\\rVert$ $[\\|,\\|\\mathbf{S}^{-1}\\|\\leq1]$ which define a Hermitian definite pencil $(\\mathbf{H},\\mathbf{S})$ . Given $k\\in[n-1]$ , accuracy $\\epsilon\\in(0,1)$ and failure probability $\\delta\\in(0,1/2)$ there exists an algorithm ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{\\mu}_{k},\\widetilde{\\mathrm{gap}}_{k}\\gets\\mathsf{G A P}(\\mathbf{H},\\mathbf{S},k,\\epsilon,\\delta)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Wwhich retumis $\\widetilde{\\mu}_{k}\\,=\\,\\mu_{k}\\,\\pm\\epsilon\\,\\mathrm{gap}_{k}$ and $\\widetilde{\\mathrm{gap}_{k}}\\,=\\,(1\\pm\\epsilon)\\,\\mathrm{gap}_{k}$ where $\\begin{array}{r}{\\mu_{k}\\,=\\,\\frac{\\lambda_{k}+\\lambda_{k+1}}{2}}\\end{array}$ and $\\mathrm{gap}_{k}\\,=$ $\\lambda_{k}-\\lambda_{k+1}$ The algorithm requires ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\log(\\frac{1}{\\delta\\epsilon\\operatorname{gap}_{k}})\\log(\\frac{1}{\\epsilon\\operatorname{gap}_{k}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "arithmetic operations using $O\\left(\\log(n)\\left(\\log^{4}({\\frac{n}{\\delta\\epsilon\\,\\mathrm{gap}_{k}}})+\\log(\\kappa(\\mathbf{S}))\\right)\\right)$ bits, where $\\lambda_{i}$ are the eigenvalues of $(\\mathbf{H},\\mathbf{S})$ f $\\kappa(\\mathbf{S})$ is unknown, additional $\\begin{array}{r}{O(T_{\\sf M M}(n)\\log(\\frac{n\\kappa(\\mathbf{S})}{\\delta})\\log(\\kappa(\\mathbf{S})))}\\end{array}$ foating point operations and $O(\\log(n)\\log^{4}({\\frac{n\\kappa(\\mathbf{S})}{\\delta}}))$ bits are sufficient to compute it with Corollary $E.I$ ", "page_idx": 6}, {"type": "text", "text": "Proof. The full proof builds upon the results that are detailed in Appendices D and E. A summary is the following. We first fix our initial error parameter $\\gamma_{0}=1/8$ and call $\\widetilde{\\mathbf{H}}=$ REDUCE $(\\mathbf{H},\\mathbf{S},\\frac{\\gamma_{0}}{4})$ ", "page_idx": 6}, {"type": "text", "text": "(Algorithm 4), which internally uses CHOLESKY to reduce the GEP to a regular Hermitian one. From Proposition C.3, a Hermitian matrix $\\widetilde{\\mathbf{H}}$ is returned such that $\\begin{array}{r}{|\\lambda_{i}(\\widetilde{\\mathbf{H}})-\\lambda_{i}^{\\overline{{}}}(\\mathbf{H},\\mathbf{S})|\\leq\\frac{\\gamma_{0}}{4}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Next, we use the same counting query model as in Proposition 3.1. We first regularize $\\widetilde{\\mathbf{H}}$ using $\\begin{array}{r}{\\widetilde{\\mathbf{X}}\\gets\\mathsf{R E G U L A R I Z E}(\\widetilde{\\mathbf{H}},\\frac{\\gamma_{0}}{2},\\delta_{0}).}\\end{array}$ where $\\delta_{0}=\\delta/2$ is the initial failure probability. Conditioning on success of Proposition 3.2 (with probability $1-\\delta_{0}\\big]$ ), for all $i\\in[n]$ , it holds that $|\\lambda_{i}(\\widetilde{\\mathbf{X}})-\\lambda_{i}(\\widetilde{\\mathbf{H}})|\\leq$ $9\\gamma_{0}/16$ . Summing the two eigenvalue error bounds, we conclude that all eigenvalues of $(\\mathbf{H},\\mathbf{S})$ \uff0c which initially lie in $[-1,1]$ , are distorted by at most $\\gamma_{0}$ in $\\widetilde{\\mathbf{X}}$ . We now have all necessary tools to go back to the counting query model of Proposition 3.1: In the first step we construct a grid $_{\\mathrm{~g~}}=$ $\\{-1,-7/8,-6/8,\\ldots,7/8,\\dot{1},9/8\\}$ . Clearly, $|\\mathtt{g}|=\\Theta(1)$ . Since we conditioned on the success of Proposition 3.2, the regularization ensures that for every $h_{j}\\in\\mathsf{g}$ it holds that $\\sigma_{\\operatorname*{min}}(h_{j}-\\widetilde{\\mathbf{X}}\\!+\\!\\mathbf{E})\\geq\\varepsilon_{0}$ with Eo = 8lgln/Tlm(4n/8)\\* . This allows us to efficiently execute COUNT $(\\widetilde{\\mathbf{X}},h_{j},\\varepsilon_{0})$ for every $h_{j}$ ", "page_idx": 7}, {"type": "text", "text": "At the end of the first iteration, we have computed two intervals $I_{k}$ and $I_{k+1}$ , where $I_{k}$ contains $\\lambda_{k}$ and $I_{k+1}$ contains $\\lambda_{k+1}$ , and each interval has size at most 1, i.e., half the size of $[-1,1]$ .We continue by halving at each step both $\\gamma$ and $\\delta$ , constructing the corresponding grids as per the proof of Proposition 3.1, and counting eigenvalues over the grid. In each iteration after the first one, we keep track of two intervals $I_{k}$ and $I_{k+1}$ , and two corresponding grids $\\mathtt{g}_{k}$ and $\\mathsf{g}_{k+1}$ with size $\\bar{|\\mathbf{g}_{k}|}=\\bar{|\\mathbf{g}_{k+1}|}=\\Theta(1)$ . We therefore only need to execute a constant number of COUNT queries, and in each iteration the size of the intervals $I_{k}$ and $I_{k+1}$ is halved. The algorithm terminates $\\begin{array}{r}{m=O\\left(\\log(\\frac{1}{\\epsilon\\operatorname{gap}_{k}})\\right)}\\end{array}$ iterations and finally provides the advertised complexity, bit requirements, failure probability, and approximation guarantees. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "3.3 Sketch proof of Theorem 1.1 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The proof of our main Theorem 1.1 directly follows from Theorem 3.1 together with Proposition 2.1 as well as the algorithm SIGMAK (described in Appendix E.3) which is used to compute the condition number of S. The full proof can be found in Appendix E.4. ", "page_idx": 7}, {"type": "text", "text": "3.4  Comparison with diagonalization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We can now compare Theorem 3.1 with a diagonalization-based approach that is detailed in Proposition B.2. We fix $\\delta=O(1/n)$ so that both algorithms succeed with the same probability. ", "page_idx": 7}, {"type": "text", "text": "For $\\epsilon,\\mathrm{gap}_{k},\\kappa^{-1}(\\mathbf{S})\\,\\in\\,\\Omega(\\mathrm{poly}(1/n))$ , the total arithmetic complexity of the algorithm of Theorem 3.1 is $O(T_{\\mathsf{M M}}(n)\\log^{2}(n))$ using $O(\\log^{5}(n))$ bits. For the same parameters, Proposition B.2 requires need a total of $O(T_{\\mathsf{M M}}(n)\\log^{3}(n))$ arithmetic operations, and $O(\\log^{5}(n))$ bits. Thus, in total, Algorithm 3.1 is faster by a factor of ${\\mathcal{O}}(\\log(n))$ ", "page_idx": 7}, {"type": "text", "text": "In the extreme case where $\\epsilon$ $\\mathrm{gap}_{k}$ \uff0c $\\kappa(\\mathbf{S})=\\Theta(1)$ , Theorem 3.1 counts $O(T_{\\mathsf{M M}}(n)\\log(n))$ arithmetic operations and $O(\\log^{5}(n))$ bits, while Proposition B.2 requires $O(T_{\\mathsf{M M}}(n)\\log^{2}(n))$ operations, and $O(\\log^{5}(n))$ bits. Thus, Proposition B.2 is again slower by a factor of ${\\cal O}(\\log(n))$ . Interestingly, in this case Theorem 3.1 is faster than even a single call to ElIG, which requires $O(T_{\\mathsf{M M}}(n)\\log^{2}(n))$ arithmetic operations. We conclude that, at least based on the currently existing algorithms, diagonalization is redundant for the computation of spectral gaps and invariant subspaces. ", "page_idx": 7}, {"type": "text", "text": "3.5  Application in DFT ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Appendix F we demonstrate how our main results can be directly applied to approximate density matrices and electron densities of atomic systems in DFT. Even though is not a machine learning problem per se, we decided to dedicate a section in the Appendix due to its importance: DFT calculations persistently occupy supercomputing clusters and the corresponding software libraries and literature receive tens of thousands of citations annually at the time of this writing [89, 59, 60, 127, 74]. Our work is the first analysis to provide forward-error guarantees in finite precision for these problems in nearly matrix multiplication time. ", "page_idx": 7}, {"type": "text", "text": "Since its introduction in the early twentieth century [116, 72], Principal Component Analysis is one of the most important tools in statistics, data science, and machine learning. It can be used, for example, to visualize data, to reduce dimensionality, or to remove noise from data; cf. [79, 45] for reviews on the vast bibliography. In its simplest formulation, given a (centered) data matrix $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ , the goal is to learn a $k$ dimensional embedding $\\mathbf{C}_{k}$ where $k<n$ , that maximizes the sample variance, which can be written as an optimization problem ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{C}_{k}=\\arg\\operatorname*{max}_{\\mathbf{C}^{\\top}\\mathbf{C}=\\mathbf{I}_{k\\times k}}\\mathrm{tr}(\\mathbf{C}^{\\top}\\mathbf{H}\\mathbf{C}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{H}=\\mathbf{X}^{\\top}\\mathbf{X}\\in\\mathbb{R}^{n\\times n}$ is the sample covariance. It can be shown that the solution $\\mathbf{C}_{k}$ corresponds to the principal $k$ singular vectors of $\\mathbf{H}$ , i.e. the ones that correspond to the largest $k$ singular values. Evidently, since the sample covariance is always symmetric and positive semi-definite, this can be written as a Hermitian eigenvalue problem ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{H}\\mathbf{C}=\\mathbf{C}\\Lambda,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "(which is indeed a definite GEP as in Equation (1) with $\\mathbf{S}=\\mathbf{I}_{\\alpha}$ 0. By solving for $\\mathbf{C}_{k}$ , we can project the data in $k$ dimensions by computing $\\mathbf{XC}_{k}$ , preserving as much of the variance in $k$ dimensions as possible. To compute $\\mathbf{C}_{k}$ we can directly use our main results. However, the solution of Equation (3) is an actual orthonormal basis for the invariant subspace rather than the spectral projector that Theorem 1.1 returns. This can be addressed with defation: Once we have the spectral projector $\\tilde{\\Pi}_{k}$ , assuming that the approximation is sufficiently tight, we can apply a subsequent deflation step based on a rank-revealing QR factorization to obtain a $k$ -dimensional basis. This can be done deterministically in $O(n^{3})$ time [66] or in randomized $O(n^{\\omega})$ [39, 15]. ", "page_idx": 8}, {"type": "text", "text": "In Appendix G.1 we prove the following Theorem 4.1 for Algorithm 7, which builds upon our main Theorem 1.1, the algorithm of Proposition E.2 to approximate $\\|{\\bf X}-{\\bf X}_{k}\\|$ ,and theDEFLATE algorithm of [19], to solve the standard PCA problem of Eq. (3). Following the existing literature, the result is stated for real matrices, but it can be trivially adapted to the complex case as well. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1 (PCA).Let $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ beacentereddatamatrix, $\\mathbf{H}$ the $n\\times n$ symmetric sample covariancematrix,i.e., $\\mathbf{H}=\\mathbf{X}^{\\top}\\mathbf{X}$ $\\|\\mathbf{H}\\|\\leq1$ \uff0c $k\\,\\in\\,[n]$ a target rank, and $\\epsilon\\,\\in\\,(0,1)$ an accuracy parameter. Given $\\mathbf{H}_{\\mathrm{}}$ wecancomputeamatrix $\\widetilde{\\mathbf{C}}_{k}$ with $k$ columns such that $\\|\\mathbf{X}-\\mathbf{X}\\widetilde{\\mathbf{C}}_{k}\\widetilde{\\mathbf{C}}_{k}^{\\top}\\|\\leq$ $(1+\\epsilon)\\|\\mathbf{X}-\\mathbf{X}\\mathbf{C}_{k}\\mathbf{C}_{k}^{\\top}\\|$ where $\\mathbf{C}_{k}\\in\\mathbb{R}^{n\\times k}$ contains the top- $k$ (right)singularvectorsof $\\mathbf{X}$ in ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\mathsf{M M}}(n)\\left(\\log(\\frac{n}{\\sigma_{k+1}})\\log(\\frac{1}{\\sigma_{k+1}})+\\log(\\frac{n}{\\log_{k}})\\log(\\frac{1}{\\operatorname{gap}_{k}})+\\log(\\log(\\frac{n}{\\epsilon\\sigma_{k+1}\\operatorname{gap}_{k}}))\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "arithmetic operations using $\\begin{array}{r}{O\\left(\\log(n)\\left(\\log^{4}(\\frac{n}{\\epsilon\\operatorname{gap}_{k}})+\\log^{4}(\\frac{n}{\\sigma_{k+1}})\\right)+\\log(\\frac{1}{\\epsilon\\sigma_{k+1}})\\right)}\\end{array}$ bits of precision, with probability at least $1-O(1/n)$ ", "page_idx": 8}, {"type": "text", "text": "4.1 Block-Krylov PCA ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In some applications, the target dimension $k$ might be small, i.e., $k~\\ll~n$ .This_condition has driven a whole area of research in so-called low-rank approximation algorithms for PCA [53, 120, 33, 67, 100, 106, 30, 29, 5]. Such approaches are also suitable for kernel PCA, since they rely on matrix-vector products and therefore the kernel matrix does not need to be explicitly formed. The techniques from the previous section can be directly applied to obtain new bit complexity upper bounds for existing state-of-the-art algorithms, which are typically analyzed in exact arithmetic. They internally rely on the computation of the principal singular vectors of submatrices, which can be improved with our methods. Specifically, in Appendix G.2 we summarize a foating point analysis of the Block-Krylov Iteration algorithm (see Algorithm 8), essentially, providing a matrix multiplication-type upper bound on the bit complexity with only a polylogarithmic dependence on the singular value gap. In a nutshell, we directly obtain the following result: ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2 (Bit complexity analysis of Block-Krylov PCA). Let $\\mathbf{X}$ be a data matrix $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ Xll \u2264 1, k E [n] a target rank, epCA E (0,1) an accuracy paramete, and q =  (%). Let $T_{\\mathsf{M M X}}(k)$ denotethecomplexitytostablymultiply $\\mathbf{X}$ or $\\mathbf{X}^{\\top}$ with a dense matrix with $k$ columns from the right (see Def. G.1). Using the Steps 1-6 that are detailed in Appendix G.3 as a foating point implementation of Algorithm 8, we can compute a matrix $\\widetilde{\\mathbf{Z}}_{k}\\in\\mathbb{R}^{m\\times k}$ thatsatisfies ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widetilde{\\mathbf{Z}}_{k}\\widetilde{\\mathbf{Z}}_{k}^{\\top}-\\mathbf{Z}_{k}\\mathbf{Z}_{k}^{\\top}\\right\\|\\leq O(\\epsilon_{\\mathsf{P C A}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "with high probability, where $\\mathbf{Z}_{k}$ is an approximate basis for the top- $k$ principal components of $\\mathbf{X}$ returned by Algorithm 8 in exact arithmetic. The total cost is at most ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(q T_{\\mathsf{M M X}}(k)\\log(\\frac{\\kappa(\\mathbf{K})}{\\mathrm{gap}_{k}(\\mathbf{M})})+m(q k)^{\\omega-1}\\log(\\frac{1}{\\mathrm{gap}_{k}(\\mathbf{M})})+(q k)^{\\omega}\\operatorname{polylog}(\\frac{q k}{\\mathrm{gap}_{k}(\\mathbf{M})})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "foating point operations, using O (polylog( mgs(k)) bits of precision. $\\mathbf{K}$ , M are as in Alg. 8. ", "page_idx": 9}, {"type": "text", "text": "Proof. The full proof can be found in Thm. G.2, Appendix G.3. The main idea is to apply the counting query methodology to compute the condition number of the Block-Krylov matrix $\\mathbf{K}$ , as well as the $k$ -th spectral gap and the midpoint of the reduced matrix $\\mathbf{M}$ in Line 5 of Alg. 8. Thereafter, we can compute a spectral projector and an approximate basis for the top- $k$ singular vectors of $\\mathbf{M}$ using PURIFY and DEFLATE, similar to the analysis of classical PCA in the previous section. \u53e3 ", "page_idx": 9}, {"type": "text", "text": "5 Open problems ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We mention some open problems and interesting future directions. ", "page_idx": 9}, {"type": "text", "text": "(i) Bit requirement of SGN: The major bottleneck for the bit requirements of our main algorithms comes from the SGN algorithm of [15]. An inverse-free Newton-Schultz iteration [83], or the implicit repeated squaring of [41, 121] can potentially give significant improvements. ", "page_idx": 9}, {"type": "text", "text": "(ii) Sparse algorithms: In applications like DFT it commonly occurs that the matrices have special structure, i.e., they are banded and/or sparse. It remains open whether Problem 1.1 can be provably solved faster than our reported results in finite precision for these special cases (recall that the tridiagonal QR algorithm requires $O(n^{3})$ operations to return the eigenvectors). An end-to-end stability analysis of existing fast eigensolvers would be the place to start [65, 137]. ", "page_idx": 9}, {"type": "text", "text": "(ii) Distributed PCA: The techniques for Block-Krylov PCA can be potentially applied to distributed or streaming PCA algorithms, which are also based on randomized low-rank approximations. E.g., in the distributed PCA algorithm of [31], it is straightforward to replace the SVD computation on the server by a counting query iteration. The full analysis of such an approach is left as future work. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work we provided an end-to-end analysis to approximate spectral projectors on $k$ dimensional invariant subspaces of Hermitian definite matrix pencils $(\\mathbf{H},\\mathbf{S})$ that require at most $O\\left(T_{\\sf M M}(n)\\,\\mathrm{polylog}(n,\\epsilon^{-1},\\kappa({\\bf S}),\\mathrm{gap}_{k}^{-1})\\right)$ bit operations in the floating point model of computation. This is the first end-to-end analysis that improves the $\\widetilde O(n^{3})$ complexity of classic eigensolvers for both the regular and the generalized case. To achieve this result we introduced a new method to approximate spectral gaps by querying the number of eigenvalues that are smaller than a threshold, and therefore completely avoid an explicit diagonalization of any matrix or pencil. This approach required proving that the Cholesky factorization can be stably computed in $\\bar{O}(T_{\\mathsf{M M}}(n))$ floating point operations, a novel result per se. Our results have direct implications on PCA problems, providing matrix multiplication type upper bounds for the bit complexity of classical and Block-Krylov PCA. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Daniel Kressner, Nian Shao, Ryan Schneider, Nicolas Deutschmann, Nikhil Srivastava, Ilse Ipsen, Cameron Musco, David Woodruff, Uria Mor, and Haim Avron for helpful discussions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dimitris Achlioptas. Database-friendly random projections.  In Proc. Twentieth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, pages 274-281, 2001. 61   \n[2]  Satoru Adachi, Satoru Iwata, Yuji Nakatsukasa, and Akiko Takeda. Solving the trust-region subproblem by a generalized eigenvalue problem. SIAM Journal on Optimization, 27(1):269- 291, 2017. 26   \n[3] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast JohnsonLindenstrausstransform. In Proc. Thirty-Eighth Annual ACM Symposium on Theory of Computing, pages 557-563, 2006. 62   \n[4]  Michael Aizenman, Ron Peled, Jeffrey Schenker, Mira Shamis, and Sasha Sodin. Matrix regularizing effects of Gaussian perturbations. Communications in Contemporary Mathematics, 19(03):1750028, 2017. 4, 47   \n[5]  Zeyuan Allen-Zhu and Yuanzhi Li. LazySVD: Even faster SVD decomposition yet without agonizing pain. Advances in Neural Information Processing Systems, 29, 2016. 9   \n[6] Josh Alman, Ran Duan, Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. More asymmetry yields faster matrix multiplication. arXiv preprint arXiv:2404.16349, 2024.2   \n[7] Diego Armentano, Carlos Beltran, Peter Birgisser, Felipe Cucker, and Michael Shub. A stable, polynomial-time algorithm for the eigenpair problem. Journal of the European Mathematical Society, 20(6):1375-1437, 2018. 26   \n[8]  Zhaojun Bai and James Demmel. On a direct algorithm for computing invariant subspaces with specified eigenvalues. University of Tennessee, Computer Science Department, 1991. 2, 26   \n[9]  Zhaojun Bai and James Demmel. Using the matrix sign function to compute invariant subspaces. SIAM Journal on Matrix Analysis and Applications, 19(1):205-225, 1998. 2, 26, 27   \n[10] Zhaojun Bai, James Demmel, and Ming Gu. An inverse free parallel spectral divide and conquer algorithm for nonsymmetric eigenproblems. Numerische Mathematik, 76(3):279- 308, 1997. 2, 26   \n[11] Gokhan H Bakir, Jason Weston, and Bernhard Scholkopf. Learning to find pre-images. Advances in Neural Information Processing Systems, 16:449-456, 2004. 1   \n[12] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53-58, 1989. 1   \n[13] Grey Ballard, James Demmel, and Ioana Dumitriu. Minimizing Communication for Eigenproblems and the Singular Value Decomposition. Technical Report UCB/EECS-2011-14, February 2011. 3   \n[14]  Grey Ballard, James Demmel, Olga Holtz, and Oded Schwartz. Minimizing Communication inNumerical Linear Algebra. SIAM Journal on Matrix Analysis and Applications, 32(3):866- 901,2011. 3   \n[15]  Jess Banks, Jorge Garza- Vargas, Archit Kulkarni, and Nikhil Srivastava. Pseudospectral Shattering, the Sign Function, and Diagonalization in Nearly Matrix Multiplication Time. Foundations of Computational Mathematics, pages 1-89, 2022. 2, 3, 4, 6, 9, 10, 19, 21, 22, 23, 26,27,31,32,33   \n[16] Jess Banks, Jorge Garza-Vargas, and Nikhil Srivastava. Global Convergence of Hessenberg Shifted QR II: Numerical Stability. arXiv preprint arXiv:2205.06810, 2022. 26   \n[17] Jess Banks, Jorge Garza-Vargas, and Nikhil Srivastava. Global Convergence of Hessenberg Shifted QR Ill: Approximate Ritz Values via Shifted Inverse Iteration. arXiv preprint arXiv:2205.06804, 2022. 26   \n[18]  Jess Banks, Jorge Garza-Vargas, and Nikhil Srivastava. Global Convergence of Hessenberg Shifted QR I: Exact Arithmetic. Foundations of Computational Mathematics, pages 1-34, 2024.26   \n[19]  Jess Banks, Archit Kulkarni, Satyaki Mukherjee, and Nikhil Srivastava. Gaussian Regularization of the Pseudospectrum and Davies' Conjecture. Communications on Pure and Applied Mathematics,74(10):2114-2131,2021. 9, 62   \n[20] Elnaz Barshan, Ali Ghodsi, Zohreh Azimifar, and Mansoor Zolghadri Jahromi. Supervised principal component analysis: Visualization, classification and regression on subspaces and submanifolds. Pattern Recognition, 44(7):1357-1371, 2011. 1   \n[21]  Yair Bartal, Ben Recht, and Leonard J Schulman. Dimensionality reduction: beyond the Johnson-Lindenstrauss bound. In Proc. 201l Anual ACM-SIAM Symposium on Discrete Algorithms, pages 868-887. SIAM, 2011. 62   \n[22]  Harold Basch, C. J. Hornback, and J. W. Moskowitz. Gaussian-Orbital Basis Sets for the FirstRow Transition-Metal Atoms. The Journal of Chemical Physics, 51(4):1311-1318, 1969. 58   \n[23]  Robert A Baston and Yuji Nakatsukasa. Stochastic diagonal estimation: probabilistic bounds and an improved algorithm. arXiv preprint arXiv:2201.10684, 2022. 62   \n[24]  Friedrich L Bauer and Charles T Fike. Norms and exclusion theorems. Numerische Mathematik, 2:137-141, 1960. 20   \n[25]  Michael Ben-Or and Lior Eldar. A Quasi-Random Approach to Matrix Spectral Analysis. In Proc. 9th Innovations in Theoretical Computer Science Conference, pages 6:1-6:22. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018. 2, 4, 26   \n[26] Peter Benner, Volker Mehrmann, and Honguo Xu. A new method for computing the stable invariant subspace of a real Hamiltonian matrix. Journal of computational and applied mathematics, 86(1):17-43, 1997. 26   \n[27] Rajendra Bhatia. Pinching, trimming, truncating, and averaging of matrices. The American Mathematical Monthly, 107(7):602-608, 2000. 25   \n[28]  Rajendra Bhatia. Perturbation bounds for matrix eigenvalues. SIAM, 2007. 20   \n[29]  Srinadh Bhojanapalli, Prateek Jain, and Sujay Sanghavi. Tighter low-rank approximation via sampling the leveraged element. In Proc. Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 902-920. SIAM, 2014. 9   \n[30] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal column-based matrix reconstruction. SIAM Journal on Computing, 43(2):687-717, 2014. 9   \n[31]  Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal component analysis in distributed and streaming models. In Proc. Forty-Eighth Annual ACM Symposium on Theory of Computing, pages 236-249, 2016. 4, 10   \n[32]  Tim Clark and Rainer Koch. Linear Combination of Atomic Orbitals, pages 5-22. Springer Berlin Heidelberg, 1999. 58   \n[33] Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In Proc. Forty-First Annual ACM Symposium on Theory of Computing,pages 205-214, 2009. 9,64   \n[34] Kenneth L Clarkson and David P Woodruff Low-rank approximation and regression in input sparsity time. Journal of the ACM, 63(6):1-45, 2017. 64   \n[35] Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimensionality reduction for k-means clustering and low rank approximation. In Proc. FortySeventh Annual ACM Symposium on Theory of Computing, pages 163-172, 2015. 1   \n[36]  Charles R. Crawford. Reduction of a band-symmetric generalized eigenvalue problem. Communications of the ACM, 16(1):41-44, 1973. 26   \n[37] Philip I Davies, Nicholas J Higham, and Francoise Tisseur. Analysis of the cholesky method with iterative refinement for solving the symmetric definite generalized eigenproblem. SIAM Journal on Matrix Analysis and Applications, 23(2):472-493, 2001. 2, 4, 26   \n[38]  Theodorus J Dekker and Joseph F Traub. The shifted QR algorithm for Hermitian matrices. Linear Algebra and its Applications, 4(3):137-154, 1971. 26   \n[39] James Demmel, Ioana Dumitriu, and Olga Holtz. Fast linear algebra is stable. Numerische Mathematik, 108(1):59-91, 2007. 2, 3, 4, 9, 21, 22, 23, 34, 47, 64 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[40] James Demmel, Ioana Dumitriu, Olga Holtz, and Robert Kleinberg. Fast matrix multiplication is stable. Numerische Mathematik, 106(2):199-224, 2007. 2, 21, 22 ", "page_idx": 12}, {"type": "text", "text": "[41]  James Demmel, Ioana Dumitriu, and Ryan Schneider. Generalized Pseudospectral Shattering and Inverse-Free Matrix Pencil Diagonalization. arXiv preprint arXiv:2306.03700, 2023. 2, 3, 4,10,27,33 ", "page_idx": 12}, {"type": "text", "text": "[42] James W Demmel. Applied numerical linear algebra. SIAM, 1997. 2, 26 [43]  James Weldon Demmel. Three methods for refining estimates of invariant subspaces. Computing, 38(1):43-57,1987. 26 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[44]  James Weldon Demmel and Bo Kagstrom. Computing stable eigendecompositions of matrix pencils. Linear Algebra and its Applications, 88:139-186, 1987. 26 ", "page_idx": 12}, {"type": "text", "text": "[45]  Konstantinos I Diamantaras and Sun Yuan Kung.  Principal component neural networks: theory and applications. John Wiley & Sons, Inc., 1996. 1, 9, 62 ", "page_idx": 12}, {"type": "text", "text": "[46]  Petros Drineas, Iordanis Kerenidis, and Prabhakar Raghavan. Competitive recommendation systems. In Proc. Thiry-Fourth Annual ACM Symposium on Theory of Computing, pages 82-90,2002.1 ", "page_idx": 12}, {"type": "text", "text": "[47] Ran Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication via asymmetric hashing. In 2023 IEEE 64th Annual Symposium on Foundations of Computer Science, pages 2129-2138.IEEE,2023. 2,21 ", "page_idx": 12}, {"type": "text", "text": "[48] Stanley C Eisenstat and Ilse CF Ipsen.  Relative perturbation results for eigenvalues and eigenvectors of diagonalisable matrices. BIT Numerical Mathematics, 38(3):502-509, 1998. 20 ", "page_idx": 12}, {"type": "text", "text": "[49] Ethan N Epperly, Joel A Tropp, and Robert J Webber. XTrace: Making the Most of Every Sample in Stochastic Trace Estimation. SIAM Journal on Matrix Analysis and Applications, 45(1):1-23,2024. 62 ", "page_idx": 12}, {"type": "text", "text": "[50] Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: Constant-size coresets for k-means, PCA, and projective clustering. SIAM Journal on Computing, 49(3):601-657, 2020. 1 ", "page_idx": 12}, {"type": "text", "text": "[51]  John GF Francis. The QR transformation a unitary analogue to the LR transformation\u2014Part 1. The Computer Journal, 4(3):265-271, 1961. 26 ", "page_idx": 12}, {"type": "text", "text": "[52]  John GF Francis. The QR transformation\u2014Part 2. The Computer Journal, 4(4):332-345, 1962.26 ", "page_idx": 12}, {"type": "text", "text": "[53] Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank approximations. In Proc. 39th Annual Symposium on Foundations of Computer Science, page 370, 1998. 9, 64 ", "page_idx": 12}, {"type": "text", "text": "[54]  Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast Monte-Carlo algorithms for finding low-rank approximations. Journal of the ACM, 51(6):1025-1041, 2004. 64 ", "page_idx": 12}, {"type": "text", "text": "[55]  Martin Firer. Faster integer multiplication. In Proc. Thirty-Ninth Annual ACM Symposium on Theory of Computing, pages 57-66, 2007. 2 ", "page_idx": 12}, {"type": "text", "text": "[56]  Giulia Galli. Linear scaling methods for electronic structure calculations and quantum molecular dynamics simulations. Current Opinion in Solid State and Materials Science, 1(6):864- 874,1996.59 ", "page_idx": 12}, {"type": "text", "text": "[57]  Giulia Galli and Michele Parrinello. Large scale electronic structure calculations.  Physical Review Letters,69(24):3547,1992. 59 ", "page_idx": 12}, {"type": "text", "text": "[58] Alan George. Nested dissection of a regular finite element mesh. SIAM Journal on Numerical Analysis, 10(2):345-363, 1973. 4 ", "page_idx": 12}, {"type": "text", "text": "[59] P Giannozzi, O Andreussi, T Brumme, O Bunau, M Buongiorno Nardelli, M Calandra, R Car, C Cavazzoni, D Ceresoli, M Cococcioni, N Colonna, I Carnimeo, A Dal Corso, S de Gironcoli, P Delugas, R A DiStasio, A Ferretti, A Floris, G Fratesi, G Fugallo, R Gebauer, U Gerstmann, F Giustino, T Gorni, J Jia, M Kawamura, H-Y Ko, A Kokalj, E Kicikbenli, M Lazzeri, M Marsili, N Marzari, F Mauri, N L Nguyen, H-V Nguyen, A Otero de-la Roza, L Paulatto, S Ponce, D Rocca, R Sabatini, B Santra, M Schlipf, A P Seitsonen, A Smogunov, I Timrov, T Thonhauser, P Umari, N Vast, X Wu, and S Baroni. Advanced capabilities for materials modelling with Quantum ESPRESSO. Journal of Physics: Condensed Matter, 29(46):465901, 0ct 2017. 8, 58 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[60] Paolo Giannozzi, Stefano Baroni, Nicola Bonini, Matteo Calandra, Roberto Car, Carlo Cavazzoni, Davide Ceresoli, Guido L Chiarotti, Matteo Cococcioni, Ismaila Dabo, Andrea Dal Corso, Stefano de Gironcoli, Stefano Fabris, Guido Fratesi, Ralph Gebauer, Uwe Gerstmann, Christos Gougoussis, Anton Kokalj, Michele Lazzeri, Layla Martin-Samos, Nicola Marzari, Francesco Mauri, Riccardo Mazzarello, Stefano Paolini, Alfredo Pasquarello, Lorenzo Paulatto, Carlo Sbraccia, Sandro Scandolo, Gabriele Sclauzero, Ari P Seitsonen, Alexander Smogunov, Paolo Umari, and Renata M Wentzcovitch. QUANTUM ESPRESSO: a modular and open-source software project for quantum simulations of materials. Journal of Physics: Condensed Matter,21(39):395502, Sep 2009. 8 ", "page_idx": 13}, {"type": "text", "text": "[61]  John R Gilbert and Robert Endre Tarjan. The analysis of a nested dissection algorithm. Numerische Mathematik, 50(4):377-404,1986. 4 ", "page_idx": 13}, {"type": "text", "text": "[62] Stefan Goedecker. Linear scaling electronic structure methods. Reviews of Modern Physics, 71(4):1085,1999. 59 ", "page_idx": 13}, {"type": "text", "text": "[63]  Stefan Goedecker and GE Scuserza. Linear scaling electronic structure methods in chemistry and physics. Computing in Science & Engineering, 5(4):14-21, 2003. 59 ", "page_idx": 13}, {"type": "text", "text": "[64]  Gene H Golub and Charles F Van Loan. Matrix Computations. Johns Hopkins University Press, 2013. 2, 19, 26, 64 ", "page_idx": 13}, {"type": "text", "text": "[65]  Ming Gu and Stanley C Eisenstat.  A divide-and-conquer algorithm for the symmetric tridiagonal eigenproblem. SIAM Journal on Matrix Analysis and Applications, 16(1):172-191, 1995.10,59 ", "page_idx": 13}, {"type": "text", "text": "[66]  Ming Gu and Stanley C Eisenstat. Efficient algorithms for computing a strong rank-revealing QR factorization. SIAM Journal on Scientific Computing, 17(4):848-869, 1996. 9 ", "page_idx": 13}, {"type": "text", "text": "[67]  Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217-288, 2011. 9, 64 ", "page_idx": 13}, {"type": "text", "text": "[68]  David Harvey and Joris Van Der Hoeven. Integer multiplication in time $\\mathsf{o}(\\mathrm{nlog}\\backslash,\\!\\mathrm{n})$ .Annals of Mathematics, 193(2):563-617, 2021. 2 ", "page_idx": 13}, {"type": "text", "text": "[69] Qiu He, Bin Yu, Zhaohuai Li, and Yan Zhao. Density Functional Theory for Battery Materials. Energy & Environmental Materials, 2(4):264-279, 2019. 58 ", "page_idx": 13}, {"type": "text", "text": "[70] Nicholas J Higham. Accuracy and stability of numerical algorithms. SIAM, 2002. 19, 20, 61,64 ", "page_idx": 13}, {"type": "text", "text": "[71]Nicholas J Higham. Cholesky factorization. Wiley Interdisciplinary Reviews: Computational Statistics, 1(2):251-254, 2009. 4 ", "page_idx": 13}, {"type": "text", "text": "[72]  Harold Hotelling.  Analysis of a complex of statistical variables into principal components. Journal of educational psychology, 24(6):417, 1933. 9, 62 ", "page_idx": 13}, {"type": "text", "text": "[73] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 1 ", "page_idx": 13}, {"type": "text", "text": "[74]  Jurg Hutter, Marcella Iannuzzi, Florian Schiffmann, and Joost Vande Vondele. cp2k: atomistic simulations of condensed matter systems. Wiley Interdisciplinary Reviews: Computational Molecular Science,4(1):15-25,2014. 8,58 ", "page_idx": 13}, {"type": "text", "text": "[75] Ise CF Ipsen. Computing an eigenvector with inverse iteration. SIAM Review, 39(2):254 291,1997.2,26 ", "page_idx": 13}, {"type": "text", "text": "[76]  Ilse CF Ipsen. Absolute and relative perturbation bounds for invariant subspaces of matrices. Linear Algebra and its Applications, 309(1-3):45-56, 2000. 26 ", "page_idx": 13}, {"type": "text", "text": "[77]  Zhongxiao Jia and GW Stewart. An analysis of the rayleigh-ritz method for approximating eigenspaces. Mathematics of computation, 70(234):637-647, 2001. 26 ", "page_idx": 13}, {"type": "text", "text": "[78]  William B Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemp. Math., 26(1):189-206, 1984. 61 ", "page_idx": 13}, {"type": "text", "text": "[79] Ian T Jolliffe. Principal component analysis for special types of data. Springer, 2002. 1, 9, 62 ", "page_idx": 13}, {"type": "text", "text": "[80]  Praneeth Kacham and David P Woodruff. Faster Algorithms for Schatten-p Low Rank Approximation. Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 2024. 64   \n[81] William Kahan. Spectra of nearly Hermitian matrces. Proc. American Mathematical Society, 48(1):11-17, 1975. 20   \n[82]  Daniel M Kane and Jelani Nelson. Sparser Johnson-Lindenstrauss transforms. Journal of the ACM, 61(1):1-23, 2014. 62   \n[83]  Charles S Kenney and Alan JLaub. The matrix sign function. IEEE transactions on automatic control, 40(8):1330-1348, 1995. 10   \n[84]  Andrzej Kietbasinski. A note on rounding-error analysis of Cholesky factorization. Linear Algebra and its Applications, 88:487-494, 1987. 4   \n[85] Cedric Klinkert, Aron Szab6, Christian Stieger, Davide Campi, Nicola Marzari, and Mathieu Luisier. 2-d Materials for Ultrascaled Field-Effect Transistors: One Hundred Candidates under the Ab Initio Microscope. ACS Nano, 14(7):8605-8615, 2020. 58   \n[86]  Walter Kohn. Density functional/Wannier function theory for systems of very many atoms. Chemical Physics Letters, 208(3-4):167-172, 1993. 59   \n[87]  Walter Kohn. Density functional and density matrix method scaling linearly with the number of atoms. Physical Review Letters, 76(17):3168, 1996. 59   \n[88]  Walter Kohn and Lu Jeu Sham. Self-consistent equations including exchange and correlation effects. Physical Review, 140(4A):A1133, 1965. 2, 58   \n[89]  Georg Kresse and Jurgen Furthmiller. Eficient iterative schemes for ab initio total-energy calculations using a plane-wave basis set. Physical Review B, 54(16):11169-11186, 1996. 8, 58   \n[90]  Vera N Kublanovskaya.  On some algorithms for the solution of the complete eigenvalue problem. USSR Computational Mathematics and Mathematical Physics, 1(3):637-657, 1962. 26   \n[91] Rasmus Kyng, Yin Tat Lee, Richard Peng, Sushant Sachdeva, and Daniel A Spielman. Sparsified Cholesky and Multigrid Solvers for Connection Laplacians. In Proc. 48th Annual ACM Symposium on Theory of Computing, pages 842-850, 2016. 4   \n[92] Rasmus Kyng and Sushant Sachdeva. Approximate Gaussian Elimination for Laplacians - Fast, Sparse,and Simle nProc. IEE57thAnnual Symposium onFoundations of Computr Science, pages 573-582, 2016. 4   \n[93]  Kasper Green Larsen and Jelani Nelson. Optimality of the Johnson-Lindenstrauss lemma. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science, pages 633-638. IEEE,2017.62   \n[94] Richard J Lipton, Donald J Rose, and Robert Endre Tarjan. Generalized nested dissection. SIAM Journal on Numerical Analysis, 16(2):346-358, 1979. 4   \n[95]  Anand Louis and Santosh S Vempala.  Accelerated Newton iteration for roots of black box polynomials. In Proc. IEEE 57th Annual Symposium on Foundations of Computer Science, pages 732-740. IEEE, 2016. 2, 3   \n[96] Arnaz Malhi and Robert X Gao. PCA-based feature selection scheme for machine defect classification. IEEE transactions on instrumentation and measurement, 53(6):1517-1525, 2004.1   \n[97]  Alexander N Malyshev. Computing invariant subspaces of a regular linear pencil of matrices. Siberian Mathematical Journal, 30(4):559-567, 1989. 26   \n[98]  lexander N Malyshev. Parallel algorithm for solving some spectral problems of linear algebra. Linear Algebra and its Applications, 188:489-520, 1993. 26   \n[99] Olvi L Mangasarian and Edward W Wild. Multisurface proximal support vector machine classifcationvia generalizedeigenvaluesEEE transactionsonatternanalysis andmachine intelligence, 28(1):69-74, 2005. 1   \n100] Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark Tygert. A randomized algorithm for the decomposition of matrices. Applied and Computational Harmonic Analysis, 30(1):47- 68, 2011. 9, 64 ", "page_idx": 14}, {"type": "text", "text": "[101] Nicola Marzari, Arash A. Mostofi, Jonathan R. Yates, Ivo Souza, and David Vanderbilt. Maximally localized Wannier functions: Theory and applications. Reviews of Modern Physics, 84(4):1419, 2012. 58 ", "page_idx": 15}, {"type": "text", "text": "[102] Reinhard J Maurer, Christoph Freysoldt, Anthony M Reilly, Jan Gerit Brandenburg, Oliver T Hofmann, Torbjorn Bjorkman, S\u00e9bastien Lebegue, and Alexandre Tkatchenko. Advances in Density-Functional Calculations for Materials Modeling. Annual Review of Materials Research,49(1):1-30,2019. 58 ", "page_idx": 15}, {"type": "text", "text": "[103] Istvan Mayer. Simple theorems, proofs, and derivations in quantum chemistry. Springer Science & Business Media, 2003. 59 ", "page_idx": 15}, {"type": "text", "text": "[104] Raphael Meyer, Cameron Musco, and Christopher Musco. On the unreasonable effectiveness of single vector Krylov methods for low-rank approximation. In Proc. 2024 Annual ACMSIAM Symposium on Discrete Algorithms, pages 811-845. SIAM, 2024. 4 ", "page_idx": 15}, {"type": "text", "text": "[105] Nariyuki Minami. Local fuctuation of the spectrum of a multidimensional Anderson tight binding model. Communications in mathematical physics, 177:709-725, 1996. 4 ", "page_idx": 15}, {"type": "text", "text": "[106]  Cameron Musco and Christopher Musco. Randomized block Krylov methods for stronger and faster approximate singular value decomposition. Advances in Neural Information Processing Systems, 28, 2015. 4, 9, 64, 65 ", "page_idx": 15}, {"type": "text", "text": "[107] Cameron Musco, Christopher Musco, and Aaron Sidford. Stability of the lanczos method for matrix function approximation. In Proc. Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1605-1624. SIAM, 2018. 2, 3, 24, 64 ", "page_idx": 15}, {"type": "text", "text": "[108]  Yuji Nakatsukasa. On the condition numbers of a multiple eigenvalue of a generalized eigenvalue problem. Numerische Mathematik, 121:531-544, 2012. 26 ", "page_idx": 15}, {"type": "text", "text": "[109]  Yuji Nakatsukasa and Nicholas J Higham.  Stable and efficient spectral divide and conquer algorithms for the symmetric eigenvalue decomposition and the SVD. SIAM Journal on Scientific Computing, 35(3):A1325-A1349, 2013. 2 ", "page_idx": 15}, {"type": "text", "text": "[110] Jelani Nelson and Huy L Nguyen. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. In Proc. IEEE 54th Annual Symposium on Foundations of Computer Science, pages 117-126. IEEE, 2013. 62 ", "page_idx": 15}, {"type": "text", "text": "[111] Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. Advances in Neural Information Processing Systems, 14, 2001. 1 ", "page_idx": 15}, {"type": "text", "text": "[112] Christopher C Paige. Error analysis of the lanczos algorithm for tridiagonalizing a symmetric matrix. IMA Journal of Applied Mathematics, 18(3):341-349, 1976. 26 ", "page_idx": 15}, {"type": "text", "text": "[113] Christopher Conway Paige. The computation of eigenvalues and eigenvectors of very large sparse matrices. PhD thesis, University of London, 1971. 2, 26 ", "page_idx": 15}, {"type": "text", "text": "[114] Victor Y Pan and Zhao Q Chen. The complexity of the matrix eigenproblem. In Proc. 31st Annual ACM Symposium on Theory of Computing, pages 507-516, 1999. 26 ", "page_idx": 15}, {"type": "text", "text": "[115] Beresford N Parlett. The Symmetric Eigenvalue Problem. SIAM, 1998. 26 [116]  Karl Pearson. Lii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559-572, 1901. 9,62 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "[117]  Andrzej Pokrzywa. Spectra of operators with fixed imaginary parts. Proc. American Mathematical Society, 81(3):359-364, 1981. 20 ", "page_idx": 15}, {"type": "text", "text": "[118]  Awwal Mohammed Rufai, Gholamreza Anbarjafari, and Hasan Demirel. Lossy image compression using singular value decomposition and wavelet difference reduction. Digital signal processing, 24:117-123, 2014. 1 ", "page_idx": 15}, {"type": "text", "text": "[119] Yousef Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011.26 ", "page_idx": 15}, {"type": "text", "text": "[120]  Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In Proc. 47th Annual Symposium on Foundations of Computer Science, pages 143-152. IEEE, 2006.9,64 ", "page_idx": 15}, {"type": "text", "text": "[121] Ryan Schneider.  When is fast, implicit squaring of $A^{-1}B$ stable? arXivpreprint arXiv:2310.00193, 2023. 2, 3, 10 ", "page_idx": 15}, {"type": "text", "text": "[122] Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muiller. Nonlinear component analysis as a kernel eigenvalue problem. Neural computation, 10(5):1299-1319, 1998. 1 ", "page_idx": 16}, {"type": "text", "text": "[123] Arnold Schonhage and Volker Strassen. Fast multiplication of large numbers.  Computing, 7:281-292,1971.2 ", "page_idx": 16}, {"type": "text", "text": "[124] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and machine intelligence, 22(8):888-905, 2000. 1 ", "page_idx": 16}, {"type": "text", "text": "[125] Max Simchowitz, Ahmed El Alaoui, and Benjamin Recht. Tight query complexity lower bounds for PCA via finite sample deformed wigner law. In Proc. 5Oth Annual ACM Symposium on Theory of Computing, pages 1249-1259, 2018. 64 ", "page_idx": 16}, {"type": "text", "text": "[126] Aleksandros Sobczyk and Mathieu Luisier.  Approximate Euclidean lengths and distances beyond Johnson-Lindenstrauss. Advances in Neural Information Processing Systems, 35:19357-19369,2022.62 ", "page_idx": 16}, {"type": "text", "text": "[127] Jos\u00e9 M Soler, Emilio Artacho, Julian D Gale, Alberto Garcia, Javier Junquera, Pablo Ordejon, and Daniel Sanchez-Portal. The SIESTA method for ab initio order-N materials simulation. Journal of Physics: Condensed Matter, 14(11):2745, 2002. 8, 58 ", "page_idx": 16}, {"type": "text", "text": "[128] Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time. Journal of the ACM, 51(3):385-463, 2004. 4 ", "page_idx": 16}, {"type": "text", "text": "[129]  Nikhil Srivastava. The complexity of diagonalization. In Proc. 2023 International Symposium on Symbolic and Algebraic Computation, pages 1-6, 2023. 26 ", "page_idx": 16}, {"type": "text", "text": "[130] Gilbert W Stewart. On the sensitivity of the eigenvalue problem ax=\u5165bx. SIAM Journal on Numerical Analysis, 9(4):669-686, 1972. 26 ", "page_idx": 16}, {"type": "text", "text": "[131] Gilbert W Stewart. Error and perturbation bounds for subspaces associated with certain eigenvalue problems. SIAM Review, 15(4):727-764, 1973. 26 ", "page_idx": 16}, {"type": "text", "text": "[132] Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares problems. SIAM Review, 19(4):634-662, 1977. 26 ", "page_idx": 16}, {"type": "text", "text": "[133] Gilbert W Stewart. Pertubation bounds for the definite generalized eigenvalue problem. Linear Algebra and its Applications, 23:69-85, 1979. 26 ", "page_idx": 16}, {"type": "text", "text": "[134] Volker Strassen. Gaussian elimination is not optimal. Numerische Mathematik, 13(4):354- 356,1969.21 ", "page_idx": 16}, {"type": "text", "text": "[135] Lloyd N Trefethen and Mark Embree. Spectra and Pseudospectra: The Behavior of Nonnormal Matrices and Operators. Princeton University Press, 2005. 20, 21, 26 ", "page_idx": 16}, {"type": "text", "text": "[136] Joost VandeVondele, Urban Borstnik, and Jurg Hutter. Linear scaling self-consistent field calculations with millions of atoms in the condensed phase. Journal of Chemical Theory and Computation, 8(10):3565-3573, 2012. 59 ", "page_idx": 16}, {"type": "text", "text": "[137] James Vogel, Jianlin Xia, Stephen Cauley, and Venkataramanan Balakrishnan. Superfast divide-and-conquer method and perturbation analysis for structured eigenvalue solutions. SIAM Journal on Scientific Computing, 38(3):A1358-A1382, 2016. 10, 59 ", "page_idx": 16}, {"type": "text", "text": "[138] Franz Wegner. Bounds on the density of states in disordered systems. Zeitschrift fir Physik B Condensed Matter, 44(1):9-15, 1981. 4, 7, 47 ", "page_idx": 16}, {"type": "text", "text": "[139] Hermann Weyl. Das asymptotische Verteilungsgesetz der Eigenwerte linearer partieller Differentialgleichungen (mit einer Anwendung auf die Theorie der Hohlraumstrahlung). Mathematische Annalen,71(4):441-479, 1912. 20 ", "page_idx": 16}, {"type": "text", "text": "[140] James Hardy Wilkinson. Global convergene of tridiagonal QR algorithm with origin shifts. Linear Algebra and its Applications, 1(3):409-420, 1968. 26 ", "page_idx": 16}, {"type": "text", "text": "[141] Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New bounds for matrix multiplication: from alpha to omega. In Proc. 2024 Annual ACM-SIAM Symposium on Discrete Algorithms, pages 3792-3835. SIAM, 2024. 2, 21 ", "page_idx": 16}, {"type": "text", "text": "[142] Zewen Xiao and Yanfa Yan. Progress in Theoretical Study of Metal Halide Perovskite Solar Cell Materials. Advanced Energy Materials, 7(22):1701136, 2017. 58 ", "page_idx": 16}, {"type": "text", "text": "[143]  Weitao Yang. Direct calculation of electron density in density-functional theory.  Physical Review Letters, 66(11):1438, 1991. 59 ", "page_idx": 16}, {"type": "text", "text": "[144] Yunkai Zhou, Yousef Saad, Murilo L Tiago, and James R Chelikowsky. _ Parallel selfconsistent-field calculations via Chebyshev-filtered subspace acceleration. Physical Review E,74(6):066704, 2006. 59   \n[145]  Yunkai Zhou, Yousef Saad, Murilo L Tiago, and James R Chelikowsky. Self-consistent-field calculations using Chebyshev-filtered subspace iteration. Journal of Computational Physics, 219(1):172-184, 2006. 59 ", "page_idx": 17}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section the model of computation is defined and basic linear algebra principles are summarized. Further details can be found in standard textbooks such as [70, 64]. ", "page_idx": 18}, {"type": "text", "text": "A.1 Forward and backward approximation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In numerical analysis, a \u201cforward-error\u201d often measures the distance between the true solution to a problem and the solution returned by a numerical algorithm. A related notion is the so-called \u201cbackward error\u2019 or \u201cbackward approximation.\" For more details we can refer to the standard textbook of Higham [7O]. In this case, the solution returned is the exact solution of a \u201cnearby problem,\u201d and the backward error quantifies the distance of the original problem to this nearby problem. In PCA, for example, a backward-approximation could be defined by $\\begin{array}{r}{\\widetilde{\\mathbf{C}}_{k}=\\arg\\operatorname*{min}_{\\mathbf{C}^{\\top}\\mathbf{C}=\\mathbf{I}_{k\\times k}}\\mathrm{tr}(\\mathbf{C}^{\\top}\\widetilde{\\mathbf{H}}\\mathbf{C})}\\end{array}$ The backward error would be given $\\|\\mathbf{H}-\\widetilde{\\mathbf{H}}\\|$ for some norm. An algorithm is backward stable if the backward error is always well-defined and bounded. A forward error type of approximation is often harder to achieve. A common rule-of-thumb states that ", "page_idx": 18}, {"type": "text", "text": "forward error $\\lesssim$ backward error $\\times$ condition number of the problem. ", "page_idx": 18}, {"type": "text", "text": "However, this does not generally hold for eigenvalue problems. For example, Proposition 1.1 in [15] can be used to transformed a backward approximate diagonalization error to a forward error for the eigenvectors, but the bound depends on the minimum eigenvalue gap between any eigenvalue pair. As the authors point in Remark 1.2, special treatment is needed in terms of the invariant subspaces in the presence of multiple eigenvalues, which are thoroughly analyzed in this work. ", "page_idx": 18}, {"type": "text", "text": "A.2 Floating point model ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We assume a standard floating point model of computation and borrow its axioms from [70, Chapter 2]. There is a fixed number of bits to represent floating point numbers, specifically, one bit is reserved for the sign $s,p$ bits are used for the exponent $e$ ,and $t$ bits are used for the significand $m$ ", "page_idx": 18}, {"type": "text", "text": "A real number $\\alpha\\in\\mathbb R$ is rounded to a foating point number ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{H}(\\alpha)=s\\times2^{e-t}\\times m.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The sign $s$ is $^+$ if the corresponding bit is one, and - if the bit is zero. The exponent $e$ is stored as a binary number in the so-called biased form, and its range is $e\\in[-M,M]$ ,where $M=2^{p-1}$ . The significand $m$ is an integer that satisfies $2^{t-1}\\leq m\\leq2^{\\overline{{t}}}-1$ , where the lower bound is enforced to ensure that the system is normalized, i.e. the first bit of $m$ is always 1. We can therefore write $\\mathbf{fl}(\\alpha)$ in a more intuitive representation ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{f l}(\\alpha)=\\pm2^{e}\\times\\left(\\frac{m_{1}}{2}+\\frac{m_{2}}{2^{2}}+\\ldots+\\frac{m_{t}}{2^{t}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first bit $m_{1}$ of $m$ is always equal to one for normalized numbers. The range of normalized numbers is therefore $\\left[2^{-M},2^{M}(2\\,\\dot{{\\bar{-}}}\\,2^{-\\dot{t}})\\right]$ . Numbers that are smaller than $2^{-M}$ are called subnormal and they will be ignored for simplicity, since we can either add more bits in the exponent, or account for them in the failure probability when the numbers are random (the latter strategy is used for example in Lemma D.1). Similarly, numbers that are larger than $2^{M}(2-2^{-t})$ are assumed to be numerically equal to infinity, denoted by INF. ", "page_idx": 18}, {"type": "text", "text": "From [70, Theorem 2.2], for all real numbers $\\alpha$ in the normalized range it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{f l}(\\alpha)=(1+\\theta)\\alpha,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\theta\\in\\mathbb{R}$ satisfies $|\\theta|\\leq2^{-t}:={\\mathbf{u}}$ where $\\mathbf{u}$ is the machine precision. Clearly, $t=O(\\log(1/\\mathbf{u}))$ in which case we can always obtain a bound for the number of required bits of a numerical algorithm if we have an upper bound for the precision u. We will write the same for complex numbers which are represented as a pair of normalized foating point numbers. ", "page_idx": 18}, {"type": "text", "text": "The floating point implementation of each arithmetic operation $\\odot\\in\\{+,-,\\times,/\\}$ also satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\sf f l}(\\alpha\\odot\\beta)=(1+\\theta)(\\alpha\\odot\\beta),\\quad|\\theta|\\leq{\\bf u}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Divisions and multiplications with 1 and 2 do not introduce errors (for the latter we simply increase/decrease the exponent). We assume that we also have an implementation of $\\sqrt{\\cdot}$ suchthat $\\mathsf{f l}(\\sqrt{\\alpha})=(1+\\theta)\\sqrt{\\alpha}$ where $\\lvert\\theta\\rvert\\leq\\mathbf{u}$ . From [70, Lemma 3.1], we can bound products of errors as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{n}(1+\\theta_{i})^{\\rho_{i}}=1+\\eta_{n},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\rho_{i}=\\pm1$ and $\\begin{array}{r}{|\\eta_{n}|\\leq\\frac{n\\mathbf{u}}{1-n\\mathbf{u}}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "The above can be extended also for complex arithmetic (see [70, Lemma 3.5]), where the bound becomes $|\\theta|\\leq O(\\mathbf{u})$ , but we will ignore the constant prefactor for simplicity. ", "page_idx": 19}, {"type": "text", "text": "Operations on matrices can be analyzed in a similar manner. Let $\\otimes$ denote the element-wise multiplication between two matrices and $\\oslash$ the element-wise division. The floating point representation of a matrix $\\mathbf{A}$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{fl}(\\mathbf{A})=\\mathbf{A}+\\Delta\\otimes\\mathbf{A},\\quad|\\Delta_{i,j}|\\leq\\mathbf{u}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It can be shown that $\\|\\Delta\\|\\leq\\mathbf{u}\\sqrt{n}\\|\\mathbf{A}\\|$ ", "page_idx": 19}, {"type": "text", "text": "For any operation $\\odot\\in\\{+,-,\\otimes,\\oslash\\}$ and matrices $\\mathbf{A}$ and $\\mathbf{B}$ it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{t}|(\\mathbf{A}\\odot\\mathbf{B})=\\mathbf{A}\\odot\\mathbf{B}+\\mathbf{A}\\otimes(\\mathbf{A}\\odot\\mathbf{B}),\\quad|\\mathbf{\\Delta}|\\mathbf{\\Delta}\\mathbf{\\Delta}|\\otimes\\mathbf{u},\\quad\\|\\mathbf{\\Delta}\\Delta\\otimes(\\mathbf{A}\\odot\\mathbf{B})\\|\\le\\mathbf{u}\\sqrt{n}\\|\\mathbf{A}\\odot\\mathbf{B}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.3  Spectral decomposition, pseudospectrum, and eigenvalue bounds ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first recall the definition of the spectral decomposition of a diagonalizable matrix ", "page_idx": 19}, {"type": "text", "text": "Definition A.1 (Spectrum and spectral decomposition). $A$ matrix A is diagonalizable if there exist invertiblematrix $\\mathbf{V}$ and diagonal matrix A such that $\\mathbf{A}=\\mathbf{V}\\mathbf{A}\\mathbf{V}^{-1}$ .This is called the spectral decomposition of A. The set $\\Lambda(\\mathbf{A})=\\{\\Lambda_{i,i}|i=1,\\dots,n\\}$ is the spectrum of A. ", "page_idx": 19}, {"type": "text", "text": "The spectral theorem states that Hermitian matrices (or, more generally, normal matrices) can be always diagonalized by unitary transformations. ", "page_idx": 19}, {"type": "text", "text": "Theorem A.1 (Spectral theorem).If $\\mathbf{A}\\in\\mathbb{C}^{n\\times n}$ is Hermitian,then there exists orthogonal matrix $\\mathbf{Q}\\in\\mathbb{C}^{n\\times n}$ Such that $\\mathbf{A}=\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{*}$ where $\\Lambda$ is a diagonal matrix with real diagonal elements. ", "page_idx": 19}, {"type": "text", "text": "In the sections that follow we need to bound the (forward) errors on the eigenvalues of matrices under perturbations that are introduced due to the finite precision arithmetic. Such bounds can be derived by the classic Bauer-Fike theorem the following (cf. [24, 48] for more details). ", "page_idx": 19}, {"type": "text", "text": "There are many bounds in the literature describing the effect of perturbations on the eigenvalues. We summarize some classic results in the following proposition and refer to Bhatia's monograph for a detailed overview [28]. ", "page_idx": 19}, {"type": "text", "text": "Fact A.1. Let A and $\\mathbf{B}$ betwo $n\\,\\times\\,n$ matrices. The following bounds are known between the eigenvalues $\\lambda_{1}(\\mathbf{A})\\leq\\lambda_{2}(\\mathbf{A})\\leq...\\leq\\lambda_{n}(\\mathbf{A})$ of Aand $\\lambda_{1}(\\mathbf{B})\\leq\\ldots\\leq\\lambda_{n}(\\mathbf{B})$ ofB: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\frac{\\mathbf{A}}{H e r m i t i a n}}&{\\frac{\\mathbf{B}}{H e r m i t i a n}}&{|\\lambda_{i}(\\mathbf{A})-\\lambda_{i}(\\mathbf{B})|\\leq\\|\\mathbf{A}-\\mathbf{B}\\|}&{\\overset{r e f e r e n c e}{/I39J}}\\\\ {H e r m i t i a n}&{N o n\\!\\cdot\\!H e r m i t i a n}&{|\\lambda_{i}(\\mathbf{A})-\\lambda_{i}(\\mathbf{B})|\\leq O(\\log(n))\\|\\mathbf{A}-\\mathbf{B}\\|}&{\\overset{r e f e r e n c e}{/B-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Pseudospectral analysis is useful when the aforementioned bounds are not applicable. ", "page_idx": 19}, {"type": "text", "text": "Definition A.2 (Pseudospectrum). For some $\\epsilon>0$ the $\\epsilon$ -pseudospectrum of a matrix $\\mathbf{M}\\in\\mathbb{C}^{n\\times n}$ is defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Lambda_{\\epsilon}(\\mathbf{M}):=\\{\\lambda\\in\\mathbb{C}:\\lambda\\in\\Lambda(\\mathbf{M}+\\mathbf{E})\\,f o r\\,s o m e\\,\\|\\mathbf{E}\\|<\\epsilon\\}}\\\\ &{\\qquad\\qquad=\\left\\{\\lambda\\in\\mathbb{C}:\\|(\\lambda\\mathbf{I}-\\mathbf{M})^{-1}\\|>1/\\epsilon\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\Lambda(\\mathbf{M})$ is the spectrum of M. ", "page_idx": 19}, {"type": "text", "text": "Recall some useful properties from the seminal book of Trefethen and Embree [135] ", "page_idx": 19}, {"type": "text", "text": "Proposition A.1 (Collective results from [135]). Let $D(z,r)$ denote the open disk of radius r in the complex planecentered at $z\\in\\mathbb{C}$ M and $\\mathbf{E}$ be two $n\\times n$ matrices, and $\\epsilon>\\|\\bf E\\|$ be a positive real number. The following hold: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Lambda_{\\epsilon-\\|\\mathbf{E}\\|}(\\mathbf{M})\\subseteq\\Lambda_{\\epsilon}(\\mathbf{M}+\\mathbf{E})\\subseteq\\Lambda_{\\epsilon+\\|\\mathbf{E}\\|}(\\mathbf{M})\\,([l35,\\,T h m\\,52.4]),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(ii\uff09 Any bounded connected component of $\\Lambda_{\\epsilon}(\\mathbf{M})$ has a nonempty intersection with $\\Lambda(\\mathbf{M})$ i.e., it contains at least one eigenvalue ([135, Thm $4.3J_{\\cdot}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bigcup_{i}D(\\lambda_{i},\\epsilon)\\subseteq\\Lambda_{\\epsilon}(\\mathbf{M})\\subseteq\\bigcup_{i}D\\bigl(\\lambda_{i},\\epsilon\\kappa_{\\mathbf{V}}(\\mathbf{M})\\bigr)\\,(I I35,\\,T h m s\\,4.3\\,a n d\\,52.2].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "A.4  Eigenvector condition number of definite pencils ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Some approximation bounds throughout the paper depend on the eigenvector condition number of the generalized eigenproblem (1). For arbitrary GEPs a bound for this quantity might not always exist, but a straightforward bound exists for the Hermitian definite case. To obtain such a bound we define the eigenvector condition number of a diagonalizable matrix as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\kappa_{\\mathbf{V}}(A):=\\operatorname*{inf}_{\\mathbf{V}\\mathbf{D}\\mathbf{V}^{-1}=\\mathbf{A}}\\|V\\|\\|V^{-1}\\|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proposition A.2. Let $\\mathbf{H}\\in\\mathbb{H}^{n}$ and $\\mathbf{S}\\in\\mathbb{H}_{++}^{n}$ and consider the definite GEP $\\mathbf{H}\\mathbf{C}=\\mathbf{S}\\mathbf{C}\\boldsymbol{\\Lambda}$ .Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\kappa_{\\mathbf{V}}(\\mathbf{S}^{-1}\\mathbf{H})\\leq\\sqrt{\\kappa(\\mathbf{S})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof.Since $\\mathbf{S}$ is Hermitian positive-definite, it can be written as $\\mathbf{S}\\,=\\,\\mathbf{LL}^{*}$ for some matrix L. Then we can transform the GEP to a Hermitian eigenproblem, specifically ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{L}^{-1}\\mathbf{H}\\mathbf{L}^{-*}\\mathbf{L}^{*}\\mathbf{C}=\\mathbf{L}^{*}\\mathbf{C}\\boldsymbol{\\Lambda}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\mathbf{L}\\mathbf{L}^{*}=\\mathbf{S}$ we have that $\\|\\mathbf{L}\\|^{2}=\\|\\mathbf{L}\\mathbf{L}^{*}\\|=\\|\\mathbf{S}\\|$ and similarly $\\lVert\\mathbf{L}^{-1}\\rVert^{2}=\\lVert\\mathbf{S}^{-1}\\rVert$ , which means that $\\kappa(\\mathbf{L})=\\sqrt{\\kappa(\\mathbf{S})}$ Since $\\mathbf{L}^{-1}\\mathbf{H}\\mathbf{L}^{-*}$ is Hermitian, it can be diagonalized by a unitary matrix, i. there exists $\\hat{C}$ such that $\\|\\mathbf{L}^{*}\\widehat{C}\\|=\\|(\\mathbf{L}^{*}\\widehat{C})^{-1}\\|=1$ and $\\mathbf{L}^{*}\\hat{C}$ diagonalizes $\\mathbf{L}^{-1}\\mathbf{H}\\mathbf{L}^{-*}$ . For $\\widehat{C}$ we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\widehat{\\boldsymbol{C}}\\|=\\|\\mathbf{L}^{-*}\\mathbf{L}^{*}\\widehat{\\boldsymbol{C}}\\|\\leq\\|\\mathbf{L}^{-*}\\|\\|\\mathbf{L}^{*}\\widehat{\\boldsymbol{C}}\\|=\\|\\mathbf{L}^{-*}\\|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\widehat{C}^{-1}\\|=\\|\\widehat{C}^{-1}\\mathbf{L}^{-*}\\mathbf{L}^{*}\\|\\leq\\|\\widehat{C}^{-1}\\mathbf{L}^{-*}\\|\\|\\mathbf{L}^{*}\\|=\\|(\\mathbf{L}^{*}\\mathbf{C})^{-1}\\|\\|\\mathbf{L}^{*}\\|=\\|\\mathbf{L}^{*}\\|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Undoing the transformation, we can see that $\\widehat{C}$ also satisfies: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{H}\\widehat{C}=\\mathbf{S}\\widehat{C}\\mathbf{\\Lambda}\\mathbf{\\Lambda},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "i.e.. $\\hat{C}$ diagonalizes $\\mathbf{S}^{-1}\\mathbf{H}$ since $\\widehat{C}^{-1}\\mathbf{S}^{-1}\\mathbf{H}\\widehat{C}\\,=\\,\\mathbf{A}$ . We conclude that $\\kappa_{\\mathbf{V}}(\\mathbf{S}^{-1}\\mathbf{H})\\,\\leq\\,\\kappa({\\widehat{C}})\\,\\leq$ $\\lVert{\\bf L}\\rVert\\lVert{\\bf L}^{-1}\\rVert=\\sqrt{\\kappa({\\bf S})}$ ", "page_idx": 20}, {"type": "text", "text": "A.5 Imported floating point algorithms for fast linear algebra ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Before we dive into the details, we recall the concept of \u201cfast matrix multiplication\", pioneered by Strassen [134], who showed that two square matrices can be multiplied in $O(n^{\\omega})$ ,where $\\omega=$ $\\log_{2}7\\approx2.807<3$ , in real arithmetic. Since then, the matrix multiplication exponent $\\omega$ hasbeen significantly reduced, the record to date being $\\omega\\leq2.371552$ [47, 141]. In two seminal works [40, 39], Demmel, Dumitriu, Holtz, and Kleinberg, proved that any fast matrix multiplication algorithm can be executed numerically stably in a floating point machine with almost the same arithmetic complexity as in real arithmetic. They also showed that other problems in Numerical Linear Algebra can be reduced to such matrix multiplications, including inversion, LU and QR factorizations, and solving linear systems of equations. Our algorithms build on the existing results for stable fast matrix multiplication [40], inversion [39], as well as backward-approximate diagonalization and the matrix sign function [15]. We import the corresponding results from the aforementioned works in the following theorems. ", "page_idx": 20}, {"type": "text", "text": "Theorem A.2 (MM, stable fast matrix multiplication [40]). For every $\\eta>0,$ there exists a fast matrix multiplication algorithm MM which takes as input two matrices $\\mathbf{A},\\mathbf{B}\\in\\mathbb{C}^{n\\times n}$ andamachine precision $\\mathbf{u}>0$ andreturns $\\mathbf{C}\\gets\\mathsf{M M}(\\mathbf{A},\\mathbf{B})$ suchthat ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathbf{C}-\\mathbf{A}\\mathbf{B}\\|\\leq\\mu_{\\sf M M}(n)\\cdot\\mathbf{u}\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "on floating point machine with precision u, where $\\mu_{\\sf M M}(n)=n^{c_{\\eta}}$ ,for some constant $c_{\\eta}$ independent of n. Such an algorithm is called $\\mu_{\\mathsf{M M}}(n)$ -stable, and it requires $T_{\\mathsf{M M}}(n)=O(n^{\\omega+\\dot{\\eta}})$ arithmetic operations, where $\\omega$ is the exponent of matrix multiplication in real arithmetic. ", "page_idx": 21}, {"type": "text", "text": "Theorem A.3 (INV, logarithmically-stable fast inversion [39]). For every $\\eta\\:>\\:0$ there exists $a$ fast inversion algorithm INV which takes as input an invertible matrix $\\mathbf{A}\\,\\in\\,\\mathbb{C}^{n\\times n}$ anda machine precision $\\mathbf{u}>0$ andreturns $\\mathbf{C}\\gets\\mathsf{I N V}(\\mathbf{A})$ suchthat ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathbf{C}-\\mathbf{A}^{-1}\\|\\leq\\mu_{\\sf I N V}(n)\\cdot\\mathbf{u}\\cdot\\kappa(\\mathbf{A})^{c_{\\sf I N V}\\log(n)}\\|\\mathbf{A}^{-1}\\|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "on floating point machine with precision u, where $\\mu_{|\\mathsf{N V}}(n)\\,=\\,O\\bigl(n^{c_{\\eta}+\\log(10)}\\bigr)$ for some constants $c_{\\mathsf{I N V}}\\,\\leq\\,8$ and $c_{\\eta}$ independent of $n$ :Such an algorithm is called $(\\mu_{mathsf{I N V}}(n),c_{\\mathsf{I N V}})$ -stable, and it requires $T_{\\mathsf{I N V}}(n)\\dot{=}\\;O(T_{\\mathsf{M M}}(n))$ arithmetic operations, where $T_{\\mathsf{M M}}(n)$ is the same as in Theorem A.2. ", "page_idx": 21}, {"type": "text", "text": "Sign function and deflation.  The next result that we import is a floating point algorithm for the matrix sign function, which, for a diagonalizable matrix $\\overset{\\,\\,-}{\\mathbf{A}}=\\mathbf{V}\\mathbf{A}\\mathbf{V}^{-1}$ such that $\\Lambda(\\mathbf{A})$ doesnot interesect with the imaginary axis, is defined as $\\operatorname{sgn}(\\mathbf{A})\\;=\\;\\mathbf{V}\\operatorname{sgn}(\\mathbf{A})\\mathbf{V}^{-1}$ where $\\operatorname{sgn}(\\mathbf{\\Lambda})$ is a diagonal matrix and each diagonal entry contains the sign of the real part of the corresponding eigenvalue. To state the main result, we first recall the definition of the Circles of Apollonius. ", "page_idx": 21}, {"type": "text", "text": "Definition A.3 (Circles of Apollonius, imported from Section 4.1 of [15]). Let $\\alpha\\,\\in\\,(0,1)$ .The Circles of Apollonius ${\\mathsf{C}}_{\\alpha}$ aredefined as $\\mathsf{C}_{\\alpha}=\\mathsf{C}_{\\alpha}^{+}\\cup\\mathsf{C}_{\\alpha}^{-}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathsf{C}}_{\\alpha}^{+}=\\{z\\in\\mathbb{C}:|m(z)|\\leq\\alpha\\},\\quad{\\mathsf{C}}_{\\alpha}^{+}=\\{z\\in\\mathbb{C}:|m(z)|^{-1}\\leq\\alpha\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where $\\begin{array}{r}{m(z)=\\frac{1-z}{1+z}}\\end{array}$ isf ${\\sf C}_{\\alpha}^{+}$ is centered at $\\scriptstyle{\\frac{1+\\alpha^{2}}{1-\\alpha^{2}}}$ and has radius $\\scriptstyle{\\frac{2\\alpha}{1-\\alpha^{2}}}$ ,and ${\\mathsf{C}}_{\\alpha}^{-}$ is its reflection with respect to the imaginary axis. ", "page_idx": 21}, {"type": "text", "text": "Theorem A.4 (SGN, imported Theorem 4.9 from_ [15]). There is a deterministic algorithm $\\mathsf{S G N}(\\mathbf{A},\\alpha,\\eta,\\epsilon)$ which takes as input amatrix $\\mathbf{A}\\in\\mathbb{C}^{n\\times n}$ anaccuracyparameter $\\epsilon\\,\\in\\,(0,1/12)$ and parameters $\\eta\\in(0,1)$ \uff0c $0<1-\\alpha<1/100,$ suchthatit isguaranteedthat $\\Lambda_{\\eta}(\\mathbf{A})\\subset\\mathsf{C}_{\\alpha}$ .The algorithmreturnsamatrix $\\widetilde{\\mathbf{S}}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lVert\\widetilde{\\mathbf{S}}-\\mathrm{sgn}(\\mathbf{A})\\rVert\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as long as the machine precision satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf u\\leq\\mathbf u_{\\mathsf{S G N}}:=\\frac{\\alpha^{2^{N+1}(c_{\\mathsf{I N V}}\\log(n)+3)}}{\\mu_{\\mathsf{I N V}}(n)\\sqrt{n}N},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "corresponding to at most ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\log(n)\\log^{3}(\\frac{1}{1-\\alpha})\\left(\\log(\\frac{1}{\\epsilon})+\\log(\\frac{1}{\\eta})\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "bits of precision. Here ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N=\\lceil\\log(\\frac{1}{1-\\alpha})+3\\log(\\log(\\frac{1}{1-\\alpha}))+\\log(\\log(\\frac{1}{\\epsilon\\eta}))+7.79\\rceil}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "denotes the number of iterations that the algorithm executes. The arithmetic complexity is ", "page_idx": 21}, {"type": "equation", "text": "$$\nO(N T_{\\mathsf{M M}}(n))).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In PCA, we will need to be able to compute a basis for the column space of an approximate spectral projector. For this, we recall the following algorithm. ", "page_idx": 21}, {"type": "text", "text": "Theorem A.5 (DEFLATE, imported Theorem 5.3 of [15]). There exists a randomized algorithm $\\widetilde{\\mathbf{C}}_{k}\\,\\gets\\,\\mathsf{D E F L A T E}(\\widetilde{\\Pi}_{k},k,\\beta,\\epsilon)$ . which takes as input a matrix $\\tilde{\\Pi}_{k}$ ,a rank parameter $k\\;\\in\\;[n]$ a parameter $\\beta$ such that $\\|\\widetilde{\\boldsymbol{\\Pi}}_{k}-\\boldsymbol{\\Pi}_{k}\\|\\,\\le\\,\\beta_{!}$ for some projector matrix $\\Pi_{k}$ of rank- $k$ ,and a desired accuracy e,andreturns a(complex) matrix $\\widetilde{\\mathbf{C}}_{k}$ such that, there exists a matrix $\\mathbf{C}_{k}$ with $k$ columns thatforms anorthonormalbasisfor $\\mathrm{im}(\\Pi_{k})$ ,and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\mathbf{C}}_{k}-\\mathbf{C}_{k}\\|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The algorithm suceeds withpobabilir at least $1-{\\frac{(20n)^{3}{\\sqrt{\\beta}}}{\\epsilon^{2}}}$ The number of aribeicoperations is at most $O(T_{\\mathsf{M M}}(n))$ , and it requires $O(\\log(n)\\!+\\!\\log({\\frac{1}{\\epsilon}}))$ bits of precision. Internally, the algorithm generates $O(n^{2})$ random numbers to form a $n\\times n$ complex Ginibre matrix.1 ", "page_idx": 22}, {"type": "text", "text": "Computing an orthonormal basis and the spectral norm. Lastly, we recall the following two results. The first one is for the QR factorization from [39], which we adapt suitably for our analysis specifically, to compute an orthonormal basis for the column space of a rectangular matrix. ", "page_idx": 22}, {"type": "text", "text": "Theorem A.6 (Basis computation, follows from Section 4 in [39]). Let $\\mathbf{A}\\;\\in\\;\\mathbb{R}^{m\\times n},m\\;\\geq\\;n$ There exists an algorithm $\\widetilde{\\mathbf{Q}},\\widetilde{\\mathbf{R}}\\;=\\;\\mathsf{Q R}(\\mathbf{A}),$ which returns a matrix $\\widetilde{\\textbf{Q}}\\in\\ \\mathbb{R}^{m\\times n}$ and an upper triangular marix $\\widetilde{\\mathbf{R}}\\,\\in\\,\\mathbb{R}^{n\\times n}$ $O(m n^{\\omega-1})$ foatig point operaionsusing $O(\\log(\\frac{n\\kappa(\\mathbf{A})}{\\epsilon_{\\mathsf{Q R}}}))$ bis, where $\\epsilon_{\\mathsf{Q R}}\\in(0,1)$ is a given accuracy. The matrix $\\widetilde{\\mathbf{Q}}$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{Q}-\\widetilde{\\mathbf{Q}}\\boldsymbol{\\Phi}\\rVert\\leq\\epsilon_{\\mathsf{Q R}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for some orthogonal matrix $\\Phi\\in\\mathbb{R}^{n\\times n}$ ,where $\\mathbf{Q}$ has orthonormal columns and $\\mathbf A=\\mathbf Q\\mathbf R$ is the true economy-QR of A. ", "page_idx": 22}, {"type": "text", "text": "Proof. We first scale $\\mathbf{A}^{\\prime}\\mathbf{\\Lambda}\\leftarrow\\mathbf{\\Lambda}\\mathbf{A}/M$ \uff0cwhere $M$ is the smallest power of two that is larger than $n\\|\\mathbf{A}\\|_{\\mathrm{max}}$ . This ensures that $\\Omega(\\mathbf{1}/n)\\,\\leq\\,\\|\\mathbf{A}^{\\prime}\\|\\,\\leq\\,1$ . Since we scale by a power of 2, then there are no floating point errors and the orthonormal basis from the QR factorization remains the same (only the upper triangular factor is scaled). ", "page_idx": 22}, {"type": "text", "text": "We now use the corresponding algorithm of [39] on $\\mathbf{A}^{\\prime}$ . It returns three matrices: an upper triangular matrix $\\widetilde{\\mathbf{R}}\\,\\in\\,\\mathbb{R}^{n\\times n}$ , a matrix $\\textbf{W}\\in\\,\\mathbb{R}^{m\\times n}$ with $\\|\\mathbf{W}\\|\\,\\in\\,{\\cal O}(n)$ , and a matrix $\\textbf{Y}\\in\\ \\mathbb{R}^{n\\times m}$ with $\\|\\mathbf{Y}\\|\\in O(n)$ . The matrix $\\boldsymbol{\\Psi}=\\mathbf{I}-\\mathbf{W}\\mathbf{Y}^{\\top}+\\mathbf{E}_{\\boldsymbol{\\Psi}}$ exactly satisfies $\\boldsymbol{\\Psi}\\boldsymbol{\\Psi}^{\\intercal}=\\mathbf{I}$ and the error matrix satisfies $\\|\\mathbf{E}_{\\Psi}\\|\\in O(\\mathrm{poly}(n)\\mathbf{u})$ . Moreover, $\\Psi\\left(\\stackrel{\\widetilde{\\mathbf{R}}}{\\mathop{0}_{m-n\\times n}}\\right)=\\widehat{A}$ where $\\widehat{\\pmb{A}}=\\mathbf{A}^{\\prime}\\!+\\!\\mathbf{E}_{\\mathbf{A}}$ and $\\lvert\\lvert\\mathbf{E_{A}}\\rvert\\rvert\\leq$ $O(\\mathrm{poly}(n)\\mathbf{u})\\|\\mathbf{A}^{\\prime}\\|\\le O(\\mathrm{poly}(n)\\mathbf{u})$ . Note that $\\|\\widetilde{\\mathbf{R}}\\|\\leq\\|\\widehat{A}\\|\\leq\\|\\mathbf{A}^{\\prime}\\|+\\|\\mathbf{E_{A}}\\|\\leq1+O(\\mathrm{poly}(n)\\mathbf{u})$ The total cost of the algorithm is $O(m n^{\\omega-1})$ floating point operations (ignoring the negligible term $\\eta>0$ in the exponent). ", "page_idx": 22}, {"type": "text", "text": "Using $\\mathbf{W}$ and $\\mathbf{Y}$ , we can construct an approximate basis $\\widetilde{\\mathbf{Q}}$ as follows. Let $\\mathbf{Y}_{n}\\in\\mathbb{R}^{n\\times n}$ contain the frst $n$ columns of $\\mathbf{Y}$ We can compute the matrix $\\widetilde{\\mathbf Q}=\\left(\\!\\!\\begin{array}{c}{{{\\mathbf I}_{n}}}\\\\ {{0_{m-n\\times n}}}\\end{array}\\!\\!\\right)-\\mathsf{M M}({\\mathbf W},{\\mathbf Y}_{n})$ in $O(m n^{\\omega-1})$ floating point operations by performing the multiplication in blocks of size $n\\times n$ .We can write $\\widetilde{\\mathbf{Q}}=\\bar{\\hat{Q}}+\\mathbf{E}_{\\widehat{Q}}$ where $\\widehat{\\pmb{Q}}$ contains the first $n$ columns of $\\Psi$ and $\\|\\mathbf{E}_{\\widehat{Q}}\\|\\in O(\\mathrm{poly}(n)\\mathbf{u})$ ", "page_idx": 22}, {"type": "text", "text": "Note that $\\widetilde{\\bf R}$ satisfies $\\widehat{Q}\\tilde{\\mathbf{R}}\\,=\\,\\mathbf{A}^{\\prime}+\\mathbf{E}_{\\mathbf{A}}$ . Since $\\widehat{\\pmb{Q}}$ has orthonormal columns, the singular values of $\\widetilde{\\bf R}$ satisfy $\\sigma_{i}(\\widetilde{\\mathbf{R}})\\,=\\,\\sigma_{i}(\\widehat{Q}\\widetilde{\\mathbf{R}})\\,=\\,\\sigma_{i}(\\mathbf{A}^{\\prime}+\\mathbf{E}_{\\mathbf{A}})\\,\\in\\,\\sigma_{i}(\\mathbf{A}^{\\prime})\\pm\\|\\mathbf{E}_{\\mathbf{A}}\\|$ , where the last comes from the stability of singular values which is a consequence of Weyl's inequality in Fact A.1. Now, if $\\begin{array}{r}{\\|\\mathbf{E}_{\\mathbf{A}}\\|\\leq\\frac{1}{4}\\sigma_{\\operatorname*{min}}(\\mathbf{A}^{\\prime})}\\end{array}$ which is achevedby seting $\\mathbf{u}\\leq c_{\\mathrm{poly}(n)\\kappa(\\mathbf{A})}$ forsome ostant $c$ then $\\widetilde{\\bf R}$ .s $\\begin{array}{r}{\\|\\widetilde{\\bf R}^{-1}\\|=1/\\sigma_{\\mathrm{min}}(\\widetilde{\\bf R})\\le\\frac{4}{3\\sigma_{\\mathrm{min}}({\\bf A}^{\\prime})}\\le O(n)\\kappa({\\bf A}).}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Since $\\widetilde{\\bf R}$ remains full rank, we have that $\\widehat{\\pmb{Q}}\\widetilde{\\mathbf{R}}=\\widehat{\\pmb{A}}\\Rightarrow\\widehat{\\pmb{Q}}=\\mathbf{A}^{\\prime}\\widetilde{\\mathbf{R}}^{-1}+\\mathbf{E_{A}}\\widetilde{\\mathbf{R}}^{-1}$ . If we write the true economy-QR of $\\mathbf{A}^{\\prime}$ as $\\mathbf{A}^{\\prime}=\\mathbf{QR}$ , then we can see that $\\|\\mathbf{R}\\widetilde{\\mathbf{R}}^{-1}\\|=\\|\\mathbf{Q}\\mathbf{R}\\widetilde{\\mathbf{R}}^{-1}\\|=\\|\\mathbf{A}\\widetilde{\\mathbf{R}}^{-1}\\|=$ $\\lVert\\widehat{\\mathbf{Q}}-\\mathbf{E}_{\\mathbf{A}}\\widetilde{\\mathbf{R}}^{-1}\\rVert$ .This implies that $\\|\\mathbf{R}\\widetilde{\\mathbf{R}}^{-1}\\|\\,\\in\\,[1-\\|\\mathbf{E_{A}}\\widetilde{\\mathbf{R}}^{-1}\\|,1+\\|\\mathbf{E_{A}}\\widetilde{\\mathbf{R}}^{-1}\\|]$ With similar argumentsweobtain ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\mathbf{R}}{\\mathbf{R}}^{-1}\\|\\in[1-\\|\\mathbf{E_{A}}\\mathbf{R}^{-1}\\|,1+\\|\\mathbf{E_{A}}\\mathbf{R}^{-1}\\|].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We previously argued that $\\lVert\\mathbf{R}\\rVert,\\lVert\\tilde{\\mathbf{R}}\\rVert\\leq O(1)$ , and $\\lVert\\mathbf{R}^{-1}\\rVert,\\lVert\\widetilde{\\mathbf{R}}^{-1}\\rVert\\leq O(n)\\kappa(\\mathbf{A})$ . Thus, if we further enforce $\\|\\mathbf{E}_{\\mathbf{A}}\\|\\leq\\epsilon_{1}c_{1}\\frac{1}{\\mathrm{poly}(n)\\kappa(\\mathbf{A})}$ for someconstant $c_{1}$ then $\\operatorname*{max}\\{\\|\\mathbf{E_{A}}\\widetilde\\mathbf{R}^{-1}\\|,\\|\\mathbf{E_{A}}\\mathbf{R}^{-1}\\|\\}\\leq$ $\\epsilon_{1}$ , for some $\\epsilon_{1}\\,\\in\\,(0,1/4)$ , which means that all the singular values of RR- are inside the interval $[1-2\\epsilon_{1},1+2\\epsilon_{1}]$ . In other words, RR  is approximately orthogonal. Indeed, we can write $\\mathbf{R}\\widetilde{\\mathbf{R}}^{-1}=\\boldsymbol{\\Phi}+\\mathbf{E}_{\\Phi}$ where $\\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^{\\intercal}=\\mathbf{I}$ and all the singular values of $\\mathbf{E}_{\\Phi}$ are inside $[-2\\epsilon_{1},2\\epsilon_{1}]$ . In particular, $\\boldsymbol{\\Phi}=\\mathbf{U}\\mathbf{V}^{\\top}$ , where $\\mathbf{U},\\mathbf{V}$ come from the SVD of $\\mathbf{R}\\widetilde{\\mathbf{R}}^{-1}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ ", "page_idx": 23}, {"type": "text", "text": "We can now go back to $\\widetilde{\\mathbf{Q}}$ . From the above, $\\widehat{\\pmb{Q}}$ can be written as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\boldsymbol{Q}}=\\widehat{\\boldsymbol{A}}\\widetilde{\\mathbf{R}}^{-1}+\\mathbf{E}_{\\mathbf{A}}\\widetilde{\\mathbf{R}}^{-1}=\\mathbf{Q}\\boldsymbol{\\Phi}+\\mathbf{Q}\\mathbf{E}_{\\Phi}+\\mathbf{E}_{\\mathbf{A}}\\widetilde{\\mathbf{R}}^{-1},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which ultimately gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{Q}}=\\widehat{\\pmb{Q}}+\\mathbf{E}_{\\widehat{\\pmb{Q}}}=\\mathbf{Q}\\pmb{\\Phi}+\\mathbf{Q}\\mathbf{E}_{\\Phi}+\\mathbf{E}_{\\mathbf{A}}\\widetilde{\\mathbf{R}}^{-1}+\\mathbf{E}_{\\widehat{\\pmb{Q}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This means that $\\widetilde{\\mathbf{Q}}$ is just a rotation of the true $\\mathbf{Q}$ plus some additive error terms that we can control with $\\mathbf{u}$ . In particular, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widetilde{\\mathbf{Q}}-\\mathbf{Q}\\Phi\\|\\leq\\|\\mathbf{Q}\\mathbf{E}_{\\Phi}+\\mathbf{E}_{\\mathbf{A}}\\widetilde{\\mathbf{R}}^{-1}+\\mathbf{E}_{\\widehat{Q}}\\|}\\\\ &{\\qquad\\qquad\\leq\\epsilon_{1}+O(\\mathrm{poly}(n)\\mathbf{u})\\|\\widetilde{\\mathbf{R}}^{-1}\\|+O(\\mathrm{poly}(n)\\mathbf{u})}\\\\ &{\\qquad\\qquad\\leq O(\\epsilon_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in the last we applied the required bound for u\u2264 e1C1 poly(n)(A)- ", "page_idx": 23}, {"type": "text", "text": "Combining everything, we conclude that if the machine precision satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq\\epsilon_{\\mathsf{Q R}}\\frac{1}{\\mathrm{poly}(n)\\kappa(\\mathbf{A})},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then we can compute a matrix $\\widetilde{\\mathbf{Q}}$ in $O(m n^{\\omega-1})$ foating point operations such that there exists orthogonal matrix $\\Phi$ satisfying ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lVert\\widetilde{\\mathbf{Q}}-\\mathbf{Q}\\Phi\\rVert\\leq\\epsilon_{\\mathsf{Q R}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "using $O(\\log(n\\kappa(\\mathbf{A})/\\epsilon_{\\mathsf{Q R}}))$ bits of precision. ", "page_idx": 23}, {"type": "text", "text": "The second result is for the computation of the spectral norm of the matrix from [107] using the Lanczos algorithm. ", "page_idx": 23}, {"type": "text", "text": "Theorem A.7 (Imported variant of Theorem 18 from the full version of [107]). Let $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ and $\\delta\\in(0,1/2)$ a failure probability parameter. We can compute a vector y such that, with probability at least $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{9}{10}\\|\\mathbf{A}\\|\\leq\\frac{\\|\\mathbf{A}\\mathbf{y}\\|}{\\|\\mathbf{y}\\|}\\leq\\|\\mathbf{A}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "in $O\\left(m n\\log(n)\\log(1/\\delta)\\right)$ foating point operations using ${\\cal O}(\\log(n))$ bits of precision. Internally the algorithm generates ${\\mathcal{O}}(n\\log(1/\\delta))$ random bits. ", "page_idx": 23}, {"type": "text", "text": "We can obtain the following corollary. ", "page_idx": 23}, {"type": "text", "text": "Corollary A.1 (Spectral norm). Let $\\mathbf{B}\\,\\in\\,\\mathbb{C}^{m\\times n}$ .We can compute a value $\\widetilde{\\Sigma}\\;\\in\\;\\Theta(\\|\\mathbf{B}\\|)$ with probability at least $1-2\\delta$ in $O\\left(m n\\log(n)\\log(1/\\delta)\\right)$ floating point operations using ${\\cal O}(\\log(n))$ bits of precision. ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $\\mathbf{R}$ be the real part of $\\mathbf{B}$ and $\\mathbf{Z}$ be the imaginary part.  We can approximate $R\\ \\in$ $[\\frac{9}{10}\\|\\mathbf{R}\\|,\\|\\mathbf{R}\\|]$ and $Z\\ \\in\\ [\\frac{9}{10}\\|\\mathbf{Z}\\|,\\|\\mathbf{Z}\\|]$ using the algorithm of Theorem A.7. We then set $\\widetilde{\\Sigma}\\,=$ $\\dot{R^{\\prime}}+Z\\in[0.9(\\|\\mathbf{R}\\|+\\|\\mathbf{Z}\\|),\\|\\mathbf{R}\\|$ $\\left\\|\\mathbf{R}\\right\\|+\\left\\|\\mathbf{Z}\\right\\|]$ . Using the triangle inequality we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n0.9\\|\\mathbf{B}\\|\\leq0.9(\\|\\mathbf{R}\\|+\\|\\mathbf{Z}\\|)\\leq\\widetilde{\\Sigma}\\leq\\|\\mathbf{R}\\|+\\|\\mathbf{Z}\\|\\leq2\\|\\mathbf{B}\\|,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "or, in other words, $\\widetilde{\\Sigma}\\in[0.9\\|\\mathbf{B}\\|,2\\|\\mathbf{B}\\|]$ ", "page_idx": 23}, {"type": "text", "text": "A.6 Symmetrization ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "There are certain cases where it is crucial to ensure that the result of a foating point multiplication or inversion remains Hermitian. The following proposition states that we can always \u201csymmetrize? a floating point matrix with small additional errors. ", "page_idx": 24}, {"type": "text", "text": "Proposition A.3 (HERM). Let C be a Hermitian matrix and ${\\bf C}^{\\prime}={\\bf C}\\!+{\\bf E}$ for some (non-Hermitian) matrix $\\mathbf{E}_{\\mathrm{:}}$ such that $\\|\\mathbf{C}-\\mathbf{C}^{\\prime}\\|=\\|\\mathbf{E}\\|$ . Consider the matrix $\\mathsf{H E R M}(\\mathbf{C}^{\\prime})$ ,where HERM(A) is an algorithm that replaces the strictly lower triangular part of A with the strictly upper triangular part of A. Then HERM $\\left(\\mathbf{C}^{\\prime}\\right)$ isHermitian and it holds that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\boldsymbol{\\mathbf{C}}-\\boldsymbol{\\mathsf{H}}\\boldsymbol{\\mathsf{E}}\\boldsymbol{\\mathsf{R}}\\boldsymbol{\\mathsf{M}}(\\boldsymbol{\\mathbf{C}}^{\\prime})\\|\\le c_{\\boldsymbol{\\mathsf{H}}\\boldsymbol{\\mathsf{E}}\\boldsymbol{\\mathsf{R}}\\boldsymbol{\\mathsf{M}}}\\log(n)\\cdot\\|\\boldsymbol{\\mathbf{E}}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some constant CHERM. ", "page_idx": 24}, {"type": "text", "text": "Proof. For any matrix A, the upper triangular part of $\\mathbf{A}$ , denoted as $\\Delta_{U}(\\mathbf{A})$ , satisfies the following inequality: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\Delta_{U}(\\mathbf{A})\\|\\leq c\\log(n)\\cdot\\|\\mathbf{A}\\|,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some constant $c$ . The same holds for the lower triangular part $\\Delta_{L}(\\mathbf{A})$ . For the diagonal part $\\operatorname{diag}(\\mathbf{A}))$ it holds that $\\|\\operatorname{diag}(\\mathbf{A})\\|\\leq\\|\\mathbf{A}\\|$ (see [27] for proofs). Let $\\mathbf{C}^{\\prime\\prime}=\\mathsf{H E R M}(\\mathbf{C}^{\\prime})$ . By the definition of $\\mathsf{H E R M}(\\cdot)$ , the matrix $\\mathbf{C}^{\\prime\\prime}$ can be written as $\\dot{{\\bf C}^{\\prime\\prime}}=\\Delta_{U}({\\bf C}^{\\prime})+\\Delta_{U}({\\bf C}^{\\prime})^{*}-\\mathrm{diag}(\\dot{\\bf C}^{\\prime})=$ $\\mathbf{C}+\\left(\\Delta_{U}(\\mathbf{E})+\\Delta_{U}(\\mathbf{E})^{*}-\\mathrm{diag}(\\mathbf{E})\\right)$ . Therefore ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\mathbf{C}^{\\prime\\prime}-\\mathbf{C}\\|=\\|\\Delta_{U}(\\mathbf{E})+\\Delta_{U}(\\mathbf{E})^{*}-\\operatorname{diag}(\\mathbf{E})\\|\\leq2c\\log(n)\\|\\mathbf{E}\\|+\\|\\mathbf{E}\\|\\leq c_{\\mathsf{H E R M}}\\log(n)\\|\\mathbf{E}\\|,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some constant $C H E R M$ ", "page_idx": 24}, {"type": "text", "text": "We can directly use this to derive bounds for matrix multiplication where the result is a Hermitian matrix and for Hermitian matrix inversion. ", "page_idx": 24}, {"type": "text", "text": "Corollary A.2 (Symmetrized matrix multiplication). Let A and $\\mathbf{B}$ betwomatricesand $\\mathbf{C}=\\mathbf{A}\\mathbf{B}$ be a Hermitian matrix. If MM is a $\\mu_{\\mathsf{M M}}$ -stable matrix multiplication algorithm then the matrix ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{C}^{\\prime}=\\mathsf{H E R M}(\\mathsf{M M}(\\mathbf{A},\\mathbf{B}))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is Hermitian and it satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{C}^{\\prime}-\\mathbf{C}\\|\\leq c_{\\mathsf{H E R M}}\\log(n)\\cdot\\mathbf{u}\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|\\cdot\\mu_{\\mathsf{M M}}(n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Straightforward combination of Theorem A.2 and Proposition A.3. ", "page_idx": 24}, {"type": "text", "text": "Corollary A.3 (Symmetrized inversion). Let A be an invertible Hermitian matrix. If INV is $a$ $(\\mu_{\\mathsf{I N V}},c_{\\mathsf{I N V}})$ -stableinversion algorithm then thematrix ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{C}=\\mathsf{H E R M}(\\mathsf{I N V}(\\mathbf{A}))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is Hermitian and it satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\mathbf{C}-\\mathbf{A}^{-1}\\|\\leq c_{\\mathsf{H E R M}}\\log(n)\\cdot\\mu_{\\mathsf{I N V}}(n)\\cdot\\mathbf{u}\\cdot\\kappa(\\mathbf{A})^{c_{\\mathsf{I N V}}\\log n}\\|\\mathbf{A}^{-1}\\|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In addition, if u satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq\\epsilon{\\frac{1}{c_{\\mathsf{H E R M}}\\log(n)\\cdot\\mu_{\\mathsf{N V}}(n)\\cdot\\kappa(\\mathbf{A})^{c_{\\mathsf{N V}}\\log(n)+1}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "forsome $\\epsilon\\in(0,1/2)$ , then both of the following hold: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{C}-\\mathbf{A}^{-1}\\|\\leq\\epsilon\\|\\mathbf{A}^{-1}\\|,}\\\\ {\\frac{1}{2}\\kappa(\\mathbf{A})\\leq\\kappa(\\mathbf{C})\\leq2\\kappa(\\mathbf{A}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The first part is straightforward combination of Theorem A.3 and Proposition A.3. For the second part, we directly bound ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\epsilon)\\|\\mathbf{A}^{-1}\\|\\leq(1-\\epsilon\\frac{1}{\\kappa(\\mathbf{A})})\\|\\mathbf{A}^{-1}\\|\\leq\\|\\mathbf{C}\\|\\leq(1+\\epsilon\\frac{1}{\\kappa(\\mathbf{A})})\\|\\mathbf{A}^{-1}\\|\\leq(1+\\epsilon)\\|\\mathbf{A}^{-1}\\|}\\\\ &{\\qquad\\qquad\\qquad\\Rightarrow\\frac{1}{2}\\|\\mathbf{A}^{-1}\\|\\leq\\|\\mathbf{C}\\|\\leq2\\|\\mathbf{A}^{-1}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $\\|\\mathbf{C}-\\mathbf{A}^{-1}\\|\\ \\leq\\ \\epsilon_{\\kappa(\\mathbf{A})}^{1}\\|\\mathbf{A}^{-1}\\|\\ =\\ \\epsilon_{\\frac{1}{\\|\\mathbf{A}\\|}}\\ =\\ \\epsilon\\sigma_{\\operatorname*{min}}(\\mathbf{A}^{-1})$ .Since $\\mathbf{C}$ and ${\\mathbf{A}}^{-1}$ are both Hermitian then from Weyl's inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\epsilon)\\sigma_{\\operatorname*{min}}(\\mathbf{A}^{-1})\\leq\\sigma_{\\operatorname*{min}}(\\mathbf{C})\\leq(1+\\epsilon)\\sigma_{\\operatorname*{min}}(\\mathbf{A}^{-1})}\\\\ &{\\quad\\Rightarrow\\frac{1}{2}\\sigma_{\\operatorname*{min}}(\\mathbf{A}^{-1})\\leq\\sigma_{\\operatorname*{min}}(\\mathbf{C})\\leq2\\sigma_{\\operatorname*{min}}(\\mathbf{A}^{-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we conclude that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{4}\\kappa(\\mathbf{A})\\leq\\kappa(\\mathbf{C})\\leq4\\kappa(\\mathbf{A}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Remark A.1. Throughout this paper, to simplify our bounds, we assume that ", "page_idx": 25}, {"type": "equation", "text": "$$\n1\\leq\\mu_{\\sf M M}(n),\\mu_{\\sf I N V}(n),c_{\\sf I N V}\\log(n),c_{\\sf H E R M}\\log(n).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, we define a global upper bound $\\mu$ suchthat ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mu_{\\sf M M}(n),c_{\\sf H E R M}\\log(n)\\mu_{\\sf M M}(n),\\mu_{\\sf I N V}(n)\\leq\\mu(n)\\leq O(n^{c_{\\eta}^{\\prime}}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where, from Theorem A.2, $c_{\\eta}^{\\prime}$ is a constant that does not depend on $n$ ", "page_idx": 25}, {"type": "text", "text": "A.7Existing algorithms for invariant subspaces and the (generalized) eigenproblem ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We give a brief overview of some results related to the computation of spectral projectors, eigenvectors, and invariant subspaces for the Hermitian definite generalized eigenproblem. Many more details can be found in standard textbooks and references therein [42, 119, 64, 135, 115], as well as the recent overview of [129] for the complexity of the eigenproblem. For further reading we can highlight some infuential works from the enormous bibliography on eigenvalue algorithms and perturbation bounds [44,43,8, 130,131,132,133,75,108,9,26,76,77,37,2,113, 112]. ", "page_idx": 25}, {"type": "text", "text": "The prominent classic method to solve the Hermitian (generalized or regular) eigenproblem is to use numerically stable Householder transformations to reduce a Hermitian matrix to tridiagonal form, and then apply a tridiagonal shifted QR algorithm to diagonalize the tridiagonal matrix, which gives both the eigenvalues and the eigenvectors. The paramount QR algorithm was originally proposed in [51, 52, 90], its convergence for the symmetric case was analyzed in [140, 38] in exact arithmetic, and the non-symmetric was recently analyzed in floating point in [18, 16, 17]. Other classical algorithms for computing invariant subspaces include [97, 98, 10, 9]. ", "page_idx": 25}, {"type": "text", "text": "With the aforementioned procedure, the floating point complexity is about $\\widetilde O(n^{3})$ floating point operations, potentially up to $\\mathrm{polylog}(1/\\epsilon,1/\\,\\mathrm{ga}\\bar{\\mathrm{p}_{k}})$ factors. It is known that for Hermitian definite pencils $(\\mathbf{H},\\mathbf{S})$ one can extend this procedure such that, simultanteously, $\\mathbf{H}$ will become (Hermitian) tridiagonal and S the identity [36]. The Hermitian tridiagonal matrix can be further reduced with similarity transformations to a symmetric tridiagonal matrix, in which case one can apply again tridiagonal symmetric QR. Using standard perturbation bounds as the ones used in this work one can potentially obtain provable forward errors invariant subspaces. ", "page_idx": 25}, {"type": "text", "text": "While there exist works that overcome the $\\widetilde O(n^{3})$ worst-case complexity barrier for some computations related to the eigenproblem, none of them provide end-to-end forward errors for individual invariant subspaces. [114] showed that the eigenvalues of a matrix can be computed in $O(n^{\\omega})$ arithmetic operations, however, it becomes $O(n^{\\overline{{\\omega}}+1})$ boolean operations in rational arithmetic (up to some other omitted factors), and therefore slower than standard $\\widetilde O(n^{3})$ floating point eigensolvers. [25] proposed a quantum-inspired method for approximate diagonalization in a backward-error sense, with $O(n^{\\omega\\bar{+}1})$ bit complexity. [15] proved that backward-approximate diagonalization can be solved in ${\\widetilde{O}}(n^{\\omega})$ bit complexity in floating point by using smoothed analysis, improving both the $\\widetilde O(n^{3})$ classic algorithms for the Hermitian case and the $O(\\mathrm{poly}(n/\\epsilon))$ algorithm of [7] for the non-Hermitian case. This was extended for the generalized eigenproblem case in [41]. As already mentioned in the introduction, to obtain forward errors for eigenvectors from the backwardapproximate solution, the corresponding bounds that are reported in [15, 41] require the existence of a minimum eigenvalue gap in the original matrix, while the analysis for invariant subspaces was left as an open problem. Our work completes this analysis, and takes a step even further, by showing that (at least based on existing results), explicit diagonalization is redundant for approximating individual invariant subspaces. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "B Proof of Proposition 2.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "PURIFY Input: Hermitian definite pencil $\\mathbf{H}\\in\\mathbb{H}^{n}$ $\\mathbf{S}\\,\\in\\,\\mathbb{H}_{++}^{n}$ , approximate condition number $\\widetilde{\\kappa}$ of $\\mathbf{S}$ approximate values ${\\widetilde{\\mu}}_{k}$ \uff0c $\\widetilde{\\mathrm{gap}_{k}}$ , accuracy $\\epsilon\\in(0,1/12)$ Requires: $\\|\\mathbf{H}\\|\\leq1$ $\\begin{array}{r}{\\|\\mathbf{S}^{-1}\\|\\,\\le\\,1,\\,\\widetilde{\\kappa}\\,\\in\\,\\Theta(\\kappa(\\mathbf{S})),\\,\\widetilde{\\mu}_{k}\\,\\in\\,\\mu_{k}\\,\\pm\\,\\frac{1}{8}\\,\\mathrm{gap}_{k}}\\end{array}$ and $\\widetilde{\\mathrm{gap}}_{k}\\,\\in\\,(1\\pm\\textstyle{\\frac{1}{8}})\\,\\mathrm{gap}_{k}$ , where $\\begin{array}{r}{\\mu_{k}=\\frac{\\lambda_{k}+\\lambda_{k+1}}{2}}\\end{array}$ X+\\*+l and gapk =\u5165 -\u5165+ forsome k E [n- 1]. Algorithm: $\\widetilde{\\mathbf{P}}\\gets\\mathsf{P U R I F Y}(\\mathbf{H},\\mathbf{S},\\widetilde{\\mu}_{k},\\widetilde{\\mathrm{gap}}_{k},\\widetilde{\\kappa},\\epsilon)$ 1: $\\mathbf{S}_{1\\mathrm{NV}}\\gets\\mathsf{I N V}(\\mathbf{S})$ DSInv = S-1 + EINV 2: $\\widetilde{\\mathbf H}\\gets{\\mathsf{M M}}(\\mathbf S_{|\\mathsf{N V}},\\mathbf H)$ H= SINVH+EMM 3: $\\widetilde{\\bf M}\\leftarrow\\widetilde{\\mu}_{k}-\\widetilde{\\bf H}+{\\bf E}_{3}^{(-)}$ 4: C\u2190 SGN (M,161, $\\mathsf{\\Delta}\\>\\widetilde{\\mathbf{C}}=\\mathrm{sgn}(\\widetilde{\\mathbf{M}})+\\mathbf{E}_{4}^{\\mathsf{S G N}}$ 5: $\\widetilde\\Pi\\leftarrow\\textstyle\\frac{1}{2}\\Big(1+\\widetilde{\\mathbf{C}}\\Big)+\\mathbf{E}_{5}^{(+)}$ 6: return II. Output: Approximate spectral projector $\\widetilde{\\bf\\Pi}^{\\prime}$ Ensures: $\\lVert\\widetilde{\\mathbf{I}}-\\mathbf{H}\\rVert\\leq\\epsilon$ ", "page_idx": 26}, {"type": "text", "text": "Algorithm 3: PURIFY. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For the analysis of Algorithm 3 we first recall that, in exact arithmetic, small perturbations do not have a severe influence in the sign function of a matrix. ", "page_idx": 26}, {"type": "text", "text": "Lemma B.1. Let $\\mathbf{H}\\,\\in\\,\\mathbb{H}^{n}$ \uff0c $\\textbf{S}\\in\\mathbb{H}_{++}^{n}$ , where $\\mu$ is a scalar, and $\\mathbf{M}\\,=\\,\\mu\\,-\\,\\mathbf{S}^{-1}\\mathbf{H}$ and $\\widetilde{\\textbf{M}}=$ \u03bc- S-H + E. I E \u2264 e for some $\\epsilon\\,\\in\\,(0,1)$ where $|\\lambda_{\\mathrm{min}}({\\bf M})|$ is the smallest eigenvalue of M in absolute value, then $\\begin{array}{r}{\\left\\|\\mathbf{\\sigma}\\mathrm{sgn}(\\mathbf{M})-\\mathrm{sgn}(\\widetilde{\\mathbf{M}})\\right\\|\\leq\\epsilon.}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "Proof. The proof uses standard techniques from holomorphic functional calculus and the properties of the pseudospectrum, but it is stated for completeness. A similar proof, for example, can be found in [9] for general matrices A (i.e. in that proof A can have complex eigenvalues). ", "page_idx": 26}, {"type": "text", "text": "Recall that for a matrix A that has no eigenvalues on the imaginary axis, it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname{sgn}(\\mathbf{A})=\\mathbf{P}_{+}(\\mathbf{A})-\\mathbf{P}_{-}(\\mathbf{A}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mathbf{P}_{-}(\\mathbf{A})$ is the spectral projector on the subspace spanned by the eigenvectors corresponding to eigenvalues with negative real part, and $\\mathbf{P}_{+}(\\mathbf{A})$ is the corresponding spectral projector to the positive halfplane. Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\operatorname{sgn}(\\mathbf{M})-\\operatorname{sgn}(\\widetilde{\\mathbf{M}})\\|=\\|\\mathbf{P}_{+}(\\mathbf{M})+\\mathbf{P}_{-}(\\mathbf{M})-\\mathbf{P}_{+}(\\mathbf{M}+\\mathbf{E})-\\mathbf{P}_{-}(\\mathbf{M}+\\mathbf{E})\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\mathbf{P}_{+}(\\mathbf{M})-\\mathbf{P}_{+}(\\mathbf{M}+\\mathbf{E})\\|+\\|\\mathbf{P}_{-}(\\mathbf{M})-\\mathbf{P}_{-}(\\mathbf{M}+\\mathbf{E})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\mathbf{S}^{-1}\\mathbf{H}$ has real eigenvalues because it is similar to a Hermitian matrix, i.e. $\\Lambda(\\mathbf{S}^{-1}\\mathbf{H})=\\Lambda(\\mathbf{L}^{*}\\mathbf{H}\\mathbf{L})$ where $\\mathbf{L}$ is the lower triangular Cholesky factor of $\\mathbf{S}^{-1}$ . Thus $\\mathbf{M}$ has also real eigenvalues. From Proposition A.1 (i) and (ii) ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Lambda_{\\|\\mathbf{E}\\|}(\\widetilde{\\mathbf{M}})\\subseteq\\Lambda_{2\\|\\mathbf{E}\\|}(\\mathbf{M})\\subseteq\\bigcup_{i}D\\biggl(\\lambda_{i}(\\mathbf{M}),2\\|\\mathbf{E}\\|\\kappa_{\\mathbf{V}}(\\mathbf{M})\\biggr).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "If El \u2264 M for some $\\epsilon_{1}\\,\\in\\,(0,1)$ then $|\\lambda_{\\operatorname*{min}}(\\widetilde{\\mathbf{M}})|\\ge|\\lambda_{\\operatorname*{min}}(\\mathbf{M})|/2$ and $|\\lambda_{\\operatorname*{max}}(\\widetilde{\\mathbf{M}})|\\leq$ $\\vert\\lambda_{\\mathrm{max}}(\\mathbf{M})\\vert+\\vert\\lambda_{\\mathrm{min}}(\\mathbf{M})\\vert/2$ ", "page_idx": 27}, {"type": "text", "text": "Let $R$ be a rectangle whose bottom-left corner is located at $-i|\\lambda_{\\operatorname*{min}}(\\mathbf{M})|$ and its top-right corner at $2+|\\lambda_{\\operatorname*{min}}(\\mathbf{M})|+i|\\lambda_{\\operatorname*{min}}(\\mathbf{M})|$ . Any point on the boundary of $R$ has a distance of at least $|\\lambda_{\\mathrm{min}}({\\bf M})|$ from $\\Lambda(\\mathbf{M})$ and at least at least $|\\lambda_{\\operatorname*{min}}(\\mathbf{M})|/2$ from $\\Lambda(\\widetilde{\\bf M})$ . From holomorphic functional calculus we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\bf P}_{+}({\\bf A})=\\oint_{\\partial R}(z-{\\bf M})^{-1}d z,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where in this case $\\partial R$ denotes a positively oriented rectifiable curve over the boundary of the rectangle. Using the resolvent identity $(z-\\overset{.}{\\mathbf{M}})^{-1}-(z-\\mathbf{M}^{\\prime})^{-1}=(z-\\mathbf{M})^{-1}(\\mathbf{M}-\\mathbf{M}^{\\prime})(z-\\mathbf{M}^{\\prime})^{-1}$ for $\\mathbf{P}_{+}$ wehave ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{P}_{+}(\\mathbf{M})-\\mathbf{P}_{+}(\\mathbf{M}+\\mathbf{E})\\|=\\left\\|\\frac{1}{2\\pi i}\\int_{\\rho_{0}}(z-\\mathbf{M})^{-1}d z-\\frac{1}{2\\pi i}\\int_{\\rho_{0}}^{t}(z-\\mathbf{M}+\\mathbf{E})^{-1}d z\\right\\|}\\\\ {=\\left\\|\\frac{1}{2\\pi i}\\int_{\\rho_{0}}(z-\\mathbf{M})^{-1}-(z-\\mathbf{M}+\\mathbf{E})^{-1}d z\\right\\|}\\\\ {=\\left\\|\\frac{1}{2\\pi i}\\int_{\\rho_{0}}(z-\\mathbf{M})^{-1}(z-\\mathbf{M}-(z-\\mathbf{M}+\\mathbf{E}))(z-\\mathbf{M}+\\mathbf{E})^{-1}d z\\right\\|}\\\\ {=\\left\\|\\frac{1}{2\\pi i}\\int_{\\rho_{0}}(z-\\mathbf{M})^{-1}\\mathbf{E}(z-\\mathbf{M}+\\mathbf{E})^{-1}d z\\right\\|}\\\\ {\\leq\\frac{1}{2\\pi i}\\int_{\\rho_{0}}(z-\\mathbf{M})^{-1}\\mathbf{E}(z-\\mathbf{M}+\\mathbf{E})^{-1}\\|d z\\right\\|}\\\\ {\\leq\\frac{1}{2\\pi i}\\int_{\\rho_{0}}(\\|z-\\mathbf{M})^{-1}\\mathbf{E}(z-\\mathbf{M}+\\mathbf{E})^{-1}\\|d z}\\\\ {\\leq\\frac{1}{2\\pi i}\\ln(R)\\cdot\\left\\|\\mathbf{E}\\right\\|_{z}\\cdot\\mathbf{M}\\cdot\\mathbf{u}\\right\\|(z-\\mathbf{M})^{-1}\\|\\leq\\frac{1}{\\varepsilon\\eta}\\ln\\left\\|(z-\\mathbf{M}+\\mathbf{E})^{-1}\\right\\|}\\\\ {\\leq\\frac{1}{2\\pi i}\\langle\\phi|_{\\lambda+\\eta}(\\mathbf{M})\\rangle\\|+4\\cdot\\hat{\\mathbf{e}}_{(\\mathbf{M})}(\\mathbf{M})\\rangle\\cdot\\frac{2}{\\left\\|\\lambda\\mathbf{m}(\\mathbf{M})\\right\\|}\\cdot\\frac{2}{\\left\\|\\lambda\\mathbf{m}(\\mathbf{M})\\right\\|}}\\\\ {\\leq c_{\\operatorname*{max}}\\|\\mathbf{M}\\|_{\\lambda+(\\mathbf{M})}(\\mathbf{M})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $|\\lambda_{\\operatorname*{min}}(\\mathbf{M})|\\;\\leq\\;2$ . then if we set $\\epsilon_{1}\\;\\le\\;\\epsilon\\frac{|\\lambda_{\\operatorname*{min}}(\\mathbf{M})|\\pi}{16}$ (which satisfies $\\epsilon_{1}\\,<\\,1\\$ where $\\epsilon$ is the desired final accuracy, this gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\mathbf{P}_{+}(\\mathbf{M})-\\mathbf{P}_{+}(\\mathbf{M}+\\mathbf{E})\\|\\le\\epsilon/2.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The same can be obtained for $\\mathbf{P}_{-}$ .Putting everything together, if $\\|\\mathbf{E}\\|\\,\\leq\\,\\epsilon\\frac{|\\lambda_{\\operatorname*{min}}(\\mathbf{M})|^{2}\\pi}{128\\kappa\\mathbf{v}(\\mathbf{M})}$ for some $\\epsilon\\,\\in\\,(0,1)$ ensures that $\\|\\mathrm{sgn}(\\mathbf{M}+\\mathbf{E})-\\mathrm{sgn}(\\mathbf{M})\\|\\leq\\epsilon$ . From Proposition $\\begin{array}{r}{\\mathrm{A}.2\\;\\kappa_{\\mathbf{V}}(\\mathbf{M})\\leq\\;\\sqrt{\\kappa(\\mathbf{S})},}\\end{array}$ which gives the final bound. ", "page_idx": 27}, {"type": "text", "text": "We can now prove Proposition 2.1, which we restate for readability. ", "page_idx": 27}, {"type": "text", "text": "Proposition B.1 (Restatement of Proposition 2.1). Let $\\mathbf{H}\\,\\in\\,\\mathbb{H}^{n}$ with $\\|\\mathbf{H}\\|\\leq1$ \uff0c $\\mathbf{S}\\,\\in\\,\\mathbb{H}_{++}^{n}$ with $\\lVert\\mathbf{S}^{-1}\\rVert\\,\\leq\\,1$ \uff0c $k\\;\\in\\;[n\\mathrm{~-~}1]$ and $\\epsilon\\,\\in\\,(0,1)$ . Let $\\begin{array}{r}{\\mu_{k}\\ =\\ \\frac{\\lambda_{k}+\\lambda_{k+1}}{2}}\\end{array}$ +k+ and gapk =-\u5165k+1, where $\\lambda_{1}\\leq...\\leq\\lambda_{n}$ are the generalized eigenvalues of the Hermitian definite pencil $(\\mathbf{H},\\mathbf{S})$ and assume that we want to compute $\\Pi_{k}$ which is the true spectral projector associated with the $k$ smallest eigenvalues. If we have access to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\mu}_{k}\\in\\mu_{k}\\pm\\frac{1}{8}\\operatorname{gap}_{k}\\quad\\widetilde{\\mathrm{gap}}_{k}\\in(1\\pm\\frac{1}{8})\\operatorname{gap}_{k},\\quad\\widetilde{\\kappa}\\in[\\kappa(\\mathbf{S}),C\\kappa(\\mathbf{S})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for some constant $C>1$ then Algorithm 3 computes $\\widetilde{\\Pi}_{k}\\,\\gets\\,\\mathsf{P U R I F Y}(\\mathbf{H},\\mathbf{S},\\widetilde{\\mu}_{k},\\widetilde{\\mathrm{gap}}_{k},\\widetilde{\\kappa},\\epsilon)$ such that $\\|\\widetilde{\\boldsymbol{\\Pi}}_{k}\\,-\\,\\boldsymbol{\\Pi}_{k}\\|\\,\\leq\\,\\epsilon$ $\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\left(\\log(\\frac{1}{\\sf g a p}_{k})+\\log(\\log(\\frac{\\kappa(\\bf S)}{\\epsilon\\operatorname{gap}_{k}}))\\right)\\right)}\\end{array}$ foating point perations using $O\\left(\\log(n)\\log^{3}({\\frac{1}{\\operatorname{gap}_{k}}})\\log\\!\\left({\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\operatorname{gap}_{k}}}\\right)\\right)$ bits of precision. ", "page_idx": 27}, {"type": "text", "text": "ProofUsing the nnofAlgort, wehav tat $\\widetilde{\\mathbf H}\\,=\\,\\mathbf S^{-1}\\mathbf H+\\mathbf E_{1}^{\\mathsf{I N V}}\\mathbf H+\\mathbf E_{2}^{\\mathsf{M M}}$ .Let us initially set ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{u}=\\epsilon_{0}\\frac{1}{\\mu(n)\\widetilde{\\kappa}^{c_{\\mathsf{I N V}}\\log(n)}}\\leq\\epsilon_{0}\\frac{1}{\\mu(n)\\kappa(\\mathbf{S})^{c_{\\mathsf{I N V}}\\log(n)}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some $\\epsilon_{0}\\in(0,1/8)$ (to be determined later). From Theorems A.3 and A.2, and from the assumption that $\\lVert\\mathbf{H}\\rVert,\\lVert\\mathbf{S}^{-1}\\rVert\\leq1$ we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\mathbf E}_{1}^{\\mathrm{INV}}\\|\\leq\\mathbf{u}\\mu_{\\mathsf{I N V}}(n)\\kappa({\\mathbf S})^{c_{\\mathsf{I N V}}\\log(n)}\\|{\\mathbf S}^{-1}\\|\\leq\\epsilon_{0},}\\\\ &{\\|{\\mathbf E}_{2}^{\\mathrm{MM}}\\|\\leq\\mathbf{u}\\mu_{\\mathsf{M M}}(n)\\|\\mathbf{H}\\|\\|\\mathbf{S}_{\\mathsf{I N V}}\\|\\leq\\mathbf{u}\\mu_{\\mathsf{M M}}(n)\\|\\mathbf{H}\\|\\left(\\|{\\mathbf S}^{-1}\\|+\\|{\\mathbf E}_{1}^{\\mathrm{INV}}\\|\\right)\\leq\\epsilon_{0}(1+\\epsilon_{0})\\leq2\\epsilon_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Wenext have $\\widetilde{\\mathbf{M}}\\leftarrow\\widetilde{\\mu}_{k}-\\widetilde{\\mathbf{H}}+\\mathbf{E}_{3}^{(-)}=\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H}+\\mathbf{E}_{1}^{\\mathrm{{IN}}}\\mathbf{H}+\\mathbf{E}_{2}^{\\mathrm{{MM}}}+\\mathbf{E}_{3}^{(-)}=\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H}+\\mathbf{B},$Wwhere ${\\bf E}_{3}^{(-)}$ is a diagonaleror mari with", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{E}_{3}^{(-)}\\|\\leq\\mathbf{u}\\|\\widetilde{\\mu}_{k}\\|\\|\\widetilde{\\mathbf{H}}\\|\\leq\\mathbf{u}\\|\\mathbf{S}^{-1}\\mathbf{H}+\\mathbf{E}_{1}^{\\mathrm{{IN}}}\\mathbf{H}+\\mathbf{E}_{2}^{\\mathrm{{MM}}}\\|\\leq\\mathbf{u}(1+\\epsilon_{0}+2\\epsilon_{0})\\leq4\\mathbf{u}\\ll4\\epsilon_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ${\\bf{B}}={\\bf{E}}_{1}^{\\sf I N V}{\\bf{H}}+{\\bf{E}}_{2}^{\\sf M M}+{\\bf{E}}_{3}^{(-)}$ .We can bound the norm of $\\mathbf{B}$ as: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\mathbf{B}\\|\\leq7\\epsilon_{0}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now we need to apply Lemma B.1 to argue that $\\begin{array}{r}{\\left\\|\\mathrm{sgn}(\\mu_{k}-\\mathbf{S}^{-1}\\mathbf{H})-\\mathrm{sgn}(\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H}+\\mathbf{B})\\right\\|\\leq}\\end{array}$ $\\epsilon_{1}$ ,forsome $\\epsilon_{1}\\,\\in\\,(0,1)$ . To satisfy the requirements of the lemma, we need to ensure that $\\|\\mathbf{B}\\|\\leq$ [ Xaiua(t-s- H)  This can be achieved by etting ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\epsilon_{0}=\\epsilon_{1}\\frac{\\pi|\\lambda_{\\mathrm{min}}^{2}(\\widetilde{\\mu}_{k}-{\\bf S}^{-1}{\\bf H})|}{7\\cdot128\\sqrt{\\kappa({\\bf S})}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "However, we do not know $|\\lambda_{\\operatorname*{min}}(\\widetilde{\\mu}_{k}\\,-\\,\\mathbf{S}^{-1}\\mathbf{H})|$ and $\\kappa(\\mathbf{S})$ .To circumvent this, we can use the assumption that $\\widetilde{\\mu}_{k}$ is well-placed close to the center of $\\mathrm{gap}_{k}$ , which implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\lambda_{\\mathrm{min}}(\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H})|\\ge\\frac{3}{8}\\,\\mathrm{gap}_{k}\\ge\\frac{1}{3}\\widetilde{\\mathrm{gap}}_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and also use the assumption that $\\kappa(\\mathbf{S})\\leq{\\widetilde{\\kappa}}$ . This means that we can set ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\epsilon_{0}=\\epsilon_{1}\\frac{\\pi}{7\\cdot128\\sqrt{\\widetilde{\\kappa}}}\\cdot\\frac{1}{9}\\widetilde{\\mathrm{gap}}_{k}^{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies the desired bound for $\\mathbf{B}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\mathbf{B}\\|\\leq7\\cdot\\epsilon_{1}\\frac{\\pi}{7\\cdot128\\sqrt{\\kappa}}\\cdot\\frac{1}{9}\\widetilde{\\mathrm{gap}}_{k}^{2}\\leq\\epsilon_{1}\\frac{\\pi|\\lambda_{\\operatorname*{min}}^{2}(\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H})|}{128\\sqrt{\\kappa(\\mathbf{S})}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can now apply Lemma B.1 to argue that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\operatorname{sgn}({\\widetilde{\\mu}}_{k}-\\mathbf{S}^{-1}\\mathbf{H})-\\operatorname{sgn}({\\widetilde{\\mu}}_{k}-\\mathbf{S}^{-1}\\mathbf{H}+\\mathbf{B})\\right\\|\\leq\\epsilon_{1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By assumption we know that $\\begin{array}{r}{\\widetilde{\\mu}_{k}=\\mu_{k}\\pm\\frac{1}{8}\\,\\mathrm{gap}_{k}}\\end{array}$ which means that $\\operatorname{sgn}(\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H})=\\operatorname{sgn}(\\mu_{k}-$ $\\mathbf{S}^{-1}\\mathbf{H})$ , and therefore ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\operatorname{sgn}(\\mu_{k}-\\mathbf{S}^{-1}\\mathbf{H})-\\operatorname{sgn}(\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H}+\\mathbf{B})\\right\\|\\leq\\epsilon_{1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To not interrupt the flow, we will skip for now the analysis of SGN and we will leave it for the end of the proof. From Theorem A.4 we know that the result returned by SGN satisfies $\\|\\widetilde{\\mathbf{C}}-\\mathrm{sgn}(\\widetilde{\\mathbf{M}})\\|\\leq$ $\\epsilon_{S\\sf G N}$ , where we have full control over $\\epsilon_{S\\sf G N}$ since it is passed as an argument to SGN, and therefore it can be set to $\\epsilon_{\\mathsf{S G N}}=\\epsilon_{1}$ . Denoting $\\mathbf{C}=\\operatorname{sgn}(\\mu_{k}-\\mathbf{S}^{-1}\\mathbf{H})$ and $\\mathbf{E}_{\\mathbf{C}}=\\tilde{\\mathbf{C}}-\\mathbf{C}$ wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\mathbf{C}}-\\operatorname{sgn}(\\mu_{k}-\\mathbf{S}^{-1}\\mathbf{H})\\right\\|=\\|\\mathbf{E}_{\\mathbf{C}}\\|\\leq\\epsilon_{1}+\\epsilon_{5\\mathsf{G N}}=2\\epsilon_{1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We now proceed to the line 5 of Algorithm 3, which computes $\\widetilde\\mathbf{I}\\,\\leftarrow\\,\\frac{1}{2}\\left(1+\\widetilde{\\mathbf{C}}\\right)+\\mathbf{E}_{5}^{(+)}$ where ${\\bf E}_{5}^{(+)}$ is a diagonal eror matrix with norm bounded by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathbf{E}_{5}^{(+)}\\|\\leq\\mathbf{u}\\|\\widetilde{\\mathbf{C}}\\|\\leq\\mathbf{u}(1+\\epsilon_{1}+\\epsilon_{5}\\mathsf{G}\\mathsf{N})\\leq3\\mathbf{u}\\ll3\\epsilon_{1}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then we can write $\\tilde{\\Pi}\\;=\\;\\textstyle{\\frac{1}{2}}\\left(1+\\mathbf{C}+\\mathbf{E}_{\\mathbf{C}}\\right)+\\mathbf{E}_{5}^{(+)}\\,=\\,\\Pi+\\textstyle{\\frac{1}{2}}\\mathbf{E}_{\\mathbf{C}}\\,+\\,\\mathbf{E}_{5}^{(+)}\\,=\\,\\Pi+\\mathbf{E}_{\\Pi},$ where $\\textstyle\\Pi={\\frac{1}{2}}(1+\\mathbf{C})$ and $\\begin{array}{r}{\\mathbf{E}_{\\mathbf{II}}=\\frac{1}{2}\\mathbf{E}_{\\mathbf{C}}+\\mathbf{E}_{5}^{(+)}}\\end{array}$ . Combining with the above: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathbf{E}_{\\mathbf{H}}\\|\\leq4\\epsilon_{1},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "in which case we can set $\\epsilon_{1}\\,=\\,\\frac{\\epsilon}{4}$ where $\\epsilon$ is the desired accuracy, to guarantee forward error of at most $\\epsilon$ for the spectral projector II. ", "page_idx": 29}, {"type": "text", "text": "Gathering all the requirements for the machine precision (except for SGN, which is detailed below) from Equations (6) and (7), and the assumption on $\\widetilde{\\kappa}$ and $\\widetilde{\\mathrm{gap}_{k}}$ it suffices to set ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq\\frac{\\epsilon}{4}\\frac{\\pi}{7\\cdot128\\sqrt{\\kappa}}\\cdot\\frac{1}{9}\\widetilde{\\mathrm{gap}}_{k}^{2}\\frac{1}{\\mu(n)\\widetilde{\\kappa}^{c_{\\mathsf{I N V}}\\log(n)}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which translates to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\log(1/\\mathfrak{u})\\right)=O\\left(\\log\\left(\\frac{\\sqrt{\\kappa}\\mu(n)\\widetilde{\\kappa}^{c_{|\\mathrm{W}}\\log(n)}}{\\epsilon\\widetilde{\\mathtt{g}}\\mathsf{a p}_{k}^{2}}\\right)\\right)=O\\left(\\log(n)\\log(\\kappa(\\mathbf{S}))+\\log\\left(\\frac{n}{\\epsilon\\operatorname{g}\\mathsf{a p}_{k}}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "bits of precision. ", "page_idx": 29}, {"type": "text", "text": "We finally proceed with the analysis of SGN. To use Theorem A.4 for the convergence of SGN, we need to find the appropriate parameters $\\alpha_{\\mathsf{S G N}},\\eta_{\\mathsf{S G N}}$ to call it. These parameters must be such that $\\Lambda_{\\eta_{\\mathsf{S G N}}}(\\widetilde{\\mathbf{M}})\\subseteq\\mathsf{C}_{\\alpha_{\\mathsf{S G N}}}$ ,and $\\begin{array}{r}{{\\frac{99}{100}}<\\alpha_{56\\mathsf{N}}<1}\\end{array}$ . From the properties of the pseudospectrum, specifically, from Proposition A.1 $(i)$ , we know that for any $\\eta>0$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Lambda_{\\eta}(\\widetilde{\\mathbf{M}})=\\Lambda_{\\eta}(\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H}+\\mathbf{B})\\subseteq\\Lambda_{\\eta+\\|\\mathbf{B}\\|}(\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H})\\subseteq\\Lambda_{\\eta+B_{\\operatorname*{max}}}(\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $B_{\\mathrm{max}}$ is the upper bound for $\\|\\mathbf B\\|$ that we obtained above: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathbf{B}\\|\\leq\\epsilon_{1}{\\frac{\\pi\\widetilde{\\mathrm{gap}}_{k}^{2}}{9\\cdot128\\sqrt{\\widetilde{\\kappa}}}}:=B_{\\mathrm{max}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For $\\eta_{\\mathsf{S G N}}=B_{\\mathrm{max}}$ and from Proposition A.1 $(i i i)$ we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\boldsymbol{\\Lambda}_{B_{\\mathrm{max}}}(\\widetilde{\\mathbf{M}})\\subseteq\\boldsymbol{\\Lambda}_{2B_{\\mathrm{max}}}(\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H})}&{}\\\\ {\\quad}&{\\subseteq\\bigcup_{i}D\\left(\\widetilde{\\mu}_{k}-\\boldsymbol{\\lambda}_{i}(\\mathbf{S}^{-1}\\mathbf{H}),2B_{\\mathrm{max}}\\kappa\\mathbf{v}(\\widetilde{\\mu}_{k}-\\mathbf{S}^{-1}\\mathbf{H})\\right)}\\\\ {\\quad}&{\\subseteq\\bigcup_{i}D\\left(\\widetilde{\\mu}_{k}-\\boldsymbol{\\lambda}_{i}(\\mathbf{S}^{-1}\\mathbf{H}),2\\epsilon_{1}\\frac{\\pi\\widetilde{\\mathrm{gap}}_{k}^{2}}{9\\cdot128\\sqrt{\\widetilde{\\kappa}}}\\kappa\\mathbf{v}(\\mathbf{S}^{-1}\\mathbf{H})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we used the fact that $\\kappa_{\\mathbf{V}}(\\mathbf{S}^{-1}\\mathbf{H})\\leq\\sqrt{\\kappa(\\mathbf{S})}$ We can get a rough upper bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2\\epsilon_{1}\\frac{\\pi\\widetilde{\\mathrm{gap}}_{k}^{2}}{9\\cdot128\\sqrt{\\kappa}}\\kappa_{\\mathbf{V}}(\\mathbf{S}^{-1}\\mathbf{H})\\leq\\epsilon_{1}\\frac{2\\pi\\cdot9^{2}}{9\\cdot128\\cdot8^{2}\\sqrt{\\kappa(\\mathbf{S})}}\\sqrt{\\kappa(\\mathbf{S})}\\,\\mathrm{gap}_{k}^{2}\\leq\\epsilon_{1}\\frac{\\mathrm{gap}_{k}^{2}}{128},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which gives ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Lambda_{B_{\\mathrm{max}}}(\\widetilde{\\mathbf{M}})\\subseteq\\bigcup_{i}D\\left(\\widetilde{\\mu}_{k}-\\lambda_{i}(\\mathbf{S}^{-1}\\mathbf{H}),\\epsilon_{1}\\frac{\\mathrm{gap}_{k}^{2}}{128}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "It suffices to find the appropriate $\\alpha_{\\mathsf{S G N}}$ such that the Apollonian circles $\\mathsf{C}_{\\alpha_{\\mathsf{S G N}}}$ will contain the disks of the pseudospectrum. Since the smallest singular value of $\\widetilde{\\mu}_{k}\\mathrm{~-~}\\mathbf{S}^{-1}\\mathbf{H}$ is lower bounded by $\\textstyle{\\frac{3}{8}}\\operatorname{gap}_{k}$ , the leftmost positive point where any pseudospectral disk intersects with the real axis is \u2265 gapk( s)  gaph 2 > P. Similarly forthe negative alfplane, the rightmost negative point where a disk intersects with the real axis is $-\\,{\\frac{\\widetilde{\\mathrm{gap}}_{k}}{4}}$ . Denoting by $\\begin{array}{r}{\\zeta=\\frac{\\widetilde{\\mathrm{gap}}_{k}}{8}}\\end{array}$ it suficesto set asGN = =P Clearly, $\\alpha_{\\mathsf{S G N}}<1$ for all $\\infty>\\mathrm{gap}_{k}>0$ For the lower bound, to ensure \u03b1sGN > 99/100 we need that gapk < : . To achieve this we can simply scale the input matrices by a constant, since, by assumption, $\\|\\mathbf{S}^{-1}\\mathbf{H}\\|\\leq1$ , and therefore $\\mathrm{gap}_{k}\\le2$ To conclude, we have all the required parameters to call SGN. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "We can now go back to the complexity analysis. We are running ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{C}}\\gets\\mathsf{S G N}\\left(\\widetilde{\\mathbf{M}},\\alpha_{\\mathsf{S G N}},\\eta_{\\mathsf{S G N}},\\epsilon_{\\mathsf{S G N}}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\alpha_{\\mathsf{S G N}}=\\frac{8-\\widetilde{\\mathrm{gap}}_{k}}{8+\\widetilde{\\mathrm{gap}}_{k}},\\qquad\\eta_{\\mathsf{S G N}}=B_{\\mathrm{max}}=\\epsilon_{1}\\frac{\\pi\\widetilde{\\mathrm{gap}}_{k}^{2}}{9\\cdot128\\sqrt{\\kappa}},\\qquad\\epsilon_{\\mathsf{S G N}}=\\epsilon_{1}=\\frac{\\epsilon}{4}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now invoke Theorem A.4. The number of iterations is bounded by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N=O\\left(\\log(\\frac{1}{1-\\alpha_{\\mathsf{S G N}}})+\\log(\\log(\\frac{1}{\\eta_{\\mathsf{S G N}}\\epsilon_{\\mathsf{S G N}}}))\\right)}\\\\ &{\\quad=O\\left(\\log(\\frac{1}{\\widetilde{\\mathrm{gap}}_{k}})+\\log(\\log(\\frac{\\widetilde{\\kappa}}{\\epsilon\\widetilde{\\mathrm{gap}}_{k}}))\\right)}\\\\ &{\\quad=O\\left(\\log(\\frac{1}{\\mathrm{gap}_{k}})+\\log(\\log(\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}}))\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The number of required bits is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{O\\left(\\log(n)\\log^{3}(\\frac{1}{1-\\alpha_{\\mathsf{S G N}}})\\log(\\frac{1}{\\eta_{\\mathsf{S G N}}\\epsilon_{\\mathsf{S G N}}})\\right)=O\\left(\\log(n)\\log^{3}(\\frac{1}{\\frac{\\exp}{\\mathrm{gap}_{k}}})\\log\\left(\\frac{\\widetilde{\\kappa}}{\\epsilon\\widetilde{\\mathtt{g a p}}_{k}}\\right)\\right)}&{}\\\\ {=O\\left(\\log(n)\\log^{3}(\\frac{1}{\\mathrm{gap}_{k}})\\log\\left(\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which dominates the previous bit bound of Equation 8. ", "page_idx": 30}, {"type": "text", "text": "It remains to bound the arithmetic complexity. There is a constant number of inversions, matrix multiplications, and scalar-matrix operations, which in total take at most $O(T_{\\mathsf{M M}}(n))$ floatingpoint operations. Thus, the dominant factor is the call to SGN, which amounts to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\left(\\log(\\frac{1}{\\sf g a p}_{k})+\\log(\\log(\\frac{\\kappa(\\bf S)}{\\epsilon\\operatorname{gap}_{k}}))\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "arithetic operations using the aforementioned number of bits. ", "page_idx": 30}, {"type": "text", "text": "B.1 Spectral gaps with diagonalization ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The straightforward approach to compute the desired spectral gaps is to iteratively compute $\\mathbf{S}^{-1}\\mathbf{H}$ and diagonalize it, until the eigenvalues are well approximated. If we compute $\\mathbf{A}\\gets$ $\\mathsf{M M}(\\mathsf{I N V}(\\mathbf{S}),\\mathbf{H})$ , then we can write $\\bar{\\mathbf{A}}\\,=\\,\\mathbf{S}^{-1}\\mathbf{H}+\\mathbf{E}.$ for some error matrix $\\mathbf{E}$ .The next step is to approximate $\\mathrm{gap}_{k}(\\mathbf{S}^{-1}\\mathbf{H})$ and $\\mu_{k}(\\mathbf{S}^{-1}\\mathbf{H})$ . For this one could use the recent state-of-the-art backward-approximate diagonalization algorithm of [15]. ", "page_idx": 30}, {"type": "text", "text": "Theorem B.1 (ElG, imported Theorem 1.6 from [15]). There exists a randomized algorithm $\\mathsf{E l G}(\\mathbf{A},\\epsilon_{\\mathsf{E l G}})$ which takes any matrix $\\mathbf{A}\\,\\in\\,\\mathbb{C}^{n\\times n}$ with $\\|\\mathbf{A}\\|\\leq1$ and a desired accuracy parameter $\\epsilon_{\\mathsf{E I G}}>0$ as inputsandreturnsadiagonal $\\mathbf{D}$ and an invertible matrix $\\mathbf{V}$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\mathbf{A}-\\mathbf{VDV}^{-1}\\|\\leq\\epsilon_{\\mathsf{E}\\mid\\mathsf{G}}\\quad a n d\\quad\\kappa(\\mathbf{V})\\leq32n^{2.5}/\\epsilon_{\\mathsf{E}\\mid\\mathsf{G}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "in ", "page_idx": 30}, {"type": "equation", "text": "$$\nO\\left(T_{\\sf M M}(n)\\log^{2}(\\frac{n}{\\epsilon_{\\sf E I G}})\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "arithmeticoperations onafloatingpointmachinewith ", "page_idx": 30}, {"type": "equation", "text": "$$\nO(\\log^{4}(n/\\epsilon_{\\mathsf{E I G}})\\log(n))\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "bits of precision, with probability at least $1-14/n$ ", "page_idx": 30}, {"type": "text", "text": "Applying ElG on A we can obtain a backward-approximate diagonalization. But we are not finished yet, since we are interested in each individual eigenvalue. To translate the backward error to a forward error for the eigenvalues, and, ultimately, the spectral gap, one can try to use Corollary 1.7 and Proposition 1.1 of [15]. However, this approach has two main limitations. First, it relies on simplicity of the spectrum, i.e., it assumes that the minimum gap between any pair of eigenvalues is larger than zero. This assumption is quite restrictive, since the desired gap might be well-defined even at the presence of other multiple eigenvalues. For example, in DFT applications it is not uncommon to have eigenenergies with algebraic multiplicity larger than one, and at the same time have a large band-gap that separates the occupied from the unoccupied orbitals. The second limitation is that the aforementioned Corollary 1.7 requires as an input parameter an actual over-estimate for the minimum eigenvalue gap. Even if such a gap exists, it is not described how to estimate it. ", "page_idx": 31}, {"type": "text", "text": "For diagonalizable matrices, we can leverage the following Corollary B.1, which is an immediate consequence of Kahan's inequality (Fact A.1), and it overcomes the aforementioned limitations. ", "page_idx": 31}, {"type": "text", "text": "Corollary B.1. If $\\mathbf{X}$ is diagonalizable and it has real eigenvalues then for any $\\mathbf{Z}$ the following bound holds for the eigenvalues of $\\mathbf{X}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n|\\lambda_{i}(\\mathbf{X})-\\lambda_{i}(\\mathbf{Z})|\\leq O(\\log(n))\\kappa_{\\mathbf{V}}(\\mathbf{X})\\|\\mathbf{X}-\\mathbf{Z}\\|.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Write $\\mathbf{X}=\\mathbf{W}\\mathbf{\\Lambda}\\mathbf{W}^{-1}$ where $\\mathbf{W}$ diagonalizes $\\mathbf{X}$ and is chosen such that $\\kappa(\\mathbf{W})=\\kappa_{\\mathbf{V}}(\\mathbf{X})$ This is always possible since if $\\mathbf{W}$ is any matrix that diagonalizes $\\mathbf{X}$ then $\\Lambda$ is similar to $\\mathbf{X}$ and since $\\mathbf{X}$ has real eigenvalues then $\\Lambda$ has to be real, and therefore symmetric. Then we can write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{\\Lambda}-\\mathbf{W}^{-1}\\mathbf{Z}\\mathbf{W}\\|\\le\\|\\mathbf{W}\\|\\|\\mathbf{W}^{-1}\\|\\|\\mathbf{X}-\\mathbf{Z}\\|=\\kappa(\\mathbf{W})\\|\\mathbf{X}-\\mathbf{Z}\\|.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $\\pmb{\\Lambda}$ is symmetric, and since $\\mathbf{W}^{-1}\\mathbf{Z}\\mathbf{W}$ is similar to $\\mathbf{Z}$ then from Kahan's inequality (Fact A.1) ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\lambda_{i}(\\mathbf{X})-\\lambda_{i}(\\mathbf{Z})|=\\left|\\lambda_{i}(\\mathbf{A})-\\lambda_{i}(\\mathbf{W}^{-1}\\mathbf{Z}\\mathbf{W})\\right|\\leq O(\\log(n))\\|\\mathbf{A}-\\mathbf{W}^{-1}\\mathbf{Z}\\mathbf{W}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq O(\\log(n))\\kappa(\\mathbf{W})\\|\\mathbf{X}-\\mathbf{Z}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=O(\\log(n))\\kappa\\mathbf{v}(\\mathbf{X})\\|\\mathbf{X}-\\mathbf{Z}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now we need to use this to get a bound for the computed generalized eigenvalues after INV and EIG. ", "page_idx": 31}, {"type": "text", "text": "Proposition B.2 (EIG-based gap). Given a definite pencil $(\\mathbf{H},\\mathbf{S})$ with $\\lVert\\mathbf{H}\\rVert,\\lVert\\mathbf{S}^{-1}\\rVert\\,\\leq\\,1,$ wecan compute $\\widetilde{\\mathrm{gap}}_{k}\\in(1\\pm\\epsilon)\\,\\mathrm{gap}_{k}$ and $\\widetilde{\\mu}_{k}\\in\\mu_{k}\\pm\\epsilon\\,\\mathrm{gap}_{k}$ by iteratively calling INV, MM, and EIG, using ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\log(\\frac{1}{\\epsilon\\operatorname{gap}_{k}})\\log^{2}(\\frac{n\\kappa(\\mathbf{S})}{\\epsilon\\operatorname{gap}_{k}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "floating point operations using ", "page_idx": 31}, {"type": "equation", "text": "$$\nO\\left(\\log^{4}(\\frac{n\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\log(n)\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "bits of precision with probability at least 1 - O(lg(1/gap) ", "page_idx": 31}, {"type": "text", "text": "Proof. Let $\\mathbf{D},\\mathbf{V}\\gets\\mathsf{E I G}(\\mathbf{A},\\epsilon_{\\mathsf{E I G}})$ be the solution returned by ElG when applied to ${\\bf A}={\\bf S}^{-1}{\\bf H}\\!+{\\bf E}$ Note that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cdot-\\mathbf{VDV}^{-1}\\|=\\|\\mathbf{S}^{-1}\\mathbf{H}+\\mathbf{E}-\\mathbf{E}-\\mathbf{VDV}^{-1}\\|\\leq\\|\\mathbf{S}^{-1}\\mathbf{H}+\\mathbf{E}-\\mathbf{VDV}^{-1}\\|+\\|\\mathbf{E}\\|\\leq\\epsilon_{\\mathsf{E}16}+\\|\\mathbf{E}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From Corollary B.1, since $\\mathbf{S}^{-1}\\mathbf{H}$ is diagonalizable with real eigenvalues then we conclude that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{i}(\\mathbf{S}^{-1}\\mathbf{H})-\\mathbf{D}_{i,i}|=\\left|\\lambda_{i}(\\mathbf{S}^{-1}\\mathbf{H})-\\lambda_{i}(\\mathbf{VDV}^{-1})\\right|\\leq O(\\log(n))\\kappa\\mathbf{v}(\\mathbf{S}^{-1}\\mathbf{H})\\|\\mathbf{S}^{-1}\\mathbf{H}-\\mathbf{VDV}^{-1}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq O(\\log(n))\\kappa\\mathbf{v}(\\mathbf{S}^{-1}\\mathbf{H})\\left(\\epsilon\\mathsf{E}\\mathsf{l c}+\\|\\mathbf{E}\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We can tune the machine precision such that $\\begin{array}{r}{\\|\\mathbf{E}\\|=\\epsilon_{\\mathsf{E}1\\mathsf{G}}=\\epsilon^{\\prime}\\frac{1}{c\\log(n)\\sqrt{\\kappa(\\mathbf{S})}}}\\end{array}$ for some chosen $\\epsilon^{\\prime}$ , and some global constant $c$ , then finally ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\lambda_{i}(\\mathbf{S}^{-1}\\mathbf{H})-\\lambda_{i}(\\mathbf{VDV}^{-1})\\right|\\leq\\epsilon^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We can now consider an iterative scheme, where we call EIG on $\\mathbf{S}^{-1}\\mathbf{H}$ , halving $\\epsilon^{\\prime}$ at each step. We need to keep halving $\\epsilon^{\\prime}$ until it reaches $\\epsilon^{\\prime}\\;=\\;\\Theta(\\epsilon\\,\\mathrm{gap}_{k})$ , in which case we have a total of $O(\\log(\\frac{1}{\\epsilon\\operatorname{gap}_{k}}))$ calls t E In the worst casevry all costs ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\cal O}\\left(T_{\\sf M M}(n)\\log^{2}(\\frac{n}{\\epsilon_{\\sf E I G}})\\right)={\\cal O}\\left(T_{\\sf M M}(n)\\log^{2}(\\frac{n\\kappa({\\bf S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "$O\\left(\\log^{4}(\\frac{n\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\log(n)\\right)$ INV to achieve accuracy $\\epsilon^{\\prime}$ are dominated by those required for EIG, and we therefore ignore them). Since we do not know $\\mathrm{gap}_{k}$ , we can set the termination criterion tobe $\\epsilon^{\\prime}\\approx\\Theta(\\widetilde{\\epsilon\\mathrm{gap}_{k}})$ ,where $\\widetilde{\\mathrm{gap}_{k}}$ is the approximate gap that we obtain from ElG. ", "page_idx": 32}, {"type": "text", "text": "Each iteration succeeds with high probability $1\\mathrm{~-~}1/n$ , in which case a union bound gives $1\\,-$ $O(\\log(1/(\\epsilon\\,\\mathrm{gap}_{k}))/n$ , which can potentially be improved to $1\\,-\\,O(1/n)$ without impacting the complexity, but we do not expand further. ", "page_idx": 32}, {"type": "text", "text": "A subtle detail in the analysis above is that we need an estimate for $\\kappa(\\mathbf{S})$ in order to be able to use INV and to set the machine precision. Since $\\kappa(\\mathbf{S})$ is generally unknown, we need to compute it. We will later show in Appendix E how to compute the condition number quickly. ", "page_idx": 32}, {"type": "text", "text": "When $\\mathbf{S}=\\mathbf{I}$ , we can avoid the inversion and the computation of $\\kappa(\\mathbf{S})$ . The arithmetic complexity and the bit requirement are the same by setting $\\kappa(\\mathbf{S})=1$ \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Remark B.1. We highlight that a similar result can be obtained by using the more recent pencil diagonalization algorithm of [41].The authors thelattermention that their algorithmshould be \u201cmore numerically stable\u2019\u201dthan EIG of [15], since it uses an inverse-free iteration internally, and they do provide strong theoretical and experimental evidence for this statement.However, as there is no formal, end-to-end proof for the bit complexity at the time of this writing, we choose to compare against ElG of [15]. Note that, any improvements on the bit requirements for the matrix sign function, directly provide the same improvements for our main Theorems 1.1 and 3.1. ", "page_idx": 32}, {"type": "text", "text": "C  Analysis of Cholesky ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "This section is devoted to the analysis of Algorithm 2 and the proof of Theorem 1.2. Let $\\textbf{M}=$ $\\left(\\stackrel{\\mathbf{A}}{\\mathbf{B}}\\stackrel{\\mathbf{B}^{*}}{\\mathbf{C}}\\right)$ be a Hermitian matrix. It can be factorized in the form ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{M}=\\left(\\mathbf{{B}}\\mathbf{{A}}^{-1}\\mathbf{\\Lambda}\\mathbf{\\Phi}\\mathbf{\\Phi}\\mathbf{I}\\right)\\left(\\mathbf{{A}}\\mathbf{\\Lambda}\\mathbf{\\Phi}_{\\mathbf{S}}\\right)\\left(\\mathbf{{I}}\\begin{array}{c}{\\mathbf{A}^{-1}\\mathbf{B}^{*}}\\\\ {\\mathbf{I}}\\end{array}\\right)=\\mathbf{W}\\mathbf{Y}\\mathbf{W}^{*},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\mathbf{S}=\\mathbf{C}-\\mathbf{B}\\mathbf{A}^{-1}\\mathbf{B}^{*}$ is the Schur complement and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{W}=\\left(\\mathbf{\\Sigma}_{\\mathbf{BA}^{-1}}\\mathbf{\\Sigma}_{\\mathbf{I}}\\right),\\qquad\\mathbf{Y}=\\left(\\mathbf{\\Sigma}_{\\mathbf{\\Sigma}\\mathbf{\\Sigma}}^{\\mathbf{A}}\\mathbf{\\Sigma}_{\\mathbf{S}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proposition C.1. Let $\\mathbf{M}\\in\\mathbb{H}_{++}^{n}$ and consider the partitioning $\\mathbf{M}=\\left(\\mathbf{\\overset{A}{B}}\\quad\\mathbf{\\overset{B}{C}}\\right)$ The following hold ", "page_idx": 32}, {"type": "text", "text": "(i) $\\mathbf{A},\\mathbf{C}$ and the Schur complement $\\mathbf{S}=\\mathbf{C}-\\mathbf{B}\\mathbf{A}^{-1}\\mathbf{B}^{*}$ are all positive definite; (ii)For all $\\mathbf{X}\\in\\{\\mathbf{A},\\mathbf{C},\\mathbf{S}\\}$ we have that $\\|\\mathbf{X}\\|\\leq\\|\\mathbf{M}\\|$ and $\\lVert\\mathbf{X}^{-1}\\rVert\\leq\\lVert\\mathbf{M}^{-1}\\rVert$ (ii) $\\|\\mathbf{B}\\|\\leq\\|\\mathbf{M}\\|/2$ ", "page_idx": 32}, {"type": "text", "text": "Proof. It is easy to see that $\\mathbf{A}$ is positive definite: since $\\textbf{M}\\succ\\ 0$ , then it must also hold that $\\mathrm{~\\bf~A~}\\succ\\mathrm{~\\bf~0~}$ since the quadratic form $\\mathbf{x}^{*}\\mathbf{Ax}$ can be written as $\\mathbf{y}^{*}\\mathbf{M}\\mathbf{y}$ for some vector $\\mathbf{y}$ .For the Schur complement we recall the factorized form of Equation (9). Consider the quadratic form $\\mathbf{x}^{*}(\\mathbf{C}-\\mathbf{BA}^{-1}\\mathbf{B}^{*})\\mathbf{x}$ Let $\\mathbf{y}\\;=\\;\\left(\\begin{array}{c}{{-\\mathbf{A}^{-1}\\mathbf{B}^{*}\\mathbf{x}}}\\\\ {{\\mathbf{x}}}\\end{array}\\right)$ Then $\\mathbf{y}^{*}\\mathbf{M}\\mathbf{y}\\;=\\;\\mathbf{x}^{*}(\\mathbf{C}\\:-\\:\\mathbf{B}\\mathbf{A}^{-1}\\mathbf{B}^{*})\\mathbf{x}.$ which means that every quadratic form for the Schur complement can be written as a quadratic form for the matrix $\\mathbf{M}$ , and therefore they are both positive definite. ", "page_idx": 32}, {"type": "text", "text": "For the spectral norm bounds, since A and $\\mathbf{C}$ are both positive definite then their norms are equal to the largest absolute eigenvalues. From the variational characterizaiton of eigenvalues and the discussion above it is easy to see that $\\|\\mathbf{A}\\|\\ \\leq\\ \\|\\mathbf{M}\\|$ . For the Schur complement, let ${\\bf z}$ be the eigenvector such that $\\|\\mathbf{C}-\\mathbf{BA}^{-1}\\mathbf{B}^{*}\\|=\\mathbf{z}^{*}(\\mathbf{C}-\\mathbf{BA}^{-1}\\mathbf{B}^{*})\\mathbf{z}$ .Then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}^{*}(\\mathbf{C}-\\mathbf{BA}^{-1}\\mathbf{B}^{*})\\mathbf{z}=\\mathbf{z}^{*}\\mathbf{Cz}-\\mathbf{z}^{*}\\mathbf{BA}^{-1}\\mathbf{B}^{*}\\mathbf{z}\\leq\\mathbf{z}^{*}\\mathbf{Cz}\\leq\\|\\mathbf{M}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we used the fact that $\\mathbf{BA}^{-1}\\mathbf{B}^{*}$ is positive semi-definite (since ${\\bf A}^{-1}$ is positive definite) and therefore for all $\\mathbf{x}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{x}^{*}\\mathbf{B}\\mathbf{A}^{-1}\\mathbf{B}^{*}\\mathbf{x}\\geq0,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where equality with zero holds only when $\\mathbf{z}\\in\\ker(\\mathbf{B}^{\\ast})$ ", "page_idx": 33}, {"type": "text", "text": "We finally prove the bound for $\\|\\mathbf B\\|$ . Let u be the top left singular vector of $\\mathbf{B}$ and $\\mathbf{v}$ be the top right singular vector. Specifically, if $\\mathbf{B}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{*}$ is the economy SVD of $\\mathbf{B}$ , then $\\mathbf{u}$ is the first column of $\\mathbf{U}$ and $\\mathbf{v}^{*}$ is the first row of $\\mathbf{V}^{*}$ . Then $\\mathbf{u}^{*}\\mathbf{B}\\mathbf{v}=\\sigma_{\\operatorname*{max}}(\\mathbf{B})\\mathbf{v}^{*}\\mathbf{v}=(\\mathbf{B})\\mathbf{u}^{*}\\mathbf{u}=\\|\\mathbf{B}\\|$ . Consider the vector $\\mathbf{z}={\\binom{\\mathbf{v}}{\\mathbf{u}}}$ . Then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{M}\\right\\|\\geq\\mathbf{z}^{*}\\mathbf{M}\\mathbf{z}=(\\mathbf{v}^{*}\\quad\\mathbf{u}^{*})\\left(\\mathbf{\\underline{{A}}}\\quad\\mathbf{B}^{*}\\right)\\left(\\mathbf{\\underline{{v}}}\\right)=\\mathbf{v}^{*}\\mathbf{A}\\mathbf{v}+2\\mathbf{u}^{*}\\mathbf{B}\\mathbf{v}+\\mathbf{u}^{*}\\mathbf{C}\\mathbf{u}\\geq2\\mathbf{u}^{*}\\mathbf{B}\\mathbf{v}=2\\|\\mathbf{B}\\|,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "whereweused the fact that $\\mathbf{A},\\mathbf{C}$ are  positive-definite.  We  can  then  obtain  bounds  for $\\|\\mathbf{C}^{-1}\\|,\\|\\mathbf{A}^{-1}\\|$ , and $\\lVert\\mathbf{S}^{-1}\\rVert$ , by observing that $\\begin{array}{r}{\\sigma_{\\mathrm{min}}({\\bf X})\\;=\\;\\operatorname*{min}_{\\|{\\bf z}\\|=1}{\\bf z}^{*}{\\bf X}{\\bf z}}\\end{array}$ and lower bound it by $\\sigma_{\\mathrm{min}}(\\bf M)$ using similar argumnents. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Since A and S are Hermitian and positive-definite, we can recursively compute their Cholesky factors. This gives rise to the recursive Algorithm 2. The complexity is as follows. ", "page_idx": 33}, {"type": "text", "text": "Proposition C.2. Algorithm 2 requires $O(T_{\\mathsf{M M}}(n))$ arithmetic operations. ", "page_idx": 33}, {"type": "text", "text": "Proof. Let $T(n)$ be the number of operations executed for a matrix of size $n\\times n$ . Steps 4 and 8 require time $\\dot{T}(n/2)$ . Step 5 requires time $T_{\\sf M M}(n/2)+T_{\\sf I N V}(n/2)$ , and step 6 requires $\\bar{T}_{\\mathsf{M M}}(n/2)$ The Schur complement in Step 7 requires $T_{\\sf M M}(n/2)\\,+\\,T_{\\sf H E R M}(n/2)\\,+\\,(n/2)^{2}\\,=\\,T_{\\sf M M}(n/2)\\,+$ $2(n/2)^{2}$ . This becomes ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T(n)=2T(\\frac{n}{2})+3T_{\\mathrm{MM}}(\\frac{n}{2})+T_{\\mathrm{INV}}(\\frac{n}{2})+2(\\frac{n}{2})^{2}\\leq2T(\\frac{n}{2})+O(T_{\\mathrm{MM}}(\\frac{n}{2}))\\leq O(T_{\\mathrm{MM}}(n)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "C.1 Error analysis ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The goal is to show that CHOLESKY(M) will return a backward-approximate Cholesky factor $\\mathbf{L}$ such that the error ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{L}\\mathbf{L}^{*}-\\mathbf{A}\\|\\leq f(\\mathbf{u},\\|\\mathbf{M}\\|,\\|\\mathbf{M}^{-1}\\|,n),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $f$ is some function, and use this bound to argue about the number of bits that are required to achieve it.Recall that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{M}=\\left(\\mathbf{A}\\begin{array}{c c}{\\mathbf{B}}&{\\mathbf{B}^{*}}\\\\ {\\mathbf{B}}&{\\mathbf{C}}\\end{array}\\right),\\quad\\mathbf{L}=\\left(\\mathbf{L}_{21}\\begin{array}{c c}{\\mathbf{L}_{11}}&\\\\ {\\mathbf{L}_{22}}\\end{array}\\right),\\quad\\mathbf{L}\\mathbf{L}^{*}=\\left(\\mathbf{L}_{11}\\mathbf{L}_{11}^{*}\\begin{array}{c c}{\\mathbf{L}_{11}\\mathbf{L}_{21}^{*}}&\\\\ {\\mathbf{L}_{21}\\mathbf{L}_{11}^{*}}&{\\mathbf{L}_{21}\\mathbf{L}_{21}^{*}+\\mathbf{L}_{22}\\mathbf{L}_{22}^{*}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "It is easy to see that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{L}\\mathbf{L}^{*}-\\mathbf{M}\\|=\\left\\|\\left(\\mathbf{L}_{11}\\mathbf{L}_{11}^{*}\\begin{array}{c c}{\\mathbf{L}_{11}\\mathbf{L}_{21}^{*}}\\\\ {\\mathbf{L}_{21}\\mathbf{L}_{11}^{*}}\\end{array}\\right)-\\left(\\mathbf{A}_{\\mathbf{\\Lambda}}^{\\mathbf{\\Lambda}}\\mathbf{\\Lambda}^{\\mathbf{B}^{*}}\\right)\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\|\\mathbf{L}_{11}\\mathbf{L}_{11}^{*}-\\mathbf{A}\\|+\\|\\mathbf{L}_{21}\\mathbf{L}_{11}^{*}-\\mathbf{B}\\|+\\|\\mathbf{L}_{21}\\mathbf{L}_{21}^{*}+\\mathbf{L}_{22}\\mathbf{L}_{22}^{*}-\\mathbf{C}\\|\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which means that it suffices to bound the individual terms in the sum. The first term is the error of the first recursive call. Following the notation of [39], let us denote $\\mathbf{err}(n)$ thenorm-wise error of the algorithm for size $n$ .Wecan thenwrite ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\mathbf{L}_{11}\\mathbf{L}_{11}^{*}-\\mathbf{A}\\|\\leq\\|\\mathbf{E}_{0}^{\\mathrm{CH}}\\|\\leq\\mathbf{err}(n/2).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To simplify the various inequalities in the proofs we use the global bound $\\mu$ from Remark A.1, where all the terms $\\mu_{\\mathsf{M M}}(n)$ and $\\mu_{\\mathsf{I N V}}(n)$ are bounded by $\\mu(n)$ . Finally, we also define ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\mathcal{E}_{1}(\\mathbf{M},n):=\\mu(n)\\cdot\\kappa(\\mathbf{M}),}\\\\ {\\mathcal{E}_{\\sf I N V}(\\mathbf{M},n):=\\mu(n)\\cdot\\kappa(\\mathbf{M})^{c_{\\sf I N V}\\log(n)}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can now get a first expression for the bounds of the terms in Equation (10). ", "page_idx": 34}, {"type": "text", "text": "Lemma C.1. In Algorithm 2, the blocks $\\mathbf{L}_{11},\\mathbf{L}_{21},\\mathbf{L}_{22}$ of the returned matrix L satisfy: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\|\\mathbf{L}_{11}\\mathbf{L}_{11}^{*}-\\mathbf{A}\\|\\leq\\mathsf{e r r}(n/2),}\\\\ {\\|\\mathbf{L}_{21}\\mathbf{L}_{11}^{*}-\\mathbf{B}\\|\\leq(\\kappa(\\mathbf{M})+\\|\\mathbf{E}_{\\mathbf{BA}i}\\|)\\,\\mathsf{e r r}(n/2)+\\|\\mathbf{E}_{\\mathbf{BA}i}\\|\\|\\mathbf{M}\\|+\\|\\mathbf{E}_{\\mathbf{3}}^{\\mathsf{M M}}\\|\\|\\mathbf{L}_{11}^{*}\\|,}\\\\ {\\|\\mathbf{L}_{21}\\mathbf{L}_{21}^{*}+\\mathbf{L}_{22}\\mathbf{L}_{22}^{*}-\\mathbf{C}\\|\\leq2\\kappa(\\mathbf{M})^{2}\\cdot\\mathsf{e r r}(n/2)+2\\kappa(\\mathbf{M})\\|\\mathbf{E}_{\\mathbf{L}_{21}}\\|\\|\\mathbf{L}_{11}\\|+\\|\\mathbf{E}_{\\mathbf{L}_{21}}\\|^{2}+\\|\\mathbf{E}_{\\mathbf{S}}\\|.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. The error for $\\mathbf{L}_{11}\\mathbf{L}_{11}^{*}$ was already described. For the second inequality, we first expand $\\mathbf{L}_{21}\\mathbf{L}_{11}^{*}$ as follows: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{L}_{21}\\mathbf{L}_{11}^{*}=(\\mathbf{B}\\mathbf{A}^{-1}+\\mathbf{E}_{\\mathbf{B}\\mathbf{A}i})\\mathbf{L}_{11}\\mathbf{L}_{11}^{*}+\\mathbf{E}_{3}^{\\mathsf{M M}}\\mathbf{L}_{11}^{*}}\\\\ &{\\qquad\\quad=(\\mathbf{B}\\mathbf{A}^{-1}+\\mathbf{E}_{\\mathbf{B}\\mathbf{A}i})(\\mathbf{A}+\\mathbf{E}_{\\mathbf{A}}^{\\mathsf{C H}})+\\mathbf{E}_{3}^{\\mathsf{M M}}\\mathbf{L}_{11}^{*}}\\\\ &{\\qquad\\quad=\\mathbf{B}+\\mathbf{B}\\mathbf{A}^{-1}\\mathbf{E}_{\\mathbf{A}}^{\\mathsf{C H}}+\\mathbf{E}_{\\mathbf{B}\\mathbf{A}i}\\mathbf{A}+\\mathbf{E}_{\\mathbf{B}\\mathbf{A}i}\\mathbf{E}_{\\mathbf{A}}^{\\mathsf{C H}}+\\mathbf{E}_{3}^{\\mathsf{M M}}\\mathbf{L}_{11}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{L}_{21}\\mathbf{L}_{11}^{*}-\\mathbf{B}\\|=\\left\\|\\mathbf{BA}^{-1}\\mathbf{E}_{\\mathbf{A}}^{\\mathrm{CH}}+\\mathbf{E}_{\\mathbf{BA}i}\\mathbf{A}+\\mathbf{E}_{\\mathbf{BA}i}\\mathbf{E}_{\\mathbf{A}}^{\\mathrm{CH}}+\\mathbf{E}_{3}^{\\mathrm{MM}}\\mathbf{L}_{11}^{*}\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\|\\mathbf{B}\\|\\|\\mathbf{A}^{-1}\\|\\|\\mathbf{E}_{\\mathbf{A}}^{\\mathrm{CH}}\\|+\\|\\mathbf{E}_{\\mathbf{BA}i}\\|\\|\\mathbf{A}\\|+\\|\\mathbf{E}_{\\mathbf{BA}i}\\|\\|\\mathbf{E}_{\\mathbf{A}}^{\\mathrm{CH}}\\|+\\|\\mathbf{E}_{3}^{\\mathrm{MM}}\\|\\|\\mathbf{L}_{11}^{*}\\|}\\\\ &{\\qquad\\qquad\\leq\\kappa(\\mathbf{M})\\mathbf{err}(n/2)+\\|\\mathbf{E}_{\\mathbf{BA}i}\\|\\|\\mathbf{M}\\|+\\|\\mathbf{E}_{\\mathbf{BA}i}\\|\\mathbf{err}(n/2)+\\|\\mathbf{E}_{3}^{\\mathrm{MM}}\\|\\|\\mathbf{L}_{11}^{*}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "With a similar procedure we can derive the third bound. We first write $\\mathbf{L}_{21}=\\mathbf{B}\\mathbf{A}^{-1}\\mathbf{L}_{11}+\\mathbf{E}_{\\mathbf{L}_{21}}$ where $\\mathbf{E_{L_{21}}}$ contains several error matrices from previous computations. Then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{L}_{21}\\mathbf{L}_{21}^{*}=\\left(\\mathbf{BA}^{-1}\\mathbf{L}_{11}+\\mathbf{E}_{\\mathbf{L}_{21}}\\right)\\left(\\mathbf{BA}^{-1}\\mathbf{L}_{11}+\\mathbf{E}_{\\mathbf{L}_{21}}\\right)^{*}}\\\\ &{\\!=\\left(\\mathbf{BA}^{-1}\\mathbf{L}_{11}+\\mathbf{E}_{\\mathbf{L}_{21}}\\right)\\left(\\mathbf{BA}^{-1}\\mathbf{L}_{11}+\\mathbf{E}_{\\mathbf{L}_{21}}\\right)^{*}}\\\\ &{\\!=\\mathbf{BA}^{-1}\\mathbf{L}_{11}\\mathbf{L}_{11}^{*}\\mathbf{A}^{-1}\\mathbf{B}^{*}+\\mathbf{E}_{\\mathbf{L}_{21}}\\mathbf{BA}^{-1}\\mathbf{L}_{11}+\\mathbf{L}_{11}^{*}\\mathbf{A}^{-1}\\mathbf{B}^{*}\\mathbf{E}_{\\mathbf{L}_{21}}^{*}+\\mathbf{E}_{\\mathbf{L}_{21}}\\mathbf{E}_{\\mathbf{L}_{21}}^{*}}\\\\ &{\\!=\\mathbf{BA}^{-1}(\\mathbf{A}+\\mathbf{E}_{\\mathbf{A}}^{\\mathbf{C}|\\mathbf{H}})\\mathbf{A}^{-1}\\mathbf{B}^{*}+\\mathbf{E}_{\\mathbf{L}_{21}}\\mathbf{BA}^{-1}\\mathbf{L}_{11}+\\mathbf{L}_{11}^{*}\\mathbf{A}^{-1}\\mathbf{B}^{*}\\mathbf{E}_{\\mathbf{L}_{21}}^{*}+\\mathbf{E}_{\\mathbf{L}_{21}}\\mathbf{E}_{\\mathbf{L}_{21}}^{*}}\\\\ &{\\!=\\mathbf{BA}^{-1}\\mathbf{B}^{*}+\\mathbf{BA}^{-1}\\mathbf{E}_{\\mathbf{A}}^{\\mathbf{C}|\\mathbf{H}}\\mathbf{A}^{-1}\\mathbf{B}^{*}+\\mathbf{E}_{\\mathbf{L}_{21}}\\mathbf{BA}^{-1}\\mathbf{L}_{11}+\\mathbf{L}_{11}^{*}\\mathbf{A}^{-1}\\mathbf{B}^{*}\\mathbf{E}_{\\mathbf{L}_{21}}^{*}+\\mathbf{E}_{\\mathbf{L}_{21}}\\mathbf{E}_{\\mathbf{L}_{21}}^{*}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Similarly ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{L}_{22}\\mathbf{L}_{22}^{*}=\\widetilde{\\mathbf{S}}+\\mathbf{E}_{\\widetilde{\\mathbf{S}}}^{\\mathsf{C H}}}\\\\ &{\\qquad\\qquad=\\mathbf{S}+\\mathbf{E}_{\\mathbf{S}}+\\mathbf{E}_{\\widetilde{\\mathbf{S}}}^{\\mathsf{C H}}}\\\\ &{\\qquad\\qquad=\\mathbf{C}-\\mathbf{B}\\mathbf{A}^{-1}\\mathbf{B}^{*}+\\mathbf{E}_{\\mathbf{S}}+\\mathbf{E}_{\\widetilde{\\mathbf{S}}}^{\\mathsf{C H}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{L}_{21}\\mathbf{L}_{21}^{*}+\\mathbf{L}_{22}\\mathbf{L}_{22}^{*}-\\mathbf{C}\\Vert}\\\\ &{\\qquad\\qquad=\\left\\Vert\\mathbf{B}\\mathbf{A}^{-1}\\mathbf{E}_{\\mathbf{A}}^{\\mathrm{CH}}\\mathbf{A}^{-1}\\mathbf{B}^{*}+\\mathbf{E}_{\\mathbf{L}_{21}}\\mathbf{B}\\mathbf{A}^{-1}\\mathbf{L}_{11}+\\mathbf{L}_{11}^{*}\\mathbf{A}^{-1}\\mathbf{B}^{*}\\mathbf{E}_{\\mathbf{L}_{21}}^{*}+\\mathbf{E}_{\\mathbf{L}_{21}}\\mathbf{E}_{\\mathbf{L}_{21}}^{*}+\\mathbf{E}_{\\mathbf{S}}+\\mathbf{E}_{\\widetilde{\\mathbf{S}}}^{\\mathrm{CH}}\\right\\Vert}\\\\ &{\\qquad\\quad\\leq\\left\\Vert\\mathbf{B}\\mathbf{A}^{-1}\\right\\Vert^{2}\\left\\Vert\\mathbf{E}_{\\mathbf{A}}^{\\mathrm{CH}}\\right\\Vert+2\\left\\Vert\\mathbf{E}_{\\mathbf{L}_{21}}\\right\\Vert\\left\\Vert\\mathbf{B}\\mathbf{A}^{-1}\\right\\Vert\\left\\Vert\\mathbf{L}_{11}\\right\\Vert+\\left\\Vert\\mathbf{E}_{\\mathbf{L}_{21}}\\right\\Vert^{2}+\\left\\Vert\\mathbf{E}_{\\mathbf{S}}\\right\\Vert+\\left\\Vert\\mathbf{E}_{\\widetilde{\\mathbf{S}}}^{\\mathrm{CH}}\\right\\Vert}\\\\ &{\\qquad\\quad\\leq\\kappa(\\mathbf{M})^{2}\\cdot\\mathbf{err}(n/2)+2\\kappa(\\mathbf{M})\\left\\Vert\\mathbf{E}_{\\mathbf{L}_{21}}\\right\\Vert\\left\\Vert\\mathbf{L}_{11}\\right\\Vert+\\left\\Vert\\mathbf{E}_{\\mathbf{L}_{21}}\\right\\Vert^{2}+\\left\\Vert\\mathbf{E}_{\\mathbf{S}}\\right\\Vert+\\mathbf{err}(n/2)}\\\\ &{\\qquad\\quad\\leq2\\kappa(\\mathbf{M})^{2}\\cdot\\mathbf{err}(n/2)+2\\kappa(\\mathbf{M})\\left\\Vert\\mathbf{E}_{\\mathbf{L}_{21}}\\right\\Vert\\left\\Vert\\mathbf{L}_{11}\\right\\Vert+\\left\\Vert\\mathbf{E}_{\\mathbf{L}_{21}}\\right\\Vert^{2}+\\left\\Vert\\mathbf{E}_{\\mathbf{S}}\\right\\Vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "With this, the task reduces to finding appropriate bounds for the norms of the matrices $\\mathbf{E}_{3}^{\\mathsf{M M}},\\mathbf{E}_{\\mathbf{BA}i},\\mathbf{E}_{\\mathbf{S}}$ $\\mathbf{E_{L_{21}}}$ We frstdrve bounds forthe normnsoftherormaties $\\mathbf{E}_{(\\cdot)}$ ", "page_idx": 34}, {"type": "text", "text": "Lemma C.2. In Algorithm 2, the following bounds hold for the error matrices ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{E}_{1}^{\\mathrm{WW}}\\right\\|\\leq\\mathbf{u}\\cdot\\xi_{\\mathrm{BW}}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\left\\|\\mathbf{M}^{-1}\\right\\|,}\\\\ &{\\left\\|\\mathbf{E}_{2}^{\\mathrm{WM}}\\right\\|\\leq\\mathbf{u}\\cdot\\xi_{\\mathrm{I}}\\left(\\frac{1}{2},\\mathbf{M}\\right)\\left(1+\\cdot6\\cdot5\\mathrm{tw}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\right),}\\\\ &{\\left\\|\\mathbf{E}_{3\\mathrm{BA}}\\right\\|\\leq\\mathbf{u}\\cdot\\xi_{\\mathrm{BW}}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\cdot\\boldsymbol{\\kappa}(\\mathbf{M})+\\cdot\\mathbf{u}\\cdot\\xi_{\\mathrm{I}}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\left(1+\\cdot\\mathbf{u}\\cdot\\xi_{\\mathrm{RW}}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\right),}\\\\ &{\\left\\|\\mathbf{E}_{3}^{\\mathrm{WM}}\\right\\|\\leq\\mathbf{u}\\cdot\\xi_{\\mathrm{I}}\\left(\\frac{1}{2},\\mathbf{M}\\right)\\left(1+\\cdot\\mathbf{u}\\cdot\\xi_{\\mathrm{RW}}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\right)^{2}\\left\\|\\mathbf{E}_{1}\\right\\|,}\\\\ &{\\left\\|\\mathbf{E}_{4}^{\\mathrm{WM}}\\right\\|\\leq\\mathbf{u}\\cdot\\xi_{\\mathrm{I}}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\left(1+\\cdot\\mathbf{u}\\cdot\\xi_{\\mathrm{RW}}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\right)^{2}\\left\\|\\mathbf{M}\\right\\|,}\\\\ &{\\left\\|\\mathbf{E}_{5}^{\\mathrm{WU}}\\right\\|\\leq\\mathbf{u}\\cdot\\sqrt{n/2}\\cdot\\left\\|\\mathbf{M}\\right\\|\\cdot\\left(1+\\cdot\\left(\\mathbf{c}(\\mathbf{M})+\\mathbf{u}\\cdot\\xi_{\\mathrm{I}}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\right)\\left(1+\\cdot\\mathbf{u}\\cdot\\xi_{\\mathrm{RW}}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\right)^{2}\\right),}\\\\ &{\\left\\|\\mathbf{E}_{6\\mathrm{H}}^{\\mathrm{WU}}\\right\\|\\leq\\mathbf{u}\\cdot\\xi_{\\mathrm{RW}}\\left(\\frac{\\eta}{2},\\mathbf{M}\\right)\\cdot\\left[\\kappa(\\mathbf{M})+\\left(2+\\cdot\\mathbf{u}\\cdot\\xi_{\\mathrm \n$$(i)(ii)(ii)(iv)(v)(vi)(vii)(vii\uff09", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "(i) For $\\lVert\\mathbf{E}_{1}^{\\sf I N V}\\rVert$ we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{E}_{1}^{\\mathsf{I N V}}\\|\\leq\\mu_{\\mathsf{I N V}}(n/2)\\cdot\\mathbf{u}\\cdot\\kappa(\\mathbf{A})^{c_{\\mathsf{I N V}}\\log\\frac{n}{2}}\\|\\mathbf{A}^{-1}\\|\\qquad\\mathrm{(..from~Thm.~A.3)}}\\\\ {\\leq\\mu(n/2)\\cdot\\mathbf{u}\\cdot\\kappa(\\mathbf{A})^{c_{\\mathsf{I N V}}\\log\\frac{n}{2}}\\|\\mathbf{M}^{-1}\\|}\\\\ {=\\mathbf{u}\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\|\\mathbf{M}^{-1}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "(i) Similarly, for $\\|\\mathbf{E}_{2}^{\\mathsf{M M}}\\|$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{E}_{2}^{\\mathrm{MM}}\\|\\leq\\mu_{\\mathsf{M M}}(n/2)\\cdot\\mathbf{u}\\|\\mathbf{B}\\|\\|\\mathbf{A}^{-1}+\\mathbf{E}_{1}^{\\mathrm{(NV)}}\\|\\qquad(\\mathrm{..from~Thm.~A.2})}\\\\ &{\\qquad\\leq\\mu_{\\mathsf{M M}}(n/2)\\cdot\\mathbf{u}\\|\\mathbf{B}\\|\\|\\mathbf{A}^{-1}\\|+\\mu_{\\mathsf{M M}}(n/2)\\cdot\\mathbf{u}\\|\\mathbf{B}\\|\\|\\mathbf{E}_{1}^{\\mathrm{(NV)}}\\|\\,.}\\\\ &{\\qquad\\leq\\mu_{\\mathsf{M M}}(n/2)\\cdot\\mathbf{u}\\cdot\\|\\mathbf{M}\\|\\|\\mathbf{M}^{-1}\\|+\\mu_{\\mathsf{M M}}(n/2)\\cdot\\mathbf{u}\\cdot\\|\\mathbf{M}\\|\\cdot\\mathbf{u}\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\|\\mathbf{M}^{-1}\\|}\\\\ &{\\qquad\\leq\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)+\\mathbf{u}^{2}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M}\\right)}\\\\ &{\\qquad=\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "(ii) Using these two inequalities we can bound the norm of $\\mathbf{E_{BA}}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{E}_{\\mathbf{BA}i}\\|=\\|\\mathbf{B}\\mathbf{E}_{1}^{\\mathrm{INV}}+\\mathbf{E}_{2}^{\\mathrm{MM}}\\|}&{}\\\\ {\\leq\\|\\mathbf{B}\\|\\|\\mathbf{E}_{1}^{\\mathrm{INV}}\\|+\\|\\mathbf{E}_{2}^{\\mathrm{MM}}\\|}&{}\\\\ {\\leq\\|\\mathbf{M}\\|\\cdot\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\|\\mathbf{M}^{-1}\\|+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right))}\\\\ {\\leq\\kappa(\\mathbf{M})\\cdot\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "(iv) Next is the error term of the matrix multiplication between the result of ${\\mathsf{M M}}(\\mathbf{B},{\\mathsf{I N V}}(A))$ and $\\mathbf{L}_{11}$ . Expanding Theorem A.2 gives ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\lVert{\\bf E}_{3}^{\\sf M M}\\rVert\\leq\\mu_{\\sf M M}(n/2)\\cdot{\\bf u}\\cdot\\lVert{\\bf L}_{11}\\rVert\\,\\lVert{\\bf B A}i\\rVert\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "It suffices to bound $\\|\\mathbf{BA}i\\|~=~\\left\\|\\mathbf{B}\\left(\\mathbf{A}^{-1}+\\mathbf{E}_{1}^{\\mathsf{I N V}}\\right)+\\mathbf{E}_{2}^{\\mathsf{M M}}\\right\\|~\\leq~\\|\\mathbf{BA}^{-1}\\|~+~\\|\\mathbf{BE}_{1}^{\\mathsf{I N V}}\\|~+~$ $\\|\\mathbf{E}_{2}^{\\mathsf{M M}}\\|$ . Using Proposition C.1, the triangle inequality, and the previous bounds, the two ", "page_idx": 35}, {"type": "text", "text": "unknown norms in the sum are bounded as follows ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{BA}^{-1}\\|\\leq\\|\\mathbf{B}\\|\\|\\mathbf{A}^{-1}\\|\\leq\\|\\mathbf{M}\\|\\|\\mathbf{M}^{-1}\\|=\\kappa(\\mathbf{M}),}\\\\ &{\\|\\mathbf{BE}_{1}^{\\mathrm{INV}}\\|\\leq\\|\\mathbf{B}\\|\\|\\mathbf{E}_{1}^{\\mathrm{INV}}\\|}\\\\ &{\\qquad\\qquad\\leq\\|\\mathbf{M}\\|\\cdot\\mathbf{u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\|\\mathbf{M}^{-1}\\|}\\\\ &{\\qquad\\qquad=\\mathbf{u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\kappa(\\mathbf{M}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This gives ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{BA}i\\right\\|\\leq\\kappa(\\mathbf{M})+\\mathbf{u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\kappa(\\mathbf{M})+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)}\\\\ &{\\qquad\\quad=\\kappa(\\mathbf{M})\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)}\\\\ &{\\qquad\\quad\\leq\\kappa(\\mathbf{M})\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where in the last inequality we simplified $\\begin{array}{r l r}{\\big(1+{\\bf u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},{\\bf M}\\right)\\big)}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\leq}&{\\big(1+{\\bf u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},{\\bf M}\\right)\\big).}\\end{array}$ Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{E}_{3}^{\\mathrm{MM}}\\|\\leq\\mu_{\\sf M M}(n/2)\\cdot\\mathbf{u}\\cdot\\|\\mathbf{L}_{11}\\|\\cdot\\kappa(\\mathbf{M})\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)}\\\\ &{\\qquad\\qquad\\leq\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\cdot\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)^{2}\\|\\mathbf{L}_{11}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "concluding the bound for EM ", "page_idx": 36}, {"type": "text", "text": "(v) The norm of $\\mathbf{E}_{4}^{\\mathsf{M M}}$ is vr similar tothat of $\\mathbf{E}_{3}^{\\mathsf{M M}}$ sinhyl $\\mathbf{BA}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{E}_{4}^{\\mathrm{MM}}\\|\\leq c_{\\mathsf{H E R M}}\\log(n/2)\\cdot\\mu_{\\mathsf{M M}}(n/2)\\cdot\\mathbf{u}\\cdot\\|\\mathbf{BA}i\\|\\|\\mathbf{B}\\|\\qquad(\\mathrm{..from~Corollary~A.2})}\\\\ &{\\qquad\\qquad\\leq c_{\\mathsf{H E R M}}\\log(n/2)\\cdot\\mu_{\\mathsf{M M}}(n/2)\\cdot\\mathbf{u}\\cdot\\kappa(\\mathbf{M})\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)^{2}\\|\\mathbf{M}\\|}\\\\ &{\\qquad\\qquad\\leq\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)^{2}\\|\\mathbf{M}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "(vi) Next is ${\\bf E}_{5}^{\\mathsf{S U B}}$ . From Equation (5): ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{E}_{5}^{\\mathrm{SUB}}\\|\\leq\\mathbf{u}\\sqrt{n/2}\\left\\|\\mathbf{C}-\\left((\\mathbf{BA}i)\\mathbf{B}^{*}+\\mathbf{E}_{4}^{\\mathrm{MM}}\\right)\\right\\|}&{}\\\\ {\\leq\\mathbf{u}\\sqrt{n/2}\\left(\\|\\mathbf{C}\\|+\\|(\\mathbf{BA}i)\\mathbf{B}^{*}\\|+\\|\\mathbf{E}_{4}^{\\mathrm{MM}}\\|\\right\\|}&{}\\\\ {\\leq\\mathbf{u}\\sqrt{n/2}\\left(\\|\\mathbf{M}\\|+\\|(\\mathbf{BA}i)\\mathbf{B}^{*}\\|+\\|\\mathbf{E}_{4}^{\\mathrm{MM}}\\|\\right)}&{}\\\\ {\\leq\\mathbf{u}\\sqrt{n/2}\\left(\\|\\mathbf{M}\\|+\\kappa(\\mathbf{M})\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{NW}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)^{2}\\|\\mathbf{M}\\|+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{NW}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)\\right)}\\\\ {=\\mathbf{u}\\sqrt{n/2}\\cdot\\|\\mathbf{M}\\|\\cdot\\left(1+\\kappa(\\mathbf{M})\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{IW}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)^{2}+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{IW}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)\\right)}&{}\\\\ {=\\mathbf{u}\\sqrt{n/2}\\cdot\\|\\mathbf{M}\\|\\cdot\\left(1+\\left(\\kappa(\\mathbf{M})+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{IW}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "(vi)  The next term is $\\mathbf{E_{L_{21}}}$ , which we can bound as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi_{\\mathbf{L}_{21}}\\|=\\|\\mathbf{E}_{\\mathbf{B}\\mathbf{A}_{i}}\\mathbf{L}_{11}+\\mathbf{E}_{3}^{\\mathrm{MM}}\\|}\\\\ &{\\qquad\\leq\\|\\mathbf{E}_{\\mathbf{B}\\mathbf{A}_{i}}\\|\\cdot\\|\\mathbf{L}_{11}\\|+\\|\\mathbf{E}_{3}^{\\mathrm{MM}}\\|}\\\\ &{\\qquad\\leq\\left[\\kappa(\\mathbf{M})\\cdot\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)\\right]\\cdot\\|\\mathbf{L}_{11}\\|+\\cdot\\cdot\\cdot}\\\\ &{\\qquad\\qquad\\cdot\\mathbf{\\Lambda}\\cdot+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)^{2}\\|\\mathbf{L}_{11}\\|}\\\\ &{\\qquad=\\left[\\kappa(\\mathbf{M})\\cdot\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\cdot\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)\\cdot\\left(2+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)\\right]}\\\\ &{\\qquad\\leq\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\cdot\\Big[\\kappa(\\mathbf{M})+\\left(2+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)^{2}\\Big]\\cdot\\|\\mathbf{L}_{11}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "(vi) The final and most involved error term is the norm of $\\mathbf{E_{S}}$ . Recall that from Line 7 of Algorithm 2 the matrix can be written as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbf{E_{S}}=\\mathbf{B}\\mathbf{E}_{1}^{\\mathsf{I N V}}\\mathbf{B}^{*}+\\mathbf{E}_{2}^{\\mathsf{M M}}\\mathbf{B}^{*}+\\mathbf{E}_{4}^{\\mathsf{M M}}+\\mathbf{E}_{5}^{\\mathsf{S U B}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Once more, the norm of each term is bounded separately. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{B}\\mathbf{E}_{1}^{\\mathsf{I N V}}\\mathbf{B}^{*}\\|\\leq\\|\\mathbf{B}\\|\\|\\mathbf{E}_{1}^{\\mathsf{I N V}}\\|\\|\\mathbf{B}^{*}\\|}&{}\\\\ {\\leq\\|\\mathbf{M}\\|^{2}\\cdot\\mathbf{u}\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\|\\mathbf{M}^{-1}\\|}&{}\\\\ {=\\kappa(\\mathbf{M})\\cdot\\|\\mathbf{M}\\|\\cdot\\mathbf{u}\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Similarly, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{E}_{2}^{\\sf M M}\\mathbf{B}^{*}\\|\\leq\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},\\mathbf{M}\\right)\\right)\\cdot\\|\\mathbf{M}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The final bound for $\\mathbf{E_{S}}$ is given by the sum of the four bounds that were derived. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n(n-1)}\\langle n,\\mathbf{b}^{\\top}\\rangle+\\mathbf{l}(\\frac{n-1}{\\beta})\\mathbf{l}(\\frac{n-1}{\\beta})}\\\\ &{\\leq\\alpha+\\epsilon\\frac{1}{n(n-1)}\\langle n,\\mathbf{1}(1+\\epsilon),\\dots(1+\\epsilon),1\\rangle}\\\\ &{\\quad-\\epsilon\\frac{1}{n(n-1)}\\langle\\mathbf{1},\\mathbf{1},\\mathbf{2}(1+\\epsilon),\\dots(1+\\epsilon),1\\rangle}\\\\ &{\\quad-\\epsilon\\frac{1}{n(n-1)}\\langle\\mathbf{1},\\mathbf{3}(1+\\epsilon),\\dots(1+\\epsilon),1\\rangle\\langle\\mathbf{1},\\mathbf{1}\\rangle}\\\\ &{\\quad-\\epsilon\\frac{1}{n(n-1)}\\langle\\mathbf{1},\\mathbf{2}(1+\\epsilon),\\dots(1+\\epsilon),1\\rangle\\langle\\mathbf{1}+\\epsilon\\rangle+\\epsilon\\delta_{n}(\\frac{1}{n-1})\\langle\\mathbf{1},\\mathbf{3}(1+\\epsilon),}\\\\ &{\\quad-\\epsilon\\rangle\\langle\\mathbf{1},\\mathbf{3}(1+\\epsilon),\\dots(1+\\epsilon),1\\rangle}\\\\ &{\\quad-\\epsilon\\frac{1}{n(n-1)}\\langle\\mathbf{1},\\mathbf{3}(1+\\epsilon),\\dots(1+\\epsilon),1\\rangle}\\\\ &{\\quad-\\epsilon\\frac{1}{n(n-1)}\\langle\\mathbf{1},\\mathbf{3}(1+\\epsilon),\\dots(1+\\epsilon),1\\rangle}\\\\ &{\\quad-\\epsilon\\frac{1}{n(n-1)}\\langle\\mathbf{1}+\\epsilon,\\dots(1+\\epsilon),1\\rangle}\\\\ &{\\quad-\\epsilon\\frac{1}{n(n-1)}\\langle\\mathbf{1}+\\epsilon,\\dots(1+\\epsilon),\\dots(1+\\epsilon),1\\rangle\\langle\\mathbf{1}+\\epsilon,\\dots(1+\\epsilon),1\\rangle\\rangle\\Big]}\\\\ &{\\quad-\\epsilon\\frac{1}{n(n-1)}\\langle\\mathbf{1}-\\epsilon,\\mathbf{1},\\mathbf{3}(1+\\epsilon),\\dots(1+\\epsilon),\\dots(1+\\epsilon),}\\\\ &{\\quad-\\epsilon\\rangle+\\epsilon\\frac{1}{n(n-1)}\\langle\\mathbf{1},\\mathbf{2}(1+\\epsilon),\\dots(1+\\epsilon),\\dots(1+\\epsilon),1\\rangle}\\\\ &{\\quad-\\epsilon\\frac{1}{n(n-1)}\\langle\\mathbf{1}+\\epsilon,\\dots(1+\\epsilon),1\\rangle\\langle\\mathbf{1}+\\epsilon,\\dots(1+\\epsilon),\\dots(1+\\epsilon),\\dots\\rangle}\\\\ &{\\quad-\\epsilon\\rangle+\\epsilon\\frac \n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "C.2 Maintaining positive-definiteness, norms, and condition numbers throughout the recursion ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Given the bounds of Lemma C.2, we can now calculate the appropriate machine precision $\\mathbf{u}$ and the corresponding number of bits such that the algorithm will not break down due to loss of positivedefiniteness of the submatrices. Hereafter, we will denote by M the matrix that is passed as an argument in any of the recursive calls of Algorithm 2, and ${{\\bf{M}}_{0}}$ will denote the original matrix that needs tobefactorized. ", "page_idx": 37}, {"type": "text", "text": "Lemma C.3. Let $\\mathbf{M}_{0}\\in\\mathbb{H}_{++}^{n}$ be a matrix that is factorized using Algorithm 2 and $\\kappa$ be its condition number: Then there exist constants $c_{1},c_{2},c_{3},c_{4}\\geq1$ suchthati $f n>c_{1}$ and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq\\mathbf{u}_{++}:=\\frac{1}{c_{2}\\cdot n^{c_{3}}\\cdot\\kappa^{c_{4}\\log n}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "then every matrix that is constructed in Line 7 during the recursion, that is, every Schur complement $\\widetilde{\\mathbf{S}}$ and every upperleftblock A,willbe Hmitian and positive-definiteMoreover, for ach such matrix $\\mathbf{X}$ itholdsthat $\\|\\mathbf{X}\\|\\leq2\\|\\mathbf{M}_{0}\\|$ and $\\kappa(\\mathbf{X})\\leq2\\kappa(\\mathbf{M}_{0})$ .This value of u translates to ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\log(1/\\mathbf{u})=O(\\log(n)\\log(\\kappa))\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "required bits of precision. ", "page_idx": 38}, {"type": "text", "text": "Proof. At each recursive step, two matrices need to remain positive-definite: $\\mathbf{A}$ and $\\widetilde{\\bf S}$ .If $\\mathbf{M}$ is positive-definite then so is $\\mathbf{A}$ . It remains to ensure the same for the Schur complement. To prove this we first fix $\\mathbf{u}$ be bounded by the value ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq\\mathbf{u}_{++}:=\\frac{1}{n^{2c_{\\mathsf{I N V}}}\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)\\cdot72\\kappa(\\mathbf{M}_{0})^{2}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Assume that we are in the first level of recursion, i.e. $\\mathbf{M}=\\mathbf{M}_{0}$ . Per Lemma C.2, we can write $\\widetilde{\\mathbf{S}}=\\mathbf{S}+\\mathbf{E}_{\\mathbf{S}}$ ,where ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}_{\\mathbf{S}}\\Vert\\leq\\mathbf{u}\\cdot\\Vert\\mathbf{M}_{0}\\Vert\\cdot\\mathcal{E}_{\\mathrm{IW}}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)\\cdot\\left(2\\kappa(\\mathbf{M}_{0})+\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{IW}}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)\\right)^{2}\\left(3\\kappa(\\mathbf{M}_{0})+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)\\right)^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the assumed value of $\\mathbf{u}$ both $\\begin{array}{r}{\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)\\leq1}\\end{array}$ and $\\begin{array}{r}{\\mathbf{u}\\cdot\\mathcal{E}_{\\sf I N V}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)\\leq1}\\end{array}$ , and we also know that $1\\leq\\kappa(\\mathbf{M}_{0})$ , in which case the bound simplifies to ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{E_{S}}\\|\\leq\\mathbf{u}\\cdot\\|\\mathbf{M_{0}}\\|\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M_{0}}\\right)\\cdot\\left(2\\kappa(\\mathbf{M_{0}})+\\left(1+1\\right)^{2}\\left(3\\kappa(\\mathbf{M_{0}})+1\\right)\\right)}\\\\ &{\\qquad\\leq\\mathbf{u}\\cdot\\|\\mathbf{M_{0}}\\|\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M_{0}}\\right)\\cdot\\left(2\\kappa(\\mathbf{M_{0}})+4\\left(3\\kappa(\\mathbf{M_{0}})+\\kappa(\\mathbf{M_{0}})\\right)\\right)}\\\\ &{\\qquad\\leq\\mathbf{u}\\cdot\\|\\mathbf{M_{0}}\\|\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M_{0}}\\right)\\cdot18\\kappa(\\mathbf{M_{0}})}\\\\ &{\\qquad\\leq\\frac{1}{n^{2c_{\\mathrm{INV}}}\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M_{0}}\\right)\\cdot72\\kappa(\\mathbf{M_{0}})^{2}}\\cdot\\|\\mathbf{M_{0}}\\|\\cdot\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M_{0}}\\right)\\cdot18\\kappa(\\mathbf{M_{0}})}\\\\ &{\\qquad\\leq\\frac{1}{n^{2c_{\\mathrm{INV}}}}\\cdot\\lambda_{\\operatorname*{min}}(\\mathbf{M_{0}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using this, we derive the following useful inequalities ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{max}}(\\widetilde{\\mathbf S})\\leq||\\mathbf S||+|\\mathbf E_{\\mathbf S}||\\leq(1+1/n^{2\\mathrm{cuv}})||\\mathbf M_{0}|,}\\\\ &{\\lambda_{\\operatorname*{min}}(\\widetilde{\\mathbf S})\\geq\\lambda_{\\operatorname*{min}}(\\mathbf S)-||\\mathbf E_{\\mathbf S}||\\geq\\lambda_{\\operatorname*{min}}(\\mathbf S)-\\frac{1}{n^{2\\mathrm{cuv}}}\\lambda_{\\operatorname*{min}}(\\mathbf M_{0})\\geq(1-1/n^{2\\mathrm{cuv}})\\lambda_{\\operatorname*{min}}(\\mathbf M_{0}),}\\\\ &{\\quad\\quad\\kappa(\\widetilde{\\mathbf S})\\leq\\frac{n^{2\\mathrm{cuv}}}{n^{2\\mathrm{cuv}}}-1}\\cdot\\kappa(\\mathbf M_{0}),}\\\\ &{\\mathcal{E}_{\\mathrm{IW}}\\left(\\frac{n}{4},\\widetilde{\\mathbf S}\\right)=\\mu(n/4)\\kappa(\\widetilde{\\mathbf S})^{\\mathrm{ouv}}\\log(n/4)}\\\\ &{\\quad\\quad\\quad\\leq\\mu(n/4)\\cdot\\left(\\frac{n^{2\\mathrm{cuv}}+1}{n^{2\\mathrm{cuv}}-1}\\right)^{\\mathrm{cuv}\\log(n/4)}\\cdot\\kappa(\\mathbf M_{0})^{\\mathrm{cuv}\\log(n/4)}}\\\\ &{\\quad\\quad\\quad\\leq n^{\\mathrm{cuv}}\\cdot\\mu(n/4)\\cdot\\kappa(\\mathbf M_{0})^{\\mathrm{cuv}\\log(n/4)}}\\\\ &{\\quad\\quad\\quad\\leq n^{\\mathrm{cuv}}\\xi_{\\mathrm{IW}}\\left(\\frac{n}{4},\\mathbf M_{0}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In the above, we used the fact that for $c_{\\mathsf{I N V}}\\geq1$ we have that $\\frac{n^{2c}\\mathsf{I N V}+1}{n^{2c}\\mathsf{I N V}-1}\\,\\leq\\,2$ , for all $n\\,\\geq\\,3$ and therefore $2^{c_{\\mathsf{I N V}}\\log(n/4)}\\,=\\,(n/4)^{c_{\\mathsf{I N V}}}\\,\\leq\\,n^{c_{\\mathsf{I N V}}}$ . Since the smallest eigenvalue of $\\widetilde{\\bf S}$ is larger than a positive value, we can conclude that it is Hermitian and positive-definite, and its condition number and spectral norm are appropriately bounded. ", "page_idx": 38}, {"type": "text", "text": "In a similar manner, we can now get a bound for the Schur complement of the Schur complement. In this recursive step we call Algorithm 2 with $\\mathbf{M}=\\widetilde{\\mathbf{S}}$ .Let ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{S}}=\\left(\\!\\!\\begin{array}{c c}{\\mathbf{A}_{1}}&{\\mathbf{B}_{1}^{*}}\\\\ {\\mathbf{B}_{1}}&{\\mathbf{C}_{1}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let $\\mathbf{S}_{1}$ be the true Schur complement of $\\widetilde{\\bf S}$ ,i.e. $\\mathbf{S}_{1}=\\mathbf{C}_{1}\\!-\\!\\mathbf{B}_{1}\\mathbf{A}_{1}^{-1}\\mathbf{B}_{1}^{*}$ , and let $\\widetilde{\\bf S}_{1}$ be the approximate Schur complement of $\\widetilde{\\mathbf{S}}$ that is constructed in line 7 of Algorithm 2 when executed on $\\widetilde{\\mathbf{S}}$ . Let $\\mathbf{E}_{\\mathbf{S}_{1}}$ be the corresponding error matrix, i.e. $\\widetilde{\\mathbf{S}}_{1}=\\mathbf{S}_{1}+\\mathbf{E}_{\\mathbf{S}_{1}}$ . Using Lemma C.2 and simplifying the terms $(1+1/n^{c_{\\sf I N V}})\\leq2$ and n\u00b2-iv 1 \u2264 2 we have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{S}_{3}}{\\leq}\\;\\boxed{\\|\\mathbf{M}_{0}\\|}}&{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The corresponding bounds for $\\widetilde{\\bf S}_{1}$ can be derived: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lambda_{\\operatorname*{max}}\\big(\\widetilde{\\mathbf{S}}_{1}\\big)\\leq|\\widetilde{\\mathbf{S}}|+|\\mathbf{E}|\\leq_{1}\\|}&{}\\\\ {\\leq\\|\\mathbf{S}\\|+\\|\\mathbf{S}_{\\infty}\\|+\\|\\mathbf{E}_{s}\\|}&{}\\\\ {\\leq\\|\\mathbf{M}_{0}\\|+\\frac{1}{r^{2}\\infty}\\|\\mathbf{M}_{0}\\|+\\frac{1}{r^{\\alpha\\infty}}\\|\\mathbf{M}_{0}\\|}\\\\ {\\leq(1+2/r^{\\alpha\\infty})\\|\\mathbf{M}_{0}\\|}&{}\\\\ {\\lambda_{\\operatorname*{min}}\\big(\\widetilde{\\mathbf{S}}_{1}\\big)\\geq(1-2/r^{\\alpha\\infty})\\lambda_{\\operatorname*{min}}(\\mathbf{M}_{0}),}\\\\ {\\kappa\\big(\\widetilde{\\mathbf{S}}_{1}\\big)\\leq\\frac{n^{\\infty}(\\mathbf{-2})}{n^{\\infty}}\\cdots\\rho\\Lambda(\\mathbf{M}_{0}),}\\\\ {\\in\\widetilde{\\mathbf{buv}}\\bigg(\\widetilde{\\mathbf{g}},\\widetilde{\\mathbf{S}}_{1}\\bigg)=\\mu(n/\\Lambda)\\times\\Big(\\frac{\\Gamma}{n^{\\infty}}\\mathbf{+2}\\Big)^{\\infty}}\\\\ {\\leq\\mu(n/\\Lambda)\\cdot\\bigg(\\frac{n^{\\infty}}{n^{\\infty}}\\cdots2\\bigg)^{\\infty}}&{\\cdots\\kappa\\big(\\mathbf{M}_{0}\\big)^{\\infty}\\mathrm{e}^{-\\mathrm{i}\\Lambda_{\\phi}}|\\mathbf{0}\\rangle}\\\\ {\\leq\\mu^{\\infty}\\cdot\\mu(n/\\Lambda)\\cdot\\bigg(\\frac{n^{\\infty}}{n^{\\infty}}\\cdots2\\bigg)^{\\infty}}\\\\ {=n^{\\infty}\\cdot\\mu(n/\\Lambda)\\cdot\\mu(\\mathbf{M}_{0})^{\\infty}\\mathrm{e}^{-\\mathrm{i}\\Lambda_{\\phi}}(n/\\Lambda)}\\\\ {=n^{\\infty}\\xi\\cdot\\widehat{\\mathbf{M}_{0}}(\\mathbf{\\bar{\\mu}},\\mathbf{M}_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We can therefore identify that $\\widetilde{\\bf S}_{1}$ is also Hermitian positive-definite and its extremal eigenvalues are similarly bounded with those of $\\widetilde{\\mathbf{S}}$ ", "page_idx": 39}, {"type": "text", "text": "If we keep applying the same analysis for all $\\log(n)$ iterations, as long as $n$ is greater than some constant such that $(n^{c_{\\mathsf{I N V}}}+\\log n)/(n^{c_{\\mathsf{I N V}}}-\\log n)\\leq2$ , then for each $i=1,...,\\log n$ it holds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|\\mathbf{E}_{\\mathbf{S}_{i}}\\|\\leq\\frac{1}{n^{c_{\\mathsf{I N V}}}}\\cdot\\lambda_{\\operatorname*{min}}(\\mathbf{M}_{0})\\leq\\frac{1}{n^{c_{\\mathsf{I N V}}}}\\|\\mathbf{M}_{0}\\|,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which implies that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\operatorname*{max}}(\\widetilde{\\mathbf S}_{i})\\leq(1+i/n^{c_{\\mathrm{INV}}})\\|\\mathbf M_{0}\\|,\\quad\\quad}\\\\ {\\lambda_{\\operatorname*{min}}(\\widetilde{\\mathbf S}_{i})\\geq(1-i/n^{c_{\\mathrm{INV}}})\\lambda_{\\operatorname*{min}}(\\mathbf M_{0}),}\\\\ {\\kappa(\\widetilde{\\mathbf S}_{i})\\leq\\frac{n^{c_{\\mathrm{INV}}}+i}{n^{c_{\\mathrm{INV}}}-i}\\cdot\\kappa(\\mathbf M_{0}),}\\\\ {\\mathcal E_{\\sf I N V}\\left(\\frac{n}{2^{i}},\\widetilde{\\mathbf S}_{i}\\right)\\leq n^{c_{\\mathrm{INV}}}\\mathcal E_{\\sf I N V}\\left(\\frac{n}{2^{i}},\\mathbf M_{0}\\right).\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The same bounds hold for the matrices $\\mathbf{A}_{i}$ , since they always originate from the top-left corner of a matrix $\\widetilde{\\mathbf{S}}_{j}$ , where $j~<i$ ,i.e., ${\\bf A}_{i}$ either has no errors or it inherits the errors from a matrix $\\widetilde{\\mathbf{S}}_{j}$ We can therefore conclude that, for the value of $\\mathbf{u}$ in Inequality (13), every matrix $\\mathbf{A}_{i}$ and $\\widetilde{\\mathbf{S}}_{i}$ that is constructed during the recursion of Algorithm 2 will be Hermitian and positive-definite and its condition number and spectral norm will be at most $2\\kappa(\\mathbf{M}_{0})$ and $2\\|\\mathbf{M}_{0}\\|$ respectively. The required number of bits is ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{log}(1/\\mathbf{u}_{++})=\\mathrm{log}\\left(n^{2c_{\\mathrm{INV}}}\\cdot\\mathcal{E}_{\\mathrm{INV}}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)\\cdot72\\kappa(\\mathbf{M}_{0})^{2}\\right)=O\\left(\\log(n)\\log(\\kappa(\\mathbf{M}_{0}))\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "C.3  Final backward-approximation bounds and proof of Theorem 1.2 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Having safeguarded the possibility of a breakdown due to loss of positive-definiteness, we can now revisit the bounds of Lemma C.1 and finalize the proof of Theorem 1.2. ", "page_idx": 40}, {"type": "text", "text": "Theorem C.1 (Restatement of Theorem 1.2). Given a Hermitian positive-definite matrix M, there exists analgorithm $\\mathbf{L}\\gets\\mathsf{C H O L E S K Y}(\\mathbf{M})$ listed in Algorithm 2, which requires $O(T_{\\mathsf{M M}}(n))$ arithmetic operations.This algorithm is logarithmically stable,in a sense that,there exist global constants $c_{1}$ ${\\dot{\\mathbf{\\rho}}}_{1},\\,c_{2},\\,c_{3}$ such that for all $\\epsilon\\in(0,1)$ if executed in a floating point machine with precision ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq\\mathbf{u}_{\\mathrm{CHOLESKY}}:=\\epsilon\\frac{1}{c_{1}n^{c_{2}}\\kappa(\\mathbf{M})^{c_{3}\\log n}},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "whichtranslatesinto $O\\left(\\log(n)\\log(\\kappa(\\mathbf{M}))+\\log(\\frac{1}{\\epsilon})\\right)$ required bits of precision, then it does not break down due to arithmetic errors, and the solution returned satisfies $\\lVert\\mathbf{L}\\mathbf{L}^{*}-\\mathbf{M}\\rVert\\leq\\epsilon\\lVert\\mathbf{M}\\rVert$ ", "page_idx": 40}, {"type": "text", "text": "Proof. The arithmetic complexity of the algorithm was already bounded in Proposition C.2 ", "page_idx": 40}, {"type": "text", "text": "For the error proof we assume the bound for $\\mathbf{u}\\,\\leq\\,\\mathbf{u}_{++}$ from Inequality (13). Combining the inequalities for a single recursion step from Lemma C.2 with the results of the previous section and the bound for $\\mathbf{u}$ , we have the following inequalities (recall once more that $\\mathbf{M}$ is the input of the algorithm for the specific recursive call, while ${{\\bf{M}}_{0}}$ is the matrix in the original call): ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\mathbf{E}_{3}^{\\mathrm{BH}}\\right|\\right|\\leq\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{1}{\\beta},\\mathbf{M}\\right)\\left(1+\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{BV}}\\left(\\frac{1}{\\beta},\\mathbf{M}\\right)\\right)^{2}\\left|\\left|\\mathbf{L}_{11}\\right|}\\\\ &{\\qquad\\leq\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{1}{\\beta},\\mathbf{M}\\right)\\left|\\left|\\mathbf{L}_{11}\\right|,}\\\\ &{\\qquad\\mathbf{E}_{31}\\right|\\leq\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{BV}}\\left(\\frac{1}{\\beta},\\mathbf{M}\\right)\\cdot\\left|\\mathbf{K}(\\mathbf{M})+\\mathbf{u}\\cdot\\mathcal{E}_{1}\\left(\\frac{1}{\\beta},\\mathbf{M}\\right)\\right|\\cdot}\\\\ &{\\qquad\\leq\\mathbf{u}\\cdot\\left(2\\pi^{\\mathrm{inf}}\\delta\\mathbf{E}_{\\mathrm{BV}}\\left(\\frac{1}{\\beta},\\mathbf{M}\\right)\\cdot\\mathbf{K}(\\mathbf{M}_{0})+4\\xi_{1}\\left(\\frac{1}{\\beta},\\mathbf{M}_{0}\\right)\\right)}\\\\ &{\\qquad\\qquad\\leq\\mathbf{u}\\cdot\\delta\\alpha^{\\mathrm{consf}}\\delta_{\\mathrm{fin}}\\left(\\frac{1}{\\beta},\\mathbf{M}_{0}\\right)\\cdot\\kappa(\\mathbf{M}_{0}),}\\\\ &{\\left|\\left|\\mathbf{E}_{121}\\right|\\leq\\mathbf{u}\\cdot\\mathcal{E}_{\\mathrm{BV}}\\left(\\frac{1}{\\beta},\\mathbf{M}\\right)\\cdot\\left[\\kappa(\\mathbf{M})+\\left(2+\\mathbf{u}\\cdot\\delta_{\\mathrm{fin}}\\left(\\frac{1}{\\beta},\\mathbf{M}\\right)\\right)^{2}\\right]\\cdot\\left|\\left|\\mathbf{L}_{11}\\right|}\\\\ &{\\qquad\\qquad\\leq\\mathbf{u}\\cdot\\pi^{\\mathrm{consf}}\\delta_{\\mathrm{fin}}\\left(\\frac{1}{\\beta},\\mathbf{M}_{0}\\right)\\cdot\\left(2\\kappa(\\mathbf{M}_{0})+9\\right)\\cdot\\left|\\left|\\mathbf{L}_{11}\\right|}\\\\ &{\\qquad\\qquad\\leq\\mathbf{u}\\cdot\\pi^{\\mathrm{consf}}\\delta_{\\mathrm{fin}}\\left(\\frac{1}{\\beta},\\mathbf{M}_{0}\\right)\\cdot\\left|\\mathbf{L}_{11}\\right|,}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbf{u}\\cdot\\delta\\alpha^{\\mathrm{consf}}\\delta \n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using those inequalities, the first term that we need to bound from Lemma C.1 is the following ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{L}_{21}\\mathbf{L}_{11}^{*}-\\mathbf{B}\\|\\leq\\mathbb{(}\\kappa(\\mathbf{M})+\\|\\mathbf{E}_{8\\lambda}(t)\\|\\mathbf{er}(\\mu)2+\\|\\mathbf{E}_{8\\lambda}(\\mathbf{\\|M})\\|\\mathbf{M}\\|+\\|\\mathbf{E}_{3}^{\\mathrm{BA}}\\|(\\mathbf{L}_{11}^{*}\\|)}\\\\ {\\leq\\big(2\\kappa(\\mathbf{M}_{0})+\\mathbf{u}\\cdot6\\operatorname{nst}\\mathcal{L}_{8\\lambda}(\\mathbf{\\hat{s}}_{0}^{\\top},\\mathbf{M}_{0})\\cdot\\kappa(\\mathbf{M}_{0})\\big)\\mathrm{er}(\\mu/2)}\\\\ {\\quad}&{\\qquad\\therefore+\\mathbf{u}\\cdot6\\operatorname{nst}^{\\mathrm{orot}}\\xi_{\\mathrm{M}0}\\cdot\\mathbf{M}_{0}\\cdot\\kappa(\\mathbf{M}_{0})\\cdot2\\|\\mathbf{M}_{0}\\|}\\\\ {\\quad}&{\\qquad\\dots+\\mathbf{u}\\cdot5\\cdot\\boldsymbol{\\mathcal{L}}_{1}^{\\mathrm{A}}\\left(\\frac{\\mathbf{\\hat{s}}_{0}}{2},\\mathbf{M}_{0}\\right)\\cdot\\frac{\\big[\\mathbf{L}_{11}\\big]\\big\\|\\mathbf{M}_{1}^{*}\\big\\|}{\\xi^{2}+\\mathbf{M}_{0}^{2}\\cdot\\mathbf{M}_{0}^{2}}}\\\\ {\\quad}&{\\qquad\\qquad\\dots+\\mathbf{u}\\cdot5\\cdot\\boldsymbol{\\mathcal{L}}_{1}^{\\mathrm{A}}\\left(\\frac{\\mathbf{\\hat{s}}_{0}}{2},\\mathbf{M}_{0}\\right)\\cdot\\frac{\\big[\\mathbf{L}_{0}\\mathbf{M}_{1}\\big]\\big\\|\\mathbf{M}_{1}^{*}\\big\\|}{\\xi^{2}+\\mathbf{M}_{0}^{2}\\cdot\\mathbf{M}_{0}^{2}}}\\\\ {\\leq\\big(2\\kappa(\\mathbf{M}_{0})+\\mathbf{u}\\cdot6\\operatorname{nst}^{\\mathrm{orot}}\\xi_{\\mathrm{M}0}\\cdot\\big(\\frac{\\mathbf{\\hat{s}}_{0}}{2},\\mathbf{M}_{0}\\big)\\cdot\\kappa(\\mathbf{M}_{0})\\big)\\mathrm{er}(\\mu/2)}\\\\ {\\quad}&{\\qquad\\dots+\\mathbf{u}\\cdot12\\kappa^{\\mathrm{tor}}\\xi_{\\mathrm{W}0}\\left(\\frac{\\mathbf{\\hat{s}}_{0}}{2},\\mathbf \n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where in the last inequality we used the assumed bound for $\\mathbf{u}$ , namely Inequality (13). Similarly ", "page_idx": 41}, {"type": "text", "text": "| + IIEsIl $\\begin{array}{r l}&{\\mathbf{S}^{\\mu\\nu}(\\mathbf{\\hat{A}},\\mathbf{A})=w^{\\nu\\rho}(\\mathbf{\\hat{A}},\\mathbf{\\hat{A}})}\\\\ &{\\quad-\\nu\\frac{1}{4}(4+\\pi)\\alpha^{2}\\sin\\theta_{0}^{\\prime}\\sin\\theta_{1}\\sin\\theta_{1}^{\\prime}(\\mathbf{\\hat{A}},\\mathbf{\\hat{A}})+\\nu\\alpha(\\hat{\\hat{A}})^{\\nu\\rho}(\\mathbf{\\hat{A}})}\\\\ &{\\quad-\\nu\\frac{1}{4}(4+\\pi)\\alpha^{2}\\cos(\\theta_{1})\\sin\\theta_{1}\\sin\\theta_{1}^{\\prime}(\\mathbf{\\hat{A}},\\mathbf{\\hat{A}})+\\nu\\alpha(\\hat{\\hat{A}})^{\\nu\\rho}(\\mathbf{\\hat{A}})}\\\\ &{\\quad-\\nu\\frac{1}{4}(4+\\pi)\\alpha^{2}\\cos(\\theta_{1})\\sin\\theta_{1}\\sin\\theta_{1}^{\\prime}(\\mathbf{\\hat{A}},\\mathbf{\\hat{A}})}\\\\ &{\\quad\\quad-\\nu\\frac{1}{4}(4\\pi)\\alpha^{2}\\sin(\\theta_{1})\\sin\\theta_{1}^{\\prime}\\sin\\theta_{1}^{\\prime}}\\\\ &{\\quad\\quad-\\nu\\frac{1}{4}(4\\pi)\\alpha^{2}\\sin(\\theta_{1})\\sin\\theta_{1}^{\\prime}\\sin\\theta_{1}^{\\prime}}\\\\ &{\\quad\\quad-\\nu\\frac{1}{4}(4(\\pi)\\alpha^{2}-4\\pi)\\alpha^{2}\\cos(\\theta_{1})\\sin\\theta_{1}^{\\prime}\\sin\\theta_{1}^{\\prime}}\\\\ &{\\quad\\quad-\\nu\\cos(\\theta_{1})\\alpha^{2}\\sin(\\theta_{1})\\sin\\theta_{1}}\\\\ &{\\quad\\quad-\\nu\\frac{1}{4}(2\\pi)\\alpha^{2}\\sin^{2}(\\theta_{1})\\sin(\\theta_{1})\\sin\\theta_{1}^{\\prime}}\\\\ &{\\quad\\quad-\\nu+\\nu\\frac{1}{4}(4\\pi)\\alpha^{3}\\cos(\\theta_{1})\\sin\\theta_{1}^{\\prime}\\sin\\theta_{1}^{\\prime}}\\\\ &{\\quad\\quad-\\nu\\frac{1}{4}(4+\\pi)\\alpha^{2}\\sin(\\theta_{1})\\sin(\\theta_{1})\\sin\\theta_{1}^{\\prime}}\\\\ &{\\quad\\quad-\\nu\\frac{1}{4}(4+\\pi)\\alpha^ $ where in the last inequality we used again (13). ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "We can now derive a recursive formula for the main error bounds, Inequality (10)) ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{err}(n)=\\|\\mathbf{LL^{*}}-\\mathbf{M}\\|}\\\\ &{\\qquad\\qquad\\leq\\|\\mathbf{L_{11}L_{11}^{*}}-\\mathbf{A}\\|+\\|\\mathbf{L_{21}L_{11}^{*}}-\\mathbf{B}\\|+\\|\\mathbf{L_{21}L_{21}^{*}}+\\mathbf{L_{22}L_{22}^{*}}-\\mathbf{C}\\|}\\\\ &{\\qquad\\leq\\mathbf{err}(n/2)+\\cdot\\cdot\\cdot}\\\\ &{\\qquad\\quad\\cdot\\cdot+22\\cdot\\kappa(\\mathbf{M}_{0})\\cdot\\mathbf{err}(n/2)+\\mathbf{u}\\cdot28n^{c_{\\mathrm{INV}}}\\mathcal{E}_{|\\mathrm{NV}}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)\\cdot\\kappa(\\mathbf{M}_{0})\\cdot\\|\\mathbf{M}_{0}\\|+\\cdot\\cdot}\\\\ &{\\qquad\\qquad\\cdot\\cdot+173\\cdot\\kappa(\\mathbf{M}_{0})^{2}\\cdot\\mathbf{err}(n/2)+\\mathbf{u}\\cdot394\\kappa(\\mathbf{M}_{0})^{2}n^{c_{\\mathrm{INV}}}\\mathcal{E}_{|\\mathrm{NV}}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)\\cdot\\|\\mathbf{M}_{0}\\|}\\\\ &{\\qquad\\leq196\\kappa(\\mathbf{M}_{0})^{2}\\cdot\\mathbf{err}(n/2)+\\mathbf{u}\\cdot422\\kappa(\\mathbf{M}_{0})^{2}n^{c_{\\mathrm{INV}}}\\mathcal{E}_{|\\mathrm{NV}}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)\\cdot\\|\\mathbf{M}_{0}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The base case $n\\,=\\,1$ is trivial since $\\mathbf{M}$ is just a positive real number? and we can compute $\\mathbf{L}=$ ${\\sf f l}(\\sqrt{\\mathbf{M}})=(1+\\delta)\\sqrt{\\mathbf{M}}$ with $\\lvert\\delta\\rvert\\leq\\mathbf{u}$ which means that $\\mathbf{L}\\mathbf{L}^{*}=\\mathbf{L}^{2}=(1+\\delta)^{2}\\mathbf{M}>0$ and ", "page_idx": 42}, {"type": "equation", "text": "$$\n|\\mathbf{M}-\\mathbf{L}\\mathbf{L}^{*}|=|\\mathbf{M}(2\\delta+\\delta^{2})|\\leq3\\mathbf{u}\\|\\mathbf{M}\\|\\leq6\\mathbf{u}\\|\\mathbf{M}_{0}\\|.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "If we denote $\\alpha=196\\kappa(\\mathbf{M}_{0})^{2}$ and $\\begin{array}{r}{\\beta=\\mathbf{u}\\cdot422\\kappa(\\mathbf{M}_{0})^{2}n^{c_{\\mathsf{I N V}}}\\mathcal{E}_{\\mathsf{I N V}}\\left(\\frac{n}{2},\\mathbf{M}_{0}\\right)}\\end{array}$ , then the solution of the recursion can be written as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{rr}(n)\\leq\\alpha^{\\log n}\\mathbf{err}(1)+\\beta\\|\\mathbf{M_{0}}\\|\\sum_{i=0}^{\\log n-1}\\alpha^{i}}\\\\ &{\\qquad\\leq\\alpha^{\\log n}\\mathfrak{d}_{\\mathbf{u}}\\|\\mathbf{M_{0}}\\|+\\log(n)\\alpha^{\\log n}\\beta\\|\\mathbf{M_{0}}\\|}\\\\ &{\\qquad=\\mathbf{u}n^{c_{1}}\\kappa(\\mathbf{M_{0}})^{2\\log n}\\|\\mathbf{M_{0}}\\|+\\log(n)n^{c_{1}}\\kappa(\\mathbf{M_{0}})^{2\\log n}\\mathbf{u}\\cdot422\\kappa(\\mathbf{M_{0}})^{2}n^{c_{\\mathrm{uv}}}\\mathcal{E}_{|\\mathbf{M}\\vee}\\left(\\frac{n}{2},\\mathbf{M_{0}}\\right)\\cdot\\|\\mathbf{M_{0}}\\|}\\\\ &{\\qquad=\\mathbf{u}\\cdot c_{4}n^{c_{2}}\\kappa(\\mathbf{M_{0}})^{c_{3}\\log n}\\cdot\\|\\mathbf{M_{0}}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We can now absorb the remaining terms inside $\\mathbf{u}$ , by tuning the constants of Inequality (13), to argue that we can achieve any desired (multiplicative) backward-accuracy $\\epsilon\\in(0,1)$ and if we set ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq\\epsilon\\frac{1}{c_{1}^{\\prime}n^{c_{2}^{\\prime}}\\kappa(\\mathbf{M}_{0})^{c_{3}^{\\prime}\\log n}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "for some constants $c_{1}^{\\prime},c_{2}^{\\prime},c_{3}^{\\prime}$ . This translates to the advertised ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log(1/\\mathbf{u})=O\\left(\\log(n^{c_{2}^{\\prime}}\\kappa(\\mathbf{M}_{0})^{c_{3}^{\\prime}\\log n}/\\epsilon)\\right)=O\\left(\\log(n)\\log(\\kappa(\\mathbf{M}_{0}))+\\log(\\frac{1}{\\epsilon})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "bits of precision. ", "page_idx": 42}, {"type": "text", "text": "C.4  Reducing the GEP to a regular Hermitian eigenproblem ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We can now use CHOLESKY to reduce the definite GEP to a regular Hermitian eigenproblem For the rest of the paper, we assume that $\\mathbf{H}\\lVert,\\lVert\\mathbf{S}^{-1}\\rVert,\\lVert\\mathbf{S}^{-1}\\mathbf{H}\\rVert\\leq1$ . This is not a limitation since we can approximate the norms of $\\mathbf{H}$ and $\\mathbf{S}^{-1}$ in floating point using the algorithm SIGMAK, which is described later, and then scale accordingly. Formally, let $\\eta\\gtrsim\\|\\mathbf{H}\\|$ and $\\sigma\\gtrsim\\|\\mathbf{S}^{-1}\\|$ . Then we can rewrite the generalized eigenproblem ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{H}\\mathbf{C}=\\mathbf{S}\\mathbf{C}\\boldsymbol{\\Lambda}\\quad\\Leftrightarrow\\quad(\\frac{1}{\\eta}\\mathbf{H})\\mathbf{C}=(\\sigma\\mathbf{S})\\mathbf{C}(\\boldsymbol{\\Lambda}\\frac{1}{\\eta\\sigma})\\quad\\Leftrightarrow\\quad\\mathbf{H}^{\\prime}\\mathbf{C}=\\mathbf{S}^{\\prime}\\mathbf{C}\\boldsymbol{\\Lambda}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "i.e. it is the same generalized eigenproblem only with scaled eigenvalues. We can thus safely make the unit-norms assumption. In Proposition C.3 we prove the properties of the reduction, which, in brief, states that we can compute in $O(T_{\\mathsf{M M}}(n))$ floating point operations using $O(\\log(n)\\log(\\kappa(\\mathbf{S}))\\,+\\,\\log(1/\\epsilon))$ bits  the  matrix $\\mathbf{\\widetilde{H}}\\ \\gets\\ \\widetilde{\\mathbf{L}}^{*}\\mathbf{H}\\widetilde{\\mathbf{L}}$ \uff0cwhere $\\widetilde{\\bf L}$ is returned by $\\mathsf{C H O L E S K Y}(\\mathsf{I N V}(\\mathbf{S}))$ . The eigenvalues of H provably approximate the pencil eigenvalues: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|\\lambda_{i}(\\widetilde{\\mathbf{H}})-\\lambda_{i}(\\mathbf{H},\\mathbf{S})\\right|\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\epsilon\\in(0,1)$ is a given accuracy. Algorithm 4 details the reduction. ", "page_idx": 42}, {"type": "text", "text": "REDUCE ", "page_idx": 43}, {"type": "text", "text": "Input: Matrix $\\mathbf{H}\\in\\mathbb{H}^{n}$ ,matrix $\\mathbf{S}\\in\\mathbb{H}_{++}^{n}$ , accuracy parameter $\\epsilon\\in(0,1)$ ", "page_idx": 43}, {"type": "text", "text": "Requires: $\\mathbf{H}$ is Hermitian and $\\|\\mathbf{H}\\|\\leq1$ , S is Hermitian and positive definite and $\\lVert\\mathbf{S}^{-1}\\rVert\\leq1$ ", "page_idx": 43}, {"type": "text", "text": "Algorithm: $\\widetilde{\\mathbf{H}}\\gets\\mathsf{R E D U C E}(\\mathbf{H},\\mathbf{S})$ ", "page_idx": 43}, {"type": "text", "text": "1: $\\mathbf{S}_{1\\mathsf{N V}}\\gets\\mathsf{H E R M}(\\mathsf{I N V}(\\mathbf{S}))$   \n2: $\\widetilde{\\mathbf{L}}\\gets\\mathsf{C H O L E S K Y}(\\mathbf{S}_{\\mathsf{I N V}})$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{H}}\\gets\\mathsf{H E R M}(\\mathsf{M M}(\\mathsf{M M}(\\widetilde{\\mathbf{L}}^{*},\\mathbf{H}),\\widetilde{\\mathbf{L}}))\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "4: return $\\widetilde{\\mathbf{H}}$ ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Output: Hermitian matrix $\\widetilde{\\mathbf{H}}$ ", "page_idx": 43}, {"type": "text", "text": "Ensures: $|\\lambda_{i}(\\widetilde{\\mathbf{H}})-\\lambda_{i}(\\mathbf{S}^{-1}\\mathbf{H})|\\,\\le\\,\\epsilon$ in $O(T_{\\mathsf{M M}}(n))$ foating point operations using $O(\\log(n)\\log(\\kappa(\\mathbf{S}))+$ $\\log(1/\\epsilon))$ bits. ", "page_idx": 43}, {"type": "text", "text": "Algorithm 4: REDUCE. ", "page_idx": 43}, {"type": "text", "text": "Proposition C.3. Given $\\mathbf{H}\\in\\mathbb{H}^{n}$ \uff0c $\\mathbf{S}\\,\\in\\,\\mathbb{H}_{++}^{n}$ ,and $\\epsilon\\,\\in\\,(0,1)$ , Algorithm $^{4}$ executes $O(T_{\\mathsf{M M}}(n))$ foating point operations and returns a matrix $\\widetilde{\\mathbf{H}}\\gets$ REDUCE $(\\mathbf{H},\\mathbf{S},\\epsilon)$ ,such that, if the machine precision satisfies ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq\\mathbf{u}_{\\mathsf{R E D U C E}}:=\\epsilon\\frac{1}{\\rho_{1}n^{\\rho_{2}}(4\\kappa(\\mathbf{S}))^{\\rho_{3}\\log(n)}},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for some constants $\\rho_{1},\\rho_{2},\\rho_{3}$ ,and $\\epsilon\\in(0,1)$ , which translates to ", "page_idx": 43}, {"type": "equation", "text": "$$\nO\\left(\\log(1/\\mathfrak{u}_{\\sf R E D U C E})\\right)=O\\left(\\log(n)\\log(\\kappa(\\mathbf{S}))+\\log(1/\\epsilon)\\right)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "required bits of precision, then for all $i\\in[n]$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n|\\lambda_{i}(\\widetilde{\\mathbf{H}})-\\lambda_{i}(\\mathbf{H},\\mathbf{S})|\\le\\epsilon\\|\\mathbf{S}^{-1}\\|\\|\\mathbf{H}\\|,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\lambda_{i}(\\mathbf{H},\\mathbf{S})$ are the eigenvalues of the Hermitian definite pencil $(\\mathbf{H},\\mathbf{S})$ ", "page_idx": 43}, {"type": "text", "text": "Proof. Let $\\mathbf{L}$ be the (exact) lower triangular Cholesky factor of $\\mathbf{S}^{-1}$ ,i.e. $\\mathbf{L}\\mathbf{L}^{*}\\,=\\,\\mathbf{S}^{-1}$ (which is unique up to column phases). Expanding the equation in line 3 of Algorithm 4 we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Lambda(\\widetilde{\\mathbf{H}})=\\Lambda\\left(\\widetilde{\\mathbf{L}}^{*}\\mathbf{H}\\widetilde{\\mathbf{L}}+\\mathbf{E}_{5}\\right)}\\quad}&{}\\\\ &{=\\Lambda\\left(\\widetilde{\\mathbf{L}}\\widetilde{\\mathbf{L}}^{*}\\mathbf{H}+\\widetilde{\\mathbf{L}}\\mathbf{E}_{5}\\widetilde{\\mathbf{L}}^{-1}\\right)}\\\\ &{=\\Lambda\\left(({\\mathbf{S}}^{-1}+{\\mathbf{E}}_{1}^{|\\mathbb{N}|}+{\\mathbf{E}}_{2}^{\\mathbb{C}|})\\mathbf{H}+\\widetilde{\\mathbf{L}}\\mathbf{E}_{5}\\widetilde{\\mathbf{L}}^{-1}\\right)}\\\\ &{=\\Lambda\\left({\\mathbf{S}}^{-1}\\mathbf{H}+{\\mathbf{E}}_{1}^{|\\mathbb{N}|}\\mathbf{H}+{\\mathbf{E}}_{2}^{\\mathbb{C}|}\\mathbf{H}+\\widetilde{\\mathbf{L}}\\mathbf{E}_{5}\\widetilde{\\mathbf{L}}^{-1}\\right)}\\\\ &{=\\Lambda\\left({\\mathbf{L}}^{*}\\mathbf{H}\\mathbf{L}+{\\mathbf{L}}^{-1}\\mathbf{E}_{1}^{|\\mathbb{N}|}\\mathbf{H}\\mathbf{L}+{\\mathbf{L}}^{-1}\\mathbf{E}_{2}^{\\mathbb{C}|}\\mathbf{H}\\mathbf{L}+{\\mathbf{L}}^{-1}\\widetilde{\\mathbf{L}}\\mathbf{E}_{5}\\widetilde{\\mathbf{L}}^{-1}\\mathbf{L}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "At the same time, $\\Lambda(\\mathbf{S}^{-1}\\mathbf{H})=\\Lambda(\\mathbf{L}^{*}\\mathbf{H}\\mathbf{L})$ . Combining this with Kahan's bound from Fact A.1 and denoting ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf B}={\\bf L}^{-1}{\\bf E}_{1}^{\\sf I N V}{\\bf H}{\\bf L}+{\\bf L}^{-1}{\\bf E}_{2}^{\\sf C H}{\\bf H}{\\bf L}+{\\bf L}^{-1}\\widetilde{\\bf L}{\\bf E}_{5}\\widetilde{{\\bf L}}^{-1}{\\bf L},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "we have that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\Big|\\lambda_{i}(\\mathbf{H},\\mathbf{S})-\\lambda_{i}(\\widetilde{\\mathbf{H}})\\Big|=|\\lambda_{i}(\\mathbf{L}^{*}\\mathbf{H}\\mathbf{L})-\\lambda_{i}(\\mathbf{L}^{*}\\mathbf{H}\\mathbf{L}+\\mathbf{B})|\\leq C\\|\\mathbf{B}\\|\\log(n),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for some constant $C$ . It remains to bound for $\\|\\mathbf B\\|$ ", "page_idx": 43}, {"type": "text", "text": "$\\mathbf{E}_{1}^{\\mathsf{I N V}}$ origiatesfrom the ivrsion of $\\operatorname{\\mathbf{S}}_{\\mathsf{I N V}}$ Let $\\mathbf{S}_{\\mathsf{I N V}}\\,=\\,\\mathsf{H E R M}(\\mathsf{I N V}(\\mathbf{S}))\\,=\\,\\mathbf{S}^{-1}+\\mathbf{E}_{1}^{\\mathsf{I N V}}$ From Corollary A.3, we know that as long as the machine precision satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbf{u}_{\\mathsf{R E D U C E}}\\leq\\delta\\frac{1}{c_{\\mathsf{H E R M}}\\log(n)\\mu_{\\mathsf{N V}}(n)\\kappa(\\mathbf{S})^{c_{\\mathsf{I N V}}\\log(n)+1}},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for some $\\delta~\\in~(0,1/2)$ , then all of the following hold: $\\lVert{\\bf E}_{1}^{\\sf I N V}\\rVert\\ \\leq\\ \\delta\\lVert{\\bf S}^{-1}\\rVert$ ,s-\u2264 $\\|\\mathbf{S}_{\\mathsf{I N V}}\\|\\leq2\\|\\mathbf{S}^{-1}\\|$ $\\begin{array}{r}{\\frac14\\kappa(\\mathbf{S})\\leq\\kappa(\\mathbf{S}_{\\mathsf{I N V}})\\leq4\\kappa(\\mathbf{S})}\\end{array}$ ", "page_idx": 44}, {"type": "text", "text": "(i)As a second step, the Cholesky factor $\\widetilde{\\mathbf{L}}=\\mathsf{C H O L E S K Y}(\\mathbf{S}_{\\mathsf{I N V}})$ is computed, such that $\\widetilde{\\mathbf{L}}\\widetilde{\\mathbf{L}}^{*}=$ ${\\bf S}_{\\mathsf{I N V}}+{\\bf E}_{2}^{\\mathsf{C H}}$ From Theorem 1.2 we know tat a long as ", "page_idx": 44}, {"type": "equation", "text": "$$\n{\\sf u}_{\\sf R E D U C E}\\le\\delta\\frac{1}{c_{1}n^{c_{2}}\\kappa\\big({\\bf S}_{|{\\sf N V}}\\big)^{c_{3}\\log(n)}},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for some constants $c_{1},c_{2},c_{3}$ then $\\|\\mathbf{E}_{2}^{\\mathrm{CH}}\\|\\leq\\delta\\|\\mathbf{S}_{\\sf I N V}\\|\\leq2\\delta\\|\\mathbf{S}^{-1}\\|$ . From the bound of $\\kappa(\\mathbf{S}_{\\mathsf{I N V}})$ in $(i)$ , the following is sufficient: ", "page_idx": 44}, {"type": "equation", "text": "$$\n{\\boldsymbol{\\mathsf{u}}}_{\\mathsf{R E D U C E}}\\leq\\delta\\frac{1}{c_{1}n^{c_{2}}\\left(4\\kappa({\\boldsymbol{\\mathsf{S}}})\\right)^{c_{3}\\log(n)}},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "(ii) Finally, we form the matrix $\\widetilde{\\mathbf{H}}\\;=\\;\\mathsf{H E R M}\\left(\\mathsf{M M}(\\mathsf{M M}(\\widetilde{\\mathbf{L}},\\mathbf{H}),\\widetilde{\\mathbf{L}}^{*})\\right)\\;=\\;\\widetilde{\\mathbf{L}}^{*}\\mathbf{H}\\widetilde{\\mathbf{L}}\\;+\\;\\mathbf{E}_{3}^{\\mathsf{M M}}\\widetilde{\\mathbf{L}}\\;+$ ${\\bf E}_{4}^{\\sf M M}={\\bf E}_{3}$ .Using Theorem A.2 and Corollary A2, as long as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbf{u}_{\\mathsf{R E D U C E}}\\leq\\delta\\frac{1}{\\mu(n)}=\\delta\\frac{1}{c_{\\mathsf{H E R M}}\\log(n)\\mu_{\\mathsf{M M}}(n)},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{E}_{\\delta}\\rVert-\\rVert\\bar{\\mathbf{H}}-\\dot{\\mathbf{E}}^{\\mathrm{H}}\\rVert_{1}^{2}}&{=\\rVert\\frac{1}{N}\\mathbf{E}_{\\delta}^{\\mathrm{H}}+\\rVert\\hat{\\mathbf{E}}_{\\delta}^{\\mathrm{H}}\\rVert_{1}^{2}+\\underbrace{\\mathrm{H}_{\\delta}^{\\mathrm{H}}-\\hat{\\mathbf{E}}_{\\delta}^{\\mathrm{H}}\\rVert_{1}^{2}}_{\\le0}}\\\\ &{=\\rVert\\mathbf{E}_{\\delta}^{\\mathrm{H}}\\rVert_{1}^{2}+\\underbrace{\\mathrm{K}_{\\delta}^{\\mathrm{H}}}+\\rVert\\mathbf{E}_{\\delta}^{\\mathrm{H}}\\rVert_{1}^{2}}\\\\ &{\\le\\operatorname*{sup}(\\delta)\\rVert\\mathcal{E}^{\\mathrm{H}}\\rVert^{1}}\\\\ &{\\le\\quad\\left[\\frac{\\sqrt{N^{2}+M_{\\theta}^{2}}}{N^{2}}\\right]\\rVert\\frac{1}{N}\\rVert\\mathrm{H}}\\\\ &{\\le\\rVert\\operatorname*{max}_{\\theta\\le n+1}\\rVert\\nabla_{\\theta}\\rVert\\nabla_{\\theta}\\rVert\\nabla_{\\theta}\\rVert\\left[\\frac{M_{\\theta}^{2}}{N}+M_{\\theta}^{2}(\\delta)\\right]+\\underbrace{\\mathrm{H}_{\\delta}^{\\mathrm{H}}-\\rVert\\nabla_{\\theta}\\rVert_{1}^{2}}_{\\le0}+\\underbrace{\\mathrm{H}_{\\delta}^{\\mathrm{H}}}+\\rVert\\bar{\\mathbf{E}}_{\\delta}^{\\mathrm{H}}}\\\\ &{\\le\\rVert\\alpha_{\\theta}-(\\delta)\\rVert\\left[\\frac{1}{N}\\right]\\rVert\\mathrm{H}_{1}^{2}+\\underbrace{\\mathrm{H}_{\\theta}(n)}\\rVert\\nabla_{\\theta}\\rVert\\left[\\frac{M_{\\theta}^{2}}{N}\\right]+\\underbrace{\\mathrm{H}_{\\theta}^{\\mathrm{H}}}+\\rVert\\nabla_{\\theta}\\rVert\\nabla_{\\theta}\\rVert\\nabla_{1}^{2}\\rVert\\mathrm{H}}\\\\ &{\\le2\\pi\\cdot\\rVert\\left(\\frac{M_{\\theta}^{2}}{N}\\right)\\rVert\\left[\\frac{M_{\\theta}^{2}}{N}\\right]\\rVert\\mathrm{H}\\rVert+\\rVert\\alpha_{\\theta}n+\\left(\\eta\\right)\\rVert\\dot{\\nabla}_{1}^{2}\\rVert\\mathrm{H}\\rVert-\\rVert\\hat{\\mathbf{E}}_{\\delta}^{\\mathrm{H}}}\\\\ &{=\\rVert\\alpha_{\\theta}-(\\delta)\\rVert \n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "(iv) For the final desired error we need to bound $\\|\\widetilde{\\mathbf{L}}\\|\\|\\widetilde{\\mathbf{L}}^{-1}\\|$ . Taking the square: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(\\|\\widetilde{\\mathbf{L}}\\|\\|\\widetilde{\\mathbf{L}}^{-1}\\|)^{2}=\\|\\widetilde{\\mathbf{L}}\\|^{2}\\|\\widetilde{\\mathbf{L}}^{-1}\\|^{2}=\\|\\widetilde{\\mathbf{L}}\\widetilde{\\mathbf{L}}^{*}\\|\\|(\\widetilde{\\mathbf{L}}\\widetilde{\\mathbf{L}}^{*})^{-1}\\|=\\|\\mathbf{S}^{\\mathrm{INV}}+\\mathbf{E}_{2}^{\\mathrm{CH}}\\|\\|(\\mathbf{S}^{\\mathrm{INV}}+\\mathbf{E}_{2}^{\\mathrm{CH}})^{-1}\\|}\\\\ &{}&{=\\kappa(\\mathbf{S}^{\\mathrm{INV}}+\\mathbf{E}_{2}^{\\mathrm{CH}}).\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For the spectral norm we have that $\\begin{array}{r}{\\|\\mathbf{S}_{\\sf I N V}\\!+\\!\\mathbf{E}_{2}^{\\mathrm{CH}}\\|\\leq\\|\\mathbf{S}_{\\sf I N V}\\|\\!+\\!\\|\\mathbf{E}_{2}^{\\mathrm{CH}}\\|\\leq2\\|\\mathbf{S}^{-1}\\|\\!+\\!2\\delta\\|\\mathbf{S}^{-1}\\|\\leq}\\end{array}$ $\\mathbf{\\nabla}3\\|\\mathbf{S}^{-1}\\|$ . For the spectral norm of the inverse we need to bound the smallest singular value of ", "page_idx": 44}, {"type": "text", "text": "$\\mathbf{S}^{\\mathsf{I N V}}+\\mathbf{E}_{2}^{\\mathsf{C H}}$ frombelow: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{\\operatorname*{min}}\\big({\\mathbf S}^{|\\mathrm{NV}}+{\\mathbf E}_{2}^{\\mathrm{CH}}\\big)\\geq\\sigma_{\\operatorname*{min}}\\big({\\mathbf S}^{|\\mathrm{NV}}\\big)-\\|{\\mathbf E}_{2}^{\\mathrm{CH}}\\|\\geq\\frac{1}{2}\\sigma_{\\operatorname*{min}}\\big({\\mathbf S}^{-1}\\big)-2\\delta\\|{\\mathbf S}^{-1}\\|=\\frac{1}{2\\|{\\mathbf S}\\|}-2\\delta\\|{\\mathbf S}^{-1}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1-4\\delta\\kappa({\\mathbf S})}{2\\|{\\mathbf S}\\|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Combining the last two bounds, that we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\kappa({\\bf S}^{\\mathsf{I N V}}+{\\bf E}_{2}^{\\mathsf{C H}})\\leq\\frac{3\\|{\\bf S}^{-1}\\|}{\\frac{1-4\\delta\\kappa({\\bf S})}{2\\|{\\bf S}\\|}}=\\frac{6\\kappa({\\bf S})}{1-4\\delta\\kappa({\\bf S})}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We now have bounds for all the required quantities to bound $\\|\\mathbf B\\|$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\mathbf{B}}\\right\\|=\\left\\|{\\mathbf{L}}^{-1}{\\mathbf{E}}_{1}^{\\mathrm{W}}{\\mathbf{H}}{\\mathbf{L}}+{\\mathbf{L}}^{-1}{\\mathbf{E}}_{2}^{\\mathrm{CH}}{\\mathbf{H}}{\\mathbf{L}}+{\\mathbf{L}}^{-1}\\widetilde{{\\mathbf{L}}}{\\mathbf{E}}_{5}\\widetilde{{\\mathbf{L}}}^{-1}{\\mathbf{L}}\\right\\|}\\\\ &{\\qquad\\leq\\sqrt{\\kappa({\\mathbf{S}})}\\left\\|{\\mathbf{E}}_{1}^{\\mathrm{WW}}{\\mathbf{H}}+{\\mathbf{E}}_{2}^{\\mathrm{CH}}{\\mathbf{H}}+\\widetilde{{\\mathbf{L}}}{\\mathbf{E}}_{5}\\widetilde{{\\mathbf{L}}}^{-1}\\right\\|}\\\\ &{\\qquad\\leq\\sqrt{\\kappa({\\mathbf{S}})}\\left(\\|{\\mathbf{E}}_{1}^{\\mathrm{WW}}\\|\\|{\\mathbf{H}}\\|+\\|{\\mathbf{E}}_{2}^{\\mathrm{CH}}{\\mathbf{H}}\\|+\\|{\\mathbf{E}}_{5}\\|\\|\\widetilde{{\\mathbf{L}}}\\|\\|\\widetilde{{\\mathbf{L}}}^{-1}\\|\\right)}\\\\ &{\\qquad\\leq\\sqrt{\\kappa({\\mathbf{S}})}\\left(\\delta\\|{\\mathbf{S}}^{-1}\\|\\|{\\mathbf{H}}\\|+2\\delta\\|{\\mathbf{S}}^{-1}\\|\\|{\\mathbf{H}}\\|+12\\delta\\|{\\mathbf{S}}^{-1}\\|\\|{\\mathbf{H}}\\|\\sqrt{\\frac{6\\kappa({\\mathbf{S}})}{1-4\\delta\\kappa({\\mathbf{S}})}}\\right)}\\\\ &{\\qquad\\leq\\delta\\sqrt{\\kappa({\\mathbf{S}})}\\|{\\mathbf{S}}^{-1}\\|\\|{\\mathbf{H}}\\|\\left(3+12\\sqrt{\\frac{6\\kappa({\\mathbf{S}})}{1-4\\delta\\kappa({\\mathbf{S}})}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Setting $\\begin{array}{r}{\\delta=\\epsilon\\frac{1}{64C\\log(n)\\kappa(\\mathbf{S})}}\\end{array}$ , where $C$ istheonstafmqalt $\\epsilon\\in(0,1)$ , ths inaly gives ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{B}\\|\\leq\\epsilon\\frac{1}{64C\\log(n)\\kappa(\\mathbf{S})}\\cdot\\sqrt{\\kappa(\\mathbf{S})}\\|\\mathbf{S}^{-1}\\|\\|\\mathbf{H}\\|\\left(3+12\\sqrt{\\frac{6\\kappa(\\mathbf{S})}{1-4\\epsilon\\frac{1}{64C\\log(n)\\kappa(\\mathbf{S})}\\kappa(\\mathbf{S})}}\\right)}\\\\ &{\\qquad=\\epsilon\\frac{1}{64C\\log(n)\\sqrt{\\kappa(\\mathbf{S})}}\\|\\mathbf{S}^{-1}\\|\\|\\mathbf{H}\\|\\left(3+12\\sqrt{\\frac{6\\kappa(\\mathbf{S})}{1-\\epsilon\\frac{1}{16}}}\\right)}\\\\ &{\\qquad\\leq\\epsilon\\frac{1}{64C\\log(n)\\sqrt{\\kappa(\\mathbf{S})}}\\|\\mathbf{S}^{-1}\\|\\|\\mathbf{H}\\|\\left(3+12\\sqrt{\\frac{96}{15}}\\kappa(\\mathbf{S})\\right)}\\\\ &{\\qquad\\leq\\epsilon\\frac{1}{64C\\log(n)\\sqrt{\\kappa(\\mathbf{S})}}\\|\\mathbf{S}^{-1}\\|\\|\\mathbf{H}\\|\\left(3+36\\sqrt{\\kappa(\\mathbf{S})}\\right)}\\\\ &{\\qquad\\leq\\epsilon\\frac{1}{6\\Gamma\\log(n)}\\|\\mathbf{S}^{-1}\\|\\|\\mathbf{H}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "which we can use in Inequality (17) to obtain ", "page_idx": 45}, {"type": "equation", "text": "$$\n|\\lambda_{i}({\\bf H},{\\bf S})-\\lambda_{i}(\\widetilde{\\bf H})|\\leq\\epsilon\\|{\\bf S}^{-1}\\|\\|{\\bf H}\\|.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We can now gather all the requirements for uREDuce. From the above, uREDucE needs to satisfy ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathtt{u}_{\\mathsf{R E D U C E}}\\le\\epsilon\\frac{1}{64C\\log(n)\\kappa(\\mathbf{S})}\\operatorname*{min}\\left\\lbrace\\frac{1}{c_{\\mathsf{H E R M}}\\log(n)\\mu_{\\mathsf{I N V}}(n)\\kappa(\\mathbf{S})^{c_{\\mathsf{I N V}}\\log(n)+1}},\\right.}\\\\ &{\\quad\\left.\\qquad\\qquad\\qquad\\frac{1}{c_{1}n^{c_{2}}\\left(4\\kappa(\\mathbf{S})\\right)^{c_{3}\\log(n)}},\\frac{1}{c_{\\mathsf{H E R M}}\\log(n)\\mu_{\\mathsf{M M}}(n)}\\right\\rbrace}\\\\ &{\\quad\\le\\epsilon\\frac{1}{\\rho_{1}n^{\\rho_{2}}\\left(4\\kappa(\\mathbf{S})\\right)^{\\rho_{3}\\log(n)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for some suitably chosen constants $\\rho_{1},\\rho_{2},\\rho_{3}$ . This translates to ", "page_idx": 45}, {"type": "equation", "text": "$$\nO\\left(\\log(1/\\mathfrak{u}_{\\sf R E D U C E})\\right)=O\\left(\\log(n)\\log(\\kappa(\\mathbf{S}))+\\log(1/\\epsilon)\\right)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "required bits of precision. Having established the eigenvalue bounds, note that $\\lambda_{i}(\\mathbf{H},\\mathbf{S})\\;\\;=$ $\\lambda_{i}(\\mathbf{L}^{*}\\mathbf{H}\\mathbf{L},\\mathbf{I})$ . Since $\\widetilde{\\mathbf{H}}$ and $\\mathbf{L}^{*}\\mathbf{H}\\mathbf{L}$ are both Hermitian, the eigenvalue bound holds also for the singular values. For scaled matrices $\\lVert\\mathbf{S}^{-1}\\rVert,\\lVert\\mathbf{H}\\rVert\\leq1$ we obtain a maximum $\\epsilon$ additive error for the eigenvalues. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "C.5  Counterexample for foating point LU ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "In this section provide a counter-example which illustrates why the backward-stable LU factorization algorithm of [39] is not sufficient to obtain a Cholesky factor for a Hermitian postive-definite matrix. We apply one-by-one the steps of the LUR algorithm of [39] on the matrix: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathbf{A}={\\binom{3}{1}}\\ \\ 1{\\Biggl)}\\,.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "(a) In the first step the matrices $\\mathbf{L}_{L}$ and ${\\bf U}_{L}$ are computed by calling $\\mathsf{L U R}(\\mathbf{A}_{1:2,1})$ , to compute the LUfactorizaoofthefrstfIissysehathisre=\uff09 where $|\\theta_{1}|\\leq\\mathbf{u}$ and $\\mathbf{U}_{L}=3$ (the ${\\bf U}_{L}$ factor is a 1-by-1 matrix in this case). ", "page_idx": 46}, {"type": "text", "text": "(b) The second step updates the upper right corner of $\\mathbf{A}$ by multiplying from the left with the inverse of the top-left block of $\\mathbf{L}_{L}$ . Assuming that multiplication and inversion with the number 1 does not incur errors, then for this specific example $\\mathbf{A}_{1,2}$ is not modified in this step, i.e., it remains equal to 1, without errors.   \n(c) The third step updates the Schur complement, which becomes $\\begin{array}{r l}{\\mathbf{A}_{2,2}\\gets\\mathbf{fl}(\\mathbf{A}_{2,2}-\\frac{1}{3}(1+\\theta_{1}))=}&{{}}\\end{array}$ $\\begin{array}{r}{(3-\\frac{1}{3}(1+\\theta_{1}))(1+\\theta_{2})}\\end{array}$ , where $|\\theta_{2}|\\leq\\mathbf{u}$ is the error term incurred by subtraction.   \n(d)  The next step computes the $\\mathbf{L}_{R},\\mathbf{U}_{R}\\gets\\mathsf{L U R}(\\mathbf{A}_{2,2})$ of the (updated) Schur complement. In this specific case it trivially returns $\\mathbf{L}_{R}=1$ and $\\begin{array}{r}{\\mathbf{U}_{R}=\\mathbf{A}_{2,2}=(3-\\frac{1}{3}(1+\\theta_{1}))(1+\\theta_{2})}\\end{array}$   \n(e) The final steps combine together the left and right LU factors to finally return $\\mathrm{~\\bf~L~}=$ $\\left({1\\atop{1\\atop3}}{\\stackrel{\\textstyle1}{(1+\\theta_{1})}}{\\stackrel{\\textstyle1}{\\quad1}}\\right).$   \n(f) and $\\mathbf{U}=\\binom{3}{0}\\begin{array}{c c}{1}&{}\\\\ {(3-\\frac{1}{3}(1+\\theta_{1}))(1+\\theta_{2})}\\end{array}.$ ", "page_idx": 46}, {"type": "text", "text": "Then, in exact arithmetic, (1+03(1+02+02(1+0)\uff09whichisnotsymmetric. ", "page_idx": 46}, {"type": "text", "text": "D  Regularization with diagonal disorder ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "We now arrive to the more interesting part of the analysis. In Section 2 we argued that if we know the gap and the midpoint then the sign function can yield the desired spectral projector. To compute the gap and midpoint efficiently, we will take advantage of the symmetry induced by the Cholesky-based reduction, and use the Wegner estimate [138] for Hermitian diagonal perturbations to regularize the problem in the spirit of smoothed analysis. We use the following variant of the Wegner estimate. ", "page_idx": 46}, {"type": "text", "text": "Proposition D.1. Let $\\mathbf{G}$ bea random diagonal matrix with independent diagonal elements sampled from ${\\mathcal{N}}(0,\\sigma^{2})$ for $\\begin{array}{r}{\\sigma=\\frac{1}{2\\sqrt{2\\ln({4n}/{\\delta})}}}\\end{array}$ andsome $\\delta\\in(0,1/2)$ . Then for any interval $I\\subset\\mathbb{R}$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Pr}\\left[|\\Lambda(\\mathbf{H}+\\mathbf{G})\\cap I|\\ge1\\right]\\le\\sqrt{4\\pi\\ln(4n/\\delta)}n|I|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\mathbf{H}$ is a fixed Hermitian matrix and $I\\subset\\mathbb{R}$ is a fixed interval. ", "page_idx": 46}, {"type": "text", "text": "Proof. This directly comes from the well-known result of Wegner [138], which states that that for anyinterval $I\\subset\\mathbb{R}$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[|\\Lambda(\\mathbf{H}+\\mathbf{G})|\\cap I|\\geq1\\right]\\leq\\pi\\|\\rho\\|_{\\infty}n|I|\\leq\\sqrt{\\frac{\\pi}{2\\sigma^{2}}}n|I|,\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where llpll = $\\begin{array}{r}{\\|\\rho\\|_{\\infty}=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}}\\end{array}$ is thesuprem the probability densiy. ", "page_idx": 46}, {"type": "text", "text": "Similar results can be obtained for other classes of random matrices $\\mathbf{G}$ .Such an example is the Gaussian Unitary Ensemble (GUE) [4], in which case $\\mathbf{G}$ is dense, but it is invariant under rotations which might be useful for other applications. ", "page_idx": 46}, {"type": "text", "text": "D.1  Sampling Gaussians and perturbing in foating point ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "The next step is to describe how to use such random diagonal perturbations in floating point. Briefly, we first assume that we have a fixed number of bits for the floating point exponent and for the mantissa. If we sample a standard normal Gaussian (in infinite precision), it can happen that it is to0 large to fit in the given number of bits. This is accounted for in the failure probability, which is very small due to the decay of the normal distribution. We describe our sampling method in the following definition, which can be thought of as sampling only the most significant bits of the floating point representation of a Gaussian. ", "page_idx": 47}, {"type": "text", "text": "Definition D.1. A foating point standard normal sampler $\\mathsf{N}(p,t)$ takes as input the number of exponent bits $p,$ and the number ofmantissa bits $t_{;}$ and returns $a$ foating point number ${\\widetilde{g}}\\gets\\mathbf{fl}(g)$ where g is sampled from ${\\mathcal{N}}(0,1)$ Following the definitions of the floating point model in Appendix A.2, if $|g|\\;\\in\\;[\\bar{2}^{-M},2^{M}(2\\,-\\,2^{-t})].$ where $\\dot{M}\\,=\\,\\dot{2}^{p-1}$ ,then $|\\widetilde{g}\\stackrel{.}{-}g|\\,\\le\\,2^{-t}|g|\\,=\\,\\mathbf{u}|g|$ if $|g|\\,>$ $2^{M}(2-\\dot{2}^{-t})$ the sampler returns $\\widetilde{g}=\\pm|\\mathsf{N F}$ i.e. the floating point representation of ifinity, and $i f$ $\\left|g\\right|<2^{-M}$ it returns zero. ", "page_idx": 47}, {"type": "text", "text": "Lemma D.1 (Diagonal Gaussian sampler). Let $\\gamma\\in(0,1/2)$ and $\\sigma>0$ and assume that we want to obtain a foating representation of the matrix $\\gamma\\mathbf{G}$ where $\\mathbf{G}$ is a diagonal matrix with independent diagonal entries from ${\\mathcal{N}}(0,\\sigma^{2})$ .Let $\\mathsf{N}(p,t)$ be a foating point standard normal sampler as in Definition $D.I$ and ${\\bf u}\\,=\\,2^{-t}$ .' Let $\\widetilde{\\sigma}$ be such that $|\\widetilde{\\sigma}\\,-\\,\\sigma|\\,\\leq\\,\\eta_{c}\\sigma$ where $|\\eta_{c}|\\ \\leq\\ \\frac{c\\mathbf{u}}{1-c\\mathbf{u}}$ for some (integer) constant $c>1$ ,and ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{G}}=\\mathrm{diag}(\\widetilde{g}_{1},\\widetilde{g}_{2},\\dots,\\widetilde{g}_{n}),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\widetilde{g}_{i}\\,=\\,\\mathsf{N}(p,t)$ .If $\\begin{array}{r}{p\\,\\geq\\,C\\log(\\log(\\frac{n}{\\delta}))}\\end{array}$ for some global constant $C$ , then the diagonal matrix $\\widetilde{{\\bf V}}={\\bf f}{\\bf l}(\\gamma\\cdot\\widetilde{\\sigma}\\cdot\\widetilde{{\\bf G}})=\\gamma{\\bf G}+{\\bf E},$ hasallthfllowinpropertiewith probabilitya ast $1-\\delta$ ", "page_idx": 47}, {"type": "text", "text": "(i) $\\mathbf{E}$ is diagonal and $\\|\\mathbf{E}\\|\\leq\\gamma\\eta_{c+2}\\sigma\\sqrt{2\\ln(4n/\\delta)},$ ", "page_idx": 47}, {"type": "text", "text": "Proof. Let us first analyze the conditions that are necessary such that all the sampled numbers lie within the foating point bounds from Appendix A.2, specifically, $|g_{i}|\\;\\in\\;[2^{-M},2^{M}(2\\,-\\,2^{-t})]$ $i\\in[n]$ ,where $M\\,{=}\\,2^{p-1}$ . For simplicity we consider $\\left|g_{i}\\right|\\in[2^{-M^{\\cdot}},2^{M}]$ ", "page_idx": 47}, {"type": "text", "text": "For the lower bound, we have that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[|g_{i}|\\leq2^{-M}]\\leq2{\\frac{1}{\\sqrt{2\\pi}}}2^{-M},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where we naively upper bounded the standard normal probability density in the interval $[0,2^{-M}]$ by the constan function $\\frac{1}{\\sqrt{2\\pi}}$ . For $\\begin{array}{r}{M=\\log_{2}\\left(\\frac{2n}{\\delta}\\sqrt{\\frac{2}{\\pi}}\\right)}\\end{array}$ this implie that there ae no subnormal $g_{i}$ for all $i\\in[n]$ simultaneously. This value of $M$ translates to $\\begin{array}{r}{p=O\\left(\\log(\\log(\\frac{n}{\\delta}))\\right)}\\end{array}$ bits. ", "page_idx": 47}, {"type": "text", "text": "For the upper bound, a standard normal tail bound gives ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[|g_{i}|\\geq2^{M}]\\leq2\\exp(-2^{2M}/2).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "For the aforementioned value of $M$ , the probability that each one of the $g_{i}$ is larger than $2^{M}$ is exponentially small in $\\delta/n$ , therefore we can conclude that the sampler does not return any INF values with exponentially high probability. ", "page_idx": 47}, {"type": "text", "text": "This is already enough to argue that the sampler returns foating point numbers that are not subnormal and not INF. However, we will need a tighter bound for the magnitude of the $g_{i}$ to bound the norm of the diagonal random matix. Analyzing Eq (18) for $\\begin{array}{r}{M=\\frac{\\check{1}}{2}\\log_{2}(2\\ln(\\frac{4n}{\\delta}))}\\end{array}$ , and taking a union bound over all $i\\in[n]$ we have that $\\begin{array}{r}{|g_{i}|\\leq\\sqrt{2\\ln(\\frac{4n}{\\delta})}\\ll2^{M}}\\end{array}$ holds for all $i$ simultaneously with probability at least $1-\\delta/2$ ", "page_idx": 47}, {"type": "text", "text": "Conditioning on the event that all the $g_{i}$ are in the correct range, from Definition D.1, $|\\widetilde{g}_{i}\\!-\\!g_{i}|\\leq\\mathbf{u}|g_{i}|$ holds for all $i$ .Then ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\widetilde{v}_{i}=\\mathsf{f l}(\\widetilde{\\sigma}\\cdot\\widetilde{g}_{i})=(1+\\theta)\\widetilde{\\sigma}\\widetilde{g}_{i}=\\sigma g_{i}(1+\\eta_{c+1}),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $c>1$ $\\begin{array}{r}{\\vert\\eta_{c+1}\\vert\\leq\\frac{(c+1)\\mathbf{u}}{1-(c+1)\\mathbf{u}}}\\end{array}$ $\\mathbf{fl}(\\widetilde{\\sigma}\\widetilde{\\mathbf{G}})$ is $(1+\\eta_{c+1})$ -far from a random variable that is sampled from ${\\mathcal{N}}(0,\\sigma^{2})$ . Multiplying each diagonal element of $\\widetilde{\\mathbf{fl}}(\\widetilde{\\sigma}\\widetilde{\\mathbf{G}})$ with $\\gamma$ to form $\\widetilde{{\\mathbf V}}$ (similar to the Step 4 of Algorithm 5) simply increases the (relative) error to $(1+\\eta_{c+2})$ . Then we can write ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{V}}=\\gamma\\mathbf{V}+\\mathbf{E},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $\\mathbf{V}$ is diagonal with independent diagonal entries from ${\\mathcal{N}}(0,\\sigma^{2})$ and $\\mathbf{E}$ is a diagonal matrix with diagonal elements $\\mathbf{E}_{i,i}$ bounded in magnitude by ", "page_idx": 48}, {"type": "equation", "text": "$$\n|\\mathbf{E}_{i,i}|\\leq\\frac{(c+2)\\mathbf{u}}{1-(c+2)\\mathbf{u}}\\gamma|\\mathbf{V}_{i,i}|.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "It also holds that $\\|\\mathbf{V}\\|\\ \\leq\\ \\sigma{\\sqrt{2\\ln(4n/\\delta)}}$ since the diagonal elements $\\mathbf{V}_{i,i}$ since they are just the $g_{i}$ 's scaled by $\\sigma$ , and we already bounded $|g_{i}|\\;\\leq\\;\\sqrt{2\\ln(4n/\\delta)}$ , which implies that $\\|\\mathbf{E}\\|\\,\\leq$ 1-(+2)\u03b1 /2 ln(4n/8). Finally ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widetilde{\\mathbf{V}}\\|\\leq\\gamma\\|\\mathbf{V}\\|+\\|\\mathbf{E}\\|\\leq\\gamma\\left(1+\\frac{(c+2)\\mathbf{u}}{1-(c+2)\\mathbf{u}}\\right)\\sigma\\sqrt{2\\ln(4n/\\delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We conditioned only on two random events, where each holds with probability at least $1-\\delta/2$ giving the final success probability of at least $1-\\delta$ ", "page_idx": 48}, {"type": "text", "text": "REGULARIZE ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Input: Hermitian matrix $\\mathbf{A}\\in\\mathbb{H}^{n}$ , perturbation scale factor $\\gamma\\in\\left(0,1/4\\right)$ , failure probability parameter $\\delta\\in$ $(0,1/4)$   \nRequires: $\\|\\mathbf{A}\\|\\leq1$   \nAlgorithm: $\\widetilde{\\mathbf{X}}\\gets\\mathsf{R E G U L A R I Z E}(\\mathbf{A},\\gamma,\\delta)$   \n1: $\\begin{array}{r}{\\widetilde{\\sigma}=\\mathbf{f}\\mathbf{l}\\left(\\frac{1}{2\\sqrt{2\\ln(4n/\\delta)}}\\right)}\\end{array}$   \n2: $p\\leftarrow\\lceil C\\log(\\log(\\frac{n}{\\delta}))\\rceil$ \uff1a   \n3: $\\widetilde{\\mathbf{G}}=\\mathrm{diag}(\\widetilde{g}_{1},\\ldots,\\widetilde{g}_{n}),\\widetilde{g}_{i}\\gets\\mathsf{N}(p,\\log(1/\\mathbf{u}))$   \n4: $\\widetilde{\\mathbf{V}}=\\mathbf{f}\\mathbf{l}(\\gamma\\cdot\\widetilde{\\sigma}\\cdot\\widetilde{\\mathbf{G}})=\\gamma\\sigma\\mathbf{G}+\\mathbf{E}$ Where $\\mathbf{G}=\\mathrm{diag}(g_{1},\\dots,g_{n}),g_{i}\\leftarrow\\mathcal{N}(0,1)$ 5: $\\widetilde{\\mathbf{X}}=\\mathbf{A}+\\widetilde{\\mathbf{V}}+\\mathbf{E}^{(+)}$   \nOutput: Hermitian perturbed matrix $\\widetilde{\\mathbf{X}}$   \nEnsures: See Proposition D.2. ", "page_idx": 48}, {"type": "text", "text": "We can now use the Wegner estimate to get a minimum singular value bound for diagonal shifts over a grid, defined as follows. ", "page_idx": 48}, {"type": "text", "text": "Definition D.2 (Grid). $A\\,1{\\,-d}$ grid in the real line is a set of s points defined as ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathrm{grid}(l,r,h)=\\{l+j h|j\\in\\mathbb{Z}_{\\ge0},l+j h\\le r\\}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The cardinality of a grid $\\mathtt{g}=\\operatorname{grid}(l,r,h)$ is denoted as $|\\mathtt{g}|$ ", "page_idx": 48}, {"type": "text", "text": "The following Proposition 3.2 summarizes the properties of Algorithm 5, REGULARIZE, that we will use to perform eficient eigenvalue counting queries to compute the spectral gap. ", "page_idx": 48}, {"type": "text", "text": "Proposition D.2. Let A with $\\|\\mathbf{A}\\|\\leq1$ be a Hermitian matrix, $\\gamma,\\delta\\in(0,1/4)$ two given parameters, and $\\widetilde{\\mathbf{X}}\\gets\\mathsf{R E G U L A R I Z E}(\\mathbf{A},\\gamma,\\delta)$ . Let $\\mathsf{g}$ be an arbitrary (but fixed) grid of points in $[-2,2]$ with size $|\\mathsf{g}|=T.$ For every element $h_{i}\\in\\mathsf{g}$ consider the matrices $\\mathbf{M}_{i}=h_{i}-\\widetilde{\\mathbf{X}}$ and $\\widetilde{\\mathbf{M}}=h_{i}-\\widetilde{\\mathbf{X}}+\\mathbf{E}_{i},$ where $\\mathbf{E}_{i}$ denote the diagonal floating point error matrices induced by the shift. If the machine precision u satisfies ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{u}\\leq\\frac{\\gamma\\delta}{32(c+2)n T\\sqrt{\\pi\\ln(4n/\\delta)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "which translates to $O(\\log({\\frac{T n}{\\gamma\\delta}}))$ bits, then all the following hold simultaneously with probability $1-2\\delta$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widetilde{\\mathbf{X}}\\|\\le4/3,\\quad\\Big|\\lambda_{i}(\\widetilde{\\mathbf{X}})-\\lambda_{i}(\\mathbf{A})\\Big|\\le\\frac{9}{16}\\gamma,\\quad\\sigma_{\\operatorname*{min}}(\\widetilde{\\mathbf{M}}_{i})\\ge\\frac{\\gamma\\delta}{4n T\\sqrt{4\\pi\\ln(4n/\\delta)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof. Let $\\mathbf{X}\\ =\\ \\mathbf{A}\\,+\\,\\gamma\\sigma\\mathbf{G}$ where $\\sigma\\mathbf G$ is a diagonal matrix with independent elements from ${\\mathcal{N}}(0,\\sigma^{2})$ where $\\begin{array}{r}{\\sigma\\ =\\ \\frac{1}{2\\sqrt{2\\ln({4n}/{\\delta})}}}\\end{array}$ Let $\\mathbf{B}\\;=\\;\\mathbf{X}{\\bar{/}}\\gamma\\;=\\;\\mathbf{A}/\\gamma\\,+\\,\\sigma\\mathbf{G}$ .Denote by $E_{\\mathbf{G}}$ the event $\\|\\sigma\\mathbf{G}\\|\\ \\leq\\ \\frac{1}{2}$ From a tail bound as in Lemma D.1, $\\operatorname*{Pr}[E_{\\mathbf{G}}]~\\geq~1\\mathrm{~-~}\\delta$ .Conditioningon $E_{\\mathbf{G}}$ \uff0c\uff01 we obtain $\\left\\|\\mathbf{B}\\right\\|\\ =\\ \\left\\|\\mathbf{A}/\\gamma\\,+\\,\\sigma\\mathbf{G}\\right\\|\\ \\leq\\ 1/\\gamma\\,+\\,1/2\\ :=\\ \\mathring{\\beta}$ ._This directly implies that $\\Lambda({\\bf B})\\ \\subseteq$ $(-1/\\gamma-1/2,1/\\gamma+1/2)$ . Let $h_{i}^{\\prime}=h_{i}/\\gamma$ for all $i=1,\\dots,T$ ", "page_idx": 49}, {"type": "text", "text": "From the Wegner estimate (Proposition D.1), for every fixed interval $I$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left[|\\Lambda(h^{\\prime}-{\\bf B})\\cap I|\\ge1\\right]\\le\\sqrt{4\\pi\\ln(4n/\\delta)}n|I|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Setng $I_{i}$ to be an interal entere t $h_{i}^{\\prime}$ Wwith size $\\begin{array}{r}{|I_{i}|=\\frac{\\delta}{T n\\sqrt{4\\pi\\ln(4n/\\delta)}}}\\end{array}$ . we have that the probability that $I_{i}$ contains any eigenvalues is at most $\\delta/T$ . Since there are no eigenvalues in $I_{i}$ , this means that the smallest singular value of $h_{i}^{\\prime}-\\mathbf{B}$ is at least half the size of $I_{i}$ , i.e. $\\sigma_{\\operatorname*{min}}(h_{i}^{\\prime}-\\mathbf{B})\\geq|I_{i}|/2=$ $\\frac{\\delta}{2T n{\\sqrt{4\\pi\\ln(4n/\\delta)}}}$ Hence, mi(h; -X) =min(h; -B)\u2265 2Tn\u221a4riln(4n/) holds as well. ", "page_idx": 49}, {"type": "text", "text": "From Lemma D.1, if the diagonal Gaussian sampler is called with $\\begin{array}{r}{p\\,=\\,\\Theta(\\log(\\log(\\frac{n}{\\delta}))}\\end{array}$ and $t\\,=$ $\\log(1/\\mathbf{u})$ then ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle||\\mathbf{E}||\\leq\\gamma\\eta_{c+2}\\frac{1}{2},}\\\\ {\\displaystyle||\\widetilde{\\mathbf{V}}||\\leq\\gamma(1+\\eta_{c+2})\\frac{1}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where |nc+2| \u2264 1-(c+2)u\u00b7 .The elements of the diagonal error matrix $\\mathbf{E}^{(+)}$ in Algorithm 5 are bounded by ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbf{E}_{i,i}^{(+)}|\\leq\\mathbf{u}|\\mathbf{A}_{i,i}+\\widetilde{\\mathbf{V}}_{i,i}|\\leq\\mathbf{u}(\\|\\mathbf{A}\\|+\\|\\widetilde{\\mathbf{V}}\\|)\\leq\\mathbf{u}\\left(1+\\frac{1}{2}\\gamma(1+\\eta_{c+2})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widetilde{\\mathbf{X}}-{\\mathbf{X}}\\|=\\|{\\mathbf{E}}^{(+)}+{\\mathbf{E}}\\|}\\\\ &{\\qquad\\qquad\\leq{\\mathbf{u}}\\|{\\mathbf{A}}+\\widetilde{\\mathbf{V}}\\|+\\frac{1}{2}\\gamma\\eta_{c+2}}\\\\ &{\\qquad\\qquad\\leq{\\mathbf{u}}\\left(1+\\frac{1}{2}\\gamma(1+\\eta_{c+2})\\right)+\\frac{1}{2}\\gamma\\eta_{c+2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbf{u}\\leq\\frac{\\epsilon}{2(c+2)}}\\end{array}$ for some $\\epsilon\\in(0,1)$ then $|\\eta_{c+2}|\\leq\\epsilon$ and since $\\gamma<1/2$ we also have that $\\|\\widetilde{\\mathbf{X}}\\!-\\!\\mathbf{X}\\|\\leq\\epsilon$ Recall that $\\|\\mathbf{X}\\|=\\|\\mathbf{A}+\\gamma\\sigma\\mathbf{G}\\|\\leq1+\\gamma/2\\leq5/4$ and thus ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{E}_{i}\\rVert\\leq\\mathbf{u}(|h_{i}|+\\lVert\\tilde{\\mathbf{X}}\\rVert)\\leq\\mathbf{u}(|h_{i}|+\\lVert\\mathbf{X}\\rVert+\\epsilon)\\leq\\mathbf{u}(2+(1+\\gamma/2)+\\epsilon)\\leq5\\mathbf{u}\\leq5\\frac{\\epsilon}{2(c+2)}\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "assuming that $c\\geq1$ . We now collect all the error bounds and apply Weyl's inequality to argue that ", "page_idx": 49}, {"type": "equation", "text": "$$\n|\\lambda_{j}(h_{i}-\\mathbf{X})-\\lambda_{j}(h_{i}-\\widetilde{\\mathbf{X}}+\\mathbf{E}_{i})|\\leq\\|\\mathbf{X}-\\widetilde{\\mathbf{X}}\\|+\\|\\mathbf{E}_{i}\\|\\leq2\\epsilon.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "The same holds for the singular values $\\sigma_{j}$ since the matrices are Hermitian. Thus if ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\epsilon\\le\\frac{\\gamma\\delta}{8T n\\sqrt{4\\pi\\ln(4n/\\delta)}}\\le\\sigma_{\\mathrm{min}}(h_{i}-\\mathbf{X})/4\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "then ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\Big|\\sigma_{\\operatorname*{min}}\\big(h_{i}-\\widetilde{\\mathbf{X}}+\\mathbf{E}_{i}\\big)-\\sigma_{\\operatorname*{min}}\\big(h_{i}-\\mathbf{X}\\big)\\Big|\\le\\sigma_{\\operatorname*{min}}\\big(h_{i}-\\mathbf{X}\\big)/2,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "which implies ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sigma_{\\operatorname*{min}}\\bigr(h_{i}-\\widetilde{\\mathbf{X}}+\\mathbf{E}_{i}\\bigr)\\geq\\sigma_{\\operatorname*{min}}\\bigr(h_{i}-\\mathbf{X}\\bigr)/2=\\frac{\\gamma\\delta}{4T n\\sqrt{4\\pi\\ln(4n/\\delta)}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "The aforementioned bound on $\\epsilon$ also gives two (loose) bounds that are useful for the analysis of algorithms later: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\mathbf{X}}\\|\\le\\|\\mathbf{X}\\|+\\epsilon\\le5/4+1/32\\le4/3,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and also from Weyl's inequality ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\lambda_{i}(\\mathbf{A})-\\lambda_{i}(\\widetilde{\\mathbf{X}})\\right|\\leq\\|\\mathbf{A}-\\mathbf{X}\\|+\\|\\mathbf{X}-\\widetilde{\\mathbf{X}}\\|\\leq\\frac{\\gamma}{2}+\\frac{\\gamma}{16}=\\frac{9}{16}\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "The requirement for the machine precision is ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq{\\frac{\\epsilon}{2(c+2)}}\\leq{\\frac{\\gamma\\delta}{32(c+2)T n{\\sqrt{\\pi\\ln(4n/\\delta)}}}},\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "which translates to ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\log(\\frac{T n}{\\gamma\\delta})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "bits of precision (recall that we also required $p=\\Theta(\\log(\\log(n/\\delta)))$ for the exponent of the Gaussian sampler, but this does not affect the total number of bits). ", "page_idx": 50}, {"type": "text", "text": "For the complexity, we have $n$ calls to $\\mathsf{N}(p,t)$ and $O(n)$ scalar additions and multiplications thereafter, which accumulate to $O(n)$ total floating point operations using $\\begin{array}{r}{O\\left(\\log(\\frac{T n}{\\gamma\\delta})\\right)}\\end{array}$ bits. ", "page_idx": 50}, {"type": "text", "text": "For the success probability, there are two types random events that we took into account: $E_{\\mathbf{G}}$ , which fails with probability at most $\\delta$ , and the Wegner estimate for $T$ points $h_{i}$ in the grid $\\mathsf{g}$ whereeachone fails with probability at most $\\delta/T$ . A union bound over all random events gives a success probability of at least $1-\\delta-T\\delta/T=1-2\\delta$ \u53e3 ", "page_idx": 50}, {"type": "text", "text": "E  Fast spectral gaps and eigenvalues with counting queries ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "The core of our methods is an algorithm to efficiently approximate spectral gaps using only \u201ceigenvalue counting queries,\u201d avoiding an explicit (and expensive) diagonalization. ", "page_idx": 50}, {"type": "text", "text": "E.1   Counting eigenvalues ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "The main subroutine of our algorithm is used to count the eigenvalues that are smaller than a given threshold efficiently. ", "page_idx": 50}, {"type": "text", "text": "Lemma E.1. Let $\\widetilde{\\mathbf{X}}$ be Hermitian, $\\|\\widetilde{\\mathbf{X}}\\|\\leq2,$ and $h\\in[-2,2]$ is a fixed point that can be exactly represented in foating point, i.e. $\\mathbf{fl}(h)\\,=\\,h.$ \uff1aThere exists an algorithm COUNT $(\\widetilde{\\mathbf{X}},h,\\varepsilon)$ which takes as input $h,\\widetilde{\\mathbf{X}}$ , and $\\varepsilon\\in(0,4/199)$ with the guarantee that $\\sigma_{\\mathrm{min}}(h-\\widetilde{\\mathbf{X}})\\geq\\varepsilon$ and it returns an integer $n_{(-)}$ , which is the precise number of eigenvalues of $\\widetilde{\\mathbf{X}}$ that are smaller than $h$ The algorithm requires ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\left(\\log(\\frac{1}{\\varepsilon})+\\log(\\log(\\frac{n}{\\varepsilon}))\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "arithmetic operations using ", "page_idx": 50}, {"type": "equation", "text": "$$\nO\\left(\\log(n)\\log^{3}({\\frac{1}{\\varepsilon}})\\log({\\frac{n}{\\varepsilon}})\\right)\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "bits of precision. ", "page_idx": 50}, {"type": "text", "text": "Proof. Let $\\mathbf{M}=h-\\widetilde{\\mathbf{X}}$ and $\\widetilde{\\mathbf{M}}=h-\\widetilde{\\mathbf{X}}+\\mathbf{E}$ be its floating point counterpart. As the shift only distorts the diagonal elements, $\\|\\mathbf{E}\\|\\,=\\,\\mathrm{max}_{i}\\,|\\mathbf{E}_{i,i}|\\,\\le\\,\\mathbf{u}\\,\\mathrm{max}_{i}\\,|h-\\widetilde{\\mathbf{X}}_{i,i}|\\,\\le\\,4\\mathbf{u}$ .By assumption $\\sigma_{\\operatorname*{min}}(\\mathbf{M})\\geq\\varepsilon$ , hence if ${\\bf u}\\le\\varepsilon/8$ then Weyl's inequality implies $\\dot{\\sigma_{\\mathrm{min}}}(\\mathbf{M}+\\mathbf{\\dot{E}})\\geq\\sigma_{\\mathrm{min}}\\overset{\\cdot}{(\\mathbf{M})}-\\varepsilon/2\\geq$ $\\varepsilon/2$ ", "page_idx": 50}, {"type": "text", "text": "Assume that we want to approximate $n_{(-)}$ by the expression ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\widetilde{n}_{(-)}=\\mathsf{C O U N T}(\\widetilde{\\mathbf{X}},h,\\varepsilon)=\\mathrm{round}\\left(\\mathsf{f l}(\\mathrm{tr}(\\mathsf{S G N}(\\widetilde{\\mathbf{M}},\\alpha_{\\mathsf{S G N}},\\eta_{\\mathsf{S G N}},\\epsilon_{\\mathsf{S G N}})))\\right),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "for some $\\alpha_{\\mathsf{S G N}},\\eta_{\\mathsf{S G N}},\\epsilon_{\\mathsf{S G N}}$ (to be determined). For simplicity we will use $\\mathsf{S G N}(\\widetilde{\\mathbf{M}})$ to denote $\\mathsf{S G N}(\\widetilde{\\mathbf{M}},\\alpha_{\\mathsf{S G N}},\\eta_{\\mathsf{S G N}},\\epsilon_{\\mathsf{S G N}})$ . Assuming for now that SGN converged to the requested error $\\epsilon_{S\\sf G N}$ \uff0c we know that $\\|\\operatorname{sgn}(\\widetilde{\\mathbf{M}})-5\\mathsf{G N}(\\widetilde{\\mathbf{M}})\\|\\leq\\epsilon_{5\\mathsf{G N}}$ . To proceed recall that for any matrix A ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbf{\\{}f l(t r(A))-t r(A)|\\leq\\frac{\\log(n)\\mathbf{u}}{1-\\log(n)\\mathbf{u}}\\|\\mathbf{A}\\|,}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which can be achieved with a binary tree-type addition. ", "page_idx": 51}, {"type": "text", "text": "Now consider the error ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{n_{(-)}-\\widetilde{n}_{(-)}|=\\left|\\mathrm{tr}(\\mathrm{sgn}(\\mathbf{M}))-\\mathrm{tr}(\\mathrm{sgn}(\\widetilde{\\mathbf{M}}))+\\mathrm{tr}(\\mathrm{sgn}(\\widetilde{\\mathbf{M}}))-\\mathrm{tr}(\\mathsf{S G N}(\\widetilde{\\mathbf{M}}))+\\mathrm{tr}(\\mathsf{S G N}(\\widetilde{\\mathbf{M}}))-\\mathsf{H}(\\mathrm{tr}(\\widetilde{\\mathbf{M}}))\\right|}}\\\\ &{}&{\\leq\\left|\\mathrm{tr}(\\mathrm{sgn}(\\mathbf{M}))-\\mathrm{tr}(\\mathrm{sgn}(\\widetilde{\\mathbf{M}}))\\right|+\\left|\\mathrm{tr}(\\mathrm{sgn}(\\widetilde{\\mathbf{M}}))-\\mathrm{tr}(\\mathsf{S G N}(\\widetilde{\\mathbf{M}}))\\right|+\\left|\\mathrm{tr}(\\mathsf{S G N}(\\widetilde{\\mathbf{M}}))-\\mathsf{H}(\\mathrm{tr}(\\widetilde{\\mathbf{M}}))\\right|}\\\\ &{}&{\\leq\\left|\\mathrm{tr}(\\mathrm{sgn}(\\mathbf{M}))-\\mathrm{tr}(\\mathrm{sgn}(\\widetilde{\\mathbf{M}}))\\right|+n\\epsilon_{\\mathsf{S G N}}+\\frac{\\log(n)\\mathbf{u}}{1-\\log(n)\\mathbf{u}}(1+\\epsilon_{\\mathsf{S G N}})}\\\\ &{}&{\\leq n\\left\\|\\mathrm{sgn}(\\mathbf{M})-\\mathrm{sgn}(\\widetilde{\\mathbf{M}})\\right\\|+n\\epsilon_{\\mathsf{S G N}}+\\frac{\\log(n)\\mathbf{u}}{1-\\log(n)\\mathbf{u}}(1+\\epsilon_{\\mathsf{S G N}}).\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "It remains to bound $\\|\\operatorname{sgn}(\\mathbf{M})-\\operatorname{sgn}(\\widetilde{\\mathbf{M}})\\|$ . For this we can directly use Lemma B.1 where we set $\\mathbf{S}=\\mathbf{I}$ $\\mathbf{H}={\\widetilde{\\mathbf{X}}}$ and $\\mu=h$ . The lemma states that if $\\begin{array}{r}{\\|\\mathbf{E}\\|\\,\\leq\\,\\epsilon_{\\mathsf{s h i f t}}\\frac{|\\lambda_{\\operatorname*{min}}(h-\\widetilde\\mathbf{X})|^{2}\\pi}{128}}\\end{array}$ Imin (h-x)\u00b2\u03c0 then I/ sgn(h - $\\widetilde{\\mathbf{X}})\\,-\\,\\mathrm{sgn}(h\\,-\\,\\widetilde{\\mathbf{X}}\\,+\\,\\mathbf{E})\\|\\;\\le\\;\\epsilon_{\\mathsf{s h i f t}}$ for some $\\epsilon_{\\mathsf{s h i f t}}\\;\\in\\;(0,1)$ that we can choose. By assumption $|\\lambda_{\\operatorname*{min}}(h-\\widetilde{\\mathbf{X}})|\\geq\\varepsilon$ , which means that ", "page_idx": 51}, {"type": "equation", "text": "$$\n4\\mathbf{u}\\leq\\epsilon_{\\mathsf{s h i f t}}\\frac{\\varepsilon^{2}\\pi}{128n}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "is sufficient to guarantee that ", "page_idx": 51}, {"type": "equation", "text": "$$\nn\\left\\|\\mathrm{sgn}(\\mathbf{M})-\\mathrm{sgn}(\\widetilde{\\mathbf{M}})\\right\\|\\leq\\epsilon_{\\mathsf{s h i f t}.}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "For SGN, we set $\\begin{array}{r}{\\eta_{\\sf S G N}=\\frac{\\varepsilon}{4}}\\end{array}$ \uff0c $\\begin{array}{r}{\\alpha_{\\mathsf{S G N}}=\\frac{4-\\varepsilon}{4+\\varepsilon}}\\end{array}$ and $\\begin{array}{r}{\\epsilon_{S G N}=\\frac{1}{8n}}\\end{array}$ Noe that $\\alpha_{\\mathsf{S G N}}$ satisfes the equirement $99/100<\\alpha_{\\mathsf{S G N}}<1$ as long as $\\varepsilon<4/199)$ 0. Then based on Theorem A.4 SGN requires ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N=O\\left(\\log(\\frac{1}{1-\\alpha_{\\sf S G N}})+\\log(\\log(\\frac{1}{\\epsilon_{\\sf S G N}\\eta_{\\sf S G N}}))\\right)=O\\left(\\log(\\frac{1}{\\varepsilon})+\\log(\\log(\\frac{n}{\\varepsilon}))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "iterations and ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\log(n)\\log^{3}(\\frac{1}{1-\\alpha_{\\sf S G N}})\\left(\\log(\\frac{1}{\\epsilon_{\\sf S G N}})+\\log(\\frac{1}{\\eta_{\\sf S G N}})\\right)\\right)=O\\left(\\log(n)\\log^{3}(\\frac{1}{\\varepsilon})\\log(\\frac{n}{\\varepsilon})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "bits of precision. ", "page_idx": 51}, {"type": "text", "text": "If we set $\\epsilon_{\\sf s h i f t}=1/8$ , then the bound for $\\mathbf{u}$ becomes ", "page_idx": 51}, {"type": "equation", "text": "$$\n{\\bf u}\\le\\frac{\\epsilon^{2}\\pi}{32\\cdot128n},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "in which case ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\log(n)\\mathbf{u}}{1-\\log(n)\\mathbf{u}}(1+\\epsilon_{5\\mathsf{G N}})\\ll\\frac{1}{8}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Then (19) becomes ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|n_{(-)}-\\widetilde{n}_{(-)}|<\\frac{1}{8}+\\frac{1}{8}+\\frac{1}{8}=3/8<1/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which means that if we round tothe closest integer thenround $(\\widetilde{n}_{(-)})=n_{(-)}$ . Note that $\\log(1/\\mathbf{u})=$ $O\\left(\\log({\\frac{n}{\\epsilon}})\\right)$ which is dominated by the bit requirements of SGN. ", "page_idx": 51}, {"type": "text", "text": "E.2  Approximate midpoint and gap ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Having an efficient way to count eigenvalues smaller than a threshold, we show how to approximate spectral gaps. We can now prove Proposition 3.1, which we restate below for readability. ", "page_idx": 52}, {"type": "text", "text": "Proposition E.1. Problem 3. 1 can be solved in time $\\begin{array}{r}{O\\left(\\log(\\frac{1}{\\epsilon\\operatorname{gap}_{k}})q(\\frac{1}{\\epsilon\\operatorname{gap}_{k}})\\right)}\\end{array}$ ", "page_idx": 52}, {"type": "text", "text": "Proof. We start with $\\gamma_{0}=1/4$ , and make a grid $\\mathtt{g}=\\mathrm{grid}(-1,1+\\gamma,\\gamma)$ which consists of the points $\\{-1,-3/4,-2/4,-1/4,0,1/4,2/4,3/4,1,5/4\\}$ (a useful note for later is that all the numbers in the grid can be exactly represented in floating point). Let $\\operatorname{count}(h_{j})$ denote the number of $\\gamma_{0}$ distorted values $\\lambda_{i}^{\\prime}$ that are smaller than $h_{j}$ , for some $h_{j}\\in\\mathfrak{g},j=1,\\dotsc,10$ . Query all $h_{j}\\in{\\mathfrak{g}}$ for $\\operatorname{count}(h_{j})$ . Set $j_{k}\\'=\\arg\\operatorname*{min}_{j}\\{h_{j}|\\ \\mathrm{count}(\\acute{h}_{j})\\geq k\\}$ and $j_{k+1}=\\arg\\operatorname*{min}_{j}\\{h_{j}|\\operatorname{count}(h_{j})\\geq k+1\\}$ Now $\\vec{\\lambda_{k}^{\\prime}}\\overset{\\cdot}{\\in}[h_{j_{k}-1},h_{j_{k}}]$ and $\\lambda_{k+1}^{\\prime}\\in[h_{j_{k+1}-1},h_{j_{k+1}}]$ . The points in the grid are equispaced so each interval has length $\\gamma$ Since by definition $\\lambda_{k}^{\\prime}\\in[\\lambda_{k}\\,\\!-\\!\\gamma_{0},\\lambda_{k}\\,\\!+\\!\\gamma_{0}]$ , then $\\lambda_{k}\\in[h_{j_{k}-1}\\!-\\!\\gamma_{0},h_{j_{k}}\\!+\\!\\gamma_{0}]=$ $[h_{j_{k}-2},h_{j_{k}+1}]$ . Similarly for $\\lambda_{k+1}$ . We have now restricted both $\\lambda_{k}$ and $\\lambda_{k+1}$ inside some intervals of length $3\\gamma_{0}$ Let us denote those intervals as $I_{k}=[h_{j_{k}-2},h_{j_{k}+1}]$ and $I_{k+1}=[h_{j_{k+1}-2},h_{j_{k+1}+1}]$ In the next step we halve $\\gamma_{0}$ to $\\gamma_{1}=1/8$ . We now create two grids, one inside $I_{k}$ and one inside $I_{k+1}\\colon\\mathtt{g}_{k}=\\operatorname{grid}(h_{j_{k}-2},h_{j_{k}+1}+\\gamma_{1},\\gamma_{1})$ and similar for $\\mathsf{g}_{k+1}$ . Each of the new grids has exactly 8 points and each point can be exactly represented in floating point. As in the previous iteration, we query all the 8 points $h_{j}$ of $\\mathtt{g}_{k}$ for $\\operatorname{count}(h_{j})$ , and we pick a new $j_{k}=\\arg\\operatorname*{min}\\{h_{j}|\\operatorname{count}(h_{j})\\geq k\\}$ and we do the same for $j_{k+1}$ . There are a total of 16 queries. As before, we have now restricted $\\lambda_{k}$ and $\\lambda_{k+1}$ inside two new intervals $I_{k}$ and $I_{k+1}$ where $\\left|I_{k}\\right|=\\left|I_{k+1}\\right|=3\\gamma_{1}$ , which is half the size of the corresponding intervals of the previous iterations. If we set $\\widetilde{\\lambda}_{k}$ equal to the midpoint of $I_{k}$ , then $|\\widetilde{\\lambda}_{k}-\\lambda_{k}|\\stackrel{}{\\leq}3\\gamma/2$ . The same for $\\lambda_{k+1}$ . We keep repeating the same procedure by halving $\\gamma_{i}$ in each step $i$ ", "page_idx": 52}, {"type": "text", "text": "ainsddi $\\begin{array}{r}{\\widetilde{\\mu}_{k}=\\frac{\\widetilde{\\lambda}_{k}+\\widetilde{\\lambda}_{k}}{2}}\\end{array}$ and $\\widetilde{\\mathrm{gap}}_{k}=\\widetilde{\\lambda}_{k}-\\widetilde{\\lambda}_{k+1}$ where $\\widetilde{\\lambda}_{k}$ and $\\widetilde{\\lambda}_{k+1}$ are as above. Then, after $m$ steps, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\mu}_{k}=\\mu_{k}\\pm3\\gamma_{m}/2,}\\\\ {\\widetilde{\\mathrm{gap}}_{k}=\\mathrm{gap}_{k}\\pm3\\gamma_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Thus, $\\gamma_{m}\\,\\approx\\,\\epsilon\\,\\mathrm{gap}_{k}\\,/3$ is a suffcient terminating criterion. We don't know $\\mathrm{gap}_{k}$ , but we can use $\\widetilde{\\mathrm{gap}_{k}}$ instead, i.e,. $\\gamma\\leq\\epsilon^{\\frac{\\widetilde{\\mathrm{gap}}_{k}-3\\gamma_{m}}{3}}$ is also a sfient, which gives a quantifable temination riterion $\\gamma_{m}\\leq\\epsilon\\widetilde{\\mathrm{gap}_{k}}/6$ . But since $\\widetilde{\\mathrm{gap}_{k}}\\ge\\mathrm{gap}_{k}-3\\gamma_{m}$ , then the terminating criterion will be reached in the worst case when ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\gamma_{m}\\leq\\epsilon\\frac{\\mathrm{gap}_{k}-3\\gamma_{m}}{6}\\Rightarrow\\gamma_{m}\\leq\\frac{\\epsilon\\,\\mathrm{gap}_{k}}{9},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "meaning that tealgortwillha andre a $\\epsilon$ accurate $\\widetilde{\\mu}_{k},\\widetilde{\\mathrm{gap}}_{k}$ in at most $\\begin{array}{r l}{O\\left(\\log(\\frac{9}{\\gamma_{m}})\\right)=}&{{}}\\end{array}$ $\\begin{array}{r}{O\\left(\\log(\\frac{1}{\\epsilon\\operatorname{gap}_{k}})\\right)}\\end{array}$ itraions. ", "page_idx": 52}, {"type": "text", "text": "Each $h_{j}$ -query at iteration $i$ costs $O(\\mathrm{polylog}(1/\\gamma_{i}))$ by assumption, and there are exactly $16\\ h_{j}$ queries in each iteration, which gives a total query cost of $O(\\mathrm{polylog}(1/\\gamma_{i}))$ per iteration, which is maximized for the smallest $\\gamma_{m}=\\Theta(\\epsilon\\,\\mathrm{gap}_{k})$ \uff1a \u53e3 ", "page_idx": 52}, {"type": "text", "text": "We can now describe the algorithm GAP that computes the $k$ -th gap and the midpoint of a Hermitian definite pencil. ", "page_idx": 52}, {"type": "text", "text": "Theorem E.1 (GAP, Restatement of Theorem 3.1). Let $\\mathbf{H}\\in\\mathbb{H}^{n}$ \uff0c $\\mathbf{S}\\in\\mathbb{H}_{++}^{n}$ and $\\lVert\\mathbf{H}\\rVert,\\lVert\\mathbf{S}^{-1}\\rVert\\leq1,$ which define a Hermitian definite pencil $(\\mathbf{H},\\mathbf{S})$ : Given $k\\in[n-1]$ accuracy $\\epsilon\\in(0,1)$ ,and failure probability $\\delta\\in(0,1/2)$ ,there exists an algorithm ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\widetilde{\\mu}_{k},\\widetilde{\\mathrm{gap}}_{k}\\gets\\mathsf{G A P}(\\mathbf{H},\\mathbf{S},k,\\epsilon,\\delta)\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "whihretmis $\\widetilde{\\mu}_{k}\\,=\\,\\mu_{k}\\,\\pm\\epsilon\\,\\mathrm{gap}_{k}$ and $\\widetilde{\\mathrm{gap}_{k}}\\,=\\,(1\\pm\\epsilon)\\,\\mathrm{gap}_{k}$ where $\\begin{array}{r}{\\mu_{k}\\,=\\,\\frac{\\lambda_{k}+\\lambda_{k+1}}{2}}\\end{array}$ and $\\mathrm{gap}_{k}\\,=$ $\\lambda_{k}-\\lambda_{k+1}$ . The algorithm requires ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\log(\\frac{1}{\\delta\\epsilon\\operatorname{gap}_{k}})\\log(\\frac{1}{\\epsilon\\operatorname{gap}_{k}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "arithmetic operations using $O\\left(\\log(n)\\left(\\log^{4}({\\frac{n}{\\delta\\epsilon\\,\\mathrm{gap}_{k}}})+\\log(\\kappa(\\mathbf{S}))\\right)\\right)$ bits, where $\\lambda_{i}$ are the eigenvalues of $(\\mathbf{H},\\mathbf{S})$ If $\\kappa(\\mathbf{S})$ is unknown, additional $\\begin{array}{r}{O(T_{\\sf M M}(n)\\log(\\frac{n\\kappa(\\mathbf{S})}{\\delta})\\log(\\kappa(\\mathbf{S})))}\\end{array}$ foating point operations and $O(\\log(n)\\log^{4}({\\frac{n\\kappa(\\mathbf{S})}{\\delta}}))$ bits are suficient to compute it with Corollary $E.I$ ", "page_idx": 53}, {"type": "text", "text": "Proof. We first set $\\gamma_{0}=1/8$ and call $\\begin{array}{r}{\\widetilde{\\mathbf{H}}=\\mathsf{R E D U C E}\\left(\\mathbf{H},\\mathbf{S},\\frac{\\gamma_{0}}{4}\\right)}\\end{array}$ . From Proposition C.3, REDUCE requires $O(T_{\\mathsf{M M}}(n))$ floating point operations using $\\mathcal{O}(\\log(n)\\log(\\kappa(\\mathbf{S}))+\\log(1/\\gamma_{0}))$ bits, and it returns a matrix $\\widetilde{\\mathbf{H}}$ that satisfies $\\begin{array}{r}{|\\lambda_{i}(\\widetilde{\\mathbf{H}})-\\lambda_{i}(\\mathbf{H},\\mathbf{S})|\\le\\frac{\\gamma_{0}}{4}}\\end{array}$ ", "page_idx": 53}, {"type": "text", "text": "We then call $\\widetilde{\\mathbf{X}}\\gets\\mathsf{R E G U L A R I Z E}(\\widetilde{\\mathbf{H}},\\frac{\\gamma_{0}}{2},\\delta_{0})$ where $\\delta_{0}=\\delta/2$ is the initial failure probability. From Proposition 3.2, for all $i$ it holds that $|\\lambda_{i}(\\widetilde{\\mathbf{X}})-\\lambda_{i}(\\widetilde{\\mathbf{H}})|\\,\\leq\\,9\\gamma_{0}/16$ . Therefore, all the eigenvalues, which initially lie in $[-1,1]$ , are distorted by at most $\\gamma_{0}$ ", "page_idx": 53}, {"type": "text", "text": "Next, recall the counting query model of Proposition 3.1: in the first step we construct a grid $_{\\mathsf{g}}=$ gric $\\left\\lfloor(-1,1+\\gamma_{0},\\gamma_{0}\\right)\\right\\rfloor$ ,where $|\\mathtt{g}|\\;=\\;10$ . From Proposition 3.2, the regularization ensures that for every h; E git holds that mih -X+E)\u2265 for o=slgnm/6 (this is included in the success probability of $1-\\delta_{0}$ of Proposition 3.2). Then we execute $\\mathsf{C O U N T}(\\widetilde{\\mathbf{X}},h_{j},\\varepsilon_{0})$ for every $h_{j}$ . We continue by halving at each step both $\\gamma$ and $\\delta$ , constructing the corresponding grids as per Proposition 3.1, and counting eigenvalues over the grid. In each iteration after the first one, we keep track of two grids $\\mathsf{g}$ with size $|\\mathtt{g}|=\\Theta(1)$ each, and therefore we can ignore $|\\mathtt{g}|$ in the complexity. After total of $m\\ =\\ O\\left(\\log(\\frac{1}{\\epsilon\\,\\mathrm{gap}_{k}})\\right)$ iteratons we have that $\\gamma_{m}~=~\\Theta(\\epsilon\\,\\mathrm{gap}_{k})$ and $\\delta_{m}\\;=\\;$ $\\Theta(\\delta_{0}\\epsilon\\,\\mathrm{gap}_{k})$ . Invoking Lemma E.1, and plugging in the value of $\\varepsilon_{m}$ , each call to COUNT during the entire algorithm costs at most ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\mathsf{M M}}(n)\\left(\\log(\\frac{1}{\\varepsilon_{m}})+\\log(\\log(\\frac{n}{\\varepsilon_{m}}))\\right)\\right)=O\\left(T_{\\mathsf{M M}}(n)\\log(\\frac{n}{\\gamma_{m}\\delta_{m}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "arithmetic operations using $\\begin{array}{r}{O\\left(\\log(n)\\log^{3}(\\frac{1}{\\varepsilon_{m}})\\log(\\frac{n}{\\varepsilon_{m}})\\right)\\,=\\,O\\left(\\log(n)\\log^{4}(\\frac{n}{\\gamma_{m}\\delta_{m}})\\right)}\\end{array}$ bits. The last iteration where $\\gamma_{m}=\\Theta(\\epsilon\\,\\mathrm{gap}_{k})$ and $\\delta_{m}=\\Theta(\\delta_{0}\\epsilon\\,\\mathrm{gap}_{k})$ is the most expensive, which gives a total of ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\mathsf{M M}}(n)\\log(\\frac{n}{\\delta_{0}\\epsilon\\operatorname{gap}_{k}})\\cdot m\\right)=O\\left(T_{\\mathsf{M M}}(n)\\log(\\frac{n}{\\delta_{0}\\epsilon\\operatorname{gap}_{k}})\\log(\\frac{1}{\\epsilon\\operatorname{gap}_{k}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "arithmetic operations and ", "page_idx": 53}, {"type": "equation", "text": "$$\nO\\left(\\log(n)\\log^{4}(\\frac{n}{\\delta_{0}{\\epsilon}\\,\\mathrm{gap}_{k}})\\right)\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "bits. The failure probability is $\\delta_{0}$ in the first iteration and halved at each subsequent iteration, in which case a union bound converges to $2\\delta_{0}=\\delta$ ", "page_idx": 53}, {"type": "text", "text": "To be able to call REDUCE in every step, we need to approximate $\\kappa(\\mathbf{S})$ in order to set the machine precision appropriately. For this we can use Corollary E.1, which is detailed in the next section, and it eturns $\\widetilde{\\kappa}\\in\\Theta(\\kappa(\\mathbf{S}))$ with probability $1-\\delta$ It requires $\\begin{array}{r}{O(T_{\\sf M M}(n)\\log(\\frac{n\\kappa(\\mathbf{S})}{\\delta})\\log(\\kappa(\\mathbf{S})))}\\end{array}$ foating point operationsusing $O(\\log(n)\\log^{4}({\\frac{n\\kappa(\\mathbf{S})}{\\delta}}))$ bits. \u53e3 ", "page_idx": 53}, {"type": "text", "text": "E.3  Singular values, singular gaps, and condition number ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "The gap finder can be extended to compute eigenvalues and singular values and singular gaps or arbitrary matrices instead of eigenvalue gaps of Hermitian matrices. The following algorithm in Proposition E.2 computes the $k$ -th singular value of a matrix, for arbitrary $k\\in[n]$ ,usingcounting queries. ", "page_idx": 53}, {"type": "text", "text": "Proposition E.2 (SIGMAK). Given a matrix $\\mathbf{A}\\in\\mathbb{C}^{m\\times n}$ $n\\geq m$ with $\\|\\mathbf{A}\\|\\leq1$ an integer $k\\in[n]$ such that ${\\mathrm{rank}}(\\mathbf{A})\\geq k,$ an accuracy $\\epsilon\\,\\in\\,(0,1)$ andfailureprobability $\\delta\\in(0,1)$ , there exists an algorithm ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\widetilde{\\sigma}_{k}\\gets\\mathsf{S l G M A K}(\\mathbf{A},k,\\epsilon,\\delta),\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which returns a value $\\widetilde{\\sigma}_{k}\\in(1\\pm\\epsilon)\\sigma_{k}(\\mathbf{A})$ with probability at least $1-\\delta$ The algorithm executes ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\left(\\frac{m}{n}+\\log(\\frac{n}{\\delta\\epsilon\\sigma_{k}})\\right)\\log(\\frac{1}{\\epsilon\\sigma_{k}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "foating point perations and requires $O\\left(\\log(n)\\log^{4}({\\frac{n}{\\delta\\epsilon\\sigma_{k}}})\\right)$ bits. ", "page_idx": 54}, {"type": "text", "text": "Notes: If $m>n$ then we can apply the algorithm on $\\mathbf{A}^{\\top}$ If $\\|\\mathbf{A}\\|>1$ , we can use Theorem A.7 to approximate $\\lVert\\mathbf{A}\\rVert$ and scale accordingly. The $\\operatorname{rank}(\\mathbf{A})\\geq k$ assumption can potentially be omitted by a more sophisticated counting query strategy. ", "page_idx": 54}, {"type": "text", "text": "Proof. We will solve the problem with counting queries, similar to Theorem 3.1, halving $\\gamma$ and $\\delta$ in every iteration. We start with $\\gamma_{0}\\,=\\,1/8$ and $\\bar{\\delta_{0}}\\,=\\,\\delta/2$ . First, we divide A by two, to ensure that $\\|\\mathbf{A}\\|\\leq1/2$ Then we construct $\\widetilde{\\mathbf{A}}=\\mathsf{H E R M}(\\mathsf{M M}(\\mathbf{A},\\mathbf{A}^{*}))=\\mathbf{AA}^{*}/4+\\mathbf{E}^{\\mathsf{M M}}$ To perform this rectangular multiplication in practice, we partition A in $m/n$ blocks of size $n\\times n$ each, perform the individual multiplications and sum the results. We can ensure that $\\|\\mathbf{E}^{\\mathsf{M M}}\\|\\leq\\gamma_{0}/4$ if we use $O(\\log(\\frac{n}{\\gamma_{0}}))$ bits.  Then from Weyl's inequality $|\\lambda_{k}(\\widetilde{\\mathbf{A}})\\rrangle-\\lambda_{k}(\\mathbf{A}\\mathbf{A}^{*}/4)|\\le\\,\\gamma_{0}/4$ .We then call $\\widetilde{\\mathbf{X}}\\gets\\mathsf{R E G U L A R I Z E}(\\widetilde{\\mathbf{A}},\\gamma_{0},\\delta_{0})$ . Conditioning on success of Proposition 3.2, for all $i$ it holds that $|\\lambda_{i}(\\widetilde{\\mathbf{X}})-\\lambda_{i}(\\widetilde{\\mathbf{A}})|\\leq9\\gamma_{0}/16.$ and therefore $|\\lambda_{i}(\\widetilde{\\mathbf{X}})-\\lambda_{i}(\\mathbf{A}\\mathbf{A}^{*}/4)|\\le\\gamma_{0}$ ", "page_idx": 54}, {"type": "text", "text": "Let $\\lambda_{i}^{\\prime}\\;=\\;\\lambda_{i}(\\widetilde{\\mathbf{X}})$ and $\\lambda_{i}~=~\\lambda_{i}(\\mathbf{A}\\mathbf{A}^{*}/4)$ . Since all the $\\lambda_{i}$ are non-negative, then all the $\\lambda_{i}^{\\prime}\\ \\in$ $[-\\gamma_{0},1+\\gamma_{0}]$ . We thus construct grid $\\mathrm{g}=\\mathrm{grid}(0,1+\\gamma_{0},\\gamma_{0})=\\{0,1/8,2/8,3/8,\\ldots,1,9/8\\}$ We now need to perform counting queries similar to Theorem 3.1. Since we conditioned on success of Proposition 3.2, REGULARIZE ensures that for every $h_{j}\\in{\\mathsf{g}}$ it holds that $\\sigma_{\\operatorname*{min}}(h_{j}-\\widetilde{\\mathbf{X}}\\!+\\!\\mathbf{E}_{h_{j}})\\geq\\varepsilon_{0}$ for E0 = slgn/\u03c0ln(4n/) .Then we execute $C_{j}\\gets\\mathsf{C O U N T}(\\widetilde{\\mathbf{X}},h_{j},\\varepsilon_{0})$ for every $h_{j}\\in{\\mathfrak{g}}$ (in total $\\Theta(1)$ calls toCOUNT).We set $j_{k}$ to be the smallest $j$ such that $C_{j}\\geq k$ .Now $\\lambda_{1}^{\\prime}\\in[h_{j_{k}},h_{j_{k}+1}]$ Since $\\lambda_{k}^{\\prime}\\in[\\lambda_{k}-\\gamma_{0},\\lambda_{k}+\\gamma_{0}]$ , then $\\lambda_{k}\\,\\in\\,\\left[h_{j_{1}-1}-\\gamma_{0},h_{j_{k}}+\\gamma_{0}\\right]^{\\cdot}\\!\\!=\\left[h_{j_{k}-2},h_{j_{k}+1}\\right]:=^{\\cdot}I_{k}$ ,where $|I_{k}|=3\\gamma_{0}$ (we can always ignore the negative portion of $I_{k}$ , if there is any). ", "page_idx": 54}, {"type": "text", "text": "In the next step we halve $\\gamma_{0}$ to $\\gamma_{1}\\textrm{\\,=\\,}1/8$ , and we create another grid inside $I_{k}$ \uff0c $\\mathrm{_{g1}}=$ $\\mathrm{grid}(h_{j_{k}-2},h_{j_{k}+1}+\\gamma_{1},\\gamma_{1})$ . The new grid has at most 8 points. We query all the points $h_{j}$ of $\\mathsfit{g}_{1}$ for $C_{j}\\gets\\mathsf{C O U N T}(\\widetilde{\\mathbf{X}},h_{j},\\epsilon_{1})$ , and we pick a new $j_{1}=\\arg\\operatorname*{min}\\{h_{j}|C_{j}\\geq k\\}$ . Now $\\lambda_{k}$ lies inside a new interval $I_{k}$ with $|\\bar{I}_{k}|=3\\gamma_{1}$ , which is half the size of the interval of the previous iteration. If we set $\\widetilde{\\lambda}_{k}$ equal to the midpoint of $I_{k}$ (which is always positive), then $|\\widetilde{\\lambda}_{k}-\\lambda_{k}|\\le3\\gamma/2$ . We keep repeating the same procedure by halving $\\gamma_{i}$ and $\\delta_{i}$ in each step $i$ ", "page_idx": 54}, {"type": "text", "text": "After $m$ steps we have that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\lambda}_{k}=\\lambda_{k}\\pm3\\gamma_{m}/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "A sufficient termination criterion is $\\gamma_{m}\\approx\\epsilon\\lambda_{k}$ . Instead of $\\lambda_{k}$ (which is unknown) we use $\\widetilde{\\lambda}_{k}$ instead. Then $\\gamma\\leq\\epsilon\\frac{\\widetilde{\\lambda}_{k}-3\\gamma_{m}}{3}$ 3m isalsfent, whicivesatminaoncritern/thatwen actually calculate. In turn, this termination criterion will be reached in the worst case when ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\gamma_{m}\\leq\\epsilon\\frac{\\lambda_{k}-3\\gamma_{m}/2}{6}\\Rightarrow\\gamma_{m}\\leq\\frac{\\epsilon\\lambda_{k}}{9},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "meaning hwia nr $\\epsilon$ acurate $\\lambda_{k}$ in t most $\\begin{array}{r l}{m=O\\left(\\log(\\frac{9}{\\gamma_{m}})\\right)=}&{{}}\\end{array}$ $\\begin{array}{r}{O\\left(\\log(\\frac{1}{\\epsilon\\lambda_{k}})\\right)}\\end{array}$ itertions. ", "page_idx": 54}, {"type": "text", "text": "Note that in each iteration we make a total of $\\Theta(1)$ calls to COUNT. From Lemma E.1, if we plug in the corresponding $\\varepsilon_{i}$ , each call to COUNT during the entire algorithm costs at most ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\log(\\frac{n}{\\gamma_{i}\\delta_{i}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "arithmetic operations using $O\\left(\\log(n)\\log^{4}({\\frac{n}{\\gamma_{i}\\delta_{i}}})\\right)$ bits. The cost is maximized in the last iteration where $\\gamma_{m}=\\Theta(\\epsilon\\lambda_{k})$ and $\\delta_{m}=\\Theta(\\delta_{0}\\epsilon\\lambda_{k})$ , which gives ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\log(\\frac{n}{\\delta_{0}\\epsilon\\lambda_{k}})\\cdot m\\right)=O\\left(T_{\\sf M M}(n)\\log(\\frac{n}{\\delta_{0}\\epsilon\\lambda_{k}})\\log(\\frac{1}{\\epsilon\\lambda_{k}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "arithmetic operations and ", "page_idx": 55}, {"type": "equation", "text": "$$\nO\\left(\\log(n)\\log^{4}({\\frac{n}{\\delta_{0}\\epsilon\\lambda_{k}}})\\right)\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "bits. As in Theorem 3.1, since the failure probability is halved in each iteration, a union bound convergesto $2\\delta_{0}=\\delta$ ", "page_idx": 55}, {"type": "text", "text": "Now we have that $|\\lambda_{k}-\\widetilde{\\lambda}_{k}|\\le\\epsilon\\lambda_{k}$ . Recall that $\\begin{array}{r}{\\sigma_{k}(\\mathbf{A})=\\sqrt{\\lambda_{k}(\\mathbf{A}\\mathbf{A}^{*})}=2\\sqrt{\\lambda_{k}}}\\end{array}$ , since we computed $\\mathbf{AA}^{*}$ and divided by two. If we set $\\widetilde{\\sigma}_{k}=2\\sqrt{\\widetilde{\\lambda}_{k}}$ then ", "page_idx": 55}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\frac{\\sigma_{k}^{2}}{4}\\leq\\frac{\\widetilde{\\sigma}_{k}^{2}}{4}\\leq(1+\\epsilon)\\frac{\\sigma_{k}^{2}}{4}\\Rightarrow(1-\\epsilon)\\sigma_{k}\\leq\\widetilde{\\sigma}_{k}\\leq(1+\\epsilon)\\sigma_{k},\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the last holds since $\\sqrt{1+\\epsilon}\\le1+\\epsilon$ and $1-\\epsilon\\le\\sqrt{1-\\epsilon}$ for $\\epsilon\\in(0,1)$ . Note that we lazily assumed that we computed $\\widetilde{\\sigma}_{k}\\,=\\,2\\sqrt{\\widetilde{\\lambda}_{k}}$ exactly, which does not hold, the square root introduces a machine precision error. The calibration is left as an exercise. If we replace $\\lambda_{k}$ with $\\sigma_{k}^{2}/4$ and $\\delta_{0}=\\delta/2$ in the complexity bounds we get the desired ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\log(\\frac{n}{\\delta\\epsilon\\sigma_{k}})\\log(\\frac{1}{\\epsilon\\sigma_{k}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "arithmetic operations and $O\\left(\\log(n)\\log^{4}({\\frac{n}{\\delta\\epsilon\\sigma_{k}}})\\right)$ bits. ", "page_idx": 55}, {"type": "text", "text": "We now have a tool for the condition number. ", "page_idx": 55}, {"type": "text", "text": "Corollary E.1 (COND). Let $\\mathbf{A}\\in\\mathbb{H}_{++}^{n}$ Given $\\delta\\in(0,1/2)$ , we can compute $\\widetilde{\\kappa}$ such that $\\kappa(\\mathbf{A})\\leq$ $\\widetilde{\\kappa}\\leq32\\kappa(\\mathbf{A})$ , with an algorithm $\\widetilde{\\kappa}\\gets\\mathsf{C O N D}(\\mathbf{A},\\delta)$ in ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\log(\\frac{n\\kappa(\\mathbf{A})}{\\delta})\\log(\\kappa(\\mathbf{A})))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "using $O\\left(\\log(n)\\log^{4}({\\frac{n\\kappa(\\mathbf{A})}{\\delta}})\\right)$ bits of precision with probability at least $1-\\delta$ ", "page_idx": 55}, {"type": "text", "text": "Proof. We first compute $\\begin{array}{r l r}{\\widetilde\\Sigma}&{{}\\in}&{[0.9\\|\\mathbf{A}\\|,2\\|\\mathbf{A}\\|]}\\end{array}$ using_Corollary_ A.1 with parameter $\\delta/3$ in $O(n^{\\dot{2}}\\log(n)\\log(1/\\delta))$ floatingpoint operations using ${\\cal O}(\\log(n))$ bits of precision. The algorithm succeeds with probability at least $\\textstyle1\\,-\\,{\\frac{2\\delta}{3}}$ We then scale $\\mathbf{A}^{\\prime}\\mathbf{\\Lambda}\\leftarrow\\mathbf{\\Lambda}\\mathbf{A}/M$ where $M$ is the smallest power of two that is larger than $4\\widetilde{\\Sigma}$ This implies $\\begin{array}{r l r}{\\frac{1}{16}\\!}&{{}\\le}&{\\!\\|{\\bf A}^{\\prime}\\|\\;\\le\\;\\frac{1}{3.6}}\\end{array}$ , and also $\\begin{array}{r}{\\sigma_{\\mathrm{min}}(\\mathbf{A}^{\\prime})\\;\\in\\;\\left[\\frac{\\sigma_{\\mathrm{min}}(\\mathbf{A})}{16||\\mathbf{A}||},\\frac{\\sigma_{\\mathrm{min}}(\\mathbf{A})}{3.6||\\mathbf{A}||}\\right]\\;=\\;\\left[\\frac{1}{16\\kappa(\\mathbf{A})},\\frac{1}{3.6\\kappa(\\mathbf{A})}\\right]}\\end{array}$ .Thus, it suffices to approximate $\\sigma_{\\mathrm{min}}(\\mathbf{A}^{\\prime})$ and scale it to obtain the desired approximation for $\\kappa(\\mathbf{A})$ ", "page_idx": 55}, {"type": "text", "text": "We now call $\\widetilde{\\sigma}_{\\mathrm{min}}^{\\prime}\\,\\gets\\,\\mathsf{S l G M A K}(\\mathbf{A}^{\\prime},1,1/2,\\delta/3)$ wWhich suceeds probabity $1-\\delta/3$ andreturns $\\begin{array}{r}{\\widetilde{\\sigma}_{\\mathrm{min}}^{\\prime}\\in(1\\pm\\frac{1}{2})\\widetilde{\\sigma}_{\\mathrm{min}}(\\mathbf{A}^{\\prime})}\\end{array}$ .It requires ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\mathsf{M M}}(n)\\left(\\log(\\frac{n}{\\delta\\sigma_{\\operatorname*{min}}(\\mathbf{A}^{\\prime})})\\log(\\frac{1}{\\sigma_{\\operatorname*{min}}(\\mathbf{A}^{\\prime})})\\right)\\right)=O\\left(T_{\\mathsf{M M}}(n)\\log(\\frac{n\\kappa(\\mathbf{A})}{\\delta})\\log(\\kappa(\\mathbf{A}))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "arithmetic operations and ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\log(n)\\log^{4}(\\frac{n}{\\delta\\sigma_{\\operatorname*{min}}(\\mathbf{A}^{\\prime})})\\right)=O\\left(\\log(n)\\log^{4}(\\frac{n\\kappa(\\mathbf{A})}{\\delta})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "bits. Then we can set $\\begin{array}{r}{\\widetilde{\\kappa}=\\frac{1}{\\widetilde{\\sigma}_{\\mathrm{min}}^{\\prime}}\\in[\\kappa(\\mathbf{A}),32\\kappa(\\mathbf{A})]}\\end{array}$ Th succes probabilityis $\\begin{array}{r}{1\\!-\\!\\frac{2\\delta}{3}\\!-\\!\\frac{\\delta}{3}=1\\!-\\!\\delta}\\end{array}$ \uff1a\u53e3 ", "page_idx": 55}, {"type": "text", "text": "E.4Proof of Theorem 1.1 ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "We now have all the prerequisites to prove that Algorithm 1 provides the guarantees of our main Theorem 1.1, which we restate for readability. ", "page_idx": 55}, {"type": "text", "text": "Theorem E.2 (Restatement of Theorem 1.1). Let $(\\mathbf{H},\\mathbf{S})$ be a Hermitian definite pencil of size $n$ with $\\lVert\\mathbf{H}\\rVert,\\lVert\\mathbf{S}^{-1}\\rVert\\leq1$ and $\\lambda_{1}\\leq\\lambda_{2}\\leq\\ldots\\leq\\lambda_{n}$ its eigenvalues. There exists an algorithm ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\widetilde{\\Pi}_{k}\\gets\\mathsf{P R O J E C T O R}(\\mathbf{H},\\mathbf{S},k,\\epsilon),\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "which takes as input H, S, an integer $1\\leq k\\leq n-1$ , an error parameter $\\epsilon\\in(0,1)$ and returns a matrix $\\tilde{\\Pi}_{k}$ such that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[\\left\\|\\widetilde{\\boldsymbol{\\Pi}}_{k}-\\boldsymbol{\\Pi}_{k}\\right\\|\\leq\\epsilon\\right]\\geq1-1/n,\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where ${\\bf{\\Pi}}_{{\\bf{I}}{\\bf{k}}}$ is the true spectral projector on the invariant subspace that is associated with the $k$ smallest eigenvalues.The algorithm executes ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\mathsf{M M}}(n)\\left(\\log(\\frac{n}{\\mathrm{gap}_{k}})\\log(\\frac{1}{\\mathrm{gap}_{k}})+\\log(n\\kappa(\\mathbf{S}))\\log(\\kappa(\\mathbf{S}))+\\log\\left(\\log(\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\right)\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "foating point operations, using ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\log(n)\\left(\\log^{4}(\\frac{n}{\\mathrm{gap}_{k}})+\\log^{4}(n\\kappa(\\mathbf{S}))+\\log^{3}(\\frac{1}{\\epsilon\\,\\mathrm{gap}_{k}})\\log(\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "bits of precision, where $\\kappa(\\mathbf{S})\\;=\\;\\|\\mathbf{S}\\|\\|\\mathbf{S}^{-1}\\|$ and $\\mathrm{{gap}}_{k}\\:=\\:\\lambda_{k+1}\\:-\\:\\lambda_{k}$ . Internally, the algorithm needs to generate a total of at most ${\\widetilde{O}}(n)$ standard normal foating point numbers using additional $O(\\log(\\log(n)))$ bits. In the regular case, when $\\mathbf{S}=\\mathbf{I}$ the $O(\\log(n\\kappa(\\mathbf{S}))\\log(\\kappa(\\mathbf{S})))$ term in the arithmetic complexity and the $O(\\log^{4}(n\\kappa(\\mathbf{S})))$ I term in the number of bits are removed. ", "page_idx": 56}, {"type": "text", "text": "Proof. We first compute ${\\widetilde{\\mu}}_{k}$ $\\sqrt{\\mathrm{gap}_{k},\\!\\widetilde{\\kappa}}$ which satisfy the requirements of Proposition 2.1 using COND and GAP. Afterwards we can use them to call PURIFY to obtain the spectral projector $\\tilde{\\Pi}_{k}$ such that $\\|\\widetilde{\\boldsymbol{\\Pi}}_{k}-\\boldsymbol{\\Pi}_{k}\\|\\leq\\epsilon$ We frst cll $\\mathsf{C O N D}(\\mathbf{S},\\textstyle\\frac{1}{4},\\textstyle\\frac{1}{2n})$ , which, from Corollary E. 1, requires ", "page_idx": 56}, {"type": "equation", "text": "$$\nO\\left(T_{\\sf M M}(n)\\log(n\\kappa(\\mathbf{S}))\\log(\\kappa(\\mathbf{S}))\\right)\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "arithmetic operations and ", "page_idx": 56}, {"type": "equation", "text": "$$\nO\\left(\\log(n)\\log^{4}(n\\kappa(\\mathbf{S}))\\right)\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "bits. Then, given the result of COND, we cal $\\mathsf{G A P}(\\mathbf{H},\\mathbf{S},k,\\frac{1}{8},\\frac{1}{2n})$ which requires ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\log(\\frac{n}{\\mathrm{gap}_{k}})\\log(\\frac{1}{\\mathrm{gap}_{k}}))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "arithmetic operations and ", "page_idx": 56}, {"type": "equation", "text": "$$\nO\\left(\\log(n)\\log^{4}({\\frac{n}{\\operatorname{gap}_{k}}})+\\log(\\kappa(\\mathbf{S}))\\right)\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "bits. They both succeed at the same time with $1-1/n$ probability. PURIFY $({\\bf H},{\\bf S},\\widetilde{\\mu}_{k},\\widetilde{\\mathrm{gap}}_{k},\\widetilde{\\kappa},\\epsilon)$ requires ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\left(\\log(\\frac{1}{\\sf g a p}_{k})+\\log(\\log(\\frac{\\kappa(\\bf S)}{\\epsilon\\operatorname{gap}_{k}}))\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "floating point operations and ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\log(n)\\log^{3}(\\frac{1}{\\mathrm{gap}_{k}})\\log(\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "bits. The total arithmetic complexity is therefore ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\mathsf{M M}}(n)\\left(\\log(\\frac{n}{\\mathrm{gap}_{k}})\\log(\\frac{1}{\\mathrm{gap}_{k}})+\\log(n\\kappa(\\mathbf{S}))\\log(\\kappa(\\mathbf{S}))+\\log\\left(\\log(\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\right)\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "and the bit requirement is ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\log(n)\\left(\\log^{4}(\\frac{n}{\\mathrm{gap}_{k}})+\\log^{4}(n\\kappa(\\mathbf{S}))+\\log^{3}(\\frac{1}{\\mathrm{gap}_{k}})\\log(\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Notice that the $\\epsilon$ term appears only inside log log in the arithmetic complexity and in only one of the log factors of the number of bits. \u53e3 ", "page_idx": 56}, {"type": "text", "text": "F DFT background ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Density Functional Theory (DFT) [88], which was awarded the Nobel prize in Chemistry in 1998, is considered as one of the most common methods to perform electronic structure calculations thanks to its ab initio character. That means that no input besides the initial atomic structure is required to determine the relaxed geometry, energy levels, and the electron structure of a given material, in their bulk or nanostructured form. The main idea behind DFT is to describe the system and its properties by the electron density only. DFT calculations are widely used in industry and in academia to predict, for example, the properties of novel materials [102] or to optimize the performance of batteries [69], solar cells [142], or nanoelectronic devices [85]. Many scientific libraries implementing DFT algorithms have been developed, which persistently occupy supercomputing clusters and receive up to tens of thousands of citations annually at the time of this writing [89, 59, 127, 74]. Despite the success of DFT calculations, important theoretical aspects of their underlying algorithms remain unclear. ", "page_idx": 57}, {"type": "text", "text": "Assume a system that is composed of a set of $n_{a}~\\in~\\mathbb{N}$ atoms positioned inside a fixed threedimensional domain. The Kohn-Sham equation in real-space describes the electronic wave function $\\psi(\\mathbf{r})$ of thesystem ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\mathbf{r})\\psi(\\mathbf{r})=E\\psi(\\mathbf{r}),\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where ${\\bf r}=(x,y,z)$ is any position in the domain. Here $\\mathcal{H}(\\mathbf{r})$ is the so-called single-particle Hamiltonian operator of the system, which can be written in the following form: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\mathbf{r})=-\\frac{1}{2}\\nabla^{2}+E_{i o n}(\\mathbf{r})+\\int\\frac{n(\\mathbf{r}^{\\prime})}{\\|\\mathbf{r}-\\mathbf{r}^{\\prime}\\|}d\\mathbf{r}^{\\prime}+E_{x c}(\\mathbf{r}),\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where the first term is the kinetic term, the second term accounts for electron-ion interactions, the third term represents the electron-electron interactions, which are solved at the Hartree level through Poisson's equation, and the fourth term is the so-called exchange-correlation term. The electron density $n(\\mathbf{r})$ is the main observable of interest in DFT. It is related to the solutions $E_{i}$ and $\\psi_{i}(\\mathbf{r})$ Of Eq. (20) through ", "page_idx": 57}, {"type": "equation", "text": "$$\nn({\\bf r})=\\sum_{i}f(E_{i};E_{F})|\\psi_{i}({\\bf r})|^{2},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $E_{F}$ is the Fermi level of the system, $E_{i}$ is the energy level (eigenvalue) corresponding to the wave function $\\psi_{i}(\\mathbf{r})$ , and $f(E_{i};E_{F})$ is the occupation term. At zero temperature $(T=0K)$ $f(E_{i};E_{F})$ is equal to one for occupied states $(E_{i}\\,<\\,E_{F})$ and zero otherwise.3  The $E_{F},\\,E_{i}$ , and $\\psi_{i}$ are the unknown quantities for which there exists no analytical solution, except for some special cases. Approximate numerical solutions can be sought after expanding the wave functions into a suitable basis. In this case, the $\\psi_{i}$ 's are approximated by a set of $n\\,=\\,\\Theta(n_{a})$ basis functions $\\chi_{j}(\\mathbf{r}),j\\,\\in\\,[n]$ , typically localized around atomic positions.4 Common basis sets that lead to efficient algorithms to solve the eigenvalue problem of Eq. (20) include Linear Combination of Atomic Orbitals (LCAO) [32], Gaussian-Type Orbitals (GTO) [22], or Maximally Localized Wannier Functions (MLWF) [101]. In this context, the wave functions take the form of linear combinations of the $n$ localized orbitals. In particular, for all $i\\in[n]$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\psi_{i}(\\mathbf{r})=\\sum_{i=1}^{n}c_{i j}\\chi_{j}(\\mathbf{r}),\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $c_{i j}$ are the (unknown) complex coeffcients. This expansion can also be written in matrix notation as ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathbf{y}(\\mathbf{r})={\\binom{\\psi_{1}(\\mathbf{r})}{\\psi_{2}(\\mathbf{r})}}={\\binom{c_{11}\\quad c_{12}\\quad\\dots\\quad c_{1n}}{c_{21}\\quad c_{22}\\quad\\dots\\quad c_{2n}}}\\,{\\binom{\\chi_{1}(\\mathbf{r})}{\\chi_{2}(\\mathbf{r})}}=\\mathbf{Cx}(\\mathbf{r}),\\quad\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $\\mathbf{y}(\\mathbf{r})\\in\\mathbb{C}^{n}$ and $\\mathbf{x}(\\mathbf{r})\\in\\mathbb{C}^{n}$ are (complex) vectors and $\\mathbf{C}$ isa $n\\times n$ matrix. By expanding the wave functions into LCAO, GTO, or MLWF basis, the Hamiltonian operator becomes a matrix ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathbf{H}_{i,j}=\\langle\\chi_{i}|\\mathcal{H}|\\chi_{j}\\rangle=\\int\\chi_{i}^{*}(\\mathbf{r})\\mathcal{H}(\\mathbf{r})\\chi_{j}(\\mathbf{r})d r.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Similarly, an overlap matrix S must be introduced ", "page_idx": 58}, {"type": "equation", "text": "$$\n{\\bf{S}}_{i,j}=\\langle\\chi_{i}|\\chi_{j}\\rangle=\\int\\chi_{i}^{*}({\\bf{r}})\\chi_{j}({\\bf{r}})d r,\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "which is equal to the identity matrix $\\mathbf{I}$ if the basis elements are orthogonal (e.g. $\\mathbf{S}=\\mathbf{I}$ for MLWF). The entries of $\\mathbf{H}$ and $\\mathbf{S}$ represent the interaction between two basis elements that can be located on the same or on different atoms. Furthermore, due to the complex conjugate property of the inner product, both $\\mathbf{H}$ and S are Hermitian matrices. Moreover, S is positive-definite ([103, Chapter 3]). Putting everything together, the expansion coefficients $c_{i j}$ are obtained by solving the following generalized eigenvalue problem as already formulated in Eq. (1): ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathbf{H}\\mathbf{C}=\\mathbf{S}\\mathbf{C}\\mathbf{A}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "The so-called density matrix $\\mathbf{P}$ can be derived from the $\\mathbf{C}$ and $\\Lambda$ solutions ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathbf{P}=\\mathbf{C}f(\\mathbf{A};E_{F})\\mathbf{C}^{*}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "The density matrix possesses interesting properties. The matrix PS is a projector matrix and therefore it is idempotent, i.e. $\\mathbf{PS}=(\\mathbf{PS})^{\\tilde{2}}$ . Moreover, it can be used to derive the electron density at anyposition $r$ ", "page_idx": 58}, {"type": "equation", "text": "$$\nn(\\mathbf{r})=\\sum_{i,j\\in n}\\mathbf{P}_{i j}\\chi_{i}(\\mathbf{r})\\chi_{j}^{*}(\\mathbf{r})=\\mathbf{x}^{*}(\\mathbf{r})\\mathbf{P}\\mathbf{x}(\\mathbf{r}),\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\mathbf{x}(\\mathbf{r})\\,\\in\\,\\mathbb{C}^{n}$ is a vector such that ${\\bf x}_{i}({\\bf r})\\;=\\;\\chi_{i}({\\bf r})$ ,i.e. $\\mathbf{x}(\\mathbf{r})$ contains the values of all the basis functions at position $r$ . Given these basic definitions, we next give an overview of existing algorithms. ", "page_idx": 58}, {"type": "text", "text": "F.1 Linear scaling methods ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Because of the strong localization of the $\\chi_{i}(\\mathbf{r})$ 's, the $(i,j)$ interactions rapidly decay with the distance between the atoms and can be discarded beyond a pre-defined cut-off radius. As a consequence, $\\mathbf{H}$ and S are often banded matrices with sometimes a sparse band. It should also be noted that at equilibrium, the number of negatively charged electrons present in the system (which corresponds to the integral of $n(\\mathbf{r})$ over space) must exactly compensate the number of positively charged protons $Z_{p}$ . This allows for the computation of the Fermi level: from Eq. (25) and the definition of S, it follows that $\\operatorname{trace}(\\mathbf{PS})\\,=\\,Z_{p}$ , which means that one can perform a binary search on the Fermi level $E_{F}$ until $\\mathrm{trace}(\\mathbf{PS})=Z_{p}^{'}$ is satisfied. These observation has led to the development of so-called linear scaling methods, which aim to exploit the sparsity of the matrices to obtain faster solutions [143, 57, 86, 87, 56, 62, 63]. Typically, the density matrix and the Fermi level are iteratively computed, until all required constraints are satisfied, i.e., the density matrix should be idempotent and $\\mathrm{trace}(\\mathbf{PS})=Z_{p}$ . This can be done, for example, by approximating the matrix sign function using a Newton iteration [136]. To optimize performance, heuristics such as truncating matrix elements with negligible magnitude are applied, at the cost of decreasing the solution accuracy. Empirical evidence shows that such methods tend to scale nearly linearly to the system size, however, they do not exhibit provable theoretical guarantees. Similar methods have been studied based on finite-differences, which aim to approximate the Fermi distribution with polynomial expansions [145, 144]. Finally, the work of [65, 137] offer the possibility to achieve lower complexities than $\\bar{O}(n^{3})$ for both the eigenvalues and eigenvectors of structured matrices, by leveraging the fast multipole method. However, an end-to-end stability analysis of those algorithms remains open. Indeed, we are not aware of any end-to-end analysis with provable approximation guarantees that has lower than $O(n^{3})$ worst-case complexity for any of the aforementioned algorithms. ", "page_idx": 58}, {"type": "text", "text": "F.2  Density matrix ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "We can directly apply Theorem 1.1 to compute density matrices in DFT. ", "page_idx": 58}, {"type": "text", "text": "The following Theorem summarizes the analysis of Algorithm 6 for the density matrix ", "page_idx": 58}, {"type": "image", "img_path": "Wyp8vsL9de/tmp/575ede9ccf8bf24a9aa2d3a0c441f8cbcc8a5015095f5501d5517828b52d3e4d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 59}, {"type": "text", "text": "Algorithm 6: DENSITY. ", "page_idx": 59}, {"type": "text", "text": "Theorem F.1 (DENSITY). Given a Hermitian definite pencil $(\\mathbf{H},\\mathbf{S})$ with $\\|\\mathbf H\\|$ $\\|\\mathbf{S}\\|\\leq1$ an integer $k\\in[n\\!-\\!1]$ denoting the number of occupied states in the system, and $\\epsilon\\in(0,1)$ Algorithm $^{6}$ returns a matrix $\\widetilde{\\mathbf{P}}\\gets\\mathsf{D E N S I T Y}(\\mathbf{H},\\mathbf{S},k,\\epsilon)$ such that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\mathbf{P}}-\\mathbf{P}\\|\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where $\\mathbf{P}$ is the true density matrix of the system and succeeds with probability at least $1-O\\left(1/n\\right)$ The floating point arithmetic complexity is ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\mathsf{M M}}(n)\\left(\\log(\\frac{n}{\\mathrm{gap}_{k}})\\log(\\frac{1}{\\mathrm{gap}_{k}})+\\log(n\\kappa(\\mathbf{S}))\\log(\\kappa(\\mathbf{S}))\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "and it requires ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\log(n)\\left(\\log^{4}(\\frac{n}{\\mathrm{gap}_{k}})+\\log^{4}(n\\kappa(\\mathbf{S}))+\\log^{3}(\\frac{1}{\\epsilon\\,\\mathrm{gap}_{k}})\\log(\\frac{\\kappa(\\mathbf{S})}{\\epsilon\\,\\mathrm{gap}_{k}})\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "bits of precision. ", "page_idx": 59}, {"type": "text", "text": "Proof. In the first step of the algorithm we compute $\\widetilde{\\bf\\Pi}^{\\prime}$ such that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Pi-\\widetilde{\\Pi}\\|\\le\\frac{\\epsilon}{32}:=\\epsilon_{\\Pi},}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where $\\mathbf{\\delta}\\pi$ is the spectral projector on the invariant subspace of the definite pencil associated with the $k$ smallest eigenvalues. ", "page_idx": 59}, {"type": "text", "text": "Then we invert $\\mathbf{S}$ , and the final step of the algorithm computes $\\widetilde{\\mathbf{P}}$ . Unrolling all the computations, $\\widetilde{\\mathbf{P}}$ can be written as $\\mathbf{\\Pi}\\mathbf{I}\\mathbf{S}^{-1}\\mathbf{I}\\mathbf{I}^{*}+\\mathbf{E_{P}}$ where $\\mathbf{E_{P}}$ is a Hermitian error matrix (yet to be detailed). It is not hard to verify that $\\mathbf{I}\\mathbf{S}^{-1}\\mathbf{I}\\mathbf{I}^{*}$ is equal to $\\mathbf{P}$ , i.e. the true density matrix. ", "page_idx": 59}, {"type": "text", "text": "It thus remains to bound $\\|\\mathbf{E_{P}}\\|$ We can directly force $\\lVert{\\bf E}_{1}^{\\sf I N V}\\rVert\\,\\leq\\,\\epsilon_{\\bf I I}\\lVert{\\bf S}^{-1}\\rVert\\,\\leq\\,\\epsilon_{\\bf I I}$ by setting ${\\bf u}\\le$ $\\epsilon_{\\boldsymbol{\\Pi}}\\frac{1}{\\mu_{|\\boldsymbol{\\mathsf{N}}\\boldsymbol{\\mathsf{V}}}(n)\\kappa(\\boldsymbol{\\mathbf{S}})^{c}\\mathsf{I}\\mathsf{N}\\mathsf{V}^{\\mathrm{\\,log}(n)}}$ Then from line 5 of Algorithm 6\" ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{{\\mathbf{P}}}=\\widetilde{{\\mathbf{I}}}{\\mathbf{S}}_{[\\mathsf{N V}}\\widetilde{{\\mathbf{I}}}^{*}+\\widetilde{{\\mathbf{I}}}{\\mathbf{E}}_{2}^{\\mathsf{M M}}+{\\mathbf{E}}_{3}^{\\mathsf{M M}}}\\\\ &{\\quad=\\left({\\mathbf{I}}+{\\mathbf{E}}_{\\Pi}\\right)\\left({\\mathbf{S}}^{-1}+{\\mathbf{E}}_{1}^{\\mathsf{M V}}\\right)\\left({\\mathbf{II}}+{\\mathbf{E}}_{\\Pi}\\right)^{*}+\\left({\\mathbf{II}}+{\\mathbf{E}}_{\\Pi}\\right){\\mathbf{E}}_{2}^{\\mathsf{M M}}+{\\mathbf{E}}_{3}^{\\mathsf{M M}}}\\\\ &{\\quad={\\mathbf{IIS}}^{-1}{\\mathbf{I}}^{*}+{\\mathbf{IIE}}_{1}^{\\mathsf{I N}}{\\mathbf{I}}^{*}+{\\mathbf{II}}\\left({\\mathbf{S}}^{-1}+{\\mathbf{E}}_{1}^{\\mathsf{I N}}\\right){\\mathbf{E}}_{\\Pi}^{*}+{\\mathbf{E}}_{\\Pi}\\left({\\mathbf{S}}^{-1}+{\\mathbf{E}}_{1}^{\\mathsf{I N}}\\right){\\mathbf{I}}^{*}+\\dots}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\cdot+{\\mathbf{E}}_{\\Pi}\\left({\\mathbf{S}}^{-1}+{\\mathbf{E}}_{1}^{\\mathsf{I N}}\\right){\\mathbf{E}}_{\\Pi}^{*}+\\left({\\mathbf{II}}+{\\mathbf{E}}_{\\Pi}\\right){\\mathbf{E}}_{2}^{\\mathsf{M M}}+{\\mathbf{E}}_{3}^{\\mathsf{M M}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "We bound each term as follows. ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{\\Pi}\\mathbf{E}_{1}^{\\mathsf{I N V}}\\boldsymbol{\\Pi}^{*}\\|\\leq\\|\\mathbf{E}_{1}^{\\mathsf{I N V}}\\|\\leq\\epsilon\\mathbf{\\Pi},\n$$", "text_format": "latex", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Pi\\left(\\mathbf{S}^{-1}+\\mathbf{E}_{1}^{\\mathrm{[NV]}}\\right)\\mathbf{E}_{\\Pi}^{*}\\right\\|\\leq(1+\\epsilon_{\\Pi})\\cdot\\epsilon_{\\Pi}\\leq2\\epsilon_{\\Pi},}\\\\ &{\\left\\|\\mathbf{E}_{\\Pi}\\left(\\mathbf{S}^{-1}+\\mathbf{E}_{1}^{\\mathrm{[NV]}}\\right)\\mathbf{E}_{\\Pi}^{*}\\right\\|\\leq(1+\\epsilon_{\\Pi})\\cdot\\epsilon_{\\Pi}^{2}\\leq2\\epsilon_{\\Pi}^{2},}\\\\ &{\\|\\mathbf{E}_{2}^{\\mathrm{MM}}\\|\\leq\\mathbf{u}\\mu(n)\\|\\mathbf{S}_{\\Pi\\mathrm{NV}}\\|\\|\\widetilde{\\mathbf{I}}^{*}\\|\\leq\\mathbf{u}\\mu(n)\\left(\\|\\mathbf{S}^{-1}\\|+\\|\\mathbf{E}_{1}^{\\mathrm{[NV]}}\\|\\right)(1+\\|\\mathbf{E}_{\\Pi}\\|)\\ll\\epsilon_{\\Pi}(1+\\epsilon_{\\Pi})(1+\\epsilon_{\\Pi})\\|\\mathbf{\\Gamma}(1+\\epsilon_{\\Pi})\\|\\mathbf{S}^{*}\\|,}\\\\ &{\\epsilon_{\\Pi}\\leq4\\epsilon_{\\Pi},}\\\\ &{\\|\\mathbf{E}_{3}^{\\mathrm{MM}}\\|\\leq\\mathbf{u}\\mu(n)\\|\\widetilde{\\mathbf{I}}\\|\\|\\mathbf{S}_{\\Pi\\mathrm{V}}\\widetilde{\\mathbf{I}}^{*}+\\mathbf{E}_{2}^{\\mathrm{MM}}\\|\\ll\\epsilon_{\\Pi}(1+\\epsilon_{\\Pi})\\left[(1+\\epsilon_{\\Pi})(1+\\epsilon_{\\Pi})+4\\epsilon_{\\Pi}\\right]\\leq\\epsilon_{\\Pi}\\cdot}\\\\ &{2\\cdot(4+4\\epsilon_{\\Pi})\\leq16\\epsilon_{\\Pi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Putting everything together we have that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathbf{P}-\\tilde{\\mathbf{P}}\\right\\Vert\\leq\\epsilon\\mathbf{\\Pi}+2\\epsilon\\mathbf{\\Pi}+2\\epsilon\\mathbf{\\Pi}+4\\epsilon\\mathbf{\\Pi}+16\\epsilon\\mathbf{\\Pi}\\leq25\\epsilon\\mathbf{\\Pi}=\\epsilon\\frac{25}{32}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Note that the aforementioned value of $\\mathbf{u}$ is already dominated by the requirement of PROJECTOR. The same holds for the arithmetic complexity. Therefore, the arithmetic complexity, the success probability, and the number of required bits, similar to those of Theorem 1.1 up to a constant factor. ", "page_idx": 60}, {"type": "text", "text": "F.3  Electron density queries ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Having a good approximation of the density matrix, we can use it to query for the electron density at any point of interest. A direct application of Theorem F.1 gives the following. ", "page_idx": 60}, {"type": "text", "text": "Corollary F.1. Given a position r in the atomic domain, let $\\mathbf{x}$ beavector $\\begin{array}{r l}{\\mathbf{x}}&{{}=}\\end{array}$ $(\\chi_{1}(\\mathbf{r})\\quad\\chi_{2}(\\mathbf{r})\\quad.\\dots\\quad\\chi_{n}(\\mathbf{r}))$ , where $\\chi_{i}$ are the basis functions that were used to construct the Hamiltonian and overlap matrices H, S. Let $\\widetilde{\\mathbf P}\\gets\\mathsf{D E N S I T Y}(\\mathbf H,\\mathbf S,\\epsilon,k)$ where $k$ is the number of occupied states in the atomic system,and let $\\mathbf{P}$ be the true density matrix. Let $n(\\mathbf{r})=\\mathbf{x}^{*}\\mathbf{P}\\mathbf{x}$ be the true electron density at $\\mathbf{r}$ and $\\widetilde{n}(\\mathbf{r})=\\mathbf{x}^{*}\\widetilde{\\mathbf{P}}\\mathbf{x}$ . Then as long as DENSITY succeeds, ", "page_idx": 60}, {"type": "equation", "text": "$$\n|n(\\mathbf{r})-\\widetilde{n}(\\mathbf{r})|\\leq8\\epsilon\\|\\mathbf{x}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Proof. First we recall that $\\epsilon\\,\\in\\,(0,1/12)$ and note that $\\|\\widetilde{\\mathbf{P}}\\|\\leq\\|\\mathbf{P}\\|+\\epsilon\\,\\leq\\,1+\\epsilon$ The quadratic form $\\mathbf{x}^{*}\\widetilde{\\mathbf{P}}\\mathbf{x}$ is computed in two steps: $\\mathbf{y}\\,\\leftarrow\\,\\mathbf{fl}(\\widetilde{\\mathbf{P}}\\mathbf{x})$ and $\\widetilde{n}(\\mathbf{r})\\,=\\,\\mathbf{fl}(\\mathbf{x}^{*}\\mathbf{y})$ . From [70], Eq. (3.12) and Lemma 6.6, we know that the matrix-vector product satisfies $\\mathbf{y}\\;=\\;\\widetilde{\\mathbf{P}}\\mathbf{x}+\\mathbf{e}$ where $\\lVert\\mathbf{e}\\rVert=$ IIPx - yll \u2264\u221annn|IPll\u00d7ll, and nn := 1-u . For the chosen $\\mathbf{u}$ we have $\\sqrt{n}\\eta_{n}\\,\\leq\\,2\\epsilon$ From the same equations, the subsequent dot product satisfies $\\widetilde{n}(\\mathbf{r})=\\mathbf{fl}(\\mathbf{x}^{*}\\mathbf{y})=\\mathbf{x}^{*}\\mathbf{y}+\\epsilon$ where $|\\epsilon|\\le$ $\\eta_{n}\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|\\leq\\eta_{n}\\|\\mathbf{x}\\|\\left(\\|\\tilde{\\mathbf{P}}\\mathbf{x}\\|+\\|\\mathbf{e}\\|\\right)\\leq\\eta_{n}\\|\\tilde{\\mathbf{P}}\\|\\|\\mathbf{x}\\|^{2}\\left(1+\\sqrt{n}\\eta_{n}\\right)\\leq4\\epsilon\\|\\mathbf{x}\\|^{2}$ . Then $|n(\\mathbf{r})-\\widetilde{n}(\\mathbf{r})|=$ $\\begin{array}{r}{\\mathbf{x}^{*}\\mathbf{P}\\mathbf{x}-(\\mathbf{x}^{*}\\mathbf{y}+\\epsilon)|=|\\mathbf{x}^{*}\\mathbf{P}\\mathbf{x}-\\mathbf{x}^{*}\\mathbf{\\tilde{P}}\\mathbf{x}-\\mathbf{x}^{*}\\mathbf{e}-\\epsilon|\\leq|\\mathbf{x}^{*}\\mathbf{P}\\mathbf{x}-\\mathbf{x}^{*}\\mathbf{\\tilde{P}}\\mathbf{x}|+|\\mathbf{x}^{*}\\mathbf{e}|+|\\epsilon|\\leq2\\epsilon\\|\\mathbf{x}\\|^{2}+}\\\\ {*\\epsilon\\|\\mathbf{x}\\|^{2}+4\\epsilon\\|\\mathbf{x}\\|^{2}=8\\epsilon\\|\\mathbf{x}\\|^{2}.}\\end{array}$ ", "page_idx": 60}, {"type": "text", "text": "This can be generalized for many different points $\\mathbf{r}$ . If $N$ is the number of those points and $\\cal{N}~=~\\Theta(n)$ (which is typically the case in applications), then one can stack the vectors $\\mathbf{x}(\\mathbf{r}_{1}),\\mathbf{x}(\\mathbf{r}_{2}),\\ldots,\\mathbf{x}(\\mathbf{r}_{N})$ as the rows of a matrix $\\mathbf{X}$ and use fast matrix multiplication to compute all $n(\\mathbf{r}_{i})$ simultaneously, instead of querying each $n({\\bf r}_{i})$ one-by-one. ", "page_idx": 60}, {"type": "text", "text": "If $N$ is asymptotically larger than the system size, i.e., $N\\,=\\,{\\cal O}(\\mathrm{poly}(n))$ ,thequerycomplexity can be reduced by applying the Johnson-Lindenstrauss (JL) lemma [78], at the cost of a polynomial dependence on the accuracy. In brief, since the matrix PS is an orthogonal projector, and therefore idempotent, it can be deduced that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathbf{x}^{*}\\mathbf{P}\\mathbf{x}=\\mathbf{x}^{*}\\mathbf{P}\\mathbf{S}\\mathbf{P}\\mathbf{x}=\\mathbf{x}^{*}\\mathbf{P}\\mathbf{L}^{-*}\\mathbf{L}^{-1}\\mathbf{P}\\mathbf{x}=\\|\\mathbf{x}^{*}\\mathbf{P}\\mathbf{L}^{-*}\\|^{2},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $\\mathbf{L}$ is the lower triangular Cholesky factor of $\\mathbf{S}^{-1}$ . Then $\\lVert\\mathbf{x}^{*}\\mathbf{P}\\mathbf{L}^{-*}\\rVert^{2}$ can be approximated by $\\|\\mathbf{x}^{*}\\mathbf{P}\\mathbf{L}^{-*}\\mathbf{T}\\|^{2}$ , where $\\mathbf{T}$ is a $n\\times r$ sparse random sign matrix scaled by $1/\\sqrt{r}$ [1], with $r=$ $O(\\log(N)/\\epsilon^{2})$ columns. The representation of $\\mathbf{T}$ requires $O(n\\!\\times\\!r)=O(n\\log(N)/\\epsilon^{2})$ random bits. Alternative random matrix distributions satisfying the $\\mathrm{JL}$ property, with varying sparsity, runtime, and dimension requirements exist in the literature [3, 82, 110]. The value of $r$ is worst-case optimal [93], however, tighter bounds have been studied in the literature [21, 126, 23, 49], thus potentially leading to more practical algorithms. The full analysis is omitted. ", "page_idx": 60}, {"type": "text", "text": "", "page_idx": 61}, {"type": "text", "text": "G   Deflation and bit complexity of PCA ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Since its introduction in the early twentieth century [116, 72], Principal Component Analysis is one of the most important tools in statistics, data science, and machine learning. It can be used, for example, to visualize data, to reduce dimensionality, or to remove noise in measurements; cf. [79, 45] for reviews on the vast bibliography. In its simplest formulation, given a (centered) data matrix $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ , the goal is to find a $k$ dimensional embedding $\\mathbf{C}_{k}$ where $k<n$ , that maximizes the sample variance, which can be written as an optimization problem ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathbf{C}_{k}=\\arg\\operatorname*{max}_{\\mathbf{C}^{\\top}\\mathbf{C}=\\mathbf{I}_{k\\times k}}\\mathrm{tr}(\\mathbf{C}^{\\top}\\mathbf{H}\\mathbf{C}),\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where $\\mathbf{H}=\\mathbf{X}^{\\top}\\mathbf{X}\\in\\mathbb{R}^{n\\times n}$ is the sample covariance. It can be shown that the solution $\\mathbf{C}_{k}$ corresponds to the principal $k$ singular vectors of $\\mathbf{H}$ , i.e. the ones that correspond to the largest $k$ singular values. Evidently, since the sample covariance is always symmetric and positive semi-definite, this can be written as a Hermitian eigenvalue problem ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathbf{H}\\mathbf{C}=\\mathbf{C}\\Lambda,\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "(which is indeed a definite GEP as in Equation (1) with $\\mathbf{S}=\\mathbf{I}_{\\alpha}$ 0. This way we can project the data in $k$ dimensions by computing $\\mathbf{XC}_{k}$ , preserving as much of the variance in $k$ dimensions as possible. Classically, PCA can be solved by approximating the SVD of $\\mathbf{H}$ via diagonalization. ", "page_idx": 61}, {"type": "text", "text": "G.1 Vanilla PCA ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "With this short introduction, we can now describe how to enhance the analysis of PCA algorithms by using our results for spectral projectors to obtain forward-error guarantees for all the SVDrelated computations. Using our Theorem 1.1, the SIGMAK algorithm of Proposition E.2, and the DEFLATE algorithm of [19], as described in Algorithm 7, we can state the following result for the classic (\"vanilla\") PCA, which provides forward error guarantees in matrix multiplication time regardlesshowlarge $k$ is. This is can serve as the backbone for more advanced algorithms that are analyzed in the next section, which typically first use random projections to reduce the matrix size toapproximately $O(k\\times k)$ and then perform the SVD computation on the smaller matrix. ", "page_idx": 61}, {"type": "text", "text": "Remark G.1. For simplicity, we have assumed that the covariance matrix contains no errors. This might not be true if H is computed numerically from the data matrix X. It is not hard to extend the algorithm to take this into account as well, and nothing changes in the analysis (except some negligible constant factors). To handle errors in the input matrix one can adapt the analysis from thenextSectionG.2. ", "page_idx": 61}, {"type": "text", "text": "Theorem G.1 (PCA). Let $\\mathbf{H}$ be an $n\\times n$ symmetric sample covariance matrix of a centered data matrix $\\textbf{X}\\in\\,\\mathbb{R}^{m\\times n}$ , i.e. $\\mathbf{H}\\,=\\,\\mathbf{X}^{\\top}\\mathbf{X}$ $\\|\\mathbf{H}\\|\\,\\leq\\,1$ \uff0c $k~\\in~[n]$ is a target rank, and $\\epsilon\\,\\in\\,(0,1)$ an accuracy parameter Then we can compute a matrix $\\tilde{\\mathbf{C}}_{k}$ with $k$ columns such that $\\|\\mathbf{X}{-}\\mathbf{X}\\widetilde{\\mathbf{C}}_{k}\\widetilde{\\mathbf{C}}_{k}^{\\top}\\|\\leq$ $(1+\\epsilon)\\|\\mathbf{X}-\\mathbf{X}\\mathbf{C}_{k}\\mathbf{C}_{k}^{\\top}\\|$ where $\\mathbf{C}_{k}\\in\\mathbb{R}^{n\\times k}$ contains the top- $k$ (right) singular vectors of $\\mathbf{X}$ in ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\mathsf{M M}}(n)\\left(\\log(\\frac{n}{\\sigma_{k+1}})\\log(\\frac{1}{\\sigma_{k+1}})+\\log(\\frac{n}{\\textsubscript{g a p}_{k}})\\log(\\frac{1}{\\textsubscript{g a p}_{k}})+\\log(\\log(\\frac{n}{\\epsilon\\sigma_{k+1}\\textsubscript{g a p}_{k}}))\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "arithmetic oprationsusing $\\begin{array}{r}{O\\left(\\log(n)\\left(\\log^{4}(\\frac{n}{\\epsilon\\operatorname{gap}_{k}})+\\log^{4}(\\frac{n}{\\sigma_{k+1}})\\right)+\\log(\\frac{1}{\\epsilon\\sigma_{k+1}})\\right)}\\end{array}$ bits of precision, with probability at least $1-O(1/n)$ ", "page_idx": 61}, {"type": "text", "text": "ProofWefrstcompute $\\widetilde\\sigma_{k+1}\\leftarrow\\mathsf{S l G M A K}(\\mathbf{H},k\\!+\\!1,\\frac12,\\frac1n)$ such that from Proposition E.2, $\\widetilde{\\sigma}_{k+1}\\in$ $(1\\pm{\\textstyle{\\frac{1}{2}}})\\sigma_{k+1}(\\mathbf{H})$ with probability $1-1/n$ . It requires ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(T_{\\sf M M}(n)\\log(\\frac{n}{\\sigma_{k+1}})\\log(\\frac{1}{\\sigma_{k+1}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "PCA Input: Sample Covariance Matrix $\\mathbf{H}\\in\\mathbb{R}^{n\\times n}$ of a centered data matrx $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ target rank $k$ , accuracy $\\epsilon\\in(0,\\bar{1})$ Requires: $\\|\\mathbf{H}\\|\\leq1$ $k\\in[n-1]$ Algorithm: $\\widetilde{\\mathbf{C}}_{k}\\gets\\mathsf{P C A}(\\mathbf{H},k,\\epsilon)$ $\\widetilde\\sigma_{k}\\gets\\mathsf{S l G M A K}(\\mathbf{H},k,\\frac{1}{2},\\frac{1}{3n})$ 2: $\\epsilon^{\\prime}\\leftarrow\\epsilon\\widetilde{\\sigma}_{k}/4$ 3 $\\begin{array}{r}{\\widetilde{\\Pi}_{k}\\gets\\mathsf{P R O J E C T O R}(\\mathbf{H},\\mathbf{I},k,\\big(\\frac{\\epsilon^{\\prime2}}{20^{3}n^{4}}\\big)^{2}\\big).}\\end{array}$ 4 $\\begin{array}{r}{\\widetilde{\\mathbf{C}}_{k}\\gets\\mathsf{D E F L A T E}(\\widetilde{\\Pi}_{k},k,\\big(\\frac{\\epsilon^{\\prime2}}{20^{3}n^{4}}\\big)^{2},\\epsilon^{\\prime}).}\\end{array}$ 5: return $\\widetilde{\\mathbf{C}}_{k}$ Output: Approximate principal component matrix $\\widetilde{\\mathbf{C}}_{k}$ Ensures: $\\|\\mathbf{X}-\\mathbf{X}\\widetilde{\\mathbf{C}}_{k}\\widetilde{\\mathbf{C}}_{k}^{\\top}\\|\\leq(1+\\epsilon)\\|\\mathbf{X}-\\mathbf{X}\\mathbf{C}_{k}\\mathbf{C}_{k}^{T}\\|$ with probability at least $1-O(1/n)$ ", "page_idx": 62}, {"type": "text", "text": "Algorithm 7: PCA. ", "page_idx": 62}, {"type": "text", "text": "foating point operations and $O\\left(\\log(n)\\log^{4}({\\frac{n}{\\sigma_{k+1}}})\\right)$ bits. Then we set $\\epsilon^{\\prime}\\leftarrow\\epsilon\\widetilde{\\sigma}_{k+1}/4$ In the next step, we compute $\\widetilde{\\Pi}_{k}\\;\\gets\\;\\mathsf{P R O J E C T O R}(\\mathbf{H},\\mathbf{I},k,(\\frac{\\epsilon^{\\prime2}}{20^{3}n^{4}})^{2})$ , which, from Theorem 1.1, retums a spectral projctor $\\tilde{\\Pi}_{k}$ that satisies $\\begin{array}{r}{\\|\\widetilde{\\boldsymbol{\\Pi}}_{k}-\\boldsymbol{\\Pi}_{k}\\|\\leq\\big(\\frac{\\epsilon^{\\prime2}}{20^{3}n^{4}}\\big)^{2}}\\end{array}$ with probability $1-1/n$ Itrequres ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{O\\left(T_{\\mathsf{M M}}(n)\\left(\\log(\\frac{n}{\\mathrm{gap}_{k}})\\log(\\frac{1}{\\mathrm{gap}_{k}})+\\log(\\log(\\frac{n}{\\epsilon^{\\prime}\\,\\mathrm{gap}_{k}}))\\right)\\right)}\\\\ &{\\qquad=O\\left(T_{\\mathsf{M M}}(n)\\left(\\log(\\frac{n}{\\mathrm{gap}_{k}})\\log(\\frac{1}{\\mathrm{gap}_{k}})+\\log(\\log(\\frac{n}{\\epsilon\\sigma_{k+1}\\,\\mathrm{gap}_{k}}))\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "$O\\left(\\log(n)\\log^{4}({\\frac{n}{\\epsilon^{\\prime}\\operatorname{gap}_{k}}})\\right)=O\\left(\\log(n)\\log^{4}({\\frac{n}{\\epsilon\\sigma_{k+1}\\operatorname{gap}_{k}}})\\right)$ bits The e use DEFLATE to compute the matix $\\mathbf{C}_{k}^{\\prime}\\gets\\mathsf{D E F L A T E}(\\widetilde{\\Pi}_{k},k,(\\frac{\\epsilon^{\\prime2}}{20^{3}n^{4}})^{2},\\epsilon^{\\prime})$ . From Theorem A.5, DEFLATE succeeds with probability ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1-\\frac{(20n)^{3}\\sqrt{(\\epsilon^{\\prime2}/(20^{3}n^{4}))^{2}}}{\\epsilon^{\\prime2}}=1-1/n,}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "and it requires $O(T_{\\mathsf{M M}}(n))$ floating point operations using $O(\\log(n/\\epsilon^{\\prime}))\\;=\\;O(\\log(n/(\\sigma_{k+1}\\epsilon)))$ bits, and it internally generates. $O(n^{\\bar{2}})$ random complex normal variables. On success it returns $\\mathbf{C}_{k}^{\\prime}\\in\\mathbb{C}^{n\\times k}$ such that $\\|\\mathbf{C}_{k}-\\mathbf{C}_{k}^{\\prime}\\|\\leq\\epsilon^{\\prime}$ , where $\\mathbf{C}_{k}$ is a matrix whose columns form an orthonormal basis for $\\Pi_{k}$ . We can then keep only the real part of $\\mathbf{C}_{k}^{\\prime}$ , and set $\\widetilde{\\mathbf{C}}_{k}\\gets R e(\\mathbf{C}_{k}^{\\prime})$ . The spectral norm can only decrease by removing the imaginary part, therefore $\\|\\widetilde{\\mathbf{C}}_{k}-\\mathbf{C}_{k}\\|\\le\\epsilon^{\\prime}$ holds as well. Now we can write $\\widetilde{\\mathbf{C}}_{k}=\\mathbf{C}_{k}+\\mathbf{E}$ where $\\|\\mathbf{E}\\|\\leq\\epsilon^{\\prime}$ , which means that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\mathbf X}-{\\mathbf X}\\widetilde{\\mathbf C}_{k}\\widetilde{\\mathbf C}_{k}^{\\top}\\right\\|=\\left\\|{\\mathbf X}-{\\mathbf X}({\\mathbf C}_{k}+{\\mathbf E})({\\mathbf C}_{k}+{\\mathbf E})^{\\top}\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\left\\|{\\mathbf X}-{\\mathbf X}{\\mathbf C}_{k}{\\mathbf C}_{k}^{\\top}\\right\\|+\\left\\|{\\mathbf X}{\\mathbf E}{\\mathbf C}_{k}^{\\top}\\right\\|+\\left\\|{\\mathbf X}{\\mathbf C}_{k}{\\mathbf E}^{\\top}\\right\\|+\\left\\|{\\mathbf X}{\\mathbf E}{\\mathbf E}^{\\top}\\right\\|+\\left\\|{\\mathbf X}{\\mathbf E}{\\mathbf E}^{\\top}\\right\\|}\\\\ &{~~~~~~~~~~~~~~~~~\\leq\\left\\|{\\mathbf X}-{\\mathbf X}{\\mathbf C}_{k}{\\mathbf C}_{k}^{\\top}\\right\\|+2\\epsilon^{\\prime}\\|{\\mathbf X}\\|+\\epsilon^{\\prime2}\\|{\\mathbf X}\\|}\\\\ &{~~~~~~~~~~~~~~~~~~~~~\\leq\\left\\|{\\mathbf X}-{\\mathbf C}_{k}{\\mathbf C}_{k}^{\\top}{\\mathbf X}\\right\\|+3\\frac{\\epsilon\\widetilde{\\sigma}_{k+1}}{4}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~\\leq(1+\\frac{9}{8}\\epsilon)\\left\\|{\\mathbf X}-{\\mathbf X}{\\mathbf C}_{k}{\\mathbf C}_{k}^{\\top}\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where in the last we used the fact that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\widetilde{\\sigma}_{k+1}(\\mathbf{H})\\leq3\\sigma_{k+1}(\\mathbf{H})/2=3\\sigma_{k+1}^{2}(\\mathbf{X})/2\\leq3\\sigma_{k+1}(\\mathbf{X})/2=3\\|\\mathbf{X}-\\mathbf{X}\\mathbf{C}_{k}\\mathbf{C}_{k}^{\\top}\\|/2.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "The result follows by rescaling $\\epsilon$ by a constant and by summing together the individual algorithm complexities and bits. \u25a0 ", "page_idx": 62}, {"type": "text", "text": "In many applications of PCA the parameter $k$ is chosen to be small, i.e. $k\\ll d$ .This has led to an extensive research area on the so-called low-rank approximation algorithms for PCA. One of the earliest works which introduced this type of randomized low-rank approximations is [53, 54]. Some other landmark works in the field include the analysis of randomized PCA and low-rank approximation [100, 67, 120, 33, 34], and the pioneering Block-Krylov PCA of [106], which is essentially optimal in the matrix-vector query model [125]. The approximation accuracy of low-rank approximation-based PCA methods is often measured with respect to the spectral or the Frobenius norm error, i.e. the matrix $\\widetilde{\\mathbf{C}}_{k}$ that is returned should satisfy $\\mathbf{C}_{k}^{\\top}\\mathbf{C}_{k}=\\mathbf{I}_{k\\times k}$ and: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{X}-\\mathbf{X}\\widetilde{\\mathbf{C}}_{k}\\widetilde{\\mathbf{C}}_{k}^{\\top}\\right\\|_{\\{2,F\\}}\\leq(1+\\epsilon)\\left\\|\\|\\mathbf{X}-\\mathbf{X}\\mathbf{C}_{k}\\mathbf{C}_{k}^{\\top}\\right\\|_{\\{2,F\\}}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Most of the aforementioned low-rank approximation type algorithms assume exact arithmetic. A subtle detail is that they often rely on the computation of an (exact) SVD of some smaller submatrix, which is not realistic due to the Abel-Ruffini theorem. This can be justified for practical reasons since SVD can be approximated in polynomial time to arbitrary accuracy using classical solvers [64]. Quoting [67]: ", "page_idx": 63}, {"type": "text", "text": "Techniques for computing the SVD are iterative by necessity, but they converge so fast that we can treat them as finite for practical purposes. ", "page_idx": 63}, {"type": "text", "text": "However, if one wants to rigorously prove forward error approximations and end-to-end complexity upper bounds, there is necessarily a dependence on the singular value gap that separates the principal invariant subspace from the rest of the spectrum (consider a small perturbation of the 2-by-2 identity matrix and $k=1$ as a straightforward example). ", "page_idx": 63}, {"type": "text", "text": "Assuming an exact SVD algorithm, the seminal analysis of the Block-Krylov PCA of [106] can in fact provide per-singular vector guarantees, which are much stronger than the classical norm-wise bounds. Then arithmetic complexity of Block-Krylov PCA depends on poly $\\big(\\frac{k}{\\sqrt{\\epsilon}}\\big)$ . It is favorable for coarse accuracy $\\epsilon$ (i.e. when $\\epsilon=\\Theta(1).$ ) and small rank $k$ . It is not suitable, however, for larger $k$ E.g., it can be the case in applications where we need to keep $k=n/20$ of the original dimensions. The same holds when higher accuracy is required, e.g. when $\\epsilon=1/\\operatorname{poly}(n)$ . Then the complexity of these methods is already higher than standard eigensolvers. ", "page_idx": 63}, {"type": "text", "text": "In finite precision, the landscape is even less clear. At the time of this submission, the only work related to low-rank approximation PCA that we are aware of with end-to-end bit complexity bounds is [107] for the approximation of matrix functions applied on vectors. They prove that the Lanczos method can be stably applied to compute a vector $\\mathbf{u}$ such that the quantity $\\|\\mathbf{Au}\\|$ approximates $\\lVert\\mathbf{A}\\rVert$ This is a backward-approximate solution for the top-1 singular vector. Concurrently with this work, and independently, [80] analyzed the bit complexity of Block-Krylov PCA and achieved similar bounds as ours, albeit with different techniques. However, [80] did not describe how to compute the condition number which is required in order to adjust the machine precision. Our analysis covers the computation of all the involved parameters. ", "page_idx": 63}, {"type": "text", "text": "For general invariant subspaces $\\left(k\\,>\\,1\\right)$ ), consider one of the simplest randomized low-rank approximation algorithms, often referred to as \u201csubspace iteration\" [67] or \u201csimultaneous iteration\" [106]. The algorithm first samples a matrix $\\mathbf{G}\\in\\mathbb{R}^{n\\times l}$ with i.i.d standard normal elements, where $l=\\Theta(k)$ , and computes $\\mathbf{Y}\\leftarrow(\\mathbf{A}\\mathbf{A}^{T})^{q}\\mathbf{A}\\mathbf{G}$ , where typically $q\\approx\\log(\\operatorname*{min}\\{m,n\\})$ . It then returns the $\\mathbf{Q}$ factor from the economy QR factorization of $\\mathbf{Y}$ . It can be shown (see e.g. Corollary 10.10 of [67]) that ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\lVert(\\mathbf{I}-\\mathbf{QQ}^{\\top})\\mathbf{A}\\rVert\\lesssim\\lVert\\mathbf{A}-\\mathbf{A}_{k}\\rVert.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "The returned matrix has slightly more than $k$ columns, often referred to as \u201coversampling,\u201d but, importantly, there is no explicit SVD involved in the computation. Even in this case, foating point arithmetic already spoils the approximation guarantees: we cannot compute $\\mathbf{Q}$ without rounding errors. Stability analysis of QR factorization is typically carried out in the backward-error sense [70, 39], which makes the analysis of low-rank approximation PCA algorithms even more complicated. ", "page_idx": 63}, {"type": "text", "text": "G.3  Bit complexity analysis of Block-Krylov PCA ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "We now analyze the seminal Block-Krylov iteration algorithm of [106], which is listed in Algorithm 8 for convenience (in exact arithmetic). For the foating point analysis the following methodology is used. The main result is stated in Theorem 4.2. We shall denote by $T_{\\mathsf{M M}}(\\mathbf{X},q)$ the cost of multiplying $\\mathbf{X}$ with a dense matrix with $q$ columns with a numerically stable multiplication algorithm, like the one of Theorem A.2. Formally, for the rest of this section, we assume the following subroutine. ", "page_idx": 64}, {"type": "text", "text": "Definition G.1 (MMX). Let $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ be the input matrix of Algorithm 8. We assume a subroutine $\\mathsf{M M}\\mathsf{X}(\\mathbf{B})$ , which takes as input a matrix $\\mathbf{B}$ with $k$ columns. It returns a matrix $\\mathbf{C}\\,\\in\\,\\mathbb{R}^{m\\times r}$ such that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\|\\mathbf{C}-\\mathbf{X}\\mathbf{B}\\|\\leq\\mathbf{u}\\,\\mathrm{poly}(m,k)\\|\\mathbf{X}\\|\\,\\|\\mathbf{B}\\|,\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "in a totalof $T_{\\mathsf{M M X}}(k)$ foating point operations, using at most ${\\cal O}(\\log(1/\\mathbf{u}))$ bits of precision. We also assume $\\mathsf{M M X}^{\\top}(\\mathbf{B})$ which approximates the product $\\mathbf{X}^{\\top}\\mathbf{B}$ with the same cost and approximation bounds. ", "page_idx": 64}, {"type": "text", "text": "The standard inner-product based algorithm as well as a block variant of the MM algorithm of Theorem A.2 satisfy this definition. ", "page_idx": 64}, {"type": "text", "text": "Block-Krylov Iteration (exact arithmetic) Input: Data matrix $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ , target rank $k$ , accuracy $\\epsilon\\in(0,1)$ Requires: Exact arithmetic. Algorithm: $\\mathbf{Z}_{k}\\leftarrow$ Block-Krylov Iteration $(\\mathbf{X},k,\\epsilon)$ \uff1a 1 $q\\leftarrow\\Theta({\\frac{\\log(n)}{\\sqrt{\\epsilon}}})$ 2: $\\mathbf{G}\\gets\\mathcal{N}(0,1)^{n\\times k}$ 3: $\\mathbf{K}\\gets[\\mathbf{X}\\mathbf{G},(\\mathbf{X}\\mathbf{X}^{\\top})\\mathbf{X}\\mathbf{G},\\dotsc,(\\mathbf{X}\\mathbf{X}^{\\top})^{q}\\mathbf{X}\\mathbf{G}].$ 4 $:\\mathbf{Q},\\mathbf{R}\\gets\\mathrm{Economy{-}}\\mathrm{QR}(\\mathbf{K})$ Q e Rmxgk. 5: M \u2190- QTxXTQ E R9kxqk. 6: $\\bar{\\mathbf{U}}_{k}\\gets\\mathbf{top}{\\mathbf{-}}k$ singular vectors of $\\mathbf{M}$ \uff1a 7: return ${\\bf Z}_{k}\\leftarrow{\\bf Q}\\bar{\\bf U}_{k}$ Output: Approximate principal component matrix $\\mathbf{Z}$ \uff1a Ensures: $\\|\\mathbf{X}-\\mathbf{Z}_{k}\\mathbf{Z}_{k}^{\\top}\\mathbf{X}\\|\\leq(1+\\epsilon)\\|\\mathbf{X}-\\mathbf{X}_{k}\\|$ with high probability (in exact arithmetic). ", "page_idx": 64}, {"type": "text", "text": "Algorithm 8: Block-Krylov Iteration (Alg. 2 of [106]) ", "page_idx": 64}, {"type": "text", "text": "Step 1: Constructing the Block-Krylov matrix. We first observe that we can scale the matrix $\\mathbf{G}$ in Algorithm 8 to have norm at most one, since this does not affect the Krylov basis. We assume that the (scaled) Gaussian matrix $\\mathbf{G}$ in Algorithm 8 is given exactly. This is not realistic, but it greatly simplifies the analysis and it can be addressed by using the floating point Gaussian sampler of Definition D.1 to obtain a small error with high probability. ", "page_idx": 64}, {"type": "text", "text": "Given $\\mathbf{G}$ and $\\mathbf{X}$ , we can construct the Krylov matrix $\\mathbf{K}$ using $O(q)$ calls to $\\mathsf{M M X}(\\mathbf{G})$ . The first call $\\mathbf{X}_{1}\\gets\\mathsf{M M X}(\\mathbf{G})$ returns a matrix $\\mathbf{X}_{1}=\\mathbf{X}\\mathbf{G}+\\mathbf{E}_{1}$ where $\\|\\mathbf{E}_{1}\\|\\leq\\mathbf{u}\\cdot O(\\mathrm{poly}(m k))\\|\\mathbf{X}\\|\\|\\mathbf{G}\\|\\leq$ ${\\bf u}\\cdot O(\\mathrm{poly}(m k))$ , since we assumed constant norms. If we keep performing multiplications recursively to build the Krylov matrix $\\|\\widetilde{\\mathbf K}\\|$ , after $O(q)$ multiplications it holds that $\\|\\widetilde{\\mathbf K}-{\\mathbf K}\\|\\le$ ${\\bf u}\\cdot O(\\mathrm{poly}(m k q))$ . The total cost is $O(q T_{\\mathsf{M M}}\\times(k))$ floating point operations. If $\\mathbf{X}$ is sparse and we use the standard inner product-based algorithm, ${\\dot{T}}_{\\mathtt{M M X}}(k){\\dot{=}}\\;O(k{\\dot{\\mathtt{n}}}\\mathtt{n}z(\\mathbf{X}))$ . If $\\mathbf{X}$ is dense and we use Theorem A.2 then $\\bar{O}(q m k^{\\omega-2})$ floating point operations are sufficient. ", "page_idx": 64}, {"type": "text", "text": "Step 2: Condition number of the Block-Krylov matrix. So far we have approximated $\\mathbf{K}$ by $\\widetilde{\\mathbf{K}}=\\mathbf{K}+\\mathbf{E_{K}}$ ", "page_idx": 64}, {"type": "text", "text": "In the following steps we will need an approximation for the condition number of $\\mathbf{K}$ Wecanget such an approximation using a variant of the COND algorithm (Corollary E.1), which internally uses the SIGMAK algorithm of Proposition E.2 based on counting queries to approximate the condition number. In that case, we start with some arbitrary error $\\|\\mathbf{E_{K}}\\|$ , and we keep keep halving it until we have a sufficiently good approximation. At each iteration $t$ , the algorithm executes ", "page_idx": 64}, {"type": "text", "text": "", "page_idx": 65}, {"type": "equation", "text": "$$\nO\\left(q T_{\\mathsf{M M}}(k)\\right))\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "floating point operations to construct the Krylov matrix using $\\begin{array}{r}{O(\\mathrm{polylog}(m,q,\\frac{1}{\\epsilon_{t}})}\\end{array}$ bits. At most $O(\\log(\\kappa(\\mathbf{K})))$ iterations are required. The most expensive iteration is the last one, when $\\epsilon_{t}~\\approx$ $1/\\kappa(\\mathbf{K})$ , which gives a total cost of at most ", "page_idx": 65}, {"type": "equation", "text": "$$\nO\\left(q\\log(\\kappa(\\mathbf{K}))T_{\\sf M M}\\times(k)\\right)\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "floating point operations using at most $O(\\mathrm{polylog}(m,q,\\kappa(\\mathbf{K})))$ bits. It returns a value $\\widetilde{\\kappa}\\ \\in$ $\\Theta\\big(\\kappa(\\bar{\\bf K})\\big)$ , and it succeeds with high probability. ", "page_idx": 65}, {"type": "text", "text": "Step 3: Computing a basis. The computation of the basis in Step 4 of Algorithm 8 is arguably the hardest part in the analysis of the Block-Krylov iteration. With similar arguments as in the proof of Theorem A.6, if we write the economy-QR factorizations $\\widetilde{\\mathbf{K}}=\\mathbf{Q}_{\\widetilde{\\mathbf{K}}}\\mathbf{R}_{\\widetilde{\\mathbf{K}}}$ and $\\mathbf{K}=\\mathbf{Q}\\mathbf{R}$ ,there exists an orthogonal matrix $\\Phi_{1}$ such that ${\\bf Q}-{\\bf Q}_{\\tilde{\\bf K}}{\\bf\\Phi}_{1}={\\bf E}_{1}$ and $\\lVert\\mathbf{E}_{1}\\rVert^{\\mathbf{\\Omega}}\\stackrel{\\mathbf{\\Omega}}{=}\\dot{\\epsilon}^{\\gamma}$ , for some $\\epsilon^{\\prime}$ .This requires that $\\begin{array}{r}{\\|\\widetilde\\mathbf{K}-\\mathbf{K}\\|\\le\\epsilon^{\\prime}\\frac{1}{4}\\frac{1}{\\mathrm{poly}\\left(m n q k\\right)\\kappa\\left(\\mathbf{K}\\right)}}\\end{array}$ whichimplies aequirement ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathbf{u}\\leq\\epsilon^{\\prime}\\frac{c}{\\mathrm{poly}(m q)\\kappa(\\mathbf{K})},\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "for some constant $c$ . Instead of $\\kappa(\\mathbf{K})$ we can use $\\widetilde{\\kappa}$ from the previous Step 2. We can now use Theorem A.6 (which computes a basis via QR) on $\\tilde{\\bf K}$ to approximate $\\mathbf{Q}_{\\widetilde{\\mathbf{K}}}$ by the matrix ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{Q}}=\\mathbf{Q}_{\\tilde{\\mathbf{K}}}\\Phi_{2}+\\mathbf{E}_{\\mathrm{QR}}=(\\mathbf{Q}\\Phi_{1}+\\mathbf{E}_{1})\\Phi_{2}+\\mathbf{E}_{\\mathrm{QR}}=\\mathbf{Q}\\Phi_{1}\\Phi_{2}+\\mathbf{E}_{1}\\Phi_{2}+\\mathbf{E}_{\\mathrm{QR}}.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Let $\\mathbf{E}_{\\mathbf{Q}}\\,=\\,\\mathbf{E}_{1}\\boldsymbol{\\Phi}_{2}+\\mathbf{E}_{\\mathsf{Q R}}$ .We have that $\\left\\|\\mathbf{E}_{\\mathbf{Q}}\\right\\|\\,=\\,\\left\\|\\mathbf{E}_{1}\\boldsymbol{\\Phi}_{2}+\\mathbf{E}_{\\mathsf{Q R}}\\right\\|\\,\\leq\\,\\epsilon^{\\prime}+\\mathbf{E}_{\\mathsf{Q R}}$ . From Theorem A.6 we can ensure that $\\|\\mathbf{E}_{\\mathsf{Q R}}\\|\\,\\leq\\,\\epsilon^{\\prime}$ if we use $O(\\log(\\frac{m q\\kappa(\\widetilde{\\mathbf{K}})}{\\epsilon^{\\prime}}))$ bits. Thus, $\\widetilde{\\mathbf{Q}}$ is an approximate orthogonal basis for the range of the true Block-Krylov matrix $\\mathbf{K}$ ", "page_idx": 65}, {"type": "text", "text": "To summarize, as long as poly(mq)(K), which can be achieved by replacing (K) with above, then we can compute $\\widetilde{\\mathbf{Q}}=\\mathbf{Q}\\Phi+\\mathbf{E_{Q}}$ , where $\\mathbf{Q}$ is the true orthonormal basis from the QR factorization of $\\mathbf{K}$ \uff0c $\\Phi$ is a $q k\\!\\times\\!q k$ orthogonal matrix, and $\\|\\mathbf{E}_{\\mathbf{Q}}\\|\\leq\\epsilon^{\\prime}$ . The total cost is $O(m(q k)^{\\omega-1})$ floating point operations for Theorem A.6 using $O\\left(\\log(\\frac{m q\\kappa(\\mathbf{K})}{\\epsilon^{\\prime}})\\right)$ bits. ", "page_idx": 65}, {"type": "text", "text": "Step 4: Computing the reduced matrix. In Step 5, Algorithm 8 forms the matrix $\\mathbf{M}$ to compute its top- $k$ singular vectors. To analyze the computation of $\\mathbf{M}$ , the first observation is that $\\mathbf{Q}$ in lines 4- 7 can be replaced by any basis for the column space of $\\mathbf{K}$ . In particular, we replace it by_ $\\widetilde{\\mathbf{Q}}$ We then perform the multiplication in two steps: M = MMXT(Q), which returns M = QX + EMM, where $\\|\\mathbf{E}_{1}^{\\mathrm{MM}}\\|\\ \\leq\\ \\mathbf{u}\\cdot O(\\mathrm{poly}(q k n m))\\|\\widetilde{\\mathbf{Q}}\\|\\|\\mathbf{X}\\|\\ \\in\\ \\mathbf{u}\\cdot O(\\mathrm{poly}(q m))$ . Then we compute $\\widetilde{\\textbf{M}}\\gets$ $\\mathsf{M M}(\\widetilde{\\mathbf{M}}_{1},\\widetilde{\\mathbf{M}}_{1}^{\\mathrm{~\\tiny~1~}})=\\widetilde{\\mathbf{M}}_{1}\\widetilde{\\mathbf{M}}_{1}^{\\mathrm{~\\tiny~1~}}+\\mathbf{E}_{2}^{\\mathsf{M M}}$ where $\\|\\mathbf{E}_{2}^{\\mathsf{M M}}\\|\\leq\\mathbf{u}\\cdot O(\\mathrm{poly}(q k n))\\|\\mathbf{M}_{1}\\|^{2}\\in\\mathbf{u}\\cdot O(\\mathrm{poly}(q k n))$ where the last is implied if $\\mathbf{u}$ is sufficiently smaller than $1/\\operatorname{poly}(q m)$ . Putting everything together we can write ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\widetilde{\\bf M}=\\Phi_{2}^{\\top}\\Phi_{1}^{\\top}{\\bf Q}^{\\top}{\\bf X}{\\bf X}^{\\top}{\\bf Q}\\Phi_{1}\\Phi_{2}+{\\bf E}_{\\mathrm{M}},\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where $\\mathbf{E}_{\\mathbf{M}}$ contains the errors in $\\widetilde{\\mathbf{Q}}$ and also the ones from the multiplication in floating point, and $\\|\\mathbf{E}_{\\mathbf{M}}\\|\\leq O(\\|\\mathbf{E}_{\\mathbf{Q}}\\|)\\in O(\\epsilon^{\\prime}).\\ \\epsilon^{\\prime}$ is as in Step 3. So far we have assumed $O(\\log(\\frac{m q\\kappa(\\widetilde{\\mathbf{K}})}{\\epsilon^{\\prime}}))$ bits. ", "page_idx": 65}, {"type": "text", "text": "Step 5: Spectral gap and midpoint. Now we have written $\\widetilde{\\mathbf{M}}=\\boldsymbol{\\Phi}^{\\top}\\mathbf{Q}^{\\top}\\mathbf{X}\\mathbf{X}^{\\top}\\mathbf{Q}\\boldsymbol{\\Phi}+\\mathbf{E}_{\\mathbf{M}}$ where $\\Phi=\\Phi_{2}\\Phi_{1}$ is orthogonal and $\\|\\mathbf{\\bar{E}_{M}}\\|\\le O(\\epsilon^{\\prime})$ for some $\\epsilon^{\\prime}\\in(0,1)$ . We can use the counting queries in the spirit of Theorem 3.1 to compute the spectral gap and the midpoint of $\\mathbf{M}$ ", "page_idx": 65}, {"type": "text", "text": "In particular, we start with some desired bound $\\epsilon_{0}^{\\prime}\\;\\in\\;(0,1)$ for $\\left\\|\\mathbf{E_{M}}\\right\\|$ by seting the number of bits to $O\\big(\\log\\bigl(\\frac{q m\\kappa(\\widetilde{\\mathbf{K}})}{\\epsilon_{0}^{\\prime}}\\bigr)\\big)$ .Atachiat,awntd again the condition number of $\\mathbf{K}$ in Step 2). After at most $m=O(\\log(1/\\operatorname{gap}_{k}(\\mathbf{M})))$ iterations, the error $\\epsilon_{m}^{\\prime}$ satisfies $\\epsilon_{m}^{\\prime}=\\Theta(\\mathrm{gap}_{k}(\\mathbf{M}))$ , and we obtain two quantities $\\begin{array}{r}{\\widetilde{\\mathrm{gap}}_{k}\\in(1\\pm\\frac{1}{8})\\,\\mathrm{gap}_{k}(\\mathbf{M})}\\end{array}$ and $\\begin{array}{r}{\\widetilde{\\mu}_{k}\\in\\mu_{k}(\\mathbf{M})+\\frac{1}{8}\\operatorname{gap}_{k}(\\mathbf{M})}\\end{array}$ with high probability. In each iteration we need to construct $\\widetilde{\\bf M}$ using Steps 1, 3, and 4 with the specified number of bits. ", "page_idx": 65}, {"type": "text", "text": "", "page_idx": 66}, {"type": "text", "text": "Step 1 costs $O(q T_{\\mathsf{M M X}}(k))$ operations. Step 3 requires $O(m(q k)^{\\omega-1})$ operations.Step 4 executes $O(T_{\\sf M M X}(q k)\\:+\\:m(q k)^{\\omega-1})$ operations. The result is the $q k\\,\\times\\,q k$ matrix $\\widetilde{\\textbf{M}}$ on which we call COUNT on each iteration. From Lemma E.1, assuming that we have regularized $\\widetilde{\\bf M}$ appropriately using REGULARIZE, COUNT costs at most $O((q k)^{\\omega}\\operatorname{polylog}(q k/\\epsilon_{m}^{\\prime}))\\,=$ $O((q k)^{\\omega}\\operatorname{polylog}(q k/\\operatorname{gap}_{k}(\\mathbf{M})))$ . The maximum number of bits is in the last iteration, which is equal to O (polylog (mg&(K)) ", "page_idx": 66}, {"type": "text", "text": "Step 6: Principal singular vectors. Given a suitable approximation for the gap and the midpoint, we next use Lemma B.1 to prove forward error bounds between the true $k$ -spectral projector of $\\mathbf{M}=$ $\\Phi^{\\top}{\\mathbf Q}^{\\top}{\\mathbf X}{\\mathbf X}^{\\top}{\\mathbf Q}\\Phi$ and the one of $\\widetilde{\\mathbf{M}}=\\mathbf{M}+\\mathbf{E}_{\\mathbf{M}}$ . From Lemma B.1, if we set $\\mu=\\widetilde{\\mu}_{k}$ , we have that $\\|\\operatorname{sgn}(\\mathbf{M}-{\\widetilde{\\mu}}_{k})-\\operatorname{sgn}(\\widetilde{\\mathbf{M}}-{\\widetilde{\\mu}}_{k})\\|\\leq\\epsilon_{\\mathsf{S G N}}$ if $\\begin{array}{r}{\\|{\\bf E}_{\\bf M}\\|\\le\\epsilon_{5\\sf G N}\\frac{|\\lambda_{\\mathrm{min}}(\\widetilde{\\mu}_{k}-{\\bf M})|^{2}\\pi}{128}=\\epsilon_{5\\sf G N}\\Theta(\\mathrm{gap}_{k}({\\bf M})^{2})}\\end{array}$ The same holds for the spectral projectors, i.e. $\\|\\Pi_{k}(\\mathbf{M})-\\Pi_{k}(\\widetilde{\\mathbf{M}})\\|\\leq\\epsilon_{5\\mathsf{G N}}$ . The bound for $\\left\\|\\mathbf{E_{M}}\\right\\|$ $\\begin{array}{r}{\\mathbf{u}\\leq\\frac{\\epsilon_{\\mathsf{S G N}}\\widetilde{\\mathrm{gap}}_{k}^{2}}{\\mathrm{poly}(m q)\\widetilde{\\kappa}}}\\end{array}$ $\\begin{array}{r}{{\\cal O}\\left(\\log(\\frac{m q\\kappa({\\bf K})}{\\epsilon_{\\sf S G N S}\\widetilde{\\mathrm{ap}}_{k}})\\right)}\\end{array}$ bis. ", "page_idx": 66}, {"type": "text", "text": "It remains to approximate $\\Pi_{k}(\\widetilde{\\bf M})$ , denoted as $\\Pi_{k}$ for simplicity, and then use defation, similar to the vanilla PCA Algorithm 7. We frst set EPURIFY $\\begin{array}{r}{=(\\frac{\\epsilon_{\\mathsf{P C A}}^{2}}{20^{3}(q k)^{4}})^{2}}\\end{array}$ and thn use Algrthm 3, ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\widetilde{\\Pi}_{k}\\gets\\mathsf{P U R I F Y}(\\widetilde{\\mathbf{M}},\\mathbf{I},\\widetilde{\\mu}_{k},\\widetilde{\\mathrm{gap}}_{k},\\epsilon_{\\mathsf{P U R I F Y}}),\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "followedby $\\widetilde{\\mathbf{U}}_{k}~\\gets~\\mathsf{D E F L A T E}(\\widetilde{\\Pi}_{k},k,\\epsilon_{\\mathsf{P U R I F Y}},\\epsilon_{\\mathsf{P C A}})$ . From Proposition 2.1, PURIFY costs $O((q k)^{\\omega}\\log(q k/\\operatorname{gap}_{k}))$ floating point operations using ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O(\\mathrm{polylog}(\\frac{q k}{\\epsilon_{\\mathsf{P U R I F Y}}\\operatorname{gap}_{k}}))=O(\\mathrm{polylog}(\\frac{q k}{\\epsilon_{\\mathsf{P C A}}\\operatorname{gap}_{k}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "bits. From Theorem A.5, DEFLATE costs $O((q k)^{\\omega})$ operations using $O(\\mathrm{polylog}(q k/\\epsilon_{\\mathsf{P C A}}))$ bits, and succeeds with high probability. The returned matrix $\\widetilde{\\mathbf{U}}_{k}$ satisfies $\\|\\widetilde{\\mathbf{U}}_{k}-\\mathbf{U}_{k}\\|\\,\\le\\,\\epsilon_{\\mathsf{P C A}}$ , where $\\mathbf{U}_{k}$ is a matrix whose columns form an orthonormal basis for the span of the top- $k$ singular vectors of M. This also implies that $\\|\\widetilde{\\mathbf{U}}_{k}\\widetilde{\\mathbf{U}}_{k}^{\\top}-\\mathbf{U}_{k}\\mathbf{U}_{k}^{\\top}\\|\\leq3\\epsilon_{\\mathsf{P C A}}$ . Then ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\|\\Pi_{k}(\\mathbf{M})-\\widetilde{\\mathbf{U}}_{k}\\widetilde{\\mathbf{U}}_{k}^{\\top}\\|\\leq\\|\\Pi_{k}(\\mathbf{M})-\\Pi_{k}(\\widetilde{\\mathbf{M}})\\|+\\|\\Pi_{k}(\\widetilde{\\mathbf{M}})-\\widetilde{\\mathbf{U}}_{k}\\widetilde{\\mathbf{U}}_{k}^{\\top}\\|\\leq\\epsilon_{56\\mathsf{N}}+3\\epsilon_{97\\mathsf{A}}.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "We finally return $\\widetilde{\\bf Z}_{k}={\\sf M M}(\\widetilde{\\bf Q},\\widetilde{\\bf U}_{k})$ . It holds that where $\\|\\widetilde{\\mathbf Z}_{k}-\\widetilde{\\mathbf Q}\\widetilde{\\mathbf U}_{k}\\|\\leq{\\mathbf u}\\cdot O(\\mathrm{poly}(m q))$ .We can then write ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathbf{Z}}_{k}\\widetilde{\\mathbf{Z}}_{k}^{\\top}=\\widetilde{\\mathbf{Q}}\\widetilde{\\mathbf{U}}_{k}\\widetilde{\\mathbf{U}}_{k}^{\\top}\\widetilde{\\mathbf{Q}}^{\\top}+\\mathbf{E}_{\\mathbf{Z}}}\\\\ &{\\quad\\quad\\quad=\\widetilde{\\mathbf{Q}}(\\Pi_{k}(\\mathbf{M})+\\mathbf{E}_{\\mathbf{II}})\\widetilde{\\mathbf{Q}}^{\\top}+\\mathbf{E}_{\\mathbf{Z}}}\\\\ &{\\quad\\quad\\quad=\\widetilde{\\mathbf{Q}}\\Pi_{k}(\\mathbf{M})\\widetilde{\\mathbf{Q}}^{\\top}+\\widetilde{\\mathbf{Q}}\\mathbf{E}_{\\mathbf{II}}\\widetilde{\\mathbf{Q}}^{\\top}+\\mathbf{E}_{\\mathbf{Z}}}\\\\ &{\\quad\\quad\\quad=(\\mathbf{Q}\\Phi+\\mathbf{E}_{\\mathbf{Q}})\\Pi_{k}(\\mathbf{M})(\\mathbf{Q}\\Phi+\\mathbf{E}_{\\mathbf{Q}})^{\\top}+\\widetilde{\\mathbf{Q}}\\mathbf{E}_{\\mathbf{II}}\\widetilde{\\mathbf{Q}}^{\\top}+\\mathbf{E}_{\\mathbf{Z}}}\\\\ &{\\quad\\quad\\quad=\\mathbf{Q}\\Phi\\Pi_{k}(\\mathbf{M})\\Phi^{\\top}\\mathbf{Q}^{\\top}+\\mathbf{E}^{\\prime}+\\widetilde{\\mathbf{Q}}\\mathbf{E}_{\\mathbf{II}}\\widetilde{\\mathbf{Q}}^{\\top}+\\mathbf{E}_{\\mathbf{Z}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "where $\\mathbf{Q}\\boldsymbol{\\Phi}\\mathbf{I}\\mathbf{I}_{k}(\\mathbf{M})\\boldsymbol{\\Phi}^{\\top}\\mathbf{Q}^{\\top}\\,=\\,\\mathbf{Z}_{k}\\mathbf{Z}_{k}^{\\top}$ is equivalent to the true, exact arithmetic $\\mathbf{Z}_{k}$ of Algorithm 8. The error matrices satisfy $\\|\\mathbf{E}_{\\mathbf{Z}}\\|\\,\\leq\\,\\mathbf{u}\\cdot O(\\mathrm{poly}(m q))$ \uff0c $\\lVert\\widetilde{\\mathbf{Q}}\\mathbf{E}_{\\Pi}\\widetilde{\\mathbf{Q}}^{\\top}\\rVert\\,\\le\\,O(\\epsilon_{\\mathsf{S G N}}+\\epsilon_{\\mathsf{P C A}})$ , and $\\lVert\\mathbf{E}^{\\prime}\\rVert\\leq O(\\lVert\\mathbf{E_{Q}}\\rVert)\\in O(\\lVert\\mathbf{E}_{\\mathrm{M}}\\rVert)\\in\\dot{O}(\\epsilon_{\\mathsf{S G N}}\\operatorname{gap}_{k}(\\dot{\\mathbf{M}})^{2})$ ", "page_idx": 66}, {"type": "text", "text": "Putting everything together.  We can now state the main result by summarizing the Steps 1-6. ", "page_idx": 66}, {"type": "text", "text": "Theorem G.2 (Restatement of Theorem 4.2). Let $\\mathbf{X}$ be a data matrix $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ \uff0c $\\|\\mathbf{X}\\|\\leq1\\,$ $k\\in[n]$ a target rank, $\\epsilon_{\\mathsf{P C A}}\\in(0,1)$ an accuracy parameter, and $\\begin{array}{r}{q=\\Theta\\left(\\frac{\\log\\left(n\\right)}{\\sqrt{\\epsilon_{\\mathsf{P C A}}}}\\right)}\\end{array}$ . Let $T_{\\mathsf{M M X}}(k)$ denote the ", "page_idx": 66}, {"type": "text", "text": "complexity to stably multiply $\\mathbf{X}$ or $\\mathbf{X}^{\\top}$ with a dense matrix with $k$ columns from the right (see Def. G.1). Using the Steps 1-6 that are detailed in Appendix G.3 as a floating point implementation of Algorithm $\\boldsymbol{\\mathrm{\\&}}$ we can compute a matrix $\\widetilde{\\mathbf{Z}}_{k}\\in\\mathbb{R}^{m\\times k}$ that satisfies ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widetilde{\\mathbf{Z}}_{k}\\widetilde{\\mathbf{Z}}_{k}^{\\top}-\\mathbf{Z}_{k}\\mathbf{Z}_{k}^{\\top}\\right\\|\\leq O(\\epsilon_{\\mathsf{P C A}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "with high probability, where $\\mathbf{Z}_{k}$ is an approximate basis for the top- $k$ principalcomponents of $\\mathbf{X}$ returned by Algorithm 8 in exact arithmetic. The total cost is at most ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(q T_{\\mathrm{MMX}}(k)\\log(\\frac{\\kappa(\\mathbf{K})}{\\mathrm{gap}_{k}(\\mathbf{M})})+m(q k)^{\\omega-1}\\log(\\frac{1}{\\mathrm{gap}_{k}(\\mathbf{M})})+(q k)^{\\omega}\\operatorname{polylog}(\\frac{q k}{\\mathrm{gap}_{k}(\\mathbf{M})})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "foating poin operations, using O (polylog(mgs(K) ) bits of precision. $\\mathbf{K},\\mathbf{M}$ are the same as in Alg.8.Theonlyparametersthat are initialyquirdar $k$ and $\\mathsf{\\epsilon}_{\\mathsf{P C A}}$ ", "page_idx": 67}, {"type": "text", "text": "Proof. We first compute $\\widetilde{\\kappa}\\in\\Theta\\big(\\kappa(\\mathbf{K})\\big)$ in Steps 1 and 2. This costs ", "page_idx": 67}, {"type": "equation", "text": "$$\nO\\left(q\\log(\\kappa(\\mathbf{K}))T_{\\sf M M}\\times(k)\\right)\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "floating point operations using $O(\\mathrm{polylog}(q m\\kappa(\\mathbf{K})))$ bits. ", "page_idx": 67}, {"type": "text", "text": "Then, in Step 5 we iteratively use Steps 1, 3, and 4 to compute the midpoint and the gap, in a total Of $O(\\log(1/\\operatorname{gap}_{k}))$ iterations. The total cost is ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\left(q T_{\\mathsf{M M X}}(k)+m(q k)^{\\omega-1}\\right)\\log\\bigl(\\frac{1}{\\mathrm{gap}_{k}(\\mathbf{M})}\\bigr)+(q k)^{\\omega}\\operatorname{polylog}\\bigl(\\frac{q k}{\\mathrm{gap}_{k}(\\mathbf{M})}\\bigr)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "floating point operations using at most ", "page_idx": 67}, {"type": "equation", "text": "$$\nO\\left(\\mathrm{polylog}(\\frac{m q\\kappa(\\mathbf{K})}{\\mathrm{gap}_{k}(\\mathbf{M})})\\right)\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "bits of precision. The only parameter that we require to know beforehand, except for the matrix sizes,is $\\widetilde{\\kappa}$ from the previous step. ", "page_idx": 67}, {"type": "text", "text": "In the last Step 6, assuming that $\\epsilon_{\\mathsf{S G N}}~=~\\epsilon_{\\mathsf{P C A}}$ , we need to run Steps 1, 3, and 4, using $O\\left(\\mathrm{polylog}(\\frac{m\\bar{q}\\kappa({\\bf K})}{\\epsilon_{\\sf P C A}\\,\\mathrm{gap}_{k}})\\right)$   \nthe previous step. Thereafter the costs of DEFLATE, PURIFY, and MM are negligible compared to the costs of Steps 1, 3, and 4. The final matrix $\\widetilde{\\mathbf{Z}}_{k}$ satisfies ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widetilde{\\mathbf{Z}}_{k}\\widetilde{\\mathbf{Z}}_{k}^{\\top}-\\mathbf{Z}_{k}\\mathbf{Z}_{k}^{\\top}\\right\\|\\leq O(\\epsilon_{\\mathsf{P C A}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where $\\mathbf{Z}_{k}$ is the true, exact arithmetic projector in Algorithm 8. ", "page_idx": 67}, {"type": "text", "text": "Summarizing everything, we can compute $\\widetilde{\\mathbf{Z}}_{k}$ as advertised above in a total of at most ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(q T_{\\mathrm{MMX}}(k)\\log(\\frac{\\kappa(\\mathbf{K})}{\\mathrm{gap}_{k}(\\mathbf{M})})+m(q k)^{\\omega-1}\\log(\\frac{1}{\\mathrm{gap}_{k}(\\mathbf{M})})+(q k)^{\\omega}\\operatorname{polylog}(\\frac{q k}{\\mathrm{gap}_{k}(\\mathbf{M})})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "floating point operations, using $O\\left(\\mathrm{polylog}\\big(\\frac{m q\\kappa(\\mathbf{K})}{\\epsilon_{\\mathsf{P C A}}\\operatorname{gap}_{k}}\\big)\\right)$ bits of precision. ", "page_idx": 67}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 68}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 68}, {"type": "text", "text": "Justification: All claims accurately follow the paper contributions. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 68}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 68}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Justification: Limitations have been clearly discussed in the paper. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\"\u2019' section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that infuence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 68}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 68}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 68}, {"type": "text", "text": "Justification: All theoretical results include an informal proof in the main paper and a reference to the full, rigorous proof in the appendix. All definitions and assumptions are clearly stated. ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 69}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 69}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 69}, {"type": "text", "text": "Justification: There are no experiments, only theoretical results. ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d)  We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 69}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 70}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 70}, {"type": "text", "text": "Justification: There are no experiments and no code associated with the paper. Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 70}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 70}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 70}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 70}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 70}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 70}, {"type": "text", "text": "Justification: There are no experiments in the paper. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\"\u2019 if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 70}, {"type": "text", "text": "", "page_idx": 71}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 71}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 71}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 71}, {"type": "text", "text": "Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 71}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 71}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 71}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 71}, {"type": "text", "text": "Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 71}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 71}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 71}, {"type": "text", "text": "Justification: There is no apparant societal impact of the work performed as it only includes mathematical results. ", "page_idx": 71}, {"type": "text", "text": "Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 71}, {"type": "text", "text": "\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 72}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 72}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 72}, {"type": "text", "text": "Justification: There is no data or models involved in the paper. ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faitheffort. ", "page_idx": 72}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 72}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 72}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 72}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 73}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 73}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 73}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 73}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 73}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 73}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the datacollector. ", "page_idx": 73}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 73}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 73}]