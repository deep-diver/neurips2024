[{"type": "text", "text": "Dense Connector for MLLMs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Huanjin $\\mathbf{Yao^{1,3*}}$ , Wenhao $\\mathbf{W}\\mathbf{u}^{2*\\boxtimes}$ , Taojiannan $\\mathbf{Yang^{4}}$ , Yuxin Song3, Mengxi Zhang3 Haocheng Feng3, Yifan $\\mathbf{Sun^{3}}$ , Zhiheng $\\mathbf{Li^{\\mathbb{N}}}$ , Wanli Ouyang5, Jingdong Wang3 1Shenzhen International Graduate School, Tsinghua University 2The University of Sydney 3Baidu Inc. 4Amazon 5 The Chinese University of Hong Kong \u2217Equal Contribution Corresponding Author ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Do we fully leverage the potential of visual encoder in Multimodal Large Language Models (MLLMs)? The recent outstanding performance of MLLMs in multimodal understanding has garnered broad attention from both academia and industry. In the current MLLM rat race, the focus seems to be predominantly on the linguistic side. We witness the rise of larger and higher-quality instruction datasets, as well as the involvement of larger-sized LLMs. Yet, scant attention has been directed towards the visual signals utilized by MLLMs, often assumed to be the final high-level features extracted by a frozen visual encoder. In this paper, we introduce the Dense Connector - a simple, effective, and plug-and-play vision-language connector that significantly enhances existing MLLMs by leveraging multi-layer visual features, with minimal additional computational overhead. Building on this, we also propose the Efficient Dense Connector, which achieves performance comparable to LLaVAv1.5 with only $25\\%$ of the visual tokens. Furthermore, our model, trained solely on images, showcases remarkable zero-shot capabilities in video understanding as well. Experimental results across various vision encoders, image resolutions, training dataset scales, varying sizes of LLMs $(2.7\\mathrm{B}{\\rightarrow}70\\mathrm{B})$ ), and diverse architectures of MLLMs (e.g., LLaVA-v1.5, LLaVA-NeXT and Mini-Gemini) validate the versatility and scalability of our approach, achieving state-of-the-art performance across 19 image and video benchmarks. We hope that this work will provide valuable experience and serve as a basic module for future MLLM development. Code is available at https://github.com/HJYao00/DenseConnector. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, Large Language Models (LLMs) led by ChatGPT [1] have made remarkable advancements in text comprehension and generation. Furthermore, cutting-edge Multimodal Large Language Models (MLLMs) [2, 3] have rapidly expanded the capabilities of LLMs to include visual understanding, evolving into models capable of integrating both vision and text modalities. This has elevated MLLMs to become a new focal point for research and discussion [4, 5, 6, 7]. ", "page_idx": 0}, {"type": "text", "text": "In broad terms, the architecture of existing MLLMs can be delineated into three components: the pre-trained vision encoder (e.g., CLIP\u2019s ViT-L [8] or EVA-CLIP\u2019s ViT-G [9]), the pre-trained LLM (e.g., OPT [10], Llama [11], Vicuna [12], etc.), and the connector (e.g., Q-former [13, 14] or linear projection [15, 16]) trained from scratch to bridge the vision and language models. An intriguing trend in current MLLM research is that the focus of model learning and performance improvement seems to primarily center around the language aspect (e.g., utilizing larger-scale and higher-quality visual instruction data [17, 16, 18], larger-sized LLMs [19, 20]), with less exploration into the visual signals fed into the connector. Typically, the visual encoder is frozen to extract high-level visual features, which are then fed into the connector. This leads us to rethink: Have we fully utilized the existing pre-trained visual encoder? ", "page_idx": 0}, {"type": "image", "img_path": "Ioabr42B44/tmp/cfdb6682188084810c828069c8d277a7376d1ddcda00b15a9b9b809b192c7d66.jpg", "img_caption": ["Figure 1: Exploring Multi-layer Visual Features Empowering existing MLLMs. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In addition to the common practice of feeding the connector with final high-level visual features from visual encoder, an intuitive yet overlooked idea is to integrate visual features from various layers to complement the high-level features. In Fig. 1 (a), we illustrate attention maps from different layers of a 24-layer CLIP [8] pre-trained ViT-L [21], showing that different layers of the same visual encoder emphasize different regions of interest. Moreover, looking back at the history of computer vision, classic models (e.g., Densenet [22], FPN [23]) utilize multi-layer features to enhance visual representations. In MLLMs, the vision encoder is typically frozen to mitigate significant computational costs. In this context, our idea leverages the \u201cfree lunch\u201d of utilizing offline features from different layers as an implicit enhancement of visual information without the need for additional computational overhead. Furthermore, this way also complements techniques that directly increase visual signals, e.g., increasing image resolution [18, 24, 25, 26, 27, 28] or introducing additional visual encoders [29, 30, 18]. This idea is both simple and efficient, while also being sufficiently generic, logically allowing for seamless integration with any existing MLLMs. ", "page_idx": 1}, {"type": "text", "text": "In light of this, we propose the Dense Connector $(D C)$ , serving as a plug-and-play vision-language connector that involves offline features from various layers of the frozen visual encoder to provide the LLM with more visual cues. We explore three intuitive instantiations for the Dense Connector: 1) Sparse Token Integration (STI): We explicitly consider increasing the number of visual tokens by aggregating visual tokens from different specified layers and the final visual token. These tokens are then fed into a learnable projector for mapping into the text space. 2) Sparse Channel Integration (SCI): To avoid increasing the number of tokens, we concatenate visual tokens from different specified layers in the feature dimension. They are then passed to the projector, which not only maps visual tokens into the text space but also serves to reduce the feature dimensionality. 3) Dense Channel Integration $(D C I)$ : In addition to incorporating features from specified layers, we further attempt to utilize visual features from all layers. All of these instantiations yield significant improvements while utilizing just one simple learnable projector (comprising two linear layers) without introducing any extra parameters. Moreover, we conduct extensive empirical studies to demonstrate its scalability and compatibility. We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a simple, effective, and plug-and-play Dense Connector that enhances the visual representation of existing MLLMs with minimal additional computational overhead. We hope it can serve as a basic module to continuously benefit future MLLMs. \u2022 We demonstrate the versatility and scalability of our approach across various visual encoders, image resolutions $(336\\mathrm{px}{\\rightarrow}768\\mathrm{px})$ ), training dataset scales, varying sizes of LLMs $(2\\mathbf{B}\\mathrm{\\rightarrow}70\\mathbf{B})$ ), and diverse MLLMs architectures (e.g., LLaVA-v1.5 [16], LLaVA-NeXT [25], Mini-Gemini [18]). \u2022 Our method exhibits exceptional performance across 11 image benchmarks and achieves state-of-the-art results on 8 video benchmarks without the need for specific video tuning. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Large Pre-trained Vision Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The advent of pre-trained Vision Transformers (ViT) [21] has significantly propelled the advancement of computer vision. Furthermore, pre-training ViT models on web-scale image-text pairs, e.g., CLIP [8] and its subsequent iterations [9, 31, 32, 33], where vision and text encoders are simultaneously trained to bridge the gap between visual and textual modalities, has introduced zero-shot visual perception capabilities. Since then, CLIP-like models have served as effective initializations and have been incorporated into various vision-language cross-modal models (e.g., video-text alignment [34, 35, 36, 37], large vision-language models [14, 15, 38], etc.). Recently, SigLIP [31] introduced pairwise sigmoid loss during training, enabling the visual encoder to demonstrate more advanced visual perception capabilities. To validate the compatibility of our Dense Connector, this paper conducted experiments on different visual encoders, including those of CLIP [8] and SigLIP [31]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Large Language Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The exceptional text understanding and generation capabilities demonstrated by auto-regressive Large Language Models (LLMs) [39, 40, 41] have garnered significant attention. Subsequently, a plethora of LLMs [42, 11, 10, 43] have emerged, with notable open-source efforts like LLaMA [42] greatly propelling community contributions to LLMs research. Through instruction fine-tuning techniques, these models showcase human-like language interaction abilities, further further propelling advancements in natural language processing. Recent developments have seen LLMs scaled up or down to meet various application needs. Lightweight LLMs [44, 45, 20, 46] have been developed to address computational constraints, facilitating edge deployment. Conversely, in the pursuit of exploring the upper limits of LLMs, works such as [47, 19, 11, 20] have expanded LLM parameters, continuously pushing the boundaries of language capabilities. In this study, we validated the scalability of our Dense Connector by employing multiple LLMs ranging from 2.7B to 70B parameters. ", "page_idx": 2}, {"type": "text", "text": "2.3 Multimodal Large Language Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "After witnessing the success of LLMs, researchers have shifted their focus towards enabling LLMs to understand visual signals. To achieve this, prior research has proposed compressing visual embeddings using Q-former [14] into query embeddings, followed by transforming them into text embeddings through linear projection, or directly employing MLP projection [15] to connect the visual encoder with LLM. Furthermore, following the instruction tuning paradigm [48, 49], pioneering works [38, 15, 50] significantly boost the development of MLLMs through visual instruction tuning. Subsequently, by introducing larger-scale and higher-quality datasets, efforts such as [18, 25, 17, 24] have notably enhanced the visual understanding and reasoning capabilities of MLLMs. Additionally, there are works that introduce additional visual encoders [29, 30] or utilize higher-resolution images [25, 18, 24] to provide richer visual signal sources. Meanwhile, many studies [51, 52, 53] directly extend these above image-based methods to video conversational models by leveraging video instruction tuning datasets. In summary, these studies typically utilize high-level features from the frozen visual encoder as visual embeddings. However, we find that effectively leveraging offline features from different layers\u2014the overlooked \u201cfree lunch\u201d\u2014can yield significant beneftis. Then, we follow FreeVA [54] to directly extend the image model for video understanding without any additional video training. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Fig. 2(a), we illustrate the overall architecture of our model, using the mainstream LLaVA [15] framework as an example. It includes the pre-trained visual encoder $V i s(\\cdot)$ and the Large Language Model $L L M(\\cdot)$ , alongside with our proposed Dense Connector $D C(\\cdot)$ . Similarly, our $D C$ can be seamlessly extended to other high-resolution or dual-branch MLLMs, such as LLaVA-NeXT [25] and Mini-Gemini [18]. Formally, the introduction is as follows: ", "page_idx": 2}, {"type": "text", "text": "Visual Encoder: We utilize a CLIP pre-trained Vision Transformer (ViT) [21] as the visual encoder for extracting visual features. Initially, ViT partitions an image $X_{i}\\in\\mathbb{R}^{\\dot{H}\\times W\\times C}$ into a sequence of non-overlapping patches. Each patch is then processed via convolution to produce visual tokens, which are subsequently input into ViT. This procedure yields $L$ layers of visual features $V\\in$ RL\u00d7N\u00d7Dv, where N denotes the number of the visual tokens and Dv denotes the feature dimension. ", "page_idx": 2}, {"type": "image", "img_path": "Ioabr42B44/tmp/d5881693abdbb8ca2aae2eeed62676ea8d7f5066952a9e3f87e73ba3b672f218.jpg", "img_caption": ["Figure 2: Dense Connector in MLLM: Overview and Three Instantiations. $N$ is the number of tokens, $D$ is the feature dimension, and $\\alpha$ is the downsampling ratio. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Dense Connector: The Dense Connector comprises two components: the first integrates multi-layer visual features, elaborated upon in detail in Sec. 3.2, while the second employs a learnable MLP to map the integrated visual features to the LLM\u2019s text space. The MLP consists of two linear layers with a GELU [55] activation function sandwiched between them. The first layer adjusts the visual hidden size $D_{v}$ to align with the LLM\u2019s hidden dimension $D_{t}$ , while the second layer maintains the dimensionality at $D_{t}$ . Upon processing through the Dense Connector, we acquire visual embeddings $e_{v}\\in\\mathbb{R}^{N\\times D_{t}}$ that encapsulate information from multiple layers. ", "page_idx": 3}, {"type": "text", "text": "Large Language Model: The LLM processes textual data using a tokenizer and text embedding module to convert language into its input feature space. These text embeddings are concatenated with transformed visual embeddings before being fed into the LLM for subsequent predictions. ", "page_idx": 3}, {"type": "text", "text": "3.2 Dense Connector ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here we delve into three intuitive instantiations of the Dense Connector, each demonstrating superior performance compared to the baseline (e.g., LLaVA-1.5 [16]). Among them, Sparse Token Integration (STI) and Sparse Channel Integration $(S C I)$ sparsely select visual features from $K$ layers (indexed as $l_{n}$ , where $1\\leq l_{n}<L$ and $1\\leq n\\leq K)$ spanning shallow, middle, and high levels out of the total $L$ layers of ViT, while Dense Channel Integration $(D C I)$ utilizes features from all layers. These features are then fed into Dense Connector to generate visual embedding that can be \u201cunderstood\u201d by LLM. ", "page_idx": 3}, {"type": "text", "text": "Sparse Token Integration (STI): While existing methods typically rely solely on features from the final layer as the visual representation input for the LLM, our STI approach diverges from this by integrating features from multiple layers to enrich the visual input for the LLM. Recognizing that higher-level features contain richer semantic information crucial for visual signal perception in VLMs, we maintain the final layer features unchanged while downsampling additional visual features from other layers by using average pooling $a v g(\\cdot)$ with a stride $\\alpha$ . This downsampling reduces the number of visual tokens to $N^{\\prime}=\\bar{N}/\\alpha$ , mitigating computational overhead and redundancy. These visual features from various layers are concatenated along the token dimension and processed through a shared MLP(\u00b7), yielding more robust visual embedding ev \u2208R(N+(k\u22121)\u00d7N\u2032)\u00d7Dt: ", "page_idx": 3}, {"type": "text", "text": "Sparse Channel Integration (SCI): We delve deeper into connecting multi-level features along the channel dimension. Subsequently, this feature is processed through an MLP projector to obtain visual embedding ev \u2208RN\u00d7Dt: ", "page_idx": 4}, {"type": "equation", "text": "$$\ne_{v}=M L P(C o n c a t e n a t e([V_{l_{1}},...,V_{l_{K}},V_{L}],d i m=c h a n n e l)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The MLP projector serves dual functions: integrating various features as a fusion tool and facilitating the transformation of visual inputs into linguistic representations. This design ingeniously leverages the dimensionality scaling effect of the MLP projector, enabling the transformation of connected multilayer features into the feature space of the LLM without requiring additional modules. Moreover, this method does not increase the number of tokens fed into the LLM, thereby avoiding any increase in the computational overhead of the LLM. ", "page_idx": 4}, {"type": "text", "text": "Dense Channel Integration (DCI): While Sparse Channel Integration incorporates features from $K$ layers, many visual feature from other layers remain unused. Concatenating all visual feature layers using STI or SCI leads to excessively high dimensions, posing challenges during training. To address these issues, we propose DCI, which builds upon the SCI method by integrating adjacent layers to reduce redundancy and high dimensionality. This approach ensures dense connectivity across a wider range of visual layers. Specifically, we partition the features of $L$ layers into $G$ groups, where each group comprises $M$ adjacent visual features, with $M=L/G$ . Summing the features within each group, denoted as $G V_{g}$ , finally yields $G$ fused visual representations: ", "page_idx": 4}, {"type": "equation", "text": "$$\nG V_{g}=\\frac{1}{M}\\sum_{i=(g-1)M+1}^{g M}V_{i},\\quad1\\leq g\\leq G.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Subsequently, we concatenate these features from the $G$ groups with the final layer\u2019s features along the channel dimension before passing them through an MLP: ", "page_idx": 4}, {"type": "equation", "text": "$$\ne_{v}=M L P(C o n c a t e n a t e([G V_{1},...,G V_{G},V_{L}],d i m=c h a n n e l)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Efficient Dense Connector for Visual Token Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For MLLMs, each image is converted into hundreds or even thousands of visual tokens, and this large number of tokens increases the computational burden on autoregressive models. However, reducing the number of visual tokens generally leads to a noticeable drop in performance. In this paper, we leverage multi-layer visual features to compensate for the information loss caused by reducing visual tokens, enabling our method to achieve performance on par with LLaVA-v1.5 [16] while using several times fewer visual tokens, and surpassing other carefully designed efficient connectors [24, 38, 50, 56, 57] Specifically, after obtaining the visual embedding $e_{v}$ through the Dense Connector, we apply a parameter-free module, i.e. a 2D interpolation function, to downsample visual tokens. Then, these discrete visual tokens $e_{v}^{\\prime}$ are concatenated with the text tokens and fed into the LLM, resulting in a 3 times improvement in inference speed. ", "page_idx": 4}, {"type": "text", "text": "3.4 Training-Free Extension from Image to Video Conversational Models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following FreeVA [54], we extend the image-based models trained as described above to video domain for video understanding. Specifically, given a video, we uniformly sample $T$ frames, and each frame is processed through the visual encoder and dense connector to obtain a visual embedding $e_{v}$ . Consequently, we obtain an embedding sequence $\\{e_{v_{1}},...,e_{v_{T}}\\}$ , which is then fed into the LLM. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Architecture. 1) Visual Encoders: To explore the generalization of the Dense Connector across different visual encoders, we select two mainstream options, namely CLIP-ViT-L-336px [8] and SigLIP-ViT-SO [31]. 2) LLMs: The Dense Connector is applied across various LLMs, spanning from 2.7B to 70B parameters. This includes Phi-2-2.7B [44], Vicuna-7B&13B [12], Hermes-2-Yi-34B [19], and Llama3-8B&70B-Instruct. 3) Dense Connector: For the 24-layer CLIP-ViT-L-336px [8], we specifically target visual features from the 8th, 16th, and final 24th layers for both STI and SCI. For STI, we apply a downsampling factor of $\\alpha=8$ for the features from the 8th and 16th layers. For DCI, we divide all layer features into two groups, each containing 12 layers (i.e., 1-12, 13-24). Note that we use 1 to denote the stem layer of ViT, e.g., 2-25 correspond to the 24 layers of ViT-L. Similarly, for the SigLIP ViT-SO [31], which has 27 layers, we partition the first 26 layers into two groups (i.e., 1-13, 14-26). ", "page_idx": 5}, {"type": "text", "text": "Training Datasets. Data quality plays a crucial role in determining the performance of MLLMs. In this study, we examine the impact of two high-quality training datasets on our model: LLaVA-1.5 [16] and Mini-Gemini [18]. The LLaVA-1.5 pre-training dataset comprises 558K image captions, while its instruction tuning dataset contains 665K conversations. Mini-Gemini builds upon LLaVA-1.5, offering a larger dataset with 1.2M image-text caption pairs for alignment and $1.5\\mathrm{M}$ conversations for instruction tuning. Unless otherwise specified, all experimental results are based on the LLaVA-1.5 dataset to reduce training costs. ", "page_idx": 5}, {"type": "text", "text": "Training Recipe. We train all models on 8 NVDIA A100 GPUs with 40GB VRAM, except for the Hermes-2-Yi-34B and LLama-3-70B-Instruct, which utilize 32 NVDIA A100 GPUs with 80GB VRAM. Our training process comprises two stages: pre-training and instruction fine-tuning. In the pre-training phase, we initialize the visual encoder and LLM with pre-trained weights, while the Dense Connector is randomly initialized. Here, we freeze the visual encoder and the LLM, updating only the parameters of the Dense Connector. The model undergoes pre-training for one epoch with a global batch size of 256 and a learning rate of 1e-3. Subsequently, in the instruction fine-tuning stage, we maintain the visual encoder frozen while updating the Dense Connector and the LLM. Fine-tuning is performed for 1 epoch with a global batch size of 128 and a learning rate of 2e-5. For models using LoRA fine-tuning, we set the LoRA rank to 128 and LoRA alpha to 256. When scaling up the LLM to larger parameter sizes, such as LLama-3-70B-Instruct, we apply LoRA fine-tuning due to memory constraints. In this setup, we set the LoRA rank to 128 and LoRA alpha to 256. ", "page_idx": 5}, {"type": "text", "text": "Evaluation. We present comprehensive results across various image and video evaluation benchmarks. For image datasets, we include GQA [58], VQAV2 $\\mathrm{(VOA}^{v2}\\mathrm{)}$ ) [59], ScienceQA $({\\mathrm{SOA}}^{I})$ [60], TextVQA $(\\mathsf{V O}\\bar{\\mathsf{A}}^{T})$ [61], POPE [62], MathVista (Math) [63], MMBench (MMB) [64], MM-Vet (MMV) [65], MMMU [66], LLaVA-Bench-In-the-Wild (LBW) [15], and MME [67]. Additionally, we evaluate zero-shot performance on open-ended video question-answering benchmarks such as MSVD-QA [68], ActivityNet-QA [69], MSRVTT-QA [70], and the newly proposed generative performance benchmark [51], include evaluation metrics such as Correctness of Information (CI), Detail Orientation (DO), Contextual Understanding (CU), Temporal Understanding (TU), and Consistency (CO). ", "page_idx": 5}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Study on Instantiations of Dense Connector. In Tab. 1, we discuss three proposed methods of Dense Connector. 1) Sparse Token Integration (STI): This method uses visual features from different hierarchical levels as independent visual prompts for the LLM, allowing it to perceive a more diverse set of visual features. We select features from the 8th, 16th, and 24th layers, resulting in significant improvements across various datasets, particularly achieving a $2.9\\%$ increase on the MMB [64]. Further expanding the selection to include the 8th, 16th, 20th, and 24th layers enhances performance but also increases token count, leading to higher training costs and inference time. 2) Sparse Channel Integration (SCI): This method efficiently uses the MLP projector for feature fusion and projection, integrating multiple layers of visual features into visual embeddings. SCI boosts performance with minimal additional computational cost, achieving peak performance with features from the 8th, 16th, and 24th layers, resulting in a $1.7\\%$ improvement on GQA. SCI performs better than STI and mitigates the computational costs associated with higher token counts. However, expanding the range of visual ", "page_idx": 5}, {"type": "text", "text": "Table 1: Ablations on Visual Layer Selection in Dense Connector. Here, we explore three instantiations (STI, SCI, and ${\\cal D}C{\\cal I}$ ) of our Dense Connector integrated with the baseline (i.e., LLaVA-1.5 [16]), which utilizes a 24-layer CLIP-ViT-L-336px. ", "page_idx": 6}, {"type": "table", "img_path": "Ioabr42B44/tmp/8ebde894a3255abafb4f2cc083fe2e498db2ccf506bc44b5478636bd5a8634a0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Exploring the Compatibility and Scalability of Dense Connector (DC). Scaling results on visual encoder (VE), resolution (Res.), pre-training (PT) / instruction tuning (IT) data, and LLM are provided. \" $\\mathrm{^{\\prime}0.5M\\!+\\!0.6M^{\\prime\\prime}}$ denotes the training data from LLaVA-1.5 [16], while \" $\\mathbf{\\sigma}^{1.2\\mathrm{M}+1.5\\mathrm{M}^{\\prime\\prime}}$ denotes the data from Mini-Gemini [18]. \u2217indicates results evaluated using official model. ", "page_idx": 6}, {"type": "table", "img_path": "Ioabr42B44/tmp/1983c04d8896f27f36c0b91d754fa86afbab73e036fddfda0a5fd98b51083971.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "layers within SCI does not yield additional performance gains, suggesting that merely extending the range of visual feature layers is ineffective. 3) Dense Channel Integration (DCI): Building on the performance enhancements of SCI, DCI integrates a broader array of visual features using grouped additive fusion to produce robust visual representations. We divide the visual features into 2 or 3 groups, each with an equal number of layers. For the CLIP-L model, each group combines features from 12 or 8 layers, respectively. Splitting them into 2 groups demonstrates superior performance, achieving improvements of $2.7\\%$ on SQA [60] and $2.5\\%$ on MMB [64] compared to the baseline. The experimental results illustrate that utilizing multi-layer visual features enhances the visual perception capabilities of the MLLMs, leading to more accurate responses. Unless otherwise specified, we employ DCI as the default instantiation of the Dense Connector for optimal performance. ", "page_idx": 6}, {"type": "text", "text": "Study on Visual Encoders and Training Dataset Impacts. Given our method\u2019s reliance on multilayer visual features, it is crucial to assess its impact across various visual backbones. As shown in Tab. 2, we first replace the CLIP-ViT-L [8] with the more advanced visual encoder SigLIP-ViT", "page_idx": 6}, {"type": "table", "img_path": "Ioabr42B44/tmp/158ee58d6d43722dfdd1336343fffbd42b74888643417e76ec9ff6602bdf79e4.jpg", "table_caption": ["Table 3: Comparison of Efficient Dense Connector with Other Efficient Methods. \u2217indicates results evaluated using official model. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "SO [31]. Leveraging the enhanced multi-layer visual features from SigLIP-ViT-SO, our Dense Connector demonstrates further performance improvements. Additionally, we investigate the influence of training datasets on the effectiveness of the Dense Connector. By fine-tuning our model using the larger dataset [18], we observe notable performance gains across most benchmark evaluations. The results indicate that more training data significantly enhance model performance. Specifically, our model with Vicuna-13B achieves accuracy of $36.5\\%$ on MathVista [63] and $77.1\\%$ on SQA [60], underscoring the significant beneftis of increased training data. Moreover, when using the same training data and the LLM (i.e., Vicuna-7B), our Dense Connector surpasses the dual encoder structure of Mini-Gemini [18] across the majority of benchmarks. Specifically, it achieves performance gains of $2.4\\%$ on the MMB [64], $4.2\\%$ on MM-Vet [65], and $1.7\\%$ on the MathVista [63] benchmark. ", "page_idx": 7}, {"type": "text", "text": "Study on High-Resolution Setting. The use of high-resolution images to enhance detail representation in MLLMs has garnered considerable attention [25, 18, 26, 27, 28]. In this paper, we extend Dense Connector to the Mini-Gemini (MGM) [18] and LLaVA-NeXT [25], showcasing its plug-and-play capability. For MGM, we keep the high-resolution features from ConvNeXT [71] intact, applying DCI exclusively to the CLIP features. We observe significant improvements across various benchmarks, as detailed in Tab. 2, including MathVista [63], MMB [64], and MM-Vet [65], with enhancements of $1.1\\%$ , $2.2\\%$ , and $3.8\\%$ , respectively. In addition to the high-resolution architecture of the dual visual encoder, we also extend the Dense Connector to dynamic high-resolution, specifically using the AnyRes technology from LLaVA-NeXT [25]. For a fair comparison, we provide a baseline for LLaVA-NeXT trained on the same dataset. As shown in Tab. 2, Dense Connector achieves overall improvements compared to the dynamic resolution method LLaVA-NeXT as well. ", "page_idx": 7}, {"type": "text", "text": "Study on Efficient Dense Connector. To achieve faster inference speed, we investigate an efficient Dense Connector in this study, which can accelerate inference by 3 times. As described in Sec. 3.3, we use a 2D bilinear interpolation function to downsample the visual tokens by a factor of 4, reducing the number of tokens from 576 to 144, which decreases the training time during in the second stage on 8 A100 GPUs from 9 hours to 6.5 hours. With the same configuration of using 144 visual tokens, Dense Connector outperforms the carefully designed efficient connector method Tokenpacker [56] by $0.9\\%$ , $1.5\\%$ , $2.5\\%$ and $1.4\\%$ on GQA [58], VQAv2 [59], MMB [64], and MM-Vet [65], respectively. ", "page_idx": 7}, {"type": "text", "text": "4.3 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Comparison with SoTAs in Image Understanding. In Tab. 4, we scale the LLMs from 2.7B to 70B parameters and compare them with state-of-the-art MLLMs. When considering lightweight models, our Dense Connector surpasses the previous MLLM, TinyLlava [72], achieving a $1.7\\%$ enhancement on the MM-Vet benchmark using the same fine-tuning data and foundation model. Furthermore, using same training data and LLM, our Dense Connector outperforms the LLaVA-1.5 Vicuna 13B [16] with substantial gains of $2.1\\%$ , $3.7\\%$ , and $5.5\\%$ on the GQA [58], MMB [64], and MM-Vet [65] benchmarks, respectively. Notably, even with data solely from LLaVA-1.5, our 13B model achieves performance comparable to MGM [18], which is trained on larger datasets, including $1.2\\mathrm{M}{+}1.5\\mathrm{M}$ data. Moreover, utilizing the advanced open-source LLM Llama3-8B-Instruct, our model significantly surpasses LLaVA-LLama3 [76] with improvements of $5.5\\%$ , and 52 on MMB [64], and $\\mathbf{M}\\mathbf{M}\\mathbf{E}^{p}$ [67], respectively, highlighting the contribution of our Dense Connector. By scaling up the LLM to 34B and 70B, Dense Connector achieves further improvements leveraging more powerful language models. The 70B model attains scores of $82.4\\%$ on SQA [60] and $79.4\\%$ on MMBench [64]. We then increase the resolution using AnyRes technology [25] and fully fine-tuned the LLM. Our 13B model outperforms MGM and LLaVA-NeXT on MMBench [64] and SQA [60], achieving scores of $72.3\\%$ and $72.6\\%$ . The 34B model achieves scores of $81.2\\%$ , $59.2\\%$ , and $97.7\\%$ on MMBench [64], MM-Vet [65], and LLaVA-Bench-in-the-Wild [16], respectively. ", "page_idx": 7}, {"type": "text", "text": "Table 4: Comparisons with State-of-the-Arts. \u2217indicates the dataset have been used for training, and \u2020 indicates the dataset is not publicly accessible. \"PT,\" \"IT,\" and \"Res.\" denote pre-training data, instruction fine-tuning data, and image resolution, respectively. ", "page_idx": 8}, {"type": "table", "img_path": "Ioabr42B44/tmp/e887ef9237d9f26053bf48601dc73528138377136e0ee1c91db87d3ce3a1b48f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Ioabr42B44/tmp/e6c7241d5fc76dac47088688179af0c6e3a804436e7eb3eee6f9f3bb91b01e13.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Ioabr42B44/tmp/d69751a803c51d832f8afc4b7cc939489bfcf3cd1300bf3d2aa00283818b0126.jpg", "table_caption": ["Table 5: Comparisons with Leading Methods on Zero-shot Video QA Benchmarks. Following FreeVA [54], we specify the GPT-3.5 versions used for evaluation to ensure fairness in performance comparison across different versions. \u201cMAR\u201d denotes the GPT-3.5-Turbo-0301, \u201cJUN\u201d denotes the GPT-3.5-Turbo-0613, and \u201cJAN\u201d denotes the latest GPT-3.5-Turbo-0125. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Comparison with SoTAs in Video Understanding. Building on the training-free paradigm of FreeVA [54] for image-to-video adaptation, we directly apply our models, originally trained on image-text datasets, to video dialogues. As indicated in Tab. 5, the visual enhancement capabilities of the Dense Connector significantly enhance the video comprehension of our 13B model. This model surpasses the baseline LLaVA-1.5 [16] with FreeVA [54] on MSVD [68] and ActivityNet [69]. By scaling up the LLM to 34B, our model demonstrates a marked improvement in video understanding capabilities, achieving state-of-the-art performance on video evaluation benchmarks with accuracies of $77.4\\%$ on MSVD [68], $62.1\\%$ on MSR-VTT [70], and $55.8\\%$ on ActivityNet [69]. ", "page_idx": 8}, {"type": "image", "img_path": "Ioabr42B44/tmp/4f80e0c39d15d4e413bb843e7fd84228d00a02ec237ee3f619c011e2d06763d4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3: Quantitative Results for Image and Video dialogues. Figures (a) through (d) pertain to image understanding, while figures (e) and (f) relate to video understanding. ", "page_idx": 9}, {"type": "text", "text": "Qualitative Results. In Fig. 3, we illustrate our model\u2019s exceptional visual understanding and text generation capabilities across various scenarios, encompassing both image and video. More qualitative results are provided in Appendix A.5. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce the Dense Connector, a novel plug-and-play module that enhances visual perception capabilities of MLLMs by densely integrating multi-layer visual features. We instantiated three types of Dense Connector and validate the efficacy of it across a diverse array of vision encoders, LLMs, and training datasets, demonstrating substantial improvements in performance across multiple evaluation benchmarks. Dense Connector can be easily integrated into existing MLLMs. In this work, we incorporate the Dense Connector into mainstream model LLaVA and high-resolution method Mini-Gemini, demonstrating its versatility and generalization capabilities. ", "page_idx": 9}, {"type": "text", "text": "Limitations: Our three Dense Connector instantiations do not introduce additional parameters, leaving room for further exploration. We have not yet found an effective method for incorporating additional parameters. Future research will focus on discovering more efficient ways to connect visual and language models for better modality alignment. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[2] OpenAI. Gpt-4v(ision) system card. 2023.   \n[3] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[4] Wenhao Wu, Huanjin Yao, Mengxi Zhang, Yuxin Song, Wanli Ouyang, and Jingdong Wang. Gpt4vis: What can gpt-4 do for zero-shot visual recognition? arXiv preprint arXiv:2311.15732, 2023. [5] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023.   \n[6] Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis. arXiv preprint arXiv:2310.09909, 2023. [7] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. Gpt-4v (ision) for robotics: Multimodal task planning from human demonstration. arXiv preprint arXiv:2311.12015, 2023. [8] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[9] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023.   \n[10] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n[11] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%^{*}$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023.   \n[13] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736, 2022.   \n[14] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[16] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.   \n[17] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.   \n[18] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024.   \n[19] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.   \n[20] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020.   \n[22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[23] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117\u20132125, 2017.   \n[24] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.   \n[25] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.   \n[26] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu. Otterhd: A highresolution multi-modality model. arXiv preprint arXiv:2311.04219, 2023.   \n[27] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023.   \n[28] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024.   \n[29] Dongsheng Jiang, Yuchen Liu, Songlin Liu, Xiaopeng Zhang, Jin Li, Hongkai Xiong, and Qi Tian. From clip to dino: Visual encoders shout in multi-modal large language models. arXiv preprint arXiv:2310.08825, 2023.   \n[30] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.   \n[31] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975\u201311986, 2023.   \n[32] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2818\u20132829, 2023.   \n[33] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. In ICLR, 2021.   \n[34] Bo Fang, Wenhao Wu, Chang Liu, Yu Zhou, Yuxin Song, Weiping Wang, Xiangbo Shu, Xiangyang Ji, and Jingdong Wang. Uatvr: Uncertainty-adaptive text-video retrieval. In ICCV, 2023.   \n[35] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. Cap4video: What can auxiliary captions do for text-video retrieval? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10704\u201310713, 2023.   \n[36] Wenhao Wu, Zhun Sun, Yuxin Song, Jingdong Wang, and Wanli Ouyang. Transferring vision-language models for visual recognition: A classifier perspective. International Journal of Computer Vision, pages 1\u201318, 2023.   \n[37] Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, and Wanli Ouyang. Bidirectional cross-modal knowledge exploration for video recognition with pre-trained vision-language models. In CVPR, pages 6620\u20136630, 2023.   \n[38] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[39] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[41] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.   \n[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[43] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023.   \n[44] Mojan Javaheripi, S\u00e9bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. Phi-2: The surprising power of small language models. Microsoft Research Blog, 2023.   \n[45] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024.   \n[46] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 b technical report. arXiv preprint arXiv:2402.17834, 2024.   \n[47] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \n[48] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.   \n[49] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.   \n[50] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.   \n[51] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023.   \n[52] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023.   \n[53] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.   \n[54] Wenhao Wu. Freeva: Offilne mllm as training-free video assistant. arXiv preprint arXiv:2405.07798, 2024.   \n[55] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \n[56] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024.   \n[57] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024.   \n[58] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019.   \n[59] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.   \n[60] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.   \n[61] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317\u20138326, 2019.   \n[62] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.   \n[63] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.   \n[64] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.   \n[65] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.   \n[66] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.   \n[67] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.   \n[68] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In ACL, 2011.   \n[69] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In CVPR, 2015.   \n[70] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, 2016.   \n[71] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976\u201311986, 2022.   \n[72] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: A framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024.   \n[73] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257, 2023.   \n[74] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043, 2023.   \n[75] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024.   \n[76] XTuner Contributors. Xtuner: A toolkit for efficiently fine-tuning llm. https://github.com/InternLM/ xtuner, 2023.   \n[77] Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-Wen Kuo, Lu Xu, Fan Chen, Jitesh Jain, Humphrey Shi, and Longyin Wen. Cumo: Scaling multimodal llm with co-upcycled mixture-of-experts. arXiv preprint arXiv:2405.05949, 2024.   \n[78] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533, 2023.   \n[79] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. Advances in Neural Information Processing Systems, 35:124\u2013141, 2022.   \n[80] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.   \n[81] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023.   \n[82] Yizhou Wang, Ruiyi Zhang, Haoliang Wang, Uttaran Bhattacharya, Yun Fu, and Gang Wu. Vaquita: Enhancing alignment in llm-assisted video understanding. arXiv preprint arXiv:2312.02310, 2023.   \n[83] Ruyang Liu, Chen Li, Yixiao Ge, Ying Shan, Thomas H Li, and Ge Li. One for all: Video conversation is feasible without video instruction tuning. arXiv preprint arXiv:2309.15785, 2023.   \n[84] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[85] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024.   \n[86] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Further Exploration and Analysis of the Dense Connector ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Sec. 3.2, we provide a detailed discussion of three instantiation methods of the Dense Connector: Sparse Token Integration (STI), Sparse Channel Integration (SCI), and Dense Channel Integration (DCI). Applying our proposed Dense Connector to representative VLMs (e.g., LLaVA [16]) results in notable performance improvements. In this section, building on the STI, SCI, and DCI methods, we explore instantiations of Dense Connectors with additional learnable parameters. ", "page_idx": 15}, {"type": "text", "text": "Sparse Token Integration with 1D Convolutional Downsampling. The STI instantiation method concatenates visual tokens from various layers, which significantly increases the total number of tokens. To reduce redundancy and computational overhead, we initially employ average pooling to reduce the number of tokens from shallow layer features. In computer vision, using convolution for downsampling is a common practice [84]. Here, we replace average pooling with a single 1D convolutional layer $C o n v_{1D}(\\cdot)$ , to downsample the shallow layer tokens. We set both the kernel size and stride to 8, maintaining consistency with the average pooling as detailed in Sec. 4.1. The formula is as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne_{v}=M L P(C o n c a t e n a t e([C o n v_{1D}(V_{l_{1}}),...,C o n v_{1D}(V_{l_{K}}),V_{L}],d i m=t o k e n)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Sparse Channel Integration with 2D Convolutional Modelling. The SCI method utilizes the projector as both a feature fusion tool and a modality mapper, transforming multi-layer visual features concatenated along the channel dimension into the input feature space of the LLMs. By straightforwardly concatenating multi-layer visual features, the SCI method achieves significant performance enhancements for VLMs with minimal computational overhead. As a central element of the Dense Connector, we aim to enhance local perception abilities by processing visual features through a $3\\!\\times\\!3$ 2D convolution $C o n v_{2D}$ , with shared weights prior to feature concatenation: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne_{v}=M L P(C o n c a t e n a t e([C o n v_{2D}(V_{l_{1}}),...,C o n v_{2D}(V_{l_{K}}),C o n v_{2D}(V_{L})],d i m=c h a n n e l)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Dense Channel Integration with Linear Layer. The DCI method initially groups visual features and then aggregates them, enabling the fusion of adjacent visual features without expanding the dimensionality. This technique mitigates the issues of feature redundancy and excessive channel dimensions that arise in the SCI method from incorporating too many layers. To enhance the integration of features from different visual layers across groups, each layer of visual features is processed through a linear layer with shared weights. The formula is as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nG V_{g}=\\frac{1}{M}\\sum_{i=(g-1)M+1}^{g M}L i n e a r(L n(V_{i})),\\quad1\\le g\\le G.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\ne_{v}=M L P(C o n c a t e n a t e([G V_{1},...,G V_{G},V_{L}],d i m=c h a n n e l)),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $L n(\\cdot)$ and $L i n e a r(\\cdot)$ denotes Layer Normalization and Linear layer, respectively. ", "page_idx": 15}, {"type": "text", "text": "Experimental Results and Analysis. We present the detailed results of these described instantiations in Tab. 6. 1) For the STI method, using features from the 8th, 16th, and 24th layers, average pooling demonstrates superior performance compared to 1D convolutional downsampling, especially on the GQA [58] and MMBench [64] benchmarks. 2) In the SCI method, applying 2D convolution to enhance local feature modeling with offline ViT features from the 8th, 16th, and 24th layers achieves comparable performance to methods without 2D convolution. 3) Furthermore, incorporating additional aggregation information, such as a linear layer, into the DCI method does not lead to improved outcomes. ", "page_idx": 15}, {"type": "text", "text": "In summary, our attempts to introduce additional parameterized modules did not yield the anticipated improvements. We hypothesize that since the connector is randomly initialized, maintaining ease of convergence is crucial for effectively training the connector to align visual and language models under limited training data. In current MLLMs, the MLP structure of LLaVA [16] efficiently facilitates convergence between visual and language components. However, adding extra parameters may disrupt this inherent characteristic, leading to diminished performance. Future research will explore more complex and effective implementations of the Dense Connector. ", "page_idx": 15}, {"type": "table", "img_path": "Ioabr42B44/tmp/02b48efcf69178009d184218e039afc5a33be83550e6c1bf00bcc70d5b8f3a53.jpg", "table_caption": ["Table 6: Additional studies on the Dense Connector. "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "Ioabr42B44/tmp/e7683d6f18624bb4e39a97eca2f6b87665d7bb6e4d9030692d0a36eb12a444e8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "", "img_caption": ["(d) Results comparison on POPE (e) Results comparison on MMB (f) Results comparison on MM-Vet ", "Figure 4: Comparison of three instantiations of Dense Connector with LLaVA-1.5. STI stands for Sparse Token Integration, SCI for Sparse Channel Integration and DCI for Dense Channel Integration. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A.2 Visualized Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Appendix A.1, we further explored more complex Dense Connector structures. However, we found that non-parameterized methods yielded better average results. Therefore, we also visualized the comparison of three main instantiations of the Dense Connector with LLaVA-1.5 in Fig. 4, clearly demonstrating DCI\u2019s superior performance across different benchmarks. ", "page_idx": 16}, {"type": "table", "img_path": "Ioabr42B44/tmp/bc9038a2fc27dfe087f3357ac68954e2817f06a6ba46424270225e6de68498b5.jpg", "table_caption": ["Table 7: More comprehensive evaluation results of the Dense Connector. \"PT\", \"IT\", and \"Res.\" denote pre-training data, instruction fine-tuning data, and image resolution, respectively. $^{\\ddagger}$ indicates the pre-training data $(0.5\\mathsf{M})$ from LLaVA-1.5 [16] and instruction data (1.5M) from Mini-Gemini [18]. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "Ioabr42B44/tmp/8fdc42bdd8ea131065645fcde0fd272554df20db5f2903000df783bd0ee24e91.jpg", "table_caption": ["Table 8: Ablation study on fine-tuning Vision Transformer. In this table, all the results are conducted using LLaVA 1.5 data, comprising 558K pre-training data and 665K instruction-tuning data. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.3 Model Zoo ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To probe the upper limits of MLLMs, we attempt to utilize a larger dataset to fine-tune our model based on Llama3-70B-Instruct. Given the limitations of computational resources, we commence with the Dense Connector pre-trained on LLaVA\u2019s initial stage data [16] and subsequently refine it using the instructional dataset from Mini-Gemini [18]. This model achieves accuracy of $40.7\\%$ on the MathVista [63] and $53.3\\%$ on MM-Vet [65], as illustrated in Tab. 7. Remarkably, our 70B model exhibits performance that is on par with GPT-4V on the LLaVA-Bench-in-the-Wild evaluation [16]. Specifically, our model achieves $97.8\\%$ , closely trailing GPT-4V\u2019s $98\\%$ [85]. However, it is worth noting that the full potential of our 70B model remains untapped due to the lack of initial stage pre-training data and the inherent constraints of LoRA fine-tuning [86]. ", "page_idx": 17}, {"type": "text", "text": "A.4 Further Exploration in Training Visual Encoder ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the experiments above, we used frozen multi-layer features from a pre-trained ViT to enhance the model\u2019s visual perception capabilities. Recently, however, there has been a growing trend of fine-tuning the entire model, including ViT, during the instruction-tuning phase to achieve better results. Following this trend, we present additional results in Tab. 8 on the performance impact of finetuning the ViT within the multi-layer visual connector. Specifically, in the first stage, we maintain the original training strategy. In the second stage, we fine-tune the ViT with a smaller learning rate of 2e-6, keeping other hyperparameters unchanged. Tab. 8 shows that fine-tuning ViT yields performance gains on benchmarks with a higher reliance on visual information, such as MMBench [64] and TextVQA [64]. For instance, fine-tuning CLIP-Large encoder resulted in improvements of $1.8\\%$ and $1.0\\%$ on MMBench and TextVQA, respectively. Conversely, on benchmarks that are less visually dependent, such as SQA [60], performance exhibited a slight decline. ", "page_idx": 17}, {"type": "text", "text": "A.5 More Qualitative Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "From Fig. 6 to Fig. 13, we present additional multimodal dialogue illustrations with images and videos. Fine-tuned on image-text datasets, Dense Connector achieves effective alignment between visual and linguistic modalities, enhancing the model\u2019s visual perception capabilities. Qualitative evaluations showcases that our model can adeptly handle a broad spectrum of challenges across various domains, including mathematics, flowcharts, and daily scenarios. Additionally, the incorporation of the FreeVA method [54] equips our model with video understanding capabilities, enabling our model to address dynamic temporal challenges. However, due to the lack of training on video data, our model may occasionally misinterpret images as videos. For instance, as shown in Fig. 12, the model mistakenly interprets videos as images, incorrectly responding with \u2018The image\u2019 instead of \u2018The video\u2019. ", "page_idx": 17}, {"type": "image", "img_path": "Ioabr42B44/tmp/7b6ad9a7da96051aa8308e7cece606afa8baffd3bb1a92bac35659088a0881a1.jpg", "img_caption": ["The solution if the lamp is plugged in and the bulb has burned out, as indicated by the flowchart, is to \u201cREPLACE BULB.\u201d "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "Ioabr42B44/tmp/589928d2f3919c159321dcae80816b9cdb4dc83ae1ad3600d5ad454e1b2c95e8.jpg", "img_caption": ["The step in the flowchart that is drawn with a diamond shape is \"Water boiled?\" This is a decision point in the flowchart, where the process can continue in two different directions based on the outcome of the condition being tested. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "Ioabr42B44/tmp/0da53cc94ed83ee43ae9c2308d411fd3945fae362b2242d16157ecc9c50fb76d.jpg", "img_caption": ["Figure 5: Qualitative results of the flowchart understanding. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "After using the probe to spread the tire puncture, the next step is to remove the repair plug from the plastic. This is shown in step 5 of the guide, where the hands are holding the repair plug and the plastic, preparing to separate them. ", "page_idx": 18}, {"type": "image", "img_path": "Ioabr42B44/tmp/315efd40daabb178c67b3271059f9b4454a1b822e45c77b4e1f73268767948aa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "The answer to the bottom left equation, which is $\"9+3=\"$ , is 12. ", "page_idx": 19}, {"type": "text", "text": "What is the answer to the second equation on the right? ", "page_idx": 19}, {"type": "image", "img_path": "Ioabr42B44/tmp/14788bb1e51f62e6dc01eaefaff5b6592022049c8153da669a1b9275602a46d4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "The answer to the second equation on the right, which is $\"6+1=\"$ , is 7. ", "page_idx": 19}, {"type": "text", "text": "Fix both ends (A and B) of a naturally straight elastic band, measuring $16\\;\\mathrm{cm}$ in length, on a horizontal tabletop. Then, vertically pull the midpoint C upward by 6 cm to point D (as shown in the diagram). How much has the elastic band been stretched?\" ", "page_idx": 19}, {"type": "text", "text": "", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To determine how much the elastic band has been stretched, we need to calculate the length of the elastic band from point A to point D and then from point D to point B. This will give us the total length of the stretched elastic band. ", "page_idx": 19}, {"type": "text", "text": "Using the Pythagorean theorem, we can calculate the length of AD and DB as follows: ", "page_idx": 19}, {"type": "image", "img_path": "Ioabr42B44/tmp/6d6b226141db987928b74a6f67714dd8cb1e00cdb06803128c5a823f3ce84ac2.jpg", "img_caption": ["Figure 6: Qualitative results of the math solving skills. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "For triangle ACD: ", "page_idx": 19}, {"type": "text", "text": "$\\mathrm{AC}=8~\\mathrm{cm}$ (half of the original length of $16\\;\\mathrm{cm}$ ) $\\mathrm{CD}=6$ cm (the height to which the midpoint is pulled) AD = \u221a( $\\mathrm{AC^{2}}+\\mathrm{CD^{2}})=\\sqrt{(8^{2}+6^{2})=\\sqrt{(64^{'}+36)=\\sqrt{100}=10~\\mathrm{cm}}}$ 1 $\\mathrm{BC}=8$ cm (half of the original length of $16\\;\\mathrm{cm}$ ) $\\mathrm{CD}=6$ cm (the height to which the midpoint is pulled) $\\mathrm{BD}=\\sqrt{(\\mathrm{BC}^{2}+\\mathrm{CD}^{2})\\stackrel{\\cdot}{=}}\\,\\sqrt{(8^{2}+6^{2})=\\sqrt{(64\\stackrel{\\cdot}{+}36)\\stackrel{\\cdot}{=}}\\sqrt{100\\stackrel{\\cdot}{=}10\\mathrm{~cm}}}$ ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Since AD and BD are equal, the total length of the stretched elastic band is AD $+\\;\\mathrm{DB}=10\\;\\mathrm{cm}+10\\;\\mathrm{cm}=20\\;\\mathrm{cm}.$ . ", "page_idx": 19}, {"type": "text", "text": "Therefore, the elastic band has been stretched from its original length of $16\\;\\mathrm{cm}$ to a total length of $20\\;\\mathrm{cm}$ , which means it has been stretched by $4\\,\\mathrm{cm}$ . ", "page_idx": 19}, {"type": "text", "text": "Can you help me understand or explain a meme? Please describe the meme or provide details about its context. ", "page_idx": 20}, {"type": "image", "img_path": "Ioabr42B44/tmp/25a5ad5ff6acb2a146dcfafc44de66c60de46a1bc2f9b281441e5388ae38fbf8.jpg", "img_caption": ["The image is a humorous take on a McDonald's meal, with a sandwich and fries made out of fruits and vegetables. The sandwich is made from a cut apple, and the fries are made from carrot sticks. The arrangement is placed on a dining table, and the words \"Nice Try Mom\" are written above the image. The meme is poking fun at the idea of a mother trying to make a healthy version of a fast-food meal, but the result is not quite convincing. The phrase \"Nice Try Mom\" is a lighthearted way of acknowledging the effort while also implying that the outcome is not entirely successful. ", "Please explain the meme in detail. ", "Figure 7: Qualitative results of the meme understanding. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "The meme features a photograph of a small, light-colored dog lying on its side on a wooden floor. The dog's posture is relaxed, with its head resting on the floor and its body stretched out. The image is overlaid with text in a bold, white font that reads \"MONDAY, JUST... MONDAY.\" The text is a play on the common sentiment that Mondays are often dreaded as the start of the workweek, and the dog's posture humorously suggests a reluctance to get up and face the day, which is a feeling many people can relate to. The meme uses the dog's expression and body language to anthropomorphize the animal, attributing human emotions and behaviors to it, which is a common technique in meme creation to evoke humor and empathy.\", ", "page_idx": 20}, {"type": "image", "img_path": "Ioabr42B44/tmp/3af1afac6f935313a79690e0f1e1fbef3a7c1a252a3898a41429de8ffa103b34.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "What is the name of this dish, and how is it made? ", "page_idx": 21}, {"type": "text", "text": "\u3002\u3002 The dish in the image is commonly known as \"Mapo Tofu,\" a traditional Sichuan dish. It is made by first marinating and then pan-frying tofu cubes until they are lightly browned. The sauce is prepared separately by stir-frying a mixture of ground pork, garlic, ginger, and fermented soybeans in chili oil. The sauce is then seasoned with soy sauce, rice vinegar, and cornstarch to thicken it. The tofu is then added to the sauce and cooked until it is well coated. The dish is garnished with green onions and served hot. ", "page_idx": 21}, {"type": "image", "img_path": "Ioabr42B44/tmp/f7e4e1846d360da9b86d969589229769384c067d168d4ce4e9165c5fbbf21ada.jpg", "img_caption": ["How to cook this dish? ", "Figure 8: Qualitative results of the delicacies understanding. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "@ ", "page_idx": 21}, {"type": "text", "text": "To cook this dish, which appears to be a type of sushi roll, you would typically follow these steps: ", "page_idx": 21}, {"type": "text", "text": "1. Prepare the sushi rice: Rinse the rice thoroughly and cook it according to the package instructions. Once cooked, allow it to cool. Mix the rice with rice vinegar, sugar, and salt to give it the characteristic sushi flavor.   \n2. Prepare the filling: Slice the ingredients you want to include in the roll, such as cucumber, avocado, and imitation crab, into thin strips.   \n3. Assemble the roll: Lay a sheet of nori seaweed flat on a sushi mat. Spread a thin layer of sushi rice onto the seaweed, leaving a small border at the top. Arrange the filling ingredients horizontally in the middle of the rice.   \n4. Roll the sushi: Using the sushi mat, roll the nori and rice tightly but gently, applying even pressure. Make sure the roll is tight and the ingredients are evenly distributed.   \n5. Slice the roll: Once the roll is formed, place it in the refrigerator for about 30 minutes to set. Then, using a sharp knife, slice the roll into individual pieces. $\\backslash\\mathfrak{n}\\backslash\\mathfrak{n}6$ . Serve: Serve the sushi with soy sauce, wasabi, and pickled ginger, as shown in the image. The yellow condiment could be grated daikon or a similar garnish. ", "page_idx": 21}, {"type": "text", "text": "Remember to handle the sushi rice and ingredients with clean hands or utensils to maintain hygiene. Enjoy your homemade sushi! ", "page_idx": 21}, {"type": "image", "img_path": "Ioabr42B44/tmp/b98bcb5c4ff05ceba13d1f3898e57b2ad53ef39d9832e8a60b3850ecc3864666.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Which movie is this image from? ", "page_idx": 22}, {"type": "text", "text": "\u3002 The image appears to be a still from the movie \"Pulp Fiction\" (1994) directed by Quentin Tarantino. The character depicted is Mia Wallace, played by actress Uma Thurman. This particular scene is memorable for its iconic dialogue and the character's distinctive style, which has become a cultural reference point. The image captures a moment of Mia Wallace's character in a diner, which is a pivotal scene in the film. ", "page_idx": 22}, {"type": "text", "text": "Dense Connector ", "page_idx": 22}, {"type": "text", "text": "What's the ending of this movie? ", "page_idx": 22}, {"type": "text", "text": "The ending of the movie \\\"Titanic\\\" is a tragic one. The film is based on the real-life story of the RMS Titanic, which sank on its maiden voyage in 1912. The movie's plot follows the fictional characters Jack Dawson and Rose DeWitt Bukater, who fall in love aboard the ship during its ill-fated journey. The ship's collision with an iceberg and subsequent sinking lead to the death of Jack, who sacrifices his life to save Rose. The film ends with an elderly Rose, who has been telling the story of her time on the Titanic to her granddaughter and a group of explorers, throwing the \\\"Heart of the Sea\\\" diamond into the ocean, finally letting go of the past. The ending is poignant, reflecting on the themes of love, loss, and the passage of time. ", "page_idx": 22}, {"type": "image", "img_path": "Ioabr42B44/tmp/db778415a9e0e8f995bad5d1e4d20b719d381cf424dc5d5f5bca3075b60847cd.jpg", "img_caption": ["Figure 9: Qualitative results of the movie analysis. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "What is the significance of this scene in the context of the movie? ", "page_idx": 22}, {"type": "text", "text": "The significance of this scene in the context of the movie is multifaceted. It represents a pivotal moment of freedom and joy for the characters, who are from different social classes and are experiencing a moment of escape from their societal constraints. The act of standing at the bow of the ship with arms outstretched is a classic symbol of liberation and the pursuit of dreams. It also serves as a visual metaphor for the characters' aspirations and the limitless possibilities that lie ahead, both literally as they travel across the ocean and metaphorically in their personal lives. This scene is often associated with a sense of hope and the beginning of a new chapter, which is a central theme in the narrative of the film. ", "page_idx": 22}, {"type": "image", "img_path": "Ioabr42B44/tmp/07d38293daccf5d090aa03c47c0ff06297ccc6d4650f4aff96533f009616f92b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Describe the video in detail. ", "page_idx": 23}, {"type": "text", "text": "\u897f The video depicts a person drawing a tree with pink flowers on a white canvas. The artist is using a brush to create the intricate details of the tree, which is surrounded by a large circle. The tree appears to be a cherry blossom tree, as it is covered in pink flowers. The artist's hand is visible, holding the brush and working on the painting. The scene captures the creative process of the artist as they bring the tree to life on the canvas. ", "page_idx": 23}, {"type": "text", "text": "Compose a poem inspired by this artwork. ", "page_idx": 23}, {"type": "text", "text": "\u3002\u3002 In the midst of a tranquil scene, A tree stands tall, adorned with pink leaves, Its branches reaching out, As if to embrace the world. A hand, holding a brush, Paints the tree with care and grace, Creating a masterpiece, A reflection of nature's beauty. The artist's skillful strokes, Bring life to the canvas, A tree in bloom, A symbol of growth and renewal. The moon, a gentle companion, Shines above the tree, Guiding the artist's hand, As the painting takes shape. A moment captured in time, A tree in bloom, a hand at work, A testament to the power of art, To evoke emotions and inspire. ", "page_idx": 23}, {"type": "text", "text": "Describe the video in detail. ", "page_idx": 23}, {"type": "image", "img_path": "Ioabr42B44/tmp/98c42459ffa11bc0f5192b38a794598647087faa77c738e3263c9137c5822127.jpg", "img_caption": ["Figure 10: Qualitative results of the video understanding. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "The video depicts a group of young men gathered around a dining table, engaged in a friendly arm wrestling competition. There are at least five people in the scene, with two of them actively participating in the arm wrestling match. The other individuals are watching the competition, likely cheering on their friends or waiting for their turn. The table is cluttered with various items, including a bottle and a cup placed near the center of the table. The atmosphere appears to be lively and competitive, as the young men enjoy their time together. ", "page_idx": 23}, {"type": "image", "img_path": "Ioabr42B44/tmp/7fb96e82e0a881a63dbe495bc2c1a595a951b78ccbdf18ab74487b49aa271e8a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Describe the video in detail. ", "page_idx": 24}, {"type": "text", "text": "The video features a young child sitting on a bed, wearing glasses and holding a book. The child appears to be reading or looking at the book, possibly enjoying a story or learning something new. The bed is covered with a blanket, providing a comfortable and cozy environment for the child to engage in their reading activity. ", "page_idx": 24}, {"type": "text", "text": "Why is this video funny? ", "page_idx": 24}, {"type": "text", "text": "The video is funny because it features a baby wearing glasses and holding a book, pretending to read. The baby's actions and the fact that it is wearing glasses, which are typically associated with adults, create a humorous and adorable scene. The baby's innocent and curious expression as it looks at the book adds to the humor and charm of the moment. The combination of the baby's actions and the visual of a young child wearing glasses creates a lighthearted and entertaining scene that can bring a smile to the viewer's face. ", "page_idx": 24}, {"type": "text", "text": "What is strange about this video? ", "page_idx": 24}, {"type": "image", "img_path": "Ioabr42B44/tmp/ec373c74b6b55978ae35d7555558f4e772c6bd38a55ebaea679eaff3e44b4585.jpg", "img_caption": ["Figure 11: Qualitative results of the video understanding. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "The strange aspect of this video is that a man is sitting in the middle of a group of lions, which is an unusual and potentially dangerous situation. Lions are wild animals and are known for their strength and predatory instincts. It is highly unusual for a person to be in such close proximity to a group of lions without any protective measures or barriers. The man's presence among the lions raises concerns about his safety and the potential risks involved in such an encounter. ", "page_idx": 24}, {"type": "image", "img_path": "Ioabr42B44/tmp/ebf91da774d5db1fa67835c122a478af31aed56763255eab46ed30b802193615.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "The image features a woman sitting at a table, holding a tube of sunscreen in her hand. She is applying the sunscreen to her face, ensuring that she is protected from the sun's harmful rays. The sunscreen is a product from Skin Authority, and the woman is using it to moisturize her skin and maintain its health. The scene emphasizes the importance of using sunscreen as a daily skincare routine to prevent sun damage and maintain a healthy complexion. ", "page_idx": 25}, {"type": "text", "text": "Could you recommend some products similar to the one featured in the video? ", "page_idx": 25}, {"type": "text", "text": "Based on the image, the product featured is a tube of sunscreen moisturizer from Skin Authority. Some similar products that you might consider are other sunscreen moisturizers from reputable brands, such as Neutrogena, Aveeno, or CeraVe. These brands offer a range of sunscreen moisturizers with varying SPF levels and formulations to cater to different skin types and preferences. It is essential to choose a product that suits your skin type and provides adequate protection against harmful UV rays while moisturizing your skin. ", "page_idx": 25}, {"type": "text", "text": "Can you identify any safety hazards in this video? ", "page_idx": 25}, {"type": "text", "text": "", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "Ioabr42B44/tmp/3dbd29eb3401d5e133db31701c13cb3bd0eb6734f3d17aa6c04ae878c6df405d.jpg", "img_caption": ["Figure 12: Qualitative results of the video understanding. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "In the video, there is a baby playing with a toy truck near a fireplace. This situation presents a potential safety hazard, as the baby might accidentally fall into the fireplace or get too close to the fire, which could lead to burns or other injuries. It is important for parents or caregivers to supervise the child closely and ensure that the baby is playing in a safe distance from the fireplace to prevent any accidents. ", "page_idx": 25}, {"type": "image", "img_path": "Ioabr42B44/tmp/2cb642ac333ed161fc2f160a6cda347c9e3393759c630f27f67c0ffc7a1348eb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "The image features a person standing in a kitchen, preparing a meal by cutting up a variety of bell peppers. The person is holding a knife and is in the process of slicing the peppers into smaller pieces. The sliced peppers are placed on a tray, with some of them already cut and others still needing to be prepared. The scene captures the person's attention to detail and focus on preparing the meal. ", "page_idx": 26}, {"type": "text", "text": "How many slices are on the baking sheet. ", "page_idx": 26}, {"type": "text", "text": "There are four slices of red peppers on the baking sheet. ", "page_idx": 26}, {"type": "image", "img_path": "Ioabr42B44/tmp/9bf79bed4c9fbd0cc44cadf2d657d9835081a44c21ba0258f2940b4445f626ba.jpg", "img_caption": ["Figure 13: Qualitative results of the video understanding. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "The image captures a snowy day at a track and field event, where a man is preparing to throw a javelin. He is standing on a red track, wearing a blue shirt and black pants, and is focused on his performance. There are several other people in the scene, some of whom are standing near the track, possibly watching the event or waiting for their turn. In addition to the main javelin thrower, there are two other people visible in the image, one on the left side and another on the right side of the scene. The presence of multiple people suggests that this is a competitive event or a practice session with multiple participants. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide our contributions both in the abstract and introduction. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We discuss the limitation in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Yes, we provide all necessary information to reproduce the experimental results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We will provide the anonymous code, and our code and data will be publicly available. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The training and test details are described in the paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide these information in the implementation details. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our research follows the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We cite the original paper for assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We will provide an anonymous URL. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]