[{"figure_path": "zFHJUSTZka/tables/tables_3_1.jpg", "caption": "Table 1: Comparison between OAIF (proposed) and existing DAP methods, with or without a separate RM. Technically, training RMs on pre-collected preference data still suffers from the distribution shift problem, as RMs cannot get feedback for responses from the model \u03c0\u03b8t.", "description": "This table compares the proposed Online AI Feedback (OAIF) method with existing Direct Preference Alignment (DAP) methods.  It highlights key differences in whether a separate reward model (RM) is needed, whether the method is on-policy (meaning the model learns from its own generations), and whether feedback is provided online during training.  The table shows that OAIF offers advantages by being both on-policy and providing online feedback, unlike the mostly offline and off-policy approaches in the other listed methods.  The distribution shift problem described refers to the mismatch between the data used to train the reward model (if one is used) and the data generated by the model being trained.", "section": "LLM-based online feedback for DAP methods"}, {"figure_path": "zFHJUSTZka/tables/tables_3_2.jpg", "caption": "Table 1: Comparison between OAIF (proposed) and existing DAP methods, with or without a separate RM. Technically, training RMs on pre-collected preference data still suffers from the distribution shift problem, as RMs cannot get feedback for responses from the model \u03c0\u03b8t.", "description": "This table compares the proposed Online AI Feedback (OAIF) method with existing Direct Preference Alignment (DAP) methods.  It highlights key differences in whether a Reward Model (RM) is used, whether the method is on-policy (learning from the current model's generations) or off-policy, and whether the feedback is online or offline.  The table emphasizes that even RM-based online methods suffer from a distribution shift because the RM is trained on pre-collected data that may not accurately reflect the current model's responses.", "section": "LLM-based online feedback for DAP methods"}, {"figure_path": "zFHJUSTZka/tables/tables_5_1.jpg", "caption": "Table 2: Win/tie/loss rate of DPO with OAIF (online DPO) against vanilla DPO (offline DPO) on the TL; DR, Helpfulness, Harmlessness tasks, along with the quality score of their generations, judged by human raters.", "description": "This table presents the results of a human evaluation comparing the performance of online DPO (using the OAIF method) against offline DPO across three different tasks: TL;DR, Helpfulness, and Harmlessness.  The evaluation metrics include win rate, tie rate, loss rate, and a quality score (on a scale, presumably 1-5, although the exact scale is not explicitly stated in the provided text) for the model's generated responses.  The results highlight the superior performance of the online DPO method compared to offline DPO.", "section": "4.2 How effective is OAIF for LLM alignment?"}, {"figure_path": "zFHJUSTZka/tables/tables_5_2.jpg", "caption": "Table 2: Win/tie/loss rate of DPO with OAIF (online DPO) against vanilla DPO (offline DPO) on the TL;DR, Helpfulness, Harmlessness tasks, along with the quality score of their generations, judged by human raters.", "description": "This table presents the results of a human evaluation comparing the performance of DPO with and without online AI feedback (OAIF).  The evaluation was conducted across three different tasks: TL;DR, Helpfulness, and Harmlessness.  For each task, the table shows the win rate, tie rate, and loss rate of the online DPO against the offline DPO.  Additionally, it provides the average quality score assigned by human raters to the generations from each method.", "section": "4.2 How effective is OAIF for LLM alignment?"}, {"figure_path": "zFHJUSTZka/tables/tables_13_1.jpg", "caption": "Table 1: Comparison between OAIF (proposed) and existing DAP methods, with or without a separate RM. Technically, training RMs on pre-collected preference data still suffers from the distribution shift problem, as RMs cannot get feedback for responses from the model \u03c0\u03b8t.", "description": "This table compares the proposed Online AI Feedback (OAIF) method with existing Direct Preference Alignment methods, highlighting whether they use a Reward Model (RM), if the feedback is online and on-policy, and if an RM is needed.", "section": "LLM-based online feedback for DAP methods"}, {"figure_path": "zFHJUSTZka/tables/tables_17_1.jpg", "caption": "Table 1: Comparison between OAIF (proposed) and existing DAP methods, with or without a separate RM. Technically, training RMs on pre-collected preference data still suffers from the distribution shift problem, as RMs cannot get feedback for responses from the model \u03c0\u03b8t.", "description": "This table compares the proposed Online AI Feedback (OAIF) method with existing Direct Alignment from Preferences (DAP) methods.  It highlights key differences in whether a separate reward model (RM) is needed, whether the feedback is online or offline, and whether the methods are on-policy or off-policy. The table emphasizes that even RM-based online methods still suffer from a distribution shift problem because the RM is trained on pre-collected data and not on the model's own generations during training.", "section": "LLM-based online feedback for DAP methods"}]