[{"figure_path": "dJ9KzkQ0oH/figures/figures_2_1.jpg", "caption": "Figure 1: Automata-theoretic neural model checking via fair termination", "description": "This figure illustrates the automata-theoretic approach to neural model checking.  Panel (a) shows a block diagram of the system. The hardware model *M* and B\u00fcchi automaton *A\u00ac\u03a6* operate synchronously.  The outputs of *M* (obs *X<sub>M</sub>*) are fed as inputs to *A\u00ac\u03a6*, which produces a state *q*.  These, along with register values (reg *X<sub>M</sub>*), are used as input to a neural network *V* with trainable parameters *\u03b8<sub>q</sub>*  for each state. Panel (b) shows a trace, visualizing the ranking function *V* and indicator function 1<sub>F</sub>(*q*). The ranking function decreases strictly at every transition from a fair state, demonstrating fair termination and proving that the system satisfies the specification.", "section": "Automata-theoretic Linear Temporal Logic Model Checking"}, {"figure_path": "dJ9KzkQ0oH/figures/figures_4_1.jpg", "caption": "Figure 3: Neural ranking function architecture", "description": "The figure shows the architecture of the neural ranking function used in the paper.  It's a feed-forward network consisting of a normalization layer, an element-wise multiplication layer, and a multi-layer perceptron with clamped ReLU activation functions. The normalization layer scales the input values. The element-wise multiplication layer applies a trainable scaling factor to each neuron. The multi-layer perceptron has trainable weights and biases. The clamp operation restricts the output values to a specific range. The architecture is designed to efficiently represent a ranking function for fair termination.", "section": "3 Neural Ranking Functions for Fair Termination"}, {"figure_path": "dJ9KzkQ0oH/figures/figures_5_1.jpg", "caption": "Figure 1: Automata-theoretic neural model checking via fair termination", "description": "This figure illustrates the automata-theoretic approach to neural model checking.  Panel (a) shows a synchronous composition of the hardware model (M) and the B\u00fcchi automaton (A\u00ac\u03a6) for the negation of the LTL specification (\u00ac\u03a6). The output of the model (obs XM) serves as the input for the automaton. The automaton identifies counterexamples to the specification. Panel (b) illustrates the concept of fair termination using a ranking function. A ranking function V assigns a value to each state of the composition M || A\u00ac\u03a6.  For a fair execution, the ranking function must strictly decrease each time a transition from a fair state is taken, ensuring that no fair executions exist.", "section": "Automata-theoretic Linear Temporal Logic Model Checking"}, {"figure_path": "dJ9KzkQ0oH/figures/figures_7_1.jpg", "caption": "Figure 5: Solved tasks in terms of state space size and logic gate count (log scale)", "description": "This figure shows the number of tasks successfully solved by different model checkers (ABC, nuXmv, Industry tool X, and the proposed neural method) plotted against the state space size and logic gate count.  The log scale highlights the differences in scalability of the various methods. The proposed neural method shows significantly better performance than the others across a wide range of sizes and complexities.", "section": "Experimental Evaluation"}, {"figure_path": "dJ9KzkQ0oH/figures/figures_8_1.jpg", "caption": "Figure 6: Runtime comparison with the state of the art (all times are in log scale)", "description": "This figure presents a comprehensive runtime comparison between the proposed neural model checking method and state-of-the-art tools (ABC, nuXmv, and Industry tool X).  Subfigure (a) shows a cactus plot illustrating the cumulative number of tasks completed by each tool within a given time limit (5 hours). Subfigure (b) presents a scatter plot showing the individual runtime of each tool for each task. The size and brightness of the points represent the state space size. The plot indicates how often the proposed method is faster than the others. Subfigure (c) depicts the learning and checking times for the proposed approach, highlighting the efficiency of the neural network training process. ", "section": "5 Experimental Evaluation"}]