[{"figure_path": "lD7ziaMHbf/figures/figures_1_1.jpg", "caption": "Figure 1: Accuracy-Latency-MACs comparison among various models on ImageNet-1K. Latency is measured on Oneplus 11 (Snapdragon 8 Gen 2 SoC). The radius of each circle represents the model's MACS (Multiply-Accumulate Operations). Our ECP-ViT-Base achieves the highest top-1 accuracy and offers the fastest inference speed among transformer-based models, providing 6\u00d7 faster than ViT-Base. We simplify the notation as \u201cBase\u201d = B, \u201cSmall\u201d = S, and \u201cTiny\u201d = T.", "description": "This figure compares various vision transformer models (including ECP-ViT) on ImageNet-1K dataset based on Top-1 accuracy, latency (measured on Oneplus 11), and multiply-accumulate operations (MACs).  ECP-ViT-Base is highlighted for achieving the highest accuracy and significantly faster inference speed (6x faster than ViT-Base). The size of each circle visually represents the model's MACs.", "section": "1 Introduction"}, {"figure_path": "lD7ziaMHbf/figures/figures_3_1.jpg", "caption": "Figure 2: Co-Design of Core-Periphery Principle Guided self-attention mechanism in ECP-ViT. At the software-level design, the rescheduled interactions between patches in ECP-ViT are guided by the generated Core-Periphery (CP) graphs. Moreover, the multiplication of query, key, and value matrices in the self-attention mechanism of ECP-ViT is also under the guidance of the core-periphery graph. At the hardware-level design, the slice and transpose reshape operations are eliminated.", "description": "This figure illustrates the co-design framework of ECP-ViT, which consists of software-level and hardware-level designs. The software design shows how core-periphery graphs guide the self-attention mechanism in ECP-ViT, rescheduling interactions between patches and guiding the multiplication of query, key, and value matrices.  The hardware design focuses on eliminating the slice and transpose reshape operations to improve efficiency.  In essence, it explains how the algorithm and system are co-optimized for faster and more efficient processing on mobile devices.", "section": "3 Methodology"}, {"figure_path": "lD7ziaMHbf/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of ViT on attention module. In this example, we use the NCHW format for presenting our data. Words with red color are explicit data layout and transformative operators.", "description": "This figure illustrates the computation flow of a standard Vision Transformer (ViT) attention module. It highlights the data layout transformations (Slice & Reshape and Transpose) involved in processing query (Q), key (K), and value (V) matrices. The red words indicate explicit data layout transformations that are computationally expensive.  The figure shows how these transformations impact the overall efficiency and performance of the ViT model on mobile devices.", "section": "3.2 Core-Periphery Guided Self-Attention in ViT"}, {"figure_path": "lD7ziaMHbf/figures/figures_4_2.jpg", "caption": "Figure 4: Examples of Core-Periphery Nodes in the graph, where the core ratio is calculated by dividing the number of core nodes by the total number of nodes. The white area represents pruned nodes. In adjacency matrices, black color indicates connections between nodes, while white represents no edge.", "description": "This figure shows different examples of core-periphery graphs with varying core ratios (0.1, 0.3, 0.5, and 1.0).  The core ratio represents the proportion of core nodes to the total number of nodes in the graph.  Each graph is visualized in two ways:  as an adjacency matrix (showing connections between nodes) and a visual representation where core nodes are colored red and periphery nodes are black.  As the core ratio increases, the number of connections (black areas in the matrix and visual representations) increases, demonstrating how the structure of the graph evolves with different core ratios.  A core ratio of 1.0 represents a fully connected graph (a complete graph) without any pruned nodes.", "section": "3.2 Core-Periphery Guided Self-Attention in ViT"}, {"figure_path": "lD7ziaMHbf/figures/figures_5_1.jpg", "caption": "Figure 5: The Core-Periphery Principle guides Self-Attention, where the Query, Key, and Value matrices are partitioned into core (C) and periphery (P) components. The conventional self-attention mechanism is transformed into Core-Periphery (CP) attention through the guidance of Core-Periphery graphs.", "description": "This figure illustrates how the core-periphery principle is applied to the self-attention mechanism in the ECP-ViT model.  The Query, Key, and Value matrices, fundamental components of self-attention, are divided into core and periphery parts. This partitioning, guided by the core-periphery graph, transforms the standard self-attention into a Core-Periphery (CP) attention mechanism. The figure visually represents this transformation, showcasing how the core and periphery components interact differently, leading to a more efficient computation.", "section": "3.2 Core-Periphery Guided Self-Attention in ViT"}, {"figure_path": "lD7ziaMHbf/figures/figures_6_1.jpg", "caption": "Figure 3: Illustration of ViT on attention module. In this example, we use the NCHW format for presenting our data. Words with red color are explicit data layout and transformative operators.", "description": "This figure illustrates the data flow and layout transformations in a standard ViT attention module (left) and how ECP-ViT optimizes it (right).  The left side shows the original ViT process, highlighting explicit data layout transformations (red) like Transpose and Reshape, which are computationally expensive on mobile devices. The middle panel illustrates the data layout transformation for fused attention (middle) and the right panel illustrates the efficient data access pattern of ECP-ViT (right). The ECP-ViT design reduces or eliminates these transformations by reorganizing computations and data flow to make the process more efficient for mobile hardware.", "section": "3.2 Core-Periphery Guided Self-Attention in ViT"}, {"figure_path": "lD7ziaMHbf/figures/figures_7_1.jpg", "caption": "Figure 7: Latency Speedup Comparison Over Low-end Device (Xiao Mi 6). B, S, T short for Base, Small, and Tiny models, respectively.", "description": "This figure compares the latency speedup achieved by the proposed ECP-ViT model against the MNN framework on a low-end Xiaomi 6 device.  It shows speedup results for three different sizes of the ViT model (Base, Small, Tiny) and their corresponding ECP-ViT counterparts. The comparison highlights the performance gains of ECP-ViT across various model scales on less powerful mobile hardware.", "section": "4.2 Accuracy Comparison"}]