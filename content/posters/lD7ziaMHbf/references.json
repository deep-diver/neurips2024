{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This paper introduces the Vision Transformer (ViT), a foundational model for the current work, and its performance is compared throughout the paper."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduces the attention mechanism, a core component of Vision Transformers, and its efficiency is a key focus of this paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-00-00", "reason": "This paper presents data-efficient training methods for Vision Transformers that improve performance and reduce computational costs, directly influencing the current research."}, {"fullname_first_author": "Ze Liu", "paper_title": "Swin transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-00-00", "reason": "This paper introduces Swin Transformer, an architecture that improves ViT efficiency, and its performance is compared against the proposed method."}, {"fullname_first_author": "Kai Han", "paper_title": "Transformer in transformer", "publication_date": "2021-00-00", "reason": "This paper proposes another efficient Vision Transformer architecture, which is compared and analyzed in this paper."}]}