[{"figure_path": "1ELFGSNBGC/tables/tables_5_1.jpg", "caption": "Table 1: Main results. Our method uses DINOv2[49] as the backbone. GDino stands for the detector GroundingDINO[39]. AoMSG-2 and AoMSG-4 represent AoMSG models with 2 and 4 layers of Transformer decoder respectively. The best results are underlined. * indicates a trivial result since its input is given in temporal order, and consecutive frames are trivially recalled.", "description": "This table presents the main results of the proposed AoMSG model for Multiview Scene Graph (MSG) generation, compared against various baseline methods.  The metrics evaluated include Recall@1 (for place recognition), PP IoU (place-place IoU), and PO IoU (place-object IoU).  Two versions of PO IoU are shown: one using ground truth object detections and another using the GroundingDINO detector.  The table highlights the superior performance of AoMSG-4, particularly in terms of PO IoU.", "section": "5.4 Results"}, {"figure_path": "1ELFGSNBGC/tables/tables_5_2.jpg", "caption": "Table 2: Comparison of different projector dimensions in AoMSG and SepMSG models. Both are using DINOv2-base[49] as the backbone. Results are evaluated at 30 epochs.", "description": "This table compares the performance of AoMSG and SepMSG models with varying projector dimensions (512, 1024, and 2048).  The backbone used for both models is DINOv2-base. The results, including Recall@1, PP IoU, and PO IoU, are reported for each model and projector dimension after 30 training epochs. This allows for an analysis of how the projector dimension affects the performance of the two models on the multiview scene graph generation task.", "section": "5.4 Results"}, {"figure_path": "1ELFGSNBGC/tables/tables_14_1.jpg", "caption": "Table 1: Main results. Our method uses DINOv2[49] as the backbone. GDino stands for the detector GroundingDINO[39]. AoMSG-2 and AoMSG-4 represent AoMSG models with 2 and 4 layers of Transformer decoder respectively. The best results are underlined. * indicates a trivial result since its input is given in temporal order, and consecutive frames are trivially recalled.", "description": "This table presents the main results of the proposed AoMSG model and compares it with other baseline methods on the Multiview Scene Graph (MSG) task.  The metrics used for evaluation include place and object recall, place-place IoU (PP IoU), place-object IoU (PO IoU), and are calculated with and without ground truth object detection.  Different variants of AoMSG (with varying numbers of transformer decoder layers) are compared, highlighting the superior performance of the proposed method across various metrics.", "section": "5.4 Results"}, {"figure_path": "1ELFGSNBGC/tables/tables_17_1.jpg", "caption": "Table 4: Pilot study for MLLM on MSG. For the MLLM, we use GPT40. The \"model adjusted\" is evaluated on the same set of images as the VLM.", "description": "This table presents a pilot study comparing the performance of a multimodal large language model (MLLM) and a vision-language model (VLM) on the Multiview Scene Graph (MSG) task.  The results are shown for two metrics: PP IoU (Place-Place Intersection over Union) and PO IoU (Place-Object Intersection over Union).  The \"model total\" column shows the MLLM's performance on the entire scene, while the \"model adjusted\" column shows its performance on a subset of images (22 images) comparable to the VLM's input. The VLM column presents the VLM's performance on the same subset of images.  The results indicate the potential benefits of MLLMs for MSG generation, although further research is needed.", "section": "C.3 Approaching MSG Generation with Multimodal Large Language Model"}]