[{"figure_path": "bxH6T1w1FW/figures/figures_1_1.jpg", "caption": "Figure 1: The Ambiguity of Superpixels. This figure compares three superpixel estimates from a recent method named BASS. [5]2While all three samples achieve a similar segmentation quality, some regions are different, and some are almost identical. Since no single segmentation is the \"best\", this suggests that superpixel assignments are not as important as superpixel probabilities.", "description": "The figure shows three different superpixel segmentations of the same image produced by the BASS algorithm.  Despite the similar overall segmentation quality, there are noticeable differences in the boundaries of the superpixels between the three examples. Some areas show consistent segmentation across all three, while others vary significantly. This illustrates the inherent ambiguity in superpixel segmentation, highlighting the fact that a single, \"best\" segmentation doesn't exist.  The authors argue that, due to this ambiguity, using superpixel probabilities (the likelihood of a pixel belonging to a given superpixel) is more effective than using hard superpixel assignments (assigning each pixel to only one superpixel).", "section": "1 Introduction"}, {"figure_path": "bxH6T1w1FW/figures/figures_2_1.jpg", "caption": "Figure 2: Each Pixel is Connected to Nine Superpixels. This figure illustrates the anatomy of the SLIC superpixels. The left-most figure illustrates how superpixels are conceptually distributed across a grid on the input image with stride Ssp. The right figure illustrates a single pixel is connected to (at most) nine centroids.", "description": "This figure explains how SLIC superpixels are created and how they relate to individual pixels in an image.  The left panel shows a conceptual grid of superpixels overlaid on an image, where each superpixel is a square region with side length *Ssp*. The middle panel zooms in on a region of the image showing a single pixel, highlighting the nine nearest superpixel centers. The right panel shows those nine superpixel centroids and their associated pixel regions, illustrating how each pixel is connected to multiple superpixels.", "section": "3 Preliminaries"}, {"figure_path": "bxH6T1w1FW/figures/figures_3_1.jpg", "caption": "Figure 3: Superpixel Neighborhood Attention. The yellow region represents the attention window and the red contours are a superpixel boundary. NA considers all pixels, mixing the dissimilar orange and blue pixels. H-SNA considers only pixels within its own superpixel, which is too few pixels for denoising. SNA excludes the dissimilar blue pixels but retains the similar orange pixels.", "description": "This figure illustrates the differences between three types of neighborhood attention mechanisms: standard neighborhood attention (NA), hard superpixel neighborhood attention (H-SNA), and soft superpixel neighborhood attention (SNA).  NA considers all pixels within a square window, regardless of their perceptual grouping. H-SNA only considers pixels belonging to the same superpixel as the central pixel, potentially missing crucial information. SNA intelligently incorporates superpixel probabilities to selectively weigh pixels, effectively combining the strengths of both NA and H-SNA by including similar pixels and excluding dissimilar ones.", "section": "3 Preliminaries"}, {"figure_path": "bxH6T1w1FW/figures/figures_4_1.jpg", "caption": "Figure 4: The Latent Superpixel Model. The latent superpixel model assumes superpixel probabilities are sampled for each image pixel. This figure illustrates the data-generating process. The leftmost image allows the reader to visually compare the similarities among superpixels by representing each pixel by its most likely superpixel means. Informally, this looks like a \"low-resolution image\". The superpixels and image pixels are sampled as usual.", "description": "This figure illustrates the process of generating images according to the latent superpixel model proposed in the paper.  First, superpixel probabilities are sampled for each pixel (Step 1). Then, based on these probabilities, a superpixel assignment is made for each pixel (Step 2). Finally, the image pixel values are sampled based on the assigned superpixel (Step 3). The leftmost image shows the superpixel means, creating a low-resolution image representation. The model uses this approach to account for the inherent ambiguity in superpixel assignment.", "section": "4.2 The Latent Superpixel Model"}, {"figure_path": "bxH6T1w1FW/figures/figures_5_1.jpg", "caption": "Figure 6: Estimating Superpixel Probabilities. The superpixel probabilities are learned during training using one of these two methods. The left method uses a shallow UNet followed by a softmax layer with nine channels (g\u03b8,Deep). The right method estimates hyperparameters to be used within SLIC iterations (g\u03b8,SLIC).", "description": "This figure illustrates two different methods for estimating superpixel probabilities used in the Soft Superpixel Neighborhood Attention (SNA) module.  The left side shows a method using a UNet followed by a softmax layer to directly predict probabilities for each superpixel. The right side depicts a method that uses a UNet to estimate hyperparameters for the SLIC superpixel algorithm, which then generates the superpixel probabilities.  Both approaches aim to capture the uncertainty in superpixel assignments, providing a soft weighting for attention rather than hard assignments.", "section": "4.5 Estimating Superpixel Probabilities"}, {"figure_path": "bxH6T1w1FW/figures/figures_5_2.jpg", "caption": "Figure 3: Superpixel Neighborhood Attention. The yellow region represents the attention window and the red contours are a superpixel boundary. NA considers all pixels, mixing the dissimilar orange and blue pixels. H-SNA considers only pixels within its own superpixel, which is too few pixels for denoising. SNA excludes the dissimilar blue pixels but retains the similar orange pixels.", "description": "This figure compares three different attention mechanisms: standard neighborhood attention (NA), hard superpixel neighborhood attention (H-SNA), and soft superpixel neighborhood attention (SNA).  NA considers all pixels in a square window, leading to the mixing of unrelated features. H-SNA only considers pixels within a single superpixel, which can be insufficient for tasks like denoising. SNA offers a compromise by weighting pixels according to their superpixel probabilities, effectively integrating information from multiple superpixels while focusing on perceptually similar regions.", "section": "3.3 Neighborhood Attention"}, {"figure_path": "bxH6T1w1FW/figures/figures_7_1.jpg", "caption": "Figure 7: Denoised Examples [PSNR\u2191]. This figure compares the quality of denoised images using the Simple Network and noise intensity \u03c3 = 20. The attention scale (\u03bbat) is either fixed or learned with a deep network. In both cases, the NA module mixes perceptually dissimilar information, while the SNA module excludes dissimilar regions.", "description": "This figure shows a comparison of denoised images produced by the Simple Network using three different attention methods: standard neighborhood attention (NA), with a fixed or learnable attention scale, and soft superpixel neighborhood attention (SNA), also with fixed or learnable attention scale.  The results demonstrate that SNA effectively eliminates perceptually unrelated information, leading to higher quality denoised images, especially evident in the examples where dissimilar image regions are mixed in the NA results.  The PSNR values provided quantify this improved quality.", "section": "5.2 Gaussian Denoising"}, {"figure_path": "bxH6T1w1FW/figures/figures_9_1.jpg", "caption": "Figure 8: Comparing Superpixel Probabilites via Superpixel Pooling [PSNR\u2191]. This figure uses superpixel pooling to qualitatively compare superpixel probabilities learned with different loss functions. Learning superpixel probabilities with only a denoising loss yields better superpixel pooling than supervised learning with segmentation labels. However, jointly training superpixel probabilities for denoising and image superpixel pooling improves denoising and pooling quality, which suggests a useful relationship between the two tasks.", "description": "This figure compares the results of superpixel pooling using superpixel probabilities trained with different loss functions.  The top row shows an example image and its superpixel pooled versions. The bottom row shows another example. Each column represents a different training scenario: using only the denoising loss, using only the superpixel loss, using both losses. The PSNR values are shown below each image. The results suggest that training superpixel probabilities with a combination of both denoising and superpixel losses leads to better results than using either loss alone.", "section": "5.3 Inspecting the Learned Superpixel Probabilities"}, {"figure_path": "bxH6T1w1FW/figures/figures_17_1.jpg", "caption": "Figure 9: Neighborhood Window Size. Increasing the neighborhood window size includes more samples to decrease the variance but also adds bias since the noisy samples can be weighted improperly. This bias-variance trade-off is illustrated by the increasing optimal window size as the noise intensity increases. Since the bias of H-SNA is nearly zero, the increasing neighborhood size only increases the denoising quality within the selected grid.", "description": "This figure shows the effect of increasing neighborhood size on denoising performance for three different noise levels (\u03c3 = 10, 20, 30).  As the neighborhood size grows, the variance decreases (leading to better denoising), but the bias increases (introducing errors). This is a bias-variance tradeoff, and the optimal neighborhood size increases as noise increases.  The Hard Superpixel Neighborhood Attention (H-SNA) method, having a bias close to zero, shows improved denoising performance with increasing neighborhood size.", "section": "B.1 Ablation Experiments"}, {"figure_path": "bxH6T1w1FW/figures/figures_17_2.jpg", "caption": "Figure 10: Number of Auxiliary Parameters. This ablation experiment expands the size of the auxiliary network by increasing the number of UNet channels. The x-axis plots the number of parameters in the auxiliary network (||). The y-axis plots the PSNR of several denoiser networks. Generally, more parameters improve the denoising quality. The drop in denoising quality for the final network may be due to under-training.", "description": "This ablation study investigates the effect of varying the size of the auxiliary network on denoising performance.  The plot shows PSNR values for different noise levels (\u03c3 = 10, 20, 30) as a function of the number of parameters in the auxiliary network. The results suggest that larger auxiliary networks generally lead to better denoising performance, although there's a possible indication of overfitting at the largest network size tested.", "section": "B.1 Ablation Experiments"}, {"figure_path": "bxH6T1w1FW/figures/figures_17_3.jpg", "caption": "Figure 11: Number of Superpixels. The number of superpixels has a limited effect on the denoising quality when compared with other hyperparameters. Generally, increasing the number of superpixels increases the denoising quality. When the noise is low (\u03c3 = 10) the number of superpixels seems irrelevant. As the noise increases, the benefit of more superpixels becomes more apparent. Generally, using explicit SLIC iterations is more effective than predicting superpixel probabilities directly.", "description": "This figure shows the effect of the number of superpixels on denoising performance for different noise levels (\u03c3 = 10, 20, 30).  The results indicate that while increasing the number of superpixels generally improves denoising quality, this effect is more pronounced at higher noise levels. At low noise levels, the number of superpixels appears less critical.  Additionally, using the SLIC algorithm directly to generate superpixels produces slightly better results than learning them from a neural network.", "section": "B.1 Ablation Experiments"}, {"figure_path": "bxH6T1w1FW/figures/figures_18_1.jpg", "caption": "Figure 7: Denoised Examples [PSNR\u2191]. This figure compares the quality of denoised images using the Simple Network and noise intensity \u03c3 = 20. The attention scale (\u03bbat) is either fixed or learned with a deep network. In both cases, the NA module mixes perceptually dissimilar information, while the SNA module excludes dissimilar regions. ", "description": "This figure compares the denoising results of the proposed SNA and standard NA methods on several images with noise intensity \u03c3 = 20. It shows that SNA outperforms NA in terms of visual quality and PSNR values by excluding dissimilar regions and preserving similar regions within the attention windows, while NA mixes perceptually unrelated pixels leading to a decrease in the overall quality.", "section": "5.2 Gaussian Denoising"}]