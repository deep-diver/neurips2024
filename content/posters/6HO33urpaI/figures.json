[{"figure_path": "6HO33urpaI/figures/figures_3_1.jpg", "caption": "Figure 1: An illustration of the open-book framework. At each reasoning step t, we simultaneously input (x, y(t-1)) and instances from the training set T, yielding y(t).", "description": "The figure illustrates the architecture of the proposed open-book neural algorithmic reasoning framework.  The framework enhances standard encoder-processor-decoder models by incorporating a 'Dataset Encoder' and an 'Open-Book Processor'. The Dataset Encoder creates representations (R) of the training data.  During reasoning, the Open-Book Processor combines the processor's hidden state (h^(t)) with these training data representations (R) before feeding the result (\u0125^(t)) into the decoder to predict the next step in the algorithm execution (y^(t)).  This allows the model to access and utilize all training instances while reasoning for a given instance, mimicking the ability to consult examples during an open-book exam.", "section": "3.1 Framework"}, {"figure_path": "6HO33urpaI/figures/figures_6_1.jpg", "caption": "Figure 2: Comparison of the MPNN architecture's performance before and after augmentation with the open-book framework. The 30 tasks are arranged in descending order of improvement magnitude.", "description": "This figure compares the performance of the MPNN architecture before and after integrating the open-book framework.  The x-axis lists the 30 algorithmic tasks from the CLRS benchmark, ordered from the largest to smallest improvement in performance after adding the open-book framework. The y-axis represents the average score (likely F1-score) achieved on each task.  The blue bars show the performance with the open-book framework, and the orange bars represent the performance without it.  Error bars are included to indicate the variability in performance across multiple runs.", "section": "4.2 Single-Task Augmenting"}, {"figure_path": "6HO33urpaI/figures/figures_7_1.jpg", "caption": "Figure 3: Comparisons between our multi-task augmented approach and Triplet-GMPNN. The 30 tasks are arranged in descending order of improvement magnitude.", "description": "This figure compares the performance of the proposed multi-task augmented approach with the Triplet-GMPNN baseline on 30 algorithmic reasoning tasks from the CLRS benchmark. The tasks are sorted by the magnitude of performance improvement achieved by the proposed approach, showcasing how significantly the proposed method enhances performance across a variety of tasks.", "section": "4.3 Multi-Task Augmenting"}]