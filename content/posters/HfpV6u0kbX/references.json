{"references": [{"fullname_first_author": "Edward J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2022-00-00", "reason": "This paper introduces the LoRA (Low-Rank Adaptation) technique, a core method used in the described multi-task LLM serving system."}, {"fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-10-17", "reason": "This paper details GPTQ, a crucial quantization method used to reduce memory consumption and improve efficiency in the multi-task LLM serving system."}, {"fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention", "publication_date": "2023-00-00", "reason": "This paper introduces vLLM, a state-of-the-art LLM serving system that is used as a baseline for comparison, highlighting the advancements of the proposed system."}, {"fullname_first_author": "Ying Sheng", "paper_title": "S-lora: Serving thousands of concurrent lora adapters", "publication_date": "2023-00-00", "reason": "This paper presents S-LoRA, another state-of-the-art multi-task LLM serving system that is used as a comparison baseline, highlighting the improvements in efficiency and throughput of the proposed system."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "Gpt3.int8(): 8-bit matrix multiplication for transformers at scale", "publication_date": "2022-00-00", "reason": "This paper explores 8-bit quantization for transformers, a relevant technique to the model quantization strategies explored in the paper."}]}