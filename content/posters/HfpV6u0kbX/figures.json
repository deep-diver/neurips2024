[{"figure_path": "HfpV6u0kbX/figures/figures_3_1.jpg", "caption": "Figure 1: Design overview of LoRA-Inlaid. The workflow is labeled with numbers in the diagram. Quantize and Deploy indicates the initiation of the server performing the multi-task quantization and deploying the quantized model and LoRA online. \u2461 Schedule involves utilizing a multi-task scheduling strategy for Inference. If a new task is detected, it invokes \u2463 Add Task to dynamically add the new task without interrupting the ongoing services.", "description": "This figure illustrates the LoRA-Inlaid system's architecture. It shows the process flow, starting from the base model through joint quantization, deployment of the quantized model and multiple LoRA adapters, dynamic task addition, request scheduling, and inference.  The numbers correspond to steps described in the paper's section 3.", "section": "3 LORA-Inlaid"}, {"figure_path": "HfpV6u0kbX/figures/figures_4_1.jpg", "caption": "Figure 2: Process of MLGPTQ vs GPTQ. Both MLGPTQ and GPTQ follow the Forward-Aggregate Info-Modify Weight-Quant paradigm. MLGPTQ primarily improves the first three steps, aiming to better gather and highlight critical information for all tasks.", "description": "This figure compares the multi-task quantization algorithm MLGPTQ with the single-task GPTQ algorithm. Both algorithms follow a four-step process: Forward, Aggregate Info, Modify Weight, and Quant.  The key difference is in how they handle multiple tasks. GPTQ uses a naive mix-aggregated Hessian matrix, which dilutes task-specific information. In contrast, MLGPTQ uses a max-aggregated Hessian matrix, which effectively captures and highlights critical information for each task, leading to better accuracy.", "section": "3.1 Multi-task Joint Quantization"}, {"figure_path": "HfpV6u0kbX/figures/figures_5_1.jpg", "caption": "Figure 3: Length distributions of different tasks.", "description": "The figure shows the distribution of input length, output length, and total length (input+output) for four different tasks: French-English Translation, Table Summary, Text Summary, and Code Generation.  The y-axis is the log (base 10) of the number of tokens, and the x-axis shows the task type. Each task has boxplots showing the distribution of lengths for each type. The boxplots illustrate the variability in sequence lengths for different tasks, highlighting the challenges for efficient scheduling in multi-task LLM serving systems.", "section": "3.3 Multi-task Scheduling"}, {"figure_path": "HfpV6u0kbX/figures/figures_5_2.jpg", "caption": "Figure 4: Number of tasks in each scheduling step of different scheduling strategies.", "description": "The figure shows the number of tasks processed in each scheduling step by three different scheduling strategies: Skip-join MLFQ (FastServe), FIFO (S-LORA), and the proposed LoRA-Inlaid approach.  LoRA-Inlaid consistently processes a significantly smaller number of tasks per step compared to the baselines, leading to more efficient memory management and reduced swapping overhead. The mean and standard deviation of the number of tasks per step are shown for each strategy.", "section": "3.3 Multi-task Scheduling"}, {"figure_path": "HfpV6u0kbX/figures/figures_7_1.jpg", "caption": "Figure 5: Effectiveness anatomy. The radar charts show the relative accuracy drops compared to no quantization (outer is better).", "description": "This figure presents a comparison of different quantization methods' effectiveness in maintaining model accuracy across various machine translation tasks.  Each radar chart represents a specific translation task (e.g., trans-sw, trans-cs, etc.), comparing the relative accuracy drops of four methods: MLGPTQ, GPTQtweaked, GPTQ, and MLGPTQno_target, against a baseline of unquantized models. The outer the data point is on the radar chart, the better is the method's performance relative to the baseline in that specific metric. Each axis of the radar chart displays a different metric (ROUGE2, ROUGE1, NIST MT, S_BLEU, METEOR, G_BLEU) commonly used in machine translation evaluation. The figure's goal is to illustrate that MLGPTQ effectively minimizes accuracy loss compared to other methods, highlighting the importance of its multi-task approach.", "section": "4.2 Model Quality after Quantization"}, {"figure_path": "HfpV6u0kbX/figures/figures_8_1.jpg", "caption": "Figure 6: System performance in terms of throughput (higher is better), latency (lower is better), and JCT (lower is better) under various request rates (x-axis) and numbers of tasks (T).", "description": "This figure shows the performance comparison of LoRA-Inlaid against S-LORA and vLLM in terms of throughput, latency, and job completion time (JCT).  The performance is evaluated under different request rates and varying numbers of tasks.  It highlights the superior performance of LoRA-Inlaid across various conditions.", "section": "4.3 End-to-end System Performance"}, {"figure_path": "HfpV6u0kbX/figures/figures_8_2.jpg", "caption": "Figure 7: SLO Attainment (higher is better) under various serving loads (RTX 4090).", "description": "This figure displays the SLO attainment under different serving loads for three LLM serving systems: LoRA-Inlaid, S-LORA, and vLLM.  The x-axis represents combinations of request rates and maximum sequence lengths.  The y-axis shows the percentage of requests completed within the expected latency (SLO). The figure shows that LoRA-Inlaid consistently outperforms S-LORA and vLLM across all load conditions, and that S-LORA and vLLM experience a significant drop in SLO attainment at higher loads.  The results highlight LoRA-Inlaid's ability to maintain high performance and efficiency under various load conditions.", "section": "4.3 End-to-end System Performance"}, {"figure_path": "HfpV6u0kbX/figures/figures_9_1.jpg", "caption": "Figure 8: Left: Ablation studies of scheduling strategies on LoRA-Inlaid (100 tasks). Right: Effectiveness of quantization to SLO Attainment (Llama-2-13B is not shown due to OOM of the other two methods).", "description": "The left part of the figure shows the ablation study of the multi-task scheduling strategies proposed in the paper. Different scheduling strategies are compared based on SLO attainment with LLaMA2-7B and LLaMA2-13B models and 100 tasks. The right part shows the effectiveness of quantization on SLO attainment by comparing LoRA-Inlaid with and without quantization, along with S-LORA.  LLaMA2-13B is excluded from the right graph because the other methods caused out-of-memory (OOM) errors.", "section": "4.3 End-to-end System Performance"}, {"figure_path": "HfpV6u0kbX/figures/figures_15_1.jpg", "caption": "Figure 1: Design overview of LoRA-Inlaid. The workflow is labeled with numbers in the diagram. Quantize and Deploy indicates the initiation of the server performing the multi-task quantization and deploying the quantized model and LoRA online. \u2461 Schedule involves utilizing a multi-task scheduling strategy for Inference. If a new task is detected, it invokes \u2463 Add Task to dynamically add the new task without interrupting the ongoing services.", "description": "This figure provides a high-level overview of the LoRA-Inlaid system architecture.  It shows the main components and the workflow, highlighting the joint quantization process, dynamic task addition module, and the multi-task scheduling strategy.  The diagram visually represents how the system handles multiple tasks concurrently, quantizes the base model efficiently, and adds new tasks without service interruption.", "section": "3 LORA-Inlaid"}, {"figure_path": "HfpV6u0kbX/figures/figures_16_1.jpg", "caption": "Figure 1: Design overview of LoRA-Inlaid. The workflow is labeled with numbers in the diagram. Quantize and Deploy indicates the initiation of the server performing the multi-task quantization and deploying the quantized model and LoRA online. \u2461 Schedule involves utilizing a multi-task scheduling strategy for Inference. If a new task is detected, it invokes \u2463 Add Task to dynamically add the new task without interrupting the ongoing services.", "description": "This figure shows a high-level overview of the LoRA-Inlaid system architecture.  The system starts by jointly quantizing a base model and multiple LoRA adapters. Then, it uses a multi-task scheduling strategy to efficiently serve requests from multiple tasks.  The system also includes a mechanism for dynamically adding new LoRA adapters as needed, allowing the system to adapt to changing workload demands without interruption.", "section": "3 LORA-Inlaid"}, {"figure_path": "HfpV6u0kbX/figures/figures_20_1.jpg", "caption": "Figure 1: Design overview of LoRA-Inlaid. The workflow is labeled with numbers in the diagram. Quantize and Deploy indicates the initiation of the server performing the multi-task quantization and deploying the quantized model and LoRA online. \u2461 Schedule involves utilizing a multi-task scheduling strategy for Inference. If a new task is detected, it invokes \u2463 Add Task to dynamically add the new task without interrupting the ongoing services.", "description": "This figure shows a high-level overview of the LoRA-Inlaid system architecture.  It illustrates the different stages of the process, starting with joint quantization and deployment, moving to the online inference phase, which utilizes a multi-task scheduling strategy.  The figure also highlights the dynamic task addition module, enabling the system to incorporate new tasks on-the-fly without disrupting existing services.", "section": "3 LORA-Inlaid"}]