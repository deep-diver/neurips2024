[{"figure_path": "FisyQfoJCm/figures/figures_1_1.jpg", "caption": "Figure 1: Framework overview. (a) In motion quantization, human motion is quantized into a spatial-temporal 2D token map by a joint VQ-VAE. (b) In motion generation, a temporal-spatial 2D masking is performed to obtain a masked map, and then a spatial-temporal 2D transformer is designed to infer the masked tokens.", "description": "This figure illustrates the overall framework of the proposed method. (a) shows the motion quantization process, where human motion is quantized into a 2D token map using a joint Vector Quantized Variational Autoencoder (VQ-VAE). (b) shows the motion generation process, which involves temporal-spatial 2D masking, followed by a spatial-temporal 2D transformer to predict the masked tokens and generate the motion sequence.", "section": "3 Method"}, {"figure_path": "FisyQfoJCm/figures/figures_3_1.jpg", "caption": "Figure 2: The structure of our spatial-temporal 2D Joint VQ-VAE for motion quantization. Independently. This not only makes the quantization task more tractable and reduces approximation errors, but also preserves crucial spatial information between the joints.", "description": "This figure illustrates the architecture of the spatial-temporal 2D Joint VQ-VAE used for motion quantization. The input is a motion sequence represented as a spatial-temporal 2D vector map.  An encoder processes this map, converting it into a lower-dimensional representation.  Vector Quantization (VQ) then maps these vectors to a discrete codebook, creating a code map. Finally, a decoder reconstructs the original motion from the code map. This process is designed to simplify the quantization, reduce approximation errors, and preserve the spatial relationships between joints.", "section": "3.3 Spatial-temporal 2D Joint Quantization of Motion"}, {"figure_path": "FisyQfoJCm/figures/figures_5_1.jpg", "caption": "Figure 1: Framework overview. (a) In motion quantization, human motion is quantized into a spatial-temporal 2D token map by a joint VQ-VAE. (b) In motion generation, a temporal-spatial 2D masking is performed to obtain a masked map, and then a spatial-temporal 2D transformer is designed to infer the masked tokens.", "description": "This figure illustrates the proposed framework for human motion generation.  Part (a) shows the motion quantization process, where human motion is converted into a 2D token map by a joint Vector Quantized Variational Autoencoder (VQ-VAE). This 2D representation maintains spatial and temporal relationships between joints. Part (b) illustrates the motion generation process. A temporal-spatial 2D masking technique is applied to the 2D token map, creating a masked representation.  A spatial-temporal 2D transformer then uses this masked data and text input to predict the masked tokens, generating a new motion sequence.", "section": "3 Method"}, {"figure_path": "FisyQfoJCm/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative results on the test set of HumanML3D. The color from light blue to dark blue indicates the motion sequence order. An arrow indicates this sequence is unfolded in the time axis.", "description": "This figure shows several examples of motion sequences generated by three different methods: T2M-GPT, MoMask, and the proposed method. Each row represents a different text prompt, and the columns show the generated motion sequences. The color gradient from light blue to dark blue indicates the order of the frames in each sequence.  An arrow indicates that the sequence is shown unfolded in time. The figure aims to demonstrate the qualitative differences in the generated motions, highlighting the superiority of the proposed method in generating more accurate and realistic motions aligning with the text prompt.  It showcases the ability of the proposed method to generate smooth, natural movements consistent with the descriptions.", "section": "4.4 Ablation Study"}, {"figure_path": "FisyQfoJCm/figures/figures_9_1.jpg", "caption": "Figure 5: Motion Editing. The edited regions are indicated in green.", "description": "This figure demonstrates the motion editing capabilities of the proposed model.  The leftmost image shows the original generated motion sequence. The middle image shows temporal editing, where a portion of the motion sequence has been replaced with a different sequence (indicated in green), altering the overall motion pattern. The rightmost image demonstrates both temporal and spatial editing, where not only the temporal sequence but also specific joints within the sequence are changed (again indicated in green). This showcases the model's capacity to alter existing generated motions in both temporal and spatial domains.", "section": "4.5 Motion Editing / Inpainting"}, {"figure_path": "FisyQfoJCm/figures/figures_16_1.jpg", "caption": "Figure 6: More qualitative results of our method are presented. The color from light blue to dark blue indicates the motion sequence order. An arrow indicates this sequence is unfolded in the time axis.", "description": "This figure shows qualitative results of motion generation for four different text prompts.  The results from three different methods are compared: T2M-GPT, MoMask, and the authors' method. Each row corresponds to one text prompt, and each column represents one of the four motions generated in response. The color gradient from light to dark blue indicates the temporal progression of the motion. An arrow indicates that the sequence of motions has been unfolded across the time axis, meaning the frames have been rearranged to emphasize the complete motion for each individual action.", "section": "A.3.2 More Qualitative Results"}, {"figure_path": "FisyQfoJCm/figures/figures_17_1.jpg", "caption": "Figure 3: The temporal-spatial masking strategy (a) and the spatial-temporal attention (b) for motion generation.", "description": "This figure illustrates the two main steps of the motion generation process in the MoGenTS model. (a) shows the temporal-spatial masking process, where frames are randomly masked in the temporal dimension, and then joints are randomly masked within the remaining frames in the spatial dimension. (b) shows the spatial-temporal attention mechanism, which consists of three types of attention: spatial-temporal 2D attention, joint spatial attention, and joint temporal attention. This mechanism takes into account both the spatial relationships between joints and the temporal sequence of motions.", "section": "3.5 Spatial-temporal 2D Motion Generation"}]