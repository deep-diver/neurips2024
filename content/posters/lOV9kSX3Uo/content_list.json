[{"type": "text", "text": "Optimizing over Multiple Distributions under Generalized Quasar-Convexity Condition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shihong Ding dingshihong@stu.pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Long Yang1 YANGLONG001@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Luo Luo2,4 luoluo@fudan.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Cong Fang1,37 fangcong@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "I State Key Lab of General AI, School of Inteligence Science and Technology, Peking University 2 School of Data Science, Fudan University 3 Institute for Artificial Intelligence, Peking University 4 Shanghai Key Laboratory for Contemporary Applied Mathematics ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study a typical optimization model where the optimization variable is composed of multiple probability distributions. Though the model appears frequently in practice, such as for policy problems, it lacks specific analysis in the general setting. For this optimization problem, we propose a new structural condition/landscape description named generalized quasar-convexity (GQC) beyond the realms of convexity. In contrast to original quasar-convexity [24], GQC allows an individual quasar-convex parameter $\\gamma_{i}$ for each variable block $i$ and the smaller of $\\gamma_{i}$ implies less block-convexity. To minimize the objective function, we consider a generalized oracle termed as the internal function that includes the standard gradient oracle as a special case. We provide optimistic mirror descent (OMD) for multiple distributions and prove that the algorithm can achieve an adaptive $\\tilde{\\mathcal{O}}((\\sum_{i=1}^{d}{1/\\gamma_{i}})\\varepsilon^{-1})$ iteration complexity to find an $\\varepsilon$ -suboptimal global solution without pre-known the exact values of $\\gamma_{i}$ when the objective admits \u201cpolynomial-like\u201d structural. Notably, it achieves iteration complexity that does not explicitly depend on the numberof distribuions andstily fster(1/a s.dmaxi/(1a/Fa)then mirror decent methods. We also extend GQC to the minimax optimization problem proposing the generalized quasar-convexity-concavity (GQCC) condition and a decentralized variant of OMD with regularization. Finally, we show the applications of our algorithmic framework on discounted Markov Decision Processes problem and Markov games, which bring new insights on the landscape analysis of reinforcement learning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study a common class of generic minimization problem ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{x}\\in\\mathcal{X}}f(\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where the optimization variable $\\textbf{\\em x}$ is composed of $d$ probability distributions $\\{{\\pmb x}_{i}\\}_{i=1}^{d}$ and $\\mathcal{X}$ denotes the product space of the $d$ probability simplexes. Problem (1) meets widespread applications in reinforcement learning optimization [62, 2, 35], multi-class classification [53] and model selection type aggregation [29]. In this paper, we are particularly interested in the case where $d$ is reasonably large and we manage to obtain complexities dependent of $d$ non-explicitly. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "When $f$ is convex with respect to $\\textbf{\\em x}$ , many efficient algorithms can be powerful tools for solving Problem (1). One well-known algorithm is mirror descent (MD) [5] which is based on Bregman divergence. The wide choices of Bregman divergence enable the algorithm to iterate and converge under specifically constrained region [34]. In particular, if one applies the usual Euclidean distance, the algorithm reduces to project gradient descent [37]. One common and more sophisticated selection is the Kullback-Leibler (KL) divergence, the algorithm thereby becoming the variant of multiplicative weights update (MWU) [41] over probability distribution. ", "page_idx": 1}, {"type": "text", "text": "Turning to the non-convex world, specific analysis for Problem (1) is rare. In general, finding an approximate global solution suffers from the curse of dimensionality [51, 46]. And one interesting direction is to consider suitable relaxations for the desired solutions, such as an approximate local stationary point of smooth functions [31, 19]. However, for many cases, local solutions may not be sufficient. Moreover, the algorithms often converge much faster in practice than the theoretic lower bounds in non-convex optimization suggest. This observed discrepancy can be attributed to the fairly weak assumptions underpinning these generic bounds. For example, many generic non-convex optimization theories, e.g. Carmon et al. [7, 8] only focus on the consideration of Lipschitz continuity of the gradient and some higher-order derivatives. In practice, the objective is often more \u201cstructured\". For example, the recent progress in neural networks shows that systems of neural networks approximate convex kernel systems when the model is overparameterized [28]. As pointed out by Hinder et al. [24], much more research is needed to characterize structured sets of functions for which minimizers can be efficiently found; It was also noted by Yurii Nesterov [47] that lots of functions are essentially convex; Our work follows this research line. ", "page_idx": 1}, {"type": "text", "text": "We propose generalized quasar-convexity (GQC) for the class of \u201cstructure\". The original quasarconvex functions [22] is parameterized by a constant $\\gamma\\,\\in\\,(0,1]$ and requires $f(\\pmb{x})^{-}-f(\\bar{\\pmb{x}}^{*})\\leq$ $\\begin{array}{r}{\\frac{1}{\\gamma}\\langle\\nabla f(\\pmb{x}),\\pmb{x}-\\pmb{x}^{*}\\rangle}\\end{array}$ . These functions are unimodal on all lines that pass through a global minimizer and so all critical points are minimizers. We extend quasar-convexity by introducing individual quasar-convex parameter $\\gamma_{i}$ for each distribution $\\pmb{x}_{i}$ . Therefore GQC is parameterized by $d$ constants $\\{\\gamma_{i}\\}_{i=1}^{d}$ and implies quasar-convexity in the case $d=1$ . The main intuition of the generalization is the observation that $d/\\operatorname*{min}_{i\\in[1:d]}\\gamma_{i}$ often depends on the number of distributions $d$ in real problems, whereas, $\\textstyle\\sum_{i=1}^{d}1/\\gamma_{i}$ may notTha istosay the hardnsfordisributon $i$ divergesaccording to the magnitude of $\\gamma_{i}$ . The larger of $\\gamma_{i}$ implies more convexity and the simpler to solve $\\pmb{x}_{i}$ . In general, one always have $\\begin{array}{r}{\\sum_{i=1}^{d}1/\\gamma_{i}\\,\\le\\,d\\operatorname*{max}_{i\\in[1:d]}1/\\gamma_{i}}\\end{array}$ . In the worst case, $\\textstyle\\sum_{i=1}^{d}1/\\gamma_{i}$ can be $d$ times smaller than $d\\operatorname*{max}_{i\\in[1:d]}1/\\gamma_{i}$ (see discussions in Section 3.3), which motivates us to study the GQC condition. ", "page_idx": 1}, {"type": "text", "text": "We then study designing efcient algorithms to solve (1). One simple case is when $\\{\\gamma_{i}\\}_{i=1}^{m}$ is pre-known by the algorithms. The possible direction is to impose a $\\gamma_{i}$ -dependent update rule, such as by non-uniform sampling. However, in general cases, $\\{\\gamma_{i}\\}_{i=1}^{m^{-}}$ is not known and determining $\\{\\gamma_{i}\\}_{i=1}^{m}$ require non-negligible costs. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we consider a generalized oracle, which we refer to as the internal function. Here the standard gradient oracle can be viewed as a special case of the internal function. We provide the optimistic mirror descent algorithm for multiple distributions, which makes sure that each probability distribution is updated according to its own internal function. We first establish $\\begin{array}{r}{\\mathcal{O}((d\\gamma_{\\operatorname*{max}})^{1/2}(\\sum_{i=1}^{d}\\gamma_{i}^{-1})^{3/2}L\\varepsilon^{-1}\\log(N))}\\end{array}$ $N\\,=\\,\\operatorname*{max}_{i\\in[1:d]}\\,n_{i}$ $\\gamma_{\\mathrm{max}}\\,=$ $\\operatorname*{max}_{i\\in[1:d]}\\gamma_{i}$ $\\gamma_{\\mathrm{max}}<\\infty$ $d\\gamma_{\\mathrm{max}}$ size rely on pre-known $\\gamma_{\\operatorname*{max}}\\sum_{i=1}^{d}\\gamma_{i}^{-1}$ We then consider $f$ satisfies \u201cpolynomial-like\u201d structural (see Assumption 3.3). We show the assumption can be achieved in a variety of function classes and important machine learning problems. Under the assumption, we show the algorithm can adapt to the values of $\\{\\gamma_{i}\\}_{i=1}^{m}$ and guarantees an reduced iteration complexity $\\mathcal{O}((\\sum_{i=1}^{d}1/\\gamma_{i})\\varepsilon^{-1}\\log(N)\\log^{4.5}(\\varepsilon^{-1}))$ In the following, the $\\widetilde O(\\cdot)$ notation hides factors that are polynomial in $\\log(\\varepsilon^{-1})$ and $\\log(N)$ ", "page_idx": 1}, {"type": "text", "text": "We also extend our framework to the minimax optimization ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{x}\\in\\mathcal{X}}\\operatorname*{max}_{\\pmb{y}\\in\\mathcal{Y}}f(\\pmb{x},\\pmb{y}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "table", "img_path": "lOV9kSX3Uo/tmp/3db1a42f630dc72e18f82dd6f5c2ce83c49f06670daf8923774be1b456df7e8c.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of policy optimization methods for finding an $\\varepsilon$ -approximateNEof infinite horizon two-player zero-sum Markov games in terms of the max-min gap (see Eq. (4)). Since the iteration complexity of several research works (such as Zhao et al. [75], Alacaoglu et al. [3] and Zeng et al. [72]) involve concentrability coefficient and initial distribution mismatch coefficient, we will not delve into them here. "], "page_idx": 2}, {"type": "text", "text": "where both $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ are composed of $d$ probability distributions, and $\\mathcal{Z}=\\mathcal{X}\\times\\mathcal{Y}$ is a joint region. In the general non-convex and non-concave setting, it is known that finding even an approximated local solution for (2) is computationally intractable [16]. We introduce the generalized quasar-convexityconcavity (GQCC) condition analogous to GQC and demonstrate the feasibility of obtaining an $\\varepsilon$ -approximate Nash equilibrium with $\\begin{array}{r}{\\mathcal{O}((1-\\theta)^{-2.5}\\operatorname*{max}_{z\\in\\mathcal{Z}}(\\sum_{i=1}^{d}\\psi_{i}(z))\\varepsilon^{-1}\\log(M)\\log(\\varepsilon^{-1}))}\\end{array}$ iteration complexities, where $\\begin{array}{r}{\\operatorname*{max}_{\\boldsymbol{z}\\in\\mathcal{Z}}(\\sum_{i=1}^{d}\\psi_{i}(\\boldsymbol{z}))}\\end{array}$ is analogous to $(\\sum_{i=1}^{d}1/\\gamma_{i})$ with $\\psi_{i}(z)$ defined in the GQCC condition; $\\theta$ is the discount parameter; $M=\\operatorname*{max}_{i\\in[1:d]}\\{\\bar{m}_{i}+n_{i}\\}$ Intuitively, the GQCC condition can be viewed as the generalization of convexity-concavity condition. Similarly, the $\\widetilde O(\\cdot)$ notation hides factors that are polynomial in $\\log(\\varepsilon^{-1})$ and $\\log(M)$ ", "page_idx": 2}, {"type": "text", "text": "Finally, we demonstrate the applications of our framework. For problem (1), we consider both infinite horizon discounted and finite horizon MDPs problem. For problem (2), we study the infinite horizon two-player zero-sum Markov games. We prove the learning objectives admit the GQC and GQCC conditions, respectively. This provides new landscape description for RL problems, thereby bringing new insights. Accordingly, our algorithms achieve state-of-the-art iteration complexities up to logarithmic factors. We provide $\\widetilde{\\mathcal{O}}(\\varepsilon^{-1})$ iteration bound for finding an $\\varepsilon$ -approximate Nash equilibrium of infinite horizon two-player zero-sum Markov games, which outperforms the $\\widetilde{\\mathcal{O}}(|S|^{3}\\varepsilon^{-2})$ bound of Wei et al. [67] and the $\\widetilde{\\mathcal{O}}(|S|\\varepsilon^{-1})$ bound of Cen et al. [10] by factors of $|\\boldsymbol{S}|^{3}\\varepsilon^{-1}$ and $|{\\mathcal{S}}|$ , respectively, up to a logarithmic factor. ", "page_idx": 2}, {"type": "text", "text": "1.1 Contribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "(A) We introduce new structural conditions GQC for minimization problems and GQCC for minimax problems over multiple distributions.   \n(B) We provide adaptive algorithm that achieves $\\widetilde{\\mathcal{O}}((\\sum_{i=1}^{d}1/{\\gamma_{i}})\\varepsilon^{-1})$ iteration complexitiesto find an $\\varepsilon$ -suboptimal global minimum of \u201cpolynomial-like\u201d function under GQC. We also provide an implementable minimax algorithm, given a generalized quasar-convex-concave founctin with proper conionsuses $\\bar{\\tilde{\\mathcal{O}}}((1-\\theta)^{-2.5}\\operatorname*{max}_{z\\in\\mathcal{Z}}({\\sum_{i=1}^{d}}^{\\prime}\\psi_{i}(z))\\,\\varepsilon^{-1})$ iterations to find an -approximate Nash equilibrium.   \n(C) We show that discounted MDP and infinite horizon two-player zero-sum Markov games admit the GQC and GQCC conditions, respectively, and also satisfy our mild assumptions. In addition, we provide $\\widetilde{\\mathcal{O}}((1-\\theta)^{-2.5}\\varepsilon^{-1})$ iteration bound for finding an $\\varepsilon$ -approximate Nash equilibrium of infinite horizon two-player zero-sum Markov games. Detailed comparisons between our method and prior arts are provided in Table 1. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Minimization: Convexity condition has been studied at length and plays a critical role in optimizing minimization problems [59, 44, 25, 60, 6, 49]. Several other \u201cconvexity-like\u201d conditions have attracted considerable attention, which provide opportunity for designing algorithmic framework to achieve global convergence. Star-convexity [47] is a typical example that relaxes convexity, showing potential in machine learning recently [32, 76]. Quasi-convexity, which admits that the highest point along any line segment is one of the endpoints, is also an important condition [6]. Following this, the concept of weak quasi-convexity is proposed by Hardt et al. [22] which is an extension of star-convexity in the differentiable case, and Hinder et al. [24] provides lower bound for the number of gradient evaluations to find an $\\varepsilon$ -minimizer of a quasar-convex function (a linguistically clearer redefinition of weak quasi-convex function claimed by Hinder et al. [24] ). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Minimax Optimization: Minimax problem attracted considerable attention in machine learning. There exist a variety of algorithms to find the approximate Nash equilibrium points [63, 43, 48, 45, 40, 33, 55, 66, 27] or stationary points [71] for convex-concave functions. Without convexconcave assumption, there exist related work considered specific structures in objective, including nonconvex-(strongly-)concave assumption [39, 73, 50], Kurdyka-Lojasiewicz condition (or specific PL condition) [68, 11, 69, 38], interaction dominant condition [21] and negative comonotonicity [17,36]. ", "page_idx": 3}, {"type": "text", "text": "RL Landscape Descriptions: For the policy gradient based model of infinite horizon reinforcement learning problems, Agarwal et al. [2] provides a convergence proof for the natural policy gradient descent, which is the same as the mirror descent-modified policy iteration algorithm [20] with negative entropy as the Bregman divergence. Subsequently, Lan [35] focuses on exploring the structural properties of infinite horizon reinforcement learning problems with convex regularizers. For two-player zero-sum Markov games [61, 42] under full information setting, there are various algorithms [26, 54, 64, 18, 42, 67, 9, 74, 70] have been proposed. Specifically, Cen et al. [9] focus on finding approximate minimax soft $Q$ -function in regularized infinite horizon setting; Zhao et al. [74] focus on finding one-sided approximate Nash equilibrium in standard infinite horizon setting with $\\tilde{\\mathcal{O}}(\\varepsilon^{-1})$ iteration bound which depends on the concentrability coefficient; Yang and Ma [70] focus on finding approximate Nash equilibrium in standard finite horizon setting with $\\bar{\\tilde{O}}(\\varepsilon^{-1})$ iteration bound. ", "page_idx": 3}, {"type": "text", "text": "Related Works on Optimistic Mirror Descent (OMD) and Optimistic Multiplicative Weights Update (OMWU): The connection between online learning and game theory [58, 4, 23, 1] has since led to the discovery of broad learning algorithms such as multiplicative weights update (MWU) [41]. Rakhlin and Sridharan [57] introduces an optimistic variant of online mirror descent [56, 14]- optimistic mirror descent. Daskalakis et al. [15] shows that the external regret of each player achieves near-optimal growth in multi-player general-sum games, with all players employ the optimistic multiplicative weights update. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notation: Let $\\pmb{x}\\;=\\;(\\pmb{x}_{1},\\cdot\\cdot\\cdot\\cdot\\,,\\pmb{x}_{d})\\;\\in\\;\\mathbb{R}^{\\sum_{i=1}^{d}n_{i}}$ be the joint vector variable, for every vector variable $\\pmb{x}_{i}\\in\\mathbb{R}^{n_{i}}$ . Let $\\pmb{\\alpha}=(\\pmb{\\alpha}(1),\\cdot\\cdot\\cdot\\,,\\pmb{\\alpha}(n))$ be the multi-indices, where $\\alpha(i)\\in\\mathbb{Z}_{+}$ , we define $\\textstyle|\\alpha|\\,=\\,\\sum_{i=1}^{n}\\alpha(i)$ and $\\pmb{\\alpha}!=\\pmb{\\alpha}(1)!\\cdot\\cdot\\cdot\\pmb{\\alpha}(n)!$ . For any vector ${\\pmb u}\\,=\\,({\\pmb u}(1),\\cdot\\cdot\\cdot\\cdot\\,,{\\pmb u}(n))\\,\\in\\,\\mathbb{R}^{n}$ we define $\\pmb{u}^{\\alpha}=\\pmb{u}(1)^{\\alpha(1)}\\cdot\\cdot\\cdot\\pmb{u}(n)^{\\alpha(n)}$ . Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a smooth function, we expand its Taylor expansion with Lagrange remainder $R_{K,w}^{f}(\\pmb{u})$ as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{K,w}^{f}(\\pmb{u})=f(\\pmb{u})-\\sum_{i=0}^{K}\\sum_{|\\alpha|=i}\\frac{D^{\\alpha}f(\\pmb{w})}{\\alpha!}\\cdot(\\pmb{u}-\\pmb{w})^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given matrices $\\mathbf{Q}$ and $\\mathbf{P}$ in $\\mathbb{R}^{\\ell_{1}\\times\\ell_{2}}$ we claim that $\\mathbf Q\\leq\\mathbf P$ \u00fc $[\\mathbf{Q}]_{i,j}\\,-\\,[\\mathbf{P}]_{i,j}\\,\\leq\\,0$ for every $i,j$ For a sequence of vector-valued functions $\\{\\pmb{F}_{i}\\}_{i=1}^{d}$ , we say that $\\{{\\pmb F}_{i}\\}_{i=1}^{d}$ is uniformly $L$ Lipschitz continuous with respect to $\\lVert\\cdot\\rVert,$ under $\\lVert\\cdot\\rVert$ if $\\|\\pmb{F}_{i}\\left(\\pmb{x}_{i}\\right)-\\pmb{F}_{i}\\left(\\pmb{u}_{i}\\right)\\|,\\ \\leq\\dot{L}\\|\\pmb{x}_{i}-\\pmb{u}_{i}\\|$ for every $i\\in[1:d]$ and any $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{u}}\\in\\mathcal{X}$ .We denote by $\\|\\cdot\\|_{*}$ the dual norm of $\\|\\cdot\\|$ . Let $\\mathbf{P}:\\mathbb{R}^{\\ell_{1}\\times\\ell_{2}}\\rightarrow\\mathbb{R}^{n_{1}\\times n_{2}}$ be a matrix function, we say that $\\mathbf{P}$ is a $\\theta$ -contraction mapping under $\\left\\|\\cdot\\right\\|$ if $\\|\\mathbf{P}(\\mathbf{Q}_{1})-\\mathbf{P}(\\mathbf{Q}_{2})\\|_{\\infty}\\leq\\theta\\|\\mathbf{Q}_{1}-\\mathbf{Q}_{2}\\|$ for any $\\mathbf{Q}_{1},\\mathbf{Q}_{2}\\in\\mathbb{R}^{\\ell_{1}\\times\\ell_{2}}$ . For matrix-valued function $\\mathbf{P}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{\\ell_{1}\\times\\ell_{2}}$ ,we define $\\mathbf{D_{P}}(x,x^{\\prime})=$ ${\\bf P}({\\pmb x})\\mathrm{~-~}{\\bf P}({\\pmb x}^{\\prime})$ for any $\\mathbf{\\Delta}\\mathbf{x},\\mathbf{x}^{\\prime}~\\in~\\mathbb{R}^{n}$ The $\\mathrm{KL}$ diverence $\\begin{array}{r}{\\mathrm{KL}(p||q)\\,=\\,\\sum_{j=1}^{n}p(j)\\cdot\\log\\left(\\frac{p(j)}{q(j)}\\right)}\\end{array}$ between distributions $\\pmb{p}$ and $\\pmb q$ is defined on probability simplex $\\Delta_{n}$ . And the variance of $\\textbf{\\em x}$ over $\\textbf{\\emph{p}}$ is defined by $\\begin{array}{r}{\\mathrm{Var}_{p}(\\pmb{x})=\\sum_{j=1}^{n}p(j)\\cdot\\left(\\pmb{x}(j)-\\mathbb{E}_{j^{\\prime}\\sim p}[\\pmb{x}(j^{\\prime})]\\right)^{2}}\\end{array}$ . We define max-min gap of function ", "page_idx": 3}, {"type": "text", "text": "$f:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{G}_{f}(\\pmb{x},\\pmb{y}):=\\operatorname*{max}_{\\pmb{y}^{\\prime}\\in\\mathcal{Y}}f(\\pmb{x},\\pmb{y}^{\\prime})-\\operatorname*{min}_{\\pmb{x}^{\\prime}\\in\\mathcal{X}}f(\\pmb{x}^{\\prime},\\pmb{y}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We claim that $\\left({\\pmb x},{\\pmb y}\\right)$ is an $\\varepsilon$ -approximate Nash equilibrium ( $\\dot{\\varepsilon}$ -approximate NE) if $\\mathcal{G}_{f}(\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}})\\,\\leq\\,\\varepsilon$ When $\\varepsilon=0$ \uff0c $\\left({\\pmb x},{\\pmb y}\\right)$ is a Nash equilibrium. ", "page_idx": 4}, {"type": "text", "text": "Infinite Horizon Discounted Markov Decision Process: We consider the setting of an infinite horizon discounted Markov decision process (MDP), denoted by $\\mathcal{M}:=(\\mathcal{S},\\mathcal{A},\\mathbb{P},\\bar{\\sigma},\\boldsymbol{\\theta},\\pmb{\\rho}_{0})$ $\\boldsymbol{S}$ is a finite state space; $\\boldsymbol{\\mathcal{A}}$ is a finite action space; $\\mathbb{P}(s|s^{\\prime},a^{\\prime})$ denotes the probability of transitioning from $s$ to $s^{\\prime}$ under playing action $a^{\\prime};\\,\\sigma:S\\times A\\rightarrow[0,1]$ is a cost function, which quantifies the cost associated with taking action $a$ in state $s$ $\\theta\\in[0,1)$ is a discount factor; $\\rho_{\\mathrm{0}}$ is an initial state distribution over $\\boldsymbol{S}$ ", "page_idx": 4}, {"type": "text", "text": "$\\pi:{\\mathcal{S}}\\rightarrow\\Delta_{{\\mathcal{A}}}$ (where $\\Delta_{\\mathcal{A}}$ is the probability simplex over $\\mathcal{A}$ ) denotes a stochastic policy, i.e., the agent play actions according to $a\\sim\\pi(\\cdot|s)$ . We use $\\mathbf{Pr}_{t}^{\\pi}(s^{\\prime}|s)=\\mathbf{Pr}^{\\pi}(s_{t}=s^{\\prime}|s_{0}\\stackrel{\\cdot}{=}s)$ to denote the probability of visiting the state $s^{\\prime}$ from the state $s$ after $t$ time steps according to policy $\\pi$ . Let trajectory $\\tau=\\{(s_{t},a_{t})\\}_{t=0}^{\\infty}$ , where $s_{0}\\sim\\rho_{\\mathrm{0}}$ , and, for all subsequent time steps $t$ $a_{t}\\sim\\pi(\\cdot|s_{t})$ and $s_{t+1}\\sim\\mathbb{P}(\\cdot|s_{t},a_{t})$ . The value function $V^{\\pi}:S\\rightarrow\\mathbb{R}$ is defined as the discounted sum of future cost starting at state $s$ and executing $\\pi$ , i.e. ", "page_idx": 4}, {"type": "equation", "text": "$$\nV^{\\pi}(s)=(1-\\theta)\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\theta^{t}\\sigma(s_{t},a_{t})\\right]\\pmb{\\pi},s_{0}=s\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, we define the action-value function $Q^{\\pi}\\,:\\,S\\,\\times\\,A\\,\\rightarrow\\,\\mathbb{R}$ and the advantage function $A^{\\pi}:S\\times A\\rightarrow\\mathbb{R}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ^{\\pi}(s,a)=(1-\\theta)\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\theta^{t}\\sigma(s_{t},a_{t})\\Bigg|\\,\\pi,s_{0}=s,a_{0}=a\\right],\\;\\;A^{\\pi}(s,a)=Q^{\\pi}(s,a)-V^{\\pi}(s).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It's also useful to define the discounted state visitation distribution $d_{s_{0}}^{\\pi}$ of a policy $\\pi$ as $d_{s_{0}}^{\\pi}(s)=$ $\\begin{array}{r}{\\left(1-\\theta\\right)\\sum_{t=0}^{\\infty}{\\theta^{t}}{\\bf P r}_{t}^{\\pi}(s|s_{0})}\\end{array}$ In order to simplify notation, we write $d_{\\rho_{0}}^{\\pi}(\\bar{s})=\\mathbb{E}_{s_{0}\\sim\\rho_{0}}[d_{s_{0}}^{\\pi}(s)]$ where $d_{\\rho_{\\mathrm{0}}}^{\\pi}$ is the discounted state visitation distribution under initialdistribution $\\rho_{\\mathrm{0}}$ ", "page_idx": 4}, {"type": "text", "text": "3  Minimization Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we propose the generalized quasar-convexity (GQC) condition, and analyze a related algorithmic framework for minimization over $\\begin{array}{r}{\\mathcal{X}=\\prod_{i=1}^{d}\\Delta_{n_{i}}}\\end{array}$ , under mild assumptions. ", "page_idx": 4}, {"type": "text", "text": "3.1  Generalized Quasar-Convexity (GQC) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We provide a novel depiction of function structure-generalized quasar-convexity, which is defined as follows: ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1 (Generalized Quasar-Convexity (GQC)). Let $\\pmb{x}^{*}\\in\\mathcal{X}\\subset\\mathbb{R}^{\\sum_{i=1}^{d}n_{i}}$ be a minimizer of the function $f:\\mathcal{X}\\to\\mathbb{R}$ .We say that $f$ is generalized quasar-convex on $\\mathcal{X}$ with respect to $x^{*}$ if for all $x\\in\\mathcal{X}$ , there exist a sequence of vector-valued functions $\\{\\pmb{F}_{i}:\\mathcal{X}\\rightarrow\\mathbb{R}^{n_{i}}\\}_{i=1}^{d}$ and a sequence of positive scalars $\\{\\gamma_{i}\\}_{i=1}^{d}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(\\pmb{x}^{*})\\geq f(\\pmb{x})+\\sum_{i=1}^{d}\\frac{1}{\\gamma_{i}}\\left\\langle F_{i}(\\pmb{x}),\\pmb{x}_{i}^{*}-\\pmb{x}_{i}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If Eq. (5) holds, we say that $\\pmb{F}=(\\pmb{F}_{1}^{\\top},\\cdots,\\pmb{F}_{d}^{\\top})^{\\top}$ is the internal function of $f$ Given $i\\in[1:d]$ we say that $\\pmb{F}_{i}$ is the internal function of $f$ for variable block $\\pmb{x}_{i}$ ", "page_idx": 4}, {"type": "text", "text": "Our proposed GQC condition concerns the multi-variable generalized extension of the quasarconvexity condition. In the case $d=1$ , the GQC condition degenerates into the $\\gamma$ -quasar-convexity condition as studied in Hinder et al. [24] with the gradient $\\nabla f({\\pmb x})$ belongs to the internal functions of $f$ . In the case $d>1$ , the GQC condition is instrumental in capturing the crucial characteristic of those optimization applications with each variable block has difficulty to be optimized. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Optimistic Mirror Descent for Multi-Distributions ", "page_idx": 5}, {"type": "text", "text": "Input: $\\left\\{\\pmb{g}_{i}^{0}=\\pmb{x}_{i}^{0}=(1/n_{i},\\cdot\\cdot\\cdot\\,,1/n_{i})\\right\\}_{i=1}^{d},$ $\\eta$ and $T$   \nOutput: Randomly pick up $t\\in\\{1,\\cdot\\cdot\\cdot\\,,T\\}$ following the probability $\\mathbb{P}[t]=1/T$ and return $\\pmb{x}^{t}$   \n1: while $t\\leq T$ do   \n2: for all $i\\in[1:d]$ do   \n3: $\\begin{array}{r l}&{\\pmb{x}_{i}^{t}=\\operatorname*{argmin}_{\\pmb{x}_{i}\\in\\Delta_{n_{i}}}^{\\pmb{\\mathrm{\\mu}}^{2}}\\eta\\left\\langle\\pmb{F}_{i}(\\pmb{x}^{t-1}),\\pmb{x}_{i}\\right\\rangle+\\mathrm{KL}\\left(\\pmb{x}_{i}\\mathrm{\\Delta}\\|\\pmb{g}_{i}^{t-1}\\right)\\,,}\\\\ &{\\pmb{g}_{i}^{t}=\\operatorname*{argmin}_{\\pmb{\\eta}}\\eta\\left\\langle\\pmb{F}_{i}(\\pmb{x}^{t}),\\pmb{g}_{i}\\right\\rangle+\\mathrm{KL}\\left(\\pmb{g}_{i}\\left\\|\\pmb{g}_{i}^{t-1}\\right)\\,.}\\end{array}$   \n4:   \n5: end for   \n6: $t\\gets t+1$   \n7: end while ", "page_idx": 5}, {"type": "text", "text": "3.2  Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recall that GQC condition provides a perspective to bound function error $f(\\mathbf{x})-f(\\mathbf{x}^{*})$ based on internal function, which is different from that based on gradient oracle. We therefore aim to provide an algorithmic framework for finding an approximate suboptimal global solution using internal function. Given an objective function $f:\\mathcal{X}\\to\\mathbb{R}$ with internal function $\\pmb{F}$ , our algorithm (Algorithm 1) independently computes points $\\pmb{g}_{i}^{t}$ and $\\pmb{x}_{i}^{t}$ following OMD over each block. If $\\operatorname*{max}_{i\\in[1:d]}\\gamma_{i}<\\infty$ and internal function $\\pmb{F}$ has Lipschitz continuity, we have following basic and primary convergence result of Algorithm 1, ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. Assuming that $\\pmb{F}$ is $L$ -Lipschitz continuous with respect to $\\|\\cdot\\|_{*}$ under $\\Vert\\cdot\\Vert$ and $\\gamma_{\\mathrm{max}}=\\operatorname*{max}_{i\\in[1:d]}\\gamma_{i}<\\infty,$ and setting $\\eta=(L^{2}d\\gamma_{\\operatorname*{max}}\\textstyle\\sum_{i=1}^{d}\\gamma_{i}^{-1})^{-1/2}/2$ we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}(f(\\pmb{x}^{t})-f(\\pmb{x}^{*}))\\leq\\frac{2L\\operatorname*{max}_{i\\in[1:d]}\\log(n_{i})\\left(d\\gamma_{\\operatorname*{max}}\\right)^{1/2}\\left(\\sum_{i=1}^{d}\\gamma_{i}^{-1}\\right)^{3/2}}{T}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "However, the estimation provided by Theorem 3.2 depends on $d\\gamma_{\\mathrm{max}}$ . And the step size relying on $\\begin{array}{r l}{\\gamma_{\\operatorname*{max}}\\left(\\sum_{i=1}^{d}\\gamma_{i}^{-1}\\right)}\\end{array}$ might be difficult to set when $\\{\\gamma_{i}\\}_{i=1}^{d}$ is unknown. ", "page_idx": 5}, {"type": "text", "text": "We then hope to propose an alternative analytical method that can adapt to unknown $\\{\\gamma_{i}\\}_{i=1}^{d}$ and obtain complexity which does not depends on block dimension $d$ explicitly. The challenges includes: 1) The algorithm does not know the weight $1/\\gamma_{i}$ ; 2) every $\\pmb{F}_{i}$ has dependence on the joint variable $\\textbf{\\em x}$ instead of depending on $\\pmb{x}_{i}$ . Before we present the details of convergence analysis, we need the following notations and assumptions: ", "page_idx": 5}, {"type": "text", "text": "Denote $\\begin{array}{r}{P_{K,\\pmb{y}}^{f}(\\pmb{x}))\\,=\\,\\sum_{i=0}^{K}\\sum_{|\\pmb{\\alpha}|=i}\\frac{|D^{\\alpha}f(\\pmb{y})|}{\\alpha!}\\cdot(|\\pmb{x}|+|\\pmb{y}|)^{\\alpha}}\\end{array}$ and let ${\\cal P}_{K,y}^{\\phi}(x)\\,=\\,({\\cal P}_{K,y}^{\\phi(1)}(x),\\cdot\\cdot\\cdot\\,,$ $P_{K,\\pmb{y}}^{\\phi(\\ell)}(\\pmb{x}))$ foranyvectr-vald fnton $\\phi:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{\\ell}$ Recaling thedeinton of $R_{K,w}^{f}$ in Eq (3), $R_{K,y}^{\\phi}({\\pmb x})=(R_{K,{\\pmb y}}^{\\phi(1)}({\\pmb x}),\\cdot\\cdot\\cdot\\,,R_{K,{\\pmb y}}^{\\phi(\\ell)}({\\pmb x})).$ ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.3. Let $\\pmb{F}$ be the internal function of $f$ .There exists $\\Theta_{1},\\Theta_{2}\\,>\\,0$ \uff0c $K_{0}\\,\\in\\,\\mathbb{Z}_{+}$ , and $\\theta\\in[0,1)$ , and a fixed $\\pmb{y}\\in\\mathbb{R}^{\\sum_{i=1}^{d}n_{i}}$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\mathbf{A}_{1}]~\\Big\\|R_{K,y}^{F}(x)\\Big\\|_{\\infty}\\leq\\Theta_{1}\\theta^{K}\\;\\mathrm{for~any~integer}\\;K>K_{0}\\;\\mathrm{and}\\;x\\in\\mathcal{X}.}\\\\ &{[\\mathbf{A}_{2}]\\;\\Big\\|P_{K,y}^{F}(x)\\Big\\|_{\\infty}\\leq\\Theta_{2}\\;\\;\\mathrm{for~any~integer}\\;K\\in\\mathbb{Z}_{+}\\;\\mathrm{and}\\;x\\in\\mathcal{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assumption 3.3 is a characterization of \u201cpolynomial-like\u2019 functions. We clarify this view as follows. For a standard polynomial function $p$ , it's clear that $p$ satisfies Assumption 3.3, since the Taylor expansion of $p$ after order $K_{0}$ is always equal to 0 ( $\\left[\\mathbf{A_{1}}\\right]$ in Assumption 3.3 holds) and $\\mathcal{X}$ is a bounded and closed set $\\left(\\left[\\mathbf{A}_{2}\\right]\\right.$ in Assumption 3.3 holds). Assumption 3.3 is easy to achieve. Shown in Proposition B.2 and Remark B.3 in Appendix B, Assumption 3.3 can be satisfied by many smooth functions defined on bounded region $\\mathcal{X}$ . In addition, we introduce a simple machine learning example: learning one single neuron network over a simplex in the realizable setting. ", "page_idx": 5}, {"type": "text", "text": "Example 3.4. The objective function is written as $\\begin{array}{r}{f(\\mathbf{\\boldsymbol{p}},\\mathbf{P})=\\frac{1}{2}\\mathbb{E}_{\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}}(\\sum_{i=1}^{m}p_{i}\\sigma(\\mathbf{\\boldsymbol{x}}^{\\top}\\mathbf{P}_{i})-y)^{2}}\\end{array}$ , where $\\pmb{p}\\in\\Delta_{m}$ and $\\mathbf{P}\\,=\\,\\left(\\mathbf{P}_{1},\\cdot\\cdot\\cdot\\,,\\mathbf{P}_{m}\\right)\\,\\in\\,\\prod_{i=1}^{m}\\Delta_{d}$ and the target $y$ given $\\pmb{x}\\in[-C,C]^{d}$ admits $y=$ $\\sigma({\\pmb x}^{\\top}{\\bf P}_{1}^{*})$ for some $\\mathbf{P}_{1}^{*}\\in\\Delta_{d}$ . For activation function $\\sigma(x)=\\exp\\{x\\}$ \uff0c $f$ satisfies GQC condition and Assumption 3.3 with the internal functions $\\begin{array}{r}{F_{p}=\\{\\mathbb{E}[(\\sum_{j=1}^{m}p_{j}\\sigma({\\boldsymbol x}^{\\top}{\\mathbf P}_{j})-{\\boldsymbol y})\\sigma({\\boldsymbol x}^{\\top}{\\mathbf P}_{i})]\\}_{i=1}^{m}}\\end{array}$ for block $\\textbf{\\emph{p}}$ and $F_{\\mathbf{P}_{i}}=\\mathbb{E}[(\\sigma(\\mathbf{x}^{\\top}\\mathbf{P}_{i})-y)\\mathbf{x}]$ for block $\\mathbf{P}_{i}$ ", "page_idx": 6}, {"type": "text", "text": "Note previous work [65] studies single neuron learning by considering $\\mathbf{P}_{1}^{*}$ in the sphere and assuming $\\textbf{\\em x}$ follows from a Gaussian distribution. To our knowledge, there is no evidence shows that objective function of Example 3.4 has quasar-convexity. This example demonstrates the advantage of studying the GQC framework over the previous approach. The proof of Example 3.4 is in Section B.2. ", "page_idx": 6}, {"type": "text", "text": "Parameter Setting Before stating the convergence result, we set the parameters as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{I}=\\Theta_{1}+\\Theta_{2}+1,\\quad H=\\lceil\\log(T)\\rceil,\\quad\\beta_{0}=(4H)^{-1},\\quad\\beta=\\operatorname*{min}\\left\\{\\frac{\\sqrt{\\beta_{0}/8}}{H^{3}},\\frac{1}{2\\Theta(H+3)}\\right\\},}\\\\ &{\\Upsilon=e^{2}+\\mathcal{O}(\\Theta_{2}),\\quad\\hat{K}=\\operatorname*{max}\\left\\{\\frac{H\\log(4\\beta^{-1})+\\log(\\Theta_{1})}{\\log(\\theta^{-1})},K_{0}\\right\\},\\quad\\eta=\\operatorname*{min}\\left\\{\\frac{\\beta}{6e^{3}\\hat{K}\\Gamma\\operatorname*{max}\\{\\Theta,1\\}},\\frac{\\beta_{0}^{4}}{\\mathcal{O}(\\Theta)}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem3.5. Let $f$ satisfiestheGQCconditionanddenote $N=\\operatorname*{max}_{i\\in[1:d]}\\left\\{n_{i}\\right\\}$ UnderAssumption 3.3, the following estimation holds for Algorithm $^{\\,I}$ 's output $\\{\\mathbf{x}^{t}\\}_{t=1}^{T}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}(f(\\pmb{x}^{t})-f(\\pmb{x}^{*}))\\le\\left(\\sum_{i=1}^{d}1/\\gamma_{i}\\right)\\left[\\frac{1}{\\eta}\\log(N)+\\eta\\Theta^{3}(6+330240\\Theta H^{5})\\right]T^{-1},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 3.5 implies that for any generalized quasar-convex function $f$ satisfies Assumption 3.3, the $T$ -step random solution outputted by Algorithm 1 is a $\\begin{array}{r}{\\mathcal{O}((\\sum_{i=1}^{d}1/\\gamma_{i})T^{-1}\\log(N)\\log^{4.5}(T)).}\\end{array}$ suboptimal solution. Ignoring the logarithmic factor, the iteration complexity of our algorithm is competitive to the state-of-the-art algorithm when applied to specific application (i.e. policy optimization of reinforcement learning [2]). Moreover, our algorithm makes iteration complexity depend on $\\textstyle\\sum_{i=1}^{d}1/\\gamma_{i}$ linearlyIecmmnaplia $\\textstyle\\sum_{i=1}^{d}1/\\gamma_{i}$ has no dependence on $d$ which is the number of variable blocks (see discussions in Section 3.3). ", "page_idx": 6}, {"type": "text", "text": "3.3 Application to Reinforcement Learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section reveals that GQC condition provides a novel analytical approach to reinforcement learning. We show how to leverage Algorithm 1 to find $\\varepsilon$ -suboptimal global solution for infinite horizon reinforcement learning problem. And in Appendix B.3.2, we show how to leverage Algorithm 1 to minimize finite horizon reinforcement learning problem. ", "page_idx": 6}, {"type": "text", "text": "The infinite horizon reinforcement learning is formulated as the following policy optimization problem: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi\\in\\mathcal{X}}J^{\\pi}(\\rho_{0}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $J^{\\pi}(\\pmb{\\rho}_{0})=\\mathbb{E}_{s_{0}\\sim\\pmb{\\rho}_{0}}[V^{\\pi}(s_{0})]$ and $\\begin{array}{r}{\\mathcal{X}=\\prod_{i=1}^{|S|}\\Delta_{\\mathcal{A}}}\\end{array}$ denotes $|{\\cal S}|$ probability simplexes. We write $\\boldsymbol{S}=\\{s_{i}\\}_{i=1}^{|\\mathcal{S}|}$ and denote the action-valuevectoronstate $s_{i}$ $Q^{\\pi}(s_{i},\\cdot)$ . The next Proposition 3.6 states that $\\bar{J^{\\pi}}(\\rho_{0})$ satisfies theGQC condition for anyinitial state distribution ${\\boldsymbol{\\rho}}_{\\mathrm{0}}$ ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.6. Let $\\{\\pi^{*}(\\cdot|s)\\in\\Delta_{A}\\}_{s\\in\\cal S}$ denote the optimal global solution of problem (9). We have that $J^{\\pi}(\\rho_{0})$ satisfies the GQC condition in $E q$ (5) with internal function $\\pmb{F}_{i}(\\pmb{\\pi})=\\pmb{Q}^{\\pi}(s_{i},\\cdot)$ for variable block $\\pi_{i}$ and $\\pmb{F}$ satisfies Assumption3.3with $\\Theta_{1}=\\theta,\\Theta_{2}=1$ and $K_{0}=1$ ", "page_idx": 6}, {"type": "text", "text": "According to Theorem 3.5, if we apply Algorithm 1 to the infinite horizon reinforcement learning basing action-valuevector $\\mathbf{Q}^{\\pi}$ with parameter selection Eq. (7), which is actually a simple variant of natural policy gradient descent [2], then the iteations $T$ we need to find an $\\varepsilon$ -suboptimal global solution is upper-bounded by $\\mathcal{O}(\\operatorname*{max}\\{1,\\log^{-1}(\\theta^{-1})\\}(1-\\theta)^{-1}\\varepsilon^{-1}\\log^{4.5}(\\varepsilon^{-1})\\log(|\\mathcal{A}|))$ under Agarwal et al. [2]'s setting. Therefore, the iteration complexity of Algorithm 1 does not depend on the size of states, since the summation of $d_{\\rho_{\\mathrm{0}}}^{\\pi^{*}}$ over $\\begin{array}{r}{S\\left(\\sum_{i=1}^{|\\bar{S}|}1/\\gamma_{i}=\\bar{\\sum_{i=1}^{|S|}}\\,d_{\\rho_{0}}^{\\pi^{*}}(s_{i})=1\\right)}\\end{array}$ mollifies the accumulation of the maximum of $d_{\\rho_{\\mathrm{0}}}^{\\pi^{*}}$ over $\\boldsymbol{S}$ with $|{\\mathcal{S}}|$ times. Specially,if we take into account the losest upper bound $|S|\\operatorname*{max}_{i\\in[1:|S|]}{d_{\\rho_{0}}^{\\pi^{*}}(s_{i})}$ , then the iteration complexity of algorithm may suffer from the linear dependence on $|{\\cal S}|$ , since $\\begin{array}{r}{\\operatorname*{max}_{i\\in[1:|S|]}d_{\\rho_{0}}^{\\pi^{*}}(s_{i})\\;\\geq\\;(1\\,-\\,\\theta)\\operatorname*{max}_{i\\in[1:|S|]}\\rho_{0}(s_{i})}\\end{array}$ Previous research [2, Theorem 5.3] has demonstrated that utilizing the information of joint variables to separately update each variable block ensures global convergence for problem (9) with $\\mathcal{O}((1-\\theta)^{-2}\\varepsilon^{-1})$ iteration complexity. However, their analytical approach is carefully designed for infinite horizon reinforcement learning problems. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4  Minimax Optimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we introduce the generalized quasar-convexity-concavity (GQCC) condition, which can be verified in real applications such as two-player zero-sum Markov games. We provide a related algorithm for minimax optimization (minimizing $\\mathcal{G}_{f}(x,y)$ has been defined in Eq. (4)) over $\\begin{array}{r}{\\mathcal{Z}=\\prod_{i=1}^{d}\\mathcal{Z}_{i}=\\prod_{i=1}^{d}\\left(\\Delta_{n_{i}}\\times\\Delta_{m_{i}}\\right)}\\end{array}$ ,under proer assuonsWe efy t divegenerating function $v$ as $\\bar{v(\\pmb{x})}=\\mathbb{E}_{i\\sim{\\pmb{x}(\\cdot)}}[\\log({\\pmb{x}(i)})]$ in probability simplexes setting. We also provide a framework for minimax problem over the general compact convex regions in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "4.1  Generalized Quasar-Convexity-Concavity (GQCC) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We provide a new notion called generalized quasar-convexity-concavity for nonconvex-nonconcave minimax optimization, which is defined as follows: ", "page_idx": 7}, {"type": "text", "text": "Definition 4.1 (Generalized Quasar-Convexity-Concavity (GQCC)). Denote $\\mathcal{Z}_{i}=\\mathcal{X}_{i}\\times\\mathcal{Y}_{i}$ for any $i\\in[1:d]$ , and let $f:\\mathcal{Z}\\to\\mathbb{R}$ be the objective function. We say that $f$ is generalized quasar-convexconcave on $\\mathcal{Z}$ if for all $z=(x,y)\\in{\\mathcal{Z}}$ , there exist a sequence of functions $\\left\\{f_{i}:\\bar{\\mathbb{R}}^{\\ell\\times d}\\times\\mathcal{Z}_{i}\\right.\\rightarrow$ $\\mathbb{R}\\}_{i=1}^{d}$ , a sequence of non-negative functions $\\{\\psi_{i}:\\mathcal{Z}\\to\\mathbb{R}_{+}\\cup0\\}_{i=1}^{d}$ and a matrix-valued function $\\mathbf{P}=(\\mathbf{P}_{1},\\cdots,\\mathbf{P}_{d}):\\mathcal{Z}\\to\\mathbb{R}^{\\ell\\times d}$ where every $\\mathbf{P}_{i}$ is a $\\ell$ -dimensional vector-valued function, such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{G}_{f}(\\mathbf{x},\\mathbf{y})\\leq\\sum_{i=1}^{d}\\psi_{i}(z)\\mathcal{G}_{f_{i}(\\mathbf{P}(z),\\cdot,\\cdot)}(x_{i},\\pmb{y}_{i}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where each $f_{i}(\\mathbf{Q},\\cdot,\\cdot)$ is convex-concave for a fixed $\\mathbf{Q}=(\\mathbf{Q}_{1},\\cdot\\cdot\\cdot\\mathbf{\\mu,Q}_{d})\\in\\mathbb{R}^{\\ell\\times d}$ We denote the internal operator of $f$ for variable block $\\boldsymbol{z}_{i}$ by $\\pmb{F}_{i}$ where $\\mathbf{\\boldsymbol{F}}_{i}(\\mathbf{\\boldsymbol{Q}},z_{i})=((\\nabla_{x_{i}}f_{i}(\\mathbf{\\boldsymbol{Q}},z_{i}))^{\\top},(-\\nabla_{y_{i}}f_{i}(\\mathbf{\\boldsymbol{Q}},$ $z_{i}))^{\\top})^{\\top}$ . Moreover, we say that $\\pmb{F}=(\\pmb{F}_{1}^{\\top},\\cdots,\\pmb{F}_{d}^{\\top})^{\\top}$ is the internal operator of $f$ ", "page_idx": 7}, {"type": "text", "text": "The GQCC condition is an extension of the GQC condition in minimax optimization setting. The specific connection between them can be found in Appendix C. The GQCC condition can be viewed as an extension of the convexity-concavity condition in multi-variable optimization; it seamlessly reduces to the convexity-concavity condition with $f_{1}(\\mathbf{P}(z),z)=f(z)$ and $\\psi_{1}(z)\\equiv1$ , in the case $d=1$ .Assuming every $\\psi_{i}$ is bounded, $f_{i}(\\mathbf{P}(z),z_{i})\\equiv f_{i}(\\mathbf{0},z_{i})$ with Lipschitz continuous gradient and is convex-concave with respect to $\\boldsymbol{z}_{i}$ , then finding the Nash equilibrium point of $f$ is reduced to finding the Nash equilibrium points of $d$ independent convex-concave minimax problems. However, how to find the approximate Nash equilibrium points in more general case has not been well-studied. Most of existing work for minimax optimization without convex-concave assumption are focused on finding the approximate stationary points. ", "page_idx": 7}, {"type": "text", "text": "4.2  Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For simplicity, we denote by $\\boldsymbol{F}_{i}^{x}$ and $\\boldsymbol{F}_{i}^{y}$ the projection of $\\pmb{F}_{i}$ in the $\\pmb{x}_{i}$ and $\\pmb{y}_{i}$ directions, respectively, i.e., $\\pmb{F}_{i}^{\\top}=\\left((\\pmb{F}_{i}^{\\pmb{x}})^{\\top},(\\pmb{F}_{i}^{\\pmb{y}})^{\\top}\\right)$ . Given an objective function $f:\\mathcal{Z}\\to\\mathbb{R}$ with internal operator $\\pmb{F}$ our algorithm (Algorithm 2) employs regularized OMD over each distribution independently basing on $\\pmb{F}_{i}$ and updates matrix $\\mathbf{Q}^{t}$ to track the behavior of function $\\mathbf{P}$ iteratively. It's worth noting that each iteration of Algorithm 2 provides explicit expressions for $\\pmb{x}_{i}^{t}$ and $\\pmb{g}_{i}^{t}$ (see the proof of Theorem 3.5 in Appendix B). Consequently, Algorithm 2 essentially operates as a single-loop algorithm. ", "page_idx": 7}, {"type": "text", "text": "Assumption 4.2. In Definition 4.1, we assume that matrix-valued function $\\mathbf{P}$ has the form of $\\mathbf{P}(\\mathbf{Q}^{z},z)$ where $\\mathbf{Q}^{z}\\,\\in\\,\\mathbb{R}^{\\ell\\times d}$ depends on $_{\\textit{z}}$ , and $\\mathbf{P}$ satisfies the following properties on region $\\left\\{\\mathbf{Q}\\in\\mathbb{R}^{\\ell\\times d}\\right\\vert\\ \\|\\mathbf{Q}\\|_{\\infty}\\leq C\\}\\times\\mathcal{Z}$ for some constant $C>0$ ", "page_idx": 7}, {"type": "text", "text": "Input: $\\begin{array}{r}{\\left\\{z_{i}^{0}\\right\\}_{i=1}^{d}=\\left\\{g_{i}^{0}\\right\\}_{i=1}^{d}=\\left\\{(1/n_{i},\\cdots,1/n_{i}),(1/m_{i},\\cdots,1/m_{i})\\right\\}_{i=1}^{d},\\left\\{\\alpha_{t}\\ge0\\right\\}_{t=1}^{T}\\mathbb{w}\\mathbb{i}\\hbar\\sum_{t=1}^{T}\\alpha_{t}=1,}\\end{array}$   \n$\\{\\gamma_{t}\\geq0\\}_{t=1}^{T}$ $\\{\\lambda_{t}\\geq0\\}_{t=1}^{T}$ $\\eta$ and $\\mathbf{Q}^{0}=\\mathbf{0}$ \uff0c   \nOutput: $\\begin{array}{r}{\\bar{z}_{T}=\\sum_{t=1}^{T}\\alpha_{t}\\pmb{z}^{t}}\\end{array}$   \n1: while $t\\leq T$ do   \n2: $\\begin{array}{r l}&{\\mathbf{\\Phi}_{t}^{\\varepsilon}=\\frac{\\mathbf{\\Phi}_{-}^{\\varepsilon}+\\mathbf{\\Phi}_{\\infty}^{\\varepsilon}}{\\varepsilon}}\\\\ &{\\mathbf{\\Phi}\\quad=(1-\\beta_{t-1})\\mathbf{Q}^{t-1}+\\beta_{t-1}\\mathbf{P}(\\mathbf{Q}^{t-1},z_{t-1}).}\\\\ &{\\mathbf{\\Phi}\\quad\\mathbf{a}\\mathbf{I}\\mathbf{I}\\mathbf{i}\\in[1:d]\\mathbf{\\Phi}\\mathbf{q}}\\\\ &{\\mathbf{\\Phi}\\quad\\mathbf{x}_{i}^{t}=\\arctan\\,\\,\\eta\\left\\langle F_{i}^{x}(\\mathbf{Q}^{t-1},z_{i}^{t-1}),\\mathbf{\\Phi}\\mathbf{x}_{i}\\right\\rangle+\\gamma_{t}\\mathrm{KL}\\left(\\mathbf{x}_{i}\\,\\big\\|(g_{i}^{x})^{t-1}\\right)+\\lambda_{t}v(\\mathbf{x}_{i}),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.y_{i}^{t}=\\arctan\\,\\,\\eta\\left\\langle F_{i}^{y}(\\mathbf{Q}^{t-1},z_{i}^{t-1}),y_{i}\\right\\rangle+\\gamma_{t}\\mathrm{KL}\\left(y_{i}\\,\\big\\|(g_{i}^{y})^{t-1}\\right)+\\lambda_{t}v(\\mathbf{y}_{i}),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad y_{i}\\in\\mathcal{P}_{i}\\operatorname*{min}\\,\\,\\eta\\left\\langle F_{i}^{x}(\\mathbf{Q}^{t},z_{i}^{t}),g_{i}^{x}\\right\\rangle+\\gamma_{t}\\mathrm{KL}\\left(g_{i}^{x}\\,\\big\\|(g_{i}^{x})^{t-1}\\right)+\\lambda_{t}v(g_{i}^{x}),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad g_{i}^{\\mathbf{x}}\\in\\mathcal{X}_{i}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad g_{i}^{\\mathbf{y}}\\in\\mathcal{X}_{i}\\operatorname*{min}\\,\\,\\eta\\left\\langle F_{i}^{y}(\\mathbf{Q}^{t},z_{i}^{t}),g_{i}^{y}\\right\\rangle+\\gamma_{t}\\mathrm{KL}\\left(g_{i}^{y}\\,\\big\\|(g_{i}^{y})^{t-1}\\right)+\\lambda_{t}v(g_{i}^{y}).}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad g_{i}^{y}\\in\\mathcal{S}_$   \n3:fo   \n4:   \n5:   \n6:   \n7:   \n8: end for   \n9: $t\\gets t+1$   \n10:endwhile ", "page_idx": 8}, {"type": "text", "text": "$[\\mathbf{A_{1}}]$ There exist constants $L_{1},L_{2}\\ge0$ such that $F_{i}(\\cdot,z_{i})$ is uniformly $L_{1}$ -Lipschitz continuous with respect to $\\|\\cdot\\|_{\\infty}$ under $\\|\\cdot\\|_{\\infty}$ , and $F_{i}(\\mathbf{Q},\\cdot)$ is uniformly $L_{2}$ -Lipschitz continuous with respect to $\\|\\cdot\\|_{\\infty}$ under $\\|\\cdot\\|_{1}$ ", "page_idx": 8}, {"type": "text", "text": "$[\\mathbf{A}_{2}]$ There are a positive constant $\\gamma~>~0$ and a set of non-negative constant matrices $\\{\\mathbf{B}_{i},\\mathbf{C}_{i}\\}_{i=1}^{d}$ satisfying $\\begin{array}{r}{\\left\\|\\,\\sum_{i=1}^{d}(\\mathbf{B}_{i}\\,+\\,\\mathbf{C}_{i})\\,\\right\\|_{\\infty}\\ \\leq\\ \\gamma.}\\end{array}$ .such that $\\mathbf{D_{P(Q,\\mathcal{\\cdot},y)}}(x,\\ x^{\\prime})\\ \\leq$ $\\begin{array}{r}{\\sum_{i=1}^{d}\\mathbf{C}_{i}\\langle\\mathbf{F}_{i}^{x}(\\mathbf{Q},z_{i}),\\mathbf{x}_{i}-\\mathbf{x}_{i}^{\\prime}\\rangle}\\end{array}$ and $\\begin{array}{r}{\\mathbf{D_{P(Q,x,\\cdot)}}(\\pmb{y},\\pmb{y}^{\\prime})\\geq\\sum_{i=1}^{d}\\mathbf{B}_{i}\\langle F_{i}^{y}(\\mathbf{Q},z_{i}),\\pmb{y}_{i}^{\\prime}-\\pmb{y}_{i}\\rangle}\\end{array}$ $[\\mathbf{A}_{3}]$ There exists $\\theta\\,\\in\\,[0,1)$ such that $\\mathbf{P}(\\cdot,z)$ is a $\\theta$ -contraction mapping under $\\|\\cdot\\|_{\\infty}$ , and $\\|\\mathbf{P}(\\mathbf{Q},z)\\|_{\\infty}\\leq C$ for any $z\\in{\\mathcal{Z}}$ \uff1a ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We present Lemma 4.3 to demonstrate that there exist $\\mathbf{Q}^{\\ast}\\in\\mathbb{R}^{\\ell\\times d}$ $x^{*}\\in\\mathcal{X}$ and $\\pmb{y}^{\\ast}\\in\\mathcal{V}$ satisfy the saddle point and fixed point conditions of function $\\mathbf{P}$ , i.e., Eq. (11), under proper assumptions. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.3. Assuming that Assumption 4.2 holds, $[\\mathbf{P}(\\mathbf{Q},\\cdot,\\cdot)]_{k,j}$ is continuous, convex with respect 10 $\\textbf{\\em x}$ concave with respect to $\\textit{\\textbf{y}}$ for any $(k,j)$ and mintCak,Ba]kil \u2265> C' for some C' > 0, then there exist $\\mathbf{Q}^{\\ast}\\in\\mathbb{R}^{\\ell\\times d}$ and $z^{*}\\in{\\mathcal{Z}}$ such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{Q}^{*}=\\mathbf{P}(\\mathbf{Q}^{*},x^{*},y^{*}),\\quad\\mathbf{Q}^{*}\\leq\\mathbf{P}(\\mathbf{Q}^{*},x,y^{*}),\\quad a n d\\quad\\mathbf{Q}^{*}\\geq\\mathbf{P}(\\mathbf{Q}^{*},x^{*},y).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For Algorithm 2, we let $\\begin{array}{r}{\\beta_{T,t}=\\beta_{t}\\prod_{j=t+1}^{T}(1-\\beta_{j})}\\end{array}$ for any $T\\geq t$ and $\\beta_{T,T}=\\beta_{T}$ , and set parameters ", "page_idx": 8}, {"type": "equation", "text": "$$\nc=2{(1-\\theta)}^{-1},\\,\\eta\\leq\\frac{\\left(1-\\theta\\right)^{1/2}}{16L_{2}\\left((\\gamma L_{1})^{1/2}+1\\right)},\\,\\beta_{t}=\\frac{c}{c+t},\\,\\alpha_{t}=\\beta_{T,t},\\,\\gamma_{t}=\\frac{\\alpha_{t-1}}{\\alpha_{t}},\\,\\lambda_{t}=1-\\gamma_{t}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then we have the following convergence result by denoting $M=\\operatorname*{max}_{i\\in[1:d]}\\,\\{m_{i}+n_{i}\\}.$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.4. For any generalized quasar-convex-concave function $f$ which satisfies Assumption 4.2 with $\\mathbf{P}\\equiv\\mathbf{Q}^{*}$ ,where $\\mathbf{Q}^{*}$ satisfies Eq. (11). Algorithm $^2$ 's output $\\bar{\\pmb z}_{T}=(\\bar{\\pmb x}_{T},\\bar{\\pmb y}_{T})$ satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{G}_{f}(\\bar{x}_{T},\\bar{y}_{T})\\leq60\\operatorname*{max}_{z\\in\\mathcal{Z}}\\left(\\sum_{i=1}^{d}\\psi_{i}(z)\\right)(1-\\theta)^{-1}\\left(\\frac{2}{\\eta}\\log(M)+\\eta L_{1}^{2}+L_{1}Y_{T}^{\\eta}\\right)T^{-1},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "equation", "text": "$\\begin{array}{r}{Y_{T}^{\\eta}=8(c+1)[\\frac{4\\gamma}{\\eta}\\log(M)+160\\gamma L_{2}+2\\eta\\gamma L_{1}^{2}(1+64C^{2})](\\log(c+T)+1).}\\end{array}$ ", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Similar to minimization Algorithm 1, the iteration complexity of minimax Algorithm 2 linearly depends on the upper bound of $\\textstyle\\sum_{i=1}^{d}\\psi_{i}$ over $\\mathcal{Z}$ Generally, the upper bound of $\\textstyle\\sum_{i=1}^{d}\\psi_{i}$ on $\\mathcal{Z}$ is related to $d$ . In specific problems of multi-variable optimization (such as two-player zero-sum Markov games), one can uniformly bound $\\textstyle\\sum_{i=1}^{d}\\psi_{i}$ on $\\mathcal{Z}$ by a constant. ", "page_idx": 8}, {"type": "text", "text": "4.3 Application to Infinite Horizon Two-Player Zero-Sum Markov Games ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we show how to leverage Algorithm 2 to achieve accelerated rates for optimizing infinite horizon two-player zero-sum Markov games. Our algorithm use $\\tilde{\\mathcal{O}}(\\varepsilon^{-1})$ iterationbound to find an $\\varepsilon$ -approximate Nash equilibrium of infinite horizon two-player zero-sum Markov games. ", "page_idx": 9}, {"type": "text", "text": "As similar as the definition of discounted MDP in Preliminary, we utilize $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\mathcal{B},\\mathbb{P},\\sigma,\\theta,\\pmb{\\rho}_{0})$ to define a infinite horizon two-player zero-sum Markov game. The difference here compared to Section 3.3 is that the cost function $\\sigma$ is defined on $S\\times A\\times B$ with values in $[0,1]$ , and the transition model $\\mathbb{P}(s|s^{\\prime},a^{\\prime},b^{\\prime})$ denotes the probability of transitioning into state $s$ upon player 1 taking action $a^{\\prime}$ and player 2 taking action $b^{\\prime}$ in state $s^{\\prime}$ . We can define the value function $V^{z}$ and action-value function $Q^{z}$ on the joint distribution $\\begin{array}{r}{z=(\\pmb{x},\\pmb{y})\\in\\mathcal{Z}=\\prod_{i=1}^{|S|}\\Delta_{A}\\times\\prod_{i=1}^{|S|}\\Delta_{B}}\\end{array}$ The infinite horizon two-player zero-sum Markov games consider the following policy optimization problem: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{x}\\in\\mathcal{X}}\\operatorname*{max}_{\\pmb{y}\\in\\mathcal{Y}}J^{\\pmb{x},\\pmb{y}}(\\pmb{\\rho}_{0}),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $J^{z}(\\rho_{0})=\\mathbb{E}_{s_{0}\\sim\\rho_{0}}[V^{z}(s_{0})]$ . The following proposition indicates that $J^{z}$ is general quasar convex-concave, and satisfies Assumption 4.2 and the condition of Theorem 4.4, ", "page_idx": 9}, {"type": "text", "text": "Proposition 4.5. For any $\\mathbf{Q}\\ =\\ (\\mathbf{Q}_{1},\\cdots,\\mathbf{Q}_{|S|})$ with every $\\mathbf{Q}_{i}\\ \\in\\ \\mathbb{R}^{|A|\\times|B|}$ , define function $f_{i}(\\mathbf{Q},z_{i})\\,:=\\,x_{i}^{\\top}\\mathbf{Q}_{i}y_{i}$ for any $i\\,\\in\\,[1\\,:\\,|S|]$ :There exists a tensor-valued function $\\mathbf{P}$ such that $J^{z}(\\rho_{0})$ satisfies GQCC condition with $f_{i}(\\mathbf{P}(z),z_{i})\\,=\\,f_{i}(\\mathbf{Q}^{*},z_{i})$ for any $\\rho_{0}\\,\\in\\,\\Delta_{\\mathcal{S}}$ ,where $\\mathbf{Q}^{*}$ satisfies the conditions mentioned in Eq. (11). Moreover, $\\mathbf{P}$ satisfies Assumption 4.2. ", "page_idx": 9}, {"type": "text", "text": "According to Proposition 4.5 and Theorem 4.4, if we apply Algorithm 2 to the infinite horizon two-player Markov games basing internal operator $F_{i}(\\dot{\\mathbf{Q}_{\\ i}^{\\prime}}\\dot{z})=\\check{(}y_{i}^{\\top}\\mathbf{Q}_{i}^{\\top},-x_{i}^{\\top}\\mathbf{Q}_{i})^{\\top}$ for block $\\boldsymbol{z}_{i}$ with parameter selection Eq. (12), which is actually a variant of optimistic gradient descent/ascent for Markov games [67], then the iterations $T$ we need to find an $\\varepsilon$ -approximate Nash equilibrium is upperbounded by $\\tilde{\\mathcal{O}}((1-\\theta)^{-2.5}\\varepsilon^{-1})$ . To the best of our knowledge, our iteration bound matches state-ofthe-art iteration bound and is a factor of $(1-\\theta)^{-1.5}|S|$ better than $\\tilde{\\mathcal{O}}((1-\\theta)^{-4}|S|\\varepsilon^{-1})$ bound of Cen et al. [10]. Since the upper bound of $\\sum_{i=1}^{|S|}\\psi_{i}$ over feasible region $\\mathcal{Z}$ in infinite horizon two-player zero-sum Markov games' setin satisies $\\begin{array}{r}{\\sum_{i=1}^{|\\mathcal{S}|}\\psi_{i}(\\boldsymbol{z})\\leq\\sum_{i=1}^{|\\mathcal{S}|}[d_{\\rho_{0}}^{\\boldsymbol{x},\\boldsymbol{y}^{\\ast}(\\boldsymbol{x})}(s_{i})+d_{\\rho_{0}}^{\\boldsymbol{x}^{\\ast}(\\boldsymbol{y}),\\boldsymbol{y}}(s_{i})]\\leq2}\\end{array}$ for any $z\\in{\\mathcal{Z}}$ , our algorithm's iteration bound does not depend on the size of states. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce two function structures: GQC and GQCC and provide related algorithmic frameworks with convergence result. To complement our result, we also show that discounted MDP and infinite horizon two-player zero-sum Markov games admit the GQC and GQCC condition, respectively, and satisfy our mild assumptions. ", "page_idx": 9}, {"type": "text", "text": "6Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "C. Fang was supported by National Key R&D Program of China (2022ZD0114902) and the NSF China (No.62376008). L. Luo was supported by National Natural Science Foundation of China (No. 62206058), Shanghai Sailing Program (22YF1402900), Shanghai Basic Research Program (23JC1401000), and the Major Key Project of PCL under Grant PCL2024A06. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Jacob D. Abernethy, Peter L. Bartlett, and Elad Hazan. Blackwell approachability and no-regret learning are equivalent. arXiv preprint arXiv: 1011.1936, 2010.   \n[2] Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine LearningResearch,2021.   \n[3]  Ahmet Alacaoglu, Luca Viano, Niao He, and Volkan Cevher. A natural actor-critic framework for zero-sum Markov games. In International Conference on Machine Learning. PMLR, 2022.   \n[4]  David Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathematics, 1956. [5]  Charles E. Blair. Problem complexity and method efficiency in optimization (a. s. nemirovsky and d. b. yudin). Siam Review, 1985.   \n[6]  Stephen P. Boyd and Lieven Vandenberghe. Convex optimization. Journal of the American Statistical Association, 2005.   \n[7] Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points I. Mathematical Programming, 2017.   \n[8]  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points I: first-order methods. Mathematical Programming, 2017. [9]  Shicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive games with entropy regularization. Advances in Neural Information Processing Systems, 2021.   \n[10] Shicong Cen, Yuejie Chi, Simon Shaolei Du, and Lin Xiao. Faster last-iterate convergence of policy optimization in zero-sum markov games. International Conference on Learning Representations, 2023.   \n[11]  Lesi Chen, Boyuan Yao, and Luo Luo. Faster stochastic algorithms for minimax optimization under Polyak Lojasiewicz condition. Advances in Neural Information Processing Systems, 2022.   \n[12] Ziyi Chen, Shaocong Ma, and Yi Zhou._ Sample efficient stochastic policy extragradient algorithm for zero-sum markov game. 2021.   \n[13] Ching-An Cheng, Remi Tachet des Combes, Byron Boots, and Geoff Gordon. A reduction from reinforcement learning to no-regret online learning. Proceedings of the Twenty Third International Conference on Artificial Inteligence and Statistics, 2020.   \n[14]  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. Conference on Learning Theory, 2012.   \n[15]  Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning in general games. Advances in Neural Information Processing Systems, 2021.   \n[16] Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained min-max optimization. Procedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, 2021.   \n[17]  Jelena Diakonikolas, Constantinos Daskalakis, and Michael Jordan. Effcient methods for structured nonconvex-nonconcave min-max optimization. Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, 2021.   \n[18]  Jerzy A. Filar and Boleslaw Tolwinski. On the algorithm of Pollatschek and Avi-ltzhak. 1991.   \n[19]  Anders Forsgren, Philip E. Gill, and Margaret H. Wright. Interior methods for nonlinear optimization. SIAM Rev., 2002.   \n[20]  Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. International Conference on Machine Learning, 2019.   \n[21] Benjamin Grimmer, Haihao Lu, Pratik Worah, and Vahab Mirrokni. The landscape of the proximal point method for nonconvex-nonconcave minimax optimization. Mathematical Programming, 2023.   \n[22] Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. The Journal of Machine Learning Research, 2018.   \n[23]  Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 2000.   \n[24] Oliver Hinder, Aaron Sidford, and Nimit Sohoni. Near-optimal methods for minimizing star-convex functions and beyond. Conference on learning theory, 2020.   \n[25]  Jean-Baptiste Hiriart-Urruty and Claude Lemarechal.  Convex analysis and minimization algorithms. 1993.   \n[26] Alan J. Hoffman and Richard M. Karp. On nonterminating stochastic games. Management Science, 1966.   \n[27] Feihu Huang, Xidong Wu, and Heng Huang. Efficient mirror descent ascent methods for nonsmooth minimax problems. Advances in Neural Information Processing Systems, 2021.   \n[28]  Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 2018.   \n[29]  Anatoli B. Juditsky, Philippe Rigollet, and A. Tsybakov. Learning by mirror averaging. Annals of Statistics, 2005.   \n[30]  Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. International Conference on Machine Learning, 2002.   \n[31]  Alexander Kaplan and Rainer Tichatschke. Proximal point methods and nonconvex optimization. Journal of Global Optimization, 1998.   \n[32] Robert D. Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape local minima? International Conference on Machine Learning, 2018.   \n[33]  GM Korpelevich. Extragradient method for finding saddle points and other problems. Matekon, 1977.   \n[34]  Guanghui Lan. First-order and Stochastic Optimization Methods for Machine Learning. Springer Cham, 2020.   \n[35] Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. Mathematical Programming, 2022.   \n[36]  Sucheol Lee and Donghwan Kim. Fast extra gradient methods for smooth structured nonconvexnonconcave minimax problems. Advances in Neural Information Processing Systems, 2021.   \n[37] E.S. Levitin and Boris Polyak. Constrained minimization methods. USSR Computational Mathematics and Mathematical Physics, 1966.   \n[38] Jiajin Li, Linglingzhi Zhu, and Anthony Man-Cho So. Nonsmooth composite nonconvexconcave minimax optimization. arXiv preprint arXiv:2209.10825, 2022.   \n[39]  Tianyi Lin, Chi Jin, and Michael I. Jordan. On gradient descent ascent for nonconvex-concave minimax problems. International Conference on Machine Learning, 2020.   \n[40]  Tianyi Lin, Chi Jin, and Michael I. Jordan. Near-optimal algorithms for minimax optimization. Proceedings of Thirty Third Conference on Learning Theory, 2020.   \n[41] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. 30th Annual Symposium on Foundations of Computer Science, 1989.   \n[42] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. International Conference on Machine Learning, 1994.   \n[43]  Arkadi Nemirovski. Prox-method with rate of convergence $\\mathcal{O}(1/t)$ for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 2004.   \n[44]  Yuri Nesterov. A method of solving a convex programming problem with convergence rate $\\mathcal{O}\\left(k^{-2}\\right)$ . In Doklady Akademii Nauk. Russian Academy of Sciences, 1983.   \n[45]  Yuri Nesterov. Dual extrapolation and its applications to solving variational inequalities and related problems. Mathematical Programming, 2007.   \n[46]  Yuri Nesterov. Introductory lectures on convex optimization - a basic course. In Applied Optimization, 2014.   \n[47]  Yuri Nesterov and Boris T Polyak.  Cubic regularization of newton method and its global performance. Mathematical Programming, 2006.   \n[48]  Yuri Nesterov and Laura Rosa Maria Scrimali. Solving strongly monotone variational and quasi-variational inequalities. Econometrics eJournal, 2006.   \n[49]  Jorge Nocedal and Stephen J. Wright. Numerical optimization. In Fundamental Statistical Inference, 2018.   \n[50] Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving a class of non-convex min-max games using iterative first order methods. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems. Curran Associates, Inc., 2019.   \n[51]  Erich Novak. Deterministic and stochastic error bounds in numerical analysis. 1988.   \n[52]  Yuyuan Ouyang and Yangyang Xu. Lower complexity bounds of frst-order methods for convex-concave bilinear saddle-point problems. Mathematical Programming, 2018.   \n[53]  Sam Patterson and Yee Whye Teh. Stochastic gradient riemannian langevin dynamics on the probability simplex. In Neural Information Processing Systems, 2013.   \n[54]  Moshe Asher Pollatschek and Benjamin Avi-Itzhak. Algorithms for stochastic games with geometrical interpretation. Management Science, 1969.   \n[55]  Leonid Denisovich Popov. A modification of the arrow-hurwicz method for search of saddle points. Mathematical notes of the Academy of Sciences of the USSR, 1980.   \n[56]  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. arXiv preprint arXiv:1208.3728, 2012.   \n[57]  Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. Advances in Neural Information Processing Systems, 2013.   \n[58]  Julia Jean Robinson. An iterative method of solving a game. Classics in Game Theory, 1951.   \n[59]  R. Tyrrell Rockafellar. Convex analysis: (pms-28). 1970.   \n[60]  R. Tyrrell Rockafellar, Roger J.-B. Wets, and Maria Wets. Variational analysis. In Grundlehren der mathematischen Wissenschaften, 1998.   \n[61] Lloyd S. Shapley. Stochastic games\\*. Proceedings of the National Academy of Sciences, 1953.   \n[62] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 1999.   \n[63]  Paul Tseng. On linear convergence of iterative methods for the variational inequality problem. Journal of Computational and Applied Mathematics, 1995.   \n[64]  Jan van der Wal. Discounted markov games: Generalized policy iteration method. Journal of Optimization Theory and Applications, 1978.   \n[65]  Gal Vardi, Gilad Yehudai, and Ohad Shamir. Learning a single neuron with bias using gradient descent. ArXiv, 2021.   \n[66]  Yuanhao Wang and Jjian Li. Improved algorithms for convex-concave minimax optimization. Advances in Neural Information Processing Systems, 2020.   \n[67]  Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games. In Annual Conference Computational Learning Theory, 2021.   \n[68]  Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance-reduced optimization for a class of nonconvex-nonconcave minimax problems. arXiv preprint arXiv:2002.09621, 2020.   \n[69] Junchi Yang, Antonio Orvieto, Aurelien Lucchi, and Niao He. Faster single-loop algorithms for minimax optimization without strong concavity. In International Conference on Artificial Intelligence and Statistics, 2022.   \n[70]  Yuepeng Yang and Cong Ma. ${\\mathcal{O}}(\\Gamma^{-1})$ convergence of optimistic-follow-the-regularized-leader in two-player zero-sum markov games. arXiv preprint arXiv:2209.12430, 2022.   \n[71] TaeHo Yoon and Ernest K Ryu. Accelerated algorithms for smooth convex-concave minimax problems with ${\\mathcal{O}}(1/k^{2})$ rate on squared gradient norm. International Conference on Machine Learning, 2021.   \n[72] Sihan Zeng, Thinh T. Doan, and Justin Romberg. Regularized gradient descent ascent for two-player zero-sum Markov games. Advances in Neural Information Processing Systems, 2022.   \n[73] Jiawei Zhang, Peijun Xiao, Ruoyu Sun, and Zhi-Quan Luo. A single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems. Advances in Neural Information Processing Systems, 2020.   \n[74] Yulai Zhao, Yuandong Tian, Jason D. Lee, and Simon Shaolei Du. Provably efficient policy optimization for two-player zero-sum markov games. In International Conference on Artificial Intelligence and Statistics, 2021.   \n[75]  Yulai Zhao, Yuandong Tian, Jason Lee, and Simon Du. Provably efficient policy optimization for two-player zero-sum markov games. In International Conference on Artificial Intelligence and Statistics. PMLR, 2022.   \n[76] Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to global minimum in deep learning via star-convex path. arXiv preprint arXiv: 1901.00451, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1.1 Contribution . 3   \n1.2 Related Works . 3 ", "page_idx": 14}, {"type": "text", "text": "2   Preliminary 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "3  Minimization Optimization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "3.1 Generalized Quasar-Convexity (GQC) 5   \n3.2 Main Results . 6   \n3.3 Application to Reinforcement Learning 7 ", "page_idx": 14}, {"type": "text", "text": "4   Minimax Optimization 8 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "4.1 Generalized Quasar-Convexity-Concavity (GQCC) 8   \n4.2 MainResults 8   \n4.3 Application to Infinite Horizon Two-Player Zero-Sum Markov Games 10 ", "page_idx": 14}, {"type": "text", "text": "5  Conclusion 10 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "6  Acknowledgements 10 ", "page_idx": 14}, {"type": "text", "text": "A Preliminary 16 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1Supplemental Notation 16   \nA.2 Finite Differences . 16   \nA.3 Finite Horizon Markov Decision Process 16 ", "page_idx": 14}, {"type": "text", "text": "B  Minimization Optimization 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Theorem 3.5 19   \nB.1.1 Part I 19   \nB.1.2 Part II . 21   \nB.1.3 The Last Step . . . 24   \nB.2 Simple Example . . . . 24   \nB.3Application to Reinforcement Learning 25   \nB.3.1 Analysis of Infinite Horizon Reinforcement Learning 25   \nB.3.2  Analysis of Finite Horizon Reinforcement Learning . 26 ", "page_idx": 14}, {"type": "text", "text": "C   Minimax Optimization 26 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Preparatory Discussion 26   \nC.2 Theorem C.7 and Relate Proof 29   \nC.2.1 Part I 30   \nC.2.2 Part II: Estimation of Approximation Error $\\lVert\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\rVert$ 32   \nC.2.3 The Last Step . . . 35   \nC.3 Application to Minimax Problems 36   \nC.3.1 Infinite Horizon Two-Player Zero-Sum Markov Games 36   \nC.3.2Convex-Concave Minimax Problems 37 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "D  Auxiliary Lemma ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "37 ", "page_idx": 15}, {"type": "text", "text": "E Limitation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "41 ", "page_idx": 15}, {"type": "text", "text": "A Preliminary ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Supplemental Notation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For simplicity,we denote $\\begin{array}{r}{g(\\Gamma):=\\sum_{k=1}^{\\infty}\\Gamma^{-k}[k^{7}+(k+1)\\exp\\{2k\\}]}\\end{array}$ thechi-squared divergence between $\\pmb{p},\\pmb{q}$ as $\\begin{array}{r}{\\chi^{2}(\\pmb{p}||\\pmb{q})\\;:=\\;\\sum_{j=1}^{n}\\frac{(\\pmb{p}(j)-\\pmb{q}(j))^{2}}{\\pmb{q}(j)}}\\end{array}$ \uff0c $\\begin{array}{r}{\\mathbb{E}_{p}({\\pmb x})~:=~\\sum_{j=1}^{n}{\\pmb p}(j){\\pmb x}(j)}\\end{array}$ and $\\operatorname{Var}_{p}(x)\\;:=\\;$ $\\begin{array}{r}{\\sum_{j=1}^{n}\\pmb{p}(j)\\cdot\\left(\\pmb{x}(j)-\\mathbb{E}_{\\pmb{p}}(\\pmb{x})\\right)^{2}}\\end{array}$ for any $\\textstyle p,q\\;\\in\\;\\Delta_{n}$ and $\\textbf{\\em x}\\in\\mathbb{R}^{n}$ . For $\\zeta\\;>\\;0,n\\;\\in\\;\\mathbb{Z}_{+}$ , we say that a sequence of distributions $p^{1},\\cdots\\,,p^{T}\\in\\Delta_{n}$ is $\\zeta$ consecutivelycloseif foreach $1\\leq t<T$ , it holds that max $\\left\\{\\left\\|{\\frac{p^{t}}{p^{t+1}}}\\right\\|,\\left\\|{\\frac{p^{t+1}}{p^{t}}}\\right\\|\\right\\}\\leq1+\\zeta$ For posive scalar $\\theta\\in[0,1)$ non-negativ integers $t$ and $T$ , we define $\\begin{array}{r}{\\beta_{T,t}^{\\theta}:=\\beta_{t}\\prod_{j=t}^{T-1}(1-\\beta_{j}+\\theta\\beta_{j})}\\end{array}$ , and $\\beta_{T,T}^{\\theta}=1$ ", "page_idx": 15}, {"type": "text", "text": "A.2 Finite Differences ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Definition A.1 (Finite Differences). For a sequence of vectors $\\pmb{L}\\,=\\,(\\pmb{L}^{0},\\cdot\\cdot\\cdot\\,,\\pmb{L}^{T})$ where each $\\pmb{L}^{t}\\in\\mathbb{R}^{n}$ ,and integers $h\\in\\mathbb{Z}_{+}$ , the order- $h$ finite difference sequence for the sequence $\\textbf{\\emph{L}}$ is denoted by $D_{h}{\\cal L}:=\\left((D_{h}{\\cal L})^{0},\\cdot\\cdot\\cdot,(D_{h}{\\cal L})^{T-h}\\right)$ recursivelywith $(D_{0}L)^{t}:=L^{t}$ for all $t\\in[0:T]$ , and ", "page_idx": 15}, {"type": "equation", "text": "$$\n(D_{h}{\\cal L})^{t}:=(D_{h-1}{\\cal L})^{t+1}-(D_{h-1}{\\cal L})^{t},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all $h\\geq1$ and $t\\in[1:T-h]$ ", "page_idx": 15}, {"type": "text", "text": "As stated in [15, Remark 4.3], we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n(D_{h}{\\cal L})^{t}=\\sum_{s=0}^{h}\\binom{h}{s}\\,(-1)^{h-s}{\\cal L}^{t+s}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To guarantee the coherence of the analysis's structure, we introduce the definition of the shift operator $E_{s}$ asfollows: ", "page_idx": 15}, {"type": "text", "text": "Definition A.2 (Shift Operator). For a sequence of vectors $\\pmb{L}=(\\pmb{L}^{0},\\cdot\\cdot\\cdot\\leq,\\pmb{L}^{T})$ where each $\\pmb{L}^{t}\\in\\mathbb{R}^{n}$ and integers $s\\in\\mathbb{Z}_{+}$ , the $s$ -shift sequence for the sequence $\\textbf{\\emph{L}}$ is denoted by $E_{s}L:=((E_{s}{\\cal L})^{0},\\cdot\\cdot\\cdot$ \uff0c $(E_{s}{\\bf{{L}}})^{T-h})$ with $(E_{s}\\pmb{L})^{t}=\\pmb{L}^{t+s}$ for $t\\in[1:T-s]$ ", "page_idx": 15}, {"type": "text", "text": "A.3 Finite Horizon Markov Decision Process ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We also consider the following finite horizon Markov decision process (MDP), denoted by ${\\mathcal{M}}:=$ $\\left(H,S_{1:H},\\mathcal{A}_{1:H},\\mathbb{P}_{2:H},\\sigma,\\pmb{\\rho}_{1}\\right)$ $H\\in\\mathbb{Z}_{+}$ denotes the number of horizon; $S_{1:H}=(S_{1},\\cdot\\cdot\\cdot,S_{H})$ is a sequence of $H$ finite state spaces; $\\mathcal{A}_{1:H}=(\\mathcal{A}_{1},\\cdot\\cdot\\cdot,\\mathcal{A}_{H})$ is a sequence of $H$ finite action spaces; $\\mathbb{P}_{h}\\!\\left(s_{h}\\middle|s_{h-1},a_{h-1}\\right)$ denotes the probability of transitioning from $s h\\!-\\!1$ to $s_{h}$ under playing action $a_{h-1}$ at horizon $h-1;\\sigma:S_{1:H}\\times A_{1:H}\\rightarrow[0,1].$ is a cost function; $\\rho_{1}$ is a initial state distribution over $S_{1}$ ", "page_idx": 15}, {"type": "text", "text": "$\\pi\\;=\\;(\\pi_{1},\\cdot\\cdot\\cdot\\;,\\pi_{H})\\;:\\;S_{1:H}\\;\\rightarrow\\;\\Delta_{A_{1}}\\;\\times\\;\\cdot\\cdot\\;\\times\\;\\Delta$ An denotes a stochastic policy. Similarly, we use Prh $\\mathbf{Pr}_{h}^{\\pi_{1:h-1}}(s^{\\prime}|s)=\\mathbf{Pr}_{h}^{\\pi_{1:h-1}}(s_{h}=s^{\\prime}|s_{1}=s)$ to denote the probability of visiting the state $s^{\\prime}$ from the state $s$ at horizon $h$ according to policy $\\pi_{1:h-1}$ . Let trajectory $\\tau=(s_{h},a_{h})_{h=1}^{H}$ , where $s_{1}\\sim\\rho_{1}$ , and, for all subsequent horizon $h$ $\\iota,a_{h}\\sim\\pi_{h}(\\cdot|s_{h})$ and $s_{h+1}\\sim\\mathbb{P}_{h+1}(\\cdot|s_{h},a_{h})$ The value function $V_{h}^{\\pi_{h:H}}\\,:\\,S_{h}\\,\\rightarrow\\,\\mathbb{R}$ is defined as the sum of future cost starting at state $s_{h}$ and executing ", "page_idx": 15}, {"type": "text", "text": "Algorithm 3 Optimistic Mirior Descent for Multi-Variables ", "page_idx": 16}, {"type": "text", "text": "Input: $\\left\\{\\pmb{g}_{i}^{0}=\\pmb{x}_{i}^{0}\\right\\}_{i=1}^{d}$ \uff0c $\\eta$ and $T$   \nOutput: Randomly pick up $t\\in\\{1,\\cdot\\cdot\\cdot\\,,T\\}$ following the probability $\\mathbb{P}[t]=1/T$ and return $\\pmb{x}^{t}$   \n1: while $t\\leq T$ do   \n2: for all $i\\in[1:d]$ do   \n3: $\\begin{array}{r l}&{\\pmb{x}_{i}^{t}=\\underset{\\pmb{x}_{i}\\in\\mathcal{X}_{i}}{\\mathrm{argmin~}}^{\\mathrm{!}}\\eta\\left\\langle\\pmb{F}_{i}(\\pmb{x}^{t-1}),\\pmb{x}_{i}\\right\\rangle+V\\left(\\pmb{x}_{i},\\pmb{g}_{i}^{t-1}\\right),}\\\\ &{\\pmb{g}_{i}^{t}=\\underset{\\pmb{g}_{i}\\in\\mathcal{X}_{i}}{\\mathrm{argmin~}}\\eta\\left\\langle\\pmb{F}_{i}(\\pmb{x}^{t}),\\pmb{g}_{i}\\right\\rangle+V\\left(\\pmb{g}_{i},\\pmb{g}_{i}^{t-1}\\right).}\\end{array}$   \n4:   \n5: end for   \n6: $t\\gets t+1$   \n7: end while ", "page_idx": 16}, {"type": "text", "text": "$\\pmb{\\pi}_{h:H}=(\\pmb{\\pi}_{h},\\pmb{\\cdot}\\cdot\\cdot\\pmb{\\,},\\pmb{\\pi}_{H})$ , i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{h}^{\\pi_{h:H}}(s_{h})=\\mathbb{E}\\left[\\sum_{h^{\\prime}=h}^{H}\\sigma(s_{h^{\\prime}},a_{h^{\\prime}})\\bigg|\\,\\pi_{h:H},s_{h}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For convenience, we define $V_{1}^{\\pmb{\\pi}}(s_{1})=V_{1}^{\\pmb{\\pi}_{1:H}}(s_{1})$ . Moreover, we define the action-value function $Q_{h}^{\\pi_{h+1:H}}:S_{h}\\times{\\cal A}_{h}\\rightarrow[0,1+H-h]$ as folowsa ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ_{h}^{\\pi_{h+1:H}}(s_{h},a_{h})=\\sigma(s_{h},a_{h})+\\mathbb{E}\\left[\\sum_{h^{\\prime}=h+1}^{H}\\sigma(s_{h^{\\prime}},a_{h^{\\prime}})\\bigg\\rvert\\;\\pi_{h+1:H},s_{h},a_{h}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B  Minimization Optimization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We begin with a general version of Theorem 3.2 basing Algorithm 3 in this part. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.1. [General Version of Theorem 3.21 We consider the divergence-generating function $v$ withBregman's divergence $V(\\pmb{x}_{i},\\pmb{u}_{i})=v(\\pmb{x}_{i})-v(\\pmb{u}_{i})-\\langle\\nabla v(\\pmb{u}_{i}),\\pmb{x}_{i}-\\pmb{u}_{i}\\rangle$ foranyblock $\\mathcal{X}_{i}$ and any $\\pmb{x}_{i},\\pmb{u}_{i}\\in\\mathcal{X}_{i}$ Assumingthat $\\pmb{F}$ is $L$ -Lipschitzcontinuouswithrespect to $\\|\\cdot\\|_{*}$ under $\\Vert\\cdot\\Vert$ $V(\\pmb{x}_{i},\\pmb{u}_{i})\\geq\\|\\pmb{x}_{i}-\\pmb{u}_{i}\\|^{2}$ for any $\\pmb{x}_{i},\\pmb{u}_{i}\\in\\mathcal{X}_{i}$ and $\\gamma_{m a x}=\\operatorname*{max}_{i\\in[1:d]}\\gamma_{i}<\\infty$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}(f(\\pmb{x}^{t})-f(\\pmb{x}^{*}))\\leq\\frac{2L\\left(d\\gamma_{m a x}\\right)^{1/2}\\left(\\sum_{i=1}^{d}\\gamma_{i}^{-1}\\right)^{3/2}}{T}\\operatorname*{max}_{i\\in[1:d]}\\left[\\operatorname*{max}_{x_{i}\\in\\mathcal{X}_{i}}V(\\pmb{x}_{i},g_{i}^{0})\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with setting $\\begin{array}{r}{\\eta=(L^{2}d\\gamma_{m a x}\\sum_{i=1}^{d}\\gamma_{i}^{-1})^{-1/2}/2.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Proof. According to GQC condition (Definition 3.1), we have the following estimation ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(f(\\pmb{x}^{t})-f(\\pmb{x}^{*}))\\leq\\sum_{i=1}^{d}\\frac{1}{\\gamma_{i}}\\sum_{t=1}^{T}\\left\\langle\\pmb{F}_{i}(\\pmb{x}^{t}),\\pmb{x}_{i}^{t}-\\pmb{x}_{i}^{*}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For any fixed $i\\in[1:d]$ , we obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle F_{i}(x^{t}),x_{i}^{t}-x_{i}^{*}\\rangle=\\underbrace{\\langle F_{i}(x^{t})-F_{i}(x^{t-1}),x_{i}^{t}-g_{i}^{t}\\rangle}_{\\mathbb{Z}}+\\underbrace{\\langle F_{i}(x^{t-1}),x_{i}^{t}-g_{i}^{t}\\rangle}_{\\mathbb{Z}\\mathbb{Z}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\underbrace{\\langle F_{i}(x^{t}),g_{i}^{t}-x_{i}^{*}\\rangle}_{\\mathbb{Z}\\mathbb{Z}\\mathbb{Z}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\pmb{F}$ .s $L$ -Lipschitz continuous with respect to $\\|\\cdot\\|_{*}$ under $\\Vert\\cdot\\Vert$ , we have following estimation of $\\mathcal{T}$ by using Cauchy-Schwarz inequality ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{Z}\\leq\\frac{L^{2}\\eta}{2}\\left\\|\\pmb{x}^{t}-\\pmb{x}^{t-1}\\right\\|^{2}+\\frac{1}{2\\eta}\\left\\|\\pmb{x}_{i}^{t}-\\pmb{g}_{i}^{t}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In addition, utilizing the result of [Lemma 3.4, [34]] on step-3 and step-4 of Algorithm 3, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}\\mathbb{Z}\\leq\\displaystyle\\frac{1}{\\eta}\\left[V\\left(g_{i}^{t},g_{i}^{t-1}\\right)-V\\left(g_{i}^{t},x_{i}^{t}\\right)-V\\left(x_{i}^{t},g_{i}^{t-1}\\right)\\right],}\\\\ {\\mathcal{Z}\\mathbb{Z}\\mathbb{Z}\\leq\\displaystyle\\frac{1}{\\eta}\\left[V\\left(x_{i}^{*},g_{i}^{t-1}\\right)-V\\left(x_{i}^{*},g_{i}^{t}\\right)-V\\left(g_{i}^{t},g_{i}^{t-1}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, by applying Eq. (19), (20) and (21) into Eq. (18), we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}(F_{t}(x^{t}),x_{t}^{t}-x_{i}^{t})\\leq\\frac{1}{\\eta}V(x^{*},g_{t}^{*})+\\displaystyle\\sum_{t=1}^{T}\\left[\\frac{L^{2}\\eta}{2}\\|x^{t}-x^{t-1}\\|^{2}+\\frac{1}{2\\eta}\\|x_{t}^{t}-g_{t}^{*}\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle-\\frac{1}{\\eta}\\sum_{t=1}^{T}V(g_{t}^{*},x_{t}^{t})-\\frac{1}{\\eta}\\sum_{t=1}^{T}V(x^{t},g_{t}^{*(t-1)})}\\\\ &{\\leq\\displaystyle\\frac{1}{6\\eta}V(x^{*},g_{t}^{*})+\\frac{L^{2}\\eta}{2}\\sum_{t=1}^{T}\\|x^{t}-x^{t-1}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\displaystyle-\\frac{1}{2\\eta}\\displaystyle\\sum_{t=1}^{T}\\|g_{t}^{*}-x^{t}\\|^{2}-\\frac{1}{2\\eta}\\displaystyle\\sum_{t=1}^{T}\\|x_{t}^{t}-g_{t}^{*(t-1)}\\|^{2}}\\\\ &{\\leq\\displaystyle\\frac{1}{6\\eta}V(x^{*},g_{t}^{*})+\\frac{1}{2\\eta}\\|g_{t}^{*}-x^{\\theta}\\|^{2}+\\frac{L^{2}\\eta}{2}\\displaystyle\\sum_{t=1}^{T}\\|x^{t}-x^{t-1}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\displaystyle-\\frac{1}{4\\eta}\\displaystyle\\sum_{t=1}^{T}\\|x_{t}^{t}-x_{t}^{t-1}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (a) is derived from the assumption that $V(\\pmb{x}_{i},\\pmb{u}_{i})\\geq\\|\\pmb{x}_{i}-\\pmb{u}_{i}\\|^{2}$ for any $\\pmb{x}_{i},\\pmb{u}_{i}\\in\\mathcal{X}_{i}$ and (b) follows from the convexity of $\\Vert\\cdot\\Vert$ . Applying Eq. (22) to Eq. (17), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}(f(\\mathbf{x}^{t})-f(\\mathbf{x}^{*}))\\,{\\underbrace{<_{\\mathrm{I}}^{1}}_{\\mathrm{(v)}}}\\frac{1}{\\eta}\\sum_{i=1}^{d}\\frac{V(\\mathbf{x}_{i}^{*},g_{i}^{0})}{\\gamma_{i}}+\\frac{L^{2}\\eta}{2}\\left(\\sum_{i=1}^{d}\\gamma_{i}^{-1}\\right)\\sum_{t=1}^{T}\\left\\|\\mathbf{x}^{t}-\\mathbf{x}^{t-1}\\right\\|^{2}}}\\\\ &{}&{\\quad-\\,\\frac{1}{4\\eta}\\sum_{t=1}^{T}\\left[\\displaystyle\\sum_{i=1}^{d}\\frac{\\left\\|\\mathbf{x}_{i}^{t}-\\mathbf{x}_{i}^{t-1}\\right\\|^{2}}{\\gamma_{i}}\\right]}\\\\ &{}&{\\quad\\stackrel{<}{\\mathrm{(d)}}\\frac{\\mathcal{I}_{i1}^{-1}}{\\eta}\\,\\displaystyle\\operatorname*{max}_{i\\in[1:d]}\\left[\\operatorname*{max}_{i}V(\\mathbf{x}_{i},g_{i}^{0})\\right]}\\\\ &{}&{\\quad-\\,\\left(\\frac{1}{4d\\eta\\gamma_{\\mathrm{max}}}-\\frac{L^{2}\\eta}{2}\\displaystyle\\sum_{i=1}^{d}\\gamma_{i}^{-1}\\right)\\sum_{t=1}^{T}\\left\\|\\mathbf{x}^{t}-\\mathbf{x}^{t-1}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (c) is derived from the fact that $\\pmb{g}_{i}^{0}=\\pmb{x}_{i}^{0}$ for any $i\\in[1:d]$ and (d) follows from the convexity f $\\begin{array}{r}{\\|\\cdot\\|\\,(\\frac{1}{d}\\sum_{i=1}^{d}\\|\\pmb{x}_{i}\\|^{2}\\leq\\|\\frac{1}{d}\\sum_{i=1}^{d}\\pmb{x}_{i}\\|^{2})}\\end{array}$ \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Since KL divergence satisfies $\\mathrm{KL}(\\pmb{x}_{i}\\|\\pmb{u}_{i})\\geq\\|\\pmb{x}_{i}-\\pmb{u}_{i}\\|_{1}^{2}$ (Pinsker's inequality), Theorem 3.2 can be directly derived from Theorem B.1. Next, we propose Proposition B.2 and provide related proof. Proposition B.2. We denote $\\textstyle N=\\sum_{i=1}^{d}n_{i}$ and let a smooth vector-valued function $\\pmb{F}:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{\\ell}$ satisfies: ", "page_idx": 17}, {"type": "text", "text": "2. For any positive integer $k$ greater than $K$ \uff0c $\\|D^{\\alpha}F\\|_{\\infty}\\leq\\gamma^{k}$ with $|\\alpha|=k$ uniformly over $\\mathcal{X}$ \uff0c with a positive constant $\\gamma$ and a positive integer $K$ then $\\pmb{F}$ satisfies Assumption 3.3. ", "page_idx": 17}, {"type": "text", "text": "Proof of Proposition B.2. For any $k\\in\\mathbb{Z}_{+}$ and $j\\in[1:l]$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{k,y}^{F(j)}(x)\\leq\\sum_{i=0}^{k}\\sum_{|\\alpha|=i}\\frac{\\gamma^{i}}{\\alpha!}\\cdot(|x|+|y|)^{\\alpha}=\\sum_{i=0}^{k}\\frac{\\left[\\gamma(d+\\|y\\|_{1})\\right]^{i}}{i!}\\leq\\exp\\{\\gamma(d+\\|y\\|_{1})\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "using the fact that $\\|D^{\\alpha}F(y)\\|_{\\infty}\\,\\leq\\,\\gamma^{k}$ for any $k\\in\\mathbb{Z}_{+}$ and $|\\alpha|\\,=\\,k$ . In addition, by the Taylor expansion of $F(j)$ with Lagrange remainder formula for any $j\\in[1:l]$ and $k>1$ , we can obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|R_{k,y}^{F(j)}(x)\\right|=\\left|\\sum_{|\\alpha|=k}\\frac{D^{\\alpha}F(j)(y+t(x-y))}{\\alpha!}(x-y)^{\\alpha}\\right|\\leq\\frac{[\\gamma(d+\\|y\\|_{1})]^{k}}{k!},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $t\\,\\in\\,[0,1]$ depends on $\\textstyle F(j),x$ and $\\textit{\\textbf{y}}$ . Letting $k_{0}\\,=\\,\\lceil3\\gamma(d+\\|y\\|_{1})\\rceil$ and supposing $k\\ge$ $\\begin{array}{r}{k_{0}\\left(1+\\frac{\\log(1+\\gamma(d+\\|\\pmb{y}\\|_{1}))}{\\log(3/2)}\\right)}\\end{array}$ , we derv that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{[\\gamma(d+\\lVert\\pmb{y}\\rVert_{1})]^{k}}{k!}\\leq3^{k_{0}-k}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, in the light of Eq. (24), Eq. (25) and Eq. (26), it's direct to derive that $\\pmb{F}$ statisfies Assumption 3 with $\\begin{array}{r}{K_{0}\\,=\\,k_{0}\\,\\Big(1+\\frac{\\log(1+\\gamma(d+\\|\\pmb{y}\\|_{1}))}{\\log(3/2)}\\Big)}\\end{array}$ $\\textstyle\\theta\\,=\\,{\\frac{1}{3}}$ $\\Theta_{1}\\,=\\,3^{k_{0}}$ and $\\Theta_{2}\\,=\\,\\exp\\{\\gamma(d+$ $\\|\\pmb{y}\\|_{1})\\}$ \u53e3 ", "page_idx": 18}, {"type": "text", "text": "The following remark discusses the reasonability of Proposition B.2 conditions, which supports the reasonability of Assumption 3.3. ", "page_idx": 18}, {"type": "text", "text": "Remark B.3. Since region $\\begin{array}{r}{\\mathcal{X}=\\prod_{i=1}^{d}\\Delta_{n_{i}}}\\end{array}$ is bounded, it's reasonable to assume that the growth rate of the upper bound of internal function's high-order derivatives is not faster than linear growth rate. For example, the upper bounds of high-order derivatives of $\\sin(C x)$ \uff0c $\\cos(C x)$ and $\\exp\\bar{\\{}C x\\}$ have linear growth rate over $\\mathcal{X}$ for fixed constant $C$ . Therefore, if the internal function $\\pmb{F}$ can be generated by the linear combination of $\\{\\sin(C_{k}{\\pmb x})\\}_{k=1}^{K}$ and $\\{\\cos(C_{k}{\\mathbfit{x}})\\}_{k=1}^{K}$ (or $\\{\\exp\\{C_{k}{\\pmb x}\\}\\}_{k=1}^{K})$ With finite $K$ \uff0c $\\pmb{F}$ satisfies Assumption 3.3 by using Proposition B.2. ", "page_idx": 18}, {"type": "text", "text": "B.1Proof of Theorem 3.5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We briefly introduce our techniques to make the proof of Theorem 3.5 more comprehensible in this part. Our proof consists of two ingredients. The first is applying Lemma B.4 to construct a variant upper bound of average function eror $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=1}^{T}(f(\\pmb{x}^{t})~-~\\bar{f}(\\pmb{x}^{*}))}\\end{array}$ that is different from the upper bound derived from the classical OMD algorithm. This bound is composed of a) $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{1}{\\eta T}\\right)}\\end{array}$ invariant ero and b) weighted sum of the variance for fnite diffrence sequence $\\{(D_{1}F_{i}({\\pmb x}^{t-1})\\}_{t=1}^{T}$ and $\\{(D_{0}F_{i}({\\pmb x}^{t-1})\\}_{t=1}^{T}$ over $i\\;\\in\\;[1\\;:\\;d]$ , which has the form of $\\begin{array}{r}{\\sum_{i=1}^{d}\\frac{1}{\\gamma_{i}}[\\frac{\\mathcal{O}(1)}{T}\\sum_{t=1}^{T}\\mathrm{Var}_{{\\pmb x}_{i}^{t}}(D_{1}{\\pmb F}_{i}(\\pmb x^{t-1}))-\\frac{\\mathcal{O}(1)}{T}\\sum_{t=1}^{T}\\mathrm{Var}_{{\\pmb x}_{i}^{t}}({\\pmb F}_{i}(\\pmb x^{t-1}))]}\\end{array}$ . The second is applying Lemma D.7 (refer to it as control lemma) on each $\\{{\\pmb F}_{i}({\\pmb x}^{t})\\}_{t=1}^{T}$ to bound (b) by a quantity that grows poly-logarithmically in $T$ . Therefore, it's necessary to leverage Theorem B.5 and Lemma B.7 to show that every sequence $\\left\\{{\\pmb F}_{i}({\\pmb x}^{t})\\right\\}_{t=0}^{T}$ outputted by Algorithm 1 satisfies the preconditions of Lemma D.7. ", "page_idx": 18}, {"type": "text", "text": "B.1.1 Part I ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The next Lemma B.4 provides a variant convergence proof of the OMD algorithm. In this Lemma, basing on KL divergence, an explicit expression for the optimal solution of the OMD sub-problem is utilized to provide an upper bound of $\\begin{array}{r}{\\sum_{t=1}^{T}\\left(f(\\pmb{x}^{t})-f(\\pmb{x}^{*})\\right)}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Lemma B.4. Suppose $\\left\\|F(\\mathbf{x})\\right\\|_{\\infty}\\leq\\Theta\\left(\\Theta\\geq1\\right)$ for any $x\\in\\mathscr{X}$ and policy set $\\{\\mathbf{\\boldsymbol{x}}^{t}\\}_{t=1}^{T}$ follows the iteration of Algorithm $^{\\,l}$ with step size $\\begin{array}{r}{\\eta\\in(0,\\frac{1}{32\\Theta})}\\end{array}$ . Then, it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left(f({\\mathbf x}^{t})-f({\\mathbf x}^{*})\\right)\\leq\\displaystyle\\sum_{i=1}^{d}\\frac{1}{\\gamma_{i}}\\left[\\frac{\\log(n_{i})}{\\eta}+\\hat{g}_{1}(\\eta\\Theta)\\eta\\Theta^{2}\\sum_{t=1}^{T}\\mathrm{Var}_{x_{i}^{t}}\\left(F_{i}({\\mathbf x}^{t})-F_{i}(x^{t-1})\\right)\\right.}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\left.-\\hat{g}_{2}(\\eta\\Theta)\\eta\\Theta^{2}\\sum_{t=1}^{T}\\mathrm{Var}_{x_{i}^{t}}\\!(F_{i}(x^{t-1}))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{g}_{1}(\\eta):=\\frac{1}{2}+64\\left(\\frac{1}{3(1-16\\eta)}+2\\right)\\eta}\\end{array}$ and $\\begin{array}{r}{\\hat{g}_{2}(\\eta):=\\frac{1}{2}-16\\left(\\frac{1}{3(1-16\\eta)}+2\\right)\\eta.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. As claimed by Definition 3.1, we have the following estimation ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(f(\\pmb{x}^{t})-f(\\pmb{x}^{*}))\\leq\\sum_{i=1}^{d}\\frac{1}{\\gamma_{i}}\\sum_{t=1}^{T}\\left\\langle\\pmb{F}_{i}(\\pmb{x}^{t}),\\pmb{x}_{i}^{t}-\\pmb{x}_{i}^{*}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the following, considering a fixed $i\\in[1:d]$ , it's easy to obtain that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle F_{i}(x^{t}),x_{i}^{t}-x_{i}^{*}\\rangle=\\underbrace{\\langle F_{i}(x^{t})-F_{i}(x^{t-1}),x_{i}^{t}-g_{i}^{t}\\rangle}_{\\mathbb{Z}}+\\underbrace{\\langle F_{i}(x^{t-1}),x_{i}^{t}-g_{i}^{t}\\rangle}_{\\mathbb{Z}\\mathbb{Z}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\underbrace{\\langle F_{i}(x^{t}),g_{i}^{t}-x_{i}^{*}\\rangle}_{\\mathbb{Z}\\mathbb{Z}\\mathbb{Z}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall the update of Algorithm 1 can be devided into two parts: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad g_{i}^{t}=\\arg\\operatorname*{min}_{g_{i}\\in\\Delta_{n_{i}}}\\eta\\left\\langle F_{i}(\\pmb{x}^{t}),g_{i}\\right\\rangle+\\mathrm{KL}(\\pmb{g}_{i}\\|\\pmb{g}_{i}^{t-1}),}\\\\ &{\\pmb{x}_{i}^{t+1}=\\arg\\operatorname*{min}_{\\pmb{x}_{i}\\in\\Delta_{n_{i}}}\\eta\\left\\langle F_{i}(\\pmb{x}^{t}),\\pmb{x}_{i}\\right\\rangle+\\mathrm{KL}(\\pmb{x}_{i}\\|\\pmb{g}_{i}^{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "forany $i\\in[1:d]$ Where $g_{i}^{0}\\propto{\\pmb x}_{i}^{0}\\cdot\\exp\\{\\eta({\\pmb F}_{i}({\\pmb x}^{0})-{\\pmb F}_{i}({\\pmb x}^{-1}))\\}$ and $\\begin{array}{r}{\\pmb{x}_{i}^{-1}=\\pmb{x}_{i}^{0}=\\left(\\frac{1}{n_{i}},\\pmb{\\cdot}\\cdot\\cdot\\mathbf{\\nabla},\\frac{1}{n_{i}}\\right)^{\\top}}\\end{array}$ According to Cauchy-Schwarz inequality, we can evaluate $\\mathcal{T}$ asfollows ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{Z}\\leq\\left\\|g_{i}^{t}-x_{i}^{t}\\right\\|_{x_{i}^{t}}^{*}\\cdot\\sqrt{\\operatorname{Var}_{x_{i}^{t}}\\left(F_{i}(x^{t})-F_{i}(x^{t-1})\\right)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In addition, utilizing the result of Lemma D.2, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}\\mathcal{Z}=\\displaystyle\\frac{1}{\\eta}\\left[\\mathrm{KL}\\left(g_{i}^{t}||g_{i}^{t-1}\\right)-\\mathrm{KL}\\left(g_{i}^{t}||x_{i}^{t}\\right)-\\mathrm{KL}\\left(x_{i}^{t}||g_{i}^{t-1}\\right)\\right],}\\\\ {\\mathcal{Z}\\mathcal{Z}\\mathcal{Z}=\\displaystyle\\frac{1}{\\eta}\\left[\\mathrm{KL}\\left(x_{i}^{*}||g_{i}^{t-1}\\right)-\\mathrm{KL}\\left(x_{i}^{*}||g_{i}^{t}\\right)-\\mathrm{KL}\\left(g_{i}^{t}||g_{i}^{t-1}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, by applying Eq. (32), (33) and (34) into Eq. (29), we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle F_{i}(x^{t}),x_{i}^{t}-x_{i}^{*}\\rangle\\leq\\frac{1}{\\eta}\\mathrm{KL}(x_{i}^{*}\\|g_{i}^{0})+\\sum_{t=1}^{T}\\|g_{i}^{t}-x_{i}^{t}\\|_{x_{i}^{t}}^{*}\\cdot\\sqrt{\\mathrm{Var}_{x_{i}^{t}}\\left(F_{i}(x^{t})-F_{i}(x^{t-1})\\right)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\displaystyle\\frac{1}{\\eta}\\sum_{t=1}^{T}\\mathrm{KL}(g_{i}^{t}\\|x_{i}^{t})-\\displaystyle\\frac{1}{\\eta}\\sum_{t=1}^{T}\\mathrm{KL}(x_{i}^{t}\\|g_{i}^{t-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since there is a vector $\\boldsymbol{F}_{i}(\\boldsymbol{x}^{t})-\\boldsymbol{F}_{i}(\\boldsymbol{x}^{t-1})$ such that for any $j\\in[1:n_{i}]$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\pmb g}_{i}^{t}(j)=\\frac{{\\pmb x}_{i}^{t}(j)\\exp\\left\\{\\eta\\left({\\pmb F}_{i}(j)({\\pmb x}^{t})-{\\pmb F}_{i}(j)({\\pmb x}^{t-1})\\right)\\right\\}}{\\sum_{j^{\\prime}=1}^{n_{i}}{\\pmb x}_{i}^{t}(j^{\\prime})\\exp\\left\\{\\eta\\left({\\pmb F}_{i}(j^{\\prime})({\\pmb x}^{t})-{\\pmb F}_{i}(j^{\\prime})({\\pmb x}^{t-1})\\right)\\right\\}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[1:d]}\\left\\|\\frac{g_{i}^{t}}{x_{i}^{t}}\\right\\|_{\\infty}\\le\\exp\\{2\\eta\\left\\|F_{i}(x^{t})-F_{i}(x^{t-1})\\right\\|_{\\infty}\\}\\le\\exp\\{4\\eta\\Theta\\}\\le1+8\\eta\\Theta,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[1:d]}\\left\\|\\frac{x_{i}^{t}}{g_{i}^{t-1}}\\right\\|_{\\infty}\\le\\exp\\{2\\eta\\|F_{i}(x^{t-1})\\|_{\\infty}\\}\\le\\exp\\{2\\eta\\Theta\\}\\le1+4\\eta\\Theta,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with combining Eq. (31) and choosing proper $\\eta$ such that $\\begin{array}{r}{\\eta\\Theta\\le\\frac{1}{4}}\\end{array}$ . According to Lemma D.3, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{KL}(g_{i}^{t}||x_{i}^{t})\\geq\\left(\\frac{1-8\\eta\\Theta}{2}-\\frac{16\\eta\\Theta}{3(1-8\\eta\\Theta)}\\right)\\chi^{2}(g_{i}^{t},x_{i}^{t}),}\\\\ &{\\mathrm{KL}(x_{i}^{t}||g_{i}^{t-1})\\geq\\left(\\frac{1-4\\eta\\Theta}{2}-\\frac{8\\eta\\Theta}{3(1-4\\eta\\Theta)}\\right)\\chi^{2}(x_{i}^{t},g_{i}^{t-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for any $i\\in[1:d]$ . Noting that $\\chi^{2}(\\rho,\\mu)=\\left(\\left\\|\\rho-\\mu\\right\\|_{\\mu}^{*}\\right)^{2}$ , in the light of Lemma D.4, we derive that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathcal{X}^{2}(g_{i}^{t},x_{i}^{t})\\leq\\left(1+32\\left(\\frac{1}{3(1-16\\eta\\Theta)}+2\\right)\\eta\\Theta\\right)(\\eta\\Theta)^{2}\\mathrm{Var}_{x_{i}^{t}}\\left(F_{i}(x^{t})-F_{i}(x^{t-1})\\right),}\\\\ &{\\displaystyle\\mathcal{X}^{2}(g_{i}^{t},x_{i}^{t})\\geq\\left(1-32\\left(\\frac{1}{3(1-16\\eta\\Theta)}+2\\right)\\eta\\Theta\\right)(\\eta\\Theta)^{2}\\mathrm{Var}_{x_{i}^{t}}\\left(F_{i}(x^{t})-F_{i}(x^{t-1})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as long as $\\begin{array}{r}{\\eta\\Theta\\le\\frac{1}{32}}\\end{array}$ . There exists a similar lower bound with respect to $\\mathcal{X}^{2}(\\pmb{x}_{i}^{t},\\pmb{g}_{i}^{t-1})$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{X}^{2}({\\pmb x}_{i}^{t},{\\pmb g}_{i}^{t-1})\\geq\\left(1-16\\left(\\frac{1}{3(1-8\\eta\\Theta)}+2\\right)\\eta\\Theta\\right)(\\eta\\Theta)^{2}\\mathrm{Var}_{{\\pmb g}_{i}^{t-1}}(F_{i}({\\pmb x}^{t-1}))}\\\\ &{\\qquad\\qquad\\geq\\left(1-16\\left(\\frac{1}{3(1-8\\eta\\Theta)}+2\\right)\\eta\\Theta\\right)(\\eta\\Theta)^{2}\\exp\\{-2\\eta\\Theta\\}\\mathrm{Var}_{{\\pmb x}_{i}^{t}}(F_{i}({\\pmb x}^{t-1}))}\\\\ &{\\qquad\\qquad\\geq\\left(1-16\\left(\\frac{1}{3(1-8\\eta\\Theta)}+3\\right)\\eta\\Theta\\right)(\\eta\\Theta)^{2}\\mathrm{Var}_{{\\pmb x}_{i}^{t}}(F_{i}({\\pmb x}^{t-1})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (a) is derived from $\\exp\\{-2\\eta\\Theta\\}\\geq1-4\\eta\\Theta$ for any $\\begin{array}{r}{\\eta\\Theta\\le\\frac{1}{32}}\\end{array}$ . Relying on Eq. (35), Eq. (38)- (40), we conclude that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{Y}{t=1};\\;\\langle F_{i}(\\pi^{t}),x_{i}^{t}-x_{i}^{*}\\rangle}\\\\ &{\\leq\\frac{\\log(n_{i})}{\\eta}+\\left(1+32\\left(\\frac{1}{3(1-16\\eta\\Theta)}+2\\right)\\eta\\Theta\\right)\\eta\\Theta^{2}\\underset{t=1}{\\overset{T}{\\sum}}\\mathrm{Var}_{z_{i}}\\left(F_{i}(\\pi^{t})-F_{i}(\\alpha^{t-1})\\right)}\\\\ &{\\quad-\\left(\\frac{1}{2}-\\left(\\frac{32}{3(1-16\\eta\\Theta)}+36\\right)\\eta\\Theta\\right)\\eta\\Theta^{2}\\underset{t=1}{\\overset{T}{\\sum}}\\mathrm{Var}_{z_{i}}\\left(F_{i}(\\pi^{t})-F_{i}(\\alpha^{t-1})\\right)}\\\\ &{\\quad-\\left(\\frac{1}{2}-\\left(\\frac{16}{3(1-8\\eta\\Theta)}+2\\tau\\right)\\eta\\Theta\\right)\\eta\\Theta^{2}\\underset{t=1}{\\overset{T}{\\sum}}\\mathrm{Var}_{z_{i}}(F_{i}(\\pi^{t-1}))}\\\\ &{\\leq\\frac{\\log(n_{i})}{\\eta}+\\left(\\frac{1}{2}+64\\left(\\frac{1}{3(1-16\\eta\\Theta)}+2\\right)\\eta\\Theta\\right)\\eta\\Theta^{2}\\underset{t=1}{\\overset{T}{\\sum}}\\mathrm{Var}_{z_{i}}\\left(F_{i}(\\pi^{t})-F_{i}(\\alpha^{t-1})\\right)}\\\\ &{\\quad-\\left(\\frac{1}{2}-16\\left(\\frac{1}{3(1-16\\eta\\Theta)}+2\\right)\\eta\\Theta\\right)\\eta\\Theta^{2}\\underset{t=1}{\\overset{T}{\\sum}}\\mathrm{Var}_{z_{i}}\\left(F_{i}(\\pi^{t-1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, applying the estimation Eq. (41) to Eq. (28), we complete the proof. ", "page_idx": 20}, {"type": "text", "text": "B.1.2 Part II ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Basing on the conclusion of Lemma B.4, if the finite sum of $\\operatorname{Var}_{\\pmb{x}_{i}^{t}}(\\pmb{F}_{i}(\\pmb{x}^{t})-\\pmb{F}_{i}(\\pmb{x}^{t-1}))$ can be controlled by the finite sum of $\\operatorname{Var}_{\\pmb{x}_{i}^{t}}(\\pmb{F}_{i}(\\pmb{x}^{t-1}))$ with a $\\mathcal{O}(\\mathrm{poly}(\\log(T)))$ constant for each $i\\in[1:d]$ the final convergence result can be obtained directly. Hence, to demonstrate this relationship, we require the assistance of auxiliary Lemma D.7. Our initial step is to prove that $\\pmb{F}_{i}(\\pmb{x}^{t})$ satisfies the first condition in Lemma D.7 for any $i\\in[1:d]$ ", "page_idx": 20}, {"type": "text", "text": "Theorem B.5. Assuming $f$ satisfies GQC condition and Assumption 3.3 holds, $\\pmb{x}^{t}$ follows the iteration of Algorihm $^{\\,l}$ we set $\\begin{array}{r}{\\beta\\in\\left(0,\\frac{1}{(\\Theta_{1}+\\Theta_{2}+1)(H+3)}\\right)}\\end{array}$ \uff0c $\\Gamma\\geq e^{2}+322560\\Theta_{2}$ \uff0c $\\hat{K}\\geq\\operatorname*{max}\\{K_{0},$ $\\frac{H\\log(4\\beta^{-1})\\!+\\!\\log(\\Theta_{1})}{\\log(\\theta^{-1})}\\Bigr\\}$ \uff0cdh $\\begin{array}{r}{\\eta\\,=\\,\\frac{\\beta}{6e^{3}\\hat{K}\\Gamma\\,\\mathrm{max}\\{\\Theta,1\\}}}\\end{array}$   \nrespect to $\\{{\\pmb F}_{i}({\\pmb x}^{t})\\}_{i=1}^{d}$ holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[1:d]}\\left\\|(D_{h}\\pmb{F}_{i}(\\pmb{x}))^{t_{0}}\\right\\|_{\\infty}\\leq\\beta^{h}h^{3h+1},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for all $h\\in[1:H]$ and $t_{0}\\in[0:T-h]$ .Without loss of generality, we require that $H$ doesnot exceed $T$ ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem B.5. According to the Taylor expansion of each component $k$ of $\\pmb{F}_{i}$ at $\\textit{\\textbf{y}}$ ,one can noticethat ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|(D_{h}F_{i}(k)(x))^{t_{0}}\\right|\\leq\\sum_{j=0}^{\\tilde{K}}\\sum_{|\\alpha|=j}\\frac{|D^{\\alpha}F_{i}(k)(y)|}{\\alpha!}\\left|(D_{h}(x-y)^{\\alpha})^{t_{0}}\\right|+\\left|\\left(D_{h}R_{\\hat{K},y}^{F_{i}(k)}(x)\\right)^{t_{0}}\\right|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $\\hat{K}\\in\\mathbb{Z}_{+}$ . Therefore, setting H log(4-1)-1og(0) K0 and combining teremark Eq. (15) of operator $D_{h}$ in Appendix A.2, we can guarantee the validity of the following estimation ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\left(D_{h}R_{\\hat{K},y}^{F_{i}(k)}(\\pmb{x})\\right)^{t_{0}}\\right|\\leq2^{h}\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}\\left|R_{\\hat{K},y}^{F_{i}(k)}(\\pmb{x})\\right|\\leq\\Theta_{1}2^{h}\\theta^{\\hat{K}}\\leq\\frac{1}{2}\\beta^{H}\\leq\\frac{1}{2}\\beta^{H}h^{B h+1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $h\\in[1:H]$ . Moreover, as stated by Assumption 3.3, we obtain max i\u2208[1:d] $\\left\\|\\pmb{F}_{i}(\\pmb{x})\\right\\|_{\\infty}\\leq\\Theta_{1}+\\Theta_{2}$ for any $\\pmb{x}\\in\\mathcal{X}$ . Suppose that $\\operatorname*{max}_{i\\in[1:d]}\\|(D_{h^{\\prime}}F_{i}(\\pmb{x}))^{t_{0}}\\|_{\\infty}\\leq\\beta^{h^{\\prime}}h^{\\prime B h^{\\prime}+1}$ holds for any $h^{\\prime}\\in[1:h]$ and $t_{0}\\in[0:T-h^{\\prime}]$ ,we deduce ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|(D_{h+1}F_{i}(k)(x))^{t_{0}}\\right|\\leq\\!g(\\Gamma)\\beta^{h+1}(h+1)^{B(h+1)+1}P_{\\hat{K},y}^{F_{i}(k)}(x^{t_{0}})+\\frac{1}{2}\\beta^{h+1}(h+1)^{B(h+1)+1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left(\\frac{1}{2}+g(\\Gamma)\\Theta_{2}\\right)\\beta^{h+1}(h+1)^{B(h+1)+1}\\leq\\beta^{h+1}(h+1)^{B(h+1)+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "by using Lemma B6 with $\\begin{array}{r}{p({\\pmb x}):=\\frac{|D^{\\alpha}F_{i}(k)({\\pmb y})|}{\\alpha!}({\\pmb x}-{\\pmb y})^{\\alpha}}\\end{array}$ and te faet that $g(\\Gamma)\\Theta_{2}\\,\\leq\\,\\frac{1}{2}$ (Which can be derived from Lemma D.1). Therefore, to apply mathematical induction, it suffices to prove that m $\\operatorname{ax}_{1:d}\\|(D_{h^{\\prime}}F_{i}(\\pmb{x}))^{t_{0}}\\|_{\\infty}\\,\\leq\\,\\beta^{h^{\\prime}}h^{\\prime B h^{\\prime}+1}$ holds when $h^{\\prime}\\,=\\,1$ Observe that Lemma ie[   \nB.6 holds in the case $h\\,=\\,0$ . Thus, we can obtain Eq. (45) for $h\\,=\\,0$ as well.Hence,we have max $_\\mathrm{\\Omega}_{\\mathrm{{J}}}\\left\\|(D_{1}{\\cal{F}}_{i}({\\pmb x}))^{t_{0}}\\right\\|_{\\infty}\\leq\\beta$ \u53e3 $i\\!\\in\\![1;\\!d]$ ", "page_idx": 21}, {"type": "text", "text": "The proof of Theorem B.5 relies on the next Lemma B.6. ", "page_idx": 21}, {"type": "text", "text": "Lemma B.6. Assume $\\operatorname*{max}_{i\\in[1:d]}\\|F_{i}(\\pmb{x})\\|_{\\infty}\\,\\leq\\,\\Theta$ for any $\\pmb{x}\\in\\mathcal{X}$ and each element in $\\pmb{u}_{t}$ belongs to neofdbidiseadgo $^{\\,l}$ with $\\begin{array}{r}{\\eta\\leq\\frac{\\beta}{6e^{3}\\Gamma\\hat{K}\\operatorname*{max}\\{\\Theta,1\\}}}\\end{array}$ for some T > 1,Kk \u2265 K in iteration t, and consider positive constants B \u2265 3, \u03b2 e (0, (o+1)(H+3) and polynomial function $\\begin{array}{r}{p(\\pmb{u})\\,:=\\,C\\prod_{k=1}^{K}(\\pmb{u}(k)\\,-\\,\\pmb{y}(k))}\\end{array}$ where $\\pmb{u}:=(\\pmb{u}(1),\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{u}(K))^{\\top}$ and $\\pmb{y}:=(\\pmb{y}(1),\\cdot\\cdot\\cdot\\,,\\pmb{y}(K))^{\\top}\\in\\mathbb{R}^{K}$ is a fixed point. Given $h\\in[1:H-1]$ we derive that ", "page_idx": 21}, {"type": "equation", "text": "$$\n|(D_{h+1}p(\\pmb{u}))^{t_{0}}|\\le g(\\Gamma)C\\prod_{k=1}^{K}({u}^{t_{0}}(k)+|\\pmb{y}(k)|)\\beta^{h+1}(h+1)^{B(h+1)+1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "if thecondition m $\\underset{1:d]}{\\operatorname{ax}}\\,\\|(D_{h^{\\prime}}\\boldsymbol{F}_{i}(\\boldsymbol{x}))^{t_{0}}\\|_{\\infty}\\,\\le\\,\\beta^{h^{\\prime}}h^{\\prime B h^{\\prime}+1}$ holdsfor any $h^{\\prime}\\in[1:h]$ and $t_{0}\\in[0:$ i\u2208[   \n$T-h^{\\prime}]$ ", "page_idx": 21}, {"type": "text", "text": "Proof. Drawing on the premises outlined in the lemma, we assume that each $\\pmb{u}(k)$ corresponds to a unique $\\pmb{x}^{i(k)}(j(k))$ . According to the iteration of Algorithm 1, we can obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\!\\!\\!u^{t+1}(k)=\\!\\!\\frac{\\mathbf{x}_{i(k)}^{t}(j(k))\\cdot\\exp\\big\\{\\eta\\cdot\\big(2\\mathbf{F}_{i(k)}(j(k))\\left(x^{t}\\right)-\\mathbf{F}_{i(k)}(j(k))\\left(x^{t-1}\\right)\\big)\\big\\}}{\\sum_{j=1}^{n_{i(k)}}\\mathbf{x}_{i(k)}^{t}(j)\\cdot\\exp\\big\\{\\eta\\cdot\\big(2\\mathbf{F}_{i(k)}(j)\\left(x^{t}\\right)-\\mathbf{F}_{i(k)}(j)\\left(x^{t-1}\\right)\\big)\\big\\}},}\\\\ &{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $k\\in[1:K]$ and $t\\in[1:T-1]$ . Given the sequence $\\mathbf{\\boldsymbol{x}}^{1},\\cdots,\\mathbf{\\boldsymbol{x}}^{t_{0}+h}$ generated by Algorithm 1, it is straightforward to derive that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{u}^{t_{0}+t+1}(k)=(N_{\\pmb{u}}^{k})^{-1}\\pmb{u}^{t_{0}}(k)\\cdot\\exp\\{\\eta\\cdot(F_{i(k)}(j(k))(\\pmb{x}^{t_{0}+t})+\\displaystyle\\sum_{t^{\\prime}=0}^{t}F_{i(k)}(j(k))(\\pmb{x}^{t_{0}+t^{\\prime}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-F_{i(k)}(j(k))(\\pmb{x}^{t_{0}-1}))\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $k\\in[1:K]$ \uff0c $t_{0}\\in[1:T-h-1]$ and $t\\in[1:h]$ where $\\begin{array}{r}{N_{u}^{k}=\\sum_{j=1}^{n_{i(k)}}\\pmb{x}_{i(k)}^{t_{0}}(j)\\cdot\\exp\\{\\eta\\cdot}\\end{array}$ $\\begin{array}{r}{(F_{i(k)}(j)({x}^{t_{0}+t})+\\sum_{t^{\\prime}=0}^{t}F_{i(k)}(j)({x}^{t_{0}+t^{\\prime}})-F_{i(k)}(j)({x}^{t_{0}-1}))\\}.}\\end{array}$ We write ", "page_idx": 22}, {"type": "equation", "text": "$$\nr_{t_{0},k}^{t}:=F_{i(k)}(\\pmb{x}^{t_{0}+t-1})+\\sum_{t^{\\prime}=0}^{t-1}F_{i(k)}(\\pmb{x}^{t_{0}+t^{\\prime}})-F_{i(k)}(\\pmb{x}^{t_{0}-1}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Also, for a vector $z\\in\\mathbb{R}^{n_{i(k)}}$ and an index $j\\in\\left[1:n_{i(k)}\\right]$ , define ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\psi_{t_{0},k}^{j}(z)=\\frac{\\exp\\{z(j)\\}}{\\sum_{j^{\\prime}=1}^{n_{i(k)}}{\\pmb x}_{t_{0}}^{i(k)}(j^{\\prime})\\cdot\\exp\\{z(j^{\\prime})\\}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "so that $\\begin{array}{r}{\\boldsymbol{u}^{t_{0}+t}(\\boldsymbol{k})=\\boldsymbol{x}_{i(\\boldsymbol{k})}^{t_{0}}(j(\\boldsymbol{k}))\\cdot\\boldsymbol{\\psi}_{t_{0},\\boldsymbol{k}}^{j(\\boldsymbol{k})}\\left(\\eta\\boldsymbol{r}_{t_{0},\\boldsymbol{k}}^{t}\\right)=\\boldsymbol{u}^{t_{0}}(\\boldsymbol{k})\\cdot\\boldsymbol{\\psi}_{t_{0},\\boldsymbol{k}}^{j(\\boldsymbol{k})}\\left(\\eta\\boldsymbol{r}_{t_{0},\\boldsymbol{k}}^{t}\\right)}\\end{array}$ 0 $t\\geq1$ For covnience, we denote that $\\mathcal{D}:=\\left\\{\\alpha\\in\\mathbb{N}^{K}|\\alpha(i)\\in\\{0,1\\},\\forall\\,i\\in[1:K]\\right\\}$ and $e:=(1,\\cdot\\cdot\\cdot\\,,1)\\in\\mathbb{N}^{K}$ . In particular, for any $\\alpha\\in{\\mathcal{D}}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big|\\big(D_{h^{\\prime}}(u^{e-\\alpha})\\big)^{t_{0}}\\Big|\\leq(u^{t_{0}})^{e-\\alpha}\\underbrace{\\Big|\\big(D_{h^{\\prime}}(\\psi_{t_{0}}(\\eta r_{t_{0}}))^{e-\\alpha}\\big)^{0}\\Big|}_{\\mathcal{Z}(\\alpha,h^{\\prime},t_{0})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\psi_{t_{0}}(\\eta r_{t_{0}}^{t}):=\\Big(\\psi_{t_{0},1}^{j(1)}(\\eta r_{t_{0},1}^{t}),\\cdot\\cdot\\cdot\\cdot,\\psi_{t_{0},K}^{j(K)}(\\eta r_{t_{0},K}^{t})\\Big),h^{\\prime}\\in[1:h+1]$ and $t_{0}\\in[1:T\\!-h\\!-\\!1]$ $\\psi_{t_{0},k}^{j(k)}(\\eta\\mathbf{r}_{t_{0},k}^{t})$ Notice that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(D_{1}\\pmb{r}_{t_{0},k}\\right)^{t}=2(E_{1}\\pmb{F}_{i(k)}(\\pmb{x}))^{t_{0}+t-1}-\\pmb{F}_{i(k)}(\\pmb{x}^{t_{0}+t-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $t\\in[0:h]$ . Therefore, for any $h^{\\prime}\\in[1:h+1]$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(D_{h^{\\prime}}r_{t_{0},k}\\right)^{t}=2\\left(E_{1}D_{h^{\\prime}-1}\\left(F_{i(k)}(\\pmb{x})\\right)\\right)^{t_{0}+t-1}-\\left(D_{h^{\\prime}-1}\\left(F_{i(k)}(\\pmb{x})\\right)\\right)^{t_{0}+t-1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $t\\in[0:h+1-h^{\\prime}]$ Becaus the step ise $\\eta$ satisfes $\\begin{array}{r}{\\eta\\leq\\frac{\\beta}{6e^{3}\\Gamma\\hat{K}\\operatorname*{max}\\{\\Theta,1\\}},\\left\\|(D_{0}\\eta\\pmb{r}_{t_{0}})^{t}\\right\\|_{\\infty}\\leq}\\end{array}$ $\\begin{array}{r}{\\eta H\\Theta\\,\\le\\,\\frac{1}{6e^{3}\\Gamma\\hat{K}}\\,\\,\\mathrm{and}\\,\\,\\underset{i\\in[1:d]}{\\operatorname*{max}}\\,\\left\\|\\left(D_{h^{\\prime}}F_{i}(x)\\right)^{t_{0}}\\right\\|_{\\infty}\\,\\le\\,\\beta^{h^{\\prime}}h^{\\prime B h^{\\prime}+1}}\\end{array}$ for all $h^{\\prime}\\,\\in\\,[1\\,:\\,h]$ th follwing estimation holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|(D_{h^{\\prime}+1}\\eta\\pmb{r}_{t_{0}})^{0}\\right\\|_{\\infty}\\leq\\frac{1}{2e^{2}\\Gamma\\hat{K}}\\beta^{h^{\\prime}+1}(h^{\\prime}+1)^{B(h^{\\prime}+1)},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $h^{\\prime}\\in[0:h]$ by using Eq. (53) where $\\pmb{r}_{t_{0}}^{t}:=\\left(\\pmb{r}_{t_{0},1}^{t}(j(1)),\\cdot\\cdot\\cdot\\;,\\pmb{r}_{t_{0},K}^{t}(j(K))\\right)$ . By Lemma D.5 and Lemma D.6, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}(\\alpha,h+1,t_{0})\\leq g(\\Gamma)\\beta^{h+1}(h+1)^{B(h+1)+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Noting that ", "page_idx": 22}, {"type": "equation", "text": "$$\np(\\pmb{u})=C\\sum_{i=0}^{K}(-1)^{i}\\left(\\sum_{\\alpha\\in\\mathcal{D}:\\lvert\\alpha\\rvert=i}y^{\\alpha}\\pmb{u}^{e-\\alpha}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and applying bound Eq. (55) to Eq. (50), we can derive ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|(D_{h+1}p(u))^{t_{0}}\\right|\\frac{\\kappa}{(a)}C|C|\\displaystyle\\left|\\sum_{i=0}^{K}(-1)^{i}\\left(\\displaystyle\\sum_{\\alpha\\in D\\backslash\\{\\alpha\\}=i}y^{\\alpha}\\displaystyle\\sum_{h^{\\prime}=1}^{h+1}{\\binom{h+1}{h^{\\prime}}}(-1)^{h^{\\prime}}(u^{t_{0}+h^{\\prime}})^{e-\\alpha}\\right)\\right|}&{}\\\\ &{\\leq\\!|C|\\displaystyle\\sum_{i=0}^{K}\\displaystyle\\sum_{\\alpha\\in D\\backslash\\{\\alpha\\}=i}|y|^{\\alpha}\\left|\\left(D_{h+1}\\left(u^{e-\\alpha}\\right)\\right)^{t_{0}}\\right|}\\\\ &{\\leq\\!|C|\\displaystyle\\sum_{i=0}^{K}\\displaystyle\\sum_{\\alpha\\in D\\backslash\\{\\alpha\\}=i}|y|^{\\alpha}(u^{t_{0}})^{e-\\alpha}g(\\Gamma)\\beta^{h+1}(h+1)^{B(h+1)+1}}\\\\ &{\\leq\\!g(\\Gamma)\\beta^{h+1}(h+1)^{B(h+1)+1}C\\displaystyle\\prod_{k=1}^{K}(u^{t_{0}}(k)+|y(k)|),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $t_{0}\\in[1:T-h-1]$ where (a) is derived from the equivalent expression Eq. (56) of the polynomial $p(\\pmb{u})$ and Eq. (15) of the finite difference $(D_{h}f({\\pmb x}))^{t_{0}}$ W.r.t function $f$ respectively. ", "page_idx": 22}, {"type": "text", "text": "Recalling that we set parameters as follows ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T\\ge4,\\,H:=\\lceil\\log(T)\\rceil,\\beta=\\frac{1}{8(\\Theta_{1}+\\Theta_{2}+1)H^{7/2}},\\,\\Gamma=e^{2}+322560\\Theta_{2},}\\\\ &{\\quad\\,\\hat{K}=\\operatorname*{max}\\left\\{\\frac{H\\log(4\\beta^{-1})+\\log(\\Theta_{1})}{\\log(\\theta^{-1})},K_{0}\\right\\},\\,\\eta=\\frac{\\beta}{6e^{2}\\hat{K}\\Gamma},\\,B\\ge3,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "According to Theorem B.5, we have $\\operatorname*{max}_{i\\in[1:d]}\\left\\|\\left(D_{h}\\pmb{F}_{i}(\\pmb{x})\\right)^{t}\\right\\|_{\\infty}\\leq\\beta^{h}H^{3h+1}$ for each $h\\in[0:H]$ and $t\\in[1:T-h]$ . We are now prepared to prove that $\\pmb{x}_{i}^{t}$ satisfies the second condition of Lemma D.7. Lemma B.7. The sequence $\\{\\mathbf{x}_{i}^{t}\\}_{t=1}^{T}$ which has been generated from Algorithm $^{\\,I}$ is $7\\eta(\\Theta_{1}+\\Theta_{2})-$ consecutively close when $H\\ge1$ $\\bar{\\beta_{0}}=(4H)^{-1}$ and $\\bar{\\eta}\\in(0,\\beta_{0}^{4}(\\Theta_{1}+\\bar{\\Theta}_{2}+1)^{-1}/57\\bar{7}92]$ ", "page_idx": 23}, {"type": "text", "text": "Proof. According to the iteration of Algorithm 1, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pmb{x}_{i}^{t+1}(k)=\\frac{\\pmb{x}_{i}^{t}(k)\\cdot\\exp\\{\\eta\\cdot(2\\pmb{F}_{i}(k)(\\pmb{x}^{t})-\\pmb{F}_{i}(k)(\\pmb{x}^{t-1}))\\}}{\\sum_{k^{\\prime}=1}^{n_{i}}\\pmb{x}_{i}^{t}(k^{\\prime})\\cdot\\exp\\{\\eta\\cdot(2\\pmb{F}_{i}(k^{\\prime})(\\pmb{x}^{t})-2\\pmb{F}_{i}(k^{\\prime})(\\pmb{x}^{t-1}))\\}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any $i\\in[1:d]$ and $k\\in[1:n_{i}]$ . Therefore, for any $i\\in[1:d]$ and $t\\in[1:T-1]$ , we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\left\\lVert\\frac{x_{i}^{t}}{x_{i}^{t+1}}\\right\\rVert_{\\infty},\\left\\lVert\\frac{x_{i}^{t+1}}{x_{i}^{t}}\\right\\rVert_{\\infty}\\right\\}\\le\\exp\\{6\\eta(\\Theta_{1}+\\Theta_{2})\\}\\underset{(\\mathrm{a})}{=}(1+7\\eta(\\Theta_{1}+\\Theta_{2})),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (a) is derived from the fact that $\\begin{array}{r}{\\exp(x)\\leq1+\\frac{7}{6}x}\\end{array}$ for $x\\in[0,1/24]$ ", "page_idx": 23}, {"type": "text", "text": "B.1.3 The Last Step ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "With the preparatory work for proving Theorem 3.5 is completed, we now turn to providing the final proof: ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 3.5. Applying Theorem B.5 and Lemma B.7 to Lemma D.7, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\operatorname{Var}_{x_{i}^{t}}(F_{i}(x^{t})-F_{i}(x^{t-1}))\\leq2\\beta_{0}\\sum_{t=1}^{T}\\operatorname{Var}_{x_{i}^{t}}(F_{i}(x^{t-1}))+165120\\Theta^{2}(1+7\\eta\\Theta)H^{5}+2.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "According to the result of Lemma B.4, we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(f({\\pmb x}^{t})-f({\\pmb x}^{*}))\\leq\\sum_{i=1}^{d}\\frac{1}{\\gamma_{i}}\\left[\\frac{\\log(n_{i})}{\\eta}-\\left(\\hat{g}_{2}(\\eta\\Theta)\\eta\\Theta^{2}-2\\beta_{0}\\hat{g}_{1}(\\eta\\Theta)\\eta\\Theta^{2}\\right)\\sum_{t=1}^{T}\\mathrm{Var}_{x_{i}^{t}}(F_{i}({\\pmb x}^{t}))\\right]}\\\\ {+\\,\\hat{g}_{1}(\\eta\\Theta)\\eta\\Theta^{2}\\sum_{i=1}^{d}\\frac{1}{\\gamma_{i}}[8\\beta_{0}\\Theta^{2}+165120\\Theta^{2}(1+7\\eta\\Theta)H^{5}+2].\\qquad\\qquad(62)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining Assumptions 3.3 and parameters selection Eq. (7), we complete the proof. ", "page_idx": 23}, {"type": "text", "text": "B.2  Simple Example ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide the proof of Example 3.4 which satisfies GQC condition and Assumption 3.3. ", "page_idx": 23}, {"type": "text", "text": "Proof of Example 3.4. Recalling the objective function $\\begin{array}{r}{f(\\pmb{p},\\mathbf{P})=\\frac{1}{2}\\mathbb{E}_{\\pmb{x},\\pmb{y}}(\\sum_{i=1}^{m}\\pmb{p}_{i}\\sigma(\\pmb{x}^{\\top}\\mathbf{P}_{i})-y)^{2},}\\end{array}$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(p^{*},{\\bf P})-f(p,{\\bf P})\\geq\\left\\langle F_{p}(p,{\\bf P}),p^{*}-p\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since $f(\\cdot,\\mathbf{P})$ is convex for any fixed $\\mathbf{P}$ . In addition, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f({\\pmb p}^{*},{\\bf P}^{*})-f({\\pmb p}^{*},{\\bf P})=-\\,\\frac{1}{2}\\mathbb{E}\\left[(\\sigma({\\pmb x}^{\\top}{\\bf P}_{1})-\\sigma({\\pmb x}^{\\top}{\\bf P}_{1}^{*}))^{2}\\right]}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\geq\\frac{\\mathrm{(i)}\\,B_{C}}{2}\\mathbb{E}\\left[\\langle\\sigma({\\pmb x}^{\\top}{\\bf P}_{1})-\\sigma({\\pmb x}^{\\top}{\\bf P}_{1}^{*}),{\\pmb x}^{\\top}({\\bf P}_{1}^{*}-{\\bf P}_{1})\\rangle\\right]}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=\\frac{B_{C}}{2}\\mathbb{E}\\left[(\\sigma({\\pmb x}^{\\top}{\\bf P}_{1}^{*})-y){\\pmb x},{\\bf P}_{1}^{*}-{\\bf P}_{1}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $B_{C}$ is a constant depends on $C$ and (a) is derived from the fact that $B_{C}\\;\\;\\langle\\exp\\{x_{1}\\}\\;-$ $\\mathrm{exp}\\{x_{2}\\},x_{1}-x_{2}\\rangle\\,\\geq\\,|\\,\\mathrm{exp}\\{x_{1}\\}-\\,\\mathrm{exp}\\{x_{2}\\}|^{2}$ for any $x_{1},x_{2}\\;\\in\\;[-C,C]$ .Therefore, summing up Eq. (63) and Eq. (64),_we have that $f$ satisfies GQC condition with the internal functions $\\bar{F}_{p}\\,\\bar{=}\\,\\{\\mathbb{E}[(\\sum_{j=1}^{m}\\bar{p_{j}}\\sigma(\\boldsymbol{x}^{\\intercal}\\mathbf{P}_{j})-\\boldsymbol{y})]\\sigma(\\boldsymbol{x}^{\\intercal}\\mathbf{P}_{i})\\}_{i=1}^{m}$ for block $\\pmb{p}$ and $F_{\\mathbf{P}_{i}}\\,=\\,\\mathbb{E}\\left[(\\sigma({\\pmb x}^{\\top}{\\mathbf P}_{i})-y){\\pmb x}\\right]$ for block $\\mathbf{P}_{i}$ $\\gamma_{p}\\,=\\,1$ $\\begin{array}{r}{\\gamma_{\\mathbf{P}_{1}}\\ =\\ \\frac{B_{C}}{2}}\\end{array}$ and $\\gamma_{\\mathbf{P}_{i}}\\;=\\;0$ for any $i\\neq1$ Furtermore we have $\\|D^{\\alpha}F_{p}(\\cdot)\\|_{\\infty}\\;\\leq\\;2\\exp\\{C\\}(2C)^{|\\alpha|}$ and $\\|D^{\\alpha}F_{\\mathbf{P}_{i}}(\\cdot)\\|_{\\infty}\\ \\leq\\ \\exp\\{C\\}C^{|\\alpha|+1}$ by using and $\\pmb{x}\\in[-C,C]^{d}$ . According to Proposition B.2, we complete the proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "There is also a toy example satisfying GQC condition and Assumption 3.3. ", "page_idx": 24}, {"type": "text", "text": "Example B.8. Assuming $(p_{1},p_{2})\\;\\in\\;\\Delta_{m}\\,\\times\\,\\Delta_{n}$ , the function $f(p_{1},p_{2})\\;=\\;{\\textstyle\\frac{1}{2}}\\|p_{1}p_{2}^{\\top}\\|_{\\mathrm{F}}^{2}$ satisfies GQC condition and Assumption 3.3 with the internal functions $F_{p_{1}}=\\|p_{2}\\|^{2}p_{1}$ for block $\\pmb{p}_{1}$ and $F_{p_{2}}=p_{2}$ for block $\\pmb{p}_{2}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. We have $\\begin{array}{r l}{\\frac12\\|(p_{1}^{*})^{\\top}p_{2}\\|_{F}\\;-\\;\\frac12\\|p_{1}^{\\top}p_{2}\\|_{F}}&{\\ge\\;\\;\\|p_{2}\\|^{2}p_{1}^{\\top}(p_{1}^{*}\\;-\\;p_{1})}\\end{array}$ and $\\frac{1}{2}\\|(p_{1}^{*})^{\\top}p_{2}^{*}\\|_{F}\\,-$ $\\begin{array}{r}{\\frac12\\|(\\pmb{\\{p}_{1}^{*}})^{\\top}\\pmb{p}_{2}\\|_{F}\\ge\\sqrt[]{\\|\\pmb{p}_{1}^{*}\\|^{2}\\pmb{p}_{2}^{\\top}(\\pmb{p}_{2}^{*}-\\pmb{p}_{2}^{'})}}\\end{array}$ . Therefore, we have that $f$ satisfies GQC condition with the internal functions $F_{p_{1}}=\\|p_{2}\\|^{2}p_{1}$ for block $\\pmb{p}_{1}$ and $F_{p_{2}}=p_{2}$ for block $\\pmb{p}_{2}$ . Notice that $\\gamma_{p_{1}}=1$ and $\\gamma_{p_{2}}=\\|p_{1}^{*}\\|^{2}$ . Since both $||p_{2}||^{2}p_{1}$ and $\\pmb{p}_{2}$ are polynomials with respect to $(p_{1},p_{2})$ , we derive that the internal function of $f$ satisfies Assumption 3.3. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "B.3  Application to Reinforcement Learning ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "B.3.1 Analysis of Infinite Horizon Reinforcement Learning ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Proposition 3.6. The following performance difference lemma [30, 13, 2, 35] plays an important role in the policy gradient based model of infinite horizon reinforcement learning problems, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{\\pi^{*}}(\\rho_{0})-V^{\\pi}(\\rho_{0})=\\mathbb{E}_{s\\sim d_{\\rho_{0}}^{\\pi^{*}}}\\left\\langle A^{\\pi}(s,\\cdot),\\pi^{*}(\\cdot|s)-\\pi(\\cdot|s)\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $d~=~|S|,S~=~\\{s_{i}\\}_{i=1}^{d}$ and write $1/\\gamma_{i}\\;\\;=\\;\\;{d_{\\rho_{0}}^{\\pi}}^{*}(s_{i})$ \uff0c ${\\cal F}_{i}(\\pi)\\ =\\ {\\cal Q}^{\\pi}(s_{i},\\cdot)$ \uff1aAccording to Eq. (65) whose proof is given in Cheng et al. [13] and $\\langle{\\pmb A}^{\\pi}(s_{i},\\cdot),{\\pmb\\pi}^{\\prime}(\\cdot|s_{i})-\\;\\,{\\pmb\\pi}(\\cdot|s)\\rangle\\;\\,=$ $\\langle Q^{\\pi}(s_{i},\\cdot),\\pi^{\\prime}(\\cdot|s_{i})\\bar{-}\\pi(\\cdot|s)\\rangle$ for any policy $\\pi$ and $\\pi^{\\prime}$ , we obtain that ", "page_idx": 24}, {"type": "equation", "text": "$$\n{V^{\\pi}}^{*}(\\rho_{0})-V^{\\pi}(\\rho_{0})=\\sum_{i=1}^{d}\\frac{1}{\\gamma_{i}}\\left\\langle F_{i}(\\pi),\\pi^{*}(\\cdot|s_{i})-\\pi(\\cdot|s_{i})\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Eq. (66) implies that $V^{\\pi}(\\rho_{0})$ satisfies GQC condition. For every $a\\in A$ , the Taylor expansion of $Q^{\\pi}(s_{i},a)$ up to $K$ -th order at origin is the same as its truncation at horizon $K$ , which indicates ", "page_idx": 24}, {"type": "equation", "text": "$$\nR_{K,\\mathbf{0}}^{Q^{\\pi}(s_{i},a)}(\\pi)=\\theta^{K+1}\\mathbb{E}_{s_{K+1}}[V^{\\pi}(s_{K+1})|s_{0}=s_{i},a_{0}=a]\\leq\\theta^{K+1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, according to the fact that ", "page_idx": 24}, {"type": "equation", "text": "$$\nP_{K,0}^{Q^{\\pi}(s_{i},a)}(\\pi)\\leq Q^{\\pi}(s_{i},a)\\leq1,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have that $Q^{\\pi}(s_{i},\\cdot)$ satisfies Assumption 3.3 with $\\Theta_{1}=\\theta$ $\\Theta_{2}=1$ and $K_{0}=1$ ", "page_idx": 24}, {"type": "text", "text": "B.3.2 Analysis of Finite Horizon Reinforcement Learning ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The function structure of finite horizon reinforcement learning on policy is strictly polynomial. Moreover, since the action-value functions on horizon $h$ is only dependent of policy. $\\pi_{h+1:H}$ wemay therefore verify that the objective function of finite horizon reinforcement learning satisfies GQC condition by utilizing finite difference expansion on function error $J_{1}^{\\pi}(\\pmb{\\rho}_{1})-J_{1}^{\\pi^{*}}(\\bar{\\pmb{\\rho}_{1}})$ ", "page_idx": 25}, {"type": "text", "text": "The finite horizon reinforcement learning considers the following policy optimization problem: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi\\in\\mathcal{X}}J_{1}^{\\pi}(\\rho_{1}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $J_{1}^{\\pi}(\\pmb{\\rho}_{1})=\\mathbb{E}_{s_{1}\\sim\\pmb{\\rho}_{1}}[V_{1}^{\\pi}(s_{1})]$ , and $\\mathcal{X}=\\mathcal{X}_{1}\\times\\cdot\\cdot\\cdot\\times\\mathcal{X}_{H}$ and each $\\chi_{h}$ denotes $\\lvert S_{h}\\rvert$ probability simplexes. We write Sh = {sh,sn]is=1 for any $h\\in[1:H]$ and denote the action-valuvetoron state $s_{h,i_{h}}$ at horizon $h$ by $Q_{h}^{\\pi_{h+1:H}}(s_{h,i_{h}},\\cdot)$ According to the denition of fnitehorizonal function $V_{h}^{\\pi_{h:H}}$ we obtain the obervation as Eq.. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\boldsymbol{J}_{1}^{\\pi^{*}}(\\rho_{1})-\\boldsymbol{J}_{1}^{\\pi}(\\rho_{1})=\\displaystyle\\sum_{h=1}^{H}\\left[\\boldsymbol{J}_{1}^{\\pi_{1:h}^{*},\\pi_{h+1:H}}(\\rho_{1})-\\boldsymbol{J}_{1}^{\\pi_{1:h-1}^{*},\\pi_{h:H}}(\\rho_{1})\\right]}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{s_{h}\\sim\\mathbb{E}_{s_{1}\\sim\\rho_{1}}\\boldsymbol{\\mathbf{P}}\\boldsymbol{\\mathbf{r}}_{h}^{\\pi_{1:h-1}^{*}}(\\cdot|s_{1})}\\left\\langle\\boldsymbol{Q}_{h}^{\\pi_{h+1:H}}(s_{h},\\cdot),\\pi_{h}^{*}(\\cdot|s_{h})-\\pi_{h}(\\cdot|s_{h})\\right\\rangle,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\boldsymbol{Q}_{h}^{\\pi_{h+1:H}}(\\boldsymbol{s}_{h},\\boldsymbol{a}_{h})$ is a polynomial with respect to policy $\\pi_{1:H}$ , whose value is bounded by $1+H-h$ for any $s_{h}\\in S_{h}$ and $a_{h}\\in\\mathcal{A}_{h}$ , we derive that $J_{1}^{\\pi}(\\rho_{1})$ satisfies GQC condition with intemal function ${\\cal F}_{h,i_{h}}(\\pi)\\,=\\,Q_{h}^{\\pi_{h+1:H}}(s_{h,i_{h}},\\cdot)$ for variable bock $\\boldsymbol{x}_{h,i_{h}}$ Where $i_{h}\\,\\in\\,[1\\,:\\,|S_{h}|]$ and satisfiesAssumption 3.3with $\\theta\\,=\\,0$ \uff0c $\\Theta_{1}\\,=\\,0$ \uff0c $\\Theta_{2}\\,=\\,H$ and $K_{0}\\;=\\;H$ Therefore, for finite horizon reinforcement learning, it follows from Theorem 3.5 that Algorithm 1 with parameter selection Eq. (7) finds an $\\varepsilon$ -suboptimal global solution in a number of iterations that is at most $\\mathcal{O}(H\\operatorname*{max}_{h\\in[0:H]}\\log(|\\mathcal{A}_{h}|)\\varepsilon^{-1}\\log^{4}(\\varepsilon^{-1}))$ ", "page_idx": 25}, {"type": "text", "text": "C Minimax Optimization ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We begin with showing the connection between GQCC condition and GQC condition. Without loss of generality,we assume $n_{i}=n$ and $m_{i}=m$ for any $i\\in[1:d]$ , and let $\\ell=n+m$ .If $f(\\cdot,y)$ and $-f(x,\\cdot)$ satisfy GQC condition with respect to a pair of minimizers $\\mathbf{\\boldsymbol{x}}^{*}(\\mathbf{\\boldsymbol{y}})$ and ${\\pmb y}^{*}({\\pmb x})$ ,respectively, then we have the following estimations of function error ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle f({\\pmb x},{\\pmb y})-f({\\pmb x}^{*}({\\pmb y}),{\\pmb y})\\leq\\sum_{i=1}^{d}\\frac{1}{\\gamma_{i}({\\pmb y})}\\left(f_{i}({\\bf P}({\\pmb z}),\\pmb x_{i},y_{i})-f_{i}({\\bf P}({\\pmb z}),\\pmb x^{*}({\\pmb y})_{i},y_{i})\\right),}}\\\\ {{\\displaystyle f({\\pmb x},{\\pmb y}^{*}({\\pmb x}))-f({\\pmb x},{\\pmb y})\\leq\\sum_{i=1}^{d}\\frac{1}{\\tau_{i}({\\pmb x})}\\left(f_{i}({\\bf P}({\\pmb z}),\\pmb x_{i},{\\pmb y}^{*}({\\pmb x})_{i})-f_{i}({\\bf P}({\\pmb z}),\\pmb x_{i},y_{i})\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $f_{i}(\\mathbf{Q},z_{i})=\\langle\\mathbf{Q}_{i},z_{i}\\rangle$ for any $\\mathbf{Q}\\in\\mathbb{R}^{\\ell\\times d}$ and $z_{i}\\in\\mathbb{R}^{n+m}$ , and each $\\mathbf{P}_{i}$ includes the internal function of $f(\\cdot,y)$ for varriable block $\\pmb{x}_{i}$ and the internal function of $-f(x,\\cdot)$ for variable block $\\pmb{y}_{i}$ follows from Eq. (69) and Eq. (70) that Eq. (10) holds for $f$ with $\\psi_{i}(\\pmb{\\mathscr{z}})=\\operatorname*{max}\\{1/\\gamma_{i}(\\pmb{y}),1/\\tau_{i}(\\pmb{\\mathscr{x}})\\}$ ", "page_idx": 25}, {"type": "text", "text": "C.1 Preparatory Discussion ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we provide the convergence analysis of general version of Algorithm 2, i.e., Algorithm 4. We consider the divergence-generating function $v$ with Bregman's divergence $V$ (i.e., $V(\\pmb{x},\\pmb{u})=$ $v(\\pmb{x})-v(\\pmb{u})-\\langle\\nabla v(\\pmb{u})\\bar{\\pmb{\\alpha}}-\\bar{\\pmb{u}}\\rangle$ for any $\\pmb{x},\\pmb{u})$ over general compact convex regions $\\mathcal{Z}=\\mathcal{X}\\times\\mathcal{Y}\\subset$ $\\mathbb{R}^{\\sum_{i=1}^{d}n_{i}}\\times\\mathbb{R}^{\\sum_{i=1}^{d}m_{i}}$ . Before we introduce the main theorem, we need the following assumptions: ", "page_idx": 25}, {"type": "text", "text": "Assumption C.1. There exists positive constants $A,D$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\mathbf{A}_{2}]\\;\\operatorname*{max}\\left\\{\\operatorname*{max}_{\\stackrel{z_{i}\\in{\\mathcal{Z}}_{i}}{i\\in[1:d]}}\\left(V\\left(x_{i},(g_{i}^{x})^{0}\\right)+V\\left(y_{i},(g_{i}^{y})^{0}\\right)\\right),\\operatorname*{max}_{\\stackrel{z_{i}\\in{\\mathcal{Z}}_{i}}{i\\in[1:d]}}\\left(v(x_{i})+v(y_{i})\\right)\\right\\}\\leq D.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "[A3] $v$ modulus 2with respect to $\\|\\cdot\\|$ (i.e., $\\forall i\\,\\in\\,[1\\,:\\,d],\\,V(\\pmb{x}_{i},\\pmb{u}_{i})\\,\\geq\\,\\|\\pmb{x}_{i}-\\pmb{u}_{i}\\|^{2}$ for any $\\pmb{x}_{i},\\pmb{u}_{i}\\in\\mathcal{X}_{i}$ and $V(\\pmb{y}_{i},\\pmb{w}_{i})\\geq\\|\\pmb{y}_{i}-\\pmb{w}_{i}\\|^{2}$ for any $\\pmb{{y}}_{i},\\pmb{{w}}_{i}\\in\\mathcal{D}_{i})$ ", "page_idx": 26}, {"type": "text", "text": "If we choose $\\begin{array}{r}{v(\\pmb{x})=\\sum_{j=1}^{n}\\pmb{x}(j)\\log(\\pmb{x}(j))}\\end{array}$ and $\\|\\cdot\\|=\\|\\cdot\\|_{1}$ , then (1) in Assumption C.1 holds with $A=2$ (2) in Assumption C.1 holds with $D=2\\operatorname*{max}_{i\\in[1:d]}\\{\\log(n_{i})+\\log(m_{i})\\}$ ; (3) in Assumption C.1 holds following Pinsker's inequality. According to Remark C.2, we state that there exist some compact convex regions in $\\mathbb{R}{\\sum}_{i=1}^{d}\\bar{(n_{i}{+}m_{i})}$ with proper divergence-generating function $v$ and proper choice of $g^{0}$ satisfyAssumption C.1. ", "page_idx": 26}, {"type": "text", "text": "Remark C.2. If the feasible region $\\mathcal{Z}$ is a compact set of Euclidean space, then it is reasonable that assuming the divergence-generating function $v$ (i.e., $\\begin{array}{r}{v(\\pmb{x})\\,=\\,\\sum_{j=1}^{n^{\\star}}\\pmb{x}(j)\\log(\\pmb{x}(j))}\\end{array}$ over the probability simplex or $v(\\pmb{x})=\\|\\pmb{x}\\|_{2}^{2}$ over the standard compact set) and the norm $\\Vert\\cdot\\Vert$ are uniformly bounded on every $\\mathcal{Z}_{i}$ . For some Bregman divergences, if $x^{0}$ is a fixed point, $V(\\cdot,\\pmb{x}^{0})$ can be bounded by a constant (may depend on the dimension of space) on a compact feasible region, such as $V(\\cdot,\\pmb{x}^{0})=\\|\\cdot-\\pmb{x}^{0}\\|_{2}^{2}$ With $\\mathbf{\\boldsymbol{x}}^{0}=\\mathbf{0}$ on the closed ball ${\\mathbb B}_{R}(\\mathbf{0})$ for radius $R\\in(0,\\infty)$ and $V(\\cdot,\\pmb{x}^{0})=\\mathrm{KL}(\\cdot\\|\\pmb{x}^{0})$ with $\\pmb{x}^{0}=(1/n,\\cdots\\,,1/n)$ on the probability simplex $\\Delta_{n}$ ", "page_idx": 26}, {"type": "text", "text": "Assumption C.3. In Definition 4.1, let matrix-valued function $\\mathbf{P}$ has the form of $\\mathbf{P}(\\mathbf{Q}^{z},z)$ where $\\mathbf{Q}^{z}\\in\\mathbb{R}^{\\ell\\times d}$ depends on $_{\\textit{z}}$ , and assume that $\\mathbf{P}$ satisfies the following properties on region $\\left\\{\\mathbf{Q}\\in\\mathbb{R}^{\\ell\\times d}\\big|\\ \\left\\|\\mathbf{Q}\\right\\|_{\\infty}\\leq C\\right\\}\\times\\mathcal{Z}$ for some constant $C>0$ ", "page_idx": 26}, {"type": "text", "text": "$[\\mathbf{A}_{4}]$ There exist constants $L_{1},L_{2}\\ge0$ such that $F_{i}(\\cdot,z_{i})$ is uniformly $L_{1}$ -Lipschitz continuous with respect to $\\|\\cdot\\|_{*}$ under $\\|\\cdot\\|_{\\infty}$ , and ${\\cal F}_{i}({\\bf P},\\cdot)$ is uniformly $L_{2}$ -Lipschitz continuous with respect to $\\|\\cdot\\|_{*}$ under $\\|\\cdot\\|$ ", "page_idx": 26}, {"type": "text", "text": "$[\\mathbf{A}_{\\pmb{\\5}}]$ There are a positive constant $\\gamma>0$ and a pair of sets of matrices $\\left\\{\\left\\{\\mathbf{B}_{i}\\right\\}_{i=1}^{d},\\left\\{\\mathbf{C}_{i}\\right\\}_{i=1}^{d}\\right\\}\\subset$ $\\mathbb{R}_{+}^{\\ell\\times d}\\cup\\mathbf{0}$ satisfying $\\begin{array}{r}{\\left\\|\\sum_{i=1}^{d}(\\mathbf{B}_{i}+\\mathbf{C}_{i})\\right\\|_{\\infty}\\leq\\gamma}\\end{array}$ such that the folowin ounds hold ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbf{D}_{\\mathbf{P}(\\mathbf{Q},\\cdot,y)}(\\mathbf{x},\\mathbf{x}^{\\prime})\\leq\\sum_{i=1}^{d}\\mathbf{C}_{i}\\left\\langle F_{i}^{x}(\\mathbf{Q},z_{i}),\\mathbf{x}_{i}-\\mathbf{x}_{i}^{\\prime}\\right\\rangle,}\\\\ &{\\displaystyle\\mathbf{D}_{\\mathbf{P}(\\mathbf{Q},x,\\cdot)}(\\mathbf{y}^{\\prime},\\mathbf{y})\\leq\\sum_{i=1}^{d}\\mathbf{B}_{i}\\left\\langle-F_{i}^{y}(\\mathbf{Q},z_{i}),\\mathbf{y}_{i}^{\\prime}-\\mathbf{y}_{i}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for any $\\pmb{y},\\pmb{y}^{\\prime}\\in\\mathcal{Y}$ and $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}\\in\\mathcal{X}$ ", "page_idx": 26}, {"type": "text", "text": "$[\\mathbf{A}_{6}]$ There exists $\\theta\\,\\in\\,[0,1)$ such that $\\mathbf{P}(\\cdot,z)$ is a $\\theta$ -contraction mapping under $\\|\\cdot\\|_{\\infty}$ , and $\\|\\mathbf{P}(\\mathbf{Q},z)\\|_{\\infty}\\leq C$ for any $z\\in{\\mathcal{Z}}$ ", "page_idx": 26}, {"type": "text", "text": "Lemma C.4 (General Version of Lemma 4.3). Assuming that Assumption C.1 and C.3 hold, $\\mathbf{[P(Q,\\cdot,}$   \n$\\cdot)]_{k,j}$ is continuous, and convex with respect to $\\textbf{\\em x}$ , and concave with respect to y for any $(k,j)$ and $\\begin{array}{r l}{\\mathrm{.}}&{\\frac{\\operatorname*{min}\\left\\{\\left[{\\mathbf{C}}_{i}\\right]_{k,j},\\left[{\\mathbf{B}}_{i}\\right]_{k,j}\\right\\}}{\\left[{\\mathbf{C}}_{i}\\right]_{k,j}+\\left[{\\mathbf{B}}_{i}\\right]_{k,j}}\\,\\geq\\,C^{\\prime}}\\\\ {\\mathrm{.}}\\end{array}$ for some $C^{\\prime}>0$ then weclain that therexist $\\mathbf{Q}^{\\ast}\\,\\in\\,\\mathbb{R}^{\\ell\\times d}$ and   \ni\u2208[1:d   \n$z^{*}\\in{\\mathcal{Z}}$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Q}^{*}=\\mathbf{P}(\\mathbf{Q}^{*},x^{*},y^{*}),}\\\\ &{\\mathbf{Q}^{*}\\leq\\mathbf{P}(\\mathbf{Q}^{*},x,y^{*}),}\\\\ &{\\mathbf{Q}^{*}\\geq\\mathbf{P}(\\mathbf{Q}^{*},x^{*},y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We shall begin the proof by proving the following lemma. ", "page_idx": 26}, {"type": "text", "text": "Lemma C.5. Under the conditions of Lemma C.4, it can be proven that for any $\\mathbf{Q}\\in\\mathbb{R}^{\\ell\\times d}$ there exists apair of $\\mathbf{\\boldsymbol{x}}^{*},\\mathbf{\\boldsymbol{y}}^{*}$ that satisfy the following ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{P}(\\mathbf{Q},x^{*},y)\\leq\\mathbf{P}(\\mathbf{Q},x^{*},y^{*})\\leq\\mathbf{P}(\\mathbf{Q},x,y^{*}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Considering the following iteration ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{i}^{t}=\\underset{z_{i}\\in\\boldsymbol{\\mathcal{Z}}_{i}}{\\mathrm{argmin}}\\,\\eta\\left\\langle F_{i}(\\mathbf{Q},z_{i}^{t-1}),z_{i}\\right\\rangle+V\\left(\\mathbf{x}_{i},(g_{i}^{x})^{t-1}\\right)+V\\left(\\pmb{y}_{i},(g_{i}^{y})^{t-1}\\right),}\\\\ &{\\,\\,\\,\\,\\,\\,g_{i}^{t}=\\underset{g_{i}\\in\\boldsymbol{\\mathcal{Z}}_{i}}{\\mathrm{argmin}}\\,\\eta\\left\\langle F_{i}(\\mathbf{Q},z_{i}^{t}),g_{i}\\right\\rangle+V\\left(g_{i}^{x},(g_{i}^{x})^{t-1}\\right)+V\\left(g_{i}^{y},(g_{i}^{y})^{t-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for any $i\\in[1:d]$ and combining [57, Lemma 1], we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\nabla_{\\tau}|_{i,j}\\sum_{i=1}^{r}\\left(F_{i}^{\\tau}(\\mathbf{q},z_{i}^{t}),x_{i}^{t}-x_{i}^{t}\\right)+|\\mathbf{B}_{1,j}\\sum_{i=1}^{r}\\left(F_{i}^{\\tau}(\\mathbf{q},z_{i}^{t}),y_{i}^{t}-y_{i}^{t}\\right)}\\\\ &{\\le(|\\mathbf{C}_{1,j,k}|+|\\mathbf{B}_{1,j}|)\\eta^{-1}D+|\\mathbf{C}_{1,j}|_{L^{\\infty}}\\sum_{i=1}^{r}\\left|\\|F_{i}^{\\tau}(\\mathbf{q},z_{i}^{t})-F_{i}^{\\tau}(\\mathbf{q},z_{i}^{t-1})\\right|,\\ \\left\\|x_{i}^{t}-(\\mathbf{g}_{i}^{\\tau})\\right\\|}\\\\ &{\\quad+\\left.\\mathbb{B}_{1,j}\\sum_{i=1}^{r}\\left|\\|F_{i}^{\\tau}(\\mathbf{q},z_{i}^{t})-F_{i}^{\\tau}(\\mathbf{q},z_{i}^{t-1})\\right|,\\ \\left\\|y_{i}^{t}-(\\mathbf{g}_{i}^{\\tau})^{t}\\right|\\right|}\\\\ &{\\quad-\\frac{\\mathbb{C}_{1,j,k}}{\\eta}\\frac{1}{r_{i}}\\sum_{i=1}^{r}\\left(\\|x_{i}^{t}-(\\mathbf{g}_{i}^{\\tau^{\\prime}})^{t}\\|^{2}+\\|\\mathbf{g}_{i}^{\\tau^{\\prime}}\\|^{-1}-x_{i}^{t}\\|^{2}\\right)}\\\\ &{\\quad-\\frac{\\|\\mathbf{B}_{1,j}\\sum_{i=1}^{r}}{\\eta}\\left(\\|y_{i}^{t}-(\\mathbf{g}_{i}^{\\tau^{\\prime}})^{t}\\|^{2}+\\|\\mathbf{g}_{i}^{\\tau^{\\prime}}\\|^{-1}-y_{i}^{t}\\|^{2}\\right)}\\\\ &{\\quad\\frac{\\mathbb{C}_{2,j}^{\\tau}}{\\eta}\\frac{1}{r_{i}}\\left(\\|\\mathbf{g}_{i}^{\\tau^{\\prime}}(\\mathbf{b}_{i}^{\\tau^{\\prime}})\\|^{2}-\\|\\mathbf{g}_{i}^{\\tau^{\\prime}}\\|^{2}\\right)}\\\\ & \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for any $i\\;\\in\\;[1\\;:\\;d]\\;,\\;(k,j)\\;\\in\\;[1\\;:\\;\\ell]\\;\\times\\;[1\\;:\\;d]$ .and $z_{i}^{\\prime}\\in\\mathcal{Z}_{i}$ , where (a) is derived from ${\\bf A_{4}}$ in Assumption C.3 and Cauchy-Schwarz inequality. Therefore, by setting $\\begin{array}{r}{\\eta=\\frac{\\sqrt{C^{\\prime}}}{2L_{2}}}\\end{array}$ and $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=1}^{T}z^{t}=}\\end{array}$ $\\bar{\\pmb z}_{T}=(\\bar{\\pmb x}_{T},\\bar{\\pmb y}_{T})$ , the following estimation holds for any $(k,j)$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{\\mu}_{\\mathbf{z}^{\\prime}=(\\alpha^{\\prime},\\mathbf{y}^{\\prime})\\in\\mathcal{Z}}[\\mathbf{P}(\\mathbf{Q},\\bar{x}_{T},{y^{\\prime}})-\\mathbf{P}(\\mathbf{Q},{x^{\\prime}},\\bar{y}_{T})]_{k,j}}\\\\ {\\displaystyle\\leq\\frac{1}{\\bar{\\Gamma}}\\sum_{t=1}^{T}\\big[\\mathbf{P}(\\mathbf{Q},x^{t},\\bar{y}_{T}^{*})-\\mathbf{P}(\\mathbf{Q},\\bar{x}_{T}^{*},{y^{t}})\\big]_{k,j}}\\\\ {\\displaystyle\\leq\\frac{1}{\\bar{\\Gamma}}\\sum_{i=1}^{d}\\sum_{t=1}^{T}\\big([\\mathbf{C}_{i}]_{k,j}\\left\\langle F_{i}(\\mathbf{Q},z_{i}^{t}),\\mathbf{x}_{i}^{t}-(\\bar{x}_{T}^{*})_{i}\\right\\rangle+[\\mathbf{B}_{i}]_{k,j}\\left\\langle F_{i}(\\mathbf{Q},z_{i}^{t}),{y}_{i}^{t}-(\\bar{y}_{T}^{*})_{i}\\right\\rangle\\big)}\\\\ {\\displaystyle\\leq\\frac{2\\gamma\\eta^{-1}D+4\\eta\\gamma A^{2}L_{2}^{2}}{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the convexity of function $[\\mathbf{P}(\\mathbf{Q},\\cdot,\\pmb{w})-\\mathbf{P}(\\mathbf{Q},\\pmb{u},\\cdot)]_{k,j}$ for fixed $\\mathbf{Q}$ and $\\boldsymbol{v}=(u,w)$ implies (b), and (c) is derived from Eq. (76) and the definition that $(\\bar{\\mathbf{x}}_{T}^{*},\\bar{\\mathbf{y}}_{T}^{*}):=\\underset{z^{\\prime}\\in\\mathcal{Z}}{\\mathrm{argmax}}[\\mathbf{P}(\\mathbf{Q},\\bar{\\mathbf{x}}_{T},\\pmb{y}^{\\prime})-$ ", "page_idx": 27}, {"type": "text", "text": "${\\bf P}({\\bf Q},x^{\\prime},\\bar{y}_{T})\\big]_{k,j}$ . Since $\\mathcal{Z}$ is a compact set, the sequence $\\{(\\bar{\\pmb{x}}_{T},\\bar{\\pmb{y}}_{T})\\}_{T=1}^{\\infty}$ must have a convergent subsequence. Therefore, all accumulation points of the sequence $\\{(\\bar{\\pmb{x}}_{T},\\bar{\\pmb{y}}_{T})\\}_{T=1}^{\\infty}$ satisfy Eq. (74) by using the continuity of $\\mathbf P(\\mathbf Q,\\cdot,\\cdot)$ \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Now, we define the iterately update as follows ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{Q}^{t+1}=\\mathbf{P}(\\mathbf{Q}^{t},\\boldsymbol{x}_{t}^{*},\\boldsymbol{y}_{t}^{*}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $(\\mathbf{\\boldsymbol{x}}_{t}^{*},\\mathbf{\\boldsymbol{y}}_{t}^{*})$ satisfies Eq. (74) in Lemma C.5 w.r.t $\\mathbf{P}(\\mathbf{Q}^{t},\\cdot,\\cdot)$ . It's direct to derive that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Q}^{t+1}-\\mathbf{Q}^{t}\\leq\\!\\mathbf{P}(\\mathbf{Q}^{t},{\\boldsymbol{x}}_{t-1}^{*},{\\boldsymbol{y}}_{t}^{*})-\\mathbf{P}(\\mathbf{Q}^{t-1},{\\boldsymbol{x}}_{t-1}^{*},{\\boldsymbol{y}}_{t}^{*})}\\\\ &{\\qquad\\qquad\\leq\\!\\theta\\left\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{t-1}\\right\\|_{\\infty},}\\\\ &{\\mathbf{Q}^{t+1}-\\mathbf{Q}^{t}\\geq\\!\\mathbf{P}(\\mathbf{Q}^{t},{\\boldsymbol{x}}_{t}^{*},{\\boldsymbol{y}}_{t-1}^{*})-\\mathbf{P}(\\mathbf{Q}^{t-1},{\\boldsymbol{x}}_{t}^{*},{\\boldsymbol{y}}_{t-1}^{*})}\\\\ &{\\qquad\\qquad\\geq-\\,\\theta\\left\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{t-1}\\right\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, according to the contraction mapping principle, we complete the proof. ", "page_idx": 28}, {"type": "text", "text": "Corollary C.6. Assuming preconditions of Lemma C.4 hold, and leting $\\left\\{f_{i}(\\mathbf{Q},\\cdot)\\,:\\,\\mathbb{R}^{n_{i}+m_{i}}\\;\\rightarrow\\right.$ $\\mathbb{R}\\}_{i=1}^{d}$ be a sequence of continuous convex-concave functions which satisfies $\\nabla f_{i}(\\mathbf{Q},\\cdot)=(F_{i}^{x}(\\mathbf{Q},\\cdot)$ $-\\pmb{F}_{i}^{y}(\\mathbf{Q},\\cdot))$ for any fixed $\\mathbf{Q}\\in\\mathbb{R}^{\\ell\\times d}$ and $i\\in[1:d]$ then there exist a matrix $\\mathbf{Q}^{*}$ and a pair of $(x^{*},y^{*})$ which satisfy Eq. (71)-Eq. (73) and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{i}(\\mathbf{Q}^{*},x_{i}^{*},\\pmb{y}_{i}^{*})\\geq f_{i}(\\mathbf{Q}^{*},x_{i}^{*},\\pmb{y}_{i}),}\\\\ {f_{i}(\\mathbf{Q}^{*},x_{i}^{*},\\pmb{y}_{i}^{*})\\leq f_{i}(\\mathbf{Q}^{*},x_{i},\\pmb{y}_{i}^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for any $z_{i}\\in\\mathcal{Z}_{i}$ and $i\\in[1:d]$ ", "page_idx": 28}, {"type": "text", "text": "Proof. With proper selection of $\\eta$ we have the following bound which is similar to that derived from Eq.(77) ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{y_{i}^{\\prime}\\in\\mathcal{Y}_{i}}{\\operatorname*{max}}f_{i}(\\mathbf{Q},(\\bar{\\mathbf{x}}_{T})_{i},y_{i}^{\\prime})-\\underset{x_{i}^{\\prime}\\in\\mathcal{X}_{i}}{\\operatorname*{min}}f_{i}(\\mathbf{Q},x_{i}^{\\prime},(\\bar{\\mathbf{y}}_{T})_{i})}\\\\ &{\\le\\underset{t=1}{\\overset{T}{\\sum}}[f_{i}(\\mathbf{Q},x_{i}^{t},(\\bar{\\mathbf{y}}_{T}^{*})_{i})-f_{i}(\\mathbf{Q},(\\bar{\\mathbf{x}}_{T}^{*})_{i},y_{i}^{t})]}\\\\ &{\\le\\underset{t=1}{\\overset{T}{\\sum}}\\left[\\big\\langle F_{i}(\\mathbf{Q},z_{i}^{t}),x_{i}^{t}-(\\bar{\\mathbf{x}}_{T}^{*})_{i}\\big\\rangle+\\big\\langle F_{i}(\\mathbf{Q},z_{i}^{t}),y_{i}^{t}-(\\bar{\\mathbf{y}}_{T}^{*})_{i}\\big\\rangle\\right]}\\\\ &{\\le\\frac{4\\eta^{-1}D+8\\eta A^{2}L_{2}^{2}}{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for every $i\\;\\in\\;[1\\;:\\;d]$ ,where $\\left\\{z_{t}\\;=\\;(\\mathbf{x}_{t},\\pmb{y}_{t})\\right\\}_{t=1}^{T}$ follows from the iteration (75) and $(\\Bar{z}_{T}^{*})_{i}\\;=\\;$ $((\\bar{\\pmb x}_{T}^{*})_{i},(\\bar{\\pmb y}_{T}^{*})_{i})$ denotes $\\arg\\operatorname*{max}[f_{i}(\\mathbf{Q},(\\bar{\\mathbf{x}}_{T})_{i},\\pmb{y}_{i}^{\\prime})-f_{i}(\\mathbf{Q},\\pmb{x}_{i}^{\\prime},(\\bar{\\mathbf{y}}_{T})_{i})]$ . Hence, by directly lveraging Z'EZi the result of Lemma 4.3, we obtain the result. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Before stating the general version of Theorem 4.4 as follows, we define ", "page_idx": 28}, {"type": "equation", "text": "$$\nY_{T}^{\\eta}=8(c+1)\\left[\\gamma D\\left(\\frac{1}{\\eta}+16\\eta L_{2}\\right)+40\\eta^{3}\\gamma A^{2}L_{2}^{4}+2\\eta\\gamma L_{1}^{2}(1+64\\eta^{2}L_{2}^{2}C^{2})\\right](\\log(c+T)+1).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "C.2 Theorem C.7 and Relate Proof ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Theorem C.7. [General Version of Theorem 4.4] For any generaized quasar-convex-concave function $f$ which satisfies Assumption C.1 and C.3 with constant matrix function $\\mathbf{P}\\equiv\\mathbf{Q}^{*}$ where $\\mathbf{Q}^{*}$ is unknown and satisfies Eq. (71)-Eq. (73), with parameter configuration in Eq. (12), the weighted average ofAlgorithm $2\\,\\mathrm{:}$ outputs $\\{\\bar{z}_{t}\\}_{t=1}^{T}$ satisfies the following inequality ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{G}_{f}(\\bar{x}_{T},\\bar{y}_{T})\\leq\\frac{6\\left(\\operatorname*{max}_{z\\in\\mathcal{Z}}\\sum_{i=1}^{d}\\psi_{i}(z)\\right)(1-\\theta)^{-1}\\left(\\frac{3D}{\\eta}+10\\eta L_{1}^{2}+5A L_{1}Y_{T}^{\\eta}+4\\eta A^{2}L_{2}^{2}\\right)}{T+3}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For a generalized quasar-convex-concave function satisfying smoothness and recurrence conditions, the iteration complexity of our algorithm matches the lower bound [52] for solving $\\varepsilon$ -approximate Nash equilibrium points in the smooth convex-concave setting, up to a logarithmic factor. Furthermore, we prove that standard smooth convex-concave functions satisfy the preconditions of Theorem C.7 (as discussed in Appendix C.3.2). ", "page_idx": 28}, {"type": "text", "text": "Input: $\\left\\{z_{i}^{0}\\right\\}_{i=1}^{d}=\\left\\{\\pmb{g}_{i}^{0}\\right\\}_{i=1}^{d}$ $\\begin{array}{r}{\\sum_{t=1}^{T}\\alpha_{t}=1}\\end{array}$ \uff0c $\\{\\gamma_{t}\\ge0\\}_{t=1}^{T}$ \uff0c $\\{\\lambda_{t}\\geq0\\}_{t=1}^{T}$ $\\eta$ ${\\bf Q}^{0}={\\bf0}$   \nOutput: $\\begin{array}{r}{\\bar{z}_{T}=\\sum_{t=1}^{T}\\alpha_{t}\\pmb{z}^{t}}\\end{array}$   \n1: while $t\\leq T$ do   \n2: 3:fo $\\begin{array}{r l}&{t=(1-\\beta_{t-1})\\mathbf{Q}^{t-1}+\\beta_{t-1}\\mathbf{P}(\\mathbf{Q}^{t-1},z_{t-1}).}\\\\ &{\\mathbf{rall}\\ i\\in[1:d]\\ \\mathbf{q}^{0}}\\\\ &{x_{i}^{t}=\\underset{x_{i}\\in\\mathcal{X}_{i}}{\\mathrm{argmin}}\\ \\eta\\big\\langle F_{i}^{x}(\\mathbf{Q}^{t-1},z_{i}^{t-1}),x_{i}\\big\\rangle+\\gamma_{t}V\\left(\\mathbf{x}_{i},(g_{i}^{x})^{t-1}\\right)+\\lambda_{t}v(\\mathbf{x}_{i}),}\\\\ &{y_{i}^{t}=\\underset{y_{i}\\in\\mathcal{Y}_{i}}{\\mathrm{argmin}}\\ \\eta\\big\\langle F_{i}^{y}(\\mathbf{Q}^{t-1},z_{i}^{t-1}),y_{i}\\big\\rangle+\\gamma_{t}V\\left(y_{i},(g_{i}^{y})^{t-1}\\right)+\\lambda_{t}v(y_{i}),}\\\\ &{(g_{i}^{x})^{t}=\\underset{y_{i}^{\\ast}\\in\\mathcal{X}_{i}}{\\mathrm{argmin}}\\ \\eta\\big\\langle F_{i}^{x}(\\mathbf{Q}^{t},z_{i}^{t}),g_{i}^{x}\\big\\rangle+\\gamma_{t}V\\left(g_{i}^{x},(g_{i}^{x})^{t-1}\\right)+\\lambda_{t}v(g_{i}^{x}),}\\\\ &{(g_{i}^{y})^{t}=\\underset{y_{i}^{\\ast}\\in\\mathcal{Y}_{i}}{\\mathrm{argmin}}\\ \\eta\\big\\langle F_{i}^{y}(\\mathbf{Q}^{t},z_{i}^{t}),g_{i}^{y}\\big\\rangle+\\gamma_{t}V\\left(g_{i}^{y},(g_{i}^{y})^{t-1}\\right)+\\lambda_{t}v(g_{i}^{y}).}\\end{array}$   \n4:   \n5:   \n6:   \n7:   \n8: end for   \n9: $t\\gets t+1$   \n10: end while ", "page_idx": 29}, {"type": "text", "text": "Our analysis relies on the connection between $\\pmb{F}_{i}(\\mathbf{Q}^{t},\\pmb{z}_{i}^{t})$ and $\\pmb{F}_{i}(\\mathbf{Q}^{*},z_{i}^{t})$ . Theorem C.8 combines a) classical $\\mathcal{O}(\\log(T)/T)$ bound derived from regularized OMD, and b) the weighted average of iteration error $\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{t-1}\\|_{\\infty}^{2}$ over $t\\in[1:T]$ which has the form of $\\begin{array}{r}{\\sum_{t=1}^{T}\\alpha_{t}\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{t-1}\\|_{\\infty}^{2}}\\end{array}$ with magnitude $O(T^{-1}\\log(T))$ , and c) weighted average of approximation eror $\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\|_{\\infty}$ over $t\\in[1:T]$ which has the form of $\\begin{array}{r}{\\sum_{t=1}^{T}\\alpha_{t}\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\|_{\\infty}}\\end{array}$ to bound the max-min gap of $f$ $\\bar{z}_{T}$ . Next, we leverage Lemma C.9 to show the decreasing trend of approximation error $\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\|_{\\infty}$ and bound $\\begin{array}{r}{\\sum_{t=1}^{T}\\alpha_{t}\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\|_{\\infty}}\\end{array}$ by a quantity that grows onlylogarithmically in $T$ We may therefore obtain the result of Theorem C.7 by applying the estimation of weighted average of approximation error $\\begin{array}{r}{\\sum_{t=1}^{T}\\alpha_{t}\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\|_{\\infty}}\\end{array}$ to Theorem C.8. ", "page_idx": 29}, {"type": "text", "text": "C.2.1 Part I ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorem C.8. Assuming that Assumption C.1 holds, we set the hyper-parameters for Algorithm 2 carefullysuchthat ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{t}(\\gamma_{t}+\\lambda_{t})\\geq\\alpha_{t+1}\\gamma_{t+1},}\\\\ &{\\qquad\\qquad\\eta\\leq\\displaystyle\\operatorname*{min}_{t\\in[1:T]}\\frac{\\sqrt{\\gamma_{t}(\\gamma_{t}+\\lambda_{t})}}{4L_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Suppose that v modulus $2\\;w.r.t\\;\\|\\cdot\\|,\\;\\|z\\|\\leq A$ for any $z\\in{\\mathcal{Z}}$ and $\\{z_{t}\\}_{t=1}^{T}$ follows the iterations of Algorithm 4, then we can show that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{\\eta\\in S}f(\\bar{\\pi}_{T},y^{\\prime})-\\displaystyle\\operatorname*{min}_{x^{\\prime}\\in\\mathcal{X}}f(x^{\\prime},\\bar{y}_{T})\\leq{\\cal B}(\\psi)\\displaystyle\\operatorname*{max}_{i\\in[1]}\\left\\lbrace\\frac{\\alpha_{1}\\gamma_{1}}{\\eta}\\displaystyle\\operatorname*{max}_{z_{i}\\in\\mathcal{L}_{i}}\\left(V\\left(x_{i},(g_{i}^{x_{i}})^{0}\\right)+V\\left(y_{i},(g_{i}^{y})^{0}\\right)\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.-\\displaystyle\\frac{1}{2\\eta}\\displaystyle\\sum_{t=1}^{T}\\alpha_{t}\\left[\\gamma_{i}|z_{i}^{t}-g_{i}^{t-1}||^{2}+\\displaystyle\\frac{\\gamma_{i}+\\lambda_{i}}{2}||g_{i}^{t}-z_{i}^{t}||^{2}\\right]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.+\\displaystyle\\frac{2\\sum_{t=1}^{T}\\alpha_{t}\\lambda_{i}}{\\eta}\\displaystyle\\operatorname*{max}_{z_{i}\\in\\mathcal{L}_{i}}\\left(v(x_{i})+v(y_{i})\\right)\\right\\rbrace}\\\\ &{\\qquad+\\displaystyle\\left.2{\\cal B}(\\psi)\\eta L_{1}^{2}\\displaystyle\\sum_{t=1}^{T}\\left[\\frac{\\alpha_{t}}{\\gamma_{t}+\\lambda_{t}}||\\mathbf{g}^{t}-\\mathbf{Q}^{t-1}||_{\\infty}^{2}\\right]}\\\\ &{\\qquad+\\displaystyle2{\\cal A}{\\cal B}(\\psi)L_{1}\\displaystyle\\sum_{t=1}^{T}\\alpha_{t}||\\mathbf{g}^{t}-\\mathbf{Q}^{t}||_{\\infty}+\\displaystyle\\frac{{\\cal A}{\\cal A}^{2}{\\cal B}(\\psi)L_{2}^{2}\\alpha_{1}\\eta}{\\gamma_{1}+\\lambda_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{B(\\psi):=\\underset{z\\in\\mathcal{Z}}{\\operatorname*{max}}\\left|\\sum_{i=1}^{d}\\psi_{i}(z)\\right|}\\end{array}$ and $\\begin{array}{r}{\\bar{z}_{T}:=\\sum_{t=1}^{T}\\alpha_{t}z^{t}.}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Proof. Recalling the definition of GQCC, we derive that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y^{\\prime}\\in\\mathcal{Y}}f(x,y^{\\prime})-\\operatorname*{min}_{x^{\\prime}\\in\\mathcal{X}}f(x^{\\prime},y)\\leq\\!B(\\psi)\\operatorname*{max}_{i\\in[1:d]}\\left[\\operatorname*{max}_{w_{i}\\in\\mathcal{Y}_{i}}f_{i}(\\mathbf{Q}^{*},x_{i},w_{i})-\\operatorname*{min}_{u_{i}\\in\\mathcal{X}_{i}}f_{i}(\\mathbf{Q}^{*},u_{i},y_{i})\\right],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for any $\\pmb{z}=(\\pmb{x},\\pmb{y}),\\pmb{v}=(\\pmb{u},\\pmb{w})\\in\\mathcal{Z}$ and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{\\Sigma}_{i}^{\\top}(\\mathbf{Q}^{*},(\\bar{\\mathbf{x}}_{T})_{i},{\\boldsymbol{w}}_{i})-f_{i}(\\mathbf{Q}^{*},{\\boldsymbol{u}}_{i},(\\bar{\\boldsymbol{y}}_{T})_{i})\\leq\\sum_{t=1}^{T}\\alpha_{t}\\left[f_{i}(\\mathbf{Q}^{*},{\\boldsymbol{x}}_{i}^{t},{\\boldsymbol{w}}_{i})-f_{i}(\\mathbf{Q}^{*},{\\boldsymbol{u}}_{i},{\\boldsymbol{y}}_{i}^{t})\\right]}}&{{}}&{{\\displaystyle(88)}}\\\\ &{}&{\\leq\\sum_{t=1}^{T}\\alpha_{t}\\left\\langle F_{i}(\\mathbf{Q}^{*},{\\boldsymbol{z}}_{i}^{t}),{\\boldsymbol{z}}_{i}^{t}-{\\boldsymbol{v}}_{i}\\right\\rangle}\\\\ &{}&{\\leq\\sum_{t=1}^{T}\\alpha_{t}\\left\\langle F_{i}(\\mathbf{Q}^{t},{\\boldsymbol{z}}_{i}^{t}),{\\boldsymbol{z}}_{i}^{t}-{\\boldsymbol{v}}_{i}\\right\\rangle+2A L_{1}\\sum_{t=1}^{T}\\alpha_{t}\\lVert\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\rVert_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for any $\\pmb{v}_{i}\\in\\mathcal{Z}_{i}$ . Using the optimality condition, we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle F_{i}(Q^{t-1},\\underline{{s}}_{t}^{t-1}),\\underline{{s}}_{t}^{t},\\underline{{s}}_{t}^{t}\\rangle\\dot{\\times}\\frac{\\gamma_{t}^{2}}{\\eta}\\langle V\\left((\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}},(\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}})^{t-1})+V\\left((\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}},(\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}})^{t-1}\\right)\\right)}\\\\ &{-\\frac{\\gamma_{t}}{\\eta}\\left(V\\left(x_{t}^{\\pi_{t}^{t}},(\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}})^{t-1}\\right)+V\\left(y_{t}^{\\pi_{t}^{t}},(\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}})^{t-1}\\right)\\right)}\\\\ &{-\\frac{\\gamma_{t}+\\lambda_{t}}{\\eta}\\left(V\\left((\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}},\\underline{{s}}_{t}^{\\pi_{t}^{t}})+V\\left((\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}})^{t},y_{t}^{\\pi_{t}^{t}}\\right)\\right)}\\\\ &{+\\frac{\\lambda_{t}}{\\eta}\\left(v\\left((\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}})^{t}\\right)+v\\left((\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}})^{t}-v\\left(y_{t}^{\\pi_{t}^{t}}\\right)\\right)\\right)}\\\\ &{\\langle F_{i}(Q^{t},\\underline{{s}}_{t}^{t}),\\underline{{s}}_{t}^{t}-v_{i}\\rangle\\dot{\\times}\\frac{\\gamma_{t}^{2}}{\\eta}\\langle V\\left(u_{t},(\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}})^{t-1}\\right)+V\\left(w_{i},(\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}})^{t-1}\\right)\\rangle}\\\\ &{-\\frac{\\gamma_{t}}{\\eta}\\left(V\\left((\\underline{{\\theta}}_{t}^{\\pi_{t}^{t}})^{t},(\\underline \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For each $t\\in[1:T]$ , we can apply Eq. (89) and Eq. (90) to the following equation ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad_{\\tau_{i}}\\left\\langle F_{i}(\\mathbf{Q}^{t},z_{i}^{t}),z_{i}^{t}-v_{i}\\right\\rangle=\\alpha_{i}\\left[\\left\\langle F_{i}(\\mathbf{Q}^{t},z_{i}^{t}),g_{i}^{t}-v_{i}\\right\\rangle+\\left\\langle F_{i}(\\mathbf{Q}^{t-1},z_{i}^{t-1}),z_{i}^{t}-g_{i}^{t}\\right\\rangle\\right.}&{}\\\\ {+\\left.\\left\\langle F_{i}(\\mathbf{Q}^{t},z_{i}^{t})-F_{i}(\\mathbf{Q}^{t-1},z_{i}^{t-1}),z_{i}^{t}-g_{i}^{t}\\right\\rangle\\right],}&{}\\\\ {\\left.\\leq\\alpha_{i}\\left[\\frac{\\gamma_{t}}{\\eta}\\left(V\\left(u_{i},(g_{i}^{x})^{t-1}\\right)+V\\left(w_{i},(g_{i}^{y})^{t-1}\\right)\\right)-\\frac{\\gamma_{t}+\\lambda_{t}}{\\eta}\\left(V\\left(u_{i},(g_{i}^{x})^{t}\\right)\\right.\\right.}&{}\\\\ {+\\left.V\\left(w_{i},(g_{i}^{y})^{t}\\right)\\right)-\\frac{\\gamma_{t}}{\\eta}\\left(V\\left(x_{i}^{t},(g_{i}^{x})^{t-1}\\right)+V\\left(y_{i}^{t},(g_{i}^{y})^{t-1}\\right)\\right)}\\\\ {\\left.\\left.-\\frac{\\gamma_{t}+\\lambda_{t}}{\\eta}\\left(V\\left((g_{i}^{x})^{t},x_{i}^{t}\\right)+V\\left((g_{i}^{y})^{t},y_{i}^{t}\\right)\\right)\\right]+\\frac{\\alpha_{t}\\lambda_{t}}{\\eta}\\left(v(u_{i})+v(w_{i})\\right.\\right.}&{}\\\\ {\\left.\\left.-v\\left((g_{i}^{x})^{t}\\right)-v\\left((g_{i}^{y})^{t}\\right)\\right)+\\alpha_{t}\\left\\langle F_{i}(\\mathbf{Q}^{t},z_{i}^{t})-F_{i}(\\mathbf{Q}^{t-1},z_{i}^{t-1}),z_{i}^{t}-g_{i}^{t}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, by summing Eq.(91) from $t=1$ to $t=T$ and utilizing Eq. (84), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\alpha_{t}\\left<F_{i}(\\mathbf{Q}^{t},z_{i}^{t}),z_{i}^{t}-v_{i}\\right>\\le\\frac{\\alpha_{1}\\gamma_{1}}{\\eta}\\left(V\\left(u_{i},(g_{i}^{x})^{0}\\right)+V\\left(w_{i},(g_{i}^{y})^{0}\\right)\\right)}&{}\\\\ {\\displaystyle}&{\\ +\\,\\frac{2\\sum_{t=1}^{T}\\alpha_{t}\\lambda_{t}}{\\eta}\\operatorname*{max}_{z_{i}\\in\\mathcal{L}_{i}}\\left(v(x_{i})+v(y_{i})\\right)}\\\\ &{-\\displaystyle\\frac{1}{\\eta}\\sum_{t=1}^{T}\\alpha_{t}\\left[\\gamma_{t}\\|z_{i}^{t}-g_{i}^{t-1}\\|^{2}+\\frac{\\gamma_{t}+\\lambda_{t}}{2}\\|g_{i}^{t}-z_{i}^{t}\\|^{2}\\right]}\\\\ &{+\\displaystyle\\eta\\sum_{t=1}^{T}\\left[\\frac{\\alpha_{t}}{\\gamma_{t}+\\lambda_{t}}\\left\\|F_{i}(\\mathbf{Q}^{t},z_{i}^{t})-F_{i}(\\mathbf{Q}^{t-1},z_{i}^{t-1})\\right\\|_{*}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "According to the Lipschitz continuity of $\\pmb{F}_{i}$ , we derive that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\displaystyle\\sum_{t=1}^{T}\\left[\\frac{\\alpha_{t}}{\\gamma_{t}+\\lambda_{t}}\\left\\|F_{i}(\\mathbf{Q}^{t},z_{i}^{t})-F_{i}(\\mathbf{Q}^{t-1},z_{i}^{t-1})\\right\\|_{*}^{2}\\right]}\\\\ &{\\leq\\!2\\eta L_{2}^{2}\\displaystyle\\sum_{t=1}^{T}\\left[\\frac{\\alpha_{t}}{\\gamma_{t}+\\lambda_{t}}\\|z_{i}^{t}-z_{i}^{t-1}\\|^{2}\\right]+2\\eta L_{1}^{2}\\displaystyle\\sum_{t=1}^{T}\\left[\\frac{\\alpha_{t}}{\\gamma_{t}+\\lambda_{t}}\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{t-1}\\|_{\\infty}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "It follows from parameter setting Eq. (84) and Cauchy-Schwarz inequality that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\alpha_{t}\\gamma_{t}}{2}\\|z_{i}^{t}-g_{i}^{t-1}\\|^{2}+\\frac{\\alpha_{t-1}\\big(\\gamma_{t-1}+\\lambda_{t-1}\\big)}{2}\\|g_{i}^{t-1}-z_{i}^{t-1}\\|^{2}\\geq\\frac{\\alpha_{t}\\gamma_{t}}{4}\\|z_{i}^{t}-z_{i}^{t-1}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining Eq. (85) and Eq. (94), we may therefore obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle}&{\\displaystyle-\\frac{1}{\\eta}\\sum_{t=1}^{T}\\alpha_{t}\\left[\\frac{\\gamma_{t}}{2}\\|z_{i}^{t}-{\\pmb g}_{i}^{t-1}\\|^{2}+\\frac{\\gamma_{t}+\\lambda_{t}}{4}\\|{\\pmb g}_{i}^{t}-{\\pmb z}_{i}^{t}\\|^{2}\\right]+2\\eta L_{2}^{2}\\sum_{t=1}^{T}\\left[\\frac{\\alpha_{t}}{\\gamma_{t}+\\lambda_{t}}\\|z_{i}^{t}-z_{i}^{t-1}\\|^{2}\\right]}\\\\ {\\displaystyle}&{\\le\\frac{2\\eta L_{2}^{2}\\alpha_{1}}{\\gamma_{1}+\\lambda_{1}}\\|z_{i}^{1}-z_{i}^{0}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Applying Eq. (93) and Eq. (95) to Eq. (92) and utilizing Eq. (87), Eq. (88), we complete the proof. ", "page_idx": 31}, {"type": "text", "text": "C.2.2 Part II: Estimation of Approximation Error $\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\|$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "According to the iterately update of $\\mathbf{Q}^{t}$ , we can derive the upper bound of weighted average of $\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{\\check{t}-1}\\|_{\\infty}^{2}$ Next, we aim to bound $\\lVert\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\rVert$ for each iteration $t$ In this section, we select the following parameter settings: ", "page_idx": 31}, {"type": "equation", "text": "$$\nc=2(1-\\theta)^{-1},\\,\\eta\\leq\\frac{(1-\\theta)^{1/2}}{8(\\gamma A L_{1})^{1/2}L_{2}},\\,\\beta_{t}=\\frac{c}{c+t},\\,\\alpha_{t}=\\beta_{T,t},\\,\\gamma_{t}=\\frac{\\alpha_{t-1}}{\\alpha_{t}},\\,\\lambda_{t}=1-\\gamma_{t}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma C.9. Consider the setings: $\\begin{array}{r}{\\gamma_{t}=\\frac{\\alpha_{t-1}}{\\alpha_{t}}\\le1,\\lambda_{t}=1-\\gamma_{t},}\\end{array}$ and $\\begin{array}{r}{\\eta\\le\\frac{(1-\\theta)^{1/2}}{8(\\gamma A L_{1}L_{2})^{1/2}}}\\end{array}$ 8(YAL,L2)1/2. Then, we obtain the estimation of $\\lVert\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\rVert$ as follows, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\|_{\\infty}\\leq\\sum_{j=2}^{t}\\beta_{t,j}^{(1+\\theta)/2}H_{j},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for any $t\\geq2$ where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{H_{j}:=\\gamma D\\left(\\frac{1}{\\eta}+16\\eta L_{2}\\right)\\left(\\beta_{j-1,1}\\gamma_{1}+2\\sum_{\\kappa=1}^{j-1}\\beta_{j-1,\\kappa}\\lambda_{\\kappa}\\right)}\\quad}&{}\\\\ &{+\\ 2\\eta\\gamma L_{1}^{2}(1+64\\eta^{2}L_{2}^{2}C^{2})\\sum_{\\kappa=1}^{j-1}\\beta_{j-1,\\kappa}\\beta_{\\kappa-1}^{2}+128\\eta^{3}\\gamma A^{2}L_{2}^{4}\\beta_{j-1,1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. According to the fact that $\\mathbf{Q}^{*}$ is a fixed point of function $_{P}$ ,wehave ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathbf{2}^{t}-\\mathbf{Q}^{*}=\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}[P(\\mathbf{Q}^{\\kappa},z^{\\kappa})-P(\\mathbf{Q}^{*},z^{*})]}\\\\ &{\\quad\\quad\\leq\\displaystyle\\sum_{\\mathrm{(a)}}^{t-1}\\beta_{t-1,\\kappa}\\left\\{[P(\\mathbf{Q}^{\\kappa},x^{\\kappa},y^{\\kappa})-P(\\mathbf{Q}^{\\kappa},x^{*},y^{\\kappa})]+[P(\\mathbf{Q}^{\\kappa},x^{*},y^{\\kappa})-P(\\mathbf{Q}^{*},x^{*},y^{\\kappa})]\\right\\}}\\\\ &{\\quad\\displaystyle\\leq\\sum_{i=1}^{d}\\left(\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\left\\langle F_{i}^{x}(\\mathbf{Q}^{\\kappa},z_{i}^{\\kappa}),x_{i}^{\\kappa}-x_{i}^{*}\\right\\rangle\\right)\\mathbf{C}_{i}+\\theta\\left(\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\lVert\\mathbf{Q}^{\\kappa}-\\mathbf{Q}^{*}\\rVert_{\\infty}\\right)e_{d}e_{d}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Where (a) is derived from the maximizer's property of $\\boldsymbol{y}^{*}$ for matrix-valued function $P(\\mathbf{Q}^{*},x^{*},\\cdot)$ Similarly, we can obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{3}^{t}-\\mathbf{Q}^{*}\\geq-\\sum_{i=1}^{d}\\left(\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\,\\langle F_{i}^{y}(\\mathbf{Q}^{\\kappa},z_{i}^{\\kappa}),y_{i}^{\\kappa}-y_{i}^{*}\\rangle\\right)\\mathbf{B}_{i}-\\theta\\left(\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\|\\mathbf{Q}^{\\kappa}-\\mathbf{Q}^{*}\\|_{\\infty}\\right)e_{d}e_{d}^{\\top}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence, we derive ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\|_{\\infty}\\leq\\gamma\\displaystyle\\operatorname*{max}_{i\\in[1:d]}\\left\\{\\frac{\\beta_{t-1,1}\\gamma_{1}}{\\eta}\\operatorname*{max}_{z_{i}\\in\\mathcal{L}_{i}}\\left(V\\left(x_{i},(g_{i}^{x})^{0}\\right)+V\\left(y_{i},(g_{i}^{y})^{0}\\right)\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\frac{2\\sum_{t=1}^{t-1}\\beta_{t-1,\\kappa}\\lambda_{\\kappa}}{\\eta}\\displaystyle\\operatorname*{max}_{z_{i}\\in\\mathcal{L}_{i}}\\left(v(x_{i})+v(y_{i})\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+2\\eta L_{2}^{2}\\displaystyle\\sum_{\\kappa=1}^{t-1}\\frac{\\beta_{t-1,\\kappa}}{\\gamma_{\\kappa}+\\lambda_{\\kappa}}\\left\\|z_{i}^{\\kappa}-z_{i}^{\\kappa-1}\\right\\|^{2}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.+2\\eta\\gamma L_{1}^{2}\\displaystyle\\sum_{\\kappa=1}^{t-1}\\frac{\\beta_{t-1,\\kappa}}{\\gamma_{\\kappa}+\\lambda_{\\kappa}}\\|\\mathbf{Q}^{\\kappa}-\\mathbf{Q}^{\\kappa-1}\\|_{\\infty}^{2}+\\theta\\left(\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\|\\mathbf{Q}^{\\kappa}-\\mathbf{Q}^{*}\\|_{\\infty}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by combining $\\begin{array}{r}{\\beta_{t-1,\\kappa}\\prod_{j=t}^{T}(1\\,-\\,\\beta_{j})\\ =\\ \\alpha_{\\kappa}}\\end{array}$ and Eq. (99), and using the proof technique of Theorem C.8.  Next, for any $i\\in\\ [1\\ :\\ d]$ , we can obtain an upper bound estimation of $\\begin{array}{r}{\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\langle F_{i}(\\mathbf{Q}^{\\kappa},z_{i}^{\\kappa}),z_{i}^{\\kappa}-v_{i}\\rangle}\\end{array}$ as folows ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\nu\\in\\mathcal{Z}_{i}}\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\left\\langle F_{i}(\\mathbf{Q}^{\\kappa},z_{i}^{\\kappa}),z_{i}^{\\kappa}-\\pmb{v}_{i}\\right\\rangle\\leq\\displaystyle\\frac{D}{\\eta}\\left(\\beta_{t-1,1}\\gamma_{1}+2\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\lambda_{\\kappa}\\right)+8\\eta A^{2}L_{2}^{2}\\beta_{t-1,1}}\\\\ &{\\quad+\\displaystyle8\\eta L_{1}^{2}C^{2}\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\beta_{\\kappa-1}^{2}-\\displaystyle\\frac{1}{8\\eta}\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\left\\|z_{\\kappa}^{k}-z_{\\kappa-1}^{k}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Furthermore, we also have a lower bound estimation of it ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{v_{i}\\in\\mathcal{Z}_{i}}{\\mathrm{max}}\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\left\\langle F_{i}(\\mathbf{Q}^{\\kappa},z_{i}^{\\kappa}),z_{i}^{\\kappa}-v_{i}\\right\\rangle\\geq\\underset{v_{i}\\in\\mathcal{Z}_{i}}{\\mathrm{max}}\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\left\\langle F_{i}(\\mathbf{Q}^{\\kappa},v_{i}),z_{i}^{\\kappa}-v_{i}\\right\\rangle}\\\\ &{\\geq\\underset{v_{i}\\in\\mathcal{Z}_{i}}{\\mathrm{max}}\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\left\\langle F_{i}(\\mathbf{Q}^{\\kappa},v_{i}),z_{i}^{\\kappa}-v_{i}\\right\\rangle}\\\\ &{\\quad-\\displaystyle2A L_{1}\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\|\\mathbf{Q}^{\\kappa}-\\mathbf{Q}^{*}\\|_{\\infty}}\\\\ &{\\geq-2A L_{1}\\underset{\\kappa=1}{\\overset{t-1}{\\sum}}\\beta_{t-1,\\kappa}\\|\\mathbf{Q}^{\\kappa}-\\mathbf{Q}^{*}\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, combining Eq. (102) and Eq. (103), we derive the following result ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\left\\|z_{i}^{\\kappa}-z_{i}^{\\kappa-1}\\right\\|^{2}\\leq16\\eta A L_{1}\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\|\\mathbf{Q}^{\\kappa}-\\mathbf{Q}^{*}\\|_{\\infty}+64\\eta^{2}A^{2}L_{2}^{2}\\beta_{t-1,1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+8D\\left(\\beta_{t-1,1}\\gamma_{1}+2\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\lambda_{\\kappa}\\right)+64\\eta^{2}C^{2}L_{1}^{2}\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\beta_{\\kappa-1}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for any $i\\in[1:d]$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\|{\\bf Q}^{t}-{\\bf Q}^{*}\\|_{\\infty}\\leq\\gamma D\\left(\\frac{1}{\\eta}+16\\eta L_{2}^{2}\\right)\\left(\\beta_{t-1,1}\\gamma_{1}+2\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\lambda_{\\kappa}\\right)+128\\eta^{3}\\gamma A^{2}L_{2}^{4}\\beta_{t-1,1}}}\\\\ {{\\displaystyle\\qquad\\qquad+\\;2\\eta\\gamma L_{1}^{2}\\left(1+64\\eta^{2}C^{2}L_{2}^{2}\\right)\\displaystyle\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\beta_{\\kappa-1}^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad+\\;\\big(32\\eta^{2}\\gamma A L_{1}L_{2}^{2}+\\theta\\big)\\sum_{\\kappa=1}^{t-1}\\beta_{t-1,\\kappa}\\|{\\bf Q}^{\\kappa}-{\\bf Q}^{*}\\|_{\\infty}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Finally, by applying [67, Lemma 33] to Eq. (105), we complete the proof ", "page_idx": 33}, {"type": "text", "text": "Under parameter settings Eq. (96), the following auxiliary Lemma C.10 provides both lower bound and upper bound of $\\beta_{T,t}$ ", "page_idx": 33}, {"type": "text", "text": "Lemma C.10. Assuming that $\\begin{array}{r}{\\beta_{t}=\\frac{c^{\\prime}}{c+t}}\\end{array}$ and e \u2265 e, we can obtain the following result: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\exp\\left\\{-\\frac{(c^{\\prime}+c^{\\prime}c)^{2}}{2c}\\right\\}\\frac{(c+t)^{c^{\\prime}-1}}{(c+T)^{c^{\\prime}}}\\leq\\beta_{T,t}\\leq\\frac{(1+c)(c+t+1)^{c^{\\prime}-1}}{(c+T+1)^{c^{\\prime}}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for any $T\\geq t\\geq1$ ", "page_idx": 33}, {"type": "text", "text": "Proof. Recalling that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\beta_{T,t}=\\frac{c^{\\prime}}{c+t}\\prod_{k=t+1}^{T}\\left(1-\\frac{c^{\\prime}}{c+k}\\right)=\\frac{c^{\\prime}}{c+t}\\exp\\left\\{\\sum_{k=t+1}^{T}\\log\\left(1-\\frac{c^{\\prime}}{c+k}\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\beta_{T,t}\\leq\\frac{c^{\\prime}}{c+t}\\left(\\frac{c+t+1}{c+T+1}\\right)^{c^{\\prime}}\\leq\\frac{(1+c)(c+t+1)^{c^{\\prime}-1}}{(c+T+1)^{c^{\\prime}}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\beta_{T,t}\\geq\\exp\\left\\lbrace-\\frac{(c^{\\prime}+c^{\\prime}c)^{2}}{2c}\\right\\rbrace\\frac{(c+t)^{c^{\\prime}-1}}{(c+T)^{c^{\\prime}}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "by combining the result of Lemma D.9. ", "page_idx": 33}, {"type": "text", "text": "Corollary C.11. Assuming that $\\begin{array}{r}{\\beta_{t}=\\frac{c^{\\prime}}{c+t}}\\end{array}$ ct, c\u2265 1and c(1 - 0) \u22651, we can obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\beta_{T,t}^{\\theta}\\leq\\frac{c^{\\prime}}{c+T},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for any $T\\geq t\\geq1$ ", "page_idx": 33}, {"type": "text", "text": "By utilizing the result of Lemma C.10, we notice that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\sum_{\\kappa=1}^{j-1}\\beta_{j-1,\\kappa}\\lambda_{\\kappa}\\leq\\displaystyle\\sum_{\\kappa=1}^{j-1}\\displaystyle\\frac{(1+c)^{2}(c+\\kappa+1)^{c-2}}{(c+j)^{c}}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{(1+c)^{2}}{(\\alpha)}\\displaystyle\\int_{1}^{j-1}(c+x+1)^{c-2}\\mathrm{d}x+\\displaystyle\\frac{(1+c)^{2}}{(c+j)^{2}}}\\\\ {\\displaystyle\\qquad\\qquad\\leq\\displaystyle\\frac{(1+c)^{2}}{(c-1)(c+j)}+\\displaystyle\\frac{(1+c)^{2}}{(c+j)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\kappa=1}^{j-1}\\beta_{j-1,\\kappa}\\beta_{\\kappa-1}^{2}\\le\\displaystyle\\sum_{\\kappa=1}^{j-1}\\frac{(1+c)^{4}(c+\\kappa+1)^{c-3}}{c(c+j)^{c}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{(1+c)^{3}}{\\varnothing}\\int_{1}^{j-1}(c+x+1)^{c-2}\\mathrm{d}x+\\frac{(1+c)^{4}}{c(c+j)^{3}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{(1+c)^{3}}{c(c-1)(c+j)}+\\frac{(1+c)^{4}}{c(c+j)^{3}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where (a) and (b) are derived from the fact that $\\begin{array}{r}{\\sum_{\\kappa=1}^{j-2}(c+\\kappa+1)^{c-2}\\,\\le\\,\\int_{1}^{j-1}(c+x+1)^{c-2}\\mathrm{d}x}\\end{array}$ Next, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{j}\\le\\gamma D\\left(\\displaystyle\\frac{1}{\\eta}+16\\eta L_{2}\\right)\\left(\\frac{2(c+2)^{c-1}}{(c+j)^{c}}+\\frac{2(1+c)^{2}}{(c-1)(c+j)}+\\frac{2(1+c)^{2}}{(c+j)^{2}}\\right)}\\\\ &{\\qquad+\\,2\\eta\\gamma L_{1}^{2}(1+64\\eta^{2}L_{2}^{2}C^{2})\\left(\\frac{(1+c)^{3}}{c(c-1)(c+j)}+\\frac{(1+c)^{4}}{c(c+j)^{3}}\\right)}\\\\ &{\\qquad+\\,128\\eta^{3}\\gamma A^{2}L_{2}^{4}\\frac{(c+2)^{c}}{(c+j)^{c}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=2}^{t}\\beta_{i,j}^{(1+\\theta)/2}H_{j}\\underbrace{\\leq\\frac{c}{c}}_{i}\\left\\{\\gamma D\\left(\\frac{1}{\\eta}+16\\eta L_{2}\\right)\\left[\\int_{2}^{t}\\frac{2(c+2)^{c-1}}{(c+x)^{c}}\\mathrm{d}x\\right.\\right.}&{}\\\\ {\\left.\\left.+\\int_{1}^{t}\\left(\\frac{2(1+c)^{2}}{(c-1)(c+x)}+\\frac{2(1+c)^{2}}{(c+x)^{2}}\\right)\\mathrm{d}x+1\\right]\\right.}&{}\\\\ {\\left.+2\\eta\\gamma L_{1}^{2}(1+64\\eta^{2}L_{2}^{2}c^{2})\\int_{1}^{t}\\left(\\frac{(1+c)^{3}}{(c(c-1)(c+x)}+\\frac{(1+c)^{4}}{c(c+x)^{3}}\\right)\\mathrm{d}x\\right\\}}\\\\ &{\\left.\\qquad+128\\eta^{3}\\gamma A^{2}L_{2}^{4}\\left[1+\\int_{2}^{t}\\frac{(c+2)^{c}}{(c+x)^{4}}\\mathrm{d}x\\right]}\\\\ {\\left.\\leq\\frac{c}{c+t}\\left\\{\\gamma D\\left(\\frac{1}{\\eta}+16\\eta L_{2}\\right)\\left[\\frac{2(c+1)^{2}}{c-1}\\log(c+t)+5+2c\\right]+640\\eta^{3}\\gamma A^{2}L_{2}^{4}\\right.}&{}\\\\ {\\left.\\left.+2\\eta\\gamma L_{1}^{2}(1+64\\eta^{2}L_{2}^{2}c^{2})\\left[\\frac{2(c+1)^{2}}{c-1}\\log(c+t)+\\frac{(c+1)^{2}}{2c}\\right]\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where (c) follows from Corollary C.11. For simplicity, we denote ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\l_{T}^{\\eta}:=8(c+1)\\left[\\gamma D\\left(\\frac{1}{\\eta}+16\\eta L_{2}\\right)+40\\eta^{3}\\gamma A^{2}L_{2}^{4}+2\\eta\\gamma L_{1}^{2}(1+64\\eta^{2}L_{2}^{2}C^{2})\\right](\\log(c+T)+1).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, it follows from Eq. (114) and Lemma C.9 that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|\\mathbf{Q}^{t}-\\mathbf{Q}^{*}\\|_{\\infty}\\leq\\sum_{j=2}^{t}\\beta_{t,j}^{(1+\\theta)/2}H_{j}\\leq\\frac{c}{c+t}Y_{t}^{\\eta}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "C.2.3 The Last Step ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "According to Eq. (116) and the initial ${\\bf{Q}}^{0}$ satisfies $\\|\\mathbf{Q}^{0}\\|_{\\infty}\\leq C$ ,we have $\\|\\mathbf{Q}^{*}\\|\\leq C$ . We are ready to complete the proof of Theorem C.7. ", "page_idx": 34}, {"type": "text", "text": "Proof of Theorem C.7. It is noteworthy that the hyper-parameters selected in Eq. (96) satisfies the preconditions of Theorem C.8. Combining the conclusion of Theorem C.8, Eq. (116) and the ", "page_idx": 34}, {"type": "text", "text": "estimation of $\\alpha_{t}$ (i.e. $\\beta_{T,t}$ ) in Lemma C.10, we obtain: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{f}(\\mathbf{x},\\mathbf{y})\\leq B\\operatorname*{max}_{i\\in\\mathcal{A}}\\left\\{\\underset{\\eta}{\\operatorname{max}}\\left(\\nu_{i}\\right)\\underset{\\mathbf{z}\\in\\mathcal{E}_{i}}{\\operatorname{max}}\\left(V\\left(x_{i},(\\rho_{i}^{\\mathbf{z}})^{0}\\right)+V\\left(y_{i},(\\rho_{i}^{\\mathbf{y}})^{0}\\right)\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\frac{2\\sum_{t=1}^{T}\\alpha_{t}\\lambda_{t}}{\\eta}\\frac{\\operatorname{max}_{i}}{x_{i}\\in\\mathcal{L}_{i}}\\left(\\nu(\\mathbf{x}_{i})+\\nu(y_{i})\\right)\\right\\}+2\\eta B L_{\\Gamma}^{2}\\underset{t=1}{\\sum_{t=1}^{T}\\alpha_{t}\\beta_{t-1}^{2}}}\\\\ &{\\qquad\\qquad\\qquad+2A B c L_{\\Gamma}\\gamma_{\\frac{\\mathbf{r}}{t}=\\overline{{c}}}^{\\frac{\\mathbf{\\alpha}}{\\mathbf{\\alpha}}}\\frac{\\alpha_{t}}{c}+8\\eta A^{2}B L_{\\mathcal{F}}^{2}\\alpha_{1}^{2}}}\\\\ &{\\qquad\\qquad\\leq\\frac{2B D}{\\eta}\\left(\\frac{(c+2)^{2}-1}{(c+T)}-\\frac{(c+2)}{(c+T)}\\right)^{2}+\\frac{2(c+1)}{c}\\right)}\\\\ &{\\qquad\\qquad+2\\eta B L_{\\mathcal{F}}^{2}\\left(\\frac{(c+1)^{3}}{c(c+T)}+\\frac{(c+2)}{c(c+T+1)^{3}}\\right)}\\\\ &{\\qquad\\qquad+2A B L_{\\mathcal{F}}^{3}\\gamma_{\\overline{{r}}}^{2}\\left(\\frac{4c}{c+T+1}+\\frac{c(c+2)}{(c+T+1)^{2}}\\right)+8\\eta A^{2}B L_{\\mathcal{F}}^{2}\\frac{(1+c)(c+2)^{\\epsilon-1}}{(c+T+1)^{\\epsilon}}}\\\\ &{\\qquad\\qquad\\leq2B\\left(\\frac{3D}{2}+10a L_{\\mathcal{F}}^{2}+5A L_{\\mathcal{F}}\\gamma_{\\overline{{r}}}^{2}+4\\eta\\lambda_{t}^{2}\\right)\\frac{c+L}{c}+1}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "when $T\\geq1$ where $B$ denotes $\\begin{array}{r}{\\operatorname*{max}_{z\\in\\mathcal{Z}}\\sum_{i=1}^{d}\\psi_{i}(z)}\\end{array}$ and (a) is derived from parameter settings Eq. (96) and the result of Lemma C.10. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "C.3 Application to Minimax Problems ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "C.3.1  Infinite Horizon Two-Player Zero-Sum Markov Games ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "To simplify ntations, in the following discussion, we write $\\mathcal{S}\\,=\\,\\{s_{i}\\}_{i=1}^{|S|}$ , and denote by ${\\bf Q}^{z}=$ $\\left(\\mathbf{Q}^{z}(s_{1},\\cdot,\\cdot),\\cdot\\cdot\\cdot\\,,\\mathbf{Q}^{z}(s_{|S|},\\cdot,\\cdot)\\right)$ the joint action-value matrix where $\\mathbf{Q}^{\\bar{z}}(\\bar{s}_{i},\\cdot,\\cdot)\\in\\mathbb{R}^{|A|\\times|B|}$ is an action-value matrix on state $s_{i}$ . According to the connection between value function and action-value function ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{z}(s_{i})=\\mathbb{E}_{a\\sim\\alpha(\\cdot\\vert s_{i})}[Q^{z}(s_{i},a,b)],\\ Q^{z}(s_{i},a,b)=(1-\\theta)\\sigma(s_{i},a,b)+\\theta\\mathbb{E}_{s_{i^{\\prime}}\\sim\\mathbb{P}(\\cdot\\vert s_{i},a,b)}[V^{z}(s_{i^{\\prime}})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "we provide the following proof for Proposition 4.5. ", "page_idx": 35}, {"type": "text", "text": "Proof of Proposition 4.5. By defining ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\mathbf{P}_{i}(\\mathbf{Q},z)]_{a,b}:=(1-\\theta)\\sigma(s_{i},a,b)+\\theta\\mathbb{E}_{s_{i^{\\prime}}\\sim\\mathbb{P}(\\cdot|s_{i},a,b)}\\left[\\langle\\mathbf{Q}_{i^{\\prime}}y_{i^{\\prime}},x_{i^{\\prime}}\\rangle\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "we derive that ${\\bf Q}^{z}={\\bf P}({\\bf Q}^{z},z)$ . We can notice that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\mathbf{P}_{i}(\\mathbf{Q},x,y)-\\mathbf{P}_{i}(\\mathbf{Q},x^{\\prime},y)]_{a,b}=\\!\\!\\theta\\mathbb{E}_{s_{i^{\\prime}}\\sim\\mathbb{P}(\\cdot|s_{i},a,b)}\\left[\\langle\\mathbf{Q}_{i^{\\prime}}y_{i^{\\prime}},x_{i^{\\prime}}-(x^{\\prime})_{i^{\\prime}}\\rangle\\right],}\\\\ &{[\\mathbf{P}_{i}(\\mathbf{Q},x,y^{\\prime})-\\mathbf{P}_{i}(\\mathbf{Q},x,y)]_{a,b}=\\!\\!\\theta\\mathbb{E}_{s_{i^{\\prime}}\\sim\\mathbb{P}(\\cdot|s_{i},a,b)}\\left[\\langle-\\mathbf{Q}_{i^{\\prime}}^{\\top}x_{i^{\\prime}},y_{i^{\\prime}}-(y^{\\prime})_{i^{\\prime}}\\rangle\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, for any $\\mathbf{Q}$ satisfies $\\|\\mathbf{Q}\\|_{\\infty}\\leq1$ , it's easy to verify that ", "page_idx": 35}, {"type": "text", "text": "1. $F_{i}(\\cdot,z_{i})$ is uniformly 2-Lipschitz continuous with respect to $\\|\\cdot\\|_{\\infty}$ under $\\|\\cdot\\|_{\\infty}$ for any $z_{i}\\in\\mathcal{Z}_{i}$ , and $F_{i}(\\mathbf{Q},\\cdot)$ is uniformly 1-Lipschitz continuous with respect to $\\|\\cdot\\|_{\\infty}$ under $\\|\\cdot\\|_{1}$ , since $F(\\mathbf{Q},z_{i})=\\left(\\pmb{y}_{i}^{\\top}\\mathbf{Q}_{i}^{\\top},-\\pmb{x}_{i}^{\\top}\\mathbf{Q}_{i}\\right)^{\\top}$ \uff0c   \n2. $\\mathbf{P}$ satisfies $[\\mathbf{A}_{2}]$ in Assumptions 4.2 with $[\\mathbf{B}_{i}]_{s,a,b}=[\\mathbf{C}_{i}]_{s,a,b}=\\theta\\mathbb{P}(s_{i}|s,a,b)$ and $\\gamma=2\\theta$ since Eq. (119),   \n3. $\\mathbf{P}(\\cdot,z)$ is a $\\theta$ -contraction mapping under $\\|\\cdot\\|_{\\infty}$ , and $\\|\\mathbf{P}(\\cdot,\\cdot)\\|_{\\infty}\\leq1$ , since the definition of $\\mathbf{P}$ \uff0c   \n4. $[\\mathbf{P}_{i}(\\mathbf{Q},\\cdot,\\cdot)]_{a,b}$ is bi-linear with respect to $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ , and min[Ca]b,Bi]b = 1/2 for any $i$ and $s,a,b$ ", "page_idx": 35}, {"type": "text", "text": "Therefore, according to Lemma 4.3, there exist a tensor $\\mathbf{Q}^{*}$ and a pair of $(x^{*},y^{*})$ satisfy Eq. (11). Furthermore, the $(\\boldsymbol{x}^{*},\\boldsymbol{y}^{*})$ mentioned above is a Nash equilibrium of $J^{x,y}(\\rho_{0})$ by utilizing Corollary C.6. We may therefore derive that $\\mathbf{Q}^{*}\\,\\equiv\\,\\mathbf{Q}^{z^{*}}$ . Leveraging Eq. (65) for any Nash equilibrium $(\\pmb{x}^{\\ast},\\pmb{y}^{\\ast})\\in\\mathcal{Z}$ and denoting $\\mathbf{Q}_{i}^{*}=\\mathbf{Q}^{x^{*},y^{*}}(s_{i},\\cdot,\\cdot)$ we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\boldsymbol{J}^{x^{*},y^{*}}(\\rho_{0})-\\boldsymbol{J}^{x^{*}(y),y}(\\rho_{0})=\\sum_{s\\in\\mathcal{S}}\\boldsymbol{d}_{\\rho_{0}}^{x^{*}(y),y}(s)\\left[\\langle\\mathbf{Q}^{x^{*},y^{*}}(s,\\cdot,\\cdot)y^{*}(\\cdot|s),x^{*}(\\cdot|s)\\rangle\\right.}}\\\\ &{}&{\\left.-\\langle\\mathbf{Q}^{x^{*},y^{*}}(s,\\cdot,\\cdot)y(\\cdot|s),x^{*}(y)(\\cdot|s)\\rangle\\right]}\\\\ &{}&{\\leq\\sum_{i=1}^{|\\mathcal{S}|}\\boldsymbol{d}_{\\rho_{0}}^{x^{*}(y),y}(s_{i})\\left[\\langle\\mathbf{Q}_{i}^{*}y_{i}^{*},x_{i}^{*}\\rangle-\\operatorname*{min}_{u_{i}\\in\\mathcal{X}_{i}}\\langle\\mathbf{Q}_{i}^{*}y_{i},u_{i}\\rangle\\right],\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\pmb{x}^{\\ast}(\\pmb{y})=\\underset{\\pmb{u}\\in\\mathcal{X}}{\\mathrm{argmin}}\\,J^{\\pmb{u},\\pmb{y}}(\\pmb{\\rho}_{0})$ and $\\begin{array}{r}{\\pmb{y}^{*}(\\pmb{x})=\\underset{\\pmb{w}\\in\\mathcal{Y}}{\\mathrm{argmax}}\\,J^{\\pmb{x},\\pmb{w}}(\\pmb{\\rho}_{0})}\\end{array}$ Similarly, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nJ^{x,y^{*}(x)}(\\rho_{0})-J^{x^{*},y^{*}}(\\rho_{0})\\leq\\sum_{i=1}^{|S|}d_{\\rho_{0}}^{x,y^{*}(x)}(s_{i})\\left[\\operatorname*{max}_{w_{i}\\in\\mathcal{Y}_{i}}\\langle(\\mathbf{Q}_{i}^{*})^{\\top}x_{i},w_{i}\\rangle-\\langle(\\mathbf{Q}_{i}^{*})^{\\top}x_{i}^{*},y_{i}^{*}\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By setting $\\psi_{i}(z):=\\operatorname*{max}\\{d_{\\rho_{0}}^{x,y^{\\ast}(x)}(s_{i}),d_{\\rho_{0}}^{x^{\\ast}(y),y}(s_{i})\\}$ and combining the facts that $f_{i}(\\mathbf{Q}^{*},\\pmb{x}_{i}^{*},\\pmb{y}_{i}^{*})-$ $\\operatorname*{min}_{{\\pmb u}_{i}\\in\\mathcal{X}_{i}}f_{i}({\\bf Q}^{*},{\\pmb u}_{i},{\\pmb y}_{i})\\geq0$ and $\\operatorname*{max}_{\\pmb{w}_{i}\\in\\mathcal{V}_{i}}f_{i}(\\mathbf{Q}^{\\ast},\\pmb{x}_{i},\\pmb{w}_{i})-f_{i}(\\mathbf{Q}^{\\ast},\\pmb{x}_{i}^{\\ast},\\pmb{y}_{i}^{\\ast})\\geq0$ derived from Corollary C.6, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nJ^{x,y^{*}(x)}(\\rho_{0})-J^{x^{*}(y),y}(\\rho_{0})\\leq\\sum_{i=1}^{d}\\psi_{i}(z)\\left[\\operatorname*{max}_{w_{i}\\in\\mathcal{Y}_{i}}f_{i}(\\mathbf{Q}^{*},x_{i},w_{i})-\\operatorname*{min}_{u_{i}\\in\\mathcal{X}_{i}}f_{i}(\\mathbf{Q}^{*},u_{i},y_{i})\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "C.3.2 Convex-Concave Minimax Problems ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we consider convex-concave minimax problem over compact concave region $\\mathcal{Z}=$ $\\mathcal{X}\\times\\mathcal{Y}\\subset\\mathbb{R}^{\\sum_{i=1}^{d}n_{i}}\\times\\mathbb{R}^{\\sum_{i=1}^{d}m_{i}}$ which satisfies Assumption C.1 with divergence-generating function $v$ . The standard convex-concave minimax problem is formulated as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{x}\\in\\mathcal{X}}\\operatorname*{max}_{\\pmb{y}\\in\\mathcal{Y}}f(\\pmb{x},\\pmb{y}),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $f$ is convex with respect to $\\textbf{\\em x}$ and concave with respect to $\\textit{\\textbf{y}}$ . Therefore, we obtain that ", "page_idx": 36}, {"type": "equation", "text": "$$\nf(\\mathbf{x},y^{*}(\\mathbf{x}))-f(\\mathbf{x}^{*}(y),y)\\leq\\left\\langle\\nabla_{x}f(z),x-x^{*}(y)\\right\\rangle+\\left\\langle-\\nabla_{y}f(z),y-y^{*}(x)\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for any $z=(x,y)\\in{\\mathcal{Z}}$ . We may therefore derive that $f$ satisfies GQCC condition with $g(z)\\equiv1$ and $f(\\mathbf{P}(z),z)\\,=\\,f(z)$ . Furthermore, assuming $\\nabla f$ is $L$ -lipschitz continuous (i.e., $\\|\\nabla f(z)-$ $\\nabla f(\\pmb{v})\\|_{*}\\,\\leq\\,L\\|z\\,-\\,\\pmb{v}\\|$ for any $z,v\\,\\in\\,{\\mathcal{Z}})$ and choosing $\\mathbf{P}\\equiv\\mathbf{0}$ , then verifying that $f$ satisfies the preconditions of general version of Theorem C.7 is reduced to verifying that $f$ satisfies (1) in Assumption C.3. Since $\\pmb{F}=\\nabla f$ only depends on variable $_{\\textit{z}}$ , it is evident that $f$ satisfies (1) in Assumption C.3 when $\\nabla f$ is L-Lipschitz. Therefore, under the smoothness condition of $f$ , Theorem C.7 implies that ${\\mathcal{O}}(\\varepsilon^{-1})$ iterations Algorithm 4 needs to find an $\\varepsilon$ -approximate Nash equilibrium of $f$ matches the lower bounds of $\\bar{\\Omega(\\varepsilon^{-1})}$ [52] for the number of iterations that any deterministic first-order method requires to find an $\\varepsilon$ -approximate Nash equilibrium of a smooth convex-concave function. ", "page_idx": 36}, {"type": "text", "text": "D Auxiliary Lemma ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Lemma D.1. For $\\Gamma\\geq17$ thefunction $g(\\Gamma)$ canbeboundedby $\\begin{array}{r}{\\frac{80640}{\\Gamma-1}+\\frac{2}{\\Gamma e^{-2}-1}}\\end{array}$ + re-2-1. Let g(T) be defined $\\begin{array}{r}{a s\\sum_{k=1}^{\\infty}\\Gamma^{-k}[k^{7}+(k+1)\\exp\\{2k\\}]}\\end{array}$ ", "page_idx": 36}, {"type": "text", "text": "Proof. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{g(\\Gamma)\\leq\\sum_{k=1}^{\\infty}\\left[\\Gamma^{-k}\\frac{\\left(k+7\\right)!}{k!}+\\left(\\frac{e^{2}}{\\Gamma}\\right)^{k}\\left(k+1\\right)\\right]}}\\ ~}\\\\ {{\\displaystyle{=\\frac{\\mathrm{d}^{7}}{\\mathrm{d}\\alpha^{7}}\\left(\\frac{\\alpha^{8}}{1-\\alpha}\\right)\\bigg\\vert_{\\alpha=\\Gamma^{-1}}+\\frac{\\mathrm{d}}{\\mathrm{d}\\alpha}\\left(\\frac{\\alpha^{2}}{1-\\alpha}\\right)\\bigg\\vert_{\\alpha=e^{2}\\Gamma^{-1}}}\\ ~}}\\\\ {{\\displaystyle{\\leq\\frac{80640}{(\\mathrm{}\\Gamma-1}+\\frac{2}{\\Gamma e^{-2}-1},}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "(a) can be deduced based on the following inequality ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\mathrm{d}^{7}}{\\mathrm{d}\\alpha^{7}}\\left(\\frac{\\alpha^{8}}{1-\\alpha}\\right)=\\displaystyle\\sum_{k=0}^{7}(-1)^{k}\\left(\\frac{7}{k}\\right)\\frac{8k!}{(k+1)!}\\left(\\frac{\\alpha}{1-\\alpha}\\right)^{k+1}}&{}\\\\ {\\frac{\\mathrm{s}7!}{6!}\\displaystyle\\sum_{k=1}^{8}\\left(\\frac{8}{k}\\right)\\left(\\frac{\\alpha}{1-\\alpha}\\right)^{k}}&{}\\\\ {=\\displaystyle\\mathcal{T}\\left[\\left(1+\\frac{\\alpha}{1-\\alpha}\\right)^{8}-1\\right]}&{}\\\\ {\\leq\\mathcal{T}\\left[\\exp\\left\\{\\frac{8\\alpha}{1-\\alpha}\\right\\}-1\\right]}&{}\\\\ {\\frac{\\mathrm{s}9640\\alpha}{(6)!-1\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where (b) and (c) are derived from Leibniz equation, and the inequality $e^{x}-1\\,\\leq\\,2x$ holds for $0\\leq x\\leq1/2$ respectively. ", "page_idx": 37}, {"type": "text", "text": "Lemma D.2. For any $n\\in\\mathbb{N},r\\in\\mathbb{R}^{n},\\pmb{p}\\in\\Delta^{n}$ if it holds that $\\pmb{p}^{*}=\\arg\\operatorname*{min}_{\\pmb{p}\\in\\Delta_{n}}\\eta\\left<\\pmb{p},\\pmb{r}\\right>+\\mathrm{KL}(\\pmb{p}\\|\\pmb{q}),$ then we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\langle p^{*}-p,r\\rangle=\\!{\\frac{1}{\\eta}}\\left(\\mathrm{KL}(p\\|q)-\\mathrm{KL}(p\\|p^{*})-\\mathrm{KL}(p^{*}\\|q)\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. We just need to prove $\\begin{array}{r}{\\pmb{p}^{\\ast}(i)\\equiv\\pmb{p}^{\\prime}(i):=\\frac{\\pmb{q}(i)\\exp\\left\\{-\\eta\\pmb{r}(i)\\right\\}}{\\sum_{j=1}^{n}\\pmb{q}(j)\\exp\\left\\{-\\eta\\pmb{r}(j)\\right\\}}}\\end{array}$ for any $i\\in[n]$ which satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\langle p-p^{\\prime},\\eta r+\\log(p^{\\prime})-\\log(q)\\rangle=0,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for any $\\pmb{p}\\in\\Delta_{n}$ . Assume that $F(\\pmb{p}):=\\eta\\left<\\pmb{p},\\pmb{r}\\right>+\\mathrm{KL}(\\pmb{p}\\|\\pmb{q})$ and define $\\begin{array}{r}{\\mathcal{E}(\\pmb{p})=\\sum_{i=1}^{n}p(i)\\log(p(i))}\\end{array}$ for any $\\pmb{p}\\in\\Delta_{n}$ . Clearly, ${\\pmb{p}}^{\\prime}\\in\\Delta_{n}$ . Hence, for all $\\pmb{p}\\in\\Delta_{n}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\pmb{p})=\\eta\\left\\langle\\pmb{p},\\pmb{r}\\right\\rangle+\\mathrm{KL}(\\pmb{p}||\\pmb{q})}\\\\ &{\\quad\\quad=\\eta\\left\\langle\\pmb{p}^{\\prime},\\pmb{r}\\right\\rangle+\\mathrm{KL}(\\pmb{p}^{\\prime}||\\pmb{q})+\\left\\langle\\pmb{p}-\\pmb{p}^{\\prime},\\eta\\pmb{r}-\\log(\\pmb{q})\\right\\rangle+\\mathcal{E}(\\pmb{p})-\\mathcal{E}(\\pmb{p}^{\\prime})}\\\\ &{\\quad\\quad\\quad=\\eta\\left\\langle\\pmb{p}^{\\prime},\\pmb{r}\\right\\rangle+\\mathrm{KL}(\\pmb{p}^{\\prime}||\\pmb{q})+\\mathcal{E}(\\pmb{p})-\\mathcal{E}(\\pmb{p}^{\\prime})+\\left\\langle\\pmb{p}-\\pmb{p}^{\\prime},-\\log(\\pmb{p}^{\\prime})\\right\\rangle}\\\\ &{\\quad\\quad\\quad=F(\\pmb{p}^{\\prime})-\\mathrm{KL}(\\pmb{p}||\\pmb{p}^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where (a) is derived from Eq. (129). Therefore, we obtain that $\\ensuremath{\\boldsymbol{p}}^{*}\\equiv\\ensuremath{\\boldsymbol{p}}^{\\prime}$ . By using equality (b), we finish the proof. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Lemma D.3. Suppose thatfor $\\tau\\in(0,1)$ we have $\\left\\|{\\frac{p}{q}}\\right\\|_{\\infty}\\leq1+\\tau$ Then ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left(\\frac{1-\\tau}{2}-\\frac{2\\tau}{3(1-\\tau)}\\right)\\chi^{2}(\\pmb{p},\\pmb{q})\\leq\\mathrm{KL}(\\pmb{p}||\\pmb{q}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Profidalxaionoft $\\begin{array}{r}{\\log(1+x)=\\sum_{k=1}^{\\infty}\\frac{(-1)^{k-1}}{k}x^{k}}\\end{array}$ and define $\\begin{array}{r}{Q_{\\tau,D}(x):=x-\\left(\\frac{1}{2}+D\\tau\\right)x^{2}}\\end{array}$ . According to ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\log(1+x)-Q_{\\tau,D}(x)\\geq D\\tau x^{2}-{\\frac{|x|^{3}}{3(1-\\tau)}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for any $x\\in[-\\tau,\\tau]$ we have $\\log(1+x)\\geq Q_{\\tau,D}(x)$ when $\\begin{array}{r}{D\\ge\\frac{1}{3(1-\\tau)}}\\end{array}$ and $x\\in[-\\tau,\\tau]$ Therefore, we obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(p\\|q)=\\displaystyle\\sum_{j=1}^{n}p(j)\\log\\left(\\frac{p(j)}{q(j)}\\right)}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\sum_{j=1}^{n}p(j)\\left[\\left(\\frac{p(j)}{q(j)}-1\\right)-\\left(\\frac{1}{2}+D\\tau\\right)\\left(\\frac{p(j)}{q(j)}-1\\right)^{2}\\right]}\\\\ &{\\qquad=\\displaystyle X^{2}(p,q)-\\left(\\frac{1}{2}+D\\tau\\right)\\sum_{j=1}^{n}\\frac{p(j)}{q(j)}q(j)\\left(\\frac{p(j)}{q(j)}-1\\right)^{2}}\\\\ &{\\qquad\\qquad\\geq X^{2}(p,q)-\\left(\\frac{1+\\tau}{2}+D\\tau(1+\\tau)\\right)X^{2}(p,q)}\\\\ &{\\qquad=\\left(\\frac{1-\\tau}{2}-D\\tau(1+\\tau)\\right)X^{2}(p,q).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We complete the proof if $\\begin{array}{r}{D=\\frac{1}{3(1-\\tau)}}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "Lemma D.4. Suppose that $\\pmb{r}\\,\\in\\,\\mathbb{R}^{n},\\pmb{\\tau}\\,\\in\\,(0,1/2),\\|\\pmb{r}\\|_{\\infty}\\,\\le\\,\\frac{\\tau}{2}$ , and $\\pmb{p},\\tilde{\\pmb{p}}\\,\\in\\,\\Delta_{n}$ satisfy, for each $j\\in[n]$ \uff0c ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{p}}(j)=\\frac{\\pmb{p}(j)\\cdot\\exp\\{\\pmb{r}(j)\\}}{\\sum_{j^{\\prime}\\in[n]}\\pmb{p}(j^{\\prime})\\cdot\\exp\\{\\pmb{r}(j^{\\prime})\\}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left(1-\\left(\\frac{2}{3(1-\\tau)}+4\\right)\\tau\\right)\\mathrm{Var}_{p}(r)\\leq\\mathcal{X}^{2}(\\tilde{p},p)\\leq\\left(1+\\left(\\frac{2}{3(1-\\tau)}+4\\right)\\tau\\right)\\mathrm{Var}_{p}(r).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Without loss of generality, we consider the case $\\langle\\pmb{p},\\pmb{r}\\rangle=0$ . If not, redefine $\\tilde{\\pmb{r}}:=\\pmb{r}-\\langle\\pmb{p},\\pmb{r}\\rangle$ $e(\\|\\tilde{\\boldsymbol{r}}\\|_{\\infty}\\leq\\tau)$ and analyze $\\tilde{\\pmb{r}}$ where $e\\in\\mathbb{R}^{n}$ is an all 1 vector. It's clear that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{X}^{2}(\\tilde{\\pmb{p}},\\pmb{p})=-1+\\sum_{j=1}^{n}p(j)\\left(\\frac{\\tilde{p}(j)}{p(j)}\\right)^{2}=-1+\\mathbb{E}_{p}\\left[\\frac{\\exp\\{r\\}}{\\mathbb{E}_{p}[\\exp\\{r\\}]}\\right]^{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We define $\\begin{array}{r}{F_{D}^{1}(x):=1+x+\\frac{1-D\\tau}{2}x^{2},F_{D}^{2}(x):=1+x+\\frac{1+D\\tau}{2}x^{2}}\\end{array}$ and note that fo any $x\\in[-\\tau,\\tau]$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\exp\\{x\\}-F_{D}^{1}(\\boldsymbol{x})\\ge\\frac{D\\tau}{2}x^{2}-\\frac{x^{3}}{6},}\\\\ {\\displaystyle F_{D}^{2}(\\boldsymbol{x})-\\exp\\{x\\}\\ge\\frac{D\\tau}{2}x^{2}-\\frac{|x|^{3}}{6(1-\\tau)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where Eq. (136) is derived from the summation of the $2k$ -th and $2k+1$ -th $\\left(k\\geq2\\right)$ terms in the Taylor expansion of exp[} isalways non-negative, Eq.(137)is derived from3 ( () for any $x\\in[-\\tau,\\tau]$ . Therefore, we have $\\exp\\{x\\}-F_{D}^{1}(x)\\geq0$ and $F_{D}^{2}(x)-\\exp\\{x\\}\\geq0$ for all $x\\in[-\\tau,\\tau]$ \u00fc $\\begin{array}{r}{D\\ge\\frac{1}{3(1-\\tau)}}\\end{array}$ . Then, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n1+2x+(2-(D+2)\\tau)x^{2}\\leq(\\exp\\{x\\})^{2}\\leq1+2x+(2+(D+2)\\tau)x^{2},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "when $D\\tau\\leq{\\frac{1}{2}}$ . In addition, by $\\langle\\pmb{p},\\pmb{r}\\rangle=0$ , it's obvious that ", "page_idx": 38}, {"type": "equation", "text": "$$\n1+\\frac{1-D\\tau}{2}\\mathbb{E}_{p}[r^{2}]\\leq\\mathbb{E}_{p}[\\exp\\{r\\}]\\leq1+\\frac{1+D\\tau}{2}\\mathbb{E}_{p}[r^{2}].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Combining Eq. (138) and (139), we derived that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1+(1-(D+1)\\tau)\\mathbb{E}_{p}[r^{2}]\\leq\\left(\\mathbb{E}_{p}[\\exp\\{r\\}]\\right)^{2}\\leq1+(1+(D+1)\\tau)\\mathbb{E}_{p}[r^{2}],}\\\\ &{1+(2-(D+2)\\tau)\\mathbb{E}_{p}[r^{2}]\\leq\\!\\mathbb{E}_{p}\\left[(\\exp\\{r\\})^{2}\\right]\\leq1+(2+(D+2)\\tau)\\mathbb{E}_{p}[r^{2}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for $D\\tau\\leq{\\frac{1}{2}}$ . According to Eq. (135) ,(140) and (141), we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-1+\\mathbb{E}_{p}\\left[\\frac{\\exp\\{r\\}}{\\mathbb{E}_{p}[\\exp\\{r\\}]}\\right]^{2}\\ge\\frac{(1-(2D+3)\\tau)\\mathbb{E}_{p}[r^{2}]}{1+(1+(D+1)\\tau)\\mathbb{E}_{p}[r^{2}]}\\ge(1-(2D+4)\\tau)\\mathbb{E}_{p}[r^{2}],}\\\\ &{-1+\\mathbb{E}_{p}\\left[\\frac{\\exp\\{r\\}}{\\mathbb{E}_{p}[\\exp\\{r\\}]}\\right]^{2}\\le\\frac{(1+(2D+3)\\tau)\\mathbb{E}_{p}[r^{2}]}{1+(1-(D+1)\\tau)\\mathbb{E}_{p}[r^{2}]}\\le(1-(2D+4)\\tau)\\mathbb{E}_{p}[r^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We derive Eq. (134) by setting $\\begin{array}{r}{D=\\frac{1}{3(1-\\tau)}}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Lemma D.5 (Lemma B.6, [15]). Let $\\phi_{1},\\cdot\\cdot\\cdot\\,,\\phi_{l}$ be softmax-type functions. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\phi_{i}(\\pmb{x})=\\frac{\\exp\\{\\pmb{x}(j_{i})\\}}{\\sum_{k=1}^{n}\\tau_{i k}\\exp\\{\\pmb{x}(k)\\}},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $j_{i}\\in[1,\\cdot\\cdot\\cdot,n],\\textstyle\\sum_{k=1}^{n}\\tau_{i k}=1$ for any $i\\in[1,\\cdots,l]$ Let $\\begin{array}{r}{P(\\pmb{x})=\\sum_{k=0}\\sum_{|\\alpha|=k}\\frac{D^{\\alpha}P(\\mathbf{0})}{\\alpha!}\\pmb{x}^{\\alpha}}\\end{array}$ denote the Taylor series of $\\textstyle\\prod_{i=1}^{l}\\phi_{i}$ .Then for any integer $k$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{|\\alpha|=k}{\\frac{|D^{\\alpha}P(\\mathbf{0})|}{\\alpha!}}\\leq(e^{3}l)^{k}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We introduce the conception of $(Q,R)$ -bounded function briefly. Suppose $\\phi\\,:\\,\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ is realanalytic in a neighborhood of the origin. For real numbers $Q,R>0$ , we say that $\\phi$ is $(Q,R)$ -bounded if the Taylor expansion of \u03a6 at 0, denoted Ps(x) =Z=o \u2265l\u03b1l=k ( Df\u03b1, satisfies, for each $\\begin{array}{r}{i\\geq0,\\sum_{|\\alpha|=k}\\frac{|D^{\\alpha}\\phi(\\mathbf{0})|}{\\alpha!}\\leq Q\\cdot R^{k}}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Lemma D.6 (Detailed version of Lemma 4.5, [15]). Suppose that $h,n\\in\\mathbb{N},\\phi:\\mathbb{R}^{n}\\to\\mathbb{R}$ is $a$ $(Q,R)$ -bounded function such that the radius of convergence of its power series at O is at least $\\nu~>~0$ and ${\\cal Z}\\;=\\;\\{{\\cal Z}^{0},\\cdot\\cdot\\cdot\\cdot,{\\cal Z}^{T}\\}\\;\\subset\\;\\mathbb{R}^{n}$ is a sequence of vectors satisfying $\\left\\|Z^{t}\\right\\|_{\\infty}\\,\\le\\,\\nu$ for $t\\in[0,\\cdots\\,,T]$ . Suppose for some $\\beta\\in(0,1)$ for each $0\\leq h^{\\prime}\\leq h$ and $t\\in[0,\\cdot\\cdot\\cdot\\,,T-{\\bar{h}}^{\\prime}],$ it holds that $\\begin{array}{r}{\\left\\|D_{h^{\\prime}}Z^{t}\\right\\|_{\\infty}\\leq\\frac{1}{\\Gamma R}\\beta^{h^{\\prime}}(h^{\\prime})^{B h^{\\prime}}}\\end{array}$ for some $B\\geq3,\\Gamma\\geq e^{3}$ .Then for all $t\\in[0,\\cdot\\cdot\\cdot\\,,T-h],$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|(D_{h}(\\phi\\circ Z))^{t}\\right|\\leq Q\\cdot g(\\Gamma)\\cdot\\beta^{h}h^{B h+1},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $g(\\Gamma)$ is a bounded function with respect to $\\Gamma$ ", "page_idx": 39}, {"type": "text", "text": "Proof. Without loss of generality, we assume $\\phi(\\mathbf{0})\\quad=\\quad0$ . We define $\\begin{array}{r l}{(\\phi\\;\\circ\\;Z)^{t}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{\\gamma\\in\\mathbb{Z}_{\\geq0}^{n}:|\\gamma|=k}a_{\\gamma}\\left(Z^{t}\\right)^{\\gamma}}\\end{array}$ and obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left(D_{k}(\\phi\\circ Z)\\right)^{*}\\Bigg|^{2}=\\left|\\frac{\\displaystyle\\sum_{i=1}^{N}}{\\displaystyle\\sum_{i=1}^{N}\\tau_{i}\\hat{Z}_{z,i}^{[i]}\\hat{\\eta}^{[i]}}a_{z}(D_{h}Z^{*})^{\\dagger}\\right|}\\ ~~}\\\\ &{\\leq\\displaystyle\\sum_{k=1}^{N}\\sum_{\\tau\\in\\mathbb{Z}_{\\geq0}^{N}\\cap\\{r\\}=1}^{N}\\left|a_{\\tau}\\right|\\left(\\sum_{\\sigma\\mid h\\mid k}\\prod_{i=1}^{N}\\left|\\left(E_{t_{i},r_{i}}^{\\prime}D_{h\\times,j}^{\\prime}Z(t_{i,j}^{\\tau})\\right)^{\\dagger}\\right|\\right)}\\\\ &{\\leq\\displaystyle\\sum_{k=1}^{N}\\sum_{\\tau\\in\\mathbb{Z}_{\\geq0}^{N}\\cap\\{r\\}=1}^{N}\\left|a_{\\tau}\\right|\\cdot\\left(\\frac{\\displaystyle\\hat{\\beta}^{k}}{\\displaystyle\\prod^{N}}\\cdot\\left(\\sum_{i\\in[N]^{k}\\setminus i\\in[N]}\\frac{\\displaystyle\\hat{\\beta}^{k}W_{t_{i},r_{i}}^{\\prime}}{\\displaystyle\\sum_{s\\in\\mathbb{Z}_{\\geq0}^{N}\\setminus i\\}}\\right)\\right.}\\\\ &{\\leq\\displaystyle\\sum_{k=1}^{N}\\sum_{\\tau\\in\\mathbb{Z}_{\\geq0}^{N}\\cap\\{r\\}=1}^{N}\\left|a_{\\tau}\\right|\\cdot\\frac{\\displaystyle\\hat{\\beta}^{k}}{\\displaystyle\\prod^{N}}b^{\\tau}\\operatorname*{lims}\\left\\{k^{\\tau}(h k+1)\\exp\\left\\{\\frac{2k}{h^{k-1}}\\right\\}\\right\\}}\\\\ &{\\left.\\leq\\displaystyle\\sum_{k=1}^{N}\\sum_{\\tau\\in\\mathbb{Z}_{\\geq0}^{N}\\cap\\{r\\}}\\left|\\frac{\\displaystyle\\hat{\\beta}^{k}}{\\displaystyle\\prod^{N}}\\cdot\\operatorname*{max}\\left\\{k^{\\tau}(k+1)\\exp\\left\\{2k\\right\\}\\cdot\\beta^{k}h^{\\tau_{i}+1}\\right.}\\\\ &{\\leq\\displaystyle\\operatorname*{det}(N)\\cdot\\beta^{k}h^{\\tau_{i}+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma D.7 (Lemma C.4, [15]). Let $\\{n,T\\}\\,\\subset\\,\\mathbb{Z}_{+}$ with $n\\ \\geq\\ 2$ and $T\\ \\geq\\ 4$ . we select $H:=$ $\\begin{array}{r}{\\lceil\\log(T)\\rceil,\\beta_{0}=\\frac{1}{4H}}\\end{array}$ $\\begin{array}{r}{\\beta=\\frac{\\sqrt{\\beta_{0}/8}}{H^{3}}}\\end{array}$ .Assume that $\\{z^{t}\\}_{t=1}^{T}\\subset[0,1]^{n}$ and $\\{p^{t}\\}_{t=1}^{T}\\subset\\Delta_{n}$ satisfy the following condition ", "page_idx": 40}, {"type": "text", "text": "Then, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathrm{Var}_{p^{t}}(z^{t}-z^{t-1})\\leq2\\beta_{0}\\sum_{t=1}^{T}\\mathrm{Var}_{p^{t}}(z^{t-1})+165120(1+\\zeta)H^{5}+2.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proposition D.8. Given a constant $c>0$ wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{t}\\left({\\frac{c}{c+k}}\\right)^{2}\\leq c.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Lemma D.9. For a constant $c\\geq c^{\\prime}>0,$ the following inequality holds ", "page_idx": 40}, {"type": "equation", "text": "$$\nc^{\\prime}\\log\\left(\\frac{c+t-1}{c+T}\\right)-\\frac{(c^{\\prime}+c^{\\prime}c)^{2}}{2c}\\leq\\sum_{k=t}^{T}\\log\\left(1-\\frac{c^{\\prime}}{c+k}\\right)\\leq c^{\\prime}\\log\\left(\\frac{c+t}{c+1+T}\\right),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "when $T>t\\geq1$ ", "page_idx": 40}, {"type": "text", "text": "Proof. Accoding to the Taylor expansion of $\\log(1-x)$ when $x<1$ , we obtain the estimation of $\\begin{array}{r}{\\log\\left(1-\\frac{c^{\\prime}}{c+k}\\right)}\\end{array}$ for any $k\\geq1$ as folows ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\log\\left(1-\\displaystyle\\frac{c^{\\prime}}{c+k}\\right)\\leq-\\displaystyle\\frac{c^{\\prime}}{c+k},}\\\\ {\\log\\left(1-\\displaystyle\\frac{c^{\\prime}}{c+k}\\right)\\geq-\\displaystyle\\frac{c^{\\prime}}{c+k}-\\displaystyle\\frac{(c^{\\prime}+c c^{\\prime})^{2}}{2}\\left(\\displaystyle\\frac{1}{c+k}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Next, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=t}^{T}-\\frac{c^{\\prime}}{c+k}\\leq-\\int_{t}^{T+1}\\frac{c^{\\prime}}{c+x}d x=c^{\\prime}\\log\\left(\\frac{c+t}{c+1+T}\\right),}\\\\ &{\\displaystyle\\sum_{k=t}^{T}\\left[-\\frac{c^{\\prime}}{c+k}-\\frac{(c^{\\prime}+c c^{\\prime})^{2}}{2}\\left(\\frac{1}{c+k}\\right)^{2}\\right]\\geq-\\int_{t-1}^{T}\\left[\\frac{c^{\\prime}}{c+x}+\\frac{(c^{\\prime}+c c^{\\prime})^{2}}{2}\\left(\\frac{1}{c+x}\\right)^{2}\\right]\\mathrm{d}x}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\geq c^{\\prime}\\log\\left(\\frac{c+t-1}{c+T}\\right)-\\frac{(c^{\\prime}+c c^{\\prime})^{2}}{2c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "E Limitation ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "For objectives with GQC condition (GQCC condition) and general smooth internal function (i.e. Lipschitz continuous internal function), our analytical method might not provide similar iteration complexity. We leave the related algorithmic analysis on more generalized smoothness conditions as afuturework. ", "page_idx": 40}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 41}, {"type": "text", "text": "Answer: YES ", "page_idx": 41}, {"type": "text", "text": "Justification: We have a detailed explanation in the contribution section of the introduction. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 41}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 41}, {"type": "text", "text": "Answer: YES ", "page_idx": 41}, {"type": "text", "text": "Justification: See Section E in appendix. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: YES ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide the assumptions and the associated theoretical results in Section 3 and Section 4, respectively. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 42}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 42}, {"type": "text", "text": "Answer: NA ", "page_idx": 42}, {"type": "text", "text": "Justification: ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPs does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: NA Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : / /nips . cC/ public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 43}, {"type": "text", "text": "Answer: NA Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: NA Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: NA Justification: Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: / /neurips.Cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: YES ", "page_idx": 44}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: NA ", "page_idx": 44}, {"type": "text", "text": "Justification: Since this paper is a theoretical paper, it may not have other social impacts. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 45}, {"type": "text", "text": "Answer: NA ", "page_idx": 45}, {"type": "text", "text": "Justification: ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 45}, {"type": "text", "text": "Answer: NA Justification: Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/ dataset s has curated licenses for some datasets. Their licensing guide can help determine the license of adataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 46}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 46}, {"type": "text", "text": "Answer: NA Justification: Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 46}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 46}, {"type": "text", "text": "Answer: NA Justification: Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]