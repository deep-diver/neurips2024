[{"figure_path": "7Kz7icCZ6H/tables/tables_4_1.jpg", "caption": "Table 2: Zero-Shot evaluation on TV-Caption dataset. All the models are provided with the names of the characters in the scene. All the models use 7B LLMs.", "description": "This table presents the zero-shot evaluation results on the TV-Caption dataset for several video-LLMs, including CALVIN.  The models were not fine-tuned on this dataset; instead, character names were added to the prompts to provide context. The table compares the performance of different models across four metrics: BertScore, CIDEr, ROUGE-L, and SPICE, all of which are commonly used to evaluate the quality of video captions.  The inclusion of character names in prompts aims to evaluate the models' ability to generate captions that incorporate contextual information.", "section": "4.1 Results"}, {"figure_path": "7Kz7icCZ6H/tables/tables_5_1.jpg", "caption": "Table 1: Evaluation on MAD-named-eval split. The top half represents models evaluated without context. The bottom half shows the models trained/evaluated with context. Context column - the numbers in brackets show the number of scenes used in context. \u2020- The numbers are from the original papers as the models are not public.", "description": "This table presents the performance comparison of different models on the MAD-named-eval dataset for contextual video captioning.  The models are evaluated with and without context.  The metrics used for evaluation are BertScore, CIDER, ROUGE-L, and SPICE.  The table highlights the significant improvement achieved by CALVIN, particularly when using contextual information.  Note that some SOTA results are taken from other papers, indicated by \u2020.", "section": "4.1 Results"}, {"figure_path": "7Kz7icCZ6H/tables/tables_5_2.jpg", "caption": "Table 2: Zero-Shot evaluation on TV-Caption dataset. All the models are provided with the names of the characters in the scene. All the models use 7B LLMs.", "description": "This table presents a zero-shot evaluation of various video-LLMs on the TV-Caption dataset.  Zero-shot means the models weren't fine-tuned on this dataset; they were evaluated using only their pre-trained weights.  The models were all 7B LLMs (large language models).  The results show the performance of each model on four metrics: BertScore, CIDER, ROUGE-L, and SPICE, which measure different aspects of caption quality. Note that the character names were provided as context to all models.", "section": "4.1 Results"}, {"figure_path": "7Kz7icCZ6H/tables/tables_6_1.jpg", "caption": "Table 3: Model Ablations: Unless otherwise stated, all models are trained for the same number of iterations and on the same dataset. Unless otherwise stated, all the models are evaluated with a 3-scene context on MAD-named-eval split. \u2605-refers to the CALVIN 7B variant which we discuss throughout the paper", "description": "This table presents the results of ablation studies performed on the CALVIN model.  The ablation studies systematically remove or modify different components or training parameters to assess their impact on the model's performance.  The table shows the effect on four key metrics (BertScore, CIDER, ROUGE-L, and SPICE)  by varying training data (removing MAD, WebVideo, image VQA data, or all stage-2 data),  finetuning components (Q-Former, LLM, or both), adjusting hyperparameters (number of Q-Former tokens and LLM tuning methods), and changing the training strategy (placing MAD in Stage 2 or 3, or adjusting learning rate).  The star symbol (\u2605) indicates the configuration of CALVIN 7B used as the baseline for comparison.", "section": "4.2 Ablations"}, {"figure_path": "7Kz7icCZ6H/tables/tables_8_1.jpg", "caption": "Table 4: Test-time adaptation results: (first row) CALVIN evaluation without context. (second row) 'Entities' means the context has just the list of entities in the scene as discussed in Sec. 5.1. (third and fourth rows) For few-shot training from Sec. 5.2, the number in brackets counts examples used in finetuning. Both strategies improve performance over the no-context.(\u2020 Numbers differ slightly from Tab. 1 since the numbers presented here are an average of metrics computed one movie at a time, and some metrics like CIDEr depend on the word distribution of evaluation set.)", "description": "This table presents the results of different test-time adaptation strategies for the CALVIN model on the MAD-eval dataset. It compares the performance of CALVIN without any context, with only entity information in the prompt, and with few-shot finetuning on 20 or 50 samples from each movie. The results are shown for BertScore, CIDEr, ROUGE-L, and SPICE metrics. The numbers in parentheses indicate the improvement over the baseline model without context. Note that slight discrepancies from Table 1 may exist due to averaging metrics calculated per movie, affecting metrics like CIDEr based on word distribution.", "section": "5.3 Results"}, {"figure_path": "7Kz7icCZ6H/tables/tables_17_1.jpg", "caption": "Table 1: Evaluation on MAD-named-eval split. The top half represents models evaluated without context. The bottom half shows the models trained/evaluated with context. Context column - the numbers in brackets show the number of scenes used in context. \u2020- The numbers are from the original papers as the models are not public.", "description": "This table presents the results of evaluating different models on the MAD-named-eval dataset, which is a subset of the Movie Audio Descriptions dataset that includes character names in the captions.  The top half shows the performance of models evaluated without using any context from previous scenes.  The bottom half shows the results for models trained and/or evaluated using context from previous scenes (the number of scenes used as context is shown in brackets).  The table compares different metrics, including BertScore, CIDEr, ROUGE-L, and SPICE, to assess the quality of the generated captions. The \u2020 symbol indicates that the numbers for some models were taken from the original papers, as the model implementations are not publicly available.", "section": "4.1 Results"}, {"figure_path": "7Kz7icCZ6H/tables/tables_18_1.jpg", "caption": "Table 6: Ground truth vs Predicted captions on CALVIN-3S model: We present an example from each movie in the MAD-eval dataset.", "description": "This table provides a qualitative comparison of the ground truth captions and the captions generated by the CALVIN-3S model for a subset of movies from the MAD-eval dataset.  Each row represents a movie, showing the original caption written by a human and the corresponding prediction from the model.  The purpose is to illustrate the model's ability to generate captions that are similar in meaning and style to the original captions.", "section": "Qualitative analysis of CALVIN vs ground truth"}, {"figure_path": "7Kz7icCZ6H/tables/tables_18_2.jpg", "caption": "Table 7: Human acceptable predictions: Some ground truth captions vs CALVIN-3S generated captions which the authors felt are acceptable despite being different from the GT. All the examples are from the MAD-eval movie, How do you know?", "description": "This table shows a qualitative comparison of ground truth captions and captions generated by the CALVIN-3S model.  The authors selected examples where the generated captions, while differing from the ground truth, were still considered acceptable interpretations of the scene.  All examples are from the \"How Do You Know?\" movie in the MAD-eval dataset.", "section": "Results"}]