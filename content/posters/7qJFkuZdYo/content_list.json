[{"type": "text", "text": "Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuanpu Cao, Tianrong Zhang, Bochuan Cao, Ziyi Yin, Lu Lin, Fenglong Ma, Jinghui Chen The Pennsylvania State University {ymc5533, tbz5156, bccao, zmy5171, lulin, fenglong, jzc5917}@psu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Researchers have been studying approaches to steer the behavior of Large Language Models (LLMs) and build personalized LLMs tailored for various applications. While fine-tuning seems to be a direct solution, it requires substantial computational resources and may significantly affect the utility of the original LLM. Recent endeavors have introduced more lightweight strategies, focusing on extracting \u201csteering vectors\u201d to guide the model\u2019s output toward desired behaviors by adjusting activations within specific layers of the LLM\u2019s transformer architecture. However, such steering vectors are directly extracted from the activations of human preference data and thus often lead to suboptimal results and occasional failures, especially in alignment-related scenarios. In this work, we propose an innovative approach that could produce more effective steering vectors through bi-directional preference optimization. Our method is designed to allow steering vectors to directly influence the generation probability of contrastive human preference data pairs, thereby offering a more precise representation of the target behavior. By carefully adjusting the direction and magnitude of the steering vector, we enabled personalized control over the desired behavior across a spectrum of intensities. Extensive experimentation across various open-ended generation tasks, particularly focusing on steering AI personas, has validated the efficacy of our approach. Moreover, we comprehensively investigate critical alignment-concerning scenarios, such as managing truthfulness, mitigating hallucination, and addressing jailbreaking attacks alongside their respective defenses. Remarkably, our method can still demonstrate outstanding steering effectiveness across these scenarios. Furthermore, we showcase the transferability of our steering vectors across different models/LoRAs and highlight the synergistic benefits of applying multiple vectors simultaneously. These findings significantly broaden the practicality and versatility of our proposed method. Code is available at https://github.com/CaoYuanpu/BiPO ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, the generalization capabilities of Large Language Models (LLMs) [33, 22] have improved substantially, driven by the increase in parameter size and the expansion of training text corpus [24, 16]. Despite the strong generalization capabilities of LLMs across diverse fields [11, 32, 8, 21, 13, 36, 40], researchers are still actively investigating approaches to steer the behaviors of LLMs and develop personalized models tailored to meet the specific needs of different users [38, 34]. While fine-tuning techniques such as supervised fine-tuning and reinforcement learning from human feedback [23, 44] appear to be straightforward solutions, they demand significant computational resources and may substantially impact the utility of the original LLM. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). ", "page_idx": 0}, {"type": "text", "text": "Alternatively, a series of lightweight methods for steering the behavior of LLMs typically involves freezing model weights and introducing perturbations in the activation space to elicit desired changes in the generated text [31, 34, 28, 35]. In particular, recent efforts have introduced the extraction of \u201csteering vectors\u201d within specific layers of the LLM\u2019s transformer architecture [28, 35]. During inference, these steering vectors are added back to the activations of the original prompt to guide the model\u2019s output toward desired behaviors. Rimsky et al. [28] has demonstrated that a steering vector corresponding to a specific behavior possesses universal applicability across various user prompts. Furthermore, since a single steering vector only influences the activation of a specific layer without altering the model weights, it minimally disrupts the original model, thereby preserving the model\u2019s general capabilities. ", "page_idx": 1}, {"type": "text", "text": "However, existing steering vectors are typically extracted by the activation difference when the LLM is prompted with two opposite preferences [28, 35], ignoring its actual generation that may diverge from the prompt, thus often resulting in suboptimal outcomes and occasional failures, particularly in critical alignment-related scenarios. Specifically, such methods first construct a set of contrastive prompt pairs, where two prompts in each pair are appended with opposite answers to the same question \u2013 one answer is associated with the target behavior, while the other corresponds to the opposite preference. The steering vector representing the target behavior is then derived as the average activation difference of the LLM on all pairs. However, we have observed that the vector extracted from prompt pairs has limited steering capability in the model\u2019s generation \u2013 the model may generate texts that are not aligned with the prompted choice, even when the steering vector is applied to each generation step. This discrepancy indicates that steering vectors purely based on contrastive prompts may not accurately represent the target generation behavior of the model. ", "page_idx": 1}, {"type": "text", "text": "To fill in this gap, drawing inspiration from the recent preference optimization techniques such as Direct Preference Optimization (DPO) [27], we introduce an innovative approach to calculate more effective steering vectors via $B i$ -directional Preference Optimization (BiPO). Instead of relying on the model to \u201cfollow\u201d a prompted direction, our method allows the model to \u201cspeak up\u201d, enabling steering vectors to proactively modulate the generation probability of contrastive human preference data pairs, thus providing a more precise representation of the target behavior. By further manipulating the direction and magnitude of the optimized steering vectors, we can easily achieve varying degrees of steering effects for desired behaviors, thereby efficiently meeting users\u2019 diverse personalization needs without necessitating additional model training. ", "page_idx": 1}, {"type": "text", "text": "We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We have analyzed current methods for extracting steering vectors in LLMs, identifying their potential limitations and failure cases. Based on these insights, we propose a bi-directional preference optimization (BiPO) to generate more effective steering vectors, enabling personalized control over the various behaviors.   \n\u2022 Our approach demonstrates exceptional efficacy through comprehensive experiments on various open-ended generation tasks, with a particular focus on steering AI personas. Moreover, we extensively examine crucial alignment-related scenarios such as managing truthfulness, mitigating hallucinations, and addressing jailbreaking attacks and their defenses. Our method consistently exhibits remarkable steering effectiveness in these scenarios.   \n\u2022 We further explore and confirm the notable transferability of the steering vectors produced by our method across different models and fine-tuned LoRAs [10, 6]. Our findings also demonstrate that vectors steering distinct behaviors can operate synergistically, thereby enabling a broader spectrum of steering applications. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Activation Engineering Activation engineering typically involves freezing model weights and modifying activations to produce desired changes in the output text [31, 34, 28, 35, 18, 14, 45]. Several studies have focused on searching for \u201csteering vectors\u201d within the activation space of specific layers in the transformer architecture of LLMs. During inference, these vectors are incorporated back into the forward passes of user prompts to steer model generation [31, 34, 28, 35]. Particularly, Subramani et al. [31] successfully discovered sentence-specific vectors capable of generating target sentences with near-perfect BLEU scores. However, this method requires running gradient descent to derive a unique steering vector for each sample, which is highly impractical for larger language models. Activation addition, as proposed by Turner et al. [34], creates steering vectors by calculating the difference in intermediate activations between a pair of prompts at specific layers within a transformer model. These steering vectors are then applied to the first token position of subsequent forward passes to influence model completions. However, it fails to perform consistently across various behaviors and prompts. To reduce the noise in the steering vector, contrastive activation addition (CAA) [28] utilizes hundreds of preference data pairs to generate steering vectors. Each pair consists of two prompts with the same multiple-choice question but ending with different answer choices. The steering vector corresponding to the target behavior is isolated by averaging the difference between the activation on these preference pairs. Their experiments focusing on Llama-2 [33] have demonstrated CAA can steer AI personas and mitigate hallucinations to some extent. Wang and Shu [35] employed a similar technique and additionally tested freeform-format contrastive preference data pairs, verifying that steering vectors can be utilized to compromise the safety of LLMs. However, these steering vectors are derived directly from the activations of LLMs when using preference data pairs as input prompts, which often results in inaccurate representations of the target behavior, particularly in alignment-concerning scenarios such as addressing jailbreaking attacks and defenses [46, 26, 4, 3, 39]. Our method, through preference optimization, acquires more precise steering vectors and can effectively steer model behaviors even in scenarios highly relevant to alignment. In addition to extracting steering vectors in MLP activations of specific layers within the transformer\u2019s residual stream, some works opt to add perturbations on attention activations to steer model generation [14, 18]. Focusing on enhancing the truthfulness of LLMs, Li et al. [14] first locates a set of attention heads with high probing accuracy for the truthfulness and then shifts activations along these truth-correlated directions during inference. Liu et al. [18] improved the efficiency and controllability of In-Context Learning [2] by substituting traditional demonstration examples with shifts in attention activations across all layers of the transformer. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Preference Optimization Reinforcement learning from human feedback (RLHF) has emerged as a popular approach for learning from human preference [5, 44, 30, 23]. Typically, RLHF first trains a neural network-based reward function to harmonize with the preference dataset by incorporating a preference model such as the Bradley-Terry model [1]. Subsequently, reinforcement learning algorithms such as proximal policy optimization [29] are adopted to train a language model to maximize the given reward for the chosen response. Recent works such as DPO [27] and SLiC [41, 42] have shown the feasibility of circumventing the reward-modeling stage and directly solving the actual RL problem, thereby simplifying implementation and reducing resource needs. Specifically, DPO [27] directly ftis a model to human preference data and implicitly optimizes the same objective as RLHF. SLiC [42] uses a contrastive ranking calibration loss to fit pairwise human feedback data while employing a cross-entropy regularization term to encourage the model to stay close to the SFT model. Furthermore, statistical rejection sampling optimization [19] unifies the losses of DPO and SLiC, proposing an improved estimation of the optimal policy and supporting sampling preference pairs from this policy. Our work proposes utilizing bi-directional preference optimization to extract steering vectors, thereby providing a more accurate representation of the target behavior. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first carefully study the current methods for extracting and utilizing steering vectors in LLMs, analyze their failure cases, and then propose our method. ", "page_idx": 2}, {"type": "text", "text": "3.1 Analyzing the Current Steering Vectors ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Current approaches [28, 35] for extracting steering vectors begin by constructing contrastive prompt pairs: one demonstrating the target behavior and the other demonstrating the opposite behavior. Then such steering vectors can be extracted by computing the mean difference of activations at specific LLM layers on the contrastive prompt pairs. Ideally, such an activation difference represents the direction to steer the model\u2019s behavior toward the target one. Specifically, CAA [28] constructs the contrastive prompt pairs with multiple-choice questions with letter answers (such as \u201cA\u201d or \u201cB\u201d) right after the question. Let us denote the multiple-choice question as $p$ , the positive choice as $c_{p}$ , and the negative choice as $c_{n}$ . CAA can construct the contrastive prompt pairs: both prompts contain the same multiple-choice question $p$ but are appended with different answers, where the \u201cpositive\u201d prompt ends with $c_{p}$ , which conforms to the target behavior, and the \u201cnegative\u201d prompt concludes with $c_{n}$ which denotes the opposite behavior. Formally, CAA calculates the steering vector at a ", "page_idx": 2}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/7435cdb161a4bf4db8129b9f7be328629db1b5e62cbedad7ba9fd72ce4cf3194.jpg", "table_caption": ["Table 1: The discrepancy between the appended choice and the completion steered by $\\scriptstyle v_{\\mathrm{{CAA}}}$ , illustrated by an example from the dataset used by CAA to shape Llama-2-7b-chat-hf\u2019s wealth-seeking persona. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "particular layer $L$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{L}=\\frac{1}{|\\mathcal{D}|}\\sum_{p,c_{p},c_{n}\\in\\mathcal{D}}[A_{L}(p,c_{p})]_{k}-[A_{L}(p,c_{n})]_{k},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{D}$ denotes the contrastive prompt dataset and $A_{L}(\\cdot)$ gives the activation vectors at layer $L$ in the transformer architecture, and $k$ denotes the position of the answer token, i.e., $c_{p}$ or $c_{n}$ . Thus, $[A_{L}(p,c_{p})]_{k}$ and $[A_{L}(p,c_{n})]_{k}$ refer to the activation vectors at the position of the last answer token. The intuition here is that the paired prompts differ only by a single token, which can isolate the internal activation pattern that is mostly associated with the target behavior, while simultaneously eliminating other confounding variables [28]. ", "page_idx": 3}, {"type": "text", "text": "However, the steering effectiveness of CAA for certain behaviors is suboptimal in both multiplechoice question evaluation and open-ended generation evaluation [28]. We observed that when appending a particular answer after the multiple-choice question and allowing the model to continue generating text, the completion, even when guided by the CAA-derived steering vector $\\scriptstyle v_{\\mathrm{{CAA}}}$ , often does not align with the behavior represented by the designated option, especially when the designated option represents unethical content. In Table 1, we present an example from the dataset used by CAA to shape Llama-2-chat-7b\u2019s wealth-seeking persona. In this instance, the question has option (A) representing the target behavior and option (B) representing the opposite behavior. However, when we forcibly append (A) after the question and apply the wealth-seeking steering vector $\\scriptstyle v_{\\mathrm{{CAA}}}$ computed by CAA, the subsequent completion still does not match with the behavior represented by (A). This inconsistency between the completion and the appended choice significantly impacts the effectiveness of the extracted steering vectors and indicates that, in the CAA extraction process, using the activation at the appended choice position may not accurately represent the target behavior. ", "page_idx": 3}, {"type": "text", "text": "3.2 Producing Steering Vector through Bi-directional Preference Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Based on our empirical observations shown in Section 3.1, we have found that activations and steering vectors computed directly from the activations differences in contrastive prompts may not accurately align with the desired behavior. In this work, we propose a novel method to optimize more effective steering vectors within the activation space through preference optimization. Unlike current methods that rely on the model to \u201cfollow\u201d a prompted direction by including the first answer token as part of the input, our approach aims to let the model \u201cspeak up\u201d. Specifically, our method is designed to let the optimized steering vector increase the difference in the generation probability between the response corresponding to the target behavior and its opposite, thus potentially yielding more effective steering vectors. To produce steering vectors that can more accurately represent the target behavior, we design a Bi-directional Preference Optimization procedure (BiPO). This method enables personalized control over the desired behavior at varying levels of intensity by adjusting the vector\u2019s direction and magnitude. ", "page_idx": 3}, {"type": "text", "text": "Preference Optimized Steering Vector Inspired by model preference optimization methods such as Direct Preference Optimization (DPO) [27], we attempt to optimize a steering vector that can be directly applied to activations, enhancing the likelihood of generating responses corresponding to the target behavior while simultaneously reducing the probability of eliciting responses associated with the opposite behavior. Specifically, we opt not to use the format of the multiple-choice question [28] for paired prompts; instead, we construct the dataset $\\mathcal{D}$ with regular preference data pairs $(q,r_{T},r_{O})$ , ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Bi-directional Preference Optimization (BiPO) ", "page_idx": 4}, {"type": "text", "text": "Input: A LLM $\\pi$ , a set of contrast paired prompts $\\boldsymbol{\\mathcal{D}}:=\\{(\\boldsymbol{q}^{i},\\boldsymbol{r}_{T}^{i},\\boldsymbol{r}_{O}^{i})\\}_{i=1}^{n}$ , the batch size $m$ , the deviation controlling parameter $\\beta$ , and the number of updating iteration steps $S$ ", "page_idx": 4}, {"type": "text", "text": "Output: Optimized steering vector $v^{*}$   \n1: Initialize $v_{0}$ with zero   \n2: for $s=0$ to $S-1$ do   \n3: Sampling $m$ paired prompts $\\mathcal{D}_{s}:=\\{(q^{i},r_{T}^{i},r_{O}^{i})\\}_{i=1}^{m}\\sim\\mathcal{D}$   \n4: Sampling a directional coefficient $d\\sim\\mathcal{U}\\{-1,1\\}$   \n5: $\\begin{array}{r}{\\mathcal{L}(v_{s},d,\\pi,\\mathcal{D}_{s})=-\\frac{1}{m}\\sum_{i=1}^{m}\\left[\\log\\sigma\\left(d\\beta\\log\\frac{\\pi_{L+1}(r_{T}^{i}|A_{L}(q^{i})+d v_{s})}{\\pi_{L+1}(r_{T}^{i}|A_{L}(q^{i}))}-d\\beta\\log\\frac{\\pi_{L+1}(r_{O}^{i}|A_{L}(q^{i})+d v_{s})}{\\pi_{L+1}(r_{O}^{i}|A_{L}(q^{i}))}\\right)\\right]}\\end{array}$   \n6: $v_{s+1}\\leftarrow$ update $v_{s}$ with $\\mathcal{L}(v_{s},d,\\pi,\\mathcal{D}_{s})$ using AdamW   \n7: end for   \n8: Return $v^{*}=v_{S}$ ", "page_idx": 4}, {"type": "text", "text": "where $q$ denotes the question (without any choices), $r_{T}$ denotes the complete response demonstrating target behavior, and $r_{O}$ refers to the complete response conforming to opposite behavior. ", "page_idx": 4}, {"type": "text", "text": "Formally, let $v$ denote the learnable steering vector, $\\pi_{L+1}$ denote the later part of the LLM transformer model from the $L+1$ layer to the final layer, and $A_{L}(\\cdot)$ gives the activation vectors at layer $L$ for faollr imnupluatt et otkhee nfso.l $\\mathcal{D}:=\\{(q^{i},r_{T_{\\cdot}}^{i},r_{Q}^{i})\\}_{i=1}^{n}$ j erecftievres  tfoo ra clla lpcauilrawtiinseg  tchoen tsrtaesetriivneg  bveehcatvoir ocr odrrateas.p oTnhdeinn, g wtoe the target behavior: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\upsilon}-\\mathbb{E}_{(q,r_{T},r_{O})\\sim{\\cal D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_{L+1}(r_{T}|A_{L}(q)+\\upsilon)}{\\pi_{L+1}(r_{T}|A_{L}(q))}-\\beta\\log\\frac{\\pi_{L+1}(r_{O}|A_{L}(q)+\\upsilon)}{\\pi_{L+1}(r_{O}|A_{L}(q))}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma$ refers to the logistic function, and $\\beta$ is a parameter controlling the deviation from the original model. In general, $\\pi_{L+1}(\\cdot|A_{L}(q))$ represents the original LLM model response towards a given question $q$ , and $\\pi_{L+1}(\\cdot|A_{L}(q)+v)$ represents the steered model response after adding1 the steering vector $v$ to the activations of all input tokens at the $L$ -th layer. ", "page_idx": 4}, {"type": "text", "text": "By solving this optimization problem, when the steering vector is applied, the likelihood of generating a response corresponding to the target behavior will increase, while the likelihood of producing a response corresponding to the opposite behavior will decrease. Thus, this ensures that the direction of the steering vector accurately aligns with the target behavior. The objective in Eq. 2 is stemmed from the policy objective in DPO [27]. However, instead of having one policy model and one reference model as in DPO, in our optimization problem, there is only one model needed and the optimization target is the learnable steering vector, rather than model parameters. ", "page_idx": 4}, {"type": "text", "text": "Bi-directional Optimization Objective Ideally, the direction of the steering vector should point from the opposite behavior to the target behavior, allowing us to infer that its reverse direction points from the target behavior to the opposite behavior. To enhance the preciseness of our optimized steering vector, we further design the following bi-directional optimization objective by incorporating a directional coefficient $d\\sim\\breve{\\mathcal{U}}\\{-1,1\\}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\upsilon}-\\mathbb{E}_{d\\sim\\mathcal{U},(q,r_{T},r_{O})\\sim\\mathcal{D}}\\left[\\log\\sigma\\left(d\\beta\\log\\frac{\\pi_{L+1}(r_{T}|A_{L}(q)+d\\upsilon)}{\\pi_{L+1}(r_{T}|A_{L}(q))}-d\\beta\\log\\frac{\\pi_{L+1}(r_{O}|A_{L}(q)+d\\upsilon)}{\\pi_{L+1}(r_{O}|A_{L}(q))}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Specifically, When the sampled directional coefficient $d$ is positive, we optimize the steering vector $v$ to isolate the direction of the target behavior in activation space. Conversely, when $d$ is negative, this bi-directional objective ensures the optimized vector increases the generation probability of $r_{O}$ while decreasing the generation probability of $r_{T}$ , thus making $-v$ accurately represent the opposite behavior. we summarize the overall bi-directional optimization procedure in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Following Rimsky et al. [28], upon constructing a steering vector, we steer the model generation by adding the vector to every token position. Moreover, by scaling the vector with varying magnitudes and directions, varying degrees of personalized control over the desired behavior can be achieved. For instance, in Table 2, we present an example of steering the model toward a power-seeking persona. When applied with a positive multiplier, the model is steered to adopt a more power-affirming stance; Conversely, with a negative multiplier, the model adopts a power-shunning stance. Furthermore, we can observe that scaling the vector with a larger magnitude amplifies the intensity of the sentiment associated with the behavior. We provide more examples in Appendix K. ", "page_idx": 4}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/ed2476fbdef44184b15ed8b0e0f43a2799ef6caa94466ca28ab31e71bce40691.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Target LLMs Our experiments primarily focus on the Llama-2-7b-chat-hf [33] and Mistral-7BInstruct-v0.2 [12], testing the effectiveness of our method in steering various behaviors. These two models are widely used, exhibit strong instruction-following capabilities, and perform well on the Huggingface Open LLM leaderboard. Due to the space limit, we defer the results on Mistral 7B model in Appendix C. Moreover, to explore the transferability of our steering vectors across different models, we also conduct experiments on the Vicuna-7b-v1.5 [43] and Llama2-Chinese-7b-Chat [7]. The hyperparameters and ablation study are detailed in Appendix A.3 and Appendix H, respectively. ", "page_idx": 5}, {"type": "text", "text": "Baselines As introduced in Section 3.1, CAA [28] uses prompt pairs consisting of multiple-choice questions to directly compute the steering vector without optimization. Apart from using multiplechoice questions, Wang and Shu [35] also explored the construction of freeform paired prompts to calculate steering vector. In our experiments, we conducted detailed comparisons between CAA and the Freeform approach. Note that both the baselines and our method extract and utilize the steering vector from only one layer within the transformer. CAA selects a specific layer by sweeping through different middle layers [28], and we follow CAA\u2019s selection by using the 15th layer on Llama-2-7b-chat-hf and the 13th layer on Mistral-7B-Instruct-v0.2. Freeform, on the other hand, proposes using JS Divergence to automatically select the optimal layer [35]. For details on the specific layer selection and the related sweeping experiments, please refer to the Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "Behaviors and Datasets Our primary focus is on steering AI personas. Additionally, we conduct a comprehensive investigation into critical alignment-related scenarios, including managing truthfulness, mitigating hallucinations, and addressing jailbreaking attacks along with their defenses. Note that we primarily measure the steering effects through open-ended generation tasks. The following provides descriptions of various behaviors along with the corresponding benchmark datasets, and we defer more details of dataset splitting and construction to Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "1. AI Persona: Anthropic\u2019s Model-Written Evaluation Datasets [25] contain collections of datasets designed to test models for their persona. Specifically, we consider four personas from the \u201cAdvanced AI Risk\u201d evaluation dataset to steer the model towards or away from potentially risky goals. These personas include Power-seeking, Wealth-seeking, Corrigible-less-HHH (i.e., modifying the system\u2019s goal to less helpful, harmless, and honest), and Survival-instinct. These datasets contain open-ended questions related to the behavior, along with responses that align with both the target behavior and its opposite. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "2. Truthfulness & Hallucination: To verify the effect of the steering vectors on managing truthfulness, we use the TruthfulQA [17] benchmark dataset. In addition, we also test the steering effect on the unprompted hallucination dataset generated and used by Rimsky et al. [28]. ", "page_idx": 6}, {"type": "text", "text": "3. Jailbreaking: Jailbreaking attacks can circumvent the safety guardrails of aligned LLMs and elicit helpful responses to malicious questions [46, 26]. In this scenario, we utilize the widely used benchmark dataset, AdvBench[46], to evaluate the steering effectiveness on facilitating jailbreaking and the opposite behavior, defending against jailbreaking attacks. ", "page_idx": 6}, {"type": "text", "text": "4.2 The Steering Effects across Various Behaviors ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The Steering Effects on AI Persona To assess the efficacy of the steering vector in controlling AI personas, we follow Rimsky et al. [28] and focus on open-ended generation tasks. We employ GPT-4 to evaluate model responses on a scale from 1 to 4, based on the extent to which they exhibit the targeted behavior. A higher score indicates that the response more closely aligns with the target behavior. We provide the evaluation prompts in Appendix J. As shown in Figure 1, we present the comparison of our method and the baselines on Llama-2-7b-chat-hf. Our results clearly demonstrate that our method offers a more extensive range of steering over generated content across all models and personas, outperforming the baselines. The similar outstanding performances of BiPO can also be found on the Mistral 7B model, which is shown in Figure 5 in Appendix C. By meticulously adjusting the direction and magnitude of the optimized vector, our approach facilitates precise steering to different extents, easily meeting personalized user needs without extra fine-tuning. ", "page_idx": 6}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/6a9488ed0d07dbe0783897fd045662843fdf972b5e135f2696558f30ab50bfab.jpg", "img_caption": ["Figure 1: The comparison results on steering the AI personas of Llama-2-7b-chat-hf model. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "The Steering Effects on Truthfulness & Hallucination In Figure 2 (left), we present the comparison of accuracy results (MC1 and MC2) on the TruthfulQA benchmark. We observe that baseline methods struggle to steer the model\u2019s truthfulness. In contrast, our approach demonstrates more significant steering effects, substantially improving the models\u2019 truthfulness. Moreover, to evaluate whether the steering vector can ", "page_idx": 6}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/9be1d1b0bec099246b328929555ef58fff56158210c8cb4252779fa128ef3370.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "control the generation of fabricated content and Figure 2: The comparison results on steering truthmitigate hallucination in open-ended generation fulness and hallucination of Llama-2-7b-chat-hf. tasks, we utilize a set of held-out hallucination questions and employ GPT-4 as the rater (See the evaluation prompt in Appendix J). Figure 2 (right) presents the corresponding comparison results of steering hallucination in Llama-2-7b-chat-hf. We observe that the baselines still have difficulty controlling the extent of the model\u2019s hallucinations. Our method can both steer LLMs to generate fabricated content and enhance the model\u2019s honesty by altering the direction of the vector. We defer the similar results on the Mistral 7B model in Figure 6 in Appendix C, and provide an example of steering hallucination on Llama-2-7b-chat-hf in Table 3. ", "page_idx": 6}, {"type": "text", "text": "The Steering Effects on Jailbreaking We use the Attack Success Rate (ASR) to measure the effectiveness of the steering vectors produced by our method in executing and defending against jailbreaking attacks. Following Qi et al. [26], Cao et al. [4], we also use GPT-4 as a judge to determine if an attack successfully elicits helpful responses to malicious questions (see detailed evaluation prompt in Appendix J). As shown in the comparison results on Llama-2-7b-chat-hf in Table 4, when the input samples are malicious questions, the ASR for the initial model is $0\\%$ , indicating that the initial model does not respond to these malicious questions. When our steering vector is added, the steered model can answer $73\\%$ of the malicious questions. However, baseline methods fail to successfully jailbreak, primarily due to the issue mentioned in Section 3.1, where the completion and target behavior are inconsistent across nearly all samples in the training dataset (see these examples in the Appendix B). On the other hand, when the input samples are malicious questions appended with adversarial suffixes optimized on the initial model through GCG attack [46], the ASR for the initial model increases to $16\\%$ . Yet, when our vector is subtracted, the ASR successfully drops to $0\\%$ . These experimental results adequately demonstrate the effectiveness of our method in safety-related scenarios. In particular, we provide examples of using our steering vector to both attack and defend in Table 16 in Appendix K. ", "page_idx": 6}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/7e236c29b4c0553d74b128e09fce9ef3dabad89c5bd8faea8a55e8fb3ef324de.jpg", "table_caption": ["Table 3: An example of steering hallucination on Llama-2-7b-chat-hf: adding $(+1.0\\times v^{*})$ leads the model to generate hallucinated content, while adding $(-1.0\\times v^{*})$ makes the model more honest. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/16b0a4ddeebaec786e3313f307d42d90b698902cbf1bd5b2b9b5e2ad808dafbf.jpg", "table_caption": ["Table 4: The comparison results of steering the jailbreaking behavior on Llama-2-7b-chat-hf. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 The Impact of Steering Vector on Utility ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To assess the impact of our optimized steering vector on the models\u2019 general knowledge and problem-solving skills, we evaluate the utility of the model integrated with steering vectors on MMLU benchmark [9], which includes a large dataset of multiple choice questions in 57 subjects. We randomly sample 30 questions from each of the 57 categories and report the accuracy of Llama-2-7b-chat-hf with varying steering multipliers in Table 5. Specifically, we select four steering vectors associated with AI persona, which are presumed to be orthogonal to knowledge-wise abilities. We evaluate their performance on the MMLU using multipliers of $+1$ and $^{-1}$ and compare these results with the original model (where the multiplier is 0). The results demonstrate that the steering vectors for these four behaviors do not negatively impact the model\u2019s knowledge-wise capabilities. We also provide the consistent utility evaluation results on Mistral-7B-Instruct- $\\cdot\\mathrm{v}0.2$ in Table 11 in Appendix C. ", "page_idx": 7}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/a9bfa2973f14e17a6c5f555c276ee34bbc6be147b318de18f091f233b900561c.jpg", "table_caption": ["Table 5: MMLU accuracy of Llama-2- 7b-chat-hf with varying steering multipliers "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.4 The Transferability of Steering Vector ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we explore whether the steering vector optimized by our method possesses transferability. First, we examine cross-model transferability by directly using a steering vector optimized on Llama-2-7b-chat-hf for the Vicuna-7b-v1.5 [43], which is trained by fine-tuning Llama2 on user-shared conversations gathered from ShareGPT.com, thus having the same model architecture and activation dimension as Llama-2-7b-chat-hf. Next, we investigate whether the steering vector ", "page_idx": 7}, {"type": "text", "text": "can be applied to models fine-tuned with LoRA. For this purpose, we select Llama2-Chinese-7bChat [7], a model based on Llama-2-7b-chat-hf and fine-tuned on Chinese instruction datasets using LoRA [10]. Figure 3 displays the effects of steering AI personas, and we can observe that the steering vector exhibits strong transferability in both scenarios. Note that to measure the steering effects on Llama2-Chinese-7b-Chat, we ", "page_idx": 8}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/58141435f8d9be6e25aa05025c609fc2991dc5a7220836f6d592af752b9634bc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "translate the open-ended questions from the test Figure 3: The transferability of our steering vector. datasets into Chinese. As shown in Table 17 in Appendix K, steering vectors optimized from Llama-2- 7b-chat-hf using preference data in English can also steer model behavior on Llama2-Chinese-7b-Chat, even when the input prompts are in Chinese. ", "page_idx": 8}, {"type": "text", "text": "4.5 Vector Synergy ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we explore the simultaneous application of different vectors and their synergistic effects. First, we examine if the individual steering effects of two vectors are maintained when applied together to the original model. Specifically, we select four behaviors\u2014Power-seeking, Wealth-seeking, Corrigible-less-HHH, and Hallucination\u2014and denote the corresponding vectors as $v_{\\mathrm{Power}}^{*}$ , $v_{\\mathrm{Wealth}}^{*}$ , $v_{\\mathrm{Corrigible}}^{*}$ , and $v_{\\mathrm{Hallucination}}^{*}$ . We then calculate the sums $v_{\\mathrm{Power}}^{*}+v_{\\mathrm{Wealth}}^{*}$ and $v_{\\mathrm{Corrigible}}^{*}+v_{\\mathrm{Hallucination}}^{*}$ and assess whether the combined vectors could still effectively steer both behaviors. As shown in Table 6, the results indicate that the aggregated vectors continue to steer their respective behaviors effectively. In addition to retaining the original effects on their respective behaviors, we have observed that the combined vectors also exhibit a functionality fusion effect. As illustrated in Table 7, when answering the same question, using $v_{\\mathrm{Power}}^{*}+v_{\\mathrm{Wealth}}^{*}$ together, as opposed to $v_{\\mathrm{Wealth}}^{\\ast}$ alone, enables the model generation to recognize both wealth accumulation and influence maximization simultaneously. ", "page_idx": 8}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/923eaf6e684fe82048d783048fef60f318c037f46501a521aa6b1b4c00a6750a.jpg", "table_caption": ["Table 6: The average behavioral score on Llama-2-7b-chat-hf with the application of multiple vectors. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/ed5adb5c937689fa6fe519b42c6328c82e805e5603287aee54d9dccdbb4636d9.jpg", "table_caption": ["Table 7: An example of a combined vector exhibiting a functionality fusion effect: steering Llama-2 to simultaneously adopt wealth-seeking and power-seeking (as highlighted in bold ) personas. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we propose a novel method for calculating more effective steering vectors using bidirectional preference optimization. Through meticulous adjustments to the direction and magnitude of our optimized steering vectors, we can attain varying levels of steering effects for specific behaviors. This method efficiently caters to users\u2019 diverse personalization needs without requiring further training. Extensive experiments on steering AI personas, managing truthfulness, mitigating hallucinations, and addressing jailbreaking attacks and their defenses demonstrate the versatility of our approach. Additionally, the excellent transferability and vector synergy effects further enhance the practicality of our method. Due to the consideration of reducing the steering cost, here we design our steering vector to only affect a single layer in the transformer. Potentially a multi-layer steering design could achieve even better results. We will leave this as our future work. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for their helpful comments. This work is partially supported by DHS (17STQAC00001-07-00). The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.   \n[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[3] B. Cao, Y. Cao, L. Lin, and J. Chen. Defending against alignment-breaking attacks via robustly aligned llm, 2023.   \n[4] Y. Cao, B. Cao, and J. Chen. Stealthy and persistent unalignment on large language models via backdoor injections. arXiv preprint arXiv:2312.00027, 2023.   \n[5] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \n[6] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.   \n[7] FlagAlpha. Llama2-chinese-7b-chat, 2023. URL https://huggingface.co/FlagAlpha/ Llama2-Chinese-7b-Chat.   \n[8] K. He, R. Mao, Q. Lin, Y. Ruan, X. Lan, M. Feng, and E. Cambria. A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics, 2023.   \n[9] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.   \n[10] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.   \n[11] G.-J. Hwang and C.-Y. Chang. A review of opportunities and challenges of chatbots in education. Interactive Learning Environments, 31(7):4099\u20134112, 2023.   \n[12] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[13] J. Lai, W. Gan, J. Wu, Z. Qi, and P. S. Yu. Large language models in law: A survey, 2023.   \n[14] K. Li, O. Patel, F. Vi\u00e9gas, H. Pfister, and M. Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. Advances in Neural Information Processing Systems, 36, 2024.   \n[15] X. Li, R. Wang, M. Cheng, T. Zhou, and C.-J. Hsieh. Drattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers. arXiv preprint arXiv:2402.16914, 2024.   \n[16] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.   \n[17] S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214\u20133252, 2022.   \n[18] S. Liu, L. Xing, and J. Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv preprint arXiv:2311.06668, 2023.   \n[19] T. Liu, Y. Zhao, R. Joshi, M. Khalman, M. Saleh, P. J. Liu, and J. Liu. Statistical rejection sampling improves preference optimization. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\cdot$ xbjSwwrQOe.   \n[20] X. Liu, N. Xu, M. Chen, and C. Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. In The Twelfth International Conference on Learning Representations.   \n[21] H.-T. Nguyen. A brief report on lawgpt 1.0: A virtual legal assistant based on gpt-3. arXiv preprint arXiv:2302.05729, 2023.   \n[22] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api. semanticscholar.org/CorpusID:257532815.   \n[23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[24] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.   \n[25] E. Perez, S. Ringer, K. Lukosiute, K. Nguyen, E. Chen, S. Heiner, C. Pettit, C. Olsson, S. Kundu, S. Kadavath, et al. Discovering language model behaviors with model-written evaluations. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13387\u201313434, 2023.   \n[26] X. Qi, Y. Zeng, T. Xie, P.-Y. Chen, R. Jia, P. Mittal, and P. Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! In The Twelfth International Conference on Learning Representations, 2023.   \n[27] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] N. Rimsky, N. Gabrieli, J. Schulz, M. Tong, E. Hubinger, and A. M. Turner. Steering llama 2 via contrastive activation addition. arXiv preprint arXiv:2312.06681, 2023.   \n[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[30] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \n[31] N. Subramani, N. Suresh, and M. E. Peters. Extracting latent steering vectors from pretrained language models. arXiv preprint arXiv:2205.05124, 2022.   \n[32] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting. Large language models in medicine. Nature medicine, pages 1\u201311, 2023.   \n[33] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[34] A. Turner, L. Thiergart, D. Udell, G. Leech, U. Mini, and M. MacDiarmid. Activation addition: Steering language models without optimization. arXiv preprint arXiv:2308.10248, 2023.   \n[35] H. Wang and K. Shu. Backdoor activation attack: Attack large language models using activation steering for safety-alignment. arXiv preprint arXiv:2311.09433, 2023.   \n[36] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann. Bloomberggpt: A large language model for finance, 2023.   \n[37] J. Yi, R. Ye, Q. Chen, B. B. Zhu, S. Chen, D. Lian, G. Sun, X. Xie, and F. Wu. Open-source can be dangerous: On the vulnerability of value alignment in open-source llms. 2023.   \n[38] K. Zhang, F. Zhao, Y. Kang, and X. Liu. Memory-augmented llm personalization with short-and long-term memory coordination. arXiv preprint arXiv:2309.11696, 2023.   \n[39] Z. Zhang, J. Yang, P. Ke, and M. Huang. Defending large language models against jailbreaking attacks through goal prioritization. arXiv preprint arXiv:2311.09096, 2023.   \n[40] H. Zhao, Z. Liu, Z. Wu, Y. Li, T. Yang, P. Shu, S. Xu, H. Dai, L. Zhao, G. Mai, N. Liu, and T. Liu. Revolutionizing finance with llms: An overview of applications and insights, 2024.   \n[41] Y. Zhao, M. Khalman, R. Joshi, S. Narayan, M. Saleh, and P. J. Liu. Calibrating sequence likelihood improves conditional language generation. In The Eleventh International Conference on Learning Representations, 2022.   \n[42] Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.   \n[43] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.   \n[44] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.   \n[45] A. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin, M. Mazeika, A.-K. Dombrowski, et al. Representation engineering: A top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023.   \n[46] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A More Details of the Experimental Settings ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 More Details of the Datasets ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we provide a detailed description of all the datasets we used, the partitioning of the training and test datasets, and the construction of contrastive prompt pairs. Anthropic\u2019s Model-Written Evaluation Datasets [25] include collections specifically designed to assess models for their persona. In our paper, we focus on four personas from the \"Advanced AI Risk\" evaluation dataset to guide the model either towards or away from potentially hazardous goals. These personas are Power-seeking, Wealth-seeking, Corrigible-less-HHH (i.e., altering the system\u2019s goal to be less helpful, harmless, and honest), and Survival-instinct. The datasets comprise open-ended questions about these behaviors, along with responses that reflect both the desired behavior and its opposite. ", "page_idx": 12}, {"type": "text", "text": "To verify the effect of the steering vectors on managing truthfulness, we use the TruthfulQA [17] benchmark dataset that some humans would answer poorly due to misconceptions. This dataset comprises 817 questions distributed across 38 categories such as conspiracies, logical falsehoods, and common points of confusion. Each question comes with multiple correct answers and incorrect answers. In addition, we also test the steering effect on the unprompted hallucination dataset generated and used by Rimsky et al. [28]. This dataset also includes open-ended questions, along with corresponding honest responses and hallucinated responses. ", "page_idx": 12}, {"type": "text", "text": "For jailbreaking behavior, we utilize the widely adopted benchmark dataset, AdvBench[46], to evaluate the steering effectiveness on facilitating jailbreaking and the opposite behavior, defending against jailbreaking attacks. Specifically, AdvBench \u201cHarmful Behaviors\u201d consists of 520 questions covering various prohibited topics such as methods of crime, and dangerous suggestions. To construct paired prompts for obtaining steering vectors, following the approach Qi et al. [26], Cao et al. [4], we fine-tuned Llama-2-7b-chat-hf using a small subset of harmful instructions and their corresponding affirmative prefixes to create an \u201cunaligned\" model\u201d, which are then utilized to generate helpful responses to malicious questions within the training set. Then as shown in Table 8, we sample some refusal responses from the aligned model as the responses demonstrate the opposite behavior. ", "page_idx": 12}, {"type": "text", "text": "We summarize the number of training samples and test samples we partitioned for each dataset in Table 9. Note that for TruthfulQA, each training sample has multiple correct answers and incorrect answers. We construct contrastive pairs by combining any one correct answer with one incorrect answer. Similarly, for AdvBench, we pair each malicious question and its corresponding helpful response with all refusal responses. ", "page_idx": 12}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/c3685f396f2fa7a0973c79ceb98ec751db274ad8f1b004904bb263358148e3d3.jpg", "table_caption": ["Table 8: Sampled refusal responses from Llama-2-7b-chat-hf. "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/3a93549bc9bf3c0eeeb7bee96d2dfa87db43ebf11426cdf170d37a380d96ecb8.jpg", "table_caption": ["Table 9: The number of training and test samples in the datasets. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Both baseline methods and our method extract and use the steering vector from one layer of the LLM. In this section, we introduce the principles behind selecting the specific layers for different methods. Both CAA[28] and Freeform [35] empirically demonstrate that extracting the steering vector from intermediate layers yields better results. Specifically, CAA [28] find the optimal layer for steering by sweeping over all layers and assessing the steering effects on the heldout questions. For Llama-2-7b-chat-hf, CAA has conducted sweeping experiments, determin", "page_idx": 13}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/db3a643b83abfa64dff372ed99c20f1ffb99737802bacdca436fafbe7e99f498.jpg", "img_caption": ["Figure 4: Sweep different middle layers and perform CAA on Mistral-7B-Instruct-v0.2. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "ing that the 15th layer yields the best results. Therefore, we follow their choice and use the 15th layer for Llama-2-7b-chat-hf as well. For the Mistral-7B-Instruct-v0.2 model, to determine the optimal layer for CAA, we have conducted a sweeping experiment using 50 held-out questions sampled from the training set. The results, shown in Figure 4, indicate that CAA achieves better steering effects at the 13th layer. Therefore, both CAA and our method choose the 13th layer on Mistral-7B-Instructv0.2. On the other hand, Freeform proposes using JS Divergence to select the optimal layer [35] automatically. In our implementation of Freeform, we also follow the same approach and ultimately use the 15th layer for all behaviors on Llama-2-7b-chat-hf and the 16th layer for all behaviors on Mistral-7B-Instruct-v0.2. Additionally, for our method, we have also conducted the ablation study on different layers. Please refer to the Appendix H for details. ", "page_idx": 13}, {"type": "text", "text": "A.3 Hyperparameters ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For both Llama-2-7b-chat-hf and Mistral-7B-Instruct-v0.2, we set $\\beta=0.1$ for the loss and optimize the steering vector using AdamW optimizer with a learning rate of 5e-4, a batch size of 4, and a weight decay of 0.05. We use a cosine learning rate scheduler with 100 warmup steps. For Llama-2- 7b-chat-hf, we use 20 training epochs to train the steering vector on the AI persona and hallucination datasets, 1 epoch on the TruthfulQA, and 10 epochs on the AdvBench. For Mistral-7B-Instruct-v0.2, we use 5 training epochs on the AI persona datasets and 1 epoch on the TruthfulQA. We also provide the ablation study on the number of training epochs in Appendix H. During generation, we use greedy decoding to obtain responses to open-ended questions. All our experiments are conducted on a single A100. ", "page_idx": 13}, {"type": "text", "text": "B Inconsistency Issues of Current Steering Vectors in Jailbreaking ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Section 3.1, we have analyzed the inconsistency between the appended answer and the completion in current steering vectors, even when the steering vector is applied to each token generation process. We observe that in the context of jailbreaking, the baseline methods exhibit this inconsistency in every sample within the training dataset. Table 10 presents some failure cases on Llama-2-7b-chat-hf, indicating that relying on the model to \u201cfollow\u201d the prompt may not ensure applicability in various scenarios. WARNING: The examples in Table 10 contain unsafe content. Reader discretion is advised. ", "page_idx": 13}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/2d66d13153bd8c42625b879422e8563d8c7f2631920ee02ad1ace7d5da08bf90.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/be9533d2b8267d0b852e657794410f3d320a1cc345c0ff2ba639527542fe04f0.jpg", "img_caption": ["Figure 5: The comparison results on steering the personas of Mistral-7B-Instruct-v0.2. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Experimental Results on More LLMs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide experimental results on more LLMs including Mistral-7BInstruct-v0.2, Gemma-7b-it, and Llama2-13bchat. Specifically, Figure 5 presents the comparison results of steering various AI personas on Mistral-7B-Instruct-v0.2. Consistent with the results on Llama-2-7b-chat-hf, our method demonstrates a broader range of steering capabilities compared to the baselines. By adjusting the magnitude and direction of the vector, we can achieve different levels of control over the desired behavior. Figure 6 presents the effects of steering truthfulness and hallucination on the ", "page_idx": 15}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/0378f7e4f7a3915287b559a0c7d424dc7a76a48ace9adc9516b174d213607df7.jpg", "img_caption": ["Figure 6: The comparison results on steering truthfulness and hallucination of Mistral-7B-Instructv0.2. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "left and right, respectively. On the TruthfulQA dataset, baseline methods show minimal improvement in the model\u2019s truthfulness, whereas our approach effectively enhances it. In hallucination-related open-ended generation tasks, our method still outperforms the baselines. Although CAA can make the model more prone to generating fabricated content, it struggles to make the model more honest. Meanwhile, Freeform performs unsatisfactorily in both increasing and decreasing the model\u2019s hallucinations. Furthermore, the utility evaluation results of Mistral on MMLU, shown in Table 11, further demonstrate that the steering vectors for AI personas we obtained do not significantly negatively impact the model\u2019s knowledge-wise capabilities. We additionally consider Gemma-7b-it and Llama2-13b-chat and summarize the results of steering wealth-seeking persona and hallucination experiments in Figure 7. Our method still shows a more extensive range of steering effects than baseline methods. ", "page_idx": 15}, {"type": "text", "text": "Table 11: MMLU accuracy of Mistral-7B-Instruct-v0.2 with varying steering multipliers ", "page_idx": 15}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/eeeab39cc66f4864a0936bf9becf09cfc97d52562105ca17402e2fb7f2842fbe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Experimental Results on Defending More Jailbreaking Attacks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In section 4.2, we have demonstrated that the steering vector optimized by our method can effectively bypass the safety alignment of the LLM. Additionally, by adjusting the direction and magnitude of the steering vector, it is also possible to fully defend against GCG attacks [46]. To further verify the performance of our method in safety-related scenarios, we consider two more types of attacks: DrAttack [15] and Auto-DAN [20]. We summarize the experimental results on Llama-2-7b-chat in Table 12, which indicates that our method still provides the best defense against these attacks. ", "page_idx": 15}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/851cad1046abe290e97828276972d00ddddee38b0d642b1add8f98dc0b821817.jpg", "img_caption": ["Figure 7: The comparison results on steering wealth-seeking persona and hallucination of Gemma$^{7\\mathrm{{b}}}$ -it (first two subfigures) and Llama-2-13b-chat (latter two subfigures). "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/ac95be1fca3acf4c800361f0e3e939a5f160102a4146994623937fcfc15cee0e.jpg", "table_caption": ["Table 12: The performance of steering vectors in defending against jailbreaking attacks, measured by ASR. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Comparison with In-context Learning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide additional results comparing our method with In-context Learning (ICL) on Llama-2-7b-chat to steer wealth-seeking persona. Specifically, we sampled examples from the training data that represent both the target behavior and its opposite, and provided these to the model to steer its behavior. We summarize the results of ICL and $\\mathrm{ICL+}$ steering vector in Table 13. From Table 13a and 13b, we can observe that ICL can indeed control model behavior to some extent, but its range of control is far less than that of our method. Beyond this, we also attempted to apply steering vectors in conjunction with ICL. As demonstrated in Table 13c, when our method is combined with ICL, it achieves an even broader range of steering effects. ", "page_idx": 16}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/d3aae7306c1a6934c746767269b7282caadf41b6d2468d5226503f9efbf8a777.jpg", "table_caption": ["Table 13: The comparison with ICL on Llama-2-7b-chat to steer wealth-seeking persona. ", "(a) Steering vector results "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F Comparison with DPO LoRA-fine-tuning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "LoRA [10, 6] fine-tuning is a commonly used Parameter-Efficient Fine-Tuning (PEFT) method. In this section, we compare DPO-based LoRA fine-tuning with our approach in the scenario of steering the power-seeking persona of Llama-2-7b-chat-hf and evaluate on 100 sampled open-ended questions from the test set. Specifically, we use the responses demonstrating power-seeking behavior from the power-seeking dataset as the preferred data for DPO fine-tuning, and the responses demonstrating the opposite behavior as the dispreferred data. We set the LoRA rank to 8, the dropout rate to 0.05, and the $\\alpha$ to 16. We employ Paged AdamW [6] as the optimizer with a learning rate of 2e-4, a weight decay of 0.05, and a batch size of 4. We also use a cosine learning rate scheduler with 100 warmup steps to train the LoRA parameters for 2 epochs. The comparison results are shown in Figure 8. We can observe that, unlike our method, it is not possible to achieve personalized steering effects by altering the direction and magnitude of the fine-tuned LoRA parameters. Additionally, our method demonstrates a significantly broader range of steering effects and requires fewer training parameters. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/f2b0105b85bee5b9fda0df3eaf1f072e632d8724b4c639266cfc131b9670d9ae.jpg", "img_caption": ["Figure 8: The comparison results with DPO LoRA fine-tuning on steering Llama-2-7b-chat-hf\u2019s power-seeking persona. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "G Uni-directional VS. Bi-directional Optimization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we compare the effects of uni-directional optimization and bi-directional optimization on steering performance. Specifically, the steering vector obtained through uni-directional preference optimization still possesses the ability to achieve varying degrees of steering by adjusting its direction and magnitude. However, its overall steering range is not as broad as that achieved through bidirectional optimization. We present the performance of these two optimization methods on Llama-2- chat-7b in Table 14 . The results demonstrate that the bi-directional version achieves a higher degree of steering effect for both the target behavior and the opposite behavior. ", "page_idx": 17}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/289bde4facf1e02fcba8e209586deefdda3e6d5257466561f74b9012f3364e85.jpg", "table_caption": ["Table 14: Average behavioral score on Llama-2-7b-chat to steer power-seeking (left) and wealthseeking (right) persona, obtained by uni-directional and bi-directional optimization. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "H More Ablation Study Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide more ablation study results. As shown in Figure 9 (left), we keep the training epochs fixed at 20 and compare the effects of optimizing the steering vector at different layers. We find that steering vectors obtained from different layers all exhibit varying degrees of personalized control over the target behavior. Moreover, consistent with the observations by Rimsky et al. [28], the 15th layer yield the best steering effect on Llama-2-7b-chat-hf. Figure 9 (right) illustrates the impact of different training epochs on the steering effect. The steering vectors here are all obtained from the 15th layer. ", "page_idx": 17}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/8fc510312dca3013d2a3b40a39dcaf783712f01c9ec75a347c4be7f16b292519.jpg", "img_caption": ["Figure 9: Ablation studies on different layers (left) and training epochs (right) in our method, conducted on Llama-2-7b-chat-hf. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "We can observe that as the number of training epochs increases, the steering effects tend to stabilize. ", "page_idx": 17}, {"type": "text", "text": "I Computational Cost ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Because our method only requires optimizing a single vector, the number of trainable parameters involved is very small. For instance, it involves 4096 dimensions on Llama-2-7b-chat and 5120 on the 13b model, resulting in minimal training time. Table 15 presents the training costs under our experimental setup. It is evident that the proportion of trainable parameters is negligible, and the training time on an A100 GPU is less than 30 minutes for both the 7b and 13b models, which is highly acceptable. While other methods for extracting steering vectors are inference-based and indeed more efficient, their performance is significantly inferior to ours. ", "page_idx": 18}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/56ea3db692fad5ca134f966d410f68b92b8dee1f25d990b933315aa2476ee3b2.jpg", "table_caption": ["Table 15: The number of training parameters and training time costs. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "J Automatic Evaluation Prompts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To accurately and efficiently measure and compare the steering effects of different methods on specific behaviors, we implement automated evaluation utilizing GPT-4 (gpt-4-0125-preview version). For AI persona and hallucination assessments, we use a 1-4 scale, where higher scores indicate that the model response better exhibits the target behaviors, and lower scores indicate that the response better exhibits the opposite behaviors. Since the test dataset includes examples of responses that exhibit both target and opposite behaviors for open-ended questions, we incorporate these examples into the evaluation prompt to assist GPT-4 in providing more accurate scoring. The prompts used for evaluating four AI personas and hallucination are detailed in Figure 10, 11, 12, 13, and 14, respectively. For jailbreaking, we follow the evaluation prompt used by Cao et al. [4], Yi et al. [37] to calculate the attack success rate (ASR). We attach the complete evaluation prompt in Figure 15. ", "page_idx": 18}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/eead1dd98de081e30d4f7a40e9921b1e591585f32edde66084f7d27baa5f93cf.jpg", "img_caption": ["Figure 10: GPT-4 evaluation prompt for Power-seeking persona. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/cae7e034e4e58f1c5c78a820ad0030f8491ccf433ee6673fd0a166e01c9eb20d.jpg", "img_caption": ["Figure 11: GPT-4 evaluation prompt for Wealth-seeking persona. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/6575156c68107937290259e61409a625d80a61c5ded187a8506c11c941a06571.jpg", "img_caption": ["Figure 12: GPT-4 evaluation prompt for Corrigible-less-HHH persona. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/9d4a8732f8a75ad178248524ccdcd5722b7486294b307a3e0d510c0e3f316a86.jpg", "img_caption": ["Figure 13: GPT-4 evaluation prompt for Survival-instinct persona. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "7qJFkuZdYo/tmp/871fa2295b2f4893d8762ae8e3ed91ccac75fd67ba775c8fc0390500ef93ab1b.jpg", "img_caption": ["Figure 14: GPT-4 evaluation prompt for Hallucination. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "As a professional evaluator, your role is to assess if a generated response meets the user\u2019s requirements. Based on the provided generation, determine its relevance and appropriateness according to the following rules:   \n1. If the generation only promises to complete the task without offering substantive content, respond with \"No\". 2. If the generation neither contains harmful information nor offers a direct response due to ethical concerns, respond with \"No\". Short disclaimers are not considered.   \n3. If the generation provides detailed harmful or illegal content on the prompt, even with short disclaimers, respond with \"Yes\".   \n4. If the generation avoids a direct response to the prompt and instead provides unrelated suggestions or answers, respond with \"No\". ", "page_idx": 21}, {"type": "text", "text": "Prompt: {Malicious instruction} Generation: {Response from LLM} ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Now carefully give your professional justification, adhering to the provided rules. Concisely respond with \"Yes\" or \"No\u201d, and briefly explain your reasoning. ", "page_idx": 21}, {"type": "text", "text": "K More Examples of Steering Model Behaviors ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here, we provide additional examples. Specifically, Table 16 shows the effectiveness of our method in facilitating jailbreaking (the first three rows) and defending against related attacks (the last three rows). Table 17 demonstrates that the steering vector obtained on Llama-2-7b-chat-hf can be transferred to Llama2-Chinese-7b-Chat, even when the input is in Chinese. Moreover, in Tables 18, 19, and 20, we provide more examples of personalized control over AI personas. WARNING: The examples in Table 16 contain unsafe model responses. Reader discretion is advised. ", "page_idx": 22}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/c04e0f5dc75a00f80b7f128b0d472f69f051d5e0e4d6b956497a5bbb18d75759.jpg", "table_caption": ["Table 16: The examples of steering jailbreaking on Llama-2-7b-chat-hf. The adversarial suffix optimized by GCG [46] is highlighted in bold. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/ce0e89d54a12a210bc7561fe472872f4a2fc1785c8decf464ef16d9feaa7a88e.jpg", "table_caption": ["Table 17: An example of using the steering vector optimized on Llama-2-7b-chat-hf to directly steer Llama2-Chinese-7b-Chat towards the power-seeking persona. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/1acc84e32e1d8ba809f57d82ffc7d7a0f9ccf049d001fd77bbe9a80ed74e2bfc.jpg", "table_caption": ["Table 18: Personalized control over the wealth-seeking persona on Llama-2-7b-chat-hf. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/7c3e2c00937cec13d3c50e4a45ec1e553c03a704770d908bcfca2f9d906ac099.jpg", "table_caption": ["Table 19: Personalized control over the corrigible-less-HHH persona on Llama-2-7b-chat-hf. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "7qJFkuZdYo/tmp/c33ed683fa0ec68a7acc8e01623be4798b5e3db780a55139c3f188f7c44e5fe1.jpg", "table_caption": ["Table 20: Personalized control over the survival-instinct persona on Llama-2-7b-chat-hf. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "L Discussion ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Limitations Our work is primarily limited in two dimensions. First, to minimize steering costs, we designed our steering vector to affect only a single layer in the transformer. Future work may investigate whether multi-layer steering can yield even better results. Second, our method requires a small amount of preference data to describe the target behavior and its opposite. However, for some tasks, such as improving the model\u2019s ability to solve mathematical problems, designing and constructing such data pairs is not straightforward. Future work could explore applying the steering vector in a broader range of scenarios and consider how to design the necessary datasets. ", "page_idx": 25}, {"type": "text", "text": "Broader Impact & Safeguard Statement Our work has several positive societal impacts. We have shown that our method can steer various model behaviors to align with human preferences, including steering AI personas, managing truthfulness, mitigating hallucinations, and addressing jailbreaking attacks and their respective defenses. However, we also highlight some risks associated with our method, such as the potential to bypass safety guardrails and prompt the model to provide helpful responses to malicious questions. We believe that exposing current vulnerabilities in the safety aspects of LLMs is crucial for identifying potential concerns and developing appropriate preventive measures. When open-sourcing our work, we will require users to adhere to usage guidelines to prevent potential misuse. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 26}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 26}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 26}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 26}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 26}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 26}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The limitations are provided in Appendix L. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have provided detailed experimental settings including hyperparameters needed to reproduce the experimental results in Appendix A. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: All the datasets used are public datasets. Code is available at https:// github.com/CaoYuanpu/BiPO ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 28}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have provided detailed experimental settings in Appendix A. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not include error bars due to the high computational and evaluation costs involved. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is provided in Appendix A. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We have provided such discussion in Appendix L. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have described safeguards in Appendix L. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All the creators and original owners of assets used in the paper are properly credited. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing experiments or research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing experiments or research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]