{"references": [{"fullname_first_author": "A. Agrawal", "paper_title": "Don't just assume; look and answer: Overcoming priors for visual question answering", "publication_date": "2018-00-00", "reason": "This paper is highly relevant as it addresses the issue of overcoming priors in visual question answering, a problem directly related to the challenges in multi-modal learning tackled in the main paper."}, {"fullname_first_author": "P. Anderson", "paper_title": "Bottom-up and top-down attention for image captioning and visual question answering", "publication_date": "2018-00-00", "reason": "This work is fundamental as it introduced a method that combines bottom-up and top-down attention mechanisms for image captioning and visual question answering, a highly influential technique in the field that is relevant to the techniques discussed in the main paper."}, {"fullname_first_author": "S. Antol", "paper_title": "VQA: Visual question answering", "publication_date": "2015-00-00", "reason": "This paper introduced the Visual Question Answering (VQA) task, a benchmark dataset and problem setting for evaluating multi-modal models that is used in this paper for validation, making it highly important for the field and its evaluation."}, {"fullname_first_author": "T. Baltru\u0161aitis", "paper_title": "Multimodal machine learning: A survey and taxonomy", "publication_date": "2018-00-00", "reason": "This survey paper provides a comprehensive overview and taxonomy of multimodal machine learning, which is fundamental to understanding the landscape of the field and placing the main paper's contributions within it."}, {"fullname_first_author": "G. Barnum", "paper_title": "On the benefits of early fusion in multimodal representation learning", "publication_date": "2020-11-00", "reason": "This paper investigates the benefits of early fusion in multimodal representation learning, a key architectural choice with implications for the design and performance of multi-modal models as discussed in this paper."}]}