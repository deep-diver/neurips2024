{"references": [{"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduces the chain-of-thought prompting method, which is the foundation for the current work and significantly impacts the field of large language model reasoning."}, {"fullname_first_author": "Shunyu Yao", "paper_title": "Tree of thoughts: Deliberate problem solving with large language models", "publication_date": "2024-00-00", "reason": "This paper introduces the tree-of-thought method, which is directly compared to and improved upon in the current work."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-00-00", "reason": "This paper introduces the direct preference optimization method, which is the core algorithm used in the proposed CPO method."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-00", "reason": "This paper introduces the LLaMA 2 model, one of the base LLMs used in the experiments."}, {"fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "publication_date": "2023-10-00", "reason": "This paper introduces the Mistral 7B model, the other base LLM used in the experiments."}]}