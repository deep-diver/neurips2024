{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture and the scaled dot-product attention mechanism, which are fundamental to many modern deep learning models and directly inspired the work in this paper."}, {"fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "publication_date": "2022-12-01", "reason": "This paper introduced FlashAttention, a highly efficient implementation of self-attention that reduces both the time and memory complexity, which this paper builds upon to improve neighborhood attention."}, {"fullname_first_author": "Ali Hassani", "paper_title": "Neighborhood attention transformer", "publication_date": "2023-01-01", "reason": "This paper introduced the concept of neighborhood attention, which is the core focus of the current paper's improvements, and provides the baseline method for performance comparisons."}, {"fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "publication_date": "2020-04-01", "reason": "This paper introduced Longformer, another approach to address the quadratic complexity of self-attention which this paper uses as a comparison point for the approach of neighborhood attention."}, {"fullname_first_author": "Prajit Ramachandran", "paper_title": "Stand-alone self-attention in vision models", "publication_date": "2019-12-01", "reason": "This paper explored the use of self-attention in vision models, addressing challenges and providing alternative implementations, which is relevant to this paper's focus on improving attention mechanisms for higher-dimensional data."}]}