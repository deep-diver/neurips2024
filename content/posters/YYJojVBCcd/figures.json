[{"figure_path": "YYJojVBCcd/figures/figures_1_1.jpg", "caption": "Figure 1: We compare the Pareto frontiers between the model trained with scarce data and that trained with rich data. Acquiring more data is capable of shifting the Pareto frontier toward lower disparity and lower error rates. In consequence, we can reach a new trade-off point that offers improved fairness and accuracy simultaneously, surpassing the original trade-off point.", "description": "This figure compares Pareto frontiers for models trained on datasets with different sizes (scarce vs. rich data).  The x-axis represents the model error rate, and the y-axis represents fairness violation. The plot shows that a model trained on a larger dataset (rich data) achieves a better Pareto frontier, allowing for lower error rates and lower fairness violations simultaneously than a model trained on a smaller dataset (scarce data).  Acquiring more data shifts the Pareto frontier, leading to a superior trade-off point.", "section": "1 Introduction"}, {"figure_path": "YYJojVBCcd/figures/figures_8_1.jpg", "caption": "Figure 2: Main results on CelebA, Adult and Compas datasets. The Y axis shows fairness_violation; X axis denotes test_accuracy. CelebA: Four binary targets: Smiling, Attractive, Young, and Big_Nose; Sensitive attribute: gender. Adult: Binary target: Income; Sensitive attribute: Age. Compas: Binary target: Recidivism; Sensitive attribute: Race. We select two fairness metrics DP and Eop to measure fairness violations for each setting. The vertical dotted line at the random baseline accuracy helps easily identify which results achieve fairness without sacrificing performance (accuracy).", "description": "This figure compares the performance of different fairness-aware algorithms on three datasets (CelebA, Adult, and COMPAS) using two fairness metrics (DP and EOp). Each point in the graph represents the test accuracy and fairness violation of a particular algorithm on a specific dataset and metric. The goal is to achieve low fairness violation while maintaining high test accuracy. The vertical dotted line indicates the baseline accuracy achieved by a random model.  The results show that the proposed FIS algorithm significantly improves fairness without sacrificing accuracy, surpassing other existing methods.", "section": "6 Empirical results"}, {"figure_path": "YYJojVBCcd/figures/figures_9_1.jpg", "caption": "Figure 2: Main results on CelebA, Adult and Compas datasets. The Y axis shows fairness_violation; X axis denotes test_accuracy. CelebA: Four binary targets: Smiling, Attractive, Young, and Big_Nose; Sensitive attribute: gender. Adult: Binary target: Income; Sensitive attribute: Age. Compas: Binary target: Recidivism; Sensitive attribute: Race. We select two fairness metrics DP and Eop to measure fairness violations for each setting. The vertical dotted line at the random baseline accuracy helps easily identify which results achieve fairness without sacrificing performance (accuracy).", "description": "This figure compares the performance of different fairness-aware machine learning methods on three real-world datasets (CelebA, Adult, and COMPAS).  The Y-axis represents the fairness violation (measured using demographic parity and equality of opportunity), while the X-axis shows the test accuracy.  Each dataset and fairness metric is shown as a separate graph.  The figure demonstrates that the proposed Fair Influential Sampling (FIS) method achieves a better trade-off between fairness and accuracy than several baseline methods. The vertical dotted line indicates the baseline accuracy achieved by random sampling.", "section": "6 Empirical results"}, {"figure_path": "YYJojVBCcd/figures/figures_18_1.jpg", "caption": "Figure 4: We validate how accurate the first-order estimation of the influence is in comparison to the real influence. The x-axis represents the actual influence per sample, and the y-axis represents the estimated influence. We observe that while some of the examples are away from the diagonal line (which indicates the estimation is inaccurate), the estimated influences for most of the data samples are very close to their actual influence values.", "description": "This figure validates the accuracy of the first-order approximation used to estimate the influence of a data sample on model accuracy and fairness. The plot shows a strong positive correlation between the actual influence and the first-order estimation. Although some points deviate from the diagonal line representing perfect agreement, most of the points cluster around it, indicating that the first-order approximation provides a reasonably accurate estimate.", "section": "C Detailed analysis of the fair influential sampling algorithm"}, {"figure_path": "YYJojVBCcd/figures/figures_27_1.jpg", "caption": "Figure 2: Main results on CelebA, Adult and Compas datasets. The Y axis shows fairness_violation; X axis denotes test_accuracy. CelebA: Four binary targets: Smiling, Attractive, Young, and Big_Nose; Sensitive attribute: gender. Adult: Binary target: Income; Sensitive attribute: Age. Compas: Binary target: Recidivism; Sensitive attribute: Race. We select two fairness metrics DP and Eop to measure fairness violations for each setting. The vertical dotted line at the random baseline accuracy helps easily identify which results achieve fairness without sacrificing performance (accuracy).", "description": "This figure compares the performance of different fairness-aware active learning algorithms on three datasets (CelebA, Adult, and Compas).  The y-axis represents the fairness violation (lower is better), while the x-axis represents the test accuracy (higher is better).  Each point represents a different algorithm. The dotted vertical line indicates the baseline accuracy achieved by random sampling. The figure shows that FIS generally achieves better fairness-accuracy tradeoffs than other algorithms tested.", "section": "6 Empirical results"}, {"figure_path": "YYJojVBCcd/figures/figures_27_2.jpg", "caption": "Figure 2: Main results on CelebA, Adult and Compas datasets. The Y axis shows fairness_violation; X axis denotes test_accuracy. CelebA: Four binary targets: Smiling, Attractive, Young, and Big_Nose; Sensitive attribute: gender. Adult: Binary target: Income; Sensitive attribute: Age. Compas: Binary target: Recidivism; Sensitive attribute: Race. We select two fairness metrics DP and Eop to measure fairness violations for each setting. The vertical dotted line at the random baseline accuracy helps easily identify which results achieve fairness without sacrificing performance (accuracy).", "description": "The figure compares the performance of different fairness-aware algorithms (Base(ERM), Random, BALD, ISAL, JTT-20, and FIS) on three datasets (CelebA, Adult, and Compas) using two fairness metrics (DP and EOp).  Each point on the graphs represents the test accuracy and fairness violation of a model trained using the specific algorithm. The vertical dotted line indicates the baseline accuracy achieved by random sampling.  The figure demonstrates that FIS generally achieves lower fairness violations while maintaining comparable accuracy compared to other methods.", "section": "6 Empirical results"}, {"figure_path": "YYJojVBCcd/figures/figures_28_1.jpg", "caption": "Figure 2: Main results on CelebA, Adult and Compas datasets. The Y axis shows fairness_violation; X axis denotes test_accuracy. CelebA: Four binary targets: Smiling, Attractive, Young, and Big_Nose; Sensitive attribute: gender. Adult: Binary target: Income; Sensitive attribute: Age. Compas: Binary target: Recidivism; Sensitive attribute: Race. We select two fairness metrics DP and Eop to measure fairness violations for each setting. The vertical dotted line at the random baseline accuracy helps easily identify which results achieve fairness without sacrificing performance (accuracy).", "description": "The figure compares the performance of different fairness-enhancing methods on three benchmark datasets (CelebA, Adult, and Compas). It shows the trade-off between test accuracy and fairness violation (measured by demographic parity and equal opportunity).  The results demonstrate that FIS achieves lower fairness violations while maintaining comparable accuracy compared to other baselines.", "section": "6 Empirical results"}]