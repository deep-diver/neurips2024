{"importance": "This paper is crucial for researchers striving for fairness in machine learning.  It offers a novel active sampling approach that avoids the typical fairness-accuracy trade-off and doesn't need sensitive attributes for training, opening new avenues for research in privacy-preserving fairness.", "summary": "FairnessWithoutHarm achieves fairer ML models without sacrificing accuracy by using an influence-guided active sampling method that doesn't require sensitive training data.", "takeaways": ["Proposes a new active sampling method to improve fairness without harming accuracy.", "The method avoids using sensitive attributes during training, enhancing privacy.", "Theoretically analyzes how data acquisition improves fairness and empirically validates its effectiveness."], "tldr": "Many machine learning models exhibit bias towards protected groups, often at the cost of accuracy.  Existing fairness-focused active learning methods typically require annotating sensitive attributes, raising privacy concerns.  This creates a fairness-accuracy trade-off that is difficult to overcome. \nThis paper introduces Fair Influential Sampling (FIS), a novel active learning approach that addresses these issues. FIS scores data points based on their influence on fairness and accuracy using a small validation set without sensitive attributes.  It then selects the most influential samples for training, improving model fairness without compromising accuracy.  The algorithm's effectiveness is demonstrated through theoretical analysis and real-world experiments, providing upper bounds for generalization error and risk disparity.", "affiliation": "UC Santa Cruz", "categories": {"main_category": "AI Theory", "sub_category": "Fairness"}, "podcast_path": "YYJojVBCcd/podcast.wav"}