[{"type": "text", "text": "Last-Iterate Global Convergence of Policy Gradients for Constrained Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alessandro Montenegro Politecnico di Milano, Milan, Italy alessandro.montenegro@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Marco Mussi Politecnico di Milano, Milan, Italy marco.mussi@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Matteo Papini Politecnico di Milano, Milan, Italy matteo.papini@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Alberto Maria Metelli Politecnico di Milano, Milan, Italy albertomaria.metelli@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Constrained Reinforcement Learning (CRL) tackles sequential decision-making problems where agents are required to achieve goals by maximizing the expected return while meeting domain-specific constraints, which are often formulated as expected costs. In this setting, policy-based methods are widely used since they come with several advantages when dealing with continuous-control problems. These methods search in the policy space with an action-based or parameter-based exploration strategy, depending on whether they learn directly the parameters of a stochastic policy or those of a stochastic hyperpolicy. In this paper, we propose a general framework for addressing CRL problems via gradient-based primal-dual algorithms, relying on an alternate ascent/descent scheme with dualvariable regularization. We introduce an exploration-agnostic algorithm, called C-PG, which exhibits global last-iterate convergence guarantees under (weak) gradient domination assumptions, improving and generalizing existing results. Then, we design C-PGAE and C-PGPE, the action-based and the parameter-based versions of C-PG, respectively, and we illustrate how they naturally extend to constraints defined in terms of risk measures over the costs, as it is often requested in safety-critical scenarios. Finally, we numerically validate our algorithms on constrained control problems, and compare them with state-of-the-art baselines, demonstrating their effectiveness. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When applying Reinforcement Learning (RL, Sutton and Barto, 2018) to real-world scenarios, we are tasked with addressing large-scale continuous control problems where, in addition to reaching a goal, it is necessary to meet structural or utility-based constraints. For instance, an autonomousdriving car has its main objective of getting to the desired destination (i.e., goal) while avoiding collisions, ensuring the safety of people on the streets, adhering to traffic rules, and respecting the physical requirements of the engine to avoid damaging it (i.e., constraints) (Likmeta et al., 2020). To pursue such an objective, it is necessary to extend the RL problem formulation with the possibility to account for constraints. Constrained Reinforcement Learning (CRL, Uchibe and Doya, 2007) aims at solving this family of problems by employing RL techniques to tackle Constrained Markov Decision Processes (CMDPs, Altman, 1999), which provide an established and widely-used framework for modeling constrained control tasks. The conventional CRL framework primarily focuses on constraints related directly to expected costs (Stooke et al., 2020; Ding et al., 2020; Ying et al., 2022; Ding et al., 2024). However, especially in safety-critical contexts, the expected cost may not represent a reliable index of safe behavior. In response to this issue, chance constraints were introduced to ensure that the probability of unsafe events is minimized. Nonetheless, employing chance constraints presents several challenges (Chow et al., 2017). To strike a balance between these two extremes, constraints are defined in terms of risk measures over the costs. Examples include the Conditional Value at Risk (CVaR, Rockafellar and Uryasev, 2000) and the Mean-Variance (MV, Markowitz and Todd, 2000; Li and Ng, 2000). These risk measures offer a generalization of the previous concepts, allowing for the consideration of uncertainties while preserving the focus on the cost. When incorporating constraints on risk measures, CRL is often referred to as Risk-CRL (Chow et al., 2017). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Among the RL methods applicable to CMDPs, Policy Gradients (PGs, Deisenroth et al., 2013) are particularly appealing. Indeed, PGs have demonstrably achieved impressive results in continuouscontrol problems due to several advantages that make them well-suited for real-world applications. These advantages include the ability to handle continuous state and action spaces (Peters and Schaal, 2006), resilience to sensor and actuator noise (Gravell et al., 2020), robustness in partially-observable environments (Azizzadenesheli et al., 2018), and the possibility of incorporating expert knowledge during policy design (Ghavamzadeh and Engel, 2006), which can simplify the learning process and improve the efficacy, safety, and interpretability of the learned policy (Likmeta et al., 2020). PGs can be categorized into two key families depending on the way exploration is carried out in the policy space (Montenegro et al., 2024). Following their taxonomy, we distinguish between the action-based and the parameter-based exploration paradigms. The former, employed by REINFORCE (Williams, 1992) and GPOMDP (Baxter and Bartlett, 2001), focuses on directly learning the parameters of a parametric stochastic policy. The latter, employed by PGPE (Sehnke et al., 2010), is tasked with learning the parameters of a parametric stochastic hyperpolicy from which the parameters of the actual policy (often deterministic) are sampled. ", "page_idx": 1}, {"type": "text", "text": "Policy-based CRL has gained significant popularity in solving constrained control problems (Achiam et al., 2017). Within this field, algorithms are primarily developed using primal-dual methods (Chow et al., 2017; Tessler et al., 2019; Ding et al., 2020, 2021; Bai et al., 2022), which can be formulated through Lagrangian optimization of the primal (i.e., policy parameters) and dual variable (i.e., Lagrange multipliers). Even though the distinction between the exploration paradigms is well known in the PG methods literature, the current state of the art in Policy-based CRL focuses only on the action-based exploration approach (Achiam et al., 2017; Stooke et al., 2020; Bai et al., 2023), while the parameter-based one remains unexplored. A critical challenge for policy-based Lagrangian optimization algorithms is ensuring convergence guarantees. Existing works have spent a notable effort in this direction (Ying et al., 2022; Gladin et al., 2023; Ding et al., 2024). Recently, (Ying et al., 2022; Gladin et al., 2023; Ding et al., 2024) manage to ensure global last-iterate convergence guarantees. However, these approaches are affected by some notable limitations: $(i)$ the provided convergence rates depend on the problem dimension (e.g., the cardinality of the state and action spaces), limiting their applicability to tabular CMPDs and preventing scaling to realistic continuous control problems; (ii) they focus on softmax policies only, disregarding other more realistic policy models; (iii) (Ding et al., 2024) ensure convergence when a single constraint only is present. ", "page_idx": 1}, {"type": "text", "text": "Original Contribution. The goal of this work is to introduce a framework for solving constrained continuous control problems using policy-based primal-dual algorithms that operate in both the action-based and parameter-based policy gradient exploration scenarios, while providing global last-iterate convergence guarantees with general (hyper)policy parameterization. Specifically, the main contributions of this work can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 In Section 2, we introduce a general constrained optimization problem, which is agnostic w.r.t. both the action-based or parameter-based paradigm.   \n\u2022 In Section 3, we introduce C-PG, a general policy-based primal-dual algorithm optimizing the regularized Lagrangian function associated with the general constrained optimization problem shown in Section 2. We show that, under (weak) domination assumptions, it simultaneously achieves the following: (i) last-iterate convergence guarantees to a globally optimal feasible policy (i.e., satisfying all constraints); $(i i)$ compatibility with CMDPs having continuous state and action spaces; (iii) the ability to handle multiple constraints.   \n\u2022 In Section 4, we introduce C-PGAE and C-PGPE, the action-based and parameter-based versions of C-PG, respectively. Both algorithms are designed to also handle constraints on risk measures, thus extending the applicability space of the father algorithm C-PG. This is achieved by employing ", "page_idx": 1}, {"type": "text", "text": "a parametric unified risk measure formulation, for which we show the mapping to several risk measures of the unified one and we present the specific form of all the estimators. ", "page_idx": 2}, {"type": "text", "text": "In Section 5, we numerically validate our proposals against state-of-the-art baselines in constrained control problems. Related works are discussed in Appendix B. The proofs of all the statements are reported in Appendix E. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. For a measurable set $\\mathcal{X}$ , we denote as $\\Delta(\\mathcal X)$ the set of probability measures over $\\mathcal{X}$ . For $P\\in\\Delta(\\mathcal{X})$ , we denote with $p$ its density function and we will interchangeably use $x\\sim P$ or $x\\sim p$ to express that random variable $x$ is distributed according to $P$ . For $n,m\\in\\mathbb{N}$ with $n\\leqslant m$ , we denote $[\\![n]\\!]:=\\{1,2,\\dots,n\\}$ and with $[n,m]\\,:=\\,\\{n,n+1,\\dots,m\\}$ . For a vector $\\pmb{x}\\in\\mathbb{R}^{d}$ , we denote as $x_{i}$ t he $i$ -th component of $\\textbf{\\em x}$ . F o r $a\\in\\mathbb{R}$ , we define $(a)^{+}:=\\operatorname*{max}\\{0,a\\}$ and we extend the notation to vectors as $(\\pmb{x})^{+}\\,=\\,((x_{1})^{+},\\ldots,(x_{d})^{+})^{\\top}$ . Given a set $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , we denote with $\\Pi_{\\mathcal{X}}$ the Euclidean norm projection, i.e., $\\begin{array}{r}{\\Pi_{\\mathcal{X}}\\pmb{x}\\in\\mathrm{\\arg\\operatorname*{min}}_{\\pmb{y}\\in\\mathcal{X}}\\,\\|\\pmb{y}-\\pmb{x}\\|_{2}}\\end{array}$ for any $\\pmb{x}\\in\\mathbb{R}^{d}$ . For two vectors $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}\\in\\mathbb{R}^{d}$ , we denote with $\\langle\\pmb{x},\\pmb{y}\\rangle$ their inner product. A function $f\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is $L_{1}$ - Lipschitz continuous if $|f(\\pmb{x})-f(\\pmb{x}^{\\prime})|\\leqslant L_{1}\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|_{2}^{-}$ and $L_{2}$ -smooth if it is differentiable and $\\|\\nabla_{x}f(\\pmb{x})-\\nabla_{x}f(\\pmb{x}^{\\prime})\\|_{2}\\leqslant L_{2}\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|_{2}$ , for every $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}\\in\\mathbb{R}^{d}$ . ", "page_idx": 2}, {"type": "text", "text": "Constrained Markov Decision Processes. A Constrained Markov Decision Process (CMDP, Altman, 1999) with $U$ constraints is represented by $\\mathcal{M}_{\\mathcal{C}}:=\\left(\\mathcal{S},\\mathcal{A},p,r,\\{c_{i}\\}_{i\\in[[U]]},\\{b_{i}\\}_{i\\in[[U]]},\\mu_{0},\\gamma\\right)$ , where $s\\subseteq\\mathbb{R}^{d_{S}}$ and $\\mathcal{A}\\subseteq\\mathbb{R}^{d_{\\mathcal{A}}}$ are the measurable state an\\`d action spaces, $p:\\mathcal{S}\\times\\mathcal{A}\\to\\Delta\\left(\\mathcal{S}\\right)$ is the transition model, where $p(s^{\\prime}|s,a)$ is the probability density of getting to state $s^{\\prime}\\in\\mathcal{S}$ given that action $\\pmb{a}\\in\\mathcal{A}$ is taken in state $s\\in S$ , $r:S\\times A\\to[-1,0]$ is the reward function, where $r(s,a)$ is the instantaneous reward obtained by playing action $\\textbf{\\em a}$ in state $\\pmb{s}$ , $c_{i}:S\\times A\\to[0,1]$ is the $i$ -th cost function, where $c_{i}(s,a)$ is the $i$ -th instantaneous cost obtained by playing action $\\textbf{\\em a}$ in state $\\pmb{s}$ $b_{i}\\in[0,J_{\\operatorname*{max}}]$ with $\\begin{array}{r}{J_{\\mathrm{max}}:=\\frac{1-\\gamma^{T}}{1-\\gamma}}\\end{array}$ 11\u00b4\u03b3\u03b3 is the threshold for the i-th cost for every i P U , \u00b50 P \u2206pSq is the initial state distribution, and $\\gamma\\in[0,1]$ is the discount factor. A trajectory $\\tau$ of length $T\\in\\mathbb{N}\\cup\\{+\\infty\\}$ is a sequence of $T$ state-action pairs: $\\tau=\\left({{s_{\\tau,0}},{a_{\\tau,0}},\\ldots,{s_{\\tau,T-1}},{a_{\\tau,T-1}}}\\right)$ . The discounted return over a trajectory $\\tau$ is $\\begin{array}{r}{R(\\tau):=\\sum_{t=0}^{T-1}\\gamma^{t}r(s_{\\tau,t},\\mathbf{a}_{\\tau,t})}\\end{array}$ , while the $i^{\\th}$ -th discounted cumulative cost is $\\begin{array}{r}{C_{i}(\\tau):=\\sum_{t=0}^{T-1}\\gamma^{t}c_{i}(s_{\\tau,t},\\mathbf{a}_{\\tau,t})}\\end{array}$ tfioorn aelv ecroys oann $c_{0}(s,a):=-r(s,a)$ and $C_{0}(\\tau):=-R(\\tau)$ $\\dot{R(\\tau)},C_{i}(\\tau)\\in[0,J_{\\operatorname*{max}}]$ $i\\in\\mathbb{[}U]$ $\\tau$ ", "page_idx": 2}, {"type": "text", "text": "Action-based Policy Gradients. Action-based (AB) PG methods focus on learning the parameters $\\pmb{\\theta}\\in\\Theta\\subseteq\\mathbb{R}^{d_{\\Theta}}$ of a parametric stochastic policy $\\pi_{\\pmb{\\theta}}:{\\mathcal{S}}\\rightarrow\\Delta({\\mathcal{A}})$ , where $\\pi_{\\pmb\\theta}(\\pmb{a}|\\pmb{s})$ represents the probability density of selecting action $\\mathbf{a}\\in{\\mathcal{A}}$ being in state $s\\in S$ . At each step $t$ of the interaction with the environment, the stochastic policy is employed to sample an action $\\mathbf{\\boldsymbol{a}}_{t}\\,\\sim\\,\\pi_{\\pmb{\\theta}_{t}}(\\cdot|\\mathbf{\\boldsymbol{s}}_{t})$ . To assess the performance of $\\pi_{\\theta}$ w.r.t. the $i^{\\th}$ -th cost function, with $i\\;\\in\\;\\mathbb{[}0,U]$ , we employ the $A B$ performance index $J_{\\mathrm{A},i}~:~\\Theta~\\rightarrow~\\mathbb{R}$ , which is defined as $J_{\\mathrm{A},i}(\\pmb\\theta)~:=~\\overline{{\\mathbb{E}}}_{\\tau\\sim p_{\\mathrm{A}}(\\cdot|\\pmb\\theta)}\\left[C_{i}(\\tau)\\right]$ , where $\\begin{array}{r}{p_{\\mathrm{A}}(\\tau,\\boldsymbol{\\theta}):=\\mu_{0}(s_{\\tau,0})\\prod_{t=0}^{T-1}\\pi_{\\boldsymbol{\\theta}}(a_{\\tau,t}|s_{\\tau,t})p(s_{\\tau,t+1}|s_{\\tau,t},a_{\\tau,t})}\\end{array}$ is the density of trajectory $\\tau$ induced by policy $\\pi_{\\theta}$ . ", "page_idx": 2}, {"type": "text", "text": "Parameter-based Policy Gradients. Parameter-based (PB) PG methods focus on learning the parameters $\\pmb{\\rho}\\in\\mathcal{R}\\subseteq\\mathbb{R}^{d_{\\mathcal{R}}}$ of a parametric stochastic hyperpolicy $\\nu_{\\rho}\\in\\Delta(\\Theta)$ . The hyperpolicy $\\nu_{\\rho}$ is used to sample parameter configurations $\\theta\\sim\\nu_{\\rho}$ to be plugged into an underlying parametric policy $\\pi_{\\theta}$ , that will be then used for the interaction with the environment. Notice that $\\pi_{\\theta}$ can also be deterministic. To assess the performance of $\\nu_{\\rho}$ w.r.t. the $i$ -th cost function, with $i\\in[0,U]$ , we employ the $P B$ performance index $J_{\\mathrm{P},i}:\\mathcal{R}\\rightarrow\\mathbb{R}$ , which is defined as $\\begin{array}{r}{J_{\\mathrm{P},i}(\\pmb{\\rho}):=\\mathbb{E}_{\\pmb{\\theta}\\sim\\nu_{\\rho}}[\\mathbb{E}_{\\tau\\sim p_{\\mathrm{A}}(\\cdot|\\pmb{\\theta})}^{-}\\,[C_{i}(\\tau)]]}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Constrained Optimization Problem. Having introduced the AB and PB performance indices, we formulate a constrained optimization problem (COP) agnostic w.r.t. the exploration paradigm: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\bf{v}}\\in\\mathcal{V}}J_{\\dagger,0}({\\boldsymbol{\\mathbf{v}}})\\quad\\mathrm{s.t.}\\quad J_{\\dagger,i}({\\boldsymbol{\\mathbf{v}}})\\leqslant b_{i},\\ \\ \\forall i\\in\\[U],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\dagger\\in\\{\\mathrm{A},\\mathrm{P}\\}$ and $\\pmb{v}$ is a generic parameter vector belonging to the parameter space $\\mathcal{V}$ . When $\\dagger\\,=\\,{\\mathrm A}$ , we are considering the AB exploration paradigm, then $\\mathcal{V}=\\Theta$ . On the other hand, when $\\dagger=\\mathrm{P}$ , we are in the PB exploration paradigm, then $\\mathcal{V}=\\mathcal{R}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Last-Iterate Global Convergence of C-PG ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present C-PG, a general primal-dual algorithm that optimizes a regularized version of the Lagrangian function (Section 3.1) associated with the COP of Equation (1). After having introduced the necessary assumptions (Section 3.2), we show that C-PG exhibit dimension-free last-iterate global convergence guarantees (Section 3.3). While C-PG is designed to be agnostic w.r.t. the exploration approach, we introduce two specific versions of C-PG in Section 4 for AB or PB, respectively. For notation convenience, in the rest of this section, we use $J_{i}$ in place of $J_{\\dagger,i}$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Regularized Lagrangian Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To solve the COP of Equation (1) we resort to the method of Lagrange multipliers (Bertsekas, 2014) introducing the Lagrangian function $\\begin{array}{r}{\\mathcal{L}_{0}(v,\\lambda):=J_{0}(v)+\\sum_{i=1}^{\\bar{U}}\\dot{\\lambda_{i}}\\left(J_{i}(v)\\overset{\\cdot}{-}b_{i}\\right)=J_{0}(v)+}\\end{array}$ $\\langle\\lambda,\\mathbf{J}(v)-\\mathbf{b}\\rangle$ , where $\\pmb{v}\\in\\mathcal{V}$ is the primal variable and $\\lambda\\!\\in\\mathbb{R}_{\\geqslant0}^{U}$ a\u0159re the Lagrangian multipliers or dual variable, $\\mathbf{J}\\,=\\,(J_{1},\\ldots,J_{U})^{\\top}$ , and $\\mathbf{b}\\,=\\,(b_{1},\\ldots,b_{U})^{\\top}$ . This allows to rephrase the COP in Equation (1) as a min-max optimization problem $\\begin{array}{r}{\\operatorname*{min}_{\\boldsymbol{v}\\in\\mathcal{V}}\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{\\ge0}^{U}}\\mathcal{L}_{0}(\\boldsymbol{v},\\lambda)}\\end{array}$ and we denote with $\\begin{array}{r}{H_{0}(v)\\,:=\\,\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{\\geq0}^{U}}\\mathcal{L}_{0}(v,\\lambda)}\\end{array}$ the primal function and with $\\begin{array}{r}{H_{0}^{*}:=\\,\\operatorname*{min}_{\\pmb{v}\\in\\mathcal{V}}H_{0}(\\pmb{v})}\\end{array}$ . To obtain a last-iterate convergence guarantee, we make use of a regularization approach. Specifically, let $\\omega>0$ be a regularization parameter, we define the $\\omega$ -regularized Lagrangian function as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal L_{\\boldsymbol\\omega}(\\boldsymbol\\upsilon,\\boldsymbol\\lambda):=J_{0}(\\boldsymbol\\upsilon)+\\sum_{i=1}^{U}\\lambda_{i}\\left(J_{i}(\\boldsymbol\\upsilon)-b_{i}\\right)-\\frac{\\omega}{2}\\left\\Vert\\boldsymbol\\lambda\\right\\Vert_{2}^{2}=J_{0}(\\boldsymbol\\upsilon)+\\left\\langle\\boldsymbol\\lambda,\\mathbf{J}(\\boldsymbol\\upsilon)-\\boldsymbol\\mathbf{b}\\right\\rangle-\\frac{\\omega}{2}\\left\\Vert\\boldsymbol\\lambda\\right\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The ridge regularization makes $\\mathcal{L}_{\\omega}(v,\\lambda)$ a strongly concave function of $\\lambda$ at the price of a bias that is quantified in Lemmas E.1, E.2 and E.3. Thus, we address the $\\omega$ -regularized min-max optimization problem $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{v}\\in\\mathcal{V}}\\operatorname*{max}_{\\pmb{\\lambda}\\in\\Lambda}\\mathcal{L}_{\\omega}(\\pmb{v},\\pmb{\\lambda})}\\end{array}$ , where $\\Lambda:=\\{\\lambda\\in\\mathbb{R}_{\\geqslant0}^{U}\\,:\\,\\|\\lambda\\|_{2}^{-}\\leqslant\\omega^{-1}\\sqrt{U}J_{\\operatorname*{max}}\\}$ , in replacement of the original (non-regularized) one. For this problem, we introduce the primal function $H_{\\omega}(\\pmb{v}):=\\operatorname*{max}_{\\pmb{\\lambda}\\in\\Lambda}\\mathcal{L}_{\\omega}(\\pmb{v},\\pmb{\\lambda})$ , that, thanks to the ridge regularization, admits the closedform expression $\\begin{array}{r}{H_{\\omega}({\\pmb v})\\;=\\;J_{0}({\\pmb v})\\,+\\,\\frac{1}{2\\omega}\\sum_{i=1}^{U}\\bigl(\\bigl(J_{i}({\\pmb v})-b_{i}\\bigr)^{+}\\bigr)^{2}\\;=\\;J_{0}({\\pmb v})\\,+\\,\\frac{1}{2\\omega}\\|\\bigl({\\bf J}({\\pmb v})-{\\bf b}\\bigr)^{+}\\|_{2}^{2}\\,}\\end{array}$ where the optimal values of the Lagrange \u0159multipliers is given by $\\begin{array}{r}{\\lambda^{*}(\\boldsymbol{v})=\\Pi_{\\Lambda}\\left(\\frac{1}{\\omega}(\\mathbf{J}(\\boldsymbol{v})-\\mathbf{b})\\right)=}\\end{array}$ $\\begin{array}{r}{\\frac{1}{\\omega}({\\bf J}({\\bf v})-{\\bf b})^{+}}\\end{array}$ that is guaranteed to have norm smaller than $\\omega^{-1}\\sqrt{U}J_{\\mathrm{max}}$ . Furth\\`ermore, we de\u02d8fine $\\begin{array}{r}{\\tilde{H}_{\\omega}^{*}:=\\operatorname*{min}_{v\\in\\mathcal{V}}H_{\\omega}(v)}\\end{array}$ . C-PG updates the parameters $(v_{k},\\lambda_{k})$ with an alternate gradient descentascent scheme for every $k\\in\\mathbb{N}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{v_{k+1}\\leftarrow\\Pi_{\\mathcal{V}}\\Big(v_{k}-\\zeta_{v,k}^{\\star}\\hat{\\nabla}_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})\\Big)}&{\\lambda_{k+1}\\leftarrow\\Pi_{\\Lambda}\\Big(\\lambda_{k}+\\zeta_{\\lambda,k}^{\\star}\\hat{\\nabla}_{\\lambda}\\mathcal{L}_{\\omega}(v_{k+1},\\lambda_{k})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\zeta_{v,k},\\zeta_{\\lambda,k}>0$ are the learning rates and $\\widehat{\\nabla}_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k}),\\widehat{\\nabla}_{\\lambda}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})$ are estimators of the gradients $\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k}),\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(\\bar{v_{k}},\\lambda_{k})$ of thep regularized Lagprangian function. ", "page_idx": 3}, {"type": "text", "text": "3.2 Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before diving into the study of the convergence guarantees of C-PG, in this part, we list and motivate the assumptions necessary for our analysis. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1 (Existence of Saddle Points). There exist $\\pmb{v}_{0}^{*}\\in\\mathcal{V}$ and $\\lambda_{0}^{*}\\ \\in\\ \\mathbb{R}_{\\geqslant0}^{U}$ such that $\\begin{array}{r}{\\mathcal{L}_{0}(v_{0}^{*},\\bar{\\lambda}_{0}^{*})=\\operatorname*{min}_{v\\in\\mathcal{V}}\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{\\geqslant0}^{U}}\\mathcal{L}_{0}(v,\\lambda)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1 ensures that the value of the min-max problem is attained by a pair of primal-dual values $v_{0}^{*}\\in\\mathcal{V}$ and $\\lambda_{0}^{*}\\in\\mathbb{R}_{\\geqslant0}^{U}$ which, consequently, satisfy $\\mathcal{L}_{0}(v_{0}^{*},\\lambda)\\leqslant\\mathcal{L}_{0}(v_{0}^{*},\\lambda_{0}^{*})\\leqslant\\mathcal{L}_{0}(v,\\lambda_{0}^{*})$ for every $\\textbf{\\textit{v}}\\in\\ V$ and $\\lambda\\in\\mathbb{R}_{\\geqslant0}^{U}$ . Analogous assumptions have been considered by Yang et al. (2020) and Ying et al. (2022). Thus, $(v_{0}^{*},\\lambda_{0}^{*})$ is a saddle point of the Lagrangian function $\\scriptstyle{\\mathcal{L}}_{0}$ and, consequently, strong duality holds. Alternatively, as commonly requested in CRL works, assuming ", "page_idx": 3}, {"type": "text", "text": "Slater\u2019s condition combined with the requirement that the policy space covers all Markovian policies ensures strong duality (e.g., Paternain et al., 2019; Ding et al., 2020, 2024).2 ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2 (Weak $\\psi$ -Gradient Domination). Let $\\psi\\in[1,2]$ . There exist $\\alpha_{1}>0$ and $\\beta_{1}\\geqslant0$ such that, for every $\\boldsymbol{v}\\in\\mathcal{V}$ and $\\lambda{\\in}\\,\\Lambda$ , it holds that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\nabla_{v}\\mathcal{L}_{0}(v,\\lambda)\\|_{2}^{\\psi}\\geqslant\\alpha_{1}\\Big(\\mathcal{L}_{0}(v,\\lambda)-\\operatorname*{min}_{v^{\\prime}\\in\\mathcal{V}}\\mathcal{L}_{0}(v^{\\prime},\\lambda)\\Big)-\\beta_{1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2 is customary in the convergence analysis of policy gradient methods and it is usually enforced on the objective $J_{0}$ only (Yuan et al., 2022; Masiha et al., 2022; Fatkhullin et al., 2023). In particular, when $\\beta_{1}\\,=\\,0$ , we speak of (strong) $\\psi$ -gradient domination. In this form, for a generic exponent $\\psi\\in[1,2]$ , this assumption has been employed by Masiha et al. (2022). Particular cases are when $\\psi=1$ , which corresponds to the standard (weak) gradient domination (GD), while for $\\psi=2$ , we have the so-called Polyak-\u0141ojasiewicz (PL) condition. Notice that Assumption 3.2 is enforced on the non-regularized Lagrangian function ${\\mathcal{L}}_{0}$ (i.e., $\\omega=0$ ). However, it is easy to realize that it holds for the regularized one $\\mathcal{L}_{\\omega}$ by simply computing the terms of Equation (3) replacing ${\\mathcal{L}}_{0}$ with $\\mathcal{L}_{\\omega}$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 3.1 (When does Assumption 3.2 holds?). As remarked by Ding et al. (2024), the Lagrangian function, for a fixed value of $\\lambda_{i}$ , can be regarded as the expected return of a new reward function $-C_{0}-$ $\\langle\\lambda,\\mathbf{C}\\rangle$ , where $\\mathbf{C}=(C_{1},\\hdots,C_{U})^{\\top}$ . As a consequence, a sufficient condition for Assumption 3.2 is when the selected class of policies guarantees the $\\psi$ -gradient domination regardless of the reward function. For instance, in tabular environments with natural policy parametrization, i.e., $\\pi_{\\pmb{\\theta}}(s)=\\pmb{\\theta}_{s}$ for every $s\\in S$ , the $P L$ condition $\\psi=2$ and $\\beta_{1}=0_{,}$ ) holds (Bhandari and Russo, 2024). Moreover, in tabular environments with softmax policy, i.e., $\\pi_{\\theta}(a|s)\\propto\\exp(\\theta(s,a))$ , $G D$ $\\stackrel{\\cdot}{\\psi}=1$ and $\\beta_{1}=0$ ) holds (Mei et al., 2020). This enables a meaningful comparison of our results with resorting to softmax policies (e.g., Ding et al., 2020; Gladin et al., 2023; Ding et al., 2024). More in general, when (i) the Fisher information matrix induced by policy $\\pi_{\\theta}$ is non-degenerate for every $\\pmb\\theta\\in\\Theta$ , i.e., ${\\bf F}(\\pmb\\theta)=$ $\\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}]\\;\\ge\\;\\mu_{F}{\\bf I}$ for some $\\mu_{F}~>~0$ and (ii) a compatible function approximation bias bound holds, i.e., $\\mathbb{E}_{\\pi_{\\theta}\\ast}\\big[(A^{\\pi_{\\theta}}(s,a)-(1-\\gamma)\\pmb{u}^{\\top}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s))^{2}\\big]\\,\\leqslant\\,\\epsilon_{b i a s}$ being $\\pmb{u}=\\mathbf{F}(\\pmb{\\theta})^{\\dagger}\\nabla_{\\pmb{\\theta}}J_{0}(\\pmb{\\theta})$ and the advantage function $A^{\\pi\\theta}$ computed w.r.t. reward $-C_{0}-\\langle\\lambda,\\mathbf{C}\\rangle,$ , the weak $G D$ ( $\\psi\\,=\\,1,$ ) holds with $\\alpha_{1}\\,=\\,G\\mu_{F}^{-1}$ and $\\beta_{1}\\,=\\,(1-\\gamma)^{-1}\\sqrt{\\epsilon_{b i a s}},$ , where $G$ is such that $\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\|_{2}\\leqslant G$ (Masiha et al., 2022). ", "page_idx": 4}, {"type": "text", "text": "In principle, we could have enforced Assumption 3.2 on the primal function $H_{\\omega}(v)$ only. However, this would come with two drawbacks: $(i)$ the assumption would now depend explicitly on $\\omega$ ; $(i i)$ the considerations of Remark 3.1 would no longer hold. Nevertheless, in Lemma E.4, we prove that Assumption 3.2 induces an analogous property on the primal function $H_{\\omega}(v)$ in the regularized case. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.3 (Regularity of the Regularized Lagrangian $\\mathcal{L}_{\\omega}$ ). There exists $L_{1},L_{2},L_{3}>0$ such that, for every $\\pmb{v},\\pmb{v}^{\\prime}\\in\\mathcal{V}_{.}$ , and for every $\\lambda,\\lambda^{\\prime}\\!\\in\\Lambda,$ , the following hold: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{\\lambda}\\mathcal{L}_{0}(\\cdot,\\lambda)\\;L_{1}\\!\\cdot\\!L i p s c h i t z\\;w.r.t.\\;\\upsilon:\\;\\;}&{\\left\\|\\nabla_{\\lambda}\\mathcal{L}_{0}(\\upsilon,\\lambda)-\\nabla_{\\lambda}\\mathcal{L}_{0}(\\upsilon^{\\prime},\\lambda)\\right\\|_{2}\\leqslant L_{1}\\big\\|\\upsilon-\\upsilon^{\\prime}\\big\\|_{2}\\,,}\\\\ {\\mathcal{L}_{0}(\\cdot,\\lambda)\\;L_{2}\\!\\cdot\\!S m o o t h\\;w.r.t.\\;\\upsilon:\\;\\;}&{\\left\\|\\nabla_{\\upsilon}\\mathcal{L}_{0}(\\upsilon,\\lambda)-\\nabla_{\\upsilon}\\mathcal{L}_{0}(\\upsilon^{\\prime},\\lambda)\\right\\|_{2}\\leqslant L_{2}\\|\\upsilon-\\upsilon^{\\prime}\\|_{2},}\\\\ {\\nabla_{\\upsilon}\\mathcal{L}_{0}(\\upsilon,\\cdot)\\;L_{3}\\!\\cdot\\!L i p s c h i t z\\;w.r.t\\;\\lambda:\\;\\;}&{\\left\\|\\nabla_{\\upsilon}\\mathcal{L}_{0}(\\upsilon,\\lambda)-\\nabla_{\\upsilon}\\mathcal{L}_{0}(\\upsilon,\\lambda^{\\prime})\\right\\|_{2}\\leqslant L_{3}\\|\\lambda-\\lambda^{\\prime}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notice that, similarly to Assumption 3.2, we realize that if Assumption 3.3 holds for the nonregularized Lagrangian $\\scriptstyle{\\mathcal{L}}_{0}$ , it also holds (with the same constants) for the regularized one $\\mathcal{L}_{\\omega}$ for every $\\omega\\,>\\,0$ . The regularity conditions of Assumption 3.3 are common in the literature (Yang et al., 2020) and mild when regarded from the policy optimization perspective. Equation (4) is satisfied whenever the constraint functions $J_{i}$ are Lipschitz continuous w.r.t. $\\pmb{v}$ . Indeed, $\\|\\nabla_{\\lambda}\\mathcal{L}_{0}(v,\\lambda)-\\nabla_{\\lambda}\\mathcal{L}_{0}(v^{\\prime},\\lambda)\\|_{2}~=~\\|\\mathbf{J}(v)-\\mathbf{J}(v^{\\prime})\\|_{2}$ . Equation (5) is fulfilled when the objective function $J_{0}$ and the constraint functions $J_{i}$ are smooth w.r.t. $\\pmb{v}$ and the Lagrange multipliers are bounded (guaranteed thanks to the projection $\\Pi_{\\Lambda_{-}}$ ), since $\\|\\nabla_{v}\\mathcal{L}_{0}(v,\\lambda)-\\bar{\\nabla}_{v}\\bar{\\mathcal{L}}_{0}(v^{\\prime},\\lambda)\\|_{2}\\leqslant$ $\\begin{array}{r}{|\\nabla_{v}J_{0}(v)-\\nabla_{v}J_{0}(v^{\\prime})|+\\sum_{i=1}^{U}\\lambda_{i}|\\nabla_{v}J_{i}(v)-\\nabla_{v}J_{i}(v)|}\\end{array}$ . Finally, Equation (6) is fulfliled whenever functions $J_{i}$ admit bounded \u0159gradients, since $\\|\\nabla_{v}\\mathcal{L}_{0}(v,\\lambda)-\\nabla_{v}\\mathcal{L}_{0}(v,\\lambda^{\\prime})\\|_{2}\\leqslant\\|\\nabla_{v}\\mathbf{J}(v)(\\lambda\\!-\\!\\lambda^{\\prime})\\|_{2}$ It is worth noting that $L_{2}$ depends on the norm of the Lagrange multipliers and, consequently, due to the projection operator, we have that $L_{2}=O(\\omega^{-1})$ , whereas $L_{1}$ and $L_{3}$ are independent on $\\omega$ .3 Explicit conditions on the constitutive elements of the MDP and (hyper)policies to ensure Lipshitzness and smoothness of these quantities are reported in (Montenegro et al., 2024, Appendix E) for both the AB and PB cases. These regularity properties enforced on $\\mathcal{L}_{\\omega}$ are inherited by the primal function $H_{\\omega}$ which results to be $\\left(L_{2}\\bar{+}\\,L_{1}^{2}\\dot{\\omega}^{-1}\\right)$ -smooth (Lemma E.7). Concerning the regularity of $\\mathcal{L}_{\\omega}$ w.r.t. $\\lambda$ , we observe that it is \\`a quadratic fu\u02d8nction and, therefore, it is $\\omega$ -smooth and satisfies the PL condition, i.e., Assumption 3.2 with $\\psi=2$ , $\\beta_{1}=0$ , and with $\\alpha_{1}=\\omega$ (Lemma E.5). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Assumption 3.4 (Bounded Estimator Variance). For every $\\textbf{\\em v}\\in\\ V$ and $\\pmb{\\lambda}\\in\\Lambda$ , the estimators $\\hat{\\nabla}_{v}\\mathcal{L}_{\\omega}(\\bar{v},\\lambda)$ and $\\hat{\\nabla}_{\\lambda}\\mathcal{L}_{\\omega}(\\boldsymbol{v},\\lambda)$ are unbiased for $\\begin{array}{r}{\\nabla_{v}\\mathcal{L}_{\\omega}(v,\\lambda)=\\nabla_{v}J_{0}(v)+\\sum_{i=1}^{U}\\lambda_{i}\\nabla_{v}J_{i}(v)}\\end{array}$ and $\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(v,\\lambda)=\\mathbf{J}(v)-\\mathbf{b}-\\omega\\lambda$ with bounded variance, i.e., there exist $V_{v},V_{\\lambda}<+\\infty$ such that: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{\\mathbb{V}a r}[\\widehat{\\nabla}_{v}\\mathcal{L}_{\\omega}(v,\\lambda)]\\leqslant V_{v},\\qquad\\mathrm{\\mathbb{V}a r}[\\widehat{\\nabla}_{\\lambda}\\mathcal{L}_{\\omega}(v,\\lambda)]\\leqslant V_{\\lambda}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that $V_{v}$ typically depends on the Lagrange multipliers and, for standard sample mean estimators, it is of order $\\bar{V_{v}}=\\dot{O}(\\omega^{-2})$ thanks to the projection operator. Contrary, $V_{\\lambda}$ is usually not affected by $\\omega$ since the term $\\omega\\lambda$ is not estimated and, thus, it does not impact on the variance of the sample mean estimator. In Section 4, explicit estimators are provided for both the AB and PB cases. The variance of such estimators can be easily controlled by leveraging on the properties of the score function as done in previous works (see Papini et al. 2022 and Montenegro et al. 2024, Appendix E). ", "page_idx": 5}, {"type": "text", "text": "3.3 Convergence Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We are now ready to attack the convergence analysis of C-PG to the global optimum of the COP of Equation (1). To this end, we study the potential function defined as ${\\mathcal P}_{k}(\\chi):=\\,a_{k}\\,+\\,\\chi b_{k}$ , where $\\mathring{a_{k}}:=\\mathbb{E}[H_{\\omega}(v_{k})\\,-\\,H_{\\omega}^{*}]$ and $b_{k}\\,:=\\,\\mathbb{E}[H_{\\omega}(\\pmb{v}_{k})-\\mathcal{L}_{\\omega}(\\pmb{v}_{k},\\pmb{\\lambda}_{k})]$ and $\\chi\\,\\in\\,(0,1)$ will be specified later. Since $a_{k},b_{k}\\geqslant0$ , intuitively, if $\\mathcal{P}_{k}(\\boldsymbol{\\chi})\\approx0$ we have that both $a_{k},b_{k}\\approx0$ and, consequently, convergence is achieved. Let us start relating $\\mathcal{P}_{k}(\\chi)$ , with the solution of the COP in Equation (1). ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (Objective Function Gap and Constraint Violation). Let $\\epsilon>0$ . Under Assumption 3.1, $i f\\mathcal{P}_{k}\\leqslant\\epsilon$ , it holds that: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[J_{0}(v_{k})-J_{0}(v_{0}^{*})]\\leqslant\\epsilon+\\frac{\\omega}{2}\\|\\lambda_{0}^{*}\\|_{2}^{2},\\qquad\\mathbb{E}[(J_{i}(v_{k})-b_{i})^{+}]\\leqslant4\\epsilon+\\omega\\|\\lambda_{0}^{*}\\|_{2},\\quad\\forall i\\in\\mathbb{I}[U].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 justifies the study of the potential $\\mathcal{P}_{k}$ as a technical tool to ensure convergence. Indeed, whenever $\\mathcal{P}_{k}\\leqslant\\epsilon$ both the $(i)$ objective function gap and $(i i)$ the constraint violation scale linearly with $\\epsilon$ and with the regularization parameter $\\omega$ of the regularized Lagrangian $\\mathcal{L}_{\\omega}$ multiplied by the norm of the Lagrange multipliers of the non-regularized problem $\\lVert\\lambda_{0}^{*}\\rVert_{2}$ , which are finite under Assumption 3.1. This expression also suggests a choice of $\\omega=\\mathcal{O}(\\epsilon)$ to enforce an overall $\\epsilon$ error on both quantities. Note that, from Theorem 3.1, it is immediate to employ a conservative constraint $(b_{i}^{\\prime}\\approx\\bar{b}_{i}-4\\epsilon-\\omega\\|\\boldsymbol{\\lambda}_{0}^{*}\\|_{2}^{2})$ to achieve zero constraint violation with no modification of the algorithm. ", "page_idx": 5}, {"type": "text", "text": "We are now ready to state the convergence guarantees for the potential function. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Convergence of $\\mathcal{P}_{K}$ ). Under Assumptions 3.2, 3.3, 3.4, for $\\chi<1/5,$ , sufficiently small $\\epsilon$ and $\\omega$ , and a choice of constant learning rates $\\zeta_{v},\\zeta_{\\lambda}$ , we have $\\mathcal{P}_{K}(\\chi)\\leqslant\\epsilon+\\beta_{1}/\\alpha_{1}$ whenever:4 \u2022 $K=\\mathcal{O}(\\omega^{-1}\\log(\\epsilon^{-1}))$ if $\\psi=2$ and the gradients are exact (i.e., $V_{v}=V_{\\lambda}=0,$ ); \u2022 $K=\\mathcal{O}(\\omega^{-1}\\epsilon^{-\\frac{2}{\\psi}-1})\\,i f\\psi\\in[1,2)$ and the gradients are exact (i.e., $V_{v}=V_{\\lambda}=0,$ ); \u2022 $K\\;=\\;\\mathcal{O}(\\omega^{-3}\\epsilon^{-\\frac{4}{\\psi}+1})$ if $\\dot{\\textit{\\psi}}\\in\\,[1,2]$ and the gradients are estimated (i.e., $V_{v}\\ =\\ \\mathcal{O}(\\omega^{-2})$ and $V_{\\lambda}=\\mathcal{O}(1),$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Some comments are in order. First, Theorem 3.2 holds for a specific choice of the constant $\\chi\\in$ $(0,1/5)$ defining the potential function $\\mathcal{P}_{K}$ . Second, the presented rates hold for sufficiently small values of $\\epsilon$ and $\\omega$ . This is just for presentation purposes, as the sample complexity5 can only improve if we increase the values of $\\epsilon$ and $\\omega$ . Third, in the proof, an explicit expression of the learning rates is provided. Concerning their orders, for the case of exact gradients, we choose $\\zeta_{\\lambda}=\\omega^{-1}$ and $\\zeta_{v}=$ $\\underline{{\\mathcal{O}(\\omega)}}$ , whereas for the estimated gradient case, we choose $\\zeta_{\\lambda}={\\mathcal O}(\\omega\\epsilon^{2/\\psi})$ and $\\zeta_{\\pmb{v}}=\\mathcal{O}(\\omega^{3}\\epsilon^{2/\\psi})$ ", "page_idx": 5}, {"type": "table", "img_path": "2vywag2lVC/tmp/fcd0a1a92d546382e8c7e8675b5573c2e2c1fda8842eeec682eb0b4600adf135.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of the sample complexity results of C-PG when either keeping $\\omega$ fixed or setting it as $\\omega=\\mathcal{O}(\\epsilon)$ . "], "page_idx": 6}, {"type": "text", "text": "Assuming $\\omega$ to be a constant, we observe that both learning rates display the same dependence on $\\epsilon$ and, consequently, they are in single-time scale. However, as we have seen in Theorem 3.1, in order to obtain guarantees on the original non-regularized problem, we have to set $\\omega=\\mathcal{O}(\\epsilon)$ , leading to a two-time scales algorithm. Fourth, we observe that, for both exact and estimated gradients, the sample complexity degrades as the constant $\\psi$ of the gradient domination moves from 2 to 1, delivering the smallest sample complexity when the PL condition holds. Finally, we highlight that C-PG jointly: $(i)$ converges to the global optimum of the COP problem of Equation (1); (ii) delivers a last-iterate guarantee; $(i i i)$ has no dependence on the cardinality of the state or action spaces, making it completely dimension-free. Table 1 and Figure 1 summarize the results of Theorem 3.2. ", "page_idx": 6}, {"type": "image", "img_path": "2vywag2lVC/tmp/4ccd2899e6eab3a66aeb3ee464471d8bb66b4480eaf4106ec4306090ebd841b0.jpg", "img_caption": ["Figure 1: Plot of the exponents of $\\epsilon^{-\\breve{1}}$ in the cases of Table 1. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Action-based and Parameter-based Primal-Dual Algorithms ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we introduce C-PGAE and C-PGPE, the action-based and the parameter-based versions of C-PG, respectively. Both the algorithms have been designed to tackle continuous risk-constrained optimization problems (RCOP), generalizing the COP of Equation (1) and thus extending the applicability space of C-PG. This is done by employing a parametric unified risk measure formulation, which leads to having a framework for solving risk-constrained problems via policy-based primaldual methods operating in both the action-based and parameter-based PGs exploration scenarios. Moreover, this framework allows to handle constraints enforced on several risk measures, which will be discussed below. We first introduce a parametric unified risk measure and formulate an exploration-agnostic RCOP (Section 4.1). Then, we present both C-PGAE and C-PGPE (Section 4.2). ", "page_idx": 6}, {"type": "text", "text": "4.1 Risk-Constrained Optimization Problem ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We start presenting the notion of unified risk measure originally introduced by Bisi et al. (2022). For the AB case, to evaluate the performance of policy $\\pi_{\\theta}$ w.r.t. the $i$ -th cost, with $i\\in[0,U]$ , given two functions $f_{i}:\\mathbb{R}^{2}\\to\\mathbb{R}$ and $g_{i}:\\mathbb{R}\\to\\mathbb{R}$ , we define the $A B$ -risk measure as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{J}_{\\mathrm{A},i}(\\pmb{\\theta},\\eta_{i}):=\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot|\\pmb{\\theta})}{\\mathbb{E}}\\left[f_{i}(C_{i}(\\tau),\\eta_{i})\\right]+g_{i}(\\eta_{i}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similarly, for the PB case, to assess the performance of hyperpolicy $\\nu_{\\rho}$ w.r.t. the $i$ -th cost, with $i\\in[0,U]$ , we define the $P B$ -risk measure as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\eta_{i}\\in\\mathbb{R}}\\mathcal{I}_{\\mathrm{P},i}(\\rho,\\eta_{i})\\quad\\mathrm{where}\\quad\\mathcal{I}_{\\mathrm{P},i}(\\rho,\\eta_{i}):=\\operatorname*{l}_{\\theta\\sim\\nu_{\\rho}}\\lbrack\\operatorname*{l}_{\\tau\\sim p_{\\mathrm{A}}(\\cdot|\\theta)}\\lbrack f_{i}(C_{i}(\\tau),\\eta_{i})]\\rbrack+g_{i}(\\eta_{i}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Some observations are in order. First, by selecting the functions $f_{i}$ and $g_{i}$ , we generate different risk measures (Table 2). Details on the presented risk measures and on the mappings can be found in Appendix C. Second, the risk measure itself is defined as another minimization problem over the additional real variable $\\eta_{i}$ . In principle, if we replace the constraints on cost expectation of the COP in Equation (1) with the risk measures presented above, we are in the presence of a bilevel optimization problem. However, it is immediate to realize that we can merge variables $\\eta_{i}$ with the primal variables $\\pmb{v}$ of the COP without changing the optimum. Finally, let us appreciate the semantic difference between enforcing risk-based constraints in the AB and PB cases. Indeed, while for AB the stochasticity inducing the risk is the one generated by the policy $\\pi_{\\theta}$ and the environment, for PB we have the joint stochastic process of the hyperpolicy $\\nu_{\\rho}$ , policy $\\pi_{\\theta}$ (when stochastic), and the environment. To the best of our knowledge, this paper is the first proposing to enforce risk-based constraints for parameter-based exploration. ", "page_idx": 6}, {"type": "table", "img_path": "2vywag2lVC/tmp/49f101dcb37b91784cc4c49c82f2edb6abb3f3d8c8150e89082bc251a3024eed.jpg", "table_caption": ["Table 2: Mapping of the unified risk measure to risk measures. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Having introduced the notions of AB and PB unified risk measures, we can formulate a riskconstrained optimization problem (RCOP) agnostic w.r.t. the exploration paradigm and the risk measure:6 ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{v\\in\\mathcal{V},\\eta\\in\\mathbb{R}^{U}}}\\mathcal{I}_{\\dagger,0}(v,\\eta_{0})\\quad\\mathrm{s.t.}\\quad\\mathcal{I}_{\\dagger,i}(v,\\eta_{i})\\leqslant b_{i},\\ \\ \\forall i\\in\\ensuremath{[\\![U]\\!]},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\dagger\\in\\{\\mathrm{A},\\mathrm{P}\\}$ , $\\boldsymbol{v}\\in\\mathcal{V}$ is the parameter and $\\pmb{\\eta}=(\\eta_{1},\\dots,\\eta_{U})\\in\\mathbb{R}^{U}$ are the auxiliary variables. ", "page_idx": 7}, {"type": "text", "text": "As for the COP of Equation (1), we define the regularized Lagrangian function for the RCOP (Equation 11) as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}_{\\uparrow,\\omega}(v,\\lambda,\\eta):=\\mathcal{I}_{\\dag,0}(v,\\eta_{0})+\\sum_{u=1}^{U}\\lambda_{u}\\left(\\mathcal{I}_{\\dag,u}(v,\\eta_{u})-b_{u}\\right)-\\frac{\\omega}{2}\\left\\|\\lambda\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with $\\dagger\\in\\{\\mathrm{A},\\mathrm{P}\\}$ . We assume that $\\Tilde{\\mathcal{L}}_{\\dagger,\\omega}$ is differentiable w.r.t. $\\pmb{v}$ and subdifferentiable7 w.r.t. $\\eta$ , while, by construction, it is already differrentiable w.r.t. $\\lambda$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 4.1 (Do the Convergence Guarantees of Section 3 Apply to the RCOP?). When all the $f_{i}$ and $g_{i}$ are selected in order to consider the expected values of the corresponding $C_{i}$ (i.e., $f_{i}(C_{i}(\\tau),\\eta_{i})=$ $C_{i}(\\tau)$ and $g_{i}(\\eta_{i})\\,=\\,0,$ ), then $\\Tilde{\\mathcal{L}}_{\\dagger,\\omega}$ coincides with $\\mathcal{L}_{\\omega}$ , thus, all the theoretical results presented in Section 3 hold. This is not tru er in general. For some risk measures $\\Tilde{\\mathcal{L}}_{\\dagger,\\omega}$ may not be smooth or even differentiable in \u03c5 or \u03b7, or it may violate the weak gradient dominartion assumption. Although out of the scope of the present paper, studying the preservation of the (weak) gradient domination for risk-based objectives is an appealing future research direction. ", "page_idx": 7}, {"type": "text", "text": "4.2 Algorithms: C-PGAE and C-PGPE ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Algorithms. Both algorithms, whose pseudo-codes are deferred to Appendix A, aim at solving the RCOP of Equation (11), finding the best feasible (hyper)policy parameterization. The alternate ascent/descent primal-dual prototypical algorithm is the same as described in Section 3. In particular, at each iteration $k\\in[\\![K]\\!]$ , the algorithms collect $N$ trajectories (employing the chose exploration paradigm) to update  t he p rimal and dual variables as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{{\\bf~rrmat~spaares}}^{*}}&{\\quad}&{\\mathrm{{\\bf~Duat~spaare}}}\\\\ {v_{k+1}\\leftarrow v_{k}-\\zeta_{v,k}\\hat{\\nabla}_{v}\\tilde{\\mathcal{L}}_{\\uparrow,\\omega}(v_{k},\\lambda_{k},\\eta_{k})}&{\\quad}&{\\lambda_{k+1}\\leftarrow\\lambda_{k}+\\zeta_{\\lambda,k}\\hat{\\nabla}_{\\lambda}\\tilde{\\mathcal{L}}_{\\uparrow,\\omega}(v_{k+1},\\lambda_{k},\\eta_{k+1}),}\\\\ {\\eta_{k+1}\\leftarrow\\eta_{k}-\\zeta_{\\eta,k}\\hat{\\nabla}_{\\eta}\\tilde{\\mathcal{L}}_{\\uparrow,\\omega}(v_{k},\\lambda_{k},\\eta_{k})}&{\\quad}&{\\lambda_{k+1}\\leftarrow\\lambda_{k}+\\zeta_{\\lambda,k}\\hat{\\nabla}_{\\lambda}\\tilde{\\mathcal{L}}_{\\uparrow,\\omega}(v_{k+1},\\lambda_{k},\\eta_{k+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the values are c ons idered to be projected into their spaces, and $\\zeta_{\\pmb{v},k},\\zeta_{\\pmb{\\lambda},k},\\zeta_{\\pmb{\\eta},k}>0$ are the learning rates at the $k$ -th iteration for $v,\\lambda$ , and $\\eta$ , respectively. Notice that, since we use alternate ascent/descent, the update for $\\lambda_{k+1}$ uses the updated primal values $\\pmb{v}_{k+1}$ and $\\eta_{k+1}$ . In practice, this requires collecting a first batch of $N/2$ trajectories using the (hyper)policy $\\pmb{v}_{k}$ to perform the primal updates, computing $\\pmb{v}_{k+1}$ and $\\eta_{k+1}$ . Then, it is necessary to collect a new batch of $N/2$ trajectories using the updated (hyper)policy $\\pmb{v}_{k+1}$ in order to perform the dual update, computing $\\lambda_{k+1}$ , which also leverages $\\eta_{k+1}$ . ", "page_idx": 7}, {"type": "text", "text": "Estimators. Here, we provide the risk-agnostic form of the estimators used to perform the primal and dual updates. In particular, the gradient of $\\Tilde{\\mathcal{L}}_{\\dagger,\\omega}$ with respect to $\\eta$ is closely related to the choices of $f_{i}$ and $g_{i}$ . Therefore, the discussion is deferrerd to Appendix D, where we also explicitly derive all the estimators for the risk measures presented in Table 2 and for both exploration paradigms. The estimator of the gradient of $\\Tilde{\\mathcal{L}}_{\\dagger,\\omega}$ w.r.t. $\\lambda$ has the same form for both C-PGAE and C-PGPE, that is: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widehat{\\nabla}_{\\lambda}\\tilde{\\mathcal{L}}_{\\uparrow,\\omega}(v,\\lambda,\\eta)=\\frac{1}{N}\\sum_{i=1}^{N}\\left(f(C(\\tau_{i}),\\eta_{k+1})-g(\\eta_{k+1})\\right)-b-\\omega\\lambda_{k},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the trajectories are collected with the AB or the PB exploration paradigm , $f(C(\\tau),\\boldsymbol{\\eta}):=$ $\\left(f_{1}\\left(C_{1}(\\tau),\\eta_{1}\\right),\\dots,f_{U}\\left(C_{U}(\\tau),\\eta_{U}\\right)\\right)^{\\top}$ and $g(\\eta):=\\left(g_{1}(\\eta_{1}),\\dots,g_{U}(\\eta_{U})\\right)^{\\top}$ . ", "page_idx": 8}, {"type": "text", "text": "C-PGAE (Algorithm 1) is the action-based version of C-PG. At each iteration $k\\in[\\![K]\\!]$ , the agent collects $N$ trajectories by playing the policy $\\pi_{\\pmb{\\theta}_{k}}$ , then it alternatively updates the pol i cy p arameter $\\pmb{\\theta}$ and the risks parameter $\\eta$ , or the Lagrange multipliers $\\lambda$ . The $\\pmb{\\theta}$ update relies on the estimator: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\gamma}_{\\theta}\\tilde{\\mathcal{L}}_{\\mathrm{A},\\omega}(\\theta_{k},\\!\\lambda_{k},\\!\\eta_{k})\\!:=\\!\\frac{1}{N}\\!\\sum_{i=1}^{N}\\!\\sum_{t=0}^{T-1}\\!\\nabla_{\\theta}\\mathrm{log}\\pi_{\\theta_{k}}(a_{\\tau_{i},t}|s_{\\tau_{i},t})\\right)\\!\\Big(f_{0}(C_{0}(\\tau_{i}),\\!\\eta_{k,0})+\\sum_{u=1}^{U}\\!\\lambda_{k,u}(f_{u}(C_{u}(\\tau_{i}),\\!\\eta_{k,u}))\\Big),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which reduces to the prototypical action-based algorithm REINFORCE (Williams, 1992) in the risk-neutral case. When allowed by the choice of $f_{i}$ , we switch to a GPOMDP-style estimator (Baxter and Bartlett, 2001) which suffers less variance. See Appendix $\\mathrm{D}$ for details. ", "page_idx": 8}, {"type": "text", "text": "C-PGPE (Algorithm 2) is the parameter-based version of C-PG. At each iteration $k\\,\\in\\,[\\![K]\\!]$ , the algorithm samples $N$ parameter configurations $\\{\\pmb\\theta_{i}\\}_{i\\in[N]}$ from the hyperpolicy $\\nu_{\\rho_{k}}$ , then  c oll ects a single trajectory $\\tau_{i}$ , obtained by playing the policy $\\pi_{\\pmb{\\theta}_{i}}$ , f or each sampled parameterization $\\theta_{i}$ . The sampled trajectories and parameters are then used to alternatively update the hyperpolicy parameter vector $\\rho$ and the risks parameter vector $\\eta$ , or the vector of Lagrange multipliers $\\lambda$ . In particular, the $\\rho$ update relies on the following estimator: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widehat{\\nabla}_{\\rho}\\widetilde{\\mathcal{L}}_{\\mathrm{P},\\omega}(\\rho_{k},\\lambda_{k},\\eta_{k}):=\\frac{1}{N}\\sum_{i=1}^{N}\\nabla_{\\rho}\\log\\nu_{\\rho_{k}}(\\theta_{i})\\Big(f_{0}\\left(C_{0}(\\tau_{i}),\\eta_{k,0}\\right)+\\sum_{u=1}^{U}\\lambda_{u}f_{u}\\left(C_{u}(\\tau_{i}),\\eta_{k,u}\\right)\\Big),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which reduces to the parameter-based algorithm PGPE (Sehnke et al., 2010) in the risk-neutral case. ", "page_idx": 8}, {"type": "text", "text": "5 Numerical Validation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we empirically validate some of the theoretical results shown throughout this work. Experimental details and additional results are provided in Appendix H. The code to run the experiments in this paper is available at https://github.com/MontenegroAlessandro/MagicRL. ", "page_idx": 8}, {"type": "text", "text": "Comparison in DGWW. We compare our C-PGAE against the sample-based versions of NPGPD (Ding et al., 2020, Appendix H) and RPG-PD (Ding et al., 2024, Appendix C.9) on a Discrete Grid World with Walls (DGWW, see Appendix $\\mathrm{H}$ ) with a horizon of $T=100$ , and with a single constraint on the average trajectory cost. The methods are learning the parameters of a tabular softmax policy and the learning phase considers constant step sizes. Figure 2a shows the average return and the average cost curves over the trajectories seen during the learning. As can be noticed, C-PGAE strikes the objective of the COP with fewer trajectories w.r.t. the competitors. Indeed, both NPG-PD and RPG-PD require additional $\\mathcal{O}(|S|+|S||\\bar{\\mathcal{A}}|)$ trajectories per iteration. ", "page_idx": 8}, {"type": "text", "text": "Comparison in LQR. We compare our C-PGAE and C-PGPE against the continuous sample-based version of NPG-PD2 (Ding et al., 2022, Algorithm 1), working with generic policy parameterizations. Additionally, we consider RPG-PD2, a ridge-regularized version of NPG-PD2. The environment is a bidimensional CostLQR (Appendix H) with a horizon $T=50$ and a single cost that the algorithms should keep below $b=0.2$ on average. C-PGAE, NPG-PD2, and RPG-PD2 learn the parameter of a linear gaussian policy, while C-PGPE the ones of a gaussian hyperpolicy over a linear deterministic policy. All the step sizes are chosen with Adam (Kingma and Ba, 2015) scheduler. Figure 2b reports the learning curves for the average return and the cost, confirming that our methods solve the COP with fewer trajectories. Indeed, being both NPG-PD2 and RPG-PD2 actor-critic methods, they suffer from the inner critic loop, which requires the collection of additional trajectories (500 here). We stress that the actor-critic methods were very sentitive to the hyperparameters selection, especially the length and the step size of the inner loop (see Appendix H). ", "page_idx": 8}, {"type": "image", "img_path": "2vywag2lVC/tmp/82a5b5f5395e3880636a77c4466c7f669fe1265f9c9a565f8ffdc9153432665b.jpg", "img_caption": ["Figure 2: Average return and cost in CostLQR and DGWW environments (5 runs, mean $\\pm95\\%$ C.I.). "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "2vywag2lVC/tmp/7ab14bc4242996f0e85b7aa099f91412c1139a65f3b609ab69bac7e037842720.jpg", "img_caption": ["Figure 3: Cost distributions with (hyper)policies learned considering different risk measures (5 runs). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Risk constraints on Swimmer. In Figure 3, we show the empirical distributions of costs over 100 trajectories of the learned (hyper)policies via C-PGPE and C-PGAE. This experiment considers the cost-based version of the Swimmer- $\\cdot\\nu4$ MuJoCo (Todorov et al., 2012) environment, with a single constraint over the actions (see Appendix H), for which we set $b=50$ . The experimental results show that C-PGPE learns a hyperpolicy paying less cost when using risk measures compared to average cost, with the smallest costs attained by $\\mathrm{CVaR}_{\\alpha}$ . C-PGAE shows similar results, although the difference between $\\mathrm{CVaR}_{\\alpha}\\mathbf{or}$ the Chance constraints and average cost constraints are not very significant. Notice that, the minimum amount of cost is obtained using MV constraints even if the learned policy exhibits poor performances (Table 3c). In all the other cases, both C-PGPE and C-PGAE learns (hyper)policies exhibiting similar performance scores. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we proposed a general framework to address continuous CRL problems via primal-dual policy-based algorithms, leveraging on an alternate ascent-descent approach. Our explorationagnostic proposal C-PG exhibits dimension-free global last-iterate convergence guarantees, under the standard (weak) gradient domination assumption. Furthermore, we introduced C-PGAE and C-PGPE, the action and parameter-based versions of C-PG which enable embedding risk-based constraints, enlarging the capabilities of our framework in addressing constrained real-world problems. Future works should focus on matching the sample complexity lower bound prescribed by (Vaswani et al., 2022) and devising algorithms with the same rates of C-PG with a single time-scale. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors acknowledge the project FAIR that has been funded by the European Union \u2013 Next Generation EU within the project NRPP M4C2, Investment 1.3 DD. 341 - 15 March 2022 \u2013 FAIR \u2013 Future Artificial Intelligence Research \u2013 Spoke 4 - PE00000013 - D53C22002380006. The authors acknowledge the CINECA award under the ISCRA initiative, for the availability of high-performance computing resources and support. The authors acknowledge AI4REALNET that has received funding from European Union\u2019s Horizon Europe Research and Innovation programme under the Grant Agreement No 101119527. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. MIT Press, 2018. ", "page_idx": 10}, {"type": "text", "text": "Amarildo Likmeta, Alberto Maria Metelli, Andrea Tirinzoni, Riccardo Giol, Marcello Restelli, and Danilo Romano. Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving. Robotics Auton. Syst., 131:103568, 2020. ", "page_idx": 10}, {"type": "text", "text": "Eiji Uchibe and Kenji Doya. Constrained reinforcement learning from intrinsic and extrinsic rewards. In IEEE International Conference on Development and Learning, pages 163\u2013168. IEEE, 2007. ", "page_idx": 10}, {"type": "text", "text": "Eitan Altman. Constrained Markov Decision Processes. CRC Press, 1999. ", "page_idx": 10}, {"type": "text", "text": "Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by PID lagrangian methods. In Proceedings of the International Conference on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pages 9133\u20139143. PMLR, 2020.   \nDongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo Jovanovic. Natural policy gradient primal-dual method for constrained markov decision processes. Advances in Neural Information Processing Systems (NeurIPS), 33:8378\u20138390, 2020.   \nDonghao Ying, Yuhao Ding, and Javad Lavaei. A dual approach to constrained markov decision processes with entropy regularization. In International Conference on Artificial Intelligence and Statistics (AISTATS), volume 151 of Proceedings of Machine Learning Research, pages 1887\u20131909. PMLR, 2022.   \nDongsheng Ding, Chen-Yu Wei, Kaiqing Zhang, and Alejandro Ribeiro. Last-iterate convergent policy gradient primal-dual methods for constrained mdps. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024.   \nYinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning Research, 18(1): 6070\u20136120, 2017.   \nR Tyrrell Rockafellar and Stanislav Uryasev. Optimization of conditional value-at-risk. Journal of risk, 2:21\u201342, 2000.   \nHarry M Markowitz and G Peter Todd. Mean-variance analysis in portfolio choice and capital markets, volume 66. John Wiley & Sons, 2000.   \nDuan Li and Wan-Lung Ng. Optimal dynamic portfolio selection: Multiperiod mean-variance formulation. Mathematical Finance, 10(3):387\u2013406, 2000.   \nMarc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics. Foundations and Trends in Robotics, 2(1-2):1\u2013142, 2013.   \nJan Peters and Stefan Schaal. Policy gradient methods for robotics. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 2219\u20132225. IEEE, 2006.   \nBenjamin Gravell, Peyman Mohajerin Esfahani, and Tyler Summers. Learning optimal controllers for linear systems with multiplicative noise via policy gradient. IEEE Transactions on Automatic Control, 66(11):5283\u20135298, 2020.   \nKamyar Azizzadenesheli, Yisong Yue, and Animashree Anandkumar. Policy gradient in partially observable environments: Approximation and convergence. arXiv preprint arXiv:1810.07900, 2018.   \nMohammad Ghavamzadeh and Yaakov Engel. Bayesian policy gradient algorithms. Advances in Neural Information Processing Systems (NeurIPS), 19, 2006.   \nAlessandro Montenegro, Marco Mussi, Alberto Maria Metelli, and Matteo Papini. Learning optimal deterministic policies with stochastic policy gradients. In Proceedings of the International Conference on Machine Learning (ICML). PMLR, 2024.   \nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229\u2013256, 1992.   \nJonathan Baxter and Peter L. Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, 15:319\u2013350, 2001.   \nFrank Sehnke, Christian Osendorfer, Thomas R\u00fcckstie\u00df, Alex Graves, Jan Peters, and J\u00fcrgen Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551\u2013559, 2010.   \nJoshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Proceedings of the International Conference on Machine Learning (ICML), volume 70 of Proceedings of Machine Learning Research, pages 22\u201331. PMLR, 2017.   \nChen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In International Conference on Learning Representations (ICLR), 2019.   \nDongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo R. Jovanovic. Provably efficient safe exploration via primal-dual policy optimization. In International Conference on Artificial Intelligence and Statistics (AISTATS), volume 130 of Proceedings of Machine Learning Research, pages 3304\u20133312. PMLR, 2021.   \nQinbo Bai, Amrit Singh Bedi, Mridul Agarwal, Alec Koppel, and Vaneet Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach. In AAAI Conference on Artificial Intelligence, pages 3682\u20133689. AAAI Press, 2022.   \nQinbo Bai, Amrit Singh Bedi, and Vaneet Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm. In AAAI Conference on Artificial Intelligence, pages 6737\u20136744. AAAI Press, 2023.   \nEgor Gladin, Maksim Lavrik-Karmazin, Karina Zainullina, Varvara Rudenko, Alexander V. Gasnikov, and Martin Tak\u00e1c. Algorithm for constrained markov decision process with linear convergence. In International Conference on Artificial Intelligence and Statistics (AISTATS), volume 206 of Proceedings of Machine Learning Research, pages 11506\u201311533. PMLR, 2023.   \nTao Liu, Ruida Zhou, Dileep Kalathil, PR Kumar, and Chao Tian. Policy optimization for constrained mdps with provable fast global convergence. arXiv preprint arXiv:2111.00552, 2021.   \nDongsheng Ding, Kaiqing Zhang, Jiali Duan, Tamer Basar, and Mihailo R. Jovanovic. Convergence and sample complexity of natural policy gradient primal-dual methods for constrained mdps. CoRR, abs/2206.02346, 2022.   \nDimitri P Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press, 2014.   \nJunchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 1153\u20131165, 2020.   \nSantiago Paternain, Luiz F. O. Chamon, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained reinforcement learning has zero duality gap. In Advances in Neural Information Processing Systems (NeurIPS), pages 7553\u20137563, 2019.   \nRui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gradient. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 3332\u20133380. PMLR, 2022.   \nSaeed Masiha, Saber Salehkaleybar, Niao He, Negar Kiyavash, and Patrick Thiran. Stochastic second-order methods improve best-known sample complexity of sgd for gradient-dominated functions. Advances in Neural Information Processing Systems (NeurIPS), 35:10862\u201310875, 2022.   \nIlyas Fatkhullin, Anas Barakat, Anastasia Kireeva, and Niao He. Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies. In Proceedings of the International Conference on Machine Learning (ICML), volume 202 of Proceedings of Machine Learning Research, pages 9827\u20139869. PMLR, 2023.   \nJalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. Operations Research, 2024.   \nJincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. In Proceedings of the International Conference on Machine Learning (ICML), pages 6820\u20136829. PMLR, 2020.   \nMatteo Papini, Matteo Pirotta, and Marcello Restelli. Smoothing policies and safe policy gradients. Machine Learning, 111(11):4081\u20134137, 2022.   \nLorenzo Bisi, Davide Santambrogio, Federico Sandrelli, Andrea Tirinzoni, Brian D. Ziebart, and Marcello Restelli. Risk-averse policy optimization via risk-neutral policy optimization. Artif. Intell., 311:103765, 2022.   \nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.   \nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5026\u20135033. IEEE, 2012.   \nSharan Vaswani, Lin Yang, and Csaba Szepesv\u00e1ri. Near-optimal sample complexity bounds for constrained mdps. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \nGal Dalal, Krishnamurthy Dvijotham, Matej Vecer\u00edk, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe exploration in continuous action spaces. CoRR, abs/1801.08757, 2018.   \nYinlam Chow, Ofir Nachum, Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n, and Mohammad Ghavamzadeh. A lyapunovbased approach to safe reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 8103\u20138112, 2018.   \nMing Yu, Zhuoran Yang, Mladen Kolar, and Zhaoran Wang. Convergent policy optimization for safe reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 3121\u20133133, 2019.   \nYongshuai Liu, Jiaxin Ding, and Xin Liu. IPO: interior-point policy optimization under constraints. In AAAI Conference on Artificial Intelligence, pages 4940\u20134947. AAAI Press, 2020.   \nTengyu Xu, Yingbin Liang, and Guanghui Lan. CRPO: A new approach for safe reinforcement learning with convergence guarantee. In Proceedings of the International Conference on Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pages 11480\u201311491. PMLR, 2021.   \nFelisa J. V\u00e1zquez-Abad, Vikram Krishnamurthy, Katerine Martin, and Irina Baltcheva. Self learning control of constrained markov chains - a gradient approach. In IEEE Conference on Decision and Control (CDC), pages 1940\u20131945. IEEE, 2002. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Shalabh Bhatnagar and K. Lakshmanan. An online actor-critic algorithm with function approximation for constrained markov decision processes. J. Optim. Theory Appl., 153(3):688\u2013708, 2012. ", "page_idx": 13}, {"type": "text", "text": "Tianqi Zheng, Pengcheng You, and Enrique Mallada. Constrained reinforcement learning via dissipative saddle flow dynamics. In Asilomar Conference on Signals, Systems, and Computers (ACSSC), pages 1362\u20131366. IEEE, 2022.   \nTed Moskovitz, Brendan O\u2019Donoghue, Vivek Veeriah, Sebastian Flennerhag, Satinder Singh, and Tom Zahavy. Reload: Reinforcement learning with optimistic ascent-descent for last-iterate convergence in constrained mdps. In Proceedings of the International Conference on Machine Learning (ICML), volume 202 of Proceedings of Machine Learning Research, pages 25303\u201325336. PMLR, 2023.   \nYu-Guan Hsieh, Franck Iutzeler, J\u00e9r\u00f4me Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. In Advances in Neural Information Processing Systems (NeurIPS), pages 6936\u20136946, 2019.   \nYin-Lam Chow and Marco Pavone. Stochastic optimal control with dynamic, time-consistent risk constraints. In American Control Conference (ACC), pages 390\u2013395. IEEE, 2013.   \nVivek S. Borkar and Rahul Jain. Risk-constrained markov decision processes. IEEE Trans. Autom. Control., 59(9):2574\u20132579, 2014.   \nQiyuan Zhang, Shu Leng, Xiaoteng Ma, Qihan Liu, Xueqian Wang, Bin Liang, Yu Liu, and Jun Yang. Cvar-constrained policy optimization for safe reinforcement learning. IEEE Transactions on Neural Networks and Learning Systems, 2024.   \nAviv Tamar, Yonatan Glassner, and Shie Mannor. Policy gradients beyond expectations: Conditional value-at-risk. CoRR, abs/1404.3862, 2014.   \nLorenzo Bisi, Luca Sabbioni, Edoardo Vittori, Matteo Papini, and Marcello Restelli. Risk-averse trust region optimization for reward-volatility reduction. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pages 4583\u20134589. ijcai.org, 2020.   \nYudong Luo, Guiliang Liu, Pascal Poupart, and Yangchen Pan. An alternative to variance: Gini deviation for risk-averse policy gradient. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \nRichard S. Sutton, David A. McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems (NeurIPS), pages 1057\u20131063. The MIT Press, 1999.   \nMaher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving a class of non-convex min-max games using iterative first order methods. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.   \nBrian DO Anderson and John B Moore. Optimal control: linear quadratic methods. Courier Corporation, 2007. ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Pseudo-codes 15 ", "page_idx": 14}, {"type": "text", "text": "B Related Works 16 ", "page_idx": 14}, {"type": "text", "text": "On the Unified Risk Measure 17   \nC.1 Expected Cost and Mean-Variance 18   \nC.2 Chance Constraints, Value at Risk, and Conditional Value at Risk 18   \nC.3 Unified Risk Measure Mapping . . . 18   \nD Estimators 20   \nD.1 Parameter-based Algorithm: C-PGPE 20   \nD.2 Action-based Algorithm: C-PGAE 22 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "E Proofs 24 ", "page_idx": 14}, {"type": "text", "text": "F Technical Lemmas 36 ", "page_idx": 14}, {"type": "text", "text": "G Recurrences 36   \nG.1 Analysis when $b=0$ 37   \nG.2 Analysis for $b>0$ 38   \nH Experimental Details and Additional Results 38   \nH.1 Experimental Details . . . . . 38   \nH.2 Details for the comparison against the baselines in DGWW 40   \nH.3 Details for the comparison against baselines in CostLQR . . 40   \nH.4 Details for the risk-constrained experiment on Swimmer-v4 . . 41   \nH.5 Risk-constrained experiment on Hopper-v4 . . . . . . . . . . . 43   \nH.6 Regularization Sensitivity Study . . . 43   \nH.7 Computational Resources . 47 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Pseudo-codes ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "2vywag2lVC/tmp/ba1b1e24b7ae71113c7f50d3ded0f3b52e932eac41daea09bc651a6dd4af3e33.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Algorithm 2: C-PGPE. Input :Iterations $K$ ; batch size $N$ ; regularization $\\omega$ ; initial parameters: $\\rho_{0},\\lambda_{0}$ , and $\\scriptstyle\\eta_{0}$ ; step sizes: $\\zeta_{\\pmb{\\rho},k},\\zeta_{\\pmb{\\lambda},k},\\zeta_{\\pmb{\\eta},k}$ . 1 for $k\\in[\\![K]\\!]$ do 2 Sa m ple $N$ parameters $\\left\\{\\theta_{i}\\right\\}_{i\\in\\left[N\\right]}$ with $\\nu_{\\rho_{k-1}}$ 3 With each $\\left\\{\\pi_{\\theta_{i}}\\right\\}_{i\\in\\mathbb{I}^{N}\\mathbb{I}}$ collect  a  trajectory $\\tau_{i}$ 4 if $k$ is odd then 5 $\\begin{array}{r}{\\rho_{k}\\gets\\rho_{k-1}-\\zeta_{\\rho,k-1}\\hat{\\nabla}_{\\rho}\\tilde{\\mathcal{L}}_{\\mathrm{P},\\omega}(\\rho_{k-1},\\lambda_{k-1},\\eta_{k-1})}\\end{array}$ 6 \u03b7k \u00d0\u00dd \u03b7k\u00b41 \u00b4 \u03b6\u03b7,k\u00b41 \u2207p\u03b7 LrP,\u03c9p\u03c1k\u00b41, \u03bbk\u00b41, \u03b7k\u00b41q else 8 $\\begin{array}{r l}{\\big|}&{{}\\lambda_{k}\\gets\\lambda_{k-1}+\\zeta_{\\lambda,k-1}\\hat{\\nabla}_{\\lambda}\\tilde{\\mathcal{L}}_{\\mathrm{P},\\omega}(\\rho_{k-1},\\lambda_{k-1},\\eta_{k-1})}\\end{array}$ 9 end 10 end 11 Return \u03c1K ", "page_idx": 14}, {"type": "text", "text": "B Related Works ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Policy Optimization Approaches for Constrained Reinforcement Learning. Policy Optimization-based algorithms for Constrained Reinforcement Learning mostly follow primal-only or primal-dual approaches. Primal-only algorithms (Dalal et al., 2018; Chow et al., 2018; Yu et al., 2019; Liu et al., 2020; Xu et al., 2021) avoid considering dual variables by focusing on the design of the objective function and by designing the update rules for the policy at hand incorporating the constraint satisfaction part. ", "page_idx": 15}, {"type": "text", "text": "The main benefti of employing primal-only algorithms lies in the fact that there is no need to consider another variable to learn, and therefore, no need to tune its learning rate. However, few of the existing methods establish global convergence to an optimal feasible solution. For instance, Xu et al. (2021) propose CRPO, an algorithm employing an unconstrained policy maximization update taking into account the reward when all the constraints are satisfied, while leveraging on-policy minimization updates in the direction of violated constraint functions. Moreover, it exhibits average global convergence guarantees for the tabular setting. On the other hand, primal-dual algorithms (Chow et al., 2017; Achiam et al., 2017; Tessler et al., 2019; Stooke et al., 2020; Ding et al., 2020, 2021; Bai et al., 2022; Ying et al., 2022; Bai et al., 2023; Gladin et al., 2023; Ding et al., 2024) are the most commonly used and investigated. Indeed, the effectiveness of using the primal-dual approach is justified by Paternain et al. (2019), which states that this kind of approach has zero duality gap under Slater\u2019s condition when optimizing over the space of all the possible stochastic policies. Among the reported works, Stooke et al. (2020) propose PID Lagrangian, a method to update the dual variable, smoothing the oscillations around the threshold value of the costs during the learning. The practical strength of such a method is that can be paired with any of the existing policy optimization methods. The other cited works are treated in details in the next paragraph. ", "page_idx": 15}, {"type": "text", "text": "Lagrangian-based Policy Search Convergence Guarantees. A lot of research effort has been spent in studying the convergence guarantees for primal-dual policy optimization methods. In this field, the goal is to ensure last-iterate convergence guarantees showing rates that are dimension-free, i.e., not relying on the state and action spaces\u2019 dimensions, and working with multiple constraints. In the rest of this paragraph, we talk about single time-scale algorithms when the methods at hand prescribe the usage of the same step sizes for both the primal and dual variables\u2019 updates. V\u00e1zquezAbad et al. (2002) and Bhatnagar and Lakshmanan (2012) propose primal-dual policy gradient-based methods built upon distinct time-scales and relying on nested loops. Such methods only show asymptotic convergence guarantees. Chow et al. (2017) propose two primal-dual methods ensuring asymptotic convergence guarantees. The peculiarity of those methods lies in the fact that their notion of CMDP encapsulates risk-based constraints, introducing an additional learning variable. Their algorithms have guarantees of asymptotic convergence to stationary points. The recent works by Zheng et al. (2022) and Moskovitz et al. (2023) also propose methods ensuring asymptotic global convergence guarantees. These methods exploit occupancy-measure iterates rather than policy iterates. Ding et al. (2020) propose NPG-PD, which relies on a natural policy gradient approach and, under Slater\u2019s assumption, ensures dimension-dependent average-iterate global convergence guarantees in the single-constrained setting with a single time-scale and with exact gradients. This work has been extended by Ding et al. (2022), which strikes dimension-free rates, but still guaranteeing just average-rate convergence with exact gradients. However, sample-based versions of NPG-PD showing, under additional assumptions, the same convergence rates are provided by the authors. Another work ensuring an average-iterate rate is the one by Liu et al. (2021). The latter exhibits a convergence rate of order $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ , considering to act in tabular CMDPs with softmax policies and having access to exact graidents and to a generative model. Liu et al. (2021) propose also a sample-based version of their algorithm, keeping the same setting previously described, which ensures a convergence rate on average of order $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ . Both Ying et al. (2022) and Gladin et al. (2023) propose algorithms involving regularization. The proposed methods rely on natural policy-based subroutines and show dimension-dependent last-iterate global convergence guarantees, relying on two time-scales. These methods work also with multiple constraints. Finally, Ding et al. (2024) propose RPG-PD and OPG-PD, exhibiting last-iterate global convergence guarantees under Slater\u2019s condition in a singleconstraint setting. The former is a regularized version of the algorithm proposed by Ding et al. (2020), showing last-iterate global convergence at a sublinear rate. The latter leverages on the optimistic gradient method (Hsieh et al., 2019) to unlock a faster linear convergence rate. These methods show single time-scale dimension-dependent rates and both leverage on exact gradients. However, for ", "page_idx": 15}, {"type": "table", "img_path": "2vywag2lVC/tmp/acc06124b6774f0423080871465b7c3a1b0667b6021499b39a6726221999a10a.jpg", "table_caption": ["Table 3: Comparison among primal-dual methods ensuring last-iterate global convergence guarantees. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "RPG-PD there exists an inexact version showing, under additional assumptions on the statistical and transfer errors and the relative condition number (Ding et al., 2024, Assumption 2), the same guarantees of the exact one. It is worth noticing that all the mentioned works just consider the action-based exploration approach for policy optimization, while the parameter-based one remains unexplored. For the sake of clarity, Table 3 shows a detailed comparison among our approach and the other presented methods exhibiting last-iterate global convergence guarantees. ", "page_idx": 16}, {"type": "text", "text": "Furthermore, Vaswani et al. (2022) have recently proposed a dimension-dependent lower bound for the sample complexity of $\\mathcal{O}\\left(\\epsilon^{-2}\\right)$ , assuming to be under the Slater condition and considering single-constrained CMDPs with \\`finite\u02d8 state and action spaces. ", "page_idx": 16}, {"type": "text", "text": "Risk-Constrained Reinforcement Learning. For safety-critical problems, it is often insufficient to consider constraints solely on expected costs or utilities. Therefore, constraints are sometimes applied to risk measures over cost or utility functions (Chow and Pavone, 2013; Borkar and Jain, 2014; Chow et al., 2017; Zhang et al., 2024). The employed risk measures are those most commonly applied in risk-averse RL (Tamar et al., 2014; Bisi et al., 2020, 2022; Luo et al., 2023). For instance, Tamar et al. (2014) consider the Conditional Value at Risk $\\textstyle(\\mathrm{CVaR}_{\\alpha}$ , Rockafellar and Uryasev, 2000), while Bisi et al. (2022) consider a unified formulation embracing several risk measures, including the $\\mathrm{CVaR}_{\\alpha}$ and the Mean-Variance (MV, Markowitz and Todd, 2000; Li and Ng, 2000). More details on risk measures are discussed in Appendix C. Among the works on Risk-Constrained RL, the one by Chow et al. (2017) considers both $\\mathrm{CVaR}_{\\alpha}$ constraints and chance constraints over cost functions. In particular, the authors propose a trajectory-based policy gradient algorithm and an actor-critic one, both primal-dual methods. For what concerns the $\\mathrm{CVaR}_{\\alpha}$ -constrained problem, the authors incorporate into the constrained optimization problem the real-valued variable associated with the $\\mathrm{CVaR}_{\\alpha}$ . This leads to three variables to learn, requiring algorithms with three time-scales. Finally, the recent work by Zhang et al. (2024) considers an extension of the CPO algorithm (Achiam et al., 2017) imposing constraints over the $\\mathrm{CVaR}_{\\alpha}$ of cost functions. ", "page_idx": 16}, {"type": "text", "text": "C On the Unified Risk Measure ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we deepen the risk measures presented in our work, showing also how to select the functions $f$ and $g$ to obtain the desired risk measure. For what follows, we define $Z$ as a finite-mean random variable that has $F_{Z}(\\cdot)$ as the cumulative distribution function. Formally: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[|Z|\\right]<+\\infty\\quad\\mathrm{and}\\quad F_{Z}(z)=\\mathbb{P}(Z\\leqslant z).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For what follows, let such a random variable represent a loss. ", "page_idx": 16}, {"type": "text", "text": "C.1 Expected Cost and Mean-Variance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "One of the most natural risk measures is represented directly by the expected cost on the trajectories induced by a (hyper)policy. This \u201crisk neutral\u201d risk measure was introduced in Section 2 for both the PG exploration paradigms as $J_{\\mathrm{AC},i}$ and $J_{\\mathrm{PC},i}$ , where $i$ is the cost function index. ", "page_idx": 17}, {"type": "text", "text": "However, in some scenarios, it may be desirable to minimize jointly the expected cost and its variance. To this end, it is possible to consider the Mean-Variance (MV, Markowitz and Todd, 2000; Li and Ng, 2000) risk measure which models this kind of objective. Indeed, the MV is a combination of the mean cost and its variance over the trajectories. The MV with parameter $\\kappa$ over the random variable $Z$ , which represents a loss, is defined as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{M}\\mathbf{V}_{\\kappa}(Z):=\\mathbb{E}\\left[Z\\right]+\\kappa\\,\\mathbb{V}\\mathrm{ar}\\left[Z\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2 Chance Constraints, Value at Risk, and Conditional Value at Risk ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Value at Risk $\\displaystyle(\\mathrm{VaR}_{\\alpha})$ ) and Conditional Value at Risk $\\left(\\mathrm{CVaR}_{\\alpha}\\right)$ are two popular and strongly connected risk measures. In the following, we report their standard definition as provided by Rockafellar and Uryasev (2000) and Chow et al. (2017). ", "page_idx": 17}, {"type": "text", "text": "$\\operatorname{VaR}_{\\alpha}$ measures risk as the maximum cost that might be incurred with respect to a given confidence level $\\alpha\\in(0,1)$ . Such a quantity models the potential loss and the probability that the loss will occur. In its classical definition, the $\\operatorname{VaR}_{\\alpha}$ is the worst-case loss associated with a probability and a time horizon. This risk metric is particularly useful when there is a well defined failure state. Formally: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{VaR}_{\\alpha}(Z):=\\operatorname*{min}\\left\\{z|F_{Z}(z)\\geqslant\\alpha\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\mathrm{CVaR}_{\\alpha}$ measures risk as the expected cost given that such cost is greater than or equal to $\\operatorname{VaR}_{\\alpha}$ , and provides a number of theoretical and computational advantages. Such a quantity represents the expected loss if the worst-case threshold is passed (i.e., beyond the $\\mathrm{VaR}_{\\alpha}$ breakpoint). Formally: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{CVaR}_{\\alpha}(Z):=\\operatorname*{min}_{\\eta\\in\\mathbb{R}}\\left\\{\\eta+\\frac{1}{1-\\alpha}\\operatorname{\\mathbb{E}}\\left[(Z-\\eta)^{+}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Another risk measure, similar to the $\\operatorname{VaR}_{\\alpha}$ and the $\\mathrm{CVaR}_{\\alpha}$ , is the chance one. Considering chance constraints means to consider constraints to hold with high probability. Indeed, constraints are imposed on the probability that the loss random variable exceeds a certain threshold $n$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{Chance}_{n}(Z):=\\mathbb{P}(Z\\geqslant n).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.3 Unified Risk Measure Mapping ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we introduce an objective for writing an optimization problem for different risk measures (Bisi et al., 2022). For every $i\\in[0,U]$ , we introduce the functions $f_{i}:\\mathbb{R}^{2}\\to\\mathbb{R}$ and $g_{i}:\\mathbb{R}\\to\\mathbb{R}$ in order to express the unified risk- m inim ization objective function over a risk measure on $C_{i}$ as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\eta_{i}\\in\\mathbb{R}}\\left\\{\\operatorname*{min}_{\\theta\\in\\Theta}\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot|\\theta)}{\\mathbb{E}}\\left[f_{i}(C_{i}(\\tau),\\eta_{i})\\right]+g_{i}(\\eta_{i})\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, by fixing a policy parameter configuration $\\pmb{\\theta}$ , we obtain an AB unified risk measure over $C_{i}$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{\\mathrm{A},i}(\\pmb{\\theta}):=\\operatorname*{min}_{\\eta_{i}\\in\\mathbb{R}}\\left\\{{\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\pmb{\\theta})}{\\mathbb{E}}}\\left[f_{i}(C_{i}(\\tau),\\eta_{i})\\right]+g_{i}(\\eta_{i})\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In order to switch to the PB exploration paradigm, we need to consider also the expectation over the parameter configuration of the underlying policy. Indeed, by fixing a hyperparameter configuration $\\rho$ , we can define: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{\\mathrm{P},i}(\\pmb{\\rho}):=\\operatorname*{min}_{\\eta_{i}\\in\\mathbb{R}}\\left\\{\\underset{\\theta\\sim\\nu_{\\rho}}{\\mathbb{E}}\\left[\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\pmb{\\theta})}{\\mathbb{E}}\\left[f_{i}(C_{i}(\\tau),\\eta_{i})\\right]\\right]+g_{i}(\\eta_{i})\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By selecting the functions $f_{i}$ and $g_{i}$ we can consider distinct risk measures to minimize, as we show in Table 4. ", "page_idx": 17}, {"type": "text", "text": "The focus of this section is to derive the mapping between the functions $f$ and $g$ and the desired risk measure as shown in Table 4. In particular, we consider just the AB exploration scenario, since then passing to the PB one is straightforward. In this section, we consider a generic cost function $c:S\\times A\\to[0,1]$ with its cumulative cost index over a trajectory $\\tau$ which is $C(\\tau)=$ $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\gamma^{t}c(\\pmb{s}_{\\tau,t},\\pmb{a}_{\\tau,t})}\\end{array}$ .c eI no nw thhaet  fpoollliocwy sp, awriatmh eat leirtst awbhuisce ho ifn ndoutcaeti torna,j ewcte orwiiells .rite the risk measures $\\pmb{\\theta}$ $\\tau$ ", "page_idx": 18}, {"type": "text", "text": "Mean Cost. This is the simplest case, indeed, we can express the minimization problem of the mean cost as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{\\theta}\\in\\Theta}\\frac{\\mathbb{E}}{\\tau\\!\\sim\\!p_{\\mathrm{A}}(\\cdot|\\pmb{\\theta})}\\left[C(\\tau)\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, it suffices to select $f(C(\\tau),\\eta)=C(\\tau)$ and $g(\\eta)=0$ to make the minimization problem fit with the form of the unified cost minimization formulation. ", "page_idx": 18}, {"type": "text", "text": "Conditional Value at Risk. For what concern the $\\mathrm{CVaR}_{\\alpha}$ , we can start from its formulation: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\boldsymbol{\\mathrm{CVaR}}_{\\alpha}(\\theta)=\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\left\\lbrace\\underset{\\eta\\in\\mathbb{R}}{\\operatorname*{min}}\\left\\lbrace\\eta+\\frac{1}{1-\\alpha}\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta}{\\mathbb{E}}\\left[(C(\\tau)-\\eta)^{+}\\right]\\right\\rbrace\\right\\rbrace}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\eta\\in\\mathbb{R}}{\\operatorname*{min}}\\left\\lbrace\\eta+\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\,\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[\\frac{1}{1-\\alpha}(C(\\tau)-\\eta)^{+}\\right]\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, we need to select $\\begin{array}{r}{f(C(\\tau),\\eta)=\\frac{1}{1-\\alpha}(C(\\tau)-\\eta)^{+}}\\end{array}$ and $g(\\eta)=\\eta$ to complete the mapping to the unified cost minimization objective. ", "page_idx": 18}, {"type": "text", "text": "Mean Variance. For the $\\mathrm{MV}_{\\kappa}(\\pmb\\theta)$ objective, we need to make some preliminary observations. In particular, for a generic finite-mean random variable $X$ , we have that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{V}\\mathrm{ar}\\left[X\\right]=\\mathbb{E}\\left[X^{2}\\right]-\\mathbb{E}\\left[X\\right]^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, by Fenchel duality, we have the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[X\\right]^{2}=\\operatorname*{max}_{\\eta\\in\\mathbb{R}}\\left\\{2\\eta\\,\\mathbb{E}\\left[X\\right]-\\eta^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given that, we can start the derivation from the minimization of the Mean Variance: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\theta}\\bar{\\epsilon}\\bar{\\Theta}^{-}\\cdots\\cdots\\cdots,\\cdots,}\\\\ &{=\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\left\\{\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}[C(\\tau)]+\\kappa\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[C(\\tau)^{2}\\right]-\\kappa\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}[C(\\tau)]^{2}\\right\\}}\\\\ &{=\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\left\\{\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}[C(\\tau)]+\\kappa\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[C(\\tau)^{2}\\right]-\\kappa\\underset{\\eta\\in\\mathbb{R}}{\\operatorname*{max}}\\left\\{2\\eta\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}[C(\\tau)]-\\eta^{2}\\right\\}\\right\\}}\\\\ &{=\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\left\\{\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}[C(\\tau)]+\\kappa\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[C(\\tau)^{2}\\right]+\\kappa\\underset{\\eta\\in\\mathbb{R}}{\\operatorname*{min}}\\left\\{-2\\eta\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}[C(\\tau)]+\\eta^{2}\\right\\}\\right\\}}\\\\ &{=\\underset{\\eta\\in\\mathbb{R}}{\\operatorname*{min}}\\left\\{\\kappa\\eta^{2}+\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\left\\{(1-2\\kappa\\eta)\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}[C(\\tau)]+\\kappa\\underset{\\tau\\sim p_{\\mathbb{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[C(\\tau)^{2}\\right]\\right\\}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, to complete the mapping, we need to select ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(C(\\tau),\\eta)=(1-2\\kappa\\eta)\\mathop{\\mathbb{E}}_{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}\\left[C(\\tau)\\right]+\\kappa\\mathop{\\mathbb{E}}_{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}\\left[C(\\tau)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $g(\\eta)=\\kappa\\eta^{2}$ ", "page_idx": 18}, {"type": "text", "text": "Chance. For characterizing the chance constraints, we start by expressing the probability of an event $A$ as the expected value of the indicator function: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(A\\right)=\\mathbb{E}\\left[\\mathbb{1}\\left\\{A\\right\\}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, having to minimize a chance measure on the cost with a parameter $n$ , we can rewrite it as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\boldsymbol{\\mathrm{Chance}}_{n}(C,\\theta)=\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\underset{\\boldsymbol{\\tau}\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[\\mathbb{P}(C(\\tau)\\geqslant n)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\underset{\\boldsymbol{\\tau}\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[\\mathbb{1}\\left\\{C(\\tau)\\geqslant n\\right\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, to complete the mapping to the unified cost minimization objective, it suffices to select $f(C(\\tau),\\eta)=\\mathrm{\\dot{\\mathbb{1}}}\\left\\{C(\\tau)\\geq\\dot{n}\\right\\}$ and $g(\\eta)=0$ . ", "page_idx": 19}, {"type": "text", "text": "D Estimators ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we provide all the explicit estimators\u2019 forms for both C-PGAE and C-PGPE and for each risk measure. For the sake of clarity, we report the mapping between the $f_{i}$ and $g_{i}$ functions and the risk measures in Table 4. ", "page_idx": 19}, {"type": "table", "img_path": "2vywag2lVC/tmp/ab2c5ca825d48ccacf83f9192ee020adb733da95bca0c8d884ba7249f9dea856.jpg", "table_caption": ["Table 4: Mapping between $f_{i}$ and $g_{i}$ and the cost measures. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "The general Lagrangian function for the problem in Equation (11) is the following: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}_{\\uparrow,\\omega}(v,\\lambda,\\eta):=\\mathcal{I}_{\\dag,0}(v,\\eta_{0})+\\sum_{u=1}^{U}\\lambda_{u}\\left(\\mathcal{I}_{\\dag,u}(v,\\eta_{u})-b_{u}\\right)-\\frac{\\omega}{2}\\left\\|\\lambda\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For what follows we are going to compute the following gradients: ", "page_idx": 19}, {"type": "text", "text": "Befor e rentering into the details of the estimators, we show the general forms of the gradients of the Lagrangian. ", "page_idx": 19}, {"type": "text", "text": "General Gradient w.r.t. $\\pmb{v}$ . Notice that the following holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{v}\\tilde{\\mathcal{L}}_{\\uparrow,\\omega}(v,\\lambda,\\eta)=\\nabla_{v}\\mathcal{I}_{\\uparrow,0}(v,\\eta_{0})+\\sum_{u=1}^{U}\\lambda_{u}\\nabla_{v}\\mathcal{I}_{\\uparrow,u}(v,\\eta_{u}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "thus, in what follows, for the gradient w.r.t. $\\pmb{v}$ we focus on the single terms $\\nabla_{v}\\mathcal{J}_{\\dagger,u}(v,\\eta_{u})$ for every $u\\in\\mathbb{[0,}U]$ . ", "page_idx": 19}, {"type": "text", "text": "General Gradient w.r.t. $\\lambda$ . The general gradient w.r.t. $\\lambda$ has the form: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\nabla_{\\lambda}\\widetilde{\\mathcal{L}}_{\\uparrow,\\omega}(v,\\lambda,\\eta)=\\mathcal{J}_{\\uparrow}(v,\\eta)-b+\\omega\\lambda,}}\\\\ {{\\mathcal{J}_{\\uparrow}(v,\\eta)=(\\mathcal{J}_{\\uparrow,1}(v,\\eta_{1}),...,\\mathcal{J}_{\\uparrow,U}(v,\\eta_{U}))^{\\top}\\mathrm{~and~}b=(b_{1},...,b_{U})^{\\top}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "General Gradient w.r.t. $\\eta$ . As done for the gradient w.r.t. $\\pmb{v}$ , the following holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\eta}\\tilde{\\mathcal{L}}_{\\uparrow,\\omega}(v,\\lambda,\\eta)=\\nabla_{\\eta}\\mathcal{I}_{\\uparrow,0}(v,\\eta_{0})+\\sum_{u=1}^{U}\\lambda_{u}\\nabla_{\\eta}\\mathcal{I}_{\\uparrow,u}(v,\\eta_{u}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, also in this case, we will focus on the single terms $\\nabla_{\\eta}\\mathcal{I}_{\\dagger,u}(v,\\eta_{u})$ for every $u\\in[0,U]$ . ", "page_idx": 19}, {"type": "text", "text": "D.1 Parameter-based Algorithm: C-PGPE ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the following we consider a generic hyperpolicy $\\nu_{\\rho}$ and a generic parameterization $\\rho$ . Before starting with the derivations, we report the definition of $\\mathcal{I}_{\\mathrm{P},i}(\\pmb{\\rho},\\eta_{i})$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}_{\\mathrm{P},i}(\\pmb{\\rho},\\eta_{i}):=\\underset{\\pmb{\\theta}\\sim\\nu_{\\rho}}{\\mathbb{E}}\\left[\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot|\\pmb{\\theta})}{\\mathbb{E}}\\left[f_{i}(C_{i}(\\tau),\\eta_{i})\\right]\\right]+g_{i}(\\eta_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for every $i\\in[0,U]$ . ", "page_idx": 20}, {"type": "text", "text": "D.1.1 Gradients w.r.t. Parameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The first step of the derivation can be done via the log-trick for the parameter-based exploration paradigm as stated by Sehnke et al. (2010): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\rho}\\mathcal{J}_{\\mathrm{P},i}(\\rho,\\eta_{i})=\\nabla_{\\rho}\\underset{\\theta\\sim\\nu_{\\rho}}{\\mathbb{E}}\\left[\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[f_{i}(C_{i}(\\tau),\\eta_{i})\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\theta\\sim\\nu_{\\rho}}{\\mathbb{E}}\\left[\\nabla_{\\rho}\\log\\nu_{\\rho}(\\theta)\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[f_{i}(C_{i}(\\tau),\\eta_{i})\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To switch to the sample-based versions of all the gradients, we consider the behavior of C-PGPE described in Section 4. In particular, we have the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\nabla}_{\\rho}\\mathcal{J}_{\\mathrm{P},i}(\\pmb{\\rho},\\eta_{i})=\\frac{1}{N}\\sum_{j=1}^{N}\\nabla_{\\rho}\\log\\nu_{\\rho}(\\pmb{\\theta}_{j})f_{i}(C_{i}(\\tau_{j}),\\eta_{i}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, to obtain the estimators for all the risk measures, it suffices to map the selection of the $f_{i}$ functions in Equation (42). ", "page_idx": 20}, {"type": "text", "text": "D.1.2 Gradients w.r.t. Lagrangian Multipliers ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Given the general gradient w.r.t. $\\lambda$ of the regularized Lagrangian $\\widetilde{\\mathcal{L}}_{\\mathrm{P},\\omega}$ in the parameter-based scenario, the partial derivative w.r.t. $\\lambda_{i}$ is the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\lambda_{i}}\\widetilde{\\mathcal{L}}_{\\mathrm{P},\\omega}(\\rho,\\lambda,\\eta)=\\underset{\\theta\\sim\\nu_{\\rho}}{\\mathbb{E}}\\left[\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[f_{i}(C_{i}(\\tau),\\eta_{i})\\right]\\right]+g_{i}(\\eta_{i})-b_{i}+\\omega\\lambda_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This is defined f or any $i\\in[U]$ . ", "page_idx": 20}, {"type": "text", "text": "In order to switch to the sample-based version of the partial derivative, we consider the behavior of C-PGPE as described in Section 4. In particular, we have the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{\\nabla}_{\\lambda_{i}}\\tilde{\\mathcal{L}}_{\\mathrm{P},\\omega}(\\rho,\\lambda,\\eta)=\\frac{1}{N}\\sum_{j=1}^{N}f_{i}(C_{i}(\\tau_{j}),\\eta_{i})+g_{i}(\\eta_{i})-b_{i}+\\omega\\lambda_{i}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, to obtain the estimator for all the risk measures, it suffices to map the choices of $f_{i}$ and $g_{i}$ in Equation (44). ", "page_idx": 20}, {"type": "text", "text": "D.1.3 Gradients w.r.t. Risk Parameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For what concern the gradients w.r.t. $\\eta$ , we have to enumerate the mappings shown in Table 4. Notice that, when the employed risk measure is the expected cost or the chance, the estimator of the gradient w.r.t. $\\eta$ is not needed. Indeed, in the unified risk measure formulation such mappings do not depend on $\\eta$ . As shown at the beginning of this section, we can just focus on the terms $\\nabla_{\\pmb{\\eta}}\\mathcal{I}_{\\mathrm{P},i}(\\pmb{\\rho},\\eta_{i})$ , and in particular on the partial derivative $\\nabla_{\\eta_{i}}\\mathcal{I}_{\\mathrm{P},i}(\\pmb{\\rho},\\eta_{i})$ , that exhibits the common form: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\eta_{i}}\\mathcal{J}_{\\mathrm{P},i}\\big(\\rho,\\eta_{i}\\big)=\\nabla_{\\eta_{i}}\\underset{\\theta\\sim\\nu_{\\rho}}{\\mathbb{E}}\\left[\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[f_{i}(C_{i}(\\tau),\\eta_{i})\\right]\\right]+\\nabla_{\\eta_{i}}g_{i}\\big(\\eta_{i}\\big)}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\theta\\sim\\nu_{\\rho}}{\\mathbb{E}}\\left[\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[\\nabla_{\\eta_{i}}f_{i}(C_{i}(\\tau),\\eta_{i})\\right]\\right]+\\nabla_{\\eta_{i}}g_{i}\\big(\\eta_{i}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Mean Variance. In this case we have that $f_{i}(C_{i}(\\tau),\\eta_{i})=(1-2\\kappa_{i}\\eta_{i})C_{i}(\\tau)+\\kappa_{i}C_{i}(\\tau)^{2}$ and that $g_{i}(\\eta_{i})=\\kappa_{i}\\eta_{i}^{2}$ . Thus, from Equation (46), we get the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\eta_{i}}\\mathcal{J}_{\\mathrm{P},i}(\\pmb{\\rho},\\eta_{i})=\\underset{\\pmb{\\theta}\\sim\\nu_{\\rho}}{\\mathbb{E}}\\left[\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\pmb{\\theta})}{\\mathbb{E}}\\left[-2\\kappa_{i}C_{i}(\\tau)\\right]\\right]+2\\kappa_{i}\\eta_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Considering the behavior of C-PGPE described in Section 4, we obtain the following sample-based version: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\nabla}_{\\eta_{i}}\\mathcal{J}_{\\mathrm{P},i}(\\pmb{\\rho},\\eta_{i})=-\\frac{2\\kappa_{i}}{N}\\sum_{j=1}^{N}C_{i}(\\tau_{j})+2\\kappa_{i}\\eta_{i}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Conditional Value at Risk. In this case we have that $\\begin{array}{r}{f_{i}(C_{i}(\\tau),\\eta_{i})\\,=\\,\\frac{1}{1-\\alpha_{i}}\\left(C_{i}(\\tau)-\\eta_{i}\\right)^{+}}\\end{array}$ and that $g_{i}(\\eta_{i})=\\eta_{i}$ . As also shown by Chow et al. (2017), due to the presence of the non-differentiable term $(\\cdot)^{+}$ , we need to resort to sub-differentiability theory. The following holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\partial_{\\eta_{i}}(C_{i}(\\tau)-\\eta_{i})^{+}=\\left\\{\\begin{array}{l l}{-1}&{\\mathrm{if~}C_{i}(\\tau)>\\eta_{i}}\\\\ {-q:\\;q\\in[0,1]}&{\\mathrm{if~}C_{i}(\\tau)=\\eta_{i}}\\\\ {0}&{\\mathrm{elsewhere.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, from Equation (46), we can write what follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\mathcal{C}_{\\eta_{i}}\\mathcal{I}_{\\mathrm{P},i}(\\rho,\\eta_{i})}}\\\\ {\\displaystyle=\\frac{1}{1-\\alpha_{i}}\\operatorname*{l}_{\\theta\\sim\\nu_{\\rho}}\\left[\\underset{\\tau\\sim p_{\\mathrm{A}}(\\tau)\\theta)}{\\mathbb{E}}\\left[\\hat{\\sigma}_{\\eta_{i}}\\left(C_{i}(\\tau)-\\eta_{i}\\right)^{+}\\right]\\right]+1}\\\\ {\\displaystyle=\\frac{1}{1-\\alpha_{i}}\\operatorname*{l}_{\\theta\\sim\\nu_{\\rho}}\\left[\\int_{\\tau}p_{\\mathrm{A}}(\\tau|\\theta)\\hat{\\sigma}_{\\eta_{i}}\\left(C_{i}(\\tau)-\\eta_{i}\\right)^{+}\\mathrm{d}\\tau\\right]+1}\\\\ {\\displaystyle=\\frac{1}{1-\\alpha_{i}}\\operatorname*{l}_{\\theta\\sim\\nu_{\\rho}}\\left[-\\int_{\\tau}p_{\\mathrm{A}}(\\tau|\\theta)q1\\left\\{C_{i}(\\tau)=\\eta_{i}\\right\\}\\mathrm{d}\\tau-\\int_{\\tau}p_{\\mathrm{A}}(\\tau|\\theta)1\\left\\{C_{i}(\\tau)>\\eta_{i}\\right\\}\\mathrm{d}\\tau\\right]+1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with $q\\in[0,1]$ . In particular, for $q=1$ , we obtain the following partial derivative: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\partial_{\\eta_{i}}\\mathcal{J}_{\\mathrm{P},i}(\\pmb{\\rho},\\eta_{i})=\\frac{1}{1-\\alpha_{i}}\\underset{\\pmb{\\theta}\\sim\\nu_{\\rho}}{\\mathbb{E}}\\left[\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot|\\pmb{\\theta})}{\\mathbb{E}}\\left[\\mathbb{1}\\left\\{C_{i}(\\tau)\\geqslant\\eta_{i}\\right\\}\\right]\\right]+1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, according to the C-PGPE behavior described in Section 4, we obtain the following samplebased version: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\partial}_{\\eta_{i}}\\mathcal{J}_{\\mathrm{{P}},i}(\\pmb{\\rho},\\eta_{i})=\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{1}\\left\\{C_{i}(\\tau_{j})\\geqslant\\eta_{i}\\right\\}+1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With a little abuse of notation, we will use $\\widehat{\\nabla}_{\\eta_{i}}\\mathcal{J}_{\\mathrm{P},i}(\\pmb{\\rho},\\eta_{i})=\\widehat{\\partial}_{\\eta_{i}}\\mathcal{J}_{\\mathrm{P},i}(\\pmb{\\rho},\\eta_{i})$ for the $\\eta$ update in the case in which the $\\mathrm{CVaR}_{\\alpha}$ risk measure is empployed. ", "page_idx": 21}, {"type": "text", "text": "D.2 Action-based Algorithm: C-PGAE ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the following we consider a generic policy $\\pi_{\\theta}$ and a generic parameterization $\\pmb{\\theta}$ . Before starting with the derivations, we report the definition of $\\mathcal{I}_{\\mathrm{A},i}(\\pmb\\theta,\\eta_{i})$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{J}_{\\mathrm{A},i}(\\pmb{\\theta},\\eta_{i}):=\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot|\\pmb{\\theta})}{\\mathbb{E}}\\left[f_{i}(C_{i}(\\tau),\\eta_{i})\\right]+g_{i}(\\eta_{i}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for every $i\\in[0,U]$ ", "page_idx": 21}, {"type": "text", "text": "D.2.1 Gradients w.r.t. Parameters ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Also in this case, the first step of the derivation can be done via the log-trick, which provides an analogous result of the Policy Gradient Theorem (PGT, Sutton et al., 1999): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{A},i}(\\theta,\\eta_{i})=\\nabla_{\\theta}\\operatorname*{\\lrcorner}_{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}\\big[f_{i}(C_{i}(\\tau),\\eta_{i})\\big]}}\\\\ &{=\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[\\nabla_{\\theta}\\log p_{\\mathrm{A}}(\\tau\\vert\\theta)f_{i}(C_{i}(\\tau),\\eta_{i})\\right]}\\\\ &{=\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\theta)}{\\mathbb{E}}\\left[\\overset{T-1}{\\underset{t=0}{\\sum}}\\nabla_{\\theta}\\log\\pi(a_{\\tau,t},s_{\\tau,t})f_{i}(C_{i}(\\tau),\\eta_{i})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To switch to the sample-based versions of all the gradients, we generally resort to the Monte Carlo version of $\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{A},i}(\\overline{{\\theta_{,}}}\\,\\eta_{i})$ , obtaining a REINFORCE-like (Williams, 1992) estimator, that is: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{\\nabla}_{\\theta}\\mathcal{J}_{\\mathrm{A},i}(\\theta,\\eta_{i})=\\frac{1}{N}\\sum_{j=1}^{N}\\left(\\sum_{t=0}^{T-1}\\nabla_{\\theta}\\log\\pi(a_{\\tau_{j},t},s_{\\tau_{j},t})\\right)f_{i}(C_{i}(\\tau_{j}),\\eta_{i}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, to obtain the estimators for all the risk measures, it suffices to map the selection of the $f_{i}$ functions in Equation (60). ", "page_idx": 22}, {"type": "text", "text": "It is worth noticing that some of the choices for $f_{i}$ and $g_{i}$ allow to switch to a GPOMDP-like (Baxter and Bartlett, 2001) estimator, which suffers from less variance. This holds for the expected cost and mean variance risk measures. ", "page_idx": 22}, {"type": "text", "text": "Expected Cost GPOMDP-like Estimator. In this case $f_{i}(C_{i}(\\tau),\\eta_{i})=C_{i}(\\tau)$ , thus we can obtain exactly the GPOMDP estimator: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{\\nabla}_{\\theta}\\mathcal{J}_{\\mathrm{A},i}(\\theta,\\eta_{i})=\\frac{1}{N}\\sum_{j=1}^{N}\\left(\\sum_{t=0}^{T-1}\\gamma^{t}c_{i}\\big(s_{\\tau_{j},t},a_{\\tau_{j},t}\\big)\\sum_{h=0}^{t}\\nabla_{\\theta}\\log\\pi(a_{\\tau_{j},h},s_{\\tau_{j},h})\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Mean Variance GPOMDP-like Estimator. In this case $f_{i}(C_{i}(\\tau),{\\eta}_{i})\\;=\\;(1\\,-\\,2\\kappa_{i}{\\eta}_{i})C_{i}(\\tau)\\,+$ $\\kappa_{i}C_{i}(\\tau)^{2}$ , thus we can obtain the GPOMDP estimator just for the $C_{i}(\\tau)$ part: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\nabla}_{\\theta}\\mathcal{J}_{\\mathrm{A},i}(\\theta,\\eta_{i})}\\\\ &{=\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\left(\\sum_{t=0}^{T-1}\\nabla_{\\theta}\\log\\pi(a_{{\\tau_{j}},t},s_{{\\tau_{j}},t})\\right)\\left((1-2\\kappa_{i}\\eta_{i})C_{i}({\\tau_{j}})+\\kappa_{i}C_{i}({\\tau_{j}})^{2}\\right)}\\\\ &{=\\displaystyle\\frac{1-2\\kappa_{i}\\eta_{i}}{N}\\sum_{j=1}^{N}\\left(\\sum_{t=0}^{T-1}\\gamma^{t}c_{i}(s_{{\\tau_{j}},t},a_{{\\tau_{j}},t})\\sum_{h=0}^{t}\\nabla_{\\theta}\\log\\pi(a_{{\\tau_{j}},h},s_{{\\tau_{j}},h})\\right)}\\\\ &{\\quad+\\displaystyle\\frac{\\kappa_{i}}{N}\\sum_{j=1}^{N}\\left(\\sum_{t=0}^{T-1}\\nabla_{\\theta}\\log\\pi(a_{{\\tau_{j}},t},s_{{\\tau_{j}},t})\\right)C_{i}({\\tau_{j}})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D.2.2 Gradients w.r.t. Lagrangian Multipliers ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The result is the same as the one obtained for C-PGPE. The difference lies in the way in which trajectories are collected. The estimator for the partial derivative w.r.t. $\\lambda_{i}$ is: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{\\nabla}_{\\lambda_{i}}\\tilde{\\mathcal{L}}_{{\\mathrm{A}},\\omega}(\\pmb{\\theta},\\pmb{\\lambda},\\pmb{\\eta})=\\frac{1}{N}\\sum_{j=1}^{N}f_{i}(C_{i}(\\tau_{j}),\\eta_{i})+g_{i}(\\eta_{i})-b_{i}+\\omega\\lambda_{i}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To obtain the estimator for all the risk measures, it suffices to map the choices of $f_{i}$ and $g_{i}$ as prescribed by Table 4. ", "page_idx": 22}, {"type": "text", "text": "D.2.3 Gradients w.r.t. Risk Parameters ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As for the exploration-based case, also here we need to enumerate the mappings reported in Table 4. However, the expected cost and the chance risk measures do not depend on $\\eta$ , thus they are not treated. As shown at the beginning of the section, we can just focus on the $\\nabla_{\\eta}\\mathcal{J}_{\\mathrm{A},i}(\\pmb{\\theta},\\eta_{i})$ terms and, in particular, we consider the partial derivative $\\nabla_{\\eta_{i}}\\mathcal{I}_{\\mathrm{A},i}(\\pmb\\theta,\\eta_{i})$ . For it, we can recover the common form: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\eta_{i}}\\mathcal{J}_{\\mathrm{A},i}\\bigl(\\pmb{\\theta},\\eta_{i}\\bigr)=\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot|\\pmb{\\theta})}{\\mathbb{E}}\\left[\\nabla_{\\eta_{i}}f_{i}\\bigl(C_{i}(\\tau),\\eta_{i}\\bigr)\\right]+\\nabla_{\\eta_{i}}g_{i}\\bigl(\\eta_{i}\\bigr).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Mean Variance. In this case we have that $f_{i}(C_{i}(\\tau),\\eta_{i})=(1-2\\kappa_{i}\\eta_{i})C_{i}(\\tau)+\\kappa_{i}C_{i}(\\tau)^{2}$ and that $g_{i}(\\eta_{i})=\\kappa_{i}\\eta_{i}^{2}$ . Thus, from Equation (67), the following holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\eta_{i}}\\mathcal{J}_{\\mathrm{A},i}(\\pmb{\\theta},\\eta_{i})=\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot\\vert\\pmb{\\theta})}{\\mathbb{E}}\\left[-2\\kappa_{i}C_{i}(\\tau)\\right]+2\\kappa_{i}\\eta_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Considering the behavior of C-PGAE described in Section 4, we obtain the following sample-based version: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\nabla}_{\\eta_{i}}\\mathcal{J}_{\\mathrm{A},i}(\\pmb{\\theta},\\eta_{i})=-\\frac{2\\kappa_{i}}{N}\\sum_{j=1}^{N}C_{i}(\\tau_{j})+2\\kappa_{i}\\eta_{i}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Conditional Value at Risk. In this case we have that $\\begin{array}{r}{f_{i}(C_{i}(\\tau),\\eta_{i})\\,=\\,\\frac{1}{1-\\alpha_{i}}\\left(C_{i}(\\tau)-\\eta_{i}\\right)^{+}}\\end{array}$ and that $g_{i}(\\eta_{i})=\\eta_{i}$ . Here we face the same issues we have discussed in the corresponding C-PGPE part. With the same procedure, we obtain the following partial derivative: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\partial_{\\eta_{i}}\\mathcal{J}_{\\mathrm{A},i}(\\pmb{\\theta},\\eta_{i})=\\frac{1}{1-\\alpha_{i}}\\underset{\\tau\\sim p_{\\mathrm{A}}(\\cdot|\\pmb{\\theta})}{\\mathbb{E}}\\left[\\mathbb{1}\\left\\{C_{i}(\\tau)\\geqslant\\eta_{i}\\right\\}\\right]+1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, according to the C-PGAE behavior described in Section 4, we obtain the following samplebased version: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\partial}_{\\eta_{i}}\\mathcal{J}_{\\mathrm{A},i}(\\pmb{\\theta},\\eta_{i})=\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{1}\\left\\{C_{i}(\\tau_{j})\\geqslant\\eta_{i}\\right\\}+1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Also in this case, we will use $\\widehat{\\nabla}_{\\eta_{i}}\\mathcal{J}_{\\mathrm{A},i}(\\pmb{\\rho},\\eta_{i})=\\widehat{\\partial}_{\\eta_{i}}\\mathcal{J}_{\\mathrm{A},i}(\\pmb{\\rho},\\eta_{i})$ for the $\\eta$ update in the case in which the $\\mathrm{CVaR}_{\\alpha}$ risk measure is empployed. ", "page_idx": 23}, {"type": "text", "text": "E Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma E.1 (Regularization Bias on Saddle Points - 1). Under Assumption 3.1, for every $\\omega\\geqslant0$ , let $(v_{\\omega}^{*},\\lambda_{\\omega}^{*})$ be a saddle point of $\\mathcal{L}_{\\omega}$ , it holds that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n0\\leqslant\\mathcal{L}_{0}(v_{0}^{*},\\lambda_{0}^{*})-\\mathcal{L}_{0}(v_{\\omega}^{*},\\lambda_{\\omega}^{*})\\leqslant\\frac{\\omega}{2}\\left(\\|\\pmb{\\lambda}_{0}^{*}\\|_{2}^{2}-\\|\\pmb{\\lambda}_{\\omega}^{*}\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. From the fact that $(v_{\\omega}^{*},\\lambda_{\\omega}^{*})$ is a saddle point of $\\mathcal{L}_{\\omega}$ , we have for every $\\boldsymbol{v}\\in\\mathcal{V}$ and $\\lambda{\\in}\\,\\Lambda$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal L_{\\omega}(v,\\lambda_{\\omega}^{*})\\geqslant\\mathcal L_{\\omega}(v_{\\omega}^{*},\\lambda_{\\omega}^{*})\\geqslant\\mathcal L_{\\omega}(v_{\\omega}^{*},\\lambda)}\\\\ &{\\iff\\mathcal L_{0}(v,\\lambda_{\\omega}^{*})-\\frac{\\omega}{2}\\,\\|\\lambda_{\\omega}^{*}\\|_{2}^{2}\\geqslant\\mathcal L_{0}(v_{\\omega}^{*},\\lambda_{\\omega}^{*})-\\frac{\\omega}{2}\\,\\|\\lambda_{\\omega}^{*}\\|_{2}^{2}\\geqslant\\mathcal L_{0}(v_{\\omega}^{*},\\lambda)-\\frac{\\omega}{2}\\,\\|\\lambda\\|_{2}^{2}}\\\\ &{\\iff\\mathcal L_{0}(v,\\lambda_{\\omega}^{*})\\geqslant\\mathcal L_{0}(v_{\\omega}^{*},\\lambda_{\\omega}^{*})\\geqslant\\mathcal L_{0}(v_{\\omega}^{*},\\lambda)+\\frac{\\omega}{2}\\left(\\|\\lambda_{\\omega}^{*}\\|_{2}^{2}-\\|\\lambda\\|_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From the fact that $(v_{0}^{*},\\lambda_{0}^{*})$ is a saddle point of $\\scriptstyle{\\mathcal{L}}_{0}$ , we have for every $\\boldsymbol{v}\\in\\mathcal{V}$ and $\\lambda\\!\\in\\!\\Lambda$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{0}(v,\\lambda_{0}^{*})\\geqslant\\!\\mathcal{L}_{0}(v_{0}^{*},\\lambda_{0}^{*})\\geqslant\\mathcal{L}_{0}(v_{0}^{*},\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By setting $(\\boldsymbol{v},\\lambda)\\gets(\\boldsymbol{v}_{\\omega}^{*},\\lambda_{\\omega}^{*})$ in Equation (75) and $(v,\\lambda)\\gets(v_{0}^{*},\\lambda_{0}^{*})$ in Equation (74), we obtain: (76) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{0}(v_{\\omega}^{*},\\lambda_{0}^{*})\\geqslant\\mathcal{L}_{0}(v_{0}^{*},\\lambda_{0}^{*})\\geqslant\\mathcal{L}_{0}(v_{0}^{*},\\lambda_{\\omega}^{*})}\\\\ &{\\qquad\\qquad\\qquad=\\mathcal{L}_{0}(v_{0}^{*},\\lambda_{\\omega}^{*})\\geqslant\\mathcal{L}_{0}(v_{\\omega}^{*},\\lambda_{\\omega}^{*})\\geqslant\\mathcal{L}_{0}(v_{\\omega}^{*},\\lambda_{0}^{*})+\\frac{\\omega}{2}\\left(\\|\\lambda_{\\omega}^{*}\\|_{2}^{2}-\\|\\lambda_{0}^{*}\\|_{2}^{2}\\right)}\\\\ &{\\qquad\\qquad\\geqslant\\mathcal L_{0}(v_{0}^{*},\\lambda_{0}^{*})+\\frac{\\omega}{2}\\left(\\|\\lambda_{\\omega}^{*}\\|_{2}^{2}-\\|\\lambda_{0}^{*}\\|_{2}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "thus: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}_{0}(v_{0}^{*},\\lambda_{0}^{*})\\geqslant\\mathcal{L}_{0}(v_{\\omega}^{*},\\lambda_{\\omega}^{*})\\geqslant\\mathcal{L}_{0}(v_{0}^{*},\\lambda_{0}^{*})+\\frac{\\omega}{2}\\left(\\|\\lambda_{\\omega}^{*}\\|_{2}^{2}-\\|\\lambda_{0}^{*}\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma E.2 (Regularization Bias on Saddle Points - 2). Under Assumption 3.1, for every $\\omega\\geqslant0$ , it holds that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n0\\leqslant\\operatorname*{min}_{v\\in\\mathcal{V}}\\operatorname*{max}_{\\lambda\\in\\Lambda}\\mathcal{L}_{0}(v,\\lambda)-\\operatorname*{min}_{v\\in\\mathcal{V}}\\operatorname*{max}_{\\lambda\\in\\Lambda}\\mathcal{L}_{\\omega}(v,\\lambda)\\leqslant\\frac{\\omega}{2}\\|\\lambda_{0}^{*}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. The first inequality follows from the observation that $\\mathcal{L}_{0}(v,\\lambda)\\geqslant\\mathcal{L}_{\\omega}(v,\\lambda)$ for every $\\omega\\geqslant0$ . For the second inequality, let us denote as $(v_{\\omega}^{*},\\lambda_{\\omega}^{*})$ the saddle point for $\\mathcal{L}_{\\omega}$ and let $\\Lambda^{*}=\\{\\lambda_{0}^{*},\\lambda_{\\omega}^{*}\\}$ . We have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}_{0}(v_{0}^{*},\\lambda_{0}^{*})-\\mathcal{L}_{\\omega}(v_{\\omega}^{*},\\lambda_{\\omega}^{*})=\\operatorname*{min}_{v\\in\\mathcal{V}}\\operatorname*{max}_{\\lambda\\in\\Lambda^{*}}\\mathcal{L}_{0}(v,\\lambda)-\\operatorname*{min}_{v\\in\\mathcal{V}}\\operatorname*{max}_{\\lambda\\in\\Lambda^{*}}\\mathcal{L}_{\\omega}(v,\\lambda)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leqslant\\underset{v\\in\\mathcal{V}}{\\operatorname*{max}}\\left|\\underset{\\lambda\\in\\Lambda^{*}}{\\operatorname*{max}}\\,\\mathcal{L}_{0}(v,\\lambda)-\\underset{\\lambda\\in\\Lambda^{*}}{\\operatorname*{max}}\\,\\mathcal{L}_{\\omega}(v,\\lambda)\\right|}\\\\ &{=\\underset{v\\in\\mathcal{V},\\lambda\\in\\Lambda^{*}}{\\operatorname*{max}}\\,|\\mathcal{L}_{0}(v,\\lambda)-\\mathcal{L}_{\\omega}(v,\\lambda)|}\\\\ &{=\\frac{\\omega}{2}\\operatorname*{max}\\left\\lbrace\\|\\lambda_{0}^{*}\\|_{2}^{2};\\;\\|\\lambda_{\\omega}^{*}\\|_{2}^{2}\\right\\rbrace}\\\\ &{=\\frac{\\omega}{2}\\,\\|\\lambda_{0}^{*}\\|_{2}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used Lemma E.1 to conclude that $\\left\\|\\lambda_{0}^{*}\\right\\|_{2}^{2}\\geqslant\\left\\|\\lambda_{\\omega}^{*}\\right\\|_{2}^{2}$ ", "page_idx": 24}, {"type": "text", "text": "Lemma E.3 (Objective bound and Constraint violation). Under Assumption 3.1, for every $\\omega\\geqslant0$ , letting $(v_{\\omega}^{*},\\lambda_{\\omega}^{*})$ be a saddle point of $\\mathcal{L}_{\\omega}$ , it holds that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leqslant J_{0}(v_{0}^{*})-J_{0}(v_{\\omega}^{*})\\leqslant\\omega\\|\\lambda_{0}^{*}\\|_{2}^{2},}\\\\ &{\\|(\\mathbf{J}(v_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}\\leqslant\\omega\\|\\lambda_{0}^{*}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Since $(v_{0}^{*},\\lambda_{0}^{*})$ is a saddle point of $\\scriptstyle{\\mathcal{L}}_{0}$ , it holds that $\\pmb{v}_{0}^{*}$ is feasible and, consequently, $\\mathcal{L}_{0}(v_{0}^{*},\\lambda_{0}^{*})\\;=\\;\\check{J}_{0}(v_{0}^{*})$ . Moreover, let $\\omega\\;>\\;0$ : since $(v_{\\omega}^{*},\\lambda_{\\omega}^{*})$ is a saddle point of $\\mathcal{L}_{\\omega}$ it holds that $\\begin{array}{r l r}{\\tilde{\\lambda}_{\\omega}^{*}\\;\\stackrel{}{=}\\;\\lambda^{*}(v_{\\omega}^{*})\\;\\stackrel{}{=}\\;\\Pi_{\\Lambda}\\left(\\frac{1}{\\omega}({\\bf J}(v_{\\omega}^{*})-{\\bf b})\\right)\\;=\\;\\frac{1}{\\omega}({\\bf J}(v_{\\omega}^{*})-{\\bf\\bar{b}})^{+}}&{}\\end{array}$ , since $\\begin{array}{r}{\\frac{1}{\\omega}\\|(\\mathbf{J}(v_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}\\ \\leqslant}\\end{array}$ $\\omega^{-1}\\sqrt{U}J_{\\operatorname*{max}}$ . Thus, we have\\`: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{0}(v_{\\omega}^{*},\\lambda_{\\omega}^{*})=J_{0}(v_{\\omega}^{*})+\\langle\\lambda_{\\omega}^{*},\\mathbf{J}(v_{\\omega}^{*})-\\mathbf{b}\\rangle=J_{0}(v_{\\omega}^{*})+\\frac{1}{\\omega}\\|(\\mathbf{J}(v_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "From Lemma E.1, we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n0\\leqslant J_{0}(v_{0}^{*})-J_{0}(v_{\\omega}^{*})-\\frac{1}{\\omega}\\|(\\mathbf{J}(v_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}^{2}\\leqslant\\frac{\\omega}{2}\\|\\mathbf{\\lambda}_{0}^{*}\\|_{2}^{2}-\\frac{1}{2\\omega}\\|(\\mathbf{J}(v_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By summing $\\frac{1}{\\omega}\\|(\\mathbf{J}(\\boldsymbol{v}_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}^{2}$ to all members, we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\omega}\\|(\\mathbf{J}(v_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}^{2}\\leqslant J_{0}(v_{0}^{*})-J_{0}(v_{\\omega}^{*})\\leqslant\\frac{\\omega}{2}\\|\\mathbf{\\boldsymbol{\\lambda}}_{0}^{*}\\|_{2}^{2}+\\frac{1}{2\\omega}\\|(\\mathbf{J}(v_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now taking the first and last member, we conclude: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(\\mathbf{J}(\\boldsymbol{v}_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}^{2}\\leqslant\\omega^{2}\\|\\boldsymbol{\\lambda}_{0}^{*}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\begin{array}{r}{\\frac{1}{\\omega}\\|(\\mathbf{J}(\\mathbf{v}_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}^{2}\\geqslant0}\\end{array}$ and plugging the latter inequality into the third member of (89) we obtain: ", "page_idx": 24}, {"type": "equation", "text": "$$\n0\\leqslant J_{0}(v_{0}^{*})-J_{0}(v_{\\omega}^{*})\\leqslant\\omega\\|\\lambda_{0}^{*}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma E.4 (Weak $\\psi$ -Gradient Domination on $H_{\\omega}(v)$ ). Under Assumption 3.2, $i f\\omega>0,$ , for every $\\boldsymbol{v}\\in\\mathcal{V}$ and $\\lambda{\\in}\\ \\Lambda$ , it holds that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\nabla_{v}H_{\\omega}(v)\\|_{2}^{\\psi}\\geqslant\\alpha_{1}\\left(H_{\\omega}(v)-\\operatorname*{min}_{v^{\\prime}\\in\\mathcal{V}}H_{\\omega}(v^{\\prime})\\right)-\\beta_{1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. If $\\omega>0$ , the dual variable exist finite since the maximization problem over $\\lambda$ is concave: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\lambda^{*}(v)=\\underset{\\lambda\\in\\Lambda}{\\arg\\operatorname*{max}}\\,\\mathcal{L}_{\\omega}(v,\\lambda).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we have from Lemma E.7 tha $\\nabla_{v}H_{\\omega}(v)=\\nabla_{v}\\mathcal{L}_{\\omega}(v,\\lambda)|_{\\lambda=\\lambda^{*}(v)}$ and by Assumption 3.2 we have the following: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{v}H_{\\omega}(v)\\|_{2}=\\big\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v,\\lambda)\\big|_{\\lambda=\\lambda^{*}(v)}\\big\\|_{2}}\\\\ &{\\qquad\\qquad\\quad\\geqslant\\alpha_{1}\\,\\bigg(\\mathcal{L}_{\\omega}(v,\\lambda^{*}(v))-\\displaystyle\\operatorname*{min}_{v^{\\prime}\\in\\mathcal{V}}\\mathcal{L}_{\\omega}(v^{\\prime},\\lambda^{*}(v))\\bigg)-\\beta_{1}}\\\\ &{\\qquad\\quad\\geqslant\\alpha_{1}\\,\\bigg(H_{\\omega}(v)-\\displaystyle\\operatorname*{min}_{v^{\\prime}\\in\\mathcal{V}}\\operatorname*{max}_{\\lambda\\in\\Lambda}\\mathcal{L}_{\\omega}(v^{\\prime},\\lambda)\\bigg)-\\beta_{1}}\\\\ &{\\qquad\\quad=\\alpha_{1}\\,\\big(H_{\\omega}(v)-H_{\\omega}^{*}\\big)-\\beta_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma E.5. Let $\\omega>0$ and $\\boldsymbol{v}\\in\\mathcal{V}$ . The following statements hold: ", "page_idx": 24}, {"type": "text", "text": "\u2022 $\\mathcal{L}_{\\omega}(v,\\cdot)$ is $\\omega$ -smooth, i.e., for every $\\lambda$ , $\\pmb{\\lambda}^{\\prime}\\in\\Lambda$ it holds that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(v,\\lambda^{\\prime})-\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(v,\\lambda)\\right|\\leqslant\\omega\\left\\|\\lambda-\\lambda^{\\prime}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "\u2022 $\\mathcal{L}_{\\omega}(v,\\cdot)$ satisfies the $P L$ condition, i.e., for every $\\lambda\\!\\in\\!\\Lambda$ it holds that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(v,\\lambda)\\|_{2}^{2}\\geqslant\\omega\\left(\\operatorname*{max}_{\\lambda^{\\prime}\\in\\Lambda}\\mathcal{L}_{\\omega}(v,\\lambda^{\\prime})-\\mathcal{L}_{\\omega}(v,\\lambda)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "\u2022 $\\mathcal{L}_{\\omega}(v,\\cdot)$ satisfies the error bound $(E B)$ condition, i.e., for every $\\lambda$ , $\\pmb{\\lambda}^{\\prime}\\in\\Lambda$ it holds that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(v,\\lambda)\\|\\geqslant\\frac{\\omega}{2}\\|\\lambda^{*}(v)-\\lambda\\|_{2},}\\\\ {\\lambda^{*}(v)=\\arg\\operatorname*{max}_{\\lambda\\in\\Lambda}\\mathcal{L}_{\\omega}(v,\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "\u2022 $\\mathcal{L}_{\\omega}(v,\\cdot)$ satisfies the quadratic growth $(Q G)$ condition, i.e., for every $\\lambda$ , $\\pmb{\\lambda}^{\\prime}\\in\\Lambda$ it holds that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{H_{\\omega}(v)-\\mathcal{L}_{\\omega}(v,\\lambda)\\geqslant\\displaystyle\\frac{\\omega}{4}\\|\\lambda^{*}(v)-\\lambda\\|_{2},}\\\\ {{}^{*}(v)=\\arg\\operatorname*{max}_{\\lambda\\in\\Lambda}\\mathcal{L}_{\\omega}(v,\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "text", "text": "Proof. For the first property, it is enough to observe that $\\mathcal{L}_{\\omega}$ is twice differentiable in $\\lambda$ and that its Hessian is $\\omega\\mathbf{I}$ . For the second property, we observe that $\\mathcal{L}_{\\omega}$ is quadratic in $\\lambda$ and, consequently it satisfies the PL condition with parameter $\\omega$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(v,\\lambda)\\|_{2}^{2}\\geqslant\\omega\\left(\\operatorname*{max}_{\\lambda^{\\prime}\\in\\mathbb{R}^{U}}\\mathcal{L}_{\\omega}(v,\\lambda^{\\prime})-\\mathcal{L}_{\\omega}(v,\\lambda)\\right)\\geqslant\\omega\\left(\\operatorname*{max}_{\\lambda^{\\prime}\\in\\Lambda}\\mathcal{L}_{\\omega}(v,\\lambda^{\\prime})-\\mathcal{L}_{\\omega}(v,\\lambda)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the third and fourth properties, we refer to Lemma A.1 of Yang et al. (2020). ", "page_idx": 25}, {"type": "text", "text": "Lemma E.6. Let $\\omega>0$ . For every $\\boldsymbol{v}\\in\\mathcal{V}$ , it holds that: ", "page_idx": 25}, {"type": "equation", "text": "$$\nH_{\\omega}({\\pmb v})-H_{\\omega}^{*}\\geqslant\\frac{\\omega}{4}\\|{\\pmb\\lambda}^{*}({\\pmb v})-{\\pmb\\lambda}_{\\omega}^{*}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Let us consider the following derivation: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\omega}(v)-H_{\\omega}^{*}=H_{\\omega}(v)-\\mathcal{L}_{\\omega}(v_{\\omega}^{*},\\lambda_{\\omega}^{*})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geqslant H_{\\omega}(v)-\\mathcal{L}_{\\omega}(v,\\lambda_{\\omega}^{*})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geqslant\\frac{\\omega}{4}\\|\\lambda^{*}(v)-\\lambda_{\\omega}^{*}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "having exploited the fact that, from the saddle point property, $\\mathcal{L}_{\\omega}(v_{\\omega}^{*},\\lambda_{\\omega}^{*})\\leqslant\\mathcal{L}_{\\omega}(v,\\lambda_{\\omega}^{*})$ and, then, Lemma E.5. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma E.7. Let $\\omega>0$ . The following statements hold: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{v}H_{\\omega}(\\boldsymbol{v}^{\\prime})-\\nabla_{v}H_{\\omega}(\\boldsymbol{v})\\|_{2}\\leqslant L_{H}\\|\\boldsymbol{v}^{\\prime}-\\boldsymbol{v}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The first and second statements follow from Lemma A.5 of Nouiehed et al. (2019). ", "page_idx": 25}, {"type": "text", "text": "Theorem 3.1 (Objective Function Gap and Constraint Violation). Let $\\epsilon>0$ . Under Assumption 3.1, $i f\\,\\mathcal{P}_{k}\\leqslant\\epsilon$ , it holds that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[J_{0}(v_{k})-J_{0}(v_{0}^{*})]\\leqslant\\epsilon+\\frac{\\omega}{2}\\|\\lambda_{0}^{*}\\|_{2}^{2},\\qquad\\mathbb{E}[(J_{i}(v_{k})-b_{i})^{+}]\\leqslant4\\epsilon+\\omega\\|\\lambda_{0}^{*}\\|_{2},\\quad\\forall i\\in\\mathbb{I}[U].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Since $\\mathcal{P}_{k}\\leqslant\\epsilon$ , it follows that $a_{k}\\leqslant\\epsilon$ and, consequently, $0\\leqslant\\mathbb{E}[H_{\\omega}(v_{k})-H_{\\omega}^{*}]\\leqslant\\epsilon$ . We start by bounding the norm of the dual variables: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\b{\\lambda}^{*}(\\b{v}_{k})\\|_{2}\\leqslant\\|\\b{\\lambda}_{\\omega}^{*}\\|_{2}+\\|\\b{\\lambda}^{*}(\\b{v}_{k})-\\b{\\lambda}_{\\omega}^{*}\\|_{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\leqslant\\|\\lambda_{\\omega}^{*}\\|_{2}+\\frac{4}{\\omega}(H_{\\omega}(\\boldsymbol{v}_{k})-H_{\\omega}^{*}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we applied the triangular inequality and Lemma E.6. The projection $\\Pi_{\\Lambda}$ is such that $\\pmb{\\lambda}^{*}(\\pmb{v})=$ $\\begin{array}{r}{\\Pi_{\\Lambda}\\left(\\frac{1}{\\omega}({\\bf J}({\\bf v})-{\\bf b})\\right)=\\frac{1}{\\omega}({\\bf J}({\\bf v})-{\\bf b})^{+}}\\end{array}$ and, consequently, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|(\\mathbf{J}(v_{k})-\\mathbf{b})^{+}\\|_{2}-\\|(\\mathbf{J}(v_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}\\leqslant4(H_{\\omega}(v_{k})-H_{\\omega}^{*}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By the last inequality, together with Lemma E.3, having applied the expectation on both sides: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|(\\mathbf{J}(v_{k})-\\mathbf{b})^{+}\\|_{2}]\\leqslant\\|(\\mathbf{J}(v_{\\omega}^{*})-\\mathbf{b})^{+}\\|_{2}+4\\mathbb{E}[H_{\\omega}(v_{k})-H_{\\omega}^{*}]}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\omega\\|\\lambda_{0}^{*}\\|_{2}+4\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Recalling that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|(\\mathbf{J}(v_{k})-\\mathbf{b}))^{+}\\|_{2}]\\geqslant\\big\\|\\mathbb{E}[(\\mathbf{J}(v_{k})-\\mathbf{b})^{+}]\\big\\|_{2}\\geqslant\\|\\mathbb{E}[(\\mathbf{J}(v_{k})-\\mathbf{b})^{+}]\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the objective function bound, let us consider the following derivation. By definition of $H_{\\omega}(v)$ and $\\lambda^{*}(v)$ we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\nJ_{0}(\\pmb{v}_{k})-J_{0}(\\pmb{v}_{\\omega}^{*})=H_{\\omega}(\\pmb{v}_{k})-H_{\\omega}^{*}-\\frac{\\omega}{2}\\left(\\|\\pmb{\\lambda}^{*}(\\pmb{v}_{k})\\|_{2}^{2}-\\|\\pmb{\\lambda}_{\\omega}^{*}\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking the expectation on both sides and upper bounding $\\|\\lambda_{\\omega}^{*}\\|$ with $\\|\\lambda_{0}^{*}\\|$ from Lemma E.1: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[J_{0}(v_{k})-J_{0}(v_{\\omega}^{*})]=\\mathbb{E}[H_{\\omega}(v_{k})-H_{\\omega}^{*}]-\\displaystyle\\frac{\\omega}{2}\\,\\mathbb{E}[\\|\\lambda^{*}(v_{k})\\|_{2}^{2}-\\|\\lambda_{\\omega}^{*}\\|_{2}^{2}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leqslant\\mathbb{E}[H_{\\omega}(v_{k})-H_{\\omega}^{*}]+\\displaystyle\\frac{\\omega}{2}\\|\\lambda_{\\omega}^{*}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leqslant\\epsilon+\\displaystyle\\frac{\\omega}{2}\\|\\lambda_{0}^{*}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The result is obtained by applying Lemma E.3 as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[J_{0}(\\boldsymbol{v}_{k})-J_{0}(\\boldsymbol{v}_{0}^{*})\\big]=\\mathbb{E}\\big[J_{0}(\\boldsymbol{v}_{k})-J_{0}(\\boldsymbol{v}_{\\omega}^{*})\\big]+\\underbrace{J_{0}(\\boldsymbol{v}_{\\omega}^{*})-J_{0}(\\boldsymbol{v}_{0}^{*})}_{\\leqslant0}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Theorem 3.2 (Convergence of $\\mathcal{P}_{K}$ ). Under Assumptions 3.2, 3.3, 3.4, for $\\chi<1/5$ , sufficiently small $\\epsilon$ and $\\omega$ , and a choice of constant learning rates $\\zeta_{v},\\zeta_{\\lambda}$ , we have $\\mathcal{P}_{K}(\\chi)\\leqslant\\epsilon+\\beta_{1}/\\alpha_{1}$ whenever:8 \u2022 $K=\\mathcal{O}(\\omega^{-1}\\log(\\epsilon^{-1}))$ $i f\\psi=2$ and the gradients are exact (i.e., $V_{v}=V_{\\lambda}=0,$ ); \u2022 $K=\\mathcal{O}(\\omega^{-1}\\epsilon^{-\\frac{2}{\\psi}-1})\\,i f\\psi\\in[1,2)$ and the gradients are exact (i.e., $V_{v}=V_{\\lambda}=0,$ ); \u2022 $K\\;=\\;\\mathcal{O}(\\omega^{-3}\\epsilon^{-\\frac{4}{\\psi}+1})$ $\\colon f\\;\\psi\\;\\in\\;[1,2]$ and the gradients are estimated (i.e., $V_{v}\\ =\\ \\mathcal{O}(\\omega^{-2})$ and $V_{\\lambda}={\\mathcal{O}}(1),$ . ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Proof. The proof is subdivided into several parts. We will omit the $\\omega$ subscript for notational easiness.   \nLet us focus on a specific iteration $k\\in\\mathbb{N}$ . ", "page_idx": 26}, {"type": "text", "text": "Part I: bounding the $a_{k}$ term. Let us start with the $a_{k}$ term: ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\omega}(\\boldsymbol{v}_{k+1})-H^{*}\\leqslant H_{\\omega}(\\boldsymbol{v}_{k})-H^{*}+\\langle\\boldsymbol{v}_{k+1}-\\boldsymbol{v}_{k},\\ \\nabla_{\\boldsymbol{v}}H_{\\omega}(\\boldsymbol{v}_{k})\\rangle+\\frac{L_{H}}{2}\\left\\|\\boldsymbol{v}_{k+1}-\\boldsymbol{v}_{k}\\right\\|_{2}^{2}}\\\\ &{\\leqslant H_{\\boldsymbol{v}}(\\boldsymbol{v}_{k})-H^{*}-\\zeta_{v,k}\\left\\langle\\hat{\\nabla}_{\\boldsymbol{v}}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\boldsymbol{\\lambda}_{k}),\\ \\nabla_{\\boldsymbol{v}}H_{\\omega}(\\boldsymbol{v}_{k})\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\frac{L_{H}}{2}\\zeta_{v,k}^{2}\\left\\|\\hat{\\nabla}_{\\boldsymbol{v}}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\boldsymbol{\\lambda}_{k})\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the first line is due to the fact\u203a tphat the functio\u203an $H_{\\omega}$ is $L_{H}$ -smooth (Lemma E.7), the last inequality is due to the update rule of $\\pmb{v}$ . Now, we apply the expected value on both sides of the inequality and we use the fact that the gradient estimation is unbiased and has variance bounded by V\u03c5: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})\\vert\\mathcal{F}_{k-1}\\right]-H^{*}\\leqslant H_{\\omega}(\\boldsymbol{v}_{k})-H^{*}-\\zeta_{v,k}\\,\\langle\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k}),\\,\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{L_{H}}{2}\\zeta_{v,k}^{2}\\,\\mathbb{E}\\left[\\left\\Vert\\hat{\\nabla}_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})\\right\\Vert_{2}^{2}\\vert\\mathcal{F}_{k-1}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mathcal{F}_{k-1}$ is the flitration associated with all events realized up to interaction $k-1$ . We recall that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\widehat{\\nabla}_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})\\right\\|_{2}^{2}|\\mathcal{F}_{k-1}\\right]=\\mathbb{V}\\mathrm{ar}\\left[\\widehat{\\nabla}_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})|\\mathcal{F}_{k-1}\\right]+\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})\\|_{2}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and that $\\mathbb{V}\\mathrm{ar}\\left[\\widehat{\\nabla}_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})\\right]\\leqslant V_{v}$ by Assumption 3.4. Thus, selecting $\\zeta_{v,k}\\leqslant1/L_{H}$ , we have that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})\\vert\\mathcal{F}_{k-1}\\right]-H^{*}}\\\\ &{\\leqslant H_{\\omega}(\\boldsymbol{v}_{k})-H^{*}-\\zeta_{v,k}\\,\\langle\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})\\,\\,\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\rangle}\\\\ &{\\quad+\\,\\frac{\\zeta_{v,k}}{2}\\,\\|\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})\\|_{2}^{2}+\\frac{L_{H}}{2}\\zeta_{v,k}^{2}V_{v}}\\\\ &{=H_{\\omega}(\\boldsymbol{v}_{k})-H^{*}-\\zeta_{v,k}\\,\\langle\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})\\,\\,\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\rangle}\\\\ &{\\quad+\\,\\frac{\\zeta_{v,k}}{2}\\,\\|\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})\\pm\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\|_{2}^{2}+\\frac{L_{H}}{2}\\zeta_{v,k}^{2}V_{v}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Consider that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\zeta_{v,k}}{2}\\left\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(v_{k})+\\nabla_{v}H_{\\omega}(v_{k})\\right\\|_{2}^{2}}\\\\ &{=\\frac{\\zeta_{v,k}}{2}\\left\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(v_{k})\\right\\|_{2}^{2}-\\frac{\\zeta_{v,k}}{2}\\left\\|\\nabla_{v}H_{\\omega}(v_{k})\\right\\|_{2}^{2}}\\\\ &{\\quad+\\,\\zeta_{v,k}\\left\\langle\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k}),\\,\\nabla_{v}H_{\\omega}(v_{k})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, the following holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})\\vert\\mathcal{F}_{k-1}\\right]-H^{*}}\\\\ &{\\leqslant H_{\\omega}(\\boldsymbol{v}_{k})-H^{*}-\\zeta_{v,k}\\left\\langle\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k}),\\ \\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\right\\rangle+\\frac{L_{H}}{2}\\zeta_{v,k}^{2}V_{v}}\\\\ &{\\quad+\\frac{\\zeta_{v,k}}{2}\\left\\Vert\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\right\\Vert_{2}^{2}-\\frac{\\zeta_{v,k}}{2}\\left\\Vert\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\right\\Vert_{2}^{2}}\\\\ &{\\quad+\\zeta_{v,k}\\left\\langle\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k}),\\ \\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\right\\rangle}\\\\ &{=H_{\\omega}(\\boldsymbol{v}_{k})-H^{*}-\\frac{\\zeta_{v,k}}{2}\\left\\Vert\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\right\\Vert_{2}^{2}+\\frac{\\zeta_{v,k}}{2}\\left\\Vert\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\right\\Vert_{2}^{2}}\\\\ &{\\quad+\\frac{L_{H}}{2}\\zeta_{v,k}^{2}V_{v}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, we have obtained: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\oplus}:=\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})|\\mathcal{F}_{k-1}\\right]-H^{*}}\\\\ &{\\quad\\leqslant H_{\\omega}(\\boldsymbol{v}_{k})-H^{*}-\\frac{\\zeta_{v,k}}{2}\\left\\|\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\right\\|_{2}^{2}+\\frac{\\zeta_{v,k}}{2}\\left\\|\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\right\\|_{2}^{2}}\\\\ &{\\quad\\quad\\leqslant\\frac{L_{H}}{2}\\zeta_{v,k}^{2}V_{v},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "holding via the selection of $\\zeta_{v,k}\\leqslant1/L_{H}$ . Notice that, from $\\textsuperscript{\\textregistered}$ the following directly follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{\\mathbb{D}}:=\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})|\\mathcal{F}_{k-1}\\right]-H_{\\omega}(\\boldsymbol{v}_{k})}\\\\ {\\displaystyle\\quad\\leqslant\\,-\\frac{\\zeta_{v,k}}{2}\\,\\|\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\|_{2}^{2}+\\frac{\\zeta_{v,k}}{2}\\,\\|\\nabla_{v}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(\\boldsymbol{v}_{k})\\|_{2}^{2}+\\frac{L_{H}}{2}\\zeta_{v,k}^{2}V_{v}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Part II: bounding the $b_{k}$ term. We are ready to analyze the $b_{k}$ term. Recall that for ridge regularization of the Lagrangian function presented in the main paper, we have that $\\mathcal{L}_{\\omega}$ is $\\omega_{}$ -smooth and fulflils the PL condition with constant $\\omega$ , as shown in Lemma E.5. Since $\\mathcal{L}$ is a quadratic function of $\\lambda$ and $\\lambda^{*}(v_{k+1})\\in\\Lambda$ , we have that considering the non-projected $\\lambda_{k+1}$ can only increase the distance. Thus, we will ignore projection for the rest of the proof. We have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\omega}(v_{k+1})-\\mathcal{L}_{\\omega}(v_{k+1},\\lambda_{k+1})}\\\\ &{\\leqslant H_{\\omega}(v_{k+1})-\\mathcal{L}_{\\omega}(v_{k+1},\\lambda_{k})-\\langle\\lambda_{k+1}-\\lambda_{k},\\ \\nabla_{\\lambda}\\mathcal{L}_{\\omega}(v_{k+1},\\lambda_{k})\\rangle+\\frac{\\omega}{2}\\left\\|\\lambda_{k+1}-\\lambda_{k}\\right\\|_{2}^{2}}\\\\ &{=H_{\\omega}(v_{k+1})-\\mathcal{L}_{\\omega}(v_{k+1},\\lambda_{k})-\\zeta_{\\lambda,k}\\left\\langle\\hat{\\nabla}_{\\lambda}\\mathcal{L}_{\\omega}(v_{k+1},\\lambda_{k}),\\ \\nabla_{\\lambda}\\mathcal{L}_{\\omega}(v_{k+1},\\lambda_{k})\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n+\\left.\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}\\left\\|\\widehat{\\nabla}_{\\lambda}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "that is possible un\u203adper Assumption 3.\u203a3 (i.e., $\\mathcal{L}_{\\omega}$ is $L_{2}$ -smooth) and due to the update rules we are considering. Now, by applying the expectation on both sides, we obtain the following: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k+1})|\\mathcal{F}_{k-1}\\right]}\\\\ &{\\quad\\leqslant\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})|\\mathcal{F}_{k-1}\\right]-\\zeta_{\\lambda,k}\\left\\|\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\right\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad+\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}\\mathbb{E}\\left[\\left\\|\\hat{\\nabla}_{\\lambda}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\right\\|_{2}^{2}|\\mathcal{F}_{k-1}\\right]}\\\\ &{\\quad\\leqslant\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})|\\mathcal{F}_{k-1}\\right]-\\zeta_{\\lambda,k}\\left\\|\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\right\\|_{2}^{2}+\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}}\\\\ &{\\quad\\quad\\quad+\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}\\left\\|\\hat{\\nabla}_{\\lambda}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{})\\right\\|_{2}^{2}}\\\\ &{\\quad\\leqslant\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})|\\mathcal{F}_{k-1}\\right]-\\frac{\\zeta_{\\lambda,k}}{2}\\left\\|\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\right\\|_{2}^{2}+\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last line follows by selecting $\\zeta_{\\lambda,k}\\leqslant1/\\omega$ . Since $\\mathcal{L}_{\\omega}$ enjoys the $\\mathrm{PL}$ condition w.r.t $\\lambda$ with constant $\\omega$ , for every pair $(v,\\lambda)$ we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(v,\\lambda)\\|_{2}^{2}\\geqslant\\omega\\left(\\operatorname*{max}_{\\overline{{\\lambda}}\\in\\mathbb{R}^{U}}\\mathcal{L}_{\\omega}(v,\\overline{{\\lambda}})-\\mathcal{L}_{\\omega}(v,\\lambda)\\right)\\geqslant\\omega\\left(\\operatorname*{max}_{\\overline{{\\lambda}}\\in\\Lambda}\\mathcal{L}_{\\omega}(v,\\overline{{\\lambda}})-\\mathcal{L}_{\\omega}(v,\\lambda)\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By applying the PL condition: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k+1})\\vert\\mathcal{F}_{k-1}\\right]}\\\\ &{\\leqslant\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\vert\\mathcal{F}_{k-1}\\right]-\\frac{\\zeta_{\\lambda,k}}{2}\\left\\Vert\\nabla_{\\lambda}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\right\\Vert_{2}^{2}+\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}}\\\\ &{\\leqslant\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\vert\\mathcal{F}_{k-1}\\right]-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\vert\\mathcal{F}_{k-1}\\right]}\\\\ &{\\qquad+\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}}\\\\ &{=\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\vert\\mathcal{F}_{k-1}\\right]+\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "twhhuesr ce ownes iednefr otrhcee $\\begin{array}{r}{1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\geqslant0}\\end{array}$ , i.e., $\\zeta_{\\lambda,k}\\leqslant2/\\omega$ . However, we do not have a proper recursive term, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{\\omega}(v_{k+1})-\\mathcal{L}_{\\omega}(v_{k+1},\\lambda_{k})=\\underbrace{H_{\\omega}(v_{k})-\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})}_{\\mathrm{Recursive~Term}}+\\underbrace{\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\mathcal{L}_{\\omega}(v_{k+1},\\lambda_{k})}_{\\odot}}\\\\ {+\\underbrace{H_{\\omega}(v_{k+1})-H_{\\omega}(v_{k})}_{\\odot}.\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "A bound on $\\mathbb{O}$ has already been derived, so let us bound the term $\\mathbb{Q}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})\\leqslant-\\left\\langle\\boldsymbol{v}_{k+1}-\\boldsymbol{v}_{k},\\ \\nabla_{\\boldsymbol{v}}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})\\right\\rangle+\\frac{L_{2}}{2}\\left\\|\\boldsymbol{v}_{k+1}-\\boldsymbol{v}_{k}\\right\\|_{2}^{2}}\\\\ &{}&{\\leqslant\\zeta_{\\boldsymbol{v},k}\\left\\langle\\hat{\\nabla}_{\\boldsymbol{v}}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k}),\\ \\nabla_{\\boldsymbol{v}}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})\\right\\rangle+\\frac{L_{2}}{2}\\zeta_{\\boldsymbol{v},k}^{2}\\left\\|\\hat{\\nabla}_{\\boldsymbol{v}}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "again because of Assumpption 3.3 and the update rule. Now, as usual, \u203awpe consider the \u203aexpectation conditioned to the filtration $\\mathcal{F}_{k-1}$ and the properties of the variance, to obtain: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{C}:=\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\mathbb{E}\\left[\\mathcal{L}_{\\omega}(v_{k+1},\\lambda_{k})|\\mathcal{F}_{k-1}\\right]}\\\\ {\\leqslant\\zeta_{v,k}\\left(1+\\frac{{\\cal L}_{2}}{2}\\zeta_{v,k}\\right)\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})\\|_{2}^{2}+\\frac{{\\cal L}_{2}}{2}\\zeta_{v,k}^{2}V_{v},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "having set $\\zeta_{v,k}\\leqslant1/L_{2}$ . We are finally able to conclude the bound of the term $\\textsuperscript{\\textregistered}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{B}:=\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k+1})|\\mathcal{F}_{k-1}\\right]}}\\\\ &{\\quad\\leqslant\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k})|\\mathcal{F}_{k-1}\\right]+\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)(H_{\\omega}(\\boldsymbol{v}_{k})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\boldsymbol{\\lambda}_{k}))}}\\\\ &{+\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)(\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\boldsymbol{\\lambda}_{k})-\\mathbb{E}\\left[\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\boldsymbol{\\lambda}_{k})\\vert\\mathcal{F}_{k-1}\\right])}\\\\ &{+\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)(\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k+1})\\vert\\mathcal{F}_{k-1}\\right]-H_{\\omega}(\\boldsymbol{v}_{k}))+\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now we apply the bounds on $\\mathbb{Q}$ and $\\circledcirc$ (the latter is from Eq. 136), obtaining: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[H_{\\omega}(\\boldsymbol{v}_{k+1})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k+1},\\lambda_{k+1})|\\mathcal{F}_{k-1}]}\\\\ &{\\leqslant\\left(1-\\frac{\\tilde{\\zeta}_{\\lambda,k}}{2}\\omega\\right)(H_{\\omega}(\\boldsymbol{v}_{k})-\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k}))}\\\\ &{\\quad+\\left(1-\\frac{\\tilde{\\zeta}_{\\lambda,k}}{2}\\omega\\right)\\left(\\zeta_{\\nu,k}\\left(1+\\frac{L_{2}}{2}\\zeta_{\\nu,k}\\right)\\|\\nabla_{\\nu}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})\\|_{2}^{2}+\\frac{L_{2}}{2}\\zeta_{\\nu,k}^{2}V_{\\nu}\\right)}\\\\ &{\\quad+\\left(1-\\frac{\\tilde{\\zeta}_{\\lambda,k}}{2}\\omega\\right)\\left(-\\frac{\\zeta_{\\nu,k}}{2}\\|\\nabla_{\\nu}H_{\\omega}(\\boldsymbol{v}_{k})\\|_{2}^{2}+\\frac{\\zeta_{\\nu,k}}{2}\\left\\|\\nabla_{\\nu}\\mathcal{L}_{\\omega}(\\boldsymbol{v}_{k},\\lambda_{k})-\\nabla_{\\boldsymbol{v}}H_{\\omega}(\\boldsymbol{v}_{k})\\right\\|_{2}^{2}\\right.}\\\\ &{\\quad\\left.+\\frac{L_{H}}{2}\\zeta_{\\nu,k}^{2}V_{\\nu}\\right)+\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "that is the second fundamental term. ", "page_idx": 29}, {"type": "text", "text": "Part III: bounding the potential function $P_{k}(\\chi)$ . Before going on, we recall that so far we enforced: $\\zeta_{v,k}\\leqslant1/L_{H}$ (since $L_{H}\\geqslant L_{2}$ ) and $\\zeta_{\\lambda,k}\\leqslant1/\\omega$ , for every $t\\in[\\![K]\\!]$ . What we want to bound here is the po : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{andiff~}\\operatorname*{liminf}_{\\theta\\in\\Theta}\\operatorname*{liminf}_{\\theta\\in\\Theta}\\operatorname*{liminf}_{\\theta\\in\\Theta}\\operatorname*{liminf}_{\\theta\\in\\Theta}f_{\\theta\\in\\Theta}(\\theta_{\\theta})}\\\\ &{+\\epsilon\\operatorname{liminf}_{\\theta\\in\\Theta}\\operatorname*{liminf}_{\\theta\\in\\Theta}\\operatorname*{liminf}_{\\theta\\in\\Theta}f_{\\theta\\in\\Theta}(\\theta_{\\theta})}\\\\ &{\\quad+\\epsilon\\operatorname{limsup}_{\\theta\\in\\Theta}f_{\\theta}(\\theta_{\\theta})\\leq\\operatorname*{liminf}_{\\theta\\in\\Theta}f_{\\theta}(\\theta_{\\theta}),\\quad\\forall\\theta\\in\\Theta,\\quad\\pi_{\\theta}(\\theta_{\\theta})}\\\\ &{\\quad+\\epsilon\\operatorname{limsup}_{\\theta\\in\\Theta}\\operatorname*{liminf}_{\\theta\\in\\Theta}f_{\\theta}(\\theta_{\\theta})}\\\\ &{\\quad+\\epsilon\\operatorname{limsup}_{\\theta\\in\\Theta}\\operatorname*{liminf}_{\\theta\\in\\Theta}f_{\\theta}(\\theta_{\\theta}),\\quad\\forall\\theta\\in\\ensuremath{\\mathbb{R}}^{1},}\\\\ &{\\quad+\\epsilon\\operatorname{limsup}_{\\theta\\in\\Theta}\\operatorname*{liminf}_{\\theta\\in\\Theta}f_{\\theta}(\\theta_{\\theta})}\\\\ &{\\quad+x\\left(1-\\frac{\\Omega_{s}u}{\\theta}\\right)\\left(\\epsilon\\omega_{s}\\left(1+\\frac{\\Omega_{s}u}{\\theta}\\right)\\right)\\left[\\nabla_{\\theta}\\mathcal{E}_{u}(\\theta_{\\theta})\\sin\\theta\\right]_{\\theta}^{2}+\\frac{L_{2}}{2}\\mathcal{L}_{\\theta}\\mathcal{N}_{\\theta}\\right)}\\\\ &{\\quad+x\\left(1-\\frac{\\Omega_{s}u}{\\theta}\\right)\\left(\\epsilon\\!-\\!\\frac{\\Omega_{s}u}{\\theta}\\right)\\times\\left[\\left(\\nabla_{\\theta}u_{x}(\\theta_{\\theta})\\right)^{2}\\right]}\\\\ &{\\quad+x\\left(1-\\frac{\\Omega_{s}u}{\\theta}\\right)\\left(-\\frac{\\Omega_{s}u}{\\theta}\\right)\\left[\\nabla_{\\theta}u_{x}(\\theta_{\\theta})\\right]_{\\theta}^{2}}\\\\ &{\\quad\\quad+\\frac{\\epsilon^{2}}{2}x\\left[\\nabla_{\\theta}u_{x}(\\theta_{\\theta})\\!-\\!\\nabla u_{x}(\\theta_{\\theta})\\right]_{\\theta}^{2}+\\frac{L_{2}}{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now we can re-arrange the terms by noticing that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})\\|_{2}^{2}=\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(v_{k})+\\nabla_{v}H_{\\omega}(v_{k})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(v_{k})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\quad+\\,\\|\\nabla_{v}H_{\\omega}(v_{k})\\|_{2}^{2}+2\\,\\langle\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(v_{k}),\\,\\nabla_{v}H_{\\omega}(v_{k})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leqslant2\\,\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(v_{k})\\|_{2}^{2}+2\\,\\|\\nabla_{v}H_{\\omega}(v_{k})\\|_{2}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last inequality holds by Young\u2019s inequality. Then we can write what follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{A}^{n+1}\\mathbb{\\Lambda}^{1+\\nu\\delta+1}}\\\\ &{\\leqslant a_{k}+\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)b_{k}}\\\\ &{\\quad+\\left(2\\zeta_{\\mathrm{w},k}\\left(1+\\frac{\\zeta_{2}}{2}\\zeta_{\\mathrm{w},k}\\right)\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)\\right.}\\\\ &{\\qquad\\left.-\\frac{\\zeta_{\\mathrm{w},k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)\\right)\\right)\\mathbb{E}\\left[\\left|\\nabla_{\\mathrm{v}}H_{\\omega}(v_{k})\\right|_{2}^{2}\\right]}\\\\ &{\\quad+\\left(2\\zeta_{\\mathrm{w},k}\\left(1+\\frac{L_{2}}{2}\\zeta_{\\mathrm{w},k}\\right)\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)+\\frac{\\zeta_{\\mathrm{w},k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)\\right)\\right)}\\\\ &{\\qquad\\mathbb{E}\\left[\\left|\\nabla_{\\mathrm{v}}\\zeta_{\\mathrm{w}}(v_{k},\\lambda_{k})-\\nabla_{\\mathrm{v}}H_{\\omega}(v_{k})\\right|_{2}^{2}\\right]}\\\\ &{\\quad+\\frac{\\zeta_{\\mathrm{w},k}^{2}}{2}\\left(L H+\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)\\left(L H+L_{2}\\right)\\right)V_{\\mathrm{w}}+\\chi\\frac{\\zeta_{\\mathrm{w}}^{2}}{2}\\zeta_{\\mathrm{w},k}^{2}V_{\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let us now proceed to bound $\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(v_{k})\\|_{2}^{2}$ . By Lemma E.7, we have that $\\nabla_{v}H_{\\omega}(v)=\\nabla_{v}\\mathcal{L}_{\\omega}(v,\\lambda^{*}(v))$ for every $\\pmb{\\lambda}^{*}(\\pmb{v})\\in\\arg\\operatorname*{max}_{\\pmb{\\lambda}\\in\\Lambda}\\mathcal{L}_{\\omega}(\\pmb{v},\\overline{{\\lambda}})$ , thus we can write: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(v_{k})\\|_{2}^{2}=\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda^{*}(v_{k}))\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant L_{3}^{2}\\,\\|\\lambda^{*}(v_{k})-\\lambda_{k}\\|_{2}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "since Assumption 3.3 holds. ", "page_idx": 30}, {"type": "text", "text": "For a fixed value of $\\pmb{v}$ , by Lemma E.5 it follows that $\\mathcal{L}_{\\omega}(\\boldsymbol{v},\\cdot)$ satisfies the quadratic growth condition (since it satisfies the PL condition), for which the following holds: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\pmb{\\lambda}^{*}(\\pmb{v}_{k})-\\pmb{\\lambda}_{k}\\|_{2}^{2}\\leqslant\\frac{4}{\\omega}\\left(H_{\\omega}(\\pmb{v}_{k})-\\mathcal{L}_{\\omega}(\\pmb{v}_{k},\\pmb{\\lambda}_{k})\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and thus we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(v_{k})\\right\\|_{2}^{2}\\leqslant\\frac{4L_{3}^{2}}{\\omega}\\left(H_{\\omega}(v_{k})-\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By applying the total expectation, it trivially follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\nabla_{v}\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})-\\nabla_{v}H_{\\omega}(v_{k})\\Vert_{2}^{2}\\right]\\leqslant\\frac{4L_{3}^{2}}{\\omega}\\,\\mathbb{E}\\left[H_{\\omega}(v_{k})-\\mathcal{L}_{\\omega}(v_{k},\\lambda_{k})\\right]=\\frac{4L_{3}^{2}}{\\omega}b_{k}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{k+1}+\\chi b_{k+1}}\\\\ &{\\leqslant a_{k}+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)b_{k}}\\\\ &{\\quad+\\left(2\\zeta_{v,k}\\left(1+\\frac{L_{2}}{2}\\zeta_{v,k}\\right)\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right.}\\\\ &{\\quad\\left.\\quad-\\frac{\\zeta_{v,k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right)\\right)\\mathbb{E}\\left[\\left\\|\\nabla_{v}H_{\\omega}(v_{k})\\right\\|_{2}^{2}\\right]}\\\\ &{\\quad+\\left(2\\zeta_{v,k}\\left(1+\\frac{L_{2}}{2}\\zeta_{v,k}\\right)\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)+\\frac{\\zeta_{v,k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right)\\right)\\frac{4L_{3}^{2}}{\\omega}b_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n+\\left.\\frac{\\zeta_{v,k}^{2}}{2}\\left(L_{H}+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\left(L_{H}+L_{2}\\right)\\right)V_{v}+\\chi\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Part IV: apply the $\\psi$ -gradient domination. Now we need to bound the term $\\big\\|\\nabla H_{\\omega}(\\pmb{v}_{k})\\big\\|_{2}^{2}$ . We consider Assumption 3.2 and we get: $\\|\\nabla_{v}H_{\\omega}(v_{k})\\|_{2}^{\\psi}\\,\\geqslant\\,\\alpha_{1}\\,(H_{\\omega}(v_{k})-H^{*})\\,-\\,\\beta_{1}$ . By defining $\\tilde{H}^{*}:=H^{*}+\\beta_{1}/\\alpha_{1}$ , we also have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{v}H_{\\omega}(v)\\right\\|_{2}^{\\psi}\\geqslant\\alpha_{1}\\operatorname*{max}\\left\\{0,\\ H_{\\omega}(v)-\\widetilde{H}^{*}\\right\\}}\\\\ &{\\qquad\\qquad\\implies}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla_{v}H_{\\omega}(v)\\right\\|_{2}^{2}\\geqslant\\alpha_{1}^{\\frac{2}{\\psi}}\\operatorname*{max}\\left\\{0,\\ H_{\\omega}(v)-\\widetilde{H}^{*}\\right\\}^{\\frac{2}{\\psi}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "If we apply the total expectation on both sides of the inequality, w er get: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}\\left[\\|\\nabla_{v}H_{\\omega}(v)\\|_{2}^{2}\\right]\\geqslant\\alpha_{1}^{\\frac{2}{\\psi}}\\,\\mathbb{E}\\left[\\operatorname*{max}\\left\\{0,\\:H_{\\omega}(v)-\\widetilde{H}^{*}\\right\\}^{\\frac{2}{\\psi}}\\right]}\\\\ &{}&{\\geqslant\\alpha_{1}^{\\frac{2}{\\psi}}\\,\\mathbb{E}\\left[\\operatorname*{max}\\left\\{0,\\:H_{\\omega}(v)-\\widetilde{H}^{*}\\right\\}\\right]^{\\frac{2}{\\psi}}}\\\\ &{}&{\\geqslant\\alpha_{1}^{\\frac{2}{\\psi}}\\operatorname*{max}\\left\\{0,\\:\\mathbb{E}\\left[H_{\\omega}(v)-\\widetilde{H}^{*}\\right]\\right\\}^{\\frac{2}{\\psi}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which is achieved by a double application of Jensen\u2019s inequality, sinc e $z^{2/\\psi}$ is convex for $\\psi\\in[1,2]$ and $z\\geqslant0$ , and the maximum is convex. Let us start from Equation (198): ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{k+1}+\\lambda b_{k+1}}\\\\ &{\\leqslant a_{k}+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)b_{k}}\\\\ &{\\quad+\\underbrace{\\left(2\\zeta_{\\upsilon,k}\\left(1+\\frac{\\tilde{L}_{2}}{2}\\zeta_{\\upsilon,k}\\right)\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)-\\frac{\\zeta_{\\upsilon,k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right)\\right)}_{=-C}}\\\\ &{\\qquad\\cdot\\mathbb{E}\\left[\\left|\\nabla H_{\\alpha}(\\upsilon_{k})\\right|_{2}^{2}\\right]}\\\\ &{\\quad+\\left(2\\zeta_{\\upsilon,k}\\left(1+\\frac{\\tilde{L}_{2}}{2}\\zeta_{\\upsilon,k}\\right)\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)+\\frac{\\zeta_{\\upsilon,k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right)\\right)\\frac{4L_{3}^{2}}{\\omega}b_{k}}\\\\ &{\\quad+\\underbrace{\\zeta_{\\upsilon,k}^{2}}_{=}\\left(L_{H}+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)(L_{H}+L_{2})\\right)V_{\\upsilon}+\\chi\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We first enforce the negativity of $-C$ . To this end: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-C=\\left(2\\zeta_{v,k}\\underbrace{\\left(1+\\frac{\\bar{L}_{2}}{2}\\zeta_{v,k}\\right)}_{\\leqslant3/2}\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)-\\frac{\\zeta_{v,k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)\\right)\\right)}\\\\ &{\\leqslant\\zeta_{v,k}\\left(3\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)-\\frac{1}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)\\right)\\right)}\\\\ &{\\leqslant\\frac{\\zeta_{v,k}}{2}\\left(5\\chi\\left(1-\\frac{\\zeta_{3,k}}{2}\\omega\\right)-1\\right)\\leqslant\\frac{\\zeta_{v,k}}{2}\\left(5\\chi-1\\right)\\leqslant0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, it is enough to enforce $5\\chi-1\\leqslant0\\implies\\chi\\leqslant1/5$ . We now plug in the gradient domination inequalities: ", "page_idx": 31}, {"type": "equation", "text": "$$\na_{k+1}+\\chi b_{k+1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\leqslant a_{k}+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)b_{k}-C\\alpha_{1}^{\\frac{2}{\\psi}}\\operatorname*{max}\\left\\{0;\\,\\mathbb{E}\\left[H_{\\omega}(\\pmb{v})-\\tilde{H}^{*}\\right]\\right\\}^{\\frac{2}{\\psi}}+V}\\\\ {\\quad+\\left(2\\zeta_{\\pmb{v},k}\\left(1+\\frac{L_{2}}{2}\\zeta_{\\pmb{v},k}\\right)\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)+\\frac{\\zeta_{\\pmb{v},k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right)\\right)\\frac{4L_{3}^{2}}{\\omega}b_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now we introduce the symbol $\\widetilde{\\boldsymbol{a}}_{k}:=\\mathbb{E}\\left[H_{\\omega}(\\boldsymbol{v}_{k})-\\widetilde{H}^{*}\\right]=\\boldsymbol{a}_{k}-\\beta_{1}/\\alpha_{1}$ , to get: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{a}_{k+1}+\\chi b_{k+1}}\\\\ &{\\leqslant\\tilde{a}_{k}-C\\alpha_{1}^{\\frac{2}{\\psi}}\\operatorname*{max}\\left\\{0,\\tilde{a}_{k}\\right\\}^{\\frac{2}{\\psi}}+V}\\\\ &{\\mathrm{~+~}\\left(\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)+\\left(2\\zeta_{v,k}\\left(1+\\frac{L_{2}}{2}\\zeta_{v,k}\\right)\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right.\\right.}\\\\ &{\\quad\\left.\\left.+\\frac{\\zeta_{v,k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right)\\right)\\frac{4L_{3}^{2}}{\\omega}\\right)b_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For what follows, we call $B$ the term that is multiplying $b_{k}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B:=\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)+\\left(2\\zeta_{v,k}\\left(1+\\frac{L_{2}}{2}\\zeta_{v,k}\\right)\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right.}\\\\ &{\\qquad\\left.+\\frac{\\zeta_{v,k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right)\\right)\\frac{4L_{3}^{2}}{\\omega}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let refer to $\\widetilde{a}_{k}+\\chi b_{k}$ as $\\widetilde{P}_{t}(\\chi)$ with $\\chi\\in(0,1)$ . For the sake of clarity, we re-write our main inequality as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\tilde{P}_{t}(\\chi)=\\tilde{a}_{k+1}+\\chi b_{k+1}\\leqslant\\tilde{a}_{k}+B b_{k}-C\\operatorname*{max}\\left\\{0;\\;\\tilde{a}_{k}\\right\\}^{\\frac{2}{\\psi}}+V.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, from Lemmar F.1 hav irng set $a\\gets\\tilde{a}_{k}$ an dr $b\\gets\\chi b_{k}$ , we have: ", "page_idx": 32}, {"type": "text", "text": "$\\tilde{P}_{t+1}(\\chi)=\\tilde{a}_{k+1}+\\chi b_{k+1}\\leqslant\\tilde{a}_{k}+B b_{k}+C(\\chi b_{k})^{\\frac{2}{\\psi}}-2^{1-\\frac{2}{\\psi}}C\\operatorname*{max}\\left\\{0,\\,\\tilde{a}_{k}+\\chi b_{k}\\right\\}^{\\frac{2}{\\psi}}+V.$ (227) Byr choosing $\\chi$ so that $\\chi b_{k}\\leqslant1$ , ri.e., $\\chi\\leqslant1/\\operatorname*{max}_{k\\in[K]}b_{k}$ , we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{P}_{t+1}(\\chi)=\\widetilde{a}_{k+1}+\\chi b_{k+1}\\leqslant\\widetilde{a}_{k}+B b_{k}+C(\\chi b_{k})^{\\frac{2}{\\psi}}-2^{1-\\frac{2}{\\psi}}C\\operatorname*{max}\\left\\{0,\\,\\widetilde{a}_{k}+\\chi b_{k}\\right\\}^{\\frac{2}{\\psi}}+V}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\widetilde{a}_{k}+(B+\\chi C)b_{k}-2^{1-\\frac{2}{\\psi}}C\\operatorname*{max}\\left\\{0,\\,\\widetilde{a}_{k}+\\chi b_{k}\\right\\}^{\\frac{2}{\\psi}}+V}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\widetilde{P}_{t}(B+\\chi C)-2^{1-\\frac{2}{\\psi}}C\\operatorname*{max}\\left\\{0,\\,\\widetilde{P}_{t}(\\chi)\\right\\}^{\\frac{2}{\\psi}}+V.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To unfold the recursion, we need to ensure that $B+\\chi C\\leqslant\\chi$ , which leads to a condition relating the two learning rates: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D^{\\pm}+\\mathrm{\\mathcal{X}}\\quad}\\\\ &{=x\\left(1-\\frac{\\zeta_{\\pm}\\lambda_{\\theta}}{2}\\right)+\\left(2\\zeta_{\\pm},\\underbrace{\\kappa\\left(1+\\frac{L_{2}}{2}\\zeta_{\\infty,k}\\right)}_{\\iff\\infty}\\right)\\times\\underbrace{\\left(1-\\frac{\\zeta_{\\pm}\\lambda_{\\theta}}{2}\\right)}_{\\leqslant1}}\\\\ &{\\qquad+\\frac{\\zeta_{\\pm}\\lambda_{\\theta}}{2}\\left(1+x\\left(1-\\frac{\\zeta_{\\pm}\\lambda_{\\theta}}{2}\\right)\\right)\\right)\\left|\\frac{4L_{2}^{2}}{\\omega^{2}}}\\\\ &{\\qquad+x\\left(\\frac{-2\\zeta_{\\pm}\\lambda_{\\theta}}{2}\\left(1+\\frac{L_{2}}{2}\\zeta_{\\infty,k}\\right)x\\left(1-\\frac{\\zeta_{\\pm}\\lambda_{\\theta}}{2}\\right)+\\frac{\\zeta_{\\pm}\\lambda_{\\theta}}{2}\\left(1+x\\left(1-\\frac{\\zeta_{\\pm}\\lambda_{\\theta}}{2}\\right)\\right)\\right)\\right|\\alpha_{i}^{\\frac{3}{2}}}\\\\ &{\\qquad+\\underbrace{\\mathrm{\\mathcal{X}}\\left(\\frac{\\zeta_{\\pm}\\lambda_{\\theta}}{2}+\\zeta_{\\infty,k}\\left(\\frac{2L_{2}^{2}}{\\omega^{2}}\\left(1+\\gamma_{\\mp}\\right)+\\frac{1+\\gamma_{\\mp}}{2}\\alpha_{i}^{\\frac{2}{2}}\\right)\\leqslant x\\right.}_{\\leqslant1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\implies\\zeta_{v,k}\\leqslant\\frac{\\omega^{2}\\chi\\zeta_{\\lambda,k}}{(1+\\chi)\\omega\\alpha_{1}^{\\frac{2}{\\psi}}+4L_{3}^{2}(1+7\\chi)},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we exploited $\\zeta_{\\pmb{v},k}\\leqslant1/L_{2}$ and $\\zeta_{\\lambda,k}\\leqslant2/\\omega$ . Thus, we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\widetilde{P}_{k+1}(\\chi)\\leqslant\\widetilde{P}_{k}(\\chi)-2^{1-\\frac{2}{\\psi}}C\\operatorname*{max}\\left\\{0,\\;\\widetilde{P}_{k}(\\chi)\\right\\}^{\\frac{2}{\\psi}}+V.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Collecting all conditionrs on the lea rrning rates, we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta_{v,k}\\leqslant\\operatorname*{min}\\left\\{\\cfrac{1}{L_{H}},\\cfrac{1}{L_{2}},\\cfrac{\\omega^{2}\\chi\\zeta_{\\lambda,k}}{(1+\\chi)\\omega\\alpha_{1}^{\\frac{2}{\\psi}}+4L_{3}^{2}(1+7\\chi)}\\right\\},}\\\\ &{\\zeta_{\\lambda,k}\\leqslant\\operatorname*{min}\\left\\{\\cfrac{1}{\\omega},\\cfrac{2}{\\omega}\\right\\}=\\cfrac{1}{\\omega}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "As a further simplification, let us observe that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C=\\left(-2\\zeta_{v,k}\\underbrace{\\left(1+\\frac{{\\cal L}_{2}}{2}\\zeta_{v,k}\\right)}_{\\leqslant3/2}\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)+\\frac{\\zeta_{v,k}}{2}\\left(1+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\right)\\right)\\alpha_{1}^{\\frac{2}{\\psi}}}\\\\ &{\\geqslant\\frac{\\zeta_{v,k}}{2}\\left(1+5\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)\\chi\\right)\\alpha_{1}^{\\frac{2}{\\psi}}\\geqslant\\frac{\\zeta_{v,k}\\alpha_{1}^{\\frac{2}{\\psi}}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V=\\frac{\\zeta_{v,k}^{2}}{2}\\left(L_{H}+\\chi\\left(1-\\frac{\\zeta_{\\lambda,k}}{2}\\omega\\right)(L_{H}+L_{2})\\right)V_{v}+\\chi\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}}\\\\ &{\\quad\\leqslant\\frac{\\zeta_{v,k}^{2}}{2}\\left((1+2\\chi)L_{2}+(1+\\chi)\\frac{L_{1}^{2}}{\\omega}\\right)V_{v}+\\chi\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}=:\\widetilde{V}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Denoting withC \u201c: 21\u00b4 \u03c81 \u03b6\u03c5,k\u03b11 , we are going to study the recurrence: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\widetilde{P}_{k+1}(\\chi)\\leqslant\\widetilde{P}_{k}(\\chi)-\\widetilde{C}\\operatorname*{max}\\left\\{0,\\;\\widetilde{P}_{k}(\\chi)\\right\\}^{\\frac{2}{\\psi}}+\\widetilde{V}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Part V: Rates Computation ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Part $V(a)$ : Exact gradients We consider the case $\\tilde{V}=0$ . Let us start with $\\psi=2$ . From Lemma G.3, we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{P}_{K}(\\xi)\\leqslant\\left(1-\\widetilde{C}\\right)^{K}\\widetilde{P}_{0}(\\xi)\\leqslant\\epsilon}\\\\ &{\\implies K\\leqslant\\frac{\\log\\frac{\\widetilde{P}_{0}(\\xi)}{\\epsilon}}{\\log\\frac{1}{1-\\widetilde{C}}}\\leqslant\\widetilde{C}^{-1}\\log\\frac{\\widetilde{P}_{0}(\\xi)}{\\epsilon}=\\frac{2\\log\\frac{\\widetilde{P}_{0}(\\xi)}{\\epsilon}}{2^{1-\\frac{1}{\\psi}}\\zeta_{\\rho,t}\\alpha_{1}^{\\frac{2}{\\psi}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The inequality on $K$ holds under the con dritions: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{C}\\leqslant\\frac{2}{\\psi\\widetilde{P}_{0}(\\chi)_{\\psi}^{\\frac{2}{\\vartheta}-1}}\\implies\\zeta_{\\upsilon,k}\\leqslant\\frac{2^{1+\\frac{\\eta}{\\vartheta}}}{\\psi\\alpha_{1}^{\\frac{2}{\\vartheta}}\\widetilde{P}_{0}(\\chi)_{\\psi}^{\\frac{2}{\\vartheta}-1}},}\\\\ &{\\zeta_{\\upsilon,k}\\leqslant\\operatorname*{min}\\left\\{\\frac{1}{L H},\\frac{1}{L_{2}},\\frac{\\omega^{2}\\chi\\zeta_{3,k}}{(1+\\chi)\\omega\\alpha_{1}^{\\frac{\\eta}{\\vartheta}}+4L_{3}^{2}(1+7\\chi)}\\right\\}}\\\\ &{\\quad\\quad=\\operatorname*{min}\\left\\{\\frac{1}{L_{2}+\\frac{L_{1}^{2}}{\\omega}},\\frac{\\omega^{2}\\chi\\zeta_{3,k}}{(1+\\chi)\\omega\\alpha_{1}^{\\frac{\\eta}{\\vartheta}}+4L_{3}^{2}(1+7\\chi)}\\right\\},}\\\\ &{\\zeta_{\\lambda,k}\\leqslant\\frac{1}{\\omega},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the first one derives from the hypothesis of Lemma G.3 and the other two from the conditions on the learning rates derived in the previous parts. We set: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\zeta_{v,k}=\\operatorname*{min}\\left\\{\\frac{2^{1+\\frac{2}{\\psi}}}{\\psi\\alpha_{1}^{\\frac{2}{\\psi}}\\tilde{P}_{0}(\\chi)^{\\frac{2}{\\psi}-1}},\\frac{1}{L_{2}+\\frac{L_{1}^{2}}{\\omega}},\\frac{\\omega\\chi}{(1+\\chi)\\omega\\alpha_{1}^{\\frac{2}{\\psi}}+4L_{3}^{2}(1+7\\chi)}\\right\\}=\\mathcal{O}(\\omega).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, the sample co%mplexit yr becomes $\\begin{array}{r}{K=\\mathcal{O}\\left(\\omega^{-1}\\log\\frac{1}{\\epsilon}\\right)}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "Consider now $\\psi\\in[1,2)$ . We have from Lemma G.3: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\widetilde{P}_{K}(\\chi)\\leqslant\\left(\\left(\\frac{2}{\\psi}-1\\right)\\widetilde{C}K\\right)^{-\\frac{\\psi}{2-\\psi}}\\leqslant\\epsilon}}\\\\ {{\\displaystyle\\implies K\\leqslant\\frac{\\psi}{2-\\psi}\\widetilde{C}^{-1}\\epsilon^{-\\frac{2}{\\psi}+1}=\\frac{2\\psi}{\\left(2-\\psi\\right)2^{1-\\frac{1}{\\psi}}\\zeta_{v,k}\\alpha_{1}^{\\frac{2}{\\psi}}}\\epsilon^{-\\frac{2}{\\psi}+1},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "holding under the same conditions as before. With the same choices of learning rates, we obtain the sample complexity $K=\\mathcal{O}\\left(\\omega^{-1}\\epsilon^{-\\frac{2}{\\psi}+1}\\right)$ as sample complexity. ", "page_idx": 34}, {"type": "text", "text": "Part $V(b)$ : Estimated gradients We consider $\\tilde{V}>0$ . In this case, from Lemma G.5, we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\widetilde{P}_{K}(\\chi)\\leqslant\\left(1-\\widetilde{C}^{1-\\frac{\\psi}{2}}\\widetilde{V}^{\\frac{\\psi}{2}}\\right)^{K}\\widetilde{P}_{0}(\\chi)+\\left(\\frac{\\widetilde{V}}{\\widetilde{C}}\\right)^{\\frac{\\psi}{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We enforce both terms to be smaller or equal to $\\epsilon/2$ . With the first orne, we can evaluate the sample complexity: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\widetilde{V}^{1-\\frac{\\psi}{2}}\\widetilde{C}^{\\frac{\\psi}{2}}\\right)^{K}\\widetilde{P}_{0}(\\chi)\\leqslant\\frac{\\epsilon}{2}}\\\\ &{\\implies K\\leqslant\\frac{\\log\\frac{2\\widetilde{P}_{0}(\\chi)}{\\epsilon}}{\\widetilde{V}^{1-\\frac{\\psi}{2}}\\widetilde{C}^{\\frac{\\psi}{2}}}}\\\\ &{\\qquad\\qquad=\\frac{\\log\\frac{2\\widetilde{P}_{0}(\\chi)}{\\epsilon}}{\\left(\\frac{\\zeta_{*}^{2}}{2}\\frac{k}{2}\\left((1+2\\chi)L_{2}+(1+\\chi)\\frac{L_{1}^{2}}{\\omega}\\right)V_{v}+\\chi\\frac{\\omega}{2}\\zeta_{*,k}^{2}V_{\\lambda}\\right)^{1-\\frac{\\psi}{2}}\\left(2^{1-\\frac{1}{\\psi}}\\frac{\\zeta_{*,k}\\alpha_{1}^{\\frac{2}{\\psi}}}{2}\\right)^{\\frac{\\psi}{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Regarding the second one, we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left(\\frac{\\widetilde{V}}{\\widetilde{C}}\\right)^{\\frac{\\psi}{2}}\\leqslant\\frac{\\epsilon}{2}\\implies\\left(\\frac{\\frac{\\zeta_{v,k}^{2}}{2}\\left((1+2\\chi)L_{2}+(1+\\chi)\\frac{L_{1}^{2}}{\\omega}\\right)V_{v}+\\chi\\frac{\\omega}{2}\\zeta_{\\lambda,k}^{2}V_{\\lambda}}{2^{1-\\frac{1}{\\psi}\\zeta_{v,k}\\alpha_{1}^{\\frac{2}{\\psi}}}}\\right)^{\\frac{\\psi}{2}}\\leqslant\\frac{\\epsilon}{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By enforcing the relation between the two learning rates, we set $\\zeta_{\\pmb{v},k}=\\mathcal{O}(\\omega^{2}\\zeta_{\\pmb{\\lambda},k})$ . By enforcing the previous inequality, recalling that $L_{2}\\leqslant\\mathcal{O}(\\omega^{-1})$ and $V_{v}\\leqslant\\mathcal{O}(\\omega^{-2})$ , we obtain $\\zeta_{\\lambda}=\\mathcal{O}(\\omega\\epsilon^{2/\\psi})$ , from which $\\zeta_{\\pmb{v}}=\\mathcal{O}(\\omega^{3}\\epsilon^{2/\\psi})$ . Substituting these values into the sample complexity upper bound, we get (highlighting the terms possibly depending on $\\omega$ ): ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K\\leqslant\\mathcal{O}\\left(\\frac{\\log\\frac{1}{\\epsilon}}{\\left((L_{2}+\\omega^{-1})V_{v}\\zeta_{v}^{2}+\\omega\\zeta_{\\lambda}^{2}\\right)^{1-\\psi/2}\\zeta_{v}^{\\psi/2}}\\right)}\\\\ &{\\quad=\\mathcal{O}\\left(\\frac{\\log\\frac{1}{\\epsilon}}{\\left((L_{2}+\\omega^{-1})V_{v}\\left(\\omega^{3}\\epsilon^{2/\\psi}\\right)^{2}+\\omega\\left(\\omega\\epsilon^{2/\\psi}\\right)^{2}\\right)^{1-\\psi/2}\\left(\\omega^{3}\\epsilon^{2/\\psi}\\right)^{\\psi/2}}\\right)}\\\\ &{\\quad\\leqslant\\mathcal{O}\\left(\\frac{\\log\\frac{1}{\\epsilon}}{\\omega^{3}\\epsilon^{4/\\psi-1}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "having bounded the sum at the denominator with the second addendum. ", "page_idx": 34}, {"type": "text", "text": "F Technical Lemmas ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Lemma F.1. Let $a\\in\\mathbb{R},\\,b\\geqslant0$ , and $\\psi\\in[1,2]$ . It holds that: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{0,a\\}^{\\frac{2}{\\psi}}\\geqslant2^{1-\\frac{2}{\\psi}}\\operatorname*{max}\\{0,a+b\\}^{\\frac{2}{\\psi}}-b^{\\frac{2}{\\psi}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Let us consider the following derivation: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{max}\\{0,a\\}^{\\frac{3}{2}}=\\left\\{a^{\\frac{5}{2}}\\begin{array}{l l}{4a\\geq9}&{\\mathrm{if~}a>0}\\\\ {0\\quad\\mathrm{of~herwise}}\\\\ {0\\quad\\mathrm{of~}a<0}\\end{array}\\right.}\\\\ {\\geq\\left\\{\\begin{array}{l l}{2^{-2}\\big(a+b\\big)^{\\frac{5}{2}}-b^{2}}&{\\mathrm{if~}a>0}\\\\ {0\\quad\\mathrm{of~herwise}}\\\\ {0\\quad\\mathrm{of~}a<b<0}\\end{array}\\right.}\\\\ {=\\left\\{\\begin{array}{l l}{2^{-1}\\big(a+b\\big)^{\\frac{5}{2}}-b^{2}}&{\\mathrm{if~}a>0}\\\\ {0\\quad\\mathrm{if~}a<b<a\\leq0}\\\\ {0\\quad\\mathrm{of~herwise}}\\end{array}\\right.}\\\\ {\\geq\\left\\{\\begin{array}{l l}{2^{-\\frac{5}{2}}a\\cdot(a+b)^{\\frac{5}{2}}-b^{2}}&{\\mathrm{if~}a>b}\\\\ {2^{-\\frac{5}{2}}a\\cdot(a+b)^{\\frac{5}{2}}-b^{2}}&{\\mathrm{if~}a<b<a}\\\\ {-b^{\\frac{5}{2}}(a+b)^{\\frac{5}{2}}-b^{2}}&{\\mathrm{if~}a<b<0}\\end{array}\\right.}\\\\ {=\\left\\{\\begin{array}{l l}{2^{-\\frac{5}{2}}a\\cdot(b+b)^{\\frac{5}{2}}-b^{2}}&{\\mathrm{if~}a>b>0}\\\\ {-b^{\\frac{5}{2}}}&{\\mathrm{otherwise}}\\end{array}\\right.}\\\\ {=2^{-\\frac{5}{2}}a\\operatorname*{max}\\{0,a,b\\}^{\\frac{5}{2}}-b^{\\frac{5}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the first inequality follows from $(x+y)^{\\frac{2}{\\psi}}\\leqslant2^{\\frac{2}{\\psi}-1}(x^{\\frac{2}{\\psi}}+y^{\\frac{2}{\\psi}})$ for $x,y\\geqslant0$ , from Holder\u2019s inequality; the second inequality from observing that $2^{1-\\frac{2}{\\psi}}(a+b)^{\\frac{2}{\\psi}}-b^{\\frac{2}{\\psi}}\\leqslant(2^{1-\\frac{2}{\\psi}}-1)b^{\\frac{2}{\\psi}}\\leqslant0$ for $-b<a\\leqslant0$ . \u53e3 ", "page_idx": 35}, {"type": "text", "text": "G Recurrences ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section, we provide auxiliary results about convergence rate of a certain class of recurrences that will be employed for the convergence analysis of the proposed algorithms. Specifically, we study the recurrence: ", "page_idx": 35}, {"type": "equation", "text": "$$\nr_{k+1}\\leqslant r_{k}-a\\operatorname*{max}\\{0,r_{k}\\}^{\\phi}+b\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for $a>0,b\\geqslant0$ , and $\\phi\\in[1,2]$ . To this end, we consider the helper sequence: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\rho_{0}=r_{0}}\\\\ {\\rho_{k+1}=\\rho_{k}-a\\operatorname*{max}\\{0,\\rho_{k}\\}^{\\phi}+b}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The line of the proof follows that of Montenegro et al. (2024). Let us start showing that for sufficiently small $a$ , the sequence $\\rho_{k}$ upper bounds $r_{k}$ . ", "page_idx": 35}, {"type": "text", "text": "Lemma G.1. If $\\begin{array}{r}{a\\leqslant\\frac{1}{\\phi\\rho_{k}^{\\phi-1}}}\\end{array}$ for every $k\\geqslant0,$ , then, $r_{k}\\leqslant\\rho_{k}$ for every $k\\geqslant0$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. By induction on $k$ . For $k\\,=\\,0$ , the statement holds since $\\rho_{0}=r_{0}$ . Suppose the statement holds for every $j\\leqslant k$ , we prove that it holds for $k+1$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\rho_{k+1}=\\rho_{k}-a\\operatorname*{max}\\{0,\\rho_{k}\\}^{\\phi}+b}&{}\\\\ {\\geqslant r_{k}-a\\operatorname*{max}\\{0,r_{k}\\}^{\\phi}+b}&{}\\\\ {\\geqslant r_{k+1},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the first inequality holds by the inductive hypothesis and by observing that the function $f(x)=x-a\\operatorname*{max}\\{0,x\\}^{\\flat}$ is non-decreasing in $x$ when \u03d5\u03c1k\u03d51\u00b41. Indeed, if x \u0103 0, then fpxq \u201c x, which is non-decreasing; if $x\\geqslant0$ , we have $f(x)=x-a x^{\\phi}$ , that can be proved to be non-decreasing ", "page_idx": 35}, {"type": "text", "text": "in the interval $\\left[0,\\left(a\\phi\\right)^{-{\\frac{1}{\\phi-1}}}\\right]$ simply by studying the sign of the derivative. Thus, we enforce the following requirement to ensure that $\\rho_{k}$ falls in the non-decreasing region: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\rho_{k}\\leqslant(a\\phi)^{-\\frac{1}{\\phi-1}}\\implies a\\leqslant\\frac{1}{\\phi\\rho_{k}^{\\phi-1}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "So does $r_{k}$ by the inductive hypothesis. ", "page_idx": 36}, {"type": "text", "text": "Thus, from now on, we study the properties of the sequence $\\rho_{k}$ . Let us note that, if $\\rho_{k}$ is convergent, then it converges to the fixed-point $\\overline{{\\rho}}$ computed as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\overline{{\\rho}}=\\overline{{\\rho}}-a\\operatorname*{max}\\{0,\\overline{{\\rho}}\\}^{\\phi}+b\\implies\\overline{{\\rho}}=\\left(\\frac{b}{a}\\right)^{\\frac{1}{\\phi}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "having retained the positive solution of the equation only, since the negative one never attains the maximum $\\operatorname*{max}\\{0,\\overline{{\\rho}}\\}$ . Let us now study the monotonicity properties of the sequence $\\rho_{k}$ . ", "page_idx": 36}, {"type": "text", "text": "Lemma G.2. The following statements hold: ", "page_idx": 36}, {"type": "text", "text": "Proof. The proof is analogous to that of (Montenegro et al., 2024, Lemma F.3). ", "page_idx": 36}, {"type": "text", "text": "From now on, we focus on the case in which $r_{0}\\geqslant{\\overline{{\\rho}}}$ , since, as we shall see later, the opposite case is irrelevant for the convergence guarantees. We now consider two cases: $b=0$ and $b>0$ . ", "page_idx": 36}, {"type": "text", "text": "G.1 Analysis when $b=0$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "From the policy optimization perspective, this case corresponds to the one in which the gradients are exact (no variance). Recall that here $\\overline{{\\rho}}=0$ . We have the following convergence result. ", "page_idx": 36}, {"type": "text", "text": "Lemma G.3. If $\\begin{array}{r}{a\\leqslant\\frac{1}{\\phi r_{0}^{\\phi-1}}}\\end{array}$ \u03d5r\u03d51\u00b41, r0 \u011b 0, and b \u201c 0 it holds that: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\rho_{k+1}\\leqslant\\left\\{\\begin{array}{l l}{(1-a)^{k+1}r_{0}}&{i f\\,\\phi=1}\\\\ {\\operatorname*{min}\\left\\{r_{0},((\\phi-1)a(k+1))^{-\\frac{1}{\\phi-1}}\\right\\}}&{i f\\,\\phi\\in(1,2]}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Since $r_{0}\\geqslant0=\\overline{{\\rho}}$ , from Lemma G.2, we know that $\\rho_{k}\\geqslant0$ and, thus, $\\operatorname*{max}\\{0,\\rho_{k}\\}=\\rho_{k}$ . For $\\phi=1$ , we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\rho_{k+1}=\\rho_{k}-a\\rho_{k}=(1-a)\\rho_{k}=(1-a)^{k+1}\\rho_{0}=(1-a)^{k+1}r_{0}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For $\\phi\\in(1,2]$ , we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\rho_{k+1}=\\rho_{k}-a\\rho_{k}^{\\phi}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We proceed by induction. For $k\\,=\\,0$ , the statement hold since $\\rho_{0}~=~r_{0}$ and $r_{0}\\,\\leqslant\\,(\\phi a)^{-{\\frac{1}{\\psi-1}}}\\,\\leqslant$ $\\left((\\phi-1)a\\right)^{-{\\frac{1}{\\psi-1}}}$ from the condition on the learning rate. Suppose the thesis holds for $j\\leqslant k$ , we prove it for $k+1$ . $\\rho_{k+1}\\leqslant r_{0}$ by monotonicity, and, from the inductive hypothesis: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{k+1}=\\rho_{k}-a\\rho_{k}^{\\phi}\\leqslant(\\phi a k)^{-\\frac{1}{\\phi-1}}-a(\\phi a k)^{-\\frac{\\phi}{\\phi-1}}}\\\\ &{\\qquad\\qquad\\qquad=\\underbrace{(\\phi a k)^{-\\frac{1}{\\phi-1}}-(\\phi a(k+1))^{-\\frac{1}{\\phi-1}}-a(\\phi a k)^{-\\frac{\\phi}{\\phi-1}}}_{(*)}+(\\phi a(k+1))^{-\\frac{1}{\\phi-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We now prove that $\\left(*\\right)$ is non-positive: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(*)=((\\phi-1)a k)^{-\\frac{1}{\\phi-1}}-((\\phi-1)a(k+1))^{-\\frac{1}{\\phi-1}}-a((\\phi-1)a k)^{-\\frac{\\phi}{\\phi-1}}}\\\\ &{\\quad=((\\phi-1)a)^{-\\frac{1}{\\phi-1}}k^{-\\frac{\\phi}{\\phi-1}}\\underbrace{\\left(k-(k+1)\\left(\\displaystyle\\frac{k}{k+1}\\right)^{\\frac{\\phi}{\\phi-1}}\\right)}_{\\leqslant\\frac{1}{\\phi-1}}-a^{-\\frac{1}{\\phi-1}}((\\phi-1)k)^{-\\frac{\\phi}{\\phi-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\leqslant a^{-\\frac{1}{\\phi-1}}k^{-\\frac{\\phi}{\\phi-1}}(\\phi-1)^{-\\frac{1}{\\phi-1}}\\left(\\frac{1}{\\phi-1}-\\frac{1}{\\phi-1}\\right)\\leqslant0,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "having observed that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{k\\geqslant1}\\left(k-(k+1)\\left(\\frac{k}{k+1}\\right)^{\\frac{\\phi}{\\phi-1}}\\right)=\\operatorname*{lim}_{k\\to+\\infty}\\left(k-(k+1)\\left(\\frac{k}{k+1}\\right)^{\\frac{\\phi}{\\phi-1}}\\right)=\\frac{1}{\\phi-1}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "G.2 Analysis for $b>0$ ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "From the policy optimization perspective, this corresponds to the case in which the gradients are estimated, i.e., the variance is positive. In this case, we proceed considering the helper sequence: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\{\\!\\!\\begin{array}{l}{{\\eta_{0}=\\rho_{0}}}\\\\ {{\\eta_{k+1}=\\left(1-a\\overline{{{\\rho}}}^{\\phi-1}\\right)\\eta_{k}+b}}\\end{array}\\right.\\mathrm{if~}k\\geqslant0}\\,\\cdot\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We show that the sequence $\\eta_{k}$ upper bounds $\\rho_{k}$ when $\\rho_{0}=r_{0}\\geqslant\\overline{{\\rho}}$ . ", "page_idx": 37}, {"type": "text", "text": "Lemma G.4. If $r_{0}>\\overline{{\\rho}}$ and \u03d5r0\u03d51\u00b41, then, for every k \u011b 0, it holds that \u03b7k \u011b \u03c1k. ", "page_idx": 37}, {"type": "text", "text": "Proof. The proof is analogous to that of (Montenegro et al., 2024, Lemma F.4). ", "page_idx": 37}, {"type": "text", "text": "Thus, we can provide the convergence guarantee. ", "page_idx": 37}, {"type": "text", "text": "Lemma G.5. If $\\begin{array}{r}{a\\leqslant\\frac{1}{\\phi r_{0}^{\\phi-1}}}\\end{array}$ \u03d5r\u03d51\u00b41, r0 \u011b 0, and b \u0105 0 it holds that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\eta_{k+1}\\leqslant\\left(1-b^{1-\\frac{1}{\\phi}}a^{\\frac{1}{\\phi}}\\right)^{k+1}+\\left(\\frac{b}{a}\\right)^{\\frac{1}{\\phi}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. By unrolling the recursion: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{k+1}=\\left(1-a\\overline{{\\rho}}^{\\delta-1}\\right)\\eta_{k}+b}\\\\ &{\\hphantom{\\eta_{k+1}}=\\left(1-a\\overline{{\\rho}}^{\\delta-1}\\right)^{k+1}{\\boldsymbol{r}}_{0}+b\\sum_{j=0}^{k}\\left(1-a\\overline{{\\rho}}^{\\delta-1}\\right)^{j}}\\\\ &{\\hphantom{\\eta_{k+1}}\\leqslant\\left(1-a\\overline{{\\rho}}^{\\delta-1}\\right)^{k+1}{\\boldsymbol{r}}_{0}+b\\sum_{j=0}^{\\infty}\\left(1-a\\overline{{\\rho}}^{\\delta-1}\\right)^{j}}\\\\ &{\\hphantom{\\eta_{k+1}}=\\left(1-b^{1-\\frac{1}{\\delta}}a^{\\frac{1}{\\delta}}\\right)^{k+1}+\\cfrac{b}{a\\overline{{\\rho}}^{\\delta-1}}}\\\\ &{\\hphantom{\\eta_{k+1}}=\\left(1-b^{1-\\frac{1}{\\delta}}a^{\\frac{1}{\\delta}}\\right)^{k+1}+\\cfrac{b}{a\\overline{{\\rho}}^{\\delta-1}}}\\\\ &{\\hphantom{\\eta_{k+1}}=\\left(1-b^{1-\\frac{1}{\\delta}}a^{\\frac{1}{\\delta}}\\right)^{k+1}+\\left(\\frac{b}{a}\\right)^{\\frac{1}{\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "H Experimental Details and Additional Results ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "H.1 Experimental Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "H.1.1 Employed Policies and Hyperpolicies ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Linear Gaussian Policy. A linear parametric gaussian policy $\\pi_{\\theta}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathcal{A})$ with variance $\\sigma^{2}$ samples the actions as $a_{t}\\sim\\mathcal{N}(\\pmb{\\theta}^{\\top}\\pmb{s}_{t},\\sigma^{2}I_{d_{S}})$ , where $s_{t}$ is the observed state at time $t$ and $\\pmb{\\theta}$ is the parameter vector. ", "page_idx": 37}, {"type": "text", "text": "Tabular Softmax Policy. A tabular softmax policy $\\pi_{\\pmb{\\theta}}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathcal{A})$ with a temperature constant $\\tau$ is such that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pi_{\\theta}(a_{j}|s_{i})=\\frac{\\exp{\\left(\\frac{\\theta_{i,j}}{\\tau}\\right)}}{\\sum_{z=1}^{|A|}\\exp{\\left(\\frac{\\theta_{i,z}}{\\tau}\\right)}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\theta_{i,j}$ is the parameter associated with the $i$ -th state and the $j$ -th action. Notice that the total number of parameters for this kind of policy is $|{\\mathcal{S}}||{\\mathcal{A}}|$ . ", "page_idx": 38}, {"type": "text", "text": "Linear Deterministic Policy. A linear parametric deterministic policy $\\mu\\pmb{\\theta}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathcal{A}$ samples the actions as $\\pmb{a}_{t}=\\pmb{\\theta}^{\\top}\\pmb{s}_{t}$ , where $\\scriptstyle{s_{t}}$ is the observed state at time $t$ and $\\pmb{\\theta}$ is the parameter vector. ", "page_idx": 38}, {"type": "text", "text": "Gaussian Hyperpolicy. A parametric gaussian hyperpolicy $\\nu_{\\rho}:\\mathcal{R}\\to\\Delta(\\Theta)$ with variance $\\sigma^{2}$ samples the parameters $\\pmb{\\theta}$ for the underlying generic parametric policy $\\pi_{\\theta}$ as $\\pmb{\\theta}_{t}\\sim\\mathcal{N}(\\pmb{\\rho},\\sigma^{2}I_{d_{\\mathcal{R}}})$ , where $\\rho$ is the parameter vector for the hyperpolicy. ", "page_idx": 38}, {"type": "text", "text": "H.1.2 Environments ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Discrete Grid World with Walls. Discrete Grid World with Walls (DGWW) is a simple discrete environment we employed to compare C-PGAE against the sample-based versions of NPG-PD (Ding et al., 2020, Appendix H) and RPG-PD (Ding et al., 2024, Appendix C.9). DGWW is a grid-like bidimensional environment in which an agent can assume only integer coordinate positions and in which an agent can play four actions stating whether to go up, right, left, or down. The goal is to reach the center of the grid performing the minimum amount of steps, begin the initial state uniformly sampled among the four vertices of the grid. The agent is rewarded negatively and proportionally to its distance from the center, where the reward is 0. Around the goal state there is a \u201cU-shaped\u201d obstacle with an opening on the top side. In particular, when the agent lands in a state in which the wall is present, it receives a cost of 1, otherwise the cost signal is always equal to 0. In our experiments, we employed a DGWW environment of such a kind, with $\\vert{\\cal S}\\vert=49$ , i.e., with each dimension with length equal to 7. ", "page_idx": 38}, {"type": "text", "text": "Linear Quadratic Regulator with Costs. The Linear Quadratic Regulator (LQR, Anderson and Moore, 2007) is a continuous environment we employed in the regularization sensitivity study of C-PGAE and C-PGPE, and in the comparison among the same algorithms against the sample-based version of NPG-PD2 (Ding et al., 2022, Algorithm 1) and its ridge-regularized version RPG-PD2 (not provided by the authors, but designed by us). LQR is a dynamical system governed by the following state evolution: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{s}_{t+1}=A\\pmb{s}_{t}+B\\pmb{a}_{t},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $A\\in\\mathbb{R}^{d_{S}\\times d_{S}}$ and $B\\in\\mathbb{R}^{d_{S}\\times d_{A}}$ . ", "page_idx": 38}, {"type": "text", "text": "In the standard version of the environment, the reward is computed at each step as: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r_{t}=-s_{t}^{\\top}R\\pmb{s}_{t}-\\pmb{a}_{t}^{\\top}Q\\pmb{a}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $R\\in\\mathbb{R}^{d_{S}\\times d_{S}}$ and $Q\\in\\mathbb{R}^{d_{A}\\times d_{A}}$ . ", "page_idx": 38}, {"type": "text", "text": "We modified this version of the LQR environment introducing costs. In particular, in our CostLQR, the state evolution is treated as in the original case, while the reward at step $t$ is computed as: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r_{t}=-s_{t}^{\\top}R s_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $R\\in\\mathbb{R}^{d_{S}\\times d_{S}}$ . Moreover, we added a cost signal $c$ which is computed as follows at every time step $t$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\nc_{t}=\\pmb{a}_{t}^{\\top}Q\\pmb{a}_{t},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where Q P RdA\u02c6dA. ", "page_idx": 38}, {"type": "text", "text": "In our experiments, we consider a CostLQR environment whose main characteristics are reported in Table 5. ", "page_idx": 38}, {"type": "text", "text": "Additionally, we considered a uniform initial state distribution in $[-3,3]$ and the following matrices: ", "page_idx": 38}, {"type": "equation", "text": "$$\nA=B=0.9\\left[\\!\\!1{\\begin{array}{c c}{0}\\\\ {1}\\end{array}}\\!\\!1\\!\\!\\right],\\qquad Q=\\left[\\!\\!{\\begin{array}{c c}{0.9}&{0}\\\\ {0}&{0.1}\\end{array}}\\!\\!\\right],\\qquad R=\\left[\\!\\!{\\begin{array}{c c}{0.1}&{0}\\\\ {0}&{0.9}\\end{array}}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "table", "img_path": "2vywag2lVC/tmp/f09194a0eccda4ca2b610d15b4d610b9e539f22763b01eab3d1b4c27f3b17c86.jpg", "table_caption": ["Table 5: Main features of CostLQR, Swimmer- $\\cdot\\nu4.$ , and Hopper- $\\nu4$ . "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "MuJoCo with Costs. For our experiments on risk minimization, we utilized environments from the MuJoCo control suite (Todorov et al., 2012), which offers a variety of continuous control environments. To tailor these environments to our specific requirements, we introduced a cost function that represents the energy associated with the control actions. In standard MuJoCo environments, a portion of the reward is typically calculated as the cost of the control action, which is proportional to the deviation of the chosen action from predefined action bounds. In our MuJoCo modification, at each time step we make the environment return a cost computed as: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|\\boldsymbol{a}_{t}-\\operatorname*{min}\\left\\{\\operatorname*{max}\\left\\{\\boldsymbol{a}_{t},\\boldsymbol{a}_{\\operatorname*{min}}\\right\\},\\boldsymbol{a}_{\\operatorname*{max}}\\right\\}\\right\\|_{2},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $a_{\\mathrm{{min}}}$ and $a_{\\mathrm{max}}$ are respectively the bounds for the minimum and maximum value for each component of the action vector. Then, the action $\\operatorname*{min}\\left\\{\\operatorname*{max}\\left\\{a_{t},a_{\\operatorname*{min}}\\right\\},a_{\\operatorname*{max}}\\right\\}$ is passed to the environment. In our experiment we consider Swimmer- $\\cdot\\nu4$ and Hopper- $\\cdot\\nu4$ MuJoCo environments, whose main features are summarized in Table 5. ", "page_idx": 39}, {"type": "text", "text": "H.2 Details for the comparison against the baselines in DGWW ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In Section 5, we compare our proposal C-PGAE against the sample-based versions of NPG-PD (Ding et al., 2020, Appendix H) and RPG-PD (Ding et al., 2024, Appendix C9). The environment in which the methods are tested is the Discrete Grid World with Walls (DGWW, see Appendix $\\mathrm{H}$ ) with a horizon of $T\\,=\\,100$ . In this experiment, the methods aim at learning the parameters of a tabular softmax policy with 196 parameters, maximizing the trajectory reward while considering a single constraint on the average trajectory cost, for which we set a threshold $b=0.2$ . All the methods were run for $K=3000$ iterations with a batch size of $N=10$ trajectories per iteration, and with constant learning rates. In particular, for both C-PGAE and NPG-PD, we employed $\\zeta_{\\pmb\\theta}=0.01$ and $\\zeta_{\\lambda}=0.1$ , while for RPG-PD we selected $\\zeta_{\\pmb\\theta}\\,=\\,0.01$ and $\\zeta_{\\lambda}\\,=\\,0.01$ . For C-PGAE and RPG-PD we used a regularization constant $\\omega=10^{-4}$ . All the details about the experimental setting are summarized in Table 6. We would like to stress that, as prescribed by the respective convergence theorems, we chose a two time-scales learning rate approach for C-PGAE and a single time-scale one for RPG-PD. Figure 2a shows the performance curves (i.e., the one associated with the objective function and the cost ones. As can be noticed, C-PGAE manages to strike the objective of the constrained optimization problem with less trajectories. Indeed, the sample-based NPG-PD requires to estimate the value and the action-value functions for all the states and state-action pairs, resulting in analyzing $|S|+|S||A|$ additional trajectories w.r.t. C-PGAE for every iteration of the algorithm. The sample-based RPG-PD also requires additional trajectories to be analyzed, which in practice, for a correct learning behavior, result to be the same in number to the extra ones analyzed by NPG-PD. ", "page_idx": 39}, {"type": "text", "text": "H.3 Details for the comparison against baselines in CostLQR ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In Section 5, we compare our proposals C-PGAE and C-PGPE against the continuous sample-based version of NPG-PD (Ding et al., 2022, Algorithm 1) with works with generic policy parameterizations. In the following, we refer to this version of NPG-PD as NPG-PD2. Moreover, we added a ridgeregularized version of NPG-PD2, that we call RPG-PD2, to resemble the type of regularization we employed for our proposed methods. For all the regularized methods (i.e., C-PGAE, C-PGPE, and RPG-PD2) we selected as regularization constant $\\omega=10^{-4}$ . The setting for this experiment considers a bidimensional LQR environment with a single cost over the provided actions (see Appendix H) and with a fixed horizon $T=50$ . Here, the methods aim at maximizing the average reward over trajectories, while keeping the average cost over trajectories under the threshold $b=0.9$ . In particular, C-PGAE learns the parameters of a linear gaussian policy with a variance $\\sigma_{\\mathrm{A}}^{2}=10^{-3}$ and employing a learning rate schedule governed by the Adam scheduler (Kingma and Ba, 2015) with $\\zeta_{\\theta,0}=0.001$ and $\\zeta_{\\lambda,0}=0.01$ . C-PGPE learns the parameters of a gaussian hyperpolicy, with a variance $\\sigma_{\\mathrm{P}}^{2}=10^{-3}$ , which samples the parameters of a deterministic linear policy. It employs a learning rate schedule governed by Adam too with $\\zeta_{\\rho,0}\\,=\\,0.001$ and $\\zeta_{\\lambda,0}\\,=\\,0.01$ . Both C-PGAE and C-PGPE were run for $K\\,=\\,6000$ iterations with a batch of $N=100$ trajectories per iteration. NPG-PD2 and RPGPD2 are both actor-critic methods which were run for $K\\,=\\,1000$ iterations with a batch size of $N=600$ trajectories per iteration. In particular, among the trajectories of the reported batch size, $N_{1}=500$ were used for the inner critic-loop, while $N_{2}=100$ for performance and cost estimations. The inner loop step size was selected constant, as prescribed by the original algorithm, and with a value $\\alpha=10^{\\circ-5}$ . Furthermore, since such methods were designed for infinite-horizon discounted environments, we tested them on the same LQR as for C-PGAE and C-PGPE, but leaving $T=+\\infty$ and $\\gamma\\,=\\,0.98$ (the effective horizon is $(1-\\bar{\\gamma})^{-1}\\,=\\,50)$ ). The step sizes for the primal and dual variables updates were governed by Adam with $\\zeta_{\\theta,0}=0.003$ and $\\zeta_{\\lambda,0}=0.01$ . As for C-PGAE, both NPG-PD2 and RPG-PD2 aimed at learning the parameters of a linear gaussian policy, with variance $\\sigma_{\\mathrm{A}}^{2}=10^{-3}$ . All the details about this experiment are summarized in Table 7. Figure 2b reports the learning curves for the average return and the cost over trajectories. As can be seen, our methods manage to solve the constrained optimization problem at hand by leveraging on less trajectories. Indeed, NPG-PD2 and RPG-PD2 suffer the inner critic loop, which add additional trajectories to be analyzed per iteration (in this specific case $N_{1}=500)$ ). We would like to stress that the actor-critic methods were very sensible to the hyperparameters selection, especially the length and the step size of the inner loop. ", "page_idx": 39}, {"type": "table", "img_path": "2vywag2lVC/tmp/71cd35f78da11febeac5529833a34f3b5f588acb06f20f4107493475b76f96e3.jpg", "table_caption": ["Table 6: Details for the comparison of C-PGAE against NPG-PD and RPG-PD in DGWW. "], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "H.4 Details for the risk-constrained experiment on Swimmer-v4 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In Section 5, we presented an experiment comparing what happens when considering the two exploration approaches of C-PGAE and C-PGPE and on different risk-constrained optimization problems on the cost version of Swimmer- $\\cdot\\nu4$ (details in Appendix H). In particular, we considered such an environment with a horizon of $T\\,=\\,100$ . Both the algorithms were run for $K\\,=\\,3000$ iterations, with a batch size of $N=100$ trajectories per iteration, and with step sizes governed by the Adam scheduler. Moreover, they had a regularization constant $\\omega=10^{-4}$ . ", "page_idx": 40}, {"type": "text", "text": "C-PGAE employed a linear gaussian policy with variance $\\sigma_{\\mathrm{A}}^{2}=1$ . In Table 8, we list the characteristics for all the experiments with all the risk measures. ", "page_idx": 40}, {"type": "text", "text": "On the other hand, C-PGPE employed a gaussian hyperpolicy with variance $\\sigma_{\\mathrm{P}}^{2}\\,=\\,0.01$ . Such an hyperpolicy sampled parameters for an underlying linear deterministic policy. In Table 9, we list the characteristics for all the experiments with all the risk measures. ", "page_idx": 40}, {"type": "text", "text": "As also highlighted in the main paper, C-PGPE delivers an hyperpolicy which samples (after having sampled the parameters for the underlying policy) actions whose costs are always under the fixed threshold. Moreover, by considering risk measures different from the average cost one, the empirical distribution of costs shows that these are way lighter w.r.t. the ones associated with the deployment of an hyperpolicy learned considering average cost constraints. C-PGAE also shows results of this kind. However, the displacement between the final empirical cost distributions is not as marked as the one shown with C-PGPE. The exception is from the MV risk measure side, which seems to make C-PGAE learn a policy able to pay lighter costs, but resulting in poor-performing policy. All the other found (hyper)policies, instead, provide similar performance scores. ", "page_idx": 40}, {"type": "table", "img_path": "2vywag2lVC/tmp/b0465e58f06701493bef3341288e7216e07d33121f83a6cbe06180fdb35ff747.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "2vywag2lVC/tmp/3233cfe87d89b14dd5501191122d5da1e03fe7ea16528f6d7da56873d3873b9c.jpg", "table_caption": ["Table 7: Details for the comparison of C-PGPE and C-PGAE against NPG-PD2 and RPG-PD2 in a bodomensional CostLQR. ", "Table 8: Parameters of the risk measures employed in the experiment on Swimmer-v4 with C-PGAE. "], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "2vywag2lVC/tmp/6161b562bc159c5f605b5dc2ba67222aac198313551e0d861b7a236ba680df4f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "H.5 Risk-constrained experiment on Hopper-v4 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Here, we present a similar experiment to the one shown on Swimmer- $\\cdot\\nu4$ . Also in this case, we compare what happens when considering the two exploration approaches of C-PGAE and C-PGPE and on different risk-constrained optimization problems, but on the cost version of Hopper- $\\cdot\\nu4$ (details in Appendix H). The experimental setting is quite the same of the one considered above (i.e., $T=100$ , $K=3000$ , $N=100$ , $\\omega=10^{-4}$ ). ", "page_idx": 42}, {"type": "text", "text": "C-PGAE employed a linear gaussian policy with variance $\\sigma_{\\mathrm{A}}^{2}=1$ . The characteristics for all the risk measures\u2019 experiments are presented in Table 10. For the learning rates schedules, we employed the Adam scheduler. ", "page_idx": 42}, {"type": "table", "img_path": "2vywag2lVC/tmp/3cdce881cc78ccfdd6d079e3506d0edbd82cf57ba4c4023a005b65f78c776eb5.jpg", "table_caption": ["Table 10: Parameters of the risk measures employed in the experiment on Hopper-v4 with C-PGAE. "], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "C-PGPE employed a gaussian hyperpolicy with variance $\\sigma_{\\mathrm{P}}^{2}=0.1$ over an underlying linear deterministic policy. The characteristics for all the experiments with all the risk measures are summarized in Table 11. For the learning rates schedules, we employed the Adam scheduler. ", "page_idx": 42}, {"type": "table", "img_path": "2vywag2lVC/tmp/b5725a5feacf1aa447c7fabc6bc194981346c3aa0aeb05bab6a7f94942b3b74c.jpg", "table_caption": ["Table 11: Parameters of the risk measures employed in the experiment on Hopper-v4 with C-PGPE. "], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "Figure 5 shows the empirical distributions of costs over 100 trajectories of the learned (hyper)policies via C-PGPE and C-PGAE. C-PGPE shows a behavior that is quite the same as the one shown in the Swimmer-v4 environment. Indeed, by considering constraints on the $\\mathrm{CVaR}_{\\alpha}$ , the MV, or on the Chance, the learned hyperpolicy selects parameters for the underlying policy selecting actions that pay a way lighter cost w.r.t. the ones provided by an hyperpolicy learned by considering constraints on average costs. It is worth noticing that the lightest costs are observed when considering the $\\mathrm{CVaR}_{\\alpha}$ or the MV risk measures. In this case, C-PGAE shows a similar behavior to C-PGPE for what concern the distance in costs between the ones observed by learning considering constraints on the average cost and the ones observed by learning with other risk measures. The lightest costs here can be observed by learning with chance constraints, while, by considering constraints on the MV, the observed costs exceed the ones observed under average cost constraints. For what concern the performances of the learned (hyper)policies (see Table 4c, C-PGPE exhibit high-performing hyperpolicies under average cost or chance constraints, while the ones under $\\mathrm{CVaR}_{\\alpha}$ and MV shows similar (low) performance scores. On the other hand, for C-PGAE the learned policy under $\\mathrm{CVaR}_{\\alpha}$ exhibits the same goodperforming behavior as the one of the policy learned under average cost constraints. ", "page_idx": 42}, {"type": "text", "text": "H.6 Regularization Sensitivity Study ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Here, we study the sensitivity of C-PGAE and C-PGPE w.r.t. the regularization term $\\omega$ . We tried the algorithms on a bidimensional LQR environment which has been modified to output a cost signal based on the selected action. For the environment at hand, we considered a horizon $T=50$ . We run both the algorithms for $K=10^{4}$ iterations, with a batch size $N=100$ trajectories per iteration, and with a varying regularization term such that $\\omega\\in\\{0,10^{-4},10^{-2}\\}$ . We considered a single constraint on the average trajectory cost, for which we set a threshold $b=0.2$ . For the step size schedules, we employed Adam (Kingma and Ba, 2015) with initial rates $\\zeta_{\\pmb\\rho,0}=\\zeta_{\\pmb\\theta,0}=10^{-3}$ and $\\zeta_{\\lambda,0}=10^{-2}$ . Moreover, in this specific experiment C-PGAE employed a linear gaussian policy with a variance $\\sigma_{\\mathrm{{a}}}^{2}\\,=\\,10^{-3}$ . On the other hand, C-PGPE employed a linear gaussian hyperpolicy with a variance $\\sigma_{\\mathrm{P}}^{2}\\,=\\,10^{-3}$ over a linear deterministic policy. The experimental environment is summarized in Table 12. Figures 7 and 6 show the Lagrangian curves, the performance ones (i.e., the one associated with the objective function), and the cost-related ones. From the shown curves it is possible to notice that, for both C-PGAE and C-PGPE, a higher regularization $\\langle\\omega=10^{-2}$ ) corresponds to a higher bias w.r.t. the constraint satisfaction. This bias is compliant with what shown by Theorem 3.1, indeed, the higher the regularization, the higher the constraint threshold should be made stricter. Finally, we report in Figure 8 the evolution of the values of the Lagrangian multipliers $\\lambda$ during the learning. As expected from the theory, for both C-PGAE and C-PGPE a higher regularization leads to have smaller values of $\\lambda$ . Moreover, we empirically notice that C-PGAE reaches higher value of $\\lambda$ w.r.t. the ones seen by C-PGPE. ", "page_idx": 42}, {"type": "table", "img_path": "2vywag2lVC/tmp/9dff4f65478982c5813a90f15b3db659987729a6525a3970688fb085c689e2bf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "2vywag2lVC/tmp/2a374c981b9f4941dc740e25deb2b89f1375addee432195214c501e0636a3ff9.jpg", "img_caption": ["(c) Avg. Ret. (5 runs, mean $\\pm$ std). ", "Table 12: Details for the reguarization sensitivity study of C-PGPE and C-PGAE in a bodomensional CostLQR. "], "img_footnote": [], "page_idx": 43}, {"type": "table", "img_path": "2vywag2lVC/tmp/e132498053f5e05048c8c7117dd3028304b60a607c6fd634af69369d72ff8579.jpg", "table_caption": ["Figure 5: Cost distributions with (hyper)policies learned considering different risk measures (5 runs). "], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "C-PGPE: Lagrangian Curve ", "text_level": 1, "page_idx": 44}, {"type": "image", "img_path": "2vywag2lVC/tmp/5f7835d358ca783bc7ab1c80e437a18a0ed28a830a06d2509f118aaaac388ff2.jpg", "img_caption": ["", "Iterations "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "2vywag2lVC/tmp/f7ee8dbf0d110f7141fee6bc07c48b0b55d8007a1b15fb77dda0a583cb0b067a.jpg", "img_caption": ["Figure 6: Lagrangian, performance and cost curves for C-PGPE over CostLQR with regularization values $\\omega\\in\\{\\bar{0},10^{\\bar{-}4},1\\bar{0}^{-2}\\}$ (5 runs, mean \u02d8 $95\\%$ C.I.). "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "C-PGAE: Lagrangian Curve ", "text_level": 1, "page_idx": 45}, {"type": "image", "img_path": "2vywag2lVC/tmp/dd300cb18543142369f5901a79e8a844c6864d734dab61706a0c779f96ef210f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "2vywag2lVC/tmp/975ae380333d6b3d37808d3c7ff8922921f99702301a00808d02c4b5af4946c5.jpg", "img_caption": ["Figure 7: Lagrangian, performance and cost curves for C-PGAE over CostLQR with regularization values $\\omega\\in\\{\\bar{0},10^{\\bar{-}4},1\\bar{0}^{-2}\\}$ (5 runs, mean \u02d8 $95\\%$ C.I.). "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "2vywag2lVC/tmp/9ac304a345511640582d0b31d037d8a84c71cf0eaecb55f95429df14dcaf0ec2.jpg", "img_caption": ["Figure 8: $\\lambda$ curves for C-PGPE and C-PGAE over CostLQR with regularization values $\\omega\\in$ $\\{0,10^{-4},10^{-2}\\}$ (5 runs, mean $\\pm~95\\%$ C.I.). "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "H.7 Computational Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "All the experiments were run on a 2019 16-inches MacBook Pro. The machine was equipped as follows: ", "page_idx": 46}, {"type": "text", "text": "CPU RAM GPU Intel Core i7 (6 cores, 2.6 GHz) 16 GB 2667 MHz DDR4 Intel UHD Graphics 630 1536 MB ", "page_idx": 46}, {"type": "text", "text": "In particular, $N=100$ trajectories of the MuJoCo environments with $T=100$ scored $\\approx2$ iterations per second for C-PGAE, while $\\approx3$ iterations per second for C-PGPE. $N=100$ trajectories of the CostLQR environment with $T\\,=\\,100$ scored $\\approx\\,5$ and $\\approx\\,8$ iterations per second respectively for C-PGAE and C-PGPE. All the performances are to be considered with a parallelization over 10 CPU cores. ", "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The abstract and the introduction reflect the original contribution of the paper. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: The limitations of the work emerges in the final section of the work and in the related works section. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: All the statements are correlated with proofs. The definitions and the quantity used are clearly presented. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: All the experimental settings are described, together with choices of all the parameters for the settings at hand. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: The implemented code is provided in a zip file which contains detailed instructions on how to use it. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: All the experimental settings are described, together with choices of all the parameters for the settings at hand. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: The plots of all the experiments are made over an average of different trials and we report, for each shown curve, the $95\\%$ confidence interval. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 49}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The computational resources employed for th experiments are discussed. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We checked the guidelines and we are compliant with them. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: As a theoretical research paper, the authors do not see any negative societal impact. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 50}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The only existing asset employed is the MuJoCo (Todorov et al., 2012) control suite, that is properly mentioned. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: In order to run the experiments, we implemented a detailed code-base. This asset is provided in an anonymized zip file and, conditionally on the paper acceptance, it will be made public. The zip file includes a file explaining how to employ the code-base. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 52}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]