[{"figure_path": "iykao97YXf/figures/figures_2_1.jpg", "caption": "Figure 1: Examples of an MDP and DRA.", "description": "The figure shows two examples. (a) shows a simple MDP with two states, s0 and s1, and two actions, a and b. All transitions have a probability of 1. State s1 is labeled with {p}, representing a petrol station. (b) shows a DRA (Deterministic Rabin Automaton) with three states, q0, q1, and q2, and an alphabet {p}. The DRA accepts runs where the label {p} appears exactly once, representing the objective \"visit the petrol station exactly once\".", "section": "Background"}, {"figure_path": "iykao97YXf/figures/figures_2_2.jpg", "caption": "Figure 1: Examples of an MDP and DRA.", "description": "This figure provides two examples, one of a Markov Decision Process (MDP) and another of a Deterministic Rabin Automaton (DRA). The MDP example shows a simple graph with states and transitions labeled with actions and probabilities.  The DRA example illustrates a finite automaton that accepts or rejects infinite sequences based on specified conditions. These examples are used in the paper to illustrate the concepts of MDPs and DRAs and their relationship to the problem of translating w-regular objectives to limit-average rewards. The MDP is used to represent the environment in a reinforcement learning setting, while the DRA is used to represent the w-regular objective to be learned by an agent.", "section": "Background"}, {"figure_path": "iykao97YXf/figures/figures_4_1.jpg", "caption": "Figure 2: A reward machine and the product MDP for the running Example 1.", "description": "This figure shows two diagrams. The first diagram (a) shows a reward machine for the objective of visiting the petrol station exactly once.  The states of the reward machine represent the number of times the petrol station has been visited: 0 times, once, or more than once. The transitions are labeled with the state, action, and next state, and the reward received for each transition is given following a forward slash. The second diagram (b) shows the product MDP, which combines the MDP from Figure 1a with the DRA from Figure 1b. The product MDP's states are pairs consisting of a state from the original MDP and a state from the DRA. The transitions are labeled with the actions, and the accepting condition is specified. This product MDP is used in the optimality-preserving translation from w-regular objectives to limit-average rewards described in the paper.", "section": "Warm-Up: Transitions with Positive Probability are Known"}, {"figure_path": "iykao97YXf/figures/figures_13_1.jpg", "caption": "(a) An MDP where X(s1, a, s\u2081) = X(s3, b, s0) = {c}, and the rest are labeled with \u00d8. (b) A DRA A for the objective of visiting s1 or s3 infinitely often where F := {({q1}, \u00d8)}", "description": "This figure shows a counterexample to the claim that there is an optimality preserving translation from w-regular languages to limit average rewards provided by reward functions.  (a) depicts a Markov Decision Process (MDP) where transitions are labeled with probabilities and sets of atomic propositions. (b) shows a Deterministic Rabin Automata (DRA) representing the objective of visiting states s1 or s3 infinitely often. This example demonstrates that no memoryless reward function can guarantee optimality preservation for all possible transition probabilities in the MDP.", "section": "Supplementary Materials for Section 3"}, {"figure_path": "iykao97YXf/figures/figures_15_1.jpg", "caption": "Figure 2: A reward machine and the product MDP for the running Example 1.", "description": "This figure shows two diagrams. The left diagram (a) is a reward machine for the objective of visiting the petrol station exactly once in Example 1. The right diagram (b) is a product MDP for the same example, where the states represent combinations of the MDP's states and DRA's states.  The transitions in the product MDP synchronize the transitions in both the MDP and DRA. The accepting condition is specified for the product MDP, defining which state combinations lead to acceptance.", "section": "Warm-Up: Transitions with Positive Probability are Known"}]