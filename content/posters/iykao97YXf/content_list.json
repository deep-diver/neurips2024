[{"type": "text", "text": "Reinforcement Learning with LTL and $\\omega$ -Regular Objectives via Optimality-Preserving Translation to Average Rewards ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuan-Bach Le1\u2217 Dominik Wagner1\u2217 Leon Witzman1 Alexander Rabinovich2 Luke Ong1 ", "page_idx": 0}, {"type": "text", "text": "1NTU Singapore 2Tel Aviv University {bach.le,dominik.wagner,luke.ong}@ntu.edu.sg witz0001@e.ntu.edu.sg rabinoa@tauex.tau.ac.il ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Linear temporal logic (LTL) and, more generally, $\\omega$ -regular objectives are alternatives to the traditional discount sum and average reward objectives in reinforcement learning (RL), offering the advantage of greater comprehensibility and hence explainability. In this work, we study the relationship between these objectives. Our main result is that each RL problem for $\\omega$ -regular objectives can be reduced to a limit-average reward problem in an optimality-preserving fashion, via (finitememory) reward machines. Furthermore, we demonstrate the efficacy of this approach by showing that optimal policies for limit-average problems can be found asymptotically by solving a sequence of discount-sum problems approximately. Consequently, we resolve an open problem: optimal policies for LTL and $\\omega$ -regular objectives can be learned asymptotically. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) is a machine learning paradigm whereby an agent aims to accomplish a task in a generally unknown environment [37]. Traditionally, tasks are specified via a scalar reward signal obtained continuously through interactions with the environment. These rewards are aggregated over entire trajectories either through averaging or by summing the exponentially decayed rewards. However, in many applications, there are no reward signals that can naturally be extracted from the environment. Moreover, reward signals that are supplied by the user are prone to error in that the chosen low-level rewards often fail to accurately capture high-level objectives. Generally, policies derived from local rewards-based specifications are hard to understand because it is difficult to express or explain their global intent. ", "page_idx": 0}, {"type": "text", "text": "As a remedy, it has been proposed to specify tasks using formulas in Linear Temporal Logic (LTL) [41, 30, 9, 38, 15, 34, 14] or $\\omega$ -regular languages more generally [30]. In this framework, the aim is to maximise the probability of satisfying a logical specification. LTL can precisely express a wide range of high-level behavioural properties such as liveness (infinitely often $P$ ), safety (always $P$ ), stability (eventually always $P$ ), and priority $P$ then $Q$ then $T$ ). ", "page_idx": 0}, {"type": "text", "text": "Motivated by this, a growing body of literature study learning algorithms for RL with LTL and $\\omega$ -regular objectives (e.g. [41, 15, 30, 7, 32, 20, 21, 16]). However, to the best of our knowledge, all of these approaches may fail to learn provably optimal policies without prior knowledge of a generally unknown parameter such as the optimal $\\epsilon$ -return mixing time [15] or the $\\epsilon_{}$ -recurrence time [30], which depend on the (unavailable) transition probabilities of the MDP. Moreover, it is known that neither LTL nor (limit) average reward objectives are PAC (probably approximately correct) learnable [2]. Consequently, approximately optimal policies can only possibly be found asymptotically but not in bounded time. 1 ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we pursue a different strategy: rather than solving the RL problem directly, we study optimality-preserving translations [2] from $\\omega$ -regular objectives to more traditional rewards, in particular, limit-average rewards. This method offers a significant advantage: it enables the learning of optimal policies for $\\omega$ -regular objectives by solving a single more standard problem, for which we can leverage existing off-the-shelf algorithms (e.g. [26, 15, 30]). In this way, all future advances\u2014in both theory and practice\u2014for these much more widely studied problems carry over directly, whilst still enjoying significantly more explainable and comprehensible specifications. It is well-known that such a translation from LTL to discounted rewards is impossible [2]. Intuitively, this is because the latter cannot capture infinite horizon tasks such as reachability or safety [2, 42, 19]. Hence, we instead investigate translations to limit-average rewards in this paper. ", "page_idx": 1}, {"type": "text", "text": "Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We study reinforcement learning of $\\omega$ -regular and LTL objectives in Markov decision processes (MDPs) with unknown probability transitions, translations to limit-average reward objectives and learning algorithms for the latter. In detail: ", "page_idx": 1}, {"type": "text", "text": "1. We prove a negative result (Proposition 4): in general it is not possible to translate $\\omega$ -regular objectives to limit average objectives in an optimality-preserving manner if rewards are memoryless (i.e., independent of previously performed actions, sometimes called historyfree or Markovian).   \n2. On the other hand, our main result (Theorem 12) resolves Open Problem 1 in [2]: such an optimality-preserving translation is possible if the reward assignment may use finite memory as formalised by reward machines [23, 24].   \n3. To underpin the efficacy of our reduction approach, we provide the first convergence proof (Theorem 16) of an RL algorithm (Algorithm 1) for average rewards. To the best of our knowledge (and as indicated by [13]), this is the first proof without assumptions on the induced Markov chains. In particular, the result applies to multichain MDPs, which our translation generally produces, with unknown probability transitions. Consequently, we also resolve Open Problem 4 of [2]: RL for $\\omega$ -regular and LTL objectives can be learned in the limit (Theorem 18). ", "page_idx": 1}, {"type": "text", "text": "Outline. We start by reviewing the problem setup in Section 2. Motivated by the impossibility result for simple reward functions, we define reward machines (Section 3). In Section 4 we build intuition for the proof of our main result in Section 5. Thereafter, we demonstrate that RL with limit-average, $\\omega$ -regular and LTL objectives can be learned asymptotically (Section 6). Finally, we review related work and conclude in Section 7. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recall that a Markov Decision Process $(M D P)$ is a tuple $\\mathcal{M}\\,=\\,(S,A,s_{0},P)$ where $S$ is a finite set of states, $s_{0}\\in S$ is the initial state, $A$ is the finite set of actions and $P:S\\times A\\times S\\to[0,1]$ is the probability transition function such that $\\textstyle\\sum_{s^{\\prime}\\in S}P(s,a,s^{\\prime})=1$ for every $s\\,\\in\\,S$ and $a\\in A$ MDPs may be graphically represented; see e.g. Fig. 1a. We let $\\operatorname{Runs}_{\\operatorname{fi}}(S,A)=S\\times(A\\times S)^{*}$ and $\\operatorname{Runs}(S,\\dot{A})=\\bar{(}S^{\\bullet}\\times A)^{\\omega}$ denote the set of finite runs and the set of infinite runs in $\\mathcal{M}$ respectively. A policy $\\pi:\\operatorname{Runs}_{\\operatorname{f}}(S,A)\\to{\\mathcal{D}}(A)$ maps finite runs to distributions over actions. We let $\\Pi(S,A)$ denote the set of all such policies. A policy $\\pi$ is memoryless if $\\pi(s_{0}a_{0}\\ldots s_{n})=\\pi(s_{0}^{\\prime}a_{0}^{\\prime}\\ldots s_{m}^{\\prime})$ for all finite runs $s_{0}a_{0}\\ldots s_{n}$ and $s_{0}^{\\prime}a_{0}^{\\prime}\\ldots\\bar{s}_{m}^{\\prime}$ such that $s_{n}=s_{m}^{\\prime}$ . For each MDP $\\mathcal{M}$ and policy $\\pi$ , there is a natural induced probability measure D\u03c0Mon its runs. TMhuec hd eosfi rtahbei lRitLy  loitfe rpaotluircei efso cfours eas  goinv ednis cMouDnPt $\\mathcal{M}$ ucamn $\\mathcal{T}:\\Pi(S,A)\\rightarrow\\mathbb{R}$ . $\\mathcal{I}_{\\mathcal{R}^{\\gamma}}^{\\mathcal{M}}$ and limit-average reward objectives $\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}$ , (b) A DRA, where $F:=\\{(\\{q_{1}\\},\\emptyset)\\}$ , for the objective to visit the petrol station $p$ exactly once. ", "page_idx": 1}, {"type": "image", "img_path": "iykao97YXf/tmp/65b68acb96069a2430dcfadbc50f7728a0878102f188a4eac9f1c592a3a50def.jpg", "img_caption": ["(a) An MDP where all transitions occur with probability 1, $\\lambda(s_{0},b,s_{1})\\,=\\,\\{p\\}$ and the rest are labeled with $\\varnothing$ . "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "iykao97YXf/tmp/2bc50ce9c06a451c311370ff532cba89eab8cc705a2ec85b7dbcc10c705ed3be.jpg", "img_caption": ["Figure 1: Examples of an MDP and DRA. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "which lift a reward function $\\mathcal{R}:S\\times A\\times S\\rightarrow\\mathbb{R}$ for single transitions to runs $\\rho=s_{0}a_{0}s_{1}a_{1}\\ldots$ as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{I}_{\\mathcal{R}^{\\gamma}}^{M}(\\pi):=\\mathbb{E}_{\\rho\\sim\\mathcal{D}_{\\pi}^{M}}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}\\cdot r_{i}\\right]\\qquad\\quad\\mathcal{I}_{\\mathcal{R}^{\\infty}}^{M}(\\pi):=\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}_{\\rho\\sim\\mathcal{D}_{\\pi}^{M}}\\left[\\frac{1}{t}\\cdot\\sum_{i=0}^{t-1}r_{i}\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $r_{i}=\\mathcal{R}(s_{i},a_{i},s_{i+1})$ and $\\gamma\\in(0,1)$ is the discount factor. ", "page_idx": 2}, {"type": "text", "text": "$\\omega$ -Regular Objectives. $\\omega$ -regular objectives (which subsume LTL objectives) are an alternative to these traditional objectives. Henceforth, we fix an alphabet $A\\mathcal{P}$ and a label function $\\lambda:S\\times A\\times S\\rightarrow$ $2^{A\\mathcal{P}}$ for transitions, where $2^{X}$ is the power set of a set $X$ . Each run $\\rho=s_{0}a_{0}s_{1}a_{1}s_{2}\\,.\\,.$ . induces a sequence of labels $\\lambda(\\rho)=\\lambda(s_{0},a_{0},\\bar{s_{1}})\\lambda(s_{1},a_{1},s_{2})\\ldots$ Thus, for a set $L\\subseteq(2^{\\mathcal{A}\\mathcal{P}})^{\\omega}$ of \u201cdesirable\u201d label sequences we can consider the probability of a run\u2019s labels being in that set: $\\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi}^{\\mathcal{M}}}[\\lambda(\\rho)\\in L]$ . ", "page_idx": 2}, {"type": "text", "text": "Example 1. For instance, an autonomous car may want to \u201cvisit a petrol station exactly once\u201d to conserve resources (e.g. time or petrol). Consider the MDP in Fig. 1a where the state $s_{1}$ represents a petrol station. We let ${\\mathcal{A}}{\\mathcal{P}}=\\{p\\}$ $\\mathit{\\Delta}_{p}$ for petrol), $\\lambda(s_{0},b,s_{1})=\\{p\\}$ , and the rest are labeled with $\\varnothing$ . The desirable label sequences are $L=\\{\\lambda_{1}\\lambda_{2}\\cdot\\cdot\\cdot|$ for exactly one $i\\in\\mathbb{N},\\lambda_{i}=\\{p\\}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "In this work, we focus on $L$ which are $\\omega$ -regular languages. It is well known that $\\omega$ -regular languages are precisely the languages recognised by Deterministic Rabin Automata (DRA) [27, 29]: ", "page_idx": 2}, {"type": "text", "text": "Definition 2. A DRA is a tuple $\\mathcal{A}=(Q,2^{\\mathcal{A P}},q_{0},\\delta,F)$ where $Q$ is a finite state set, $2^{A\\mathcal{P}}$ is the alphabet, $q_{0}~\\in~Q$ is the initial state, $\\delta\\,:\\,Q\\,\\times\\,2^{\\mathcal{A P}}\\,\\rightarrow\\,Q$ is the transition function, and ${\\textbf{\\textit{F}}}=$ $\\{(A_{1},R_{1}),\\ldots,(A_{n},R_{n})\\}$ , where $A_{i},R_{i}\\subseteq Q$ , is the accepting condition. Let $\\rho\\in(2^{\\mathcal{A}\\mathcal{P}})^{\\omega}$ be an infinite run and $\\mathrm{InfS}(\\rho)$ the set of states visited infinitely often by $\\rho$ . We say $\\rho$ is accepted by $\\boldsymbol{\\mathcal{A}}$ if there exists some $(\\dot{A_{i}},\\dot{R_{i}})\\in F$ such that $\\rho$ visits some state in $A_{i}$ infinitely often whilst visiting every state in $R_{i}$ finitely often, i.e. $\\operatorname{InfS}(\\rho)\\cap A_{i}\\neq\\varnothing$ and $\\mathrm{InfS}(\\rho)\\cap R_{i}=\\emptyset$ . ", "page_idx": 2}, {"type": "text", "text": "For example, the objective in Example 1 may be represented by the DRA in Fig. 1b. ", "page_idx": 2}, {"type": "text", "text": "Thus, the desirability of $\\pi$ is the probability of $\\pi$ generating an accepting sequence in the DRA $\\boldsymbol{\\mathcal{A}}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{I}_{A}^{\\mathcal{M}}(\\pi)\\,:=\\,\\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi}^{\\mathcal{M}}}[\\lambda(\\rho)\\mathrm{~is~accepted~by~the~automaton}\\,A]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Remarks. The class of $\\omega$ -regular languages subsumes languages expressed by Linear Temporal Logic (LTL, see e.g. [4, Ch. 5]), a logical framework in which e.g. reachability (eventually $P,\\langle\\rangle P)$ , safety (always $P,\\boxed{\\Pi}P)$ and reach-avoid (eventually $P$ whilst avoiding $Q$ , $(\\neg Q)\\,\\mathcal{U}\\,P)$ properties can be expressed concisely and intuitively. The specification of our running Example 1 to visit the petrol station exactly once can be expressed as the LTL formula $(\\neg p)\\,\\mathcal{U}\\,(p\\land\\neg\\,\\bigsqcup\\neg p)$ , where ${\\bigcirc}Q$ denotes \u201c $^{\\,\\!}Q$ holds at the next step\u201d. Furthermore, our label function $\\lambda$ , which maps transitions to labels, is more general than other definitions (e.g. [41, 15, 30]) instead mapping states to labels. As a result, we are able to articulate properties that involve actions, such as \u201cto reach the state $s$ while avoiding taking the action $a^{\\gamma}$ . ", "page_idx": 2}, {"type": "text", "text": "Optimality-Preserving Specification Translations. Rather than solving the problem of synthesising optimal policies for Eq. (1) directly, we are interested in reducing it to more traditional RL problems and applying off-the-shelf RL algorithms to find optimal policies. To achieve this, the reduction needs to be optimality preserving2: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Definition 3 ([2]). An optimality-preserving specification translation from $\\omega$ -regular objectives to limit-average rewards is a computable function mapping each tuple $(S,A,\\lambda,A)$ to $\\mathcal{R}_{(S,A,\\lambda,A)}$ s.t. ", "page_idx": 3}, {"type": "text", "text": "for every MDP $\\mathcal{M}=(S,A,s_{0},P)$ , label function $\\lambda:S\\times A\\times S\\to2^{A\\mathcal{P}}$ and DRA $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 3}, {"type": "text", "text": "We stress that since the probability transition function $P$ is generally not known, the specification translation may not depend on it. ", "page_idx": 3}, {"type": "text", "text": "3 Negative Result and Reward Machines ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Reward functions emit rewards purely based on the transition being taken without being able to take the past into account. On the other hand, DRAs have finite memory. Therefore, there cannot generally be optimality-preserving translations from $\\omega$ -regular objectives to limit average rewards provided by reward functions: ", "page_idx": 3}, {"type": "text", "text": "Proposition 4. There is an MDP $\\mathcal{M}$ and an $\\omega$ -regular language $L$ for which it is impossible to find a reward function $\\mathcal{R}:S\\times A\\times S\\rightarrow\\mathbb{R}$ such that every $\\mathcal{J}_{\\mathcal{R}^{a\\nu g}}^{\\mathcal{M}}$ -optimal policy of $\\mathcal{M}$ also maximises the probability of membership in $L$ . ", "page_idx": 3}, {"type": "text", "text": "Remarkably, this rules out optimality-preserving specification translations even if transition probabilities are fully known3. ", "page_idx": 3}, {"type": "text", "text": "Proof. Consider the deterministic MDP in Fig. 1a and the objective of Example 1 \u201cto visit $s_{1}$ exactly once\u201d expressed by the DRA $\\boldsymbol{\\mathcal{A}}$ in Fig. 1b. Assume towards contradiction there exists a reward function $\\mathcal{R}:S\\times A\\times S\\rightarrow\\mathbb{R}$ such that optimal policies w.r.t. J RMavg maximise acceptance by A. Note that every policy $\\pi^{*}$ maximising acceptance by the DRA induces the run $s_{0}(a s_{0})^{n}b s_{1}b s_{0}(a s_{0})^{\\omega}$ for some $n\\in\\mathbb N$ , and $\\mathcal{J}_{A}^{\\mathcal{M}}(\\pi^{*})\\mathit{\\check{\\Psi}}=1$ . Thus, its limit-average reward is $\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi^{*})\\,=\\,\\mathcal{R}(s_{0},a,s_{0})$ Now, consider the policy $\\pi$ always selecting action $a$ with probability 1. As the run induced by $\\pi$ is $s_{0}(a s_{0})^{\\omega}$ , we deduce that $\\mathcal{J}_{A}^{\\bar{M}}(\\pi)=0$ and $\\mathcal{T}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi)\\,=\\,\\bar{\\mathcal{R}}\\big(s_{0},a,\\dot{s}_{0}\\big)\\,=\\,\\mathcal{T}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi^{*})$ , which is a contradiction since $\\pi$ is not $\\mathcal{I}_{A}^{M}$ -optimal. \u53e3 ", "page_idx": 3}, {"type": "text", "text": "Since simple reward functions lack the expressiveness to capture $\\omega$ -regular objectives, we employ a generalisation, reward machines [23, 24], whereby rewards may also depend on an internal state: ", "page_idx": 3}, {"type": "text", "text": "Definition 5. A reward machine $(R M)$ is a tuple $\\mathcal{R}\\,=\\,(U,u_{0},\\delta_{u},\\delta_{r})$ where $U$ is a finite set of states, $u_{0}~\\in~U$ is the initial state, $\\delta_{r}\\;:\\;U\\,\\times\\,(S\\,\\times\\,A\\,\\times\\,S)\\;\\to\\;\\mathbb{R}$ is the reward function, and $\\delta_{u}:U\\times(S\\times A\\times S)\\to U$ is the update function. ", "page_idx": 3}, {"type": "text", "text": "Intuitively, a RM $\\mathcal{R}$ utilises the current transition to update its states through $\\delta_{u}$ and assigns the rewards through $\\delta_{r}$ . For example, Fig. 2a depicts a reward machine for the MDP of Fig. 1a, where the states count the number of visits to $s_{1}$ (0 times, once, more than once). ", "page_idx": 3}, {"type": "text", "text": "Let $\\rho=s_{0}a_{0}s_{1}\\cdot\\cdot\\cdot$ be an infinite run. Since $\\delta_{u}$ is deterministic, it induces a sequence $\\,u_{0}u_{1}\\dots$ of states in $\\mathcal{R}$ , where $e_{i}=(s_{i},a_{i},s_{i+1})$ and $u_{i+1}=\\delta_{u}(u_{i},e_{i})$ . The limit-average reward of a policy $\\pi$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi)\\ :=\\ \\operatorname*{lim}_{t\\to\\infty}\\operatorname*{inf}_{\\L}\\mathbb{E}_{\\rho\\sim\\mathcal{D}_{\\pi}^{\\mathcal{M}}}\\left[\\frac{1}{t}\\sum_{i=0}^{t-1}\\delta_{r}(u_{i},e_{i})\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is seen that limit-average optimal policies $\\pi^{*}$ for the MDP in Fig. 1a and the RM in Fig. 2a eventually select action $b$ exactly once in state $s_{0}$ to achieve $\\bar{\\mathcal{I}}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi^{*})^{-}=1$ . ", "page_idx": 3}, {"type": "text", "text": "In the following two sections, we present a general translation from $\\omega$ -regular languages to limitaverage reward machines, and we show that our translation is optimality-preserving (Theorem 12). ", "page_idx": 3}, {"type": "image", "img_path": "iykao97YXf/tmp/c2115254af221ad56d0feb38c3598ca53996c6d518626c1f3b0c700a1481fbd3.jpg", "img_caption": ["Figure 2: A reward machine and the product MDP for the running Example 1. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Remarks. Our definition of RM is more general than the one presented in [23, 24], where $\\delta_{u}^{\\prime}:$ $U\\rightarrow[S\\times A\\times S\\rightarrow\\mathbb{R}]$ and $\\delta_{r}^{\\prime}:U\\times2^{\\mathcal{A}\\breve{\\mathcal{P}}}\\to U$ . Note that $(\\delta_{u}^{\\prime},\\delta_{r}^{\\prime})$ can be reduced to $(\\delta_{u},\\delta_{r})$ by expanding the state space of the RM to include the previous state and utilising the inverse label function $\\lambda^{-\\tilde{1}}$ . It is worth pointing out that Theorem 12 does not contradict a negative result in [2] regarding the non-existence of an optimality-preserving translation from LTL constraints to abstract limit-average reward machines (where only the label of transitions is provided to $\\delta_{u}$ and $\\delta_{r}$ ). ", "page_idx": 4}, {"type": "text", "text": "4 Warm-Up: Transitions with Positive Probability are Known ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To help the reader gain intuition about our construction, we first explore the situation where the support $\\{(s,\\bar{a_{,}}s^{\\prime})\\in S\\times\\bar{A}\\times S\\ |\\ P(s,a,s^{\\prime})>0\\}$ of the MDP\u2019s transition function is known. Crucially, we do not assume that the magnitude of these (non-zero) probabilities are known. Subsequently, in Section 5, we fully eliminate this assumption. ", "page_idx": 4}, {"type": "text", "text": "This assumption allows us to draw connections between our problem and a familiar scenario in probabilistic model checking [4, Ch. 10], where the acceptance problem for $\\omega$ -regular objectives can be transformed into a reachability problem. Intuitively, our reward machine monitors the state of the DRA and provides reward 1 if the MDP and the DRA are in certain \u201cgood\u201d states (0 otherwise). ", "page_idx": 4}, {"type": "text", "text": "For the rest of this section, we fix an MDP without transition function $(S,A,s_{0})$ , a set of possible transitions $E\\subseteq S\\times A\\times S$ , a label function $\\lambda:S\\!\\times\\!A\\!\\times\\!S\\to2^{A\\mathcal{P}}$ and a DRA $\\begin{array}{r}{\\hat{\\mathcal{A}}=\\stackrel{{}}{(}Q,2^{\\mathcal{A}\\mathcal{P}},\\stackrel{{}}{q}_{0},\\delta,F\\rangle}\\end{array}$ . Our aim is to find a reward machine $\\mathcal{R}$ such that for every transition function $P$ compatible with $E$ (formally: $E=\\{(s,a,s^{\\prime})\\mid P(s,a,s^{\\prime})>0\\})$ ), optimal policies for limit-average rewards are also optimal for the acceptance probability of the DRA $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 4}, {"type": "text", "text": "4.1 Product MDP and End Components ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "First, we form the product MDP $\\mathcal{M}\\otimes\\mathcal{A}$ (e.g. [41, 15]), which synchronises the dynamics of the MDP $\\mathcal{M}$ with the DRA $\\boldsymbol{\\mathcal{A}}$ . Formally, $\\mathcal{M}\\otimes\\mathcal{A}=(V,A,v_{0},\\Delta,F_{\\mathcal{M}})$ where $V=S\\times Q$ is the set of states, $A$ is the set of actions, $v_{0}=(s_{0},q_{0})$ is the initial state. The transition probability function $\\Delta:V\\times A\\times V\\to[0,1]$ satisfies $\\Delta(v,a,v^{\\prime})\\,=\\,P(s,a,s^{\\prime})$ given that $\\boldsymbol{v}\\,=\\,\\bar{(}s,q)$ , $\\boldsymbol{v^{\\prime}}\\,=\\,(s^{\\prime},q^{\\prime})$ , and $\\delta(q,\\lambda(s,a,s^{\\prime}))\\;=\\;q^{\\prime}$ . The accepting condition is $F_{\\mathcal{M}}\\;=\\;\\{(A_{1}^{\\prime},R_{1}^{\\prime}),(A_{2}^{\\prime},R_{2}^{\\prime}),..\\ .\\}$ where $A_{i}^{\\prime}=S\\times A_{i}$ , $R_{i}^{\\prime}=S\\times R_{i}$ , and $(A_{i},R_{i})\\in F$ . A run $\\rho=(s_{0},q_{0})a_{0}\\cdot\\cdot\\cdot$ is accepted by $\\mathcal{M}\\otimes\\mathcal{A}$ if there exists some $(A_{i}^{\\prime},R_{i}^{\\prime})\\in F_{\\mathcal{M}}$ such that $\\mathrm{InfV}(\\rho)\\cap A_{i}^{\\prime}\\neq\\ddot{\\varnothing}$ and $\\mathrm{InfV}(\\rho)\\cap R_{i}^{\\prime}=\\varnothing$ , where InfV is the set of states $(s,v)$ in the product MDP visited infinitely often by $\\rho$ . ", "page_idx": 4}, {"type": "text", "text": "Note that product MDPs have characteristics of both MDPs and DRAs which neither possesses in isolation: transitions are generally probabilistic and there is a notation of acceptance of runs. For example, the product MDP for Fig. 1 is shown in Fig. 2b. Due to the deterministic nature of the DRA $\\boldsymbol{\\mathcal{A}}$ , every run $\\rho$ in $\\mathcal{M}$ gives rise to a unique run $\\rho^{\\otimes}$ in $\\mathcal{M}\\otimes\\mathcal{A}$ . Crucially, for every policy $\\pi$ , ", "page_idx": 4}, {"type": "text", "text": "We make use of well-known almost-sure characterisation of accepting runs via the notion of accepting end components: ", "page_idx": 4}, {"type": "text", "text": "Definition 6. An end component (EC) of $\\mathcal{M}\\otimes\\mathcal{A}=(V,A,v_{0},\\Delta,F_{\\mathcal{M}})$ is a pair $(T,\\mathrm{Act})$ where $T\\subseteq V$ and $\\operatorname{Act}:T\\to2^{A}$ satisfies the following conditions ", "page_idx": 5}, {"type": "text", "text": "1. For every $v\\in T$ and $a\\in\\mathrm{Act}(v)$ , we have $\\begin{array}{r}{\\sum_{v^{\\prime}\\in T}\\Delta(v,a,v^{\\prime})=1}\\end{array}$ , and 2. The graph $(T,\\to_{\\mathrm{Act}})$ is strongly connected , where $v\\rightarrow\\neg{\\mathrm{act}}\\ v^{\\prime}$ iff $\\Delta(v,a,v^{\\prime})>\\ 0$ for some $a\\in\\operatorname{Act}(v)$ . ", "page_idx": 5}, {"type": "text", "text": "Intuitively, an EC is a strongly connected sub-MDP. For instance, for the product MDP in Fig. 2b there are five end components, $(\\{(s_{0},q_{0})\\},(s_{0},q_{0})\\mapsto\\{a\\})$ , $(\\{(s_{0},q_{1})\\},\\bar{(s_{0},q_{1})}\\mapsto\\{a\\})$ , $(\\{\\bar{({s_{0}},q_{2})}\\},(s_{0},q_{2})\\;\\mapsto\\;\\{a\\})$ , $(\\{(s_{0},q_{2})\\},(s_{0},q_{2})\\;\\mapsto\\;\\{b\\})$ and $(\\{(s_{0},q_{2})\\},(s_{0},q_{2})\\ \\mapsto\\ \\{a,b\\})$ . $(\\{(s_{0},q_{1})\\}$ , $(s_{0},q_{1})\\mapsto\\{a\\})$ ) is its only accepting end component. ", "page_idx": 5}, {"type": "text", "text": "It turns out that, almost surely, a run is accepted iff it enters an accepting end component and never leaves it [1]. Therefore, a natural idea for a reward machine is to use its state to keep track of the state $q\\in Q$ the DRA is in and give reward 1 to transitions $\\left(s,a,s^{\\prime}\\right)$ if $(s,q)$ is in some AEC (and 0 otherwise). Unfortunately, this approach falls short since the AEC may contain non-accepting ECs, thus assigning maximal reward to sub-optimal policies.4 As a remedy, we introduce a notion of minimal AEC, and ensure that only runs eventually committing to one such minimal AEC get a limit-average reward of 1. ", "page_idx": 5}, {"type": "text", "text": "Definition 7. An AEC $(T,\\mathrm{Act})$ is an accepting simple EC (ASEC) if $|\\operatorname{Act}(v)|=1$ for every $v\\in T$ . Let $\\mathcal{C}_{1}=(T_{1},\\mathrm{Act}_{1}),...\\,,\\mathcal{C}_{n}=(T_{n},\\mathrm{Act}_{n})$ be a collection of ASECs covering all states in ASECs, i.e. if $(s,q)$ is in some ASEC then $(s,q)\\in T_{1}\\cup\\cdots\\cup T_{n}$ . In particular, $n\\leq|S\\times Q|$ is sufficient. We can prove that every AEC contains an ASEC (see Lemma 20 in Appendix B). Consequently, ", "page_idx": 5}, {"type": "text", "text": "Lemma 8. Almost surely, if $\\rho$ is accepted by $\\boldsymbol{\\mathcal{A}}$ then $\\rho^{\\otimes}$ reaches a state in some ASEC $\\mathcal{C}_{i}$ of $\\mathcal{M}\\otimes\\mathcal{A}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Reward Machine and Correctness ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, to ensure that runs eventually commit to one such ASEC we introduce the following notational shorthand: for $(s,q)~\\in~T_{1}\\cup\\cdot\\cdot\\colon\\cup T_{n}$ , let $\\mathcal{C}_{(s,q)}~=~(T_{(s,q)},\\mathrm{Act}_{(s,q)})$ be the $\\mathcal{C}_{i}$ with minimal $i$ containing (s, q), i.e. C(s,q) := Cmin{1\u2264i\u2264n|(s,q)\u2208Ti}. ", "page_idx": 5}, {"type": "text", "text": "Intuitively, we give a reward of 1 if $(s,q)$ is in one of the $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}$ . However, once an action is performed which deviates from $\\mathrm{Act}_{(s,q)}$ no rewards are given thereafter, thus resulting in a limit average reward of 0. ", "page_idx": 5}, {"type": "text", "text": "A state in the reward machine has the form $q\\in Q$ , keeping track of the state in the DRA, or $\\bot$ , which is a sink state signifying that in a state in $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}$ we have previously deviated from $\\mathrm{Act}_{(s,q)}$ . ", "page_idx": 5}, {"type": "text", "text": "Finally, we are ready to formally define the reward machine $\\mathcal{R}=\\mathcal{R}_{(S,A,\\lambda,A)}$ exhibiting our specification translation as $(Q\\cup\\{\\bot\\},q_{0},\\delta_{u},\\delta_{r})$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\delta_{u}(u,(s,a,s^{\\prime})):=\\left\\{\\begin{array}{l l}{\\bot}&{\\mathrm{if~}u=\\bot\\mathrm{~or~}}\\\\ &{\\left((s,u)\\in T_{1}\\cup\\ldots\\cup T_{n}\\mathrm{~and~}a\\notin\\mathrm{Act}_{(s,u)}(s,u)\\right)}\\\\ {\\delta(u,\\lambda(s,a,s^{\\prime}))}&{\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\delta_{r}(u,(s,a,s^{\\prime})):=\\left\\{1\\!\\!\\!\\begin{array}{l l}{\\mathrm{if~}u\\neq\\perp\\mathrm{and}\\,(s,u)\\in T_{1}\\cup\\cdot\\cdot\\cdot\\cup T_{n}}\\\\ {\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For our running example, this construction essentially yields the reward machine in Fig. 2a (with some inconsequential modifications cf. Fig. 4 in Appendix B). ", "page_idx": 5}, {"type": "text", "text": "Theorem 9. For all transition probability functions $P$ with support $E$ , policies maximising the limit-average reward w.r.t. $\\mathcal{R}$ also maximise the acceptance probability of the DRA $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 5}, {"type": "text", "text": "This result follows immediately from the following (the full proof is presented in Appendix B): ", "page_idx": 5}, {"type": "text", "text": "Lemma 10. Let $P$ be a probability transition function with support $E$ and $\\mathcal{M}:=(S,A,s_{0},P)$ . ", "page_idx": 6}, {"type": "text", "text": "1. For every policy $\\pi_{i}$ , $\\mathcal{J}_{\\mathcal{R}^{a v g}}^{\\mathcal{M}}(\\pi)\\leq\\mathcal{J}_{\\mathcal{A}}^{\\mathcal{M}}(\\pi)$ .   \n2. For every policy $\\pi$ , there exists some policy $\\pi^{\\prime}$ satisfying $\\mathcal{J}_{A}^{M}(\\pi)\\leq\\mathcal{J}_{\\mathcal{R}^{a\\nu g}}^{\\mathcal{M}}(\\pi^{\\prime}).$ . ", "page_idx": 6}, {"type": "text", "text": "Proof sketch. 1. By construction, every run receiving a limit-average reward of 1, must have entered some ASEC $\\mathcal{C}_{i}$ and never left it. Furthermore, almost surely all states are visited infinitely often and the run is accepted by definition of accepting ECs. ", "page_idx": 6}, {"type": "text", "text": "2. By Lemma 8, almost surely, a run is only accepted if it enters some $\\mathcal{C}_{i}$ . We set $\\pi^{\\prime}$ to be the policy agreeing with $\\pi$ until reaching one of the $\\mathcal{C}_{1},...\\,\\mathcal{C}_{n}$ and henceforth following the action $\\mathrm{Act}_{(s_{t},q_{t})}(s_{t},q_{t})$ , where $q_{t}$ is the state of the DRA at step $t$ , yielding a guaranteed limit-average reward of 1 for the run by construction. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Remark 11. Our construction considers a collection of ASECs covering all states in ASECs. Whilst it does not necessarily require listing all possible ASECs but only (up to) one ASEC per state, it is unclear whether this can be obtained in polynomial time. In Appendix B.1, we present an alternative (yet more complicated) construction which has polynomial time complexity. ", "page_idx": 6}, {"type": "text", "text": "5 Main Result ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we generalise the approach of the preceding section to prove our main result: ", "page_idx": 6}, {"type": "text", "text": "Theorem 12. There exists an optimality-preserving translation from $\\omega$ -regular languages to limitaverage reward machines. ", "page_idx": 6}, {"type": "text", "text": "Again, we fix an MDP without transition function $(S,A,s_{0})$ , a label function $\\lambda:S\\times A\\times S\\to2^{A\\mathcal{P}}$ and a DRA $\\mathcal{A}=(Q,2^{\\mathcal{A P}},q_{0},\\delta,F)$ . Note that the ASECs of a product MDP are uniquely determined by the non-zero probability transitions. Thus, for each set of transitions $E\\subseteq(S\\times Q)\\times A\\times(S\\times Q)$ , we let $\\mathcal{C}_{1}^{E}=(\\dot{T_{1}},\\mathrm{Act}_{1}),\\dot{\\dots},\\dot{\\mathcal{C}}_{n}^{E}=(T_{n},\\mathrm{Act}_{n})$ denote a collection of ASECs covering all states in ASECs w.r.t. the MDPs in which $E$ is the set of non-zero probability transitions.5 Then, for each set $E$ and state $(s,q)\\in T_{1}^{E}\\cup\\cdots\\cup T_{n}^{E}$ , we let C(Es,q) = (T (Es,q), Act(Es,q)) be the ASEC CiE that contains $(s,q)$ in which the index $i$ is minimal. ", "page_idx": 6}, {"type": "text", "text": "Our reward machine $\\mathcal{R}=\\mathcal{R}_{(S,A,\\lambda,A)}$ extends the ideas from the preceding section. Importantly, we keep track of the set of transitions $E$ taken so far and assign rewards according to our current knowledge about the graph of the product MDP. Therefore, we propose employing states of the form $(q,f,E)$ , where $q\\in Q$ keeps track of the state of the DRA, $f\\bar{\\in}\\{\\top,\\bot\\}$ is a status flag and $E\\subseteq(S\\times Q)\\times A\\times(S\\times Q)$ memorises the transitions in the product MDP encountered thus far. ", "page_idx": 6}, {"type": "text", "text": "Intuitively, we set the flag to $\\bot$ if we are in MDP state $s$ , $(s,q)$ is in one of the $\\mathcal{C}_{1}^{E},\\ldots,\\mathcal{C}_{n}^{E}$ and the chosen action deviates from Act(Es,q)(s, q). We can recover from \u22a5by discovering new transitions. Besides, we give reward 1 if $f=\\top$ and $(s,q)$ is in one of the $\\mathcal{C}_{1}^{E},\\ldots,\\mathcal{C}_{n}^{E}$ (and 0 otherwise). ", "page_idx": 6}, {"type": "text", "text": "The status flag is required since discovering new transitions will change the structure of (accepting simple) end components. Hence, differently from the preceding section, it is not sufficient to have a single sink state. ", "page_idx": 6}, {"type": "text", "text": "The initial state of our reward machine is $u_{0}:=(q_{0},\\top,\\emptyset)$ and we formally define the update and reward functions as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{u}((q,f,E),(s,a,s^{\\prime})):=\\left\\{\\begin{array}{l l}{(q^{\\prime},\\perp,E)}&{\\mathrm{if~}f=\\perp\\mathrm{and}\\;e\\in E}\\\\ {(q^{\\prime},\\perp,E)}&{\\mathrm{if~}f={\\top},e\\in E,(s,q)\\in T_{1}^{E}\\cup\\dots\\cup T_{n}^{E}\\,\\mathrm{sind}\\;e\\in E}\\\\ &{a\\notin\\mathrm{Act}_{(s,q)}^{E}(s,q)}\\\\ {(q^{\\prime},{\\top},E\\cup\\{e\\})}&{\\mathrm{otherwise}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\delta_{r}((q,f,E),(s,a,s^{\\prime})):=\\left\\{1\\!\\!\\!\\begin{array}{l l}{\\mathrm{~if~}f=\\top,(s,q)\\in T_{1}^{E}\\cup\\dots\\cup T_{n}^{E}}\\\\ {\\mathrm{~otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Example 13. For our running example (see Example 1 and Fig. 1) initially no transitions are known (hence no ASECs). Therefore, all transitions receive reward 0. Once action $a$ has been performed in state $s_{0}$ in the MDP $\\mathcal{M}$ and $(q_{1},f,E)$ in the reward machine $\\mathcal{R}$ , we have discovered the ASEC $\\langle\\{(s_{0},q_{1})\\},(s_{0},q_{1})\\mapsto\\{a\\}\\rangle$ and a reward of 1 is given henceforth unless action $b$ is selected eventually. In that case, we leave the ASEC and we will not discover further ASECs since there is only one. From here, it is not possible to return to state $q_{1}$ in the DRA and henceforth only reward 0 will be obtained. ", "page_idx": 7}, {"type": "text", "text": "Theorem 12 is proven by demonstrating an extension of Lemma 10 (see Appendix C): ", "page_idx": 7}, {"type": "text", "text": "Lemma 14. Suppose $\\mathcal{M}=(S,A,s_{0},P)$ is an arbitrary MDP. ", "page_idx": 7}, {"type": "text", "text": "1. For every policy $\\pi_{i}$ , $\\mathcal{J}_{\\mathcal{R}^{a v g}}^{\\mathcal{M}}(\\pi)\\leq\\mathcal{J}_{A}^{\\mathcal{M}}(\\pi)$ .   \n2. For every policy $\\pi$ , there exists some policy $\\pi^{\\prime}$ satisfying $\\mathcal{J}_{A}^{M}(\\pi)\\leq\\mathcal{J}_{\\mathcal{R}^{a\\nu g}}^{\\mathcal{M}}(\\pi^{\\prime}).$ ", "page_idx": 7}, {"type": "text", "text": "Note that Lemma 14 immediately proves that the reduction is not only optimality preserving (Theorem 12) but also robust: every $\\epsilon$ -approximately limit-average optimal policy is also $\\epsilon$ -approximately optimal w.r.t. $\\mathcal{I}_{A}^{M}$ . This observation is important because exactly optimal policies for the limit average problem may be hard to find. ", "page_idx": 7}, {"type": "text", "text": "Intuitively, to see part 1 of Lemma 14 we note: If an average reward of 1 is obtained for a run, the reward machine believes, based on the partial observation of the product MDP, that the run ends up in an ASEC. Almost surely, we eventually discover all possible transitions involving the same state-action pairs as this ASEC and therefore this must also be an ASEC w.r.t. the true, unknown product MDP. For part 2, we modify the policy $\\pi$ similarly as in Lemma 10 by selecting actions $\\operatorname{Act}(s_{t},q_{t})$ once having entered an ASEC ${\\mathcal{C}}=(T,\\mathrm{Act})$ w.r.t. the true, unknown product MDP.6 ", "page_idx": 7}, {"type": "text", "text": "6 Convergence for Limit Average, $\\omega$ -Regular and LTL Objectives ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Thanks to the described translation, advances (in both theory and practice) in the study of RL with average rewards carry over to RL with $\\omega$ -regular and LTL objectives. In this section, we show that it is possible to learn optimal policies for limit average rewards in the limit. Hence, we resolve an open problem [2]: also RL with $\\omega$ -regular and LTL objectives can also be learned in the limit. ", "page_idx": 7}, {"type": "text", "text": "We start with the case of simple reward functions $\\mathcal{R}:S\\times A\\times S\\rightarrow\\mathbb{R}$ . Recently, [18, Theorem 4.2] have shown that discount optimal policies for sufficiently high discount factor $\\overline{{\\gamma}}\\in[0,1)$ are also limit average optimal.7 This result alone is not enough to demonstrate Theorem 16 since $\\overline{\\gamma}$ is generally not known and in finite time we might only obtain approximately limit average optimal policies. ", "page_idx": 7}, {"type": "text", "text": "Our approach is to reduce RL with average rewards to a sequence of discount sum problems with increasingly high discount factor, which are solved with increasingly high accuracy. Our crucial insight is that eventually the approximately optimal solutions to the discounted problems will also be limit average optimal (see Appendix D for a proof): ", "page_idx": 7}, {"type": "text", "text": "Lemma 15. Suppose $\\gamma_{k}\\neq1,\\,\\epsilon_{k}\\searrow0$ and suppose each $\\pi_{k}$ is a memoryless policy. Then there exists $k_{0}$ such that for all $K\\ni k\\geq k_{0}$ , $\\pi_{k}$ is limit average optimal, where $K$ is the set of $k\\in\\mathbb{N}$ satisfying $\\bar{\\mathcal{I}}_{\\mathcal{R}^{\\gamma_{k}}}^{M}(\\pi_{k})\\breve{\\geq}\\mathcal{J}_{\\mathcal{R}^{\\gamma_{k}}}^{M}(\\pi)-\\epsilon_{k}$ for all memoryless policies $\\pi$ . ", "page_idx": 7}, {"type": "text", "text": "Our proof harnesses yet another notion of optimality: a policy $\\pi$ is Blackwell optimal (cf. [6] and [22, Sec. 8.1]) if there exists $\\overline{{\\gamma}}\\,\\in\\,(0,1)$ such that $\\pi$ is $\\gamma$ -discount optimal for all $\\overline{{\\gamma}}\\le\\gamma<1$ . It is well-known that memoryless Blackwell optimal policies always exist [6, 18] and they are also limit-average optimal [22, 18]. ", "page_idx": 7}, {"type": "text", "text": "Thanks to the PAC (probably approximately correct) learnability of RL with discounted rewards [26, 36], there exists an algorithm Discounted which receives as inputs a simulator for $\\mathcal{M},\\mathcal{R}$ as well as $\\gamma$ , $\\epsilon$ and $\\delta$ , and with probability $1-\\delta$ returns an $\\epsilon\\cdot$ -optimal memoryless policy for discount factor $\\gamma$ . In view of Lemma 15, our approach is to run the PAC algorithm for discount-sum RL for increasingly large discount factors $\\gamma$ and increasingly low $\\delta$ and $\\epsilon$ (Algorithm 1). ", "page_idx": 7}, {"type": "table", "img_path": "iykao97YXf/tmp/39ade346b06c8c562b29db6d217bcd37b15c2b2cd94fd3db55b8f5656ba6ff1a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Theorem 16. RL with average reward functions can be learned in the limit by Algorithm 1: almost surely there exists $k_{0}\\in\\mathbb{N}$ such that $\\pi_{k}$ is limit-average optimal for $k\\geq k_{0}$ . ", "page_idx": 8}, {"type": "text", "text": "Proof. Using the definition for $K$ of Lemma 15 of iterations where the PAC-MDP algorithm succeeds, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\#(\\mathbb{N}\\setminus K)\\right]\\leq\\sum_{k\\in\\mathbb{N}}\\mathbb{P}[\\mathrm{PAC-MDP~fails~in~iteration~}k]\\leq\\sum_{k\\in\\mathbb{N}}\\delta_{k}=\\sum_{k\\in\\mathbb{N}}\\frac{1}{k^{2}}<\\infty\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The claim follows immediately with Lemma 15. ", "page_idx": 8}, {"type": "text", "text": "Next, we turn to the more general case of reward machines. [23, 24] observe that optimal policies for reward machines can be learned by learning optimal policies for the modified MDP which additionally tracks the state the reward machine is in and assigns rewards accordingly. We conclude at once: ", "page_idx": 8}, {"type": "text", "text": "Corollary 17. RL with average reward machines can be learned in the limit. ", "page_idx": 8}, {"type": "text", "text": "Finally, harnessing Theorem 12 we resolve Open Problem 4 of [2]: ", "page_idx": 8}, {"type": "text", "text": "Theorem 18. RL with $\\omega$ -regular and LTL objectives can be learned in the limit. ", "page_idx": 8}, {"type": "text", "text": "Discussion. Algorithm 1 makes independent calls to black box algorithms for discount sum rewards. Many such algorithms with PAC guarantees are model based (e.g. [26, 36]) and sample from the MDP to obtain suitable approximations of the transition probabilities. Thus, Algorithm 1 can be improved in practice by re-using approximations obtained in earlier iterations and refining them. ", "page_idx": 8}, {"type": "text", "text": "7 Related Work and Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The connection between acceptance of $\\omega$ -regular languages in the product MDP and AECs is wellknown in the field of probabilistic model checking [4, 12]. As an alternative to DRAs [41, 14, 32], Limit Deterministic B\u00fcchi Automata [35] have been employed to express $\\omega$ -regular languages for RL [38, 7, 10, 20, 21]. ", "page_idx": 8}, {"type": "text", "text": "A pioneering work on RL for $\\omega$ -regular rewards is [41], which expresses $\\omega$ -regular objectives using Deterministic Rabin Automata. Similar RL approaches for $\\omega$ -regular objectives can also be found in [14, 38, 10, 15]. The authors of [15, 30] approach RL for $\\omega$ -regular objectives directly by studying the reachability of AECs in the product MDP and developing variants of the R-MAX algorithm [8] to find optimal policies. However, these approaches require prior knowledge of the MDP, such as the structure of the MDP, the optimal $\\epsilon_{}$ -return mixing time [15], or the $\\epsilon$ -recurrence time [30]. ", "page_idx": 8}, {"type": "text", "text": "Various studies have explored reductions of $\\omega$ -regular objectives to discounted rewards, and subsequently applied Q-learning and its variants for learning optimal policies [7, 32, 20, 21, 16]. In a similar spirit, [39] present a translation from LTL objectives to eventual discounted rewards, where only strictly positive rewards are discounted. These translations are generally not optimality preserving unless the discount factor is selected in a suitable way. Again, this is impossible without prior knowledge of the exact probability transition functions in the MDP. ", "page_idx": 8}, {"type": "text", "text": "[25] propose a translation to limit-average rewards for $\\omega$ -regular specifications which are also absolute liveness properties. (In particular, optimal policies satisfy such specifications with either probability 0 or 1.) Their translation is optimality-preserving provided the MDP is communicating and the magnitute of penalty rewards in their construction are chosen sufficiently large (which requires knowledge of the MDP). ", "page_idx": 8}, {"type": "text", "text": "Furthermore, whilst there are numerous convergent RL algorithms for average rewards for unichain or communicatin $g^{8}$ MDPs (e.g. [8, 43, 17, 33, 3, 40]), it is unknown whether such an algorithm exists for general multichain MDPs with a guaranteed convergence property. In fact, a negative result in [2, 5] shows that there is no PAC (probably approximately correct) algorithm for LTL objectives and limit-average rewards when the MDP transition probabilities are unknown. ", "page_idx": 9}, {"type": "text", "text": "[8] have proposed an algorithm with PAC guarantees provided $\\epsilon$ -return mixing times are known. They informally argue that for fixed sub-optimality tolerance $\\epsilon$ , this assumption can be lifted by guessing increasingly large candidates for the $\\epsilon$ -return mixing time. This yields $\\epsilon_{}$ -approximately optimal policies in the limit. However, it is not clear how to asymptotically obtain exactly optimal policies as this would require simultaneously decreasing $\\epsilon$ and increasing guesses for the $\\epsilon$ -return mixing time (which depends on $\\epsilon$ ). ", "page_idx": 9}, {"type": "text", "text": "Conclusion. We have presented an optimality-preserving translation from $\\omega$ -regular objectives to limit-average rewards furnished by reward machines. As a consequence, off-the-shelf RL algorithms for average rewards can be employed in conjunction with our translation to learn policies for $\\omega$ -regular objectives. Furthermore, we have developed an algorithm asymptotically learning provably optimal policies for limit-average rewards. Hence, also optimal policies for $\\omega$ -regular and LTL objectives can be learned in the limit. Our results provide affirmative answers to two open problems in [2]. ", "page_idx": 9}, {"type": "text", "text": "Limitations. We focus on MDPs with finite state and action sets and assume states are fully observable. The assumption of Section 4 that the support of the MDP\u2019s probability transition function is known is eliminated in Section 5. Whilst the size of our general translation\u2014the first optimalitypreserving translation\u2014is exponential, the additional knowledge in Section 4 enables a construction of the reward machine of the same size as the DRA expressing the objective. Hence, we conjecture that this size is minimal relative to the DRA specification. Since RL with average rewards is not PAC learnable, we cannot possibly provide finite-time complexity guarantees of our Algorithm 1. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the National Research Foundation, Singapore, under its RSS Scheme (NRFRSS2022-009). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Luca Alfaro. Formal Verification of Probabilistic Systems. Phd thesis, Stanford University, Stanford, CA, USA, 1998.   \n[2] Rajeev Alur, Suguman Bansal, Osbert Bastani, and Kishor Jothimurugan. A framework for transforming specifications in reinforcement learning. In Jean-Fran\u00e7ois Raskin, Krishnendu Chatterjee, Laurent Doyen, and Rupak Majumdar, editors, Principles of Systems Design: Essays Dedicated to Thomas A. Henzinger on the Occasion of His 60th Birthday, pages 604\u2013624, Cham, 2022. Springer Nature Switzerland.   \n[3] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc., 2008.   \n[4] Christel Baier and Joost-Pieter Katoen. Principles of Model Checking. The MIT Press, 2008.   \n[5] Hugo Bazille, Blaise Genest, Cyrille J\u00e9gourel, and Jun Sun. Global PAC bounds for learning discrete time Markov chains. In Shuvendu K. Lahiri and Chao Wang, editors, Computer Aided Verification - 32nd International Conference, CAV 2020, Los Angeles, CA, USA, July 21-24, 2020, Proceedings, Part II, volume 12225 of Lecture Notes in Computer Science, pages 304\u2013326. Springer, 2020.   \n[6] David Blackwell. Discrete Dynamic Programming. The Annals of Mathematical Statistics, 33(2):719 \u2013 726, 1962.   \n[7] Alper Kamil Bozkurt, Yu Wang, Michael M. Zavlanos, and Miroslav Pajic. Control synthesis from Linear Temporal Logic specifications using model-free reinforcement learning. 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 10349\u201310355, 2019.   \n[8] Ronen I. Brafman and Moshe Tennenholtz. R-max - A general polynomial time algorithm for near-optimal reinforcement learning. J. Mach. Learn. Res., 3(null):213\u2013231, mar 2003.   \n[9] Tom\u00e1\u0161 Br\u00e1zdil, Krishnendu Chatterjee, Martin Chmelik, Vojte\u02c7ch Forejt, Jan K\u02c7ret\u00ednsk\\`y, Marta Kwiatkowska, David Parker, and Mateusz Ujma. Verification of Markov Decision Processes using learning algorithms. In Automated Technology for Verification and Analysis: 12th International Symposium, ATVA 2014, Sydney, NSW, Australia, November 3-7, 2014, Proceedings 12, pages 98\u2013114. Springer, 2014.   \n[10] Mingyu Cai, Shaoping Xiao, Zhijun Li, and Zhen Kan. Optimal probabilistic motion planning with potential infeasible LTL constraints. IEEE Transactions on Automatic Control, 68(1):301\u2013 316, 2023.   \n[11] Krishnendu Chatterjee and Monika Henzinger. Faster and dynamic algorithms for maximal end-component decomposition and related graph problems in probabilistic verification. In Dana Randall, editor, Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011, San Francisco, California, USA, January 23-25, 2011, pages 1318\u20131336. SIAM, 2011.   \n[12] Luca de Alfaro. Computing minimum and maximum reachability times in probabilistic systems. In Jos C. M. Baeten and Sjouke Mauw, editors, CONCUR\u201999 Concurrency Theory, pages 66\u201381, Berlin, Heidelberg, 1999. Springer Berlin Heidelberg.   \n[13] Vektor Dewanto, George Dunn, Ali Eshragh, Marcus Gallagher, and Fred Roosta. Averagereward model-free reinforcement learning: A systematic review and literature mapping, 2021.   \n[14] Xuchu Ding, Stephen L. Smith, Calin Belta, and Daniela Rus. Optimal control of Markov Decision Processes with Linear Temporal Logic constraints. IEEE Transactions on Automatic Control, 59(5):1244\u20131257, 2014.   \n[15] Jie Fu and Ufuk Topcu. Probably approximately correct MDP learning and control with Temporal Logic constraints. In Dieter Fox, Lydia E. Kavraki, and Hanna Kurniawati, editors, Robotics: Science and Systems X, University of California, Berkeley, USA, July 12-16, 2014, 2014.   \n[16] Qitong Gao, Davood Hajinezhad, Yan Zhang, Yiannis Kantaros, and Michael M. Zavlanos. Reduced variance deep reinforcement learning with Temporal Logic specifications. In Proceedings of the 10th ACM/IEEE International Conference on Cyber-Physical Systems, ICCPS \u201919, page 237\u2013248, New York, NY, USA, 2019. Association for Computing Machinery.   \n[17] Abhijit Gosavi. Reinforcement learning for long-run average cost. European Journal of Operational Research, 155(3):654\u2013674, 2004. Traffic and Transportation Systems Analysis.   \n[18] Julien Grand-Cl\u00e9ment and Marek Petrik. Reducing Blackwell and average optimality to discounted MDPs via the Blackwell discount factor. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[19] Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, and Dominik Wojtczak. Omega-regular objectives in model-free reinforcement learning. In Tom\u00e1\u0161 Vojnar and Lijun Zhang, editors, Tools and Algorithms for the Construction and Analysis of Systems, pages 395\u2013412, Cham, 2019. Springer International Publishing.   \n[20] Hosein Hasanbeig, Daniel Kroening, and Alessandro Abate. Certified reinforcement learning with logic guidance. Artificial Intelligence, 322:103949, 2023.   \n[21] Mohammadhosein Hasanbeig, Daniel Kroening, and Alessandro Abate. Deep reinforcement learning with Temporal Logics. In Nathalie Bertrand and Nils Jansen, editors, Formal Modeling and Analysis of Timed Systems, pages 1\u201322, Cham, 2020. Springer International Publishing.   \nUS, Boston, MA, 2002.   \n[23] Rodrigo Toro Icarte. Reward Machines. Phd thesis, University of Toronto, 03 2022.   \n[24] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward machines for high-level task specification and decomposition in reinforcement learning. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2107\u20132116. PMLR, 10\u201315 Jul 2018.   \n[25] Milad Kazemi, Mateo Perez, Fabio Somenzi, Sadegh Soudjani, Ashutosh Trivedi, and Alvaro Velasquez. Translating omega-regular specifications to average objectives for model-free reinforcement learning. In Piotr Faliszewski, Viviana Mascardi, Catherine Pelachaud, and Matthew E. Taylor, editors, 21st International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2022, Auckland, New Zealand, May 9-13, 2022, pages 732\u2013741. International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS), 2022.   \n[26] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49:209\u2013232, 2002.   \n[27] Bakhadyr Khoussainov and Anil Nerode. Automata Theory and its Applications. Birkh\u00e4user Boston, Boston, MA, 2001.   \n[28] Achim Klenke. Probability Theory: A Comprehensive Course. Universitext. Springer London, 2014.   \n[29] Dexter Kozen. Theory of Computation. Springer, London, 2006.   \n[30] Mateo Perez, Fabio Somenzi, and Ashutosh Trivedi. A PAC learning algorithm for LTL and omega-regular objectives in MDPs. Proceedings of the AAAI Conference on Artificial Intelligence, 38(19):21510\u201321517, 2024.   \n[31] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., USA, 1st edition, 1994.   \n[32] Dorsa Sadigh, Eric S. Kim, Samuel Coogan, S. Shankar Sastry, and Sanjit A. Seshia. A learning based approach to control synthesis of Markov Decision Processes for Linear Temporal Logic specifications. In 53rd IEEE Conference on Decision and Control, pages 1091\u20131096, 2014.   \n[33] Anton Schwartz. A reinforcement learning method for maximizing undiscounted rewards. In International Conference on Machine Learning, 1993.   \n[34] Daqian Shao and Marta Kwiatkowska. Sample efficient model-free reinforcement learning from LTL specifications with optimality guarantees. IJCAI International Joint Conference on Artificial Intelligence, 2023-Augus:4180\u20134189, 2023.   \n[35] Salomon Sickert, Javier Esparza, Stefan Jaax, and Jan K\u02c7ret\u00ednsk\u00fd. Limit-deterministic B\u00fcchi automata for Linear Temporal Logic. In Swarat Chaudhuri and Azadeh Farzan, editors, Computer Aided Verification, pages 312\u2013332, Cham, 2016. Springer International Publishing.   \n[36] Alexander L. Strehl, Lihong Li, and Michael L. Littman. Reinforcement learning in finite MDPs: PAC analysis. J. Mach. Learn. Res., 10:2413\u20132444, 2009.   \n[37] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA, 2018.   \n[38] Cameron Voloshin, Hoang Le, Swarat Chaudhuri, and Yisong Yue. Policy optimization with Linear Temporal Logic constraints. Advances in Neural Information Processing Systems, 35:17690\u201317702, 2022.   \n[39] Cameron Voloshin, Abhinav Verma, and Yisong Yue. Eventual discounting Temporal Logic counterfactual experience replay. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 35137\u201335150. PMLR, 2023.   \n[40] Yi Wan, Abhishek Naik, and Richard S. Sutton. Learning and planning in average-reward Markov Decision Processes. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 10653\u201310662. PMLR, 2021.   \n[41] Eric M. Wolff, Ufuk Topcu, and Richard M. Murray. Robust control of uncertain Markov Decision Processes with Temporal Logic specifications. In 2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pages 3372\u20133379, 2012.   \n[42] Cambridge Yang, Michael L. Littman, and Michael Carbin. On the (in)tractability of reinforcement learning for LTL objectives. IJCAI International Joint Conference on Artificial Intelligence, pages 3650\u20133658, 2022.   \n[43] Shangdong Yang, Yang Gao, Bo An, Hao Wang, and Xingguo Chen. Efficient average reward reinforcement learning using constant shifting values. Proceedings of the AAAI Conference on Artificial Intelligence, 30(1), 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "iykao97YXf/tmp/e49349d2f9ef4e509a8046438dd572651dea5eea48a4693a026061b1ed2b3075.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "(a) An MDP $\\mathcal{M}$ where $\\lambda(s_{1},a,s_{1})\\,=\\,\\lambda(s_{3},b,s_{0})\\,=\\,\\{c\\}$ , (b) A DRA $\\mathcal{A}$ for the objective of visiting $s_{1}$ or and the rest are labeled with $\\varnothing$ . $s_{3}$ infinitely often where $F:=\\{(\\{q_{1}\\},\\bar{\\varnothing})\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Figure 3: Counter-example for prefix-independent objectives. ", "page_idx": 13}, {"type": "text", "text": "A Supplementary Materials for Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall that a $\\omega$ -regular language $L$ is prefix-independent if for every infinite label sequence $w\\_{\\mathrm{~\\scriptsize~\\in~}\\mathrm{~\\scriptsize~(2~}^{\\!\\mathscr{A}\\!\\mathscr{P}})^{\\omega}}$ , we have $w\\,\\in\\,L$ iff $w^{\\prime}\\in L$ for every suffix $w^{\\prime}$ of $w$ . We prove that there is no optimality-preserving translation for reward functions regardless of whether $L$ is prefix-independent or not. The prefix-dependent case was given in Section 3. Here we focus on the other case: ", "page_idx": 13}, {"type": "text", "text": "Proposition 19. There exists a tuple $(S,A,s_{0},\\lambda)$ and a prefix-independent $\\omega$ -regular language $L$ for which it is impossible to find a reward function $\\mathcal{R}:S\\times A\\times S\\rightarrow\\mathbb{R}$ such that for every probability transition $P$ , let $\\mathcal{M}=(S,A,s_{0},P,\\lambda)$ , then every $\\mathcal{R}^{a\\nu g}$ -optimal policy of $\\mathcal{M}$ is also $L$ -optimal (i.e. maximizing the probability of membership in $L$ ). ", "page_idx": 13}, {"type": "text", "text": "Proof. Our proof technique is based on the fact that we can modify the transition probability function. Consider the MDP in Fig. 3a, where the objective is to visit either $s_{1}$ or $s_{3}$ infinitely often. It can be checked that the DRA in Fig. 3b captures the given objective and the language accepted by $\\boldsymbol{\\mathcal{A}}$ is prefix-independent. There are only two deterministic memoryless policies: $\\pi_{1}$ , which consistently selects action $a$ , and $\\pi_{2}$ , which consistently selects action $b$ . For the sake of contradiction, let\u2019s assume the existence of a reward function $\\mathcal{R}$ that preserves optimality for every transition probability function $P$ . Pick $p_{1}=1$ and $p_{2}=0$ . Then $\\mathcal{J}_{A}^{\\dot{\\mathcal{M}}}(\\pi_{1})=1$ and $\\mathcal{J}_{A}^{\\tilde{\\mathcal{M}}}(\\pi_{2})=\\dot{0}$ , which implies that $\\pi_{1}$ is $\\boldsymbol{\\mathcal{A}}$ -optimal whereas $\\pi_{2}$ is not. Thus $\\mathcal{R}(s_{1},a,s_{1})\\,=\\,\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\bar{\\pi}_{1})\\,>\\,\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi_{2})\\,=\\,\\mathcal{R}(s_{0},b,s_{0})$ . Now, assume $p_{1},p_{2}\\ \\in\\ (0,1)$ . Accordingly, we have $\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi_{1})\\;\\geq\\;p_{1}\\mathcal{R}(s_{1},a,s_{1})$ and we can deduce that (e.g. by solving the linear equation system described in [31, $\\S8.2.3\\]\\$ $\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi_{2})\\;=$ $\\begin{array}{r}{\\frac{p_{2}}{2-p_{2}}\\mathcal{R}(s_{0},b,s_{0})+\\frac{1-p_{2}}{2-p_{2}}\\left(\\mathcal{R}(s_{0},b,s_{3})+\\mathcal{R}(s_{3},b,s_{0})\\right)}\\end{array}$ . As a result: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{p_{1}\\to1}\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi_{1})\\;\\geq\\;\\mathcal{R}(s_{1},a,s_{1})\\;>\\;\\mathcal{R}(s_{0},b,s_{0})\\;=\\;\\operatorname*{lim}_{p_{2}\\to1}\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi_{2})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Consequently, if $p_{1},p_{2}$ are sufficiently large then $\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi_{1})>\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi_{2})$ . However, this contradicts to the fact that $\\pi_{2}$ is $\\boldsymbol{\\mathcal{A}}$ -optimal and $\\pi_{1}$ is not, since $\\mathcal{J}_{A}^{M}(\\pi_{2})=1>p_{1}=\\mathcal{J}_{A}^{M}(\\pi_{1})$ . Hence, there is no such reward function $\\mathcal{R}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "B Supplementary Materials for Section 4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 20. Every AEC contains an ASEC. ", "page_idx": 13}, {"type": "text", "text": "Proof. Consider an AEC ${\\mathcal{C}}=(T,\\mathrm{Act})$ of $\\mathcal{M}_{A}$ . We will prove this by using induction on the number of actions in $\\mathcal{C}$ , denoted as $\\begin{array}{r}{\\mathsf{s i z e}({\\mathcal C}):=\\sum_{s\\in T}|\\operatorname{Act}(s)|\\geq1}\\end{array}$ . For the base case where $\\mathsf{s i z e}(\\mathcal{C})=1$ , it can be deduced that $\\mathcal{C}$ consists of only  one accepting state with a self-loop. Therefore, $\\mathcal{C}$ itself is an ASEC. ", "page_idx": 13}, {"type": "text", "text": "Now, let\u2019s assume that ${\\mathsf{s i z e}}({\\mathcal{C}})=k+1\\geq2.$ . If $\\mathcal{C}$ is already an ASEC, then we are done. Otherwise, there exists a state $s\\in T$ such that $|\\operatorname{Act}(s)|>1$ . Since $\\mathcal{C}$ is strongly connected, there exists a finite path $\\rho=s a s_{1}a_{1}\\ldots s_{n}a_{n}s_{F}$ where $s_{F}$ is an accepting state and all the states $s_{1},\\ldots,s_{n}$ are different from $s$ . Let $a^{\\prime}\\in\\operatorname{Act}(s)$ such that $a^{\\prime}\\neq a$ . We construct a new AEC $\\mathcal{C}^{\\prime}=(T^{\\prime},\\mathrm{Act}^{\\prime})$ by first removing $a^{\\prime}$ from $\\operatorname{Act}(s)$ and then removing all the states that are no longer reachable from $s$ along with their associated transitions. It is important to note that after the removal, $s_{F}\\in T^{\\prime}$ since we can reach $s_{F}$ from $s$ without taking the action $a^{\\prime}$ . (Besides, the graph is still strongly connected.) Since ${\\mathsf{s i z e}}({\\mathcal{C}}^{\\prime})\\leq k$ , we can apply the induction hypothesis to conclude that $\\mathcal{C}^{\\prime}$ contains an ASEC, thus completing the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma 8. Almost surely, if $\\rho$ is accepted by $\\boldsymbol{\\mathcal{A}}$ then $\\rho^{\\otimes}$ reaches a state in some ASEC $\\mathcal{C}_{i}$ of $\\mathcal{M}\\otimes\\mathcal{A}$ . ", "page_idx": 14}, {"type": "text", "text": "To proof this result, we recall a well-known result in probabilistic model checking that with probability of one (wpo), every run $\\rho$ of the policy $\\pi$ eventually stays in one of the ECs of $\\mathcal{M}_{A}$ and visits every transition in that EC infinitely often. To state this formally, we define for any run $\\rho=s_{0}a_{0}s_{1}\\cdot\\cdot\\cdot$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{InfSA}(\\rho):=\\{(s,a)\\in S\\times A\\ |\\ |\\{i\\in\\mathbb{N}\\mid s_{i}=s\\wedge a_{i}=a\\}|=\\infty\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "the set of state-action-pairs occurring infinitely often in $\\rho$ . Furthermore, a state-action set $\\chi\\subseteq S\\times A$ defines a sub-MDP $\\mathrm{sub}(\\chi):=(T,\\bar{\\mathrm{A}}\\mathrm{ct})$ , where ", "page_idx": 14}, {"type": "equation", "text": "$$\nT:=\\{s\\in S\\mid(s,a)\\in\\chi{\\mathrm{~for~some~}}a\\in A\\}\\qquad\\qquad{\\mathrm{Act}}(s):=\\{a\\mid(s,a)\\in\\chi\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 21 ([12]). P\u03c1\u223cD\u03c0M\u2297A[sub(InfSA(\u03c1)) is an end component] = 1. ", "page_idx": 14}, {"type": "text", "text": "For the sake of self-containedness, we recall the proof of [12]. ", "page_idx": 14}, {"type": "text", "text": "Proof. We start with two more definitions: for any sub-MDP $(T,\\mathrm{Act})$ [1], let ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{sa}(T,\\operatorname{Act}):=\\{(s,a)\\in T\\times A\\mid a\\in\\operatorname{Act}(s)\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "be the set of state-action pairs $(s,a)$ such that $a$ is enabled in $s$ . Finally, let ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Omega^{(T,\\mathrm{Act})}:=\\{\\rho\\in\\mathrm{Runs}(S,A)\\mid\\mathrm{InfSA}(\\rho)=\\mathrm{sa}(T,\\mathrm{Act})\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "be the set of runs such that action $a$ is taken infinitely often in state $s$ iff $s\\in T$ and $a\\in\\operatorname{Act}(s)$ . Note that the $\\Omega^{(T,\\mathrm{Act})}$ constitute a partition of $\\mathrm{Runs}(S,A)$ . ", "page_idx": 14}, {"type": "text", "text": "Therefore, it suffices to establish for any sub-MDP $(T,\\mathrm{Act})$ , $(T,\\mathrm{Act})$ is an end-component or $\\mathbb{P}[\\rho\\in\\Omega^{(T,\\mathrm{Act})}]=0$ . ", "page_idx": 14}, {"type": "text", "text": "Let $(T,\\mathrm{Act})$ be an arbitrary sub-MDP. First, suppose there exist $s\\in T$ and $a\\in\\mathrm{Act}(t)$ such that $\\begin{array}{r}{p:=\\sum_{s^{\\prime}\\in T}\\Delta(t,a,t^{\\prime})<1}\\end{array}$ . By definition each $\\rho\\in\\Omega^{(T,\\mathrm{Act})}$ takes action $a$ in state $s$ infinitely often. Hence, not only $\\mathbb{P}[\\rho\\in\\Omega^{(T,\\mathrm{Act})}]\\leq p^{k}$ for all $k\\in\\mathbb{N}$ but also $\\mathbb{P}[\\rho\\in\\Omega^{(T,\\mathrm{Act})}]=0$ . ", "page_idx": 14}, {"type": "text", "text": "Thus, we can assume that for all $s\\in T$ and $a\\in\\operatorname{Act}(t),\\sum_{s^{\\prime}\\in T}\\Delta(t,a,t^{\\prime})=1$ . If $\\Omega^{(T,\\mathrm{Act})}\\,=\\,\\emptyset$ then clearly $\\mathbb{P}[\\rho\\in\\Omega^{(T,\\mathrm{Act})}]=0$ follows. Otherwise, take any $\\bar{\\rho}=s_{0}a_{0}a_{1}\\cdot\\cdot\\cdot\\in\\Omega^{(T,\\mathrm{Act})}$ , and let $t,t^{\\prime}\\in T$ be arbitrary. We show that there exists a connecting path in $(T,\\to_{\\mathrm{Act}})$ , which implies that $(T,\\mathrm{Act})$ is an end component. ", "page_idx": 14}, {"type": "text", "text": "Evidently, there exists an index $i_{0}$ such that all state-action pairs occur infinitely often in $\\rho$ , i.e. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\{(s_{i_{0}},a_{i_{0}}),(s_{i_{0}+1},a_{i_{0}+1}),\\dots\\}=\\mathrm{InfSA}(\\rho)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, for all $i\\geq i_{0}$ , $s_{i}\\in T$ and $a_{i}\\in\\mathrm{Act}(s_{i})$ , and for all $i^{\\prime}>i\\geq i_{0}$ , there is a path from $s_{i}$ to $s_{i^{\\prime}}$ in $(T,\\to_{\\mathrm{Act}})$ . Finally, it suffices to note that clearly for some $i^{\\prime}>i=i_{0}$ , $s_{i}=t$ and $s_{i^{\\prime}}=t^{\\prime}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 8. By Lemma 21, almost surely $\\operatorname{sub}(\\operatorname{InfSA}(\\rho))$ is an accepting end component. Clearly, $\\rho$ is only accepted by the product MDP if this end component is an accepting EC. By Lemma 20 this AEC contains an ASEC. Therefore, by definition of $\\operatorname{sub}(\\operatorname{InfSA}(\\rho)),$ , $\\rho$ almost surely in particular enters some ASEC. Finally, since the $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}$ cover all states in ASECs, $\\rho$ almost surely enters some $\\mathcal{C}_{i}$ . \u53e3 ", "page_idx": 14}, {"type": "image", "img_path": "iykao97YXf/tmp/11925a7e890cc043983d1ce57bc3cbc5a2da37cb1f1df9548318736c63a1d908.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 4: Reward machine yielded by our construction in Section 4 for the running example. ", "page_idx": 15}, {"type": "text", "text": "Before turning to the proof of Lemma 10, let $\\begin{array}{r}{\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\rho)=\\operatorname*{lim}\\operatorname*{inf}_{t\\rightarrow\\infty}\\frac{1}{t}\\cdot\\sum_{i=0}^{t-1}\\;r_{i}}\\end{array}$ denote the limitaverage reward of a run $\\rho$ . Note that, for any run $\\rho$ , $\\mathcal{T}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\rho)\\,\\in\\,\\{0,1\\}$ . Thus, by the dominated convergence theorem [28, Cor. 6.26], ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi}^{M}}\\left[\\mathcal{I}_{\\mathcal{R}^{\\alpha_{\\mathtt{R}}}}^{M}(\\rho)=1\\right]\\ =\\ \\mathbb{E}_{\\rho\\sim\\mathcal{D}_{\\pi}^{M}}[\\mathcal{T}_{\\mathcal{R}^{\\alpha_{\\mathtt{R}}}}^{M}(\\rho)]\\ =\\ \\operatorname*{liminf}_{t\\to\\infty}\\mathbb{E}_{\\rho\\sim\\mathcal{D}_{\\pi}^{M}}\\left[\\frac{1}{t}\\cdot\\sum_{i=0}^{t-1}r_{i}\\right]\\ =\\ \\mathcal{I}_{\\mathcal{R}^{\\alpha_{\\mathtt{R}}}}^{M}(\\pi)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma 10. Let $P$ be a probability transition function with support $E$ and $\\mathcal{M}:=(S,A,s_{0},P)$ . ", "page_idx": 15}, {"type": "text", "text": "1. For every policy $\\pi$ , $\\mathcal{J}_{\\mathcal{R}^{a v g}}^{\\mathcal{M}}(\\pi)\\leq\\mathcal{J}_{A}^{\\mathcal{M}}(\\pi)$ .   \n2. For every policy $\\pi$ , there exists some policy $\\pi^{\\prime}$ satisfying $\\mathcal{J}_{A}^{M}(\\pi)\\leq\\mathcal{J}_{\\mathcal{R}^{a\\nu g}}^{\\mathcal{M}}(\\pi^{\\prime})$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. 1. For any run $\\rho$ , $\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\rho)=1$ only if $\\rho^{\\otimes}$ enters a $\\mathcal{C}_{i}$ and never leaves it. ( $\\rho^{\\otimes}$ might have entered other $\\mathcal{C}_{j}$ \u2019s earlier but then those necessarily need to overlap with yet another $\\mathcal{C}_{k}$ such that $i\\;\\leq\\;k\\;<\\;j$ to avoid being trapped in state $\\perp$ , resulting in $\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\rho)\\,=\\,1$ Furthermore, this $\\mathcal{C}_{i}$ can only overlap with $\\mathcal{C}_{j}$ if $i<j$ . Otherwise, the reward machine would have enforced transitioning to $\\mathcal{C}_{j}$ .) ", "page_idx": 15}, {"type": "text", "text": "Since $\\mathcal{C}_{i}$ is an ASEC, $\\rho^{\\otimes}$ is accepted by the product MDP $\\mathcal{M}\\otimes\\mathcal{A}$ . Hence, by Eqs. (2) and (3), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{J}_{\\mathcal{R}^{\\mathrm{seg}}}^{M}(\\pi)\\ =\\ \\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi}^{M}}\\left[\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{M}(\\rho)=1\\right]}&{\\leq\\ \\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi}^{M}}\\left[\\rho^{\\otimes}\\ \\mathrm{accepted}\\,\\boldsymbol{\\mathrm{by}}\\,\\mathcal{M}\\otimes\\boldsymbol{A}\\right]\\ =\\ \\mathcal{J}_{\\boldsymbol{A}}^{M}(\\pi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "2. Let $\\pi$ be arbitrary. For a run $s_{0}a_{0}\\cdot\\cdot\\cdot$ let $q_{t}$ be the state of the DRA in step $t$ . Define $\\pi^{\\prime}$ to follow $\\pi$ until reaching $s_{t}$ such that $(s_{t},q_{t})\\in T_{1}\\cup\\cdots\\cup T_{n}$ . Henceforth, we select the (unique) action guaranteeing to stay in the $\\mathcal{C}_{i}$ with minimal $i$ including the current state, i.e. $\\mathrm{Act}_{(q,u)}^{\\bar{}}(q,u)$ . Formally9, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi^{\\prime}(s_{0}a_{0}\\cdot\\cdot\\cdot s_{t})\\;:=\\;\\;{\\binom{\\pi(s_{0}a_{0}\\cdot\\cdot\\cdot s_{t})}{\\mathrm{Act}_{(s_{t},q_{t})}(s_{t},q_{t})}}\\;\\;\\;{\\mathrm{if~}}(s_{t},q_{t})\\not\\in T_{1}\\cup\\cdot\\cdot\\cdot\\cup T_{n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that whenever a run $\\rho\\sim\\mathcal{D}_{\\pi^{\\prime}}^{\\mathcal{M}}$ follows the modified policy $\\pi^{\\prime}$ and its induced run $\\rho^{\\otimes}$ reaches some ASEC $\\mathcal{C}_{i}$ then $\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\rho)=1$ . Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi^{\\prime}}^{M}}[\\rho^{\\otimes}\\mathrm{\\reaches~some\\}\\mathcal{C}_{i}]\\;\\leq\\;\\mathbb{E}_{\\rho\\sim\\mathcal{D}_{\\pi^{\\prime}}^{M}}[\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{M}(\\rho)]\\;=\\;\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{M}(\\pi^{\\prime})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, by Lemma 8 almost surely, every induced run $\\rho^{\\otimes}$ accepted by the product MDP must reach some $\\mathcal{C}_{i}$ . Consequently, by Eq. (2), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{J}_{A}^{M}(\\pi)=\\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi}^{\\mathcal{M}}}[\\rho^{\\otimes}\\mathrm{\\normalfont~is~accepted}\\;\\mathsf{b y}\\;\\mathcal{M}\\otimes\\mathcal{A}]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi}^{\\mathcal{M}}}[\\rho^{\\otimes}\\mathrm{\\normalfont~reaches~some}\\;\\mathcal{C}_{i}]}\\\\ &{\\qquad\\qquad=\\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi^{\\prime}}^{\\mathcal{M}}}[\\rho^{\\otimes}\\mathrm{\\normalfont~reaches~some}\\;\\mathcal{C}_{i}]\\leq\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the penultimate step, we have exploited the fact that $\\pi$ and $\\pi^{\\prime}$ agree until reaching the first $\\mathcal{C}_{i}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B.1 Efficient Construction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We consider a different collection $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}$ of ASECs: ", "page_idx": 16}, {"type": "text", "text": "Suppose $\\mathcal{C}_{1}^{\\prime},\\ldots..\\mathcal{C}_{n}^{\\prime}$ is a collection of AECs (not necessarily simple ones) containing all states in AECs. Then we consider ASECs $\\mathcal{C}_{1},...\\,\\mathcal{C}_{n}$ such that $\\mathcal{C}_{i}$ is contained in $\\mathcal{C}_{i}^{\\prime}$ . ", "page_idx": 16}, {"type": "text", "text": "The definition of the reward machine in Section 4.2 and the extension in Section 5 do not need to be changed. Next, we argue the following: ", "page_idx": 16}, {"type": "text", "text": "1. This collection can be obtained efficiently (in time polynomial in the size of the MDP and DRA). 2. Lemma 10 and hence the correctness result (Theorem 9) still hold. ", "page_idx": 16}, {"type": "text", "text": "For 1. it is well-known that a collection of maximal AECs (covering all states in AECs) can be found efficiently using graph algorithms [1, Alg. 3.1], [15, 11] and [4, Alg. 47 and Lemma 10.125]. Subsequently, Lemma 20 can be used to obtain an ASEC contained in each of them. In particular, note that the proof of Lemma 20 immediately gives rise to an efficient algorithm. (Briefly, we iteratively remove actions and states whilst querying reachability properties.) ", "page_idx": 16}, {"type": "text", "text": "For 2., the first part of Lemma 10 clearly still holds. For the second, we modify policy $\\pi$ as follows: Once, $\\pi$ enters a maximal accepting end component we select an action on the shortest path to the respective ASEC $\\mathcal{C}_{i}$ inside $\\mathcal{C}_{i}^{\\prime}$ . Once we enter one of the $\\mathcal{C}_{i}$ we follow the actions specified by the ASEC as before. Observe that the probability that under an AEC is entered is the same as the probability that one of the $\\mathcal{C}_{i}$ is entered under the modified policy. The lemma, hence Theorem 9, follow. ", "page_idx": 16}, {"type": "text", "text": "C Supplementary Materials for Section 5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 14. Suppose $\\mathcal{M}=(S,A,s_{0},P)$ is an arbitrary MDP. ", "page_idx": 16}, {"type": "text", "text": "1. For every policy $\\pi$ , $\\mathcal{J}_{\\mathcal{R}^{a v g}}^{\\mathcal{M}}(\\pi)\\leq\\mathcal{J}_{\\mathcal{A}}^{\\mathcal{M}}(\\pi)$ .   \n2. For every policy $\\pi$ , there exists some policy $\\pi^{\\prime}$ satisfying $\\mathcal{J}_{A}^{M}(\\pi)\\leq\\mathcal{J}_{\\mathcal{R}^{a\\nu g}}^{\\mathcal{M}}(\\pi^{\\prime})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. 1. For a run $\\rho$ , let $E_{\\rho}$ be the set of transitions encountered in the product MDP. Note that $\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\rho)=1$ only if $\\rho^{\\otimes}$ enters some $\\mathcal{C}_{i}^{E_{\\rho}}$ and never leaves it. $\\dot{\\rho}^{\\otimes}$ might have entered other $\\mathcal{C}_{j}^{E}\\mathrm{s}$ earlier for $E\\subseteq E_{\\rho}$ .) ", "page_idx": 16}, {"type": "text", "text": "With probability 1, $E_{\\rho}$ contains all the transitions present in $\\mathcal{C}_{i}^{E_{\\rho}}$ in the actual MDP. (NB possible transitions outside of $\\mathcal{C}_{i}^{E_{\\rho}}$ might be missing from $E_{\\rho}$ .) In particular, with probability $1,\\mathcal{C}_{i}^{E_{\\rho}}$ is also an ASEC for the true unknown MDP and $\\rho^{\\otimes}$ is accepted by the product MDP $\\mathcal{M}\\otimes\\mathcal{A}$ . Consequently, using Eq. (3) again, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}_{\\mathcal{R}^{\\mathrm{av}}}^{\\mathcal{M}}(\\pi)=\\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi}^{\\mathcal{M}}}[\\mathcal{J}_{\\mathcal{R}^{\\mathrm{av}}}^{\\mathcal{M}}(\\rho)=1]\\leq\\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi}^{\\mathcal{M}}}[\\rho^{\\otimes}\\operatorname{accepted}\\,\\mathsf{b y}\\ \\mathcal{M}\\otimes\\mathcal{A}]=\\mathcal{I}_{\\mathcal{A}}^{\\mathcal{M}}(\\pi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "2. Let $\\pi$ be arbitrary. We modify $\\pi$ to $\\pi^{\\prime}$ as follows: until reaching an ASEC ${\\mathcal{C}}=(T,\\mathrm{Act})$ w.r.t. the true, unknown10 set of transitions $E^{*}$ follow $\\pi$ . Henceforth, select action $\\mathrm{Act}_{(s_{t},q_{t})}^{E^{*}}(s_{t},q_{t})$ . We claim that whenever $\\rho\\sim\\mathcal{D}_{\\pi^{\\prime}}^{\\mathcal{M}}$ follows the modified policy $\\pi^{\\prime}$ and $\\rho^{\\otimes}$ reaches some ASEC in the true product MDP, $\\hat{\\mathcal{J}}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\rho)=1$ . \u00b7 \u00b7 \u00b7 \u222aT nE \u2217. Let C = (T, Act)\u03c0 := C(Est\u22170,qt0). To see this, suppose $\\rho\\sim\\mathcal{D}_{\\pi^{\\prime}}^{\\mathcal{M}}$ is such that for some minimal $t_{0}\\in\\mathbb{N}$ , $\\left(s_{t_{0}},q_{t_{0}}\\right)\\in T_{1}^{E^{*}}\\cup$ Define $E_{t}$ to be the transitions encountered up to step $t\\mathrm{~\\ensuremath~{~\\in~\\mathbb~{~N~}~}~}$ , i.e. $\\begin{array}{r l}{E_{t}}&{{}:=}\\end{array}$ $\\{((s_{k},q_{k}),a_{k},(s_{k+1},q_{k+1}))\\mid0\\leq k<t\\}$ . Then almost surely for some minimal $t\\geq t_{0}$ , ", "page_idx": 16}, {"type": "text", "text": "$E_{t}$ contains all transitions in $\\mathcal{C}$ , and no further transitions will be encountered, i.e. for all $t^{\\prime}\\geq t$ , $E_{t^{\\prime}}\\,=\\,E_{t}$ . Define ${\\overline{{E}}}:=E_{t}$ . Note that for all $((s,q),a,(s^{\\prime},q^{\\prime}))\\,\\in\\,\\overrightarrow{E}$ such that $(s,q)\\in T$ , $\\operatorname{Act}(s,q)=\\{a\\}$ . (This is because upon entering the ASEC $\\mathcal{C}$ we immediately switch to following the action dictated by Act. Thus, we avoid \u201caccidentally\u201d discovering other ASECs w.r.t. the partial knowledge of the product MDP\u2019s graph, which might otherwise force us to perform actions leaving $\\mathcal{C}$ .) Consequently, there cannot be another ASEC $\\mathcal{C}^{\\prime}=(T^{\\prime},\\mathrm{Act}^{\\prime})$ w.r.t. $\\overline{E}$ overlapping with $\\mathcal{C}$ , i.e. $T\\cap T^{\\prime}\\neq\\emptyset$ . Therefore, for all $(s,q)\\in{\\mathcal{C}}$ , $\\mathrm{Act}_{(s,q)}^{\\overline{{E}}}=\\mathrm{Act}$ . Consequently, $\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\rho)=1$ . ", "page_idx": 17}, {"type": "text", "text": "Thus, ", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{P}_{\\rho\\sim\\mathcal{D}_{\\pi^{\\prime}}^{M}}[\\rho^{\\otimes}\\mathrm{~reaches~some~ASEC~in~true~product~MDP}]\\le\\mathbb{E}_{\\rho\\sim\\mathcal{D}_{\\pi^{\\prime}}^{M}}[\\mathcal{T}_{\\mathcal{R}^{\\otimes}\\!\\mathfrak{x}}^{M}(\\rho)]=\\mathcal{T}_{\\mathcal{R}^{\\otimes}\\!\\mathfrak{x}}^{M}(\\pi^{\\prime})}\\end{array}$ Consequently, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{J}_{A}^{M}(\\pi)=\\mathbb{P}_{\\rho\\sim{\\cal D}_{\\pi}^{M}}[\\rho^{\\otimes}\\mathrm{~is~accepted~by~}{\\mathcal{M}}\\otimes A]}\\\\ &{\\quad\\quad\\quad\\leq\\mathbb{P}_{\\rho\\sim{\\cal D}_{\\pi}^{M}}[\\rho^{\\otimes}\\mathrm{~reaches~some~ASEC~in~true~product~MDP}]}\\\\ &{\\quad\\quad\\quad=\\mathbb{P}_{\\rho\\sim{\\cal D}_{\\pi^{\\prime}}^{M}}[\\rho^{\\otimes}\\mathrm{~reaches~some~ASEC~in~true~product~MDP}]\\leq\\mathcal{J}_{\\mathcal{R}^{\\infty}\\otimes}^{M}(\\pi^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the penultimate step we have exploited that $\\pi$ and $\\pi^{\\prime}$ agree until reaching some ASEC in true product MDP. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "D Supplementary Materials for Section 6 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $\\Pi$ be the set of all memoryless policies and $\\Pi^{*}$ be the set of all limit-average optimal policies.   \nBesides, let $w^{\\ast}:=\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi^{\\ast})$ the limit average reward of any optimal $\\pi^{*}\\in\\Pi^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 15 is proven completely analagously to the following (where $K=\\mathbb{N}$ ): ", "page_idx": 17}, {"type": "text", "text": "Lemma 22. Suppose $\\gamma_{k}\\nearrow1,\\,\\epsilon_{k}\\searrow0$ and each $\\pi_{k}$ is a memoryless policy satisfying $\\mathcal{I}_{\\mathcal{R}^{\\gamma_{k}}}^{\\mathcal{M}}(\\pi_{k})\\geq$ $\\mathcal{J}_{\\mathcal{R}^{\\gamma_{k}}}^{\\mathcal{M}}(\\pi)-\\epsilon_{k}$ for all $\\pi\\,\\in\\,\\Pi$ . Then there exists $k_{0}$ such that for all $k\\,\\geq\\,k_{0}$ , $\\pi_{k}$ is limit average optimal. ", "page_idx": 17}, {"type": "text", "text": "Proof. We define $\\begin{array}{r}{\\Delta:=\\operatorname*{min}_{\\pi\\in\\Pi\\backslash\\Pi^{*}}\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi)-w^{*}>0.}\\end{array}$ . Recall (see e.g. [22, Sec. 8.1]) that for any policy $\\pi\\in\\Pi$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\gamma\\mathcal{I}^{1}}(1-\\gamma)\\cdot\\mathcal{I}_{\\mathcal{R}^{\\gamma}}^{\\mathcal{M}}(\\pi)=\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\Pi$ is finite, due to Eq. (5) there exists $\\gamma_{0}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\mathcal{J}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi)-(1-\\gamma)\\cdot\\mathcal{J}_{\\mathcal{R}^{\\gamma}}^{\\mathcal{M}}(\\pi)|\\leq\\frac{\\Delta}{4}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $\\pi\\in\\Pi$ and $\\gamma\\in[\\gamma_{0},1)$ . Let $\\pi^{*}$ be a memoryless Blackwell optimal policy (which exists due to [6, 18]). Note that ", "page_idx": 17}, {"type": "equation", "text": "$$\nw^{\\ast}=\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi^{\\ast})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and there exists $\\overline{{\\gamma}}\\in[0,1)$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{I}_{\\mathcal{R}\\gamma}^{\\mathcal{M}}(\\pi^{*})\\geq\\mathcal{I}_{\\mathcal{R}\\gamma}^{\\mathcal{M}}(\\pi)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $\\gamma\\in[\\overline{{\\gamma}},1)$ and $\\pi\\in\\Pi$ . Moreover, there clearly exists $k_{0}$ such that $\\epsilon_{k}\\leq\\Delta/4$ and $\\gamma_{k}\\ge\\gamma_{0},\\overline{{\\gamma}}$ for all $k\\geq k_{0}$ . ", "page_idx": 17}, {"type": "text", "text": "Therefore, for any $k\\geq k_{0}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{I}_{\\mathcal{R}^{\\mathrm{avg}}}^{\\mathcal{M}}(\\pi_{k})-w^{*}|\\le(1-\\gamma_{k})\\cdot\\big|\\mathcal{I}_{\\mathcal{R}^{\\gamma_{k}}}^{\\mathcal{M}}(\\pi_{k})-\\mathcal{J}_{\\mathcal{R}^{\\gamma_{k}}}^{\\mathcal{M}}(\\pi^{*})\\big|+\\displaystyle\\frac{\\Delta}{2}}\\\\ &{\\qquad\\qquad\\qquad\\le(1-\\gamma_{k})\\cdot\\epsilon_{k}+\\displaystyle\\frac{\\Delta}{2}}\\\\ &{\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{4}{3}\\cdot\\Delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Consequently, by definition of $\\Delta$ $\\ensuremath{\\mathbf{\\ell}},\\pi_{k}\\in\\Pi^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The main results mentioned in the abstract and introduction are Proposition 4 and Theorems 12, 16 and 18. They accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Limitations are discussed in Section 7. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Full proofs are presented in the appendices and results are cross-referenced. At the beginning of Section 4 we assume knowledge of the support of the MDP\u2019s probability transition function for presentational purposes. This assumption is fully removed in Section 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]