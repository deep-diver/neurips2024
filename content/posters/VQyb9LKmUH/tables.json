[{"figure_path": "VQyb9LKmUH/tables/tables_6_1.jpg", "caption": "Table 1: KG reasoning results in various settings.", "description": "This table presents the performance of different KG reasoning models across three different reasoning settings: inductive, fully-inductive, and transductive.  The results are measured using Mean Reciprocal Rank (MRR) and Hits@10 (H@10), reflecting the ranking accuracy of the model's predictions.  The table compares the performance of the proposed KG-ICL model (both pre-trained and fine-tuned versions) against supervised state-of-the-art (SOTA) models and the ULTRA model (both pre-trained and fine-tuned versions). The average performance across all 43 datasets is also shown.", "section": "5.2 Main Results"}, {"figure_path": "VQyb9LKmUH/tables/tables_7_1.jpg", "caption": "Table 2: Ablation study results in various settings.", "description": "This table presents the ablation study results for the proposed KG-ICL model. It shows the performance (MRR and H@10) of the intact model and three variants with different modules removed: (1) the prompt graph, (2) the unified tokenizer, and (3) using GraIL's labeling instead of the proposed method. The results are broken down by reasoning setting (Inductive, Fully-inductive, Transductive), and overall average performance is reported.  This demonstrates the importance of each component in the model's overall effectiveness.", "section": "5.3 Further Analyses"}, {"figure_path": "VQyb9LKmUH/tables/tables_8_1.jpg", "caption": "Table 3: MRR results on diverse prompt graphs.", "description": "This table presents the Mean Reciprocal Rank (MRR) and Hits@10 (H@10) results for different prompt graph variants.  The goal was to determine the optimal prompt graph design for in-context KG reasoning.  The variants compared different methods of sampling nodes within the prompt graph, evaluating the impact of including only neighbors of the subject and object entities, as well as paths of different lengths (1-hop, 2-hop, 3-hop) between those entities.  The \"Neighbor & 3-hop path\" variant represents the model's design used in the main experiments of the paper.", "section": "5.3 Further Analyses"}, {"figure_path": "VQyb9LKmUH/tables/tables_17_1.jpg", "caption": "Table 1: KG reasoning results in various settings.", "description": "This table presents the results of KG reasoning experiments conducted across various settings, including inductive, fully-inductive, and transductive. It compares the performance of different models (Supervised SOTA, ULTRA pre-train, ULTRA finetune, KG-ICL pre-train, KG-ICL finetune) in terms of MRR (Mean Reciprocal Rank) and H@10 (Hits@10) metrics, providing insights into the effectiveness of the proposed KG-ICL model in different KG reasoning scenarios.", "section": "5.2 Main Results"}, {"figure_path": "VQyb9LKmUH/tables/tables_18_1.jpg", "caption": "Table 1: KG reasoning results in various settings.", "description": "This table presents the Mean Reciprocal Rank (MRR) and Hits@10 (H@10) scores achieved by different KG reasoning models across various knowledge graph (KG) datasets. The datasets are grouped into three categories based on reasoning settings: inductive, fully-inductive, and transductive.  The models evaluated include supervised state-of-the-art (SOTA) models, ULTRA (pre-train and finetune), and KG-ICL (pre-train and finetune). The results showcase the comparative performance of KG-ICL against baselines across diverse reasoning scenarios. ", "section": "5.2 Main Results"}, {"figure_path": "VQyb9LKmUH/tables/tables_19_1.jpg", "caption": "Table 1: KG reasoning results in various settings.", "description": "This table presents the Mean Reciprocal Rank (MRR) and Hits@10 scores achieved by different KG reasoning models across three categories of datasets: inductive, fully-inductive, and transductive.  The models compared include supervised state-of-the-art (SOTA) models, ULTRA (pre-train and finetune), and the proposed KG-ICL model (pre-train and finetune). The average results across all 43 datasets are also shown.", "section": "5.2 Main Results"}, {"figure_path": "VQyb9LKmUH/tables/tables_19_2.jpg", "caption": "Table 1: KG reasoning results in various settings.", "description": "This table presents the Mean Reciprocal Rank (MRR) and Hits@10 scores achieved by different models on 43 knowledge graphs (KGs). The KGs are categorized into three groups based on their reasoning settings: inductive, fully-inductive, and transductive.  The table compares the performance of the proposed KG-ICL model and its variants (KG-ICL (NBFNet), KG-ICL pre-train, KG-ICL finetune) against supervised state-of-the-art (SOTA) models and ULTRA pre-training models.  The results illustrate the performance of each model in different KG reasoning scenarios and overall performance across all KGs.", "section": "5.2 Main Results"}, {"figure_path": "VQyb9LKmUH/tables/tables_20_1.jpg", "caption": "Table 1: KG reasoning results in various settings.", "description": "This table presents the results of KG reasoning experiments conducted using various models on different datasets, categorized by reasoning setting (inductive, fully-inductive, transductive, and average across all settings).  For each setting and model, the Mean Reciprocal Rank (MRR) and Hits@10 metrics are reported. The table allows for a comparison of the performance of different models across various KG reasoning tasks.", "section": "5.2 Main Results"}, {"figure_path": "VQyb9LKmUH/tables/tables_20_2.jpg", "caption": "Table 1: KG reasoning results in various settings.", "description": "This table presents the Mean Reciprocal Rank (MRR) and Hits@10 scores achieved by different models on various knowledge graph reasoning tasks. The models are categorized into supervised state-of-the-art models, ULTRA pre-train, ULTRA finetune, KG-ICL pre-train, and KG-ICL finetune.  The results are further broken down by reasoning setting (inductive, fully inductive, and transductive) and show the average performance across all three settings. This allows for a comprehensive comparison of the proposed KG-ICL model to other methods in different reasoning scenarios.", "section": "5.2 Main Results"}, {"figure_path": "VQyb9LKmUH/tables/tables_21_1.jpg", "caption": "Table 1: KG reasoning results in various settings.", "description": "This table presents the Mean Reciprocal Rank (MRR) and Hits@10 scores achieved by different models (Supervised SOTA, ULTRA pre-train, ULTRA finetune, KG-ICL pre-train, and KG-ICL finetune) across three different KG reasoning settings: inductive, fully-inductive, and transductive.  Each setting represents a different level of challenge in terms of unseen entities and relations. The average performance across all three settings is also shown.  This provides a comparison of the proposed KG-ICL model to state-of-the-art supervised methods and a pre-trained model.", "section": "5.2 Main Results"}, {"figure_path": "VQyb9LKmUH/tables/tables_21_2.jpg", "caption": "Table 1: KG reasoning results in various settings.", "description": "This table presents the Mean Reciprocal Rank (MRR) and Hits@10 metrics for different KG reasoning models across three settings: inductive, fully-inductive, and transductive.  It compares the performance of the proposed KG-ICL model (both pre-trained and fine-tuned versions) against supervised state-of-the-art models and the ULTRA model. The results are presented as averages across multiple knowledge graphs (KGs) within each setting, showcasing the model's universal reasoning capabilities.", "section": "5.2 Main Results"}, {"figure_path": "VQyb9LKmUH/tables/tables_22_1.jpg", "caption": "Table 1: KG reasoning results in various settings.", "description": "This table presents the performance of different KG reasoning models on various datasets categorized by their reasoning settings (inductive, fully-inductive, and transductive).  It shows the Mean Reciprocal Rank (MRR) and Hits@10 scores for each model and dataset group, allowing comparison of the universal reasoning abilities across different scenarios and model types. The table includes results for Supervised SOTA (state-of-the-art supervised models), ULTRA (pre-train and finetune versions), and KG-ICL (pre-train and finetune versions).", "section": "5.2 Main Results"}]