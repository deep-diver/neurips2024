[{"figure_path": "haSKMlrbX5/tables/tables_8_1.jpg", "caption": "Table 1: With similar win rates, only BoNBoN does not modify the off-target attributes. The responses of the same prompt are drawn from models fine tuned by BONBON, DPO and IPO on the original HH data with no sampling technique. The win rate of each model is around 85%.", "description": "This table showcases example responses generated by different models (BONBON, DPO, and IPO) for the same prompt, all achieving similar win rates. The key observation is that BONBON, unlike the other methods, doesn't alter the off-target attributes of the responses.", "section": "5.2 BONBON achieves high win rate with little off-target deviation"}, {"figure_path": "haSKMlrbX5/tables/tables_23_1.jpg", "caption": "Table 2: More responses from different models. BONBON, DPO BON, and IPO BON all utilize best-and-worst-of-8 samples as training data.", "description": "This table showcases example responses generated by different LLM alignment methods (Reference, BONBON, DPO original HH, IPO original HH, DPO BON, IPO BON) in response to the prompt: \"what are some valentine day pranks?\"  The responses highlight the varying approaches to alignment and their effects on the model's output.  BONBON, DPO BON, and IPO BON all used best-of-8 samples during training, influencing their responses.", "section": "5 Experiments"}, {"figure_path": "haSKMlrbX5/tables/tables_24_1.jpg", "caption": "Table 3: More responses from different models. BONBON, DPO BON, and IPO BON all utilize best-and-worst-of-8 samples as training data.", "description": "This table shows example responses generated by different language models for the prompt: \"Is the world flat or round?\"  The models compared are a reference model, BONBON (Best-of-n), DPO (Direct Preference Optimization), and IPO (Indirect Preference Optimization).  All three models used best-and-worst-of-8 samples during training. The responses highlight the varying levels of factual accuracy and the different styles of responses generated by each model.", "section": "5 Experiments"}, {"figure_path": "haSKMlrbX5/tables/tables_25_1.jpg", "caption": "Table 4: More responses from different models. BONBON, DPO BON, and IPO BON all utilize best-and-worst-of-8 samples as training data.", "description": "This table shows example responses generated by different models (Reference, BONBON, DPO original HH, IPO original HH, DPO BON, IPO BON) to a prompt about cheating at poker.  The models are compared to show the different responses generated by each model in relation to ethics and strategy.  All the BON models utilized best-and-worst-of-8 samples as training data.", "section": "5 Experiments"}, {"figure_path": "haSKMlrbX5/tables/tables_26_1.jpg", "caption": "Table 5: More responses from different models. BONBON, DPO BON, and IPO BON all utilize best-and-worst-of-8 samples as training data.", "description": "This table showcases example responses generated by different models for the prompt: \"Is it safe to eat fish from a river in my city?\"  The models compared are a reference model, BONBON (a model trained using the BONBON alignment method), DPO original HH (Direct Preference Optimization trained on the original Helpful and Harmless dataset), IPO original HH (Inverse Propensity Optimization trained on the original Helpful and Harmless dataset), DPO BON (Direct Preference Optimization trained on best-of-n samples), and IPO BON (Inverse Propensity Optimization trained on best-of-n samples).  The responses highlight the varying levels of detail, safety considerations, and adherence to human preferences exhibited by each model.", "section": "5 Experiments"}, {"figure_path": "haSKMlrbX5/tables/tables_27_1.jpg", "caption": "Table 2: More responses from different models. BONBON, DPO BON, and IPO BON all utilize best-and-worst-of-8 samples as training data.", "description": "This table showcases example responses generated by different language models, including the baseline model and models trained using BONBON, DPO BON, and IPO BON methods.  Each model was trained using the best-and-worst responses from 8 samples. The table aims to illustrate the differences in model output quality and style as a result of different training approaches.  The prompts are designed to assess the models' ability to generate appropriate and harmless responses.", "section": "5 Experiments"}]