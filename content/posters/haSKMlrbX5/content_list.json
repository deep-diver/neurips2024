[{"type": "text", "text": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lin Gui1, Cristina G\u00e2rbacea2, and Victor Veitch1,2 ", "page_idx": 0}, {"type": "text", "text": "1Department of Statistics, University of Chicago   \n2Data Science Institute, University of Chicago ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper concerns the problem of aligning samples from large language models to human preferences using best-of- $n$ sampling, where we draw n samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of- $n$ and approaches to alignment that train LLMs to output samples with a high expected reward (e.g., RLHF or DPO)? To answer this, we embed both the best-of- $n$ distribution and the sampling distributions learned by alignment procedures in a common class of tiltings of the base LLM distribution. We then show that, within this class, best-of- $_n$ is essentially optimal in terms of the trade-off between win-rate against the base model vs KL distance from the base model. That is, best-of- $_n$ is the best choice of alignment distribution if the goal is to maximize win rate. However, best-of- $n$ requires drawing n samples for each inference, a substantial cost. To avoid this, the second problem we consider is how to fine-tune a LLM to mimic the best-of- $\\boldsymbol{\\cdot}\\boldsymbol{n}$ sampling distribution. We derive BoNBoN Alignment to achieve this by exploiting the special structure of the best-of- $n$ distribution. Experiments show that BoNBoN alignment yields substantial improvements in producing a model that is preferred to the base policy while minimally affecting off-target aspects. Code is available at https://github.com/gl-ybnbxb/BoNBoN. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper concerns the problem of aligning large language models (LLMs) to bias their outputs toward human preferences. There are now a wealth of approaches to this problem [e.g., Ouy+22; Chr $+17$ ; Kau $+23$ ; $\\mathrm{Li+24}$ ; Raf+23; Aza+24]. Here, we interested in the best-of-n (BoN) sampling strategy. In BoN sampling, we draw n samples from the LLM, rank them on the attribute of interest, and return the best one. This simple procedure is surprisingly effective in practice [Bei+24; Wan $+24$ ; GSH23; Eis $:+2:$ 3]. We consider two fundamental questions about BoN: ", "page_idx": 0}, {"type": "text", "text": "1. What is the relationship between BoN and other approaches to alignment?   \n2. How can we effectively train a LLM to mimic the BoN sampling distribution? ", "page_idx": 0}, {"type": "text", "text": "In brief: we find that the BoN distribution is (essentially) the optimal policy for maximizing win rate while minimally affecting off-target aspects of generation, and we develop an effective method for aligning LLMs to mimic this distribution. Together, these results yield a highly effective alignment method; see Figure 1 for an illustration. ", "page_idx": 0}, {"type": "text", "text": "LLM Alignment The goal of alignment is to bias the outputs of an LLM to be good on some target attribute (e.g., helpfulness), while minimally changing the behavior of the model on off-target attributes (e.g., reasoning ability). Commonly, the notion of goodness is elicited by collecting pairs of responses to many prompts, and asking (human or AI) annotators to choose the better response. Then, these pairs are used to define a training procedure for updating the base LLM to a new, aligned, LLM that outputs responses that are better in the target attribute. ", "page_idx": 0}, {"type": "image", "img_path": "haSKMlrbX5/tmp/f7d495d0adbe289bcfd4cb7d89f528fdf2169acf673c1df4bccc8b849dc42cf2.jpg", "img_caption": ["Figure 1: BoNBoN alignment achieves high win rates while minimally affecting off-target attributes of generation. Left: Average length of responses versus win rate of models aligned using each method on the Anthropic helpful and harmless single turn dialogue task, using $n=8$ . As predicted by theory, best-of-n achieves an excellent win rate while minimally affecting the off-target attribute length. Moreover, the BoNBoN aligned model effectively mimics this optimal policy, achieving a much higher win rate at low off-target drift than other alignment approaches. Right: Sample responses from models with similar win rates to BoNBoN. Other methods require higher off-target deviation to achieve a comparably high win rate. We observe that this significantly changes their behavior on off-target aspects. Conversely, BoNBoN only minimally changes off-target behavior. See section 5 for details. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "There are two main approaches. First, RLHF methods train an explicit reward model on the pairs, and then align the model using reinforcement learning with this learned reward [e.g., Ouy $+22$ ; Kau+23]. Second, contrastive methods directly use the preference data to define an objective function for fine-tuning the LLM [Raf+23; $\\mathrm{Aza}{+24}$ ; Eth $.+24$ ; $\\mathrm{Xu}{+}24$ ; HLT24]. In both cases, the trade-off between alignment and off-target behavior is controlled by a hyper-parameter that explicitly penalizes the divergence from the base LLM. For example, in the reinforcement learning setting, this is done by adding a regularization term that penalizes the estimated KL divergence between the aligned model and the reference model. ", "page_idx": 1}, {"type": "text", "text": "The first main question we address in this paper is: what is the relationship between the sampling distribution defined by these approaches and the sampling distribution defined by best-of- $\\cdot n^{\\star}$ ? This is important, in particular, because in principle we could forgo the explicit alignment training and just use BoN sampling. However, it is not clear when each option should be preferred. ", "page_idx": 1}, {"type": "text", "text": "Now, the comparison of training-aligned models and BoN is not fully fair. The reason is that producing a BoN sample requires drawing n samples from the base LLM (instead of just one). This is a substantial computational cost. The second main question we address is: if we do in fact want to sample from the BoN distribution, how can we train a LLM to mimic this distribution? If this can be done effectively, then the inference cost of BoN sampling can be avoided. ", "page_idx": 1}, {"type": "text", "text": "We answer these questions with the following contributions: ", "page_idx": 1}, {"type": "text", "text": "1. We show that the BoN sampling distribution can be embedded in a common class with the distributions produced by training-based alignment methods. Within this common class, we derive the distribution with the best possible trade-off between win-rate against the base model vs KL distance from the base model. Then, we show that the BoN distribution is essentially equal to this Pareto-optimal distribution.   \n2. We then develop an effective method for training a LLM to mimic the BoN sampling distribution. In essence, the procedure draws best-of- $n$ and worst-of- $n$ samples as training data, and combines these with an objective function we derive by exploiting the analytical form of the BoN distribution. We call this procedure BoNBoN Alignment.   \n3. Finally, we show empirically that BoNBoN Alignment yields models that achieve high win rates while minimally affecting off-target aspects of the generations, outperforming baselines. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a prompt $x$ , a large language model (LLM) samples a text completion Y . We denote the LLM by $\\pi$ and the sampling distribution of the completions by $\\pi(y\\mid x)$ . ", "page_idx": 2}, {"type": "text", "text": "Most approaches to alignment begin with a supervised fine-tuning step where the LLM is trained with the ordinary next-word prediction task on example data illustrating the target behavior. We denote the resulting model by $\\pi_{0}$ , and call it the reference model. The problem we are interested in is how to further align this model. ", "page_idx": 2}, {"type": "text", "text": "To define the goal, we begin with some (unknown, ground truth) reward function $r(x,y)$ that measures the quality of a completion $y$ for a prompt $x$ . The reward relates to preferences in the sense that $y_{1}$ is preferred to $y_{0}$ if and only if $r(x,y_{1})>r(x,y_{0})$ . Informally, the goal is to produce a LLM $\\pi_{r}$ where the samples have high reward, but are otherwise similar to the reference model. ", "page_idx": 2}, {"type": "text", "text": "The intuitive requirement that the aligned model should be similar to the reference model is usually formalized in terms of KL divergence. The context-conditional $K L$ divergence and the $K L$ divergence from $\\pi_{r}$ to $\\pi_{0}$ on a prompt set $D$ are defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{D}_{\\mathrm{KL}}(\\pi_{r}\\|\\pi_{0}\\mid x):=\\mathbb{E}_{y\\sim\\pi_{r}(y\\mid x)}\\biggl[\\log\\biggl(\\frac{\\pi_{r}(y\\mid x)}{\\pi_{0}(y\\mid x)}\\biggr)\\biggr],}\\\\ &{\\mathbb{D}_{\\mathrm{KL}}(\\pi_{r}\\|\\pi_{0}):=\\mathbb{E}_{x\\sim D}\\left[\\mathbb{D}_{\\mathrm{KL}}(\\pi_{r}\\|\\pi_{0}\\mid x)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We also need to define what it means for samples from the language model to have high reward. Naively, we could just look at the expected reward of the samples. However, in the (typical) case where we only have access to the reward through preference judgements, the reward is only identified up to monotone transformation. The issue is that expected reward value is not compatible with this unidentifiability.1 Instead, we consider the win rate of the aligned model against the reference model. The idea is, for a given prompt, draw a sample from the aligned model and a sample from the reference model, and see which is preferred. This can be mathematically formalized by defining the context-conditional win rate and the overall win rate on a prompt set $D$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\pi_{r}\\succ\\pi_{0}}\\mid\\boldsymbol{x}:=\\mathbb{P}_{Y\\sim\\pi_{r}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}\\big(r(x,Y)\\ge r(x,Y_{0})\\big),}\\\\ &{p_{\\pi_{r}\\succ\\pi_{0}}:=\\mathbb{E}_{x\\sim D}\\left[\\mathbb{P}_{Y\\sim\\pi_{r}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}(r(x,Y)\\ge r(x,Y_{0}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Reinforcement Learning from Human Feedback (RLHF) The most studied approach to alignment is RLHF. This procedure follows two steps. First, the reward function is explicitly estimated from preference data, using the Bradley-Terry [BT52] model. Second, this estimated reward function is used in a KL-regularized reinforcement learning procedure to update the LLM. Denoting the estimated reward function by $\\hat{r}$ , the objective function for the reinforcement learning step is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{R L H F}(\\pi_{\\theta};\\pi_{0})\\!=\\!-\\!\\mathbb{E}_{x\\sim D,y\\sim\\pi_{\\theta}(y\\mid x)}\\left[\\hat{r}(x,y)\\right]\\!+\\!\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi_{\\theta}\\Vert\\pi_{0}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $D$ is a prompt set and $\\beta$ is a hyper-parameter to control the deviation of $\\pi_{\\theta}$ from the reference model $\\pi_{0}$ . The policy $\\pi_{r}$ is learned by finding the minimizer of the objective function in (2.1); e.g., using PPO $\\mathord{\\left[S c\\mathrm{h}+17\\right]}$ . ", "page_idx": 2}, {"type": "text", "text": "Contrastive methods Contrastive methods use the preference data $D=\\{(x,y_{w},y_{l})\\}$ where $x$ is the prompt, and $y_{w}$ and $y_{l}$ are preferred and dis-preferred responses, directly to define an objective function for fine-tuning the LLM, avoiding explicitly estimating the reward function. For example, the DPO [Raf+23] objective is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D P O}(\\pi_{\\theta};\\pi_{0})=-\\mathbb{E}_{(x,y_{w},y_{l})\\sim D}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}(y_{w}\\mid x)}{\\pi_{0}(y_{w}\\mid x)}-\\beta\\log\\frac{\\pi_{\\theta}(y_{l}\\mid x)}{\\pi_{0}(y_{l}\\mid x)}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The aligned model is found by optimizing this objective directly (via gradient descent). ", "page_idx": 2}, {"type": "text", "text": "Bradley-Terry and Alignment Targets In RLHF, the reward function is estimated using the Bradley-Terry model, which relates noisy observed preferences to rewards by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{P}(y_{1}\\succ y_{0}\\mid x)=\\sigma(r(x,y_{1})\\!-\\!r(x,y_{0})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma(\\cdot)$ is the sigmoid function. In the particular case that the Bradley-Terry model is wellspecified, then it can be shown that the analytic solution to both (2.1) and (2.2) is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{r}^{\\mathrm{RLHF}}(y\\mid x)\\propto\\exp\\left\\{\\frac{1}{\\beta}r(x,y)\\right\\}\\pi_{0}(y\\mid x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "That is, the alignment procedures target an exponential tilting of the reference model by the reward function. Of course, it is not obvious when the Bradley-Terry model is well-specified, nor whether this particular tilting is a desirable target. Other works have considered explicitly or implicitly transforming the reward function to change the target distribution $[\\mathsf{W a n}+24$ ; $\\mathrm{Aza}{+}24]$ . Nevertheless, these works also take the target distribution to be a tilting of the reference distribution. ", "page_idx": 3}, {"type": "text", "text": "Best-of- $n$ sampling The best-of- $n$ procedure is as follows. Given a prompt $x$ , sample $y_{1},y_{2},\\ldots,y_{n}$ independently from the reference model $\\pi_{0}(y\\mid x)$ . Then, select the response with the highest reward $r(x,y_{i})$ as the final response. That is, ", "page_idx": 3}, {"type": "equation", "text": "$$\ny=y_{i}\\qquad{\\mathrm{such~that~}}r(x,y_{i})=\\operatorname*{max}_{1\\leq j\\leq n}r(x,y_{j}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Best-of- $n$ is Win-Rate vs KL Optimal ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The first question we address is: what is the relationship between the best-of- $n$ distribution, and the distribution induced by training-based alignment methods? ", "page_idx": 3}, {"type": "text", "text": "3.1 A Common Setting for Alignment Policies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin with the underlying distribution of best-of- $n$ sampling. Let $Q_{x}$ denote the cumulative distribution function of $r(x,Y_{0})$ , where $Y_{0}\\,\\sim\\,\\pi_{0}(\\cdot\\,\\vert\\;x)$ . Suppose $r(x,\\cdot):\\mathcal{Y}\\to\\mathbb{R}$ is an one-to-one mapping and $\\pi_{0}(y|x)$ is continuous2, then the conditional density of the best-of-n policy is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{r}^{(n)}(y\\mid x):=n Q_{x}(r(x,y))^{n-1}\\pi_{0}(y\\mid x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Compare this to the RLHF policy $\\pi_{r}^{\\mathrm{RLHF}}$ in (2.4). In both cases, the sampling distribution is a re-weighted version of the reference model $\\pi_{0}$ , where higher weights are added to those responses with higher rewards. The observation is that both of these distributions\u2014and most alignment policies\u2014can be embedded in a larger class of reward-weighted models. For any prompt $x$ and reward model $r$ , we can define the $f_{x}$ -aligned model as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{r}(y\\mid x)\\propto f_{x}(r(x,y))\\pi_{0}(y\\mid x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f_{x}$ is a non-decreasing function that may vary across different prompts. ", "page_idx": 3}, {"type": "text", "text": "With this observation in hand, we can directly compare different alignment strategies, and best-of- $n$ in particular, by considering the function $f_{x}$ defining the alignment policy. ", "page_idx": 3}, {"type": "text", "text": "3.2 Optimality: Win Rate versus KL divergence ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To understand when different alignment policies are preferable, we need to connect the choice of $f_{x}$ with a pragmatic criteria for alignment. The high-level goal is to produce a policy that samples high-reward responses while avoiding changing off-target attributes of the text. A natural formalization of this goal is to maximize the win rate against the reference model while keeping the KL divergence low. ", "page_idx": 3}, {"type": "image", "img_path": "haSKMlrbX5/tmp/2dc236c190eeddb46e0c716fb94053a28d294844bc2972e74c4ac6f20588baca.jpg", "img_caption": ["Figure 2: The BoN is essentially the same as the optimal policy in terms of win rate versus KL divergence. Left: The win rate versus KL divergence curves of BoN and optimal policy. Right: The win rate difference between optimal policy and BoN policy for different $n$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Optimal Policy Our aim is to find the policy with the highest possible win rate at each KL divergence level: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\;\\mathbb{E}_{x\\sim D}\\left[\\mathbb{P}_{Y\\sim\\pi(y\\mathrm{~}|\\boldsymbol{\\it x}),Y_{0}\\sim\\pi_{0}(y\\mathrm{~}|\\boldsymbol{\\it x})}(r(x,Y)\\ge r(x,Y_{0}))\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Now, this equation only depends on $Y$ through the reward function $r(x,y)$ . Defining $Q_{x}(r(x,Y))$ as the distribution of $r(x,Y)$ under $\\pi_{0}(Y\\mid x)$ , we can rewrite the objective as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim\\pi(y\\mid x)}\\left[Q_{x}(r(x,y))\\right]\\;{\\mathrm{subject~to~}}\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{0})=d,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By duality theory [BTN01], there is some constant $\\beta>0$ such that this problem is equivalent to: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim\\pi(y\\mid x)}\\left[Q_{x}(r(x,y))\\right]-\\beta\\left(\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{0})\\!-\\!d\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Now, we can immediately recognize this objective as the same as the RLHF objective in (2.1) with the transformed reward function $\\tilde{r}(x,y)\\mathop{=}Q_{x}(r(x,y))$ . Then, the analytic solution to this problem is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{r}^{\\mathrm{optimal}}\\propto\\pi_{0}(y|x)e^{c Q_{x}(r(x,y))},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $c$ is a constant determined by the $\\mathrm{KL}$ divergence penalty. ", "page_idx": 4}, {"type": "text", "text": "The following theorem makes the preceding argument precise. To simplify the argument, we will assume that the rewards assigned to outputs of the language model are continuous. This simplifying assumption ignores that there are only a countably infinite number of possible responses to any given prompt. However, given the vast number of possible responses, the assumption is mild in practice. Refer to appendix B for a more detailed discussion. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $\\pi_{r,c}^{o p t i m a l}\\;b e$ the solution to (3.3). Then, for all $x_{.}$ , the density of the optimal policy is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{r,c}^{o p t i m a l}(y\\mid x)=\\pi_{0}(y\\mid x)\\exp\\left\\{c Q_{x}(r(x,y))\\right\\}/Z_{r}^{c},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $Z_{r}^{c}$ is the normalizing constant, and $c$ is a positive constant such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\frac{(c-1)e^{c}+1}{e^{c}-1}}-\\log\\left({\\frac{e^{c}-1}{c}}\\right)=d.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, the context-conditional win rate and KL divergence of this optimal policy are ", "page_idx": 4}, {"type": "text", "text": "Since for any prompt $x$ , both the context conditional win rate and $K L$ divergence are constants, the overall win rate p\u03c0ro,pctimal \u03c00 and KL divergence $\\mathbb{D}_{\\mathrm{KL}}(\\pi_{r,c}^{o p t i m a l}\\|\\pi_{0})$ on any prompt set $D$ are also these values. [Proof]. ", "page_idx": 4}, {"type": "text", "text": "3.3 The best-of- $_n$ policy is essentially optimal ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Now, we\u2019d like to use the previous result to understand when the best-of- $n$ policy is desirable. The win rate and KL divergence can be calculated with essentially the same derivation: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. The context-conditional win rate and $K L$ divergence of the best-of- $^n$ policy are: ", "page_idx": 5}, {"type": "text", "text": "Since both are constants, the overall win rate p\u03c0(rn) \u03c00 and Kl divergence $\\mathbb{D}_{\\mathrm{KL}}(\\pi_{r}^{(n)}||\\pi_{0})$ on any prompts set D are the same values. [Proof]. ", "page_idx": 5}, {"type": "text", "text": "We now can contrast the win-rate vs $\\mathrm{KL}$ frontier of the best-of- $\\boldsymbol{\\cdot}\\boldsymbol{n}$ policy with the optimal policy. Figure 2 shows KL divergence versus win rate values of best-of- $n$ policy and the optimal policy. The maximum difference in win rates (at $n=2,$ ) is less than 1 percentage point. Larger values of $n$ approximate the optimal policy even more closely. In summary: ", "page_idx": 5}, {"type": "text", "text": "The best-of- $n$ policy is essentially optimal in terms of win rate versus KL divergence. ", "page_idx": 5}, {"type": "text", "text": "3.4 Implicit vs Explicit KL regularization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "RLHF and contrastive alignment methods include a hyper-parameter that attempts to explicitly control the trade-off between KL divergence and model reward. By contrast, best-of- $n$ only controls the KL drift implicitly. This can actually be a substantive advantage. There are two reasons. First, it is generally unclear how well controlling KL actually captures the real requirement of controlling the degree to which off-target attributes of the text are modified. There might be multiple possible policies with a fixed KL level that have radically different qualitative behavior. Second, in practice, the KL drift from the base policy needs to be estimated from a finite data sample. This may be extremely difficult\u2014it is a very high dimensional estimation problem. Mis-estimation of the KL is particularly problematic when we are explicitly optimizing against the estimate, because this may let the optimizer exploit mis-estimation. Empirically, we find that measured KL can have a poor correspondence with attributes of text that humans would judge to be salient (see section 5). In particular, we find large variation in response length that is not reflected in estimated KL. ", "page_idx": 5}, {"type": "text", "text": "The best-of- $n$ procedure avoids both problems, since it avoids the need to estimate the KL drift, and since it does not explicitly optimize against the KL drift. ", "page_idx": 5}, {"type": "text", "text": "4 BoNBoN: Best-of- $n$ fine tuning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "From section 3, we know that the best-of- $\\boldsymbol{\\cdot}\\boldsymbol{n}$ policy is essentially optimal in terms of win rate and KL divergence. Accordingly, it is often a good choice for the alignment policy. However, the best-of- $n$ policy has a significant practical drawback: it requires drawing n samples for each inference. This is a substantial computational expense. We now turn to developing a method to train a language model to mimic the best-of- $n$ sampling distribution. We call this method BoNBoN Alignment. ", "page_idx": 5}, {"type": "text", "text": "Setup The basic strategy here will be to use best-of- $n$ samples to train a language model to mimic the best-of- $_n$ policy. We produce the training data by sampling n responses from the reference model $\\pi_{0},$ and ranking them. The best and worst data are the samples with highest and lowest reward. Their corresponding best-of and worst-of $n$ sampling distributions are denoted as $\\pi_{r}^{(n)}$ and $\\pi_{r}^{(1)}$ . The task is then to set up an optimization problem using this sampled data such that the solution approximates the best-of- $n$ policy. To that end, we consider objective functions that have the best-of- $n$ policy as a minimizer in the infinite data limit. (In practice, as usual, we approximate the expectation with an average.) ", "page_idx": 5}, {"type": "text", "text": "SFT-BoN. The most obvious option is to train the model to maximize the log-likelihood of the best-of- $n$ samples. The associated objective is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{SFT-BoN}}(\\pi_{\\theta};\\pi_{0})=-\\mathbb{E}_{x\\sim D,y_{(n)}\\sim\\pi_{r}^{(n)}}\\big[\\log\\pi_{\\theta}(y_{(n)}\\mid x)\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and it is well-known that the minimizer is $\\pi_{r}^{(n)}$ . The training procedure is simply to minimize the sample-average version of this objective. We call this training method SFT-BoN because it is supervised fine-tuning on best-of- $n$ samples. Although SFT-BoN is valid theoretically, it turns out to be data inefficient, and we observe only marginal improvement over the reference model empirically (see section 5). ", "page_idx": 6}, {"type": "text", "text": "IPO-BoN. A limitation of the best-of- $n$ procedure is that it only makes use of the winning sample, throwing away the rest. Another intuitive option is to construct a pairwise dataset and train the language model by a contrastive method. Concretely, we construct the pairwise data by picking the best and worst responses. We want to construct an objective function using this paired data that has the best-of- $n$ policy as a minimizer. ", "page_idx": 6}, {"type": "text", "text": "The key result we require is: ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. For any fixed $n,$ , ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim D,y_{(n)}\\sim\\pi_{r}^{(n)},y_{(1)}\\sim\\pi_{r}^{(1)}}\\left[\\log\\frac{\\pi_{r}^{(n)}(y_{(n)}\\mid x)}{\\pi_{r}^{(n)}(y_{(1)}\\mid x)}-\\log\\frac{\\pi_{0}(y_{(n)}\\mid x)}{\\pi_{0}(y_{(1)}\\mid x)}\\right]=\\frac{1}{2\\beta_{n}^{*}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\beta_{n}^{*}=\\frac{1}{2(n-1)\\sum_{k=1}^{n-1}1/k}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "[Proof]. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Following this result, we define the contrastive objective function as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{IPO-BoN}}(\\pi_{\\theta};\\pi_{0})=\\mathbb{E}_{x\\sim D,y_{(n)}\\sim\\pi_{r}^{(n)},y_{(1)}\\sim\\pi_{r}^{(1)}}\\left[\\left(\\log\\frac{\\pi_{\\theta}(y_{(n)}\\mid x)}{\\pi_{\\theta}(y_{(1)}\\mid x)}-\\log\\frac{\\pi_{0}(y_{(n)}\\mid x)}{\\pi_{0}(y_{(1)}\\mid x)}-\\frac{1}{2\\beta_{n}^{*}}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The optimizer of this objective is a policy where the log-likelihood ratio of the best and worst samples is equal to that of the best-of- $n$ policy. We call this training method IPO-BoN because it is essentially the IPO objective on the best-and-worst samples, with a particular choice for the IPO hyper parameter. We emphasize that the IPO-BoN objective does not involve any hyper parameters, there is only one choice for $\\beta_{n}^{*}$ for each $n$ . ", "page_idx": 6}, {"type": "text", "text": "We find in section 5 that IPO-BoN is much more data efficient than the SFT-BoN. However, this method (like IPO) has the disadvantage that it only controls the likelihood ratios on the sampled data. In particular, this means that the optimizer can cheat by reducing the likelihood of both the winning and losing responses, so long as the loser\u2019s likelihood decreases more (so the ratio still goes up). Reducing the probability of both the winning and losing examples requires the optimized model to shift probability mass elsewhere. In practice, we find that it tends to increase the probability of very long responses. ", "page_idx": 6}, {"type": "text", "text": "BonBon Alignment We can now write the BoNBoN objective: ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The BoNBoN alignment objective is: ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{BoNBoN}}(\\pi_{\\theta};\\pi_{0})=\\alpha\\mathcal{L}_{\\mathrm{SFT-BoN}}(\\pi_{\\theta};\\pi_{0})+(1-\\alpha)\\mathcal{L}_{\\mathrm{IPO-BoN}}(\\pi_{\\theta};\\pi_{0}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{SFT-BoN}}$ and $\\mathcal{L}_{\\mathrm{IPO-BON}}$ are defined in (4.1) and (4.3), and $\\alpha$ is a hyper parameter that balance\u2212s the SFT and \u2212the IPO objectives. ", "page_idx": 6}, {"type": "text", "text": "We call the procedure BoNBoN because it is a combination of two objective functions that have the best-of- $n$ policy as a minimizer. Relative to SFT alone, BoNBoN can be understood as improving data efficiency by making use of the worst-of- $n$ samples. Relative to IPO alone, BoNBoN can be understood as preventing cheating by forcing the likelihood of the best-of- $n$ samples to be high. We emphasize that both objective functions target the same policy; neither is regularizing towards some confilcting objective. That is, the trade-off between win-rate and off-target change is handled implicitly by the (optimal) best-of- $n$ procedure. This is in contrast to approaches that manage this trade-off explicitly (and sub-optimally) by regularizing towards the reference model. Reflecting this, we choose $\\alpha$ so that the contribution of each term to the total loss is approximately equal. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We study two tasks: a) single-turn dialogue generation, for which we conduct experiments on the Anthropic Helpful and Harmless (HH) dataset $\\left[\\mathrm{Bai}{+}22\\right]$ and $b_{,}$ ) text summarization, for which we use the OpenAI TL;DR dataset $[S\\mathrm{ti}+20]$ . Due to computational constraints, we fliter the HH data to only keep prompts for which response length is less than 500 characters, resulting in 106,754 training dialogues. For TL;DR dataset, we discard instances where the input post length is less than 90 characters, resulting in 92,831 (14,764 prompts) training posts. Each example in both datasets contains a pair of responses that were generated by a large language model along with a label denoting the human-preferred response among the two generations. ", "page_idx": 7}, {"type": "text", "text": "We want to compare different alignment methods on their ground truth win rate. Accordingly, we need a ground truth ranker. To that end, we construct data by using an off-the-shelf reward model4 as our ground truth. (In particular, we relabel the human preferences). ", "page_idx": 7}, {"type": "text", "text": "As the reference model, we fine-tune Pythia-2.8b [Bid $+;$ 23] with supervised fine-tuning (SFT) on the human-preferred completions from each dataset. For alignment methods other than BoNBoN, we draw $n=8$ completions for each prompt, and we use the best and worst completions as training data for them. For BoNBoN, we vary $n$ from 2 to 8. ", "page_idx": 7}, {"type": "text", "text": "We use DPO and IPO as baselines for the alignment task. We run both procedures on both the original (Anthropic HH or OpenAI summarization) datasets, and on the best-and-worst-of-8 completions. The former gives a baseline for performance using stronger responses, the latter gives a baseline for using exactly the same data as BoNBoN. Both IPO and DPO include a hyper parameter $\\beta$ controlling regularization towards the reference model. We report results for each method run with several values of $\\beta$ . For BoNBoN, we use $\\alpha=0.005$ for all experiments. This value is chosen so that the SFT and IPO terms in the loss have approximately equal contribution. Further details can be found in appendix C. ", "page_idx": 7}, {"type": "text", "text": "5.2 BoNBoN achieves high win rate with little off-target deviation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We are interested in the win-rate vs off-target deviation trade-off. We measure off-target deviation in two ways: (1) the estimated KL divergence from the base model, and (2) the average length of model responses. Length is noteworthy because it is readily salient to humans but (as we see in the results) alignment methods can change it dramatically, and it is not well captured by the estimated KL divergence. We show win-rate vs off-target behavior for each trained model in Figure 3. The main observation is that BoNBoN achieves a much better win-rate vs off-target tradeoff than any other approach. In particular, DPO/IPO $\\beta$ values that achieve comparable win-rates result in high off-target deviation\u2014e.g., nearly doubling the average response length! ", "page_idx": 7}, {"type": "text", "text": "To further explore this point, we examine sample responses from baseline models with similar win-rates to BoNBoN. Examples are shown in Figure 1 and tables 1 and 6. Other approaches can dramatically change off-target behavior. ", "page_idx": 7}, {"type": "text", "text": "5.3 BoNBoN mimics the best-of- $n$ policy ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure 3 shows SFT and IPO fine-tuned on the best-of- $_n$ data. We observe that BoNBoN dramatically outperforms these methods at all values of $\\beta$ , and is closer to the (optimal) BoN distribution. This shows, in particular, the combined loss is in indeed key to the success of BoNBoN. ", "page_idx": 7}, {"type": "text", "text": "One substantial practical advantage of BoNBoN is that it is nearly hyper-parameter free. Because the goal is to mimic the best-of- $n$ distribution, which is known to be optimal, we do not need to sweep hyper-parameters for the \u2018best\u2019 choice of win-rate vs KL. In particular, the $\\beta$ term in IPO is analytically derived in Theorem 3. In Figure 5 we show the win rate vs off-target behavior ", "page_idx": 7}, {"type": "text", "text": "Summarization ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "haSKMlrbX5/tmp/f5e5eaec35ff8e513e937486c136a5d6871a39113302d146b4786000501103d1.jpg", "img_caption": ["Figure 3: BoNBoN achieves high win-rates while minimally affecting off-target aspects of generation. Each point is a model aligned with the indicated method. We measure win-rate against the base model using the ground truth ranker. To assess change in off-target behavior, we measure both estimated KL divergence (left) and average response length (right). Above: Comparison of BoNBoN with baselines for the summarization task. Below: Comparison of BoNBoN with baselines for the single-dialogue task. ", "Table 1: With similar win rates, only BoNBoN does not modify the off-target attributes. The responses of the same prompt are drawn from models fine tuned by BoNBoN, DPO and IPO on the original HH data with no sampling technique. The win rate of each model is around $85\\%$ . "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "haSKMlrbX5/tmp/b523c340c89387dee7076629d08031ae5bc7a295011ea8795ba6788be8810d09.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "for several other choices for $\\beta$ in the IPO term. We observe that, generally, the default $\\beta_{n}^{*}$ has an excellent win-rate vs off-target trade-off. Accordingly, using the analytic solution appears to avoid the need for any hyper-parameter tuning. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion and Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Best-of-n BoN sampling is widely used for LLMs [e.g., Sti+20; Nak+21; Liu $.+23$ ; Gul $+23$ ; Tou $+23$ ; GSH23]. Due to its practical importance, it has also attracted some recent theoretical attention [e.g., Mud+23; Be $+24$ ; Yan $\\cdot+24$ ; Jin+24]. Beirami et al. [Bei+24] show a closed form probability mass function of the BoN policy in discrete case and provide a new KL estimator for it. Yang et al. $[\\mathrm{Yan}+24]$ define the optimality in terms of minimizing the cross entropy given an upper bounded KL, and show that BoN is asymptotically equivalent to the optimal policy, which is in line with our findings. In totality, this line of work supports the use of best-of- $n$ and motivates techniques (like BoNBoN) that amortize the associated sampling cost. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Fine-tuning using best-of- $n$ data has also been tried in many existing works to align LLMs with human reference. Dong et al. $[\\mathrm{Don}+23]$ and Xiong et al. [Xio+23] apply best-of- $\\boldsymbol{\\cdot}\\boldsymbol{n}$ as training data and fine-tune the LLMs with different fine-tuning methods like supervised fine-tuning and iterative DPO. Touvron et al. $[\\mathrm{Tou}+23]$ draw best-of- $\\boldsymbol{\\cdot}\\boldsymbol{n}$ samples and do gradient updates in the iterative fine-tuning step to further reinforce the human-aligned reward. ", "page_idx": 9}, {"type": "text", "text": "LLM alignment There is extensive literature on aligning LLMs [e.g., Zie $+19$ ; YK21; Qin+22; She+23; Wan $+23$ ; Mud $+23$ ; Ouy $+22$ ; Zha $.+23$ ; Raf+23; Yua $.+23$ ; Aza $+24$ ; Eth $.+24$ ; Xu+24; HLT24; Wan $.+24$ ; Liu $\\cdot+24$ ; Par+24]. Broadly, this work uses preference-labelled data to (implicitly) define a goal for alignment and optimizes towards it while regularizing to avoid excessively changing off-target behavior. Relative to this line of work, this paper makes two main contributions. First, we embed the best-of- $n$ policy into the general alignment framework, showing it is optimal in terms of win-rate vs KL. Second, we derive BoNBoN alignment as a way of training an LLM to mimic the best-of- $\\boldsymbol{\\cdot}\\boldsymbol{n}$ distribution. Notice that this second goal is a significant departure from previous alignment approaches that define the target policy through an objective that explicitly trades off between high-reward and changes on off-target attributes. We do not have any regularization towards the reference model. This has some significant practical advantages. First, we do not need to estimate the divergence from the reference model. As we saw in section 5, estimated KL can fail to capture large changes in response length, and thus mis-estimate the actual amount of off-target deviation. Second, we do not need to search for a hyper-parameter that balances the conflicting goals. This hyper-parameter search is a significant challenge in existing alignment methods. (We do need to select $\\alpha$ , but this is easier since the aim is just to balance the loss terms rather than controlling a trade-off in the final solution.) ", "page_idx": 9}, {"type": "text", "text": "Alignment methods can be divided into those that operate online\u2014in the sense of drawing samples as part of the optimization procedure\u2014and offline. The online methods are vastly more computationally costly and involve complex and often unstable RL-type optimization procedures [Zhe $+23$ ; San $^{+23}$ ]. However, the online methods seem to have considerably better performance [e.g., Tan $+24\\overset{\\cdot}{\\,}$ ]. The results in the present paper suggest this gap may be artificial. Theoretically, we have shown that best-of- $\\boldsymbol{\\cdot}\\boldsymbol{n}$ is already essentially the optimal policy, and this policy can be learned with an offilne-type learning procedure. Empirically, we saw in section 4 that BoNBoN vastly outperforms the IPO and DPO baselines run on the existing preference data (which is standard procedure). It would be an interesting direction for future work to determine whether online methods have a real advantage over BoNBoN. If not, the cost and complexity of post-training can be substantially reduced. ", "page_idx": 9}, {"type": "text", "text": "Our empirical results also support the idea that alignment methods should use on-policy data even if these samples are relatively weak\u2014we see aligning with best-of- $n$ samples substantially outperforms aligning with the original HH or summarization completions. Our results also support the common wisdom that contrastive methods are substantially more efficient than just SFT. Interestingly, we have found that the main flaw of contrastive methods\u2014they cheat by pushing down the likelihood of preferred solutions, leading drift on off-target attributes\u2014can be readily fixed by simply adding in an extra SFT term. ", "page_idx": 9}, {"type": "text", "text": "The results here can be understood as studying a particular choice of reward transformation used for alignment. Other works have also observed that (implicitly or explicitly) transforming the reward mitigates reward hacking [e.g., Aza $+24$ ; Wan $.+24$ ; LSD24; Ska+22]. Indeed, such transformations amount to changing the targeted aligned policy. Our results show how to optimize win rate. However, this is not the only possible goal. For example, Wang et al. [Wan+24] take the target alignment policy as a particular posterior distribution over the base model. Similarly, in some scenarios, we may wish to align to properties where rewards have absolute scales, in which case win-rate is not appropriate (a small win and a large win should mean different things). Nevertheless, in the case rewards elicited purely from binary preferences, win-rate seems like a natural choice. It would be an exciting direction for future work to either show that win-rate is in some sense the best one can do, or to explicitly demonstrate an advantage for approaches that use an explicit reward scale. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Thanks to Alekh Agarwal for pointing out a typo in a previous version. This work is supported by ONR grant N00014-23-1-2591 and Open Philanthropy. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "$[\\mathrm{Aza}+24]$ M. G. Azar, Z. D. Guo, B. Piot, R. Munos, M. Rowland, M. Valko, and D. Calandriello. \u201cA general theoretical paradigm to understand learning from human preferences\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2024 (cit. on pp. 1, 2, 4, 10).   \n[Bai+22] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. \u201cTraining a helpful and harmless assistant with reinforcement learning from human feedback\u201d. In: arXiv preprint arXiv:2204.05862 (2022) (cit. on p. 8).   \n[Bei+24] A. Beirami, A. Agarwal, J. Berant, A. D\u2019Amour, J. Eisenstein, C. Nagpal, and A. T. Suresh. \u201cTheoretical guarantees on the best-of-n alignment policy\u201d. In: arXiv preprint arXiv:2401.01879 (2024) (cit. on pp. 1, 6, 9, 16).   \n[BTN01] A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization: analysis, algorithms, and engineering applications. 2001 (cit. on p. 5).   \n[Bid+23] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. \u201cPythia: a suite for analyzing large language models across training and scaling\u201d. In: International Conference on Machine Learning. PMLR. 2023 (cit. on p. 8).   \n[BT52] R. A. Bradley and M. E. Terry. \u201cRank analysis of incomplete block designs: i. the method of paired comparisons\u201d. In: Biometrika 3/4 (1952) (cit. on p. 3).   \n$\\left[\\mathsf{C h r}\\!+\\!17\\right]$ P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. \u201cDeep reinforcement learning from human preferences\u201d. In: Advances in Neural Information Processing Systems. 2017 (cit. on p. 1).   \n$[\\mathrm{Don}{+}23]$ H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan, S. Diao, J. Zhang, K. SHUM, and T. Zhang. \u201cRAFT: reward ranked finetuning for generative foundation model alignment\u201d. In: Transactions on Machine Learning Research (2023) (cit. on p. 10).   \n$[\\mathrm{Eis}{+}23]$ J. Eisenstein, C. Nagpal, A. Agarwal, A. Beirami, A. D\u2019Amour, D. Dvijotham, A. Fisch, K. Heller, S. Pfohl, D. Ramachandran, et al. \u201cHelping or herding? reward model ensembles mitigate but do not eliminate reward hacking\u201d. In: arXiv preprint arXiv:2312.09244 (2023) (cit. on p. 1).   \n[Eth $\\pm24$ ] K. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, and D. Kiela. \u201cKto: model alignment as prospect theoretic optimization\u201d. In: arXiv preprint arXiv:2402.01306 (2024) (cit. on pp. 2, 10).   \n[GSH23] L. Gao, J. Schulman, and J. Hilton. \u201cScaling laws for reward model overoptimization\u201d. In: International Conference on Machine Learning. PMLR. 2023 (cit. on pp. 1, 9).   \n$[G\\mathrm{ul}+23]$ 1 C. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang, C. Gu, et al. \u201cReinforced self-training (rest) for language modeling\u201d. In: arXiv preprint arXiv:2308.08998 (2023) (cit. on p. 9).   \n[HLT24] J. Hong, N. Lee, and J. Thorne. \u201cReference-free monolithic preference optimization with odds ratio\u201d. In: arXiv preprint arXiv:2403.07691 (2024) (cit. on pp. 2, 10).   \n[JH22] L. G. Jacob Hilton. Measuring Goodhart\u2019s law. 2022 (cit. on pp. 6, 16).   \n$[\\mathrm{Jin}+24]$ Y. Jinnai, T. Morimura, K. Ariu, and K. Abe. \u201cRegularized best-of-n sampling to mitigate reward hacking for language model alignment\u201d. In: arXiv preprint arXiv:2404.01054 (2024) (cit. on p. 9).   \n[Kau+23] T. Kaufmann, P. Weng, V. Bengs, and E. H\u00fcllermeier. \u201cA survey of reinforcement learning from human feedback\u201d. In: arXiv preprint arXiv:2312.14925 (2023) (cit. on pp. 1, 2).   \n[LSD24] C. Laidlaw, S. Singhal, and A. Dragan. \u201cPreventing reward hacking with occupancy measure regularization\u201d. In: arXiv preprint arXiv:2403.03185 (2024) (cit. on p. 10).   \n$[\\mathrm{Li}+24]$ K. Li, O. Patel, F. Vi\u00e9gas, H. Pfister, and M. Wattenberg. \u201cInference-time intervention: eliciting truthful answers from a language model\u201d. In: Advances in Neural Information Processing Systems (2024) (cit. on p. 1).   \n[Liu+23] T. Liu, Y. Zhao, R. Joshi, M. Khalman, M. Saleh, P. J. Liu, and J. Liu. \u201cStatistical rejection sampling improves preference optimization\u201d. In: arXiv preprint arXiv:2309.06657 (2023) (cit. on p. 9).   \n[Liu+24] T. Liu, Y. Zhao, R. Joshi, M. Khalman, M. Saleh, P. J. Liu, and J. Liu. \u201cStatistical rejection sampling improves preference optimization\u201d. In: The Twelfth International Conference on Learning Representations. 2024 (cit. on p. 10).   \n[Mud+23] S. Mudgal, J. Lee, H. Ganapathy, Y. Li, T. Wang, Y. Huang, Z. Chen, H.-T. Cheng, M. Collins, T. Strohman, et al. \u201cControlled decoding from language models\u201d. In: arXiv preprint arXiv:2310.17022 (2023) (cit. on pp. 9, 10).   \n[Nak+21] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. \u201cWebgpt: browser-assisted question-answering with human feedback\u201d. In: arXiv preprint arXiv:2112.09332 (2021) (cit. on p. 9).   \n[Ouy+22] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. \u201cTraining language models to follow instructions with human feedback\u201d. In: Advances in Neural Information Processing Systems. 2022 (cit. on pp. 1, 2, 10).   \n[Par+24] R. Park, R. Rafailov, S. Ermon, and C. Finn. \u201cDisentangling length from quality in direct preference optimization\u201d. In: arXiv preprint arXiv:2403.19159 (2024) (cit. on p. 10).   \n[Qin+22] L. Qin, S. Welleck, D. Khashabi, and Y. Choi. \u201cCold decoding: energy-based constrained text generation with langevin dynamics\u201d. In: Advances in Neural Information Processing Systems. 2022 (cit. on p. 10).   \n$[\\mathrm{Raf}{+}23]$ R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. \u201cDirect preference optimization: your language model is secretly a reward model\u201d. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023 (cit. on pp. 1\u20133, 10).   \n$[S a n+23]$ M. Santacroce, Y. Lu, H. Yu, Y. Li, and Y. Shen. \u201cEfficient rlhf: reducing the memory usage of ppo\u201d. In: arXiv preprint arXiv:2309.00754 (2023) (cit. on p. 10).   \n$\\left[S c\\mathrm{h}{+}17\\right]$ J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. \u201cProximal policy optimization algorithms\u201d. In: arXiv preprint arXiv:1707.06347 (2017) (cit. on p. 3).   \n[She+23] T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and D. Xiong. \u201cLarge language model alignment: a survey\u201d. In: arXiv preprint arXiv:2309.15025 (2023) (cit. on p. 10).   \n[Ska+22] J. Skalse, N. Howe, D. Krasheninnikov, and D. Krueger. \u201cDefining and characterizing reward gaming\u201d. In: Advances in Neural Information Processing Systems (2022) (cit. on p. 10).   \n$[S t i+20]$ N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. \u201cLearning to summarize with human feedback\u201d. In: Advances in Neural Information Processing Systems. 2020 (cit. on pp. 8, 9).   \n[Tan+24] Y. Tang, D. Z. Guo, Z. Zheng, D. Calandriello, Y. Cao, E. Tarassov, R. Munos, B. \u00c1vila Pires, M. Valko, Y. Cheng, and W. Dabney. Understanding the performance gap between online and offline alignment algorithms. 2024 (cit. on p. 10).   \n[Tou+23] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. \u201cLlama 2: open foundation and fine-tuned chat models\u201d. In: arXiv preprint arXiv:2307.09288 (2023) (cit. on pp. 9, 10).   \n[Wan+23] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang, and Q. Liu. \u201cAligning large language models with human: a survey\u201d. In: arXiv preprint arXiv:2307.12966 (2023) (cit. on p. 10).   \n[Wan+24] Z. Wang, C. Nagpal, J. Berant, J. Eisenstein, A. D\u2019Amour, S. Koyejo, and V. Veitch. Transforming and Combining Rewards for Aligning Large Language Models. 2024 (cit. on pp. 1, 4, 10).   \n[Xio+23] W. Xiong, H. Dong, C. Ye, Z. Wang, H. Zhong, H. Ji, N. Jiang, and T. Zhang. \u201cIterative preference learning from human feedback: bridging theory and practice for rlhf under kl-constraint\u201d. In: ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2023 (cit. on p. 10).   \n$[X\\mathfrak{u}+24]$ H. Xu, A. Sharaf, Y. Chen, W. Tan, L. Shen, B. Van Durme, K. Murray, and Y. J. Kim. \u201cContrastive preference optimization: pushing the boundaries of llm performance in machine translation\u201d. In: arXiv preprint arXiv:2401.08417 (2024) (cit. on pp. 2, 10).   \n[Yan+24] J. Q. Yang, S. Salamatian, Z. Sun, A. T. Suresh, and A. Beirami. \u201cAsymptotics of language model alignment\u201d. In: arXiv preprint arXiv:2404.01730 (2024) (cit. on pp. 9, 10).   \n[YK21] K. Yang and D. Klein. \u201cFUDGE: controlled text generation with future discriminators\u201d. In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021 (cit. on p. 10).   \n$[\\mathrm{Yua}+23]$ Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. \u201cRrhf: rank responses to align language models with human feedback without tears\u201d. In: arXiv preprint arXiv:2304.05302 (2023) (cit. on p. 10).   \n$[Z\\mathrm{ha}+23]$ Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu. \u201cSlic-hf: sequence likelihood calibration with human feedback\u201d. In: arXiv preprint arXiv:2305.10425 (2023) (cit. on p. 10).   \n[Zhe+23] R. Zheng, S. Dou, S. Gao, Y. Hua, W. Shen, B. Wang, Y. Liu, S. Jin, Q. Liu, Y. Zhou, et al. \u201cSecrets of rlhf in large language models part i: ppo\u201d. In: arXiv preprint arXiv:2307.04964 (2023) (cit. on p. 10).   \n[Zie+19] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. \u201cFine-tuning language models from human preferences\u201d. In: arXiv preprint arXiv:1909.08593 (2019) (cit. on p. 10). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Theoretical Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section contains all theoretical results of the paper. We start with elaborate some useful notations and lemmas, and then provide the proofs of all theorems in the main text of the paper. ", "page_idx": 13}, {"type": "text", "text": "For simplicity of proofs below, we first define a general reward-aligned policy: ", "page_idx": 13}, {"type": "text", "text": "Definition 4 (Reward aligned model $\\pi_{r}^{f}$ ). For any prompt $x$ , the reward aligned model $\\pi_{r}^{f}$ satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pi_{r}(y\\mid x)\\!=\\!\\frac{1}{Z_{r}}\\pi_{0}(y\\mid x)f(Q_{x}(r(x,y))),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $f\\in\\mathcal{F}=\\{f:\\mathbb{R}\\to\\mathbb{R}\\ |\\ f$ is increasing and $f\\geq0.$ and $Z_{r}$ is the normalizing constant. This general policy class includes both the optimal policy $\\pi_{r}^{\\mathrm{optimal}}$ and the best-of- $n$ policy. More specifically, the optimal policy $\\pi_{r}^{\\mathrm{optimal}}$ is with the choice of exponential functions and the the best-of- $n$ policy is with the choice of power functions. ", "page_idx": 13}, {"type": "text", "text": "Before proofs of the theorems, we first illustrate a useful lemma. ", "page_idx": 13}, {"type": "text", "text": "A.1 A Useful Lemma ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 5. For $\\pi_{r}$ with the definition $\\left(A.1\\right)$ , the following conclusions hold: ", "page_idx": 13}, {"type": "text", "text": "1. Context-conditional win rate is p\u03c0 \u03c0  x = $\\begin{array}{r}{p_{\\pi_{r}\\succ\\pi_{0}\\mid x}=\\frac{\\int_{0}^{1}u f(u)d u}{\\int_{0}^{1}f(u)d u}.}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "2. Context-conditional KL divergence is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{D}_{\\mathrm{KL}}(\\pi_{r}\\|\\pi_{0}\\mid x)=\\frac{\\int_{0}^{1}f(u)\\log(f(u))d u}{\\int_{0}^{1}f(u)d u}-\\log\\left(\\int_{0}^{1}f(u)d u\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Furthermore, both win rate and KL divergence are independent of distribution of $x$ ", "page_idx": 13}, {"type": "text", "text": "Proof. The context-conditional win rate is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad p_{\\pi_{r}\\sim\\pi_{0}\\mid x}=\\mathbb{P}_{Y\\sim\\pi_{r}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}(r(x,Y)\\ge r(x,Y_{0}))}\\\\ &{=\\displaystyle\\int\\pi_{r}(y\\mid x)\\pi_{0}(y_{0}\\mid x)\\mathbb{i}_{\\{r(x,y)\\ge r(x,y_{0})\\}}d y_{0}d y}\\\\ &{=\\displaystyle\\int\\pi_{r}(y\\mid x)Q_{x}(r(x,y))d y=\\displaystyle\\int\\frac{\\pi_{0}(y\\mid x)f(Q_{x}(r(x,y)))}{\\int\\pi_{0}(y\\mid x)f(Q_{x}(r(x,y)))d y}Q_{x}(r(x,y))d y}\\\\ &{=\\displaystyle\\frac{\\int\\pi_{0}(y\\mid x)f(Q_{x}(r(x,y)))Q_{x}(r(x,y))d y}{\\int\\pi_{0}(y\\mid x)f(Q_{x}(r(x,y)))d y}=\\frac{\\int_{0}^{1}u f(u)d u}{\\int_{0}^{1}f(u)d u},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last equation is because $Q_{x}(r(x,Y_{0}))\\sim U(0,1)$ when $Y_{0}\\sim\\pi_{0}(y\\mid x)$ . ", "page_idx": 13}, {"type": "text", "text": "The context-conditional KL divergence is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{D}_{\\mathrm{KL}}(\\pi_{r}\\|\\pi_{0}\\mid x)\\!=\\!\\displaystyle\\int\\pi_{r}(y\\mid x)\\mathrm{log}\\!\\left(\\frac{\\pi_{r}(y\\mid x)}{\\pi_{0}(y\\mid x)}\\right)\\!d y}\\\\ &{\\!=\\!\\displaystyle\\int\\frac{\\pi_{0}(y\\mid x)f(Q_{x}(r(x,y)))}{\\int\\pi_{0}(y\\mid x)f(Q_{x}(r(x,y)))d y}\\log\\left(\\frac{f(Q_{x}(r(x,y)))}{\\int\\pi_{0}(y\\mid x)f(Q_{x}(r(x,y)))d y}\\right)d y}\\\\ &{\\!=\\!\\displaystyle\\int_{0}^{1}\\frac{f(u)}{\\int_{0}^{1}f(u)d u}\\log\\left(\\frac{f(u)}{\\int_{0}^{1}f(u)d u}\\right)d u\\!=\\!\\frac{\\int_{0}^{1}f(u)\\log(f(u))d u}{\\int_{0}^{1}f(u)d u}\\!-\\!\\log\\left(\\int_{0}^{1}f(u)d u\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the third equation uses the fact that $Q_{x}(r(x,Y_{0}))\\sim U(0,1)$ when $Y_{0}\\sim\\pi_{0}(y|x)$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 1. Let $\\pi_{r,c}^{o p t i m a l}\\;b e$ the solution to (3.3). Then, for all $x$ , the density of the optimal policy is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi_{r,c}^{o p t i m a l}(y\\mid x)=\\pi_{0}(y\\mid x)\\exp\\left\\{c Q_{x}(r(x,y))\\right\\}/Z_{r}^{c},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $Z_{r}^{c}$ is the normalizing constant, and $c$ is a positive constant such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{(c-1)e^{c}+1}{e^{c}-1}}-\\log\\left({\\frac{e^{c}-1}{c}}\\right)=d.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, the context-conditional win rate and KL divergence of this optimal policy are ", "page_idx": 14}, {"type": "text", "text": "1. Context-conditional win rate: $\\begin{array}{r}{p_{\\pi_{r,c}^{o p t i m a l}\\succ\\pi_{0}\\mid\\boldsymbol{x}}=\\frac{(c-1)e^{c}+1}{c(e^{c}-1)}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Since for any prompt $x_{.}$ , both the context conditional win rate and $K L$ divergence are constants, the overall win rate p\u03c0ro,pctimal \u03c00 and $K L$ divergence $\\mathbb{D}_{\\mathrm{KL}}(\\pi_{r,c}^{o p t i m a l}\\|\\pi_{0})$ on any prompt set $D$ are also these values. [Proof]. ", "page_idx": 14}, {"type": "text", "text": "Proof. Since (3.3) is equivalent to (3.4) with some $\\beta>0$ . Now we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\mathbf{u}}{\\mathrm{argmax}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second to last equation is because the normalizer $Z$ is a constant: ", "page_idx": 14}, {"type": "equation", "text": "$$\nZ:=\\int\\pi_{0}(y\\mid x)\\exp\\left({\\frac{1}{\\beta}}Q_{x}(r(x,y))\\right)d y=\\int_{0}^{1}e^{{\\frac{u}{\\beta}}}d u=\\beta\\left(e^{1/\\beta}-1\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since the $\\mathrm{KL}$ divergence is minimized at 0 if and only if the two distributions are identical, the minimizer of (A.2) is the $\\pi^{*}$ satisfying that for any prompt $x\\in D$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi^{*}(y\\mid x)\\!=\\!\\frac{1}{Z}\\pi_{0}(y\\mid x)\\exp\\left(c Q_{x}(r(x,y))\\right)\\!,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $c=1/\\beta$ . ", "page_idx": 14}, {"type": "text", "text": "Then we confirm the closed forms of the context-conditional win rate and $\\mathrm{KL}$ divergence. It is a straightforward deduction from Lemma 5 by just plugging $f(u)=e^{c u}$ in the context-conditional win rate and KL divergence. ", "page_idx": 14}, {"type": "text", "text": "Furthermore, we require ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{D}_{\\mathrm{KL}}(\\pi^{*}\\|\\pi_{0})=d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d=\\mathbb{D}_{\\mathrm{KL}}(\\pi^{*}\\|\\pi_{0})}\\\\ &{\\quad=\\!\\int\\frac{1}{Z}\\pi_{0}(y\\mid x)\\exp\\big(c Q_{x}(r(x,y))\\big)\\log\\biggr(\\frac{\\exp\\big(c Q_{x}(r(x,y))\\big)}{Z}\\biggr)\\,d y}\\\\ &{\\quad=\\!\\frac{\\int_{0}^{1}c e^{c u}u d u}{Z}-\\log(Z)}\\\\ &{\\quad=\\!\\frac{\\big(c-1\\big)e^{c}+1}{e^{c}-1}\\!-\\!\\log\\frac{e^{c}-1}{c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This completes the proof. ", "page_idx": 15}, {"type": "text", "text": "A.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 2. The context-conditional win rate and $K L$ divergence of the best-of-n policy are: ", "page_idx": 15}, {"type": "text", "text": "1. Context-conditional win rate: p\u03c0(n) \u03c0  x =nn+1.   \n2. [JH22] Context-conditional KL divergence: $\\begin{array}{r}{\\mathbb{D}_{\\mathrm{KL}}\\left(\\pi_{r}^{(n)}\\|\\pi_{0}\\mid x\\right)=\\log(n)-\\frac{n-1}{n}.}\\end{array}$ .5 ", "page_idx": 15}, {"type": "text", "text": "Since both are constants, the overall win rate p\u03c0(rn) \u03c00 and $K l$ divergence $\\mathbb{D}_{\\mathrm{KL}}(\\pi_{r}^{(n)}\\|\\pi_{0})$ on any prompts set $D$ are the same values. [Proof]. ", "page_idx": 15}, {"type": "text", "text": "Proof. Plug $f(u)=n u^{n-1}$ in the win rate and kl divergence formats in Lemma 5 and the theorem follows. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 3. For any fixed $n,$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim D,y_{(n)}\\sim\\pi_{r}^{(n)},y_{(1)}\\sim\\pi_{r}^{(1)}}\\left[\\log\\frac{\\pi_{r}^{(n)}(y_{(n)}\\mid x)}{\\pi_{r}^{(n)}(y_{(1)}\\mid x)}-\\log\\frac{\\pi_{0}(y_{(n)}\\mid x)}{\\pi_{0}(y_{(1)}\\mid x)}\\right]=\\frac{1}{2\\beta_{n}^{*}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta_{n}^{*}=\\frac{1}{2(n-1)\\sum_{k=1}^{n-1}1/k}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "[Proof]. ", "page_idx": 15}, {"type": "text", "text": "Proof. Denote $U_{(n)}$ and $U_{(1)}$ the order statistics of the uniform distribution. That is, suppose $U_{1},\\cdots,U_{n}$ are independently and identically from $U(0,1)$ , and ", "page_idx": 15}, {"type": "equation", "text": "$$\nU_{(n)}=\\operatorname*{max}_{1\\leq i\\leq n}U_{i}{\\mathrm{~and~}}U_{(1)}=\\operatorname*{min}_{1\\leq i\\leq n}U_{i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The value of $\\beta$ is derived as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{\\beta^{-1}}{2}=\\mathbb{E}_{x\\sim D,y_{(\\alpha)}\\sim\\pi_{r}^{(n)},y_{(1)}\\sim\\pi_{r}^{(n)}}\\Big[h_{\\pi_{r}^{(n)}}(y_{(n)},y_{(1)},x)\\Big]}\\\\ &{=\\!\\mathbb{E}_{x\\sim D,y_{(n)}\\sim\\pi_{r}^{(n)},y_{(1)}\\sim\\pi_{r}^{(n)}}\\Big[\\log\\left(\\frac{\\pi_{r}^{(n)}(y_{(n)}\\mid x)}{\\pi_{0}(y_{(n)}\\mid x)}\\right)\\!-\\!\\log\\left(\\frac{\\pi_{r}^{(n)}(y_{(1)}\\mid x)}{\\pi_{0}(y_{(1)}\\mid x)}\\right)\\!\\Big]}\\\\ &{=\\!\\mathbb{E}_{x\\sim D,y_{(n)}\\sim\\pi_{r}^{(n)},y_{(1)}\\sim\\pi_{r}^{(n)}}\\Big[\\log\\Big(\\frac{n Q_{x}(r(x,y_{(n)}))^{n-1}}{n Q_{x}(r(x,y_{(1)}))^{n-1}}\\Big)\\Big]=(n-1)\\mathbb{E}_{x\\sim D}\\big[\\mathbb{E}\\big[\\log\\big(U_{(n)}\\big)\\!-\\!\\log\\big(U_{(1)}\\big)\\big]\\big]}\\\\ &{=\\!(n-1)\\cdot\\!\\int_{0}^{1}n\\log(u)u^{n-1}d u-(n-1)\\!\\cdot\\!\\int_{0}^{1}n\\log(u)(1\\!-\\!u)^{n-1}d u}\\\\ &{=\\!-\\frac{n-1}{n}\\!+(n-1)\\!\\sum_{k=1}^{n}\\!\\frac{1}{k}\\!=(n-1)\\!\\sum_{i=1}^{n-1}\\!\\frac{1}{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "5Beirami et al. $\\left[\\mathrm{Bei}{+}24\\right]$ discuss that since the distribution of the language model is discrete, $\\pi_{r}^{(n)}$ has a different form from that in Theorem 2, and the actual $\\mathrm{KL}$ divergence is smaller. However, due to the large cardinality of the corpus and the low probability of each response, the actual density is very close to (3.1) and the KL divergence is almost its upper bound $\\begin{array}{r}{\\log(n)-\\frac{n-\\mathbf{\\hat{l}}}{n}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta=\\frac{1}{2(n-1)\\sum_{i=1}^{n-1}1/k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Discrete versus Continuous Uniform Distribution ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Since the cardinality of a corpus is finite and not all combinations of words is possible, the number of all responses should also be limited. Suppose given a prompt $x$ , the set of all responses is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\mathscr{Y}}=\\{\\boldsymbol{y}_{i}\\}_{i=1}^{L},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and their corresponding probabilities and rewards are $p_{i}$ and $r_{i}=r(x,y_{i}),\\,i=1,\\ldots,L$ . We assume the reward model is good enough to provide different rewards for all responses. Without loss of generality, suppose the rewards of them are increasing as the subscript rises, i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\nr_{1}<r_{2}<\\cdots<r_{L}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For simplicity, we use $p_{1:i}$ to represent the sum $\\textstyle\\sum_{j=1}^{i}p_{j}$ throughout this section. Moreover, when $i=0$ , we define $p_{1:0}=0$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.1 $Q_{x}$ normalization in discrete case ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first revisit the distribution of $Q_{x}(r(x,Y_{0}))$ where $Y_{0}\\sim\\pi_{0}(y\\mid x)$ . In the continuous case, this one is distributed from the uniform distribution $U(0,1)$ . In the discrete case, since $\\pi_{0}(y\\mid x)$ is discrete, $Q_{x}(r(x,Y_{0}))^{*}s$ distribution becomes a discrete one with the following CDF: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{U}(u)=\\sum_{i=1}^{L}p_{i}\\mathbb{1}_{\\{u\\geq p_{1:i}\\}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\tilde{U}(u)$ is a staircase function with the $i$ -th segment from the left having a length of $p_{i}$ . For all $u=p_{1:i}$ with $i$ from 1 to $L$ , this new CDF still satisfies $\\tilde{U}(u)=u$ . Figure 4 shows two examples of CDF of $\\tilde{U}$ with small and large corpus. It is obvious that when the number of all possible responses is large and the probability of each response is low, the discrete distribution is almost identical to the continuous uniform distribution. ", "page_idx": 16}, {"type": "image", "img_path": "haSKMlrbX5/tmp/fd263d5c388ed4b7ae10d6c5f6cbe3f887b345cd4e369381769ea8dd0cc2fe49.jpg", "img_caption": ["Figure 4: The CDF of discrete transformation is almost the uniform distribution when the number of responses is large. Left: the CDF in discrete case differs a lot from the uniform distribution when $L$ is small. Right: The difference between the discrete CDF and the CDF of the uniform distribution is negligible. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Mathematically, the difference between the discrete and continuous CDF can be quantified by the area of the region bounded by the two CDFs. Specifically, the area is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Area}_{\\mathrm{diff}}:=\\frac{1}{2}\\sum_{i=1}^{L}p_{i}^{2}=\\int_{0}^{1}u d u-\\sum_{i=1}^{L}p_{i}\\cdot p_{1:(i-1)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sum_{i=1}^{L}p_{i}\\cdot p_{1:(i-1)}\\,\\mathbf{g}\\mathbf{c}}\\end{array}$ es to $\\int_{0}^{1}u d u\\;\\mathrm{as}\\;\\mathrm{max}_{i}\\,p_{i}\\rightarrow0$ , this area also converges to 0 as $\\operatorname*{max}_{i}p_{i}\\to0$ . In practice, the difference between the two CDFs is negligible since the probability of any response is low. ", "page_idx": 17}, {"type": "text", "text": "B.2 KL Divergence and Win Rate ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the discrete case, the PMF of the best-of-n policy has a different form from (3.1). Specifically, its PMF is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pi^{(n)}(y_{i}\\mid x)=\\left(p_{1:i}\\right)^{n}-\\left(p_{1:(i-1)}\\right)^{n},\\;i=1,...,L.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This is actually similar to its continuous density in the sense that ", "page_idx": 17}, {"type": "equation", "text": "$$\n(p_{1:i})^{n}-\\left(p_{1:(i-1)}\\right)^{n}=\\frac{(p_{1:i})^{n}-\\left(p_{1:(i-1)}\\right)^{n}}{p_{i}}p_{i}\\approx n p_{1:i}^{n-1}p_{i}=n\\tilde{U}(p_{1:i})^{n-1}\\pi_{0}(y_{i}\\mid x),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\tilde{U}$ is the CDF of the distribution of $Q_{x}(r(x,Y_{0}))$ with $Y_{0}\\sim\\pi_{0}(y\\mid x)$ . The approximation is due to the fact that $p_{i}$ is small. ", "page_idx": 17}, {"type": "text", "text": "To show the continuous assumption is reasonable for both best-of- $n$ and the optimal policy, we again consider the general policy $\\pi_{r}^{f}$ defined in Definition 4. ", "page_idx": 17}, {"type": "text", "text": "To align with the PMF of the best-of- $n$ policy, we adapt Definition 4 to the discrete case as follows: ", "page_idx": 17}, {"type": "text", "text": "Definition 6 (Reward aligned model $\\pi_{r}^{f}$ in discrete case). For any prompt $x$ , the reward aligned model $\\pi_{r}^{f}$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pi_{r,\\mathrm{discrete}}^{f}(y_{i}|x)\\!=\\!{\\frac{F(p_{1:i})-F(p_{1:(i-1)})}{\\sum_{i=1}^{L}F(p_{1:i})-F(p_{1:(i-1)})}}={\\frac{F(p_{1:i})-F(p_{1:(i-1)})}{F(1)-F(0)}},\\;i=1,...,L,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $f\\in\\mathcal{F}=\\{f:\\mathbb{R}\\to\\mathbb{R}\\ |\\ f$ is increasing and $f\\geq0\\}$ and $F(t)=\\int_{-\\infty}^{t}f(x)d x$ is the integral function of $f$ . ", "page_idx": 17}, {"type": "text", "text": "Since $f$ is increasing, $F$ exists and is also increasing. In particular, best-of- $n$ is $\\boldsymbol{F}(\\boldsymbol{u})=\\boldsymbol{u}^{n}$ and $f(u)\\,{\\stackrel{!}{=}}\\,n u^{n-1}$ , and the optimal policy is $F(u)=e^{c u}/c$ and $\\bar{\\boldsymbol{f}}(\\boldsymbol{u})=\\boldsymbol{e}^{c u}$ . ", "page_idx": 17}, {"type": "text", "text": "Subsequently, we investigate the KL divergence and win rate of this general framework Definition 6 and compare them with their corresponding continuous case. ", "page_idx": 17}, {"type": "text", "text": "First, we calculate the KL divergence. It can be shown that this KL divergence in discrete case can be upper bounded by that of continuous case: ", "page_idx": 17}, {"type": "text", "text": "Theorem 7. Suppose \u03c0rf,discrete based on the reference model $\\pi_{0}(y\\mid x)$ is defined in Definition 6, given a reward model $r_{.}$ , a non-decreasing function $f$ , and its integral function $F$ . Then, the KL divergence is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim D}\\left[\\sum_{i=1}^{L}\\frac{F(p_{1:i})-F(p_{1:(i-1)})}{F(1)-F(0)}\\log\\left(\\frac{F(p_{1:i})-F(p_{1:(i-1)})}{p_{i}\\left(F(1)-F(0)\\right)}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is smaller than $\\begin{array}{r}{\\frac{\\int_{0}^{1}f(u)\\log(f(u))}{F(1)-F(0)}-\\log{\\left(F(1)-F(0)\\right)}.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\int_{0}^{1}f(u)\\log(f(u))}{F(1)-F(u)}-\\log(F(1)-F(0))=\\int_{0}^{1}\\frac{f(u)}{F(1)-F(0)}\\log\\Big(\\frac{f(u)}{F(1)-F(0)}\\Big)d u}\\\\ &{=\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the inequality is due to the convexity of $x\\log(x)$ and Jensen\u2019s inequality. ", "page_idx": 18}, {"type": "text", "text": "Next, we calculate the win rate. It can be shown that the win rate in continuous case can be upper bounded and lower bounded by the win rate of discrete case with ties and the win rate of discrete case without ties, respectively: ", "page_idx": 18}, {"type": "text", "text": "Theorem 8. With the same setting as Theorem 7, the following holds: ", "page_idx": 18}, {"type": "text", "text": "1. The win rate considering ties is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{x\\sim D,Y\\sim\\pi_{r,d i s r e t}^{f}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}(r(x,Y)\\ge r(x,Y_{0}))=\\mathbb{E}_{x\\sim D}\\left[\\sum_{i=1}^{L}p_{1:i}\\frac{F(p_{1:i})-F(p_{1:(i-1)})}{F(1)-F(0)}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "2. The win rate without considering ties is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{x\\sim D,Y\\sim\\pi_{r,d s c r e t}^{f}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}(r(x,Y)>r(x,Y_{0}))=\\mathbb{E}_{x\\sim D}\\left[\\sum_{i=1}^{L}p_{1:(i-1)}\\frac{F(p_{1:i})-F(p_{1:(i-1)})}{F(1)-F(0)}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Besides, the following inequality holds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{\\Pi}_{Y\\sim\\pi_{r,d i s r e t}^{f}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}^{>}(r(x,Y)>r(x,Y_{0}))\\le\\frac{\\int_{0}^{1}u f(u)d u}{\\int_{0}^{1}f(u)d u}\\le\\mathbb{P}_{Y\\sim\\pi_{r,d i s r e t}^{f}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}(r(x,Y)\\ge\\delta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We first calculate the win rate: ", "page_idx": 18}, {"type": "text", "text": "1. The win rate considering ties is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{P}_{x\\sim D,Y\\sim\\pi_{r,\\mathrm{discret}}^{f}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}(r(x,Y)\\ge r(x,Y_{0}))=\\mathbb{E}_{x\\sim D}\\left[\\sum_{\\substack{r(x,y)\\ge r(x,y_{0})}}\\pi_{r,\\mathrm{discrete}}^{f}(y\\mid x)\\pi_{0}(y_{0}\\mid x)\\right]}\\\\ &{=\\!\\mathbb{E}_{x\\sim D}\\left[\\sum_{i=1}^{L}\\sum_{j=1}^{L}\\pi_{r,\\mathrm{discret}}^{f}(y_{j}\\mid x)\\pi_{0}(y_{i}\\mid x)\\mathbb{1}_{\\{r_{j}\\ge r_{i}\\}}\\right]=\\mathbb{E}_{x\\sim D}\\left[\\sum_{i=1}^{L}\\sum_{j\\ge i}^{F}\\frac{F(p_{1:j})-F(p_{1:j-1})}{F(1)-F(0)}p_{i}\\right]}\\\\ &{=\\!\\mathbb{E}_{x\\sim D}\\left[\\sum_{i=1}^{L}p_{1:i}\\frac{F(p_{1:i})-F(p_{1:(i-1)})}{F(1)-F(0)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{x\\sim D,Y\\sim\\pi_{r,\\mathrm{discret}}^{f}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}(r(x,Y)>r(x,Y_{0}))=\\mathbb{E}_{x\\sim D}\\left[\\displaystyle\\sum_{\\substack{r(x,y)>r(x,y_{0})}}\\pi_{r,\\mathrm{discrete}}^{f}(y\\mid x)\\pi_{0}(y_{0}\\mid x)\\right]}\\\\ &{=\\mathbb{E}_{x\\sim D}\\left[\\displaystyle\\sum_{i=1}^{L}\\sum_{j=1}^{L}\\pi_{r,\\mathrm{discret}}^{f}(y_{j}\\mid x)\\pi_{0}(y_{i}\\mid x)\\mathbb{1}_{\\{r_{j}>r_{i}\\}}\\right]=\\mathbb{E}_{x\\sim D}\\left[\\displaystyle\\sum_{i=1}^{L}\\sum_{j>i}\\frac{F(p_{1:j})-F(p_{1:j-1})}{F(1)-F(0)}p_{i}\\right]}\\\\ &{=\\mathbb{E}_{x\\sim D}\\left[\\displaystyle\\sum_{i=1}^{L}p_{1:(i-1)}\\frac{F(p_{1:i})-F(p_{1:(i-1)})}{F(1)-F(0)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we show the inequality: It holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{1:(i-1)}(F(p_{1:i})-F(p_{1:(i-1)}))\\leq\\int_{p_{1:(i-1)}}^{p_{1:i}}u f(u)d u\\leq p_{1:i}(F(p_{1:i})-F(p_{1:(i-1)})),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 19}, {"type": "text", "text": "According to the condition for the equality of the Jensen\u2019s inequality and (B.7), both KL divergence and win rate difference between the discrete and continuous case would diminish when $\\operatorname*{max}_{1\\le i\\le L}p_{i}\\to0$ (as $L\\to\\infty$ ). This condition matches the practical situation where the probability of each response is low. More precisely, both differences can be quantified by the difference between $U(0,1)$ and $\\tilde{U}$ defined in (B.1). Instead of considering the difference between continuous and discrete language model in a very high dimensional space, this difference only depends on two one-dimensional distributions $U(0,1)$ and $\\tilde{U}$ . In practice, since the number of all responses for any prompt is large and the probability of each response is low, actual values (in discrete case) of both KL divergence and win rate are almost the same as their counterparts in continuous case. ", "page_idx": 19}, {"type": "text", "text": "Theoretically, we prove the KL divergence and win rate difference can be quantified by the area difference between the CDF of $U(0,1)$ and the CDF of $\\tilde{U}$ defined in (B.1). ", "page_idx": 19}, {"type": "text", "text": "Theorem 9. With the same setting as Theorem 8, we have following conclusions: ", "page_idx": 19}, {"type": "text", "text": "1. Further suppose $f$ is differentiable and not equal to $o$ . Then, the KL difference between the discrete and continuous case is upper bounded by $\\begin{array}{r}{2\\frac{\\operatorname*{max}_{x\\in[0,1]}f^{\\prime}(x)}{F(1)-F(0)}\\cdot A r e a_{d i f f}.}\\end{array}$   \n2. The win rate difference between the discrete and continuous case is upper bounded by $\\begin{array}{r}{\\frac{2f(1)}{F(1)-F(0)}\\cdot A r e a_{d i f\\!\\!f},}\\end{array}$ , ", "page_idx": 19}, {"type": "text", "text": "where $A r e a_{d i f f}$ define in (B.2) is the area between the CDF of $U(0,1)$ and $\\tilde{U}$ defined in (B.1). ", "page_idx": 19}, {"type": "text", "text": "Proof. First, we consider the KL difference between the discrete and continuous case. For simplicity, denote $\\begin{array}{r}{g(u):=\\frac{f(u)}{F(1)-F(0)}}\\end{array}$ and $\\begin{array}{r}{G(u):=\\frac{F(u)}{F(1)-F(0)}}\\end{array}$ . Then the KL difference is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\int_{0}^{t}\\mathcal{E}(a)\\log(\\xi(u))d u-\\mathfrak{E}_{x-x}\\left[\\sum_{i=1}^{k}(G(p_{11})-G(p_{1:i-1}))\\log\\left(\\frac{G(p_{11})-G(p_{1:i-1})}{p_{1}}\\right)\\right]}\\\\ &{=\\mathbb{E}_{x-x}\\left[\\sum_{i=1}^{k}\\left(\\int_{p_{i1}-\\xi_{i}}^{p_{1}}\\mathcal{E}(a)\\log(g(u))d u\\right)-(G(p_{11:-1})-G(p_{1:i-1}))\\log\\left(\\frac{G(p_{11})-G(p_{1:i-1})}{p_{1}}\\right)\\right]}\\\\ &{=\\mathbb{E}_{x-x}\\left[\\sum_{i=1}^{k}(G(p_{11:i})-G(p_{1:i-1}))\\right]\\int_{p_{i1}-\\xi_{i}}^{p_{1}}G(p_{1:i-1})\\log\\left(\\frac{G(p_{12})}{p_{1}}\\right)\\log\\left(\\frac{G(p_{12})-G(p_{1:i-1})}{1/p_{1}}\\right)}\\\\ &{\\le\\mathbb{E}_{x-x}\\left[\\sum_{i=1}^{k}(G(p_{1:i})-G(p_{1:i-1}))\\right]\\log\\left(\\frac{G(p_{11})-G(p_{1:i-1})}{1/p_{1}}\\right)}\\\\ &{\\le\\mathbb{E}_{x-x}\\left[\\sum_{i=1}^{k}(G(p_{1:i})-G(p_{1:i-1}))\\right]\\frac{1}{k}\\left(G(p_{1:i})-G(p_{1:i-1})\\right)}\\\\ &{=\\mathbb{E}_{x-y}\\left[\\sum_{i=1}^{k}(G(p_{1:i})-G(p_{1:i-1}))\\frac{1}{k}\\left(\\xi(p_{1:i-1})-\\frac{G(p_{1:i-1})-G(p_{1:i-1})}{p_{1}}\\right)\\right]}\\\\ &{\\le\\mathbb{E}_{x-y}\\left[\\sum_{i=1}^{k}\\left(g(p_{1:i})-\\frac{G(p_{1:i-1})-G(p_{1:i-1})}{p_{1}}\\right)\\right]}\\\\ &{\\le\\mathbb{E}_{x-y}\\left[\\sum_{i=1}^{\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality uses the fact that $\\log(x)$ is increasing. The fourth equality applies the mean value theorem and \u03be is some value in [G(p1:i)\u2212pG(p1:(i\u22121)), . Since $g$ is not always 0, $\\xi>0$ . The second inequality is due to G(p1:i)\u2212pG(p1:(i\u22121)). The third inequality again uses the mean value theorem. The last inequality is just the fact that the global maximum is large than the subsets\u2019 maximums. ", "page_idx": 20}, {"type": "text", "text": "For win rate, due to (B.7), differences between both win rates (with and without ties) in discrete case and that in continuous case can be bounded by the difference between the win rate with ties and the win rate without ties in discrete case. The difference is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{x\\sim D,Y\\sim\\pi_{r,\\mathrm{diser}}^{f}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}(r(x,Y)\\ge r(x,Y_{0}))-P_{x\\sim D,Y\\sim\\pi_{r,\\mathrm{diser}}^{f}(y\\mid x),Y_{0}\\sim\\pi_{0}(y\\mid x)}(r(x,Y)>r(x,\\cdot))}\\\\ &{\\le\\mathbb{E}_{x\\sim D}\\left[\\displaystyle\\sum_{i=1}^{L}P_{i}\\frac{F(P_{1:i})-F(P_{1:i-1})}{F(1)-F(0)}\\right]=\\displaystyle\\frac{1}{F(1)-F(0)}\\mathbb{E}_{x\\sim D}\\left[\\displaystyle\\sum_{i=1}^{L}P_{i}^{2}\\frac{F(P_{1:i})-F(P_{1:i-1})}{P_{i}}\\right]}\\\\ &{\\overset{=}-\\frac{1}{F(1)-F(0)}\\mathbb{E}_{x\\sim D}\\left[\\displaystyle\\sum_{i=1}^{L}P_{i}^{2}f(q_{i})\\right]\\le\\displaystyle\\frac{f(1)}{F(1)-F(0)}\\mathbb{E}_{x\\sim D}\\left[\\displaystyle\\sum_{i=1}^{L}P_{i}^{2}\\right]=\\displaystyle\\frac{2f(1)}{F(1)-F(0)}\\cdot\\mathrm{Area}_{\\mathrm{diff}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the third equation is the mean value theorem and $q_{i}\\in(p_{1:(i-1)},p_{1:i})$ . The inequality is due to the fact that $f$ is non-decreasing and $\\forall q_{i}\\leq1$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Training details for baseline methods: ", "page_idx": 20}, {"type": "text", "text": "1. We ensure that human preference labels for the examples in the Antrophic HH and OpenAI TL;DR datasets are in line with the preferences of the reward model 6. Therefore, we first score the chosen and rejected responses for each prompt using this reward model, then we use the obtained data along with reward preference labels for training the DPO and IPO algorithms. Below we present our hyper-parameter selection for these methods. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Antrophic HH dataset \u2013 DPO: $\\beta=\\{0.00333,0.01,0.05,0.1,1,5\\}$ \u2013 IPO: \u03b2 = 0.00183654729,0.00550964188,0.0275482094,0.1401869 \u2022 TL;DR text summarization dataset \u2013 DP $\\beta=\\{0.01,0.05,0.1,1,5\\}$ \u2013 IPO: \u03b2 = 0.00183654729,0.00550964188,0.0275482094,0.137741047 ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "2. We use the following hyper-parameter $\\beta s$ for IPO BoN and DPO BoN: ", "page_idx": 21}, {"type": "text", "text": "Antrophic HH: \u2013 IPO BoN: \u03b2 = 0.00550964188,0.00918273647,0.0275482094,0.0826446282,0.137741047 \u2013 $\\mathsf{D P O~B o N}\\!:\\beta=\\{0.05,0.1,0.3,0.5,0.7,1,5\\}$   \nTL;DR: \u2013 IPO BoN: \u03b2 = 0.00550964188,0.0137741047,0.0275482094,0.0550964188,0.137741047 $-~\\mathrm{DPO~BoN};\\,\\beta=\\{0.05,0.1,0.5,1,5\\}$ ", "page_idx": 21}, {"type": "text", "text": "3. In addition to default $\\alpha=0.005$ and $\\beta_{8}^{*}=0.0275482094;$ , we also optimize the reference model with the combined loss of BoNBoN with other hyper-parameters: ", "page_idx": 21}, {"type": "text", "text": "\u2022 fixed $\\beta_{8}^{*}=0.0275482094;$ , different $\\alpha\\acute{\\mathbf{s}}$ : \u2013 Antrophic HH: $\\alpha=\\{0.05,0.2\\}$ $-\\ {\\mathrm{TL}};{\\mathrm{DR}};\\,\\alpha=\\left\\{0.05,0.02\\right\\}$   \n\u2022 fixed $\\alpha=0.005$ , and a smaller $\\beta=\\beta_{8}^{*}/5$ or a larger $\\beta=\\beta_{8}^{*}\\times5$ \u2013 $\\cdot\\ \\beta=\\{0.0275482094,0.00550964188,0.137741047\\}$ for both tasks ", "page_idx": 21}, {"type": "text", "text": "We train each of these models using RMSprop optimizer with a learning rate $5e-7\\mathrm{~}^{7}$ . We use the $20\\mathrm{k}$ checkpoint for each model to sample completions for the prompts in our testing set; we evaluate these samples against the SFT baseline using the reward model as judge. ", "page_idx": 21}, {"type": "text", "text": "D Additional results ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "haSKMlrbX5/tmp/1861f29d83679b7952da1a8d94acda0ddb45d4a177fcac686585482cfe39662c.jpg", "img_caption": ["Figure 5: BoNBoN is nearly tuning free. Plots show performance of BoNBoN-like methods varying the $\\beta$ parameter. We observe that the theoretically derived value of $\\beta$ gives consistently strong results; the other values have either substantially worse win-rate, or substantially higher change on average response length. "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "haSKMlrbX5/tmp/cf8f59a26c939d9a889ff0015872ec79a0b68a6ee39f8f5b83af5876abc5f6d8.jpg", "table_caption": [], "table_footnote": ["Table 2: More responses from different models. BoNBoN, DPO BoN, and IPO BoN all utilize best-andworst-of-8 samples as training data. "], "page_idx": 23}, {"type": "table", "img_path": "haSKMlrbX5/tmp/c4af702ee52a327c81deaa3a697d7379e53177cba8ecab7cb2064e32f23e098d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "haSKMlrbX5/tmp/982beab3e860e4cfe74f2d0cab2fe152834cef489c6268aefb933dab81cef253.jpg", "table_caption": ["Table 4: More responses from different models. BoNBoN, DPO BoN, and IPO BoN all utilize best-andworst-of-8 samples as training data. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "haSKMlrbX5/tmp/fef82fe9478eb9c87c681840b190fd3671c5ebeac6b93a038fd6d8795892691c.jpg", "table_caption": ["Table 5: More responses from different models. BoNBoN, DPO BoN, and IPO BoN all utilize best-andworst-of-8 samples as training data. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "haSKMlrbX5/tmp/cd11f4858797b70982c7b32f71929c11486d460c7b30de8aad81ab6bf2cdc2d7.jpg", "table_caption": [], "table_footnote": ["Table 6: More responses from different models. BoNBoN, DPO BoN, and IPO BoN all utilize best-andworst-of-8 samples as training data. "], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the main claims in paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work in discussion. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The paper provides the assumption in theorems and proof in the appendix. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper provides experimental details in the experiment section and appendix C. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All data and models used in the paper are provided with references. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The details are included in the experimental section and appendix C. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: No such experiments are needed in the paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The details are in the appendix C. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper discusses the societal impacts of the work. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. \u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. \u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not release any public data or models. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The creators of assets used in the paper are properly credited. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: New assets introduced in the paper are well documented and the documentation is provided. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: There is no human-related experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: There is no human-related experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]