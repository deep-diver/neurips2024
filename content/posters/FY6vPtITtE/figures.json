[{"figure_path": "FY6vPtITtE/figures/figures_6_1.jpg", "caption": "Figure 1: (a) Mean and standard deviation of the spectral norm of K(0) as a function of the number of neurons m for 10 independent experiments. Left: linear case. Right: nonlinear case. (b) Mean and standard deviation of \u0394K(t) := ||K(t) - K(0)|| over the network\u2019s width m, for 10 independent experiments. Left: linear case. Right: nonlinear case.", "description": "This figure shows the results of numerical experiments that validate the theoretical findings about the Neural Tangent Kernel (NTK) for Physics-Informed Neural Networks (PINNs). Part (a) compares the spectral norm of the NTK at initialization (K(0)) for linear and nonlinear partial differential equations (PDEs) as a function of the number of neurons (m). It demonstrates that, unlike the linear case, the NTK for nonlinear PDEs is not deterministic at initialization and its spectral norm shows high variability. Part (b) shows the evolution of the NTK during training (\u0394K(t) = ||K(t) - K(0)||) for both linear and nonlinear PDEs. The results confirm that the NTK for nonlinear PDEs is not constant during training, unlike the linear case.", "section": "5 Numerical Experiments"}, {"figure_path": "FY6vPtITtE/figures/figures_8_1.jpg", "caption": "Figure 3: (a) Poisson equation: median and standard deviation of the relative L2 loss for different optimizers over training iterations (repetitions over 10 independent runs). (b) Convection equation: median and standard deviation of the L2 loss after 1000 iterations achieved over 5 independent runs with and without CT for different values of the convection coefficient \u03b2 (left) and solution obtained with LM (and no other enhancement) after 5000 iterations with \u03b2 = 100 (right).", "description": "Figure 3 shows the results of applying different optimizers (Adam, LM, LBFGS) to solve the Poisson and Convection equations.  Part (a) compares the median and standard deviation of the relative L2 loss over training iterations for the Poisson equation. Part (b) presents a comparison for the Convection equation, showing the median and standard deviation of the L2 loss after 1000 iterations with and without curriculum training (CT), for various convection coefficients (\u03b2). A sample solution obtained using the Levenberg-Marquardt (LM) optimizer after 5000 iterations with \u03b2=100 is also displayed.", "section": "5 Numerical Experiments"}, {"figure_path": "FY6vPtITtE/figures/figures_8_2.jpg", "caption": "Figure 4: (a) Burgers' equation: mean and standard deviation of the relative L2 loss for various optimizers over wall time (repetitions over 10 independent runs). (b) Navier-Stokes equation: mean and standard deviation of the relative L2 loss over the PDE time \u03c4 for PINNs trained with Adam and LM (10 independent runs). Both optimization methods are enhanced with causality training.", "description": "This figure compares the performance of Adam and LM optimizers on Burgers' and Navier-Stokes equations.  Part (a) shows the relative L2 loss against wall time (computational time) for both optimizers on the Burgers' equation, highlighting LM's faster convergence.  Part (b) illustrates the relative L2 error over time (\u03c4) for both optimizers on the Navier-Stokes equation, showcasing the superior accuracy of LM, particularly in approximating velocity and pressure.  Both optimizers incorporated causality training in this experiment.", "section": "5 Numerical Experiments"}, {"figure_path": "FY6vPtITtE/figures/figures_22_1.jpg", "caption": "Figure 5: Mean and standard deviation of the relative L2 loss on the test set on the Wave equation for Adam, L-BFGS and LM optimizer over iterations (repetition over 10 independent runs).", "description": "This figure displays the performance of three different optimization algorithms (Adam, L-BFGS, and LM) on a wave equation.  The y-axis shows the relative L2 loss on a test set, and the x-axis represents the number of iterations during training.  The plot shows the mean relative L2 loss for each algorithm across 10 independent runs, along with error bars representing the standard deviation.  The results demonstrate the comparative performance of these optimizers in minimizing the loss function for the wave equation.", "section": "5 Numerical Experiments"}, {"figure_path": "FY6vPtITtE/figures/figures_23_1.jpg", "caption": "Figure 6: Experiments on the Wave equation. Left: Prediction of the parametrized solution of a PINN trained with Adam (Left) and LM (Center) alongside with the true solution (Right).", "description": "This figure shows the results of experiments on the Wave equation. Three contour plots are displayed, each representing the predicted solution of a Physics-Informed Neural Network (PINN) trained using different optimization methods. The left plot shows the result obtained using the Adam optimizer, while the center plot shows the result obtained using the Levenberg-Marquardt (LM) optimizer. The right plot displays the true solution. The plots are presented to visually compare the accuracy of PINNs trained with different optimizers in solving the Wave equation.", "section": "5 Numerical Experiments"}, {"figure_path": "FY6vPtITtE/figures/figures_23_2.jpg", "caption": "Figure 7: Experiments on the prediction of the solution of Poisson equation with LM and Adam (with loss balancing), both compared with the exact solution.", "description": "This figure compares the prediction of the solution of the Poisson equation using three different methods: Levenberg-Marquardt (LM), Adam with loss balancing (Adam+LB), and the exact solution.  It visually demonstrates the performance differences between LM and Adam+LB, showing that LM achieves greater accuracy in approximating the true solution.", "section": "5 Numerical Experiments"}, {"figure_path": "FY6vPtITtE/figures/figures_23_3.jpg", "caption": "Figure 8: Mean and standard deviation of the training loss over the iterations for Adam, LBFGS and LM on Navier-Stokes equation (for 10 independent runs).", "description": "This figure compares the training loss curves for three different optimization algorithms (Adam, LBFGS, and LM) when applied to the Navier-Stokes equation.  The plot shows that LM consistently achieves a lower training loss compared to the other two methods.  Error bars representing standard deviation across 10 independent runs are included for each method, demonstrating the variability in performance across different random initializations. The y-axis is logarithmic scale for training loss, showing the range of loss values across multiple runs.", "section": "5 Numerical Experiments"}]