[{"figure_path": "UixTytSVOl/tables/tables_3_1.jpg", "caption": "Table 1: MMDG analysis on EPIC-Kitchens and HAC with video and audio data. \u2018Base\u2019 denotes the naive multi-modal joint training without any domain generalization strategies. \u2018Uni-video\u2019 and \u2018Uni-audio\u2019 means training only with uni-modal data. \u2018Video\u2019, \u2018Audio\u2019 and \u2018Video-Audio\u2019 denote testing with uni-modal and multi-modal data. Results are averaged by using each domain as target.", "description": "This table presents the results of a multi-modal domain generalization (MMDG) analysis conducted on two benchmark datasets: EPIC-Kitchens and HAC.  The analysis compares different training methods on their ability to generalize to unseen target domains using video and audio modalities.  The methods include naive multi-modal training ('Base'),  uni-modal training with and without Sharpness-Aware Minimization (SAM), and the proposed Cross-Modal Representation Flattening (CMRF) method.  The table shows the performance (accuracy) for each method on uni-modal and multi-modal data, with results averaged across multiple target domains.", "section": "3.2 MMDG Analysis"}, {"figure_path": "UixTytSVOl/tables/tables_6_1.jpg", "caption": "Table 2: Multi-modal multi-source DG with different modalities on EPIC-Kitchens and HAC datasets. The best is in bold, and the second best is underlined.", "description": "This table presents the results of a multi-modal multi-source domain generalization experiment conducted on two datasets: EPIC-Kitchens and HAC.  The experiment compares the performance of several methods, including the proposed Cross-Modal Representation Flattening (CMRF) method, across various combinations of modalities (video, audio, flow) and source domains. The best performing method for each combination is highlighted in bold, while the second-best is underlined.  The table provides a quantitative comparison of the effectiveness of different methods in handling multi-modal domain generalization.", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_6_2.jpg", "caption": "Table 2: Multi-modal multi-source DG with different modalities on EPIC-Kitchens and HAC datasets. The best is in bold, and the second best is underlined.", "description": "This table presents the results of a multi-modal multi-source domain generalization experiment conducted on two benchmark datasets, EPIC-Kitchens and HAC.  Multiple methods (Base, SAM, SAGM, SWAD, EoA, RNA-Net, SimMMDG, and the proposed CMRF) are compared using various combinations of modalities (Video, Audio, Flow).  The table shows the average accuracy across multiple source domains, highlighting the superior performance of the proposed CMRF method.", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_7_1.jpg", "caption": "Table 4: The average results of uni-modal performance comparison under multi-modal multi-source DG on EPIC-Kitchens with different modality combinations.", "description": "This table presents a comparison of uni-modal performance results under multi-modal, multi-source domain generalization settings on the EPIC-Kitchens dataset.  Different modality combinations (video, audio, flow, and their combinations) are evaluated using various methods (Base, SAM, EoA, SimMMDG, and the proposed CMRF). Uni-modal performance is assessed for each modality (video, audio, flow) individually to understand the impact of multi-modal training on each modality's generalization capacity.  The results illustrate how the proposed CMRF improves uni-modal performance in comparison to other baseline methods. ", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_7_2.jpg", "caption": "Table 2: Multi-modal multi-source DG with different modalities on EPIC-Kitchens and HAC datasets. The best is in bold, and the second best is underlined.", "description": "This table presents the results of a multi-modal multi-source domain generalization experiment conducted on two benchmark datasets: EPIC-Kitchens and HAC.  The experiment evaluated the performance of various methods in different modality combinations (video, audio, flow) across multiple source and target domains. The table shows the average accuracy achieved by different methods, including baselines and the proposed method (CMRF),  with the best performance highlighted in bold and the second best underlined. This allows for a comparison of different approaches in handling multi-modal data and their generalization capabilities across different domains.", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_8_1.jpg", "caption": "Table 7: The average results compared with methods designed for modality competition on HAC with video and audio data under multi-source DG.", "description": "This table compares the performance of the proposed CMRF method against several baselines designed to address modality competition in multi-modal domain generalization.  The comparison is conducted using the HAC dataset with video and audio modalities in a multi-source domain generalization setting.  The results show the validation and test accuracy for each method.  The CMRF method outperforms the baselines on the test accuracy, demonstrating its effectiveness in mitigating modality competition and improving generalization.", "section": "4.3 Ablation Studies"}, {"figure_path": "UixTytSVOl/tables/tables_13_1.jpg", "caption": "Table 9: Uni-modal performance under multi-modal multi-source DG on EPIC-Kitchens dataset with video and audio data.", "description": "This table presents the uni-modal performance results for video and audio modalities under multi-modal, multi-source domain generalization settings on the EPIC-Kitchens dataset.  It compares the average accuracy across different source domains (D1, D2, D3) for each modality under different methods: Uni-modal (training only on that modality), Base (naive multi-modal training), SAM, EoA, SimMMDG, and the proposed CMRF method. This allows for assessing the impact of multi-modal training and the effectiveness of the CMRF approach on individual modality generalization.", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_15_1.jpg", "caption": "Table 9: Uni-modal performance under multi-modal multi-source DG on EPIC-Kitchens dataset with video and audio data.", "description": "This table presents the uni-modal performance results of different methods on the EPIC-Kitchens dataset, focusing on video and audio modalities. It shows the average performance across various domain combinations (D2, D3 \u2192 D1, D1, D3 \u2192 D2, D1, D2 \u2192 D3) for each method, including the baseline (Base), SAM, EoA, SimMMDG, and the proposed CMRF method.  The results highlight the improvement in uni-modal performance achieved by the proposed CMRF, showcasing its efficacy in handling the complexities of multi-modal multi-source domain generalization.", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_15_2.jpg", "caption": "Table 3: Multi-modal single-source DG with video, flow and audio three modalities on EPIC-Kitchens and HAC datasets.", "description": "This table presents the results of a multi-modal single-source domain generalization experiment.  The models were trained on a single source domain and tested on multiple target domains using three modalities (video, flow, and audio). The table compares the performance of the proposed CMRF method against several baseline methods across different source and target domain combinations for both the EPIC-Kitchens and HAC datasets.  The \"Avg\" column represents the average performance across all target domains.", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_15_3.jpg", "caption": "Table 3: Multi-modal single-source DG with video, flow and audio three modalities on EPIC-Kitchens and HAC datasets.", "description": "This table presents the results of a multi-modal single-source domain generalization experiment.  The models were trained on a single source domain and tested on multiple target domains. Three modalities (video, flow, and audio) were used. The table compares the performance of the proposed CMRF method against several baseline methods across various source and target domain combinations on two benchmark datasets (EPIC-Kitchens and HAC).  The results are presented as Top-1 accuracy.", "section": "4.1 Experimental Setting"}, {"figure_path": "UixTytSVOl/tables/tables_15_4.jpg", "caption": "Table 12: Uni-modal performance under multi-modal multi-source DG on HAC dataset with video and audio data.", "description": "This table presents the results of a multi-modal, multi-source domain generalization experiment on the Human-Animal-Cartoon (HAC) dataset using video and audio modalities.  It shows the performance of various methods (Base, SAM, and the proposed CMRF) when testing each modality (video and audio) individually, on each of three target domains (A, C, and H).  The results highlight the improvement achieved by the proposed CMRF method compared to baseline and other uni-modal methods.", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_16_1.jpg", "caption": "Table 12: Uni-modal performance under multi-modal multi-source DG on HAC dataset with video and audio data.", "description": "This table presents the uni-modal performance results on the HAC dataset when using a multi-modal multi-source domain generalization (MMDG) approach.  It shows the average accuracy for each modality (video and audio) across various source domain combinations.  The goal is to evaluate how well each modality generalizes when trained with multiple modalities.", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_16_2.jpg", "caption": "Table 12: Uni-modal performance under multi-modal multi-source DG on HAC dataset with video and audio data.", "description": "This table presents the results of uni-modal performance evaluation under multi-modal multi-source domain generalization (MMDG) settings on the Human-Animal-Cartoon (HAC) dataset using video and audio modalities.  It shows the average accuracy for each modality (video and audio) across different source domain combinations, comparing the performance of the proposed CMRF method to baseline methods such as Base, SAM, and SimMMDG.  Each entry represents the average accuracy across various test domains.", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_16_3.jpg", "caption": "Table 9: Uni-modal performance under multi-modal multi-source DG on EPIC-Kitchens dataset with video and audio data.", "description": "This table presents the uni-modal performance results obtained from a multi-modal multi-source domain generalization experiment on the EPIC-Kitchens dataset.  Specifically, it shows the performance of each modality (video and audio) when trained and tested under different domain combinations, indicating how well each modality generalizes when trained within a multi-modal context. The results are separated into in-domain (validation) and out-of-domain (test) performances. This allows for a detailed comparison of the individual generalization capabilities of video and audio modalities within a multi-modal model.", "section": "4.2 Main Results"}, {"figure_path": "UixTytSVOl/tables/tables_16_4.jpg", "caption": "Table 12: Uni-modal performance under multi-modal multi-source DG on HAC dataset with video and audio data.", "description": "This table presents the uni-modal performance results on the HAC dataset for video and audio modalities under multi-modal multi-source domain generalization settings. The results are broken down by source domains (A, C \u2192 H, H, C \u2192 A, H, A \u2192 C), showing the performance of each modality (video, audio) when trained in a multi-modal setting.", "section": "4.2 Main Results"}]