{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of LLMs, introducing the concept of few-shot learning and demonstrating the capabilities of large language models."}, {"fullname_first_author": "Rishi Bommasani", "paper_title": "On the opportunities and risks of foundation models", "publication_date": "2021-08-07", "reason": "This paper provides a comprehensive overview of the opportunities and risks associated with foundation models, including LLMs, which is essential context for this research."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduces the Llama model, a key LLM used in the experiments of InfLLM, highlighting its importance as a baseline model."}, {"fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "publication_date": "2023-10-06", "reason": "This paper details the Mistral 7B model, another key LLM used in this research for comparisons and as a basis for InfLLM."}, {"fullname_first_author": "Chi Han", "paper_title": "LM-Infinite: Simple on-the-fly length generalization for large language models", "publication_date": "2023-08-16", "reason": "This paper introduces LM-Infinite, a related method that addresses long-sequence processing without retraining, providing a direct comparison and baseline for InfLLM."}]}