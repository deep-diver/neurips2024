[{"figure_path": "HpN4xeDJQF/tables/tables_8_1.jpg", "caption": "Table 1: Collaboration performance of CBPR with different numbers of meta-tasks and agents employing various skill levels. We reported the mean reward over 10 episodes and the values in bracket represent the standard deviation. Here, we additionally defined four meta-tasks (i.e. place onion & deliver soup, place tomato & deliver soup, pickup tomato & place mix and pickup ingredient & place mix), which are not included in Table 4.", "description": "This table presents the results of an ablation study on the number of predefined meta-tasks used in the CBPR framework. It shows the mean reward achieved over 10 episodes by agents with high, medium, and low skill levels when using different numbers of meta-tasks, including a set with and without the \"others\" category which represents tasks not explicitly modeled.", "section": "4.4 Ablation Study"}, {"figure_path": "HpN4xeDJQF/tables/tables_15_1.jpg", "caption": "Table 2: PPO hyperparameters for MTP, BCP, FCP and SP agents. Lambda is used in generalized advantage estimation (GAE) to calculate advantage function. Reward shaping parameters in Table 3 gradually anneals to zero over Reward shaping horizons.", "description": "This table lists the hyperparameters used for the Proximal Policy Optimization (PPO) algorithm in the paper's experiments.  It includes learning rate, entropy coefficient, epsilon, gamma, lambda, batch size, clipping, hidden dimension of the actor and critic neural networks, and the reward shaping horizon.  The parameters were consistent across the different agents (MTP, BCP, FCP, and SP) for fair comparison.", "section": "C Implementation details"}, {"figure_path": "HpN4xeDJQF/tables/tables_15_2.jpg", "caption": "Table 3: Reward shaping parameters for PPO.", "description": "This table presents the reward shaping parameters used in the Proximal Policy Optimization (PPO) algorithm.  It shows that placing ingredients in the pot, picking up a dish, and picking up soup are all rewarded.  Picking up soup receives the highest reward, reflecting the importance of successfully completing orders.", "section": "C Implementation details"}, {"figure_path": "HpN4xeDJQF/tables/tables_16_1.jpg", "caption": "Table 4: Predefined rule-based meta-tasks.", "description": "This table shows the predefined rule-based meta-tasks used in the Overcooked game for four different layouts: Cramped Room, Coordination Ring, Asymmetric Advantage, and Soup Coordination. Each layout has four meta-tasks defined: placing an onion in a pot, delivering soup, placing an onion and delivering soup, and others (for tasks not covered by the first three meta-tasks).  The table serves as a basis for training the meta-task playing (MTP) agents in the Collaborative Bayesian Policy Reuse (CBPR) framework.", "section": "C.1 Collaborative Bayesian Policy Reuse (CBPR)"}]