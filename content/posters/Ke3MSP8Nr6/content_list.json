[{"type": "text", "text": "Information-theoretic Limits of Online Classification with Noisy Labels ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Changlong Wu Ananth Grama Wojciech Szpankowski CSoI, Purdue University wuchangl@hawaii.edu, {ayg,szpan}@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study online classification with general hypothesis classes where the true labels are determined by some function within the class, but are corrupted by unknown stochastic noise, and the features are generated adversarially. Predictions are made using observed noisy labels and noiseless features, while the performance is measured via minimax risk when comparing against true labels. The noisy mechanism is modeled via a general noisy kernel that specifies, for any individual data point, a set of distributions from which the actual noisy label distribution is chosen. We show that minimax risk is tightly characterized (up to a logarithmic factor of the hypothesis class size) by the Hellinger gap of the noisy label distributions induced by the kernel, independent of other properties such as the means and variances of the noise. Our main technique is based on a novel reduction to an online comparison scheme of two-hypotheses, along with a new conditional version of Le Cam-Birg\u00e9 testing suitable for online settings. Our work provides the first comprehensive characterization for noisy online classification with guarantees that apply to the ground truth while addressing general noisy observations. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning from noisy data is a fundamental problem in many machine learning applications. Noise can originate from various sources, including low-precision measurements of physical quantities, communication errors, or noise intentionally injected by methods such as differential privacy. In such cases, one typically learns by training on noisy (or observed) data while aiming to build a model that performs well on the true (or latent) data. This paper focuses on online learning [20] from noisy labels, where one receives noiseless, adversarially generated features and corresponding noisy labels sequentially, and predicts the true labels as the data arrive. ", "page_idx": 0}, {"type": "text", "text": "Online learning has been primarily studied in the agnostic setting [1, 19, 7], where one receives the labels in their plain (noise-free) form and the prediction risk is evaluated on the observed labels. It is typically assumed that both the features and observed labels are generated adversarially, and prediction quality is measured via the notion of regret, which compares the actual cumulative risk incurred by the predictor with the minimal cumulative risk incurred by the best expert in a hypothesis class. While this approach is mathematically appealing, it does not adequately characterize online learning scenarios when our goal is to achieve good performance with respect to grand truth data that may be different from the observed (noisy) ones. ", "page_idx": 0}, {"type": "text", "text": "This paper considers an online learning scenario that differs from classical agnostic online learning in two aspects: (i) we assume that the noisy labels are derived from a (semi-) stochastic mechanism rather than from pure adversarial selections; (ii) our prediction risk is evaluated on the true labels, not noisy observations. To better motivate the study of such a scenario, we consider the following example first introduced by Ben-David et al. [1]: ", "page_idx": 0}, {"type": "text", "text": "Example 1. Let $\\mathcal{H}\\subset\\{0,1\\}^{\\mathcal{X}}$ be a finite hypothesis class. Consider the following online learning game between Nature/Adversary and Learner that is played over a time horizon $T$ . At the start, Nature fixes a ground truth classifier $h\\in\\mathcal H$ . At each time step $t\\leq T$ , Nature adversarially selects feature $\\mathbf{x}_{t}\\in\\mathcal{X}$ and reveals it to the learner. The learner makes a prediction $\\hat{y}_{t}$ based on prior features $\\mathbf{x}^{t}=\\{\\mathbf{x}_{1},\\cdots,\\mathbf{x}_{t}\\}$ and noisy labels $\\tilde{y}^{t-1}=\\{\\tilde{y}_{1},\\cdot\\cdot\\cdot,\\tilde{y}_{t-1}\\}$ . Nature then selects a (unknown) noise parameter $\\eta_{t}\\in[0,\\eta]$ for some given $\\eta$ (known to learner), and generates 1: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\tilde{y}_{t}=\\mathsf{B e r n o u l l i}(\\eta_{t})\\oplus y_{t},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\oplus$ denotes binary addition and $y_{t}=h(\\mathbf{x}_{t})$ is the true label. It was shown in $l I$ , Thm 15] that there exists a predictor $\\hat{y}^{T}$ such that: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h\\in\\mathcal{H},\\mathbf{x}^{T}\\in\\mathcal{X}^{T}}\\mathbb{E}\\left[\\sum_{t=1}^{T}1\\{\\hat{y}_{t}\\neq h(\\mathbf{x}_{t})\\}\\right]\\leq\\frac{\\log|\\mathcal{H}|}{1-2\\sqrt{\\eta(1-\\eta)}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Note that the risk in (1) is significant, as the error introduced by noise to the true labels increases linearly as $\\eta T$ , yet the risk remains independent of the time horizon $T$ . This mirrors the fast rates known in the PAC learning literature when benign noise is present. Despite its foundational nature, the understanding of this phenomenon beyond simple Massart\u2019s noise has been largely unexplored. ", "page_idx": 1}, {"type": "text", "text": "This paper introduces a novel online learning framework for modeling general noisy mechanisms. In particular, it encompasses (1) as a very specific instance and provides a clear and comprehensive characterization of the underlying paradigm. Formally, let $\\boldsymbol{\\wp}$ be the set of true (latent) labels and $\\tilde{\\mathcal{D}}$ be the set of noisy (observed) labels, which we assume are finite and of size $N,M$ , respectively. Let $\\mathcal{X}$ be the feature space. We model the noisy mechanism by a noisy kernel: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{K}}:\\boldsymbol{\\mathcal{X}}\\times\\boldsymbol{\\mathcal{Y}}\\rightarrow2^{\\mathcal{D}(\\tilde{\\boldsymbol{\\mathcal{Y}}})},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{D}(\\tilde{\\mathcal{D}})$ is the set of all distributions over $\\tilde{\\mathcal{D}}$ . That is, the kernel $\\kappa$ maps each pair $(\\mathbf{x},y)$ to a subset $\\mathcal{Q}_{y}^{\\mathbf{x}}:=\\mathcal{K}(\\mathbf{x},y)\\subset\\mathcal{D}(\\tilde{\\mathcal{V}})$ of distributions over $\\tilde{\\mathcal{D}}$ . Observe that the noisy kernel provides a compact way of modeling noisy label distributions without explicitly referring to the noise. This is more convenient for our discussion, as ultimately the statistical information is solely determined by the noisy label distributions. ", "page_idx": 1}, {"type": "text", "text": "For any given $\\mathcal{H}\\subset\\mathcal{V}^{\\mathcal{X}}$ and kernel $\\kappa$ , we consider the following robust (noisy) online classification   \nscenario: Nature first selects $h\\in\\mathcal H$ ; at each time step $t$ , Nature chooses (adversarially) $\\mathbf{x}_{t}\\in\\mathcal{X}$ and   \ntlahbe ellesa reveals it to the learner; the learner then makes a prediction $\\tilde{y}^{t-1}$ ; Laent vaenrds yb te htehne  ssetrlaetcetsg iae sd iosft rtihbeu ltieoarn $\\tilde{p}_{t}\\in\\mathcal{Q}_{h(\\mathbf{x}_{t})}^{\\mathbf{x}_{t}}$ $\\hat{y}_{t}$ , based on the features r, es/aadmvpelressa $\\tilde{y}_{t}\\sim\\tilde{p}_{t}$ e catnidv erley.v eTalhse $\\mathbf{x}^{t}$ and noisy $\\tilde{y}_{t}$ o taol $\\Phi$ $\\Psi$   \nof the learner is to minimize the following expected minimax risk: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\tilde{r}_{T}(\\mathcal{H},K)=\\operatorname*{inf}_{\\Phi}\\operatorname*{sup}_{\\Psi}\\mathbb{E}\\left[\\sum_{t=1}^{T}1\\{h(\\mathbf{x}_{t})\\neq\\hat{y}_{t}\\}\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\hat{y}_{t}=\\Phi(\\mathbf{x}^{t},\\tilde{y}^{t-1})$ . Note that the adversarial selection of distribution $\\tilde{p}_{t}$ from the kernel set $\\mathcal{Q}_{h(\\mathbf{x}_{t})}^{\\mathbf{x}_{t}}$ provides more flexibility for modeling scenarios when the noisy label distribution changes even with the same true label, such as the Massart\u2019s noise in Example 1. We refer to Section 2 for a more complete specification of our setting. ", "page_idx": 1}, {"type": "text", "text": "1.1 Main Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our main contributions in this paper establish the fundamental limits of minimax risk in (3) by providing nearly matching lower and upper bounds across a wide range of hypothesis classes $\\mathcal{H}$ and noisy kernels $\\kappa$ . Observe that, to allow for non-trivial prediction rules, the induced noisy label distributions must be statistically distinguishable for distinct true labels. To formalize this intuition, we define, for any noisy kernel $\\kappa$ and feature $\\textbf{x}\\in\\:\\mathcal{X}$ , the Hellinger gap as $\\gamma_{\\mathbf{H}}(\\mathbf{x})\\;=\\;$ $\\operatorname*{inf}_{y\\neq y^{\\prime}\\in\\mathcal{Y}}\\operatorname*{inf}_{p\\in\\mathcal{Q}_{y}^{\\times},q\\in\\mathcal{Q}_{y^{\\prime}}^{\\times}}\\{H^{2}(p,q)\\}$ , where $\\begin{array}{r}{H^{2}(p,q)=\\sum_{m=1}^{M}(\\sqrt{p[m]}-\\sqrt{q[m]})^{2}}\\end{array}$ is the squared Hellinger distance. That is, $\\gamma_{\\mathbf{H}}(\\mathbf{x})$ measures the minimal squared Hellinger distance of the induced noisy label distributions over all distinct true labels. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1. Let $\\mathcal{H}\\subset\\mathcal{V}^{\\mathcal{X}}$ be any finite class, and $\\kappa$ be any noisy kernel such that $\\operatorname*{inf}_{\\mathbf{x}\\in\\mathcal{X}}\\gamma_{\\mathbf{H}}(\\mathbf{x})\\geq\\gamma_{\\mathbf{H}}$ for some $\\gamma_{\\mathbf{H}}>0$ , and $\\mathcal{Q}_{y}^{\\mathbf{x}}\\subset\\mathcal{D}(\\tilde{\\mathcal{y}})$ is closed and convex for all $\\mathbf{x},y.$ . Then: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tilde{r}_{T}(\\mathcal{H},\\mathcal{K})\\leq O\\left(\\frac{\\log^{2}|\\mathcal{H}|}{\\gamma_{\\mathrm{\\scriptscriptstyleH}}}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Moreover, for any $K\\in\\mathbb N$ and any kernel $\\kappa$ with at least $\\log K$ features $\\mathbf{x}\\in\\mathcal{X}$ for which $\\gamma_{\\mathrm{{H}}}(\\mathbf{x})\\leq\\gamma_{\\mathrm{{H}}}$ , there exists a class H of size K that satisfies: r\u02dcT (H, K) \u2265\u2126 log\u03b3 |HH| . ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 shows that the Hellinger gap is the right characterization for the minimax risk upto at most a logarithmic factor. Moreover, the risk bound depends solely on the gap parameter $\\gamma_{\\mathbf{H}}$ and $\\log\\left|\\mathcal{H}\\right|$ , independent of time horizon $T$ , the size of $\\boldsymbol{\\wp}$ and $\\tilde{y}$ , and the properties of noise such as means and variances. For the bounded Bernoulli noise in Example 1, the set $\\mathcal{Q}_{y}^{\\mathbf{x}}$ corresponds to Bernoulli distributions with parameters in $[0,\\eta]$ if $y=0$ and in $[1-\\eta,1]$ if $y=1$ , leading to the Hellinger gap $\\gamma_{\\mathrm{{H}}}=1-2\\sqrt{\\eta(1-\\eta)}$ . This matches the dependency on $\\eta$ in Example $1\\,^{2}$ . However, our result holds for any noisy kernel. For instance, if we shift $\\mathcal{Q}_{\\mathrm{0}}^{\\mathbf{x}}$ to Bernoulli distribution with parameter 0 and $\\mathcal{Q}_{1}^{\\mathbf{x}}$ with parameters in $[1-2\\eta,1]$ , then $\\gamma_{\\mathrm{{H}}}=1-{\\sqrt{2\\eta}}=\\Theta(1-2\\eta)$ . This is tighter than the dependency on $\\eta$ in Example 1 (for $\\eta\\rightarrow\\textstyle{\\frac{1}{2}},$ ), since $1-2\\sqrt{\\eta(1-\\eta)}=\\Theta((1-2\\eta)^{2})$ . ", "page_idx": 2}, {"type": "text", "text": "Our main proof technique for establishing Theorem 1 is based on a novel (black box) reduction to an online comparison scheme of two-hypotheses in $\\mathcal{H}$ , as demonstrated in Theorem 3. This allows us to reduce the noisy online classification problem to a hypothesis testing problem, which effectively decouples the adversarial property of the features from the stochastic property of the noisy labels. However, due to the adversarial selection of the noisy label distributions, the classical hypothesis testing techniques does not apply. To resolve this issue, we establish in Theorem 4, a generalization of the Le Cam-Birg\u00e9 Test with varying conditional marginals for handling pairwise testing via the Hellinger gap, which is a result of independent interest. ", "page_idx": 2}, {"type": "text", "text": "Tight dependency on $\\log\\left|\\mathcal{H}\\right|$ . Although the lower and upper bounds in Theorem 1 differ by a $\\log\\left|\\mathcal{H}\\right|$ factor, this is compensated by the fact that we are dealing with the most general classes and kernels. This can be tightened for various special cases. Indeed, for a class $\\mathcal{H}$ with binary true labels and arbitrary noisy labels, we demonstrate in Theorem 5 that the minimax risk is upper bounded by $\\frac{16\\log\\left|\\mathcal{H}\\right|}{\\gamma_{\\mathbf{L}}}$ , where $\\gamma_{\\mathbf{L}}$ is the $L^{2}$ -gap that substitutes the Hellinger distance with $L^{2}$ -distance in Theorem 1. This is proved via a novel reduction to online conditional distribution estimation under $L^{2}$ -distance. Moreover, we demonstrate in Appendix G (Theorem 6) that the (optimal) $O\\big(\\frac{\\log{|\\mathcal{H}|}}{\\gamma_{\\mathbf{H}}}\\big)$ upper bound holds if $|\\mathcal{Q}_{y}^{\\bf x}|=1$ for all $\\mathbf{x},y$ , i.e., the noisy label distribution is determined by data. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Online learning with noisy data was discussed in [6], which specifically considers generalized linear functions with zero-mean and bounded variance noises. Our work differs in that we focus on classification instead of regression. Moreover, our noisy model does not require that the noise be zero-mean. To the best of our knowledge, [1] is the only work that has specifically considered the classification task, but this was limited to bounded Bernoulli noise. From a technical standpoint, analogous ideas of pairwise comparison have been considered in differential privacy literature, such as in [11], but only in batch settings. The reduction to online conditional probability estimation was also explored in [10] within the context of online decision making. However, a distinguishing feature of our work is that our conditional probability estimation problem is necessarily misspecified, as our noisy label distributions are selected adversarially and are unknown a priori to the learner. Our problem setup is further related to differentially private conditional distribution learning, as in [26], and robust hypothesis testing, discussed in [17, Chapter 16]. Online conditional probability estimation has been widely studied, see [18, 3, 2, 4, 25, 24]. Conditional density estimation in the batch setting has also been extensively studied, see [12] for KL-divergence with misspecification and [9] for ${\\bar{L}}^{2}$ loss. Learning from noisy labels in the batch case was discussed in [16] (see also the references therein) by leveraging suitably defined proxy losses. There has been a long line of research on online prediction with adversarial observable labels in the agnostic formulation, see [5, 1, 19, 7]. ", "page_idx": 2}, {"type": "text", "text": "2 Notation and Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{X}$ be a set of features (or instances), $\\boldsymbol{\\wp}$ be a set of labels, and $\\tilde{\\mathcal{D}}$ be a set of noisy observations. We assume throughout the paper that $|\\mathcal{V}|=N$ and $|\\tilde{\\mathcal{D}}|=M$ for some integers $N,M\\ge2$ . We denote $\\mathcal{D}(\\tilde{\\mathcal{D}})$ as the set of all probability distributions over $\\tilde{\\mathcal{D}}$ . ", "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{H}\\subset\\mathcal{V}^{\\mathcal{X}}$ be a hypotheses class and $\\kappa$ be a noisy kernel in (2). We consider the following robust online classification scenario: (1) Nature first selects some $h\\in\\mathcal H$ ; (2) At time $t$ , Nature adversarially selects $\\mathbf{x}_{t}\\in\\mathcal{X}$ ; (3) Learner predicts $\\hat{y}_{t}\\in\\mathcal{Y}$ , based on (noisy) history observed thus far (i.e., $\\mathbf{x}^{t},\\tilde{y}^{t-1})$ ; (4) An adversary then selects $\\tilde{p}_{t}\\in\\mathcal{Q}_{h(\\mathbf{x}_{t})}^{\\mathbf{x}_{t}}$ , and generates a noisy sample $\\tilde{y}_{t}\\sim\\tilde{p}_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "The goal of the learner is to minimize the cumulative error: $\\textstyle\\sum_{t=1}^{T}1\\{h(\\mathbf{x}_{t})\\neq\\hat{y}_{t}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Note that the cumulative error is a random variable that depends on all the randomness associated with the game. To remove the dependency on such randomness and to assess the fundamental limits of the prediction quality, we consider the following two measures ", "page_idx": 3}, {"type": "text", "text": "Definition 1. Let $\\mathcal{H}\\subset\\mathcal{V}^{\\mathcal{X}}$ be a set of hypotheses and $\\boldsymbol{\\kappa}:\\boldsymbol{\\mathcal{X}}\\times\\mathcal{Y}\\rightarrow2^{\\mathcal{D}(\\tilde{\\mathcal{Y}})}$ be a noisy kernel. We denote by $\\Phi$ the (possibly randomized) strategies of the learner. The expected minimax risk is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{r}_{T}(\\mathcal{H},K)=\\operatorname*{inf}_{\\Phi}\\operatorname*{sup}_{h\\in\\mathcal{H},{\\mathbf x}^{T}\\in\\mathcal{X}^{T}}\\mathbb{Q}_{K}^{T}\\mathbb{E}_{\\hat{y}^{T}}\\left[\\sum_{t=1}^{T}1\\{h({\\mathbf x}_{t})\\neq\\hat{y}_{t}\\}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{Q}_{K}^{T}\\equiv\\operatorname*{sup}_{\\tilde{p}_{1}\\in\\mathcal{Q}_{h(\\mathbf{x}_{1})}^{\\mathbf{x}_{1}}}\\mathbb{E}_{\\tilde{y}_{1}\\sim\\tilde{p}_{1}}\\cdot\\cdot\\cdot\\operatorname*{sup}_{\\tilde{p}_{T}\\in\\mathcal{Q}_{h(\\mathbf{x}_{T})}^{\\mathbf{x}_{T}}}\\mathbb{E}_{\\tilde{y}_{T}\\sim\\tilde{p}_{T}},\\,a n d\\,\\hat{y}_{t}\\sim\\Phi(\\mathbf{x}^{t},\\tilde{y}^{t-1}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By skolemization [19], the operator $\\mathbb{Q}_{K}^{T}$ is equivalent to $\\mathrm{sup}_{\\tilde{p}}\\,\\mathbb{E}_{\\tilde{y}^{T}\\sim\\tilde{p}}$ , where $\\tilde{p}$ runs over all (joint) distributions over Y\u02dcT such that \u2200t \u2208[T], \u2200y\u02dct\u22121 \u2208Y\u02dct\u22121 the conditional marginal p\u02dcy\u02dct|y\u02dct\u22121 \u2208Qhxt(xt). ", "page_idx": 3}, {"type": "text", "text": "Definition 2. Let $\\mathcal{H},\\,\\kappa_{:}$ , and $\\Phi$ be as in Definition 1. For any $\\delta>0$ , the high probability minimax risk at confidence $\\delta$ is the minimum quantity $B^{\\delta}({\\mathcal{H}},{\\mathcal{K}})\\geq{\\dot{0}}$ such that there exists a predictor $\\Phi$ satisfying: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h\\in\\mathcal{H},\\mathbf{x}^{T}\\in\\mathcal{X}^{T},\\tilde{p}}\\operatorname*{Pr}_{\\tilde{y}^{T}\\sim\\tilde{p},\\tilde{y}^{T}}\\left[\\sum_{t=1}^{T}1\\{h(\\mathbf{x}_{t})\\neq\\hat{y}_{t}\\}\\geq B^{\\delta}(\\mathcal{H},K)\\right]\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{p}$ is selected as in the discussion above and $\\hat{y}_{t}\\sim\\Phi(\\mathbf{x}^{t},\\tilde{y}^{t-1})$ . ", "page_idx": 3}, {"type": "text", "text": "Note that the kernel map $\\kappa$ is generally known to the learner when constructing the predictor $\\Phi$ . However, the induced kernel sets Qhxt(xt) are not, since they depend on the unknown ground truth classifier $h$ and adversarially generated features $\\mathbf{x}^{T}$ . In certain cases, such as Theorem 3 and Example 4, the kernel map $\\kappa$ is also not required to be known. ", "page_idx": 3}, {"type": "text", "text": "For any $\\mathbf{x}\\in\\mathcal{X}$ and $y\\in\\mathcal{V}$ , we denote by $\\mathcal{Q}_{y}^{\\mathbf{x}}$ the set induced by a kernel. We can assume, w.l.o.g., that the $\\mathcal{Q}_{y}^{\\mathbf{x}}\\mathbf{s}$ are convex and closed sets, since the adversary can select an arbitrary distribution from $\\mathcal{Q}_{y}^{\\mathbf{x}}\\mathbf{s}$ at each time step, including randomized strategies that effectively sample from a mixture (i.e., convex combination) of distributions in $\\mathcal{Q}_{y}^{\\mathbf{x}}\\mathbf{s}$ . ", "page_idx": 3}, {"type": "text", "text": "One must introduce some constraints on the kernel $\\kappa$ in order to obtain meaningful results. To do so, we introduce the following well-separation condition: ", "page_idx": 3}, {"type": "text", "text": "Definition 3. Let $L:\\mathcal{D}(\\tilde{\\mathcal{D}})^{2}\\to\\mathbb{R}^{\\geq0}$ be any divergence, we say a kernel $\\kappa$ is well-separated w.r.t. $L$ at scale $\\gamma>0$ , $i f\\forall\\mathbf{x}\\in\\mathcal{X}$ , $\\forall y,y^{\\prime}\\in\\mathcal{Y}$ with $y\\ne y^{\\prime}$ we have $L(\\mathcal{Q}_{y}^{\\bf x},\\mathcal{Q}_{y^{\\prime}}^{\\bf x})\\stackrel{\\mathrm{def}}{=}\\operatorname*{inf}_{p\\in\\mathcal{Q}_{y}^{\\bf x},q\\in\\mathcal{Q}_{y^{\\prime}}^{\\bf x}}L(p,q)\\geq\\gamma$ . ", "page_idx": 3}, {"type": "text", "text": "Example 2. For any $y\\in\\mathcal{V}$ , we specify $a$ canonical distribution $p_{y}\\in\\mathcal{D}(\\tilde{\\mathcal{D}})$ . A natural noisy kernel would be to define $\\mathcal{Q}_{y}^{\\mathbf{x}}=\\{p\\in\\mathcal{D}(\\tilde{\\mathcal{V}}):\\mathsf{T V}(p,p_{y})\\leq\\epsilon\\}$ , where TV denotes total variation. In this case, the kernel is well-separated with the gap $\\gamma$ under total variation $i f\\mathrm{min}_{y\\neq y^{\\prime}\\in\\mathcal{Y}}\\,\\mathsf{T V}(p_{y},p_{y^{\\prime}})\\geq$ $\\gamma+2\\epsilon$ . In particular, this subsumes Example 1 if, for $y\\in\\{0,1\\}$ , we define $p_{y}$ as the distribution that assigns probability $^{\\,I}$ to $y_{\\mathrm{:}}$ , and take $\\epsilon=\\eta,$ , where the TV-gap equals $\\gamma=1-2\\eta$ . ", "page_idx": 3}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin by stating our main result of this paper. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let $\\mathcal{H}\\subset\\mathcal{V}^{\\mathcal{X}}$ be a finite class of size $K$ , and $\\kappa$ be a kernel that is well-separated at scale $\\gamma_{\\mathbf{H}}$ w.r.t. squared Hellinger divergence (Definition 3). Then, the high probability minimax risk (Definition 2) with confidence $\\delta>0$ is upper bounded by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nB^{\\delta}(\\mathcal{H},K)\\leq\\frac{8\\log(4K/\\delta)\\log K}{\\gamma_{\\mathrm{H}}}+\\log(2/\\delta).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, for any kernel $\\kappa$ such that there exist at least $L$ distinct features $\\mathbf{x}\\,\\in\\,\\chi\\,^{4}$ for which $\\begin{array}{r}{\\operatorname*{inf}_{y\\neq y^{\\prime}\\in\\mathcal{Y}}H^{2}(\\mathcal{Q}_{y}^{\\bf x},\\mathcal{Q}_{y^{\\prime}}^{\\bf x})\\le\\gamma_{\\mathrm{H}},}\\end{array}$ , one can find a class $\\mathcal{H}$ of size $K$ such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{r}_{T}(\\mathcal{H},K)\\geq\\Omega\\left(\\frac{\\operatorname*{min}\\{L,\\log K\\}}{\\gamma_{\\mathrm{{H}}}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Observe that, the upper bound holds with high probability and the risk is independent of the time horizon (i.e., the so-called fast rates known in the PAC-learning literature). Moreover, the bound is independent of the size of $\\boldsymbol{\\wp}$ and $\\tilde{\\mathcal{D}}$ . A simple integration argument yields the expected risk upper bound $\\begin{array}{r}{\\tilde{r}(\\mathcal{H},\\mathcal{K})\\leq O\\left(\\frac{\\log^{2}K}{\\gamma_{\\mathrm{H}}}\\right)}\\end{array}$ , which matches the lower bound upto only a $\\log K$ factor. This demonstrates that, the Hellinger gap of the induced noisy label distributions is the right characterization for the minimax risk. Moreover, the Hellinger distance can be transformed from other $f$ -divergences (such total variation) without depending on the size of $\\tilde{\\mathcal{D}}$ [17, Chapter 7.6]. ", "page_idx": 4}, {"type": "text", "text": "Example 3. Let $\\kappa$ be the kernel in Example 2. Let $\\begin{array}{r}{\\lambda=\\operatorname*{min}_{y\\neq y^{\\prime}\\in\\mathcal{Y}}\\mathsf{T V}(p_{y},p_{y^{\\prime}})}\\end{array}$ . Hence, the kernel is well-separated with TV-gap $\\lambda-2\\epsilon,$ . Since $H^{2}(p,q)\\geq{\\mathsf{T V}}(p,q)^{2}\\;_{l}$ [17, Eq. $7.22J,$ the Hellinger gap is lower bounded by $(\\lambda-2\\epsilon)^{2}$ . Invoking Theorem 2, we have for any hypothesis class $\\mathcal{H}$ , the following risk upper bound holds: $\\begin{array}{r}{B^{\\delta}(\\mathcal{H},\\mathcal{K})\\leq O\\left(\\frac{\\log|\\mathcal{H}|\\log(|\\mathcal{H}|/\\delta)}{(\\lambda-2\\epsilon)^{2}}\\right)}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "The rest of this section is devoted to establishing Theorem 2. Our main proof technique is based on a novel reduction to pairwise testing of two hypotheses as developed in Section 3.1, along with explicit testing rules in Section 3.2 based on a novel conditional version of Le Cam-Birg\u00e9 testing. ", "page_idx": 4}, {"type": "text", "text": "3.1 Reduction to Pairwise Comparison: a Generic Approach ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first introduce the following key technical concept. Recall that our robust online classification problem is completely determined by the tuple $(\\mathcal{H},\\mathcal{K})$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 4. A problem $(\\mathcal{H},\\mathcal{K})$ is said to be pairwise testable with confidence $\\delta>0$ and error bound $C(\\delta)\\ge0$ if, for any pair $h_{i},h_{j}\\in\\mathcal{H}$ , the sub-problem $(\\{h_{i},h_{j}\\},K)$ admits a predictor (i.e., pairwise tester) $\\Phi_{i,j}$ that achieves cumulative $r i s k\\le C(\\delta)\\;w.p.\\ge1-\\delta$ (see Definition 2). ", "page_idx": 4}, {"type": "text", "text": "Clearly, any prediction rule for $(\\mathcal{H},\\mathcal{K})$ serves as a pairwise testing rule for all the sub-problems $(\\{h_{i},\\bar{h_{j}}\\},\\dot{\\kappa})$ with $h_{i},h_{j}\\in\\mathcal{H}$ . Perhaps surprisingly, we will show in this section that any pairwise testing rules for the sub-problems can also be converted into a prediction rule for $(\\mathcal{H},\\mathcal{K})$ , incurring only an additional logarithmic factor on the risk bounds. ", "page_idx": 4}, {"type": "text", "text": "To this end, suppose that the tuple $(\\mathcal{H},\\mathcal{K})$ is pairwise testable and the class $\\mathcal{H}=\\{h_{1},\\cdot\\cdot\\cdot,h_{K}\\}$ is finite with size $K$ . Let $\\Phi_{i,j}$ be the testing rule (will be constructed in Section 3.2) for $h_{i},h_{j}$ with error bound $C(\\delta)$ and confidence $\\delta>0$ . Let $\\mathbf{x}^{T},\\boldsymbol{\\tilde{y}}^{T}$ be any realization of problem $(\\mathcal{H},\\mathcal{K})$ . We define, for any $h_{i}\\in\\mathcal{H}$ and $t\\in[T]$ , a surrogate loss vector: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall j\\in[K],\\;\\mathbf{v}_{t}^{i}[j]=1\\{\\Phi_{i,j}(\\mathbf{x}^{t},\\tilde{y}^{t-1})\\neq h_{i}(\\mathbf{x}_{t})\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "That is, the loss $\\mathbf{v}_{t}^{i}[j]=1$ if and only if the test $\\Phi_{i,j}(\\mathbf{x}^{t},\\tilde{y}^{t-1})$ differs from $h_{i}(\\mathbf{x}_{t})$ . Given access to testers $\\Phi_{i,j}\\mathbf{s}$ , our prediction rule for $(\\mathcal{H},\\mathcal{K})$ is then presented in Algorithm 1. ", "page_idx": 4}, {"type": "image", "img_path": "Ke3MSP8Nr6/tmp/437ec6fd35c23ca03f121654fcab0f204a5f1913bd3688c279dffea24742e925.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Theorem 3. Let $\\mathcal{H}\\subset\\mathcal{V}^{\\mathcal{X}}$ be any hypothesis class of size $K$ and $\\kappa$ be any noisy kernel. If $(\\mathcal{H},\\mathcal{K})$ is pairwise testable with error bound $C(\\delta)$ as in Definition 4, then for any $\\delta>0$ , the predictor in Algorithm $^{\\,l}$ with $C=C(\\delta/(2K))$ achieves the high probability minimax risk (Definition 2): ", "page_idx": 5}, {"type": "equation", "text": "$$\nB^{\\delta}(\\mathcal{H},\\mathcal{K})\\leq2(1+2C(\\delta/(2K))\\log K)+\\log(2/\\delta).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Sketch of Proof. At a high level, our goal is to identify the ground truth classifier $h_{k^{*}}$ using the testing results of $\\Phi_{i,j}\\,{\\bf s}$ . Note that pairwise testability implies, w.p. $\\geq1-\\delta$ , the errors made by tester $\\Phi_{k,k^{*}}$ on $h_{k^{*}}$ is upper bounded by $C(\\delta/2K)$ for all $k\\in[K]$ simultaneously. However, for any other pair $i,j\\neq k^{*}$ , the tester $\\Phi_{i,j}$ does not provide any guarantees, since the samples used to test $h_{i},h_{j}$ originate from $h_{k^{*}}$ and is not realizable for $\\Phi_{i,j}$ . The key technical challenge is to extract the testing results for $\\Phi_{k,k^{*}}$ from the other irrelevant tests (i.e., $\\Phi_{i,j}$ with $k^{*}\\notin\\{i,j\\})$ , even when the $k^{*}$ is unknown. This is resolved by our definition of $l_{t}^{i}$ in Algorithm 1, which computes for each $i$ the maximum testing loss over all of its competitors. This ensures that, for the ground truth $k^{*}$ , the loss $l_{t}^{k^{*}}\\leq C(\\delta/2K)$ . While for any other $i\\neq k^{*}$ , we have $\\begin{array}{r}{l_{t}^{i}\\geq\\sum_{r=1}^{t}\\mathbf{v}_{r}^{i}[k^{*}]\\geq\\sum_{r=1}^{t}1\\{h_{i}(\\mathbf{x}_{r})\\neq}\\end{array}$ $h_{k^{*}}(\\mathbf{x}_{r})\\}-C(\\delta/2K)$ . Therefore, any hypothesis $h_{i}$ for which $l_{t}^{i}>C(\\delta/2K)$ cannot be the ground truth. Algorithm 1 then maintains an index set $S^{t}$ that eliminates all $h_{i}$ for which $l_{t}^{i}>C(\\bar{\\delta}/2K)$ , and makes prediction $\\hat{y}_{t}=h_{\\hat{k}_{t}}(\\mathbf x_{t})$ with $\\hat{k}_{t}$ sampling uniformly from $S^{t}$ . ", "page_idx": 5}, {"type": "text", "text": "To derive the risk bound, we use a potential-based analysis that relates the size of $S^{t}\\mathbf{s}$ with the prediction error $1\\{h_{k^{*}}(\\mathbf{x}_{t})\\neq\\hat{y}_{t}\\}$ . The intuition behind the analysis is that if $\\mathbb{E}[1\\{h_{k^{*}}(\\mathbf{x}_{t})\\neq\\hat{y}_{t}\\}]$ is large, then there will be many elements $i\\in S^{t}$ for which $h_{i}(\\mathbf{x}_{t})\\neq h_{k^{*}}(\\mathbf{x}_{t})$ , and thus the loss $l_{t}^{i}$ will (potentially) increase. Since Algorithm 1 constructs $S^{t+1}$ by eliminating all $i\\in S^{t}$ for which $l_{t}^{i}>\\dot{C}(\\delta/2K)$ , one can therefore bound the prediction error by the change in the size of $S^{t}\\mathbf{s}$ . The key technical challenge here is to control the hypotheses that differ from $k^{*}$ but for which the tester $\\Phi_{k,k^{*}}$ errs, which is resolved by carefully defining a potential function. The claimed upper bound then follows by a similar argument as [14, Thm 2]. See Appendix B for complete proof. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Note that, the reduction of Theorem 3 is general and does not rely on specific properties of the kernel $\\kappa$ (such as the well-separation condition). It provides a black box reduction that converts any pairwise testing rule for two-hypotheses to a general online classification rule that introduces only a logarithmic factor on the risk bounds. This effectively decouples the adversarial property of features from the stochastic property of the noisy labels. ", "page_idx": 5}, {"type": "text", "text": "To understand how Theorem 3 operates, we consider the following example: ", "page_idx": 5}, {"type": "text", "text": "Example 4. Let $\\mathcal{H}\\subset\\{0,1\\}^{\\mathcal{X}}$ , and $\\kappa$ be the bounded Bernoulli noise kernel with parameter \u03b7 in Example 1. For any $h_{i},h_{j}\\in\\mathcal{H}$ , we construct the following testing rule. We may assume, w.l.o.g., that $h_{i}(\\mathbf{x})\\neq h_{j}(\\mathbf{x})$ for all $\\mathbf{x}\\in\\mathcal{X}$ , since any $\\mathbf{x}$ for which $h_{i}(\\mathbf{x})=h_{j}(\\mathbf{x})$ do not affect our testing. Moreover, by relabeling, we can assume that $h_{i}(\\mathbf{x})=0$ and $h_{j}(\\mathbf{x})=1$ for all $\\mathbf{x}\\in\\mathcal{X}$ . At time step $t_{\\perp}$ after observing the noisy labels $\\tilde{y}^{t-1}$ , we compute $\\begin{array}{r}{\\hat{\\mu}_{t}=\\frac{1}{t-1}\\sum_{r=1}^{t-1}\\tilde{y}_{r}.}\\end{array}$ If $\\begin{array}{r}{\\hat{\\mu}_{t}\\ge\\frac{1}{2}}\\end{array}$ , the tester predicts $\\hat{y}_{t}=1$ ; else, it predicts $\\hat{y}_{t}=0$ . By Azuma\u2019s inequality, the probability of making an error at step t is upper bounded by $e^{-(1-2\\eta)^{2}(t-1)/2}$ . Thus, for any $n\\leq T$ , the probability of making any errors after step n is upper bounded by $\\begin{array}{r}{\\sum_{t=n}^{\\infty}e^{-(1-2\\eta)^{2}(t-1)/2}\\,\\le\\,\\frac{e^{-(1-2\\eta)^{2}n/2}}{(1-2\\eta)^{2}}}\\end{array}$ . Taking $\\begin{array}{r}{n\\,=\\,\\frac{2\\log(1/\\delta(1-2\\eta)^{2})}{(1-2\\eta)^{2}}}\\end{array}$ one can upper bound the probability by $\\delta$ . Therefore, the tuple $(\\mathcal{H},\\mathcal{K})$ is pairwise testable with 2 log((11/\u2212\u03b42(1\u03b7)\u221222\u03b7)2). Invoking Theorem 3, we have: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nB^{\\delta}(\\mathcal{H},\\mathcal{K})\\leq O\\left(\\frac{\\log|\\mathcal{H}|\\log(|\\mathcal{H}|/\\delta(1-2\\eta)^{2})}{(1-2\\eta)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that the risk bound in (9) recovers the risk in Example 1 up to a logarithmic factor, though it employs a completely different approach (cf. [1]). Moreover, Example 4 provides the key advantage that the risk holds with high probability and at a fast rate, which is known to be non-trivial for cumulative errors (see, e.g., [22, 21]). To our knowledge, it remains unclear whether the approach proposed in [1] admits a high probability guarantee. ", "page_idx": 6}, {"type": "text", "text": "3.2 Proof of Theorem 2: the conditional Le Cam-Birg\u00e9 Testing ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As demonstrated in Section 3.1, the risk of noisy online classification can be reduced to the pairwise testing of two hypotheses. However, we still need to construct the explicit pairwise testing rules. This section is devoted to providing a generic testing rule for general kernels. ", "page_idx": 6}, {"type": "text", "text": "Let $h_{0}$ and $h_{1}$ be any two hypotheses. We may assume, w.l.o.g., that $h_{0}(\\mathbf{x})\\neq h_{1}(\\mathbf{x})$ for all $\\mathbf{x}\\in\\mathcal{X}$ , since the features for which $h_{0}$ and $h_{1}$ agree do not affect the testing. We now provide a more compact characterization of the kernel without explicitly referring to true labels. Let $\\mathbf{x}^{T}$ be any realization of features. For any $i\\in\\{0,1\\}$ , $t\\in[T]$ , and kernel $\\kappa$ , we write $\\mathcal{Q}_{i}^{\\mathbf{x}_{t}}:=\\mathcal{K}(\\mathbf{x}_{t},h_{i}(\\mathbf{x}_{t}))$ . ", "page_idx": 6}, {"type": "text", "text": "We define $\\mathcal{Q}_{0}^{J}$ and $\\mathcal{Q}_{1}^{J}$ as the sets of all (joint) distributions over $\\tilde{\\mathcal{Y}}^{J}$ induced by the kernel upto time step $J$ for $h_{0},h_{1}$ , respectively. Equivalently, for $i\\in\\{0,1\\}$ , we have $p\\in\\mathcal{Q}_{i}^{\\tilde{J}}$ if and only if for all $t\\in[J]$ and $\\tilde{y}^{t-1}\\in\\tilde{\\mathcal{V}}^{t-1}$ , the conditional marginal $p_{\\tilde{y}_{t}|\\tilde{y}^{t-1}}\\in\\mathcal{Q}_{i}^{\\mathbf{x}_{t}}$ . ", "page_idx": 6}, {"type": "text", "text": "The pairwise testing of $h_{0},h_{1}$ at time step $J+1$ is then equivalent to the (robust) hypothesis testing w.r.t. sets $\\mathcal{Q}_{0}^{J}$ and $\\bar{\\mathcal{Q}}_{1}^{J}$ . This is typically resolved using Le Cam-Birg\u00e9 testing [17, Chapter 32.2] if the distributions are of product form. However, this does not hold for our purpose, since the distributions in $\\mathcal{Q}_{i}^{J}$ can have highly correlated marginals. Our main result for addressing this issue is a conditional version of Le Cam-Birg\u00e9 testing, as stated in Theorem 4 below. To the best of our knowledge, this conditional version is novel. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4 (conditional Le Cam-Birg\u00e9 Testing). Let $\\mathcal{Q}_{0}^{J}$ and $\\mathcal{Q}_{1}^{J}$ be the classes induced by a kernel upto time $J$ as defined above. For any $t\\leq J$ , we denote $\\gamma_{t}=H^{2}(Q_{0}^{\\mathbf{x}_{t}},Q_{1}^{\\mathbf{x}_{t}})$ and assume that $\\mathcal{Q}_{i}^{\\mathbf{x}_{t}}$ is convex for all $i\\in\\{0,1\\}$ . Then, there exists a testing rule $\\psi:\\tilde{\\mathcal{V}}^{J}\\rightarrow\\{0,1\\}$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{p\\in Q_{0}^{J},q\\in Q_{1}^{J}}\\Big\\{\\operatorname*{Pr}_{\\tilde{y}^{J}\\sim p}[\\psi(\\tilde{y}^{J})\\neq0]+\\operatorname*{Pr}_{\\tilde{y}^{J}\\sim q}[\\psi(\\tilde{y}^{J})\\neq1]\\Big\\}\\le2\\prod_{t=1}^{J}(1-\\gamma_{t}/2)\\le2e^{-\\frac{1}{2}\\sum_{t=1}^{J}\\gamma_{t}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Sketch of Proof. The proof requires a suitable application of the minimax theorem by expressing the testing error as a linear function and arguing that the $\\mathcal{Q}_{i}^{J}\\mathrm{s}$ are convex. The error bound is then controlled by a careful application of the chain-rule of R\u00e9nyi divergence. See Appendix $\\mathbf{C}$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Theorem 4 immediately implies the following cumulative risk bound: ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. Let $h_{0},h_{1}$ be any hypotheses, $\\mathbf{x}^{T}$ be any realization of features and $\\mathcal{Q}_{i}^{T},\\;\\mathcal{Q}_{i}^{\\mathbf{x}_{t}}$ be defined as above with $\\gamma_{t}=H^{2}(\\mathcal{Q}_{0}^{\\mathbf{x}_{t}},\\mathcal{Q}_{1}^{\\mathbf{x}_{t}})$ . Then, there exists a tester $\\hat{y}^{T}$ such that for all $\\delta>0$ , $i\\in\\{0,1\\}$ and $\\tilde{p}\\in\\mathcal{Q}_{i}^{T}$ , $w.p.\\geq1-\\delta$ over $\\tilde{y}^{T}\\sim\\tilde{p},$ , we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}1\\{h_{i}(\\mathbf x_{t})\\neq\\hat{y}_{t}\\}\\leq\\arg\\operatorname*{min}_{n}\\left\\{n\\in\\mathbb N:\\sum_{t=1}^{n}\\gamma_{t}\\geq2\\log(2/\\delta)\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof. Let $n^{*}$ be the minimal number satisfying the RHS. If $t\\leq n^{*}$ (this can be checked at each time step $t$ using only $\\mathbf{x}^{t}$ and $\\kappa$ ), we predict arbitrarily. If $t\\geq n^{*}+1$ , we use the tester $\\psi$ in Theorem 4 with $J=n^{*}$ to produce an index $\\hat{i}\\in\\{0,1\\}$ and make the prediction $h_{\\hat{i}}(\\mathbf x_{t})$ for all following time steps. That is, we only use the tester at step ${n^{*}}+1$ and reuse the same testing result for all following time steps. By Theorem 4, the probability of making errors after step ${n^{*}}+1$ is upper bounded by $\\delta$ . Therefore, the cumulative risk is upper bounded by $n^{*}$ with probability $\\geq1-\\delta$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Proof of Theorem 2. Let $h_{0},h_{1}\\in\\mathcal{H}$ be any two-hypotheses. For any time step $t$ such that $h_{0}(\\mathbf{x}_{t})\\neq$ $h_{1}(\\mathbf{x}_{t})$ , we have, by the well-separation condition, that the gap $\\gamma_{t}\\geq\\gamma_{\\mathbf{H}}$ in Proposition 1. Consider the following testing rule: for any time step $t$ such that $h_{1}({\\bf x}_{t})\\,=\\,h_{2}({\\bf x}_{t})$ , we predict the agreed label; else, we predict the same way as in Proposition 1. Clearly, we only make errors for the second case. Invoking Proposition 1 with $\\gamma_{t}=\\gamma_{\\mathbf{H}}$ for all $t\\in[T]$ , we have $\\begin{array}{r}{n^{\\ast}\\dot{\\leq}\\;\\frac{2\\log(2/\\delta)}{\\gamma_{\\mathrm{H}}}}\\end{array}$ . Therefore, the tuple $(\\mathcal{H},\\mathcal{K})$ is pairwise testable with $\\begin{array}{r}{C(\\delta)=\\frac{2\\log(2/\\delta)}{\\gamma_{\\mathrm{H}}}}\\end{array}$ . The upper bound on classification risk then follows by Theorem 3. The lower bound follows by Le Cam\u2019s two point method and constructing a hard hypothesis class using an epoch approach. We refer to Appendix D for the complete details. ", "page_idx": 7}, {"type": "text", "text": "Remark 1. Note that our techniques can be easily extended to infinite classes using the covering techniques from [1, 22]. Moreover, by applying Proposition $^{\\,l}$ , our results can be extended to scenarios where the gap parameters $\\gamma_{t}$ are not uniformly bounded, such as in the case of Tsybakovtype noise $l8J_{:}$ , which would lead to risk bounds that scale sublinearly with $T$ , in contrast to the constant risk in Theorem 2. We leave the details and extensions for a longer manuscript $[23].$ . ", "page_idx": 7}, {"type": "text", "text": "4 Tighter Bounds for Binary Labels via $L^{2}$ Gap ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We have demonstrated in Theorem 2 that the minimax risk is tightly characterized by the Hellinger gap induced by the kernel. However, the dependency on log $|{\\mathcal{H}}|$ remains sub-optimal. We show in this section a tight dependency on $\\log\\left|\\mathcal{H}\\right|$ for classes with binary true labels via the $L^{2}$ gap. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5. Let $\\mathcal{H}\\,\\subset\\,\\{0,1\\}^{\\mathcal{X}}$ be any finite binary valued class, $\\kappa$ be any noisy kernel that is well-separated at scale $\\gamma_{\\mathbf{L}}$ w.r.t. the $L^{2}$ -distance 5 (Definition 3). Then, the expected minimax risk, as in Definition $^{\\,I}$ , is upper bounded by: $\\begin{array}{r}{\\tilde{r}_{T}(\\mathcal{H},\\mathcal{K})\\leq\\frac{16\\log|\\mathcal{H}|}{\\gamma_{\\mathrm{L}}}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "We begin with the following simple geometry fact that is crucial to our proof. ", "page_idx": 7}, {"type": "text", "text": "Lemma 1. Let $\\mathcal{Q}\\;\\subset\\;\\mathcal{D}(\\tilde{\\mathcal{D}})$ be a convex and closed set, $p$ be a point outside of $\\mathcal{Q}$ with $\\gamma\\ {\\stackrel{\\mathrm{def}}{=}}$ $\\operatorname*{inf}_{q\\in\\mathcal{Q}}L^{2}(p,q)$ . Denote by $q^{*}\\in\\mathcal{Q}$ the (unique) point that attains $L^{2}(p,q^{*})=\\gamma$ . Then for any $q\\in\\mathcal{Q},$ , we have $L^{2}(q,p)-L^{2}(q,q^{*})\\geq L^{2}(p,q^{*})=\\gamma$ . ", "page_idx": 7}, {"type": "text", "text": "Proof. By the hyperplane separation theorem, the hyperplane perpendicular to line segment $p-q^{*}$ at $q^{*}$ separates $\\mathcal{Q}$ and $p$ . Therefore, the degree $\\theta$ of angle formed by $p-q^{*}-q$ is greater than $\\pi/2$ By the law of cosines, $L^{2}(q,p)\\geq L^{2}(q,q^{\\ast})+L^{2}(q^{\\ast},\\bar{p})=L^{2}(q,\\dot{q^{\\ast}}\\bar{)}+\\gamma$ . \u53e3 ", "page_idx": 7}, {"type": "text", "text": "Our key idea of proving Theorem 5 is to reduce the robust (noisy) online classification problem to a suitable conditional distribution estimation problem, as discussed next. ", "page_idx": 7}, {"type": "text", "text": "Online conditional distribution estimation. Let $\\mathcal{F}\\subset\\mathcal{D}(\\tilde{\\mathcal{D}})^{\\chi}$ be a class of functions mapping $\\mathcal{X}$ to distributions in $\\mathcal{D}(\\tilde{\\mathcal{D}})$ . Online conditional distribution estimation is a game between Nature and an estimator that follows the following protocol: (1) at each times step $t$ , Nature selects some $\\mathbf{x}_{t}\\in\\mathcal{X}$ and reveals it to the estimator; (2) the estimator then makes an estimation $\\hat{p}_{t}\\in\\mathcal{D}(\\tilde{\\mathcal{D}})$ , based on $\\mathbf{x}^{t},\\tilde{y}^{t-1}$ ; (3) Nature then selects some $\\tilde{p}_{t}\\in\\mathcal{D}(\\tilde{\\mathcal{D}})$ , samples $\\tilde{y}_{t}\\sim\\tilde{p}_{t}$ and reveals $\\tilde{y}_{t}$ to the estimator. The goal is to find a (deterministic) estimator $\\Phi$ that minimizes the regret: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}(\\mathcal{F},\\Phi)=\\operatorname*{sup}_{f\\in\\mathcal{F},\\mathbf{x}^{T}\\in\\mathcal{X}^{T}}\\mathbb{Q}^{T}\\left[\\sum_{t=1}^{T}L(\\tilde{p}_{t},\\hat{p}_{t})-L(\\tilde{p}_{t},f(\\mathbf{x}_{t}))\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\hat{p}_{t}=\\Phi(\\mathbf{x}^{t},\\tilde{y}^{t-1})$ , $\\mathbb{Q}^{T}$ is the operator specified in Definition 1 by setting $\\mathcal{Q}_{y}^{\\mathbf{x}}:=\\mathcal{D}(\\tilde{\\mathcal{V}})$ for all $\\mathbf{x},y$ , and $L$ is any divergence. We emphasize that the distributions $\\tilde{p}^{T}$ are not necessarily realizable by $f$ and are selected completely arbitrarily. This contrasts with the well-specified cases employed in [10, 4], and is the key that enables us to handle the unknown noisy label distributions. ", "page_idx": 7}, {"type": "text", "text": "We now establish the following key technical lemma, see Appendix $\\boldsymbol{\\mathrm E}$ for proof. ", "page_idx": 7}, {"type": "text", "text": "Lemma 2. Let $\\mathcal{F}\\subset\\mathcal{D}(\\tilde{\\mathcal{D}})^{\\chi}$ be a finite distribution-valued function class. Then, for the $L^{2}$ divergence, there exists an estimator $\\Phi$ , i.e., the Exponential Weight Average (EWA) algorithm, such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}(\\mathcal F,\\Phi)\\leq4\\log|\\mathcal F|.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Moreover, estimation $\\hat{p}_{t}$ is a convex combination of $\\{f(\\mathbf{x}_{t}):f\\in\\mathcal{F}\\}$ . ", "page_idx": 8}, {"type": "text", "text": "Proof Sketch of Theorem 5. We provide the high level ideas and refer to Appendix F for complete details. We define the following distribution-valued function class $\\mathcal{F}$ using hypothesis class $\\mathcal{H}$ and noisy kernel $\\kappa$ . For any $\\textbf{x}\\in\\:\\mathcal{X}$ , we denote by $\\mathcal{Q}_{\\mathrm{0}}^{\\mathbf{x}}$ and $\\mathcal{Q}_{1}^{\\mathbf{x}}$ the sets of noisy label distributions corresponding to labels 0 and 1, respectively. Since the kernel $\\kappa$ is well-separated at scale $\\gamma_{\\mathbf{L}}$ under $L^{2}$ divergence, we have, by the hyperplane separation theorem, that there must be $q_{0}^{\\mathbf{x}}\\in\\mathcal{Q}_{0}^{\\mathbf{x}}$ and $q_{1}^{\\mathbf{x}}\\in\\mathcal{Q}_{1}^{\\mathbf{x}}$ such that $L^{2}(q_{0}^{\\bf x},\\bar{q}_{1}^{\\bf x})=\\bar{L}^{2}(\\bar{\\mathcal{Q}}_{0}^{\\bf x},\\mathcal{Q}_{1}^{\\bf x})\\stackrel{.}{\\geq}\\gamma_{\\bf L}$ . We now define for any $h\\in\\mathcal H$ the function $f_{h}$ such that $\\forall\\mathbf{x}\\in\\mathcal{X}$ , $f_{h}({\\bf x})=q_{h({\\bf x})}^{\\bf x}.$ . Let $\\mathcal{F}=\\{f_{h}:h\\in\\mathcal{H}\\}$ and $\\Phi$ be the estimator from Lemma 2 with class $\\mathcal{F}$ and $L^{2}$ divergence (using $\\mathbf{x}^{T},\\boldsymbol{\\tilde{y}}^{T}$ from the original noisy classification game). Our classification rule is defined as $\\hat{y}_{t}=\\operatorname*{arg\\,min}_{y}\\{L^{2}(q_{y}^{\\mathbf{x}_{t}},\\hat{p}_{t}):y\\in\\{0,\\bar{1}\\}\\}$ . That is, we predict the label $y$ so that $q_{y}^{\\mathbf{x}_{t}}$ is closer to $\\hat{p}_{t}$ under $L^{2}$ divergence, where $\\hat{p}_{t}=\\Phi(\\mathbf{x}^{t},\\tilde{y}^{t-1})$ . ", "page_idx": 8}, {"type": "text", "text": "Let $h^{*}\\in\\mathcal{H}$ be the underlying true classification function. We have by Lemma 2 that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{x}^{T}\\in\\mathcal{X}^{T}}\\mathbb{Q}_{K}^{T}\\left[\\sum_{t=1}^{T}L^{2}(\\tilde{p}_{t},\\hat{p}_{t})-L^{2}(\\tilde{p}_{t},f_{h^{*}}(\\mathbf{x}_{t}))\\right]\\leq4\\log|\\mathcal{F}|\\leq4\\log|\\mathcal{H}|,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbb{Q}_{K}^{T}$ is the operator in Definition 1. Now, our key technical goal is to show that $L^{2}(\\tilde{p}_{t},\\hat{p}_{t})\\gets$ $\\begin{array}{r}{L^{2}(\\tilde{p}_{t},\\bar{f}_{h^{*}}^{'\\sim}(\\mathbf x_{t}))\\stackrel{{}\\sim}\\ L^{2}(\\hat{p}_{t},f_{h^{*}}(\\mathbf x_{t}))\\geq\\frac{\\gamma_{\\mathrm{L}}}{4}1\\{\\hat{y}_{t}\\neq h^{*}(\\mathbf x_{t})\\}}\\end{array}$ via Lemma 1 and a geometric argument, as illustrated in the figure below: ", "page_idx": 8}, {"type": "image", "img_path": "Ke3MSP8Nr6/tmp/284879174f2c3ec82b3817930c75f71bec00e0253d15ef28d4421eadc5a60b06.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The expected minimax risk bound $\\begin{array}{r}{\\sum_{t=1}^{T}1\\{\\hat{y}_{t}\\ne h^{*}(\\mathbf x_{t})\\}\\le\\frac{16\\log|\\mathcal{H}|}{\\gamma_{\\mathrm{L}}}}\\end{array}$ 16 lo\u03b3g |H|then follows from (11). ", "page_idx": 8}, {"type": "text", "text": "Although both our proofs and those provided in [1] are based on the EWA algorithm, the analysis and resulting algorithms are fundamentally different. For instance, in [1], the learning rate of EWA depends on the parameter $\\eta$ , while we set it to $1/4$ (see Appendix E). More importantly, our proof applies to any noisy kernel that satisfies the well-separation condition (including cases where $|\\tilde{\\mathcal{D}}|>2)$ , which beneftis from our geometric interpretation of the kernels. Interestingly, for the specific setting investigated in [1] (i.e., Example 1), our result yields the same order up to a constant factor, since $1-2\\sqrt{\\eta(1-\\eta)}=\\Theta((1-2\\eta)^{2})$ for $\\eta\\in\\left[0,\\frac{1}{2}\\right)$ . In general, we have $4\\gamma_{\\mathrm{L}}\\leq\\gamma_{\\mathrm{H}}\\leq\\sqrt{M\\gamma_{\\mathrm{L}}}$ . ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we provide nearly matching lower and upper bounds for online classification with noisy labels via the Hellinger gap of the induced noisy label distributions. Our approach works for a wide range of hypothesis classes and noisy mechanisms. We expect our results to have a wide range of applications, such as online learning under (local) differential privacy constraints and online denoising tasks involving data derived from (noisy) physical measurements (such as learning from quantum data [15]). The main open problem remaining is to close the logarithmic gap in Theorem 2 for general kernels. While our work primarily focuses on the information-theoretically achievable minimax risks, we believe that finding computationally efficient predictors (including oracle-efficient methods as in [14]) would also be of significant interest. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by the NSF Center for Science of Information (CSoI) Grant CCF-0939370, and also by NSF Grants CCF-2006440, CCF-2007238, and CCF-2211423. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Shai Ben-David, D\u00e1vid P\u00e1l, and Shai Shalev-Shwartz. Agnostic online learning. In Conference on Learning Theory, volume 3, 2009.   \n[2] Alankrita Bhatt and Young-Han Kim. Sequential prediction under log-loss with side information. In Algorithmic Learning Theory, pages 340\u2013344. PMLR, 2021.   \n[3] Blair Bilodeau, Dylan Foster, and Daniel Roy. Tight bounds on minimax regret under logarithmic loss via self-concordance. In International Conference on Machine Learning, pages 919\u2013929. PMLR, 2020.   \n[4] Blair Bilodeau, Dylan J Foster, and Daniel M Roy. Minimax rates for conditional density estimation via empirical entropy. arXiv preprint arXiv:2109.10461, 2021.   \n[5] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning and Games. Cambridge University Press, 2006.   \n[6] Nicolo Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. Online learning of noisy data. IEEE Transactions on Information Theory, 57(12):7907\u20137931, 2011.   \n[7] Amit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz. Multiclass learnability and the erm principle. J. Mach. Learn. Res., 16(1):2377\u20132404, 2015.   \n[8] Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Efficiently learning halfspaces with tsybakov noise. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 88\u2013101, 2021.   \n[9] Sam Efromovich. Conditional density estimation in a regression setting. The Annals of Statistics, 35:2504\u20132535, 2007.   \n[10] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \n[11] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. In Conference on Learning Theory, pages 1785\u20131816. PMLR, 2020.   \n[12] Peter D Gr\u00fcnwald and Nishant A Mehta. Fast rates for general unbounded loss functions: from erm to generalized bayes. The Journal of Machine Learning Research, 21(1):2040\u20132119, 2020.   \n[13] Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends\u00ae in Optimization, 2(3-4):157\u2013325, 2016.   \n[14] Sham Kakade and Adam T Kalai. From batch to transductive online learning. Advances in Neural Information Processing Systems, 18, 2005.   \n[15] Abram Magner and Arun Padakandla. Fat shattering, joint measurability, and pac learnability of povm hypothesis classes. arXiv preprint arXiv:2308.12304, 2023.   \n[16] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. Advances in neural information processing systems, 26, 2013.   \n[17] Yury Polyanskiy and Yihong Wu. Information Theory: From Coding to Learning. Cambridge University Press, 2022.   \n[18] Alexander Rakhlin and Karthik Sridharan. Sequential probability assignment with binary alphabets and large classes of experts. arXiv preprint arXiv:1501.07340, 2015.   \n[19] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. In Advances in Neural Information Processing Systems, 2010.   \n[20] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.   \n[21] Dirk van der Hoeven, Nikita Zhivotovskiy, and Nicol\u00f2 Cesa-Bianchi. High-probability risk bounds via sequential predictors. arXiv preprint arXiv:2308.07588, 2023.   \n[22] Changlong Wu, Ananth Grama, and Wojciech Szpankowski. Online learning in dynamically changing environments. In Conference on Learning Theory, pages 325\u2013358. PMLR 195, 2023.   \n[23] Changlong Wu, Ananth Grama, and Wojciech Szpankowski. Robust online classification: From estimation to denoising. arXiv preprint arXiv:2309.01698, 2023.   \n[24] Changlong Wu, Mohsen Heidari, Ananth Grama, and Wojciech Szpankowski. Expected worst case regret via stochastic sequential covering. arXiv preprint arXiv:2209.04417, 2022.   \n[25] Changlong Wu, Mohsen Heidari, Ananth Grama, and Wojciech Szpankowski. Precise regret bounds for log-loss via a truncated bayesian algorithm. In Advances in Neural Information Processing Systems, volume 35, pages 26903\u201326914, 2022.   \n[26] Changlong Wu, Yifan Wang, Ananth Grama, and Wojciech Szpankowski. Learning functional distributions with private labels. In International Conference on Machine Learning (ICML), volume 202 of PMLR, pages 37728\u201337744. PMLR, 23\u201329 Jul 2023.   \n[27] Tong Zhang. Mathematical analysis of machine learning algorithms. Cambridge University Press, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Martingale Concentration Inequalities ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In this appendix, we present some standard concentration results for martingales, which will be useful for deriving high probability guarantees. We refer to [27, Chapter 13.1] for the proofs. ", "page_idx": 11}, {"type": "text", "text": "Lemma 3 (Azuma\u2019s Inequality). Let $X_{1},\\cdot\\cdot\\cdot\\,,X_{T}$ be an arbitrary random process adaptive to some filtration $\\{\\mathcal{F}_{t}\\}_{t\\le T}$ such that $\\left|X_{t}\\right|\\leq M$ for all $t\\leq T$ . Let $Y_{t}=\\mathbb{E}[X_{t}\\mid\\mathcal{F}_{t-1}]$ be the conditional expected random variable of $X_{t}$ . Then for all $\\delta>0$ , we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\sum_{t=1}^{T}Y_{t}<\\sum_{t=1}^{T}X_{t}+M\\sqrt{(T/2)\\log(1/\\delta)}\\right]\\geq1-\\delta,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "and ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\sum_{t=1}^{T}Y_{t}>\\sum_{t=1}^{T}X_{t}-M\\sqrt{(T/2)\\log(1/\\delta)}\\right]\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The following lemma provides a tighter concentration when $X_{t}\\ge0$ , which can be viewed as an Martingale version of the multiplicative Chernoff bound. ", "page_idx": 11}, {"type": "text", "text": "Lemma 4 ([27, Theorem 13.5]). Let $X_{1},\\cdot\\cdot\\cdot,X_{T}$ be an arbitrary random process adaptive to some filtration $\\{\\mathcal{F}_{t}\\}_{t\\le T}$ such that $0\\leq X_{t}\\leq M$ for all $t\\leq T$ . Let $Y_{t}=\\mathbb{E}[X_{t}\\mid{\\bar{\\mathcal{F}}}_{t-1}]$ be the conditional expected random variable of $X_{t}$ . Then for all $\\delta>0$ we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\sum_{t=1}^{T}Y_{t}<2\\sum_{t=1}^{T}X_{t}+2M\\log(1/\\delta)\\right]\\geq1-\\delta,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "and ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\sum_{t=1}^{T}Y_{t}>\\frac{1}{2}\\sum_{t=1}^{T}X_{t}-(M/2)\\log(1/\\delta)\\right]\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. Applying [27, Thm 13.5] with $\\xi_{t}=X_{t}/M$ and $\\lambda=1$ in the theorem. ", "page_idx": 11}, {"type": "text", "text": "B Proof of Theorem 3 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Let $h_{k^{*}}\\in\\mathcal{H}$ be the underlying true classification function and $\\mathbf{x}^{T}$ be the realization of features. We take $C=C(\\delta/2K)$ in Algorithm 1. By definition of pairwise testability and union bound, we have w.p. $\\geq1-\\delta/2$ over the randomness of $\\tilde{y}^{T}$ and the internal randomness of $\\Phi_{k,k^{*}}\\mathbf{S}$ that for all $k\\in[K]$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}1\\{h_{k^{*}}(\\mathbf x_{t})\\neq\\Phi_{k,k^{*}}(\\mathbf x^{t},\\tilde{y}^{t-1})\\}\\le C(\\delta/(2K)).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Note that for any other $\\{i,j\\}\\ \\partial\\ k^{*}$ , equation (12) may not hold for predictor $\\Phi_{i,j}$ . However, our following argument relies only on the guarantees for predictors $\\Phi_{k,k^{*}}$ , which effectively makes our pairwise testing realizable. ", "page_idx": 11}, {"type": "text", "text": "We now condition on the event defined in (12). Let $\\mathbf{v}_{t}^{k}$ with $k\\in[K]$ and $t\\in[T]$ be the surrogate loss vector, as defined in (7). We observe the following key properties ", "page_idx": 11}, {"type": "text", "text": "1. We have for all $t\\in[T]$ that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[K]}\\sum_{r=1}^{t}\\mathbf{v}_{r}^{k^{\\ast}}[j]\\leq C(\\delta/(2K));\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "2. For any $k\\neq k^{*}$ , we have for all $t\\in[T]$ : ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[K]}\\sum_{r=1}^{t}\\mathbf{v}_{r}^{k}[j]\\geq\\sum_{r=1}^{t}1\\{h_{k}(\\mathbf{x}_{r})\\neq h_{k^{*}}(\\mathbf{x}_{t})\\}-C(\\delta/(2K)).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The first property is straightforward by the definition of $\\mathbf{v}_{t}^{k}$ and (12). The second property holds since the lower bound is attained when $j=k^{*}$ . ", "page_idx": 12}, {"type": "text", "text": "We now analyze the performance of Algorithm 1. By property (13), we know that $k^{*}\\in S^{t}$ for all $t\\in[T]$ , i.e., $\\bar{|}S^{t}|\\geq1$ . Let $N_{t}=|S^{t}|$ . We define for all $t\\in[T]$ the potential: ", "page_idx": 12}, {"type": "equation", "text": "$$\nE_{t}=\\sum_{k\\in S^{t}}\\operatorname*{max}\\left\\{0,2C(\\delta/(2K))-\\sum_{r=1}^{t}1\\{h_{k}(\\mathbf{x}_{r})\\neq h_{k^{*}}(\\mathbf{x}_{r})\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Clearly, we have $E_{t}\\le2C(\\delta/(2K))N_{t}$ . Let $D_{t}=|\\{k\\in S^{t}:h_{k}(\\mathbf{x}_{t})\\neq h_{k^{*}}(\\mathbf{x}_{t})\\}|$ . We have: ", "page_idx": 12}, {"type": "equation", "text": "$$\nD_{t}\\leq N_{t}-N_{t+1}+E_{t}-E_{t+1},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "since for any $k\\in S_{t}$ such that $h_{k}({\\bf x}_{t})\\neq h_{k^{*}}({\\bf x}_{t})$ , either $k$ is removed from $S^{t+1}$ (which contributes at most $N_{t}\\mathrm{~-~}N_{t+1}$ ) or its contribution to $E_{t+1}$ is decreased by 1 when compared to $E_{t}$ (this is because by our construction of Algorithm 1 and property (14) once the contributions of $k$ to $E_{t}$ equals 0 it must be excluded from $S^{t+1}$ ). We have, by definition of $\\hat{y}_{t}$ , that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[1\\{h_{k^{*}}(\\mathbf{x}_{t})\\neq\\hat{y}_{t}\\}\\right]=\\frac{D_{t}}{|S^{t}|}\\le\\frac{N_{t}-N_{t+1}+E_{t}-E_{t+1}}{N_{t}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By a standard argument [14, Thm 2], we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\frac{N_{t}-N_{t+1}}{N_{t}}\\leq\\displaystyle\\sum_{t=1}^{T}\\left(\\frac{1}{N_{t}}+\\frac{1}{N_{t}-1}+\\dots+\\frac{1}{N_{t+1}+1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{k=1}^{K}\\frac{1}{k}\\leq\\log K.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Moreover, we observe that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\frac{E_{t}-E_{t+1}}{N_{t}}\\stackrel{(a)}{\\leq}\\frac{2C(\\delta/(2K))N_{1}-E_{2}}{N_{1}}+\\sum_{t=2}^{T}\\frac{E_{t}-E_{t+1}}{N_{t}}}}\\\\ &{\\stackrel{(b)}{\\leq}\\frac{2C(\\delta/(2K))(N_{1}-N_{2})}{N_{1}}}\\\\ &{\\quad+\\frac{2C(\\delta/(2K))N_{2}-E_{3}}{N_{2}}+\\sum_{t=3}^{T}\\frac{E_{t}-E_{t+1}}{N_{t}}}\\\\ &{\\stackrel{(c)}{\\leq}2C(\\delta/(2K))\\sum_{t=1}^{T}\\frac{N_{t}-N_{t+1}}{N_{t}}}\\\\ &{\\leq2C(\\delta/(2K))\\log K,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $(a)$ and $(b)$ follow by $E_{t}\\,\\leq\\,2C(\\delta/(2K))N_{t}$ and $N_{t}\\geq N_{t+1}$ ; $(c)$ follows by repeating the same argument for another $T-1$ steps. ", "page_idx": 12}, {"type": "text", "text": "Therefore, we conclude ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}1\\{h_{k^{*}}(\\mathbf x_{t})\\neq\\hat{y}_{t}\\}\\right]\\leq(1+2C(\\delta/(2K)))\\log K,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the randomness is on the selection of $\\hat{k}_{t}\\,\\sim\\,S^{t}$ . Since our selection of $\\hat{k}_{t}$ are independent (conditioning on $S^{t}$ ) for different $t$ , and the indicator is bounded by 1 and non-negative, we can invoke Lemma 4 (second part) to obtain a high probability guarantee of confidence $\\delta/2$ by introducing an extra $\\log(2/\\delta)$ additive term. The theorem now follows by a union bound with the event (12). ", "page_idx": 12}, {"type": "text", "text": "C Proof of Theorem 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We start with an application of the minimax theorem to hypothesis testing 6. ", "page_idx": 12}, {"type": "text", "text": "Lemma 5. Let ${\\mathcal P}_{0}$ and $\\mathcal{P}_{1}$ be two sets of distributions over a finite domain $\\Omega$ . If ${\\mathcal P}_{0}$ and $\\mathcal{P}_{1}$ are convex under $L_{1}$ distance (i.e., total variation), then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi:\\;\\Omega\\to[0,1]}\\operatorname*{sup}_{p_{0}\\in\\mathcal{P}_{0},p_{1}\\in\\mathcal{P}_{1}}\\left\\{\\mathbb{E}_{\\omega\\sim p_{0}}[1-\\phi(\\omega)]+\\mathbb{E}_{\\omega\\sim p_{1}}[\\phi(\\omega)]\\right\\}=1-\\operatorname*{inf}_{p_{0}\\in\\mathcal{P}_{0},p_{1}\\in\\mathcal{P}_{1}}\\left||p_{0}-p_{1}|\\right|\\mathrm{1}_{\\Gamma^{\\infty}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Moreover, if $\\phi^{*}$ is the function attains minimal, then the tester $\\psi^{*}(\\omega)=1\\{\\phi^{*}(\\omega)<0.5\\}$ achieves ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{p_{0}\\in\\mathcal{P}_{0},p_{1}\\in\\mathcal{P}_{1}}\\left\\{\\operatorname*{Pr}_{\\omega\\sim p_{0}}[\\psi^{*}(\\omega)\\neq0]+\\operatorname*{Pr}_{\\omega\\sim p_{1}}[\\psi^{*}(\\omega)\\neq1]\\right\\}\\leq2(1-\\operatorname*{inf}_{p_{0}\\in\\mathcal{P}_{0},p_{1}\\in\\mathcal{P}_{1}}||p_{0}-p_{1}||_{\\Upsilon}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Observe that the function $\\phi$ can be viewed as a vector in $[0,1]^{\\Omega}$ . Moreover, the distributions over $\\Omega$ can be viewed as vectors in $[0,1]^{\\Omega}$ as well. Therefore, we have $\\mathbb{E}_{\\omega\\sim p_{0}}[1-\\phi(\\omega)]+$ $\\mathbb{E}_{\\omega\\sim p_{1}}[\\phi(\\omega)]=\\langle p_{0},1-\\phi\\rangle+\\langle p_{1},\\phi\\rangle$ , which is a linear function w.r.t. both $(p_{0},p_{1})$ and $\\phi$ . Since the both $\\mathcal{P}_{0}\\times\\mathcal{P}_{1}$ and $[0,1]^{\\Omega}$ are convex and $[0,1]^{\\Omega}$ is compact, we can invoke the minimax theorem [5, Thm 7.1] to obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\phi:\\;\\Omega\\rightarrow[0,1]}{\\operatorname*{min}}\\underset{p_{0}\\in\\mathcal{P}_{0},p_{1}\\in\\mathcal{P}_{1}}{\\operatorname*{sup}}\\big\\{\\mathbb{E}_{\\omega\\sim p_{0}}[1-\\phi(\\omega)]+\\mathbb{E}_{\\omega\\sim p_{1}}[\\phi(\\omega)]\\big\\}}\\\\ &{\\qquad\\qquad\\qquad=\\underset{p_{0}\\in\\mathcal{P}_{0},p_{1}\\in\\mathcal{P}_{1}}{\\operatorname*{sup}}\\underset{\\phi:\\;\\Omega\\rightarrow[0,1]}{\\operatorname*{min}}\\big\\{\\mathbb{E}_{\\omega\\sim p_{0}}[1-\\phi(\\omega)]+\\mathbb{E}_{\\omega\\sim p_{1}}[\\phi(\\omega)]\\big\\}}\\\\ &{\\qquad\\qquad=\\underset{p_{0}\\in\\mathcal{P}_{0},p_{1}\\in\\mathcal{P}_{1}}{\\operatorname*{sup}}\\big\\{1-||p_{0}-p_{1}||_{\\Gamma}\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last equality follows by Le Cam\u2019s two point lemma [17, Theorem 7.7]. Let $\\phi^{*}$ be the function attains minimal and $\\psi^{*}(\\omega)=1\\{\\phi^{*}(\\omega)<\\bar{0}.5\\}$ . We have $1\\{\\psi^{*}(\\omega)\\neq i\\}\\leq2(1-i-\\phi^{*}(\\omega))$ for all $i\\in\\{0,1\\}$ . To see this, for $i=0$ , we have $\\psi^{*}(\\omega)\\neq0$ only if $\\phi^{*}(\\omega)<0.5$ , thus $1-\\phi^{*}(\\omega)\\geq$ 0.5 (the case for $i=1$ follows similarly). Therefore, we have for all $p_{0}\\in\\mathcal P_{0},p_{1}\\in\\mathcal P_{1}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}_{\\omega\\sim p_{0}}[\\psi^{*}(\\omega)\\neq0]+\\operatorname*{Pr}_{\\omega\\sim p_{1}}[\\psi^{*}(\\omega)\\neq1]\\leq2(\\mathbb{E}_{\\omega\\sim p_{0}}[1-\\phi^{*}(\\omega)]+\\mathbb{E}_{\\omega\\sim p_{1}}[\\phi^{*}(\\omega)]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This completes the proof. ", "page_idx": 13}, {"type": "text", "text": "We now establish the following key property, which demonstrates that the distribution classes constructed in Theorem 4 satisfy the condition of Lemma 5. ", "page_idx": 13}, {"type": "text", "text": "Lemma 6. Let $\\mathcal{Q}_{0}^{J}$ and $\\mathcal{Q}_{1}^{J}$ be the sets in Theorem 4. Then $\\mathcal{Q}_{0}^{J}$ and $\\mathcal{Q}_{1}^{J}$ are convex under $L_{1}$ distance. ", "page_idx": 13}, {"type": "text", "text": "Proof. Let $p_{1},p_{2}\\in\\mathcal{Q}_{i}^{J}$ for $i\\in\\{0,1\\}$ and $\\lambda\\in[0,1]$ . We need to show that $p=\\lambda p_{1}+(1-\\lambda)p_{2}\\in$ $\\mathcal{Q}_{i}^{J}$ as well. For any given $t\\in[T]$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\Tilde{y}_{t}\\mid\\Tilde{y}^{t-1})=\\frac{\\lambda p_{1}(\\Tilde{y}^{t})+(1-\\lambda)p_{2}(\\Tilde{y}^{t})}{\\lambda p_{1}(\\Tilde{y}^{t-1})+(1-\\lambda)p_{2}(\\Tilde{y}^{t-1})}}\\\\ &{\\qquad\\qquad=\\lambda\\frac{p_{1}(\\Tilde{y}^{t-1})}{p(\\Tilde{y}^{t-1})}p_{1}(\\Tilde{y}_{t}\\mid\\Tilde{y}^{t-1})+(1-\\lambda)\\frac{p_{2}(\\Tilde{y}^{t-1})}{p(\\Tilde{y}^{t-1})}p_{2}(\\Tilde{y}_{t}\\mid\\Tilde{y}^{t-1})\\in\\mathcal{Q}_{i}^{\\times_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last inclusion follows by convexity of $\\mathcal{Q}_{i}^{\\mathbf{x}_{t}}$ as assumed in Theorem 4. Therefore, we have $p\\in\\mathcal{Q}_{i}^{J}$ by definition of $\\mathcal{Q}_{i}^{J}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Now, our main technical part is to bound the total variation $\\mathsf{T V}(\\mathcal{Q}_{0}^{J},\\mathcal{Q}_{1}^{J})$ . The primary challenge comes from controlling the dependencies of conditional marginals of the distributions. To this end, we introduce the concept of Renyi divergence. Let $p_{1},p_{2}$ be two distributions over the same finite domain $\\Omega$ , the $\\alpha$ -Renyi divergence is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\nD_{\\alpha}(p_{1},p_{2})=\\frac{1}{\\alpha-1}\\log\\mathbb{E}_{\\omega\\sim p_{2}}\\left[\\left(\\frac{p_{1}(\\omega)}{p_{2}(\\omega)}\\right)^{\\alpha}\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If $p,q$ are distributions over domain $\\Omega_{1}\\times\\Omega_{2}$ and $r$ is a distribution over $\\Omega_{1}$ , then the conditional $\\alpha$ -Renyi divergence is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\nD_{\\alpha}(p,q\\mid r)={\\frac{1}{\\alpha-1}}\\log\\mathbb{E}_{\\omega_{1}\\sim r}\\left[\\sum_{\\omega_{2}\\in\\Omega_{2}}p(\\omega_{2}\\mid\\omega_{1})^{\\alpha}q(\\omega_{2}\\mid\\omega_{1})^{1-\\alpha}\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The following property about Renyi divergence is well known [17, Chapter 7.12]: ", "page_idx": 13}, {"type": "text", "text": "Lemma 7. Let $p,q$ be two distributions over $\\Omega_{1}\\times\\Omega_{2}$ and $\\ensuremath{p^{(1)}}$ and $q^{(1)}$ be the restrictions of $p,q$ on $\\Omega_{1}$ , respectively. Then the following chain rule holds ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{\\alpha}(p,q)=D_{\\alpha}(p^{(1)},q^{(1)})+D_{\\alpha}(p,q\\mid r),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $r(\\omega_{1})=p^{(1)}(\\omega_{1})^{\\alpha}q^{(1)}(\\omega_{1})^{1-\\alpha}e^{-(\\alpha-1)D_{\\alpha}(p^{(1)},q^{(1)})}$ is a distribution over $\\Omega_{1}$ . ", "page_idx": 14}, {"type": "text", "text": "The following key result bounds the Renyi divergence between $\\mathcal{Q}_{0}^{J}$ and $\\mathcal{Q}_{1}^{J}$ : ", "page_idx": 14}, {"type": "text", "text": "Proposition 2. Let $\\mathcal{Q}_{0}^{J}$ and $\\mathcal{Q}_{1}^{J}$ be the sets in Theorem 4. If ${\\operatorname*{inf}}_{p\\in{\\mathcal{Q}}_{0}^{\\mathbf{x}_{t}},q\\in{\\mathcal{Q}}_{1}^{\\mathbf{x}_{t}}}\\,D_{\\alpha}(p,q)\\geq\\eta_{t}$ holds for all $t\\leq J$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{p\\in\\mathcal{Q}_{0}^{J},q\\in\\mathcal{Q}_{1}^{J}}D_{\\alpha}(p,q)\\geq\\sum_{t=1}^{J}\\eta_{t}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We prove by induction on $J$ . The base case for $J\\,=\\,1$ is trivial, since $\\mathcal{Q}_{0}^{1}\\;=\\;\\mathcal{Q}_{0}^{{\\bf x}_{1}}$ and $\\mathcal{Q}_{1}^{1}=\\mathcal{Q}_{1}^{\\mathbf{x}_{1}}$ . We now prove the induction step with $J\\ge2$ . For any pair $p\\in\\mathcal{Q}_{0}^{J}$ and $q\\in\\mathcal{Q}_{1}^{J}$ , we have by Lemma 7 that $D_{\\alpha}(p,q)=D_{\\alpha}(p^{(1)},q^{(1)})+D_{\\alpha}(p,q\\mid r)$ , where $\\ensuremath{p^{(1)}}$ , $q^{(1)}$ are restrictions of $p,q$ on $\\tilde{y}^{J-1}$ and $r$ is a distribution over $\\tilde{\\mathcal{D}}^{J-1}$ . By definition of $\\alpha$ -Renyi divergence, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\alpha}(p,q\\mid r)\\ge\\displaystyle\\operatorname*{inf}_{\\tilde{y}^{J-1}}\\frac{1}{\\alpha-1}\\log\\sum_{\\tilde{y},r\\in\\tilde{\\mathcal{Y}}}p(\\tilde{y}_{J}\\mid\\tilde{y}^{J-1})^{\\alpha}q(\\tilde{y}_{J}\\mid\\tilde{y}^{J-1})^{1-\\alpha}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\operatorname*{inf}_{\\tilde{y}^{J-1}}D_{\\alpha}(p_{\\tilde{y}_{J}\\mid\\tilde{y}^{J-1}},q_{\\tilde{y}_{J}\\mid\\tilde{y}^{J-1}})}\\\\ &{\\overset{(a)}{\\ge}\\displaystyle\\operatorname*{inf}_{p\\in\\mathcal{Q}_{0}^{\\times J},q\\in\\mathcal{Q}_{1}^{\\times J}}D_{\\alpha}(p,q)\\overset{(b)}{\\ge}\\eta_{J},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(a)$ follows since $p_{\\tilde{y}_{J}|\\tilde{y}^{J-1}}\\in\\mathcal{Q}_{0}^{\\mathbf{x}_{J}}$ and $q_{\\tilde{y}_{J}|\\tilde{y}^{J-1}}\\in\\mathcal{Q}_{1}^{\\mathbf{x}_{J}}$ by the definition of $\\mathcal{Q}_{\\mathrm{0}}^{J}$ and $\\mathcal{Q}_{1}^{J};\\left(b\\right)$ follows by assumption. The result then follows by induction hypothesis $\\begin{array}{r}{D_{\\alpha}(p^{(1)},q^{(1)})\\ge\\sum_{t=1}^{J-1}\\eta_{t}}\\end{array}$ , since $p^{(1)}\\in\\mathcal{Q}_{0}^{J-1}$ and $q^{(1)}\\in\\mathcal{Q}_{1}^{J-1}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "The following result converts the Renyi divergence based bounds to that with Hellinger divergence. ", "page_idx": 14}, {"type": "text", "text": "Proposition 3. Let $\\mathcal{Q}_{0}^{J}$ and $\\mathcal{Q}_{1}^{J}$ be the sets in Theorem 4. If $H^{2}(\\mathcal{Q}_{0}^{\\mathbf{x}_{t}},\\mathcal{Q}_{1}^{\\mathbf{x}_{t}})\\geq\\gamma_{t}\\geq0$ holds for all $t\\in\\bar{[J]}$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{p\\in Q_{0}^{J},q\\in Q_{1}^{J}}H^{2}(p,q)\\geq2\\left(1-\\prod_{t=1}^{J}(1-\\gamma_{t}/2)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Observe that, for any distributions $p,q$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nH^{2}(p,q)=2(1-e^{-\\frac{1}{2}D_{1/2}(p,q)}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Specifically, for give $p\\in\\mathcal{Q}_{0}^{J}$ and $q\\in\\mathcal{Q}_{1}^{J}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n1-H^{2}(p,q)/2=e^{-\\frac{1}{2}D_{1/2}(p,q)}\\leq e^{-\\frac{1}{2}\\sum_{t=1}^{J}\\eta_{t}}=\\prod_{t=1}^{J}e^{-\\frac{1}{2}\\eta_{t}}\\leq\\prod_{t=1}^{J}(1-\\gamma_{t}/2),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\eta_{t}\\mathbf{s}$ are the constants in Proposition 2 and the last inequality follows by $e^{-\\frac{1}{2}\\eta_{t}}\\leq1-\\gamma_{t}/2$ due to (17) again. This completes the proof. ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 4. We have, by Lemma 5, that the testing error is upper bounded by $2(1~-$ $\\operatorname*{inf}_{p\\in\\mathcal{Q}_{0}^{J},q\\in\\mathcal{Q}_{1}^{J}}\\big|\\big|p-q\\big|\\big|\\mathsf{T v}\\big)$ . Fix any such $p,q$ , we have by [17, Equation 7.22] that $1-||p-q||_{\\mathsf{T V}}\\leq$ $1-{\\textstyle\\frac{1}{2}}H^{2}(p,q)$ . The result then follows by Proposition 3. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "D Proof of Theorem 2 (Lower Bound) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We denote $L\\le\\log K$ with $K\\,=\\,|\\mathcal{H}|$ , and $\\mathbf{x}_{1},\\cdots,\\mathbf{x}_{L}$ be $L$ distinct elements in $\\mathcal{X}$ satisfies the condition of the theorem. We define for any $\\mathbf{b}\\in\\{0,1\\}^{L}$ a function $h_{\\mathbf{b}}$ such that for all $i\\in[L]$ , $h_{\\mathbf{b}}(\\mathbf{x}_{i})\\,=\\,y_{i}$ if $\\mathbf{b}[i]=0$ and $h_{\\mathbf{b}}(\\mathbf{x}_{i})\\,=\\,y_{i}^{\\prime}$ otherwise, where $y_{i}\\;\\neq\\;y_{i}^{\\prime}\\;\\in\\;\\mathcal{V}$ are the elements that satisfy $\\operatorname*{inf}_{p\\in\\mathcal{Q}_{y_{i}}^{\\mathbf{x}_{i}},q\\in\\mathcal{Q}_{y_{i}^{\\prime}}^{\\mathbf{x}_{i}}}\\{H^{2}(p,q)\\}\\leq\\gamma_{\\mathbf{H}}$ . Let $\\mathcal{H}$ be the class consisting of all such $h_{\\mathbf{b}}$ . Let $q_{i}\\in\\mathcal{Q}_{y_{i}}^{\\mathbf{x}_{i}}$ and $q_{i}^{\\prime}\\in\\mathcal{Q}_{y_{i}^{\\prime}}^{\\mathbf{x}_{i}}$ be the elements satisfying $H^{2}(q_{i},q_{i}^{\\prime})\\,\\leq\\,\\gamma_{\\mathrm{H}}$ . We now partition the features $\\mathbf{x}^{T}$ into $L$ epochs, each of length $T/L$ , such that each epoch $i$ has constant feature $\\mathbf{x}_{i}$ . Let $\\mathbf{h}$ be a random function selected uniformly from $\\mathcal{H}$ . We claim that for any prediction rule $\\hat{y}_{t}$ and any epoch $i$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{h},\\tilde{y}^{T}}\\left[\\sum_{\\substack{t=i T/L-1}}^{(i+1)T/L}1\\{\\mathbf{h}(\\mathbf{x}_{t})\\neq\\hat{y}_{t}\\}\\right]\\ge\\Omega\\left(\\frac{1}{\\gamma_{\\mathbf{H}}}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\tilde{y}_{t}\\sim q_{i}$ if $\\mathbf{h}(\\mathbf{x}_{i})=y_{i}$ and $\\tilde{y}_{t}\\sim q_{i}^{\\prime}$ otherwise. The proposition now follows by counting the errors for all $L$ epochs. ", "page_idx": 15}, {"type": "text", "text": "We now establish (18) using the Le Cam\u2019s two point method. Clearly, for each epoch $i$ , the prediction performance depends only on the label $\\mathbf{y}_{i}=\\mathbf{\\bar{h}}(\\mathbf{x}_{i})$ , which is uniform over $\\{y_{i},\\bar{y}_{i}^{\\prime}\\}$ and independent for different epochs by construction. For any time step $j$ during the ith epoch, we denote by $\\tilde{y}^{j-1}$ and $\\tilde{y}^{\\prime j-1}$ the samples generated from $q_{i}$ and $q_{i}^{\\prime}$ , respectively. By the Le Cam\u2019s two point method [17, Theorem 7.7] the expected error at step $j$ is lower bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1-\\mathsf{T V}(\\widetilde{y}^{j-1},\\widetilde{y}^{\\prime j-1})}{2}\\geq\\frac{1-\\sqrt{H^{2}(\\widetilde{y}^{j-1},\\widetilde{y}^{\\prime j-1})(1-H^{2}(\\widetilde{y}^{j-1},\\widetilde{y}^{\\prime j-1})/4)}}{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inequality follows from [17, Equation 7.22]. Note that the RHS of (19) is monotone decreasing w.r.t. $H^{\\check{2}}(\\tilde{y}^{j-1},\\tilde{y}^{\\prime j-1})$ , since $H^{\\dot{2}}(p,q)\\leq2$ for all $p,q$ . ", "page_idx": 15}, {"type": "text", "text": "By the tensorization of Hellinger divergence [17, Equation 7.23], we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H^{2}(\\tilde{y}^{j-1},\\tilde{y}^{\\prime j-1})=2-2(1-H^{2}(q_{i},q_{i}^{\\prime})/2)^{j-1}\\leq2-2(1-\\gamma_{\\mathrm{{H}}}/2)^{j-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality is implied by $H^{2}(q_{i},q_{i}^{\\prime})\\leq\\gamma_{\\mathrm{H}}$ . Using the fact $\\begin{array}{r}{\\log(1-x)\\geq\\frac{-x}{1-x}}\\end{array}$ , we have if $\\gamma_{\\mathrm{{H}}}\\,\\leq\\,1$ and $\\begin{array}{r}{j-1\\leq\\frac{1}{\\gamma_{\\mathrm{H}}}}\\end{array}$ then $2-2(1-\\gamma_{\\mathrm{H}}/2)^{j-1}\\leq2(1-e^{-1})<2$ . Therefore, the RHS of (19) is lower bounded by an absolute positive constant for all $\\begin{array}{r}{j-1\\leq\\frac{1}{\\gamma_{\\mathrm{H}}}}\\end{array}$ , and hence the expected cumulative error will be lower bounded by $\\Omega(1/\\gamma_{\\mathrm{H}})$ during epoch $i$ . This completes the proof. ", "page_idx": 15}, {"type": "text", "text": "E Proof of Lemma 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before presenting a formal proof, we first develop some technical concepts. Let $\\tilde{\\mathcal{D}}$ be the noisy label set and $\\mathcal{D}(\\tilde{\\mathcal{D}})$ be the class of distributions over $\\tilde{\\mathcal{D}}$ . We say a function $\\ell:\\tilde{\\mathcal{P}}\\times\\mathcal{D}(\\tilde{\\mathcal{P}})\\to\\mathbb{R}^{+}$ is $\\alpha$ -exp-concave if for any $\\tilde{y}\\in\\tilde{\\mathcal{D}}$ , the function $e^{-\\alpha\\ell(\\tilde{y},p)}$ is concave w.r.t. $p$ for some $\\alpha\\in\\mathbb{R}^{\\geq0}$ . ", "page_idx": 15}, {"type": "text", "text": "Proposition 4. The function $\\ell(\\tilde{y},p)=||e_{\\tilde{y}}-p||_{2}^{2}$ is $1/4$ -Exp-concave, where $e_{\\tilde{y}}$ denotes distribution assigning probability 1 on $\\tilde{y}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. We have by [13, Lemma 4.2] that a function $f$ is $\\alpha$ -Exp-concave if and only if ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha\\nabla f(p)\\nabla f(p)^{\\mathsf{T}}\\preceq\\nabla^{2}f(p).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For any $q\\in\\mathcal{D}(\\tilde{\\mathcal{V}})$ , we denote $f(p)=||p-q||_{2}^{2}$ . We have $\\nabla f(p)=2(p-q)$ and $\\nabla^{2}f(p)=2I$ , where $I$ is the identity matrix. Taking any $u\\in\\mathbb{R}^{J}$ , we have $\\begin{array}{r}{\\frac{1}{4}\\langle u,2(p-q)\\rangle^{2}\\leq||u||_{2}^{2}||p-q||_{2}^{2}\\leq}\\end{array}$ $2||\\boldsymbol{u}||_{2}^{2}=2\\boldsymbol{u}^{\\top}I\\boldsymbol{u}$ , where the first inequality follows by Cauchy-Schwarz inequality and the second inequality follows by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n||p-q||_{2}^{2}=\\sum_{\\tilde{y}\\in\\tilde{\\mathcal{Y}}}(p[\\tilde{y}]-q[\\tilde{y}])^{2}\\leq\\sum_{\\tilde{y}\\in\\tilde{\\mathcal{Y}}}\\operatorname*{max}\\{p[\\tilde{y}],q[\\tilde{y}]\\}^{2}\\leq\\sum_{\\tilde{y}\\in\\tilde{\\mathcal{Y}}}p[\\tilde{y}]^{2}+q[\\tilde{y}]^{2}\\leq2,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since $p,q\\in\\mathcal{D}(\\tilde{\\mathcal{D}})$ . This completes the proof. ", "page_idx": 15}, {"type": "text", "text": "We now introduce the Exponential Weighted Average (EWA) algorithm and its regret analysis under the Exp-concave losses, which is mostly standard [5, Chapter 3.3] and we include it here for completeness. Let $\\mathcal{F}=\\{f_{1},\\cdot\\cdot\\cdot,f_{K}\\}\\subset\\mathcal{D}(\\tilde{\\mathcal{V}})^{\\bar{x}}$ be a $\\mathcal{D}(\\tilde{\\mathcal{D}})$ -valued function class and $\\ell:\\tilde{\\mathcal{P}}\\times\\tilde{\\mathcal{D}}(\\tilde{\\mathcal{P}})\\stackrel{}{\\rightarrow}\\mathbb{R}^{\\geq0}$ be $\\alpha$ -Exp-concave. The EWA algorithm is presented in Algorithm 2. ", "page_idx": 16}, {"type": "table", "img_path": "Ke3MSP8Nr6/tmp/a6715fa9d05b451435860142174a06938ce84879bff2dd0788f6a9c2305e45bc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Algorithm 2 provides the following regret bound: ", "page_idx": 16}, {"type": "text", "text": "Proposition 5 ([5, Proposition 3.1]). Let $\\mathcal{F}\\subset\\mathcal{D}(\\tilde{\\mathcal{Y}})^{\\mathcal{X}}$ be any finite class and $\\ell$ be an $\\alpha$ -Exp-concave loss. If $\\hat{p}_{t}$ is the estimator in Algorithm 2, then for any $\\mathbf{x}^{T}\\in\\mathcal{X}^{T}$ and $\\tilde{y}^{T}\\in\\tilde{\\mathcal{Y}}^{T}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell(\\tilde{y}_{t},\\hat{p}_{t})-\\ell(\\tilde{y}_{t},f(\\mathbf{x}_{t}))\\leq\\frac{\\log|\\mathcal{F}|}{\\alpha}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 2. Let $\\Phi$ be the EWA estimator as in Algorithm 2 with input class $\\mathcal{F}$ , loss $\\ell(\\tilde{y},p)$ d=ef $L^{2}(e_{\\tilde{y}},p)$ and $\\alpha=1/4$ . Let $\\tilde{y}^{T}$ be any realization of the noisy labels. We denote $e_{t}$ as the standard base of $\\mathbb{R}^{M}$ with value 1 at position $\\tilde{y}_{t}$ and zeros otherwise. By $1/4$ -Exp-concavity of loss $\\ell$ (Proposition 4) and the regret bound from Proposition 5, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F},\\mathbf{x}^{T}\\in\\mathcal{X}^{T},\\tilde{y}^{T}\\in\\tilde{\\mathcal{Y}}^{T}}\\sum_{t=1}^{T}L^{2}(e_{t},\\hat{p}_{t})-L^{2}(e_{t},f(\\mathbf{x}_{t}))\\leq4\\log|\\mathcal{F}|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that, this bound holds point-wise w.r.t. any individual $\\mathbf{x}^{T}$ and $\\tilde{y}^{T}$ . ", "page_idx": 16}, {"type": "text", "text": "Fix any $\\mathbf{x}^{T}$ and (joint) distribution $\\tilde{p}$ over $\\tilde{\\mathcal{V}}^{T}$ . We denote $\\mathbb{E}_{t}$ as the conditional expectation on $\\tilde{y}_{t}$ over the randomness of $\\tilde{y}^{T}\\sim\\tilde{p}$ conditioning on $\\tilde{y}^{t-1}$ and denote $\\tilde{p}_{t}$ as the conditional marginal. By the elementary identity $\\begin{array}{r}{\\mathbb{E}[L^{2}(X,p)-L^{2}(X,q)]=L^{2}(\\mathbb{E}[X],p)-L^{2}(\\mathbb{E}[X],q)}\\end{array}$ for any random variable $X$ over $\\mathcal{D}(\\tilde{\\mathcal{D}})$ , we have for all $t\\in[T]$ that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\left[L^{2}(e_{t},\\hat{p}_{t})-L^{2}(e_{t},f(\\mathbf{x}_{t}))\\right]=L^{2}(\\tilde{p}_{t},\\hat{p}_{t})-L^{2}(\\tilde{p}_{t},f(\\mathbf{x}_{t})),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since $\\mathbb{E}_{t}[e_{t}]=\\tilde{p}_{t}$ for $\\tilde{y}_{t}\\sim\\tilde{p}_{t}$ and $\\hat{p}_{t}$ depends only on $\\tilde{y}^{t-1}$ . We now take $\\mathbb{E}_{\\tilde{y}^{T}}$ on both sides of (20). By sup $\\mathbb{E}\\leq\\mathbb{E}$ sup and the law of total probability (i.e., $\\mathbb{E}_{\\tilde{y}^{T}}[X_{1}+\\cdot\\cdot\\cdot+X_{T}]=\\mathbb{E}_{\\tilde{y}^{T}}[\\mathbb{E}_{1}[X_{1}]+\\cdot\\cdot\\cdot+$ $\\mathbb{E}_{T}[X_{T}]]$ for any random variables $X^{T}$ ), we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F},\\mathbf{x}^{T}\\in\\mathcal{X}^{T}}\\operatorname*{sup}_{\\tilde{p}}\\mathbb{E}_{\\tilde{y}^{T}\\sim\\tilde{p}}\\left[\\sum_{t=1}^{T}L^{2}(\\tilde{p}_{t},\\hat{p}_{t})-L^{2}(\\tilde{p}_{t},f(\\mathbf{x}_{t}))\\right]\\leq4\\log|\\mathcal{F}|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\tilde{p}$ runs over all (joint) distributions over $\\tilde{\\mathcal{V}}^{T}$ . The lemma then follows by the equivalence between operators $\\mathbb{Q}_{K}^{T}\\,\\equiv\\,\\operatorname*{sup}_{-}\\mathbb{E}_{\\tilde{y}^{T}}$ when taking the kernel set $\\mathcal{Q}_{y}^{\\mathbf{x}}\\;:=\\;\\mathcal{D}(\\tilde{\\mathcal{V}})$ for all $\\mathbf{x},y$ (see the discussion following Definition 1). The last part follows by the fact that the EWA estimator automatically ensures $\\hat{p}_{t}$ is a convex combination of $\\{f(\\mathbf{x}_{t}):f\\in\\mathcal{F}\\}$ for all $t\\in[T]$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "F Proof of Theorem 5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We define the following distribution valued function class $\\mathcal{F}$ using hypothesis class $\\mathcal{H}$ and noisy kernel $\\kappa$ . For any $\\mathbf{x}\\in\\mathcal{X}$ , we denote by $\\mathcal{Q}_{\\mathrm{0}}^{\\mathbf{x}}$ and $\\mathcal{Q}_{1}^{\\mathbf{x}}$ the sets of noisy label distributions corresponding to labels 0 and 1, respectively. Since the kernel $\\kappa$ is well-separated at scale $\\gamma_{\\mathbf{L}}$ under $L^{2}$ divergence, we have, by the hyperplane separation theorem, that there must be $q_{0}^{\\mathbf{x}}\\in\\mathcal{Q}_{0}^{\\mathbf{x}}$ and $q_{1}^{\\mathbf{x}}\\in\\mathcal{Q}_{1}^{\\mathbf{x}}$ such that $L^{2}(q_{0}^{\\bf x},q_{1}^{\\bf x})\\,=\\,^{\\top}L^{2}\\bar{(}\\,\\mathcal{Q}_{0}^{\\bf x},\\mathcal{Q}_{1}^{\\bf x})\\,\\ge\\,\\gamma_{\\mathrm{L}}$ . We now define for any $h\\ \\in\\ \\mathcal H$ the function $f_{h}$ such that $\\forall\\mathbf{x}\\in\\mathcal{X}$ , $f_{h}({\\bf x})=q_{h({\\bf x})}^{\\bf x}.$ Let $\\dot{\\mathcal{F}}=\\{f_{h}:h\\in\\mathcal{H}\\}$ and $\\Phi$ be the estimator from Lemma 2 with class $\\mathcal{F}$ and $L^{2}$ divergence (using $\\mathbf{x}^{T},\\boldsymbol{\\tilde{y}}^{T}$ from the original noisy classification game). Our classification predictor is as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{y}_{t}=\\arg\\operatorname*{min}_{y}\\{L^{2}(q_{y}^{\\mathbf{x}_{t}},\\hat{p}_{t}):y\\in\\{0,1\\}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "That is, we predict the label $y$ so that $q_{y}^{\\mathbf{x}_{t}}$ is closer to $\\hat{p}_{t}$ under $L^{2}$ divergence, where $\\hat{p}_{t}=\\Phi(\\mathbf{x}^{t},\\tilde{y}^{t-1})$ . ", "page_idx": 17}, {"type": "text", "text": "Let $h^{*}\\in\\mathcal{H}$ be the underlying true classification function and $\\mathbf{x}^{T}$ be the realization of features. We have by Lemma 2 and $1/4$ -Exp-concavity of $L^{2}$ divergence that 7 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{Q}_{K}^{T}\\left[\\sum_{t=1}^{T}L^{2}(\\tilde{p}_{t},\\hat{p}_{t})-L^{2}(\\tilde{p}_{t},f_{h^{*}}(\\mathbf{x}_{t}))\\right]\\leq4\\log|\\mathcal{F}|,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbb{Q}_{K}^{T}$ is the operator in Definition 1. ", "page_idx": 17}, {"type": "text", "text": "For any time step $t$ , we denote by $y_{t}=h^{*}(\\mathbf x_{t})$ the true label. Since $q_{y}^{\\mathbf{x}_{t}}\\,\\in\\,\\mathcal{Q}_{y}^{\\mathbf{x}_{t}}$ are the elements satisfying $L^{2}(q_{0}^{\\mathbf{x}_{t}},q_{1}^{\\mathbf{x}_{t}})\\;=\\;L^{2}(\\mathcal{Q}_{0}^{\\mathbf{x}_{1}},\\mathcal{Q}_{1}^{\\mathbf{x}_{t}})\\;\\ge\\;\\gamma_{\\mathbf{L}}$ and $\\hat{q}_{t}$ is a convex combination of $q_{0}^{\\mathbf{x}_{t}}$ and $q_{1}^{\\mathbf{x}_{t}}$ (Lemma 2), we have $q_{y_{t}}^{\\mathbf{x}_{t}}$ is the closest element in $\\mathcal{Q}_{y_{t}}^{\\mathbf{x}_{t}}$ to $\\hat{p}_{t}$ under $L^{2}$ divergence. Note that, we also have $\\tilde{p}_{t}\\in\\mathcal{Q}_{y_{t}}^{\\mathbf{x}_{t}}$ . Invoking Lemma 1, we find ", "page_idx": 17}, {"type": "equation", "text": "$$\nL^{2}(\\tilde{p}_{t},\\hat{p}_{t})-L^{2}(\\tilde{p}_{t},q_{y_{t}}^{{\\bf x}_{t}})\\geq L^{2}(\\hat{p}_{t},q_{y_{t}}^{{\\bf x}_{t}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Denote $a_{t}\\;=\\;L^{2}(\\tilde{p}_{t},\\hat{p}_{t})\\,-\\,L^{2}(\\tilde{p}_{t},\\,f_{h^{*}}({\\bf x}_{t}))$ . We have, by (23) and $f_{h^{*}}(\\mathbf{x}_{t})\\ =\\ q_{y_{t}}^{\\mathbf{x}_{t}}$ that $a_{t}~\\geq$ $L^{2}(\\hat{p}_{t},f_{h^{*}}(\\mathbf x_{t}))$ . Therefore: ", "page_idx": 17}, {"type": "text", "text": "1. For all $t\\in[T],a_{t}\\geq0,\\mathrm{since}\\;\\forall p,q,\\;L^{2}(p,q)\\geq0;$   \n2. If $\\hat{y}_{t}\\neq y_{t}$ , then $a_{t}\\geq\\gamma_{\\mathrm{L}}/4$ . This is because the event $\\{\\hat{y}_{t}\\neq y_{t}\\}$ implies that $L^{2}(\\hat{p}_{t},q_{y_{t}}^{{\\mathbf x}_{t}})\\ge$ $L^{2}(\\hat{p}_{t},q_{1-y_{t}}^{{\\mathbf x}_{t}})$ . Hence, $L^{2}(\\hat{p}_{t},f_{h^{*}}(\\mathbf x_{t}))=L^{2}(\\hat{p}_{t},q_{y_{t}}^{\\mathbf x_{t}})\\geq\\gamma_{\\mathrm{L}}/4$ . Here, we used the following geometric fact: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{L^{2}(\\hat{p}_{t},q_{y_{t}}^{\\mathbf{x}_{t}})}\\ge\\sqrt{L^{2}(\\hat{p}_{t},q_{y_{t}}^{\\mathbf{x}_{t}})}+\\sqrt{L^{2}(\\hat{p}_{t},q_{1-y_{t}}^{\\mathbf{x}_{t}})}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\sqrt{L^{2}(q_{y_{t}}^{\\mathbf{x}_{t}},q_{1-y_{t}}^{\\mathbf{x}_{t}})}\\ge\\sqrt{\\gamma_{\\mathbf{L}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This implies that $\\begin{array}{r}{\\forall t\\in[T],\\ a_{t}\\geq\\frac{\\gamma_{\\mathrm{L}}}{4}1\\{\\hat{y}_{t}\\neq y_{t}\\}}\\end{array}$ , therefore: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}1\\{\\hat{y}_{t}\\neq y_{t}\\}\\le\\frac{4}{\\gamma_{\\mathrm{L}}}\\sum_{t=1}^{T}L^{2}(\\tilde{p}_{t},\\hat{p}_{t})-L^{2}(\\tilde{p}_{t},f_{h^{*}}(\\mathbf x_{t})).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The expected minimax risk now follows from (22). ", "page_idx": 17}, {"type": "text", "text": "G Tight Bounds for Kernel Sets of Size One ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this appendix, we establish an upper bound for the case when the kernel set size $|\\mathcal{Q}_{y}^{\\bf x}|=1$ for all $\\mathbf{x},y$ . This includes, for instance, the case when the parameter $\\eta_{t}$ is known in Example 1. ", "page_idx": 17}, {"type": "text", "text": "Theorem 6. Let $\\mathcal{H}\\subset\\mathcal{V}^{\\mathcal{X}}$ be any finite class and $\\kappa$ be any noisy kernel that is well-separated at scale $\\gamma_{\\mathrm{{H}}}\\ w.r.t.$ squared Hellinger distance such that $|\\mathcal{Q}_{y}^{\\bf x}|=1$ for all $\\mathbf{x},y.$ . Then the high probability minimax risk at confidence $\\delta>0$ is upper bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underline{{B}}^{\\delta}(\\mathcal{H},\\mathcal{K})\\leq O\\left(\\frac{\\log(|\\mathcal{H}|/\\delta)}{\\gamma_{\\mathrm{H}}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Our proof follows a similar path as in the proof of Theorem 5, but replacing the $L^{2}$ loss with log-loss. Specifically, for any $h\\in\\mathcal H$ , we define $\\bar{f_{h}}({\\bf x})=q_{h({\\bf x})}^{{\\bf x}}$ , where $q_{h(\\mathbf{x})}^{\\mathbf{x}}$ is the unique element in $\\mathcal{Q}_{h(\\mathbf{x})}^{\\mathbf{x}}$ . Denote $\\mathcal{F}=\\{f_{h}:h\\in\\mathcal{H}\\}$ . We run the EWA algorithm (Algorithm 2) over $\\mathcal{F}$ with $\\alpha=1$ and $\\ell$ being the log-loss [10], and produce an estimator $\\Hat{p}^{T}$ . The classifier is then given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{y}_{t}=\\arg\\operatorname*{min}_{y\\in\\mathcal{Y}}\\{H^{2}(q_{y}^{\\mathbf{x}_{t}},\\hat{p}_{t})\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, our key observation is that the noisy label distribution $\\tilde{p}_{t}=f_{h^{\\ast}}(\\mathbf x_{t})$ is well-specified (since $|\\mathcal{Q}_{y}^{\\mathbf{x}}|\\,=1$ , the only choice for $\\tilde{p}_{t}$ is $f_{h^{*}(\\mathbf{x}_{t})},$ , where $h^{*}$ is the ground truth classifier. Therefore, invoking [10, Lemma A.14], we find ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\sum_{t=1}^{T}H^{2}(\\tilde{p}_{t},\\hat{p}_{t})\\leq\\log|\\mathcal{F}|+2\\log(1/\\delta)\\right]\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We claim that $\\begin{array}{r}{1\\{\\hat{y}_{t}\\neq h^{*}(\\mathbf x_{t})\\}\\leq\\frac{4}{\\gamma_{\\mathrm{H}}}H^{2}(\\tilde{p}_{t},\\hat{p}_{t})}\\end{array}$ . Clearly, this automatically satisfies if $\\hat{y}_{t}=h^{*}({\\mathbf x}_{t})$ For $\\hat{y}_{t}\\neq h^{*}({\\mathbf x}_{t})$ , we have $H^{2}(q_{\\hat{y}_{t}}^{\\mathbf{x}_{t}},\\hat{p}_{t})\\,\\leq\\,H^{2}(q_{h^{\\ast}(\\mathbf{x}_{t})}^{\\mathbf{x}_{t}},\\hat{p}_{t})\\,=\\,H^{2}(\\tilde{p}_{t},\\hat{p}_{t})$ by definition of $\\hat{y}_{t}$ . This implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\nH^{2}(\\tilde{p},\\hat{p}_{t})\\geq\\frac{1}{4}H^{2}(q_{\\hat{y}_{t}}^{\\mathbf{x}_{t}},q_{h^{*}(\\mathbf{x}_{t})}^{\\mathbf{x}_{t}})\\geq\\frac{\\gamma_{\\mathbf{H}}}{4},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality follows by triangle inequality of Hellinger distance (the factor $\\frac{1}{4}$ comes from the conversion form squared Hellinger distance to Hellinger distance), and the second inequality follows by definition of $\\gamma_{\\mathbf{H}}$ . Therefore, we have w.p. $\\geq1-\\delta$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}1\\{\\hat{y}_{t}\\neq h^{*}(\\mathbf x_{t})\\}\\leq\\frac{4}{\\gamma_{\\mathrm{\\scriptscriptstyleH}}}(\\log|\\mathcal F|+2\\log(1/\\delta)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This completes the proof since $|{\\mathcal{H}}|\\geq|{\\mathcal{F}}|$ . ", "page_idx": 18}, {"type": "text", "text": "Observe that the key ingredient in the proof of Theorem 6 is the realizability of $\\tilde{p}_{t}$ by $f_{h^{*}}$ (i.e., well-specified) due to the property $|\\mathcal{Q}_{y}^{\\bf x}|=1$ , which does not hold for general kernels. ", "page_idx": 18}, {"type": "text", "text": "H Adaptive v.s. Oblivious Adversaries ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this appendix, we explain how the guarantees for oblivious adversaries can be extended to adaptive adversaries. This primarily follows from [5, Lemma 4.1], but needs careful adaptation to fti our needs. We consider the following abstract treatment: we assume that the adversary performs any operation $\\mathbb{Q}_{t}$ at time step $t$ and produces an action $\\mathbf{z}_{t}$ . For any randomized prediction rule $\\hat{y}^{T}$ , the adaptive risk can be expressed as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{Q}_{1}\\mathbb{E}_{\\hat{y}_{1}}\\cdot\\cdot\\cdot\\mathbb{Q}_{T}\\mathbb{E}_{\\hat{y}_{T}}\\left[\\sum_{t=1}^{T}\\ell(\\mathbf{z}_{t},\\hat{y}_{t})\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Assume now that the randomness of $\\hat{y}_{t}$ \u2019s is independent and that $\\hat{y}_{t}$ depends only on $\\mathbf{z}^{t}$ . We claim that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{Q}_{1}\\mathbb{E}_{\\hat{y}_{1}}\\cdot\\cdot\\cdot\\mathbb{Q}_{T}\\mathbb{E}_{\\hat{y}_{T}}\\left[\\sum_{t=1}^{T}\\ell(\\mathbf{z}_{t},\\hat{y}_{t})\\right]=\\mathbb{Q}^{T}\\mathbb{E}_{\\hat{y}^{T}}\\left[\\sum_{t=1}^{T}\\ell(\\mathbf{z}_{t},\\hat{y}_{t})\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We prove the case for $T=2$ to demonstrate the ideas; the general case follows by induction. Observe that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{Q}_{1}\\mathbb{E}_{\\hat{y}_{1}}\\mathbb{Q}_{2}\\mathbb{E}_{\\hat{y}_{2}}\\big[\\ell(\\mathbf{z}_{1},\\hat{y}_{1})+\\ell(\\mathbf{z}_{2},\\hat{y}_{2})\\big]\\stackrel{(a)}{=}\\mathbb{Q}_{1}\\mathbb{E}_{\\hat{y}_{1}}\\big[\\ell(\\mathbf{z}_{1},\\hat{y}_{1})+\\mathbb{Q}_{2}\\mathbb{E}_{\\hat{y}_{2}}\\ell(\\mathbf{z}_{2},\\hat{y}_{2})\\big]}\\\\ &{\\stackrel{(b)}{=}\\mathbb{Q}_{1}\\big[\\mathbb{E}_{\\hat{y}_{1}}[\\ell(\\mathbf{z}_{1},\\hat{y}_{1})]+\\mathbb{E}_{\\hat{y}_{1}}\\mathbb{Q}_{2}\\mathbb{E}_{\\hat{y}_{2}}\\ell(\\mathbf{z}_{2},\\hat{y}_{2})\\big]}\\\\ &{\\stackrel{(c)}{=}\\mathbb{Q}_{1}\\big[\\mathbb{E}_{\\hat{y}_{1}}[\\ell(\\mathbf{z}_{1},\\hat{y}_{1})]+\\mathbb{Q}_{2}\\mathbb{E}_{\\hat{y}_{2}}\\ell(\\mathbf{z}_{2},\\hat{y}_{2})\\big]}\\\\ &{\\stackrel{(d)}{=}\\mathbb{Q}_{1}\\mathbb{Q}_{2}\\big[\\mathbb{E}_{\\hat{y}_{1}}[\\ell(\\mathbf{z}_{1},\\hat{y}_{1})]+\\mathbb{E}_{\\hat{y}_{2}}[\\ell(\\mathbf{z}_{2},\\hat{y}_{2})]\\big]}\\\\ &{\\stackrel{(e)}{=}\\mathbb{Q}_{1}\\mathbb{Q}_{2}\\mathbb{E}_{\\hat{y}_{1}}\\mathbb{E}_{\\hat{y}_{2}}[\\ell(\\mathbf{z}_{1},\\hat{y}_{1})+\\ell(\\mathbf{z}_{2},\\hat{y}_{2})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(a)$ follows since $\\ell(\\mathbf{z}_{1},\\hat{y}_{1})$ is independent of $\\mathbb{Q}_{2}\\mathbb{E}_{\\hat{y}_{2}};(b)$ follows by the linearity of expectation; $(c)$ follows by the independence of $\\hat{y}_{1}$ and $\\hat{y}_{2}$ , since the term $\\mathbb{Q}_{2}\\mathbb{E}_{\\hat{y}_{2}}\\ell(\\mathbf{z}_{2},\\hat{y}_{2})$ has nothing to do with the realization of $\\hat{y}_{1};(d)$ follows since $\\mathbb{E}_{\\hat{y}_{1}}[\\ell(\\mathbf{z}_{1},\\hat{y}_{1})]$ is independent of $\\mathbf{z}_{2}$ ; $(e)$ follows by the linearity of expectation. ", "page_idx": 19}, {"type": "text", "text": "Observe that, all the predictors constructed in this paper have independent internal randomness (in fact, the only place where randomness is introduced is in Algorithm 1); thus, our derived risk bounds hold for adaptive adversaries as well. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The abstract and introduction is accurate to reflect the paper\u2019s contributions. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Limitation is discussed in the Discussion section ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The assumptions are accurate and the proofs are complete. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This is a pure theory paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This is a pure theory paper. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This is a pure theory paper. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This is a pure theory paper. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is a pure theory paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We confirm the paper meets the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is a pure theory paper, we do not see any direct societal impacts. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 23}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This is a pure theory paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This is a pure theory paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This is a pure theory paper. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This is a pure theory paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This is a pure theory paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]