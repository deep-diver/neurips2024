[{"figure_path": "4sueqIwb4o/figures/figures_5_1.jpg", "caption": "Illustrative explanation on the regularized projection. The Figure 1c implies that as \u03b7 \u2192 \u221e, \u03b3\u0393\u03b7 can potentially move outside of the unit ball satisfying ||x||\u221e < 1, and this phase is indicated with the term \u201cblowing up\u201d phase. The quantity ||\u03b3\u0393\u03b7||\u221e actually blows up initially as \u03b7 \u2192 \u221e. However, since limn\u2192\u221e ||\u03b3\u0393\u03b7 ||\u221e = 0, we know that \u03b3\u0393\u03b7 will eventually converge to the origin and move inside the unit ball. This behavior is indicated by the \u201cshrinking", "description": "The figure illustrates how the regularized projection operator (\u0393\u03b7) behaves as the regularization parameter (\u03b7) varies.  Panel (a) shows the standard projection, while (b) shows the effect of regularization.  As \u03b7 approaches infinity, the regularized projection shrinks to zero (the 'shrinking phase').  Panel (c) provides an illustrative example of this effect in a one-dimensional case, highlighting how the projection's bound changes with \u03b7. ", "section": "Projected Bellman equation"}, {"figure_path": "4sueqIwb4o/figures/figures_8_1.jpg", "caption": "Figure 2: Experiment results", "description": "The figure shows the experimental results of four different Q-learning algorithms: CoupledQ, GreedyGQ, TargetQ, and RegQ.  Two plots are presented. Plot (a) displays the results for the 0 \u2192 20 environment from Tsitsiklis and Van Roy [1996], while plot (b) shows the results for the Baird seven-star counter example from Baird [1995]. The y-axis represents the error ||\u03b8 - \u03b8*||, where \u03b8 is the estimated parameter vector and \u03b8* is the optimal parameter vector, and the x-axis shows the number of steps or iterations. The shaded area represents the standard deviation or error bars.  The results demonstrate that RegQ converges faster than the other algorithms in both environments.", "section": "6 Experiments"}, {"figure_path": "4sueqIwb4o/figures/figures_22_1.jpg", "caption": "Figure 3: State transition diagram", "description": "This figure depicts a Markov Decision Process (MDP) with three states (s=1, s=2, s=3) and two actions (a=1, a=2).  The transitions between states are represented by arrows, and the probabilities are implicitly defined in the paper.  The feature vectors x(s,a) associated with each state-action pair are also shown. This MDP is a simple example used in the paper to illustrate a situation where the standard projected Bellman equation may not have a solution, but a regularized version does.", "section": "A.14 Example for non-existence of solution of PBE"}, {"figure_path": "4sueqIwb4o/figures/figures_24_1.jpg", "caption": "Figure 4: Counter-examples where Q-learning with linear function approximation diverges", "description": "This figure shows two examples where standard Q-learning with linear function approximation fails to converge.  (a) illustrates a simple Markov Decision Process (MDP) with two states (0 and 20), where Q-learning is known to diverge.  (b) depicts the Baird seven-star counter example, a more complex MDP, also known for causing divergence in Q-learning with function approximation. These examples highlight the instability of Q-learning when combined with function approximation, a problem addressed by the proposed RegQ algorithm.", "section": "B.4 Diagrams for 0 \u2192 20 and Baird Seven Star Counter Example"}, {"figure_path": "4sueqIwb4o/figures/figures_25_1.jpg", "caption": "Figure 5: Learning curve under different learning rate and regularization coefficient", "description": "This figure shows the learning curves obtained with different learning rates (0.01 and 0.05) and regularization coefficients (\u03b7 \u2208 {2\u207b\u00b2, 2\u207b\u00b9, 1, 2}).  The x-axis represents the number of steps, and the y-axis represents the error ||\u03b8 - \u03b8*||. The curves show the convergence process of the RegQ algorithm in two different environments, namely the Tsitsiklis and Van Roy (0\u219220) environment and the Baird counter-example. The results demonstrate that the convergence rate improves as the regularization coefficient increases.", "section": "Experiments"}, {"figure_path": "4sueqIwb4o/figures/figures_26_1.jpg", "caption": "Figure 6: O.D.E. results", "description": "This figure shows the trajectories of the upper and lower systems for the regularized Q-learning algorithm. The trajectories of the original system are bounded by the trajectories of the upper and lower systems, illustrating the theoretical analysis of the algorithm's convergence.", "section": "B.7 O.D.E. experiment"}]