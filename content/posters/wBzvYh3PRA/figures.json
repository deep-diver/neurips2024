[{"figure_path": "wBzvYh3PRA/figures/figures_0_1.jpg", "caption": "Figure 1: Overview of FACTORSIM. FACTORSIM takes language documentation as input, uses Chain-of-Thought to derive a series of steps to be implemented, adopts a Factored POMDP representation to facilitate efficient context selection during each generation step, trains agents on the generated simulations, and tests the resulting policy on previously unseen RL environments.", "description": "This figure provides a high-level overview of the FACTORSIM framework. It shows how the system takes language documentation as input, uses chain-of-thought reasoning to break down the task into smaller steps, utilizes a factored POMDP representation for efficient context management, trains reinforcement learning agents on the generated simulations, and finally evaluates the performance of the trained agents on unseen environments.  The diagram visually depicts the flow of information and processes within the FACTORSIM pipeline, highlighting key components and their interactions.", "section": "1 Introduction"}, {"figure_path": "wBzvYh3PRA/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of FACTORSIM. FACTORSIM takes language documentation as input, uses Chain-of-Thought to derive a series of steps to be implemented, adopts a Factored POMDP representation to facilitate efficient context selection during each generation step, trains agents on the generated simulations, and tests the resulting policy on previously unseen RL environments.", "description": "This figure illustrates the overall process of FACTORSIM.  It starts with language documentation as input, which is processed using Chain-of-Thought reasoning to break down the task into smaller, manageable steps.  Each step involves selecting relevant context from a factored Partially Observable Markov Decision Process (POMDP) representation, simplifying the generation process. FACTORSIM then uses this information to generate code for the simulation.  The generated simulations are then used to train reinforcement learning (RL) agents, which are subsequently tested on unseen environments to evaluate the zero-shot transfer capabilities of the generated simulations.", "section": "1 Introduction"}, {"figure_path": "wBzvYh3PRA/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of FACTORSIM. FACTORSIM takes language documentation as input, uses Chain-of-Thought to derive a series of steps to be implemented, adopts a Factored POMDP representation to facilitate efficient context selection during each generation step, trains agents on the generated simulations, and tests the resulting policy on previously unseen RL environments.", "description": "This figure illustrates the overall process of FACTORSIM. It starts with language documentation as input, uses Chain-of-Thought reasoning to break down the task into smaller steps.  A factored POMDP representation is used to manage the complexity of context selection during simulation generation.  The generated simulation code is then used to train reinforcement learning agents. Finally, the trained policy is evaluated on unseen environments to assess the generalization ability of the generated simulations.", "section": "1 Introduction"}, {"figure_path": "wBzvYh3PRA/figures/figures_4_1.jpg", "caption": "Figure 2: An illustrative example of how the five main prompts in FactorSim correspond to our formulation in Algorithm 1. Note that the function red_puck_respawn is retrieved as part of the context to Prompt 3, 4, and 5 because it modifies the state variable red_puck_position, a state variable LLM identified as relevant in prompt 2.", "description": "This figure illustrates how the five main prompts in the FACTORSIM framework correspond to the steps of Algorithm 1.  It demonstrates the process of decomposing a complex task into smaller, manageable modules. The figure shows how relevant states and functions are identified and utilized in each step to generate new code based on the factored POMDP representation.  It highlights the iterative nature of the process and how the selection of context from the current codebase helps reduce the complexity for the LLM at each step.  Specifically, it showcases how the 'red_puck_respawn' function, identified in Prompt 2 as modifying the relevant 'red_puck_position' state variable, is reused as part of the context for Prompts 3, 4, and 5, to efficiently update the code.", "section": "3 FACTORSIM: Generating Simulations via Factorized Representation"}, {"figure_path": "wBzvYh3PRA/figures/figures_7_1.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure presents a comparison of different methods using GPT-4 for generating reinforcement learning games.  It shows the relationship between the percentage of successfully passed system tests (a measure of code correctness) and the number of tokens used by each method.  The methods compared include vanilla GPT-4, GPT-4 with self-debugging, GPT-4 with Chain-of-Thought and self-debugging, and GPT-4 with FactorSim.  Ellipses represent 90% confidence intervals, indicating the variability of performance across different games.  The results show FactorSim's superior performance and efficiency in generating correct code compared to other baselines.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_8_1.jpg", "caption": "Figure 4: Zero-shot transfer results on previously unseen environments (i.e., environments in the original RL benchmark [33]).", "description": "This figure displays the results of a zero-shot transfer experiment.  Three different methods of generating simulations (GPT-4 with self-debugging, GPT-4 CoT with self-debugging, and GPT-4 with FactorSim) were used to train reinforcement learning agents. The agents were then tested on unseen environments from the original RL benchmark (PyGame Learning Environment). The normalized reward is plotted for each game in the benchmark, with error bars indicating the variability of performance.  The figure shows that FactorSim significantly outperforms the baseline methods in the zero-shot transfer setting, indicating the generated simulations better generalize to unseen environments.", "section": "Zero-shot Transfer Results"}, {"figure_path": "wBzvYh3PRA/figures/figures_8_2.jpg", "caption": "Figure 6: Left: an overview of our robotics task generation experimental setting. Right: Tasks successfully generated using FactorSim, which all other baselines fail on.", "description": "This figure provides a visual representation of the robotics task generation experimental setting. The left side illustrates the process, which involves taking a task description as input, verifying syntax and runtime feasibility, checking for task completion using an oracle agent, and finally, conducting a human verification to ensure that the generated demonstration aligns with the original prompt.  The right side showcases examples of tasks successfully generated by the FACTORSIM model, tasks that other baseline methods were unable to generate successfully, highlighting the advanced capabilities of the proposed model.", "section": "4.2 Robotics Task Generation"}, {"figure_path": "wBzvYh3PRA/figures/figures_8_3.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure compares the performance (percentage of system tests passed) and token usage of different methods for generating 2D RL games using the GPT-4 language model.  The methods compared include the vanilla GPT-4 approach, GPT-4 with self-debugging, GPT-4 with Chain-of-Thought reasoning and self-debugging, and GPT-4 combined with the FACTORSIM framework.  The ellipses represent 90% confidence intervals for each method's performance across all eight games in the benchmark.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_9_1.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure compares the performance (percentage of system tests passed) and token usage of different GPT-4 based methods for generating 2D reinforcement learning games.  The methods include the vanilla approach (no decomposition), self-debugging, Chain-of-Thought reasoning with self-debugging, and FactorSim.  The ellipses represent 90% confidence intervals, showing the variability in performance and token usage across different games. The figure highlights that FactorSim achieves the best balance between high accuracy and moderate token usage.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_9_2.jpg", "caption": "Figure 6: Left: an overview of our robotics task generation experimental setting. Right: Tasks successfully generated using FactorSim, which all other baselines fail on.", "description": "The figure showcases the experimental setup for robotics task generation using the proposed FACTORSIM method.  The left side provides a flowchart illustrating the process: a task is specified in natural language, decomposed into subtasks, and each subtask is generated into code using FACTORISM. The code undergoes syntax correctness and runtime verification checks before being used to train a robot to complete the task. The right side presents examples of successful task generations using FACTORSIM, demonstrating tasks that other baseline methods failed to produce correctly.", "section": "4.2 Robotics Task Generation"}, {"figure_path": "wBzvYh3PRA/figures/figures_27_1.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure shows the performance (percentage of system tests passed) and token usage of different methods for generating 2D RL games using the GPT-4 language model.  It compares the vanilla approach (no additional techniques), self-debugging, Chain-of-Thought with self-debugging, and FactorSim.  The ellipses represent the 90% confidence intervals, indicating the variability of the results across the eight different RL games. FactorSim demonstrates the best balance of high performance and relatively low token usage, highlighting its efficiency.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_27_2.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure compares the performance (percentage of system tests passed) and token usage of different GPT-4 based methods for generating 2D reinforcement learning games.  The methods include the vanilla method (no special techniques), self-debugging, Chain-of-Thought reasoning with self-debugging, and FactorSim.  The ellipses represent 90% confidence intervals, showing the variability in performance and token usage across the eight games tested. The results indicate that FACTORSim achieves the best balance of high accuracy and relatively low token usage.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_27_3.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure compares the performance (percentage of passed system tests) and token usage of different methods for generating 2D RL games using the GPT-4 language model.  The methods include a vanilla approach, self-debugging, Chain-of-Thought reasoning with self-debugging, and the proposed FACTORSIM approach.  The ellipses represent 90% confidence intervals, showing the variability in performance and token usage across different games. FACTORSIM is shown to achieve high accuracy with modest token usage, indicating its efficiency in generating simulations.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_27_4.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure compares the performance (percentage of passed system tests) and token usage of various methods using GPT-4 for generating 2D RL games.  The vanilla method uses the fewest tokens but achieves moderate accuracy. Combining Chain-of-Thought (CoT) reasoning with self-debugging results in the highest token usage but only marginally improves accuracy.  FACTORSIM achieves the highest accuracy with modest token usage, suggesting that task decomposition reduces the need for extensive debugging.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_27_5.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure compares the performance (percentage of passed system tests) and token usage of different methods for generating 2D RL games using GPT-4.  It shows that while the vanilla method uses the fewest tokens, it has lower accuracy.  Combining Chain-of-Thought (CoT) reasoning with self-debugging results in the highest token usage, but only marginally improves accuracy.  In contrast, FACTORSIM achieves the highest accuracy with modest token usage, suggesting that decomposition of tasks reduces the need for extensive iterative debugging.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_27_6.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure shows the performance (percentage of system tests passed) and token usage of different methods for generating 2D RL games using GPT-4.  It compares the vanilla GPT-4 approach, GPT-4 with self-debugging, GPT-4 with Chain-of-Thought (CoT) and self-debugging, and GPT-4 with the proposed FACTORSIM method.  The ellipses represent 90% confidence intervals, indicating the variability of the results across different games.  FACTORSIM demonstrates a good balance between high accuracy and low token usage, suggesting its efficiency in generating simulation code.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_27_7.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure compares the performance (in terms of percentage of passed system tests) and token usage of different methods for generating 2D RL games using the GPT-4 language model.  The methods compared include the vanilla approach (no decomposition), self-debugging, Chain-of-Thought (CoT) with self-debugging, and FactorSim.  Ellipses around each data point represent the 90% confidence intervals, showing the variability in performance across different games for each method.  It demonstrates FactorSim's superior performance with modest token usage compared to other methods.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_27_8.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure compares the performance (percentage of passed system tests) and token usage of different methods for generating 2D RL games using GPT-4.  It shows that while the vanilla method uses the fewest tokens, it has lower accuracy.  Combining Chain-of-Thought (CoT) reasoning with self-debugging increases both token usage and accuracy, but FACTORSIM achieves the best balance of high accuracy and relatively low token usage.  The ellipses represent 90% confidence intervals, averaged across all eight games in the benchmark.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_27_9.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure compares the performance (percentage of system tests passed) and token usage of various GPT-4-based methods for generating 2D reinforcement learning games.  It shows that FACTORSIM achieves high accuracy with relatively low token usage compared to other methods, including those using Chain-of-Thought reasoning and self-debugging. The ellipses represent 90% confidence intervals, indicating the variability in performance and token usage across the different games.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_27_10.jpg", "caption": "Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.", "description": "This figure compares the performance (percentage of passed system tests) and token usage of different methods for generating 2D RL games using GPT-4.  It shows that while the vanilla method uses the fewest tokens, it achieves lower accuracy.  Combining Chain-of-Thought (CoT) reasoning with self-debugging results in the highest token usage but only marginally improves accuracy.  FACTORSIM achieves the highest accuracy with modest token usage, suggesting that its decomposition of tasks reduces the need for extensive iterative debugging.", "section": "Code Generation Evaluation"}, {"figure_path": "wBzvYh3PRA/figures/figures_28_1.jpg", "caption": "Figure 10: Human study interface screenshot.", "description": "The figure shows a screenshot of the user interface used in the human study. The interface allows users to select a game, generate a game using the FACTORSIM model, and replay the game. The generated game is displayed in a separate window, showing the game's score and the current state of the game. The prompt used to generate the game is displayed above the game window.", "section": "4.2 Robotics Task Generation"}]