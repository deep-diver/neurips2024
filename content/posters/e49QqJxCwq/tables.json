[{"figure_path": "e49QqJxCwq/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparisons of our PLIP with some other SoTA pre-trained models on downstream tasks. The table is divided into two parts: the upper part covers some general-domain pre-trained models and the lower part focuses on some person-domain pre-trained models. The baseline downstream methods for the five tasks are CMPM/C [112], ABDNet [9], Rethinking [41], SeqNet [54] and SCHP [50], respectively. Due to the inability of certain non-hierarchical models to be applied to some downstream methods, their performance results are indicated as \"-\".", "description": "This table compares the performance of the proposed PLIP model with other state-of-the-art (SoTA) pre-trained models on five downstream person-centric tasks: text-based person re-identification, image-based person re-identification, attribute recognition, person search, and human parsing.  The table is divided into two sections: one for general-domain pre-trained models and another for person-domain pre-trained models.  Baseline methods for each task are listed along with the results.  The '-' indicates that some methods are not applicable to all tasks.", "section": "4.1 Comparison With Other Pre-trained Models"}, {"figure_path": "e49QqJxCwq/tables/tables_6_2.jpg", "caption": "Table 2: The results of our transfer experiments. We show the best score in bold. z-s: zero-shot setting; l-p: linear-probing setting; f-t: fine-tune setting. \u2021 stands for the results reproduced with public checkpoints released by the authors.", "description": "This table presents the results of transfer learning experiments for text-based person re-identification using different settings (zero-shot, linear probing, and fine-tuning).  The table compares the performance of the proposed model (PLIP) against state-of-the-art methods (VITAA, SSAN, LapsCore, TIPCB, LGUR) on two benchmark datasets (CUHK-PEDES and ICFG-PEDES). The performance metrics used are Recall@1, Recall@5, and Recall@10.", "section": "4.2 Evaluation on Text-based Person Re-ID"}, {"figure_path": "e49QqJxCwq/tables/tables_6_3.jpg", "caption": "Table 3: Comparison on domain generalization. \u201cC\u201d and \u201cI\u201d denote CUHK-PEDES and ICFG-PEDES, respectively.", "description": "This table presents the results of a domain generalization experiment in text-based person re-identification.  Two datasets, CUHK-PEDES (C) and ICFG-PEDES (I), were used. The experiment evaluates the performance of different models when transferring knowledge from one dataset to the other.  The results show the Rank-1, Rank-5, and Rank-10 accuracy rates for each model in both cross-domain settings (C\u2192I and I\u2192C). The table highlights the performance of the proposed PLIP model compared to several existing state-of-the-art models.", "section": "4.2 Evaluation on Text-based Person Re-ID"}, {"figure_path": "e49QqJxCwq/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparisons of our PLIP with some other SoTA pre-trained models on downstream tasks. The table is divided into two parts: the upper part covers some general-domain pre-trained models and the lower part focuses on some person-domain pre-trained models. The baseline downstream methods for the five tasks are CMPM/C [112], ABDNet [9], Rethinking [41], SeqNet [54] and SCHP [50], respectively. Due to the inability of certain non-hierarchical models to be applied to some downstream methods, their performance results are indicated as \"-\".", "description": "This table compares the performance of the proposed PLIP model against other state-of-the-art (SoTA) pre-trained models on five downstream person-centric tasks: image-based Re-ID, text-based Re-ID, attribute recognition, person search, and human parsing.  The table is divided into two sections: the first showing general-domain pre-trained models and the second showing person-domain pre-trained models. Baseline methods for each task are also included, providing context for comparing the performance improvements achieved by using different pre-trained models.  Note that some models' results are marked with a \"-\" due to incompatibility with certain downstream methods.", "section": "4.1 Comparison With Other Pre-trained Models"}, {"figure_path": "e49QqJxCwq/tables/tables_7_2.jpg", "caption": "Table 5: Comparison with the state-of-the-art methods on text-based person Re-ID. We show the best score in bold.", "description": "This table compares the performance of the proposed PLIP model against other state-of-the-art (SOTA) methods on the task of text-based person re-identification.  It evaluates the models on two datasets: CUHK-PEDES and ICFG-PEDES. The table shows the results for various performance metrics including rank-1, rank-5, and rank-10 accuracy.  The results highlight the superior performance of PLIP compared to existing methods.", "section": "4.2 Evaluation on Text-based Person Re-ID"}, {"figure_path": "e49QqJxCwq/tables/tables_7_3.jpg", "caption": "Table 7: Comparison with SoTA methods on fully supervised image-based person Re-ID. The best results are shown in bold.", "description": "This table compares the performance of the proposed PLIP model against other state-of-the-art (SoTA) methods on two widely used datasets for image-based person re-identification: Market1501 and DukeMTMC.  The results showcase the improvements achieved by PLIP on both datasets.  Note that results from post-processing techniques (like re-ranking) are excluded to maintain a fair comparison.", "section": "4.3 Evaluation on Image-based Re-ID"}, {"figure_path": "e49QqJxCwq/tables/tables_7_4.jpg", "caption": "Table 7: Comparison with SoTA methods on fully supervised image-based person Re-ID. The best results are shown in bold.", "description": "This table compares the performance of the proposed PLIP model against other state-of-the-art (SoTA) methods on two benchmark datasets for image-based person re-identification: Market1501 and DukeMTMC.  The table shows the mAP (mean Average Precision) and Rank-1 accuracy for each method, broken down by backbone architecture (e.g., ResNet50, ViT-B, Swin-B) and pre-training method (IMG, LUP, LUP-NL, SYNTH). The results demonstrate that PLIP, particularly with Swin-B backbone and SYNTH pre-training, significantly outperforms SoTA methods.", "section": "4.3 Evaluation on Image-based Re-ID"}, {"figure_path": "e49QqJxCwq/tables/tables_8_1.jpg", "caption": "Table 5: Comparison with the state-of-the-art methods on text-based person Re-ID. We show the best score in bold.", "description": "This table compares the performance of the proposed PLIP model with other state-of-the-art (SoTA) methods on text-based person re-identification.  The comparison includes both methods that rely on multi-modal pre-trained models and methods that do not.  The results are presented for different metrics (R@1, R@5, R@10) on multiple datasets (CUHK-PEDES and ICFG-PEDES). The table highlights that PLIP consistently outperforms other methods, especially those not using multi-modal pre-trained models.", "section": "4.2 Evaluation on Text-based Person Re-ID"}, {"figure_path": "e49QqJxCwq/tables/tables_8_2.jpg", "caption": "Table 9: Comparison with state-of-the-art methods on the end-to-end person search. We show the best score in bold.", "description": "This table compares the performance of the proposed PLIP model against other state-of-the-art (SoTA) methods on two person search benchmark datasets: CUHK-SYSU and PRW.  The results are shown for two metrics: mean Average Precision (mAP) and Rank@1.  PLIP consistently outperforms prior SoTA methods, demonstrating its effectiveness for person search tasks.", "section": "4.4 Evaluation on Person Search"}, {"figure_path": "e49QqJxCwq/tables/tables_8_3.jpg", "caption": "Table 13: Ablation study on the impact of each pretext task, all using default settings.", "description": "This ablation study investigates the individual and combined effects of the three pretext tasks (IVLC, TIC, and IAP) on the performance of the PLIP model.  It shows the zero-shot performance (R@1, R@5, R@10) on the CUHK-PEDES and Market1501 datasets for different combinations of the pretext tasks, demonstrating their individual and combined contributions to the overall model performance.", "section": "A.9 Ablation studies and analyses"}, {"figure_path": "e49QqJxCwq/tables/tables_20_1.jpg", "caption": "Table 1: Performance comparisons of our PLIP with some other SoTA pre-trained models on downstream tasks. The table is divided into two parts: the upper part covers some general-domain pre-trained models and the lower part focuses on some person-domain pre-trained models. The baseline downstream methods for the five tasks are CMPM/C [112], ABDNet [9], Rethinking [41], SeqNet [54] and SCHP [50], respectively. Due to the inability of certain non-hierarchical models to be applied to some downstream methods, their performance results are indicated as \"-\".", "description": "This table compares the performance of the proposed PLIP model with other state-of-the-art (SoTA) pre-trained models on five downstream person-centric tasks: image-based Re-ID, text-based Re-ID, attribute recognition, person search, and human parsing.  It's divided into two sections: general-domain pre-trained models and person-domain pre-trained models.  The baseline methods used for each task are also specified.  Some models are not applicable to all tasks due to their hierarchical or non-hierarchical nature, indicated by a \"-\" in the table.", "section": "4.1 Comparison With Other Pre-trained Models"}, {"figure_path": "e49QqJxCwq/tables/tables_22_1.jpg", "caption": "Table 11: Statistics comparison on existing popular datasets. SYNTH-PEDES is by far the largest person dataset with textual descriptions without any human annotation effort.", "description": "This table compares the statistics of several existing person re-identification datasets, including the number of images, identities, descriptions, camera type, label type, labeling method, and crop size.  It highlights that the proposed SYNTH-PEDES dataset is significantly larger than other datasets, containing a much greater number of images, identities, and textual descriptions, while not requiring manual annotation.", "section": "3 SYNTH-PEDES: A Large-scale Image-text Person Dataset"}, {"figure_path": "e49QqJxCwq/tables/tables_23_1.jpg", "caption": "Table 1: Performance comparisons of our PLIP with some other SoTA pre-trained models on downstream tasks. The table is divided into two parts: the upper part covers some general-domain pre-trained models and the lower part focuses on some person-domain pre-trained models. The baseline downstream methods for the five tasks are CMPM/C [112], ABDNet [9], Rethinking [41], SeqNet [54] and SCHP [50], respectively. Due to the inability of certain non-hierarchical models to be applied to some downstream methods, their performance results are indicated as \"-\".", "description": "This table compares the performance of the proposed PLIP model with other state-of-the-art (SoTA) pre-trained models on five downstream person-centric tasks: image-based Re-ID, text-based Re-ID, attribute recognition, person search, and human parsing.  The table is divided into two sections: general-domain pre-trained models and person-domain pre-trained models.  For each task, baseline methods are listed, and the performance (e.g., mAP, Rank-1) of each pre-trained model is shown.  Some baselines are not applicable to all models, resulting in \"-\" entries.", "section": "4.1 Comparison With Other Pre-trained Models"}, {"figure_path": "e49QqJxCwq/tables/tables_25_1.jpg", "caption": "Table 13: Ablation study on the impact of each pretext task, all using default settings.", "description": "This table presents the ablation study results on the impact of each pretext task (IVLC, TIC, IAP) in the proposed PLIP framework.  It shows the performance (R@1, R@5, R@10) on two person re-identification benchmarks, CUHK-PEDES and Market1501, under different combinations of the pretext tasks. Row 1 shows results with only IVLC; Row 2 adds TIC; Row 3 adds IAP. Row 4 shows only using TIC and IAP, and Row 5 uses all three pretext tasks. This demonstrates the contribution of each component to the overall performance.", "section": "4.5 Ablation studies and analyses"}, {"figure_path": "e49QqJxCwq/tables/tables_26_1.jpg", "caption": "Table 14: Manual evaluation results. Our SYNTH-PEDES dataset is competitive with manual datasets and surpasses an existing synthetic dataset by a large margin.", "description": "This table presents the results of a manual evaluation comparing the quality of the SYNTH-PEDES dataset to three other manually annotated datasets (CUHK-PEDES, ICFG-PEDES, RSTPReid) and one synthetic dataset (FineGPR-C).  The evaluation used a five-point scale, assessing aspects like correctness and detail in the textual descriptions of images. SYNTH-PEDES demonstrates comparable quality to the manually annotated datasets, despite its significantly larger size, and outperforms the synthetic dataset.", "section": "A.9.2 Dataset Quality Evaluation"}, {"figure_path": "e49QqJxCwq/tables/tables_27_1.jpg", "caption": "Table 16: Ablation studies on training objectives in (a), prediction difficulty functions in (b) and pooling methods in (c).", "description": "This ablation study analyzes the impact of different choices in the training process of the PLIP model.  Specifically, it investigates the effect of different training objectives for the IVLC and TIC pretext tasks, explores the influence of varying prediction difficulty levels (controlled by a function G(x)), and examines the impact of different pooling methods for combining visual and textual features.  The results show the relative importance of each component and help to optimize the model's performance.", "section": "A.9 Ablation studies and analyses"}, {"figure_path": "e49QqJxCwq/tables/tables_28_1.jpg", "caption": "Table 17: Comparable results of different hyper-parameters in overall objective function. We show the best score in bold.", "description": "This table presents the results of experiments conducted to find the optimal hyperparameters for the overall objective function used in the PLIP model.  The objective function is a combination of three loss functions (IVLC, TIC, and IAP), and the table shows the performance (R@1, R@5, R@10) on the CUHK-PEDES and Market1501 datasets for various combinations of the hyperparameters \u03bb\u2081 and \u03bb\u2082.  The results demonstrate the impact of different weighting schemes on the overall performance of the model, indicating that a particular combination yields the best performance.", "section": "4.5 Ablation studies and analyses"}, {"figure_path": "e49QqJxCwq/tables/tables_28_2.jpg", "caption": "Table 1: Performance comparisons of our PLIP with some other SoTA pre-trained models on downstream tasks. The table is divided into two parts: the upper part covers general-domain pre-trained models and the lower part focuses on person-domain pre-trained models. The baseline downstream methods for the five tasks are CMPM/C [112], ABDNet [9], Rethinking [41], SeqNet [54] and SCHP [50], respectively. Due to the inability of certain non-hierarchical models to be applied to some downstream methods, their performance results are indicated as \"-\".", "description": "This table compares the performance of the PLIP pre-trained model against other state-of-the-art (SoTA) pre-trained models on five downstream person-centric tasks: image-based re-identification, text-based re-identification, attribute recognition, person search, and human parsing.  The table is divided into two sections: the first compares general-domain pre-trained models, and the second compares person-domain pre-trained models.  Baseline methods for each task are listed, and the performance (metrics vary by task) of each model using different backbones (e.g., ResNet50, ResNet101, etc.) is shown.  Dashes indicate when a model could not be applied to a particular task.", "section": "4.1 Comparison With Other Pre-trained Models"}, {"figure_path": "e49QqJxCwq/tables/tables_28_3.jpg", "caption": "Table 3: Comparison on domain generalization. \u201cC\u201d and \u201cI\u201d denote CUHK-PEDES and ICFG-PEDES, respectively.", "description": "This table presents the results of domain generalization experiments for text-based person re-identification.  The model's performance is evaluated on two datasets, CUHK-PEDES (C) and ICFG-PEDES (I), with each model tested on both datasets as the target domain, given a source dataset for training. This assesses the model's ability to generalize to unseen domains. The metrics used to evaluate performance are R@1, R@5, and R@10.", "section": "4.2 Evaluation on Text-based Person Re-ID"}, {"figure_path": "e49QqJxCwq/tables/tables_29_1.jpg", "caption": "Table 20: Improving two person attribute recognition baseline methods. The results of DeepMAR\u2021 are from a re-implementation by replacing the backbone with ResNet50, which are much better than the original.", "description": "This table compares the performance of different pre-trained models (MoCov2, CLIP, LUP, LUP-NL, and PLIP) when used with two baseline person attribute recognition methods (Rethink and DeepMAR).  It shows the improvement in mean accuracy (mA), accuracy (Acc), recall (Rec), and F1-score for both the PA100k and PETA datasets.  The results demonstrate that the PLIP pre-trained model significantly improves the performance of both baseline methods.", "section": "4.4 Evaluation on Person Attribute Recognition"}, {"figure_path": "e49QqJxCwq/tables/tables_29_2.jpg", "caption": "Table 21: Comparison with SoTA methods on human parsing. We show the best score in bold.", "description": "This table compares the performance of the proposed PLIP model against other state-of-the-art (SoTA) methods on the task of human parsing.  Two datasets are used for evaluation: LIP and PASCAL-Person-Part.  The table shows the mean Intersection over Union (mIoU) metric, a common evaluation metric for semantic segmentation tasks like human parsing.  Different backbone networks are used for the models.  The results demonstrate that PLIP achieves competitive or superior performance compared to SoTA methods.", "section": "4.5 Evaluation on Human Parsing"}]