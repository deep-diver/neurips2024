[{"type": "text", "text": "Improved Regret of Linear Ensemble Sampling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Harin Lee   \nSeoul National University   \nSeoul, South Korea   \nharinboy@snu.ac.kr   \nMin-hwan Oh   \nSeoul National University   \nSeoul, South Korea   \nminoh@snu.ac.kr ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we close the fundamental gap of theory and practice by providing an improved regret bound for linear ensemble sampling. We prove that with an ensemble size logarithmic in $T$ , linear ensemble sampling can achieve a frequentist regret bound of $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ , matching state-of-the-art results for randomized linear bandit algorithms, where $d$ and $T$ are the dimension of the parameter and the time horizon respectively. Our approach introduces a general regret analysis framework for linear bandit algorithms. Additionally, we reveal a significant relationship between linear ensemble sampling and Linear Perturbed-History Exploration (LinPHE), showing that LinPHE is a special case of linear ensemble sampling when the ensemble size equals $T$ . This insight allows us to derive a new regret bound of $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ for LinPHE, independent of the number of arms. Our contributions ad vance the theoretical foundation of ensemble sampling, bringing its regret bounds in line with the best known bounds for other randomized exploration algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ensemble sampling [16] has emerged as an empirically effective randomized exploration technique in various online decision-making problems, such as online recommendation [17, 27, 26] and deep reinforcement learning [18\u201320]. Despite its popularity, the theoretical understanding of ensemble sampling has lagged behind, even for the linear bandit problem, with previous results revealing suboptimal outc\u221aomes. For instance, a prior work [21] demonstrated that linear ensemble sampling could achieve $\\mathcal{O}(\\sqrt{T})$ Bayesian regret with an ensemble size growing at least linearly with $T$ . However, the requirement for the ensemble size to be linear in $T$ is highly unfavorable and prohibitive in many practical settings. A recent work [10] showed that a symmetrized version of linear ensemble sampling could provide an improv\u221aement in dependence on ensemble size of $\\Theta(d\\log T)$ and show a frequentist regret bound of $\\widetilde{\\mathcal{O}}(\\bar{d}^{5/2}\\sqrt{T})$ . However, this regret bound clearly falls short of the existing frequentist regret achieved by standard randomized algorithms such as Thompson Sampling (TS) [4, 2] and Perturbed-History Exploration (PHE) [11\u201313].1 ", "page_idx": 0}, {"type": "text", "text": "In this work, we close this fundamental gap by providing an improved regret bound for linear ensemble sampling. We prove that linear ensemble sampling with an ensemble size logarithmic in $T$ can still attain a frequentist regret bound of $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ , marking the first time that linear ensemble sampling achieves a state-of-the-art result for randomized linear bandit algorithms. Our approach not only improves upon the regret bound but also simplifies the algorithm by avoiding the use of symmetrized perturbations, making it more practical for implementation. For regret analysis, we present a general, concise framework for analyzing linear bandit algorithms, which may be of independent interest. Furthermore, we rigorously reveal the significant relationship between ensemble sampling and PHE for the first time, showing that in the regime where the ensemble size equals $T$ , linear PHE (LinPHE) is a special case of linear ensemble sampling. With this new insight, we can use the regret analysis for ensemble sampling to derive a new regret bound of $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ for LinPHE, which is independent of the number of arms $K$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We prove a $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ regret bound (Theorem 1) for linear ensemble sampling with an ensemble size of $m=\\Omega(K\\log T)$ , where $K$ denotes the number of arms. Importantly, our regret bo\u221aund does not depend on $K$ or $m$ even logarithmically. Our result is the first to establish $\\bar{\\tilde{\\mathcal{O}}}(d^{3/2}{\\sqrt{T}})$ regret for linear ensemble sampling with an ensemble size sublinear in $T$ , improving the previous bound by the factor $d$ while maintaining the ensemble size to be logarithmic in $T$ . \u2022 As part of the regret analysis, we present a general regret analysis framework (Theorem 2) for linear bandit algorithms. This framework not only generalizes the regret analysis of randomized algorithms such as ensemble sampling and PHE but also applies to other optimism-based deterministic algorithms. This result can be of independent interest beyond ensemble sampling. \u2022 We rigorously investigate the relationship between linear ensemble sampling and LinPHE. We show that in the regime of ensemble size $m=T$ , LinPHE is a special case of the linear ensemble sampling algorithm. To our best knowledge, this is the first result to show the equivalence between linear ensemble sampling and LinPHE. \u2022 As a byproduct, with this new insight into the relationship between linear ensemble sampling and LinPHE, we provide an alternate analys\u221ais for LinPHE as an extension of the analysis for linear ensemble sampling, achieving a $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ regret bound with no dependence on $K$ . ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The stochastic linear bandit problem [5, 1, 14] is a foundational sequential decision-making problem and a core model for multi-armed bandits with features. Numerous algorithms have been developed for this problem, including deterministic approaches such as UCB-based methods [5, 8, 1] and randomized algorithms such as Thompson sampling [24, 7, 4, 2] and PHE [11\u201313]. ", "page_idx": 1}, {"type": "text", "text": "Thompson sampling [24], a classical randomized method, utilizes the posterior distribution of hidden parameters based on observed data. Initially proposed for Bayesian settings [22, 23], it has also demonstrated strong performance in frequentist settings [3, 4, 2]. For the stochastic linear bandit, Agrawal and Goy\u221aal [4] showed that Thompson sampling with a Gaussian prior achieves a regret bound of $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ , which can be reduced to $\\widetilde{\\mathcal{O}}(d\\sqrt{T\\log K})$ for small $K$ . However, applying Thompso n sampling to more complex problems  remains challenging, especially when posterior computation becomes intractable, though approximate methods have been proposed [16, 25]. ", "page_idx": 1}, {"type": "text", "text": "PHE [11\u201313] is another class of randomized algorithms that does not rely on posterior distributions, making it potentially applicable to more complex settings. In the finite-armed linear bandit model, PHE achieves a $\\widetilde{\\mathcal{O}}(d\\sqrt{T\\log K})$ regret bound, matching the performance of Thompson sampling for finite arms [4].  However, the relationship between PHE and ensemble sampling remains unexplored in previous studies. ", "page_idx": 1}, {"type": "text", "text": "Ensemble sampling [16] has gained popularity as a randomized exploration method across various decision-making tasks [17, 27, 26, 18\u201320]. Despite its empirical success, its theoretical foundation, particularly for linear bandits, is \u221astill relatively underdeveloped. Qin et al. [21] showed that linear ensemble sampling achieves $\\mathcal{O}(\\sqrt{T})$ Bayesian regret but requires an impractically large ensemble size that scales linearly with $T$ . More recently, Janz et al. [10] reduced\u221a the dependence on ensemble size to $\\Theta(d\\log T)$ and achieved a frequentist regret bound of $\\widetilde{\\mathcal{O}}(d^{5/2}\\sqrt{T})$ . However, the frequentist regret bound of $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ for ensemble sampling has yet to be achieved. ", "page_idx": 1}, {"type": "text", "text": "Input $:$ regularization parameter $\\lambda>0$ , ensemble size $m\\in\\mathbb{N}$ , initial perturbation distribution $\\mathcal{P}_{I}$ on $\\mathbb{R}^{d}$ , reward-perturbation distribution $\\mathcal{P}_{R}$ on $\\mathbb{R}$ , ensemble sampling distribution $\\{{\\mathcal{I}}_{t}\\}_{t=1}^{T}$ on $[m]$   \nSample $W^{j}\\sim\\mathcal{P}_{I}$ for each $j\\in[m]$ .   \nInitialize $V_{0}=\\lambda I_{d}$ , $S_{0}^{j}=W^{j}$ , $\\theta_{0}^{j}=V_{0}^{-1}S_{0}^{j}$ for each $j\\in[m]$   \nfor $t=1,2,\\ldots T$ do Sample $j_{t}\\sim\\mathcal{T}_{t}$ Pull arm $X_{t}=\\operatorname{argmax}_{x\\in\\mathcal{X}}\\boldsymbol{x}^{\\top}\\theta_{t-1}^{j_{t}}$ and observe $Y_{t}$ Update $V_{t}=V_{t-1}+X_{t}X_{t}^{\\top}$ for $j=1,2,\\dots,m$ do Sample $Z_{t}^{j}\\sim\\mathcal{P}_{R}$ Update $S_{t}^{j}=S_{t-1}^{j}+X_{t}(Y_{t}+Z_{t}^{j})$ and $\\theta_{t}^{j}=V_{t}^{-1}S_{t}^{j}$ end for   \nend for ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "$\\mathbb{N}$ denotes the set of natural numbers starting from 1. For a positive integer $M$ , $[M]$ denotes the set $\\{1,2,\\ldots,M\\}$ . $\\mathbf{0}_{d}$ denotes the zero vector in $\\mathbb{R}^{d}$ and $I_{d}$ denotes the identity matrix in $\\mathbb{R}^{d\\times d}$ . We define and work within a probability space $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ , where $\\Omega$ is the sample space, $\\mathcal{F}$ is the event set, and $\\mathbb{P}$ is the probability measure. $\\mathcal{N}(\\mu,\\Sigma)$ denotes the uni- or multi-variate Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$ . $\\wedge$ denotes logical conjunction (\u201cand\u201d) and $\\vee$ denotes logical disjunction (\u201cor\u201d). With slight abuse of notation, we write $\\{\\omega\\in\\Omega:A\\}$ and $A$ interchangeably when $A$ is some condition, for simplicity. $O(\\cdot)$ denotes the asymptotic growth rate with respect to problem parameters $d,T$ , and $K$ . $\\tilde{\\mathcal{O}}(\\cdot)$ further hides logarithmic factors of $T$ and $d$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the stochastic linear bandit problem. The learning agent is presented with a non-empty arm set $\\mathcal{X}\\subset\\mathbb{R}^{d}$ . For $T$ time steps, where $T\\in\\mathbb N$ is the time horizon, the agent selects an arm $X_{t}\\in\\mathcal{X}$ and receives a real-valued reward $Y_{t}$ , where the reward is generated based on a hidden true parameter vector, $\\theta^{*}\\in\\mathbb{R}^{d}$ . Specifically, $Y_{t}$ is defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y_{t}=X_{t}^{\\top}\\theta^{*}+\\eta_{t}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\eta_{t}$ is a zero-mean random noise. The objective of the agent is to maximize the cumulative reward, or equivalently, to minimize the cumulative regret $R(T)$ defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\nR(T):=\\sum_{t=1}^{T}\\left(\\operatorname*{sup}_{x\\in\\mathcal{X}}{x^{\\top}\\theta^{*}}-X_{t}^{\\top}\\theta^{*}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Ensemble Sampling for Linear Bandits ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Algorithm 1 describes linear ensemble sampling. The learner maintains an ensemble of $m$ estimators, where each estimator fits perturbed rewards. For the $j$ -th estimator, a random vector $W^{j}\\in\\mathbb{R}^{d}$ acts as an initial perturbation on the estimator, and a random variable $Z_{t}^{j}$ perturbs the reward at time $t$ . Specifically, $\\theta_{t}^{j}$ is the solution of the following minimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{\\boldsymbol{\\theta}\\in\\mathbb{R}^{d}}{\\operatorname*{minimize}}\\,\\lambda\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{W}^{j}/\\lambda\\right\\|_{2}^{2}+\\sum_{i=1}^{t}\\left(\\boldsymbol{X}_{i}^{\\top}\\boldsymbol{\\theta}-\\left(\\boldsymbol{Y}_{i}+\\boldsymbol{Z}_{i}^{j}\\right)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "When selecting an arm, one of the $m$ estimators is chosen according to an ensemble sampling distribution $\\mathcal{I}_{t}$ and acts greedily with respect to the sampled estimator. Previous ensemble sampling algorithms [16, 21, 10] sample the estimators uniformly from the ensemble, but we allow any policy for selecting the estimator. Further distinguishing from the algorithm presented in Janz et al. [10], we do not sample Rademacher random variables for symmetrization, making our algorithm simpler. Ensemble sampling is capable of being generalized to complex settings whenever solving minimization problem (1) is tractable. Especially when incremental updates of the minimization problem are cheap, for instance with neural networks or other gradient descent-based models, ensemble sampling can be an efficient exploration strategy. The algorithm may simply store an ensemble of models, sample one to select an action, and then update the models incrementally based on the observed reward and generated perturbation. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 Regret Bound of Linear Ensemble Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before we present the regret bound of linear ensemble sampling (Algorithm 1), we present the following standard assumptions on the problem structure. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Arm set and parameter). $\\mathcal{X}$ is closed and for all $x\\in\\mathscr{X}$ , $\\|{\\boldsymbol{x}}\\|_{2}\\leq1$ . There exists $S>0$ such that $\\|\\theta^{*}\\|_{2}\\leq S$ . Both bounds are known to the agent. ", "page_idx": 3}, {"type": "text", "text": "Remark 1. Under Assumption 1, $\\mathcal{X}$ is a compact set. Therefore, we can define $x^{*}:=$ $\\operatorname{argmax}_{x\\in{\\mathcal{X}}}x^{\\top}\\theta^{*}$ and rewrite the definition of $R(T)$ as $\\begin{array}{r}{\\sum_{t=1}^{T}{x^{*}}^{\\top}\\theta^{*}-X_{t}^{\\top}\\theta^{*}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "For a rigorous statement of the second assumption, we define several filtrations. For $t\\in[T]\\cup\\{0\\}$ , let $\\mathcal{F}_{t}^{X}:=\\sigma\\left(X_{1},\\ldots X_{t}\\right)$ and $\\bar{\\mathcal{F}}_{t}^{\\eta}:=\\sigma\\left(\\eta_{1},\\bar{\\ldots},\\eta_{t}\\right)$ be the $\\sigma$ -algebras generated by $X_{i}$ and $\\eta_{i}$ up to time $t$ respectively. We also define the $\\sigma$ -algebra generated by the algorithm\u2019s internal randomness up until the choice of $X_{t}$ as $\\mathcal{F}_{t}^{A}$ . Let $\\check{\\mathcal{F}}_{t}:=\\sigma\\left(\\check{\\mathcal{F}}_{t}^{A}\\cup\\check{\\mathcal{F}}_{t}^{X}\\stackrel{\\bullet}{\\cup}\\check{\\mathcal{F}}_{t}^{\\eta}\\right)$ be the $\\sigma$ -algebra generated by the first $t$ iterations of the interaction between the environment and the agent. In addition, let $\\mathcal{F}_{t}^{-}:=\\sigma\\left(\\mathcal{F}_{t}^{A}\\cup\\mathcal{F}_{t}^{X}\\cup\\mathcal{F}_{t-1}^{\\eta}\\right)$ be the $\\sigma$ -algebra generated in the same way as $\\mathcal{F}_{t}$ , but excluding $\\eta_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Noise). There exists $\\sigma\\geq0$ such that $\\eta_{t}$ is ${\\mathcal{F}}_{t}^{-}$ -conditionally $\\sigma$ -subGaussian for all $t\\in[T],$ , i.e., $\\mathbb{E}\\left[\\exp(s\\eta_{t})\\vert\\mathcal{F}_{t}^{-}\\right]\\le\\exp\\left(\\sigma^{2}s^{2}/2\\right)$ holds almost surely for all $s\\in\\mathbb R$ . ", "page_idx": 3}, {"type": "text", "text": "Now, we define a value $\\beta_{t}$ to describe the variance of the generated perturbation values. Define a sequence $\\{\\beta_{t}(\\delta)\\}_{t=0}^{\\infty}$ as $\\begin{array}{r}{\\beta_{t}(\\delta)\\,:=\\,\\sigma\\sqrt{d\\log\\left(1+\\frac{t}{d\\lambda}\\right)+2\\log\\frac{1}{\\delta}}+\\sqrt{\\lambda}S}\\end{array}$ . We may omit $\\delta$ when its value is clear from the context. The definition of $\\beta_{t}$ comes from Abbasi-Yadkori et al. [1] as a confidence radius of the ridge estimator, which we later specify in Lemma 1. We now present the regret bound of linear ensemble sampling (Algorithm 1). ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Regret bound of linear ensemble sampling). Fix $\\delta\\in(0,1]$ . Assume $|{\\mathcal{X}}|=K<\\infty$ and run Algorithm $^{\\,l}$ with $\\lambda\\geq1$ , $\\begin{array}{r}{m\\geq C(K\\log T+\\log\\frac{1}{\\delta})}\\end{array}$ , $\\mathcal{P}_{I}=\\mathcal{N}(\\mathbf{0}_{d},\\bar{\\lambda}\\beta_{T}^{2}I_{d})$ , $\\mathcal{P}_{R}=\\mathcal{N}(0,\\beta_{T}^{2})$ , and ${\\mathcal{J}}_{t}=U n i f(m)$ , where $C$ is a universal constant and $U n i f(m)$ denotes the uniform distribution over $[m]$ . Then, with probability at least $1-4\\delta$ , the cumulative regret of Algorithm $^{\\,I}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\nR(T)={\\mathcal{O}}{\\Big(}(d\\log T)^{{\\frac{3}{2}}}{\\sqrt{T}}{\\Big)}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Discussion of Theorem 1. Theorem 1 shows that Algorithm 1 achieves a $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ frequentist regret bound with an ensemble size of $m=\\Omega(K\\log T)$ . Importantly, our regret bound does not depend on $K$ or $m$ even logarithmically. Hence, this regret bound matches the state-of-the-art frequentist regret bound of linear Thompson sampling [4, 2]. Our result is the first to establish $\\widetilde{\\mathcal{O}}(\\bar{d}^{3/2}\\sqrt{T})$ regret for linear ensemble sampling with an ensemble size sublinear in $T$ , improving the previous bound by the factor $d$ compared to the existing result in Janz et al. [10]. We conjecture that $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ regret is highly likely to be the best bound for linear ensemble sampling based on the negative result in Hamidi and Bayati [9] for LinTS.2 Comparing with the algorithm in Janz et al. [10], our version of linear ensemble sampling algorithm does not utilize Rademacher random variable for symmetrized perturbation. This allows our algorithm to be simpler than that of Janz et al. [10]. Partially due to this algorithmic difference, our regret analysis is quite distinct from the analysis of Janz et al. [10] (see the proof in Section 5.2). ", "page_idx": 3}, {"type": "table", "img_path": "6SSzMq3WTn/tmp/c7b1a1941b48c77e4d464de193bcddd80764a155825ca18f761bbc19f67c7296.jpg", "table_caption": ["Table 1: Comparison of regret bounds for linear ensemble sampling "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "As in Lu and Van Roy [16] and Qin et al. [21], we study the finite-armed problem setting. Both studies analyze the excess regret of ensemble sampling compared to Thompson sampling through an information theoretical approach. However, direct comparisons of the regret bounds are non-trivial. The analysis by Lu and Van Roy [16] includes an error admitted by the authors and Qin et al. [21] analyze the Bayesian regret, which is a weaker notion of regret than the frequentist regret that we analyze in this work. Along with Janz et al. [10], our result also makes a progress in reducing the size of the ensemble compared to Lu and Van Roy [16] and Qin et al. [21]. The size of the ensemble required by Janz et al. [10] is $\\Theta(d\\log T)$ .This requirement implies that their ensemble size may be smaller than ours when $K$ is larger than $d$ and also allows $K$ to be infinite. However, it is important to note that their resulting regret bound of $\\widetilde{\\mathcal{O}}(d^{5/2}\\sqrt{T})$ is clearly sub-optimal compared to regret bounds of other randomized exploration algorithms. Theorem 1 achieves the tighter regret bound while simultaneously reducing the size of the ensemble. ", "page_idx": 4}, {"type": "text", "text": "Remark 2 (Counter-intuitive dependence on ensemble size in Janz et al. [10]). The regret bound in Janz et al. [10] actually grows super-linearly with the ensemble size, which is counter-intuitive. Their regret bound implies that as the ensemble size increases, the performance of the algorithm deteriorates. This fails to explain the superior empirical performance observed for ensemble sampling even with a large ensemble. On the contrary, our result in Theorem 1 does not show any performance degradation as the ensemble size increases. ", "page_idx": 4}, {"type": "text", "text": "Remark 3 (Generalizability of perturbation distributions). We show that Gaussian distribution for perturbation is not essential. The only properties of the Gaussian distribution we utilize are its tail probability and anti-concentration property, stated as Lemma 4 and Fact 1 in Section 5.2. Therefore, any other distributions exhibiting similar behaviors can instead be adopted. In Appendix H, we rigorously demonstrate that any symmetric subGaussian distribution with lower-bounded variance can be employed, possibly at a cost of a constant factor. A large class of distributions, including uniform distribution, spherical distribution, Rademacher distribution, and centered binomial distribution with $p=1/2$ satisfy this condition. This result can be of independent interest. ", "page_idx": 4}, {"type": "text", "text": "5 Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "5.1 General Regret Analysis for Linear Bandits ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin by presenting a general regret bound for any algorithm that selects the best arm based on an estimated parameter. This result can be of independent interest. This general bound and analysis serve as a general framework that includes the regret analysis of linear ensemble sampling (Theorem 1). ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (General regret bound for linear bandit algorithm). Fix $T\\in\\mathbb{N}$ . Assume that at each time step $t\\in[T]$ , the agent chooses $X_{t}=\\operatorname{argmax}_{x\\in{\\mathcal{X}}}x^{\\top}{\\theta}_{t}$ , where $\\theta_{t}\\in\\mathbb{R}^{d}$ is chosen by the agent under some (either deterministic or random) policy. Let $\\lambda>0$ and $\\begin{array}{r}{V_{t}=\\lambda I+\\sum_{i=1}^{t}X_{i}X_{i}^{\\top}}\\end{array}$ . Let $\\left\\{\\mathcal{E}_{1,t}\\right\\}_{t=1}^{T}$ and $\\left\\{\\mathcal{E}_{2,t}\\right\\}_{t=1}^{T}$ be sequences of events that satisfy two conditions: ", "page_idx": 4}, {"type": "text", "text": "1. (Concentration) There exists a constant $\\gamma>0$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\theta_{t}-\\theta^{*}\\|_{V_{t-1}}\\mathbb{1}\\left\\{\\mathcal{E}_{1,t}\\right\\}\\leq\\gamma\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "holds almost surely for all $t\\in[T]$ . ", "page_idx": 4}, {"type": "text", "text": "2. (Optimism) $\\mathcal{E}_{2,t}\\in\\mathcal{F}_{t-1}$ holds and there exists a constant $p\\in(0,1]$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left(x^{*\\,\\top}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t}\\;a n d\\,\\mathcal{E}_{1,t}\\right)\\;o r\\,\\mathcal{E}_{2,t}^{\\subset}\\mid\\mathcal{F}_{t-1}\\right)\\geq p\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "holds almost surely for all $t\\in[T]$ . ", "page_idx": 4}, {"type": "text", "text": "Take $\\mathcal{E}=\\cap_{t=1}^{T}\\left(\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right)$ and any $\\delta\\in(0,1]$ . Then, under the event $\\mathcal{E}$ and an additional event whose probability is at least $1-\\delta$ , the cumulative regret is bounded as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nR(T)\\leq\\gamma\\left(1+\\frac{2}{p}\\right)\\sqrt{2d T\\log\\left(1+\\frac{T}{d\\lambda}\\right)}+\\frac{\\gamma}{p}\\sqrt{\\frac{2T}{\\lambda}\\log\\frac{1}{\\delta}}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Discussion of Theorem 2. The audience well-versed in the regret analysis of randomized algorithms such as TS and PHE would recognize that bounding the regret using the probability of being optimistic is a standard procedure, also presented in Theorem 1 of Kveton et al. [12] and Theorem 2 of Janz et al. [10] as generalizations of the results in Agrawal and Goyal [4] and Abeille and Lazaric [2] respectively. However, our regret analysis offers much more concise approach than the existing techniques, which can be of independent interest beyond the analysis of ensemble sampling. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 states that if the two co\u221anditions, specifically concentration in (2) and optimism in (3), are met, then the algorithm achieves $\\sqrt{T}$ regret. We provide the proof of Theorem 2 in Appendix B. Our proof technique generalizes the well-studied analysis of Abeille and Lazaric [2]. While their work poses conditions on a $d$ -dimensional perturbation vector that is added to the ridge estimator, we do not assume the use of ridge regression nor we assume that the estimator is perturbed. Instead, we only pose conditions on the final estimator the algorithm exploits. Due to this generalization, Theorem 2 is even capable of inducing the regret bound of LinUCB [1], which always opts for an optimistic estimator, by setting $\\gamma=2\\beta_{T}$ and $p=1$ with appropriate concentration events assigned to $\\mathcal{E}_{1,t}$ and $\\mathcal{E}_{2,t}$ . In addition, there are several improvements that simplify the proof which are worth noting. To exploit the optimism condition, we apply Markov\u2019s inequality on a well-defined random variable. The proof of Abeille and Lazaric [2] relies on defining a conditional distribution, conditioned on both history and the event of being optimistic. However, such distribution may not be well-defined if the probability of the event is 0 for given history. Janz et al. [10] try to solve this problem by separately handling such exceptional cases using conditional measures. However, their conditional measures depend on the random history, leading the probability $p$ to be a random variable, which complicates the analysis. We also note that our proof does not require convex analysis studied in Abeille and Lazaric [2]. ", "page_idx": 5}, {"type": "text", "text": "Remark 4 (Role of event $\\mathcal{E}_{2,t}$ ). Previous results that utilize the probability of being optimistic [4, 2, 12, 10] do not explicitly define events $\\{\\mathcal{E}_{2,t}\\}_{t}$ . However, their existence is crucial in our analysis of linear ensemble sampling. Since the perturbation sequences are also part of the history in ensemble sampling, the probability of $\\theta_{t}$ being optimistic may be extremely small under some events in $\\mathcal{F}_{t-1}$ that sample unfavorable sequences. The role of $\\mathcal{E}_{2,t}$ is to confine our analysis to the case where such undesirable events do not occur. ", "page_idx": 5}, {"type": "text", "text": "5.2 Proof of Regret Bound in Theorem 1 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We prove the regret bound of linear ensemble sampling stated in Theorem 1. To apply Theorem 2, high-probabilities of the sequences of events, namely $\\langle\\overline{{\\mathcal{E}}}_{1,t},\\mathcal{E}_{2,t}\\}_{t=1}^{T}$ , should be guaranteed with an appropriate values of $\\gamma$ and $p$ . We show that separate constraints can be imposed on the randomness of the rewards and the perturbations respectively to guarantee the probabilities of the events. We begin by decomposing the estimator into two parts: one that fits the observed rewards and the other that perturbs the estimator. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta_{t}^{j}=V_{t}^{-1}S_{t}^{j}=V_{t}^{-1}\\left(W^{j}+\\sum_{i=1}^{t}X_{i}\\left(Y_{i}+Z_{i}^{j}\\right)\\right)=V_{t}^{-1}\\sum_{i=1}^{t}X_{i}Y_{i}+V_{t}^{-1}\\left(W^{j}+\\sum_{i=1}^{t}X_{i}Z_{i}^{j}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we define $\\begin{array}{r}{\\hat{\\theta}_{t}\\;:=\\;V_{t}^{-1}\\sum_{i=1}^{t}X_{i}Y_{i}}\\end{array}$ and $\\widetilde{\\theta}_{t}^{j}\\;:=\\;V_{t}^{-1}(W^{j}\\,+\\textstyle\\sum_{i=1}^{t}X_{i}Z_{i}^{j})$ . $\\widehat{\\theta}_{t}$ is the ridge regression estimator of the observed data, and  its randomness main ly comes from the noise of the rewards, $\\{\\eta_{i}\\}_{i=1}^{t}.\\,\\widetilde{\\theta}_{t}^{j}$ is the perturbation added to $\\widehat{\\theta}_{t}$ , and its randomness comes from the generated perturbation, $W^{j}$ and $\\{Z_{i}^{j}\\}_{i=1}^{t}$ . The following lemma states the well-known concentration result for the ridge estimator. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 (Theorem 2 of Abbasi-Yadkori et al. [1]). Fi $\\textstyle{x\\ \\delta\\in\\ (0,1]}$ . For $t\\in\\mathbb{N}\\cup\\{0\\}$ , define a sequence of events with $\\begin{array}{r}{\\beta_{t}(\\delta)=\\sigma\\sqrt{d\\log\\left(1+\\frac{t}{d\\lambda}\\right)+2\\log\\frac{1}{\\delta}}+\\sqrt{\\lambda}S\\;a s}\\end{array}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{E}}_{t}:=\\left\\{\\omega\\in\\Omega:\\left\\lVert\\hat{{\\boldsymbol{\\theta}}}_{t}-{\\boldsymbol{\\theta}}^{*}\\right\\rVert_{V_{t}}\\leq\\beta_{t}(\\delta)\\right\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and their intersection $\\begin{array}{r}{\\hat{\\mathcal{E}}:=\\bigcap_{t=0}^{\\infty}\\hat{\\mathcal{E}}_{t}}\\end{array}$ . Then, $\\mathbb{P}(\\hat{\\mathcal{E}})\\geq1-\\delta$ ", "page_idx": 6}, {"type": "text", "text": "Now, we address $\\widetilde{\\theta}_{t}^{j}$ . Define a perturbation vector that represents the perturbation sequence for each model as follows : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf Z_{t}^{j}:=\\left(\\frac{1}{\\sqrt{\\lambda}}W^{j\\top}\\quad Z_{1}^{j}\\quad\\cdot\\cdot\\cdot\\quad Z_{t-1}^{j}\\right)^{\\top}\\in\\mathbb{R}^{d+t-1},\\forall j\\in[m]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Let $\\mathbf{Z}_{t}:=\\mathbf{Z}_{t}^{j_{t}}$ so that $\\mathbf{Z}_{t}$ is the perturbation vector of the model chosen at time $t$ . The following lemma demonstrates that optimism condition (3) can be satisfied by an anti-concentration property of $\\mathbf{Z}_{t}$ alone. ", "page_idx": 6}, {"type": "text", "text": "Lemma 2 (Sufficient condition for optimism). For $t\\in[T]$ , define a vector $U_{t-1}\\in\\mathbb{R}^{d+t-1}$ by ", "page_idx": 6}, {"type": "equation", "text": "$$\nU_{t-1}^{\\top}:=x^{*\\top}V_{t-1}^{-1}\\left(\\sqrt{\\lambda}I_{d}\\quad X_{1}\\quad\\cdot\\cdot\\quad X_{t-1}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, $x^{*^{\\top}}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t}$ holds whenever there exists a constant $c>0$ such that $U_{t-1}^{\\top}\\mathbf Z_{t}\\geq c\\|U_{t-1}\\|_{2}$ and $\\|\\theta^{*}-\\hat{\\theta}_{t-1}\\|_{V_{t-1}}\\leq c$ hold. ", "page_idx": 6}, {"type": "text", "text": "We present a straightforward proof of Lemma 2 in Appendix C. We significantly deviate from the analyses of Abeille and Lazaric [2] and Janz et al. [10] in the method of guaranteeing the optimism condition. Their analyses require a $d$ -dimensional perturbation vector to have a constant probability of having positive component, so-called anti-concentrated, in \u201cevery\u201d possible direction in $\\mathbb{R}^{d}$ since they only prove the existence of a direction that implies optimism. We observe and exploit the fact that it suffices to consider just \u201cone\u201d direction, specifically $U_{t-1}$ , to produce an optimistic estimator. Since $U_{t-1}$ depends only on the sequence of selected arms, the dependency between $U_{t-1}$ and $\\mathbf{Z}_{t}$ decouples when the dependency between $\\{X_{t}\\}_{t=1}^{T}$ and $\\{\\mathbf{Z}_{t}\\}_{t=1}^{T}$ are decoupled, which we later achieve by taking the union bound in a unique way. ", "page_idx": 6}, {"type": "text", "text": "The following lemma shows that concentration and anti-concentration properties of the perturbation are sufficient conditions for Theorem 2. We provide a sketch of its proof and defer the remaining details to Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3. Suppose the agent runs Algorithm $^{\\,l}$ with some parameters. $F i x\\,\\widetilde{\\gamma}>0$ and $p\\in(0,1]$ . For each $t\\in[T]$ , define two events ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathcal{E}}_{1,t}:=\\left\\{\\omega\\in\\Omega:\\left\\Vert\\widetilde{\\theta}_{t-1}^{j_{t}}\\right\\Vert_{V_{t-1}}\\leq\\widetilde{\\gamma}\\right\\},}\\\\ &{\\widetilde{\\mathcal{E}}_{2,t}:=\\left\\{\\omega\\in\\Omega:\\mathbb{P}\\left(U_{t-1}^{\\top}\\mathbf{Z}_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2}\\,a n d\\,\\widetilde{\\mathcal{E}}_{1,t}\\right\\vert\\,\\mathcal{F}_{t-1}\\right)\\geq p\\right\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $U_{t-1}$ is defined as in Lemma 2. Take $\\mathcal{E}_{1,t}=\\widetilde{\\mathcal{E}}_{1,t}\\cap\\hat{\\mathcal{E}}_{t-1}$ and $\\mathcal{E}_{2,t}=\\widetilde{\\mathcal{E}}_{2,t}\\cap\\hat{\\mathcal{E}}_{t-1}$ . Then, $\\mathcal{E}_{1,t}$ and $\\mathcal{E}_{2,t}$ satisfy concentration condition (2) with $\\gamma=\\widetilde{\\gamma}+\\beta_{T}$ and optimism condition (3) with the same value of $p$ . Consequently, with probability at least $1-2\\delta-\\mathbb{P}(\\widetilde{\\mathcal{E}}^{\\mathtt{C}})$ , where $\\widetilde{\\mathcal{E}}:=\\cap_{t=1}^{T}(\\widetilde{\\mathcal{E}}_{1,t}\\cap\\widetilde{\\mathcal{E}}_{2,t})$ , Algorithm 1 achieves regret bound (4) of Theorem 2. ", "page_idx": 6}, {"type": "text", "text": "Remark 5. Lemma 3 applies to any perturbation-based algorithm that exploits $\\theta_{t}=\\widehat{\\theta}_{t-1}+\\widetilde{\\theta}_{t-1}$ , where $\\widetilde{\\theta}_{t-1}$ is a linear transform of a random perturbation vector $\\mathbf{Z}_{t}$ . A version of linear Thompson sampling [2] and LinPHE also fall into this category. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3 shifts the problem of constructing the regret bound of Algorithm 1 to lower-bounding the probabilities of the two sequences of events, $\\{\\widetilde{\\mathcal{E}}_{1,t}\\}_{t=1}^{T}$ and $\\{\\widetilde{\\mathcal{E}}_{2,t}\\}_{t=1}^{T}$ . Note that $\\widetilde{\\mathcal{E}}_{1,t}$ and $\\widetilde{\\mathcal{E}}_{2,t}$ regard $\\{X_{i}\\}_{i=1}^{t-1}$ and $\\mathbf{Z}_{t}$ only, and are independent of further randomness of $\\{\\eta_{t}\\}_{t=1}^{T}$ . ", "page_idx": 6}, {"type": "text", "text": "Sketch of Proof of Lemma 3. The concentration condition follows immediately by the triangle inequality. To show the optimism condition, we verify the following logical implication relationship: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left(\\left(U_{t-1}^{\\top}\\mathbf{Z}_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2}\\right)\\wedge\\widetilde{\\mathcal{E}}_{1,t}\\right)\\vee\\mathcal{E}_{2,t}^{\\subset}\\Rightarrow\\left(\\left(x^{*\\top}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t}\\right)\\wedge\\mathcal{E}_{1,t}\\right)\\vee\\mathcal{E}_{2,t}^{\\subset}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where Lemma 2 bridges the anti-concentration on the left hand side to the optimism on the right hand side. This implication relationship is converted to the following probability inequality: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\L^{>}\\big(\\big((x^{*\\top}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t})\\wedge\\mathcal{E}_{1,t}\\big)\\vee\\mathcal{E}_{2,t}^{C}~|~\\mathcal{F}_{t-1}\\big)\\geq\\mathbb{P}\\Big(\\Big((U_{t-1}^{\\top}\\mathbf{Z}_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2})\\wedge\\widetilde{\\mathcal{E}}_{1,t}\\Big)\\vee\\mathcal{E}_{2,t}^{C}~|~\\mathcal{F}_{t-1}~\\Big)\\times\\mathbb{P}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By the definition of $\\widetilde{\\mathcal{E}}_{2,t}$ , the right hand side is bounded below by $p$ , implying optimism condition (3). The probability of failure is bounded by the union bound and Lemma 1. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 employs the Gaussian perturbation for a concrete instantiation of Algorithm 1. We define two values $\\widetilde{\\gamma}_{T}$ and $\\gamma_{T}$ , which serve as the confidence radii of $\\widetilde{\\theta}_{t}$ and $\\theta_{t}$ for $t\\in[T]$ under the Gaussian perturbation. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{\\gamma}_{T}:=\\beta_{T}\\left(\\sqrt{d\\log\\left(1+\\frac{T}{d\\lambda}\\right)+2\\log\\frac{2T}{\\delta}}+\\sqrt{d}+\\sqrt{2\\log\\frac{2T}{\\delta}}\\right),\\;\\;\\;\\;\\;\\gamma_{T}:=\\tilde{\\gamma}_{T}+\\beta_{T}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that in terms of $d$ and $T$ , both $\\widetilde{\\gamma}_{T}$ and $\\gamma_{T}$ are in ${\\mathcal{O}}\\left(d\\log T\\right)$ . Lemma 4 illustrates the concentration result. Its proof is a simple application of Lemma 1, and is presented in Appendix $\\boldsymbol{\\mathrm E}$ . ", "page_idx": 7}, {"type": "text", "text": "Lemma 4. Suppose Algorithm $^{\\,l}$ is run with parameters specified in Theorem $^{\\,l}$ . Fix $t\\in[T]$ and $j\\in[m]$ . Suppose the sequence of arms $X_{1},\\ldots,X_{t}$ is chosen arbitrarily randomly, not necessarily by the the agent. Let $\\widetilde{\\theta}_{t}^{j}$ be defined as in Eq. (5). Then, with probability at least $1-\\delta/T$ , $\\|\\widetilde{\\theta}_{t}^{j}\\|_{V_{t}}\\le\\widetilde{\\gamma}_{T}$ holds. ", "page_idx": 7}, {"type": "text", "text": "The following fact describes an anti-concentration property of Gaussian distribution, which follows from the fact that a linear combination of independent Gaussians is again Gaussian. ", "page_idx": 7}, {"type": "text", "text": "Fact 1. If $Z\\sim\\mathcal{N}(0,\\alpha^{2}I_{n})$ for some $\\alpha\\geq0$ and $u\\in\\mathbb{R}^{n}$ is a fixed vector for some $n\\in\\mathbb{N},$ , then $\\mathbb{P}\\left(u^{\\top}Z\\geq\\alpha\\left\\|u\\right\\|_{2}\\right)\\geq\\mathbb{P}\\left(z\\geq1\\right)=:p_{N}$ , where $z\\sim\\mathcal{N}(0,1)$ . We note that $p_{N}\\ge0.15$ . ", "page_idx": 7}, {"type": "text", "text": "All the building blocks we need to prove Theorem 1 is ready. The proof illustrates that the events specified in Lemma 3 occur with high probability. ", "page_idx": 7}, {"type": "text", "text": "Proof of Theorem 1. Define $\\widetilde{\\mathcal{E}}_{1,t}$ and $\\widetilde{\\mathcal{E}}_{2,t}$ as in Lemma 3 with $\\widetilde{\\gamma}=\\widetilde{\\gamma}_{T}$ and $p=p_{N}/4$ , where $\\widetilde{\\gamma}_{T}$ is defined in Eq. (7) and $p_{N}$ is  defined in  Fact 1. We show that the s e ev e nts occur with high proba b ility, and the rest follows from Lemma 3. For the sake of the analysis, assume that $\\delta/T\\le p_{N}/2\\approx0.08$ , which holds whenever $T\\geq14$ or $\\delta<0.07$ . ", "page_idx": 7}, {"type": "text", "text": "Assume that the perturbation values $W^{j}$ and $Z_{t}^{j}$ are $\\mathcal{F}_{0}^{A}$ -measurable for all $j\\in[m]$ and $t\\in[T]$ . An interpretation of this assumption is that the algorithm samples all the required values in advance. Note that we still obtain an equivalent algorithm and this modification need not actually take place in the execution. Under this assumption, the uniform sampling of $j_{t}\\sim\\mathcal{T}_{t}$ is the only source of randomness regarding the choice of $\\theta_{t}$ and $X_{t}$ when conditioned on the history $\\mathcal{F}_{t-1}$ . It may seem unintuitive, but this modification simplifies the proof because we only need to deal with $j_{t}$ . ", "page_idx": 7}, {"type": "text", "text": "We first lower-bound the probability of $\\widetilde{\\mathcal{E}}_{1,t}$ . When $j\\in[m]$ is fixed, we can apply Lemma 4 and obtain $\\mathbb{P}(||\\widetilde{\\theta}_{t-1}^{j}||_{V_{t-1}}\\leq\\widetilde{\\gamma}_{T})\\geq1-\\delta/T$ . Since $j_{t}$ is sampled independently of $\\ensuremath{\\widetilde{\\theta}}_{t-1}^{j}$ , it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\widetilde{\\mathcal{E}}_{1,t}\\right)=\\sum_{j=1}^{m}\\mathbb{P}\\left(j_{t}=j,\\lVert\\widetilde{\\theta}_{t-1}^{j}\\rVert_{V_{t-1}}\\leq\\widetilde{\\gamma}_{T}\\right)=\\sum_{j=1}^{m}\\frac{1}{m}\\mathbb{P}\\left(\\left\\lVert\\widetilde{\\theta}_{t-1}^{j}\\right\\rVert_{V_{t-1}}\\leq\\widetilde{\\gamma}_{T}\\right)\\geq1-\\frac{\\delta}{T}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Now, we bound the probability of $\\widetilde{\\mathcal{E}}_{2,t}$ . Fix $j\\,\\in\\,[m]$ . Recall that $\\mathbf{Z}_{t}^{j}$ is the perturbation vector of the $j$ -th model, defined in Eq. (6). The choice of Gaussian perturbation implies that ${\\bf Z}_{t}^{j}\\mathrm{~\\boldmath~\\sim~}$ $\\mathcal{N}(\\mathbf{0}_{d+t-1},\\beta_{T}^{2}I_{d+t-1})$ . Suppose that the sequence of arms $X_{1},\\ldots,X_{T}$ is fixed. Then, we can apply Fact 1, obtaining that $\\mathbb{P}(U_{t-1}^{\\top}\\mathbf{Z}_{t}^{j}\\geq\\beta_{t-1}\\|U_{t-1}\\|_{2})\\geq p_{N}$ , where the probability is measured over the randomness of the perturbation sequence $\\mathbf{Z}_{t}^{j}$ . Let $I_{t}^{j}:=\\mathbb{1}\\{(U_{t-1}^{\\top}\\mathbf{Z}_{t}^{j}\\ge\\beta_{t-1}\\|U_{t-1}\\|_{2})\\wedge$ $\\big(\\|\\widetilde{\\theta}_{t-1}^{j}\\|_{V_{t-1}}\\leq\\widetilde{\\gamma}_{T}\\big)\\big\\}$ . Then, we have that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb P\\big(I_{t}^{j}=1\\big)\\geq\\mathbb P\\big(U_{t-1}^{\\top}\\mathbf Z_{t}^{j}\\geq\\beta_{t-1}\\|U_{t-1}\\|_{2}\\big)-\\mathbb P\\Big(\\big\\|\\widetilde\\theta_{t-1}^{j}\\big\\|_{V_{t-1}}>\\widetilde\\gamma_{T}\\Big)\\geq p_{N}-\\delta/T\\geq p_{N}/2\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the first inequality uses that $\\mathbb{P}(A\\cap B)\\ge\\mathbb{P}(A)-\\mathbb{P}(B^{\\complement})$ holds for any events $A$ and $B$ , and the last inequality holds by the assumption $\\delta/T\\le p_{N}/2$ . However, as we assumed that the perturbation sequence is F0A - measurable, it is $\\mathcal{F}_{t-1}$ -measurable, hence $I_{t}^{j}$ is also $\\mathcal{F}_{t-1}$ -measurable. It means that the value of $I_{t}^{j}$ is determined when the history up to time $t-1$ is fixed. The only remaining source of randomness in choosing $\\theta_{t}$ conditioned on $\\mathcal{F}_{t-1}$ is the sampling of $j_{t}\\sim\\mathcal{T}_{t}$ . Therefore, it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left(U_{t-1}^{\\top}\\mathbf{Z}_{t}^{j_{t}}\\geq\\beta_{t-1}\\|U_{t-1}\\|_{2}\\right)\\wedge\\left(\\left\\|\\widetilde{\\theta}_{t-1}^{j_{t}}\\right\\|_{V_{t-1}}\\leq\\widetilde{\\gamma}_{T}\\right)\\mid\\mathcal{F}_{t-1}\\right)=\\frac{1}{m}\\sum_{j=1}^{m}I_{t}^{j}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 Linear Perturbed-History Exploration (LinPHE) ", "page_idx": 8}, {"type": "text", "text": "Input $:$ regularization parameter $\\lambda>0$ , initial perturbation distribution $\\mathcal{P}_{I}$ on $\\mathbb{R}^{d}$ reward-perturbation distribution $\\mathcal{P}_{R}$ on $\\mathbb{R}$   \nInitialize $V_{0}=\\lambda I_{d}$   \nfor $t=1,2,\\ldots T$ do Sample $\\begin{array}{r}{V_{t}\\sim\\mathcal{P}_{I},\\,Z_{t,1},\\dotsc...\\,,Z_{t,t-1}\\overset{i.i.d.}{\\sim}\\mathcal{P}_{R}}\\end{array}$ Update $\\begin{array}{r}{\\theta_{t}=V_{t-1}^{-1}\\left(W_{t}+\\sum_{i=1}^{t-1}X_{i}(Y_{i}+Z_{t,i})\\right)}\\end{array}$ Pull arm $X_{t}=\\operatorname{argmax}_{x\\in{\\mathcal{X}}}x^{\\top}\\theta_{t}$ , and observe $Y_{t}$ Update $V_{t}=V_{t-1}+X_{t}X_{t}^{\\top}$   \nend for ", "page_idx": 8}, {"type": "text", "text": "Since we have verified that the expectation of the right hand side is greater than $p_{N}/2$ , AzumaHoeffding inequality (Lemma 12) implies that $\\begin{array}{r}{\\mathbb{P}(\\frac{1}{m}\\sum_{j=1}^{m}I_{t}^{j}<p_{N}/4)\\leq\\exp\\left(-p_{N}^{2}m/8\\right)\\!.}\\end{array}$ . Recall that this result is obtained assuming that $X_{1},\\ldots,X_{T}$ are fixed. We take the union bound over all possible sequences of arms. However, a na\u00efve union bound multiplies $K^{T}$ to the failure probability, which leads to an undesirable result of $m$ scaling linearly with $T$ . We present the following claim inspired by an observation from Lu and Van Roy [16] that a permutation of selected arms can be regarded as equivalent. We note that the strong result of Lemma 6 in Lu and Van Roy [16] is not applicable to our setting since we do not assume that $\\{\\eta_{t}\\}_{t=1}^{T}$ is distributed identically nor independently. Although we present all the main ideas to support Claim 1 in this section, there may be a few points that readers find require further justification. Due to limited space, we provide a full, rigorous justification of Claim 1 in Appendix $\\boldsymbol{\\mathrm{F}}$ , where we present a different perspective on the sampling of perturbation. We note that this claim is not a conjecture nor an assumption. ", "page_idx": 8}, {"type": "text", "text": "Claim 1. There exists an event $\\mathcal{E}_{2}^{*}$ such that under $\\mathcal{E}_{2}^{*}$ , $\\begin{array}{r}{\\frac{1}{m}\\sum_{j=1}^{m}I_{t}^{j}\\ge p_{N}/4}\\end{array}$ holds for $t=1,\\dots,T$ and $\\mathbb{P}(\\mathcal{E}_{2}^{*\\mathsf{C}})\\leq T^{K}\\exp(-p_{N}^{2}m/8)$ . ", "page_idx": 8}, {"type": "text", "text": "The key observation in Claim 1 is that the perturbation vector consists of i.i.d. components, hence its distribution is invariant under independent permutations. Therefore, the distributions of $\\ensuremath{\\widetilde{\\theta}}_{t-1}^{j}$ and $U_{t-1}^{\\top}\\mathbf{Z}_{t}^{j}$ remain invariant under the permutation of selected arms. Although the sequence of arms and the perturbation vector are not independent as a whole, the permutation that sorts the selected arms preserves the distribution of $\\mathbf{Z}_{t}$ since $X_{t}$ and $Z_{t}$ are independent for all $t\\in[T]$ . The number of equivalence classes up to permutation over sequences of arms with lengths at most $T-1$ is less than $T^{K}$ , since each arm can be selected 0 to $T-1$ times inclusively. Therefore, we take the union bound over the $T^{K}$ sequences of arms and attain $\\mathcal{E}_{2}^{*}$ . ", "page_idx": 8}, {"type": "text", "text": "Taking $\\begin{array}{r}{m=\\frac{8}{p_{N}^{2}}(K\\log T+\\log\\frac{1}{\\delta})}\\end{array}$ , we obtain that $\\mathbb{P}(\\mathcal{E}_{2}^{\\ast\\mathsf{C}})\\leq\\delta$ . Eq. (10) implies that $\\mathbb{P}(\\cap_{t=1}^{T}\\widetilde{\\mathcal{E}}_{2,t})\\geq$ $\\mathbb{P}(\\mathcal{E}_{2}^{*})\\geq1-\\ddot{\\delta}$ . Therefore, by Lemma 3, with probability at least $1-4\\delta$ , the cumulative regret is bounded as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\nR(T)\\leq\\gamma_{T}\\left(1+\\frac{8}{p_{N}}\\right)\\sqrt{2d T\\log\\left(1+\\frac{T}{d\\lambda}\\right)}+\\frac{4\\gamma_{T}}{p_{N}}\\sqrt{\\frac{2T}{\\lambda}\\log\\frac{1}{\\delta}}=O\\left((d\\log T)^{\\frac{3}{2}}\\sqrt{T}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "6 Ensemble Sampling and Perturbed-History Exploration ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we rigorously investigate the relationship between linear ensemble sampling and LinPHE. A generalized version of LinPHE is described in Algorithm 2. Note that the perturbed estimator, $\\begin{array}{r}{\\theta_{t}\\;=\\;V_{t-1}^{-1}\\big(W_{t}\\,+\\,\\sum_{i=1}^{t-1}X_{i}(Y_{i}\\,+\\,Z_{t,i})\\big)}\\end{array}$ , resembles the estimator of linear ensemble sampling, which becomes evident when compared with Eq. (5). The main difference is that in LinPHE (Algorithm 2), the perturbation sequence is generated independently of the history at every time step, whereas in linear ensemble sampling (Algorithm 1), the sequence is not renewed but is incremented at each time step. However, we further observe that in linear ensemble sampling, as long as an estimator is not sampled for the arm selection, its perturbation sequence is independent of the selected arms and rewards. This implies that the estimator in the ensemble that is selected for the first time is equivalent to the estimator computed by the policy of LinPHE. Specifically, if the $j$ -th estimator is selected for the first time at time step $t$ , then the perturbation values of the estimator, $W^{j}$ and $\\{Z_{i}^{j}\\}_{i=1}^{t-1}$ , have had no effect on previous interactions. Therefore, newly sampling them as $W_{t}^{j}\\sim\\mathcal{P}_{I}$ and $\\{Z_{t,i}^{j}\\}_{i=1}^{t-1}\\overset{i.i.d.}{\\sim}\\mathcal{P}_{R}$ , as in LinPHE, does not alter future interactions. We conclude that in the case where the ensemble size is greater than or equal to $T$ , linear ensemble sampling becomes equivalent to LinPHE by selecting the estimators in a round robin. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Proposition 1. Linear ensemble sampling (Algorithm $^{\\,l}$ ) with $m=T$ and deterministic policy of choosing a model, e.g., $\\mathcal{I}_{t}\\equiv t$ for $t=1,\\dots,T$ , is equivalent to LinPHE (Algorithm 2). ", "page_idx": 9}, {"type": "text", "text": "Proposition 1 shows that LinPHE is a special case of linear ensemble sampling and provides insightful consequences in both directions of the equivalence. To our best knowledge, Proposition 1 is the first result to formally demonstrate the relationship between linear ensemble sampling and LinPHE. ", "page_idx": 9}, {"type": "text", "text": "Linear ensemble sampling with $T$ models is LinPHE: Since an ensemble of $T$ models is equivalent to LinPHE which achieves a regret bound $\\widetilde{\\mathcal{O}}(d\\sqrt{T\\log K})$ , the ensemble size larger than $T$ is not necessary. This implication certainly empha sizes the sub-optimal requirements of the ensemble size in Lu and Van Roy [16], Qin et al. [21]. Even when $K>T$ in our problem setting, this equivalence provides the ground for upper bounding the ensemble size by $T$ . ", "page_idx": 9}, {"type": "text", "text": "LinPHE is linear ensemble sampling with $T$ models: Conversely, since LinPHE can be regarded as linear ensemble sampling with $T$ models, it is possible to derive a regret bound of LinPHE by following the proof of Theorem 1. We present Corollary 1, which states a new regret bound of $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ for LinPHE. Note that in this case, the regret bound is independent of $K$ . ", "page_idx": 9}, {"type": "text", "text": "Corollary 1 (Regret bound of LinPHE). $F i x\\,\\delta\\in(0,1]$ . Algorithm 2 with $\\lambda\\geq1$ , $\\mathcal{P}_{I}=\\mathcal{N}(\\mathbf{0}_{d},\\lambda\\beta_{T}^{2}I_{d})$ and $\\mathcal{P}_{R}=\\mathcal{N}(0,\\beta_{T}^{2})$ achieves $\\mathcal{O}((d\\log T)^{3/2}\\sqrt{T})$ cumulative regret with probability at least $1-3\\delta$ . ", "page_idx": 9}, {"type": "text", "text": "Discussion of Corollary 1. Kveton et al. [12] provide a $\\widetilde{\\mathcal{O}}(d\\sqrt{T\\log K})$ regret bound when the number of arms is finite. Our result is the first to prove that LinPHE achieves a $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ regret bound that is independent of the number of arms. Hence, we view our overall results as a generalization of Kveton et al. [12]. I\u221at is widely observed that assuming the size of the arm set to be $K$ may lead to an interchanging of a $\\sqrt{d}$ factor with a $\\sqrt{\\log K}$ factor in the regret bound [4], although attaining such reduction may not always be done in a trivial manner [5, 8, 6]. Our focus is not merely on proving another regret bound for LinPHE, but rather on highlighting the close relationship between linear ensemble sampling and LinPHE, which, to our knowledge, has been overlooked in the literature. ", "page_idx": 9}, {"type": "text", "text": "The proof of Corollary 1 follows the proof of Theorem 1. Note that the latter part of the proof of Theorem 1 focuses on decoupling the dependency between $\\{X_{t}\\}_{t=1}^{T}$ and $\\mathbf{Z}_{t}$ . However, in the case of LinPHE, they are already independent since the perturbation sequence is freshly sampled at every time step, enabling a more elegant and concise proof. Especially, as it skips the parts that require the number of arms to be finite, for instance the use of Claim 1, Corollary 1 holds even when the number of arms is infinite. The whole proof is presented in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We prove that linear ensemble sampling achieves a $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ regret bound, marking the first such result in the frequentist setting and matching the bes t-known regret bound for randomized algorithms. The required ensemble size scales logarithmically with the time horizon as $\\Omega(K\\log T)$ . Additionally, we expand our analysis to LinPHE, demonstrating that it is a special case of linear ensemble sampling with an ensemble of $T$ models, achieving the same regret bound of $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ . While our work focuses on linear bandits, ensemble sampling applications have shown superior performance in more complex settings. This suggests that theoretical extensions beyond the linear setting are worth pursuing, with our results providing an important foundation for possibly understanding these extensions. Extending the results to general contextual settings, where the arm set may change over time with potentially non-linear reward functions, represents a promising direction for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2022R1C1C1006859, 2022R1A4A1030579, and RS-2023-00222663) and by AI-Bio Research Grant through Seoul National University. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. Advances in Neural Information Processing Systems, 24:2312\u20132320, 2011.   \n[2] Marc Abeille and Alessandro Lazaric. Linear Thompson Sampling Revisited. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 176\u2013184. PMLR, PMLR, 20\u201322 Apr 2017.   \n[3] Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In Conference on learning theory, pages 39\u20131. JMLR Workshop and Conference Proceedings, 2012.   \n[4] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In International conference on machine learning, pages 127\u2013135. PMLR, 2013.   \n[5] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002. [6] S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, and Sham M Kakade. Towards minimax policies for online linear optimization with bandit feedback. In Conference on Learning Theory, pages 41\u20131. JMLR Workshop and Conference Proceedings, 2012.   \n[7] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in neural information processing systems, 24, 2011.   \n[8] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 208\u2013214. JMLR Workshop and Conference Proceedings, 2011.   \n[9] Nima Hamidi and Mohsen Bayati. On frequentist regret of linear thompson sampling. arXiv preprint arXiv:2006.06790, 2020.   \n[10] David Janz, Alexander E Litvak, and Csaba Szepesv\u00e1ri. Ensemble sampling for linear bandits: small ensembles suffice. arXiv preprint arXiv:2311.08376, 2023.   \n[11] Branislav Kveton, Csaba Szepesvari, Mohammad Ghavamzadeh, and Craig Boutilier. Perturbedhistory exploration in stochastic multi-armed bandits. In International Joint Conference on Artificial Intelligence, 2019. URL https://api.semanticscholar.org/CorpusID:67856126.   \n[12] Branislav Kveton, Csaba Szepesv\u00e1ri, Mohammad Ghavamzadeh, and Craig Boutilier. Perturbedhistory exploration in stochastic linear bandits. In Uncertainty in Artificial Intelligence, pages 530\u2013540. PMLR, 2020.   \n[13] Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and Craig Boutilier. Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence and Statistics, pages 2066\u20132076. PMLR, 2020.   \n[14] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \n[15] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of statistics, pages 1302\u20131338, 2000.   \n[16] Xiuyuan Lu and Benjamin Van Roy. Ensemble sampling. Advances in Neural Information Processing Systems, 30, 2017.   \n[17] Xiuyuan Lu, Zheng Wen, and Branislav Kveton. Efficient online recommendation via low-rank ensemble sampling. In Proceedings of the 12th ACM Conference on Recommender Systems, pages 460\u2013464, 2018.   \n[18] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. Advances in Neural Information Processing Systems, 29, 2016.   \n[19] Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement learning. Advances in Neural Information Processing Systems, 31, 2018.   \n[20] Ian Osband, Benjamin Van Roy, Daniel J Russo, Zheng Wen, et al. Deep exploration via randomized value functions. Journal of Machine Learning Research, 20(124):1\u201362, 2019.   \n[21] Chao Qin, Zheng Wen, Xiuyuan Lu, and Benjamin Van Roy. An analysis of ensemble sampling. Advances in Neural Information Processing Systems, 35:21602\u201321614, 2022.   \n[22] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221\u20131243, 2014.   \n[23] Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on thompson sampling. Foundations and Trends\u00ae in Machine Learning, 11(1):1\u201396, 2018.   \n[24] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933.   \n[25] Runzhe Wan, Haoyu Wei, Branislav Kveton, and Rui Song. Multiplier bootstrap-based exploration. In International Conference on Machine Learning, pages 35444\u201335490. PMLR, 2023.   \n[26] Jie Zhou, Botao Hao, Zheng Wen, Jingfei Zhang, and Will Wei Sun. Stochastic low-rank tensor bandits for multi-dimensional online decision making. Journal of the American Statistical Association, pages 1\u201314, 2024.   \n[27] Zheqing Zhu and Benjamin Van Roy. Deep exploration for recommendation systems. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 963\u2013970, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We summarizes the notations in this paper in Table 2 and Table 3 ", "page_idx": 12}, {"type": "table", "img_path": "6SSzMq3WTn/tmp/6b945e4d5df200298cf3284559f656ff6acd60a436df804a43b201a457f4a6eb.jpg", "table_caption": ["Table 2: Notations specific to this paper "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "6SSzMq3WTn/tmp/7f035d3473a6b74dc089c32c71d81682f7401973b4c64b1e825c72e232f96841.jpg", "table_caption": ["Table 3: Generic notations "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 5 (Lemmas 10 and 11 in Abbasi-Yadkori et al. [1]). Let $\\lambda\\geq1$ , $\\left\\{X_{t}\\right\\}_{t=1}^{T}$ be any sequence of $d$ -dimensional vectors such that $\\|X_{t}\\|_{2}\\leq1$ for all $t\\in[T]$ , and $\\begin{array}{r}{V_{t}=\\lambda I+\\sum_{i=1}^{t}X_{i}X_{i}^{\\top}}\\end{array}$ . Then, $\\begin{array}{r}{\\sum_{t=1}^{T}\\|X_{t}\\|_{V_{t-1}^{-1}}^{2}\\leq2d\\log\\left(1+\\frac{T}{d\\lambda}\\right)}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "As alluded in the discussion of Theorem 2, we utilize Markov\u2019s inequality and the random variable of interest is $X_{t}^{\\top}\\theta_{t}\\mathbb{1}\\left\\{\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right\\}$ . However, this random variable may not be non-negative, precluding the use of Markov\u2019s inequality. The following lemma shows that adding an appropriate term guarantees its non-negativity. ", "page_idx": 13}, {"type": "text", "text": "Lemma 6. Assume the conditions of Theorem 2. Let $J(\\theta)\\,=\\,\\operatorname*{sup}_{x\\in\\mathcal{X}}x^{\\top}\\theta$ , where $\\theta\\in\\mathbb{R}^{d}$ . Let $\\Theta_{t}=\\{\\theta\\in\\mathbb{R}^{d}\\mathrm{~}|\\mathrm{~}||\\theta-\\theta^{*}||_{V_{t-1}}\\leq\\gamma\\}$ . Define $\\theta_{t}^{-}=\\mathrm{argmin}_{\\theta\\in\\Theta_{t}}\\,J(\\theta)$ and $X_{t}^{-}=\\operatorname{argmax}_{x\\in\\mathcal{X}}x^{\\top}\\theta_{t}^{-}$ . For any $\\theta\\in\\mathbb{R}^{d}$ and an event ${\\mathcal{E}}^{\\prime}$ , we introduce the following notation: ", "page_idx": 13}, {"type": "equation", "text": "$$\ng_{t}\\left(\\theta,\\mathcal{E}^{\\prime}\\right)=\\left(J(\\theta)-J(\\theta_{t}^{-})\\right)\\mathbb{1}\\{\\mathcal{E}^{\\prime}\\}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, $g_{t}(\\theta^{*},\\mathcal{E}^{\\prime})\\ge0$ holds for any event $\\mathcal{E}^{\\prime}\\in\\mathcal{F}$ , and $g_{t}(\\theta_{t},\\mathcal{E}^{\\prime\\prime})\\geq0$ holds almost surely for any event such that $\\mathcal{E}^{\\prime\\prime}\\subset\\mathcal{E}_{1,t}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. We first prove $g_{t}(\\theta^{*},\\mathcal{E}^{\\prime})\\geq0$ . Since $\\theta^{*}\\in\\Theta_{t}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\theta^{*})\\geq\\operatorname*{inf}_{\\theta\\in\\Theta_{t}}J(\\theta)=J(\\theta_{t}^{-})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "always holds. Therefore, for any event ${\\mathcal{E}}^{\\prime}$ , $g_{t}(\\theta^{*},\\mathcal{E}^{\\prime})\\geq0$ holds. ", "page_idx": 13}, {"type": "text", "text": "We now suppose $\\mathcal{E}^{\\prime\\prime}\\subset\\mathcal{E}_{1,t}$ and prove $g_{t}(\\theta_{t},\\mathcal{E}^{\\prime\\prime})\\geq0$ . We consider two cases where $\\mathcal{E}^{\\prime\\prime}$ does and does not hold. Under $\\mathcal{E}^{\\prime\\prime}$ , since $\\mathcal{E}^{\\prime\\prime}\\subset\\mathcal{E}_{1,t}$ and by concentration condition (2), $\\theta_{t}\\in\\Theta_{t}$ holds almost surely. Then, $J(\\theta_{t})\\ge\\operatorname*{inf}_{\\theta\\in\\Theta_{t}}J(\\theta)=J(\\theta_{t}^{-})$ holds. Under $\\mathcal{E}^{\\prime\\prime}{}^{\\mathsf{C}}$ , $g_{t}(\\theta_{t},\\mathcal{E}^{\\prime\\prime})=0\\geq0$ trivially holds. Therefore, $g_{t}(\\theta_{t},\\mathcal{E}^{\\prime\\prime})\\geq0$ holds almost surely. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 2. We show that with probability at least $1-\\delta$ , it holds that ", "page_idx": 13}, {"type": "equation", "text": "$$\nR(T)\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}\\leq\\gamma\\left(1+\\frac{2}{p}\\right)\\sqrt{2d T\\log\\left(1+\\frac{T}{d\\lambda}\\right)}+\\frac{\\gamma}{p}\\sqrt{\\frac{2T}{\\lambda}\\log\\frac{1}{\\delta}}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We first bound the instantaneous regret under the event $\\mathcal{E}$ at time $t$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left({x^{*}}^{\\top}\\theta^{*}-X_{t}^{\\top}\\theta^{*}\\right)\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}=\\left({x^{*}}^{\\top}\\theta^{*}-X_{t}^{\\top}\\theta_{t}+X_{t}^{\\top}\\theta_{t}-X_{t}^{\\top}\\theta^{*}\\right)\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\underbrace{\\left({x^{*}}^{\\top}\\theta^{*}-X_{t}^{\\top}\\theta_{t}\\right)\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}}_{I_{1}}+\\underbrace{\\left(X_{t}^{\\top}\\theta_{t}-X_{t}^{\\top}\\theta^{*}\\right)\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}}_{I_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "$I_{2}$ is directly bounded under $\\mathcal{E}$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{2}=X_{t}^{\\top}\\left(\\theta_{t}-\\theta^{*}\\right)\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}}\\\\ &{\\quad\\leq\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}}\\left\\Vert\\theta_{t}-\\theta^{*}\\right\\Vert_{V_{t-1}}\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}}\\\\ &{\\quad\\leq\\gamma\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first inequality is due to the Cauchy-Schwarz inequality and the second inequality comes from condition (2). ", "page_idx": 14}, {"type": "text", "text": "Now, we bound $I_{1}$ . Let $X_{t}^{-}$ , $\\theta_{t}^{-}$ , and $g_{t}$ be defined as in Lemma 6. By Lemma 6, $g_{t}(\\theta_{t},\\mathcal{E})\\ge0$ holds almost surely. Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{1}=\\boldsymbol{x}^{*\\top}\\boldsymbol{\\theta}^{*}\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}-\\boldsymbol{X}_{t}^{\\top}\\boldsymbol{\\theta}_{t}\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}}\\\\ &{\\quad=\\boldsymbol{x}^{*\\top}\\boldsymbol{\\theta}^{*}\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}-\\boldsymbol{X}_{t}^{-\\top}\\boldsymbol{\\theta}_{t}^{-}\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}+\\boldsymbol{X}_{t}^{-\\top}\\boldsymbol{\\theta}_{t}^{-}\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}-\\boldsymbol{X}_{t}^{\\top}\\boldsymbol{\\theta}_{t}\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}}\\\\ &{\\quad=g_{t}\\left(\\boldsymbol{\\theta}^{*},\\mathcal{E}\\right)-g_{t}\\left(\\boldsymbol{\\theta}_{t},\\mathcal{E}\\right)}\\\\ &{\\quad\\le g_{t}\\left(\\boldsymbol{\\theta}^{*},\\mathcal{E}\\right)}\\\\ &{\\quad\\le g_{t}\\left(\\boldsymbol{\\theta}^{*},\\mathcal{E}_{2,t}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality holds since $\\mathcal{E}\\subset\\mathcal{E}_{2,t}$ . Again by Lemma 6, $g_{t}\\left(\\theta^{*},{\\mathcal{E}}_{2,t}\\right)$ and $g_{t}(\\theta_{t},\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t})$ are non-negative almost surely. Note that by the first part of condition (3), $\\mathcal{E}_{2,t}$ is $\\mathcal{F}_{t-1}$ -measurable and hence $g_{t}\\left(\\theta^{*},\\mathcal{E}_{2,t}\\right)=({x^{*}}^{\\top}\\theta^{*}-{X_{t}^{-}}^{\\top}\\theta_{t}^{-})\\mathbb{1}\\{\\mathcal{E}_{2,t}\\}$ is also $\\mathcal{F}_{t-1}$ -measurable. Applying Markov\u2019s inequality conditioned on $\\mathcal{F}_{t-1}$ , we obtain that ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{t}(\\theta^{*},\\mathcal{E}_{2,t})\\mathbb{P}\\Big(g_{t}(\\theta^{*},\\mathcal{E}_{2,t})\\le g_{t}\\left(\\theta_{t},\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right)\\mid\\mathcal{F}_{t-1}\\Big)\\le\\mathbb{E}\\Big[g_{t}\\left(\\theta_{t},\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right)\\mid\\mathcal{F}_{t-1}\\Big]\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We lower-bound the probability on the left hand side utilizing condition (3). Suppose the event of interest in condition (3), namely $'(x^{*\\top}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t})\\wedge\\mathcal{E}_{1,t})\\;\\bar{\\vee}\\;\\mathcal{E}_{2,t}^{\\subset}$ , holds. Under the event, either $\\mathcal{E}_{2,t}^{\\mathsf{C}}$ or $({x^{*}}^{\\top}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t})\\wedge\\mathcal{E}_{1,t}\\wedge\\mathcal{E}_{2,t}$ holds. Under $\\mathcal{E}_{2,t}^{\\mathsf{C}}$ , $g_{t}(\\theta^{*},\\mathcal{E}_{2,t})\\leq g_{t}\\left(\\theta_{t},\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right)$ becomes $0\\leq0$ , which trivially holds. Otherwise, we have $({x^{*}}^{\\top}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t})\\wedge\\mathcal{E}_{1,t}\\wedge\\mathcal{E}_{2,t}$ . Under this event, we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x^{*\\top}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t}\\Leftrightarrow x^{*\\top}\\theta^{*}-X_{t}^{-\\top}\\theta_{t}^{-}\\leq X_{t}^{\\top}\\theta_{t}-X_{t}^{-\\top}\\theta_{t}^{-}}\\\\ &{\\qquad\\qquad\\Leftrightarrow\\left(x^{*\\top}\\theta^{*}-X_{t}^{-\\top}\\theta_{t}^{-}\\right)\\mathbb{1}\\left\\{\\mathcal{E}_{2,t}\\right\\}\\leq\\left(X_{t}^{\\top}\\theta_{t}-X_{t}^{-\\top}\\theta_{t}^{-}\\right)\\mathbb{1}\\left\\{\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right\\}}\\\\ &{\\qquad\\qquad\\Leftrightarrow g_{t}(\\theta^{*},\\mathcal{E}_{2,t})\\leq g_{t}(\\theta_{t},\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we have shown that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\big(\\big(\\boldsymbol{x}^{*\\top}\\boldsymbol{\\theta}^{*}\\leq\\boldsymbol{X}_{t}^{\\top}\\boldsymbol{\\theta}_{t}\\big)\\wedge\\boldsymbol{\\mathcal{E}}_{1,t}\\big)\\vee\\boldsymbol{\\mathcal{E}}_{2,t}^{\\subset}\\Rightarrow g_{t}(\\boldsymbol{\\theta}^{*},\\boldsymbol{\\mathcal{E}}_{2,t})\\leq g_{t}(\\boldsymbol{\\theta}_{t},\\boldsymbol{\\mathcal{E}}_{1,t}\\cap\\mathcal{E}_{2,t})\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which implies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb P\\left(g_{t}(\\theta^{*},\\mathcal{E}_{2,t})\\leq g_{t}\\left(\\theta_{t},\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right)\\mid\\mathcal{F}_{t-1}\\right)\\geq\\mathbb P\\left(\\left(x^{*^{\\top}}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t}\\wedge\\mathcal{E}_{1,t}\\right)\\vee\\mathcal{E}_{2,t}^{C}\\mid\\mathcal{F}_{t-1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq p}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "by condition (3). Therefore, we obtain that $\\begin{array}{r}{g_{t}(\\theta^{*},\\mathcal{E}_{2,t})\\leq\\frac{1}{p}\\mathbb{E}[g_{t}(\\theta_{t},\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t})\\mid\\mathcal{F}_{t-1}]}\\end{array}$ from inequality (11). Lastly, we bound $g_{t}\\big(\\theta_{t},\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\big)$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{t}\\left(\\theta_{t},\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right)=\\left(X_{t}^{\\top}\\theta_{t}-X_{t}^{-\\top}\\theta_{t}^{-}\\right)\\mathbb{1}\\left\\{\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right\\}}\\\\ &{\\leq\\left(X_{t}^{\\top}\\theta_{t}-X_{t}^{\\top}\\theta_{t}^{-}\\right)\\mathbb{1}\\left\\{\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right\\}}\\\\ &{\\leq\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}}\\left\\Vert\\theta_{t}-\\theta_{t}^{-}\\right\\Vert_{V_{t-1}}\\mathbb{1}\\left\\{\\mathcal{E}_{1,t}\\right\\}}\\\\ &{\\leq\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}}\\left(\\left\\Vert\\theta_{t}-\\theta^{*}\\right\\Vert_{V_{t-1}}+\\left\\Vert\\theta_{t}^{-}-\\theta^{*}\\right\\Vert_{V_{t-1}}\\right)\\mathbb{1}\\left\\{\\mathcal{E}_{1,t}\\right\\}}\\\\ &{\\leq2\\gamma\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first inequality uses that $X_{t}^{-}=\\operatorname*{sup}_{x\\in\\mathcal{X}}x^{\\top}\\theta_{t}^{-}$ , the second inequality is due to the CauchySchwarz inequality, the third inequality holds by the triangle inequality, and the last inequality comes ", "page_idx": 14}, {"type": "text", "text": "from condition (2) and that $\\theta_{t}^{-}\\in\\Theta_{t}$ as defined in Lemma 6. Combining all, the instantaneous regret at time $t$ under the event $\\mathcal{E}$ is bounded as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left({x^{*}}^{\\top}\\theta^{*}-X_{t}^{\\top}\\theta^{*}\\right)\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}\\leq\\gamma\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}}+\\frac{2\\gamma}{p}\\mathbb{E}\\left[\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}}\\left\\vert\\mathcal{F}_{t-1}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left(\\gamma+\\frac{2\\gamma}{p}\\right)\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}}+\\frac{2\\gamma}{p}\\left(\\mathbb{E}\\left[\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}}\\left\\vert\\mathcal{F}_{t-1}\\right]-\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, the cumulative regret under $\\mathcal{E}$ is bounded as ", "page_idx": 15}, {"type": "equation", "text": "$$\nR(T)\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}\\leq\\gamma\\left(1+\\frac{2}{p}\\right)\\sum_{t=1}^{T}\\left\\|X_{t}\\right\\|_{V_{t-1}^{-1}}+\\frac{2\\gamma}{p}\\sum_{t=1}^{T}\\left(\\mathbb{E}\\left[\\left\\|X_{t}\\right\\|_{V_{t-1}^{-1}}\\big|\\;\\mathcal{F}_{t-1}\\right]-\\left\\|X_{t}\\right\\|_{V_{t-1}^{-1}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To bound the first sum, we apply the Cauchy-Schwarz inequality and then Lemma 5. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\|X_{t}\\|_{V_{t-1}^{-1}}\\leq\\sqrt{T\\displaystyle\\sum_{t=1}^{T}\\|X_{t}\\|_{V_{t-1}^{-1}}^{2}}}}\\\\ &{\\leq\\sqrt{2d T\\log\\left(1+\\displaystyle\\frac{T}{d\\lambda}\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second sum is bounded by Azuma-Hoeffding inequality. Note that 0 \u2264 \u2225Xt\u2225V \u22121 \u2264 $\\begin{array}{r}{\\sqrt{\\lambda_{\\operatorname*{max}}(V_{t-1}^{-1})}\\,\\|X_{t}\\|_{2}\\,\\leq\\,\\frac{1}{\\sqrt{\\lambda}}}\\end{array}$ , where $\\lambda_{\\operatorname*{max}}(\\cdot)$ denotes the maximum eigenvalue. By Lemma 12, with probability at least $1-\\delta$ , it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(\\mathbb{E}\\left[\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}}\\vert\\,\\mathcal{F}_{t-1}\\right]-\\left\\Vert X_{t}\\right\\Vert_{V_{t-1}^{-1}}\\right)\\leq\\sqrt{\\frac{T}{2\\lambda}\\log\\frac{1}{\\delta}}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, with probability at least $1-\\delta$ , the cumulative regret under $\\mathcal{E}$ is bounded as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nR(T)\\mathbb{1}\\left\\{\\mathcal{E}\\right\\}\\leq\\gamma\\left(1+\\frac{2}{p}\\right)\\sqrt{2d T\\log\\left(1+\\frac{T}{d\\lambda}\\right)}+\\frac{\\gamma}{p}\\sqrt{\\frac{2T}{\\lambda}\\log\\frac{1}{\\delta}}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Proof of Lemma 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 2. The proof is simple algebra utilizing a useful matrix $\\Phi_{t}$ . Define $\\Phi_{t}$ to be the matrix that stacks $X_{i}$ in addition to an identity matrix as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Phi_{t}:=\\left(\\sqrt{\\lambda}I_{d}\\quad X_{1}\\quad\\cdot\\cdot\\cdot\\quad X_{t}\\right)\\in\\mathbb{R}^{d\\times\\left(d+t\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can express a relevant matrix and vectors using $\\Phi_{t}$ , which are $V_{t}=\\Phi_{t}\\Phi_{t}^{\\top}$ , $\\widetilde{\\theta}_{t}^{j}=V_{t}^{-1}\\Phi_{t}\\mathbf{Z}_{t+1}^{j}$ , and $U_{t}^{\\top}=x^{*\\top}V_{t}^{-1}\\Phi_{t}$ . Defining $\\widetilde{\\theta}_{t-1}=\\widetilde{\\theta}_{t-1}^{j_{t}}$ to be the perturbation in the selected estimator at time $t$ , we obtain that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{x^{*}}^{\\top}\\widetilde{\\theta}_{t-1}={x^{*}}^{\\top}V_{t-1}^{-1}\\Phi_{t-1}\\mathbf{Z}_{t}}\\\\ &{\\qquad\\qquad\\qquad=U_{t-1}^{\\top}\\mathbf{Z}_{t}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In addition, it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|U_{t-1}\\|_{2}=\\sqrt{{x^{*}}^{\\top}V_{t-1}^{-1}\\Phi_{t-1}\\Phi_{t-1}^{\\top}V_{t-1}^{-1}x^{*}}}\\\\ &{\\qquad\\qquad=\\sqrt{{x^{*}}^{\\top}V_{t-1}^{-1}x^{*}}}\\\\ &{\\qquad\\qquad=\\|x^{*}\\|_{V_{t-1}^{-1}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By $X_{t}^{\\top}\\theta_{t}=\\operatorname*{sup}_{x\\in\\mathcal{X}}x^{\\top}\\theta_{t}$ , it holds that $x^{*\\top}\\theta_{t}\\leq X_{t}^{\\top}\\theta_{t}$ . Then, we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{t}^{\\top}\\theta_{t}-x^{*^{\\top}}\\theta^{*}\\geq x^{*^{\\top}}\\theta_{t}-x^{*^{\\top}}\\theta^{*}}\\\\ &{=x^{*^{\\top}}\\left(\\theta_{t}-\\theta^{*}\\right)}\\\\ &{=x^{*^{\\top}}\\left(\\widetilde{\\theta}_{t-1}+\\widehat{\\theta}_{t-1}-\\theta^{*}\\right)}\\\\ &{=U_{t-1}\\mathbf{Z}_{t}+x^{*^{\\top}}\\left(\\widehat{\\theta}_{t-1}-\\theta^{*}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the condition of the lemma, there exists a positive constant $c$ such that $\\|\\hat{\\theta}_{t-1}-\\theta^{*}\\|_{V_{t-1}}\\leq c$ and $U_{t-1}^{\\top}\\mathbf{Z}_{t}-c\\|U_{t-1}\\|_{2}\\geq0$ . By the Cauchy-Schwarz inequality, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x^{*\\top}\\left(\\widehat{\\theta}_{t-1}-\\theta^{*}\\right)\\geq-\\left\\Vert x^{*}\\right\\Vert_{V_{t-1}^{-1}}\\left\\Vert\\widehat{\\theta}_{t-1}-\\theta^{*}\\right\\Vert_{V_{t-1}}}\\\\ &{\\qquad\\qquad\\qquad\\geq-c\\left\\Vert U_{t-1}\\right\\Vert_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{t}^{\\top}\\theta_{t}-x^{*\\top}\\theta^{*}\\geq U_{t-1}^{\\top}\\mathbf{Z}_{t}-c\\,\\|U_{t-1}\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\geq0\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we proved that $X_{t}^{\\top}\\theta_{t}\\geq x^{*\\top}\\theta^{*}$ . ", "page_idx": 16}, {"type": "text", "text": "D Proof of Lemma 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 3. Under $\\mathcal{E}_{1,t}=\\widetilde{\\mathcal{E}}_{1,t}\\cap\\hat{\\mathcal{E}}_{t-1}$ , the concentration condition, specifically condition (2) in Theorem 2, holds by the triang le inequality. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\theta_{t}-\\theta^{*}\\right\\rVert_{V_{t-1}}=\\left\\lVert\\widetilde{\\theta}_{t-1}+\\widehat{\\theta}_{t-1}-\\theta^{*}\\right\\rVert_{V_{t-1}}\\le\\left\\lVert\\widetilde{\\theta}_{t-1}\\right\\rVert_{V_{t-1}}+\\left\\lVert\\widehat{\\theta}_{t-1}-\\theta^{*}\\right\\rVert_{V_{t-1}}\\le\\widetilde{\\gamma}+\\beta_{t-1}\\le\\gamma\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To show the optimism condition, condition (3) in Theorem 2, we first show that $\\mathscr{E}_{2,t}~\\in~\\mathscr{F}_{t-1}$ . $\\hat{\\mathcal{E}}_{t-1}\\in\\mathcal{F}_{t-1}$ holds since it regards $\\{X_{i},\\eta_{i}\\}_{i=1}^{t-1}$ only. Since $\\mathbb{P}((U_{t-1}^{\\top}\\mathbf Z_{t}\\geq\\beta_{t-1}\\left\\|U_{t-1}\\right\\|_{2})\\wedge\\widetilde{\\mathcal{E}}_{1,t}\\ |$ $\\mathcal{F}_{t-1})$ is a $\\mathcal{F}_{t-1}$ -measurable random variable and $\\widetilde{\\mathcal{E}}_{2,t}$ is an event that the specified random variable is greater than or equal to $p,\\widetilde{\\mathcal{E}}_{2,t}$ is in $\\mathcal{F}_{t-1}$ . Therefore, we obtain that $\\mathcal{E}_{2,t}=\\hat{\\mathcal{E}}_{t-1}\\cap\\widetilde{\\mathcal{E}}_{2,t}\\in\\mathcal{F}_{t-1}$ . To prove the remaining part  of condition (3), we demonstrate the following logical implication relationships: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\left(U_{t-1}^{\\top}\\mathbf Z_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2}\\right)\\wedge\\widetilde{\\mathcal E}_{1,t}\\right)\\vee\\mathcal E_{2,t}^{\\subset}\\Rightarrow\\left(\\left(U_{t-1}^{\\top}\\mathbf Z_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2}\\right)\\wedge\\widetilde{\\mathcal E}_{1,t}\\wedge\\mathcal E_{2,t}\\right)\\vee\\mathcal E_{2,t}^{\\subset}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\Rightarrow\\left(\\left(U_{t-1}^{\\top}\\mathbf Z_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2}\\right)\\wedge\\widetilde{\\mathcal E}_{1,t}\\wedge\\hat{\\mathcal E}_{t-1}\\right)\\vee\\mathcal E_{2,t}^{\\subset}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\Rightarrow\\left(\\left(x^{*^{\\top}}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t}\\right)\\wedge\\widetilde{\\mathcal E}_{1,t}\\wedge\\hat{\\mathcal E}_{t-1}\\right)\\vee\\mathcal E_{2,t}^{\\subset}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\Rightarrow\\left(\\left(x^{*^{\\top}}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t}\\right)\\wedge\\mathcal E_{1,t}\\right)\\vee\\mathcal E_{2,t}^{\\subset}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first implication follows from $A\\lor B^{\\complement}\\Leftrightarrow(A\\land B)\\lor B^{\\complement}$ , the second implication holds since $\\mathcal{E}_{2,t}\\subset\\hat{\\mathcal{E}}_{t-1}$ , the third implication holds by Lemma 2, which states that $(U_{t-1}^{\\top}\\mathbf{Z}_{t}\\geq\\beta_{t-1}\\,\\|U_{t-1}\\|_{2}\\wedge$ $\\hat{\\mathcal{E}}_{t-1})\\Rightarrow(\\boldsymbol{x}^{*\\top}\\boldsymbol{\\theta}^{*}\\leq\\boldsymbol{X}_{t}^{\\top}\\boldsymbol{\\theta}_{t})$ , and the last by $\\mathcal{E}_{1,t}=\\widetilde{\\mathcal{E}}_{1,t}\\cap\\hat{\\mathcal{E}}_{t-1}$ . This implication relationship shows that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\L^{>}\\big(\\big(\\left(x^{*\\top}\\theta^{*}\\leq X_{t}^{\\top}\\theta_{t}\\right)\\wedge\\mathcal{E}_{1,t}\\big)\\vee\\mathcal{E}_{2,t}^{C}\\mid\\mathcal{F}_{t-1}\\big)\\geq\\mathbb{P}\\left(\\Big(\\big(U_{t-1}^{\\top}\\mathbf{Z}_{t}\\geq\\beta_{t-1}\\,\\|U_{t-1}\\|_{2}\\big)\\wedge\\widetilde{\\mathcal{E}}_{1,t}\\Big)\\vee\\mathcal{E}_{2,t}^{C}\\mid\\mathcal{F}_{t-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We bound the right hand side using the definition of $\\mathcal{E}_{2,t}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left(\\left(U_{t-1}^{\\top}\\mathbf Z_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2}\\right)\\wedge\\widetilde{\\mathcal{E}}_{1,t}\\right)\\vee\\mathcal{E}_{2,t}^{\\top}\\mid\\mathcal{F}_{t-1}\\right)}\\\\ &{=\\mathbb{E}\\left[\\mathbb{1}\\left\\{\\left(\\left(U_{t-1}^{\\top}\\mathbf Z_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2}\\right)\\wedge\\widetilde{\\mathcal{E}}_{1,t}\\right)\\vee\\mathcal{E}_{2,t}^{\\top}\\right\\}\\mid\\mathcal{F}_{t-1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\mathbb{1}\\left\\{\\left(U_{t-1}^{\\top}\\mathbf Z_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2}\\right)\\wedge\\widetilde{\\mathcal{E}}_{1,t}\\right\\}\\mathbb{1}\\left\\{\\mathcal{E}_{2,t}\\right\\}+\\mathbb{1}\\left\\{\\mathcal{E}_{2,t}^{\\top}\\right\\}\\mid\\mathcal{F}_{t-1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\mathbb{1}\\left\\{\\left(U_{t-1}^{\\top}\\mathbf Z_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2}\\right)\\wedge\\widetilde{\\mathcal{E}}_{1,t}\\right\\}\\mid\\mathcal{F}_{t-1}\\right]\\mathbb{1}\\left\\{\\mathcal{E}_{2,t}\\right\\}+\\mathbb{1}\\left\\{\\mathcal{E}_{2,t}^{\\top}\\right\\}}\\\\ &{\\geq p\\mathbb{1}\\left\\{\\mathcal{E}_{2,t}\\right\\}+\\mathbb{1}\\left\\{\\mathcal{E}_{2,t}^{\\top}\\right\\}}\\\\ &{\\geq p\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the third equality uses that $\\mathcal{E}_{2,t}\\in\\mathcal{F}_{t-1}$ and the first inequality holds since under $\\mathcal{E}_{2,t}\\subset\\widetilde{\\mathcal{E}}_{2,t}$ , $\\mathbb{E}\\left[\\mathbb{1}\\left\\{\\left(U_{t-1}^{\\top}\\mathbf{Z}_{t}\\geq\\beta_{t-1}\\left\\Vert U_{t-1}\\right\\Vert_{2}\\right)\\wedge\\widetilde{\\mathcal{E}}_{1,t}\\right\\}\\mid\\mathcal{F}_{t-1}\\right]\\geq p$ holds by the definition of $\\widetilde{\\mathcal{E}}_{2,t}$ . ", "page_idx": 17}, {"type": "text", "text": "Therefore, $\\mathcal{E}_{1,t}$ and $\\mathcal{E}_{2,t}$ satisfy conditions (2) and (3). By Theorem 2, the regret bound stated in inequality (4) holds with probability at least $1-\\delta-\\mathbb{P}(\\mathcal{E}^{\\mathsf{C}})$ , where the union bound is taken. Note that $\\mathcal{E}=\\cap_{t=1}^{T}\\left(\\mathcal{E}_{1,t}\\cap\\mathcal{E}_{2,t}\\right)=\\cap_{t=1}^{T}\\left(\\widetilde{\\mathcal{E}}_{1,t}\\cap\\widetilde{\\mathcal{E}}_{2,t}\\cap\\hat{\\mathcal{E}}_{t-1}\\right)\\supset\\hat{\\mathcal{E}}\\cap\\widetilde{\\mathcal{E}}.$ . The failure probability is bounded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta+\\mathbb{P}\\left(\\mathcal{E}^{\\mathtt{C}}\\right)\\leq\\delta+\\mathbb{P}\\left(\\hat{\\mathcal{E}}^{\\mathtt{C}}\\right)+\\mathbb{P}\\left(\\widetilde{\\mathcal{E}}^{\\mathtt{C}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\delta+\\mathbb{P}\\left(\\widetilde{\\mathcal{E}}^{\\mathtt{C}}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality takes the union bound over $\\mathcal{E}^{\\mathtt{C}}\\subset\\hat{\\mathcal{E}}^{\\mathtt{C}}\\cup\\widetilde{\\mathcal{E}}^{\\mathtt{C}}$ and the second inequality is due to Lemma 1. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "E Proof of Lemma 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 4 is a special case of Lemma 9, which generalizes Gaussian distribution to any subGaussian distribution. We first provide a general chi-squared concentration result, which is required to bound the perturbation induced by $W$ . A generalized version of Lemma 7 is presented in Lemma 10. ", "page_idx": 17}, {"type": "text", "text": "Lemma 7. If $Z\\sim\\mathcal{N}(0,I_{d})$ is a $d$ -dimensional multivariate Gaussian vector, then for any $\\delta\\in(0,1],$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|Z\\|_{2}\\geq\\sqrt{d}+\\sqrt{2\\log\\frac{1}{\\delta}}\\right)\\leq\\delta\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. By Lemma 13 with $\\textstyle x=\\log{\\frac{1}{\\delta}}$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\|Z\\right\\|_{2}^{2}-d\\ge2\\sqrt{d\\log\\frac{1}{\\delta}}+2\\log\\frac{1}{\\delta}\\right)\\le\\delta\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\begin{array}{r}{d+2\\sqrt{d\\log\\frac{1}{\\delta}}+2\\log\\frac{1}{\\delta}\\leq\\left(\\sqrt{d}+\\sqrt{2\\log\\frac{1}{\\delta}}\\right)^{2}}\\end{array}$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\|Z\\|_{2}\\geq\\sqrt{d}+\\sqrt{2\\log\\frac{1}{\\delta}}\\right)\\leq\\mathbb{P}\\left(\\|Z\\|_{2}^{2}\\geq d+2\\sqrt{d\\log\\frac{1}{\\delta}}+2\\log\\frac{1}{\\delta}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\delta\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 4. By Lemma 7, with $\\delta/2T$ instead of $\\delta$ , yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\|W^{j}\\right\\|_{2}\\geq\\sqrt{\\lambda}\\beta_{T}\\left(\\sqrt{d}+\\sqrt{2\\log\\frac{2T}{\\delta}}\\right)\\right)\\leq\\frac{\\delta}{2T}\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since $W^{j}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\lambda\\beta_{T}^{2}I_{d})$ . Applying Lemma 9 with $\\delta/2T$ instead of $\\delta$ yields that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\Vert\\tilde{\\theta}_{t-1}^{j}\\right\\Vert_{V_{t-1}}\\leq\\beta_{T}\\sqrt{d\\log\\left(1+\\frac{T}{d\\lambda}\\right)+2\\log\\frac{2T}{\\delta}}+\\beta_{T}\\left(\\sqrt{d}+\\sqrt{2\\log\\frac{2T}{\\delta}}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "holds with probability at least $1-\\delta/T$ . Note that the right hand side is equal to the definition of $\\widetilde{\\gamma}_{T}$ , defined in Eq. (7). \u53e3 ", "page_idx": 18}, {"type": "text", "text": "F Rigorous Justification of Claim 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we rigorously justify Claim 1 that is stated in the proof of Theorem 1. To do so, we present a different viewpoint on the perturbation sequences. ", "page_idx": 18}, {"type": "text", "text": "Denote the arms as $\\mathcal{X}=\\{x^{1},x^{2},\\ldots,x^{K}\\}$ . For the sake of the analysis, assume that $\\delta/T\\le p_{N}/2\\approx$ 0.08, which holds whenever $T\\geq14$ or $\\delta<0.07$ . ", "page_idx": 18}, {"type": "text", "text": "We reconstruct the perturbation sampled by Algorithm 1. Assume that in addition to $\\{W^{j}\\}_{j=1}^{m}\\stackrel{i.i.d}{\\sim}$ . PI, Algorithm 1 samples mKT samples of {{Zkj,t}(k,t)}jm=1i.i\u223c.d.PR at the beginning, where the subscript $(k,t)$ enumerates from $(1,1)$ to $(K,T)$ . Define $\\begin{array}{r}{N_{k,t}\\,=\\,\\sum_{i=1}^{t}\\mathbb{1}\\left\\{X_{i}=x^{k}\\right\\}}\\end{array}$ to be the number of times arm $k$ has been chosen up to time $t$ . If the $a_{t}$ -th  arm, $\\boldsymbol{x}^{a_{t}}$ , is selected at time $t$ , then we assign $Z_{t}^{j}=Z_{a_{t},N_{a_{t},t}}^{j}$ . Since $Z_{a_{t},N_{a_{t},t}}^{j}$ is still an i.i.d. sample of $\\mathcal{P}_{R}$ conditioned on history, specifically on $\\sigma(\\mathcal{F}_{t}^{X}\\cup\\mathcal{F}_{t}^{\\eta}\\cup\\sigma(\\{\\{Z_{i}^{j}\\}_{i=1}^{t-1}\\}_{j=1}^{m}))$ , we attain an equivalent algorithm with Algorithm 1. We note that these modifications need not be taken in the execution of the algorithm, and their purpose is purely for the analysis. Define $\\mathcal{F}_{0}^{A}=\\sigma\\big(\\{W^{j},\\{Z_{k,t}^{j}\\}_{(k,t)}\\}_{j=1}^{m}\\big)$ , which reflects the fact that they are sampled in advance. For $t\\in[T]$ , define $\\mathcal{F}_{t}^{A}=\\sigma\\left(\\mathcal{F}_{t-1}\\cup\\sigma(j_{t})\\right)$ , which indicates that the only additional randomness of the algorithm when choosing $\\theta_{t}$ is the sampling of $j_{t}\\sim{\\mathcal{I}}_{t}$ . Define the extended perturbation vector as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Z}_{K T}^{j}=\\left(\\frac{1}{\\sqrt{\\lambda}}W^{j^{\\top}}\\quad Z_{1,1}^{j}\\quad...\\quad Z_{1,T}^{j}\\quad Z_{2,1}^{j}\\quad...\\quad Z_{K,T}^{j}\\right)^{\\top}\\in\\mathbb{R}^{d+K T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Removing some components and reordering ${\\bf Z}_{K T}^{j}$ yields ${\\bf Z}_{t}^{j}$ , where the removal and reordering depend on the sequence of chosen arms, namely $a_{1},\\dotsc,a_{t-1}$ . Note that $\\mathbf{Z}_{K T}^{j}\\sim\\mathcal{N}(\\mathbf{0}_{d+K T},\\beta_{T}^{2}I_{d+K T})$ We also define the corresponding extensions of $\\Phi_{t}$ and $U_{t}$ . Define a matrix that has $n$ columns, first $a$ of which are copies of $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ and the rest are $\\mathbf{0}_{d}$ as follows. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{rep}(v,a,n):=(v\\quad\\ldots\\quad v\\quad\\mathbf{0}_{d}\\quad.\\ldots\\quad\\mathbf{0}_{d})\\in\\mathbb{R}^{d\\times n}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Define the extended version of $\\Phi_{t}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Phi_{t}=\\left(\\sqrt{\\lambda}I_{d}\\quad\\mathrm{rep}(x^{1},N_{1,t},T)\\quad.\\dots\\quad\\mathrm{rep}(x^{K},N_{K,t},T)\\right)\\in\\mathbb{R}^{d\\times(d+K T)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\Phi_{t}$ extends $\\Phi_{t}$ by permuting the columns so that the feature vectors from the same arm appear in consecutive columns, then inserting multiple $\\mathbf{0}_{d}$ appropriately. Then, we have $V_{t}\\,=\\,\\Phi_{t}\\dot{\\Phi}_{t}^{\\top}$ and $\\widetilde{\\theta}_{t}=V_{t}^{-1}\\Phi_{t}\\mathbf{Z}_{K T}^{j}.$ , since ${\\bf Z}_{K T}^{j}$ is permuted and extended from $\\mathbf{Z}_{t}^{j}$ in a similar manner. We define the extended version of $U_{t}$ as $\\mathbf{\\dot{U}}_{t}=(\\boldsymbol{x}^{*\\top}\\boldsymbol{V}_{t}^{-1}\\Phi_{t})^{\\top}\\in\\mathbb{R}^{d+K T}$ . $\\mathbf{U}_{t}$ is also a permutation of $U_{t}$ with additional zeros inserted. It holds that $U_{t}^{\\top}\\mathbf{Z}_{t}^{j}=\\mathbf{U}_{t}^{\\top}\\mathbf{Z}_{K T}^{j}$ and $\\left\\|U_{t}\\right\\|_{2}=\\left\\|\\mathbf{U}_{t}\\right\\|_{2}$ . ", "page_idx": 18}, {"type": "text", "text": "Let ${\\mathbb X}_{t}$ be the set of all possible $\\Phi_{t}$ . Since $\\Phi_{t}$ is fully determined by $N_{1,t},\\bar{.}..N_{K,t}$ and each $N_{k,t}$ takes value between 0 and $T\\!-\\!1$ inclusively when $0\\leq t\\leq T{-}1$ , we obtain that $\\left|\\cup_{t=0}^{T-1}\\mathbb{X}_{t}\\right|\\leq T^{K}$ . For any $t\\in[T]$ , take any $\\Phi_{t-1}\\in\\mathbb{X}_{t-1}$ . Note that $\\mathbf{U}_{t-1}=\\boldsymbol{x}^{*\\top}(\\Phi_{t-1}\\Phi_{t-1}^{\\top})^{-1}\\Phi_{t-1}$ is fully determined by $\\Phi_{t-1}$ , and $\\widetilde{\\theta}_{t-1}^{j}=(\\Phi_{t-1}\\Phi_{t-1}^{\\top})^{-1}\\Phi_{t-1}\\mathbf{Z}_{K T}^{j}$ is determined by $\\Phi_{t-1}$ and ${\\bf Z}_{K T}^{j}$ . Assuming that $\\Phi_{t-1}$ is fixed, $\\mathbf{U}_{t-1}$ is also fixed, therefore we can apply Fact 1 and obtain that $\\mathbb{P}\\big(\\mathbf{U}_{t-1}^{\\top}\\mathbf{Z}_{K T}^{j}\\geq$ $\\beta_{T}\\|\\mathbf{U}_{t-1}\\|_{2}\\right)\\geq p_{N}$ , where the only source of randomness comes from ${\\bf Z}_{K T}^{j}$ . Applying Lemma 4, we obtain that $\\mathbb{P}(||\\widetilde{\\theta}_{t-1}^{j}||_{V_{t-1}}\\leq\\widetilde{\\gamma}_{T})\\geq1-\\delta/T.$ Let $I^{j}(\\Phi_{t-1})=\\mathbb{1}\\{(\\mathbf{U}_{t-1}^{\\top}\\mathbf{Z}_{K T}^{j}\\geq\\beta_{T}\\|\\mathbf{U}_{t-1}\\|_{2})\\wedge$ $(\\|\\widetilde{\\theta}_{t-1}^{j}\\|_{V_{t-1}}\\le\\widetilde{\\gamma}_{T})\\}$ . Then, $\\mathbb{P}(I^{j}(\\Phi_{t-1})=1)\\ge p_{N}-\\delta/T\\ge p_{N}/2$ . Since the only randomness on choosing the  arm conditioned on $\\mathcal{F}_{t-1}$ comes from sampling $j_{t}$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left(\\mathbf{U}_{t-1}^{\\top}\\mathbf{Z}_{K T}^{j_{t}}\\geq\\beta_{T}\\left\\Vert\\mathbf{U}_{t-1}\\right\\Vert_{2}\\right)\\wedge\\left(\\left\\Vert\\widetilde{\\theta}_{t-1}^{j_{t}}\\right\\Vert_{V_{t-1}}\\leq\\widetilde{\\gamma}_{T}\\right)\\mid\\mathcal{F}_{t-1}\\right)=\\frac{1}{m}\\sum_{j=1}^{m}I^{j}\\left(\\Phi_{t-1}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We apply Azuma-Hoeffding inequality to show that $\\begin{array}{r}{\\frac{1}{m}\\sum_{j=1}^{m}I^{j}\\left(\\Phi_{t-1}\\right)}\\end{array}$ is bounded below with high probability. Since $\\{I^{j}(\\Phi_{t-1})\\}_{j=1}^{m}$ are i.i.d. Bernoulli random variables with the associated probability greater than $p_{N}/2$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}I^{j}(\\Phi_{t-1})\\leq\\frac{p_{N}}{4}\\right)}\\\\ &{=\\mathbb{P}\\left(\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}I^{j}(\\Phi_{t-1})-\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\mathbb{E}[I^{j}(\\Phi_{t-1})]\\leq\\frac{p_{N}}{4}-\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\mathbb{E}[I^{j}(\\Phi_{t-1})]\\right)}\\\\ &{\\leq\\mathbb{P}\\left(\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}I^{j}(\\Phi_{t-1})-\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\mathbb{E}[I^{j}(\\Phi_{t-1})]\\leq\\frac{p_{N}}{4}-\\frac{p_{N}}{2}\\right)}\\\\ &{=\\mathbb{P}\\left(\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}I^{j}(\\Phi_{t-1})-\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\mathbb{E}[I^{j}(\\Phi_{t-1})]\\leq-\\frac{p_{N}}{4}\\right)}\\\\ &{\\leq\\exp\\left(-\\frac{p_{N}^{2}m}{8}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where Lemma 12 is applied at the end. By taking the union bound over $\\cup_{t=0}^{T-1}\\mathbb{X}_{t}$ , we obtain that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists\\Phi\\in\\cup_{t=0}^{T-1}\\mathbb{X}_{t},\\frac{1}{m}\\sum_{j=1}^{m}I^{j}(\\Phi)\\leq\\frac{p_{N}}{4}\\right)\\leq T^{K}\\exp\\left(-\\frac{p_{N}^{2}m}{8}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The event $\\mathcal{E}_{2}^{*}$ is defined as the complement of the event above. The proof is complete. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{E}_{2}^{*}:=\\left\\{\\omega\\in\\Omega:\\forall\\Phi\\in\\cup_{t=0}^{T-1}\\mathbb{X}_{t},\\frac{1}{m}\\sum_{j=1}^{m}I^{j}(\\Phi)>\\frac{p_{N}}{4}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "G Proof of Corollary 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof of Corollary $^{\\,I}$ . Let $\\widetilde{\\mathcal{E}}_{1,t}$ and $\\widetilde{\\mathcal{E}}_{2,t}$ be defined as in Lemma 3 with $\\widetilde{\\gamma}=\\widetilde{\\gamma}_{T}$ and $p=p_{N}/2$ . We redefine a couple of notations to adapt Algorithm 2. Let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Z}_{t}:=\\left(\\frac{1}{\\sqrt{\\lambda}}W_{t}^{\\top}\\quad Z_{t,1}\\quad.\\ldots\\quad Z_{t,t-1}\\right)^{\\top}\\in\\mathbb{R}^{d+t-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "to be the perturbation vector at time $t$ , and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{\\theta}_{t-1}:=V_{t-1}^{-1}\\left(W_{t}+\\sum_{i=1}^{t-1}X_{i}Z_{t,i}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "be the perturbation in the estimator $\\theta_{t}$ . Regarding $\\widetilde{\\theta}_{t-1}$ as one of $\\ensuremath{\\widetilde{\\theta}}_{t-1}^{j}$ in the proof of Theorem 1, we obtain that $\\mathbb{P}(\\widetilde{\\mathcal{E}}_{1,t})\\ge1\\!-\\!\\delta/T$ and $\\mathbb{P}((U_{t-1}^{\\top}\\mathbf Z_{t}\\geq\\beta_{t-1}\\|U_{t-1}\\|_{2})\\wedge\\widetilde{\\mathcal{E}}_{1,t})\\geq p_{N}/2$ hold, analogously to inequalities (8) and (9) respectively. Moreover, in contrast to the proof of Theorem 1, the perturbation vector $\\mathbf{Z}_{t}$ is now independent of $\\mathcal{F}_{t-1}$ . Noting that $U_{t-1}$ is $\\mathcal{F}_{t-1}$ -measurable, it always holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left((U_{t-1}^{\\top}\\mathbf{Z}_{t}\\geq\\beta_{t-1}\\|U_{t-1}\\|_{2})\\wedge\\widetilde{\\mathcal{E}}_{1,t}\\ |\\ \\mathcal{F}_{t-1}\\right)\\geq\\frac{p_{N}}{2}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This proves that $\\widetilde{\\mathcal{E}}_{2,t}$ is in fact the whole event. Taking the union bound, we obtain that $\\mathbb{P}(\\widetilde{\\mathcal{E}}^{\\mathtt{C}})\\leq$ $\\sum_{t=1}^{T}\\mathbb{P}(\\widetilde{\\mathcal{E}}_{1,t}^{\\mathtt{C}})\\leq\\delta$ . By Lemma 3, with probability at least $1-3\\delta$ , the cumulative regret is bounded by ", "page_idx": 19}, {"type": "equation", "text": "$$\nR(T)\\leq\\gamma_{T}\\left(1+\\frac{4}{p_{N}}\\right)\\sqrt{2d T\\log\\left(1+\\frac{T}{d\\lambda}\\right)}+\\frac{2\\gamma_{T}}{p_{N}}\\sqrt{\\frac{2T}{\\lambda}\\log\\frac{1}{\\delta}}=\\mathcal{O}\\left((d\\log T)^{\\frac{3}{2}}\\sqrt{T}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "H Generalizability of Perturbation Distributions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we demonstrate that any distribution that is symmetric, subGaussian, and has lowerbounded variance satisfies the results of Lemma 4 and Fact 1, possibly up to a constant factor. As mentioned in Remark 3, it implies that our results are valid when the Gaussian distribution is replaced with any symmetric non-degenerate subGaussian distribution. The following lemma is a standard concentration result for vector martingales with subGaussian noises. ", "page_idx": 20}, {"type": "text", "text": "Lemma 8 (Theorem 1 in Abbasi-Yadkori et al. [1]). Let $\\{\\mathcal{F}_{t}\\}_{t=0}^{\\infty}$ be a filtration. Let $\\{\\xi_{t}\\}_{t=1}^{\\infty}$ be $a$ sequence of real-valued random variables such that $\\xi_{t}$ is $\\mathcal{F}_{t}$ -measurable and is $\\mathcal{F}_{t-1}$ -conditionally $\\sigma$ - subGaussian for some $\\sigma\\geq0$ . Let $\\{X_{t}\\}_{t=1}^{\\infty}$ be a sequence of $\\mathbb{R}^{d}$ -valued random vectors such that $X_{t}$ is $\\mathcal{F}_{t-1}$ -measurable and $\\|X_{t}\\|_{2}\\leq1$ almost surely for all $t\\geq1$ . Fix $\\lambda\\geq1$ . Let $\\begin{array}{r}{V_{t}=\\lambda I\\!+\\!\\sum_{i=1}^{t}X_{t}X_{t}^{\\top}}\\end{array}$ . Then, for any $\\delta\\in(0,1]$ , with probability at least $1-\\delta$ , the following inequality holds for all $t\\geq0$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\Vert\\sum_{i=1}^{t}\\xi_{i}X_{i}\\right\\Vert_{V_{t}^{-1}}\\leq\\sigma\\sqrt{d\\log\\left(1+\\frac{t}{d\\lambda}\\right)+2\\log\\frac{1}{\\delta}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next lemma is a simple application of Lemma 8, which proves the concentration result of $\\widetilde{\\theta}_{t}$ under the subGaussianity of $\\mathcal{P}_{R}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 9 (Sufficient condition for concentration). Fix any $\\delta~\\in~(0,1]$ and $t~\\in~[T]$ . Assume that $\\mathbb{P}\\left(\\lVert W\\rVert_{2}>L_{t,0}^{\\delta}\\right)\\,\\le\\,\\delta_{!}$ , and $\\{Z_{i}\\}_{i=1}^{t-1}$ are mutually independent of each other and are $\\mathcal{F}_{i}^{X}$ - conditionally $\\sigma_{R}^{2}$ -subGaussian respectively for each $i\\in[t-1]$ . Then, with probability at least $1-2\\delta$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\Vert\\widetilde{\\theta}_{t-1}\\right\\Vert_{V_{t-1}}\\leq\\sigma_{R}\\sqrt{d\\log\\left(1+\\frac{T}{d\\lambda}\\right)+2\\log\\frac{1}{\\delta}}+\\frac{L_{t,0}^{\\delta}}{\\sqrt{\\lambda}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Recall that $\\begin{array}{r}{\\widetilde{\\theta}_{t-1}=V_{t-1}^{-1}(W+\\sum_{i=1}^{t-1}X_{i}Z_{i})}\\end{array}$ . Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\Vert\\widetilde{\\theta}_{t-1}\\right\\Vert_{V_{t-1}}=\\left\\Vert V_{t-1}\\widetilde{\\theta}_{t-1}\\right\\Vert_{V_{t-1}^{-1}}}\\\\ {\\qquad=\\left\\Vert W+\\displaystyle\\sum_{i=1}^{t-1}X_{i}Z_{t,i}\\right\\Vert_{V_{t-1}^{-1}}}\\\\ {\\qquad\\leq\\left\\Vert W\\right\\Vert_{V_{t-1}^{-1}}+\\left\\Vert\\displaystyle\\sum_{i=1}^{t-1}X_{i}Z_{i}\\right\\Vert_{V_{t-1}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the triangle inequality is used for the last inequality. To bound the first term, we use the fact that $\\begin{array}{r}{\\lambda_{\\operatorname*{max}}(V_{t-1}^{-1}\\bar{)}\\leq\\frac{1}{\\lambda}}\\end{array}$ , where $\\lambda_{\\operatorname*{max}}(\\cdot)$ denotes the maximum eigenvalue. It implies that $\\|W\\|_{V_{t-1}^{-1}}\\leq$ $\\frac{1}{\\sqrt{\\lambda}}\\|W\\|_{2}$ . Since $\\mathbb{P}(\\|W\\|_{2}>L_{t,0}^{\\delta})\\le\\delta$ by assumption, $\\|W\\|_{V_{t-1}^{-1}}\\leq L_{t,0}^{\\delta}/\\sqrt{\\lambda}$ holds with probability at least $1-\\delta$ . The second term is bounded by Lemma 8. With probability $1-\\delta$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\Vert\\sum_{i=1}^{t-1}X_{i}Z_{t,i}\\right\\Vert_{V_{t-1}^{-1}}\\leq\\sigma_{R}\\sqrt{d\\log\\left(1+\\frac{t}{d\\lambda}\\right)+2\\log\\frac{1}{\\delta}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By taking the union bound over the two events, we obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\Vert\\widetilde{\\theta}_{t-1}\\right\\Vert_{V_{t-1}}\\leq\\frac{L_{t,0}^{\\delta}}{\\sqrt{\\lambda}}+\\sigma_{R}\\sqrt{d\\log\\left(1+\\frac{t}{d\\lambda}\\right)+2\\log\\frac{1}{\\delta}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "holds with probability at least $1-2\\delta$ , which proves the the lemma. ", "page_idx": 20}, {"type": "text", "text": "We also provide that the $\\ell_{2}$ -norm of the vector $W$ whose components are i.i.d. samples of a subGaussian distribution is upper-bounded with high-probability. This lemma, combined with Lemma 9, justifies $\\mathcal{P}_{I}$ to be a distribution over $\\mathbb{R}^{d}$ such that each component is an i.i.d. sample of a subGaussian distribution. ", "page_idx": 20}, {"type": "text", "text": "Lemma 10. Suppose that $W~\\in~\\mathbb{R}^{d}$ and each component of $W$ is sampled i.i.d. from a $\\sigma_{I}^{2}$ - subGaussian distribution. Take any $\\delta\\in(0,1]$ . Then, with probability $1-\\delta$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|W\\|_{2}\\leq\\sigma_{I}\\sqrt{2d+4\\log\\frac{1}{\\delta}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Take $X_{1}=\\mathbf{e}_{1},\\ldots,X_{d}=\\mathbf{e}_{d}$ , where $\\{\\mathbf{e}_{1},\\ldots,\\mathbf{e}_{d}\\}$ is the standard basis of $\\mathbb{R}^{d}$ . By Lemma 8 with $\\xi_{i}=(W)_{i}$ and $\\lambda=1$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\lVert\\sum_{i=1}^{d}(W)_{i}\\mathbf{e}_{i}\\right\\rVert_{(2I_{d})^{-1}}\\le\\sigma_{I}\\sqrt{d\\log2+2\\log\\frac{1}{\\delta}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with probability at least $1-\\delta$ . The proof is completed by noting that $\\begin{array}{r}{\\|\\sum_{i=1}^{d}(W)_{i}\\mathbf{e}_{i}\\|_{(2I_{d})^{-1}}=}\\end{array}$ ${\\textstyle\\frac{1}{\\sqrt{2}}}\\|W\\|_{2}$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Finally, we demonstrate that subGaussian distribution with lower-bounded variance satisfies the anticoncentration condition analogous to Fact 1. We normalize the distribution so that it is 1-subGaussian. ", "page_idx": 21}, {"type": "text", "text": "Lemma 11 (Sufficient condition for anti-concentration). Suppose that $\\mathcal{P}$ is a real-valued distribution that is symmetric, $^{\\,l}$ -subGaussian, and has variance at least $1/2$ . Suppose $Z\\in\\mathbb{R}^{n}$ for some $n\\in\\mathbb N$ and its components are i.i.d. samples of $\\mathcal{P}$ . Then, for any fixed $u\\in\\mathbb{R}^{n}$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(u^{\\top}Z\\geq\\frac{1}{3}\\|u\\|_{2}\\right)\\geq0.01.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Without loss of generality, we may assume that $\\|u\\|_{2}=1$ . Let $Y=u^{\\top}Z$ . Then, $\\mathrm{Var}(Y)=$ $\\begin{array}{r}{\\mathrm{Var}(\\sum_{i=1}^{n}(u)_{i}(Z)_{i})=\\sum_{i=1}^{n}(u)_{i}^{2}\\,\\mathrm{Var}((Z)_{i})=\\mathrm{Var}(\\mathcal{P})\\geq\\frac{1}{2}}\\end{array}$ . On the other hand, we attain an upper bound of $\\operatorname{Var}(Y)$ as fol lows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}(Y)=\\mathbb{E}\\left[Y^{2}\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}\\left[Y^{2}\\mathbb{1}\\left\\{|Y|\\leq\\frac{1}{3}\\right\\}\\right]+\\mathbb{E}\\left[Y^{2}\\mathbb{1}\\left\\{\\frac{1}{3}<|Y|\\leq4\\right\\}\\right]+\\mathbb{E}\\left[Y^{2}\\mathbb{1}\\left\\{|Y|\\geq4\\right\\}\\right]}\\\\ &{\\quad\\quad\\quad\\leq\\frac{1}{9}\\mathbb{P}\\left(|Y|\\leq\\frac{1}{3}\\right)+16\\mathbb{P}\\left(\\frac{1}{3}<|Y|\\leq4\\right)+\\mathbb{E}\\left[Y^{2}\\mathbb{1}\\left\\{|Y|\\geq4\\right\\}\\right]}\\\\ &{\\quad\\quad\\quad\\leq\\frac{1}{9}+16\\mathbb{P}\\left(\\frac{1}{3}<|Y|\\right)+\\mathbb{E}\\left[Y^{2}\\mathbb{1}\\left\\{|Y|\\geq4\\right\\}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We upper-bound $\\mathbb{E}[Y^{2}\\mathbb{1}\\{|Y|\\,\\geq\\,4\\}]$ using its subGaussian property. Note that $Y\\,=\\,u^{\\top}Z$ is 1- subGaussian since $\\|u\\|_{2}=1$ and the components of $Z$ are independent and 1-subGaussian. Applying the standard tail bound of subGaussian random variables, it holds that $\\mathbb{P}\\left(|Y|\\ge x\\right)\\le2\\exp\\left(-x^{2}/2\\right)$ , or equivalently, $\\mathbb{P}\\left(Y^{2}\\geq x\\right)\\leq2\\exp\\left(-x/2\\right)$ . Then, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[Y^{2}\\mathbb{1}\\left\\{Y^{2}\\geq16\\right\\}\\right]=\\int_{0}^{\\infty}\\mathbb{P}\\left(Y^{2}\\mathbb{1}\\left\\{Y^{2}\\geq16\\right\\}\\geq x\\right)\\,d x}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\int_{0}^{16}\\mathbb{P}\\left(Y^{2}\\mathbb{1}\\left\\{Y^{2}\\geq16\\right\\}\\geq x\\right)\\,d x+\\int_{16}^{\\infty}\\mathbb{P}\\left(Y^{2}\\mathbb{1}\\left\\{Y^{2}\\geq16\\right\\}\\geq x\\right)\\,d x}\\\\ &{\\qquad\\qquad\\qquad\\qquad=16\\mathbb{P}\\left(Y^{2}\\geq16\\right)+\\int_{16}^{\\infty}\\mathbb{P}\\left(Y^{2}\\geq x\\right)\\,d x}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq32e^{-8}+\\int_{16}^{\\infty}2e^{-\\frac{x}{2}}\\,d x}\\\\ &{\\qquad\\qquad\\qquad\\qquad=32e^{-8}+4e^{-8}}\\\\ &{\\qquad\\qquad\\qquad\\quad<0.0121\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By plugging in the upper bound of (13) to inequality (12) and reordering the terms, we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|Y|>\\frac{1}{3}\\right)\\ge\\frac{1}{16}\\left(\\frac{1}{2}-\\frac{1}{9}-0.0121\\right)\\ge0.023\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, recall that $Z$ is symmetric, therefore $Y$ is symmetric. Therefore, we have $\\mathbb{P}\\left(Y\\geq{\\frac{1}{3}}\\right)\\geq$ 0.0223\u22650.01. ", "page_idx": 21}, {"type": "text", "text": "I Auxiliary Lemmas ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 12 (Azuma-Hoeffding Inequality). Fix $n\\in\\mathbb N$ . Let $\\{Z_{i}\\}_{i=1}^{n}$ be a sequence of real-valued random variables adapted to a flitration $\\left\\{\\mathcal{F}_{i}\\right\\}_{i=0}^{n}$ . Suppose that there exists $a<b$ such that $Z_{i}\\in[a,b]$ holds almost surely for all $i\\,\\in\\,[n]$ . Then, for any $\\delta\\,\\in\\,(0,1],$ , the following inequality holds with probability at least $1-\\delta$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}{\\big(}Z_{i}-\\mathbb{E}\\left[Z_{i}\\mid{\\mathcal F}_{t-1}\\right]{\\big)}\\leq(b-a){\\sqrt{{\\frac{n}{2}}\\log{\\frac{1}{\\delta}}}}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 13 (Lemma 1 of Laurent and Massart [15]). Let $Y_{1},\\ldots,Y_{d}$ be i.i.d. standard Gaussian variables. Set $\\begin{array}{r}{Z=\\sum_{i=1}^{d}(Y_{i}^{2}-1)}\\end{array}$ . Then, the following inequality holds for any $x>0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}(Z\\geq\\sqrt{d x}+2x)\\leq e^{-x}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We accurately describe the problem we set and the results we obtain in the abstract and introduction, specifically Section 1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the limitations of the work at the end of Section 7. Our work is limited to the linear bandit setting, while ensemble sampling also exhibit superior performance in complex settings. However, our results provide a necessary stepping stone to expand the theoretical analysis of ensemble sampling to such regimes. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The full set of assumptions is presented in Section 4. The complete and correct proof is provided throughout Section 5 and the Appendix. All the important ideas are provided as a sketch in the main paper, if not provided as a whole. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 26}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]