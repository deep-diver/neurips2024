[{"figure_path": "lsd27JUJ8v/tables/tables_4_1.jpg", "caption": "Table 1: Hyper-parameters of DQN on Atari environments.", "description": "This table shows the hyperparameters used for the DQN algorithm in the Atari game experiments.  It lists values for parameters such as batch size, replay memory size, target network update frequency, replay ratio, discount factor, optimizer, initial and final exploration rates, and the total number of steps in the environment.", "section": "4.1 Setup"}, {"figure_path": "lsd27JUJ8v/tables/tables_19_1.jpg", "caption": "Table 1: Hyper-parameters of DQN on Atari environments.", "description": "This table lists the hyperparameters used for the Deep Q-Network (DQN) algorithm when applied to Atari game environments.  It specifies values for key parameters such as batch size, replay memory size, target network update frequency, replay ratio, discount factor, optimizer, initial and final exploration rates, exploration decay steps, and the total number of steps in the environment during training.", "section": "B.2 Arcade Learning Environment"}, {"figure_path": "lsd27JUJ8v/tables/tables_20_1.jpg", "caption": "Table 2: Hyper-parameters of TD3 on MuJoCo environments.", "description": "This table lists the hyperparameters used for the TD3 algorithm on MuJoCo environments.  It shows the values used for batch size, replay memory size, discount factor, optimizer, learning rate, target update rate, policy noise, noise clip, delayed policy update frequency, and total steps in the environment. These parameters are crucial for the training and performance of the TD3 algorithm.", "section": "B.3 MuJoCo"}]