[{"figure_path": "01qa1ZJs65/figures/figures_1_1.jpg", "caption": "Figure 1: Paradigm of Language-Only VLM Selection (LOVM). Users describe the details of their target tasks in text form, such as class names and image domains. Then, LOVM utilizes this information to generate class-related labeled texts through ChatGPT. These texts serve as substitutes for image samples in subsequent model selection algorithms. The model selection algorithm uses two types of data, including the open-source datasets (which have image and text data) and the text data from the target dataset, to predict the VLM's absolute or relative performance on a target dataset. It then selects the most appropriate VLM based on the predicted performance.", "description": "This figure illustrates the process of Language-Only VLM Selection (LOVM).  A user starts by describing their task (e.g., classifying cats and dogs in natural images).  A large language model (LLM) like ChatGPT then generates auxiliary text data related to the classes. This text data acts as a proxy for the actual images, which is helpful when image data isn't available for the target dataset.  A model selection algorithm then uses this text data, along with data from open-source datasets (containing both images and text), to predict how different Vision-Language Models (VLMs) will perform on the task. The best-performing VLM is finally selected.", "section": "1 Introduction"}, {"figure_path": "01qa1ZJs65/figures/figures_4_1.jpg", "caption": "Figure 2: Validation Experiments on the Modality Gap and Capability Gap. (a) Predicted VLMs' zero-shot image classification accuracy based on generated text data vs. VLM's true accuracy based on test images. Each point in the graph represents a model. From the result, we can find that the predicted accuracy poorly aligns with the true accuracy, indicating these text data are ineffective substitutes for image data. (b) We calculate the zero-shot image classification performance rankings of 43 VLMs across 23 datasets. We compute the average standard deviations and the mean value of differences between each VLM's maximum and minimum ranking. The result shows the performance of a VLM varies greatly across different datasets.", "description": "This figure presents validation experiments demonstrating the challenges of Language-Only VLM Selection (LOVM).  Panel (a) shows a scatter plot comparing predicted and actual zero-shot image classification accuracy using generated text as a proxy for images. The low correlation highlights the \"\"Modality Gap\"\" \u2013 text is a poor substitute for images. Panel (b) illustrates the \"\"Capability Gap\"\" by showing the variability of VLM performance across different datasets.  The large standard deviation and range of performance differences emphasize that a VLM's overall performance is not a reliable indicator of its performance on a specific dataset.", "section": "2.3 Analysis of the Two Gaps in LOVM"}, {"figure_path": "01qa1ZJs65/figures/figures_5_1.jpg", "caption": "Figure 3: The workflow of SWAB. SWAB first constructs a transport matrix \u03b3* \u2208 Rks\u00d7kt using optimal transport, based on textual semantic similarity between classes in the open-source datasets Cs = {c1,\u2026, cks} and the target dataset's classes CT = {c1,\u2026, ckt}. Using this matrix, SWAB estimates VLM fm's class-specific gap vectors {\u011dm,1,\u2026} on the target dataset T from the gap vectors Gm \u2208 Rks\u00d7d in the open-source datasets. These estimated gap vectors help modify text data to act as more effective substitutes for image data. The modified text data will then be input into the Ranker Model fR, which predicts VLM's performance rT,(1)m on the target dataset. Besides, SWAB also uses the transport matrix \u03b3* to predict VLM's performance ranking on the target dataset based on VLM's class-specific rankings rs \u2208 Rks on open-source datasets. Finally, SWAB combines these two ranking predictions rT,(1)m and rT,(2)m to determine the VLM's final ranking prediction.", "description": "This figure illustrates the workflow of the proposed VLM Selection With gap Bridging (SWAB) method.  SWAB leverages optimal transport to create a bridge matrix based on textual similarity between classes in open-source and target datasets. This matrix is used in two key steps: 1) bridging the modality gap by transferring gap vectors (differences between image and text embeddings) from open-source to target datasets, thus improving the use of text as image proxies; and 2) bridging the capability gap by transferring ranking information from open-source to target datasets, improving the prediction of VLM performance on the target dataset. Finally, SWAB combines these two predictions to generate a final ranking of VLMs.", "section": "3 VLM Selection with Gap Bridging"}, {"figure_path": "01qa1ZJs65/figures/figures_20_1.jpg", "caption": "Figure 2: Validation Experiments on the Modality Gap and Capability Gap. (a) Predicted VLMs' zero-shot image classification accuracy based on generated text data vs. VLM's true accuracy based on test images. Each point in the graph represents a model. From the result, we can find that the predicted accuracy poorly aligns with the true accuracy, indicating these text data are ineffective substitutes for image data. (b) We calculate the zero-shot image classification performance rankings of 43 VLMs across 23 datasets. We compute the average standard deviations and the mean value of differences between each VLM's maximum and minimum ranking. The result shows the performance of a VLM varies greatly across different datasets.", "description": "This figure presents validation experiments to demonstrate the presence of two inherent challenges in Language-Only VLM Selection: Modality Gap and Capability Gap. The left subfigure (a) shows a comparison between predicted and actual zero-shot image classification accuracy using generated text data, revealing the poor alignment and highlighting the ineffectiveness of text data as image substitutes due to the Modality Gap. The right subfigure (b) illustrates the Capability Gap by showing the significant variation in VLM performance rankings across different datasets, demonstrating the unreliability of predicting dataset-specific performance from overall performance.", "section": "2.3 Analysis of the Two Gaps in LOVM"}, {"figure_path": "01qa1ZJs65/figures/figures_20_2.jpg", "caption": "Figure 2: Validation Experiments on the Modality Gap and Capability Gap. (a) Predicted VLMs' zero-shot image classification accuracy based on generated text data vs. VLM's true accuracy based on test images. Each point in the graph represents a model. From the result, we can find that the predicted accuracy poorly aligns with the true accuracy, indicating these text data are ineffective substitutes for image data. (b) We calculate the zero-shot image classification performance rankings of 43 VLMs across 23 datasets. We compute the average standard deviations and the mean value of differences between each VLM's maximum and minimum ranking. The result shows the performance of a VLM varies greatly across different datasets.", "description": "This figure presents validation experiments demonstrating the challenges in Language-Only VLM Selection (LOVM).  (a) shows a comparison of predicted vs. actual zero-shot image classification accuracy using generated text data as image proxies, revealing a significant discrepancy due to the \"Modality Gap\". (b) illustrates the \"Capability Gap\" by showing the inconsistent performance rankings of VLMs across different datasets, highlighting the difficulty in selecting a VLM based solely on general performance.", "section": "2 Preliminary"}, {"figure_path": "01qa1ZJs65/figures/figures_21_1.jpg", "caption": "Figure 6: UMAP visualization of image sample features and text sample features from different BEIT-3 and BLIP models.", "description": "This figure shows the results of applying UMAP dimensionality reduction to image and text features extracted from various BEIT-3 and BLIP models.  The plots visualize the separation or clustering of these features in a low-dimensional space, providing insights into the degree of modality gap between the image and text modalities for different models.  Distinct clusters indicate a significant modality gap, where image and text features are not well-aligned in the model's embedding space. Overlapping clusters suggest a smaller modality gap. The analysis helps to understand how well the models integrate image and text information and how this integration might affect their performance in zero-shot image classification tasks.", "section": "E More Experiment Results"}, {"figure_path": "01qa1ZJs65/figures/figures_21_2.jpg", "caption": "Figure 2: Validation Experiments on the Modality Gap and Capability Gap. (a) Predicted VLMs' zero-shot image classification accuracy based on generated text data vs. VLM's true accuracy based on test images. Each point in the graph represents a model. From the result, we can find that the predicted accuracy poorly aligns with the true accuracy, indicating these text data are ineffective substitutes for image data. (b) We calculate the zero-shot image classification performance rankings of 43 VLMs across 23 datasets. We compute the average standard deviations and the mean value of differences between each VLM's maximum and minimum ranking. The result shows the performance of a VLM varies greatly across different datasets.", "description": "This figure presents validation experiments demonstrating two key challenges in Language-Only VLM Selection (LOVM): the Modality Gap and the Capability Gap.  Subfigure (a) shows a scatter plot comparing predicted VLM accuracy (using generated text data as image proxies) against the true accuracy (using actual images).  The significant discrepancy highlights the ineffectiveness of using text alone. Subfigure (b) illustrates the Capability Gap by showing the wide variation in VLM performance across different datasets. The average standard deviation and the range of performance rankings for each VLM emphasize the inconsistency of a VLM's general performance compared to its performance on a specific target dataset.", "section": "2.3 Analysis of the Two Gaps in LOVM"}]