{"references": [{"fullname_first_author": "J.-B. Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-01", "reason": "This paper introduces Flamingo, a visual language model that is foundational to the DeepStack approach, demonstrating a key advance in multimodal learning."}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "CLIP, introduced in this paper, is the basis for visual token generation in DeepStack, providing a critical component of the model's architecture."}, {"fullname_first_author": "J. Li", "paper_title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-01-01", "reason": "BLIP provides a critical comparative model for DeepStack, showcasing a competing method for multimodal understanding that DeepStack aims to surpass."}, {"fullname_first_author": "H. Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-12-01", "reason": "LLaVA, presented in this paper, serves as the base LLM for DeepStack's experiments, providing a strong benchmark for comparison."}, {"fullname_first_author": "J. Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-01-01", "reason": "BLIP-2 offers another key comparative model, highlighting advancements in multimodal learning that DeepStack builds upon and improves."}]}