[{"figure_path": "fXDpDzHTDV/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with other LMMs on 9 benchmarks. Eff. Res. indicates the effective image resolution taken by each method. Vis. Tok. indicates the number of visual tokens used for LLMs (not only for the input layers); Cxt. Len. indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the Vis. Tok. = Cxt. Len. all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. \u2020 indicates that our model is fine-tuned from LLaVA-Next. * The training images of the datasets are observed during training. \u2260 denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)", "description": "This table compares the performance of DeepStack with other Large Multimodal Models (LMMs) on nine benchmark datasets.  It shows the effective resolution, number of visual tokens, context length, and performance metrics (VQAv2, GQA, TextVQA, DocVQA, InfoVQA, SEED, POPE, MM-MU, MM-Vet) for various models.  Noteworthy is the comparison of DeepStack with different configurations (DeepStack-V, DeepStack-L, DeepStack-L-HD) and the impact of finetuning the vision encoder.", "section": "4.2 Quantitive Results"}, {"figure_path": "fXDpDzHTDV/tables/tables_7_1.jpg", "caption": "Table 1: Comparison with other LMMs on 9 benchmarks. Eff. Res. indicates the effective image resolution taken by each method. Vis. Tok. indicates the number of visual tokens used for LLMs (not only for the input layers); Cxt. Len. indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the Vis. Tok. = Cxt. Len. all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. \u2020 indicates that our model is fine-tuned from LLaVA-Next. * The training images of the datasets are observed during training. \u2260 denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)", "description": "This table compares the performance of DeepStack with other Large Multimodal Models (LMMs) across nine benchmarks.  It details the effective image resolution, number of visual tokens, context length, and performance metrics (VQAv2, GQA, SEED, POPE, MM-MU, MM-Vet, TextVQA, DocVQA, InfoVQA) for various models, highlighting the improvements achieved by DeepStack.  Key differences in training data and vision encoder fine-tuning are also noted.", "section": "4.2 Quantitive Results"}, {"figure_path": "fXDpDzHTDV/tables/tables_7_2.jpg", "caption": "Table 3: Zero-shot evaluation on Video QA benchmarks. We collate 6 frames uniformly sampled from each video into 2 \u00d7 3 grid and resize the resulting image to square. Our model clearly outperforms the baseline because more visual information is included with the same context length. We mark the best performance bold.", "description": "This table presents the zero-shot performance of LLaVA-1.5-7B and DeepStack-L-7B on video question answering (VQA) benchmarks.  The benchmarks are categorized into multi-choice and open-ended VQA tasks. For each benchmark, the table shows the accuracy (Acc) and score achieved by each model. The results demonstrate that DeepStack-L-7B outperforms LLaVA-1.5-7B on most benchmarks, highlighting its ability to effectively handle video data.", "section": "4.2 Quantitive Results"}, {"figure_path": "fXDpDzHTDV/tables/tables_8_1.jpg", "caption": "Table 1: Comparison with other LMMs on 9 benchmarks. Eff. Res. indicates the effective image resolution taken by each method. Vis. Tok. indicates the number of visual tokens used for LLMs (not only for the input layers); Cxt. Len. indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the Vis. Tok. = Cxt. Len. all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. \u2020 indicates that our model is fine-tuned from LLaVA-Next. * The training images of the datasets are observed during training. \u2260 denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)", "description": "This table compares the performance of DeepStack with other Large Multimodal Models (LMMs) across nine benchmarks.  It shows various metrics including effective resolution, number of visual tokens, context length, and performance scores on several tasks (VQAv2, GQA, SEED, POPE, MM-MU, MM-Vet, TextVQA, DocVQA, and InfoVQA).  Key differences in training data and model configurations are also noted.", "section": "4.2 Quantitive Results"}, {"figure_path": "fXDpDzHTDV/tables/tables_8_2.jpg", "caption": "Table 1: Comparison with other LMMs on 9 benchmarks. Eff. Res. indicates the effective image resolution taken by each method. Vis. Tok. indicates the number of visual tokens used for LLMs (not only for the input layers); Cxt. Len. indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the Vis. Tok. = Cxt. Len. all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. \u2020 indicates that our model is fine-tuned from LLaVA-Next. * The training images of the datasets are observed during training. \u2260 denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)", "description": "This table compares the performance of DeepStack with other Large Multimodal Models (LMMs) across nine benchmarks.  It shows the effective image resolution, number of visual tokens, context length, and performance metrics (e.g., VQAv2, GQA, TextVQA) for each model.  The table highlights DeepStack's ability to handle significantly more visual tokens while maintaining comparable or superior performance to other methods, especially when considering context length.", "section": "4.2 Quantitive Results"}, {"figure_path": "fXDpDzHTDV/tables/tables_9_1.jpg", "caption": "Table 6: Ablations on high-resolution visual tokens for stacking. Dummy refers to repeating the original visual tokens for stacking; Hi-Res is our default setting that uses high-resolution visual tokens for stacking.", "description": "This table presents ablation studies on the impact of using high-resolution visual tokens versus dummy (repeated original) tokens within the DeepStack framework.  It compares the performance across various benchmarks (GQA, POPE, SEED, TextVQA, DocVQA, ChartQA, InfoVQA) when using either dummy tokens or high-resolution tokens for stacking in the DeepStack method. The results demonstrate that utilizing high-resolution tokens significantly improves the performance compared to using dummy tokens.", "section": "4.3 Model Inspection"}, {"figure_path": "fXDpDzHTDV/tables/tables_9_2.jpg", "caption": "Table 1: Comparison with other LMMs on 9 benchmarks. Eff. Res. indicates the effective image resolution taken by each method. Vis. Tok. indicates the number of visual tokens used for LLMs (not only for the input layers); Cxt. Len. indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the Vis. Tok. = Cxt. Len. all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. \u2020 indicates that our model is fine-tuned from LLaVA-Next. * The training images of the datasets are observed during training. \u2260 denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)", "description": "This table compares the performance of DeepStack with other Large Multimodal Models (LMMs) across nine benchmarks.  It shows the effective image resolution, the number of visual tokens, context length, and performance metrics (e.g., VQAv2, GQA, TextVQA) for each model.  Key differences in training data and vision encoder fine-tuning are also noted. The table highlights DeepStack's superior performance, especially when using a shorter context length.", "section": "4.2 Quantitive Results"}, {"figure_path": "fXDpDzHTDV/tables/tables_9_3.jpg", "caption": "Table 1: Comparison with other LMMs on 9 benchmarks. Eff. Res. indicates the effective image resolution taken by each method. Vis. Tok. indicates the number of visual tokens used for LLMs (not only for the input layers); Cxt. Len. indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the Vis. Tok. = Cxt. Len. all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. \u2020 indicates that our model is fine-tuned from LLaVA-Next. * The training images of the datasets are observed during training. \u2260 denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)", "description": "This table compares the performance of DeepStack with other Large Multimodal Models (LMMs) across nine benchmark datasets.  It shows the effective resolution, number of visual tokens, and context length used by each model.  Key performance metrics are presented for each model on each benchmark, highlighting DeepStack's improvements, particularly when using a shorter context length.  The table also notes differences in fine-tuning datasets and vision encoder freezing/unfreezing.", "section": "4.2 Quantitive Results"}, {"figure_path": "fXDpDzHTDV/tables/tables_15_1.jpg", "caption": "Table 1: Comparison with other LMMs on 9 benchmarks. Eff. Res. indicates the effective image resolution taken by each method; Vis. Tok. indicates the number of visual tokens used for LLMs (not only for the input layers); Cxt. Len. indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the Vis. Tok. = Cxt. Len. all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. \u2020 indicates that our model is fine-tuned from LLaVA-Next. * The training images of the datasets are observed during training. \u2260 denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)", "description": "This table compares the performance of DeepStack with other Large Multimodal Models (LMMs) across nine benchmarks.  It shows the effective resolution, number of visual tokens, context length, pre-training data size, supervised fine-tuning data size, and performance on various VQA and LMM tasks.  The table highlights DeepStack's improved performance, particularly with fewer visual tokens, showcasing the effectiveness of the proposed approach.", "section": "4.2 Quantitive Results"}, {"figure_path": "fXDpDzHTDV/tables/tables_15_2.jpg", "caption": "Table 1: Comparison with other LMMs on 9 benchmarks. Eff. Res. indicates the effective image resolution taken by each method; Vis. Tok. indicates the number of visual tokens used for LLMs (not only for the input layers); Cxt. Len. indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the Vis. Tok. = Cxt. Len. all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. \u2020 indicates that our model is fine-tuned from LLaVA-Next. * The training images of the datasets are observed during training. \u2260 denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)", "description": "This table compares the performance of DeepStack with other Large Multimodal Models (LMMs) on nine benchmark datasets.  It shows the effective resolution, number of visual tokens, context length, pre-training data size, and supervised fine-tuning data size for each model.  Key performance metrics across various VQA and LMM benchmarks are presented, highlighting DeepStack's superior performance, especially when using a shorter context length. Note that some differences in training data and fine-tuning between DeepStack and other methods exist.", "section": "4.2 Quantitive Results"}, {"figure_path": "fXDpDzHTDV/tables/tables_15_3.jpg", "caption": "Table 1: Comparison with other LMMs on 9 benchmarks. Eff. Res. indicates the effective image resolution taken by each method. Vis. Tok. indicates the number of visual tokens used for LLMs (not only for the input layers); Cxt. Len. indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the Vis. Tok. = Cxt. Len. all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. \u2020 indicates that our model is fine-tuned from LLaVA-Next. * The training images of the datasets are observed during training. \u2260 denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)", "description": "This table compares the performance of DeepStack with other Large Multimodal Models (LMMs) across nine benchmark datasets.  It shows metrics such as effective image resolution, number of visual tokens, context length, and performance scores on various VQA and LMM tasks.  The table highlights DeepStack's improved performance, especially when considering the context length.", "section": "4.2 Quantitive Results"}]