[{"type": "text", "text": "A Functional Extension of Semi-Structured Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "David R\u00fcgamer Department of Statistics, LMU Munich Munich Center for Machine Learning (MCML) Munich, Germany david@stat.uni-muenchen.de ", "page_idx": 0}, {"type": "text", "text": "Bernard X.W. Liew, Zainab Altai School of Sport, Rehabilitation and Exercise Sciences University of Essex Colchester, UK [bl19622,z.altai]@essex.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Almond St\u00f6cker Institute of Mathematics \u00c9cole Polytechnic F\u00e9d\u00e9ral de Lausanne (EPFL) Lausanne, Switzerland almond.stoecker@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Semi-structured networks (SSNs) merge the structures familiar from additive models with deep neural networks, allowing the modeling of interpretable partial feature effects while capturing higher-order non-linearities at the same time. A significant challenge in this integration is maintaining the interpretability of the additive model component. Inspired by large-scale biomechanics datasets, this paper explores extending SSNs to functional data. Existing methods in functional data analysis are promising but often not expressive enough to account for all interactions and non-linearities and do not scale well to large datasets. Although the SSN approach presents a compelling potential solution, its adaptation to functional data remains complex. In this work, we propose a functional SSN method that retains the advantageous properties of classical functional regression approaches while also improving scalability. Our numerical experiments demonstrate that this approach accurately recovers underlying signals, enhances predictive performance, and performs favorably compared to competing methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Incorporating additive structures in neural networks to enhance interpretability has been a major theme of recent advances in deep learning. For example, neural additive models (NAMs; [1]) allow users to learn basis functions in an automatic, data-driven fashion using feature subnetworks and thereby provide an alternative modeling option to basis function approaches from statistics such as generalized additive models (GAMs; [17]). Various extensions and related proposals have been published in recent years. Notable examples include the joint basis function learning by [41] or the combination of GAMs and neural oblivious decision ensembles [8]. When combining structural assumptions with arbitrary deep architectures, the resulting network is often referred to as wideand-deep [10] or semi-structured network (SSN; [49, 52]). Various extensions of SSNs have been proposed in recent years, including SSNs for survival analysis [24, 25], ordinal data fitting [22], for uncertainty quantification [2, 12], Bayesian SSNs [11], or SSNs incorporated into normalizing flows [23]. One of the key challenges in SSNs is to ensure the identifiability of the additive model part in the presence of a deep network predictor in order to maintain interpretability. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, motivated by a large-scale biomechanical application, we study how to efficiently transfer these two concepts \u2014 the embedding of basis functions into neural networks and the extension to SSNs \u2014 for functional data analysis. ", "page_idx": 1}, {"type": "text", "text": "Functional data analysis Functional data analysis (FDA; [42]) is a research field of increasing significance in statistics and machine learning (e.g., [3, 37, 38, 57]) that extends existing modeling approaches to address the functional nature of some data types (e.g., sensor signals, waves, or growth curves). One well-known extension is the class of functional additive models [15, 53], which represent the functional analog of GAMs. These models adopt the classical regression model point of view but allow for both in- and outputs to be (discretized) functions. Functional additive models, however, are often not expressive enough to account for all interactions and non-linearities and do not scale well to large datasets. ", "page_idx": 1}, {"type": "text", "text": "Biomechanics A prominent example of a research field dealing with functional data is biomechanics. As in many other applications concerned with physical or biological signals, the integration of machine learning in biomechanics research offers many advantages (see, e.g., [26, 29, 30]). The research on joint moments in biomechanics, for example, provides insight into muscular activities, motor control strategies, chronic pain, and injury risks (see, e.g., [13, 31, 36, 47, 56]). By combining kinematic features with machine and deep learning to predict joint moments, researchers can bypass the need to gather high-quality biomechanics data and instead use \u201ccheap\u201d sensor data obtained through, e.g., everyday mobile devices which in turn allow predicting the \u201cexpensive\u201d signals that can only be obtained in laboratory setups [20, 32, 33]. While a promising research direction, existing methods face various challenges including a lack of generalization and a missing clear understanding of the relationship learned. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work in functional data analysis ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In FDA, there has been a successful translation of many different methods for scalar data to functionvalued data, including functional regression models and functional machine learning techniques. ", "page_idx": 1}, {"type": "text", "text": "Functional regression models Regression models with scalar outputs and functional inputs are called scalar-on-function regression models. An overview of such methods can be found in [45]. Models with functional output and scalar inputs are called function-on-scalar models [9, 15]. They can be applied when the outcome or error function is assumed to be repeated realizations of a stochastic process. Combining these two approaches yields the function-on-function regression model with functions as input and output [see, e.g., 35, 53]. We provide a more technical description of function-on-function regression is given in Section 2.1. ", "page_idx": 1}, {"type": "text", "text": "Functional machine learning In recent years, several machine learning approaches for functional data have been proposed. One is Gaussian process functional regression [GPFR; 21, 54] which combines mean estimation and a multivariate Gaussian process for covariance estimation. As for classical Gaussian process regression, the GPFR suffers from scalability issues. Gradient boosting approaches for functional regression models have been proposed by [4, 6]. These approaches result in sparse and interpretable models, rendering them especially meaningful in high dimensions, but lack expressivity provided by neural network approaches. Pioneered by the functional multi-layer perceptron (MLP) of [48], various researchers have suggested different functional neural networks (FNNs) [16, 44, 55], including MLPs with functional outcome (cf. Section 2.2 for technical details). A recent overview is given in [59]. Existing approaches are similar in that they learn a type of embedding to represent functions in a space where classical computations can be applied (cf. Fig. 2(a)). ", "page_idx": 1}, {"type": "text", "text": "1.2 Current limitation and our contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "From the related literature, we can identify the most relevant methods suitable for our modeling challenge. Neural network approaches such as the functional MLP provide the most flexible class for functional data, whereas additive models and additive boosting approaches such as [6, 53] yield interpretable models. An additive combination of these efforts to obtain both an interpretable model part and the possibility of making the model more flexible is hence an attractive option. ", "page_idx": 1}, {"type": "text", "text": "Open challenges While semi-structured models are suited for a combination of interpretable additive structures and deep neural networks, existing approaches cannot simply be transferred for application with functional data. Due to the structure and implementation of SSNs, a na\u00efve implementation would neither be scalable to large-scale datasets nor is it clear how to incorporate identifiability constraints to ensure interpretability of the structured functional regression part in the presence of a deep neural network. ", "page_idx": 2}, {"type": "text", "text": "Our contributions In this work, we address these limitations and propose an SSN approach for functional data. Our method is not only the first semi-structured modeling approach in the domain of functional data, but also presents a more scalable and flexible alternative to existing functional regression approaches. In order to preserve the original properties of functional regression models, we further suggest an orthogonalization routine to solve the accompanied identifiability issue. ", "page_idx": 2}, {"type": "text", "text": "2 Notation and background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We assume functional inputs are given by $J\\ge1$ second-order, zero-mean stochastic processes $X_{j}$ with square integrable realizations $x_{j}:S_{j}\\rightarrow\\mathbb{R}$ , i.e. with $x_{j}\\in L^{2}(S_{j}),j=1,\\ldots,J$ on real intervals ${\\mathbf{}}S_{j}$ . Similarly, a functional outcome is given by a suitable stochastic process $Y$ over a compact set $\\tau\\subset\\mathbb{R}$ with realizations $y:\\tau\\to\\mathbb{R}$ , $y\\,\\in\\,L^{2}(\\mathcal{T})=:\\mathcal{G}$ . For the functional inputs, we simplify notation by combining all (random) input functions into a vector $X(\\pmb{\\mathscr{s}})=(X_{1}(s_{1}),\\ldots,X_{J}(s_{J}))$ with realization $\\boldsymbol{x}(s)\\overset{\\bullet}{\\in}\\mathbb{R}^{J}$ , where $\\pmb{\\bar{s}}^{\\prime}:=\\ (s_{1},\\ldots,s_{J})\\ \\in\\ \\pmb{S}\\ \\subset\\ \\mathbb{R}^{J}$ is a $J$ -tuple from domain $S:=S_{1}\\times\\cdots\\times S_{J}$ , and $\\mathcal{H}:=L^{2}(S_{1})\\times\\cdots\\times L^{2}(S_{J})$ . ", "page_idx": 2}, {"type": "text", "text": "Next, we introduce function-on-function regression (FFR) and establish a link to FNNs to motivate our own approach. ", "page_idx": 2}, {"type": "text", "text": "2.1 Function-on-function regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "An FFR for the expected outcome $\\mu(t):=\\mathbb{E}(Y(t)|X)$ of $Y(t),t\\in\\mathcal{T}$ using inputs $X$ can be defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}(Y(t)|X=x)=\\tau\\left(b(t)+\\sum_{j=1}^{J}\\int_{S_{j}}w_{j}(s,t)x_{j}(s)d s\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $b(t)$ is a functional intercept/bias and the $w_{j}(s,t)$ are weight surfaces describing the influence of the $j$ th functional predictor at time point $s\\in S_{j}$ on the functional outcome at time point $t\\in\\mathcal T$ . $\\tau$ is a point-wise transformation function, mapping the affine transformation of functional predictors to a suitable domain (e.g., $\\tau(\\cdot)\\,=\\,\\exp(\\cdot)$ to obtain positive values in case $Y$ is a count process). Various extensions of the FFR model in (1) exist. Examples include time-varying integration limits or changing the linear influence of $x(t)$ to a non-linear mapping. ", "page_idx": 2}, {"type": "text", "text": "Example Figure 1 shows an example of a function-on-function regression by visualizing the corresponding learned weight surface, with three highlighted areas: a) An isolated positive weight multiplied with a positive feature signal at $x(90)$ induces a small spike at $y(10)$ . b) A more extensive negative weight multiplied with mostly positive feature values for $\\bar{s^{\\prime}}\\in\\,[1,20]$ and integrated over all time points results in a bell-shaped negative outcome signal at around $t=30$ . c) A large but faded positive weight region multiplied with positive feature values results in a slight increase in the response function in the range $t\\in[50,100]$ . In most applications of FFR, the goal is to find these salient areas in the weight surface to better understand the relationship between input and output signal. ", "page_idx": 2}, {"type": "image", "img_path": "WJAiaslhin/tmp/58704c1a94ad8b49b4c508da5cbb15e3ca78e6173e607bdeec0e1da77acbb371.jpg", "img_caption": ["Figure 1: Exemplary weight surface (center), feature signal (bottom), and the resulting response signal (left) when integrating $\\textstyle\\int x(s)w(s,t)d{\\bar{s}}$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.2 Functional neural networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To establish the connection between FFR and FNNs, it is instructive to consider a function-on-function MLP (FFMLP). In its basic form, an MLP for scalar values consists of neurons $h:\\mathbb{R}\\to\\mathbb{R},x\\mapsto$ $\\tau(b+w^{\\top}x)$ arranged in multiple layers, each layer stacked one on top of the other. The extension to a fully functional $L$ -layer MLP can be defined recursively by the $k$ th output neuron $h_{k}^{(l)}$ of a functional layer $l=1,\\dots,L$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{k}^{(l)}(t)=\\tau^{(l)}\\left(b_{k}^{(l)}(t)+\\sum_{m=1}^{M_{l-1}}\\int w_{m,k}^{(l)}(s,t)h_{m}^{(l-1)}(s)d s\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $M_{l}$ denotes the number of neurons in layer $l$ , $b_{k}^{(l)}\\in L^{2}(\\mathcal{U}_{m}^{(l)})$ and $w_{m,k}^{(l)}\\in L^{2}(\\mathcal{U}_{m}^{(l-1)}\\times\\mathcal{U}_{m}^{(l)})$ for some functional domains $\\mathcal{U}_{m}^{(l)}$ , input layer $h_{m}^{(0)}(\\cdot)=x_{m}(\\cdot)$ for all predictors $m=1,\\hdots,J$ , and last layer with $M_{L}=1$ neuron $h^{(L)}(t)=\\mathbb{E}(Y(t)|X)$ . ", "page_idx": 3}, {"type": "text", "text": "3 Semi-structured functional networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our approach generalizes the previous models by considering a general neural network $\\Lambda:{\\mathcal{H}}\\to{\\mathcal{G}}$ that models the input-output mapping from the set of functional features $X$ to the expected outcome $\\mu(t)$ of the functional response for $t\\in\\mathcal T$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu(t)=\\Lambda(X)(t)=\\tau\\left(\\lambda^{+}(X)(t)+\\lambda^{-}(X)(t)\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "comprising an FFR part $\\lambda^{+}(X)(t)$ and a deeper FNN architecture $\\lambda^{-}(X)(t)$ , which are added and transformed using an activation function $\\tau$ . This combination can also be thought of as a functional version of a residual connection. In the following, we drop the functional input arguments of $\\lambda^{+}$ and $\\lambda^{-}$ for better readability. The model in (3) combines structured interpretable models as described in Section 2.1 with an arbitrary deep network such as the one in Section 2.2. In particular, this allows for improving the performance of a simple FFR while retaining as much interpretability as possible (cf. Section 3.3). ", "page_idx": 3}, {"type": "text", "text": "Interpretable model part As in (1), we model the interpretable part $\\begin{array}{r}{\\lambda^{+}(t)=\\sum_{j=0}^{J}\\lambda_{j}^{+}(t)}\\end{array}$ as a sum of linear functional terms $\\begin{array}{r}{\\lambda_{j}^{+}(t)=\\int_{S_{j}}w_{j}(s,t)x_{j}(s)d s}\\end{array}$ . To make our model more concrete, we can expand each weight surface $w_{j}$ in a finite-dimensional tensor-product basis as ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{j}(s,t)=\\psi(t)^{\\top}\\Theta_{j}\\phi_{j}(s)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "over fixed function bases $\\phi_{j}\\,=\\,\\{\\phi_{j k}\\}_{k=1}^{K_{j}}$ , $\\phi_{j k}\\,\\in\\,L^{2}(S_{j})$ and $\\pmb{\\psi}(t)\\,=\\,\\{\\psi_{u}\\}_{u=1}^{U}$ , $\\psi_{u}\\,\\in\\,L^{2}(\\mathcal{T})$ with weight matrix $\\Theta_{j}\\,\\in\\,\\mathbb{R}^{U}\\,\\times\\,\\mathbb{R}^{K_{j}}$ . Analogously, we can represent the intercept as $\\lambda_{0}^{+}(t)\\,=$ $b(t)\\;=\\;\\pmb{\\psi}(t)^{\\top}\\pmb{\\Theta}_{0}$ . We may understand the model\u2019s interpretable part for the $j$ th feature as a functional encoder $\\phi_{j}^{\\ast}(X_{j})\\ =\\ \\{\\phi_{j k}^{\\ast}(X_{j})\\ =\\ \\int\\phi_{j k}(s)X_{j}(s)d s\\}_{k=1}^{K_{j}}$ , encoding $X_{j}$ into a latent variable $\\boldsymbol{z}_{j}\\in\\mathbb{R}^{K_{j}}$ , and a linear decoder $\\psi$ with weights $\\Theta_{j}$ mapping $z_{j}$ to the function $\\lambda_{j}^{+}(t)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{j}\\overset{\\phi_{j}^{*}}{\\mapsto}z_{j}\\overset{\\Theta_{j}\\psi}{\\mapsto}\\lambda_{j}^{+}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\phi_{j}^{\\ast}$ presents the dual basis to $\\phi_{j}$ . Visually, the weight surface $w_{j}$ in (4) can be interpreted as exemplarily shown in Figure 1. ", "page_idx": 3}, {"type": "text", "text": "Deep model part The part $\\lambda^{-}(t)$ in (3) is a placeholder for a more complex network. For FNNs, a conventional neural network might be applied after encoding $X$ with $\\phi^{*}$ and before decoding its results with $\\psi$ , such that en- and decoding are shared with the interpretable part $\\lambda^{+}$ as depicted in Fig. 2a. Alternatively, a general FFMLP as described in Section 2.2 can be embedded into the approach in Fig. 2a. As researchers have often already found well-working architectures for their data, a practically more realistic architecture for semi-structured FNNs is to allow for a more generic deep model as depicted in Fig. 2b. This is also the case in our application on biomechanical sensor data in Section 4.2. Here, we choose $\\lambda^{-}(t)$ to be a specific InceptionTime network architecture [19] that is well-established in the field and train its weights along with the weights $\\Theta$ of $\\lambda^{+}(t)$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Implementation for discretized features ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As functional predictors are usually only observed on a discretized grid on the domains ${\\mathcal{S}}_{j}$ , we now derive a way to implement $\\lambda^{+}(t)$ in practice. Depending on the architecture used for the deep part, we might proceed analogously with $\\lambda^{-}(t)$ . Let $\\boldsymbol{x}_{j}^{(i)}(\\boldsymbol{s}_{r})\\in\\mathbb{R}$ be the evaluation of the ith realization x(ji)of Xj at time points sr, r = 1, . . . , R, stacked into a vector xj \u2208RR. For better readability, we assume that these time points are equal across all $J$ features. With only discrete evaluations available, we approximate integrals over ${\\mathcal{S}}_{j}$ numerically with integration weights $\\Delta{}(s_{r})$ , e.g., using trapezoidal Riemann weights. Given the tensor-product basis representation of the weight surface $w_{j}(s,t)$ in (4), we effectively evaluate $\\phi_{j}(s_{r})$ for all $R$ time points, yielding $\\Phi_{j}=[\\phi_{j}(s_{1}),\\dots,\\phi_{j}(s_{R})]\\in\\mathbb{R}^{K_{j}\\times R}$ , and obtain the row-vector $\\Phi_{j}^{*}$ of approximate functionals ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{j k}^{\\ast}(x_{j})=\\int\\phi_{j k}(s)x_{j}(s)d s\\approx\\sum_{r=1}^{R}\\Delta_{j}(s_{r})\\phi_{j k}(s_{r})x_{j}(s_{r})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\Phi}_{j}^{*}=(\\pmb{\\Delta}_{j}\\circ\\pmb{x}_{j})\\pmb{\\Phi}_{j}^{\\top}\\in\\mathbb{R}^{1\\times K_{j}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\circ$ denotes the Hadamard product and $\\mathbf{\\Delta}_{\\Delta_{j}}\\,=\\,[\\Delta_{j}(s_{1}),\\hdots,\\Delta_{j}(s_{R})]^{\\intercal}$ . Putting everything together, we can represent the $j$ th interpretable functional model term $\\lambda_{j}^{+}(t)$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{j}^{+}(t)=\\int_{S_{j}}x_{j}(s)w_{j}(s,t)d s\\approx\\Phi_{j}^{*}\\Theta_{j}\\psi(t).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This model part is still a function over the outcome domain $\\tau$ , but discretized over the predictor domains. We can now proceed with the optimization when the functional outcome is represented with finitely many observations. ", "page_idx": 4}, {"type": "text", "text": "3.2 Optimization for discretized outcomes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For the $i$ th observation, we can measure the goodness-of-fti of our model $\\mu(t)$ by taking the point-wise loss function $l(y^{(i)}(t),\\hat{\\mu}^{(i)}(t))$ of the $i$ th realized function $\\boldsymbol y^{(i)}$ and the predicted outcome $\\hat{\\mu}^{\\left(i\\right)}$ given $\\pmb{x}^{(i)}$ and integrate over the functional domain to obtain the $i$ th functional loss contribution $\\ell$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell(y^{(i)},\\hat{\\mu}^{(i)})=\\int_{\\mathcal{T}}l(y^{(i)}(t),\\hat{\\mu}^{(i)}(t))d t.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The overall objective, our empirical risk, is then given by the sum of (6) over all $n$ observations. In practice, integrals in (6) are not available in closed form in general and the functional outcome is only observed on a finite grid. Therefore, similar to the discretization of the weight surface, let $\\dot{y}^{(i)}(t_{q})$ be the observations of the outcome for time points $t_{q}\\in\\mathcal{T},q=1,\\ldots,Q$ , summarized for all time points as $\\pmb{y}^{(i)}=(y^{(i)}(t_{1}),\\dots,y^{(i)}(t_{Q}))$ . Given a dataset $\\mathcal{D}$ with $n$ observations of these functions, i.e., $\\mathcal{D}=\\{(\\pmb{x}^{(i)},\\pmb{y}^{(i)})\\}_{i=1,\\dots,n}$ , our overall objective then becomes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\sum_{i=1}^{n}\\sum_{q=1}^{Q}\\bar{\\Xi}(t_{q}){l}({y}^{(i)}(t_{q}),\\hat{\\mu}^{(i)}(t_{q})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Xi(\\cdot)$ are integration weights. ", "page_idx": 4}, {"type": "text", "text": "As it less efficient to represent $\\mu$ as an actual function of $t$ if $Y$ is only given for fixed observed time points $t_{q}$ , we can adapt (5) to predict a $Q$ -dimensional vector using ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{j}^{+}\\approx\\Phi_{j}^{*}\\Theta_{j}\\Psi\\in\\mathbb{R}^{1\\times Q}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "WJAiaslhin/tmp/7f5e45f0dc9c89e3d46a5c50a2959099a694a1f165591368ee438cdd9f36ddbf.jpg", "img_caption": ["(a) A semi-str. FNN with shared en- and decoding. (b) A semi-str. FNN with shared in- and output layer. ", "Figure 2: Different semi-structured architectures. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $\\Psi=[\\psi(t_{1}),\\ldots,\\psi(t_{Q})]$ . The deep network $\\lambda^{-}$ naturally yields the same-dimensional output when sharing the decoder. If a flexible deep model is used for $\\lambda^{-}$ as in Fig. 2b, $\\lambda^{-}$ can be defined with $Q$ output units to match the dimension of $\\lambda^{+}$ . For one observation, the objective (7) can now be written as a vector-based loss function of $\\pmb{y}^{(i)}\\in\\mathbb{R}^{1\\times Q}$ and $\\begin{array}{r}{\\pmb{\\mu}^{\\left(i\\right)}=\\sum_{j}\\pmb{\\lambda}_{j}^{+\\left(i\\right)}+\\pmb{\\lambda}^{-\\left(i\\right)}\\in\\mathbb{R}^{1\\times Q}}\\end{array}$ , or, when stacking these vectors, as loss function between two $n\\times Q$ matrices. ", "page_idx": 5}, {"type": "image", "img_path": "WJAiaslhin/tmp/6c4095825aab7d3010e7d1485190c78a014cfb0a8e55983b0ce2e4488708cf92.jpg", "img_caption": ["3.3 Post-hoc orthogonalization "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Estimated weight surfaces of the one functional shank gyroscope predictor in $\\lambda^{+}$ for the different joints (columns), before and after correcting with the orthogonalization (rows). The color of each surface corresponds to the weight a predictor sensor signal at time points $s$ ( $\\bf\\dot{x}$ -axis) is estimated to have on the tth time point $\\mathrm{~y~}$ -axis) of the outcome (cf. Fig. 1). Without correction (upper row), the interpretable model part is not only incorrectly estimated but no effect at all is attributed to it. After correction, distinct patterns for some of the time point combinations and joints are visible, e.g., suggesting that an increased gyroscope value for early time points $s$ has a negative influence (dark purple color) on the first half of the hip adduction moment (bottom row, second plot from the left). ", "page_idx": 5}, {"type": "text", "text": "Since $\\lambda^{-}$ is a potentially very expressive deep neural network that can also capture all functional linear dependencies modeled by $\\lambda^{+}$ , the interpretable model part of the semi-structured FNN is not identifiable without further constraints. Non-identifiability, in turn, means that we cannot simply interpret the weights of the interpretable model part as it is not clear how much these are diluted by the deep network part. This can, e.g., be seen in Figure 3 which shows selected weight surfaces from our biomechanical application in Section 4. In this case, the deep network (an InceptionNet) captures almost all linearity of the structured part (upper row) and only after correction do we obtain the actual weight surfaces. We achieve this by extending the post-hoc orthogonalization [PHO; 49] for functional in- and outputs. First, we rewrite $\\lambda^{+}(t)\\in\\overline{{\\mathbb{R}}}$ , the sum of all $J$ terms in (5), as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda^{+}(t)=\\operatorname{vec}(\\lambda^{+}(t))\\overset{(5)}{=}\\operatorname{vec}\\left(\\sum_{j=1}^{J}\\Phi_{j}^{*}\\Theta_{j}\\psi(t)\\right)=\\sum_{j=1}^{J}(\\psi(t)^{\\top}\\otimes\\Phi_{j}^{*})\\mathrm{vec}(\\Theta_{j})=:\\sum_{j=1}^{J}\\Omega_{j}(t)\\theta_{j}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\Omega_{j}(t)\\,=\\,(\\psi(t)^{\\top}\\,\\otimes\\,\\Phi_{j}^{*})\\,\\in\\,\\mathbb{R}^{1\\times K_{j}\\cdot U}$ , where $\\otimes$ denotes the Kronecker product, and $\\pmb\\theta_{j}\\ =$ $\\mathrm{vec}(\\Theta_{j})\\,\\in\\,\\mathbb{R}^{K_{j}\\cdot U}$ . This reformulation allows us to represent the functional interpretable model part as a linear combination $\\Omega(t)\\pmb{\\theta}=[\\Omega_{1}(t),\\ldots,\\Omega_{J}(t)][\\pmb{\\theta}_{1}^{\\top},\\ldots,\\pmb{\\theta}_{J}^{\\top}]^{\\top}$ of basis products and their coefficients, and thereby to apply PHO for semi-structured FNNs. Let $\\Omega_{\\{N\\}}$ and $\\lambda_{\\{N\\}}$ be the stacked matrices for the $N=n\\cdot Q$ data points in the training set $\\mathcal{D}$ after stacking the $\\Omega(t)$ and $\\lambda^{-}(t)$ for all $Q$ time points of all $n$ observed curves. Then we can obtain an identifiable and thus interpretable functional model part from the trained semi-structured FNN with basis matrix $\\Omega_{\\{N\\}}$ , parameters $\\pmb{\\theta}$ , and deep network part $\\lambda_{\\{N\\}}^{-}$ by computing the following: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{\\theta}}=\\pmb{\\theta}+\\Omega_{\\{N\\}}^{\\dagger}\\pmb{\\lambda}_{\\{N\\}}^{-}\\quad\\mathrm{and}\\quad\\pmb{\\lambda}_{\\{N\\}}^{\\perp}=\\mathcal{P}_{\\pmb{\\Omega}_{\\{N\\}}}^{\\perp}\\pmb{\\lambda}_{\\{N\\}}^{-},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\widetilde{\\pmb{\\theta}}$ is the corrected basis coefficient vector, $\\Omega_{\\{N\\}}^{\\dag}$ is the Moore-Penrose pseudoinverse of $\\Omega_{\\{N\\}}$ , $\\mathcal{P}_{\\Omega_{\\{N\\}}}^{\\perp}$ is a projection matrix projecting into the orthogonal complement spanned by columns of $\\Omega_{\\{N\\}}$ , and $\\lambda_{\\{N\\}}^{\\perp}$ are the corrected deep network predictions orthogonal to the interpretable part. Using $\\widetilde{\\pmb{\\theta}}_{j}$ from $\\pmb{\\widetilde{\\theta}}=[\\widetilde{\\pmb{\\theta}}_{1},\\dots,\\widetilde{\\pmb{\\theta}}_{J}]$ instead of the original $\\theta_{j}\\mathbf{s}$ will yield updated weights $\\widetilde{w}_{j}$ for the surfac es from ( 4) of the interp retable functional model part, which can be interpreted irre s pective of the presence of a deep network. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.4 Scalable implementation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "When implementing semi-structured FNNs, $\\lambda^{-}$ can be chosen arbitrarily to the needs of the data modeler. Hence, we do not explicitly elaborate on a particular option here and assume an efficient implementation of this part. Instead, we focus on a careful implementation of the structured model part to avoid unfavorable scaling as in other existing implementations (cf. Figure 4(b)). To mitigate this problem (in particular reducing the space complexity), we propose two implementation tricks. While these mainly reduce the complexity scaling w.r.t. $J$ and $R$ , implementing an FFR in a neural network already yields an improvement by allowing to cap the memory costs to a fixed amount through the use of mini-batch training. In contrast, a full-batch optimization routine of (7) as implemented in classic approaches (e.g., [5, 53]) scales both with $n$ and $Q$ as data is transformed in a long-format, which can grow particularly fast for sensor data. For example, only $n=1000$ observations of a functional response evaluated at $Q=1000$ time points already results in a data matrix of 1 million rows. Classic implementations also scale unfavorably in terms of the number of features $J$ and observed times points $R$ for these functions. In the case where all functional predictors are measured on the same scale, fitting an FFR model requires inverting a matrix of size $\\begin{array}{r}{\\bar{(n\\cdot Q)}\\times((\\sum_{j=1}^{J}K_{j})\\cdot U)}\\end{array}$ , which becomes infeasible for larger amounts of predictors. Even setting up and storing this matrix can be a bottleneck. ", "page_idx": 6}, {"type": "text", "text": "Array computation In contrast, when using the representation in (5), the basis matrices in $s-$ and $t$ -dimension (i.e. for in- and output) are never explicitly combined into a larger matrix, but set up individually, and a network forward-pass only requires their multiplication with the weight matrix $\\Theta$ . As the matrix $\\Psi$ is the same for all $J$ model terms, we can further recycle it for these computations. ", "page_idx": 6}, {"type": "text", "text": "Basis recycling In addition, if some or all predictors in $X$ share the same time domain and evaluation points $s_{1},\\ldots,s_{R}$ , we can save additional memory by not having to set up $J$ individual bases $\\Phi_{j}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Numerical experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the following, we show that our model is able to recover the true signal in simulation experiments and compares favorably to state-of-the-art methods both in simulated and real-world applications. As comparison methods, we use an additive model formulation of the FFR model as proposed in [53], an FFR model based on boosting [5] and a deep neural network without interpretable model part. For the latter and the deep part of the semi-structured model, we use a tuned InceptionTime [19] architecture from the biomechanics literature [32]. As FFR and boosting provide an automatic mechanism for smoothing, no additional tuning is required. In all our experiments, we use thin plate regression splines [60] but also obtained similar results with B-splines of order three and first-order difference penalties. ", "page_idx": 6}, {"type": "text", "text": "Hypotheses In our numerical experiments, we specifically investigate the following hypotheses: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Comparable performance with better scalability: For additive models without the deep part, our model can recover complex simulated function-on-function relationships (H1a) with similar estimation and prediction performance as other interpretable models (H1b) while scaling better than existing approaches (H1c). ", "page_idx": 6}, {"type": "text", "text": "\u2022 Favorable real-world properties: In real-world applications, our model is on par or better in prediction performance with current approaches (H2a), with a similar or better resemblance when generating output functions (H2b), and provides a much more meaningful interpretation than the deep MLP (H2c). Further, we conjecture that our approach is better in prediction performance than its two (structured, deep) individual components alone (H2d). ", "page_idx": 6}, {"type": "image", "img_path": "WJAiaslhin/tmp/f9fac76737504cf1559445559904185f610dd0db95bf0f5848f577cfd557c590.jpg", "img_caption": ["(a) True weight surface $w(s,t)$ used in the simulation study for large SNR along with estimation results of different methods. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "WJAiaslhin/tmp/71454ae65d7c675d80095b35c4dc857daea8b28f3886ec87c06bcec28073df8a.jpg", "img_caption": ["(b) Memory consumption of different methods (colors) for different amounts of functional observations $n$ (left), functional predictors $_J$ (center), and time points $R$ (right). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Simulation study results. ", "page_idx": 7}, {"type": "text", "text": "4.1 Simulation study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Prediction and estimation performance (H1a,b) For the model\u2019s estimation and prediction performance, we simulate a function-on-function relationship based on a complex weight relationship $w(s,t)$ between one functional predictor and a functional outcome. Figure 4(a) depicts this true weight along with an exemplary estimation by the different methods. The estimation performance of the weight surface and all methods\u2019 prediction performance over 30 different simulation runs each with a signal-to-noise ratio $\\mathrm{SNR}\\in\\{0.1,1\\}$ and $n\\in\\{320,640,1280\\}$ functional observations is summarized in Figure 6 in the Appendix. While the models perform on par in most cases, we find that the network works better for more noisy settings $(\\mathrm{SNR}=0.1]$ ) and is not as good as other methods for higher signal-to-noise ratio (as also shown in Figure 4(a)), but with negligible differences. Overall, there are only minor differences in both estimation and prediction performance. This result suggests that our method works just as well as the other methods for estimating FFR. ", "page_idx": 7}, {"type": "text", "text": "Scalability (H1c) To investigate the scalability of all methods, we subsample the data from the Carmago study presented in the following subsection by reducing observations $n$ , features $J$ , and/or the number of observed time points $R$ . More specifically, we investigate the memory consumption of the three implementations previously discussed (additive model, boosting, neural network) for $n\\,\\in\\,\\{25,50,100\\}$ functional observations, $Q\\,=\\,R\\,\\in\\,\\{25,50,100\\}$ observed data points, and $J\\,\\in\\,\\{1,2,4\\}$ functional features. We restrict this study to rather small functional datasets as the memory consumption of other methods is superlinear. This can also be seen in Figure 4(b) presented earlier, which summarizes the result of this simulation study and clearly shows the advantage of our implementation compared to existing ones form [5, 53]. ", "page_idx": 7}, {"type": "text", "text": "4.2 Real-world datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our method being originally motivated by applications in the field of biomechanics, we now analyze two real-world datasets with biomechanical sensor data. In addition to these applications in biomechanics, we provide further applications to EMG-EEG synchronization, air quality prediction and hot water consumption profiles in Appendix C to demonstrate the versatility of our approach. ", "page_idx": 7}, {"type": "text", "text": "4.2.1 Fukuchi and Liew datasets (H2a) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The data analyzed in the first experiment is a collection of three publicly available running datasets [14, 27, 28]. A recent study of [32] used this data set to show the potential of deep learning approaches for predicting joint moments during gait (in total 12 different outcome features). The given data set collection is challenging as it provides only $n=490$ functional samples but $J=27$ partially highly correlated functional features. We follow the preprocessing steps of [32] and split the data into $80\\%$ training and $20\\%$ test data for measuring performance according to the authors\u2019 split indices. ", "page_idx": 7}, {"type": "text", "text": "As predictors we use the gait cycle of all available joint measurements, that is the 3D joint angle, velocity, and acceleration of the bilateral ankle, knee, and hip joints. Predictors in both training and test datasets are separately demeaned and scaled using only the information from the training set. As in the simulation studies, we run the additive model, Boosting, and our approach and compare the performance across all 6 different outcome variables. We use the relative and absolute root mean squared error (RMSE) as suggested in [46] and the Pearson correlation coefficient [20] to measure performance. In contrast to our simulation study, the additive model implementation of FFR cannot deal with this large amount of functional features. We, therefore, split our analysis into one comparison of the additive model, an FNN, and a semi-structured FNN (SSFNN) on a selected set of features, and a comparison of SSFNN against boosting (which has an inbuilt feature selection). ", "page_idx": 8}, {"type": "text", "text": "Results in Table 1 summarize test performances across all 12 different outcome types. We see that the (SS)FNN approaches perform equally well or better compared to the additive model on the selected set of features, and SSFNN also matches the performance of Boosting when using all features. ", "page_idx": 8}, {"type": "text", "text": "Table 1: Median (and mean absolute deviation in brackets) of the relative RMSE across all outcome responses in the Fukuchi and Liew datasets for different methods (columns) using either selected (sel.) or all features (all). The best method is highlighted in bold. ", "page_idx": 8}, {"type": "table", "img_path": "WJAiaslhin/tmp/df05f3a28b2b37c6f13850796d678c8600b468ff7d50345795decb387835619c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2.2 Carmago data set (H2a-c) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The second data set is a publicly available dataset on lower limb biomechanics in walking across different surfaces level-ground walking, treadmill walking, stair ascent, stair descent, ramp ascent, and ramp descent [7]. Due to the size of the data set consisting of 21787 functional observations, $Q=R=101$ observed time points, and $J~=~24$ predictors, it is not feasible to apply either additive model-based FFR or boosting. We, therefore, compare SSFNN to a (structured) neuralbased FFR model and a deep-only neural network. Based on the pre-defined $70/30\\%$ split for training and testing, we run the models for each of the 5 outcomes (joints) and compute the relative MSE difference, a functional $\\mathbf{\\bar{\\alpha}}^{2}$ analogon with values in $(-\\infty,1]$ (see Ap", "page_idx": 8}, {"type": "image", "img_path": "WJAiaslhin/tmp/5d70ea1ad239a066ca30d5903a4fc4378bdedfb424a87b9eb614e35a7152f8f9.jpg", "img_caption": ["Figure 5: Comparison of performance improvements (larger is better) in mean squared error (MSE) for different joints (outcomes). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "pendix A.1 for definition). Table 2 shows the results averaged for all outcome types while Figure 5 provides performance changes individually for the 5 different joints. ", "page_idx": 8}, {"type": "text", "text": "We observe that the semi-structured model performs better than the deep model, which in turn is better than the structured model. Figure 8 in the Appendix further depicts the predicted functions for all 5 outcomes using the 3 models. While for some joints the prediction of the semi-structured and deep-only model is very similar, the performance for the subtalar joint is notably better when combining a structured model and the deep neural network to form a semi-structured model. ", "page_idx": 8}, {"type": "text", "text": "Table 2: Mean performance (standard deviation in brackets) of the relative MSE difference (a value of 1 corresponds to the optimal model with zero error) across all five outcomes. ", "page_idx": 8}, {"type": "table", "img_path": "WJAiaslhin/tmp/db6061914cf44d4f0ef134da5df66b6862fd9ce7700c0f84efd988a9b384a5c4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our proposed method is an extension of semi-structured networks to cater to functional data and deals with the issue of identifiability by proposing a functional extension of post-hoc orthogonalization. Using array computations and basis recycling the method also solves scalability issues present in existing methods. Our experimental results reveal that functional semi-structured networks yield similar or superior performance compared to other methods across varied biomechanical sensor datasets. ", "page_idx": 9}, {"type": "text", "text": "Novelty The approach proposed in Section 3 shows similarities to an autoeconder architecture. While a functional autoencoder has been proposed in the past [18] and most recently by [61], these studies focus on the representation learning (and reconstruction) of the same signal. Other papers focusing on function-on-function regression (e.g., [34, 43, 58]) suggest similar approaches to ours but without the option to jointly train a structured model and a deep network. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Research While our model shows good performance on various datasets and in simulations, we believe that our approach can be further improved for applications on highdimensional sensor data by incorporating appropriate sparsity penalties. Due to the functional nature of the data, feature selection would result in a great reduction of complexity and thereby potentially yield better generalization. Our general model formulation would further allow the extension to a nonlinear FFR model $\\begin{array}{r}{\\lambda^{-}(t)=\\sum_{j=1}^{J}\\int_{S_{j}}f_{j}(x_{j}(s),t)d s}\\end{array}$ using smooth functions $f_{j}$ in three dimensions (feature-, $s-$ and $t$ -direction). Although this extension is still an additive model in the functional features, it is challenging to interpret the resulting additive effects. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the four anonymous reviewers and the area chair for providing valuable feedback. ", "page_idx": 9}, {"type": "text", "text": "DR\u2019s research is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 548823575. BL is funded by the Medical Research Council, UK (MR/Y013557/1) and Innovate UK (10093679). AS acknowledges support from SNSF Grant 200020_207367. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, and Geoffrey E Hinton. Neural additive models: Interpretable machine learning with neural nets. Advances in Neural Information Processing Systems, 34:4699\u20134711, 2021.   \n[2] Philipp F. M. Baumann, Torsten Hothorn, and David R\u00fcgamer. Deep conditional transformation models. In Machine Learning and Knowledge Discovery in Databases (ECML-PKDD), pages 3\u201318. Springer International Publishing, 2021.   \n[3] Tobia Boschi, Matthew Reimherr, and Francesca Chiaromonte. A highly-efficient group elastic net algorithm with an application to function-on-scalar regression. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 9264\u20139277. Curran Associates, Inc., 2021.   \n[4] Sarah Brockhaus, Michael Melcher, Friedrich Leisch, and Sonja Greven. Boosting flexible functional regression models with a high number of functional historical effects. Statistics and Computing, 27(4):913\u2013926, 2017.   \n[5] Sarah Brockhaus, David R\u00fcgamer, and Sonja Greven. Boosting functional regression models with fdboost. Journal of Statistical Software, Articles, 94(10):1\u201350, 2020.   \n[6] Sarah Brockhaus, Fabian Scheipl, Torsten Hothorn, and Sonja Greven. The functional linear array model. Statistical Modelling, 15(3):279\u2013300, 2015.   \n[7] Jonathan Camargo, Aditya Ramanathan, Will Flanagan, and Aaron Young. A comprehensive, open-source dataset of lower limb biomechanics in multiple conditions of stairs, ramps, and level-ground ambulation and transitions. Journal of Biomechanics, 119:110320, 2021.   \n[8] Chun-Hao Chang, Rich Caruana, and Anna Goldenberg. NODE-GAM: Neural generalized additive model for interpretable deep learning. In International Conference on Learning Representations, 2022.   \n[9] Yakuan Chen, Jeff Goldsmith, and R. Todd Ogden. Variable selection in function-on-scalar regression. Stat, 5(1):88\u2013101, 2016.   \n[10] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, pages 7\u201310, 2016.   \n[11] Daniel Dold, David R\u00fcgamer, Beate Sick, and Oliver D\u00fcrr. Semi-structured subspace inference. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research. PMLR, 2024.   \n[12] Emilio Dorigatti, Benjamin Schubert, Bernd Bischl, and David R\u00fcgamer. Frequentist uncertainty quantification in semi-structured neural networks. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 1924\u20131941. PMLR, 2023.   \n[13] Deborah Falla, Valter Devecchi, David Jim\u00e9nez-Grande, David R\u00fcgamer, and Bernard XW Liew. Machine learning approaches applied in spinal pain research. Journal of Electromyography and Kinesiology, 61:102599, 2021.   \n[14] Reginaldo K Fukuchi, Claudiane A Fukuchi, and Marcos Duarte. A public dataset of running biomechanics and the effects of running speed on lower extremity kinematics and kinetics. PeerJ, 5:e3298, 2017.   \n[15] Sonja Greven and Fabian Scheipl. A general framework for functional regression modelling. Statistical Modelling, 17(1-2):1\u201335, 2017.   \n[16] William H Guss. Deep function machines: Generalized neural networks for topological layer expression. arXiv preprint arXiv:1612.04799, 2016.   \n[17] Trevor Hastie and Robert Tibshirani. Generalized additive models. Statist. Sci., 1(4):297\u2013310, 1986.   \n[18] Tsung-Yu Hsieh, Yiwei Sun, Suhang Wang, and Vasant G Honavar. Functional autoencoders for functional data representation learning. In Proceedings of the SIAM Conference on Data Mining, 2021.   \n[19] Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier, Daniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre-Alain Muller, and Fran\u00e7ois Petitjean. Inceptiontime: Finding alexnet for time series classification. Data Mining and Knowledge Discovery, 34(6):1936\u20131962, 2020.   \n[20] William Robert Johnson, Jacqueline Alderson, David Lloyd, and Ajmal Mian. Predicting athlete ground reaction forces and moments from spatio-temporal driven cnn models. IEEE Transactions on Biomedical Engineering, 66(3):689\u2013694, 2018.   \n[21] Evandro Konzen, Yafeng Cheng, and Jian Qing Shi. Gaussian process for functional data analysis: The gpfda package for r, 2021.   \n[22] Lucas Kook, Lisa Herzog, Torsten Hothorn, Oliver D\u00fcrr, and Beate Sick. Deep and interpretable regression models for ordinal outcomes. Pattern Recognition, 122:108263, 2022.   \n[23] Lucas Kook, Philipp Schiele, Chris Kolb, Daniel Dold, Marcel Arpogaus, Cornelius Fritz, Philipp Baumann, Philipp Kopper, Tobias Pielok, Emilio Dorigatti, and David R\u00fcgamer. How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, Proceedings of Machine Learning Research. PMLR, 2024.   \n[24] Philipp Kopper, Sebastian P\u00f6lsterl, Christian Wachinger, Bernd Bischl, Andreas Bender, and David R\u00fcgamer. Semi-structured deep piecewise exponential models. In Proceedings of AAAI Spring Symposium on Survival Prediction \u2013 Algorithms, Challenges, and Applications, PMLR, pages 40\u201353, 2021.   \n[25] Philipp Kopper, Simon Wiegrebe, Bernd Bischl, Andreas Bender, and David R\u00fcgamer. Deeppamm: Deep piecewise exponential additive mixed models for complex hazard structures in survival analysis. In Advances in Knowledge Discovery and Data Mining (PAKDD), pages 249\u2013261. Springer International Publishing, 2022.   \n[26] Bernard Liew, Ho Yin Lee, David R\u00fcgamer, Alessandro Marco De Nunzio, Nicola R Heneghan, Deborah Falla, and David W Evans. A novel metric of reliability in pressure pain threshold measurement. Scientific Reports, 11(1):6944, 2021.   \n[27] Bernard XW Liew, Susan Morris, Justin WL Keogh, Brendyn Appleby, and Kevin Netto. Effects of two neuromuscular training programs on running biomechanics with load carriage: a study protocol for a randomised controlled trial. BMC Musculoskeletal Disorders, 17:1\u201310, 2016.   \n[28] Bernard XW Liew, Susan Morris, and Kevin Netto. The effects of load carriage on joint work at different running velocities. Journal of Biomechanics, 49(14):3275\u20133280, 2016.   \n[29] Bernard X.W. Liew, David R\u00fcgamer, Deepa Abichandani, and Alessandro Marco De Nunzio. Classifying individuals with and without patellofemoral pain syndrome using ground force profiles \u2013 development of a method using functional data boosting. Gait & Posture, 80:90\u201395, 2020.   \n[30] Bernard XW Liew, David R\u00fcgamer, Alessandro Marco De Nunzio, and Deborah Falla. Interpretable machine learning models for classifying low back pain status using functional physiological variables. European Spine Journal, 29(8):1845\u20131859, 2020.   \n[31] Bernard X.W. Liew, David R\u00fcgamer, Almond Stocker, and Alessandro Marco De Nunzio. Classifying neck pain status using scalar and functional biomechanical variables \u2013 development of a method using functional data boosting. Gait & Posture, 76:146\u2013150, 2020.   \n[32] Bernard X.W. Liew, David R\u00fcgamer, Xiaojun Zhai, Yucheng Wang, Susan Morris, and Kevin Netto. Comparing shallow, deep, and transfer learning in predicting joint moments in running. Journal of Biomechanics, 129:110820, 2021.   \n[33] Yu Liu, Shi-Min Shih, Shi-Liu Tian, Yun-Jian Zhong, and Li Li. Lower extremity joint torque predicted by using artificial neural network during vertical jump. Journal of biomechanics, 42(7):906\u2013911, 2009.   \n[34] Ruiyan Luo and Xin Qi. General nonlinear function-on-function regression via functional universal approximation. Journal of Computational and Graphical Statistics, 33(2):578\u2013587, 2024.   \n[35] Mark J Meyer, Brent A Coull, Francesco Versace, Paul Cinciripini, and Jeffrey S Morris. Bayesian function-on-function regression for multilevel functional data. Biometrics, 71(3):563\u2013 574, 2015.   \n[36] Gregory D Myer, Kevin R Ford, Stephanie L Di Stasi, Kim D Barber Foss, Lyle J Micheli, and Timothy E Hewett. High knee abduction moments are common risk factors for patellofemoral pain (pfp) and anterior cruciate ligament (acl) injury in girls: is pfp itself a predictor for subsequent acl injury? British journal of sports medicine, 49(2):118\u2013122, 2015.   \n[37] Felix Ott, David R\u00fcgamer, Lucas Heublein, Bernd Bischl, and Christopher Mutschler. Joint classification and trajectory regression of online handwriting using a multi-task learning approach. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 266\u2013276, 2022.   \n[38] Carlos Misael Madrid Padilla, Daren Wang, Zifeng Zhao, and Yi Yu. Change-point detection for sparse and dense functional data in general dimensions. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[39] Jan Priesmann, Lars Nolting, Christina Kockel, and Aaron Praktiknjo. Time series of useful energy consumption patterns for energy system modeling. Scientific Data, 8(1):148, 2021.   \n[40] Xin Qi and Ruiyan Luo. Nonlinear function-on-function additive model with multiple predictor curves. Statistica Sinica, 29(2):719\u2013739, 2019.   \n[41] Filip Radenovic, Abhimanyu Dubey, and Dhruv Mahajan. Neural basis models for interpretability. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[42] J. O. Ramsay and B. W. Silverman. Functional Data Analysis. Springer Series in Statistics. Springer, New York, NY, 2005.   \n[43] Aniruddha Rajendra Rao and Matthew Reimherr. Modern non-linear function-on-function regression. Statistics and Computing, 33(6):130, 2023.   \n[44] Aniruddha Rajendra Rao and Matthew Reimherr. Nonlinear functional modeling using neural networks. Journal of Computational and Graphical Statistics, 0(0):1\u201310, 2023.   \n[45] Philip T. Reiss, Jeff Goldsmith, Han Lin Shang, and R. Todd Ogden. Methods for Scalar-onFunction Regression. International Statistical Review, 85(2):228\u2013249, 2017.   \n[46] Lei Ren, Richard K Jones, and David Howard. Whole body inverse dynamics over a complete gait cycle based only on measured kinematics. Journal of biomechanics, 41(12):2750\u20132759, 2008.   \n[47] Rosie E Richards, Michael Skipper Andersen, Jaap Harlaar, and JC Van Den Noort. Relationship between knee joint contact forces and external knee joint moments in patients with medial knee osteoarthritis: effects of gait modifications. Osteoarthritis and Cartilage, 26(9):1203\u20131214, 2018.   \n[48] Fabrice Rossi, Brieuc Conan-Guez, and Fran\u00e7ois Fleuret. Functional data analysis with multi layer perceptrons. In Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN\u201902 (Cat. No. 02CH37290), volume 3, pages 2843\u20132848. IEEE, 2002.   \n[49] David R\u00fcgamer. A New PHO-rmula for Improved Performance of Semi-Structured Networks. In International Conference on Machine Learning, 2023.   \n[50] David R\u00fcgamer, Sarah Brockhaus, Kornelia Gentsch, Klaus Scherer, and Sonja Greven. Boosting factor-specific functional historical models for the detection of synchronization in bioelectrical signals. Journal of the Royal Statistical Society: Series $C$ (Applied Statistics), 67(3):621\u2013642, 2018.   \n[51] David R\u00fcgamer, Chris Kolb, Cornelius Fritz, Florian Pfisterer, Philipp Kopper, Bernd Bischl, Ruolin Shen, Christina Bukas, Lisa Barros de Andrade e Sousa, Dominik Thalmeier, Philipp Baumann, Lucas Kook, Nadja Klein, and Christian L. M\u00fcller. deepregression: a flexible neural network framework for semi-structured deep distributional regression, 2023.   \n[52] David R\u00fcgamer, Chris Kolb, and Nadja Klein. Semi-structured distributional regression. The American Statistician, pages 1\u201312, 2023.   \n[53] Fabian Scheipl, Ana-Maria Staicu, and Sonja Greven. Functional additive mixed models. Journal of Computational and Graphical Statistics, 24(2):477\u2013501, 2015.   \n[54] Jian Qing Shi and Taeryon Choi. Gaussian process regression analysis for functional data. CRC Press, 2011.   \n[55] Barinder Thind, Kevin Multani, and Jiguo Cao. Deep learning with functional inputs. arXiv preprint arXiv:2006.09590, 2020.   \n[56] Megan E Toney and Young-Hui Chang. The motor and the brake of the trailing leg in human walking: leg force control through ankle modulation and knee covariance. Experimental brain research, 234(10):3011\u20133023, 2016.   \n[57] Jos\u00e9 L Torrecilla and Alberto Su\u00e1rez. Feature selection in functional data classification with recursive maxima hunting. Advances in Neural Information Processing Systems, 29, 2016.   \n[58] Qiyao Wang, Haiyan Wang, Chetan Gupta, Aniruddha Rajendra Rao, and Hamed Khorasgani. A non-linear function-on-function model for regression with time series data. In 2020 IEEE International Conference on Big Data (Big Data), pages 232\u2013239. IEEE, 2020.   \n[59] Shuoyang Wang, Wanyu Zhang, Guanqun Cao, and Yuan Huang. Functional data analysis using deep neural networks. WIREs Computational Statistics, 16(4):e70001, 2024.   \n[60] Simon N Wood. Thin plate regression splines. Journal of the Royal Statistical Society Series B: Statistical Methodology, 65(1):95\u2013114, 2003.   \n[61] Sidi Wu, C\u00e9dric Beaulac, and Jiguo Cao. Functional autoencoder for smoothing and representation learning. arXiv preprint arXiv:2401.09499, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Further details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Functional $R^{2}$ definition ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $n$ be the number of observed curves indexed with $i$ . Given the $i$ th observation of a function output signal $y^{(i)}(t)$ for $t\\in\\tau$ with functional domain $\\tau$ and corresponding predicted value $\\mu^{(i)}(t)$ , we define the functional $R^{2}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varkappa(\\{y^{(i)},\\mu^{(i)}\\}_{i=1,\\dots,n}):=n^{-1}\\sum_{i=1}^{n}\\frac{\\int_{\\mathcal{T}}(y^{(i)}(t))^{2}d t-\\int_{\\mathcal{T}}(y^{(i)}(t)-\\mu^{(i)}(t))^{2}d t}{\\int_{\\mathcal{T}}(y^{(i)}(t))^{2}d t}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\textstyle\\int_{\\mathcal T}(y^{(i)}(t)-\\mu^{(i)}(t))^{2}d t\\geq0$ , the nominator is the smallest for $y(t)\\equiv\\mu(t)$ , in which case $\\varkappa=1$ . In contrast, if the prediction $\\mu$ is $\\mu\\equiv0$ , $\\varkappa=0$ . For models that perform worse than the constant zero predictor $\\textstyle\\int_{\\mathcal T}(y^{(i)}(t)-\\mu^{(i)}(t))^{2}d t$ can be larger than $\\textstyle\\int_{\\mathcal T}(y^{(i)}(t))^{2}d t$ , yielding a negative nominator and hence negative $\\varkappa$ values. ", "page_idx": 14}, {"type": "text", "text": "A.2 Penalization scheme ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In order to enforce smoothness of estimated functional relationships in $\\lambda^{+}$ in both $s-$ - and $t$ -direction, we can employ a quadratic smoothing penalty. For regression splines with evaluated bases $\\Phi_{j}$ and $\\Psi$ , smoothness is usually defined by some form of difference penalties, which we define to be encoded in the matrices $P_{s}$ and $P_{t}$ , respectively. For the array computation, we want to penalize all differences in $s$ -direction, which can be done by using ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{pen}_{s}=\\mathrm{vec}(\\Theta)^{\\top}(I_{U}\\otimes P_{s})\\mathrm{vec}(\\Theta)=\\mathrm{vec}(\\Theta\\circ(P_{s}\\Theta))^{\\top}\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Analogously, the $t$ -direction can be penalized as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{pen}_{t}=\\mathrm{vec}(\\Theta)^{\\top}(P_{t}\\otimes K_{j})\\mathrm{vec}(\\Theta)=\\mathrm{vec}((\\Theta P_{t})\\circ\\Theta)^{\\top}\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Putting both penalties together, we obtain the total penalty for the array product as ${\\mathfrak{p e n}}_{s}+{\\mathfrak{p e n}}_{t}$ ", "page_idx": 14}, {"type": "text", "text": "B Additional results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Simulation study ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 6 shows the estimation error of the weight surface using different established methods and our proposed implementation (Neural Network). ", "page_idx": 14}, {"type": "text", "text": "All methods perform equally well with minor differences in the weight surface estimation. As expected, neural networks perform better in small signal-to-noise (SNR) settings due to their implicit regularization when trained with stochastic gradient descent and using mini-batches, whereas for large SNR, the full batch routines implemented by boosting and additive models perform slightly better. As becomes apparent in Figure 4(a), these differences are negligible in practice. ", "page_idx": 14}, {"type": "text", "text": "B.2 Application ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 7 gives an excerpt of the estimated weight surfaces that provide insights into how the relationship between input features (rows; here acceleration measures from the ${\\bf X}$ -dimension) and outcome joints is estimated. ", "page_idx": 14}, {"type": "text", "text": "For example, the trunk acceleration (last row in Figure 7) having a short negative and almost instantaneous effect (dark circle) on the ankle and hip adduction moment (first and second column) is a reasonable result from a biomechanical perspective. ", "page_idx": 14}, {"type": "text", "text": "We can further plot the predicted curves by all employed methods (Figure 8) and compare the results with the ground truth. While for some joints the prediction of the semi-structured and deep-only model is very similar, the performance for the subtalar joint is notably better when combining a structured model and the deep neural network to form a semi-structured model. ", "page_idx": 14}, {"type": "image", "img_path": "WJAiaslhin/tmp/6ab8047c289738fcd0c88b92a4e0f242452f3f88732e14ddd68e2d3b8f70280e.jpg", "img_caption": ["Figure 6: Surface estimation and prediction performance (top and bottom row, respectively) using different optimization methods (colors) for different $n$ $\\bf\\Tilde{x}$ -axis) and signal-to-noise ratios (SNR; columns). ", "Boosting Additive Model Neural Network "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "WJAiaslhin/tmp/c138526cf20b9140cdf1eed13eff36f9c6eaae512f5a1735cd2227c0f2dc8a9a.jpg", "img_caption": ["Figure 7: Estimated weight surfaces for different accelerometer predictors (rows) for the different joints (columns). The color of each surface corresponds to the weight a predictor sensor signal at time points $s$ ( $\\bf\\Tilde{x}$ -axis) is estimated to have on the tth time point (y-axis) of the outcome. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Comparison with [32] As requested by an anonymous reviewer, we have also compared our method against the approach suggested in [32]. Compared to [32], we use a different pre-processing of functional predictors which is more in line with our main results. We then run their best model once using a deep-only variant and once using a semi-structured model. The RelRMSE results are given in Table 3, showing that the semi-structured extension works similarly well or in most cases better. ", "page_idx": 15}, {"type": "text", "text": "C Additional experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In order to demonstrate our method\u2019s applicability beyond biomechanical applications, we run several additional experiments. Datasets come from different fields of application and have different sizes ", "page_idx": 15}, {"type": "image", "img_path": "WJAiaslhin/tmp/f516944f4bf46304a7d8de3eab9fb967c2a7e402b07d7529ef9263ba36e752ba.jpg", "img_caption": ["Figure 8: Comparison of predictions of the structured, deep, and semi-structured model as well as the true sensor data (columns) for different joints (rows). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 3: Comparison of a plain deep and a semi-structured approach, where the former is based on [32] and the latter its extension to a SSN. ", "page_idx": 16}, {"type": "table", "img_path": "WJAiaslhin/tmp/0ceac6a9f7ab083c55009ebe0e01375390b72f2179a08c9bf4b639f832872ee2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "(number of observations, number of functional predictors, number of observed time points). We also use these examples to 1) compare the two different architectures suggested in Fig. 2(a) and Fig. 2(b), 2) compare against a non-linear FFR implementation from [40]. ", "page_idx": 16}, {"type": "text", "text": "For all the additional experiments, the deep network version from Fig. 2(a) uses the same encoding and decoding basis for both $\\lambda^{+}$ and $\\lambda^{-}$ (using $K_{j}\\,=\\,20$ basis functions for the $s$ -direction and $U=20$ basis function for the $t$ -direction). The trunk of the deep network part $\\lambda^{-}$ is defined as a fully connected neural network with 100 neurons each, followed by a dropout layer with dropout rate of 0.2 and a batch normalization layer. The deep network version from Fig. 2(b) uses the same architecture, but instead of using a shared encoding and decoding part, it simply concatenates the inputs and processes the three-dimensional object until the last layer, which is then reshaped to have the correct dimensions. We have not tuned this architecture, the number of layers, neurons, or dropout rate, as the purpose of these additional experiments is solely to demonstrate that the (SS)FNN approach yields competitive results with only the structured part while the deep part can further help to improve predictive performance. ", "page_idx": 16}, {"type": "text", "text": "C.1 Predicting EMG from EEG signals in cognitive affective neuroscience ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The dataset analyzed in [50] contains 192 observations of three different EMG signals from facial muscles used as different outcomes (similar to the different joint moments) and up to 64 EEG signals measured at different parts of the brain. Both EMG and EEG signals are measured at 384 equidistant time points. Following [50], we choose the EXG4-EXG5 signal as output signal and the Fz-, FCz-, POz-, and $\\mathbf{P}\\mathbf{Z}$ -electrode as input signals. Based on 10 train/test-splits, we obtain the following table: ", "page_idx": 17}, {"type": "table", "img_path": "WJAiaslhin/tmp/1b38b39ef61ba9820a0af366fed781bc535b4812184b819d96cc2d1068995ad3.jpg", "table_caption": ["Table 4: Model performance comparison "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Results confirm again that our FNN implementation leads to similar values as Boosting, while including a deep neural network part can improve the performance. ", "page_idx": 17}, {"type": "text", "text": "C.2 Predicting air quality from temperature curves ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The dataset analyzed in [40] contains 355 observations of daily NO2 measurement curves (i.e., the functional domain is the hours 1 to 24) together with four other functional pollutants as well as the temperature and relative humidity. The goal is to predict the NO2 concentration using other pollutants, the temperature, and the humidity. The proposed approach in [40] uses a non-linear function-on-function regression (FFR) which we compare against. The results are given in the following table: ", "page_idx": 17}, {"type": "table", "img_path": "WJAiaslhin/tmp/57dc7ec6639b144b53e6d34f2d6399ae35fa6f4ad862b8728e399b7e95d0616b.jpg", "table_caption": ["Table 5: Comparison of Model Metrics "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Results suggest that the given task is rather easy and a linear FFR model is already sufficient to obtain good prediction performance. Hence FNN outperforms both the non-linear FFR model and the two semi-structured approaches ", "page_idx": 17}, {"type": "text", "text": "C.3 Predicting hot water consumption profiles in urban regions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The dataset taken from [39] contains hourly hot water consumption profiles (i.e., the functional domain is again 1 to 24) which are available for different administrative districts. The goal of the analysis is to predict the hot water consumption of a specific region given multiple (here 6) far-distant administrative districts. The results are given in the following table: ", "page_idx": 17}, {"type": "table", "img_path": "WJAiaslhin/tmp/1b04dea501f44c102547a6b7dd02d5bc6c7bd375878b5f09849585fa039611c2.jpg", "table_caption": ["Table 6: Model Performance Evaluation "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Similar to the first dataset, the FNN achieves similar performance as the reference model while including a deep neural network improves the performance. ", "page_idx": 17}, {"type": "text", "text": "D Experimental details and computational environment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Experimental details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In all experiments, we use the Adam optimizer with default hyperparameters. No additional learning rate schedule was used. The batch size, maximum number of epochs and early stopping patience was adjusted depending on the size of the dataset. A prototypical implementation is available as an add-on package of deepregression [51] at https://github.com/ neural-structured-additive-learning/funnel. ", "page_idx": 18}, {"type": "text", "text": "D.2 Computational environment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All computations were performed on a user PC with Intel(R) Core(TM) i7-8665U CPU $\\ @\\ 1.90\\mathrm{GHz}$ , 8 cores, 16 GB RAM using Python 3.8, R 4.2.1, and TensorFlow 2.10.0. Run times of each experiment do not exceed 48 hours. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Claims reflect the contribution and scope. Experimental results support the theoretical claims. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Limitations are discussed in the Discussion Section. Computational efficiency is part of the numerical experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 20}, {"type": "text", "text": "Justification: While mathematical details are not formulated as theorems or similar, the notation and background section defines the necessary basis for further theoretical elaboration Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The algorithm\u2019s implementation is given in detail, allowing readers to reconstruct our implementation. The details provided in the numerical experiments should further allow readers to reproduce simulation studies and real-world experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Code is attached as supplementary material. Open access data is referenced. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The information is given in the respective section and in Appendix D. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Simulation studies are repeated multiple times and report error bars; for realworld data, replications are done for different data scenarios, also allowing to report standard deviations. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The information is given in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have read the code of ethics. There are no ethical concerns or similar. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: While biomechanical applications often involve data from humans, our work does not deal with this type of data in a way that would imply any particular societal impact. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not provide data. Models themselves do not pose a risk for misuse. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The datasets used are referenced and code is CC-BY licensed. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code is documented. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: No crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: No crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]