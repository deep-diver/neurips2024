[{"figure_path": "k9SH68MvJs/tables/tables_5_1.jpg", "caption": "Table 1: Converged performance. We report the quantitative results of the converged performance across all experiments.", "description": "This table presents a comprehensive summary of the final performance metrics achieved by various imitation learning methods across different continuous control tasks.  The results encompass both main experiments and generalization experiments under varied conditions.  Metrics such as success rate and return are reported, providing a quantitative comparison of the algorithms' effectiveness in different scenarios and varying amounts of training data. The table allows for an in-depth assessment of each algorithm's robustness, generalization capability, and data efficiency.", "section": "Experimental results"}, {"figure_path": "k9SH68MvJs/tables/tables_20_1.jpg", "caption": "Table 1: Converged performance. We report the quantitative results of the converged performance across all experiments.", "description": "This table presents a comprehensive summary of the final performance metrics achieved by different imitation learning methods across various tasks and experimental settings.  The results include success rates (for tasks with binary success/failure outcomes) and average returns (for tasks with continuous reward signals). Results are shown for the main experiments, generalization experiments (with varying levels of noise and data coverage), and data efficiency experiments (with different amounts of expert data).  The table allows for direct comparison of DRAIL against baselines across different scenarios.", "section": "Extended results of generalization experiments"}, {"figure_path": "k9SH68MvJs/tables/tables_21_1.jpg", "caption": "Table 2: Model architectures of policies and discriminators. We report the architectures used for all the methods on all the tasks. Note that \u03c0 denotes the neural network policy, D represents a multilayer perceptron discriminator used in GAIL, GAIL-GP, and WAIL, and D<sub>\u03b8</sub> represents a diffusion model discriminator used in DiffAIL and our method DRAIL.", "description": "This table details the architecture of the neural networks used for policies and discriminators in different methods (BC, Diffusion Policy, GAIL, GAIL-GP, WAIL, DiffAIL, and DRAIL) across six tasks.  For each method, it lists the number of layers, input dimensions, hidden dimensions, and output dimensions of the respective networks. It also specifies additional parameters like the regularization value (e) for WAIL and the label dimension (c) for DRAIL.  The table provides a comprehensive overview of the network configurations for each imitation learning method.", "section": "F Model architecture"}, {"figure_path": "k9SH68MvJs/tables/tables_22_1.jpg", "caption": "Table 2: Model architectures of policies and discriminators. We report the architectures used for all the methods on all the tasks. Note that \u03c0 denotes the neural network policy, D represents a multilayer perceptron discriminator used in GAIL, GAIL-GP, and WAIL, and Ddiff represents a diffusion model discriminator used in DiffAIL and our method DRAIL.", "description": "This table details the architecture of the models used in the experiments.  It shows the number of layers, input dimensions, hidden layer dimensions, and output dimensions for different components (policy and discriminator) of various imitation learning methods (BC, Diffusion Policy, GAIL, GAIL-GP, WAIL, DiffAIL, DRAIL) across six different tasks.  Note that it differentiates between the multilayer perceptron (MLP) used in some methods and the diffusion model used in others.", "section": "F Model architecture"}, {"figure_path": "k9SH68MvJs/tables/tables_22_2.jpg", "caption": "Table 4: PPO training parameters. This table reports the PPO training hyperparameters used for each task.", "description": "This table lists the hyperparameters used for Proximal Policy Optimization (PPO) during the training process for different tasks in the imitation learning experiments.  It includes the clipping range (epsilon), discount factor (gamma), generalized advantage estimation (GAE) parameter (lambda), value function coefficient, and entropy coefficient.  These hyperparameters influence the stability and performance of the PPO algorithm in various continuous control environments.", "section": "G Training details"}]