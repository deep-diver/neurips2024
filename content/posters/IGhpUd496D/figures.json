[{"figure_path": "IGhpUd496D/figures/figures_1_1.jpg", "caption": "Figure 1: Linear relaxations for y def ReLU(x). - and - denote the upper and lower bounds, denotes the relaxation. (a) Relaxation in prior approaches [49, 39], where the bounds are overapproximated using linear functions, and the upper bound is only sound within given input bounds [x, X]; (b) This work exactly represents the upper bound for any input upper bound variable x, results in a tighter approximation; (c) Illustration of how this work exactly represents the upper bound \u00ff (\u2014) by capturing the epigraph (1) of ReLU(x) with the linear constraint \u00ff \u2265 0 \u2227 \u00ff \u2265 x. The lower bound \u00ff (\u2014) uses a linear relaxation \u00ff def cx with a constant c \u2208 [0, 1].", "description": "This figure compares different linear relaxations for the ReLU activation function.  (a) shows the traditional approach where linear functions overapproximate the ReLU function, and the upper bound is only valid within a specific input range. (b) demonstrates the proposed method, which accurately represents the upper bound for any upper bound input value. (c) illustrates how the proposed method achieves this accurate representation by using linear constraints to capture the epigraph of the ReLU function.", "section": "3 Provable editing of DNNs using Parametric Linear Relaxation"}, {"figure_path": "IGhpUd496D/figures/figures_2_1.jpg", "caption": "Figure 2: Linear relaxations for bounding v = xw for all x \u2208 [x, x], with variable parameter w.  and  denote the upper and lower bound overapproximations.  denotes the relaxation. (a) A loose relaxation for all x \u2208 [x, x] and for all w \u2208 [w, w], where w is not treated as a parameter but another input within constant bounds [w, w]. (b) This work exactly represents the bounds for all inputs x \u2208 [x, x], parameterized by variable w, without any relaxation. Here we plot a case when w < 0, while our parametric bounds are exact for any w. (c) Illustration of how our parametric bounds are defined for the function v = xw in terms of the variable parameter w. denotes the true output region for all slope (input) x \u2208 [x, x]. The upper bound v (\u2014) is exactly captured by the epigraph ( ), defined by linear constraint (v \u2265 xw \u2227 v \u2265 xw); the lower bound v (\u2014) is exactly captured by the hypograph ( ), defined by linear constraint (v \u2264 xw \u2227 v \u2264 xw).", "description": "This figure illustrates three different linear relaxations for bounding v = xw, where w is a variable parameter. (a) shows a loose relaxation from prior work, which uses constant bounds for w. (b) shows the proposed method which provides exact bounds parameterized by w for all x \u2208 [x, x]. (c) illustrates how the proposed method achieves this by capturing the epigraph and hypograph of the function.", "section": "3.1 Parametric Linear Relaxation"}, {"figure_path": "IGhpUd496D/figures/figures_5_1.jpg", "caption": "Figure 3: Illustration of Parameterized Linear Relaxations for Tanh, Sigmoid and ELU.  and denote the upper and lower bound approximations; and denotes the convex underapproximations (\u00ff, \u017c) and (\u00ff, \u017c) for the epigraph and hypograph; denotes relaxation.", "description": "This figure shows the parameterized linear relaxations for Tanh, Sigmoid, and ELU activation functions.  It illustrates how the approach constructs tight bounds for these functions by capturing their epigraph and hypograph using linear constraints, parameterized by the layer parameters,  resulting in more precise approximations than prior approaches.", "section": "3.6 Parametric Linear Relaxation for Tanh, Sigmoid and ELU layers"}, {"figure_path": "IGhpUd496D/figures/figures_6_1.jpg", "caption": "Figure 4: Results of (a) single-property and (b) all-properties editing problems from VNN-COMP\u203222.", "description": "This figure presents the results of applying PREPARED and two baseline methods (PFT(DL2) and PFT(APRNN)) to solve single-property and all-properties editing problems from the VNN-COMP'22 benchmark.  The plots show the runtime and success rate of each method. PREPARED significantly outperforms the baseline methods in both effectiveness and efficiency.", "section": "4 Experimental evaluation"}]