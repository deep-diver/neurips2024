[{"figure_path": "IGhpUd496D/tables/tables_8_1.jpg", "caption": "Table 1: Local robustness editing for image-recognition DNNs. Comparison of efficacy (Effic.), standard (Acc.), certified (Cert.) and generalization (Gen.) accuracy. Rows with <100% efficacy are shaded. Best results are highlighted.", "description": "This table presents the results of local robustness editing experiments on image-recognition deep neural networks (DNNs).  Two different DNN architectures are evaluated: one trained on CIFAR10 and another trained on TinyImageNet.  The efficacy of the editing methods in achieving local robustness (measured as percentage of correctly classified perturbed images) is compared, along with the standard accuracy, certified accuracy (a robustness measure), and generalization accuracy.  Several methods are compared, highlighting the efficiency and effectiveness of the PREPARED approach in achieving high efficacy while maintaining good accuracy metrics.", "section": "4.2 Local robustness editing for image-recognition DNNs"}, {"figure_path": "IGhpUd496D/tables/tables_8_2.jpg", "caption": "Table 2: Local robustness editing for sentiment-classification transformers. Comparison of the lower bound of efficacy before (Og. Effic.) and after edit (Effic.), as well as the standard accuracy (Acc.). Rows with <100% efficacy are shaded. Best results are highlighted.", "description": "This table presents the results of local robustness editing experiments for sentiment classification using BERT transformers.  It compares the performance of different methods (PFT(DL2), APRNN, and PREPARED) in terms of efficacy (percentage of successfully edited inputs), standard accuracy (overall accuracy), and runtime.  Two different perturbation levels (epsilon) are tested. The original efficacy (Og. Effic.) before editing is included as a baseline.  Rows where the efficacy is below 100% are highlighted to emphasize that the goal is to achieve perfect local robustness.", "section": "4.3 Local robustness editing for sentiment-classification BERT transformers"}, {"figure_path": "IGhpUd496D/tables/tables_9_1.jpg", "caption": "Table 3: Global physics property repair for geodynamics DNN. Comparison of relative error (Rel. Err.), continuity (Cont. Err.) and boundary-condition (BC Err.) errors. Errors on the test set (Ref.) are shaded. Best results are highlighted.", "description": "This table presents a comparison of different methods for repairing a geodynamic DNN to satisfy global physics properties.  The methods compared are DL2, GD (vanilla gradient descent), GD(APRNN) and GD(PREPARED). The table shows the relative error, continuity error, boundary condition error, and training time for each method. The reference errors from the test set are included for comparison. The best results (lowest errors) are highlighted.", "section": "4.4 Provable training for physics-plausible DNNs"}, {"figure_path": "IGhpUd496D/tables/tables_28_1.jpg", "caption": "Table 4: Comparison of the number of succeed instances for provably editing single-instances and all-properties instances in each benchmark.", "description": "This table presents the results of comparing PREPARED against PFT(DL2) and PFT(APRNN) on VNN-COMP\u201922 benchmarks for two scenarios: single-property editing and all-properties editing.  For single-property editing, it shows the number of successful edits for each benchmark using the three methods. For all-properties editing, it shows the same information, addressing the scenario of a DNN violating multiple properties simultaneously.  The \"Total\" column sums up successful instances across all benchmarks for each method.", "section": "4.1 Provable editing on VNN competition benchmarks"}, {"figure_path": "IGhpUd496D/tables/tables_28_2.jpg", "caption": "Table 4: Comparison of the number of succeed instances for provably editing single-instances and all-properties instances in each benchmark.", "description": "This table presents the results of applying three different provable fine-tuning methods (PFT(DL2), PFT(APRNN), and PREPARED) to solve single-property and all-properties editing problems from the VNN-COMP'22 benchmark.  It shows the number of successful edits for each method on various DNNs and properties, comparing their efficiency and effectiveness.", "section": "4.1 Provable editing on VNN competition benchmarks"}]