[{"figure_path": "cuO0DenqMl/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of WGBoost trained on a set {xi, pi}101 whose inputs are 10 grid points in [-3.5, 3.5] and each output distribution is a normal distribution \u03bci(\u03b8) = N(\u03b8 | sin(xi), 0.5) over \u03b8\u2208 R. The blue area indicates the 95% high probability region of the conditional distribution N(\u03b8 | sin(x), 0.5). WGBoost returns N = 10 particles (red lines) to predict the output distribution for each input x. This illustration uses the Gaussian kernel regressor for every weaker learner.", "description": "This figure shows the result of training the Wasserstein Gradient Boosting (WGBoost) model on a dataset with 10 input points and corresponding normal output distributions.  Each subfigure represents the model's performance after training with a different number of weak learners (0, 15, and 100). The blue shaded area shows the 95% highest probability density region of the true underlying function (sin(x)), while the red lines represent the 10 particles (approximations of the output distribution) generated by WGBoost for each input. As the number of weak learners increases, the red lines converge towards the true distribution, demonstrating the effectiveness of the model.", "section": "1 Introduction"}, {"figure_path": "cuO0DenqMl/figures/figures_2_1.jpg", "caption": "Figure 2: Comparison of the pipeline of (a) Bayesian learning and (b) evidential learning based on WGBoost. The former uses the (global-level) posterior p(w | {xi, Yi}=1) of the model parameter w conditional on all data, and samples multiple models from it. The latter uses the individual-level posterior p(0 | yi) of the response parameter 0 as the output distribution of the training set, and trains WGBoost that returns a particle-based distributional estimate p(0 | x) of 0 for each input x.", "description": "This figure compares Bayesian learning and evidential learning using WGBoost.  Bayesian learning uses a global posterior distribution of model parameters to sample multiple models, while WGBoost uses individual-level posterior distributions as training outputs and returns particle approximations of the response parameter distribution for new inputs.", "section": "2 Wasserstein Gradient Boosting"}, {"figure_path": "cuO0DenqMl/figures/figures_2_2.jpg", "caption": "Figure 2: Comparison of the pipeline of (a) Bayesian learning and (b) evidential learning based on WGBoost. The former uses the (global-level) posterior p(w | {xi, Yi}=1) of the model parameter w conditional on all data, and samples multiple models from it. The latter uses the individual-level posterior p(0 | yi) of the response parameter 0 as the output distribution of the training set, and trains WGBoost that returns a particle-based distributional estimate p(0 | x) of 0 for each input x.", "description": "This figure compares Bayesian learning and evidential learning using WGBoost.  Bayesian learning uses the global posterior distribution of model parameters to sample multiple models, while evidential learning leverages individual-level posterior distributions of response parameters as training outputs for WGBoost, resulting in a particle-based estimate of the response parameter's distribution for new inputs.", "section": "Application to Evidential Learning"}, {"figure_path": "cuO0DenqMl/figures/figures_7_1.jpg", "caption": "Figure 1: Illustration of WGBoost trained on a set {xi, pi}101 whose inputs are 10 grid points in [-3.5, 3.5] and each output distribution is a normal distribution \u03bci(\u03b8) = N(\u03b8 | sin(xi), 0.5) over \u03b8\u2208 R. The blue area indicates the 95% high probability region of the conditional distribution N(\u03b8 | sin(x), 0.5). WGBoost returns N = 10 particles (red lines) to predict the output distribution for each input x. This illustration uses the Gaussian kernel regressor for every weaker learner.", "description": "This figure illustrates how Wasserstein Gradient Boosting (WGBoost) works.  It shows the output distribution learned by WGBoost for a simple dataset where inputs are 10 points along the x-axis, and each input is associated with a normal distribution as its output. The blue shaded area represents the 95% confidence interval of the true distribution, while the red lines represent particle approximations of the learned output distribution generated by WGBoost. The figure demonstrates how WGBoost improves its approximation of the true distribution as more weak learners are added (panels a-c).", "section": "2 Wasserstein Gradient Boosting"}, {"figure_path": "cuO0DenqMl/figures/figures_8_1.jpg", "caption": "Figure 3: Conditional density estimation for the bone mineral density dataset (grey dots) by WEvidential, where the normal response distribution N(y | m, \u03c3) is used for the response variable y. Left: distributional estimate (10 particles) of the location parameter {m<sup>n</sup>(x)}<sub>n=1</sub><sup>10</sup> for each input. Right: estimated conditional density (6) through marginalisation of the output particles {(m<sup>n</sup>(x), \u03c3<sup>n</sup>(x))}<sub>n=1</sub><sup>10</sup>.", "description": "This figure shows the conditional density estimation results for the bone mineral density dataset using the WEvidential method.  The left panel displays the distributional estimates (10 particles) of the location parameter for each input x. The right panel illustrates the estimated conditional density obtained by marginalizing over the output particles. Grey dots represent the actual data.", "section": "4.1 Illustrative Conditional Density Estimation"}, {"figure_path": "cuO0DenqMl/figures/figures_9_1.jpg", "caption": "Figure 4: Examples of the output particles (red dot) of WEvidential on the segment dataset, where the coloured area indicate the kernel density estimation of the output particles for each class.", "description": "This figure shows examples of output distributions generated by the WEvidential model for both in-distribution (non-OOD) and out-of-distribution (OOD) inputs.  The left panel displays the output for a non-OOD sample belonging to class 4. The right panel shows the output for an OOD sample.  In each panel, the red dots represent the individual particles generated by the model for each input. The shaded areas represent the kernel density estimation of the particle distributions. The difference in the spread and concentration of the particles for the in-distribution versus out-of-distribution sample highlights the model's ability to capture and quantify uncertainty.", "section": "4.3 Classification and Out-of-Distribution Detection"}, {"figure_path": "cuO0DenqMl/figures/figures_19_1.jpg", "caption": "Figure 5: The total MMD error and example outputs of WEvidential for different kernel scales. Panel (a): the total MMD error for different scale values h = 0.001, 0.01, 0.1, 1.0, 10, 100 both plotted in the common log scale. Panel (b): the output of WEvidential for h = 0.1. Panel (c): the output of WEvidential for h = 0.01. Panel (d): the output of WEvidential for h = 100.", "description": "This figure shows the impact of the kernel bandwidth (h) on the performance of the WEvidential algorithm.  Panel (a) presents the total MMD (Maximum Mean Discrepancy) error, a measure of the approximation error, across different bandwidths. Panels (b), (c), and (d) show example outputs of WEvidential using the three selected kernel bandwidths (h=0.10, h=0.01, and h=100), visually demonstrating the effect of the kernel choice on the algorithm's prediction of the conditional density.", "section": "D Simulation Study for WEvidential"}, {"figure_path": "cuO0DenqMl/figures/figures_20_1.jpg", "caption": "Figure 6: The approximation error and computational time of the four different WGBoost algorithms. Panel (a): the approximation error of each algorithm measured by the MMD averaged over the inputs with respect to the number of weak learners. Panel (b): the computational time with respect to the number of weak learners in common logarithm scale. Panel (c)-(f): the outputs of the four algorithms each with 100 weak learners used.", "description": "This figure compares four different WGBoost algorithms based on different Wasserstein gradient estimates.  Panel (a) shows the approximation error (measured by Maximum Mean Discrepancy or MMD) versus the number of weak learners.  Panel (b) shows the computation time versus the number of weak learners. Panels (c) through (f) display the output of each algorithm with 100 weak learners trained.", "section": "D.2 Comparison of Different Wasserstein Gradient Estimates"}, {"figure_path": "cuO0DenqMl/figures/figures_21_1.jpg", "caption": "Figure 3: Conditional density estimation for the bone mineral density dataset (grey dots) by WEvidential, where the normal response distribution N(y | m, \u03c3) is used for the response variable y. Left: distributional estimate (10 particles) of the location parameter {m<sup>n</sup>(x)}<sub>n=1</sub><sup>10</sup> for each input. Right: estimated conditional density (6) through marginalisation of the output particles {(m<sup>n</sup>(x), \u03c3<sup>n</sup>(x))}<sub>n=1</sub><sup>10</sup>.", "description": "This figure shows the results of applying the WEvidential algorithm to the bone mineral density dataset. The left panel displays the estimated location parameter for each input x as a distribution of 10 particles, while the right panel illustrates the resulting marginal conditional density obtained by integrating over these particles.", "section": "4.1 Illustrative Conditional Density Estimation"}]