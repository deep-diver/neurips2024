[{"figure_path": "V3QZCM1AQv/tables/tables_6_1.jpg", "caption": "Table 1: PER/WER on LibriSpeech using 100 hours speech data. \u2020: Our reproduction. (wav2vec-U and wav2vec-U 2.0 only report results of using 960 hours of unlabeled speech). HMM ST indicates HMM self-training (Appendix C.2).", "description": "This table presents the Phoneme Error Rate (PER) and Word Error Rate (WER) for different unsupervised automatic speech recognition (ASR) models on the LibriSpeech dataset using 100 hours of speech data.  It compares REBORN's performance against several baseline models, including wav2vec-U, wav2vec-U 2.0, and EURO, both with and without HMM self-training.  The results showcase REBORN's superior performance compared to existing methods.", "section": "5 Results"}, {"figure_path": "V3QZCM1AQv/tables/tables_6_2.jpg", "caption": "Table 2: PER results on TIMIT. The cross-mark (X) in the greedy-decoding column indicates that an additional LM (4-gram) is used during decoding. REBORN reaches the best performance with no LM used for decoding, showing that REBORN can benefit from the external LM via RL.", "description": "This table presents the Phoneme Error Rate (PER) results on the TIMIT dataset for different approaches.  It compares the performance of REBORN against several baseline methods, including using oracle boundaries, wav2vec-U with and without WFST decoding, and EURO with different feature extractors.  The table highlights that REBORN achieves the lowest PER, even without using a language model (LM) during decoding. This indicates that REBORN effectively learns to leverage the language model's information implicitly through the reinforcement learning process.", "section": "5 Results"}, {"figure_path": "V3QZCM1AQv/tables/tables_7_1.jpg", "caption": "Table 3: WER on MLS. \u2020: Results from Baevski et al. [4]. *: Our reproduction of wav2vec-U, used as the initialization of the phoneme prediction model in REBORN. \u2021: Results from Liu et al. [30].", "description": "This table compares the Word Error Rate (WER) achieved by different unsupervised Automatic Speech Recognition (ASR) models on the Multilingual LibriSpeech (MLS) dataset.  The models compared include wav2vec-U (with and without the authors' reproduction), wav2vec-U 2.0, and the proposed REBORN model.  WER is reported for six different languages within MLS (de, nl, fr, es, it, pt) as well as the average WER across all languages.  The table highlights REBORN's improved performance compared to previous state-of-the-art methods.", "section": "4.1 Dataset"}, {"figure_path": "V3QZCM1AQv/tables/tables_7_2.jpg", "caption": "Table 4: Boundary evaluation results of different segmentation methods on LibriSpeech test-clean split. The second-last column (Freq.) is the number of segments per second. All the methods share the same phoneme prediction model trained with the k-means-based segmentation of wav2vec-U.", "description": "This table compares the performance of four different boundary segmentation methods on the LibriSpeech test-clean dataset.  The methods are: Oracle (ground truth), k-means-based (from wav2vec-U), Strgar and Harwath (a state-of-the-art unsupervised method), and REBORN.  The table shows precision, recall, F1-score, frequency (segments per second), and phoneme error rate (PER) for each method.  All methods use the same phoneme prediction model, highlighting the impact of different segmentation approaches.", "section": "5.2 Boundary Analysis"}, {"figure_path": "V3QZCM1AQv/tables/tables_8_1.jpg", "caption": "Table 5: We compare different reward functions and the effect of BC initialization with first iteration results on LibriSpeech test-clean. Row (c) is REBORN.", "description": "This table presents an ablation study comparing different reward function combinations used in the REBORN model for unsupervised automatic speech recognition.  It shows the impact of using perplexity difference reward (Rppl), edit distance reward (Redit), and length difference reward (Rlen) individually and in combination on the model's performance. Additionally, it demonstrates the effect of behavior cloning (BC) initialization on the first iteration's results.  Row (c) represents the full REBORN model configuration.", "section": "5.3 Ablation Study"}, {"figure_path": "V3QZCM1AQv/tables/tables_8_2.jpg", "caption": "Table 4: Boundary evaluation results of different segmentation methods on LibriSpeech test-clean split. The second-last column (Freq.) is the number of segments per second. All the methods share the same phoneme prediction model trained with the k-means-based segmentation of wav2vec-U.", "description": "This table compares the performance of different boundary segmentation methods on the LibriSpeech test-clean dataset.  It shows precision, recall, F1-score, R-value (a measure of boundary quality), and phoneme error rate (PER) for each method. The methods compared include the original k-means based method from wav2vec-U, the method proposed by Strgar and Harwath, and the REBORN method (both before and after boundary merging). The table demonstrates that REBORN, particularly after boundary merging, achieves better boundary precision and lower PER compared to other methods despite having a lower F1 score compared to Strgar and Harwath.", "section": "5.2 Boundary Analysis"}, {"figure_path": "V3QZCM1AQv/tables/tables_14_1.jpg", "caption": "Table 7: The table for stability analysis. Each method is trained for 5 runs on LibriSpeech and evaluated on the test-clean split. REBORN performs steadily well while being fully converged in both of the stages. The ablations show the effectiveness of applying boundary merging and parameter initialization from the previous iteration for the stage 2 GAN training. The abbreviation \"adj.\" indicates adjacent pooling, which is the 2nd stage of pooling in wav2vec-U (see Appendix B). The \"Freq.\" in the last column is the number of boundaries per second.", "description": "This table presents the results of a stability analysis conducted on the REBORN model.  The main finding is that the model demonstrates consistent and stable performance across multiple runs, achieving a 100% convergence rate in both training stages. Ablation studies show the importance of boundary merging and using the previous iteration's parameters to initialize the GAN training in Stage 2. A comparison with a topline result using oracle boundaries further highlights the effectiveness of the learned boundaries in REBORN.", "section": "A Additional Results"}, {"figure_path": "V3QZCM1AQv/tables/tables_15_1.jpg", "caption": "Table 8: Ablation study results of various configurations on the unsupervised metric assessed using validation set and phoneme error rates evaluated on the LibriSpeech dataset.", "description": "This table presents the ablation study on different reward functions and behavior cloning initialization.  It shows the unsupervised metric (lower is better), dev-clean PER, and test-clean PER for different configurations: using only Rppl (perplexity difference reward), adding Redit (edit distance reward), and finally adding Rlen (length difference reward)  and comparing with REBORN without behavior cloning.  The results demonstrate that using all three reward functions, along with behavior cloning, leads to improved performance, particularly in reducing the phoneme error rate.", "section": "5.3 Ablation Study"}, {"figure_path": "V3QZCM1AQv/tables/tables_15_2.jpg", "caption": "Table 9: We demonstrate that the REBORN phoneme predictors are gradually improved though our iterative training. Each of the phoneme predictors takes the same oracle-segmented features as input.", "description": "This table shows the Phoneme Error Rate (PER) when using the oracle segmented features as input, for different stages of the REBORN model training. It demonstrates that the iterative training process in REBORN leads to progressively better phoneme prediction models. The PER is evaluated on the test-clean set of LibriSpeech. The initial stage uses wav2vec-U. The topline represents results using the oracle boundaries which are not available in real unsupervised scenarios.", "section": "A.3 Iteratively Polished Phoneme Predictors"}, {"figure_path": "V3QZCM1AQv/tables/tables_16_1.jpg", "caption": "Table 10: We implement REBORN across different speech foundation models on LibriSpeech. The results are evaluated on test-clean. REBORN has exhibited strong generalizability by providing substantial performance improvements across different speech foundation models. We extract the 15th layer representations from HuBERT and WavLM following EURO [18].", "description": "This table presents the results of an ablation study evaluating the generalizability of the REBORN model across three different speech foundation models: wav2vec 2.0, HuBERT, and WavLM.  The performance (PER) is measured on the LibriSpeech test-clean dataset.  For each model, the table shows the initial performance, the performance after the first stage of training in REBORN, the performance after the second stage, and the overall performance improvement after one iteration of REBORN. The results demonstrate that REBORN consistently improves performance across all three foundation models, showcasing its robustness and generalizability.", "section": "A.4 Generalizability across Different Speech Foundation Models"}, {"figure_path": "V3QZCM1AQv/tables/tables_19_1.jpg", "caption": "Table 11: Best reward configurations obtained through hyperparameter searches on each dataset.", "description": "This table presents the optimal weighting coefficients (C<sub>ppl</sub>, C<sub>edit</sub>, C<sub>len</sub>) for the reward function used in the REBORN model's training. These coefficients were determined through hyperparameter searches performed separately for each dataset (LibriSpeech, TIMIT, and MLS) to achieve optimal performance. The table is organized to show the optimal coefficients for each stage (Stage 1 and Stage 2) of training of REBORN model. The different iterations of the model training are also taken into account.", "section": "C.6 Training Details"}]