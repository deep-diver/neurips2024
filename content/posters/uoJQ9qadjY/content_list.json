[{"type": "text", "text": "Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shantanu Jaiswal1,2 Debaditya Roy2 Basura Fernando2,3 Cheston Tan2,3 1 Carnegie Mellon University 2 IHPC, A\\*STAR Singapore 3 Centre for Frontier AI Research, A\\*STAR Singapore Correspondence to: sjaiswa3@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Complex visual reasoning and question answering (VQA) is a challenging task that requires compositional multi-step processing and higher-level reasoning capabilities beyond the immediate recognition and localization of objects and events. Here, we introduce a fully neural Iterative and Parallel Reasoning Mechanism (IPRM) that combines two distinct forms of computation \u2013 iterative and parallel \u2013 to better address complex VQA scenarios. Specifically, IPRM\u2019s \u201citerative\" computation facilitates compositional step-by-step reasoning for scenarios wherein individual operations need to be computed, stored, and recalled dynamically (e.g. when computing the query \u201cdetermine the color of pen to the left of the child in red t-shirt sitting at the white table\u201d). Meanwhile, its \u201cparallel\u201d computation allows for the simultaneous exploration of different reasoning paths and benefits more robust and efficient execution of operations that are mutually independent (e.g. when counting individual colors for the query: \u201cdetermine the maximum occurring color amongst all t-shirts\u201d). We design IPRM as a lightweight and fully-differentiable neural module that can be conveniently applied to both transformer and non-transformer vision-language backbones. It notably outperforms prior task-specific methods and transformer-based attention modules across various image and video VQA benchmarks testing distinct complex reasoning capabilities such as compositional spatiotemporal reasoning (AGQA), situational reasoning (STAR), multi-hop reasoning generalization (CLEVR-Humans) and causal event linking (CLEVRER-Humans). Further, IPRM\u2019s internal computations can be visualized across reasoning steps, aiding interpretability and diagnosis of its errors. Source code at: https://github.com/shantanuj/IPRM_Iterative and_Parallel_Reasoning_Mechanism ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual reasoning and question answering (VQA) at its core requires a model to identify relevant visual operations, execute those operations, and then combine their results to make an inference. Complex visual reasoning scenarios (depicted in fig. 1) are particularly challenging in this regard. They require models to reason compositionally over a large number of reasoning steps and to engage in a variety of higher-level reasoning operations such as causal linking, logical reasoning, and spatiotemporal processing that extend beyond core perception capabilities. In this context, two powerful computational priors exist \u2013 iterative and parallel. While each has its own limitations, when combined, they can complement each other and effectively address the challenges of complex VQA tasks. Specifically, iterative computation, wherein individual operations are identified and composed in a step-by-step manner, is a beneficial prior for multi-step reasoning scenarios explored by past VQA works [28, 25, 18]. However, pure iterative computation can exhibit limitations in scenarios wherein the entailed visual operations are independent of one-another, or where distinct stimuli need to be processed and tracked simultaneously. ", "page_idx": 0}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/248c0976c6860148483594a71c3e05144a1bf01cdf8eeb4f8ebbe643d9bdb72b.jpg", "img_caption": ["Figure 1: Complex VQA scenarios (CLEVR-Humans [35], GQA [29], CLEVRER-Humans[51]), AGQA $\\mathbb{\\Pi}$ and STAR[79]) wherein combination of iterative (step-by-step) computation (blue phrases) and parallel computation (orange phrases) can be beneficial for reasoning. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "For example, consider the first scenario shown in fig. 1. When executing the language phrase \u201cmaximum occurring shape\" (i.e. \u201cwhat shape appears the most\"), a purely iterative method would: (i) compute the count of each shape (each of which itself could take multiple iterations), (ii) then update and maintain the counts in memory (without forgetting count of all previous shapes), and (iii) finally, recall each shape\u2019s count to compute the \u201cmaximum\" 1. Besides taking more reasoning steps than required, such computation also increases the demand for information retention and recall in memory, which in this scenario could scale by the number of shapes to be counted. In complex video reasoning scenarios, a purely iterative method would similarly struggle in tracking and reasoning over multiple co-occurring events. In such scenarios, from both efficiency and efficacy perspectives, it is advantageous to process operations or stimuli in parallel, rather than solely iteratively. ", "page_idx": 1}, {"type": "text", "text": "More generally, parallel computation facilitates the simultaneous maintenance and exploration of distinct reasoning paths, and thereby enables reasoning to be more comprehensive, efficient and robust. For example, to compute \u201cmaximum occuring shape\u201d, parallel compute can enable distinct shape queries to be simultaneously computed prior to computing the \u201cmaximum\" operation. Similarly, it is effective for other scenarios illustrated in fig. $\\boxed{\\mathbb{I}}$ such as in processing independent logical clauses (\u201c{are $X\\!\\!\\mid$ and $\\langle Y$ made of plastic}\u201d) or when tracking and processing co-occurring events in videos. ", "page_idx": 1}, {"type": "text", "text": "Such computation can be implicitly realized in conventional transformer-based parallel attention mechanisms $\\mathbb{\\lVert Z0\\rVert}$ . However, transformer-based attention does not explicitly incorporate iterative compositional computation [14, 41], which as described is beneficial for multi-step reasoning scenarios wherein operations need to be composed sequentially. Accordingly, while parallel computation may effectively compute the result of \u201cmaximum occurring shape\" in fig. $\\mathbb{L}$ it would potentially struggle to integrate the result with further operations such as \u201cgreen object with ..\", \u201csmall object in front of green ..\", and \u201ccolor of ..\" that need to be computed step-by-step to answer the question. ", "page_idx": 1}, {"type": "text", "text": "Based on the above insights, we design the Iterative and Parallel Reasoning Mechanism (IPRM), a novel neural reasoning architecture that combines step-by-step iterative computation with the ability to process multiple independent operations and stimuli simultaneously. Inspired by how humans utilize working memory [17, 9] to facilitate complex reasoning, IPRM internally maintains a latent memory of parallel \u201coperation states\", keyed to which are \u201cresults states\". Given vision and language inputs, IPRM performs the following iterative computation. First, it forms a new set of parallel operations by retrieving relevant language information conditioned on its prior operation states. Then, it \u201cexecutes\" these operations parallelly by retrieving relevant visual information conditioned on its new operations as well as prior result states. Finally, it integrates its new operations (and their results) into memory by dynamically composing these operations with one-another as well as prior operation states, and subsequently, repeats the entire process in its next iterative step. ", "page_idx": 1}, {"type": "text", "text": "This strategy effectively enables us to take advantage of both parallel and iterative computations and notably helps improve state-of-arts across various complex image and video reasoning tasks using a single reasoning mechanism. Equally importantly, IPRM\u2019s internal computations can be visualized ", "page_idx": 1}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/46070ef76a2ce86ffc640d3eaec67520b3b4806c1b71d976217acde4f8f0884e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: IPRM\u2019s computation flow diagram. First, a new set of $\\mathbf{N}.$ -parallel latent operations $\\mathbf{Z_{op}}$ are retrieved from language features $\\mathbf{X_{L}}$ conditioned on prior operation states $\\mathbf{M}_{\\mathbf{op}}$ . Then, visual features $\\mathbf{X_{V}}$ are queried conditioned on both $\\mathbf{Z_{op}}$ and prior result states results $\\mathbf{M}_{\\mathbf{res}}$ , to form the new results $\\mathbf{Z_{res}}$ . Finally, both $\\mathbf{Z_{res}}$ and $\\mathbf{Z_{op}}$ are passed to the Operation Composition Unit (see $\\boxed{2.3}$ , the output of which becomes the new memory state $\\mathbf{M}$ . ", "page_idx": 2}, {"type": "text", "text": "across reasoning steps, which helps better interpret what operations it was doing and accordingly where it was looking visually when processing a complex reasoning scenario. ", "page_idx": 2}, {"type": "text", "text": "2 Iterative and Parallel Reasoning Mechanism ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our proposed iterative-and parallel-reasoning mechanism (IPRM) is a fully-differentiable neural architecture. Given visual features $\\mathbf{X}_{\\mathbf{V}}\\,\\in\\,\\breve{\\mathbb{R}}^{N_{V}\\times D_{V}}$ and language or task-description features $\\mathbf{X_{L}}\\in\\mathbb{R}^{N_{L}\\times D_{L}}$ , IPRM outputs a \u201creasoning result\" $\\mathbf{y_{s}}\\in\\mathbb{R}^{D_{m}}$ and, optionally, a set of \u201creasoning result tokens\" $\\mathbf{Y_{R}}\\in\\mathbb{R}^{N_{m}\\times\\hat{D}_{m}}$ . As previously mentioned, IPRM operates iteratively for $T$ reasoning steps and internally, maintains an explicit memory $\\mathbf{M}:\\{\\mathbf{M}_{\\mathbf{op}},\\mathbf{M}_{\\mathbf{res}}\\}$ . The memory is modelled as a set of \u201coperation states\" $\\mathbf{\\delta}^{\\prime}\\mathbf{M_{op}}\\in\\mathbb{R}^{N_{o p}\\times D_{m}}$ , keyed to which are \u201cresult states\" $'M_{\\mathbf{res}}\\in\\mathbb{R}^{N_{o p}\\times D_{m}}$ as shown in fig. $\\bigstar$ Here, $N_{o p}$ denotes the number of parallel operations to be computed while $D_{m}$ denotes the mechanism\u2019s internal feature dimension. On a high level, at each reasoning step (denoted by $t\\in\\{1,\\cdot\\cdot\\cdot\\,,T\\})$ , IPRM performs the following computations: ", "page_idx": 2}, {"type": "text", "text": ". First, conditioned on the existing operation states $\\mathbf{M_{op,t}}$ , we retrieve relevant information from language or task-description features $\\mathbf{X_{L}}$ to form a new set of latent operations $\\mathbf{Z_{op,t}}\\in\\breve{\\mathbb{R}}^{N_{o p}^{\\breve{\\mathbf{\\alpha}}}\\times D_{m}}$ . We term this computation as \u201cOperation Formation\". ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Z_{op,t}}=\\mathbf{Op\\mathrm{{_{JOrm}}(X_{L};M_{o p,t})}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2. Then, conditioned on the latent operations $\\mathbf{Z_{op,t}}$ and the existing result state $\\mathbf{M_{res,t}}$ , we attend and retrieve relevant information from visual features $\\mathbf{X_{V}}$ which represents a new set of latent results $\\mathbf{Z_{res,t}}\\in\\mathbb{R}^{N_{o p}\\times D_{m}}$ corresponding to $\\mathbf{Z_{op,t}}$ . We term this computation as \u201cOperation Execution\". ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Z_{res,t}}=\\mathbf{Op\\mathrm{{_{LXec}(X_{V};[Z_{o p,t},M_{r e s,t}])}}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3. Finally, to facilitate interaction amongst parallel operations, we perform inter-operation attention. Here, each operation $\\mathbf{Z_{op\\_k,t}};\\mathbf{k}\\in\\left\\{1,\\cdots,N_{o p}\\right\\}$ , is composed with other operations in $\\mathbf{Z_{op,t}}$ as well as prior operation states $\\mathbf{M}_{\\mathbf{op}[\\mathbf{t-W:t}]}$ within a lookback-window $W$ . The corresponding result $\\mathbf{Z}_{\\mathbf{res\\_k,t}}$ is similarly composed with other results $\\mathbf{Z_{res,t}}$ and prior result states denoted as $\\mathbf{M}_{\\mathbf{res}[(\\mathbf{t}-\\mathbf{W}):\\mathbf{t}]}$ . We term this computation as \u201cOperation Composition\" ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{M}_{\\mathbf{t}+\\mathbf{1}}=\\mathbf{O}\\mathbf{p}_{-}\\mathbf{Comp}(\\{\\mathbf{Z_{op,t}},\\mathbf{Z_{res,t}}\\},\\mathbf{M}_{[(\\mathbf{t}-\\mathbf{W}):\\mathbf{t}]})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As shown in eq. $\\Cup$ , this output is the new memory state $\\mathbf{M_{t+1}}:\\{\\mathbf{M_{op}},\\mathbf{M_{res}}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "The overall computation flow is illustrated in fig. $\\bigstar$ and we provide specific details and intuitions behind these computations in the following sub-sections. ", "page_idx": 2}, {"type": "text", "text": "2.1 Operation Formation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The \u201coperation formation\" stage conceptually models a reasoner that based on its prior set of operations, decides what language features to retrieve in order to form the next set of relevant operations. This can be effectively implemented through conventional attention mechanisms. Specifically, the cumulative set of prior operations (maintained in $\\mathbf{M_{op,t}}$ ) can be projected to form the \u2018query\u2019 $\\mathbf{Q_{L,t}}\\,\\in\\,\\mathbb{R}^{N_{o p}\\,\\times\\,D_{m}}$ representing \u201cwhat features to look for\". The language features $\\mathbf{X_{L}}$ can be projected to form the \u2018key\u2019 $\\mathbf{K_{L}}\\in\\mathbb{R}^{N_{L}\\times D_{m}}$ and \u2018value\u2019 $\\mathbf{V_{L}}\\in\\mathbb{R}^{N_{L}\\times D_{m}}$ . Finally, the new set of latent operations $\\mathbf{Z_{op,t}}$ can be retrieved by computing attn $\\mathbf{\\tau}_{1}(\\mathbf{Q_{L}},\\mathbf{K_{L}},\\mathbf{V_{L}})$ . These steps are formally represented below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Q}_{\\mathbf{L},\\mathbf{t}}=\\mathbf{W}_{\\mathbf{L},\\mathbf{q}2}(\\mathrm{Tanh}(\\mathbf{W}_{\\mathbf{L},\\mathbf{q1}}(\\mathbf{M}_{\\mathbf{op},\\mathbf{t}}))),\\mathbf{K}_{\\mathbf{L}}=\\mathbf{W}_{\\mathbf{L},\\mathbf{k}}(\\mathbf{X}_{\\mathbf{L}}),\\mathbf{V}_{\\mathbf{L}}=\\mathbf{W}_{\\mathbf{L},\\mathbf{v}}(\\mathbf{X}_{\\mathbf{L}})}\\\\ {\\mathbf{Z}_{\\mathbf{op},\\mathbf{t}}=\\mathrm{attn}(\\mathbf{Q}_{\\mathbf{L},\\mathbf{t}},\\mathbf{K}_{\\mathbf{L}},\\mathbf{V}_{\\mathbf{L}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathbf{W_{L,q2}}\\in\\mathbb{R}^{D_{m}\\times D_{m}}$ , $\\mathbf{W_{L,q1}}\\in\\mathbb{R}^{D_{m}\\times D_{m}}$ , $\\mathbf{W_{L,k}}\\in\\mathbb{R}^{D_{m}\\times D_{l}}$ and $\\mathbf{W_{L,v}}\\in\\mathbb{R}^{D_{m}\\times D_{l}}$ . Note ${\\bf K_{L}}$ and $\\mathbf{V_{L}}$ are not computation-step dependent and only computed once. We use a simple linearmodulated formulation (with appropriate broadcasting and projection weight $\\mathbf{W_{a}}\\,\\in\\,\\bar{\\mathbb{R}}^{D_{k}\\times1})$ to implement attn(.) (further details in appendix sec. $\\boxed{13}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Operation Execution ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the \u201coperation execution\" stage, the reasoner determines what visual features need to be retrieved depending on both the newly formed operations and existing result states. To model the constituent visual attention mechanism, we draw insights from existing recurrent visual reasoning methods $\\mathbb{1}28,\\mathbb{\\left[69\\right]}$ that incorporate feature modulation for memory-guided attention. Specifically, we retrieve a set of feature modulation weights $\\mathbf{S_{V,t}}\\;\\in\\;\\mathbb{R}^{N_{o p}\\times\\dot{D_{m}}\\bar{/}r}$ through a joint projection of the new operations $\\mathbf{Z_{op,t}}$ and prior results $\\mathbf{M_{res,t}}$ as shown in eq. $\\Cup$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{S}_{\\mathbf{V},\\mathbf{t}}=\\mathbf{W}_{\\mathbf{V},\\mathbf{s}}([\\mathbf{W}_{\\mathbf{V},\\mathbf{op}}(\\mathbf{Z}_{\\mathbf{op},\\mathbf{t}}),\\mathbf{W}_{\\mathbf{V},\\mathbf{res}}(\\mathbf{M}_{\\mathbf{res},\\mathbf{t}})])}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $r$ is a feature reduction ratio $\\leftrightharpoons\\leftmoon$ . $\\mathbf{S_{V,t}}$ is then applied dimension wise to a projection of $\\mathbf{X_{V}}$ to retrieve an intermediate attention key $\\mathbf{K}_{\\mathbf{V},\\mathbf{t}}^{\\prime}\\in\\mathbb{R}^{N_{o p}\\times N_{k}\\times D_{m}/r}$ . The final attention key $\\mathbf{K}\\mathbf{v}_{,\\mathbf{t}}$ is then obtained through a joint multi-layer-projection of $\\mathbf{K}_{\\mathbf{V},\\mathbf{t}}^{\\prime}$ and the previously projected representation of $\\mathbf{X_{V}}$ as shown in eq. $\\mathbb{\\registered}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{K}_{\\mathbf{V},\\mathbf{t}}^{\\prime}=\\mathbf{S}_{\\mathbf{V},\\mathbf{t}}\\odot\\mathbf{W}_{\\mathbf{V},\\mathbf{k}1}(\\mathbf{X}_{\\mathbf{V}}),\\ \\mathbf{K}_{\\mathbf{V},\\mathbf{t}}=\\mathbf{W}_{\\mathbf{V},\\mathbf{k}3}(\\phi(\\mathbf{W}_{\\mathbf{V},\\mathbf{k}2}([\\mathbf{W}_{\\mathbf{V},\\mathbf{k}1}(\\mathbf{X}_{\\mathbf{V}}),\\mathbf{K}_{\\mathbf{V},\\mathbf{t}}^{\\prime}])))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, the attention query and value are formed through separate projections of $\\mathbf{Z_{op,t}}$ and $\\mathbf{X_{V}}$ respectively. These are then fed together with $\\mathbf{K}\\mathbf{v}_{,\\mathbf{t}}$ to the attention function to retrieve the new operation results $\\mathbf{Z_{res,t}}$ as shown in eq. $\\Cup$ . Intuitively, the overall process allows for both prior results and the new set of operations to jointly guide visual attention. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{\\mathbf{V},\\mathbf{t}},\\mathbf{V}_{\\mathbf{V},\\mathbf{t}}=\\mathbf{W}_{\\mathbf{V},\\mathbf{q}}(\\mathbf{Z}_{\\mathbf{op},\\mathbf{t}}),\\mathbf{W}_{\\mathbf{V},\\mathbf{v}}(\\mathbf{X}_{\\mathbf{V}}),\\mathbf{Z}_{\\mathbf{res},\\mathbf{t}}=\\mathbf{attn}(\\mathbf{Q}_{\\mathbf{V},\\mathbf{t}},\\mathbf{K}_{\\mathbf{V},\\mathbf{t}},\\mathbf{V}_{\\mathbf{V},\\mathbf{t}})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Here},\\ \\mathbf{W}_{\\mathbf{V},\\mathbf{op}}\\ \\in\\ \\mathbb{R}^{D_{m}/r\\times D_{m}},\\ \\mathbf{W}_{\\mathbf{V},\\mathbf{r}\\mathbf{s}}\\ \\in\\ \\mathbb{R}^{D_{m}/r\\times D_{m}},\\ \\mathbf{W}_{\\mathbf{V},\\mathbf{s}}\\ \\in\\ \\mathbb{R}^{D_{m}/r\\times2D_{m}/r},\\ \\mathbf{W}_{\\mathbf{V},\\mathbf{k}1}\\ }\\\\ &{\\mathbb{R}^{D_{m}/r\\times D_{v}},\\ \\mathbf{W}_{\\mathbf{V},\\mathbf{k}2}\\ \\in\\ \\mathbb{R}^{D_{m}/r\\times2D_{m}/r},\\ \\mathbf{W}_{\\mathbf{V},\\mathbf{k}3}\\ \\in\\ \\mathbb{R}^{D_{m}/r\\times D_{m}/r},\\ \\mathbf{W}_{\\mathbf{V},\\mathbf{q}}\\ \\in\\ \\mathbb{R}^{D_{m}/r\\times D_{m}}\\ \\mathrm{arc},}\\\\ &{\\mathbf{W}_{\\mathbf{V},\\mathbf{v}}\\in\\mathbb{R}^{D_{m}\\times D_{m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.3 Operation Composition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Finally, in the \u201coperation composition\" stage, the reasoner first integrates the executed operations $\\mathbf{Z_{op,t}}$ and their results $\\mathbf{Z_{res,t}}$ into the existing memory state $\\mathbf{M_{t}}$ through a simple recurrent update as shown in eqs. $\\Cup$ and $\\mathbb{\\oplus}$ . Then, to mitigate redundancy amongst parallel operations and to retrieve relevant knowledge from prior-step operations, it dynamically composes individual operation states \u2032op,t+1 with other operation states in M\u2032o $\\mathbf{M}_{\\mathbf{op,t+1}}^{\\prime}$ and also prior operation states in $\\mathbf{M}_{\\mathbf{op,t-W:t}}$ . Here, $W$ is an attention look-back window. ", "page_idx": 3}, {"type": "text", "text": "This composition is achieved through computing inter-operation attention as illustrated in fig. 3. Specifically, $\\mathbf{M}_{\\mathbf{op,t+1}}^{\\prime}$ is projected to obtain a set of queries $\\mathbf{Q_{op,t}}$ , while the token-wise concatenation of $\\mathbf{M}_{\\mathbf{op,t+1}}^{\\prime}$ and $\\mathbf{M}_{\\mathbf{op,t-W:t}}$ are projected to obtain the operation attention keys $\\mathbf{K_{op,t}}$ and values $\\mathbf{V_{op,t}}$ . A second set of values $\\mathbf{V_{res,t}}$ are also formed through projection of respective result states as shown in eq. $\\oplus$ . Further, an identity attention mask $\\mathbf{I_{N_{op}}}$ is used to ensure that operations in $\\mathbf{Q_{op,t}}$ , can only attend to other operations and not themselves. This is done to enable a higher degree of operation composition. As shown in eq. $\\sun$ , $\\mathbf{Q_{op,t}}$ , $\\mathbf{K_{op,t}}$ , $\\mathbf{V_{op,t}}$ and $\\mathbf{I_{N_{op}}}$ are passed to the attention operation, which outputs an intermediate representation \u2032o\u2032p,t+1 and the softmaxedattention weights $\\mathbf{A_{op,t}}$ . \u2032o\u2032p,t+1 is then added to a projection of M\u2032o p,t+1 to effectively combine attended operation states with the original operation states, and thereby form the next mem. operation state Mop,t+1. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Finally, the next result states are obtained by applying $\\mathbf{A_{op,t}}$ on $\\mathbf{V_{res,t}}$ and then adding a projection of $\\mathbf{M}_{\\mathbf{res,t+1}}^{\\prime}$ as shown in eq. $\\boxed{17}$ . Note $\\mathbf{A_{op,t}}$ is specifically utilized to ensure that results are composed based on attentions between operation states. Here, all the mentioned weights $\\mathbf{W}_{..}\\in\\mathbb{R}^{D_{m}\\times D_{m}}$ and $[\\cdot;\\cdot]$ represents token-wise concatenation. $\\mathbf{I_{N_{op}}}$ in eq. $\\boxed{15}$ is an identity matrix which is concatenated with zeros (i.e. unmasked) for window tokens if window len. $\\mathrm{W}>0$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{M}_{\\mathrm{op,t+1}}^{\\prime}=\\mathbf{W}_{\\mathrm{opU}}(\\mathbf{Z}_{\\mathbf{op},\\mathbf{t}})+\\mathbf{W}_{\\mathrm{opH}}(\\mathbf{M}_{\\mathrm{op,t}})}\\\\ &{\\mathbf{M}_{\\mathrm{res,t+1}}^{\\prime}=\\mathbf{W}_{\\mathrm{res}}(\\mathbf{Z}_{\\mathbf{res},\\mathbf{t}})+\\mathbf{W}_{\\mathrm{resH}}(\\mathbf{M}_{\\mathrm{res,t}})}\\\\ &{\\mathbf{Q}_{\\mathrm{op,t}}=\\mathbf{W}_{\\mathrm{op,q}}(\\mathbf{M}_{\\mathrm{op,t+1}}^{\\prime})}\\\\ &{\\mathbf{K}_{\\mathrm{op,t}}=\\mathbf{W}_{\\mathrm{op,t}}([\\mathbf{M}_{\\mathrm{op,t+1}}^{\\prime};\\mathbf{M}_{\\mathrm{op,t-W:t}}])}\\\\ &{\\mathbf{V}_{\\mathrm{op,t}}=\\mathbf{W}_{\\mathrm{op,v}}([\\mathbf{M}_{\\mathrm{op,t+1}}^{\\prime};\\mathbf{M}_{\\mathrm{op,t-W:t}}])}\\\\ &{\\mathbf{V}_{\\mathrm{res,t}}=\\mathbf{W}_{\\mathrm{res,v}}([\\mathbf{M}_{\\mathrm{res,t+1}}^{\\prime};\\mathbf{M}_{\\mathrm{res,t-W:t}}])}\\\\ &{\\mathbf{V}_{\\mathrm{res,t}}=\\mathbf{W}_{\\mathrm{res,v}}([\\mathbf{M}_{\\mathrm{res,t+1}}^{\\prime};\\mathbf{M}_{\\mathrm{res,t}}])}\\\\ &{\\mathbf{M}_{\\mathrm{op,t+1}}^{\\prime\\prime},\\mathbf{A}_{\\mathrm{op,t}}=\\mathbf{attn}(\\mathbf{Q}_{\\mathrm{op,t}},\\mathbf{K}_{\\mathrm{op,t}},\\mathbf{V}_{\\mathrm{op,t}},\\mathbf{mask}=\\mathbf{I}_{\\mathrm{Nop}})}\\\\ &{\\mathbf{M}_{\\mathrm{op,t+1}}=\\mathbf{M}_{\\mathrm{op,t+1}}^{\\prime\\prime}+\\mathbf{W}_{\\mathrm{op,u2}}(\\mathbf{M}_{\\mathrm{op,t+1}}^{\\prime})}\\\\ &{\\mathbf{M}_{\\mathrm{res,t+1}}=\\mathbf{A}_{\\mathrm{op,t}}(\\mathbf{V}_{\\mathrm{res,t}})+\\mathbf{W}_{\\mathrm{res,v2}}(\\mathbf{M}_\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Obtaining Reasoning Summary As mentioned before, our proposed mechanism outputs a set of \u201creasoning result tokens $\\prime\\prime\\,\\mathbf{Y_{R}}$ and a \u201creasoning result\" $\\mathbf{\\dot{y}_{s}}$ . $\\mathbf{Y_{R}}$ is simply equivalent to the last memory result states $\\mathbf{M}_{\\mathbf{res},\\mathbf{T}+\\mathbf{1}}$ . To obtain $\\mathbf{y_{s}}$ , we perform attention on the last operation states $\\mathbf{M}_{\\mathbf{op},\\mathbf{T}+1}$ by utilizing a summary representation $\\mathbf{l_{s}}\\in\\mathbb{R}^{D_{l}}$ of $\\mathbf{X_{L}}$ as the attention-query. ", "page_idx": 4}, {"type": "text", "text": "We set $\\mathbf{l_{s}}$ to be the first token in case of transformer-based language backbones and as last hidden state in case of LSTM-based language backbones. As shown in eq. $\\boxed{18}$ , $\\mathbf{l_{s}}$ is projected to obtain a single-token attention query $\\mathbf{p_{q}}$ while $\\mathbf{M}_{\\mathbf{op},\\mathbf{T}+1}^{-}$ is projected to obtain the attention keys $\\mathbf{P_{k}}$ . The attention value is simply the result states $\\mathbf{M}_{\\mathbf{res},\\mathbf{T}+\\mathbf{1}}$ , and the output of the attention function is the \u201creasoning result\". Intuitively, this computation corresponds to the reasoner deciding which final operation states in $\\mathbf{M}_{\\mathbf{op},\\mathbf{T}+1}$ are most relevant to the summary of the input language or task-description $\\mathbf{X_{L}}$ , based on which corresponding result states $\\mathbf{M}_{\\mathbf{res},\\mathbf{T}+\\mathbf{1}}$ are weighted and retrieved. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{p_{q}},\\mathbf{P_{k}}=\\mathbf{W_{pq,q}}(\\mathbf{l_{s}}),\\mathbf{W_{pk,k}}(\\mathbf{M_{op,T+1}})}\\\\ {\\mathbf{y_{s}}=\\arctan(\\mathbf{p_{q}},\\mathbf{P_{k}},\\mathbf{M_{res,T+1}})\\,\\,\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/b0dee1d468cf114cc73869e255754b4c25e76bcd2c81817c44e30230c31db5b8.jpg", "img_caption": ["Figure 3: Operation Composition Unit "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Reasoning mechanism general applicability. Our proposed iterative and   \nparallel reasoning mechanism is an end-to-end trainable neural module.   \nIt can be conveniently applied on top of different vision and language   \nbackbones, and be trained directly as a new computational block with no   \nspecific adjustments. Further, IPRM is weight-tied which means that its number of parameters is constant regardless of number of computation steps and parallel operations. We provide parameter and computational details along with module implementation in appendix sec. C. ", "page_idx": 4}, {"type": "text", "text": "Table 1: Comparison of IPRM with videoQA methods on STAR (left) and AGQAv2 (right). All methods operate on 32 frames unless otherwise mentioned in (). \\*Not directly compared as utilizes additional surrogate tasks / benchmarks and num. of frames not reported. ", "page_idx": 5}, {"type": "table", "img_path": "uoJQ9qadjY/tmp/d2adbc672e283a2665e033a97bb9c73ff47b72d887e9d58db2bcd6656fc8fb22.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate IPRM on STAR[79], AGQAv2[20] and CLEVRER-Humans[51] for video reasoning tasks and CLEVR-Humans $\\mathbb{135}\\mathbb{1}$ , GQA $\\mathbb{1}29\\mathbb{I}$ and CLEVR-CoGenT $\\lVert34\\rVert$ for image reasoning tasks. For all tasks, we set IPRM\u2019s parallel operations $(N_{o p})$ to 6, reasoning steps $(T)$ to 9, reduction ratio $(r)$ to 2 and window length $(W)$ to 2 (informed by ablative analysis detailed in sec. $\\boxed{3.3}$ . We follow task-specific practices (detailed in appendix $\\mathbb{C}.\\mathbb{D}$ for respective vision and language backbones in our primary experiments, besides also demonstrating integration of IPRM with large-scale VL backbones such as CLIP. Further, besides task-specific methods, we also consider two prominent transformer-based VL modules as baselines \u2013 concat-att (where lang. and vis. tokens are concatenated as in $\\mathbb{1}6\\mathbb{1}\\mathbb{3}7\\mathbb{1})$ and cross-att (where lang. tokens are \u201cquery\" to vis. tokens as \u201ckey\" and \u201cvalue\"; as in [43, 1]). Further implementation and training details are provided in appendix sec.C. ", "page_idx": 5}, {"type": "text", "text": "3.1 Video Reasoning and Question Answering (STAR, AGQAv2 and CLEVRER-Humans) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first evaluate IPRM on recent video reasoning benchmarks. STAR $\\mathbb{|}\\!\\!79\\!\\!\\!\\|$ and AGQAv2 comprise real-world videos and test multiple reasoning skills in context of situational reasoning and compositional spatiotemporal reasoning respectively. STAR contains 60K questions testing four broad types of video reasoning abilities: feasibility, interaction, prediction and sequence. Meanwhile, AGQAv2 contains 2.27M balanced questions distributed across 16 different question types. As shown in table 1, IPRM obtains $69.9\\%$ average acc. on STAR and $60.4\\%$ overall acc. on AGQAv2, outperforming prior videoQA-specific methods by $5\\%$ on both benchmarks. ", "page_idx": 5}, {"type": "text", "text": "Interestingly, on STAR IPRM obtains an $8\\%$ and $7\\%$ improvement over SeViLA-BLIP2 on the predictive and sequencing scenarios respectively. This is possibly due to IPRM\u2019s capability to reason over multiple events simultaneously across frames, which may enhance its capacity to retrieve and cumulatively reason on relevant information needed to predict future events and determine appropriate sequences. However, IPRM performs less effectively than SeViLA-BLIP2 in feasibility scenarios, possibly because these scenarios require not only visual reasoning but also commonsense knowledge, which can benefti from integration with larger-scale vision-language backbones and auxiliary training tasks such as introduced in LRR [5]. ", "page_idx": 5}, {"type": "text", "text": "Similarly, On AGQAv2, IPRM improves performances across various question types, notably achieving a $8\\%$ improvement in questions that require determining sequence of events. Further, IPRM also outperforms both 4-layer concat- and cross-attention modules on STAR (scaling further attentionlayers was not found to benefit performance as detailed in appendix table 7). ", "page_idx": 5}, {"type": "text", "text": "Next, we evaluate IPRM on the CLEVRER-Humans benchmark $\\mathbb{1}\\overline{{\\mathbb{5}\\Pi}}$ , which comprises synthetic videos of simultaneous object motions and multiple collisions, and tests a model\u2019s ability to determine causal links between events. We perform zero-shot, finetuned and from-scratch evaluation. As shown in table $\\blacktriangledown$ IPRM outpeforms task-specific neurosymbolic models NS-DR and VR-DP as well as state-of-the-art ALOE across the three settings. Specifically, IPRM improves zero-shot per-question acc. by $7\\%$ , finetuned per-question acc. by $18.8\\%$ and scratch per-question acc. by $6.2\\%$ . These results further suggest that IPRM can better track and process co-occuring events, and in this case, more accurately determine causal links. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Comparison of methods for CLEVRERHumans [51] (Opt. is per option acc. and Qs. is per question acc.). IPRM achieves state-of-art across settings. ", "page_idx": 6}, {"type": "table", "img_path": "uoJQ9qadjY/tmp/2ac35cafe91079617673c4c256436e3d18efd9b6833fd7cd5fe0d00f23e7510e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/b353fddce397196d86891e758e1c5ef56c95f25b7d6987e3a70f37f4be236dd1.jpg", "img_caption": ["Figure 4: Acc. of IPRM (blue) across program lengths for CLEVR (left) and STAR (right). IPRM has signicantly higher accs. at longer program lengths. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 3: Comparison of methods on CLEVRHumans (zero-shot and finetuned setting), CLEVRCoGenT (ValA is in-domain; ValB is out-ofdomain setting) and CLOSURE. IPRM performs strongly without requiring extra supervision. ", "page_idx": 6}, {"type": "table", "img_path": "uoJQ9qadjY/tmp/64318f91b4eb34c65ff3e339ce2f602fb6d68c2d901ae2dba822f738d85f1267.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/245334298ac512f9144b996524e248b386d629f6e15314586476563d492c9c23.jpg", "img_caption": ["Figure 5: IPRM performance on CLEVR-Humans at different training data ratios of Cross- and Concat-Att. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.2 Compositional Image Reasoning (CLEVR-Humans, CLEVR-CoGen and GQA) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Here, we evaluate IPRM on challenging compositional image reasoning benchmarks. CLEVRHumans tests generalization of multi-hop reasoning to free-form human crowdsourced questions which entail reasoning skills/scenarios beyond a model\u2019s training on the original CLEVR dataset and provides a limited finetuning set $2.5\\%$ of CLEVR). Similarly, CLEVR-CoGenT tests generalization on novel attribute compositions not observed in training (e.g.\u201cgray cubes\" and \u201cred cylinders\" are in training but eval. is on \u201cgray cylinders\" and \u201cred cubes\"; see suppl. for exact specification). The CLOSURE [3] benchmark further tests systematic generalization for different question type compositions. ", "page_idx": 6}, {"type": "text", "text": "As shown in table $\\perp$ IPRM achieves $3.9\\%$ and $3.8\\%$ improvements in zero-shot and fully-finetuned performance over prior state-of-art vision-language model MDETR. Further, IPRM neither requires bounding-box pre-training supervision (as done in MDETR) nor functional programs, and can be trained directly with only vision-language inputs and task supervision. fig. $\\blacktriangledown$ illustrates IPRM\u2019s performance across different training ratios compared to MDETR and cross- and concat-att transformer modules. Notably, IPRM exceeds MDETR\u2019s fully-finetuned performance by $1.1\\%$ with only half of training data. Further, IPRM exhibits these improvements while being relatively lightweight (4.4M params) in comparison to MDETR\u2019s transformer blocks (17.4M; ${\\sim}4\\mathbf{x}$ more parameters) and crossand concat-VL attention methods (16.8M and 12.6M respectively). These results suggest that IPRM exhibits strong generalization capabilities and more sample-efficient learning of novel reasoning skills and scenarios in context of multi-step imageQA. ", "page_idx": 6}, {"type": "text", "text": "For CLEVR-CoGenT, IPRM achieves state-of-art results in out-of-domain generalization on novel attribute compositions and outperforms MDETR (having parallel transformer compute) by $3.6\\%$ and MAC (iterative method) by $2\\%$ . This suggests the combination of iterative and parallel computation as done in IPRM can implicitly enable more disentangled feature processing and thereby improve compositional learning of primitive attributes in context of multi-step imageQA. Similarly, on CLOSURE, IPRM achieves an average zero-shot accuracy of $75.6\\%$ which is highest amongst fully neural reasoning methods (that require no extra supervision) and is close to the neurosymbolic method NS-VQA which utilizes ground truth programs and object bounding boxes supervision. Further, detailed breakdown of models on CLOSURE question types is provided in appendix table 6. ", "page_idx": 6}, {"type": "text", "text": "Table 4: Performance comparison on GQA with imageQA methods and large-scale models that do not utilize ground-truth scene graphs. \\* indicates large-scale pretrained VL model. \\*\\*Utilizes ground truth scene graphs, programs and bounding boxes for auxiliary training. ", "page_idx": 7}, {"type": "text", "text": "LCGN [25] MCAN[87] LXMERT\\*[66] 12-in-1\\*[49] OSCAR\\*[46] CFR\\*\\* [55] IPRM GQA 55.8 57.4 60.0 60.0 61.6 72.1 60.3 ", "page_idx": 7}, {"type": "text", "text": "We also evaluate IPRM on the GQA benchmark which tests compositional VQA on real-world images. We perform comparisons with prior VQA methods as well as large-scale VL models such as LXMERT and 12-in-1 that do not utilize the ground truth scene graphs in GQA. As shown in table 4, IPRM achieves $60.3\\%$ , outperforming prior reasoning methods such as MCAN and LCGN as well as large-scale VL models such as LXMERT and 12-in-1. However, IPRM\u2019s performance is behind the larger VL model OSCAR which performs pretraining on $4.3\\mathbb{M}$ data samples collated from COCO, VG, SBU and Flickr. Note, IPRM is a standalone reasoning module only trained on GQA balanced with no pre-training. Further, IPRM when trained with perfect perception (i.e. ground truth object bounding boxes and attributes), achieves $87.2\\%$ on GQA validation set suggesting strong reasoning performances can be achieved through advancements in visual detectors and backbones. Finally, in appendix $\\boxed{\\textcircled{1.3}}$ we demonstrate that IPRM more effectively enhances the performance of frozen CLIP variants on complex reasoning benchmarks compared to scaling traditional cross- and concat-attention modules. ", "page_idx": 7}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/5d4e5618e80cfcea4d1d7a3b82976238003d759d503e1c9ecf713d4c0e5bab22.jpg", "img_caption": ["Figure 6: IPRM Model ablations in order: (i) Impact of number of parallel operations $(N_{o p})$ vs computation steps $(T)$ . (ii) Impact of Operation Composition Block (OPC). (iii): Impact of reduction ratio $(r)$ and (iv) memory window length $(W)$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "3.3 Model ablations and reasoning visualization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We perform ablations to study the contributions of IPRM\u2019s salient components. First, we analyze the impact of varying number of parallel operations $(N_{o p})$ against number of iterative computation steps $(T)$ . We compare models with $N_{o p}\\in\\{1,3,6,9\\}$ and $\\bar{T}\\in\\{1,3,6,9\\}$ , resulting in 16 different models. Given the large amount of ablative models, we perform analysis on a reduced-resolution setting of CLEVR-Humans with pretraining on CLEVR for 15 epochs. As shown in fig. $\\boxed{6}$ (plot (i)), we find that $T$ and $N_{o p}$ appear to be co-dependent and that neither by itself can lead to high performance. E.g. setting $\\bar{T}=1$ generally results in peformances around $75\\%$ regardless of $N_{o p}$ , while setting $N_{o p}{=}\\,1$ or 3 results in a sharp performance drop of $3\\%$ when changing $T$ to 9 from 6. In contrast, for $N_{o p}>3$ , we observe that performance increases steadily with higher $T$ , suggesting that a higher number of parallel operations may prevent overftiting in a model with high computation steps. Overall, we find that $N_{o p}=6$ , $T=9$ ) and $N_{o p}=9$ , $T=9$ ) are the two best performing models achieving above $82\\%$ accuracy (with the former preferred as $N_{o p}=6$ performs better for diff. $T$ compared to $N_{o p}=9$ ). ", "page_idx": 7}, {"type": "text", "text": "Next, we study the impact of the operation composition block (OPC) by evaluating $N_{o p}=6$ (and diff. computation steps $T$ ) with and without OPC. As shown in fig. $\\boxed{6}$ (plot (ii)), while $\\bar{T}=1$ has a relatively low drop of ${\\sim}2\\%$ , the performance drops are more significant for higher $T$ . The $\\mathit{\\Pi}_{\\leftmoon}=0$ , $T=9$ ) model which reached ${\\sim}82\\%$ acc. with OPC, drops to ${\\sim}74\\%$ acc. without OPC. ", "page_idx": 7}, {"type": "text", "text": "Finally, we study the impacts of the dimension reduction ratio $(r)$ and memory-lookback window length $(W)$ . As shown in fig. $\\boxed{6}$ (plot (iii)), $r=2$ leads to negligible drop $(0.3\\%)$ in performance compared to no dimension reduction (i.e. $r=1$ ) while being computationally faster and requiring less memory. However, setting $r$ to 8 significantly deteriorates performance. Similarly, as shown in fig. $\\bigstar$ (plot (iv)), a setting of window-length $W$ is 2 works sufficiently well, and decreasing it below that significantly impacts performance. ", "page_idx": 7}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/c76b33960e7cda08bc40f94bd1a9fe3b9d669cad2e260bc3c5fb6254c4eaac9d.jpg", "img_caption": ["Figure 7: Condensed reasoning visualization of IPRM. In the top two examples, IPRM correctly utilizes both parallel and iterative computation to arrive at the correct answer. The bottom left example shows IPRM\u2019s cumulative lang. and visual attentions when solving a real-world GQA example. The bottom right example, shows an error case where IPRM seems to misunderstand question and outputs wrong ans. with less relevant attentions. See appendix for further reasoning visualizations and error cases, and supplemental for further CLIP integrated visualizations on GQA examples. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In fig. 4, we assess the performance of IPRM across different functional program lengths (a proxy for reasoning steps) on the CLEVR and STAR benchmarks. As shown, IPRM maintains a high performance across both short and longer reasoning steps and for both image and video VQA scenarios. We also provide a condensed illustration of IPRM\u2019s reasoning visualization in fig. 7. Here, for brevity, we only show the model\u2019s visual attention for a subset of parallel operations and computation steps (see suppl. for full visualization). The top left example (\u201cwhat shape .. max occuring color\") illustrates the model\u2019s usage of both iterative and parallel computation. At the first reasoning step, the model appears to be doing parallel operations to both identify \u201clarge cylinder\" and compute \u201cmax occuring color\". In the next step, it appears to have found \u201cmax occuring color\" (cyan) and seems to check which of the two large cylinders match the color. Next, it highlights the correct cylinder and in the final step, it locates and predicts the correct object behind. The top right example in fig. 7 similarly shows computation for a question involving spatial (\u201cfarthest right\"), similarity (\u201csame shape\") and counting (\u201chow many\") operations. The bottom left example illustrates IPRM\u2019s simultaneous language and visual attentions updated across reasoning steps when solving a real world (GQA) example. Finally, the bottom right example is an error-case where IPRM seems to incorrectly understand or execute requisite operations, thereby not producing expected attentions for \u201citems with matching material and color\" throughout steps. ", "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Visual reasoning methods and vision-language models. Multiple prior works have introduced effective visual reasoning methods in context of image and video question answering [35, 52, 2, 85, 15, 50, 69, 56, 32, 54, 53, 76]. Prominent works include NMN[2], FILM [60], NSM [27], MAC [28], MCAN [87], NS-VQA [85], ALOE [12], VR-DP [13], SHG-VQA [68], MIST [18] and OCRA $\\mathbb{1}\\!\\!\\!\\prod\\!\\!\\!\\!\\bigcirc\\!\\!\\!\\!\\!\\|}$ . In contrast to these works that show applicability of methods for particular VQA benchmarks/tasks, our work explores a more general direction of integrating parallel and iterative computation in a reasoning framework that we show to be effective for multiple complex VQA benchmarks and reasoning scenarios. More recently, vision-language models [44, 73, 37, 66, 45, 46, 74] and multimodal large-language-models [57, 47] with transformer-based mechanisms have shown impressive reasoning capabilities at scale. Notable examples include CLIP [61], GPT [57], Gemini [67], MDETR [36], LXMERT [66], VinVL [88], BLIP [44, 43], Flamingo [1], LlavA [47], BEiT [74] and All-in-One [72]. We believe our work is complimentary to these developments, as it contributes an alternative and possibly more effective reasoning mechanism that can be integrated with such models in future to enhance complex VQA capabilities. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Memory and recurrence-augmented transformers. Multiple works have identified limitations of purely feedforward computation as realized in transformers and worked on encoding recurrence [30, 26, 11, 71] and memory-augmented computation $\\lVert80,\\rVert8\\rVert$ . Notably, Recurrent Memory Transformer $\\lVert\\mathbf{8}\\rVert$ and MemFormer $\\boxed{81}$ introduce recurrent and dynamic memory to improve language modelling capabilities. More recently, EMAT $\\lVert\\boldsymbol{82}\\rVert$ introduces efficient memory to augment knowledge retrieval, MemViT $\\lVert\\overline{{80}}\\rVert$ introduces a cache memory to retain prior context for long-video tasks, and [71] introduces memory for more effective action anticipation. While these methods study recurrent and memory-augmented computation on specific natural language processing and computer vision tasks, our work focuses on the integration of iterative-parallel computation and working memory in a single neural reasoning mechanism beneficial for complex VQA scenarios. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced a novel fully-differentiable and end-to-end trainable iterative and parallel reasoning mechanism (IPRM) to address complex VQA scenarios. We comprehensively evaluated IPRM on various complex image and video VQA benchmarks testing distinct reasoning capabilities, and found it improves state-of-arts on multiple such benchmarks. We also performed quantiative ablations to study individual impacts of parallel and iterative computation besides qualitative analysis of IPRM\u2019s reasoning computation visualization. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Here, we note possible limitations of IPRM. Similar to existing VQA and deep-learning methods, IPRM may reflect biases that are present in the training distribution of VQA benchmarks. This may lead it to overfit to certain image inputs or question forms and possibly provide skewed answers in such scenarios. Further, the utilized vision-language backbones in our experiments may also entail visual, language and cultural biases in their original training distribution which may permeate to IPRM upon integration for VQA scenarios. In this regard, we hope the capability to visualize intermediate reasoning of IPRM and diagnose its error cases (as shown in section $\\boxed{3.3\\}$ can serve a useful tool to benefti interpretability in VQA and identify possible reasoning biases that may emerge in the model. ", "page_idx": 9}, {"type": "text", "text": "For future work, scaling the IPRM architecture to a foundational video-language model by integrating it with large-scale transformer-based vision-language models and relevant instruction-tuning approaches presents an exciting research opportunity. Moreover, while we designed and evaluated IPRM in the context of complex VQA, we believe it has the potential to operate as a general reasoning mechanism applicable to tasks beyond visual reasoning and question answering, such as language processing and embodied reasoning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment This research/project is supported by the National Research Foundation, Singapore, under its NRF Fellowship (Award# NRF-NRFF14-2022-0001). This research is also supported by funding allocation to B.F. and C.T. by the Agency for Science, Technology and Research (A\\*STAR) under its SERC Central Research Fund (CRF), as well as its Centre for Frontier AI Research (CFAR). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.   \n[2] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Learning to compose neural networks for question answering. arXiv preprint arXiv:1601.01705, 2016.   \n[3] D. Bahdanau, H. de Vries, T. J. O\u2019Donnell, S. Murty, P. Beaudoin, Y. Bengio, and A. Courville. Closure: Assessing systematic generalization of clevr models. arXiv preprint arXiv:1912.05783, 2019.   \n[4] Z. Bai, R. Wang, and X. Chen. Glance and focus: Memory prompting for multi-event video question answering. Advances in Neural Information Processing Systems, 36, 2024.   \n[5] A. Bhattacharyya, S. Panchal, R. Pourreza, M. Lee, P. Madan, and R. Memisevic. Look, remember and reason: Grounded reasoning in videos with language models. In The Twelfth International Conference on Learning Representations.   \n[6] B. Bogin, S. Subramanian, M. Gardner, and J. Berant. Latent compositional representations improve systematic generalization in grounded question answering. Transactions of the Association for Computational Linguistics, 9:195\u2013210, 2021.   \n[7] S. Buch, C. Eyzaguirre, A. Gaidon, J. Wu, L. Fei-Fei, and J. C. Niebles. Revisiting the\" video\" in video-language understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2917\u20132927, 2022.   \n[8] A. Bulatov, Y. Kuratov, and M. Burtsev. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11079\u201311091, 2022.   \n[9] A. Capon, S. Handley, and I. Dennis. Working memory and reasoning: An individual differences perspective. Thinking & Reasoning, 9(3):203\u2013244, 2003.   \n[10] Y. Cong, W. Liao, H. Ackermann, B. Rosenhahn, and M. Y. Yang. Spatial-temporal transformer for dynamic scene graph generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 16372\u201316382, 2021.   \n[11] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and \u0141. Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.   \n[12] D. Ding, F. Hill, A. Santoro, M. Reynolds, and M. Botvinick. Attention over learned object embeddings enables complex visual reasoning. Advances in neural information processing systems, 34:9112\u20139124, 2021.   \n[13] M. Ding, Z. Chen, T. Du, P. Luo, J. Tenenbaum, and C. Gan. Dynamic visual reasoning by learning differentiable physics models from video and language. Advances in Neural Information Processing Systems, 34:887\u2013899, 2021.   \n[14] N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin, S. Welleck, P. West, C. Bhagavatula, R. Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024.   \n[15] C. Fan, X. Zhang, S. Zhang, W. Wang, C. Zhang, and H. Huang. Heterogeneous memory enhanced multimodal attention model for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1999\u20132007, 2019.   \n[16] H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu. Rmpe: Regional multi-person pose estimation. In Proceedings of the IEEE international conference on computer vision, pages 2334\u20132343, 2017.   \n[17] D. Fougnie. The relationship between attention and working memory. New research on short-term memory, 1:45, 2008.   \n[18] D. Gao, L. Zhou, L. Ji, L. Zhu, Y. Yang, and M. Z. Shou. Mist: Multi-modal iterative spatialtemporal transformer for long-form video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14773\u201314783, 2023.   \n[19] R. Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 1440\u20131448, 2015.   \n[20] M. Grunde-McLaughlin, R. Krishna, and M. Agrawala. Agqa: A benchmark for compositional spatio-temporal reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11287\u201311297, 2021.   \n[21] M. Grunde-McLaughlin, R. Krishna, and M. Agrawala. Agqa 2.0: An updated benchmark for compositional spatio-temporal reasoning. arXiv preprint arXiv:2204.06105, 2022.   \n[22] J. Hsu, J. Mao, J. Tenenbaum, and J. Wu. What\u2019s left? concept grounding with logic-enhanced foundation models. Advances in Neural Information Processing Systems, 36, 2024.   \n[23] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132\u20137141, 2018.   \n[24] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko. Learning to reason: End-to-end module networks for visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 804\u2013813, 2017.   \n[25] R. Hu, A. Rohrbach, T. Darrell, and K. Saenko. Language-conditioned graph networks for relational reasoning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10294\u201310303, 2019.   \n[26] F. Huang, K. Lu, C. Yuxi, Z. Qin, Y. Fang, G. Tian, and G. Li. Encoding recurrence into transformers. In The Eleventh International Conference on Learning Representations, 2022.   \n[27] D. Hudson and C. D. Manning. Learning by abstraction: The neural state machine. Advances in Neural Information Processing Systems, 32, 2019.   \n[28] D. A. Hudson and C. D. Manning. Compositional attention networks for machine reasoning. arXiv preprint arXiv:1803.03067, 2018.   \n[29] D. A. Hudson and C. D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019.   \n[30] D. Hutchins, I. Schlag, Y. Wu, E. Dyer, and B. Neyshabur. Block-recurrent transformers. Advances in Neural Information Processing Systems, 35:33248\u201333261, 2022.   \n[31] S. Jaiswal, B. Fernando, and C. Tan. Tdam: Top-down attention module for contextually guided feature selection in cnns. In European Conference on Computer Vision, pages 259\u2013276. Springer, 2022.   \n[32] A. Jha, B. Patro, L. Van Gool, and T. Tuytelaars. Barlow constrained optimization for visual question answering. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1084\u20131093, 2023.   \n[33] C. Jing, Y. Jia, Y. Wu, X. Liu, and Q. Wu. Maintaining reasoning consistency in compositional visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5099\u20135108, 2022.   \n[34] J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901\u2013 2910, 2017.   \n[35] J. Johnson, B. Hariharan, L. Van Der Maaten, J. Hoffman, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick. Inferring and executing programs for visual reasoning. In Proceedings of the IEEE international conference on computer vision, pages 2989\u20132998, 2017.   \n[36] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1780\u20131790, 2021.   \n[37] W. Kim, B. Son, and I. Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021.   \n[38] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[39] T. M. Le, V. Le, S. Venkatesh, and T. Tran. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9972\u20139981, 2020.   \n[40] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7331\u20137341, 2021.   \n[41] M. Lewis, N. V. Nayak, P. Yu, Q. Yu, J. Merullo, S. H. Bach, and E. Pavlick. Does clip bind concepts? probing compositionality in large image models. arXiv preprint arXiv:2212.10537, 2022.   \n[42] C. Li, H. Xu, J. Tian, W. Wang, M. Yan, B. Bi, J. Ye, H. Chen, G. Xu, Z. Cao, et al. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. arXiv preprint arXiv:2205.12005, 2022.   \n[43] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.   \n[44] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.   \n[45] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.   \n[46] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXX 16, pages 121\u2013137. Springer, 2020.   \n[47] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.   \n[48] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[49] J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee. 12-in-1: Multi-task vision and language representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10437\u201310446, 2020.   \n[50] J. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. arXiv preprint arXiv:1904.12584, 2019.   \n[51] J. Mao, X. Yang, X. Zhang, N. Goodman, and J. Wu. Clevrer-humans: Describing physical and causal events the human way. Advances in Neural Information Processing Systems, 35:7755\u2013 7768, 2022.   \n[52] D. Mascharka, P. Tran, R. Soklaski, and A. Majumdar. Transparency by design: Closing the gap between performance and interpretability in visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4942\u20134950, 2018.   \n[53] S. S. Mondal, J. D. Cohen, and T. W. Webb. Slot abstractors: Toward scalable abstract visual reasoning. arXiv preprint arXiv:2403.03458, 2024.   \n[54] S. S. Mondal, T. Webb, and J. D. Cohen. Learning to reason over visual objects. arXiv preprint arXiv:2303.02260, 2023.   \n[55] B. X. Nguyen, T. Do, H. Tran, E. Tjiputra, Q. D. Tran, and A. Nguyen. Coarse-to-fine reasoning for visual question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4558\u20134566, 2022.   \n[56] D. K. Nguyen, V. Goswami, and X. Chen. Movie: Revisiting modulated convolutions for visual counting and beyond. In International Conference on Learning Representations, 2020.   \n[57] R. OpenAI. Gpt-4 technical report. ArXiv, 2303, 2023.   \n[58] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[59] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.   \n[60] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[61] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[62] R. Shrestha, K. Kafle, and C. Kanan. Answer them all! toward universal visual question answering models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10472\u201310481, 2019.   \n[63] A. Suhr, M. Lewis, J. Yeh, and Y. Artzi. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217\u2013223, 2017.   \n[64] A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y. Artzi. A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6418\u20136428, 2019.   \n[65] H. Tan and M. Bansal. Object ordering with bidirectional matchings for visual reasoning. In Proceedings of NAACL-HLT, pages 444\u2013451, 2018.   \n[66] H. Tan and M. Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100\u20135111, 2019.   \n[67] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[68] A. Urooj, H. Kuehne, B. Wu, K. Chheu, W. Bousselham, C. Gan, N. Lobo, and M. Shah. Learning situation hyper-graphs for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14879\u201314889, 2023.   \n[69] M. Vaishnav and T. Serre. Gamr: A guided attention model for (visual) reasoning. In The Eleventh International Conference on Learning Representations, 2022.   \n[70] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[71] J. Wang, G. Chen, Y. Huang, L. Wang, and T. Lu. Memory-and-anticipation transformer for online action understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13824\u201313835, 2023.   \n[72] J. Wang, Y. Ge, R. Yan, Y. Ge, K. Q. Lin, S. Tsutsui, X. Lin, G. Cai, J. Wu, Y. Shan, et al. All in one: Exploring unified video-language pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6598\u20136608, 2023.   \n[73] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340. PMLR, 2022.   \n[74] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som, et al. Image as a foreign language: Beit pretraining for vision and visionlanguage tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19175\u201319186, 2023.   \n[75] Y. Wang, K. Li, Y. Li, Y. He, B. Huang, Z. Zhao, H. Zhang, J. Xu, Y. Liu, Z. Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022.   \n[76] T. Webb, S. S. Mondal, and J. D. Cohen. Systematic visual reasoning through object-centric relational abstraction. Advances in Neural Information Processing Systems, 36, 2024.   \n[77] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n[78] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38\u201345, 2020.   \n[79] B. Wu, S. Yu, Z. Chen, J. B. Tenenbaum, and C. Gan. Star: A benchmark for situated reasoning in real-world videos. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.   \n[80] C.-Y. Wu, Y. Li, K. Mangalam, H. Fan, B. Xiong, J. Malik, and C. Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13587\u201313597, 2022.   \n[81] Q. Wu, Z. Lan, K. Qian, J. Gu, A. Geramifard, and Z. Yu. Memformer: A memory-augmented transformer for sequence modeling. In Y. He, H. Ji, S. Li, Y. Liu, and C.-H. Chang, editors, Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, pages 308\u2013318, Online only, Nov. 2022. Association for Computational Linguistics.   \n[82] Y. Wu, Y. Zhao, B. Hu, P. Minervini, P. Stenetorp, and S. Riedel. An efficient memoryaugmented transformer for knowledge-intensive nlp tasks. arXiv preprint arXiv:2210.16773, 2022.   \n[83] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked attention networks for image question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21\u201329, 2016.   \n[84] K. Yi, C. Gan, Y. Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019.   \n[85] K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. Advances in neural information processing systems, 31, 2018.   \n[86] S. Yu, J. Cho, P. Yadav, and M. Bansal. Self-chained image-language model for video localization and question answering. arXiv preprint arXiv:2305.06988, 2023.   \n[87] Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian. Deep modular co-attention networks for visual question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6281\u20136290, 2019.   \n[88] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5579\u20135588, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B Elaborated Experiments and Results Discussion ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Further comparisons on CLEVR-Humans, CLEVR-CoGenT, CLOSURE and NLVRv1 ", "page_idx": 15}, {"type": "text", "text": "Here, we provide further comparisons with benchmark-specific methods for CLEVR-Humans $\\mathbb{|35|}$ , CLEVR-CoGenT $\\lVert\\boldsymbol{34}\\rVert$ , CLOSURE $\\mathbb{|3|}$ and NLVRv1 $\\lVert6\\bar{3}\\rVert$ (not reported in main paper due to space limitations). As mentioned in main paper, these benchmarks utilize synthetic images and are a test of pure visual reasoning capabilities that are minimally influenced by increased world knowledge or usage of stronger visual backbones. ", "page_idx": 15}, {"type": "text", "text": "CLEVR-Humans as already mentioned in main paper evaluates a model\u2019s reasoning generalization capabilities to unseen scenarios or question forms. CLEVR-CoGenT studies compositional attribute generalization. Specifically, it has two conditions \u2013 i) cond.A wherein all cubes have color $\\in\\ \\{g r a y,b l u e,b r o w n,y e\\bar{l}l o w\\}$ and cylinders $\\in\\ \\{r e d,g r e e n,p u r p l e,c y a n\\}$ (spheres can be any color), and ii) cond.B wherein color-sets are switched b/w cubes and cylinders. A model is then trained on one condition and evaluated on both the original and alternate condition. A higher accuracy on the alternate condition indicates that the model learns more \u2018compositionally\u2019 as it generalizes better to novel shape-color combinations with less feature/attribute combination overfitting. ", "page_idx": 15}, {"type": "text", "text": "Table 5: Elaborated results on CLEVR-Humans (left), CLEVR-CoGenT (middle) and NLVRv1 (right). IPRM achieves state-of-art across the three benchmarks and does not require additional supervision such as bounding boxes or functional programs. \\* requires func. programs supervision / pre-defined dataset-specific neural modules. \u25bcrequires object bounding-boxes supervision. ", "page_idx": 15}, {"type": "table", "img_path": "uoJQ9qadjY/tmp/7ade0c7863c49c03d784d1075ce47018063ace1c19d2a30d135c2b1e8522923e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Finally, NLVRv1 evaluates language-grounded visual reasoning. Each sample of this benchmark comprises a set of three synthetic images and a composite natural language statement about the images which can evaluate to True or False and requires various visual-linguistic reasoning skills. ", "page_idx": 15}, {"type": "text", "text": "As shown in table $\\triangledown$ IPRM achieves state-of-art results across the three benchmarks and does not require pre-annotated bounding-boxes or functional programs as additional supervision. For CLEVR-Humans (table $\\blacktriangledown$ left), it outperforms larger-scale models such as MDETR and RAMEN in zero-shot performance even though the latter is pre-trained on multiple VQA datasets. It also increases state-of-art in finetuned setting by $3.8\\%$ . ", "page_idx": 15}, {"type": "text", "text": "For CLEVR-CogenT (table $\\blacktriangleleft$ centre) , IPRM achieves the highest generalization results amongst methods in both the CoGen-Train A and Finetune B. Specifically, it obtains $80.3\\%$ acc. on cond. B (when trained on cond. A), which is $1.5\\%$ higher than the previous state-of-art cond.B method FILM and $3.6\\%$ higher than MDETR. When further finetuned on cond.B, IPRM generalizes for both cond.A and cond.B achieving $98.0\\%$ and $98.2\\%$ unlike FILM which overftis to cond.B and thereby has poor performance on cond.A. Further, its performance on cond.A $(99.1\\%)$ is highest amongst methods that do not utilize bounding box or localization supervision and marginally lower than MDETR and NS-VQA (which utilize bounding-box supervision). ", "page_idx": 15}, {"type": "text", "text": "For NLVRv1 (table $\\blacktriangledown$ right), IPRM model trained from scratch achieves $63.8\\%$ acc. and performs competitively with existing task-specific state-of-art model CNN-BiAtt. When finetuned from its CLEVR checkpoint, we find IPRM achieves $73.0\\%$ acc. which is $7\\%$ higher than existing visual inputs state-of-art for NLVRv1 and suggests strong reasoning transfer capabilities of IPRM. It further outperforms the N2NMN method which requires pre-defined neural modules to be identified for the dataset. ", "page_idx": 15}, {"type": "text", "text": "Finally, we have also provided results of models on CLOSURE for different subsets / question types in table 6. For the CLEVR-environment, we have visualized some failure cases in figs. 10 and 11. More cases can be run through the visualization framework in provided source code. ", "page_idx": 16}, {"type": "table", "img_path": "uoJQ9qadjY/tmp/126f54bbea94cc0dd794efa0c142f38b3b3eb645d511e08dcb4b736f38fec55d.jpg", "table_caption": ["Table 6: Zero-shot performances of models on CLOSURE subsets (2 trials run for IPRM) "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Elaborated STAR results and ablations for video reasoning tasks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide results on the STAR Test, further baselines and model ablations for video reasoning tasks in table 7. ", "page_idx": 16}, {"type": "text", "text": "Table 7: Left: Results on STAR $\\mathbb{1}\\!\\!\\mathscr{D}\\mathbb{1}$ official hidden test set (evaluation server) with ground-truth vision (GT V) and predicted vision (PR V); Right: Results on STAR val. set with num. of sampled frames $=\\!32$ unless otherwise stated in (). ", "page_idx": 16}, {"type": "table", "img_path": "uoJQ9qadjY/tmp/1e714f38bff2897104c650526eaaef7758b340c3f1f0d57fd07c20dab8e9abfd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.3 CLIP Integration Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide results with frozen CLIP $\\mathbb{1}\\underline{{6}}\\Pi$ visual backbones including CLIP VIT-L/14, CLIP VITB/16 and CLIP VIT- $\\cdot{\\mathrm{L}}/14@336{\\mathrm{px}}$ on GQA [29], NLVRv2 [64] and CLEVR-Humans in table 8. We empirically found utilizing a Distil-roberta language backbone to be more effective than the associated CLIP language backbone, and hence used the former for question processing. We compare with alternate prominent vision-language attention mechanisms including Cross-att and Concat-att blocks as well as a simple joint projection of vision and language pooled representations (referred as Wt-Proj-Att). As shown in the table, IPRM can enhance performance for the CLIP variants across GQA, NLVRv2 and CLV-Humans in comparison to concat and cross-att blocks. Further, it is more parameter efficient with only 5.5M additional parameters in comparison to 4-layer as well as 2-layer stacks of Cross-Att (9.2M 2-layer, 17.6M 4-layer) and Concat-Att (7.2M 2-layer, 13.6M 4-layer). With regards to computational FLOPs, IPRM consumes 5.9GFLOPs which is marginally higher than Cross-Att 4-layer config (3.1GFLOPs) and lower than Concat-Att 4-layer config (8.9GFLOPs). Note, that the performance benefits of adding further layers of cross- or concat-att blocks are observed to be minimal after 4 layers, and can also depend on the amount of training data available. E.g. Both cross- and concat-att blocks of 2 layers had better performances on NLVRv2 (which has a limited set of training questions relative to GQA and CLEVR) in comparison to 4 layer config. ", "page_idx": 16}, {"type": "text", "text": "B.4 Further reasoning computation visualizations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide elaborate reasoning computation visualizations of IPRM showing the lang. and vis. attentions across parallel operations and computation steps during operation formation and operation execution stages. Fig. $\\boxed{8}$ shows a scenario wherein IPRM correctly utilizes parallel and iterative computations to compute intermediate operations of \u201cfind object close to front\", \u201cretrieve/compare ", "page_idx": 16}, {"type": "text", "text": "Table 8: Left: Comparison of IPRM with prominent vision-language attention mechanisms with CLIP VIT-L/14 backbones on CLEVR-Humans, GQA and NLVRv2 benchmarks (\u20184L\u2019 indicates 4 att layers; $\\mathbf{\\dot{\\omega}}_{\\mathbf{X}}\\,\\mathbf{\\dot{\\omega}}_{}$ indicates model did not converge). Right: Results with other CLIP variants VIT-B and VIT-L $@$ 336 on GQA and NLVRv2. ", "page_idx": 17}, {"type": "table", "img_path": "uoJQ9qadjY/tmp/832b1fd98dab94b5e780167540424cc53454c90ea0282f7e6e13302f9deffb09.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "uoJQ9qadjY/tmp/c6fba6cbb7b6e0162814403146fb237fededa4e7715b20974f1c5eb925942990.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "shape and size\", \u201cfind applicable objects with both same shape and size\". Fig. 9 shows another correct prediction of IPRM, and this time, its intermediate reasoning visualization is useful to determine that the entailed reasoning appears sensible. Fig. 10 shows an incorrect prediction by IPRM and its intermediate reasoning visualizations also suggest that IPRM did not understand the question and thereby did not attend to relevant objects. Finally, Fig. 11 shows a scenario wherein while IPRM produces the correct answer, it\u2019s intermediate reasoning appears imprecise which makes the prediction (and underlying reasoning) less reliable. We provide further visualizations with a CLIP VIT-L/14 backbone on GQA samples in the supplemental jupyter notebook output (html format for easier viewing). ", "page_idx": 17}, {"type": "text", "text": "C Model implementation and experiment details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We implement IPRM in PyTorch $\\lVert\\boldsymbol{58}\\rVert$ as a generic vision-language module receiving a set of input vision (or scene-representation) tokens and input language (or task-representation) tokens. We provide Python-style pseudocode of IPRM in figs 12, 13 and 14. For all experiments, we set the internal dimension of IPRM to 512 and use the same configuration of num. parallel operations $(N_{o p}){=}6$ , num. computation steps $(\\mathrm{T}){=}9$ , reduction ratio $(\\mathbf{r}){=}2$ and window size $(\\mathbf{W}){=}2$ . We follow benchmark-specific conventions for vision-language backbones that are detailed below in sec. C.1. For CLIP $\\lVert6\\rVert$ , we utilize the official models from Huggingface $\\mathbb{1}\\!\\!\\!/\\!\\!\\!\\!/7\\!\\!\\!\\!\\!/\\!\\!\\!\\!\\!/$ . All experiments are performed on a single NVIDIA A40 GPU with 46GB memory and averaged over 3 trials with different random seeds wherever possible (done for primary experiments on STAR, AGQA, CLEVRER-Humans, CLEVR-Humans). Unless otherwise specified, the learning rate is initialized to 1e-4 with Adam [38] optimizer and gradient clipping value of 8. The learning-rate is reduced based on validation acc. plateau with reduction factor 0.5, threshold 0.001 and patience 0. Further experiment hyperparameters and settings are provided below. Source code at: https://github.com/shantanuj/ IPRM_Iterative_and_Parallel_Reasoning_Mechanism. ", "page_idx": 17}, {"type": "text", "text": "C.1 Benchmark-specific experiment details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "CLEVR-Humans. We use the CLEVR-Humans dataset from $\\mathbb{135}\\mathbb{1}$ which comprises images from original CLEVR dataset $\\mathbb{134}\\mathbb{1}$ and human crowdsourced questions. We use a batch size of 216 for training. We use the same language encoder (Distil-Roberta[48] from Huggingface $\\lVert\\boldsymbol{78}\\rVert$ ) as in existing state-of-art MDETR $\\lVert\\vec{36}\\rVert$ and frozen ResNet101 backbone layer 3 spatial features (as in $\\lVert\\overline{{28}},\\rVert\\overline{{52}},\\lvert35\\rvert$ ). We perform all ablation experiments with $14\\mathrm{x}14\\mathrm{x}1024$ visual features. Each ablation model is first pretrained for 10 epochs on the original CLEVR dataset (the initial learning rate for IPRM is 1e-4 and for language encoder is 1e-5) and then finetuned on CLEVR-Humans for 40 epochs with early stopping (learning rate of 1e-4 throughout). As observed in prior work $\\lVert52\\rVert$ , we similarly found in multiple scenarios with occluded objects that visual attention only partially identified such objects. Hence, we simply resampled (bilinear sampling) visual input to obtain 16x16x1024 features and empirically found more complete visual attentions with a corresponding $1.1\\%$ improvement in accuracy. The final two best performing model configurations (Nop $=6,$ , $T{=}9$ , $W{=}2$ , $R{=}2$ and $N o p{=}6_{\\cdot}$ , $T{=}9$ , $W{=}2$ , $\\scriptstyle R=I$ ) from ablations were then pre-trained for 35 epochs on CLEVR and finetuned on CLEVR-Humans. While we found that configuration $N o p{=}\\delta,$ , $T{=}9$ , $W{=}2$ , $R{=}I$ obtains highest zero-shot (ZS) acc. of $65.6\\%$ and finetuned (FT) acc. of $86.3\\%$ , we adopt $N o p{=}6$ , $T{=}9,$ , $W{=}2$ , $R{=}2$ (with $63.3\\%$ ZS and $85.4\\%$ FT acc.) as our optimal model given its lesser parameters and FLOPs. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "GQA. We use the GQA compositional real-world image question answering dataset from $\\mathbb{|29|}$ . Based on prior VQA methods on GQA [29, 25, 45, 33], we utilize pre-extracted bounding-box object proposal features and object label predictions obtained from a pretrained object detector [19, $\\bar{\\left|88\\right|}$ . The bounding box coordinates is normalized to range of 0 to 1 based on the original input image size, and the 4 coordinates are transformed to a distributed representation through a learned nonlinear projection. This representation is concatenated with a learned projection of the predicted object labels (initialized with glove $\\lVert59\\rVert$ 300dim embeddings) to form the final visual input. We train IPRM for 25 epochs with a batch-size of 192 and same hyperparameters as before. We evaluate the final model on both the test-dev split and the official test evaluation server (https: //eval.ai/featured-challenges/225/evaluation). ", "page_idx": 18}, {"type": "text", "text": "STAR-VideoQA. We use the STAR-VideoQA dataset for situational reasoning on real-world videos from $\\mathbb{\\lVert}79\\rVert$ . Based on previous videoQA methods [79, 40, 45] for STAR, we utilize object bounding boxes, labels, human pose and human-object relations across frames (note: we do not use the situation hyper-graphs or functional programs). We first perform experiments with the provided ground truth object bounding boxes, labels and human-object relations as well as provided human pose predictions from Alphapose $\\boxed{16}$ as reported in main paper. Each of these is projected to a distributed representation through learned non-linear projections to obtain object token-wise representations. A further learnable positional embedding for each frame is added to these representations which are then flattened across frames to form the visual input to IPRM. For the language encoder, we found both a simple Bi-LSTM and Distil-roberta language encoder obtain similar performance, and hence choose the simpler Bi-LSTM as the language model. We evaluated models on both 16 sampled frames and 32 sampled frames, and empirically found using 32 frames has ${\\sim}1.3\\%$ higher performance and used it for our primary experiments. A batch size of 64 was used with learning rate 1e-4 over 20 epochs with early stopping. We evaluated the models on both the validation split and official test evaluation server https://eval.ai/web/challenges/challenge-page/1325/overview. For the all-predicted (no ground-truth) visual input setup, similar to $\\mathbb{1}\\overline{{\\mathbb{[}9]}}$ , we utilize a fasterRCNN [19] object extractor, ST-Trans scene graph extractor $\\mathbb{\\lVert\\Pi\\rVert}$ and the same Alphapose predictor to obtain predicted object bounding boxes, labels, human-object relations and human poses. We observe a drop of ${\\sim}9\\%$ in predicted setup similar to observations in $\\mathbb{1}\\!\\!\\mathscr{D}\\mathbb{1}$ , suggesting further performance can be achieved through better object and relationship detection backbones. ", "page_idx": 18}, {"type": "text", "text": "AGQAv2. We use the AGQAv2 [20, 21] benchmark that comprises balanced training and test splits. We followed the same methodology as in STAR for language and visual backbones. Since AGQAv2 has a very large training and validation set, we found 8 epochs to be a sufficient number (with most performance increment observed in the 1st epoch itself). ", "page_idx": 18}, {"type": "text", "text": "CLEVRER-Humans. We use the CLEVRER-Humans dataset introduced in [51] for temporal, physical and causal video reasoning which comprises videos from the original CLEVRER dataset [84]. Similar to in STAR-VideoQA and neurosymbolic models [84, 85], we utilize a pretrained faster-RCNN based object localization and attribute prediction network from [84]. We again form object-level representations by concatenating learned projections of object-bounding box coordinates and predicted object attributes (i.e. color, shape and material). A frame-level learnable positional embedding is added and object-tokens across frames are flattened to form the final visual input to IPRM. For the language encoder, we used a simple bi-LSTM similar to existing methods. Note we do not use the functional programs or event causal graphs in our model. The batch size was 128 with a learning rate of 8e-5 and every 4 frames sampled (resulting on average 32 sampled frames). We evaluated models in the three setups \u2013 from scratch, zero-shot (CLEVRER-pretrained) and finetuned (CLEVRER-pretrained). Since the CLEVRER-Humans dataset is relatively small (comprising only 1076 questions $\\sim8$ batches; $\\sim0.5\\%$ of original CLEVRER), for scratch training we trained for 250 epochs (with early stopping) while for finetuning we finetuned for 150 epochs (with 35 epochs for original CLEVRER training). ", "page_idx": 18}, {"type": "text", "text": "CLEVR-CoGen. We use the CLEVR-CoGen dataset from $\\lVert34\\rVert$ and follow the same setup as in CLEVR-Humans. We use a simpler bi-LSTM language encoder for experiments since the questions are synthetic program-generated unlike in CLEVR-Humans (crowdsourced free-form). We trained our model on condition A for 40 epochs (with early stopping) and used the best cond. A validation performance model to evaluate generalization performance on cond.B. For finetuning on cond.B we finetuned the best cond.A model for 20 epochs and used the best cond.B validation performance model to also evaluate on cond.A. All other hyperparameters are the same as mentioned for CLEVRHumans. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "NLVR. We use the NLVRv1 and NLVRv2 datasets from [63, 64]. NLVRv1 comprises 3 synthetic images and a language statement while NLVRv2 comprises 2 real-world images and a lang. statement. For both datasets, the obtained visual tokens for each images was flattened to obtain the final visual input and an image-wise positional embedding was added to indicate image order. For the language encoder, we used a simple Bi-LSTM. ", "page_idx": 19}, {"type": "text", "text": "D Potential Negative Impact ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In relation to VQA and deep-learning methods in general, the deployment of IPRM in real-world applications without thorough consideration of dataset or training distribution biases, could inadvertently reinforce existing vision, language and cultural biases present in the data, leading to erroneous outcomes or skewed answers. Further, the deployment of VQA methods such as IPRM in sensitive domains such as healthcare or scene/footage analysis could raise ethical concerns, including privacy violations, algorithmic reliability, and the potential for unintended consequences stemming from erroneous or biased predictions. ", "page_idx": 19}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/f6ac38e3d8f0da70daa30b6f09c6ac8a8816fcbfd4f754f1a6423a62a5080a9c.jpg", "img_caption": ["Figure 8: Top: original image and question; middle: language attentions across parallel operations (clubbed together; op_k represents parallel operation $\\mathbf{k}$ ) and computation steps. Bottom: Visual attentions across parallel ops and computation steps. Here, IPRM correctly utilizes parallel and iterative compute to locate the correct candidate object for prediction (to which all operations attend in last step). ", "Reasoning Computation Steps "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/db8dde7b5db2b64e97a6f883cdfa87a59739c2b2b51176ac5f0979831771b86b.jpg", "img_caption": ["Figure 9: In this example, IPRM predicts the correct answer and its visual attention trace provides evidence of correct intermediate reasoning. In penultimate reasoning step, IPRM correctly localizes the gray object with maximum occuring shape (cylinder) and in the final step, the parallel operations attend to both the cyan cube and the brown cylinder closest to previously identified gray cylinder. ", "Reasoning Computation Steps "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/cd2a6a2e152fa2b47ab17cf46c748cd4c6fe9d9acdc3bb6a73ba7c9674fc4785.jpg", "img_caption": ["Figure 10: Example where IPRM outputs incorrect answer and the intermediate reasoning appears faulty possibly due to lack of understanding what a \u201cprimary color is\". The pair of blue (a primary color) cubes in this case should have been identified but are not visually attended in any of the operations across reasoning steps). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/f7a9a49e3e33cb5f3a0a12a4f00162dc3ba9d7c7bd1d90f455b7f83745e9375f.jpg", "img_caption": ["Figure 11: Example wherein IPRM produces correct answer but its visual attention trace suggests intermediate reasoning may be imprecise. The maximum occuring shape is cube; however both the blue small cylinder and blue small cube appear to be attended in the penultimate step as the \u201cblue small object with max occuring shape\" making the reasoning and prediction less reliable. ", ""], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "1   \n2   \n3   \n4   \n5   \n6   \n7   \n8   \n9   \n10   \n11   \n12   \n13   \n14   \n15   \n16   \n17   \n18   \n19   \n20   \n21   \n22   \n23   \n24   \n25   \n26   \n27   \n28   \n29   \n30   \n31   \n32   \n33   \n34   \n35   \n36   \n37   \n38   \n39 ", "page_idx": 24}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/d4a0769a7ee7cce548a604671a505049133602e71bce25fad15ab449eb91da73.jpg", "img_caption": ["Figure 12: IPRM pseudocode (1/3) "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/656c715163a6ffcc31eddc94e8f0c2f13c944c5afee347d8e365587f89f5300f.jpg", "img_caption": ["Figure 13: IPRM pseudocode (2/3) "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "uoJQ9qadjY/tmp/463affce9c79b319d7c9ace949a213f2c54174f90a60cc9bbdd137e7bb30ca4d.jpg", "img_caption": ["Figure 14: IPRM pseudocode (3/3) "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 27}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 27}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 27}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 27}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 27}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 27}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Abstract and introduction reflects the motivation of method and experimental results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Limitations are provided in appendix Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Paper does not include theoretical results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Benchmark and training details are provided along with module pseudocode and example CLIP integration. Source code for experiments will made publicly available along with checkpoints. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Paper provides training and implementations details and provides module pseudocode. Full source code will be made publicly available upon acceptance. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 29}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Appendix provides details on hyperparameters and dataset specific settings for all experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Appendix mentions that results were averaged over atleast 3 seeds for primary experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Appendix mentions the type of GPU and its memory for all experiments along with batch size of experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Authors reviewed code of ethics during submission. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Paper mentions potential negative impacts of work in appendix ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: answerNA ", "page_idx": 32}, {"type": "text", "text": "Justification: Paper does not explicitly pose such risks; however in limitations and potential negative impact this is mentioned. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Yes, all coding libraries and datasets are properly cited and credited. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification:No new asset ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Paper does not involve crowdsourcing or human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Paper does not involve crowdsourcing or human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]