[{"type": "text", "text": "The Secretary Problem with Predicted Additive Gap ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Braun \u2217 ", "page_idx": 0}, {"type": "text", "text": "Sherry Sarkar \u2020 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The secretary problem is one of the fundamental problems in online decision making; a tight competitive ratio for this problem of $\\mathrm{^1/e}\\approx0.368$ has been known since the 1960s. Much more recently, the study of algorithms with predictions was introduced: The algorithm is equipped with a (possibly erroneous) additional piece of information upfront which can be used to improve the algorithm\u2019s performance. Complementing previous work on secretary problems with prior knowledge, we tackle the following question: ", "page_idx": 0}, {"type": "text", "text": "What is the weakest piece of information that allows us to break the $^1\\!/\\mathrm{e}$ barrier? To this end, we introduce the secretary problem with predicted additive gap. As in the classical problem, weights are fixed by an adversary and elements appear in random order. In contrast to previous variants of predictions, our algorithm only has access to a much weaker piece of information: an additive gap $c$ . This gap is the difference between the highest and $k$ -th highest weight in the sequence. Unlike previous pieces of advice, knowing an exact additive gap does not make the problem trivial. Our contribution is twofold. First, we show that for any index $k$ and any gap $c$ , we can obtain a competitive ratio of 0.4 when knowing the exact gap (even if we do not know $k$ ), hence beating the prevalent bound for the classical problem by a constant. Second, a slightly modified version of our algorithm allows to prove standard robustness-consistency properties as well as improved guarantees when knowing a range for the error of the prediction. ", "page_idx": 0}, {"type": "text", "text": "The full version with proofs can be found at https://arxiv.org/abs/2409.   \n20460 [Braun and Sarkar, 2024]. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The secretary problem is a fundamental problem in online decision making: An adversary fixes non-negative, real-valued weights $w_{1}\\geq w_{2}\\geq\\cdots\\geq w_{n}$ which are revealed online in random order. The decision maker is allowed to accept (at most) one element. At the time of arrival of an element, the decision maker is required to make an immediate and irrevocable acceptance decision. The goal is to maximize the weight of the selected element. A tight guarantee3 of $1/\\mathrm{e}$ is known since the seminal work of Lindley [1961] and Dynkin [1963] (also see Ferguson [1989] or Freeman [1983]) and can be achieved with a very simple threshold policy. ", "page_idx": 0}, {"type": "text", "text": "In the modern era, the assumption of having no prior information on the weights is highly pessimistic. To go beyond a worst case analysis, researchers have recently considered the setting where we have some sort of learned prediction that our algorithm may use up front. This setting spawned the recent and very successful field of algorithms with predictions. Antoniadis et al. [2020] and D\u00fctting et al. [2021] studied the secretary problem with a prediction of the largest weight in the sequence, and resolve this setting with an algorithm which yields a nice robustness-consistency trade-off. Fujii and Yoshida [2023] consider the secretary problem with an even stronger prediction: A prediction for every weight in the sequence. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, predicting the largest element or weight can sometimes be difficult or unfavorable. For example in retail, past data may only contain information about prices and not the true values of buyers [Kleinberg and Leighton, 2003, Leme et al., 2023]; or for data privacy reasons (see e.g. Asi et al. [2023]), only surrogates for largest weights are revealed in history data. This motivates to advance our understanding of the following question: ", "page_idx": 1}, {"type": "text", "text": "What is the weakest piece of information we can predict that still allows us to break the $^1\\!/e$ barrier? ", "page_idx": 1}, {"type": "text", "text": "Stated another way, is there a different parameter we can predict, one that does not require us to learn the best value, but is still strong enough to beat $1/e?$ This brings us to the idea of predicting the gap between the highest and $k$ -th highest weight, or in other words, predicting how valuable $w_{k}$ is with respect to $w_{1}$ . Coming back to data privacy for example, such a parameter does successfully anonymize the largest weight in the sequence. ", "page_idx": 1}, {"type": "text", "text": "From a theoretical perspective, for some special cases of gaps, previous work directly implies improved algorithms. For example, if we know $w_{1}$ and $w_{2}$ have the same weight, using an algorithm for the two-best secretary problem [Gilbert and Mosteller, 1966] directly leads to a better guarantee. More generally, if we know the multiplicative gap $w_{1}/\\protect w_{2}$ , we observe that we can generalize the optimal algorithm for $w_{1}=w_{2}$ (see e.g. Gilbert and Mosteller [1966], Buchbinder et al. [2014]). However, if we instead only know $w_{n}/_{w_{1}}\\,=\\,0$ , this does not help at all. The only insight is that $w_{n}=0$ ; we have no insight on the range of values taken. This essentially boils down to the classical secretary problem and the best competitive ratio again is $^1\\!/\\mathrm{e}$ . So while the multiplicative gap advises about the relative values without needing to know $w_{1}$ entirely, it is not strong enough in general to beat $^1\\!/\\mathrm{e}$ . In this paper, we consider instead predicting an additive gap $w_{1}-w_{k}$ . ", "page_idx": 1}, {"type": "text", "text": "The additive gap between $w_{1}$ and $w_{k}$ can be viewed as interpolating between two previously studied setups: when $w_{1}-w_{k}$ gets small, we get closer towards the $k$ -best secretary problem, and when $w_{1}-w_{k}$ is very large, the additive gap acts as a surrogate prediction of $w_{1}$ , the prediction setting in Antoniadis et al. [2020] and D\u00fctting et al. [2021]. As we will see, even though the additive gap is much weaker than a direct prediction for $w_{1}$ , it strikes the perfect middle ground: it is strong enough to beat $^1\\!/e$ by a constant for any possible value of the gap $w_{1}-w_{k}$ (and even if we do not know what $k$ is upfront). In addition, in contrast to pieces of advice studied in the literature so far, knowing an exact additive gap does not make the problem trivial to solve. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Results and Techniques ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our contribution is threefold. First, in Section 3, we show the aforementioned result: knowing an exact additive gap allows us to beat the competitive ratio of $^{1/\\mathrm{e}}$ by a constant. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1 (Theorem 4, simplified form). There exists a deterministic online algorithm which achieves an expected weight of $\\mathbf{E}\\left[\\mathrm{ALG}\\right]\\geq0.4\\cdot w_{1}$ given access to a single additive gap $c_{k}$ for $c_{k}=w_{1}-w_{k}$ and some $k$ . ", "page_idx": 1}, {"type": "text", "text": "Still, getting an exact gap might be too much to expect. Hence, in Section 4, we introduce a slight modification in the algorithm to make it robust with respect to errors in the predicted gap while simultaneously outperforming the prevalent competitive ratio of $^1\\!/\\mathrm{e}$ by a constant for accurate gaps. ", "page_idx": 1}, {"type": "text", "text": "Theorem 2 (Theorem 5, simplified form). There exists a deterministic online algorithm which uses a predicted additive gap and is simultaneously $\\left({^1}/{\\mathrm{e}}+O(1)\\right)$ -consistent and $O(1)$ -robust. ", "page_idx": 1}, {"type": "text", "text": "The previous Theorem 2 does not assume any bounds on the error of the predicted additive gap used by our algorithm. In particular, the error of the prediction might be unbounded and our algorithm is still constant competitive. However, if we know that the error is bounded, we can do much better. ", "page_idx": 1}, {"type": "text", "text": "Theorem 3 (Theorem 6, simplified form). There exists a deterministic online algorithm which achieves an expected weight of $\\mathrm{E}\\left[\\mathrm{ALG}\\right]\\ge0.4\\cdot w_{1}-2\\epsilon$ given access to a bound \u03f5 on the error of the predicted gap. ", "page_idx": 1}, {"type": "text", "text": "Our algorithms are inspired by the one for classical secretary, but additionally incorporate the gap: Wait for some time to get a flavor for the weights in the sequence, set a threshold based on the past observations and the gap, pick the first element exceeding the threshold. ", "page_idx": 1}, {"type": "text", "text": "At first glance, this might not sound promising: In cases when the gap is small, incorporating the gap in the threshold does not really affect the best-so-far term. Hence, it may seem that beating $^1\\!/\\mathrm{e}$ is still hard. However, in these cases, even though the threshold will be dominated by the best-so-far term most of the time, the gap reveals the information that the best value and all other values up to $w_{k}$ are not too far off. That is, even accepting a weight which is at least $w_{k}$ ensures a sufficient contribution. ", "page_idx": 2}, {"type": "text", "text": "Our analyses use this fact in a foundational way: Either the gap is large in which case we do not consider many elements in the sequence for acceptance at all. Or the gap is small which implies that accepting any of the $k$ highest elements is reasonably good. For each of the cases we derive lower bounds on the weight achieved by the algorithm. ", "page_idx": 2}, {"type": "text", "text": "Since we do not know upfront which case the instance belongs to, we optimize our initial waiting time for the worse of the two cases. In other words, the waiting time cannot be tailored to the respective case but rather needs to be able to deal with both cases simultaneously. This introduces some sort of tension: For instances which have a large gap, we would like the waiting time to be small. By this, we could minimize the loss which we incur by waiting instead of accepting high weighted elements at the beginning of the sequence. For instances which have a small gap, the contribution of the gap to the algorithm\u2019s threshold can be negligible. This results in the need of a longer waiting time at the beginning to learn the magnitude of weights reasonably well. We solve this issue by using a waiting time which balances between these two extremes: It is (for most cases) shorter than the waiting time of $^1\\!/\\mathrm{e}$ from the classical secretary algorithm. Still, it is large enough to gain some information on the instance with reasonable probability. ", "page_idx": 2}, {"type": "text", "text": "As a corollary of our main theorem, we show that we can beat the competitive ratio of $1/\\mathrm{e}$ even if we only know the gap $w_{1}-w_{k}$ but do not get to know the index $k$ . In particular, this proves that even an information like \u201cThere is a gap of $c$ in the instance\u201d is helpful to beat $^1\\!/\\mathrm{e}$ , no matter which weights are in the sequence and which value $c$ attains. ", "page_idx": 2}, {"type": "text", "text": "Complementing theoretical results, we run simulations in Section 6 which strengthen our theoretical findings. First, we show that for instances in which the classical secretary algorithm achieves a nearly tight guarantee of $^{1/\\mathrm{e}}$ , our algorithm can almost always select the highest weight. In addition, we further investigate the robustness-consistency trade-off of our algorithm. In particular, as it will turn out, underestimating is not as much of an issue as overestimating the exact gap. ", "page_idx": 2}, {"type": "text", "text": "1.2 Additional Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Implications of related results on the additive gap. In the two-best secretary problem, we can pick at most one element but win when selecting either the best or second best element. For this problem, the competitive ratio is upper bounded by approximately 0.5736 [Buchbinder et al., 2014, Chan et al., 2015] (the authors provide an algorithm which matches this bound, so the guarantee is tight). As our setting with $w_{1}-w_{2}=0$ can be viewed as a special case, this yields a hardness result; the best any algorithm can perform with the exact additive gap provided upfront is approximately 0.5736. ", "page_idx": 2}, {"type": "text", "text": "A non-exhaustive list of related work on secretary problems. Since the introduction of the secretary problem in the 1960s, there have been a lot of extensions and generalizations of this problem with beautiful algorithmic ideas to solve them Kleinberg [2005], Babaioff et al. [2007, 2018], Feldman et al. [2018], Korula and P\u00e1l [2009], Mahdian and Yan [2011], Kesselheim et al. [2018], Rubinstein [2016]. Beyond classical setups, recent work by Kaplan et al. [2020] and Correa et al. [2021] studies the secretary problem with sampled information upfront. Here, some elements are revealed upfront to the algorithm which then tries to pick the best of the remaining weights. Guarantees are achieved with respect to the best remaining element in the sequence. In addition, there are also papers bridging between the secretary problem and the prophet inequality world, e.g. Correa et al. [2020] or Correa et al. [2019] and many more [Bradac et al., 2020, Kesselheim and Molinaro, 2020, Argue et al., 2022]. ", "page_idx": 2}, {"type": "text", "text": "Algorithms with machine learned advice. In the introduction, we already scratched the surface of the field on algorithms with predictions. Here, the algorithm has access to some machine learned advice upfront and may use this information to adapt decisions. Initiated by the work of Lykouris and Vassilvitskii [2021] and Purohit et al. [2018], there have been many new and interesting results in completely classical problems within the last years, including ski rental Wei and Zhang [2020], online bipartite matching Lavastida et al. [2021], load balancing Ahmadian et al. [2023], and many more (see e.g. Im et al. [2021], Zeynali et al. [2021], Almanza et al. [2021]). Since this area is developing very fast, we refer the reader to the excellent website Algorithms-with-Predictions for references of literature. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "As mentioned before, also the secretary problem itself has been studied in this framework. Antoniadis et al. [2020] consider the secretary problem when the machine learned advice predicts the weight of the largest element $w_{1}$ . Their algorithm\u2019s performance depends on the error of the prediction as well as some confidence parameter by how much the decision maker trusts the advice. In complementary work, D\u00fctting et al. [2021] give a bigger picture for secretary problems with machine learned advice. Their approach is LP based and can capture a variety of settings. They assume that the prediction is one variable for each weight (e.g. a $0/1$ -variable indicating if the current element is the best overall or not). Fujii and Yoshida [2023] assume an even stronger prediction: Their algorithm is given a prediction for every weight in the sequence. In contrast, we go into the opposite direction and deal with a less informative piece of information. ", "page_idx": 3}, {"type": "text", "text": "Our work also fits into the body of literature studying weak prediction models, previously studied for e.g. paging [Antoniadis et al., 2023], online search [Angelopoulos, 2021], to just mention a few. In these, several different directions for weak prediction models were considered. For example, the setting in scheduling or caching where the number of predictions is smaller than the input size [Im et al., 2022, Benomar and Perchet, 2024]. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the secretary problem, an adversary fixes $n$ non-negative, real-valued weights, denoted $w_{1}~\\geq$ $w_{2}\\geq\\dots\\geq w_{n}$ . For each of the elements, there is an arrival time4 $t_{i}\\overset{\\mathrm{iid}}{\\sim}\\mathrm{Unif}[0,1]$ . Weight $w_{i}$ is revealed at time $t_{i}$ and we immediately and irrevocably need to decide if we want to accept or reject this element. Overall, we are allowed to accept at most one element with the objective of maximizing the selected weight. We say that an algorithm is $\\alpha$ -competitive or achieves a competitive ratio of $\\alpha$ if $\\mathbf{E}\\left[{\\mathrm{ALG}}\\right]\\geq{\\boldsymbol{\\alpha}}\\cdot{\\boldsymbol{w}}_{1}={\\boldsymbol{\\alpha}}\\cdot\\operatorname*{max}_{i}w_{i}.$ , where the expectation is taken over the random arrival times of elements (and possible internal randomness of the algorithm). ", "page_idx": 3}, {"type": "text", "text": "In addition to the random arrival order, we assume to have access to a single prediction $\\hat{c}_{k}$ for one additive gap together with its index $k$ . The additive gap for some index $2\\leq k\\leq n$ is $c_{k}:=w_{1}-w_{k}$ We say that an algorithm has access to an exact or accurate gap if $\\hat{c}_{k}=c_{k}$ (as in Section 3). When the algorithm gets a predicted additive gap $\\hat{c}_{k}$ which might not be accurate (as in Section 4 or Section 5), we say that $\\hat{c}_{k}$ has error $\\eta=|\\hat{c}_{k}-c_{k}|$ . We call an algorithm $\\rho$ -robust if the algorithm is $\\rho$ -competitive regardless of error $\\eta$ and we say the algorithm is $\\psi$ -consistent if the algorithm is $\\psi$ -competitive when $\\eta=0$ . To fix notation, for any time $\\tau\\in[0,1]$ , we denote by $\\mathrm{BSF}(\\tau)$ (read best-so-far at time $\\tau$ ) the highest weight which did appear up to time $\\tau$ . In other words, $\\mathrm{BSF}(\\tau)=\\mathrm{max}_{i:t_{i}\\leq\\tau}\\,w_{i}$ . Also, when clear from the context, we drop the index $k$ at the gap and only call it $c$ or $\\hat{c}$ respectively. ", "page_idx": 3}, {"type": "text", "text": "3 Knowing an Exact Gap ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before diving into the cases where the predicted gap may be inaccurate in Section 4 and Section 5, we start with the setup of getting a precise prediction for the gap. That is, we are given the exact gap $c_{k}=w_{1}-w_{k}$ for some $2\\leq k\\leq n$ . We assume that we get to know the index $k$ as well as the value of $c_{k}$ , but neither $w_{1}$ nor $w_{k}$ . ", "page_idx": 3}, {"type": "text", "text": "Our algorithm takes as input the gap $c$ as well as a waiting time $\\tau$ . This gives us the freedom to potentially choose $\\tau$ independent of $k$ if required. As a consequence, we could make the algorithm oblivious to the index $k$ of the element to which the gap is revealed. We will use this in Corollary 1. ", "page_idx": 3}, {"type": "table", "img_path": "Lbuxdzg1pd/tmp/018902bac80aaa4d999a51b7c8ea0410780f21148564623e072ba9427af1d07c.jpg", "table_caption": [], "table_footnote": ["4Note that this setting is equivalent to drawing a random permutation of the $n$ elements and revealing elements in this order one by one. "], "page_idx": 3}, {"type": "text", "text": "This algorithm beats the prevalent competitive ratio of $1/\\mathrm{e}\\approx0.368$ by a constant. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4. Given any additive gap $c_{k}=w_{1}-w_{k}$ , for $\\tau=1-\\big(1/k\\!+\\!1\\big)^{1/k},$ , Algorithm 1 achieves a competitive ratio of $\\operatorname*{max}\\left(0.4,1/2\\left(1/k{+}1\\right)^{1/k}\\right)$ ", "page_idx": 4}, {"type": "text", "text": "Note that as $k$ tends towards $n$ and both become large, the competitive ratio approaches $1/2$ . ", "page_idx": 4}, {"type": "text", "text": "We split the proof of Theorem 4 in the following two lemmas. Each of them gives a suitable bound on the performance of our algorithm for general waiting times $\\tau$ in settings when $w_{k}$ is small or large. ", "page_idx": 4}, {"type": "text", "text": "The first lemma gives a lower bound in cases when $w_{k}$ is small in comparison to $w_{1}$ ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. If $w_{k}<\\textstyle{\\frac{1}{2}}w_{1}$ , then $\\begin{array}{r}{\\mathbf{E}\\left[\\mathrm{ALG}\\right]\\geq(1-\\tau)\\left(\\frac{1}{2}+\\frac{1}{2(k-1)}\\right)\\cdot w_{1}.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "The second lemma will be used to give a bound when $w_{k}$ is large compared to $w_{1}$ ", "page_idx": 4}, {"type": "text", "text": "Lemma 2. If $w_{k}\\geq\\textstyle{\\frac{1}{2}}w_{1}$ , then the following two bounds hold: ", "page_idx": 4}, {"type": "equation", "text": "$\\begin{array}{r}{(i)\\textbf{E}[\\mathrm{ALG}]\\ge\\frac{k+1}{2k}\\left(1-\\tau-(1-\\tau)^{k+1}\\right)\\cdot w_{1}\\;a n d}\\end{array}$ ", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(i i)\\mathrm{~}\\mathbf{E}\\left[\\mathrm{ALG}\\right]\\geq\\left(\\frac{3}{2}\\tau\\ln\\left(\\frac{1}{\\tau}\\right)-\\frac{1}{2}\\tau(1-\\tau)\\right)\\cdot w_{1}\\mathrm{~}\\mathrm{~.~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As a consequence, $\\mathbf{E}$ [ALG] is also at least as large as the maximum of the two bounds. ", "page_idx": 4}, {"type": "text", "text": "The proofs of the lemmas as well as their combination to prove Theorem 4 can be found in the full version [Braun and Sarkar, 2024]. From a high level perspective, the two lemmas give a reasonable bound depending of we either exclude a lot of elements in the algorithm (Lemma 1) or if the largest $k$ elements ensure a sufficient contribution (Lemma 2). ", "page_idx": 4}, {"type": "text", "text": "As a corollary of the proof of Theorem 4, we also get a lower bound on the weight achieved by the algorithm if we are only given the gap, but not the element which obtains this gap. That is, we are given $c_{k}$ but not the index $k$ . ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. If the algorithm only knows $c_{k}$ , but not $k$ , setting $\\tau=0.2$ achieves E $[\\mathrm{ALG}]\\geq0.4\\cdot w_{1}$ . ", "page_idx": 4}, {"type": "text", "text": "The full version [Braun and Sarkar, 2024] contains a proof of Corollary 1 as well. It mainly relies on the fact that some lower bound we obtained in the proof of Theorem 4 holds for any choice $\\tau\\in[0,1]$ . Also, the algorithm itself only uses the gap to contribute to the threshold. The index $k$ is only used to compute $\\tau$ . As a consequence, when choosing $\\tau=0.2$ independent of $k$ , the algorithm is oblivious to the exact value of $k$ , but only depends on the gap $c_{k}$ . For this choice of $\\tau$ , we can show that $\\alpha\\ge0.4$ . ", "page_idx": 4}, {"type": "text", "text": "As a consequence, very surprisingly, even if we only get to know some additive gap $c_{k}$ and not even the index $k$ , we can outperform the prevalent bound of $^{1/\\mathrm{e}}$ . Also, observe that this is independent of the exact value that $c_{k}$ attains and holds for any small or large gaps. ", "page_idx": 4}, {"type": "text", "text": "As mentioned before, Algorithm 1 is required to get the exact gap as input. In particular, once the gap we use in the algorithm is a tiny bit larger than the actual gap $c_{k}$ , we might end up selecting no element at all. ", "page_idx": 4}, {"type": "text", "text": "Example 1. We get to know the gap to the smallest weight $c_{n}=w_{1}-w_{n}$ and the smallest weight $w_{n}$ in the sequence satisfies $w_{n}=0$ . Let the gap which we use in Algorithm 1 be only some tiny $\\delta>0$ too large. In other words, we use $c_{n}+\\delta$ as a gap in the algorithm. Still, this implies that our threshold $\\operatorname*{nax}(\\mathrm{BSF}(\\tau),c_{n}+\\delta)$ after the waiting time satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}(\\mathrm{BSF}(\\tau),c_{n}+\\delta)\\ge c_{n}+\\delta=w_{1}+\\delta>w_{1}\\ge w_{2}\\ge\\cdot\\cdot\\cdot\\ge w_{n}\\enspace.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As a consequence, we end up selecting no weight at all and have $\\mathbf{E}\\left[\\mathrm{ALG}\\right]=0$ . ", "page_idx": 4}, {"type": "text", "text": "This naturally motivates the need to introduce more robust deterministic algorithms in this setting. In Section 4, we will show that a slight modification in the algorithm and its analysis allows to obtain robustness to errors in the predictions while simultaneously outperforming $^{1/\\mathrm{e}}$ for accurate gaps. ", "page_idx": 4}, {"type": "text", "text": "4 Robustness-Consistency Trade-offs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we show how to slightly modify our algorithm in order to still beat $^1\\!/\\mathrm{e}$ when getting the correct gap as input, but still be constant competitive in case the predicted gap is inaccurate. The modification leads to Algorithm 2: Initially, we run the same algorithm as before. After some time $1-\\gamma$ , we will lower our threshold in order to hedge against an incorrect prediction. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "Lbuxdzg1pd/tmp/07db67e2f2024caaa5199505829989ee0a0bef7308215259d9a98411ad8ef7ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Note that by $\\gamma\\in[0,1-\\tau)$ , we ensure that $\\tau<1-\\gamma$ , i.e. the waiting time $\\tau$ is not after time $1-\\gamma$ and hence, the algorithm is well-defined. Now, we can state the following theorem which gives guarantees on the consistency and the robustness of Algorithm 2. We will discuss afterwards how to choose $\\tau$ and $\\gamma$ in order to outperform the classical bound of $^{1/\\mathrm{e}}$ by a constant for accurate predictions while ensuring to be constant-robust at the same time. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5. Given a prediction $\\hat{c}_{k}$ for the additive gap $c_{k}$ , define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{1}:=1-\\gamma-\\tau+\\tau\\ln\\left(\\frac{1}{1-\\gamma}\\right)a n d}\\\\ &{\\alpha_{2}:=\\frac{1}{2}\\left((1+\\gamma)(1-\\tau-\\gamma)+\\tau\\ln\\left(\\frac{1}{\\tau}\\right)+\\tau\\ln\\left(\\frac{1}{1-\\gamma}\\right)\\right)\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, Algorithm 2 is $(i)\\,\\operatorname*{min}\\left(\\operatorname*{min}\\left(\\alpha_{1},\\alpha_{2}\\right),\\operatorname*{max}\\left(\\alpha_{3},\\alpha_{4}\\right)\\right)$ -consistent and (ii) $\\begin{array}{r}{\\left(\\tau\\cdot\\ln\\left(\\frac{1}{1-\\gamma}\\right)\\right)}\\end{array}$ - robust. ", "page_idx": 5}, {"type": "text", "text": "Observe that if we do not trust the prediction at all, we could set $\\tau=1/\\mathrm{e}$ and $1-\\gamma=1/\\mathrm{e}$ . Doing so, we do not use the prediction in our algorithm. Still, for these choices, we recover the guarantee from classical secretary of $^1\\!/\\mathrm{e}$ . In other words, we can interpret $\\gamma$ as a trust parameter for the prediction which also mirrors our risk appetite. If we do not trust the prediction at all or if we are highly risk averse, we can set $1-\\gamma\\approx\\tau$ . If we are willing to suffer a lot in case of an inaccurate prediction (or if we have high trust in the prediction), we will set $\\gamma\\approx0$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 5 yields a trade-off between robustness and consistency. In particular, for a fixed level of robustness, we can choose the optimal values for $\\tau$ and $1-\\gamma$ for the bounds in Theorem 5 to obtain the plot in Figure 1. Observe that when not focusing on robustness (i.e. choosing robustness being equal to zero), we can achieve a consistency approximately matching the upper bound of 0.5736 described in Section 1.2. ", "page_idx": 5}, {"type": "image", "img_path": "Lbuxdzg1pd/tmp/4f0877ac583fa105014a0b733e539e0bd80bbd8516eded43a8a26a14ede88036.jpg", "img_caption": ["Figure 1: Choosing the optimal parameters $\\tau$ and $1-\\gamma$ for our analysis in Theorem 5: For a given level of robustness, what is the best consistency we can obtain with our analysis. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 5 can be found in the full version [Braun and Sarkar, 2024]. Concerning robustness, we can only obtain a reasonable contribution by accepting the best weight. Therefore, we derive a lower bound on the probability of accepting the highest weight via Algorithm 2. Concerning consistency, we can perform a case distinction whether $w_{k}$ is small or large. Crucially, one is required to take the drop in the threshold after time $1-\\gamma$ into account. ", "page_idx": 6}, {"type": "text", "text": "For example, when using a waiting time $\\tau\\,=\\,0.2$ as in Corollary 1 independent of the index $k$ and a value of $\\gamma\\,=\\,0.6$ , i.e. $1\\mathrm{~-~}\\gamma\\mathrm{~=~}0.4$ , we get the following: Algorithm 2 is approximately 0.383-consistent and 0.183-robust (also see Figure 2). ", "page_idx": 6}, {"type": "image", "img_path": "Lbuxdzg1pd/tmp/731eb805d3af1c912a00cf16d1572028989b36cffde85b8955c362aa44c5bf6a.jpg", "img_caption": ["Figure 2: Trade-off between robustness and consistency as a function of the time $1-\\gamma$ for fixed choice of $\\tau=0.2$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "In particular, we can outperform the prevalent bound of $^1\\!/\\mathrm{e}$ by a constant if the predicted gap is accurate while ensuring to be constant competitive even if our predicted gap is horribly off. Of course, when being more risk averse, one could also increase the robustness guarantee for the cost of decreasing the competitive ratio for consistent predictions.5 ", "page_idx": 6}, {"type": "text", "text": "We highlight that these guarantees as well as Theorem 5 hold independent of any bounds on the error of the predicted gap. However, it is reasonable to assume that we have some bounds on how inaccurate our predicted gap is (for example, if our predicted gap is learned from independent random samples). We show in Section 5 that we can achieve much better competitive ratios when we know a range for the error. ", "page_idx": 6}, {"type": "text", "text": "5 Improved Guarantees for Bounded Errors ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Complementing the previous sections where we had either access to the exact gap (Section 3) or no information on a possible error in the prediction (Section 4), we now assume that the error is bounded6. That is, we get to know some $\\widetilde{c}_{k}\\in[c_{k}-\\epsilon;c_{k}+\\epsilon]$ which is ensured to be at most an $\\epsilon$ off. Also, the bound $\\epsilon$ on the error is revealed to us. Still, the true gap $c_{k}$ remains unknown. ", "page_idx": 6}, {"type": "text", "text": "Our algorithm follows the template which we discussed before. Still, we slightly perturb $\\widetilde{c}_{k}$ to ensure that the threshold is not exceeding $w_{1}$ . This algorithm allows to state an approximat e  version of Theorem 4 for the same lower bounds of $\\alpha$ as in the exact gap case. ", "page_idx": 6}, {"type": "table", "img_path": "Lbuxdzg1pd/tmp/64ca8e93ba08d42e4e469855b18226077de8309cf2d6748d3b6a2432285daf2b.jpg", "table_caption": ["Algorithm 3 Secretary with Bounded Prediction Error "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Theorem 6. Given any prediction of the gap $\\widetilde{c}_{k}\\in[c_{k}-\\epsilon;c_{k}+\\epsilon],$ , where $c_{k}=w_{1}-w_{k}$ , Algorithm 3 satisfies $\\mathbf{E}\\left[\\mathrm{ALG}\\right]\\geq\\boldsymbol{\\alpha}\\cdot\\boldsymbol{w}_{1}-2\\boldsymbol{\\epsilon}$ . For $\\begin{array}{r}{\\tau=1-\\left(\\frac{1}{k+1}\\right)^{1/k}}\\end{array}$ , $\\begin{array}{r}{\\alpha\\geq\\operatorname*{max}\\left(0.4,\\frac{1}{2}\\left(\\frac{1}{k+1}\\right)^{1/k}\\right)}\\end{array}$ and for $\\tau=0.2,\\,\\alpha\\geq0.4.$ . ", "page_idx": 7}, {"type": "text", "text": "As a consequence, the guarantees from the exact gap case in Section 3 carry over with an additional loss of $2\\epsilon$ . Also, the results when not knowing the index $k$ carry over. In particular, this nicely complements the robustness result from Theorem 5 as follows: Once we can bound the error in a reasonable range, even not knowing the gap exactly does not cause too much of an issue. The proof of Theorem 6 can be found in the full version [Braun and Sarkar, 2024]. ", "page_idx": 7}, {"type": "text", "text": "6 Simulations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In order to gain a more fine-grained understanding of the underlying habits, we run experiments7 with simulated weights and compare our algorithms among each other and to the classical secretary algorithm8. ", "page_idx": 7}, {"type": "text", "text": "In Section 6.1, we compare our Algorithm 1 to the classical secretary algorithm. To this end, we draw weights i.i.d. from distributions and execute our algorithm and the classical one. As it will turn out, instances which are hard in the normal secretary setting (i.e. when not knowing any additive gap) become significantly easier with additive gap; we can select the best candidate with a much higher probability. We also demonstrate that for some instances, knowing the gap has a smaller impact, though our Algorithm 1 still outperforms the classical one. ", "page_idx": 7}, {"type": "text", "text": "Second, in Section 6.2, we turn towards inaccurate gaps and compare Algorithm 1 developed in Section 3 to the robust and consistent variant of Algorithm 2 from Section 4. As a matter of fact, we will see that underestimating the exact gap is not as much of an issue as an overestimation. In particular, underestimating the gap implies a smooth decay in the competitive ratio while overestimating can immediately lead to a huge drop. ", "page_idx": 7}, {"type": "text", "text": "6.1 The Impact of Knowing the Gap ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare our algorithm with additive gap to the classical secretary algorithm (see e.g. [Dynkin, 1963]) with a waiting time of $^1\\!/\\mathrm{e}$ . ", "page_idx": 7}, {"type": "text", "text": "6.1.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We run the comparison on three different classes of instances: ", "page_idx": 7}, {"type": "text", "text": "(i) Pareto: We first draw some $\\theta\\sim\\mathrm{Pareto}(5/n,1)$ . Afterwards, each weight $w_{i}$ is determined as follows: Draw $Y_{i}\\sim\\mathrm{Unif}[0,\\theta]$ i.i.d. and set $w_{i}={Y_{i}^{\\left({n^{1.5}}\\right)}}$ Y i(n1.5)(for more details on Pareto distributions and secretary problems, see e.g. Ferguson [1989]).   \n(ii) Exponential: Here, all $w_{i}\\sim\\mathrm{Exp}(1)$ .   \n(iii) Chi-Squared: Draw $w_{i}\\sim\\chi^{2}(10)$ . That is, each $w_{i}$ is drawn from a chi-squared distribution which sums over ten squared i.i.d. standard normal random variables. ", "page_idx": 7}, {"type": "text", "text": "For each class of instances, we average over 5000 iterations. In each iteration, we draw $n=200$ weights i.i.d. from the respective distribution together with 200 arrival times which are drawn i.i.d. from $\\mathrm{Unif}[0,1]$ . The benchmark is the classical secretary algorithm with a waiting time of $\\tau=1/\\mathrm{e}$ : Set the largest weight up to time $\\tau$ as a threshold and accepts the first element afterwards exceeding this threshold. Algorithm 1 is executed with waiting times $\\tau=0.2$ as well as $\\tau=1-\\left(1/k+1\\right)^{1/k}$ . ", "page_idx": 8}, {"type": "text", "text": "6.1.2 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "When weights are sampled based on the procedure explained in (i), we observe an interesting phenomenon (see Figure 3). For the classical secretary algorithm, we achieve approximately the tight guarantee of $^1\\!/\\mathrm{e}$ . Our algorithm, however, achieves a competitive ratio of approximately 0.8 for $\\tau\\mathrm{~\\ensuremath~{~=~}~}0.2$ . When having a waiting time depending on $k$ , we improve the competitive ratio for large $k$ while suffering a worse ratio for small $k$ . ", "page_idx": 8}, {"type": "text", "text": "This can be explained as follows. Weights which are distributed according to (i) almost always have a very large gap between the highest and second highest weight. Hence, no matter which gap we observe, it will always be sufficiently large to exclude all elements except the best one. Therefore, we only incur a loss if we do not accept anything (which happens if and only if the best element arrives before the waiting time). As a consequence, for $\\tau=0.2$ , we observe the ratio of 0.8 (which is the probability of the highest weight arriving after time $\\tau$ ). For the waiting times depending on $k$ , the waiting time turns out to be larger for smaller $k$ and vice versa. The improvement in the competitive ratio for large $k$ comes from the reduced waiting time and hence a smaller probability of facing an arrival of $w_{1}$ during the waiting period. ", "page_idx": 8}, {"type": "text", "text": "Interestingly, this shows that there are instances for which the classical secretary algorithm almost obtains its tight guarantee of $^1\\!/\\mathrm{e}$ while these instances become easy when knowing an additive gap. As a side remark: One might wonder if it is always true that the index ", "page_idx": 8}, {"type": "image", "img_path": "Lbuxdzg1pd/tmp/66ff828ae80bbb6987d315e60f7f6b774fc504f275cd8ca41250b0e06545664a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Competitive ratios for weights based on (i). On the $x$ -axis, we have the index $k$ from 2 to $n$ . The $y$ -axis shows the competitive ratios. ", "page_idx": 8}, {"type": "text", "text": "$k$ does not play a pivotal role when using a constant waiting time $\\tau=0.2$ . In the full version [Braun and Sarkar, 2024], we show that this is not the case for exponentially distributed weights as in (ii) or Chi-Squared distributed ones as in (iii). ", "page_idx": 8}, {"type": "text", "text": "6.2 Dealing with Inaccurate Gaps ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In order to get a better understanding concerning inaccuracies in the gap, we run a simulation with different errors. ", "page_idx": 8}, {"type": "text", "text": "6.2.1 Experimental Setup ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Again, we average over 5000 iterations. In each iteration, we set $n=200$ , draw arrival times as before and weights as follows: ", "page_idx": 8}, {"type": "text", "text": "(iv) Exponential: Here, all $w_{i}\\sim\\mathrm{Exp}(1)$ . ", "page_idx": 8}, {"type": "text", "text": "(v) Exponential with superstar: Here, $w_{i}\\sim\\mathrm{Exp}(1)$ for $n-1$ weights and we add a superstar element with weight $100\\cdot\\operatorname*{max}_{i}w_{i}$ . ", "page_idx": 8}, {"type": "text", "text": "We compare Algorithm 1 to Algorithm 2 both with waiting time $\\tau=0.2$ . In addition, Algorithm 2 will drop the gap from the threshold after a time of $1-\\gamma=0.95$ , in other words $\\gamma=0.05$ . ", "page_idx": 8}, {"type": "text", "text": "The comparison is done for three different gaps: A small one where $k=2$ , i.e. the gap between the largest and second largest element, $k=n/2$ and $k=n$ , i.e. the gap to the smallest element. Given a multiplication factor $\\sigma$ for the error, we feed our algorithm with a predicted gap $\\hat{c}_{k}=\\boldsymbol{\\sigma}\\cdot\\boldsymbol{c}_{k}$ for $\\sigma$ going from zero to three in step size of 0.1. In other words, for $\\sigma=1$ , we get an accurate gap, for $\\sigma<1$ , we underestimate the gap, for $\\sigma>1$ we overestimate the gap and for $\\sigma=0$ , the algorithms are equivalent to the classical secretary algorithms with waiting time $\\tau$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.2.2 Experimental Results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "For exponentially distributed weights (see Figure 4), we can observe that underestimating the gap does not cause too many issues. In particular, when highly underestimating the gap (i.e. $\\sigma_{\\mathrm{~\\tiny~\\textnormal~{~\\textcent~}~}}(\\mathrm{{}).5)}$ , both algorithms achieve a competitive ratio of approximately 0.65, similar to an algorithm not knowing any gap. For an accurate gap, $\\sigma\\mathrm{~\\=~}$ 1, larger gaps are more helpful as they block more elements from being considered. ", "page_idx": 9}, {"type": "text", "text": "Still, $\\sigma\\ >\\ 1$ introduces a transition. For $\\sigma\\ >\\ 1$ and gaps between the best and a small element (e.g. $k\\,=\\,100$ or $k\\,=\\,200]$ ), overestimating the gap reduces the selection probability of any weight of Algorithm 1 to zero: The predicted gap is simply too large and even exceeds $w_{1}$ . Still, Algorithm 2 is robust in a sense that we still achieve a competitive ratio of approximately 0.15. This constant depends on our choice of $\\gamma$ . As mentioned before, there is the natural trade-off: Increasing $\\gamma$ for an improved robustness and suffer a decrease in the competitive ratio for $\\sigma=1$ . ", "page_idx": 9}, {"type": "text", "text": "Interestingly, for the gap between the best and second best element, both algorithms are much more robust. This can be explained as the gap is small in this case anyway, so overestimating by a factor of three does not cause too much issues yet. One would require to overestimate by a much larger factor here to see a significant difference in the performance of both algorithms. ", "page_idx": 9}, {"type": "image", "img_path": "Lbuxdzg1pd/tmp/98977f13ece6ad69681b5576de6bfe8ffc894477f6298ac6aeaee7211e94d865.jpg", "img_caption": ["Figure 4: Competitive ratios for weights based on (iv). The $x$ -axis shows $\\sigma$ , where the predicted gap $\\hat{c}_{k}$ used by the algorithms satisfies $\\hat{c}_{k}=\\boldsymbol{\\sigma}\\cdot\\boldsymbol{c}_{k}$ for $\\sigma\\in[0,3]$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In the full version [Braun and Sarkar, 2024], we show   \nwhat happens when shifting our perspective towards the more adversarial setting of exponential weights with one additional superstar as listed in (v). Here, the drop when overestimating is even more significant. Still, Algorithm 2 achieves the desired constant competitive ratio even if gaps are fairly inaccurate. However, the trade-off between consistency and robustness plays a much more important role in the choices of $\\tau$ and $\\gamma$ here. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As we have seen, a single simple piece of information of the form \u201cThere is a gap of $c$ in the instance\u201d helps to improve the competitive ratio for the secretary problem. In addition, our algorithm can be made robust against inaccurate predictions without sacrificing too much in the competitive ratio. ", "page_idx": 9}, {"type": "text", "text": "Our results directly impose some open questions for future research. First, our guarantees seem to be not tight. Can we achieve a better competitive ratio for any gap? Or is there a matching hardness result? As a second open question, the gaps that we consider are of the form $w_{1}-w_{k}$ for some $k$ . As a generalization, one could consider arbitrary gaps $w_{i}-w_{j}$ for some $1\\leq i<j\\leq n$ . Can we do something in this regime? (as sketched in the full version [Braun and Sarkar, 2024], we can for e.g. $w_{2}-w_{3}=0,$ ). ", "page_idx": 9}, {"type": "text", "text": "Also, going beyond the single selection problem is interesting, for example by considering the multi-selection variant. For this, we give a reasonable starting point in the full version [Braun and Sarkar, 2024]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Thomas Kesselheim for very helpful discussions on the secretary problem as well as suggestions on different models of advice. Also, many thanks to the anonymous reviewers for helpful feedback and remarks concerning previous versions of this paper. This work was done in part while the authors were visiting the Simons Institute for the Theory of Computing. Alexander Braun has been funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), Project No. 437739576. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "S. Ahmadian, H. Esfandiari, V. Mirrokni, and B. Peng. Robust load balancing with machine learned advice. J. Mach. Learn. Res., 24:44:1\u201344:46, 2023. URL http://jmlr.org/papers/v24/ 22-0629.html.   \nAlgorithms-with-Predictions. Website with list of papers on algorithms with predictions. URL https://algorithms-with-predictions.github.io.   \nM. Almanza, F. Chierichetti, S. Lattanzi, A. Panconesi, and G. Re. Online facility location with multiple advice. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 4661\u20134673. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/ file/250473494b245120a7eaf8b2e6b1f17c-Paper.pdf.   \nS. Angelopoulos. Online search with a hint. In J. R. Lee, editor, 12th Innovations in Theoretical Computer Science Conference, ITCS 2021, January 6-8, 2021, Virtual Conference, volume 185 of LIPIcs, pages 51:1\u201351:16. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2021. doi: 10.4230/LIPICS.ITCS.2021.51. URL https://doi.org/10.4230/LIPIcs.ITCS.2021.51.   \nA. Antoniadis, T. Gouleakis, P. Kleer, and P. Kolev. Secretary and online matching problems with machine learned advice. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 7933\u20137944. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/5a378f8490c8d6af8647a753812f6e31-Paper.pdf.   \nA. Antoniadis, J. Boyar, M. Eli\u00e1s, L. M. Favrholdt, R. Hoeksma, K. S. Larsen, A. Polak, and B. Simon. Paging with succinct predictions. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 952\u2013 968. PMLR, 2023. URL https://proceedings.mlr.press/v202/antoniadis23a.html.   \nC. Argue, A. Gupta, M. Molinaro, and S. Singla. Robust Secretary and Prophet Algorithms for Packing Integer Programs, pages 1273\u20131297. 2022. doi: 10.1137/1.9781611977073.53. URL https://epubs.siam.org/doi/abs/10.1137/1.9781611977073.53.   \nH. Asi, V. Feldman, T. Koren, and K. Talwar. Private online prediction from experts: Separations and faster rates. In G. Neu and L. Rosasco, editors, The Thirty Sixth Annual Conference on Learning Theory, COLT 2023, 12-15 July 2023, Bangalore, India, volume 195 of Proceedings of Machine Learning Research, pages 674\u2013699. PMLR, 2023. URL https://proceedings.mlr.press/ v195/asi23a.html.   \nM. Babaioff, N. Immorlica, D. Kempe, and R. Kleinberg. A knapsack secretary problem with applications. In M. Charikar, K. Jansen, O. Reingold, and J. D. P. Rolim, editors, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 10th International Workshop, APPROX 2007, and 11th International Workshop, RANDOM 2007, Princeton, NJ, USA, August 20-22, 2007, Proceedings, volume 4627 of Lecture Notes in Computer Science, pages 16\u201328. Springer, 2007. doi: 10.1007/978-3-540-74208-1\\_2. URL https://doi.org/10.1007/ 978-3-540-74208-1_2.   \nM. Babaioff, N. Immorlica, D. Kempe, and R. Kleinberg. Matroid secretary problems. J. ACM, 65 (6), nov 2018. ISSN 0004-5411. doi: 10.1145/3212512. URL https://doi.org/10.1145/ 3212512.   \nZ. Benomar and V. Perchet. Non-clairvoyant scheduling with partial predictions. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id $=$ jJLcXGB2uA.   \nD. Bradac, A. Gupta, S. Singla, and G. Zuzic. Robust Algorithms for the Secretary Problem. In T. Vidick, editor, 11th Innovations in Theoretical Computer Science Conference (ITCS 2020), volume 151 of Leibniz International Proceedings in Informatics (LIPIcs), pages 32:1\u201332:26, Dagstuhl, Germany, 2020. Schloss Dagstuhl\u2013Leibniz-Zentrum fuer Informatik. ISBN 978-3- 95977-134-4. doi: 10.4230/LIPIcs.ITCS.2020.32. URL https://drops.dagstuhl.de/opus/ volltexte/2020/11717.   \nA. Braun and S. Sarkar. The secretary problem with predicted additive gap, 2024. URL https: //arxiv.org/abs/2409.20460.   \nN. Buchbinder, K. Jain, and M. Singh. Secretary problems via linear programming. Math. Oper. Res., 39(1):190\u2013206, 2014. doi: 10.1287/moor.2013.0604. URL https://doi.org/10.1287/moor. 2013.0604.   \nT. H. Chan, F. Chen, and S. H. Jiang. Revealing optimal thresholds for generalized secretary problem via continuous LP: impacts on online $K$ -item auction and bipartite $K$ -matching with random arrival order. In P. Indyk, editor, Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 1169\u2013 1188. SIAM, 2015. doi: 10.1137/1.9781611973730.78. URL https://doi.org/10.1137/1. 9781611973730.78.   \nJ. Correa, P. D\u00fctting, F. Fischer, and K. Schewior. Prophet inequalities for i.i.d. random variables from an unknown distribution. In Proceedings of the 2019 ACM Conference on Economics and Computation, EC \u201919, page 3\u201317, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367929. doi: 10.1145/3328526.3329627. URL https://doi.org/ 10.1145/3328526.3329627.   \nJ. R. Correa, A. Cristi, B. Epstein, and J. A. Soto. Sample-driven optimal stopping: From the secretary problem to the i.i.d. prophet inequality. CoRR, abs/2011.06516, 2020. URL https: //arxiv.org/abs/2011.06516.   \nJ. R. Correa, A. Cristi, L. Feuilloley, T. Oosterwijk, and A. Tsigonias-Dimitriadis. The secretary problem with independent sampling. In D. Marx, editor, Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms,SODA 2021, Virtual Conference, January 10 - 13, 2021, pages 2047\u20132058. SIAM, 2021. doi: 10.1137/1.9781611976465.122. URL https://doi.org/10. 1137/1.9781611976465.122.   \nP. D\u00fctting, S. Lattanzi, R. Paes Leme, and S. Vassilvitskii. Secretaries with advice. In Proceedings of the 22nd ACM Conference on Economics and Computation, EC \u201921, page 409\u2013429, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450385541. doi: 10.1145/3465456.3467623. URL https://doi.org/10.1145/3465456.3467623.   \nE. B. Dynkin. The optimum choice of the instant for stopping a markov process. Soviet Math. Dokl, 4:627\u2013629, 1963.   \nM. Feldman, O. Svensson, and R. Zenklusen. A simple O(log log(rank))-competitive algorithm for the matroid secretary problem. Math. Oper. Res., 43(2):638\u2013650, 2018. doi: 10.1287/moor.2017.0876. URL https://doi.org/10.1287/moor.2017.0876.   \nT. S. Ferguson. Who solved the secretary problem? Statistical Science, 4(3):282\u2013289, 1989. ISSN 08834237.   \nP. R. Freeman. The secretary problem and its extensions: A review. International Statistical Review / Revue Internationale de Statistique, 51(2):189\u2013206, 1983. ISSN 03067734, 17515823.   \nK. Fujii and Y. Yoshida. The secretary problem with predictions. CoRR, abs/2306.08340, 2023. doi: 10.48550/ARXIV.2306.08340. URL https://doi.org/10.48550/arXiv.2306.08340.   \nJ. P. Gilbert and F. Mosteller. Recognizing the maximum of a sequence. Journal of the American Statistical Association, 61(313):35\u201373, 1966. ISSN 01621459.   \nS. Im, R. Kumar, M. Montazer Qaem, and M. Purohit. Online knapsack with frequency predictions. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 2733\u20132743. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/ file/161c5c5ad51fcc884157890511b3c8b0-Paper.pdf.   \nS. Im, R. Kumar, A. Petety, and M. Purohit. Parsimonious learning-augmented caching. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv\u00e1ri, G. Niu, and S. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 9588\u20139601. PMLR, 2022. URL https://proceedings.mlr.press/v162/im22a.html.   \nH. Kaplan, D. Naori, and D. Raz. Competitive Analysis with a Sample and the Secretary Problem, pages 2082\u20132095. 2020. doi: 10.1137/1.9781611975994.128. URL https://epubs.siam.org/ doi/abs/10.1137/1.9781611975994.128.   \nT. Kesselheim and M. Molinaro. Knapsack Secretary with Bursty Adversary. In A. Czumaj, A. Dawar, and E. Merelli, editors, 47th International Colloquium on Automata, Languages, and Programming (ICALP 2020), volume 168 of Leibniz International Proceedings in Informatics (LIPIcs), pages 72:1\u201372:15, Dagstuhl, Germany, 2020. Schloss Dagstuhl\u2013Leibniz-Zentrum f\u00fcr Informatik. ISBN 978-3-95977-138-2. doi: 10.4230/LIPIcs.ICALP.2020.72. URL https: //drops.dagstuhl.de/opus/volltexte/2020/12479.   \nT. Kesselheim, K. Radke, A. T\u00f6nnis, and B. V\u00f6cking. Primal beats dual on online packing lps in the random-order model. SIAM Journal on Computing, 47(5):1939\u20131964, 2018. doi: 10.1137/ 15M1033708. URL https://doi.org/10.1137/15M1033708.   \nR. Kleinberg. A multiple-choice secretary algorithm with applications to online auctions. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201905, page 630\u2013631, USA, 2005. Society for Industrial and Applied Mathematics. ISBN 0898715857.   \nR. D. Kleinberg and F. T. Leighton. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In 44th Symposium on Foundations of Computer Science (FOCS 2003), 11-14 October 2003, Cambridge, MA, USA, Proceedings, pages 594\u2013605. IEEE Computer Society, 2003. doi: 10.1109/SFCS.2003.1238232. URL https://doi.org/10.1109/SFCS. 2003.1238232.   \nN. Korula and M. P\u00e1l. Algorithms for secretary problems on graphs and hypergraphs. In Proceedings of the 36th Internatilonal Collogquium on Automata, Languages and Programming: Part II, ICALP \u201909, page 508\u2013520, Berlin, Heidelberg, 2009. Springer-Verlag. ISBN 9783642029295. doi: 10. 1007/978-3-642-02930-1_42. URL https://doi.org/10.1007/978-3-642-02930-1_42.   \nT. Lavastida, B. Moseley, R. Ravi, and C. Xu. Learnable and Instance-Robust Predictions for Online Matching, Flows and Load Balancing. In P. Mutzel, R. Pagh, and G. Herman, editors, 29th Annual European Symposium on Algorithms (ESA 2021), volume 204 of Leibniz International Proceedings in Informatics (LIPIcs), pages 59:1\u201359:17, Dagstuhl, Germany, 2021. Schloss Dagstuhl \u2013 LeibnizZentrum f\u00fcr Informatik. ISBN 978-3-95977-204-4. doi: 10.4230/LIPIcs.ESA.2021.59. URL https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ESA.2021.59.   \nR. P. Leme, B. Sivan, Y. Teng, and P. Worah. Pricing query complexity of revenue maximization. In N. Bansal and V. Nagarajan, editors, Proceedings of the 2023 ACM-SIAM Symposium on Discrete Algorithms, SODA 2023, Florence, Italy, January 22-25, 2023, pages 399\u2013415. SIAM, 2023. doi: 10.1137/1.9781611977554.CH17. URL https://doi.org/10.1137/1.9781611977554. ch17.   \nD. V. Lindley. Dynamic programming and decision theory. Journal of The Royal Statistical Society Series C-applied Statistics, 10:39\u201351, 1961.   \nT. Lykouris and S. Vassilvitskii. Competitive caching with machine learned advice. J. ACM, 68(4): 24:1\u201324:25, 2021. doi: 10.1145/3447579. URL https://doi.org/10.1145/3447579.   \nM. Mahdian and Q. Yan. Online bipartite matching with random arrivals: An approach based on strongly factor-revealing lps. In Proceedings of the Forty-Third Annual ACM Symposium on Theory of Computing, STOC \u201911, page 597\u2013606, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450306911. doi: 10.1145/1993636.1993716. URL https://doi.org/10.1145/1993636.1993716.   \nM. Purohit, Z. Svitkina, and R. Kumar. Improving online algorithms via ML predictions. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 9684\u20139693, 2018. URL https://proceedings.neurips.cc/paper/2018/ hash/73a427badebe0e32caa2e1fc7530b7f3-Abstract.html.   \nA. Rubinstein. Beyond matroids: Secretary problem and prophet inequality with general constraints. In Proceedings of the Forty-Eighth Annual ACM Symposium on Theory of Computing, STOC \u201916, page 324\u2013332, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450341325. doi: 10.1145/2897518.2897540. URL https://doi.org/10.1145/2897518. 2897540.   \nA. Wei and F. Zhang. Optimal robustness-consistency trade-offs for learning-augmented online algorithms. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.   \nA. Zeynali, B. Sun, M. Hajiesmaili, and A. Wierman. Data-driven competitive algorithms for online knapsack and set cover. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12): 10833\u201310841, May 2021. doi: 10.1609/aaai.v35i12.17294. URL https://ojs.aaai.org/ index.php/AAAI/article/view/17294. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: Proofs for the results and statements made in the abstract and introduction are given in the main body and appendix. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: Limitations of the algorithms in the paper are discussed in the main body at several positions. In addition, improved approaches are given which overcome these limitations. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Assumptions are mentioned clearly with the respective statements. Proofs are provided mainly in the appendix due to space constraints. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper provides all information such that the simulations can easily be reproduced. Algorithms should be easily implementable within a few lines, sample sizes and distributions are stated. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 16}, {"type": "text", "text": "Justification: We will make the code and data publicly available after the review process. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: There was no training, neither a testing phase. Simulations only contain samples from distributions and the performance of our algorithms. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: See answer of Question 6. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Hardware settings are provided as a footnote in the main body of the paper. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The research conforms with the Code of Ethics. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The goal of our paper is to gain a better understanding of different kinds of predictions compared to the ones studied in the literature before. There are no further societal consequences beyond those for algorithms with predictions. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not pose any risks. There are no further consequences beyond those for theoretical results on algorithms with predictions. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not use any code or data from previous work. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The code for the simulations in the paper is straight-forward. We will make a well-documented version publicly available after the review process. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}]