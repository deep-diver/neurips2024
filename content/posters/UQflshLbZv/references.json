{"references": [{"fullname_first_author": "Prafulla Dhariwal", "paper_title": "Diffusion models beat gans on image synthesis", "publication_date": "2021-12-31", "reason": "This paper demonstrates the superiority of diffusion models over GANs for image synthesis, providing a strong foundation for the proposed method which utilizes latent diffusion models for hair editing."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, introduced in this paper, is used for aligning visual and textual inputs in a shared embedding space, enabling multi-modal control for hair editing by incorporating both text and image references."}, {"fullname_first_author": "Tero Karras", "paper_title": "A style-based generator architecture for generative adversarial networks", "publication_date": "2019-06-01", "reason": "StyleGAN, introduced in this paper, is a widely used GAN architecture that's compared with the proposed diffusion model, highlighting the advantages of the latter for hair editing tasks, especially those involving multi-color hair."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-01", "reason": "This paper introduces Stable Diffusion, the core architecture used by the proposed method, highlighting its advanced capabilities in generating high-quality images and enabling the separation of hair color and hairstyle control."}, {"fullname_first_author": "Tianyi Wei", "paper_title": "Hairclip: Design your hair by text and reference image", "publication_date": "2022-06-01", "reason": "This paper introduces HairCLIP, a method for hair editing based on StyleGAN and CLIP, providing a strong baseline and inspiration for the proposed diffusion-model-based method, particularly in handling multi-color hair scenarios."}]}