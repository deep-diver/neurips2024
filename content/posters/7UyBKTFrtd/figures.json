[{"figure_path": "7UyBKTFrtd/figures/figures_1_1.jpg", "caption": "Figure 1: Visualization of SpLiCE, which converts dense, uninterpretable CLIP representations (z) into sparse semantic decompositions (w) by solving for a sparse nonnegative linear combination over an overcomplete concept set (C).", "description": "The figure illustrates the SpLiCE method.  Dense CLIP image representations are transformed into sparse, interpretable concept decompositions.  SpLiCE uses a sparse, nonnegative linear solver to find the optimal sparse combination of concepts (from a large, overcomplete concept set) that best represents the original CLIP embedding.  The input is a set of dense image representations from CLIP. The output is a set of sparse semantic decompositions, where each decomposition consists of a small number of weighted concepts. The process leverages an overcomplete concept set to enable flexible and sparse representations.", "section": "4 Method"}, {"figure_path": "7UyBKTFrtd/figures/figures_2_1.jpg", "caption": "Figure 2: Example images from MSCOCO shown with their captions below and their concept decompositions on the right. We display the top seven concepts for visualization purposes, but images in the figure had decompositions with 7\u201320 concepts.", "description": "This figure shows six example images from the MSCOCO dataset, each with its corresponding caption and top seven concepts identified using SpLiCE.  The concept decompositions illustrate how SpLiCE represents the image semantics in terms of a sparse combination of human-interpretable concepts. Note that each image actually has a decomposition with 7 to 20 concepts; only the top seven are displayed for visual clarity.", "section": "5.4 Qualitative Assessment of Decompositions"}, {"figure_path": "7UyBKTFrtd/figures/figures_5_1.jpg", "caption": "Figure 3: Performance of SpLiCE decomposition representations on zero-shot classification tasks (bottom row) and cosine similarity between CLIP embeddings and SpLiCE embeddings (top row). Our proposed semantic dictionary (yellow) closely approximates CLIP on zero-shot classification accuracy, but not on the cosine similarity. This indicates that SpLiCE captures the semantic information in CLIP, but not its non-semantic components, explaining both the high zero-shot accuracy and low cosine similarity. See \u00a75.2 for discussion.", "description": "This figure displays the performance of SpLiCE on zero-shot classification and cosine similarity reconstruction tasks.  The top row shows the cosine similarity between the original CLIP embeddings and the SpLiCE representations for different datasets (CIFAR100, MIT States, and ImageNet). The bottom row shows the zero-shot classification accuracy on the same datasets. The results indicate that SpLiCE, using a semantic dictionary (yellow line), achieves high zero-shot accuracy, comparable to CLIP (black line), while demonstrating lower cosine similarity.  This suggests that SpLiCE effectively captures the semantic information in CLIP while filtering out non-semantic components.", "section": "5.2 Sparsity-Performance Tradeoffs"}, {"figure_path": "7UyBKTFrtd/figures/figures_7_1.jpg", "caption": "Figure 4: Left: SpLiCE decompositions of ImageNet \u2018African Elephant\u2019, \u2018Curly-coated Retriever\u2019, \u2018Monarch Butterfly\u2019, \u2018Digital Clock\u2019 classes. Right: Distribution of \u201cSwimwear\u201d concept in \u2018Woman\u2019 and \u2018Man\u2019 classes of CIFAR100.", "description": "The figure shows the top concepts identified by SpLiCE for four different ImageNet classes, demonstrating the method's ability to capture semantically meaningful components in the data. It also visualizes the distribution of the \"Swimwear\" concept within the \"Woman\" and \"Man\" classes of the CIFAR-100 dataset, highlighting the presence of potential biases in the dataset.", "section": "5.4 Qualitative Assessment of Decompositions"}, {"figure_path": "7UyBKTFrtd/figures/figures_7_2.jpg", "caption": "Figure 3: Performance of SpLiCE decomposition representations on zero-shot classification tasks (bottom row) and cosine similarity between CLIP embeddings and SpLiCE embeddings (top row). Our proposed semantic dictionary (yellow) closely approximates CLIP on zero-shot classification accuracy, but not on the cosine similarity. This indicates that SpLiCE captures the semantic information in CLIP, but not its non-semantic components, explaining both the high zero-shot accuracy and low cosine similarity. See \u00a75.2 for discussion.", "description": "This figure shows the performance of SpLiCE decompositions in zero-shot classification tasks and cosine similarity with CLIP embeddings.  The top row displays the cosine similarity between original CLIP embeddings and SpLiCE representations across different datasets (CIFAR100, MIT States, and ImageNet). The bottom row shows the zero-shot accuracy of SpLiCE representations on the same datasets.  The results indicate SpLiCE successfully captures semantic information while discarding non-semantic components, leading to high zero-shot accuracy despite lower cosine similarity to the original CLIP embeddings.  The yellow line in the graphs represents the performance using SpLiCE's proposed semantic dictionary.", "section": "5.2 Sparsity-Performance Tradeoffs"}, {"figure_path": "7UyBKTFrtd/figures/figures_16_1.jpg", "caption": "Figure 3: Performance of SpLiCE decomposition representations on zero-shot classification tasks (bottom row) and cosine similarity between CLIP embeddings and SpLiCE embeddings (top row). Our proposed semantic dictionary (yellow) closely approximates CLIP on zero-shot classification accuracy, but not on the cosine similarity. This indicates that SpLiCE captures the semantic information in CLIP, but not its non-semantic components, explaining both the high zero-shot accuracy and low cosine similarity. See \u00a75.2 for discussion.", "description": "This figure displays the performance of SpLiCE decompositions on zero-shot classification and cosine similarity reconstruction tasks across multiple datasets. It compares the performance of SpLiCE using a semantic concept vocabulary to baselines using random and learned vocabularies. The results show that SpLiCE's semantic dictionary achieves comparable zero-shot classification accuracy to CLIP but lower cosine similarity. This suggests that SpLiCE captures the semantic meaning while discarding non-semantic aspects of the CLIP embeddings.", "section": "5.2 Sparsity-Performance Tradeoffs"}, {"figure_path": "7UyBKTFrtd/figures/figures_16_2.jpg", "caption": "Figure 7: Average cosine similarity across pairs of image-text, image-image, and text-text data from MSCOCO. After aligning modalities, the distribution of similarities is centered around zero.", "description": "This figure shows the distribution of cosine similarity scores between different pairs of modalities (image-image, text-text, and image-text) in the MSCOCO dataset. The left panel shows the distribution before any modality alignment, illustrating a clear separation between modalities with higher similarity within each modality than across them. The right panel displays the same distribution after applying a modality alignment technique (mean-centering), showing a centered distribution around zero, indicating successful alignment of the modalities in the vector space.", "section": "A.4 Effect of Modality Alignment"}, {"figure_path": "7UyBKTFrtd/figures/figures_17_1.jpg", "caption": "Figure 3: Performance of SpLiCE decomposition representations on zero-shot classification tasks (bottom row) and cosine similarity between CLIP embeddings and SpLiCE embeddings (top row). Our proposed semantic dictionary (yellow) closely approximates CLIP on zero-shot classification accuracy, but not on the cosine similarity. This indicates that SpLiCE captures the semantic information in CLIP, but not its non-semantic components, explaining both the high zero-shot accuracy and low cosine similarity. See \u00a75.2 for discussion.", "description": "This figure shows the results of experiments evaluating the performance of SpLiCE on zero-shot classification and the similarity between SpLiCE and CLIP embeddings. The results indicate that SpLiCE, using its proposed semantic dictionary, achieves high accuracy in zero-shot classification, comparable to CLIP. However, the cosine similarity between SpLiCE and CLIP embeddings is lower, suggesting SpLiCE captures semantic information but not non-semantic components of CLIP.", "section": "5.2 Sparsity-Performance Tradeoffs"}, {"figure_path": "7UyBKTFrtd/figures/figures_18_1.jpg", "caption": "Figure 3: Performance of SpLiCE decomposition representations on zero-shot classification tasks (bottom row) and cosine similarity between CLIP embeddings and SpLiCE embeddings (top row). Our proposed semantic dictionary (yellow) closely approximates CLIP on zero-shot classification accuracy, but not on the cosine similarity. This indicates that SpLiCE captures the semantic information in CLIP, but not its non-semantic components, explaining both the high zero-shot accuracy and low cosine similarity. See \u00a75.2 for discussion.", "description": "This figure shows the performance of SpLiCE decompositions on zero-shot classification and the cosine similarity between CLIP and SpLiCE embeddings across different datasets.  The top row displays the cosine similarity, showing that SpLiCE's concept-based representations closely match CLIP's zero-shot classification performance (bottom row) but not the raw cosine similarity.  This demonstrates that SpLiCE successfully captures the semantic aspects of CLIP representations while discarding non-semantic information, leading to high accuracy with improved interpretability.", "section": "5.2 Sparsity-Performance Tradeoffs"}, {"figure_path": "7UyBKTFrtd/figures/figures_19_1.jpg", "caption": "Figure 10: Example concept histograms of various ImageNet classes. The top seven concepts for each class are visualized along with their relative weighting, with the average l0 norm of individual sample decompositions also being 7.", "description": "This figure shows bar charts visualizing the top seven concepts with their weights for five different ImageNet classes: 'Face Powder', 'Feather Boa', 'Jack-O'-Lantern', 'Kimono', and 'Dalmatian'. Each bar chart represents a class, showing the relative importance of each concept in characterizing that class, according to SpLiCE.  The figure highlights the interpretability of SpLiCE by showing how the algorithm decomposes each image into a sparse combination of human-understandable concepts. For example, in the 'Dalmatian' class, the top concepts are dog breed, spots, and black-and-white, which are highly relevant to the visual characteristics of Dalmatians. The average l0 norm of 7 indicates that, on average, each image is represented by 7 concepts.", "section": "B.5 Additional ImageNet Concept Histograms"}, {"figure_path": "7UyBKTFrtd/figures/figures_20_1.jpg", "caption": "Figure 11: Distribution of \u201cDesert", "description": "This histogram displays the distribution of the concept weight for \"desert\" across the two CIFAR100 classes, \"Camel\" and \"Kangaroo\".  It illustrates the higher prevalence of the \"desert\" concept in images of camels compared to kangaroos, suggesting a potential spurious correlation between the presence of camels and desert environments in this dataset.", "section": "Discovering Spurious Correlations in CIFAR100"}, {"figure_path": "7UyBKTFrtd/figures/figures_21_1.jpg", "caption": "Figure 12: Visualization of the presence of convertibles (pink lines) and yellow cars (yellow lines) in Stanford Cars over time. SpLiCE concept weights (dotted) closely track the groundtruth concept prevalence (solid) for both concepts.", "description": "This figure visualizes how the prevalence of convertibles and yellow cars in the Stanford Cars dataset changes over the years. The solid lines represent the actual percentage of convertibles and yellow cars in each year, while the dotted lines show the corresponding concept weights obtained using SpLiCE.  The close alignment between the actual prevalence and SpLiCE concept weights indicates that SpLiCE accurately captures the temporal trends in these car attributes.", "section": "B.8 Additional Case Study: Distribution Shift Monitoring"}, {"figure_path": "7UyBKTFrtd/figures/figures_22_1.jpg", "caption": "Figure 3: Performance of SpLiCE decomposition representations on zero-shot classification tasks (bottom row) and cosine similarity between CLIP embeddings and SpLiCE embeddings (top row). Our proposed semantic dictionary (yellow) closely approximates CLIP on zero-shot classification accuracy, but not on the cosine similarity. This indicates that SpLiCE captures the semantic information in CLIP, but not its non-semantic components, explaining both the high zero-shot accuracy and low cosine similarity. See \u00a75.2 for discussion.", "description": "This figure presents the results of experiments evaluating SpLiCE's performance on zero-shot image classification and the similarity between its representations and those from original CLIP.  The top row shows cosine similarity, demonstrating that SpLiCE (using a learned semantic dictionary) closely matches the original CLIP representations in zero-shot classification but significantly differs in terms of cosine similarity. The bottom row shows the zero-shot accuracy, which remains high even when employing the SpLiCE semantic concept dictionary.  This suggests that SpLiCE successfully captures the semantic meaning in CLIP embeddings without replicating its non-semantic aspects.", "section": "5.2 Sparsity-Performance Tradeoffs"}, {"figure_path": "7UyBKTFrtd/figures/figures_23_1.jpg", "caption": "Figure 1: Visualization of SpLiCE, which converts dense, uninterpretable CLIP representations (z) into sparse semantic decompositions (w) by solving for a sparse nonnegative linear combination over an overcomplete concept set (C).", "description": "The figure illustrates the SpLiCE method.  It shows how dense CLIP image embeddings are converted into sparse, interpretable representations.  The process involves finding a sparse, non-negative linear combination of a set of human-interpretable concepts to approximate the original CLIP embedding.  This is represented visually with the input image, its dense CLIP representation (z), the sparse concept decomposition (w) produced by SpLiCE, and the overcomplete concept set (C) that the linear combination is drawn from.", "section": "Method"}, {"figure_path": "7UyBKTFrtd/figures/figures_23_2.jpg", "caption": "Figure 3: Performance of SpLiCE decomposition representations on zero-shot classification tasks (bottom row) and cosine similarity between CLIP embeddings and SpLiCE embeddings (top row). Our proposed semantic dictionary (yellow) closely approximates CLIP on zero-shot classification accuracy, but not on the cosine similarity. This indicates that SpLiCE captures the semantic information in CLIP, but not its non-semantic components, explaining both the high zero-shot accuracy and low cosine similarity. See \u00a75.2 for discussion.", "description": "This figure shows the performance of SpLiCE decompositions on zero-shot classification tasks and the cosine similarity between CLIP and SpLiCE embeddings across three datasets: CIFAR100, MIT States, and ImageNet.  The top row displays cosine similarity between original CLIP embeddings and SpLiCE representations with varying sparsity levels. The bottom row presents the zero-shot classification accuracy for the same representations.  The results indicate that SpLiCE effectively captures the semantic information from CLIP, achieving high zero-shot accuracy while maintaining a relatively low cosine similarity to the original embeddings.  This suggests SpLiCE is successfully disentangling semantic information from non-semantic aspects of CLIP's dense representations.", "section": "5.2 Sparsity-Performance Tradeoffs"}]