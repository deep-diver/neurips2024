{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces CLIP, a foundational model for the work, and its high performance across various tasks is a key motivation for this research."}, {"fullname_first_author": "Pang Wei Koh", "paper_title": "Understanding black-box predictions via influence functions", "publication_date": "2017-00-00", "reason": "This paper introduces a method for interpreting black-box models, which is highly relevant to this paper's goal of improving the interpretability of CLIP."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "Deep learning of representations: Looking forward", "publication_date": "2013-00-00", "reason": "This paper is a seminal work in the field of deep learning, and its focus on learning representations is highly relevant to this research."}, {"fullname_first_author": "Been Kim", "paper_title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV)", "publication_date": "2018-00-00", "reason": "This paper introduces a method for evaluating the interpretability of models, which is highly relevant to this research's approach."}, {"fullname_first_author": "Pang Wei Koh", "paper_title": "Concept bottleneck models", "publication_date": "2020-00-00", "reason": "This paper introduces concept bottleneck models, a type of model that is closely related to the methods used in this paper."}]}