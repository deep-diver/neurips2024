[{"figure_path": "7UyBKTFrtd/tables/tables_3_1.jpg", "caption": "Table 1: Sanity checking the linearity of CLIP Embeddings.", "description": "This table presents the results of a sanity check to evaluate the linearity assumption of CLIP embeddings.  The experiment combines two images or text inputs and compares their joint CLIP embedding to the average of their individual embeddings.  The table shows the weights (w\u2090 and w\u0562) obtained by solving the linear equation and the cosine similarity between the combined embedding and the weighted average of individual embeddings for four different datasets (ImageNet, CIFAR100, MIT States, COCO Text). The results show that the weights are close to 0.5 and the cosine similarity is high across all datasets, providing evidence that CLIP embeddings exhibit approximately linear behavior in concept space.", "section": "3 When do Sparse Decompositions Exist?"}, {"figure_path": "7UyBKTFrtd/tables/tables_9_1.jpg", "caption": "Table 2: Evaluation of intervention on the concept 'Glasses' for the CelebA dataset. SpLiCE allows for surgical removal of information related to whether or not someone is wearing glasses, without impacting other features such as gender. (ZS = Zero Shot Accuracy)", "description": "This table presents the results of an experiment on the CelebA dataset to evaluate the impact of removing the \"Glasses\" concept using SpLiCE. It compares the zero-shot accuracy (ZS) of CLIP, SpLiCE, and SpLiCE with intervention on two tasks: Gender and Glasses classification.  The intervention involves removing the \"Glasses\" concept, demonstrating that SpLiCE can surgically remove specific information without affecting other features. Linear probes were also used to assess the impact of removing the \"Glasses\" concept on model performance.", "section": "5.4 Qualitative Assessment of Decompositions"}, {"figure_path": "7UyBKTFrtd/tables/tables_17_1.jpg", "caption": "Table 3: Evaluation of Probing Performance on CIFAR100", "description": "This table presents the results of probing experiments conducted on the CIFAR100 dataset. Two types of probes were used: one trained on CLIP embeddings and tested on SpLiCE embeddings (CLIP Probe), and one trained and evaluated on SpLiCE embeddings (SpLiCE Probe).  The table shows the performance of these probes for various sparsity levels (l0 norms) of the SpLiCE embeddings, ranging from 3 to 117. The performance is measured in terms of accuracy and serves to illustrate the trade-off between interpretability (sparsity) and performance.", "section": "5.2 Sparsity-Performance Tradeoffs"}, {"figure_path": "7UyBKTFrtd/tables/tables_18_1.jpg", "caption": "Table 4: Evaluation of Probing Performance on MIT States", "description": "This table presents the results of probing experiments conducted on the MIT States dataset. It compares the performance of probes trained on SpLiCE embeddings and CLIP embeddings.  Different levels of sparsity (l0) are tested for SpLiCE, and the performance is measured in terms of accuracy. The results show that SpLiCE embeddings, at different sparsity levels, achieve comparable performance to CLIP embeddings on this probing task.", "section": "5.2 Sparsity-Performance Tradeoffs"}, {"figure_path": "7UyBKTFrtd/tables/tables_18_2.jpg", "caption": "Table 5: Additional zero-shot accuracy on baselines from the CLIP paper, for decompositions of sparsity 20-35. Note that at human-interpretable levels of sparsity, we see a minor drop in performance.", "description": "This table presents additional zero-shot accuracy results obtained using SpLiCE and compares them to the baseline results reported in the original CLIP paper.  The results are shown for four datasets: Caltech101, SUN397, STL10, and VOC2007.  SpLiCE uses decompositions with sparsity levels between 20 and 35. The table shows that while SpLiCE achieves comparable accuracy to CLIP's baseline, there is a slight drop in performance when using human-interpretable sparsity levels. ", "section": "B.4 Additional Zero-Shot Results"}, {"figure_path": "7UyBKTFrtd/tables/tables_19_1.jpg", "caption": "Table 6: Zero shot performance at sparsity 512. Note that SpLiCE completely recovers baseline CLIP zero shot accuracy.", "description": "This table presents the zero-shot accuracy results for three different datasets (CIFAR100, MIT States, and ImageNet) when using SpLiCE with a sparsity level of 512.  The results show that SpLiCE achieves almost identical performance to the baseline CLIP model at this high sparsity level, demonstrating that SpLiCE can fully recover the performance of CLIP while maintaining its improved interpretability.", "section": "B.5 Additional ImageNet Concept Histograms"}, {"figure_path": "7UyBKTFrtd/tables/tables_20_1.jpg", "caption": "Table 7: Evaluation of intervention on spurious correlations for Waterbirds dataset. Removing information about land backgrounds improves worst-case subgroup performance.", "description": "This table presents the results of an experiment on the Waterbirds dataset, which aims to mitigate spurious correlations between bird types and their backgrounds (land vs. water).  A linear probe model was used for classification. The first row shows the accuracy of the linear probe on landbirds and waterbirds when the model is trained on the original dataset. The second row shows the results after removing information about land backgrounds from the SpLiCE representation of the images, illustrating how this intervention can improve the performance, specifically on waterbirds on land, by reducing bias.", "section": "5.2 Sparsity-Performance Tradeoffs"}, {"figure_path": "7UyBKTFrtd/tables/tables_21_1.jpg", "caption": "Table 8: Study of the differences in distributions between train, validation, and test splits of Waterbirds. The validation and test splits are much more similar to each other than they are to the train split.", "description": "This table shows the cosine similarity between the class distributions for the train, validation, and test splits of the Waterbirds dataset. The values indicate the differences between the distributions.  The results show that the validation and test sets are much more similar to each other than either is to the training set, suggesting a potential issue with the distribution of data across splits.", "section": "B. Additional Results"}, {"figure_path": "7UyBKTFrtd/tables/tables_21_2.jpg", "caption": "Table 9: Study of the prevalence of the concept \u201cbamboo\u201d in the different classes and splits of Waterbirds.", "description": "This table presents the weighted average of the concept \"bamboo\" across different classes (landbird and waterbird) and data splits (train, validation, and test) of the Waterbirds dataset. The values show that the training set has a much higher weighted average for the concept \"bamboo\" in the landbird class than in the waterbird class.  In contrast, the validation and test sets show a more balanced distribution of this concept across both classes. This suggests a potential bias in the training data, where the concept \"bamboo\" is more strongly associated with landbirds. The table highlights the importance of examining data splits for potential biases that might affect model performance.", "section": "B.10 Checking the Interpretability of Negative Concepts"}, {"figure_path": "7UyBKTFrtd/tables/tables_22_1.jpg", "caption": "Table 10: Evaluation of the similarity of antonyms and negative concepts in CLIP.", "description": "This table presents the results of an experiment evaluating the similarity between antonyms and concepts preceded by \"not\" in CLIP embeddings.  Two sets of cosine similarity scores are reported: one without concept centering and one with concept centering. The results indicate that even with centering, CLIP does not place antonyms in opposite directions in its embedding space, as evidenced by high similarity scores between antonyms and between concepts and their negated versions. This suggests that CLIP's internal representation of semantics may not rely on simple antonym relationships.", "section": "B.10 Checking the Interpretability of Negative Concepts"}]