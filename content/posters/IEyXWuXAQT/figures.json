[{"figure_path": "IEyXWuXAQT/figures/figures_8_1.jpg", "caption": "Figure 1: Experiments results. Figure 1a compares the optimisation performance of our algorithm SuPAC-CE with gradient descent approaches on an biochemical calibration task. Optimisation procedures were repeated 20 times; median performance and quantiles 0.2 and 0.8 are represented. Figure 1b investigates train and test performance of the meta-learning approach of Section 5. Mean test performance, as well as quantiles 0.2 and 0.8 for the sequence of built prior is assessed on 40 tasks and compared to the train performance. SuPAC-CE reduced the PAC-Bayes objective to 0.121 \u00b1 0.004 (avg. risk of posterior of 0.102 \u00b1 0.003).", "description": "This figure summarizes the experimental results of the paper.  Figure 1a shows a comparison of the optimization performance of the SuPAC-CE algorithm against gradient descent methods on a biochemical calibration task. The median performance and 0.2 and 0.8 quantiles from 20 repeated optimization runs are shown. Figure 1b presents the training and testing performance of the meta-learning approach (Section 5) by visualizing the train meta-score, test meta-score, and test score fluctuation across 150 training iterations. The results showcase SuPAC-CE's efficiency in minimizing the PAC-Bayes objective and its improved performance in meta-learning.", "section": "6 Experiments"}, {"figure_path": "IEyXWuXAQT/figures/figures_16_1.jpg", "caption": "Figure 2: Overview of SuPAC-CE. At each step, some new predictors are drawn from the current posterior approximation and evaluated (top right figure). All evaluated predictors are then weighted according to the weight of their Voronoi cell (bottom right figure). These weighted evaluations are used to construct an optimal approximation of the score through a linear least square task (bottom left figure). The approximated score is used to update the posterior using a closed form expression (top left figure). This procedure is looped until convergence (center).", "description": "This figure illustrates the workflow of the SuPAC-CE algorithm. It shows how new predictors are sampled from the current posterior distribution, their scores are evaluated, weighted according to their Voronoi cells, and used in a linear least squares task to approximate the score function.  The approximated score is then used to update the posterior distribution iteratively, until convergence is achieved.", "section": "4.2 Implementing the approximation"}, {"figure_path": "IEyXWuXAQT/figures/figures_16_2.jpg", "caption": "Figure 1: Experiments results. Figure 1a compares the optimisation performance of our algorithm SuPAC-CE with gradient descent approaches on an biochemical calibration task. Optimisation procedures were repeated 20 times; median performance and quantiles 0.2 and 0.8 are represented. Figure 1b investigates train and test performance of the meta-learning approach of Section 5. Mean test performance, as well as quantiles 0.2 and 0.8 for the sequence of built prior is assessed on 40 tasks and compared to the train performance. SuPAC-CE reduced the PAC-Bayes objective to 0.121 \u00b1 0.004 (avg. risk of posterior of 0.102 \u00b1 0.003).", "description": "The figure compares the performance of the proposed SuPAC-CE algorithm to standard gradient descent for an industrial biochemical calibration task (Figure 1a).  It also shows results for a meta-learning approach where the algorithm learns a better prior (Figure 1b). The key finding is that SuPAC-CE is significantly more efficient in minimizing the PAC-Bayes bound than gradient descent.", "section": "6 Experiments"}, {"figure_path": "IEyXWuXAQT/figures/figures_17_1.jpg", "caption": "Figure 1: Experiments results. Figure 1a compares the optimisation performance of our algorithm SuPAC-CE with gradient descent approaches on an biochemical calibration task. Optimisation procedures were repeated 20 times; median performance and quantiles 0.2 and 0.8 are represented. Figure 1b investigates train and test performance of the meta-learning approach of Section 5. Mean test performance, as well as quantiles 0.2 and 0.8 for the sequence of built prior is assessed on 40 tasks and compared to the train performance. SuPAC-CE reduced the PAC-Bayes objective to 0.121 \u00b1 0.004 (avg. risk of posterior of 0.102 \u00b1 0.003).", "description": "This figure displays the results of two experiments.  The first (Figure 1a) compares the performance of SuPAC-CE to gradient descent methods on a biochemical calibration task, showing SuPAC-CE's superior performance in minimizing the PAC-Bayes objective. The second (Figure 1b) illustrates the results of applying SuPAC-CE to a meta-learning framework (as discussed in Section 5), demonstrating the algorithm's effectiveness in improving meta-objective scores across multiple training tasks and generalizing well to unseen test tasks.", "section": "6 Experiments"}, {"figure_path": "IEyXWuXAQT/figures/figures_18_1.jpg", "caption": "Figure 1: Experiments results. Figure 1a compares the optimisation performance of our algorithm SuPAC-CE with gradient descent approaches on an biochemical calibration task. Optimisation procedures were repeated 20 times; median performance and quantiles 0.2 and 0.8 are represented. Figure 1b investigates train and test performance of the meta-learning approach of Section 5. Mean test performance, as well as quantiles 0.2 and 0.8 for the sequence of built prior is assessed on 40 tasks and compared to the train performance. SuPAC-CE reduced the PAC-Bayes objective to 0.121 \u00b1 0.004 (avg. risk of posterior of 0.102 \u00b1 0.003).", "description": "This figure shows the results of experiments comparing the performance of the proposed SuPAC-CE algorithm with gradient descent methods for a biochemical calibration task (Figure 1a) and a meta-learning approach (Figure 1b). Figure 1a illustrates that SuPAC-CE is more efficient and stable than gradient descent in minimizing the PAC-Bayes bound.  Figure 1b demonstrates the meta-learning approach where SuPAC-CE significantly improves the meta-objective, showing its ability to learn a good prior for better generalization.", "section": "6 Experiments"}, {"figure_path": "IEyXWuXAQT/figures/figures_18_2.jpg", "caption": "Figure 1: Experiments results. Figure 1a compares the optimisation performance of our algorithm SuPAC-CE with gradient descent approaches on an biochemical calibration task. Optimisation procedures were repeated 20 times; median performance and quantiles 0.2 and 0.8 are represented. Figure 1b investigates train and test performance of the meta-learning approach of Section 5. Mean test performance, as well as quantiles 0.2 and 0.8 for the sequence of built prior is assessed on 40 tasks and compared to the train performance. SuPAC-CE reduced the PAC-Bayes objective to 0.121 \u00b1 0.004 (avg. risk of posterior of 0.102 \u00b1 0.003).", "description": "This figure shows the results of experiments comparing the performance of the proposed algorithm SuPAC-CE with gradient descent methods on a biochemical calibration task (Figure 1a) and a meta-learning approach (Figure 1b). Figure 1a shows that SuPAC-CE is significantly more efficient than gradient descent at minimizing the PAC-Bayes bound.  Figure 1b demonstrates the effectiveness of the meta-learning strategy for improving generalization performance.", "section": "6 Experiments"}, {"figure_path": "IEyXWuXAQT/figures/figures_19_1.jpg", "caption": "Figure 7: Performance of SuPAC-CE with extreme hyperparameters values. Each optimisation procedure was repeated 8 times; the median performance and 0.2 and 0.8 quantiles are represented. SuPAC-CE exhibited noticeable instabilities and speed loss for hyperparameters leading to insufficient regularization (blue curve). Too much regularisation lead to speed decrease in the early phase of the optimisation procedure (purple curve)", "description": "This figure shows the performance of the SuPAC-CE algorithm under various hyperparameter settings.  The x-axis represents the number of risk queries, and the y-axis represents the PAC-Bayes objective.  Multiple runs are shown for each set of hyperparameters, indicating the variability in performance.  The results demonstrate a trade-off between speed and stability; insufficient regularization leads to instability, while excessive regularization slows down the optimization process.", "section": "Experiments"}]