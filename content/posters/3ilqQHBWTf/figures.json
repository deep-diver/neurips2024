[{"figure_path": "3ilqQHBWTf/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of the E2V pipeline between HyperE2VID [13] and our LaSe-E2V: The baseline method solely relies on event data, leading to ambiguity in local structures. In contrast, our approach integrates language descriptions to enrich the semantic information and ensure the video remains coherent with the event stream.", "description": "This figure compares the event-to-video (E2V) reconstruction pipelines of HyperE2VID and the proposed LaSe-E2V method.  HyperE2VID, relying solely on event data, produces ambiguous and blurry results due to the lack of semantic information.  LaSe-E2V, on the other hand, incorporates language descriptions to provide semantic context, leading to clearer and more coherent video reconstructions.", "section": "1 Introduction"}, {"figure_path": "3ilqQHBWTf/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of our proposed LaSe-E2V framework.", "description": "The figure shows the architecture of the LaSe-E2V framework, which consists of four main components: an image encoder (E\u2081), an event encoder (E\u2091), a video latent diffusion model (LDM), and a decoder (D\u2081).  The event encoder processes raw events to create an event voxel grid, which is then fed into the LDM along with image features and text embeddings from a CLIP text encoder.  The LDM incorporates an Event-guided Spatiotemporal Attention (ESA) module to better align the event data with the video generation process. Finally, a decoder generates the reconstructed video.  The framework also uses an event-aware mask loss (l\u2098) to ensure temporal consistency and improve the overall quality of the reconstructed video.", "section": "3 The Proposed LaSe-E2V Framework"}, {"figure_path": "3ilqQHBWTf/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative comparisons on four sampled sequences from the test datasets. While the previous approaches suffer from low contrast, blur, and extensive artifacts, LaSe-E2V obtains clear edges with high contrast and preserves the semantic details of the objects", "description": "This figure shows a qualitative comparison of event-to-video (E2V) reconstruction results on four test sequences using different methods, including the proposed LaSe-E2V.  The red boxes highlight specific regions for easier comparison.  The results demonstrate that LaSe-E2V produces significantly clearer and sharper images, with better contrast and preservation of details, compared to other existing methods which suffer from blur, low contrast, and noticeable artifacts. The improvements showcased highlight the superior performance of LaSe-E2V in preserving image quality and semantic details.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "3ilqQHBWTf/figures/figures_6_2.jpg", "caption": "Figure 4: Qualitative results of fast-motion condition from HS-ERGB dataset [58].", "description": "This figure shows qualitative results of the proposed LaSe-E2V model on the HS-ERGB dataset, focusing on fast motion scenarios.  It visually compares the generated video frames (LaSe-E2V (Ours)) with the ground truth (Reference Frame). The comparison demonstrates the model's ability to accurately reconstruct video sequences with fast-moving objects, preserving details and temporal consistency even under challenging conditions. The HS-ERGB dataset is particularly relevant because it offers high-quality video with events captured by Prophesee Gen4 event cameras.", "section": "4 Experiments"}, {"figure_path": "3ilqQHBWTf/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative results in low light condition from MVSEC dataset [76] (outdoor_night2). LaSe-E2V performs better to preserve the HDR characteristic of event cameras with higher contrast.", "description": "This figure presents a qualitative comparison of video reconstruction results under low-light conditions using different methods.  The top row shows results from the E2VID method; the second row, from HyperE2VID; the third row, from the proposed LaSe-E2V; and the bottom row displays ground truth reference frames. Each row presents a sequence of frames to showcase the temporal evolution of the reconstruction, highlighting the performance of LaSe-E2V in maintaining high contrast and detail even in challenging low-light situations.  It demonstrates LaSe-E2V's ability to better preserve the high dynamic range (HDR) capabilities of event cameras compared to other methods. ", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "3ilqQHBWTf/figures/figures_8_1.jpg", "caption": "Figure 3: Qualitative comparisons on four sampled sequences from the test datasets. While the previous approaches suffer from low contrast, blur, and extensive artifacts, LaSe-E2V obtains clear edges with high contrast and preserves the semantic details of the objects.", "description": "This figure presents a qualitative comparison of event-to-video reconstruction results from different methods, including LaSe-E2V and several baselines. It demonstrates the superior performance of LaSe-E2V in terms of visual clarity, contrast, and semantic detail preservation compared to other methods. The baselines exhibit low contrast, blur, and noticeable artifacts, while LaSe-E2V produces sharper images with more accurate representation of objects and their features.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "3ilqQHBWTf/figures/figures_8_2.jpg", "caption": "Figure 7: Qualitative comparison for the ablation study on text guidance and ESA module.", "description": "This figure presents a qualitative comparison of the results obtained from ablation studies on text guidance and the Event-guided Spatio-temporal Attention (ESA) module.  It visually demonstrates the impact of both text descriptions and ESA on the quality of video reconstruction. The left side shows the effects of including or excluding text descriptions, while the right side illustrates the influence of the ESA module.  Comparing the results shows the superior performance achieved with both text guidance and ESA, producing videos that more closely match the ground truth.", "section": "4.4 Ablation Study"}, {"figure_path": "3ilqQHBWTf/figures/figures_8_3.jpg", "caption": "Figure 8: Qualitative results for video editing with text.", "description": "This figure shows the results of a video editing experiment using language guidance.  The left column shows a reconstruction using the text prompt \u201cnight, dark, city street\u2026\u201d, resulting in a low-light scene. The middle column shows reconstruction using the prompt \u201cbright, day light, city street\u2026\u201d, resulting in a bright, daytime scene. The right column shows the reference frame for comparison. This demonstrates how language can be used to modify the lighting conditions in the generated video, highlighting the ability of the LaSe-E2V framework to incorporate semantic information effectively.", "section": "4.3 Discussion"}, {"figure_path": "3ilqQHBWTf/figures/figures_16_1.jpg", "caption": "Figure 3: Qualitative comparisons on four sampled sequences from the test datasets. While the previous approaches suffer from low contrast, blur, and extensive artifacts, LaSe-E2V obtains clear edges with high contrast and preserves the semantic details of the objects", "description": "This figure shows a qualitative comparison of event-to-video reconstruction results from several methods, including the proposed LaSe-E2V model.  It highlights that LaSe-E2V is superior to other approaches in terms of image clarity, contrast, and preservation of semantic details. The comparison is shown for four separate video sequences, demonstrating consistent improvements.", "section": "4.2 Comparison with State-of-the-Art Methods"}]