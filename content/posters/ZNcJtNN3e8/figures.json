[{"figure_path": "ZNcJtNN3e8/figures/figures_2_1.jpg", "caption": "Figure 1: GNNs with persistence (parallel mode).", "description": "This figure illustrates the parallel mode of integrating persistent homology (PH) into Graph Neural Networks (GNNs).  A graph G is input into a GNN, which produces node embeddings {z<sup>(l)</sup>}<sub>u</sub>. These embeddings are then used to compute a persistence diagram D<sub>G</sub>(G) via persistent homology. The persistence diagram is then vectorized using PersLay, resulting in a graph-level representation r<sub>G</sub>.  This representation, along with the graph-level representation z<sub>G</sub> from the GNN's readout layer, is concatenated and fed into a Multilayer Perceptron (MLP) for classification.", "section": "GNNs with persistence"}, {"figure_path": "ZNcJtNN3e8/figures/figures_7_1.jpg", "caption": "Figure 2: PersLay classifier: spectral norm vs. generalization gap. Overall, our bound on the spectral norm of the weights is highly correlated with the generalization gap.", "description": "This figure displays the relationship between the spectral norm of the weights in a PersLay classifier and its generalization gap across five different datasets (DHFR, MUTAG, PROTEINS, NCI1, IMDB-BINARY).  The plots show the generalization gap (the difference between generalization error and empirical error) and the spectral norm over training epochs. The strong positive correlation (indicated by the \u03c1 values) suggests that the theoretical bound on the spectral norm effectively captures the generalization performance.", "section": "6 Experiments"}, {"figure_path": "ZNcJtNN3e8/figures/figures_7_2.jpg", "caption": "Figure 3: PersLay classifier: width vs. generalization gap. The dependence of the empirical gap on the model width is captured by our bound. We obtain high average correlation for all datasets.", "description": "This figure shows the relationship between the width of the PersLay classifier model and its generalization gap (difference between the generalization and empirical errors).  The plot includes empirical generalization gap and the theoretical bound from the paper's analysis for five different datasets: DHFR, MUTAG, PROTEINS, NCI1, and IMDB-BINARY.  Each dataset has its own subplot, displaying how the generalization gap changes as the model width increases.  The high correlation between the empirical results and the theoretical bound suggests the model's generalization capacity is well-predicted by the proposed framework.", "section": "6 Experiments"}, {"figure_path": "ZNcJtNN3e8/figures/figures_8_1.jpg", "caption": "Figure 4: GNNs with persistence: empirical gap vs. PAC-Bayes bound. Again, there is positive and high correlation between our bound and the observed generalization gap.", "description": "This figure displays the empirical generalization gap (difference between the generalization error and empirical error) against the theoretical PAC-Bayes bound on four datasets (NCI1, NCI109, PROTEINS, IMDB-BINARY). Each subplot shows the trend for a specific dataset, where the x-axis represents the training epochs and the y-axis represents the generalization gap and the theoretical bound.  Shaded areas indicate the standard deviation over multiple runs. The correlation coefficient (\u03c1) between the empirical gap and our theoretical bound is displayed for each dataset, demonstrating a consistently high positive correlation.", "section": "6 Experiments"}, {"figure_path": "ZNcJtNN3e8/figures/figures_35_1.jpg", "caption": "Figure 3: PersLay classifier: width vs. generalization gap. The dependence of the empirical gap on the model width is captured by our bound. We obtain high average correlation for all datasets.", "description": "This figure compares the empirical generalization gap with the theoretical bound provided by the authors' work for a PersLay classifier. The x-axis represents the width (h) of the model, while the y-axis represents the generalization gap. Each dataset (DHFR, MUTAG, PROTEINS, NCI1, IMDB-BINARY) is shown separately, and for each dataset there are two lines: the empirical generalization gap and the theoretical bound. The shaded area represents the standard deviation of the empirical gap across different random seeds.  High correlation between the empirical gap and theoretical bounds shows that the theoretical bound accurately captures the trend of generalization performance as the model width changes.", "section": "6 Experiments"}, {"figure_path": "ZNcJtNN3e8/figures/figures_35_2.jpg", "caption": "Figure 2: PersLay classifier: spectral norm vs. generalization gap. Overall, our bound on the spectral norm of the weights is highly correlated with the generalization gap.", "description": "This figure displays the relationship between the spectral norm of weights in a PersLay classifier and its generalization gap across five different datasets (DHFR, MUTAG, PROTEINS, NCI1, IMDB-BINARY). For each dataset, the figure shows the generalization gap (the difference between the generalization error and the empirical error) and the spectral norm of the weights plotted against training epochs. The shaded area represents the standard deviation over multiple runs. The high correlation (rho values) between the spectral norm and generalization gap strongly supports the paper's claim that the theoretical bounds based on the spectral norm effectively predict the generalization performance of the model.", "section": "6 Experiments"}]