[{"figure_path": "ZNcJtNN3e8/tables/tables_1_1.jpg", "caption": "Table 2: Application of Lemma 2 to MLPs and GNNs. The detailed proof of the lemma applicability can be found in the Appendix E and the detailed description of the models in Appendix B. Here we provide brief description. We consider n-layer multilayer perceptron (MLP) with weights W1, ..., Wn. After layer i we apply Lip-Lipschitz activation function for i \u2208 [n \u2013 1]. Every input is contained in l2-ball of radius B. We consider n-layer GCN with weights W1,..., Wn. After layer i we apply Lip-Lipschitz activation function. Every node feature of the graph is contained in l2-ball of radius B and the maximum degree of the node is d \u2013 1. We denote Lip = Lip\u2081 \u00b7 ... \u00b7 Lipn\u22121. We consider n-layer (n > 2) MPGNN with weights W1, W2, W3 with activation functions g, \u0444, p with corresponding Lipschitz constants. We denote C = Lip Lip Lip ||W2||, X = ||W1||2||W3||2 and \u03be = ((dC)n\u22121\u22121)/(dc-1). Comparing to [33] we do not add Lip to \u0121 and instead of W\u012b we have W3.", "description": "This table shows how Lemma 2 can be applied to obtain generalization bounds for Multilayer Perceptrons (MLPs), Graph Convolutional Networks (GCNs), and Message-Passing Graph Neural Networks (MPGNNs).  It lists the parameters (T, Si, Ni, n, C1, C2) from Lemma 2 for each model type, demonstrating how the bounds depend on model architecture and parameters.", "section": "Section 4: Compositional PAC-Bayes"}, {"figure_path": "ZNcJtNN3e8/tables/tables_4_1.jpg", "caption": "Table 2: Application of Lemma 2 to MLPs and GNNs. The detailed proof of the lemma applicability can be found in the Appendix E and the detailed description of the models in Appendix B. Here we provide brief description. We consider n-layer multilayer perceptron (MLP) with weights W1, ..., Wn. After layer i we apply Lip-Lipschitz activation function for i \u2208 [n \u2013 1]. Every input is contained in l2-ball of radius B. We consider n-layer GCN with weights W1,..., Wn. After layer i we apply Lip-Lipschitz activation function. Every node feature of the graph is contained in l2-ball of radius B and the maximum degree of the node is d \u2013 1. We denote Lip = Lip\u2081 \u00b7 ... \u00b7 Lipn\u22121. We consider n-layer (n > 2) MPGNN with weights W1, W2, W3 with activation functions g, \u0444, p with corresponding Lipschitz constants. We denote C = Lip Lip Lip ||W2||, X = ||W1||2||W3||2 and \u03be = ((dC)n\u22121\u22121)/(dc-1). Comparing to [33] we do not add Lip to \u0121 and instead of W\u012b we have W3.", "description": "This table shows the application of Lemma 2, a core part of the proposed PAC-Bayes framework, to analyze the generalization performance of various neural network models: MLPs, GCNs, and MPGNNs.  It provides the key parameters and bounds derived from applying the Lemma.  The table summarizes how the theoretical bounds derived from the Lemma 2 correlate with the model's parameters for MLPs, GCNs and MPGNNs, highlighting the application of the framework to heterogeneous models. The table also includes references to relevant prior works.", "section": "Section 4: Compositional PAC-Bayes"}, {"figure_path": "ZNcJtNN3e8/tables/tables_5_1.jpg", "caption": "Table 2: Application of Lemma 2 to MLPs and GNNs. The detailed proof of the lemma applicability can be found in the Appendix E and the detailed description of the models in Appendix B. Here we provide brief description. We consider n-layer multilayer perceptron (MLP) with weights W1, ..., Wn. After layer i we apply Lip-Lipschitz activation function for i \u2208 [n \u2013 1]. Every input is contained in l2-ball of radius B. We consider n-layer GCN with weights W1,..., Wn. After layer i we apply Lip-Lipschitz activation function. Every node feature of the graph is contained in l2-ball of radius B and the maximum degree of the node is d \u2013 1. We denote Lip = Lip\u2081 \u00b7 ... \u00b7 Lipn\u22121. We consider n-layer (n > 2) MPGNN with weights W1, W2, W3 with activation functions g, \u03c6, p with corresponding Lipschitz constants. We denote C = Lip Lip Lip ||W2||, X = ||W1||2||W3||2 and \u03be = ((dC)n\u22121\u22121)/(dc-1). Comparing to [33] we do not add Lip to \u0121 and instead of W\u012b we have W3.", "description": "This table summarizes the application of Lemma 2 (a generalized PAC-Bayes recipe for heterogeneous models) to various neural network architectures, including Multilayer Perceptrons (MLPs), Graph Convolutional Networks (GCNs), and Message-Passing Graph Neural Networks (MPGNNs).  It shows how to derive the parameters (T, Si, Ni, n, C1, C2) required for Lemma 2, based on existing perturbation analyses found in the cited literature for these models. The table highlights the key differences and similarities in the parameter derivations for different architectures, especially noting the handling of Lipschitz constants and spectral norms. ", "section": "Section 4: Compositional PAC-Bayes"}, {"figure_path": "ZNcJtNN3e8/tables/tables_8_1.jpg", "caption": "Table 3: Comparison of PersLay with and without spectral norm regularization. We report accuracy statistics (except for MOLHIV, which uses AUROC) computed over five independent runs. In 5 out of 6 cases, the models using SpecNorm achieve better test results.", "description": "This table presents a comparison of the performance of PersLay models trained with and without spectral norm regularization.  The accuracy (or AUROC for MOLHIV) is reported for six different datasets, with the results averaged over five independent runs. The table demonstrates that spectral norm regularization generally improves the model's performance.", "section": "Regularizing PersLay"}, {"figure_path": "ZNcJtNN3e8/tables/tables_8_2.jpg", "caption": "Table 4: Test classification error (0-1 loss) and generalization gap (LD,0 \u2013 \u00ces,y) for PH-augmented GNNs. ERM means empirical risk minimizer (no regularization). We denote the best-performing methods in bold. In almost all cases, employing the method derived from our theoretical analysis leads to the smallest test errors and generalization gaps.", "description": "This table presents the results of an experiment comparing the test classification error and generalization gap of three different graph neural networks (GCN, SAGE, GIN) augmented with persistent homology, using both empirical risk minimization (ERM) and a spectral norm regularization method.  The spectral norm regularization, informed by the theoretical bounds derived in the paper, shows improvements in both test error and generalization gap across multiple datasets.", "section": "Regularizing GNNs with persistence"}, {"figure_path": "ZNcJtNN3e8/tables/tables_13_1.jpg", "caption": "Table 2: Application of Lemma 2 to MLPs and GNNs. The detailed proof of the lemma applicability can be found in the Appendix E and the detailed description of the models in Appendix B. Here we provide brief description. We consider n-layer multilayer perceptron (MLP) with weights W1, ..., Wn. After layer i we apply Lip-Lipschitz activation function for i \u2208 [n \u2013 1]. Every input is contained in l2-ball of radius B. We consider n-layer GCN with weights W1,..., Wn. After layer i we apply Lip-Lipschitz activation function. Every node feature of the graph is contained in l2-ball of radius B and the maximum degree of the node is d \u2013 1. We denote Lip = Lip\u2081 \u00b7 ... \u00b7 Lipn\u22121. We consider n-layer (n > 2) MPGNN with weights W1, W2, W3 with activation functions g, \u0444, p with corresponding Lipschitz constants. We denote C = Lip Lip Lip ||W2||, X = ||W1||2||W3||2 and \u03be = ((dC)n\u22121\u22121)/(dc-1). Comparing to [33] we do not add Lip to \u0121 and instead of W\u012b we have W3.", "description": "This table summarizes the application of Lemma 2 (a general recipe for obtaining PAC-Bayes bounds for heterogeneous models) to specific neural network architectures: MLPs, GCNs, and MPGNNs.  It provides the key parameters (T, Si, Ni, n) required for calculating the generalization bounds for each model based on the conditions described in Lemma 2. The table also references relevant previous work that supports the analysis.", "section": "3 Generalized PAC-Bayes"}, {"figure_path": "ZNcJtNN3e8/tables/tables_34_1.jpg", "caption": "Table 2: Application of Lemma 2 to MLPs and GNNs. The detailed proof of the lemma applicability can be found in the Appendix E and the detailed description of the models in Appendix B. Here we provide brief description. We consider n-layer multilayer perceptron (MLP) with weights W1, ..., Wn. After layer i we apply Lip-Lipschitz activation function for i \u2208 [n \u2013 1]. Every input is contained in l2-ball of radius B. We consider n-layer GCN with weights W1,..., Wn. After layer i we apply Lip-Lipschitz activation function. Every node feature of the graph is contained in l2-ball of radius B and the maximum degree of the node is d \u2013 1. We denote Lip = Lip\u2081 \u00b7 ... \u00b7 Lipn\u22121. We consider n-layer (n > 2) MPGNN with weights W1, W2, W3 with activation functions g, \u03c6, p with corresponding Lipschitz constants. We denote C = Lip Lip Lip ||W2||, X = ||W1||2||W3||2 and \u03be = ((dC)n\u22121\u22121)/(dc-1). Comparing to [33] we do not add Lip to \u0121 and instead of W\u012b we have W3.", "description": "This table presents the application of Lemma 2 (a generalized PAC-Bayes framework) to analyze the generalization of Multilayer Perceptrons (MLPs) and Graph Neural Networks (GNNs).  It shows how to derive generalization bounds for these models by defining specific parameters (T, Si, Ni, n, C1, C2) within the Lemma 2 framework. The table also compares the results with prior works ([40], [33]) highlighting that the new approach provides generalization bounds even for models with non-homogeneous layers and different activation functions.", "section": "Applying Lemma 2 to MLPs and GNNs"}, {"figure_path": "ZNcJtNN3e8/tables/tables_34_2.jpg", "caption": "Table 2: Application of Lemma 2 to MLPs and GNNs. The detailed proof of the lemma applicability can be found in the Appendix E and the detailed description of the models in Appendix B. Here we provide brief description. We consider n-layer multilayer perceptron (MLP) with weights W1, ..., Wn. After layer i we apply Lip-Lipschitz activation function for i \u2208 [n \u2013 1]. Every input is contained in l2-ball of radius B. We consider n-layer GCN with weights W1,..., Wn. After layer i we apply Lip-Lipschitz activation function. Every node feature of the graph is contained in l2-ball of radius B and the maximum degree of the node is d \u2013 1. We denote Lip = Lip\u2081 \u00b7 ... \u00b7 Lipn\u22121. We consider n-layer (n > 2) MPGNN with weights W1, W2, W3 with activation functions g, \u0444, p with corresponding Lipschitz constants. We denote C = Lip Lip Lip ||W2||, X = ||W1||2||W3||2 and \u03be = ((dC)n\u22121\u22121)/(dc-1). Comparing to [33] we do not add Lip to \u0121 and instead of W\u012b we have W3.", "description": "This table shows how Lemma 2, a general recipe for obtaining PAC-Bayes bounds for heterogeneous models, is applied to specific neural network architectures (MLPs, GCNs, and MPGNNs). It summarizes the parameters (T, Si, Ni) required for each model type to satisfy the conditions of Lemma 2, enabling the derivation of generalization bounds. The table also highlights differences in the required parameters based on the activation function and the type of GNN architecture used.", "section": "Applying Lemma 2 to MLPs and GNNs"}]