{"references": [{"fullname_first_author": "I. J. Goodfellow", "paper_title": "Explaining and harnessing adversarial examples", "publication_date": "2015-05-07", "reason": "This paper is foundational to the field of adversarial machine learning, introducing the concept and highlighting the vulnerability of neural networks to adversarial attacks."}, {"fullname_first_author": "A. Madry", "paper_title": "Towards deep learning models resistant to adversarial attacks", "publication_date": "2018-04-30", "reason": "This paper significantly advanced adversarial robustness research by introducing the MadryNet model and a more robust training method for defending against adversarial attacks."}, {"fullname_first_author": "C. Szegedy", "paper_title": "Intriguing properties of neural networks", "publication_date": "2014-04-14", "reason": "This influential work first revealed the vulnerability of neural networks to adversarial examples, motivating further research into understanding and mitigating this phenomenon."}, {"fullname_first_author": "N. Carlini", "paper_title": "Towards evaluating the robustness of neural networks", "publication_date": "2017-05-22", "reason": "This paper proposed a more robust evaluation methodology for assessing adversarial robustness, enabling a more comprehensive understanding of the strengths and weaknesses of defense mechanisms."}, {"fullname_first_author": "P. Dhariwal", "paper_title": "Diffusion models beat gans on image synthesis", "publication_date": "2021-12-06", "reason": "This work demonstrated the power of diffusion models in image generation, providing a strong foundation for the use of diffusion models in the generation of semantic-preserving adversarial examples."}]}