[{"figure_path": "wbE0QCBWji/figures/figures_1_1.jpg", "caption": "Figure 1: Adversarial examples generated by our method. Left: MNIST examples where we injected the subjective semantic understanding that scaling, translation, and distortion preserve digit meaning. The adversarial examples maintain digit interpretability while applying these transformations (see Figure 3 for comparison with other methods). Right: Adversarial example of a hamster image, leveraging semantic knowledge from pre-trained diffusion models. Despite substantial pixel modifications, the image remains natural-looking (see Figure 4 for comparison with other methods).", "description": "This figure showcases examples of adversarial examples generated by the proposed method.  The left side shows MNIST digits that have been modified while maintaining their semantic meaning (i.e., they are still recognizable as the same digit), even with transformations like scaling or rotation. The right side presents an example of a hamster image that has been substantially altered, but still retains its natural appearance and semantic content.", "section": "3 A Probabilistic Perspective on Adversarial Examples"}, {"figure_path": "wbE0QCBWji/figures/figures_3_1.jpg", "caption": "Figure 2: (a) and (b) display samples drawn from Pvic(\u00b7|Ytar) with the victim classifier being non-adversarially trained and adversarially trained, respectively. (c) showcases samples from Pdis(\u00b7|xori) when D is the square of L2 norm. (d) illustrates t(xori) for t ~ T, where T represents a distribution of transformations, including TPS (see Appendix D.1), scaling, rotation, and cropping. (e) Samples from Padv(\u00b7|xori, Ytar) x exp(-c1 D(xori, Xadv)) exp(-c2 f(xadv, Ytar)), where D is the L2 norm, f is the cross-entropy fCE, xori are the first 36 images from the MNIST test set, Ytar are set to 1, c\u2081 is 10-3, and c\u2082 is 10-2. A green border marks a successful attack, while red denotes failure.", "description": "This figure shows samples drawn from different distributions to illustrate the probabilistic perspective on adversarial examples.  (a) and (b) demonstrate samples from the victim distribution (Pvic) with a non-adversarially trained and adversarially trained classifier, respectively. (c) shows samples from the distance distribution (Pdis) using the L2 norm. (d) displays transformations applied to an original image, showcasing the semantic-preserving transformations used to define Pdis. Finally, (e) presents samples from the adversarial distribution (Padv), highlighting successful (green) and failed (red) adversarial examples.", "section": "The Victim Distribution"}, {"figure_path": "wbE0QCBWji/figures/figures_6_1.jpg", "caption": "Figure 3: Comparative visual analysis of PGD, Prob CW, StAdv, and our proposed method applied to MNIST. The surrogate classifier used is MadryNet with adversarial training. Images are framed with a green border to indicate a successful white-box attack, whereas a red border signifies a failed attack.", "description": "This figure compares the performance of four different methods (PGD, ProbCW, StAdv, and the proposed method) in generating adversarial examples on the MNIST dataset.  The comparison focuses on the visual appearance of the generated examples and their ability to successfully fool a MadryNet classifier (a classifier trained to be robust against adversarial attacks).  A green border around an image indicates a successful white-box attack (meaning the adversarial example fooled the classifier), while a red border means the attack failed. The figure visually demonstrates how the proposed method produces adversarial examples that maintain more of the original image's semantic information than other techniques, making them more difficult to distinguish from real examples.", "section": "6 Experiments"}, {"figure_path": "wbE0QCBWji/figures/figures_7_1.jpg", "caption": "Figure 4: Comparative visual analysis of NCF, cAdv, ACE, ColorFool and our proposed method applied to Imagenet. The surrogate classifier used is ResNet50. For additional examples, refer to Appendix I.", "description": "This figure compares the visual results of several adversarial example generation methods on the ImageNet dataset.  The methods compared include NCF, cAdv, ACE, ColorFool, and the authors' proposed method.  The goal is to show how each method modifies an original image to create an adversarial example, and how these modifications differ visually.  ResNet50 is used as the victim classifier.  The figure highlights the relatively natural-looking results of the authors' method compared to other state-of-the-art methods.", "section": "Experiments"}, {"figure_path": "wbE0QCBWji/figures/figures_8_1.jpg", "caption": "Figure 5: Average attack success rate across the blackbox transferability and defence methods v.s. human annotation success rate illustrated in Table 2.", "description": "This figure visualizes the trade-off between the attack success rate and the human's ability to detect the adversarial examples.  It compares different methods (NCF, cAdv, ACE, ColorFool, and the proposed method with varying parameter 'c') on their average attack success rates and the corresponding human annotation success rates (how often humans fail to recognize the adversarial example). This demonstrates that higher attack success rates often correlate with greater difficulty for humans to distinguish the original image from its adversarial counterpart.", "section": "6.2 Imagenet"}, {"figure_path": "wbE0QCBWji/figures/figures_16_1.jpg", "caption": "Figure 6: TPS as a data augmentation. Left: The original image xori superimposed with a 5 \u00d7 5 grid of source control points Psou. Right: The transformed image overlaid with a grid of target control points Ptar.", "description": "This figure illustrates the Thin Plate Splines (TPS) deformation used for data augmentation.  The left panel shows the original image (xori) with a 5x5 grid of control points (Psou) overlaid.  The right panel displays the transformed image after applying TPS, where the original control points have been mapped to new target points (Ptar).  The TPS transformation creates a smooth deformation that minimally alters the image's overall structure while introducing localized changes, which helps maintain semantic meaning during augmentation.", "section": "D Practical Techniques"}, {"figure_path": "wbE0QCBWji/figures/figures_18_1.jpg", "caption": "Figure 7: Annotator Interface for image annotation.", "description": "This figure shows the interface used by human annotators in a user study to identify digits in images. The interface displays an image and a set of labels (0-9 and N/A).  Annotators had 10 seconds to select the label that best matched the digit shown, even if the digit was blurry, distorted, or artificial. The design of the interface aimed to make the assessment of image interpretability more robust.", "section": "E Annotator Interface"}, {"figure_path": "wbE0QCBWji/figures/figures_18_2.jpg", "caption": "Figure 8: Annotator Interface for comparison.", "description": "This figure shows the interface used in a user study for comparing image pairs.  The annotators were asked to determine which image appeared authentic and which showed signs of computer manipulation.  If the images looked similar, they were instructed to select the image that seemed more authentic.  They were given 10 seconds to make a judgment.", "section": "E Annotator Interface"}, {"figure_path": "wbE0QCBWji/figures/figures_20_1.jpg", "caption": "Figure 9: Comparative visual analysis of NCF, cAdv, ACE, ColorFool and our proposed method applied to Imagenet. The surrogate classifier used is ResNet50.", "description": "This figure compares the visual results of several semantics-aware adversarial attacks methods, including NCF, cAdv, ACE, ColorFool, and the proposed method from the paper.  The goal is to show how each method modifies the original image (shown in the \"Origin\" column) while aiming to deceive a ResNet50 classifier. The figure highlights that the proposed method produces adversarial examples that maintain more of the original image's visual integrity compared to the other methods, which often introduce more significant and unnatural changes to the image's appearance.", "section": "I Qualitative Result (Continued)"}]