[{"figure_path": "wbE0QCBWji/tables/tables_6_1.jpg", "caption": "Table 1: Success rate (%) of the methods on MNIST.", "description": "This table presents the success rates of various adversarial attack methods on the MNIST dataset.  The methods compared are Projected Gradient Descent (PGD), Probabilistic Certified Adversarial examples (ProbCW), Strong Adversarial examples (stAdv), and the proposed method from the paper.  Success rates are shown for several scenarios: human annotation (to assess the perceptibility of the adversarial examples), white-box attacks against the MadryNet classifier (both adversarially and non-adversarially trained), transferability to other classifiers (ResNet, both adversarially and non-adversarially trained), and attacks against certified defenses.  The proposed method demonstrates higher success rates in all scenarios.", "section": "6.1 MNIST"}, {"figure_path": "wbE0QCBWji/tables/tables_7_1.jpg", "caption": "Table 2: Success rate (%) of the methods on Imagenet.", "description": "This table presents the quantitative results of several adversarial attack methods on the ImageNet dataset.  It shows the success rates of each method under white-box conditions (using ResNet50 as the victim classifier), as well as their transferability to other classifiers (VGG19, ResNet152, DenseNet161, Inception V3, EfficientNet B7) and their resilience against adversarial defenses (adversarially trained Inception V3, EfficientNet B7, and an ensemble of InceptionResNet V2).  The table also includes human annotation results, indicating the percentage of times human annotators failed to distinguish the original image from the adversarial examples generated by each method. This provides insights into the imperceptibility of the generated adversarial examples.", "section": "6.2 Imagenet"}]