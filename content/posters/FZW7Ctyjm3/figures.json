[{"figure_path": "FZW7Ctyjm3/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Accuracy improvement of our method, STIC, compared to the original LLaVA-v1.6 (Liu et al., 2024) on seven benchmarks. Right: Response examples from the original LLaVA-v1.6 and STIC (LLaVA-v1.6), which enhances image comprehension and subsequent reasoning capabilities.", "description": "The figure on the left shows a bar chart comparing the accuracy improvement achieved by the proposed STIC method against the baseline LLaVA-v1.6 model across seven different vision-language benchmarks.  The figure on the right provides example queries and responses from both the baseline LLaVA-v1.6 and the STIC-enhanced LLaVA-v1.6 model, highlighting the improved image comprehension and reasoning capabilities of the STIC method.  The example demonstrates that STIC produces a more accurate answer to a question about the price and quantity of gasoline, as seen on a gas station sign.", "section": "Main Results"}, {"figure_path": "FZW7Ctyjm3/figures/figures_2_1.jpg", "caption": "Figure 2: Framework overview of STIC, a two-stage self-training algorithm focusing on the image comprehension capability of the LVLMs. In Stage 1, the base LVLM self-constructs its preference dataset for image description using well-designed prompts, poorly-designed prompts, and distorted images. In Stage 2, a small portion of the previously used SFT data is recycled and infused with model-generated image descriptions to further fine-tune the base LVLM.", "description": "This figure illustrates the two-stage self-training process of STIC. Stage 1 focuses on building a preference dataset for image descriptions by using a base large vision language model (LVLM) to generate preferred and dispreferred responses. Preferred responses are generated using detailed step-by-step prompts, while dispreferred responses come from either bad prompts or corrupted images.  Stage 2 then leverages the created dataset, plus a small portion of existing instruction-tuning data combined with the self-generated descriptions to further fine-tune the LVLM and improve its reasoning capabilities.", "section": "4 Our Method: STIC"}, {"figure_path": "FZW7Ctyjm3/figures/figures_3_1.jpg", "caption": "Figure 3: Examples of the self-constructed preference data in STIC.", "description": "This figure shows three examples of image descriptions generated by the model. The first example is a preferred response generated using a well-crafted, step-by-step prompt that guides the model to provide a detailed and accurate description of the image. The second and third examples show dispreferred responses generated using either a bad prompt (that encourages the model to imagine objects not present in the image) or an image corrupted with color jitter or lower resolution.  These examples highlight the different qualities of data used to train the model in STIC, which helps the model learn to distinguish between preferred and dispreferred responses. The variation in description quality helps the model learn better image comprehension and reasoning.", "section": "4 Our Method: STIC"}, {"figure_path": "FZW7Ctyjm3/figures/figures_7_1.jpg", "caption": "Figure 4: Accuracy improvement of STIC compared to the base LLaVA-v1.6 model across different tasks in Left: MMBench, where the original performances are re-scaled to 60 in plotting and STIC accordingly with the same coefficient for each task. Middle: MM-Vet, where the performances of the original model are re-scaled to 40 and STIC accordingly. Right: LLaVA-Bench, where we report the error bars over three independent runs due to the randomness of GPT-4 evaluation.", "description": "This figure presents a comparison of the performance improvements achieved by the Self-Training on Image Comprehension (STIC) method compared to the original LLaVA-v1.6 model across three different vision-language reasoning benchmarks: MMBench, MM-Vet, and LLaVA-Bench.  The left panel (MMBench) and middle panel (MM-Vet) show radar charts illustrating performance gains for various sub-tasks within each benchmark.  The right panel (LLaVA-Bench) presents a bar chart summarizing the average performance improvement across multiple tasks, with error bars reflecting the variability introduced by the use of GPT-4 for evaluation.", "section": "Main Results"}, {"figure_path": "FZW7Ctyjm3/figures/figures_7_2.jpg", "caption": "Figure 5: Progression of stages in STIC.", "description": "This figure shows the performance improvement of STIC on ScienceQA across its different stages.  The baseline performance is shown first, followed by the performance after Stage 1 (image comprehension self-training), Stage 2 (description-infused fine-tuning), and finally the combined effect with the describe-and-respond (DaR) prompting method.  The graph clearly illustrates the incremental improvement achieved by each stage, demonstrating the synergistic effects of the different components of the STIC approach.", "section": "Ablation Studies and Discussions"}, {"figure_path": "FZW7Ctyjm3/figures/figures_8_1.jpg", "caption": "Figure 7: t-SNE visualization of images from MSCOCO and four benchmarks, each sampling 1k.", "description": "This figure visualizes the similarity of image distributions between MSCOCO (the dataset used for self-training in STIC) and four different benchmarks: ScienceQA, TextVQA, MathVista, and ChartQA.  Each point represents an image, and the proximity of points indicates similarity in visual features. The figure aims to show the correlation between the image distribution overlap and the performance gains observed by STIC on each benchmark. A larger overlap suggests a stronger positive impact from STIC's self-training.", "section": "Ablation Studies and Discussions"}, {"figure_path": "FZW7Ctyjm3/figures/figures_8_2.jpg", "caption": "Figure 6: Scaling law in STIC.", "description": "This figure shows the scaling law of the STIC method.  The x-axis represents the amount of preference data used in Stage 1 of the STIC algorithm (6k, 12k, and 30k images from MSCOCO). The y-axis represents the accuracy improvement (%) achieved by STIC on the LLaVA-Bench benchmark. The graph shows that increasing the amount of preference data leads to a consistent and significant increase in performance improvement, demonstrating that STIC can effectively leverage large quantities of unlabeled data.", "section": "5.2 Main Results"}, {"figure_path": "FZW7Ctyjm3/figures/figures_9_1.jpg", "caption": "Figure 1: Left: Accuracy improvement of our method, STIC, compared to the original LLaVA-v1.6 (Liu et al., 2024) on seven benchmarks. Right: Response examples from the original LLaVA-v1.6 and STIC (LLaVA-v1.6), which enhances image comprehension and subsequent reasoning capabilities.", "description": "The left part of the figure shows a bar chart comparing the accuracy improvement achieved by the proposed STIC method against the original LLaVA-v1.6 model across seven different vision-language benchmarks.  The right part presents a comparison of the model responses to a sample query, highlighting the improved image comprehension and reasoning capabilities of STIC compared to the original LLaVA-v1.6 model.", "section": "5 Main Results"}, {"figure_path": "FZW7Ctyjm3/figures/figures_17_1.jpg", "caption": "Figure 9: Example of generated preference data, where the dis-preferred response is generated from bad prompting.", "description": "This figure shows three examples of image descriptions generated by the model. The first is a preferred description, generated using a detailed, step-by-step prompt. The second and third are dispreferred descriptions, generated using either a poorly-worded prompt or a corrupted image.  These examples illustrate how the model learns to distinguish between preferred and dispreferred descriptions through self-training.", "section": "5.2 Main Results"}, {"figure_path": "FZW7Ctyjm3/figures/figures_17_2.jpg", "caption": "Figure 10: Example of generated preference data, where the dis-preferred response is generated from images with lower resolution.", "description": "This figure shows examples from the self-constructed preference dataset used in the STIC method.  The preferred response is a detailed description of a child blowing out candles on a birthday cake. The dispreferred response describes the same image but with significantly less detail and clarity due to lower image resolution.  The comparison highlights how the self-training method uses the differences between preferred and dispreferred responses to improve model performance.", "section": "4 Our Method: STIC"}, {"figure_path": "FZW7Ctyjm3/figures/figures_17_3.jpg", "caption": "Figure 3: Examples of the self-constructed preference data in STIC.", "description": "This figure shows three examples of image descriptions generated by the model using different prompting strategies. The first example uses a detailed, step-by-step prompt to generate a high-quality description. The second and third examples use either a corrupted image or a misleading prompt to generate lower-quality descriptions. These descriptions are used to construct a preference dataset for image descriptions, which is then used to fine-tune the model.", "section": "4 Our Method: STIC"}, {"figure_path": "FZW7Ctyjm3/figures/figures_18_1.jpg", "caption": "Figure 1: Left: Accuracy improvement of our method, STIC, compared to the original LLaVA-v1.6 (Liu et al., 2024) on seven benchmarks. Right: Response examples from the original LLaVA-v1.6 and STIC (LLaVA-v1.6), which enhances image comprehension and subsequent reasoning capabilities.", "description": "The figure demonstrates the effectiveness of the proposed STIC method. The left panel shows a bar chart illustrating the accuracy improvements achieved by STIC across seven different vision-language benchmarks compared to the original LLaVA-v1.6 model.  The right panel presents example outputs from both the original LLaVA-v1.6 and the STIC-enhanced LLaVA-v1.6, highlighting the improved image comprehension and reasoning capabilities of the latter.", "section": "5 Main Results"}, {"figure_path": "FZW7Ctyjm3/figures/figures_19_1.jpg", "caption": "Figure 1: Left: Accuracy improvement of our method, STIC, compared to the original LLaVA-v1.6 (Liu et al., 2024) on seven benchmarks. Right: Response examples from the original LLaVA-v1.6 and STIC (LLaVA-v1.6), which enhances image comprehension and subsequent reasoning capabilities.", "description": "The figure on the left shows a bar chart comparing the accuracy improvement achieved by STIC against the original LLaVA-v1.6 model across seven different benchmarks.  The figure on the right provides a qualitative comparison, showcasing how STIC enhances the model's ability to comprehend images and reason effectively, leading to more accurate and contextually relevant responses compared to the original model.", "section": "5 Main Results"}, {"figure_path": "FZW7Ctyjm3/figures/figures_19_2.jpg", "caption": "Figure 15: Data comparison.", "description": "This figure is a bar chart comparing the amount of supervised fine-tuning (SFT) data and unlabeled data used in the POVID and STIC methods.  It shows that POVID used significantly more SFT data and no unlabeled data, whereas STIC used a smaller amount of SFT data and a substantial amount of unlabeled data for self-training. This visualization highlights the data efficiency of the STIC approach.", "section": "C.3 Discussion with POVID"}, {"figure_path": "FZW7Ctyjm3/figures/figures_20_1.jpg", "caption": "Figure 1: Left: Accuracy improvement of our method, STIC, compared to the original LLaVA-v1.6 (Liu et al., 2024) on seven benchmarks. Right: Response examples from the original LLaVA-v1.6 and STIC (LLaVA-v1.6), which enhances image comprehension and subsequent reasoning capabilities.", "description": "The left panel of Figure 1 shows a bar chart comparing the accuracy improvements achieved by STIC against the original LLaVA-v1.6 model across seven different vision-language benchmarks.  STIC demonstrates substantial performance gains across all benchmarks, with an average improvement of 4.0%. The right panel provides a qualitative comparison, showcasing example responses from both the original LLaVA-v1.6 and the STIC-enhanced version for a given query.  The examples highlight STIC's improved ability to comprehend image content and perform subsequent reasoning tasks.", "section": "5 Main Results"}]