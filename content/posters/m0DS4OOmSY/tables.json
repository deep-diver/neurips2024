[{"figure_path": "m0DS4OOmSY/tables/tables_21_1.jpg", "caption": "Table 1: Evaluation results of GPT2-XL. experiments are conducted on a sever with 8 RTX 4090 GPUs.", "description": "This table presents the results of experiments conducted using the GPT2-XL language model.  The table shows the performance of different model editing methods (PMET, MEND, KN, MEMIT) across different numbers of edits (10, 30, 100, 500, 1000) on four benchmark tasks (MMLU, GSM8K, BBH, CSQA).  The 'w/o Edit' row shows baseline performance with no editing applied.  A value of 0 indicates complete failure after editing.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/tables/tables_22_1.jpg", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "This table presents the results of evaluating different model editing methods on the general abilities of base language models (not instruction-tuned).  It shows the performance (MMLU, GSM8K, BBH, CSQA) of Llama2-7B and Mistral-7B models after applying various editing methods with different numbers of edits.  The higher score indicates better performance. Note that MEND and GRACE methods were not tested on Mistral-7B.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/tables/tables_23_1.jpg", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "This table presents the results of experiments evaluating the impact of different model editing methods and varying numbers of edits on the general capabilities of base language models (not instruction-tuned).  The experiments were conducted using the COUNTERFACT dataset, ensuring consistent conditions across different editing approaches.  The results are scored across four different benchmark tasks (MMLU, GSM8K, BBH, CSQA), with higher scores representing better performance.", "section": "C Detailed Experimental Results"}, {"figure_path": "m0DS4OOmSY/tables/tables_24_1.jpg", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "This table presents the results of evaluating the impact of different model editing methods and varying numbers of edits on the general capabilities of several base language models.  The models were evaluated on four tasks (MMLU, GSM8K, BBH, CSQA) using the COUNTERFACT dataset. The table shows the performance (higher scores are better) for each method across different numbers of edits.  Note that MEND and GRACE methods were not used for the Mistral-7B model.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/tables/tables_25_1.jpg", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "This table presents the results of an experiment evaluating the impact of different model editing methods and varying numbers of edits on the general capabilities of base language models.  Four different evaluation tasks (MMLU, GSM8K, BBH, CSQA) were used to assess the models' performance, and the results are shown for different editing methods (ROME, MEMIT, PMET, GRACE, MEND, KN) and numbers of edits.  The table highlights that some methods are more robust to editing than others and that performance generally degrades as the number of edits increases. MEND and GRACE methods were not applicable to the Mistral-7B model.", "section": "C Detailed Experimental Results"}, {"figure_path": "m0DS4OOmSY/tables/tables_26_1.jpg", "caption": "Table 6: The statistics of the datasets used in this paper. # Ex. are the number of few-shot chain-of-thought exemplars used to prompt each task in evaluation. # TEST denote the number of training data and test data, respectively. *: CSQA do not have publicly available test set labels, so we simply follow the setting by [80; 78] to evaluate the performance of the development set.", "description": "This table presents the details of the seven datasets used in the paper's evaluation benchmark.  It shows the task type for each dataset, the number of few-shot examples used for prompting, the number of test examples, the metric used for evaluation (Accuracy or Exact Match), and the evaluation method employed (Generation-Based or Sequence-Based).  The table also notes that the CSQA dataset lacks publicly available test set labels, and the evaluation followed the setting described in references [80] and [78].", "section": "D Evaluation Benchmark"}, {"figure_path": "m0DS4OOmSY/tables/tables_26_2.jpg", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "This table presents the results of evaluating the impact of various model editing methods on the general capabilities of base language models (not instruction-tuned).  It shows the performance (MMLU, GSM8K, BBH, CSQA) of Llama2-7B and Mistral-7B models after applying different editing methods (ROME, MEMIT, PMET, GRACE, MEND, KN) with varying numbers of edits (1, 5, 10, 20, 50, 100, 500, 1000).  Higher scores indicate better performance.  Note that MEND and GRACE methods were not evaluated for the Mistral-7B model.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/tables/tables_27_1.jpg", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "This table shows the performance of different language models (Llama2-7B and Mistral-7B) after applying different editing methods with varying numbers of edits.  The performance is measured across four different tasks (MMLU, GSM8K, BBH, CSQA).  The results help to understand how different editing methods affect the models' general abilities and how the number of edits impacts performance.", "section": "C.1 RQ1: Impact of the Number of Edits"}]