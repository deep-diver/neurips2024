[{"figure_path": "VazkRbCGxt/figures/figures_1_1.jpg", "caption": "Figure 1: Overview. (a) Direct Consistency Optimization (DCO) pushes the Pareto frontier between prompt fidelity and subject fidelity towards upper-right over DreamBooth [10], and with prior preservation loss (DreamBooth+p.p.). DCO improves generating custom subject with various visual attributes (e.g., astronaut outfits and background of Mars), or various styles that pretrained model knows (e.g., flat cartoon illustration style). (b) The customized subject and style models fine-tuned with DCO can be merged as is, allowing us to generate my subject in my style [11].", "description": "This figure demonstrates the effectiveness of Direct Consistency Optimization (DCO) for text-to-image (T2I) model customization.  Panel (a) shows that DCO outperforms DreamBooth and DreamBooth with prior preservation in generating images with high subject fidelity and prompt fidelity. It achieves this by improving the balance between consistency and image-text alignment. Panel (b) showcases the key advantage of DCO: fine-tuned subject and style models can be directly merged without interference, enabling the generation of images containing a custom subject in a custom style.", "section": "1 Introduction"}, {"figure_path": "VazkRbCGxt/figures/figures_3_1.jpg", "caption": "Figure 1: Overview. (a) Direct Consistency Optimization (DCO) pushes the Pareto frontier between prompt fidelity and subject fidelity towards upper-right over DreamBooth [10], and with prior preservation loss (DreamBooth+p.p.). DCO improves generating custom subject with various visual attributes (e.g., astronaut outfits and background of Mars), or various styles that pretrained model knows (e.g., flat cartoon illustration style). (b) The customized subject and style models fine-tuned with DCO can be merged as is, allowing us to generate my subject in my style [11].", "description": "This figure shows a comparison of different methods for customizing text-to-image diffusion models.  (a) illustrates how Direct Consistency Optimization (DCO) improves upon existing methods (DreamBooth and DreamBooth with prior preservation) in terms of generating images with high fidelity to both the prompt and the subject, achieving a superior balance between these two aspects. (b) demonstrates the advantage of DCO in allowing for seamless merging of separately customized subject and style models, enabling the generation of images featuring a specific subject in a specific style.", "section": "1 Introduction"}, {"figure_path": "VazkRbCGxt/figures/figures_5_1.jpg", "caption": "Figure 1: Overview. (a) Direct Consistency Optimization (DCO) pushes the Pareto frontier between prompt fidelity and subject fidelity towards upper-right over DreamBooth [10], and with prior preservation loss (DreamBooth+p.p.). DCO improves generating custom subject with various visual attributes (e.g., astronaut outfits and background of Mars), or various styles that pretrained model knows (e.g., flat cartoon illustration style). (b) The customized subject and style models fine-tuned with DCO can be merged as is, allowing us to generate my subject in my style [11].", "description": "This figure demonstrates the effectiveness of Direct Consistency Optimization (DCO) in comparison to other methods (DreamBooth, DreamBooth with prior preservation).  Panel (a) shows DCO achieving higher prompt and subject fidelity. Panel (b) illustrates the ability of DCO to merge independently customized subject and style models for generating images combining both attributes.", "section": "1 Introduction"}, {"figure_path": "VazkRbCGxt/figures/figures_6_1.jpg", "caption": "Figure 3: Custom subject generation. We show selected generations from DreamBooth (DB), DB with prior preservation (DB+p.p.), and ours (DCO) of custom subjects with varying attributes and styles guided by text prompts. While DB captures subjects well, it does not follow text prompt well. DB+p.p. shows better textual alignment, but falls short in subject fidelity. Ours show the best in both image-text alignment and subject fidelity. Best viewed in color, zoomed in on monitor.", "description": "This figure compares the results of generating images of custom subjects with varying attributes and styles using three different methods: DreamBooth, DreamBooth with prior preservation, and the proposed DCO method.  The results demonstrate that while DreamBooth successfully generates images of the subject, it does not always align with the text prompt. Adding prior preservation improves text alignment but reduces subject fidelity. The DCO method is shown to achieve the best results, balancing both image-text alignment and subject fidelity.", "section": "5.1 Subject Personalization"}, {"figure_path": "VazkRbCGxt/figures/figures_6_2.jpg", "caption": "Figure 3: Custom subject generation. We show selected generations from DreamBooth (DB), DB with prior preservation (DB+p.p.), and ours (DCO) of custom subjects with varying attributes and styles guided by text prompts. While DB captures subjects well, it does not follow text prompt well. DB+p.p. shows better textual alignment, but falls short in subject fidelity. Ours show the best in both image-text alignment and subject fidelity. Best viewed in color, zoomed in on monitor.", "description": "This figure compares the performance of three different methods for generating images of custom subjects: DreamBooth, DreamBooth with prior preservation, and the proposed Direct Consistency Optimization (DCO) method. The results show that DCO achieves the best balance between subject fidelity (how well the generated image matches the reference image) and prompt fidelity (how well the generated image matches the text prompt).", "section": "5.1 Subject Personalization"}, {"figure_path": "VazkRbCGxt/figures/figures_7_1.jpg", "caption": "Figure 5: Quantitative results. We plot Pareto curve between subject / style fidelity (image similarity) and prompt fidelity (image-text similarity) on (a) subject personalization and (b) style personalization tasks. Scores of each point are measured with consistency guidance sampling (dots and lines) of wcon = 2.0, 3.0, 4.0, 5.0, and conventional classifier-free guidance sampling (diamond). See Sec. 5.1 and Sec. 5.2 for experimental details, and Appendix B.1 for full comparison.", "description": "This figure shows the Pareto frontier curves for both subject and style personalization tasks.  The x-axis represents image-text similarity, and the y-axis represents image similarity.  Each point on the curve represents a trade-off between these two metrics, obtained using different consistency guidance sampling values (wcon).  The figure demonstrates that the proposed DCO method outperforms the baselines (DreamBooth and DreamBooth with prior preservation) by achieving a superior balance between subject/style fidelity and image-text alignment.  Appendix B.1 provides further details and a more complete comparison.", "section": "Quantitative results"}, {"figure_path": "VazkRbCGxt/figures/figures_8_1.jpg", "caption": "Figure 1: Overview. (a) Direct Consistency Optimization (DCO) pushes the Pareto frontier between prompt fidelity and subject fidelity towards upper-right over DreamBooth [10], and with prior preservation loss (DreamBooth+p.p.). DCO improves generating custom subject with various visual attributes (e.g., astronaut outfits and background of Mars), or various styles that pretrained model knows (e.g., flat cartoon illustration style). (b) The customized subject and style models fine-tuned with DCO can be merged as is, allowing us to generate my subject in my style [11].", "description": "This figure demonstrates the effectiveness of Direct Consistency Optimization (DCO) in comparison to other methods, such as DreamBooth and DreamBooth with prior preservation loss. Part (a) shows DCO's improvement in generating images with high prompt fidelity and subject fidelity.  Part (b) highlights DCO's ability to merge customized subject and style models for generating images with a specific subject in a particular style.", "section": "1 Introduction"}, {"figure_path": "VazkRbCGxt/figures/figures_9_1.jpg", "caption": "Figure 5: Quantitative results. We plot Pareto curve between subject / style fidelity (image similarity) and prompt fidelity (image-text similarity) on (a) subject personalization and (b) style personalization tasks. Scores of each point are measured with consistency guidance sampling (dots and lines) of wcon = 2.0, 3.0, 4.0, 5.0, and conventional classifier-free guidance sampling (diamond). See Sec. 5.1 and Sec. 5.2 for experimental details, and Appendix B.1 for full comparison.", "description": "This figure shows the Pareto curves comparing image similarity (representing subject/style fidelity) against image-text similarity (representing prompt fidelity) for both subject and style personalization tasks.  The curves illustrate the trade-off between these two metrics; improving one often comes at the expense of the other.  Different sampling methods (consistency guidance and classifier-free guidance) are compared, showing how the choice of sampling affects the balance between fidelity to the prompt and fidelity to the subject or style.  The results indicate that the proposed method (DCO) achieves a better balance than previous baselines.", "section": "Quantitative results"}, {"figure_path": "VazkRbCGxt/figures/figures_15_1.jpg", "caption": "Figure 5: Quantitative results. We plot Pareto curve between subject / style fidelity (image similarity) and prompt fidelity (image-text similarity) on (a) subject personalization and (b) style personalization tasks. Scores of each point are measured with consistency guidance sampling (dots and lines) of wcon = 2.0, 3.0, 4.0, 5.0, and conventional classifier-free guidance sampling (diamond). See Sec. 5.1 and Sec. 5.2 for experimental details, and Appendix B.1 for full comparison.", "description": "This figure shows the Pareto frontier curves for both subject and style personalization tasks.  The x-axis represents image-text similarity, and the y-axis represents image similarity (fidelity to the subject or style). Each curve represents a different method (DCO, DreamBooth, DreamBooth with prior preservation), and the points on each curve represent different levels of consistency guidance during sampling.  The figure demonstrates that DCO achieves a superior balance between image-text alignment and subject/style consistency compared to the baseline methods.", "section": "5 Experiments"}, {"figure_path": "VazkRbCGxt/figures/figures_16_1.jpg", "caption": "Figure 5: Quantitative results. We plot Pareto curve between subject / style fidelity (image similarity) and prompt fidelity (image-text similarity) on (a) subject personalization and (b) style personalization tasks. Scores of each point are measured with consistency guidance sampling (dots and lines) of wcon = 2.0, 3.0, 4.0, 5.0, and conventional classifier-free guidance sampling (diamond). See Sec. 5.1 and Sec. 5.2 for experimental details, and Appendix B.1 for full comparison.", "description": "This figure presents Pareto curves showing the trade-off between image similarity (subject/style fidelity) and image-text similarity (prompt fidelity) for both subject and style personalization tasks.  The curves illustrate the performance of Direct Consistency Optimization (DCO) compared to baseline methods (DreamBooth, DreamBooth with prior preservation). Different sampling methods (consistency guidance with varying wcon values and conventional classifier-free guidance) are also shown. The results demonstrate DCO's superior performance, achieving higher fidelity in both image-text alignment and subject/style consistency.", "section": "5 Experiments"}, {"figure_path": "VazkRbCGxt/figures/figures_17_1.jpg", "caption": "Figure 10: Comparison on the alignment of DreamBooth and DCO fine-tuned subject and style LoRAs. We compute the average cosine similarity of column layers between subject and style LoRAs fine-tuned with each DreamBooth (DB) [10] and our method (DCO). The x-axis denotes the component of each U-Net layer. The cosine similarity measures the alignment between two LoRAs, and high cosine similarity values are considered as the interference between them. Interestingly, we find that there is no obvious difference in the cosine similarity values between models trained with DCO and DB methods, while DCO fine-tuned models can be successfully combined with arithmetic merge to generate images of my subject in my style (e.g., see Fig. 6 and Fig. 19). This may be in contrast with the findings of recent works [21, 58] and suggests further investigation on method to measure the compatibility between LoRA models.", "description": "This figure compares the alignment of subject and style LoRAs fine-tuned using DreamBooth (DB) and Direct Consistency Optimization (DCO).  It measures the average cosine similarity between the columns of the LoRA layers in the U-Net. High cosine similarity suggests interference between LoRAs. The figure shows that there's no significant difference in cosine similarity between the DB and DCO methods, implying that DCO effectively merges LoRAs while retaining model compatibility, unlike findings in some other works.", "section": "Implementation and Analysis"}, {"figure_path": "VazkRbCGxt/figures/figures_19_1.jpg", "caption": "Figure 1: Overview. (a) Direct Consistency Optimization (DCO) pushes the Pareto frontier between prompt fidelity and subject fidelity towards upper-right over DreamBooth [10], and with prior preservation loss (DreamBooth+p.p.). DCO improves generating custom subject with various visual attributes (e.g., astronaut outfits and background of Mars), or various styles that pretrained model knows (e.g., flat cartoon illustration style). (b) The customized subject and style models fine-tuned with DCO can be merged as is, allowing us to generate my subject in my style [11].", "description": "This figure shows a comparison of different methods for customizing text-to-image diffusion models.  (a) illustrates the improved performance of Direct Consistency Optimization (DCO) compared to DreamBooth and DreamBooth with prior preservation, showing better trade-off between image-text alignment and subject consistency. (b) demonstrates the advantage of DCO in merging separately customized subject and style models, enabling generation of images with specific subjects in desired styles.", "section": "1 Introduction"}, {"figure_path": "VazkRbCGxt/figures/figures_19_2.jpg", "caption": "Figure 1: Overview. (a) Direct Consistency Optimization (DCO) pushes the Pareto frontier between prompt fidelity and subject fidelity towards upper-right over DreamBooth [10], and with prior preservation loss (DreamBooth+p.p.). DCO improves generating custom subject with various visual attributes (e.g., astronaut outfits and background of Mars), or various styles that pretrained model knows (e.g., flat cartoon illustration style). (b) The customized subject and style models fine-tuned with DCO can be merged as is, allowing us to generate my subject in my style [11].", "description": "This figure demonstrates the advantages of Direct Consistency Optimization (DCO) over other methods for text-to-image generation.  Panel (a) shows that DCO improves the balance between generating images that match the prompt (prompt fidelity) and those that accurately depict the specified subject (subject fidelity) compared to DreamBooth and DreamBooth with prior preservation.  Panel (b) highlights DCO's ability to merge independently customized models (subject and style) to generate images with the desired subject and style.", "section": "1 Introduction"}, {"figure_path": "VazkRbCGxt/figures/figures_20_1.jpg", "caption": "Figure 3: Custom subject generation. We show selected generations from DreamBooth (DB), DB with prior preservation (DB+p.p.), and ours (DCO) of custom subjects with varying attributes and styles guided by text prompts. While DB captures subjects well, it does not follow text prompt well. DB+p.p. shows better textual alignment, but falls short in subject fidelity. Ours show the best in both image-text alignment and subject fidelity. Best viewed in color, zoomed in on monitor.", "description": "This figure compares the results of subject generation using three different methods: DreamBooth, DreamBooth with prior preservation, and the authors' proposed method (DCO).  It demonstrates that DCO achieves better image-text alignment and subject fidelity than the other methods.  The results show that while DreamBooth captures subjects well, it struggles with accurate text prompt following. Adding prior preservation improves textual alignment but sacrifices subject fidelity. The DCO approach excels in both aspects, producing images that closely match the text prompts while maintaining high fidelity to the original subject.", "section": "5.1 Subject Personalization"}, {"figure_path": "VazkRbCGxt/figures/figures_22_1.jpg", "caption": "Figure 14: Effect of consistency guidance scale. We show the effect of consistency guidance scale wcon by varying from 2.0 to 5.0. We also show the synthesized results using CFG. Note that the optimal choice of consistency guidance scale (in consideration of user\u2019s preference) might varies among reference dataset, or even input prompts.", "description": "This figure shows how changing the consistency guidance scale (wcon) affects image generation in a text-to-image model.  Different values of wcon produce images with varying degrees of fidelity to the reference images and textual consistency. This highlights the trade-off between prompt fidelity and subject consistency that can be controlled during inference.  The example uses three different subjects (a monster toy, a monster toy, and a sloth).", "section": "4.3 Consistency Guidance Sampling"}, {"figure_path": "VazkRbCGxt/figures/figures_23_1.jpg", "caption": "Figure 1: Overview. (a) Direct Consistency Optimization (DCO) pushes the Pareto frontier between prompt fidelity and subject fidelity towards upper-right over DreamBooth [10], and with prior preservation loss (DreamBooth+p.p.). DCO improves generating custom subject with various visual attributes (e.g., astronaut outfits and background of Mars), or various styles that pretrained model knows (e.g., flat cartoon illustration style). (b) The customized subject and style models fine-tuned with DCO can be merged as is, allowing us to generate my subject in my style [11].", "description": "This figure shows a comparison of different methods for customizing text-to-image diffusion models. (a) demonstrates that the proposed Direct Consistency Optimization (DCO) method outperforms other methods in terms of both prompt fidelity (how well the generated image matches the text prompt) and subject fidelity (how well the generated image matches the reference image).  (b) illustrates that models trained with DCO can be easily combined to generate images with a custom subject and style, unlike other approaches.", "section": "1 Introduction"}, {"figure_path": "VazkRbCGxt/figures/figures_24_1.jpg", "caption": "Figure 15: 1-shot personalization using synthetic images generated by SDXL. We show the capability of our method in 1-shot subject personalization using the images generated by pretrained SDXL models. For each reference image (man and pig), DCO fine-tuned T2I models can generate subjects with different actions and styles. The prompts that used to generate reference images were \u201ca photo of a 50 years old man with curly hair\u201d and \u201ca 3D animation of happy pig\u201d, respectively, as used in [62].", "description": "This figure demonstrates the 1-shot personalization capability of the proposed Direct Consistency Optimization (DCO) method using synthetic images generated by Stable Diffusion XL (SDXL).  It showcases the model's ability to generate images of a subject (man and pig) performing various actions and in diverse styles, starting from a single reference image.  The prompts used for generating the reference images are explicitly mentioned.", "section": "5.1 Subject Personalization"}, {"figure_path": "VazkRbCGxt/figures/figures_25_1.jpg", "caption": "Figure 1: Overview. (a) Direct Consistency Optimization (DCO) pushes the Pareto frontier between prompt fidelity and subject fidelity towards upper-right over DreamBooth [10], and with prior preservation loss (DreamBooth+p.p.). DCO improves generating custom subject with various visual attributes (e.g., astronaut outfits and background of Mars), or various styles that pretrained model knows (e.g., flat cartoon illustration style). (b) The customized subject and style models fine-tuned with DCO can be merged as is, allowing us to generate my subject in my style [11].", "description": "This figure demonstrates the effectiveness of Direct Consistency Optimization (DCO) in comparison to DreamBooth and DreamBooth with prior preservation loss in generating images with high subject and prompt fidelity.  The left panel (a) shows that DCO improves the balance between subject fidelity (how well the generated image matches the reference image) and prompt fidelity (how well the generated image matches the text prompt) compared to the baselines. The right panel (b) illustrates the ability of DCO to seamlessly merge fine-tuned models for subject and style, allowing for a higher degree of customization and creative control.", "section": "1 Introduction"}, {"figure_path": "VazkRbCGxt/figures/figures_25_2.jpg", "caption": "Figure 3: Custom subject generation. We show selected generations from DreamBooth (DB), DB with prior preservation (DB+p.p.), and ours (DCO) of custom subjects with varying attributes and styles guided by text prompts. While DB captures subjects well, it does not follow text prompt well. DB+p.p. shows better textual alignment, but falls short in subject fidelity. Ours show the best in both image-text alignment and subject fidelity. Best viewed in color, zoomed in on monitor.", "description": "This figure compares the image generation results of three different methods: DreamBooth, DreamBooth with prior preservation, and the proposed Direct Consistency Optimization (DCO).  For several subjects, each method is prompted to generate images with varying attributes and styles. The results demonstrate that DCO achieves the best balance between accurately capturing the subject and adhering to the text prompt's specifications.", "section": "5.1 Subject Personalization"}, {"figure_path": "VazkRbCGxt/figures/figures_26_1.jpg", "caption": "Figure 18: Custom style generation. Additional qualitative results on custom style generation. Our method (DCO) is able to generate style consistent image, while prior method, DreamBooth (DB), is prone to overfitting. For example, in the first row, the leaves of flower is inherited to butterfly, Christmas tree, and piano in DB, while our methods disentangle such attributes in generation. Those are also shown in second and third rows.", "description": "This figure shows additional qualitative results comparing the proposed Direct Consistency Optimization (DCO) method with the DreamBooth (DB) baseline for style customization.  The results demonstrate that DCO generates style-consistent images while DB tends to overfit, resulting in the undesirable inheritance of attributes from the reference image.", "section": "5.2 Style Personalization"}, {"figure_path": "VazkRbCGxt/figures/figures_27_1.jpg", "caption": "Figure 1: Overview. (a) Direct Consistency Optimization (DCO) pushes the Pareto frontier between prompt fidelity and subject fidelity towards upper-right over DreamBooth [10], and with prior preservation loss (DreamBooth+p.p.). DCO improves generating custom subject with various visual attributes (e.g., astronaut outfits and background of Mars), or various styles that pretrained model knows (e.g., flat cartoon illustration style). (b) The customized subject and style models fine-tuned with DCO can be merged as is, allowing us to generate my subject in my style [11].", "description": "This figure demonstrates the effectiveness of Direct Consistency Optimization (DCO) compared to other methods for image generation.  (a) shows that DCO improves the trade-off between generating images that closely match the prompt (prompt fidelity) and images that closely resemble the reference images used for training (subject fidelity).  (b) shows the additional benefit of DCO in that its fine-tuned models can be merged to create images with a custom subject and a custom style.  This is a significant improvement over other methods that struggle with this merging process.", "section": "1 Introduction"}, {"figure_path": "VazkRbCGxt/figures/figures_28_1.jpg", "caption": "Figure 6: My subject in my style generation. We show generated images by merging subject and style LoRAs, each trained independently with DB (DB Merge) or DCO (DCO Merge). We also show results of ZipLoRA [21] using DB models (DB ZipLoRA). DB Merge struggles to generate high-quality images. While DB ZipLoRA improves the quality, it less preserves fidelity. DCO Merge produces consistent images in both subject and style. Best viewed in color, zoomed in on monitor.", "description": "This figure compares the results of merging subject and style models trained using different methods.  DreamBooth (DB) and Direct Consistency Optimization (DCO) are compared with DB models further post-processed with ZipLoRA.  The results show DCO produces better quality and more consistent results than the other methods, highlighting its ability to preserve subject and style fidelity during merging.", "section": "5.3 My Subject in My Style"}, {"figure_path": "VazkRbCGxt/figures/figures_29_1.jpg", "caption": "Figure 6: My subject in my style generation. We show generated images by merging subject and style LoRAs, each trained independently with DB (DB Merge) or DCO (DCO Merge). We also show results of ZipLoRA [21] using DB models (DB ZipLoRA). DB Merge struggles to generate high-quality images. While DB ZipLoRA improves the quality, it less preserves fidelity. DCO Merge produces consistent images in both subject and style. Best viewed in color, zoomed in on monitor.", "description": "This figure demonstrates the results of merging subject and style models fine-tuned with DreamBooth (DB) and Direct Consistency Optimization (DCO).  It shows that merging DCO-trained models results in higher-quality images with better preservation of subject and style fidelity compared to merging DB-trained models, even when using post-optimization techniques like ZipLoRA.", "section": "5.3 My Subject in My Style"}]