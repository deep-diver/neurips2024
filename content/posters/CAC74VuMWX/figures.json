[{"figure_path": "CAC74VuMWX/figures/figures_3_1.jpg", "caption": "Figure 1: In a simplified attention-only experiment, MSSA operator with skip connection actually implements an ascent method on R\u00ba(Z; U), opposed to its design purpose (left). This is due to an artifact in approximation with its second-order term. (right)", "description": "This figure shows the results of a simplified attention-only experiment.  The left panel (a) demonstrates that the MSSA (Multi-head Subspace Self-Attention) operator with a skip connection, a core component of the CRATE model, unexpectedly increases the coding rate R\u00ba(Z;U) across layers instead of decreasing it as intended. The right panel (b) visually explains this behavior by showing that the approximation used in deriving the MSSA operator, specifically omitting the first-order term in the Taylor expansion of the coding rate, leads to an ascent instead of descent on the coding rate, hence explaining the counter-intuitive result of the experiment in (a).", "section": "4.1 Pitfalls in Deriving CRATE-C"}, {"figure_path": "CAC74VuMWX/figures/figures_3_2.jpg", "caption": "Figure 1: In a simplified attention-only experiment, MSSA operator with skip connection actually implements an ascent method on R\u00ba(Z; U), opposed to its design purpose (left). This is due to an artifact in approximation with its second-order term. (right)", "description": "The figure shows the results of a simplified attention-only experiment. The left panel shows that the MSSA operator with skip connection, which is designed to implement a descent method on R\u00ba(Z; U), actually implements an ascent method. The right panel shows that this is due to an artifact in the approximation of the second-order term in the Taylor expansion of R\u00ba(Z; U).", "section": "4.1 Pitfalls in Deriving CRATE-C"}, {"figure_path": "CAC74VuMWX/figures/figures_5_1.jpg", "caption": "Figure 2: Sparse rate reduction measure ||Z||o + R\u00ba(Z;U) \u2013 R(Z) of CRATE and its variants evaluated at different layers and epochs on CIFAR-10.", "description": "This figure shows the behavior of the sparse rate reduction measure across different layers and training epochs for four variants of the CRATE model (CRATE-C, CRATE-N, CRATE-T, and CRATE) on the CIFAR-10 dataset.  Each line represents a different epoch, illustrating how this complexity measure changes as the model trains and propagates through its layers.  The figure highlights that the measure generally decreases in the initial layers and then increases in deeper layers, offering insights into the optimization process within the CRATE model.", "section": "4.3 Behaviors of Sparse Rate Reduction"}, {"figure_path": "CAC74VuMWX/figures/figures_5_2.jpg", "caption": "Figure 2: Sparse rate reduction measure ||Z||o + R\u00ba(Z;U) \u2013 R(Z) of CRATE and its variants evaluated at different layers and epochs on CIFAR-10.", "description": "This figure shows how the sparse rate reduction measure changes across different layers of the CRATE model and its variants (CRATE-C, CRATE-N, CRATE-T) during training on the CIFAR-10 dataset.  The x-axis represents the layer number, and the y-axis represents the sparse rate reduction measure. Separate lines are shown for different training epochs, providing insight into how this measure evolves as the model trains.  The figure aims to illustrate whether the sparse rate reduction objective is being optimized during the forward pass and how it changes throughout the training process.", "section": "4.3 Behaviors of Sparse Rate Reduction"}, {"figure_path": "CAC74VuMWX/figures/figures_5_3.jpg", "caption": "Figure 2: Sparse rate reduction measure ||Z||o + R\u00ba(Z;U) \u2013 R(Z) of CRATE and its variants evaluated at different layers and epochs on CIFAR-10.", "description": "This figure displays the evolution of the sparse rate reduction (SRR) measure across different layers and training epochs for four variants of the CRATE model (CRATE-C, CRATE-N, CRATE-T, and CRATE) on the CIFAR-10 dataset. The SRR metric combines the L0 norm of the representations, the coding rate in subspaces, and the overall coding rate. The plots show how this measure changes as the model trains, indicating whether the model is successfully optimizing SRR during the forward pass. Each line represents a different epoch of training, showing how SRR changes across layers in the network over time. This visualization helps to understand the layer-wise behaviors of SRR optimization within the various CRATE models.", "section": "4.3 Behaviors of Sparse Rate Reduction"}, {"figure_path": "CAC74VuMWX/figures/figures_5_4.jpg", "caption": "Figure 2: Sparse rate reduction measure ||Z||o + R\u00ba(Z;U) \u2013 R(Z) of CRATE and its variants evaluated at different layers and epochs on CIFAR-10.", "description": "This figure shows the sparse rate reduction measure (||Z||o + R\u00ba(Z;U) \u2013 R(Z)) for CRATE and its variants (CRATE-C, CRATE-N, CRATE-T) across different layers and epochs during training on the CIFAR-10 dataset.  Each subplot represents a different variant of the CRATE model. The x-axis represents the layer number, and the y-axis represents the sparse rate reduction measure. Different colored lines represent different epochs during training. The figure aims to illustrate how the sparse rate reduction measure evolves throughout the layers of the model and over the course of training, providing insights into the optimization process.", "section": "4.3 Behaviors of Sparse Rate Reduction"}, {"figure_path": "CAC74VuMWX/figures/figures_6_1.jpg", "caption": "Figure 2: Sparse rate reduction measure ||Z||o + R\u00ba(Z;U) \u2013 R(Z) of CRATE and its variants evaluated at different layers and epochs on CIFAR-10.", "description": "This figure shows how the sparse rate reduction (SRR) measure changes across different layers of four variations of the CRATE model (CRATE-C, CRATE-N, CRATE-T, and CRATE) at various training epochs (from initialization to epoch 200) on the CIFAR-10 dataset.  The SRR measure is a complexity metric reflecting the compactness of learned representations. The plot reveals the layer-wise behaviors of SRR optimization during training and highlights differences among the CRATE variants.  The graph indicates a generally decreasing trend in SRR in the initial layers, suggesting effective compression, followed by an increase in later layers. This suggests an interplay between compression and sparsity which is not completely optimized in the CRATE model as originally proposed.", "section": "4.3 Behaviors of Sparse Rate Reduction"}, {"figure_path": "CAC74VuMWX/figures/figures_6_2.jpg", "caption": "Figure 2: Sparse rate reduction measure ||Z||o + R\u00ba(Z;U) \u2013 R(Z) of CRATE and its variants evaluated at different layers and epochs on CIFAR-10.", "description": "This figure shows the behavior of the sparse rate reduction measure across different layers and epochs for four variants of the CRATE model (CRATE-C, CRATE-N, CRATE-T, and CRATE) trained on the CIFAR-10 dataset.  The x-axis represents the layer number, and the y-axis represents the sparse rate reduction measure. Each line represents a different epoch during training, showing how this measure changes as the model learns.  The figure helps visualize how the sparse rate reduction is optimized layer-wise during training for each variant and to understand differences among variants. ", "section": "4.3 Behaviors of Sparse Rate Reduction"}, {"figure_path": "CAC74VuMWX/figures/figures_6_3.jpg", "caption": "Figure 2: Sparse rate reduction measure ||Z||o + R\u00ba(Z;U) \u2013 R(Z) of CRATE and its variants evaluated at different layers and epochs on CIFAR-10.", "description": "This figure visualizes how the sparse rate reduction measure changes across different layers of the CRATE model and its variants (CRATE-C, CRATE-N, CRATE-T) during training on the CIFAR-10 dataset. The x-axis represents the layer number, and the y-axis represents the sparse rate reduction measure. Each line corresponds to a different training epoch, showing the evolution of the measure over time.  The figure helps in understanding the layer-wise optimization behavior of the SRR objective in various model implementations.", "section": "4.3 Behaviors of Sparse Rate Reduction"}, {"figure_path": "CAC74VuMWX/figures/figures_6_4.jpg", "caption": "Figure 2: Sparse rate reduction measure ||Z||o + R\u00ba(Z;U) \u2013 R(Z) of CRATE and its variants evaluated at different layers and epochs on CIFAR-10.", "description": "This figure shows the sparse rate reduction measure across different layers and epochs of four variants of the CRATE model (CRATE-C, CRATE-N, CRATE-T, and CRATE) trained on the CIFAR-10 dataset. Each line represents a different epoch, showing how the measure evolves during training. The x-axis represents the layer number, and the y-axis represents the sparse rate reduction measure.", "section": "4.3 Behaviors of Sparse Rate Reduction"}, {"figure_path": "CAC74VuMWX/figures/figures_7_1.jpg", "caption": "Figure 4: A scatter plot illustrating the value of SRR measure and generalization gap across CRATE and variants with network width d = 384.", "description": "This figure shows a scatter plot that visualizes the relationship between the Sparse Rate Reduction (SRR) measure and the generalization gap for various CRATE models with a network width of 384. Each point represents a different model variant, with the x-coordinate indicating the SRR measure and the y-coordinate representing the generalization gap.  The plot aims to demonstrate the correlation between SRR and generalization performance. Different colors represent different CRATE variants (CRATE-C, CRATE-N, CRATE-T, CRATE).", "section": "5.2 Correlation with Generalization"}, {"figure_path": "CAC74VuMWX/figures/figures_13_1.jpg", "caption": "Figure 1: In a simplified attention-only experiment, MSSA operator with skip connection actually implements an ascent method on R\u00ba(Z; U), opposed to its design purpose (left). This is due to an artifact in approximation with its second-order term. (right)", "description": "This figure shows the results of a simplified attention-only experiment.  The left panel (a) demonstrates that, contrary to its intended purpose, the MSSA operator with skip connections actually performs an ascent on R\u00ba(Z; U) rather than a descent. The right panel (b) illustrates why this occurs: it's due to an artifact introduced by approximating the log(.) function using only its second-order Taylor expansion term.  The graph shows that this approximation leads to maximization of R\u00ba instead of minimization, illustrating the shortcomings of this simplification in the derivation of CRATE.", "section": "4.1 Pitfalls in Deriving CRATE-C"}, {"figure_path": "CAC74VuMWX/figures/figures_15_1.jpg", "caption": "Figure 6: A scatter plot illustrating the value of SRR measure and generalization gap across CRATE and variants with network width d = 768.", "description": "This figure shows the correlation between the Sparse Rate Reduction (SRR) measure and the generalization gap for different variants of the CRATE model (CRATE-C, CRATE-N, CRATE-T, and CRATE) with a network width of 768. Each point represents a model trained with different hyperparameters. The x-axis represents the SRR measure, which is used as a complexity measure, and the y-axis represents the generalization gap (the difference between validation loss and training loss). A positive correlation between SRR and generalization gap is observed, indicating that models with higher SRR values tend to have larger generalization gaps.", "section": "5.2 Correlation with Generalization"}]