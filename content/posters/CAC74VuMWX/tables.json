[{"figure_path": "CAC74VuMWX/tables/tables_7_1.jpg", "caption": "Table 1: Correlation of complexity measures with generalization gap (width d = 384).", "description": "This table presents the Kendall's rank correlation coefficients between different complexity measures and the generalization gap for Transformer-like models.  The complexity measures considered include various norm-based metrics (l2-norm, path-norm, etc.), spectral-based metrics (sum-of-spec, prod-of-spec, etc.), margin-based metrics (1/margin), and the Sparse Rate Reduction (SRR) measure. The table shows the correlation for each measure across different hyperparameter settings (batch size, learning rate, dropout, and model type), providing an overall correlation coefficient. A higher correlation indicates a stronger relationship between the complexity measure and generalization performance.  The width of the network (d) is fixed at 384 for this analysis.", "section": "5.2 Correlation with Generalization"}, {"figure_path": "CAC74VuMWX/tables/tables_8_1.jpg", "caption": "Table 2: Top-1 accuracy for CRATE and its variants trained with or without SRR regularization on CIFAR-10/100 from scratch (width d = 384).", "description": "This table shows the top-1 accuracy results achieved by four different Transformer-like models (CRATE-C, CRATE-N, CRATE-T, and CRATE) when trained on CIFAR-10 and CIFAR-100 datasets.  The models were trained using either only cross-entropy loss or cross-entropy loss combined with Sparse Rate Reduction (SRR) regularization. The table highlights the impact of SRR regularization on model performance, demonstrating improvement in accuracy for all models across both datasets when SRR regularization was used.", "section": "6 Efficient Implementation"}, {"figure_path": "CAC74VuMWX/tables/tables_14_1.jpg", "caption": "Table 3: Top-1 accuracy for CRATE and its variants trained on CIFAR-10 from scratch (width d = 384).", "description": "This table presents the top-1 accuracy results achieved by different variants of the CRATE model on the CIFAR-10 dataset.  The models were trained from scratch with a network width (d) of 384.  The variants include the original CRATE-C, along with modifications CRATE-N (negative), CRATE-T (transpose), and CRATE (original with learnable parameters), CRATE-Fix (fixed output matrix), and CRATE-Identity (identity output matrix). The table also shows the number of parameters (# Params) for each model variant.", "section": "6.2 Efficient Implementation"}, {"figure_path": "CAC74VuMWX/tables/tables_14_2.jpg", "caption": "Table 4: Choices of hyperparameters.", "description": "This table lists the hyperparameters used in the experiments and their respective choices.  These hyperparameters were varied to generate a set of models for evaluating the correlation between SRR and generalization.  The hyperparameters include batch size, initial learning rate, width (of the model), dropout rate, and model type (CRATE-C, CRATE-N, CRATE-T, and CRATE).", "section": "5.2 Correlation with Generalization"}, {"figure_path": "CAC74VuMWX/tables/tables_15_1.jpg", "caption": "Table 5: Correlation of complexity measures with generalization gap (width d = 768).", "description": "This table presents the Kendall's rank correlation coefficients (\u03c4) between different complexity measures and the generalization gap for a network width of 768.  The table shows the correlation for each measure across various hyperparameter settings (batch size, learning rate, dropout, and model type), as well as an overall correlation.  Positive values indicate a positive correlation (lower complexity associated with better generalization), and negative values indicate a negative correlation.", "section": "5.2 Correlation with Generalization"}, {"figure_path": "CAC74VuMWX/tables/tables_16_1.jpg", "caption": "Table 6: Top-1 accuracy for CRATE and its variants trained with efficient implementations of SRR regularization on CIFAR-10 from scratch (width d = 384).", "description": "This table presents the top-1 accuracy results achieved by various CRATE models (CRATE-C, CRATE-N, CRATE-T, and CRATE) when trained on the CIFAR-10 dataset using different efficient implementations of Sparse Rate Reduction (SRR) regularization.  The models were trained from scratch with a network width of 384.  The table compares the performance of models trained solely with cross-entropy loss against those trained with cross-entropy loss plus SRR regularization applied to different layers (layer 2, 4, 6, 8, 10, and 12) or randomly selected layers. The results highlight the impact of applying SRR regularization at various layers and compare the effectiveness of different layer selection strategies for regularization.", "section": "6.2 Efficient Implementation"}]