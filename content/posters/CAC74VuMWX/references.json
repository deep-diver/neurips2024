{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-00-00", "reason": "This paper is foundational to the current work as it introduces the vision transformer (ViT), a crucial architecture upon which the current research builds."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper is highly influential as it introduced the transformer architecture, a core component analyzed and built upon in this research."}, {"fullname_first_author": "Yaodong Yu", "paper_title": "White-box transformers via sparse rate reduction", "publication_date": "2023-00-00", "reason": "This is the primary source paper for the concept of Sparse Rate Reduction (SRR), which forms the basis of the investigation in the current study."}, {"fullname_first_author": "Jimmy Lei Ba", "paper_title": "Layer normalization", "publication_date": "2016-00-00", "reason": "Layer normalization is an important technique for stabilizing training of deep neural networks and used in the current work, making this paper highly relevant."}, {"fullname_first_author": "Karol Gregor", "paper_title": "Learning fast approximations of sparse coding", "publication_date": "2010-00-00", "reason": "This work is relevant as it introduces algorithm unrolling, a technique used to design the Transformer-like model studied in the current work."}]}