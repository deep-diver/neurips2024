[{"figure_path": "FCsEvaMorw/figures/figures_1_1.jpg", "caption": "Figure 1: An example archive generated by RAINBOW TEAMING when used to discover safety vulnerabilities in Llama 2-chat 7B. Here, we search over two features: Risk Category and Attack Style. Shading corresponds to the Llama Guard [26] scores of responses induced by the adversarial prompt in each cell (higher means more confidence in the response being unsafe). Some excerpts of discovered prompts from a single archive are shown.", "description": "This figure displays a heatmap visualizing the results of adversarial prompt generation using RAINBOW TEAMING on the Llama 2-chat 7B model.  The rows represent different risk categories (e.g., violence, hate, sexual content), and columns represent various attack styles (e.g., slang, role-playing, technical terms). Each cell's color intensity corresponds to the Llama Guard safety score for the generated prompt belonging to that specific risk category and attack style combination; darker shades indicate higher safety concerns. Excerpts of actual prompts generated by RAINBOW TEAMING are also shown.", "section": "1 Introduction"}, {"figure_path": "FCsEvaMorw/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of RAINBOW TEAMING in the safety domain: Our method operates on a discretised grid, archiving adversarial prompts with K defining features, such as Risk Category or Attack Style. Each iteration involves a Mutator LLM applying K mutations to generate new candidate prompts. These prompts are then fed into the Target LLM. A Judge LLM evaluates these responses against archived prompts with the same features, updating the archive with any prompt that elicits a more unsafe response from the Target.", "description": "This figure illustrates the workflow of the RAINBOW TEAMING algorithm in the context of safety. The algorithm uses a multi-dimensional grid (archive) to store adversarial prompts, categorized by features like risk category and attack style.  Each iteration involves sampling a prompt from the archive, mutating it using a Mutator LLM, generating a response using a Target LLM, and then comparing the response to the existing archive entry with a Judge LLM to determine which prompt is more effective at triggering unsafe responses. The archive is updated to retain the most effective prompts, ensuring diversity and effectiveness of the discovered prompts.  The figure visually depicts the key components (Mutator LLM, Target LLM, Judge LLM, Archive) and their interactions during the iterative process.", "section": "3 RAINBOW TEAMING"}, {"figure_path": "FCsEvaMorw/figures/figures_5_1.jpg", "caption": "Figure 3: Attack success rate of adversarial prompts discovered by RAINBOW TEAMING for different models, as evaluated by GPT-4.", "description": "This figure shows the attack success rate (ASR) of adversarial prompts generated by RAINBOW TEAMING against four different large language models (LLMs): Llama 2-chat 7B, Llama 3-Instruct 8B, Mistral 7B, and Vicuna 7B v1.5.  The ASR is evaluated using GPT-4, a large language model developed by OpenAI. The x-axis represents the number of iterations of the RAINBOW TEAMING algorithm, and the y-axis represents the ASR. The lines represent the different LLMs, and the shaded areas around the lines represent the standard error of the mean.", "section": "4.1 Results"}, {"figure_path": "FCsEvaMorw/figures/figures_7_1.jpg", "caption": "Figure 5: Attack success rate before and after fine-tuning Llama 2-chat 7B on synthetic data generated via RAINBOW TEAMING. The fine-tuned model is significantly less vulnerable to RAINBOW TEAMING on a second application, with the method achieving a substantially lower ASR after 2000 iterations. We report the mean and standard error over 3 independent runs.", "description": "This figure shows the attack success rate of adversarial prompts against Llama 2-chat 7B, both before and after fine-tuning it using synthetic data generated by Rainbow Teaming. The results demonstrate that fine-tuning significantly enhances the model's robustness to adversarial attacks.", "section": "Enhancing Robustness with Synthetic Data"}, {"figure_path": "FCsEvaMorw/figures/figures_8_1.jpg", "caption": "Figure 6: An example archive of adversarial questions discovered by RAINBOW TEAMING. Vacant cells are marked in yellow, intermediate but unsuccessful attempts are in green, and successful adversarial questions are in purple.", "description": "This figure shows a 3D archive generated by RAINBOW TEAMING for the question answering task.  The three dimensions are Topic, Interrogative Word, and Length.  Each cell in the archive represents a unique combination of these three features. The color of each cell indicates the success rate of the adversarial question in that cell, with purple indicating high success rates, green indicating intermediate success, and yellow indicating that no successful adversarial questions were found for that combination of features.", "section": "6.1 Question Answering"}, {"figure_path": "FCsEvaMorw/figures/figures_17_1.jpg", "caption": "Figure 1: An example archive generated by RAINBOW TEAMING when used to discover safety vulnerabilities in Llama 2-chat 7B. Here, we search over two features: Risk Category and Attack Style. Shading corresponds to the Llama Guard [26] scores of responses induced by the adversarial prompt in each cell (higher means more confidence in the response being unsafe). Some excerpts of discovered prompts from a single archive are shown.", "description": "This figure shows a heatmap visualizing the results of Rainbow Teaming applied to Llama 2-chat 7B.  The x and y axes represent different attack styles and risk categories, respectively. Each cell's color intensity corresponds to the Llama Guard safety score of the LLM's response to the prompt associated with that cell; darker colors indicate higher safety scores (more confidence in safety), while lighter colors indicate lower safety scores (less confidence in safety).  The figure also includes example prompts from the archive, illustrating the diverse range of prompts generated by Rainbow Teaming.", "section": "1 Introduction"}, {"figure_path": "FCsEvaMorw/figures/figures_17_2.jpg", "caption": "Figure 1: An example archive generated by RAINBOW TEAMING when used to discover safety vulnerabilities in Llama 2-chat 7B. Here, we search over two features: Risk Category and Attack Style. Shading corresponds to the Llama Guard [26] scores of responses induced by the adversarial prompt in each cell (higher means more confidence in the response being unsafe). Some excerpts of discovered prompts from a single archive are shown.", "description": "This figure shows a heatmap visualizing the results of Rainbow Teaming applied to Llama 2-chat 7B in the safety domain.  The rows and columns represent different Risk Categories and Attack Styles, respectively, which are used as dimensions in the quality-diversity (QD) search. Each cell's color intensity corresponds to the Llama Guard score, indicating the model's confidence that the response generated by that specific adversarial prompt is unsafe (darker = more unsafe).  A snippet of actual adversarial prompts that were generated for each cell is also shown for illustrative purposes.", "section": "1 Introduction"}, {"figure_path": "FCsEvaMorw/figures/figures_18_1.jpg", "caption": "Figure 1: An example archive generated by RAINBOW TEAMING when used to discover safety vulnerabilities in Llama 2-chat 7B. Here, we search over two features: Risk Category and Attack Style. Shading corresponds to the Llama Guard [26] scores of responses induced by the adversarial prompt in each cell (higher means more confidence in the response being unsafe). Some excerpts of discovered prompts from a single archive are shown.", "description": "This figure shows a heatmap visualizing the results of the RAINBOW TEAMING algorithm in discovering safety vulnerabilities within the Llama 2-chat 7B language model. The rows represent different attack styles used to generate adversarial prompts. The columns represent various risk categories (e.g., violence, hate speech, fraud). Each cell in the heatmap shows the Llama Guard score for prompts from a specific row and column combination; higher scores indicate higher confidence that the response is unsafe. The color intensity represents the Llama Guard score. The figure also includes examples of adversarial prompts generated by the model for various combinations of risk categories and attack styles.", "section": "1 Introduction"}, {"figure_path": "FCsEvaMorw/figures/figures_19_1.jpg", "caption": "Figure 8: Attack success rate of adversarial prompts discovered by RAINBOW TEAMING on Llama 2-chat 7B, 13B, and 70B, as measured by GPT-4 and Llama Guard. We report the mean and standard error over 3 independent runs.", "description": "This figure shows the attack success rate achieved by the Rainbow Teaming method against three different sizes of Llama 2 chat models (7B, 13B, and 70B parameters).  The results are evaluated using two different safety classifiers: GPT-4 and Llama Guard. The graph displays how the attack success rate changes over 2000 iterations of the Rainbow Teaming algorithm, with error bars representing the mean and standard deviation across three independent runs. It demonstrates the effectiveness of Rainbow Teaming across varying model sizes and evaluation methods.", "section": "4.1 Results"}, {"figure_path": "FCsEvaMorw/figures/figures_20_1.jpg", "caption": "Figure 9: Comparison of RAINBOW TEAMING with a pairwise comparison (Judge) and a score-based (No Judge) preference models applied to Llama 2-chat 7B. Left: ASR as evaluated by GPT-4. Centre: ASR as evaluated by Llama Guard. Right: total archive updates over time. The score-based baseline reward hacks the Llama Guard score and underperforms under GPT-4 evaluation. It also stops updating the archive after saturating the Llama Guard score, whereas the comparison method RAINBOW TEAMING performs a more open-ended search.", "description": "This figure compares two variants of the RAINBOW TEAMING algorithm: one using a pairwise comparison method (Judge) and another using a score-based method (No Judge) for evaluating the quality of adversarial prompts.  The left panel shows the Attack Success Rate (ASR) as evaluated by GPT-4, the center panel shows the ASR as evaluated by Llama Guard, and the right panel shows the total number of archive updates over time. The results indicate that the comparison-based method is more effective and robust, and performs a more thorough exploration of the prompt space.", "section": "Preference Model Ablation"}, {"figure_path": "FCsEvaMorw/figures/figures_21_1.jpg", "caption": "Figure 3: Attack success rate of adversarial prompts discovered by RAINBOW TEAMING for different models, as evaluated by GPT-4.", "description": "This figure shows the attack success rate (ASR) achieved by RAINBOW TEAMING against four different LLMs (Llama 2-chat 7B, Llama 3-Instruct 8B, Mistral 7B, and Vicuna 7B v1.5) across 2000 iterations.  The ASR is measured using GPT-4. The graph illustrates how the effectiveness of RAINBOW TEAMING in generating successful adversarial prompts varies across different language models.  It demonstrates that RAINBOW TEAMING is effective against all tested models, highlighting different levels of vulnerability.", "section": "4.1 Results"}, {"figure_path": "FCsEvaMorw/figures/figures_21_2.jpg", "caption": "Figure 4: Attack success rate of adversarial prompts discovered by RAINBOW TEAMING and baselines against the Llama 2-chat 7B model.", "description": "This figure compares the attack success rate of adversarial prompts generated by RAINBOW TEAMING against two baselines: (1) No Stepping Stones, which generates prompts without using previously discovered prompts, and (2) Same Cell Mutations, which performs mutations independently within each cell of the archive.  The plot shows the attack success rate over 2000 iterations, using both GPT-4 and Llama Guard for evaluation. RAINBOW TEAMING consistently outperforms both baselines, demonstrating the value of using stepping stones and cross-category mutations.", "section": "4.1 Results"}, {"figure_path": "FCsEvaMorw/figures/figures_22_1.jpg", "caption": "Figure 5: Attack success rate before and after fine-tuning Llama 2-chat 7B on synthetic data generated via RAINBOW TEAMING. The fine-tuned model is significantly less vulnerable to RAINBOW TEAMING on a second application, with the method achieving a substantially lower ASR after 2000 iterations. We report the mean and standard error over 3 independent runs.", "description": "This figure shows the attack success rate of adversarial prompts against the Llama 2-chat 7B model before and after fine-tuning it with synthetic data generated by RAINBOW TEAMING. The results demonstrate that fine-tuning with synthetic data generated by RAINBOW TEAMING significantly improves the model's robustness to adversarial attacks. The attack success rate is lower after the fine-tuning, demonstrating the effectiveness of the method in enhancing the model's resilience.", "section": "Enhancing Robustness with Synthetic Data"}, {"figure_path": "FCsEvaMorw/figures/figures_23_1.jpg", "caption": "Figure 1: An example archive generated by RAINBOW TEAMING when used to discover safety vulnerabilities in Llama 2-chat 7B. Here, we search over two features: Risk Category and Attack Style. Shading corresponds to the Llama Guard [26] scores of responses induced by the adversarial prompt in each cell (higher means more confidence in the response being unsafe). Some excerpts of discovered prompts from a single archive are shown.", "description": "This figure shows a heatmap visualizing the results of adversarial prompt generation using RAINBOW TEAMING. The rows represent different risk categories (e.g. violence, sexual content), and the columns represent different attack styles (e.g. slang, role-play). Each cell's color intensity represents the Llama Guard score, which measures the safety of the model's response to the corresponding adversarial prompt. Darker shades indicate more unsafe responses.  The figure also provides examples of prompts found by RAINBOW TEAMING within the categories shown.", "section": "1 Introduction"}, {"figure_path": "FCsEvaMorw/figures/figures_24_1.jpg", "caption": "Figure 1: An example archive generated by RAINBOW TEAMING when used to discover safety vulnerabilities in Llama 2-chat 7B. Here, we search over two features: Risk Category and Attack Style. Shading corresponds to the Llama Guard [26] scores of responses induced by the adversarial prompt in each cell (higher means more confidence in the response being unsafe). Some excerpts of discovered prompts from a single archive are shown.", "description": "This figure shows a heatmap visualizing the results of Rainbow Teaming's search for adversarial prompts targeting Llama 2-chat 7B. The heatmap's rows and columns represent different \"Risk Categories\" and \"Attack Styles\", respectively. Each cell's color intensity corresponds to a Llama Guard safety score, indicating how likely the model's response to the corresponding prompt is to be unsafe (darker = more unsafe).  The figure also includes examples of adversarial prompts discovered in this process, further illustrating the diverse range of prompts identified.", "section": "1 Introduction"}]