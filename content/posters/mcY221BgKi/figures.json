[{"figure_path": "mcY221BgKi/figures/figures_1_1.jpg", "caption": "Figure 1: Scheme Comparison. (a) Existing methods utilize cooperative perception information at each frame individually then performs forecasting. (b) Our V2X-Graph considers this information from a typical forecasting perspective and employs interpretable trajectory feature fusion in an end-to-end manner, to enhance the historical representation of agents for cooperative motion forecasting.", "description": "This figure compares two approaches to cooperative motion forecasting. (a) shows existing methods that use cooperative perception information frame-by-frame, which limits the use of motion and interaction contexts. (b) presents the proposed V2X-Graph, which uses a forecasting-oriented approach with interpretable trajectory feature fusion to improve historical representation of agents and enhance forecasting accuracy.", "section": "1 Introduction"}, {"figure_path": "mcY221BgKi/figures/figures_4_1.jpg", "caption": "Figure 2: V2X-Graph overview. Trajectories from the ego-view and other views, along with vector map information, are encoded as nodes and edges for graph construction to represent a cooperative scenario. The novel interpretable graph provides guidance for forecasting-oriented trajectory feature fusion, including motion and interaction features. In this figure, solid rectangles represent encodings of ego-view trajectories, hollow circles represent encodings of cooperative trajectories, distinguished by distinct colors. Specifically, within the same view, the use of the same color indicates interruptions caused by occlusion. Triangles represent encodings of lane segments. In trajectory feature fusion, grey arrow indicates an missing frame in motion case, a lane segment vector in interaction case.", "description": "This figure illustrates the V2X-Graph architecture, which is a novel framework for cooperative motion forecasting that leverages information from the ego vehicle, other vehicles, and infrastructure.  It uses a graph-based approach where nodes represent trajectories and lane segments, and edges represent spatial and temporal relationships between them. The graph is designed to be interpretable, facilitating fusion of motion and interaction features for enhanced forecasting accuracy. The figure highlights the key components: graph encoding, interpretable association, and trajectory feature fusion, all working together to generate a multi-agent prediction.", "section": "4.1 Scene Representation with Graph"}, {"figure_path": "mcY221BgKi/figures/figures_8_1.jpg", "caption": "Figure 3: V2X-Traj dataset. (a) Statistics of the total number and average length for the 8 classes of agents. (b) Visualizations. Orange boxes represent autonomous vehicles, blue elements denote other traffic participants and the green box denotes the target agent needs to be predicted.", "description": "This figure shows statistics and visualizations of the V2X-Traj dataset, a new dataset created for the research.  Panel (a) presents a bar chart summarizing the number and average length of trajectories for eight different agent classes (e.g., cars, pedestrians, buses).  Panel (b) displays example scenarios from the dataset, illustrating multiple views (ego, infrastructure, vehicle) and the agents included.  Orange boxes represent autonomous vehicles, blue represents other traffic participants, and green boxes indicate the target agent being predicted.", "section": "5.1 Experimental Setup"}, {"figure_path": "mcY221BgKi/figures/figures_8_2.jpg", "caption": "Figure 9: Visualizations of the V2X-Traj dataset. Each scenario consists trajectories from the ego-vehicle, the cooperative infrastructure and the cooperative autonomous vehicle. In this figure, orange boxes represent autonomous vehicles, blue elements denote traffic participants, and green boxes denote the target agent that needs to be predicted.", "description": "This figure shows four examples from the V2X-Traj dataset, illustrating scenarios with trajectories from ego-vehicle, infrastructure, and other vehicles.  Each scenario shows multiple agents with their predicted trajectories. Orange boxes represent autonomous vehicles, blue indicates other traffic participants, and green denotes the target agent whose future trajectory is being predicted. The figure visually demonstrates the multi-agent, multi-view nature of the dataset and the complexity of the scenarios.", "section": "5.1 Experimental Setup"}, {"figure_path": "mcY221BgKi/figures/figures_15_1.jpg", "caption": "Figure 2: V2X-Graph overview. Trajectories from the ego-view and other views, along with vector map information, are encoded as nodes and edges for graph construction to represent a cooperative scenario. The novel interpretable graph provides guidance for forecasting-oriented trajectory feature fusion, including motion and interaction features. In this figure, solid rectangles represent encodings of ego-view trajectories, hollow circles represent encodings of cooperative trajectories, distinguished by distinct colors. Specifically, within the same view, the use of the same color indicates interruptions caused by occlusion. Triangles represent encodings of lane segments. In trajectory feature fusion, grey arrow indicates an missing frame in motion case, a lane segment vector in interaction case.", "description": "This figure illustrates the V2X-Graph architecture, a novel framework for cooperative motion forecasting.  It shows how trajectories from ego and other views, along with vector map data, are encoded as nodes and edges in an interpretable graph. This graph guides the fusion of motion and interaction features to improve forecasting accuracy. The use of color-coding and shapes helps to visually distinguish the different types of trajectory data and their relationships. ", "section": "4 Methodology"}, {"figure_path": "mcY221BgKi/figures/figures_15_2.jpg", "caption": "Figure 2: V2X-Graph overview. Trajectories from the ego-view and other views, along with vector map information, are encoded as nodes and edges for graph construction to represent a cooperative scenario. The novel interpretable graph provides guidance for forecasting-oriented trajectory feature fusion, including motion and interaction features. In this figure, solid rectangles represent encodings of ego-view trajectories, hollow circles represent encodings of cooperative trajectories, distinguished by distinct colors. Specifically, within the same view, the use of the same color indicates interruptions caused by occlusion. Triangles represent encodings of lane segments. In trajectory feature fusion, grey arrow indicates an missing frame in motion case, a lane segment vector in interaction case.", "description": "This figure shows the architecture of the V2X-Graph model.  It details how trajectories from different viewpoints (ego-vehicle and other vehicles/infrastructure) are integrated via a graph-based approach.  The graph uses nodes to represent trajectories and lane segments, and edges represent relationships (spatial and temporal) between them.  The figure emphasizes the fusion of motion and interaction features, illustrating the model's design to leverage cooperative perception data for motion forecasting.", "section": "4.1 Scene Representation with Graph"}, {"figure_path": "mcY221BgKi/figures/figures_16_1.jpg", "caption": "Figure 3: V2X-Traj dataset. (a) Statistics of the total number and average length for the 8 classes of agents. (b) Visualizations. Orange boxes represent autonomous vehicles, blue elements denote other traffic participants and the green box denotes the target agent needs to be predicted.", "description": "This figure shows statistics and visualizations of the V2X-Traj dataset, a new dataset created for the research.  (a) presents a bar chart showing the number and average length of trajectories for eight different agent classes (e.g., cars, pedestrians, buses). (b) provides example visualizations of scenes from the dataset; orange boxes represent autonomous vehicles, blue boxes show other traffic participants, and green boxes highlight the target vehicle whose movement needs to be predicted.", "section": "5.1 Experimental Setup"}, {"figure_path": "mcY221BgKi/figures/figures_19_1.jpg", "caption": "Figure 9: Visualizations of the V2X-Traj dataset. Each scenario consists trajectories from the ego-vehicle, the cooperative infrastructure and the cooperative autonomous vehicle. In this figure, orange boxes represent autonomous vehicles, blue elements denote traffic participants, and green boxes denote the target agent that needs to be predicted.", "description": "This figure visualizes four scenarios from the V2X-Traj dataset. Each scenario shows trajectories from three perspectives: the ego-vehicle, the infrastructure, and another vehicle.  The visualization uses color-coding to distinguish different types of agents: orange for autonomous vehicles, blue for other traffic participants, and green for the target agent whose motion needs to be predicted. The figure demonstrates how the dataset includes multiple perspectives for cooperative motion forecasting.", "section": "5.1 Experimental Setup"}, {"figure_path": "mcY221BgKi/figures/figures_20_1.jpg", "caption": "Figure 10: Qualitative results on three challenge scenarios over V2X-Seq. The historical trajectories of the target agent are shown in yellow. The red dashed circles indicate a part of the enhanced information from the infrastructure view. The ground-truth trajectories are shown in red, and the predicted trajectories are shown in green.", "description": "This figure presents a qualitative comparison of motion forecasting results on three challenging scenarios from the V2X-Seq dataset.  It highlights the performance of the proposed V2X-Graph method against a comparison method. The visualization shows the historical trajectories, ground truth trajectories and predicted trajectories.  Red dashed circles emphasize the additional information integrated from the infrastructure view, showcasing the benefit of cooperative information in improving prediction accuracy.", "section": "E.2 Qualitative Results on V2X-Seq"}]