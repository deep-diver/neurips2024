[{"figure_path": "Cx2O6Xz03H/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of the frameworks among previous works, TAPTR, and TAPTRv2. Inspired by DETR-based detection algorithms, TAPTR formulates the point tracking problem as a detection problem and simplifies the overall pipeline to a well-studied DETR-like framework. After introducing the attention-based position update operation into Transformer decoder layers, the overall pipeline is further simplified to be as straightforward as detection methods. The operations within dashed boxes are executed only once.", "description": "This figure compares three different approaches to the Tracking Any Point (TAP) task. (a) shows previous methods, which involve a complex pipeline including video preparation, feature extraction, cost volume calculation, and various processing steps before tracking. (b) illustrates TAPTR, which simplifies the process by using a DETR-like framework. Each tracking point is treated as a point query. The pipeline is simplified into video preparation, point preparation, cost volume calculation and a transformer layer for final position update. (c) presents TAPTRv2, which further refines the TAPTR approach by removing the cost volume and integrating an attention-based position update mechanism. This results in an even more streamlined and efficient pipeline.", "section": "1 Introduction"}, {"figure_path": "Cx2O6Xz03H/figures/figures_3_1.jpg", "caption": "Figure 2: The overview of TAPTRv2. The image feature preparation part and the point query preparation part prepare the image features of each frame of an input video and the point queries for each tracking point in every frame. The target point detection part takes the prepared image features and point queries as input. For every frame, each point query aims to predict the position and visibility of its target point.", "description": "This figure illustrates the overall architecture of TAPTRv2, a method for tracking any point in a video. It consists of three main parts:\n\n1. **Image Feature Preparation:** Extracts multi-scale image features from each frame using a backbone network (e.g., ResNet-50) and a Transformer encoder. \n2. **Point Query Preparation:**  Prepares initial features and locations for each point to be tracked using bilinear interpolation on the multi-scale feature maps. \n3. **Target Point Detection:** Employs Transformer decoder layers to refine point queries using spatial and temporal attention, predicting the position and visibility of each point in each frame. A window post-processing step further improves accuracy by propagating predictions across multiple frames.", "section": "3 TAPTRv2"}, {"figure_path": "Cx2O6Xz03H/figures/figures_5_1.jpg", "caption": "Figure 3: Comparison of the decoder layer in TAPTR (a) and TAPTRv2 (b). In TAPTR (a), cost-volume aggregation will contaminate the content feature, affecting cross-attention and leading to the contaminated cost-volume in the next layer. In TAPTRv2 (b), with the introduction of Attention-based Position Update (APU) in cross attention, not only the attention weights are properly used to update the position of each point query and mitigate the domain gap, but also the content feature of each point query is kept uncontaminated, which is crucial for visibility prediction. We use an RGB image to represent the multi-scale feature maps for better visualization.", "description": "This figure compares the decoder layer of TAPTR and TAPTRv2.  TAPTR uses cost volume aggregation, which contaminates the content feature and negatively impacts performance. TAPTRv2 introduces an Attention-based Position Update (APU) operation in the cross-attention mechanism. APU uses attention weights to combine local relative positions, predicting a new query position without contaminating the content feature, leading to a performance improvement.", "section": "3.2 Analysis of Cost Volume Aggregation in TAPTR Decoder"}, {"figure_path": "Cx2O6Xz03H/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization of the tracking results of TAPTRv2 in the wild. A user writes \u201chouse\u201d on one frame and requires TAPTRv2 to track the points in the writing area. Best view in electronic version.", "description": "This figure shows the results of TAPTRv2 applied to a real-world video.  A user hand-writes the word \"house\" on a single frame of a video showing a castle.  The algorithm then tracks the points within the handwritten word throughout the video, demonstrating its ability to maintain accurate tracking even with changing viewpoints and scene conditions. The red dashed lines connect the corresponding points in consecutive frames to show the tracking trajectory.", "section": "5 Visualization"}, {"figure_path": "Cx2O6Xz03H/figures/figures_13_1.jpg", "caption": "Figure 5: Attention weight distributions for feature and position updating in our cross attention.", "description": "This figure shows the distributions of attention weights used for feature and position updates within the cross-attention mechanism.  The distinct distributions highlight that different weight distributions are required for effectively updating content features and positional information. This supports the paper's design choice to use a disentangler to separate the weight learning for these two distinct aspects.", "section": "A More discussions"}, {"figure_path": "Cx2O6Xz03H/figures/figures_14_1.jpg", "caption": "Figure 3: Comparison of the decoder layer in TAPTR (a) and TAPTRv2 (b). In TAPTR (a), cost-volume aggregation will contaminate the content feature, affecting cross-attention and leading to the contaminated cost-volume in the next layer. In TAPTRv2 (b), with the introduction of Attention-based Position Update (APU) in cross attention, not only the attention weights are properly used to update the position of each point query and mitigate the domain gap, but also the content feature of each point query is kept uncontaminated, which is crucial for visibility prediction. We use an RGB image to represent the multi-scale feature maps for better visualization.", "description": "This figure compares the decoder layer of TAPTR and TAPTRv2.  TAPTR uses cost volume aggregation, which contaminates the content feature and negatively affects cross-attention. TAPTRv2 introduces an Attention-based Position Update (APU) operation in cross-attention to resolve this issue. APU uses attention weights to update the position of each point query, mitigating the domain gap and keeping the content feature uncontaminated for improved visibility prediction. ", "section": "3.2 Analysis of Cost Volume Aggregation in TAPTR Decoder"}, {"figure_path": "Cx2O6Xz03H/figures/figures_15_1.jpg", "caption": "Figure 7: Apply TAPTRv2 in Trajectory Estimation.", "description": "This figure shows three examples of trajectory estimation using TAPTRv2.  In each example, a user clicks points on objects (fighters, horse, car) in a single frame. TAPTRv2 then tracks those points throughout the video, generating trajectories. This demonstrates the model's ability to accurately predict the movement of selected points over time, even with complex motion and scale changes.", "section": "5 Visualization"}]