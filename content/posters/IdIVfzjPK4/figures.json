[{"figure_path": "IdIVfzjPK4/figures/figures_1_1.jpg", "caption": "Figure 1: Sketch maps of (a) supervised, (b) unsupervised, and (c) generalist GAD paradigms.", "description": "This figure illustrates three different paradigms for graph anomaly detection (GAD): supervised, unsupervised, and generalist. The supervised paradigm uses labeled data to train a specific GAD model for each dataset.  The unsupervised paradigm trains a model without labels, also specific to each dataset. The generalist paradigm, which is the approach presented in this paper, trains a single model capable of detecting anomalies across various graph datasets without retraining or fine-tuning.  Instead of retraining, it leverages 'in-context learning' at the inference stage, using a small number of normal samples from the target dataset to adapt the general model.", "section": "1 Introduction"}, {"figure_path": "IdIVfzjPK4/figures/figures_3_1.jpg", "caption": "Figure 2: The overall pipeline of ARC, the proposed generalist GAD approach.", "description": "This figure shows the overall architecture of ARC, a generalist graph anomaly detection approach. It consists of three main modules: 1) Smoothness-Based Feature Alignment, which unifies features from different datasets into a common space; 2) Ego-Neighbor Residual Graph Encoder, which learns abnormality-related node embeddings using a multi-hop residual mechanism; and 3) Cross-Attentive In-Context Anomaly Scoring, which leverages few-shot normal samples to predict node abnormality using a cross-attention mechanism. The figure illustrates the flow of data through these modules and highlights the interaction between them.", "section": "4 ARC: A generalist GAD approach"}, {"figure_path": "IdIVfzjPK4/figures/figures_4_1.jpg", "caption": "Figure 3: AUROC on data with 5 groups of features.", "description": "This figure displays the Area Under the Receiver Operating Characteristic curve (AUROC) results for anomaly detection experiments conducted on the Cora and Facebook datasets. The experiments involved dividing the features into five groups based on their smoothness scores (sk). Each group represents a percentile range of smoothness values (80-100%, 60-80%, 40-60%, 20-40%, 0-20%), with lower sk indicating higher frequency and heterophily, which are found to be crucial in GAD. The figure demonstrates that features with lower smoothness scores (high-frequency signals) are more effective in discriminating anomalies, indicating that smoothness serves as a robust indicator for feature selection in graph anomaly detection.", "section": "4.1 Smoothness-Based Feature Alignment"}, {"figure_path": "IdIVfzjPK4/figures/figures_6_1.jpg", "caption": "Figure 4: Toy examples of query embeddings (), reconstructed query embeddings (), and context embeddings ().", "description": "This figure shows two toy examples to illustrate how the cross-attention mechanism works in the anomaly scoring module.  In Case I, there is a single class of normal nodes; their embeddings are clustered together and their reconstructed embeddings, based on the context embeddings, are also clustered near the normal nodes. The anomaly (node 5) is far from the cluster of normal nodes, and its reconstructed embedding is also distant, clearly indicating its anomalous nature. Case II shows a scenario with multiple normal classes. Again, the anomaly node 5 is easily distinguishable from the multiple normal node clusters.", "section": "4.3 Cross-Attentive In-Context Anomaly Scoring"}, {"figure_path": "IdIVfzjPK4/figures/figures_7_1.jpg", "caption": "Figure 5: Performance with varying nk.", "description": "This figure shows the performance of ARC with varying numbers of context nodes (nk).  The x-axis represents the number of context nodes, and the y-axis represents the AUROC and AUPRC. The figure demonstrates that as the number of context nodes increases, the performance of ARC generally improves, indicating its ability to leverage information from these few-shot normal samples during inference.  However, the improvement plateaus after a certain point, suggesting that adding more context nodes beyond a threshold doesn't significantly benefit the model's performance.", "section": "5.2 Experimental Results"}, {"figure_path": "IdIVfzjPK4/figures/figures_8_1.jpg", "caption": "Figure 6: Time comparison.", "description": "This figure compares the inference time and fine-tuning time (per epoch) for various GAD methods on the ACM dataset.  It shows that ARC has comparable inference time to the fastest GNN baselines (GCN and BWGNN) and significantly outperforms the unsupervised methods.  It also highlights that dataset-specific fine-tuning consumes significantly more time than inference.", "section": "5.2 Experimental Results"}, {"figure_path": "IdIVfzjPK4/figures/figures_8_2.jpg", "caption": "Figure 2: The overall pipeline of ARC, the proposed generalist GAD approach.", "description": "The figure illustrates the architecture of ARC, a generalist graph anomaly detection model. It consists of three main modules: 1) Smoothness-Based Feature Alignment, which unifies features from different datasets; 2) Ego-Neighbor Residual Graph Encoder, which learns abnormality-related node embeddings; and 3) Cross-Attentive In-Context Anomaly Scoring, which predicts node abnormality using few-shot normal samples. The figure shows the data flow and interactions between these modules during both training and inference stages.", "section": "4 ARC: A generalist GAD approach"}, {"figure_path": "IdIVfzjPK4/figures/figures_17_1.jpg", "caption": "Figure 3: AUROC on data with 5 groups of features.", "description": "This figure visualizes the Area Under the Receiver Operating Characteristic (AUROC) scores obtained from experiments on two datasets, Cora and Facebook.  The experiments involved dividing features into 5 groups based on their smoothness (sk), ranging from high to low. The graph illustrates the AUROC for each group of features, revealing the correlation between feature smoothness and model performance in anomaly detection. Features with lower smoothness (high-frequency graph signals) demonstrate improved AUROC scores compared to those with higher smoothness (low-frequency signals). This finding suggests that high-frequency signal features are more relevant to anomaly detection tasks.", "section": "4.1 Smoothness-Based Feature Alignment"}, {"figure_path": "IdIVfzjPK4/figures/figures_25_1.jpg", "caption": "Figure 2: The overall pipeline of ARC, the proposed generalist GAD approach.", "description": "This figure presents a visual representation of the ARC model's architecture. ARC consists of three main modules: Smoothness-Based Feature Alignment, Ego-Neighbor Residual Graph Encoder, and Cross-Attentive In-Context Anomaly Scoring.  The figure illustrates the data flow through each module, starting from the input features, and culminating in the anomaly scores for each node. It also highlights how ARC leverages few-shot normal samples during inference via in-context learning.", "section": "4 ARC: A generalist GAD approach"}, {"figure_path": "IdIVfzjPK4/figures/figures_26_1.jpg", "caption": "Figure 2: The overall pipeline of ARC, the proposed generalist GAD approach.", "description": "This figure illustrates the architecture of ARC, a generalist graph anomaly detection method.  It consists of three main modules: 1) Smoothness-Based Feature Alignment, which unifies features across different datasets; 2) Ego-Neighbor Residual Graph Encoder, which learns abnormality-aware node embeddings using a residual GNN; and 3) Cross-Attentive In-Context Anomaly Scoring, which predicts node abnormality by leveraging few-shot normal samples using cross-attention.  The figure shows the flow of data through each module and how they interact to achieve generalist anomaly detection.", "section": "4 ARC: A generalist GAD approach"}]