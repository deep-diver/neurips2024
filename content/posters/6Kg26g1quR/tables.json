[{"figure_path": "6Kg26g1quR/tables/tables_6_1.jpg", "caption": "Table 1: ROI of ROIDICE compared with offline RL and offline constrained RL algorithms. We average each score and get \u00b12\u00d7 standard error with 5 seeds across 10 episodes. The task name is succinctly stated: Hopper (H), Walker2D (W), Halfcheetah (HC), and Finance (F).", "description": "This table presents a comparison of the Return on Investment (ROI) achieved by the ROIDICE algorithm against other offline reinforcement learning methods, namely OptiDICE (offline RL) and COptiDICE (offline constrained RL), across various tasks.  The tasks include locomotion tasks (Hopper, Walker2D, HalfCheetah) and a financial task (Finance).  The results show the average ROI and standard error for each algorithm, providing a quantitative comparison of their performance in terms of efficiency (return relative to cost).", "section": "5.2 Results"}, {"figure_path": "6Kg26g1quR/tables/tables_17_1.jpg", "caption": "Table 2: Reward and cost function hyperparameters", "description": "This table lists the hyperparameters used in the reward and cost functions for the financial task in the FinRL environment.  It shows the values assigned to parameters  `w<sup>m</sup><sub>c</sub>`, `b<sup>m</sup><sub>c</sub>`, `w<sup>f</sup><sub>r</sub>`, `w<sup>f</sup><sub>c</sub>`, and `b<sup>f</sup><sub>c</sub>`, which control aspects of the reward and cost calculations related to trading volume and stock prices.", "section": "5.2.1 Environment and Offline Dataset"}, {"figure_path": "6Kg26g1quR/tables/tables_18_1.jpg", "caption": "Table 1: ROI of ROIDICE compared with offline RL and offline constrained RL algorithms. We average each score and get \u00b12\u00d7 standard error with 5 seeds across 10 episodes. The task name is succinctly stated: Hopper (H), Walker2D (W), Halfcheetah (HC), and Finance (F).", "description": "This table presents a comparison of the Return on Investment (ROI) achieved by the proposed ROIDICE algorithm against other offline reinforcement learning methods, including OptiDICE (offline RL) and COptiDICE (offline constrained RL) across various tasks.  The results show the average ROI and the standard error across multiple trials (5 seeds and 10 episodes) for each algorithm on four different tasks: Hopper, Walker2D, Halfcheetah, and Finance.  The task names are abbreviated for brevity.", "section": "5.2 Results"}, {"figure_path": "6Kg26g1quR/tables/tables_18_2.jpg", "caption": "Table 4: Comparison of the runtime and number of parameters between algorithms. All algorithms, including baseline methods, were trained for 100K iterations on a single NVIDIA RTX 4090 GPU.", "description": "This table compares the computation time and the number of parameters of four algorithms: ROIDICE, OptiDICE, COptiDICE, and CDT.  All algorithms were trained for 100,000 iterations using a single NVIDIA RTX 4090 GPU.  The comparison highlights the computational efficiency differences between the algorithms, particularly noticeable in the training time for locomotion tasks versus finance tasks.", "section": "G Experiments Compute Resources"}, {"figure_path": "6Kg26g1quR/tables/tables_20_1.jpg", "caption": "Table 5: Results of ROIDICE compared with offline RL algorithms.", "description": "This table presents a comparison of the performance of the ROIDICE algorithm against OptiDICE, an offline RL algorithm.  For various tasks (including locomotion tasks like Hopper, Walker2D, HalfCheetah and financial tasks like F-M and F-H), the table shows the average return (R\u03c0), average accumulated cost (C\u03c0), and ROI achieved by both algorithms.  The results illustrate the trade-off between return and cost achieved by each algorithm, highlighting ROIDICE's focus on maximizing ROI (Return on Investment).", "section": "5.2 ROI Maximization in Continuous Domains"}, {"figure_path": "6Kg26g1quR/tables/tables_21_1.jpg", "caption": "Table 6: Results of offline constrained RL algorithms.", "description": "This table compares the performance of ROIDICE against other offline constrained reinforcement learning algorithms, namely COptiDICE and CDT, across various tasks and dataset qualities.  The results show ROI (Return on Investment), return (R\u03c0), and cost (C\u03c0) for each algorithm and task, with error bars indicating the standard error across multiple trials.  The table highlights the trade-off between return and accumulated cost, showcasing ROIDICE's ability to achieve superior performance in several scenarios.", "section": "5.2 ROI Maximization in Continuous Domains"}, {"figure_path": "6Kg26g1quR/tables/tables_22_1.jpg", "caption": "Table 7: ROI of ROIDICE compared with offline constrained RL algorithms. We average each score and get \u00b12\u00d7 standard error with 5 seeds across 10 episodes.", "description": "This table compares the Return on Investment (ROI) achieved by ROIDICE against three other offline constrained reinforcement learning algorithms (VOCE 50th, VOCE 80th, CPQ 50th, and CPQ 80th) across three different data quality levels (medium, medium-expert, and expert).  The ROI is a measure of the efficiency of a policy, balancing return and accumulated cost.  The results show ROIDICE consistently outperforms the other algorithms across all data quality levels, indicating its effectiveness in optimizing policy efficiency. The '50th' and '80th' suffixes denote the cost constraints used for the constrained algorithms, representing the 50th and 80th percentiles of accumulated cost in the offline dataset.", "section": "I Comparison of Offline Constrained RL Algorithms"}, {"figure_path": "6Kg26g1quR/tables/tables_22_2.jpg", "caption": "Table 8: ROI of ROIDICE compared with offline constrained RL algorithms. We average each score and get \u00b12\u00d7 standard error with 5 seeds across 10 episodes.", "description": "This table presents the Return on Investment (ROI) achieved by ROIDICE and COptiDICE on two safety-related tasks from the OpenAI SafetyGym environment: CarGoal and PointPush.  The results are averaged over five different seeds, each run for ten episodes.  The table shows that ROIDICE outperforms COptiDICE, suggesting that the proposed algorithm is more efficient in these safety-critical scenarios.", "section": "J Experiment Results in Safety RL Environments"}]