[{"type": "text", "text": "Learning Representations for Hierarchies with Minimal Support ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Benjamin Rozonoyer1 Michael Boratko2 Dhruvesh Patel1 Wenlong Zhao1 Shib Dasgupta1 Hung Le1 Andrew McCallum1 ", "page_idx": 0}, {"type": "text", "text": "1University of Massachusetts Amherst 2Google DeepMind {brozonoyer,dhruveshpate,wenlongzhao,ssdasgupta,hungle,mccallum}@cs.umass.edu mboratko@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When training node embedding models to represent large directed graphs (digraphs), it is impossible to observe all entries of the adjacency matrix during training. As a consequence most methods employ sampling. For very large digraphs, however, this means many (most) entries may be unobserved during training. In general, observing every entry would be necessary to uniquely identify a graph, however if we know the graph has a certain property some entries can be omitted - for example, only half the entries would be required for a symmetric graph. ", "page_idx": 0}, {"type": "text", "text": "In this work, we develop a novel framework to identify a subset of entries required to uniquely distinguish a graph among all transitively-closed DAGs. We give an explicit algorithm to compute the provably minimal set of entries, and demonstrate empirically that one can train node embedding models with greater efficiency and performance, provided the energy function has an appropriate inductive bias. We achieve robust performance on synthetic hierarchies and a larger real-world taxonomy, observing improved convergence rates in a resource-constrained setting while reducing the set of training examples by as much as $99\\%$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Consider the directed graph and its associated adjacency matrix in Figure 1. In situations where this adjacency matrix is sparse, we can store it more efficiently by keeping a list of only the positive entries, effectively assuming any pairs not in our list are zero. But what about situations where we cannot assume that we have observed the full graph? For example, when obtaining annotations for edges of an unknown graph, the full adjacency matrix is unknown to us and we obtain the value of any particular entry by requesting an annotation. A less obvious scenario is training a model to represent a given graph. From the model\u2019s perspective, every entry of this adjacency matrix is unknown, and it is only observed as a consequence of training. Therefore, it is of interest to determine: what is the smallest set of entries necessary to uniquely determine the graph? ", "page_idx": 0}, {"type": "image", "img_path": "HFS800reZK/tmp/5982627c79a47a3e0c07268ba56d08f8df3dc7ad5566c0ec9b1f870e1a188da2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "In general, the answer is \u201call of them\u201d, but if we assume some structural prior on the graph itself, it may be possible to reduce the number of edges necessary for consideration. For example, if we knew it was a simple graph (no self-loops) we could omit the diagonal. Similarly, for a symmetric graph, we could omit half the entries. For the class of acyclic graphs, we can omit certain entries which, were they to be 1, would form a cycle, and therefore must be 0; however, the explicit characterization of these entries is not as straightforward as the preceding cases. Focusing on the case when the graph is a transitively-closed directed acyclic graph (DAG), as in Figure 1, it is easy to see that of those entries which are 1, we can omit all but the transitive reduction, i.e. the bold edges, which corresponds to omitting the non-bold \u201c1\u201ds in the adjacency matrix. But what about pruning the zeros? For the graph in Figure 1, we can prove only 14 of the 49 entries in the adjacency matrix are needed to uniquely distinguish this graph among all transitively-closed DAGs. (See Figure 2.) ", "page_idx": 1}, {"type": "text", "text": "In this work, we first develop a general-purpose framework to identify a subset of entries required to uniquely distinguish a graph among others with some arbitrary graph property (Section 3.1). We then use this framework to construct a set of entries in the adjacency matrix sufficient to uniquely distinguish transitively-closed digraphs (Section 3.2), and prove that this reduced set is also minimal for transitively-closed DAGs (Section 3.3). We show how this can be leveraged to more efficiently train node embedding models for graph representation by defining the notion of \u201ctransitivity bias\u201d (Section 4.1), and proving that box embeddings, a common graph representation model, have a transitivity bias (Section 4.3). We then combine these facts into a formal negative sampling procedure (Section 5) and demonstrate that box embeddings do benefit from training on this reduced set of entries (Section 6).1 For related work in graph theory and representation learning, we refer the reader to Appendix A. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We use the shorthand $[\\![n]\\!]:=\\{1,\\ldots,n\\}$ . A semicolon separates the main arguments of a function from parameters which are typically held constant (e.g., $\\begin{array}{r}{f(x;\\mu,\\sigma)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^{2}})}\\end{array}$ . We may omit these secondary parameters when their values are clear from context. We represent a graph $G=(V,E)$ by its adjacency matrix $A_{G}\\in\\{0,1\\}^{N\\times N}$ , where $(A_{G})_{u,v}=1$ if and only if $(u{\\to}v)\\in$ $E$ . When it is clear from context, we omit the subscript and simply write $A$ . We use arrows to represent edges: black $(a{\\rightarrow}b)$ denotes a positive edge in $E$ , and red $(a{\\rightarrow}b)$ a negative edge in the edge complement E (see below). ", "page_idx": 1}, {"type": "text", "text": "2.1 Directed Graphs (Digraphs) ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "All graphs $G=(V,E)$ in this work will be finite simple2 directed graphs (digraphs), where the edges are a subset of the complement of the diagonal, i.e. with $\\operatorname{diag}(V):=\\{(v\\!\\to\\!v)\\mid v\\in V\\}$ , we have $E\\subseteq V^{2}\\setminus\\operatorname{diag}(V)=:\\operatorname{offdiag}(V)$ . Given a simple digraph $G=(V,E)$ , the complement of $G$ is $\\overline{{G}}=(V,\\overline{{E}})$ where ${\\overline{{E}}}:=\\operatorname{offdiag}(V)\\setminus E$ . The transitive closure of $G$ is $G^{\\mathrm{tc}}=(V,E^{\\mathrm{tc}})$ where $(u,v)\\in E^{\\mathrm{tc}}$ if and only if there exists a directed path from $u$ to $v$ in $G$ . A transitive reduction of a digraph $G$ is a digraph $G^{\\prime}$ on $V$ with the fewest number of edges such that $(G^{\\prime})^{\\mathrm{tc}}=G^{\\mathrm{tc}}$ . Note that a transitive reduction need not be a subgraph of $G$ , and in general is not unique. If $G$ is acyclic, however, there is a unique transitive reduction, and it is also a subgraph of $G$ [Aho et al., 1972]. In this case we denote the transitive reduction $G^{\\mathrm{tr}}=(V,E^{\\mathrm{tr}})$ . ", "page_idx": 1}, {"type": "text", "text": "2.2 Node Embeddings for Capturing Graph Structure ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Given the task of modeling entities $V$ that have a known graph-theoretic structure $G\\,=\\,(V,E)$ , a common approach is to learn a node embedding $\\theta:V\\to Z$ which maps a node $v\\mapsto\\theta(v)$ in embedding space $Z$ . The graph structure is extracted from these geometric representations via an energy function $\\mathrm{E}:V\\times V\\rightarrow\\mathbb{R}_{>0}$ which factors through $\\theta$ , i.e. there exists a dissimilarity function $h:Z\\times Z\\to\\mathbb{R}_{\\geq0}$ such that $\\mathrm{E}_{\\theta}(\\bar{u},v):=\\mathrm{E}(u,v;\\theta)=\\bar{h}(\\theta(u),\\theta(v))$ . For example, for undirected graphs it is common to use $Z=\\mathbb{R}^{D}$ and $\\mathrm{E}_{\\theta}(u,v)=\\|\\theta(u)-\\theta(v)\\|$ . This energy is interpreted as the unnormalized negative log-probability of edge existence. We seek to minimize the energy for positive edges $(u{\\to}v)$ by learning representations for which there exists some (global) threshold $T$ such that $A_{u v}=1$ if and only if $\\bar{\\mathrm{E}}_{\\theta}(\\bar{u},v)\\leq T$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "One reason to embed nodes is to learn representations end-to-end in conjunction with other objectives via gradient descent. In such a setting, a typical loss function for the graph structure takes the form ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{full}}(\\theta;G):=\\sum_{(u\\rightarrow v)\\in E}\\ell^{+}(\\mathrm{E}_{\\theta}(u,v))+\\sum_{(u\\rightarrow v)\\in\\overline{{E}}}\\ell^{-}(\\mathrm{E}_{\\theta}(u,v))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\ell^{+}$ and $\\ell^{-}$ are referred to as the positive and negative loss functions, respectively, and the pairs in $E$ and $\\overline{E}$ are referred to as positive and negative edges accordingly. ", "page_idx": 2}, {"type": "text", "text": "The loss function $\\mathcal{L}_{\\mathrm{full}}$ has $|V|(|V|-1)$ terms, and thus is often too computationally demanding to use for training. For digraphs which are sparse, a common workaround is to define a noise probability density function (pdf) $p_{\\mathrm{n}}$ and design a loss function $\\mathcal{L}_{\\mathrm{noise}}$ which replaces the sum over negative edges with an expectation with respect to $p_{\\mathrm{n}}$ . In practice, Monte Carlo sampling is used to calculate a loss $\\mathcal{L}_{\\mathrm{sampled}}$ which approximates $\\mathcal{L}_{\\mathrm{noise}}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Distinguishing Digraphs via Sidigraphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We aim to define a new loss function with only a subset of the terms in $\\mathcal{L}_{\\mathrm{full}}$ (which currently include all entries of the adjacency matrix $A_{G}$ ), while having the same minimizer. Thus, we first attempt to determine the minimal number of entries in $A_{G}$ which would uniquely distinguish $G$ among all those with a given property. To this end, we turn to the notion of a signed digraph (a.k.a. \u201csidigraph\u201d), a digraph where edges have labels $^+$ or \u2212, as a formalism for making explicit the disjoint set of \u201cpositive\u201d and \u201cnegative\u201d edges required to uniquely determine $G$ . Edges not present in this sidigraph will be the entries we can omit from our adjacency matrix, given those edges which are present and some structural prior. ", "page_idx": 2}, {"type": "text", "text": "This framework will allow us to identify the minimal set of entries in the adjacency matrix necessary to uniquely distinguish a digraph among all those with a given property. We apply this framework to transitively-closed digraphs, in which case we prove that certain edges can be pruned. We then prove that, in the case of transitively-closed DAGs, this yields the minimal set of entries. We furthermore provide a concrete algorithm to construct this set of entries from a given adjacency matrix. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary Definitions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "While the depiction of the partial adjacency matrix in Figure 2 is clear, in order to formalize this we need a notion of a graph which has three possible values, e.g. 1, 0, and \u201cmissing\u201d. This is captured formally by the notion of a (simple) signed digraph, or sidigraph for short, which is a triple $(\\bar{V_{\\ast}}E^{+},E^{-})$ where $V$ is the vertex set, and $E^{+},E^{-}$ are disjoint subsets of offdiag $(V)$ , referred to as the positive and negative edges, respectively. ", "page_idx": 2}, {"type": "text", "text": "Definition 1. Given a digraph $G=(V,E)$ we define the equivalent sidigraph $G^{\\pm}:=(V,E,\\overline{{E}})$ . ", "page_idx": 2}, {"type": "text", "text": "This equivalence defines a bijection between digraphs and sidigraphs with $|V|(|V|-1)$ edges. See Figure 3 for a depiction of an equivalent sidigraph of the digraph in Figure 1. ", "page_idx": 2}, {"type": "text", "text": "Let $H$ and $G$ be two sidigraphs with the same vertex set $V$ . We say that $H$ is a sub-sidigraph of $G$ , denoted by $H\\subseteq G$ , if $\\check{E_{H}^{+}}\\subseteq E_{G}^{+}$ and $E_{H}^{-}\\subseteq E_{G}^{-}$ . Given some digraph $G$ , identifying a subset of entries in $A_{G}$ is equivalent to specifying a sub-sidigraph of $G^{\\pm}$ . For this reason, we introduce the following terminology. ", "page_idx": 2}, {"type": "text", "text": "Definition 2. We say that a sidigraph $H$ is a potential distinguisher of a digraph $G$ if $H\\subseteq G^{\\pm}$ . Given a sidigraph $H$ , we define the set of digraphs that could potentially be distinguished by $H$ to be $\\mathcal{G}_{H}:=\\{\\bar{G}\\,\\,|\\,\\,\\bar{H}\\subseteq G^{\\pm}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "That is, $\\mathcal{G}_{H}$ contains every digraph $G$ such that $H$ is a potential distinguisher. Another way to interpret $\\mathcal{G}_{H}$ is that the edge set of every graph $G\\in{\\mathcal{G}}_{H}$ contains all positive edges and no negative edges of $H$ . More formally: ${\\mathcal{G}}_{H}=\\{G\\mid{\\bar{V}}_{G}{\\,\\stackrel{\\textstyle.}{=}}V_{H},E_{H}^{+}\\subseteq E_{G},E_{H}^{-}\\cap E_{G}=\\emptyset\\}$ . ", "page_idx": 2}, {"type": "table", "img_path": "HFS800reZK/tmp/1961340916ca2d39a342611e81b5d101070038bf80a95d993229d2ea3626650f.jpg", "table_caption": [], "table_footnote": ["Figure 3: An adjacency matrix (a) and a representation of the edges in the associated sidigraph (b), where a $^+$ in position $(i,j)$ indicates $(i{\\to}j)\\in E^{+}$ and a \u2212indicates $(i{\\to}j)\\in E^{-}$ . The minimal sidigraph (c) formally captures the fact that Figure 2 has the minimal set of entries in the adjacency matrix to uniquely distinguish $G$ among all transitively-closed DAGs. "], "page_idx": 3}, {"type": "text", "text": "Definition 3. Given some property, we let $\\mathcal{P}$ be a set of all digraphs (on a given set of nodes) with this property. We say the sidigraph $H$ distinguishes a particular digraph $G$ among all those with the given property if $\\mathcal{G}_{H}\\cap\\mathcal{P}=\\bar{\\{G\\}}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Distinguishing Transitively-Closed Digraphs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work, we focus on the case where $\\mathcal{P}$ contains all transitively-closed digraphs, and we let $G=(V,E)\\in\\mathcal{P}$ be some fixed transitively-closed digraph. ", "page_idx": 3}, {"type": "text", "text": "We first focus on reducing the positive edges of $G^{\\pm}$ . An explicit representation of $G$ may well have $\\Omega(|V|^{2})$ edges; however any transitive reduction $G^{\\prime}=(V,\\bar{E}^{\\prime})$ often has substantially fewer edges.3 Furthermore, by definition, $G$ is the only transitively-closed digraph which contains the edges $E^{\\prime}$ . As a consequence, if we let $H=(V,E^{\\prime},\\overline{{E}})$ we have ${\\mathcal{G}}_{H}\\cap{\\mathcal{P}}=\\{G\\}$ , which proves the following result. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Let $G=(V,E)$ be a transitively-closed digraph, and $G^{\\prime}\\,=\\,(V,E^{\\prime})$ a transitive reduction. Then $H=(V,E^{\\prime},\\overline{{E}})$ distinguishes $G$ among transitively-closed digraphs. ", "page_idx": 3}, {"type": "text", "text": "The edge complement $\\overline{E}$ might also have $\\Omega(|V|^{2})$ edges, which we would like to reduce while maintaining distinguishability. To see why this should be possible, consider Figure 4. ", "page_idx": 3}, {"type": "text", "text": "If $(a{\\to}b)\\in E$ and $(a{\\rightarrow}d)\\not\\in E$ then $(b{\\rightarrow}d)\\not\\in E$ , since if it were we would need to include $(a{\\rightarrow}d)$ due to transitivity. Similarly, if $(c{\\rightarrow}d)\\in E$ and $(a{\\rightarrow}d)\\notin E$ then $(a{\\to}c)\\notin E$ , since, if it were, transitivity would imply $(a{\\rightarrow}d)\\in E$ . We formalize this in the following proposition. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2. Let $G=(V,E)$ be a transitively-closed digraph; $H=(V,E_{H}^{+},E_{H}^{-})$ a sidigraph which distinguishes $G$ among transitively-closed digraphs. $\\gamma(a{\\to}d)\\in E_{H}^{-}$ , then 1. $\\textstyle H(a\\!\\to\\!b)\\in E$ , $H^{\\prime}=(V,E_{H}^{+},E_{H}^{-}\\backslash\\{(b{\\rightarrow}d)\\})$ distinguishes $G$ among transitively-closed digraphs. 2. $\\textstyle I\\!f(c{\\rightarrow}d)\\in E$ , $H^{\\prime}=(V,E_{H}^{+},E_{H}^{-}\\backslash\\{(a\\rightarrow c)\\})$ distinguishes $G$ among transitively-closed digraphs. ", "page_idx": 3}, {"type": "text", "text": "Proof. Recall that $H$ distinguishing $G$ among transitively-closed digraphs means that $\\mathcal{G}_{H}\\cap\\mathcal{P}=\\{G\\}$ , i.e. $G=(V,E)$ is the only transitively-closed digraph for which $E_{H}^{+}\\subseteq E$ and $E_{H}^{-}\\cap E=\\emptyset$ . Now, note that $G^{\\prime}=(V,(E_{H}^{+})^{\\mathrm{tc}})$ is the smallest transitively-closed digraph containing the edges $E_{H}^{+}$ , and thus $G^{\\prime}\\subseteq G$ . But this implies $E_{H}^{-}\\cap(E_{H}^{+})^{\\mathrm{tc}}\\subseteq E_{H}^{-}\\cap E=\\emptyset$ , and thus $G^{\\prime}=G$ . ", "page_idx": 3}, {"type": "text", "text": "Now we prove the proposition at hand. We prove the first case, the second follows similarly. Let $(a{\\to}d)\\in E_{H}^{-}$ , assume $(a{\\to}b)\\in E$ , and define $H^{\\prime}=(V,E_{H}^{+},E_{H}^{-}\\setminus\\{(b\\mathrm{\\rightarrow}d)\\})$ . Note that $H^{\\prime}\\subseteq H$ and hence $\\bar{\\mathcal{G}}_{H}^{-}\\subseteq\\mathcal{G}_{H^{\\prime}}$ . ", "page_idx": 3}, {"type": "text", "text": "Now suppose $K\\,=\\,\\bigl(V,E_{K}\\bigr)\\,\\in\\,\\mathcal{G}_{H^{\\prime}}\\cap\\mathcal{P}$ , which implies $K$ is a transitively-closed digraph with $E_{H}^{+}\\subseteq E_{K}$ and $(E_{H}^{-}\\setminus\\{(b{\\rightarrow}d)\\})\\cap E_{K}=\\emptyset$ . In particular, as a consequence of our observation in the first paragraph, this means $(a{\\to}b)\\in E_{K}$ . We prove $(b{\\to}d)\\notin E_{K}$ by contradiction, for if $(b{\\to}d)\\in E_{K}$ , then since $K$ is transitively-closed we would have $(a{\\to}d)\\in E_{K}$ which violates our preliminary assumption. \u53e3 ", "page_idx": 3}, {"type": "text", "text": "These two simple prunings, illustrated in Figure 4, lead us to formulate Algorithm 1, FINDMINDISTINGUISHER, which repeatedly removes edges using Proposition 2 until no more can be removed. ", "page_idx": 4}, {"type": "image", "img_path": "HFS800reZK/tmp/6473cc35acf6d87ad21388487235a7608b43521c2cca7f9e023045315e0a64a9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3 Optimality of FINDMINDISTINGUISHER ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We note that $H^{*}=$ FINDMINDISTINGUISHER $(G)$ is only defined for DAGs, and indeed for graphs with cycles it is not possible to uniquely define such a sidigraph. (Recall that the transitive closure is not unique for graphs with cycles). In the event that $G$ is acyclic, however, we can prove that $H^{*}$ is not just capable of distinguishing $G$ among transitively-closed digraphs, but moreover it is the sidigraph with the minimum number of edges capable of doing so! Below is a proof sketch for the informal statement. For the full proof refer to Theorem 1 in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Proof sketch: The basic idea is to define a partially ordered set, a.k.a. poset, on the set of negative edges of $G^{\\pm}$ . We then show that the set of all minimal elements in this poset is necessary and sufficient to distinguish $G$ among transitively-closed digraphs. Finally, we observe that our algorithm in the previous section produces the set of all minimal elements and hence its optimality follows. ", "page_idx": 4}, {"type": "text", "text": "4 Leveraging Sufficient Sidigraphs for Training Node Embeddings ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We would like to leverage a distinguishing sidigraph for more efficient training and improved accuracy of energy-based node embedding models. The prerequisite is an energy function with a useful inductive bias for the digraph property under consideration. In the extreme case of an inductive bias which only permits models capable of representing digraphs in $\\mathcal{P}$ we should be able to train using only the positive and negative edges from any $H$ which can distinguish $G$ among those in $\\mathcal{P}$ . In some instances, depending on the proportion $[|E_{H^{*}}^{+}|+|E_{H^{*}}^{-}|]/[|V|(|V|-1)]$ , this may allow full training on digraphs which would otherwise require sampling. As we show, sampling can still be used in conjunction with the minimal edge set implied by a sidigraph, and in such situations we expect not only increased efficiency but increased performance, as ${\\mathcal{L}}_{\\mathrm{sampled}}(\\theta)$ will better approximate $\\bar{\\mathcal{L}}_{\\mathrm{full}}(\\theta)$ . ", "page_idx": 4}, {"type": "text", "text": "4.1 Transitivity Bias ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "First, we formally define \u201ctransitivity bias\u201d. ", "page_idx": 4}, {"type": "text", "text": "Definition 4. Let $Z$ be an embedding space. We say that an energy function $\\operatorname{E}_{\\theta}$ has transitivity bias if, for all embeddings $\\theta:V\\to Z$ , there exists some threshold $T\\geq0$ s.t. for all $u,v,w\\in V$ , the inequalities $\\mathrm{E}_{\\theta}(u,v),\\mathrm{E}_{\\theta}(v,w)\\le T$ imply $\\operatorname{E}_{\\theta}(u,w)\\leq T$ . ", "page_idx": 4}, {"type": "text", "text": "We illustrate transitivity bias using the following overly simple embedding model. ", "page_idx": 4}, {"type": "text", "text": "Example 1 (Bit Vectors). Let $Z\\,=\\,\\{0,1\\}^{|V|}$ , and $\\begin{array}{r}{\\mathrm{E}_{\\mathrm{BV}}(u,v;\\theta)\\,:=\\,-\\log\\frac{\\theta(u)\\cdot\\theta(v)}{\\theta(v)\\cdot\\theta(v)}}\\end{array}$ . Then $\\operatorname{E}_{\\mathrm{BV},\\theta}$ has a transitivity bias using threshold $T\\ =\\ 0$ , as $\\operatorname{E}_{\\mathrm{BV}}(u,v;\\theta)~=~0$ if and only if $\\forall i\\in V$ , $\\theta(v)_{i}=1\\implies\\^{\\cdot}\\theta(u)_{i}=1$ . Thus, if $\\operatorname{E}_{\\mathrm{BV}}(u,v;\\theta)\\,=\\,\\operatorname{E}_{\\mathrm{BV}}(v,w;\\theta)\\,=\\,0$ we have $\\theta(w)_{i}\\,=\\,1\\;\\implies$ $\\theta(v)_{i}=1\\implies\\theta(u)_{i}=1$ , hence $\\operatorname{E}_{\\mathrm{BV}}(u,w;\\theta)=0$ . ", "page_idx": 4}, {"type": "text", "text": "For a given digraph $G=(V,E)$ , we define the bit vector representation of $G$ as $\\theta_{\\mathrm{BV}}:V\\to\\{0,1\\}^{|V|}$ , where the $i^{\\mathrm{th}}$ (for $i\\in V$ ) coordinate is given by ", "page_idx": 4}, {"type": "text", "text": "Proposition 3. Let $G=(V,E)$ be any digraph, then $\\mathrm{E}_{\\mathrm{BV}}(u,v;\\theta_{\\mathrm{BV}})=0$ if and only $i f(u,v)\\in E^{t c}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. Let $(u,v)\\in E^{\\mathrm{tc}}$ , then there is some path $u=w_{1}\\rightarrow w_{2}\\rightarrow\\cdots\\rightarrow w_{n-1}\\rightarrow w_{n}=v$ in $E$ . By the definition of $\\theta_{\\mathrm{BV}}^{G}$ , we have that $\\mathrm{E_{BV}}(w_{i},w_{i+1};\\theta_{\\mathrm{BV}}^{G})=0$ for $i\\in[n]$ , and thus by the transitivity bias observation made in Example 1 this implies $\\mathrm{E}_{\\mathrm{BV}}(u,v;\\theta_{\\mathrm{BV}}^{G})=0$ . ", "page_idx": 5}, {"type": "text", "text": "Now assume $(u,v)\\ \\notin\\ E^{\\mathrm{tc}}$ , then $v$ is not a descendant of $u$ , which means $\\theta_{\\mathrm{BV}}^{G}(u)_{v}\\;=\\;0$ while $\\theta_{\\mathrm{BV}}^{G}(v)_{v}=1$ , and hence $\\mathrm{E}_{\\mathrm{BV}}(u,v;\\theta_{\\mathrm{BV}}^{G})>0$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "With an appropriate threshold, any energy function that has transitivity bias in fact represents a transitively-closed digraph: ", "page_idx": 5}, {"type": "text", "text": "Proposition 4. If $\\operatorname{E}_{\\theta}$ is an energy function with transitivity bias, then for any $\\theta$ there exists a $T\\geq0$ such that the digraph with edges $\\{(u,v)\\mid\\operatorname{E}_{\\theta}(u,v)\\leq T\\}$ is transitively closed. ", "page_idx": 5}, {"type": "text", "text": "This allows us to formalize the notion that training on any $H$ which can distinguish $G$ among transitively-closed digraphs is sufficient. ", "page_idx": 5}, {"type": "text", "text": "Proposition 5. Let $G$ be a transitively-closed digraph, $\\operatorname{E}_{\\theta}$ an energy function with transitivity bias, and $\\[H=(V,E_{H}^{+},E_{H}^{-})$ a sidigraph which distinguishes $G$ among transitively closed digraphs. If $T$ is the threshold associated with the transitivity bias for $\\theta$ , $\\mathrm{E}_{\\theta}(u,v)\\le T$ for all $(u,v)\\in E_{H}^{+}$ , and $\\mathrm{E}_{\\theta}(u,v)>T$ for all $(u,v)\\in E_{H}^{-}$ , then $\\theta$ represents the digraph $G$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Box Embeddings and T-BOX ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We would like our energy function to be a representation which is tractable and trainable via gradientdescent, which requires the space $Z$ to have differentiable structure. Box embeddings [Vilnis et al., 2018] are a trainable region-based embedding method which demonstrate strong performance for representing digraphs [Boratko et al., 2021a, Zhang et al., 2022]. We provide the requisite background on box embeddings and define the specific model that we will use for our experiments. ", "page_idx": 5}, {"type": "text", "text": "As introduced in Vilnis et al. [2018], box embeddings represent entities using a box or hyperrectangle in $\\mathbb{R}^{D}$ , i.e., a Cartesian product of intervals ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\prod_{d=1}^{D}[x_{d}^{\\mathsf{^{L}}},x_{d}^{\\mathsf{^{\\tau}}}]=[x_{1}^{\\mathsf{^L}},x_{1}^{\\mathsf{^T}}]\\times\\ldots\\times[x_{D}^{\\mathsf{^L}},x_{D}^{\\mathsf{^T}}]\\subseteq\\mathbb{R}^{D},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $x_{d}^{\\scriptscriptstyle{\\downarrow}}<x_{d}^{\\scriptscriptstyle{\\top}}$ for $d\\in[D]$ . Vilnis et al. [2018] proposed modeling a directed graph such that boxes of parents contain their  chil dren with an energy function ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{E}_{\\mathrm{Box}}(u,v;\\theta):=-\\log\\prod_{d=1}^{D}F_{\\mathrm{Box}}(\\theta(u)_{d},\\theta(v)_{d}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the per-dimension parameters are endpoints of an interval, i.e. $\\theta(u)_{d}=[\\theta(u)_{d}^{\\perp},\\theta(u)_{d}^{\\top}]$ , and the per-dimension score is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{\\mathrm{Box}}((x^{\\varepsilon},x^{\\top}),(y^{\\varepsilon},y^{\\top})):=\\frac{|[x^{\\varepsilon},x^{\\top}]\\cap[y^{\\varepsilon},y^{\\top}]|}{|[y^{\\varepsilon},y^{\\top}]|}=\\frac{\\operatorname*{max}(\\operatorname*{min}(x^{\\top},y^{\\top})-\\operatorname*{max}(x^{\\varepsilon},y^{\\bot}),0)}{\\operatorname*{max}(y^{\\top}-y^{\\bot},0)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Previous works have highlighted the difficulty of optimizing an objective including these hard min and max functions [Li et al., 2019, Dasgupta et al., 2020]. We use the Global T-BOX model, the most recent solution to this issue, introduced in Boratko et al. [2021a]. Global T-BOX (or GT-BOX) softens the volume calculation by replacing the hard min and max operators with a smooth approximation $\\begin{array}{r}{\\mathrm{LSE}_{t}(\\mathbf{x}):=t\\log(\\sum_{i}e^{x_{i}/t})}\\end{array}$ . The per-dimension score function is then given by ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{\\mathrm{GT-Box}}((x^{\\mathrm{c}},x^{\\mathrm{\\top}}),(y^{\\mathrm{c}},y^{\\mathrm{\\top}});\\tau,\\nu):=\\frac{\\mathrm{LSE}_{\\nu}(\\mathrm{LSE}_{-\\tau}(x^{\\mathrm{\\top}},y^{\\mathrm{\\top}})-\\mathrm{LSE}_{\\tau}(x^{\\mathrm{\\bot}},y^{\\mathrm{\\bot}}),0)}{\\mathrm{LSE}_{\\nu}(y^{\\mathrm{\\top}}-y^{\\mathrm{\\bot}},0)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which approximates $F_{\\mathrm{BOX}}$ for sufficiently small $\\tau,\\nu>0$ . These $\\tau,\\nu$ are additional (global) trainable parameters of the model. ", "page_idx": 5}, {"type": "text", "text": "4.3 Transitivity Bias of Box Embeddings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While the motivation and formulation of the energy for box embedding functions is quite different than that of the naive bit vector model in Example 1, it actually is more similar than it may at first appear. ", "page_idx": 6}, {"type": "text", "text": "Remark 1. As observed in Boratko et al. [2021b], the box intersection volume calculation can actually be viewed as the $L^{2}$ inner-product of characteristic functions of boxes, in which case it takes an identical form to the energy function for bit vectors. For any functions $f,g\\in L^{2}(\\mathbb{R}^{D})$ , the standard inner product is $\\langle f,g\\rangle=\\textstyle\\!\\!\\int_{\\mathbb{R}^{D}}f(x)g(x)\\,d x$ . Now, for any box $B\\subseteq\\mathbb{R}^{D}$ , we let $\\mathbb{1}_{B}(\\boldsymbol{x})$ be the characteristic function, which is $1$ when $x\\in B$ and 0 otherwise. Then the volume of intersection of boxes is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{Vol}(A\\cap B)=\\langle\\mathbb{1}_{A}(x),\\mathbb{1}_{B}(x)\\rangle,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which is also valid if $A=B$ . Thus, we can write the energy function ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname{E}_{\\mathrm{Box}}(u,v;\\theta)=-\\log{\\frac{\\langle\\mathbb{1}_{\\theta(u)},\\mathbb{1}_{\\theta(v)}\\rangle}{\\langle\\mathbb{1}_{\\theta(v)},\\mathbb{1}_{\\theta(v)}\\rangle}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 2. Box embeddings can be quantized into bit vectors in such a way that applying the bit vector energy function to the resulting quantizations preserves the set of node pairs which have zero energy. Given a box embedding $\\theta$ , for each $d\\in[\\![D]\\!]$ let $T_{d}$ be the endpoints of boxes in dimensions $d$ , i.e. $T_{d}:=\\cup_{v\\in\\mathcal{V}}\\{\\theta(v)_{d}^{-},\\theta(v)_{d}^{+}\\}$ . Let $\\begin{array}{r}{M_{d}:=|T_{d}|}\\end{array}$ ,  and assign indices $T_{d}=:\\{t_{d,m}\\}_{m=1}^{M_{d}}$ such that $t_{d,m}\\leq t_{d,m+1}$ for $m\\in\\left[\\![M_{d}-1]\\!\\right]$ . Then, for each and $d\\in[\\![D]\\!]$ , form the $M_{d}$ -dimensional vector $\\varphi(v)_{d,m}:={\\cal F}_{\\mathrm{Box}}(\\theta(v)_{d},(t_{d,m},t_{d,m+1}))$ . By construction,  this  value will be either 0 or 1. Letting $\\varphi(v)\\in\\{0,1\\}^{\\sum M_{d}}$ be the concatenation of $\\{\\varphi(v)_{d}\\}_{d=1}^{D}$ , we obtain a bit vector representation, for which the energy function $\\begin{array}{r}{\\operatorname{E}(u,v;\\varphi)=-\\log{\\frac{\\varphi(u)\\cdot\\varphi(v)}{\\varphi(v)\\cdot\\varphi(v)}}}\\end{array}$ og \u03c6\u03c6((vu))\u00b7\u00b7\u03c6\u03c6((vv)) is such that E(u, v; \u03c6) = 0 if and only if $\\varphi(v)_{d,m}=1\\implies\\varphi(u)_{d,m}=1$ , which is true if and only if $[\\theta(v)_{d}^{+},\\theta(v)_{d}^{+}]\\subseteq[\\theta(u)_{d}^{-},\\theta(u)_{d}^{+}]$ . ", "page_idx": 6}, {"type": "text", "text": "Most importantly for our purposes, however, is the following proposition. ", "page_idx": 6}, {"type": "text", "text": "Proposition 6. The energy function $\\operatorname{E}_{\\mathrm{Box},\\theta}$ has a transitivity bias. ", "page_idx": 6}, {"type": "text", "text": "Apart from prior empirical observations that box embeddings work well to embed transitively-closed DAGs Boratko et al. [2021a], Proposition 5 suggests it should be possible to train box embeddings on the output of FINDMINDISTINGUISHER to represent a transitively-closed DAG. In practice, we train the smooth approximation provided by GT-BOX. As $\\tau,\\nu\\to0^{+}$ , which often happens naturally during training Boratko et al. [2021a], we expect it to capture the transitivity even when trained only on the positive and negative edges of the distinguisher provided by the algorithm. ", "page_idx": 6}, {"type": "text", "text": "5 Hierarchy-Aware Sampling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section we formally how we sample edges for our loss function, which we term hierarchy-aware sampling. Based on the results in the preceding section, given a transitively-closed digraph $G$ , if $\\operatorname{E}_{\\theta}$ has a transitive bias and the sidigraph $\\bar{H}=(V,\\bar{E}_{H}^{-},E_{H}^{+})$ can distinguish $G$ among transitively-closed digraphs, then we can train using the following loss function: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal L_{\\mathrm{ha}}(\\theta;H):=\\sum_{(u\\rightarrow v)\\in E_{H}^{+}\\atop(u\\rightarrow v)\\in E_{H}^{+}}\\ell^{+}(\\mathrm{E}_{\\theta}(u,v))\\,+\\sum_{(u\\rightarrow v)\\in E_{H}^{-}}\\ell^{-}(\\mathrm{E}_{\\theta}(u,v)).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In particular, for a transitively-closed DAG $G$ , we can use $H^{*}$ as returned by Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "If the cardinality of $H^{*}$ is small enough, this may make training using $\\mathcal{L}_{\\mathrm{ha}}$ feasible. In general, however, we still may need to sample negative edges using some noise distribution, as mentioned in Section 2.2. In practice, we will compare using the sidigraphs $G^{\\pm}$ and $\\begin{array}{r l}{H_{*}}&{{}=}\\end{array}$ FINDMINDISTINGUISHER $(G)$ , which means positives will be sampled from either $E$ or $E^{\\mathrm{tr}}$ , and negatives will be sampled from $\\overline{E}$ or $E_{H^{*}}^{-}$ . Even with our reduced set of negatives, there are still far more negatives than positives, and so we adopt the common practice of sampling $k$ negatives for every positive within a batch. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The main aim of our experiments is to compare the efficacy on graph representation learning of models with vs without transitivity bias, and random uniform vs hierarchy-aware sampling. As per Proposition 5, we hypothesize that the greatly reduced edge set in hierarchy-aware sampling will be sufficient to faithfully represent the graphs in the embedding space if our model has transitivity bias. ", "page_idx": 7}, {"type": "text", "text": "Data. We evaluate hierarchy-aware sampling on the heterogeneous synthetic DAGs for which Boratko et al. [2021a] demonstrated superior performance using GT-BOX and random uniform negative sampling: balanced trees, where $b$ is the branching factor, the nested Chinese restaurant process (nCRP) [Blei et al., 2010], where $\\alpha$ is the normalized \u201cnew table\u201d probability, and Price\u2019s model [Price, 1976], where $m$ is the number of connections for a new node and $c$ is a constant factor added to the probability of a vertex receiving an edge4. We also test on the larger real-world Medical Subject Headings (MeSH) taxonomy Lipscomb [2000], 2020 release. For detailed statistics about all graphs we refer the reader to Table 1 in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "Models. To evaluate the importance of the model having transitivity bias in order to take advantage of this reduced set of edges, we consider the vector similarity model SIM-VEC which represents each node by an \u201cin\u201d and \u201cout\u201d vector, and computes edges via dot-product, $E_{\\mathrm{SIM-VEC}}(\\bar{u},v):=$ $-\\log(\\sigma(\\theta(u)_{\\mathrm{out}}\\cdot\\theta(v)_{\\mathrm{in}}))$ . Unlike GT-BOX, SIM-VEC does not have transitivity bias Boratko et al. [2021a]. For all of our experiments we fix the dimension to 64; this corresponds to 64-dimensional \u201cin\u201d and \u201cout\u201d embeddings for SIM-VEC, and to the two 64-dimensional corners $x^{\\perp}$ and $x^{\\top}$ of GT-BOX. This makes the number of parameters per node for both models 128. For details on hyperparameter tuning, see Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Support and Sampling. For the synthetic graphs we iterate over settings relating to the support sets from which to uniformly randomly sample positive and negative training edges, respectively: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Sampling positive edges from the transitive reduction $E^{\\mathrm{tr}}$ vs transitive closure $E^{\\mathrm{tc}}$ . \u2022 Sampling negative edges from minimal hierarchy-aware set $E_{H^{*}}^{-}$ vs edge complement $\\overline{E}$ . \u2022 Negative sampling ratio $\\bar{k}=4$ vs $k=128$ (i.e. per each positive edge seen by the model, $k$ negative edge sampled uniformly randomly from the support set also get seen). ", "page_idx": 7}, {"type": "text", "text": "Setting $k=4$ mimics the tiny proportion of sampled negatives to the pool of all possible negatives, a limitation which we expect when scaling to larger graphs, such as MeSH. ", "page_idx": 7}, {"type": "text", "text": "Evaluation. Since we are evaluating the representational capacity of each experimental setting (as opposed to generalization on some held-out edge set), our metric is the F1 between the edges of the transitively-closed DAG we are modeling and the model\u2019s scores for those edges. In other words, we are performing binary classification over the full adjacency matrix for the hierarchy in question. ", "page_idx": 7}, {"type": "text", "text": "We investigate the impact of the available positive and negative support sets on model training; since the representational capacities of vector and box models are well-studied, we are not interested in the best F1 score attainable by these models, but the effect of respective support sets on convergence. To compare across experimental settings, we plot F1 as a function of the number of total examples (both positive and negative edges) processed by the model (e.g. Figure 6). To measure the final performance and rate of convergence, we use the area under the F1 vs. Total Examples curve (AUF1C). ", "page_idx": 7}, {"type": "text", "text": "6.1 Results and Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "First note that GT-BOX universally outperforms SIM-VEC on the graph modeling experiments. This is not surprising, as Boratko et al. [2021a] demonstrate the superior inductive bias of GT-BOX for modeling digraphs generally. The aim of our experiments, however, is to investigate the impact of a hierarchy\u2019s structure on the convergence for the various sampling settings for GT-BOX specifically. With this aim, in Figure 5, we present the AUF1C for the convergence curve (area under F1 vs. number of examples) in the limited-example setting of $k\\,=\\,4$ . In this regime random uniform negative sampling may not be able iterate over all of $\\overline{E}$ in a reasonable amount of time, whereas, to the contrary, it is possible to get through all of $E_{H^{*}}^{-}$ in a limited number of epochs, since in many cases the sampling pool is reduced by more than $99\\%$ (cf. Table 1). ", "page_idx": 7}, {"type": "image", "img_path": "HFS800reZK/tmp/6320efb6945507b99827764d88252abe0466522a994c782c61ebae326c55ee67.jpg", "img_caption": ["Figure 5: Convergence measured using AUF1C on three graph types Figure 6: While GT-BOX (black squares) with a transitivity bias fpoors iStiIvMe -aVnEd Cn eagnadt ivGeT -edBgOeXs  pmroovdiedlse dw dituhr itnhge  tfrualil noinr gr,e rdeuscpeedc tsievte loyf. takedso efsu lnl oatd hvaavnte aag etr aofn stihtiev ihtiye rbairacsh, yf-aallws aarpe does not have a transitivity bias,falls apart under this negative $E_{H^{*}}^{-}$ ,e rS IthMi-s VneEgCa, tiwvheich Pos. $\\b=\\bullet$ (resp. $\\circleddash$ implies usage of transitive closure (resp. sampling procedure. The reduction in negative examples from using transitive reduction). Analogously, for Neg., the symbols imply the pruned negative edges for this particular graph is $95.83\\%$ . usage of the set of the edge complement $\\overline{E}$ and $E_{H^{*}}^{-}$ , respectively. Negative ratio $k=4$ throughout. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "HFS800reZK/tmp/f9cf3e5cb30a4a20ae8c7fb291778249e6dbf2dca8eb18641a2170fe0ed712c4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "HFS800reZK/tmp/d2ccfe5a335d266ead2d5dd441b304ad66e324324986e7bcc7438a9ced261846.jpg", "img_caption": ["Figure 7: The plots show convergence of GT-BOX for negative ratio $k=4$ . The top row shows the plots for balanced trees with branching factors $b=2$ , 5 and 10. The middle row for nCRP graphs with $\\alpha=10$ , 100, and 500, respectively going left to right. The bottom row shows the plots for Price\u2019s graph with $c=0.1$ , $\\gamma=1.0$ and three values of $m=1$ , 5, and 10. The number of vertices in each graph is $\\approx2^{13}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5 demonstrates a striking trend observed consistently in our experiments. While GT-BOX with our reduced edge set performs on par or better than with the original edge set, in contrast, the performance of SIM-VEC degrades significantly. We demonstrate this using a representative example in Figure 6, where, in stark contrast to GT-BOX taking advantage of $E_{H^{*}}^{-}$ and outperforming all SIM-VEC settings, both SIM-VEC settings that use $E_{H^{*}}^{-}$ fail to converge even to $0.4\\,\\mathrm{F}1$ . This dichotomy hearkens back to Proposition 5, which underlines the non-trivial synergy between an energy function with transitivity bias and the hierarchy-aware output of FINDMINDISTINGUISHER. ", "page_idx": 8}, {"type": "text", "text": "Balanced tree. Figure 11 Row 1 visualizes the impact on convergence of increasing the branching factor. As branching factor $b$ increases $2<5<10$ , we first note that the balanced tree with fixed $|V|$ becomes more and more shallow. While $E_{H^{*}}^{-}$ performs well on all values of $b$ , it particularly stands apart from $\\overline{E}$ in the $b=10$ setting. ", "page_idx": 9}, {"type": "text", "text": "nCRP. $\\alpha$ is the normalized \u201cnew table\u201d probability, with a smaller $\\alpha$ implying more separate clusters with a deeper hierarchy, and a larger $\\alpha$ implying fewer clusters and a more shallow hierarchy. The relative improvement of using $E_{H^{*}}^{-}$ is more significant for smaller $\\alpha$ . ", "page_idx": 9}, {"type": "text", "text": "Price. Price\u2019s model is learnable by GT-BOX especially quickly under any setting, as evidenced by the very high AUF1C scores in Figure 5. Higher $c$ has the effect of making edge attachment more uniformly distributed among nodes. In Figure 11 Row 3 we examine convergence values for $c=0.1$ , noting that $E_{H^{*}}^{-}$ typically underperforms $\\overline{E}$ but catches up in the limit. As $m$ (out-degree of newly added vertices) increases $1<5<10$ , looking at Table 1 we note that hierarchy-aware negative pruning also gets more aggressive as $52.35\\%<89.62\\%<93.77\\%$ . ", "page_idx": 9}, {"type": "text", "text": "MeSH. As seen in Figure 8, on the larger real-world MeSH taxonomy, not only does GT-BOX outperform SIM-VEC, but our minimal negative edge set $E_{H^{*}}$ within GT-BOX outperforms sampling from the edge complement, while being a $99.78\\%$ reduction. Meanwhile, $E_{H^{*}}$ with $\\mathrm{SIM-VEC}$ plummets, consistently with the trend exhibited in Figure 6. This encouraging result on a graph with $\\approx2^{15}$ nodes points not only to the economy of our approach, but to the stability and improved performance on large real-world data. ", "page_idx": 9}, {"type": "text", "text": "7 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While the connection between hierarchies and transitivity bias allows us to capitalize on it in the form of hierarchy-aware sampling, we acknowledge that the properties demanded of both the data and model are restricted to transitivity. While this does encompass a large variety of relationships observed in real-world graphs, this specific algorithm does not extend easily to new combinations of graph properties and inductive biases, which is a goal of future work. ", "page_idx": 9}, {"type": "text", "text": "Another limitation is the extent to which this sort of method would break down for graphs which are, strictly speaking, not transitively-closed, but close (in edit distance) to being transitivelyclosed. Strictly speaking, our proofs do not apply in that setting, and the efficacy of the approach may vary depending on the type of structural changes a particular removal of an edge brings about. ", "page_idx": 9}, {"type": "image", "img_path": "HFS800reZK/tmp/4b30653367238fb687619473760c1f1bbceb3396801c215ad838785f0c5b5389.jpg", "img_caption": ["Figure 8: Convergence of GT-BOX vs SIM-VEC for $\\overline{E}$ vs $E_{H^{*}}$ on MeSH 2020. The minimal negative edge set $E_{H^{*}}$ converges to the highest F1 when coupled with GT-BOX, but falls apart when combined with SIM-VEC, consistently with our hypothesis about the requirement of transitivity bias for utilizing $E_{H^{*}}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a novel framework for identifying a sufficient subset of entities in the adjacency matrix which unambiguously specify a digraph, given some prior structural knowledge. We demonstrate the usability of this framework for the property of transitively-closed DAGs, or hierarchies. We derive a characterization of the sufficient negative set for such type of graphs, and based on that we devise a novel hierarchy-aware sampling technique. Our approach is efficient and robust when used in conjunction with an energy-based node embedding model possessing the appropriate inductive bias, which, for hierarchies, is transitivity bias. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by IBM Research AI through the AI Horizons Network, the Chan Zuckerberg initiative under the project Scientific Knowledge Base Construction, and National Science Foundation (NSF) grant number IIS-1922090. Hung Le was supported by an NSF CAREER Award No. CCF-223728, an NSF Small Grant No. CCF-2121952, and a Google Research Scholar Award. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Alfred V. Aho, Michael R Garey, and Jeffrey D. Ullman. The transitive reduction of a directed graph. SIAM Journal on Computing, 1(2):131\u2013137, 1972.   \nLuke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. Probabilistic embedding of knowledge graphs with box lattice measures. ACL, 2018.   \nMichael Boratko, Dongxu Zhang, Nicholas Monath, Luke Vilnis, Kenneth Clarkson, and Andrew McCallum. Capacity and bias of learned geometric embeddings for directed graphs. NeurIPS, 2021a.   \nDongxu Zhang, Michael Boratko, Cameron Musco, and Andrew McCallum. Modeling transitivity and cyclicity in directed graphs via binary code box embeddings. Advances in Neural Information Processing Systems, 35:10587\u201310599, 2022.   \nXiang Li, Luke Vilnis, Dongxu Zhang, Michael Boratko, and Andrew McCallum. Smoothing the geometry of probabilistic box embeddings. ICLR, 2019.   \nShib Sankar Dasgupta, Michael Boratko, Dongxu Zhang, Luke Vilnis, Xiang Li, and Andrew McCallum. Improving local identifiability in probabilistic box embeddings. NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 01c9d2c5b3ff5cbba349ec39a570b5e3-Abstract.html.   \nMichael Boratko, Javier Burroni, Shib Sankar Dasgupta, and Andrew McCallum. Min/max stability and box distributions. UAI, pages 2146\u20132155, 2021b.   \nDavid M Blei, Thomas L Griffiths, and Michael I Jordan. The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies. Journal of the ACM (JACM), 57(2):1\u201330, 2010.   \nDerek de Solla Price. A general theory of bibliometric and other cumulative advantage processes. Journal of the American society for Information science, 27(5):292\u2013306, 1976.   \nCarolyn E Lipscomb. Medical subject headings (mesh). Bulletin of the Medical Library Association, 88(3):265, 2000.   \nZhen Yang, Ming Ding, Chang Zhou, Hongxia Yang, Jingren Zhou, and Jie Tang. Understanding Negative Sampling in Graph Representation Learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD \u201920, pages 1666\u20131676, New York, NY, USA, August 2020. Association for Computing Machinery. ISBN 978-1-4503-7998-4. doi: 10.1145/3394486.3403218. URL https://doi.org/10.1145/3394486.3403218.   \nMichael U. Gutmann and Aapo Hyv\u00e4rinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. The Journal of Machine Learning Research, 13(null):307\u2013361, February 2012. ISSN 1532-4435.   \nAndriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models, 2012.   \nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_ files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf.   \nHidetaka Kamigaito and Katsuhiko Hayashi. Comprehensive Analysis of Negative Sampling in Knowledge Graph Representation Learning, July 2022. URL http://arxiv.org/abs/2206. 10140. arXiv:2206.10140 [cs].   \nYu Wang, Liang Hu, Wanfu Gao, Xiaofeng Cao, and Yi Chang. AdaNS: Adaptive negative sampling for unsupervised graph representation learning. Pattern Recognition, 136:109266, April 2023. ISSN 0031-3203. doi: 10.1016/j.patcog.2022.109266. URL https://www.sciencedirect. com/science/article/pii/S0031320322007452.   \nKian Ahrabian, Aarash Feizi, Yasmin Salehi, William L. Hamilton, and Avishek Joey Bose. Structure Aware Negative Sampling in Knowledge Graphs, October 2020. URL http://arxiv.org/abs/ 2009.11355. arXiv:2009.11355 [cs, stat].   \nJunyang Chen, Zhiguo Gong, Wei Wang, and Weiwen Liu. HNS: Hierarchical negative sampling for network representation learning. Information Sciences, 542:343\u2013356, January 2021. ISSN 0020- 0255. doi: 10.1016/j.ins.2020.07.015. URL https://www.sciencedirect.com/science/ article/pii/S0020025520306770.   \nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. DeepWalk: Online Learning of Social Representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710, August 2014. doi: 10.1145/2623330.2623732. URL http://arxiv.org/abs/1403.6652. arXiv:1403.6652 [cs].   \nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating Embeddings for Modeling Multi-relational Data. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://papers.nips. cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html.   \nYankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning Entity and Relation Embeddings for Knowledge Graph Completion. Proceedings of the AAAI Conference on Artificial Intelligence, 29(1), February 2015. ISSN 2374-3468, 2159-5399. doi: 10.1609/aaai.v29i1.9491. URL https://ojs.aaai.org/index.php/AAAI/article/view/9491.   \nMaximilian Nickel, Volker Tresp, Hans-Peter Kriegel, et al. A three-way model for collective learning on multi-relational data. In Icml, volume 11, pages 3104482\u20133104584, 2011.   \nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding Entities and Relations for Learning and Inference in Knowledge Bases, August 2015. URL http://arxiv. org/abs/1412.6575. arXiv:1412.6575 [cs].   \nTh\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. Complex Embeddings for Simple Link Prediction. 2016. doi: 10.48550/ARXIV.1606.06357. URL https: //arxiv.org/abs/1606.06357. Publisher: arXiv Version Number: 1.   \nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space, February 2019a. URL http://arxiv.org/abs/1902. 10197. arXiv:1902.10197 [cs, stat].   \nTengwei Song, Jie Luo, and Lei Huang. Rot-pro: Modeling transitivity by projection in knowledge graph embedding. Advances in Neural Information Processing Systems, 34:24695\u201324706, 2021.   \nDhruvesh Patel, Shib Sankar Dasgupta, Michael Boratko, Xiang Li, Luke Vilnis, and Andrew McCallum. Representing joint hierarchies with box embeddings. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id $\\equiv$ J246NSqR_l.   \nFred S Roberts. On the boxicity and cubicity of a graph. Recent progress in combinatorics, 1(1): 301\u2013310, 1969.   \nHiroshi Maehara. Space graphs and sphericity. Discrete Applied Mathematics, 7(1):55\u201364, 1984.   \nDhruvesh Patel, Pavitra Dangati, Jay-Yoon Lee, Michael Boratko, and Andrew McCallum. Modeling label space interactions in multi-label classification using box embeddings. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= tyTH9kOxcvh.   \nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. ICLR, 2016.   \nAlice Lai and Julia Hockenmaier. Learning to predict denotational probabilities for modeling entailment. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 721\u2013730, 2017.   \nRyota Suzuki, Ryusuke Takahama, and Shun Onoda. Hyperbolic disk embeddings for directed acyclic graphs. International Conference on Machine Learning (ICML), 2019.   \nR. Sarkar. Low distortion delaunay embedding of trees in hyperbolic plane. Graph Drawing, 2011.   \nMelanie Weber and Maximilian Nickel. Curvature and representation learning: Identifying embedding spaces for relational data. NeurIPS Relational Representation Learning, 2018.   \nMelanie Weber. Neighborhood growth determines geometric priors for relational representation learning. Conference on Artificial Intelligence and Statistics (AISTATS), 2020.   \nMaximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 2015.   \nMarc Law, Renjie Liao, Jake Snell, and Richard Zemel. Lorentzian distance learning for hyperbolic representations. International Conference on Machine Learning (ICML), 2019.   \nOctavian-Eugen Ganea, Gary B\u00e9cigneul, and Thomas Hofmann. Hyperbolic Entailment Cones for Learning Hierarchical Embeddings, June 2018. URL http://arxiv.org/abs/1804.01882. arXiv:1804.01882 [cs, stat].   \nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855\u2013864, 2016.   \nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: Largescale Information Network Embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067\u20131077, May 2015. doi: 10.1145/2736277.2741093. URL http://arxiv.org/abs/1503.03578. arXiv:1503.03578 [cs].   \nMingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. Asymmetric transitivity preserving graph embedding. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1105\u20131114, 2016.   \nJiankai Sun, Bortik Bandyopadhyay, Armin Bashizade, Jiongqian Liang, P Sadayappan, and Srinivasan Parthasarathy. Atp: Directed graph embedding with asymmetric transitivity preservation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 265\u2013272, 2019b.   \nAaron Sim, Maciej L Wiatrak, Angus Brayne, P\u00e1id\u00ed Creed, and Saee Paliwal. Directed graph embeddings in pseudo-riemannian manifolds. In International Conference on Machine Learning, pages 9681\u20139690. PMLR, 2021.   \nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \nYu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. Advances in neural information processing systems, 33: 19314\u201319326, 2020.   \nChao Liu, Xinchuan Li, Dongyang Zhao, Shaolong Guo, Xiaojun Kang, Lijun Dong, and Hong Yao. A-gnn: Anchors-aware graph neural networks for node embedding. In Quality, Reliability, Security and Robustness in Heterogeneous Systems: 15th EAI International Conference, QShine 2019, Shenzhen, China, November 22\u201323, 2019, Proceedings, pages 141\u2013153. Springer, 2020.   \nZemin Liu, Trung-Kien Nguyen, and Yuan Fang. Tail-gnn: Tail-node graph neural networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1109\u20131119, 2021.   \nMicha\u0142 Adamaszek. Note: The smallest nonevasive graph property. Discussiones Mathematicae Graph Theory, 34(4):857\u2013862, 2014.   \nAleksander Kelenc, Niko Tratnik, and Ismael G Yero. Uniquely identifying the edges of a graph: the edge metric dimension. Discrete Applied Mathematics, 251:204\u2013220, 2018.   \nYisu Peng, Yuxiang Jiang, and Predrag Radivojac. Enumerating consistent sub-graphs of directed acyclic graphs: an insight into biomedical ontologies. Bioinformatics, 34(13):i313\u2013i322, 2018.   \nLukas Biewald et al. Experiment tracking with weights and biases. Software available from wandb. com, 2:233, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Negative Sampling for Graph Representation Learning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Yang et al. [2020] conceptually unify much of the research on negative sampling (NS) for graph representation learning (GRL) into the SampledNCE framework which is grounded in noise-contrastive estimation [Gutmann and Hyv\u00e4rinen, 2012, Mnih and Teh, 2012]. Additionally, they demonstrate that (for vector node representations), the optimal negative sampling distribution is positively but sub-linearly correlated to the positive sampling distribution, i.e. $\\bar{p_{n}}(\\bar{u^{\\textit{\\prime}}}|\\textit{v})\\propto p_{d}(u\\mid v)\\bar{\\alpha},0$ $0<\\alpha<1$ , which supports the empirically-determined $3/4$ in Mikolov et al. [2013]\u2019s degree-based sampling. ", "page_idx": 14}, {"type": "text", "text": "Examples of better-than-uniform NS for GRL include, in chronological order, Markov chain Monte Carlo NS (MCNS) [Yang et al., 2020], self-adversarial NS (SANS) [Kamigaito and Hayashi, 2022], Adaptive NS (AdaNS) [Wang et al., 2023]. A handful of approaches leverage the graph structure directly to gather negative examples; in particular, Structure Aware NS (also abbreviated SANS) of Ahrabian et al. [2020] selects negatives from a node\u2019s $k$ -hop neighborhood. Hierarchical Negative Sampling (HNS) [Chen et al., 2021] is only close to our HANS algorithm in nomenclature, relying on a hierarchical dirichlet process to model neighbor proximity information, while borrowing its approach from DeepWalk\u2019s [Perozzi et al., 2014] random walks for skip-gram prediction of the current vertex. ", "page_idx": 14}, {"type": "text", "text": "We emphasize that each of these above approaches presupposes undirected graphs and vector embeddings of nodes, aligning them with the vast literature on KG embeddings including TransE [Bordes et al., 2013], TransR [Lin et al., 2015], RESCAL [Nickel et al., 2011], DistMult [Yang et al., 2015], ComplEx [Trouillon et al., 2016], RotatE [Sun et al., 2019a], and Rot-Pro [Song et al., 2021] (the last of which models transitive relations via a projection operator). ", "page_idx": 14}, {"type": "text", "text": "A.2 Hierarchical Representations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Box embeddings can represent any DAG [Boratko et al., 2021a], and more recently have been extended to represent any digraph [Zhang et al., 2022]. Empirically, box embeddings have demonstrated particularly strong performance on transitively-closed graphs, making them a strong candidate for modeling hierarchies [Patel et al., 2020]. The use of regions to capture edges in a graph has roots in classic graph invariants of boxicity [Roberts, 1969] and sphericity [Maehara, 1984], which are defined in the undirected setting. In general, one might expect region-based embeddings to have a natural bias toward modeling transitivity, as the containment relation is transitive [Patel et al., 2022]. Additional region-based embeddings which use cones [Vendrov et al., 2016, Lai and Hockenmaier, 2017] or discs [Suzuki et al., 2019] have also been proposed. ", "page_idx": 14}, {"type": "text", "text": "Another prominent line of work for graph representation leverages the negative curvature of hyperbolic space to embed trees without distortion [Sarkar, 2011, Weber and Nickel, 2018, Weber, 2020]. A variety of approaches to training hyperbolic representations with gradient descent have been proposed, to great success [Nickel et al., 2015, Law et al., 2019]. The highlight of these methods is their ability to represent trees, but they are not necessarily well-aligned with representing transitively-closed digraphs [Boratko et al., 2021a]. Hyperbolic entailment cones [Ganea et al., 2018] combine the benefits of both hyperbolic space and region-based containment transitivity. ", "page_idx": 14}, {"type": "text", "text": "A.3 Learning Node Embeddings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The idea of optimizing a loss which is a sum over positive and negative edges captures the approach of training node embeddings in all the work mentioned in the preceding section, however, there are also a variety of other approaches to learning node embeddings. DeepWalk [Perozzi et al., 2014] and node2vec [Grover and Leskovec, 2016] use random walk-based algorithms to generate node embeddings for undirected graphs. LINE [Tang et al., 2015] was the first attempt to address the scalability of learning node embeddings for large-scale graphs. Meanwhile, some works focus on learning node embeddings for directed graphs [Ou et al., 2016, Sun et al., 2019b, Sim et al., 2021]. While more conventionally used in settings where node features are present, graph neural networks (GNNs) [Kipf and Welling, 2016], which iteratively aggregate node features using the local graph structure, have been used to learn node embeddings [Chen et al., 2020, Liu et al., 2020, 2021]. While these approaches are not directly addressed in this work, the general notion of using a sufficient sidigraph to reduce the set of edges and non-edges could be leveraged in such contexts as well. ", "page_idx": 14}, {"type": "text", "text": "A.4 Uniquely Distinguishing Graphs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The idea of uniquely distinguishing a graph with respect to a property has been explored in graph theory literature, albeit without a special focus on DAGs as in the current work. For a property $P$ on $n$ -vertex graphs, Adamaszek [2014] defines $P$ to be \u201cnonevasive\u201c (or \u201cnonelusive\u201d) in the context of a 2-player game where for each turn, player A is only allowed to ask player B whether a pair of nodes forms an edge or not. $P$ is nonevasive if it can be determined by A in strictly fewer than $\\binom{n}{2}$ turns, i.e. by asking fewer questions than edges in a complete graph over $n$ vertices (it follows that completeness is an evasive property). Kelenc et al. [2018] spotlight minimal uniquely distinguishing graphs by introducing the concept of an edge metric dimension as the smallest cardinality of a set $S$ such that every pair of edges in $G$ is distinguished (w.r.t. node-edge distance) by some vertex in $S$ . An appealing application of this graph-theoretic line of thought might be selecting the smallest set of edges from an ontology for the purpose of human annotations to facilitate active learning; Peng et al. [2018] present an algorithm for enumerating consistent subgraphs of DAGs, which are often used for ontologies in the biomedical domain. ", "page_idx": 15}, {"type": "text", "text": "B Proof of Optimality ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For notational convenience, for each vertex $a\\in G$ , we add a directed self-loop $(a{\\rightarrow}a)$ ) to indicate that $a$ is reachable from itself. This self-loop will be a positive edge in $G^{\\pm}$ . Adding this self-loop does not destroy the property that $G$ is transitively closed. ", "page_idx": 16}, {"type": "text", "text": "B.1 A poset for negative edges ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We define an order $\\preceq$ between negative edges as follows. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For every negative edge $(a{\\rightarrow}b)$ , we impose $(a{\\to}b)\\preceq(a{\\to}b)$ . Thus, $\\preceq$ is reflexive. Next, we define an order between two negative edges outgoing from or incoming to the same vertex. Let $a$ be an arbitrary vertex in $V$ . Let $(a{\\rightarrow}b)$ and $(a{\\rightarrow}c)$ be two negative edges out-going from $a$ . Then we define $(a{\\overset{.}{\\to}}b)\\preceq(a{\\to}c)$ if $(c{\\rightarrow}b)$ is a positive edge in $G^{\\pm}$ (in other words, if $(c{\\to}b)\\in E)$ . We call the edge pair $((a{\\rightarrow}b),(a{\\rightarrow}c))$ a primitive pair. See Figure 9(a). Similarly, let $(x{\\to}a)$ and $(y{\\to}a)$ be any two negative edges incoming to $a$ . We define $(x{\\to}a)\\preceq(y{\\to}a)$ if $(x\\rightarrow y)$ ) is a positive edge. The edge pair $(x{\\to}a)$ and $(y{\\to}a)$ is also a primitive pair. Finally, we extend $\\preceq$ to satisfy transitivity: if there exist three edges such that $(a{\\to}b)\\preceq(c{\\to}d)$ and $(c{\\to}d)\\preceq(e{\\to}f)$ such that currently there is no relationship between $(a{\\rightarrow}b)$ and $(e{\\to}f)$ under $\\preceq$ , then we impose $(a{\\rightarrow}b)\\preceq(e{\\rightarrow}^{\\prime}f)$ . ", "page_idx": 16}, {"type": "image", "img_path": "HFS800reZK/tmp/5416b3790fddba22cd80671af4fe9c8b18e438a42466d27e4dff738091ca5235.jpg", "img_caption": ["Figure 9: (a) Two primitive pairs ( $\\scriptstyle(a\\to b)$ , $(a{\\rightarrow}c)$ ) and $((x{\\to}a),(y{\\to}a))$ ). (b) Illustrating the proof of Lemma 1. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Our goal is to show that $({\\overline{{E}}},\\preceq)$ is a poset. To this end, we characterize all pairs of negative edges $(a{\\rightarrow}b)$ and $(c{\\rightarrow}d)$ such that $(a{\\to}b)\\preceq(c{\\to}d)$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 1. Let $(a{\\rightarrow}b)$ and $(c{\\rightarrow}d)$ be two edges such that $(a{\\to}b)\\preceq(c{\\to}d)$ . (It could be that $a=c$ and/or $b=d.$ .) Then $\\stackrel{}{a}\\rightarrow c$ ) and $\\!\\left(d\\to b$ ) are positive edges. ", "page_idx": 16}, {"type": "text", "text": "Proof. We prove by induction. The base cases are for primitive pairs. If $(a{\\rightarrow}b)$ and $(c{\\rightarrow}d)$ are a primitive pair, then either $a=c$ or $b=d$ . In both cases, the lemma holds by definition of the order $\\preceq$ on primitive pairs. ", "page_idx": 16}, {"type": "text", "text": "Let $(a\\mathrm{\\rightarrow}b)=e_{1}\\preceq e_{2}\\preceq\\ldots\\preceq e_{t}=(c\\mathrm{\\rightarrow}d)$ be a minimal sequence of orders realizing the order $(a{\\to}b)\\preceq(c{\\to}d)$ such that every consecutive pair of negative edges $(e_{i},e_{i+1})$ for every $i\\in[1,t-1]$ is a primitive pair. Specifically, $(e_{1},e_{2})$ is a primitive pair. We consider two cases: (i) $e_{2}=(a{\\rightarrow}x)$ and (ii) $e_{2}=(y{\\to}b)$ for some vertex $y$ . The two cases are symmetric, so we only focus on the first case; see Figure 9(b). As $(a{\\to}b)\\preceq(a{\\to}x)$ , $(x\\rightarrow b)$ ) is a positive edge by the definition of $\\preceq\\mathbf{on}$ primitive pairs. Since $(a{\\to}x)\\preceq(c{\\to}d)$ , by induction, $\\left(a\\rightarrow c\\right)$ ) and $\\left(d\\to x\\right)$ ) are positive edges. This means $\\stackrel{\\cdot}{d}\\rightarrow\\stackrel{\\cdot}{b}$ ) is also a positive edge since $G$ is transitively closed. Thus, the lemma holds. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "We know that Lemma 1 holds even when $(a{\\to}b)=(c{\\to}d)$ as in that case $a=c$ and $b=d$ and both self-loops $[a\\rightarrow a]$ ) and $(b\\rightarrow b$ ) are positive. Now, we are ready to show that $\\preceq$ is anti-symmetric; the proof uses the fact that $G$ is a DAG. ", "page_idx": 16}, {"type": "text", "text": "Lemma 2. If $G=(V,E)$ is a transitively-closed DAG, then $(\\overline{{E}},\\preceq)$ is a poset. ", "page_idx": 16}, {"type": "text", "text": "Proof. The order $\\preceq$ is transitive and reflexive by definition. We now show anti-symmetric by contradiction. Suppose that $(a{\\rightarrow}b)$ and $(x{\\to}y)$ are two different negative edges such that $(a{\\rightarrow}b)\\overset{\\cdot}{{\\preceq}}$ $(x{\\to}y)$ and $(x{\\to}y)\\preceq(a{\\to}b)$ . Since $(a{\\to}b)\\neq(x{\\to}y)$ , either $x\\neq a$ or $b\\neq y$ . We focus on the case $x\\neq a$ ; the proof for the case $b\\neq y$ is the same. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Since $(a{\\to}b)\\preceq(x{\\to}y).$ , by Lemma 1, $(a\\to x)$ ) is a positive edge. Also by Lemma 1, as $(x{\\to}y)\\preceq$ $(a{\\rightarrow}b)$ , we have that $\\dot{x}\\rightarrow a$ ) is also a positive edge. However, this contradicts that $G$ is a DAG. ", "page_idx": 17}, {"type": "text", "text": "We could reinterpret Proposition 2 in the language of the poset $(\\overline{{E}},\\preceq)$ : ", "page_idx": 17}, {"type": "text", "text": "Corollary 1. Let $G$ be a transitively closed DAG. Let $H\\,=\\,(V,E_{H}^{+},E_{H}^{-})$ be a sidigraph which distinguishes $G$ among transitively-closed digraphs. Let $e_{1},e_{2}$ be a primitive pair of negative edges in $E_{H}^{-}$ such that $e_{1}\\preceq e_{2}$ . Then $(\\dot{V},E_{H}^{+},E_{H}^{-}\\,\\Bar{\\backslash}\\,\\{\\bar{e}_{2}\\})$ also distinguishes $G$ among transitively-closed digraphs. ", "page_idx": 17}, {"type": "text", "text": "B.2 A proof of optimality ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We have shown in Lemma 2 that $({\\overline{{E}}},\\preceq)$ is a poset. Let $M_{\\prec}$ be the set of minimal elements of this poset; see Figure 10. Our proof to show that Algorithm 1 is optimal has two steps: (Step 1) the negative edges in $M_{\\preceq}$ are necessary and sufficient for any sidigraph which distinguishes $G$ among transitively-closed digraphs, and (Step 2) the sidigraph $H$ output by our algorithm has $E_{H}^{-}=M_{\\preceq}$ . ", "page_idx": 17}, {"type": "image", "img_path": "HFS800reZK/tmp/ae92bed3d4f7638d275405fea182c9ea8f17286c88f8064ca44695add1dc6066.jpg", "img_caption": ["Figure 10: (a) An equivalent sidigraph $G^{\\pm}$ . (b) The Hasse diagram of the poset on the negative edges with an artificially added element $\\infty$ . Here $M_{\\preceq}=\\{(d{\\to}a),(b{\\to}c),(a{\\to}d),(c{\\to}b)\\}$ (c) a minimal sidigraph $(V,\\bar{E}^{\\mathrm{tr}},M_{\\preceq}\\bar{)}$ which can distinguish $\\boldsymbol{G}$ among transitively-closed digraphs. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Proposition 7. If $G\\,=\\,(V,E)$ is a $D A G$ , then $H\\,=\\,(V,E^{t r},M_{\\preceq})$ can distinguish $G$ among all transitively-closed digraphs. ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $L$ be the linear ordering of all edges in $\\overline{E}$ in non-decreasing order of $\\preceq$ . That is, if $(x{\\to}\\bar{y})\\preceq(a{\\to}b)$ , then $(a{\\rightarrow}b)$ appears before $(x{\\to}y)$ in $L.~L$ can be obtained by visiting the negative edges in the BFS order in the Hasse diagram with an artificially added maximum element $\\infty$ to define the root of the BFS tree; see Figure 10(b). ", "page_idx": 17}, {"type": "text", "text": "Let $X_{0}=\\overline{{E}}$ and $i=0$ . We then visit all negative edges by the order in $L$ . For each negative edge $e$ visited in this order, if $e\\not\\in M_{\\preceq}$ , we define $\\bar{X}_{i+1}=\\bar{X}_{i}\\setminus\\{e\\}$ and increase $i$ by 1. Let $X_{t}$ be the last set of negative edges after this process terminates. Clearly, $X_{t}=M_{\\preceq}$ . ", "page_idx": 17}, {"type": "text", "text": "By Proposition 1, $\\left(V,E^{\\mathrm{tr}},X_{0}\\right)$ can distinguish $G$ . Inductively, we assume that $\\left(V,E^{\\mathrm{tr}},X_{i}\\right)$ can distinguish $G$ for some $i$ . Let $e$ be a negative edge such that $X_{i+1}=X_{i}\\setminus\\{e\\}$ . Since $e\\not\\in M_{\\preceq}$ , there exists a primitive pair of negative edges $e^{\\prime}\\preceq e$ . Since $e^{\\prime}$ appears after $e$ in $L$ , both edges are present in $X_{i}$ . By Corollary 1, $(V,\\bar{E}^{\\mathrm{tr}},X_{i+1}\\bar{)}$ can distinguish $G$ among transitively closed digraphs. ", "page_idx": 17}, {"type": "text", "text": "We have shown by induction that $\\left(V,E^{\\mathrm{tr}},X_{t}\\right)$ , which also is $(V,E^{\\mathrm{tr}},M_{\\preceq})$ , is a transitively-closed distinguisher. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Next, we show that any sidigraph capable of distinguishing $G$ among transitively-closed digraphs must contain any $M_{\\preceq}$ in it\u2019s negative set. ", "page_idx": 17}, {"type": "text", "text": "Proposition 8. Let $H\\ =\\ (V,E_{H}^{-},E_{H}^{+})$ be any sidigraph capable of distinguishing $G$ among transitively-closed digrphs. Then $M_{\\preceq}\\subseteq E_{H}^{-}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. We prove by contraction: suppose $\\boldsymbol{e}_{\\mathbf{\\Lambda}}=\\mathbf{\\Lambda}(c\\!\\!\\to\\!\\!d)$ is such that $(c{\\to}d)~\\in~M_{\\preceq}\\setminus E_{H}^{-}$ . Let $G^{\\prime}=(V,E\\,\\bar{\\cup}\\,\\{e\\})$ . We show below that $G^{\\prime}\\in\\mathcal{G}_{H}\\cap\\mathcal{P}$ , which then means that $H$ cannot distinguish $G$ from $G^{\\prime}$ among transitively-closed digraphs. ", "page_idx": 18}, {"type": "text", "text": "Note that $G$ contains all positive edges of $H$ since $H$ is a transitively-closed distinguisher. Thus, $G^{\\prime}$ contains all positive edges of $H$ . Next, we show that $G^{\\prime}$ does not contain any negative edge. Suppose otherwise, $G^{\\prime}$ contains a negative $(a{\\to}b)\\,\\in\\,E_{H}^{-}$ . Since $(a{\\rightarrow}b)~\\notin~E$ , the only way for $(a\\!\\to\\!b)\\in E\\cup\\{e\\}$ is because of $(c{\\rightarrow}d)$ , and in particular, $\\left(a\\!\\to\\!c\\right)$ and $(d\\!\\!\\to\\!\\!b)$ are both positive edges in $G$ . Thus, by Lemma 1, $(a{\\to}b)\\preceq(c{\\to}d)$ , contradicting that $(c{\\to}d)\\in M_{\\preceq}$ . ", "page_idx": 18}, {"type": "text", "text": "Finally, we show that $G^{\\prime}$ is transitively-closed. Suppose otherwise, there are two vertices $a$ and $b$ such that $b$ is reachable from $a$ but $(a\\stackrel{.}{\\to}b)\\not\\in G^{\\prime}$ . Note that $b$ is not reachable from $a$ in $G$ since $G$ is transitively closed. This implies: (i) the path from $a$ to $b$ in $G^{\\prime}$ must contain the edge $(c{\\rightarrow}d)$ and (ii) $(a{\\rightarrow}b)$ is a negative edge in $G$ . By (i), it must be that $(a{\\rightarrow}c)$ and $(d\\!\\!\\to\\!\\!b)$ are positive edges. By (ii) and Lemma 1, $(a{\\to}b)\\preceq(c{\\to}d).$ , contradicting that $(c{\\to}d)\\in M_{\\preceq}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Proposition 7 and Proposition 8 together imply that any transitively closed-distinguisher of $G$ with $M_{\\preceq}$ contains a minimum number of negative edges. Thus, to show that our algorithm gives a transitively closed distinguisher of $G$ , it suffices to show that the set of negative edges is $M_{\\preceq}$ . ", "page_idx": 18}, {"type": "text", "text": "Theorem 1. Let $G=(V,E)$ be any $D A G$ , and let $H$ be the output of FINDMINDISTINGUISHER $(G)$ , so $H=(V,E^{t r},M_{\\preceq})$ . Then $H$ can distinguish $G$ among transitively closed digraphs, and has the minimal number of edges of any such sidigraph. ", "page_idx": 18}, {"type": "text", "text": "Proof. We focus on showing that $E_{H}^{-}\\,=\\,M_{\\preceq}$ . The latter claim follows from Proposition 7 and Proposition 8. ", "page_idx": 18}, {"type": "text", "text": "Observe that the algorithm considers primitive pairs and, for each pair, removes the negative edge with a higher order in the poset $({\\overline{{E}}},\\preceq)$ . Thus, the algorithm never removes a negative edge in $M_{\\preceq}$ from $H$ , giving that $M_{\\preceq}\\subseteq E_{H}^{-}$ . ", "page_idx": 18}, {"type": "text", "text": "It remains to show that $E_{H}^{-}\\subseteq M_{\\preceq}$ . Suppose otherwise. Then there exists a negative edge $(a{\\rightarrow}b)\\in$ $E_{H}^{-}\\setminus M_{\\preceq}$ . Then by definition of $M_{\\preceq}$ , there must be a negative edge $(x{\\to}y)$ such that (i) $(a{\\rightarrow}b)$ and $(x{\\to}y)$ make up a primitive pair, and (ii) $(x{\\to}y)\\preceq(a{\\to}\\bar{b})$ . Then $\\stackrel{.}{a}\\rightarrow b$ ) will be removed from $H$ when the algorithm considers $(x{\\to}y)$ ) in line 4, contradicting that $(a{\\to}b)\\in E_{H}^{-}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C Proofs for Transitivity Bias ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proposition 4. If $\\operatorname{E}_{\\theta}$ is an energy function with transitivity bias, then for any $\\theta$ there exists a $T\\geq0$ such that the digraph with edges $\\{(u,v)\\mid\\operatorname{E}_{\\theta}(u,v)\\leq T\\}$ is transitively closed. ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $(u,v)~\\in~E^{\\mathrm{tc}}$ . Then there exists a directed path $u\\;:=\\;w_{1}\\;\\rightarrow\\;\\cdot\\cdot\\cdot\\;\\rightarrow\\;w_{N}\\;=:\\;v$ in $E$ , hence $\\operatorname{E}_{\\theta}(w_{n},w_{n+1})\\,\\leq\\,T$ for all $n\\;\\in\\;[\\![N\\mathrm{~-~}1]\\!]$ . Let $k\\ \\in\\ [\\![N\\mathrm{~-~}2]\\!]$ , and assume we have $\\operatorname{E}_{\\theta}(w_{1},w_{k+1})\\leq T$ . Since $\\operatorname{E}_{\\theta}(w_{k+1},w_{k+2})\\leq T$ as w ell, we apply  the defi nition of transitivity bias to find $\\operatorname{E}_{\\theta}(w_{1},w_{k+2})\\leq T$ . Thus, by induction, we have $\\mathrm{E}_{\\theta}(w_{1},\\dot{w}_{N})=\\mathrm{E}_{\\theta}(u,v)\\le T$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Proposition 5. Let $G$ be a transitively-closed digraph, $\\operatorname{E}_{\\theta}$ an energy function with transitivity bias, and $\\'H=(V,E_{H}^{+},E_{H}^{-})$ a sidigraph which distinguishes $G$ among transitively closed digraphs. If $T$ is the threshold associated with the transitivity bias for $\\theta$ , $\\mathrm{E}_{\\theta}(u,v)\\le T$ for all $(u,v)\\in E_{H}^{+}$ , and $\\mathrm{E}_{\\theta}(u,v)>T$ for all $(u,v)\\in E_{H}^{-},$ , then $\\theta$ represents the digraph $G$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $E^{\\prime}=\\{(u,v)\\mid\\operatorname{E}_{\\theta}(u,v)\\leq T\\}$ , then Proposition 4 implies that the digraph $G^{\\prime}=(V,E^{\\prime})$ is transitively closed. Due to the assumptions on the values of the energy we have $E_{H}^{+}\\subseteq E^{\\prime}$ and $E_{H}^{-}\\subseteq{\\overline{{E^{\\prime}}}}$ . Since $H$ is sufficient to distinguish transitively-closed digraphs, we have $G^{\\prime}=G$ , as desired. \u53e3 ", "page_idx": 19}, {"type": "table", "img_path": "HFS800reZK/tmp/4ffbbe6a5e2ad10bf544dc96da9e84ffc0692c2b8d6893fd2bd2140a85d1081c.jpg", "table_caption": ["D Results with Error Bars "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Hyperparameter Tuning ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The experiments were run in a single-node, single-GPU setup on a cluster with the following GPU architectures: ", "page_idx": 20}, {"type": "text", "text": "\u2022 NVIDIA Tesla M40 (24GB VRAM) \u2022 NVIDIA GeForce GTX TITAN X (12GB VRAM) \u2022 NVIDIA GeForce GTX 1080 Ti (11GB VRAM) \u2022 NVIDIA RTX 2080ti (11GB VRAM) \u2022 NVIDIA Quadro RTX 8000 (48GB VRAM) ", "page_idx": 20}, {"type": "text", "text": "E.1 Synthetic Graphs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To ensure the competitiveness of all experimental configurations for the synthetic graphs, we tune the learning rate and negative weight $\\lambda_{\\mathrm{neg}}$ for each graph independently by running Bayesian hyperparameter optimization with W&B Biewald et al. [2020] and for $E^{\\mathrm{tc}}$ taking the values that yielded the best F1 at the end of 12 epochs, which we empirically observed corresponds to the \u201celbow\u201d of many per-epoch F1 plots and thus gives a good indication of which settings are resulting in fast convergence. For $E^{\\mathrm{tr}}$ , we scaled the number of epochs to 60, since the number of training examples is substantially smaller for this setting than for transitive closure. ", "page_idx": 20}, {"type": "text", "text": "For each of the final experiment runs, we fetch the best learning rate and $\\lambda_{\\mathrm{neg}}$ and run for a full 40 epochs for $E^{\\mathrm{tc}}$ or 200 for $E^{\\mathrm{tr}}$ , recording F1 at every epoch to produce a convergence plot. ", "page_idx": 20}, {"type": "text", "text": "E.2 MeSH 2020 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since MeSH has at least thrice the number of nodes as any of the synthetic graphs in our experiments, it was infeasible to tune hyperparameters over all the same settings as for the smaller synthetic graphs. We therefore fix positive edge set $E^{\\mathrm{tc}}$ (which empirically performs better than $E^{\\mathrm{tr}}$ ), as well as the resource-efficient negative ratio $k=4$ , and do a Bayesian hyperparameter search for learning rate and negative rate $\\lambda_{\\mathrm{neg}}$ using W&B over the cross-product of model type (SIM-VEC vs GT-BOX) and negative edge set $\\overline{E}$ vs $E_{H^{*}}$ ). ", "page_idx": 20}, {"type": "text", "text": "F  Graph Family Hyperparameters and Edge Statistics ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "HFS800reZK/tmp/f8d04a46888bb7bc17101391e49834f04b298e304ad293b353c53e15c8a00776.jpg", "table_caption": [""], "table_footnote": ["Tablldsiqivdldisal \u201creduced\u201d expermental settings. While graps uderBalaned Tre are deteministic, the values of each entry for P andPrice are averaged over 10 randm sed. "], "page_idx": 21}, {"type": "text", "text": "G Plots for $k=128$ ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "HFS800reZK/tmp/ff302dd2c3135a54333d44e44b29e224b9c11fdc69cd923cb62fa4355084d7c4.jpg", "img_caption": ["Figure 11: The plots show convergence of GT-BOX for negative ratio $k=128$ . The top row shows the plots for balanced trees with branching factors $b=2$ , 5 and 10. The middle row for nCRP graphs with $\\alpha=10$ , 100, and 500, respectively going left to right. The bottom row shows the plots for Price\u2019s graph with $c=0.1$ , $\\gamma=1.0$ and three values of $m=1$ , 5, and 10. The number of vertices in each graph is $\\approx2^{13}$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The abstract makes concrete and specific claims, which are supported by the experiment section. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Section 7 discusses the limitations of our work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201cLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Section 3 and Section 4 contain propositions and proofs concerning signed digraphs and transitivity bias, and Appendix B and Appendix C provide full proofs whenever these were omitted from the main paper due to space constraints. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Section 6 and Appendix E discuss the settings of the experiments and the hyperparameter tuning process. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide a link to the code and data in a footnote on the second page. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we provide all the details pertaining to experimental settings in Section 6 and Appendix E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We report standard deviations for the numerical results in Appendix D. The separation between the models is clear enough to not require statistical significance tests. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201cYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We report the types of GPU used for our experiments in Appendix E. However, we are unable to report the exact compute hours. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The work confirms with NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The work done in this paper is largely theoretical and we don\u2019t see an immediate and direct societal impact for the work in its current form. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The necessary credits are given at appropriate places throughout the paper. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]