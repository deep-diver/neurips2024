{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduces the concept of large language models which are heavily used in the context of the current research."}, {"fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "publication_date": "2022-12-01", "reason": "This paper proposes FlashAttention, a faster and more memory-efficient attention mechanism that is relevant to improving pipeline parallelism."}, {"fullname_first_author": "Shiqing Fan", "paper_title": "Dapple: A pipelined data parallel approach for training large models", "publication_date": "2021-06-01", "reason": "This paper introduces Dapple, a pipelined data parallel training method that is compared against in the current work."}, {"fullname_first_author": "Alexander L Gaunt", "paper_title": "Ampnet: Asynchronous model-parallel training for dynamic neural networks", "publication_date": "2017-05-01", "reason": "This paper discusses asynchronous pipeline parallelism, a technique relevant to the current work's focus on efficient pipeline scheduling."}, {"fullname_first_author": "Aaron Harlap", "paper_title": "Pipedream: Fast and efficient pipeline parallel DNN training", "publication_date": "2018-06-01", "reason": "This paper introduces Pipedream, an early work on pipeline parallelism that is referenced and built upon by the current work."}]}