[{"figure_path": "3Rtn1OMTC4/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparations of visual representations on simulation benchmarks. We report the average score across all tasks for each simulation environment. DINOv2 uses ViT-B/14, CLIP uses ViT-B/32, and unless otherwise specified, others use ViT-B/16. Mt-Wd, Fr-Ki, DMC, Adro, Tr-fi, and WA respectively represent MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. * denotes that public VC-1 samples image frmaes form full Ego4D dataset.", "description": "This table compares the performance of different visual representation models on various robotic motor control simulation tasks.  It shows the average success rate across multiple tasks and environments for each model, highlighting the pre-training data used (e.g., Ego4D, ImageNet) and model architecture (ViT).  The results reveal the effectiveness of different self-supervised pre-training methods in robotic motor control.", "section": "4.3 Performance on Downstream Simulation Tasks"}, {"figure_path": "3Rtn1OMTC4/tables/tables_7_2.jpg", "caption": "Table 2: The ablation experiment results. Me, Fra, DMC, Adr, Tri, and WA respectively represent MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. All models use ViT-B/16.", "description": "This table presents the ablation study results, comparing the performance of STP with different settings on various simulation benchmarks. It shows the impact of various design choices, such as spatial prediction masking ratio, the temporal prediction condition, the temporal decoder architecture, and frame sampling strategy. The results are presented as average weights across MetaWorld, Franka-Kitchen, DMControl, Adroit, and Trifinger environments.", "section": "4.4 Ablation on Downstream Simulation Tasks"}, {"figure_path": "3Rtn1OMTC4/tables/tables_7_3.jpg", "caption": "Table 1: Performance comparations of visual representations on simulation benchmarks. We report the average score across all tasks for each simulation environment. DINOv2 uses ViT-B/14, CLIP uses ViT-B/32, and unless otherwise specified, others use ViT-B/16. Mt-Wd, Fr-Ki, DMC, Adro, Tr-fi, and WA respectively represent MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. * denotes that public VC-1 samples image frmaes form full Ego4D dataset.", "description": "This table compares the performance of several pre-trained visual representation models on various robotic motor control simulation tasks.  The models tested include DINOv2, CLIP, R3M, VC-1, MAE, and the proposed STP model.  The table shows the average success rate across multiple tasks within five different simulation environments (MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger), and also provides a weighted average across all environments.  The different models are trained using various datasets and architectures. The table helps evaluate the effectiveness and generalization ability of different pre-trained models for robotic motor control. The table highlights how the proposed STP model outperforms other methods.", "section": "4.3 Performance on Downstream Simulation Tasks"}, {"figure_path": "3Rtn1OMTC4/tables/tables_8_1.jpg", "caption": "Table 3: Performance comparations on real-world tasks.", "description": "This table presents the comparison of the success rate for two real-world robotic manipulation tasks, namely Picking and Pouring, achieved by two methods: MAE and STP.  The results show the average success rate for each task and the overall average success rate across both tasks.  The STP method demonstrates improvement in the pouring task,  achieving a higher success rate compared to the MAE method.", "section": "4.5 Performance on Downstream Real-world Tasks"}, {"figure_path": "3Rtn1OMTC4/tables/tables_16_1.jpg", "caption": "Table 1: Performance comparations of visual representations on simulation benchmarks. We report the average score across all tasks for each simulation environment. DINOv2 uses ViT-B/14, CLIP uses ViT-B/32, and unless otherwise specified, others use ViT-B/16. Mt-Wd, Fr-Ki, DMC, Adro, Tr-fi, and WA respectively represent MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. * denotes that public VC-1 samples image frmaes form full Ego4D dataset.", "description": "This table compares the performance of different visual representation models on various simulation benchmark tasks.  It shows the average success rate across all tasks for each environment.  The models compared include DINOv2, CLIP, R3M, VC-1, MAE, and the proposed STP model.  Different pre-training data sources and architectures are considered.", "section": "4.3 Performance on Downstream Simulation Tasks"}, {"figure_path": "3Rtn1OMTC4/tables/tables_19_1.jpg", "caption": "Table 5: STP post-pre-training hyperparameters on simulation environments.", "description": "This table shows the hyperparameters used for the STP post-pre-training phase on different simulation environments.  The hyperparameters include the total number of training epochs, the number of warmup epochs (initial epochs with a different learning rate), the effective batch size used during training, the number of expert demonstrations used to fine-tune the policy, and the frame interval used during sampling of frames from the video for training.", "section": "4.2 Implementation on Downstream Policy"}, {"figure_path": "3Rtn1OMTC4/tables/tables_19_2.jpg", "caption": "Table 1: Performance comparations of visual representations on simulation benchmarks. We report the average score across all tasks for each simulation environment. DINOv2 uses ViT-B/14, CLIP uses ViT-B/32, and unless otherwise specified, others use ViT-B/16. Mt-Wd, Fr-Ki, DMC, Adro, Tr-fi, and WA respectively represent MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. * denotes that public VC-1 samples image frmaes form full Ego4D dataset.", "description": "This table compares the performance of different visual representation models on various robotic manipulation and locomotion tasks across five simulated environments.  The models include DINOv2, CLIP, R3M, VC-1, MAE, and the proposed STP method. The average success rate or reward across 19 simulation tasks is reported for each model.  The table highlights the impact of different pre-training data and methods on the downstream performance of these models in robotic control.", "section": "4.3 Performance on Downstream Simulation Tasks"}, {"figure_path": "3Rtn1OMTC4/tables/tables_20_1.jpg", "caption": "Table 7: Evaluation schemes on simulation environments.", "description": "This table details the evaluation setup for different simulation environments used in the paper. It shows the observation space (RGB or RGB with proprioception), history window size, camera viewpoints, base seeds for random number generation, and the number of trajectories evaluated for each benchmark.", "section": "4.2 Implementation on Downstream Policy"}, {"figure_path": "3Rtn1OMTC4/tables/tables_20_2.jpg", "caption": "Table 8: The success rate for each task on simulation benchmarks.", "description": "This table presents the success rates achieved by the STP model on various simulation tasks. Each row represents a specific task from different simulation environments (Meta-World, Franka-Kitchen, Adroit, DMControl, Trifinger). The success rate represents the percentage of successful task completions.  Higher percentages indicate better performance of the STP model in performing those robotic tasks.", "section": "4.3 Performance on Downstream Simulation Tasks"}]