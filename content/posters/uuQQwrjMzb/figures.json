[{"figure_path": "uuQQwrjMzb/figures/figures_0_1.jpg", "caption": "Figure 1: Adaptive labeling to reduce epistemic uncertainty over model performance. Among the two clusters of unlabeled examples (left vs. right), we must learn to prioritize labeling inputs from the left cluster to better evaluate mean squared error.", "description": "This figure illustrates the problem of selection bias in model evaluation.  The left panel shows a dataset with labels only available for a subset of data points, leading to an underestimation of the mean squared error (MSE). The right panel shows that by adaptively selecting which points to label, the uncertainty in MSE can be reduced, particularly for regions not well represented in the original data set. The adaptive approach focuses on labeling examples to improve the estimation of the model's performance outside the support of the available data.", "section": "1 Introduction"}, {"figure_path": "uuQQwrjMzb/figures/figures_1_1.jpg", "caption": "Figure 2: Overview of our adaptive sampling framework. At each period, we select batch of inputs Xt to be labeled, and obtain a new labeled data Dt. We view posterior beliefs \u00b5t(\u00b7) on f*(Y|X) as \"states\", and update it as additional labeled data is collected. Our goal is to minimize uncertainty on the performance of the prediction model f(\u00b7) at the end of T periods.", "description": "This figure illustrates the adaptive sampling framework presented in the paper.  The framework is modeled as a Markov Decision Process (MDP). Each state in the MDP represents the posterior belief on the model's performance, which is updated with each new batch of labels.  The actions consist of selecting a subset of inputs from a pool of unlabeled data to be labeled. The goal of the MDP is to minimize the uncertainty in model performance after a fixed number of labeling periods (T).  The figure visually shows the progression through states and actions, ultimately leading to the final state and a reward calculation based on the variance of the model's performance.", "section": "3 Markov decision process over posterior beliefs"}, {"figure_path": "uuQQwrjMzb/figures/figures_5_1.jpg", "caption": "Figure 3: Differentiable one-step look ahead pipeline for efficient adaptive sampling", "description": "This figure illustrates the differentiable one-step lookahead pipeline used for efficient adaptive sampling.  The pipeline begins with the current posterior belief (state \u03bct) and policy (\u03c0t,\u03b8).  A batch of inputs (Xt+1) is sampled from the pool using K-subset sampling. Then, the selected batch is used to update the posterior belief (\u03bct+1). The variance of the model performance (g(f)) is estimated from this updated posterior. The bottom path shows the differentiable version of this pipeline. The soft K-subset sampling is used to create a differentiable approximation of sampling,  allowing for the use of smooth, differentiable posterior updates and a differentiable estimate of the model's performance variance. This allows for efficient gradient-based policy optimization through backpropagation.", "section": "4.3 Differentiable pipeline"}, {"figure_path": "uuQQwrjMzb/figures/figures_7_1.jpg", "caption": "Figure 4: (Synthetic data) Variance of mean squared loss evaluated through the posterior belief \u00b5t at each horizon t. This is the objective that policy gradient methods like REINFORCE and Autodiff 1-lookahead optimizes. 1-step lookaheads are surprisingly effective even in long horizons.", "description": "The figure shows the variance of the mean squared loss calculated using the posterior belief at each time step (horizon).  It compares the performance of different methods for adaptive sampling: REINFORCE, the proposed Autodiff 1-lookahead method, and several uncertainty sampling heuristics (static and sequential). The graph demonstrates that even a one-step lookahead policy significantly reduces uncertainty in estimating model performance, even over longer time horizons.", "section": "5 Experiments"}, {"figure_path": "uuQQwrjMzb/figures/figures_8_1.jpg", "caption": "Figure 4: (Synthetic data) Variance of mean squared loss evaluated through the posterior belief \u03bct at each horizon t. This is the objective that policy gradient methods like REINFORCE and Autodiff 1-lookahead optimizes. 1-step lookaheads are surprisingly effective even in long horizons.", "description": "The figure shows the variance of the mean squared loss (MSE) evaluated using the posterior belief at different time horizons (t) for various methods including REINFORCE, Autodiff 1-lookahead, uncertainty sampling (sequential and static), and random sampling.  It illustrates how the variance of the MSE changes as more data is collected.  The results demonstrate the effectiveness of the proposed Autodiff 1-lookahead method, even in longer time horizons, at reducing the uncertainty in the MSE estimate.", "section": "5 Experiments"}, {"figure_path": "uuQQwrjMzb/figures/figures_12_1.jpg", "caption": "Figure 3: Differentiable one-step look ahead pipeline for efficient adaptive sampling", "description": "This figure shows a detailed overview of the differentiable one-step look-ahead pipeline used in the adaptive sampling framework. It illustrates how posterior beliefs (states) are updated using a soft K-subset sampling procedure and a differentiable posterior update method.  The pipeline combines soft K-subset sampling, a differentiable posterior update method, and a differentiable estimate of the variance of MSE to efficiently optimize the policy and adapt the sampling strategy. The algorithm proceeds by: 1) obtaining a posterior state based on the data collected thus far; 2) using the policy to select a set of K inputs from a pool of unlabeled data; 3) using the UQ module (e.g., Gaussian Processes or neural network ensembles) to generate imagined pseudo labels and update the posterior; and 4) estimating the variance of MSE to compute the policy gradient for updating the policy using backpropagation. This process is repeated at each iteration. By making each step differentiable, the authors enable the use of efficient optimization methods like gradient descent to learn the optimal adaptive labeling policy.", "section": "4 Planning using pathwise policy gradients"}]