{"references": [{"fullname_first_author": "Song Han", "paper_title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network", "publication_date": "2016-00-00", "reason": "This paper introduces the Efficient Inference Engine (EIE), a foundational work in model compression that directly inspired the ShiftAddLLM's multiplication-less approach."}, {"fullname_first_author": "Mostafa Elhoushi", "paper_title": "DeepShift: Towards Multiplication-Less Neural Networks", "publication_date": "2021-00-00", "reason": "This work directly explores multiplication-less neural networks, providing a foundation for the shift-and-add reparameterization techniques used in ShiftAddLLM."}, {"fullname_first_author": "Haoran You", "paper_title": "ShiftAddNet: A Hardware-Inspired Deep Network", "publication_date": "2020-00-00", "reason": "This paper introduced ShiftAddNet, the precursor to ShiftAddLLM, which demonstrated the effectiveness of shift-and-add operations for accelerating CNNs."}, {"fullname_first_author": "Elias Frantar", "paper_title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers", "publication_date": "2022-00-00", "reason": "This paper introduced OPTQ, a state-of-the-art post-training quantization method for LLMs, that ShiftAddLLM compares against and improves upon."}, {"fullname_first_author": "Gunho Park", "paper_title": "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models", "publication_date": "2022-00-00", "reason": "This paper introduced LUT-GEMM, another highly competitive post-training quantization method for LLMs, providing a strong baseline for comparison with ShiftAddLLM."}]}