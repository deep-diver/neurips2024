[{"figure_path": "JNl6h3U3oW/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of our proposed post-training reparameterization for ShiftAddLLM.", "description": "This figure illustrates the proposed post-training reparameterization method for ShiftAddLLM.  It compares the traditional weight-only quantization method with the ShiftAddLLM approach.  The traditional method involves dequantization to FP16 before matrix multiplication, while ShiftAddLLM utilizes binary weight matrices and scaling factors.  The scaling factors are used with shift operations on the activations. LUTs (lookup tables) are created to speed up the query and add operations with the binary weights and shifted activations. The overall effect is to replace costly multiplications with efficient shift and add operations, resulting in lower latency and memory usage.", "section": "4.1 ShiftAddLLM: Post-training Reparameterization of LLMs with Shift and Add Primitives"}, {"figure_path": "JNl6h3U3oW/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of our proposed post-training reparameterization for ShiftAddLLM.", "description": "This figure illustrates the proposed post-training reparameterization method for ShiftAddLLM. It compares three approaches: (a) Previous weight-only quantization, which de-quantizes weights to FP16 before multiplication with activations; (b) the proposed ShiftAddLLM method, which directly uses a quantized weight format and replaces multiplications with shift-and-add operations; (c) FP16 shift using multiplication, showing the shift operation using FP16 multiplication; (d) Construct LUTs and Query&Add, illustrating the creation of LUTs (lookup tables) and the query and add operations for efficient computation. This approach reduces the reliance on costly multiplications, leading to improved efficiency in the model.", "section": "4.1 ShiftAddLLM: Post-training Reparameterization of LLMs with Shift and Add Primitives"}, {"figure_path": "JNl6h3U3oW/figures/figures_5_1.jpg", "caption": "Figure 4: Sensitivity analysis on OPT-1.3B [74].", "description": "This figure shows the sensitivity analysis of different layers and blocks in LLMs to shift-and-add reparameterization.  The left chart shows the quantization error per parameter for each block in OPT-1.3B model at 2, 3, and 4 bits. The right chart shows the quantization error for different layer types (K, V, Q, Out, FC1, FC2) within a block, also for 2, 3, and 4 bits. The results demonstrate that later blocks and Q/K layers are more sensitive to reparameterization, indicating varying sensitivities across different layers and blocks which motivate the use of a mixed bit allocation strategy.", "section": "4.3 ShiftAddLLM: Mixed and Automated Bit Allocation"}, {"figure_path": "JNl6h3U3oW/figures/figures_5_2.jpg", "caption": "Figure 5: Rank comparisons.", "description": "This figure shows a scatter plot comparing the ranking of linear layers in a neural network based on two different criteria: a proposed criterion for estimating the importance of linear weights and the actual reparameterization error. The strong positive correlation (Kendall \u03c4 = 0.905) indicates that the proposed criterion effectively estimates the difficulty of reparameterizing linear layers and its potential impact on accuracy.", "section": "4.3 ShiftAddLLM: Mixed and Automated Bit Allocation"}, {"figure_path": "JNl6h3U3oW/figures/figures_7_1.jpg", "caption": "Figure 6: Accuracy-latency tradeoff comparisons on the OPT, LLaMA-2/3, and Gemma models.", "description": "This figure shows the accuracy-latency trade-off comparisons for different LLMs (OPT, LLAMA-2/3, and Gemma) at different bit precisions (3-bit and 2-bit).  Each sub-figure presents a comparison of perplexity (y-axis) versus latency (x-axis) for the different LLMs, showing the performance of ShiftAddLLM against state-of-the-art baselines (OPTQ, LUT-GEMM, AWQ).  The results demonstrate ShiftAddLLM's effectiveness in achieving lower perplexity at comparable or lower latency compared to existing methods.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/figures/figures_8_1.jpg", "caption": "Figure 4: Sensitivity analysis on OPT-1.3B [74].", "description": "This figure shows the sensitivity analysis of different layers and blocks in LLMs to shift-and-add reparameterization.  It illustrates that later blocks tend to incur more quantization or reparameterization errors and that Query/Key layers within each block are generally more sensitive to reparameterization than other linear layers. This varying sensitivity across layers and blocks motivates the use of a mixed bit allocation strategy to optimize the efficiency and accuracy of the ShiftAddLLM.", "section": "4.3 ShiftAddLLM: Mixed and Automated Bit Allocation"}, {"figure_path": "JNl6h3U3oW/figures/figures_9_1.jpg", "caption": "Figure 1: Illustration of our proposed post-training reparameterization for ShiftAddLLM.", "description": "This figure illustrates the proposed post-training reparameterization method for ShiftAddLLM. It compares three approaches: (a) Previous weight-only quantization, which uses 4-bit weights and FP16 activations and requires de-quantization; (b) the proposed ShiftAddLLM, which uses lower-bit weights and FP16 activations and replaces multiplication with shift-and-add operations; (c) FP16 shift using multiplication, showing how the shift-and-add approach mimics the original multiplication operation; and (d) construction of LUTs and the query-and-add operation in ShiftAddLLM, showing how the shift and add operations and LUTs are used to reduce the computational cost.", "section": "4.1 ShiftAddLLM: Post-training Reparameterization of LLMs with Shift and Add Primitives"}, {"figure_path": "JNl6h3U3oW/figures/figures_16_1.jpg", "caption": "Figure 6: Accuracy-latency tradeoff comparisons on the OPT, LLaMA-2/3, and Gemma models.", "description": "This figure shows the accuracy-latency trade-off for different LLMs (OPT, LLaMA-2, LLaMA-3, and Gemma) using 2-bit and 3-bit quantization.  The x-axis represents latency (in milliseconds), and the y-axis represents perplexity, a measure of model performance.  Each subfigure presents a comparison for a specific LLM and bit precision.  The goal is to illustrate the effectiveness of ShiftAddLLM in achieving lower perplexity at comparable or lower latency than other state-of-the-art quantized LLMs.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}]