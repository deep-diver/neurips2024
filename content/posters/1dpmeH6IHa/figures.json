[{"figure_path": "1dpmeH6IHa/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of I2EBench, an automated system for evaluating the quality of editing results generated by instruction-based image editing (IIE) models. We collected a dataset of over 2000+ images from public datasets Lin et al. [2014], Guo et al. [2023b], Martin et al. [2001], Chen et al. [2021], Ancuti et al. [2019], Liu et al. [2021b,a], Qu et al. [2017], Nah et al. [2017], Shen et al. [2019], Wei et al. [2018] and annotated them with corresponding original editing instructions. To diversify the instructions, we used ChatGPT Achiam et al. [2023] to generate varied versions. With the collected images and the original/diverse editing instructions, we utilized existing IIE models to generate edited images. Subsequently, we developed an evaluation methodology to automatically assess the adherence of edited images to the provided instructions under different dimensions. We also implemented human evaluation to obtain human preferences for editing results of different IIE models. Finally, we analyzed the correlation between automated evaluation and human evaluation, confirming alignment with human perception.", "description": "This figure illustrates the I2EBench system's workflow.  It starts with human annotation of a large image dataset with original and diverse instructions for various editing tasks. These instructions are then used with different IIE models to generate edited images.  The quality of these edited images is assessed through automated evaluation using 16 different dimensions, and also via human evaluation comparing the automated scores with human preferences. Finally, alignment verification ensures that the automated evaluation aligns well with human perception.", "section": "1 Introduction"}, {"figure_path": "1dpmeH6IHa/figures/figures_2_1.jpg", "caption": "Figure 2: Visualization of the editing results on the proposed 16 evaluation dimensions using different IIE models, including InstructAny2Pix Li et al. [2023c], HIVE Zhang et al. [2023a], InstructEdit Wang et al. [2023b], InstructDiffusion Geng et al. [2023], InstructPix2Pix Brooks et al. [2023], MagicBrush Zhang et al. [2024a], MGIE Fu et al. [2024], and HQEdit Hui et al. [2024]. A detailed version can be found in supplementary materials.", "description": "This figure shows the results of different instruction-based image editing (IIE) models applied to various editing tasks, categorized into high-level and low-level editing. Each row presents an example image and the results of applying different models to the same editing instruction. High-level editing tasks involve changing the background or removing objects; Low-level editing tasks are related to image quality enhancements (e.g., haze removal). The figure demonstrates the effectiveness of each model on diverse editing tasks.", "section": "3 I2EBench"}, {"figure_path": "1dpmeH6IHa/figures/figures_5_1.jpg", "caption": "Figure 3: Word cloud visualization (a,b) and image quantity statistics (c) of I2EBench.", "description": "This figure visualizes the instructions used in the I2EBench dataset through word clouds, differentiating between original and diverse instructions.  The word clouds highlight the most frequent words and terms related to various editing tasks. The bar chart (c) displays the number of images collected for each of the 16 evaluation dimensions within I2EBench.", "section": "3.2 Human Annotation"}, {"figure_path": "1dpmeH6IHa/figures/figures_6_1.jpg", "caption": "Figure 4: Comparison of radar charts for I2EBench scores in different dimensions using (a) original instructions and (b) diverse instructions.", "description": "This figure presents a comparison of radar charts illustrating the I2EBench scores across various dimensions for different Instruction-based Image Editing (IIE) models.  The charts use two sets of instructions: (a) original instructions and (b) diverse instructions generated using ChatGPT. Each axis represents a specific dimension of image editing evaluation (e.g., Style Alteration, Haze Removal, Object Removal etc.). The length of each line from the center to the perimeter shows the score achieved by each model for that dimension. Comparing the (a) and (b) plots allows us to see how model performance varies with different instruction styles.", "section": "3.3 Human Evaluation"}, {"figure_path": "1dpmeH6IHa/figures/figures_8_1.jpg", "caption": "Figure 5: Alignment between I2EBench rank scores (Y-axis) and human scores (X-axis).", "description": "This figure presents a correlation analysis between I2EBench rank scores and human scores for each of the 16 evaluation dimensions.  The I2EBench rank score is derived from the automated evaluation of the I2EBench system. The human score is obtained through a separate human evaluation, where human annotators rank the editing results of different models.  Each plot shows the scatter plot of the I2EBench rank score versus the human score for a specific dimension. The Pearson correlation coefficient (\u03c1) is also provided for each dimension. This figure demonstrates the high correlation between the automated I2EBench evaluation and human perception, validating the effectiveness of the I2EBench benchmark.", "section": "3.3 Human Evaluation"}, {"figure_path": "1dpmeH6IHa/figures/figures_8_2.jpg", "caption": "Figure 4: Comparison of radar charts for I2EBench scores in different dimensions using (a) original instructions and (b) diverse instructions.", "description": "This figure compares the performance of different Instruction-based Image Editing (IIE) models across sixteen evaluation dimensions using two sets of instructions: original and diverse.  The radar charts visually represent each model's score for each dimension, allowing for a direct comparison of their strengths and weaknesses.  The use of two instruction sets helps assess the robustness of the models to variations in instruction phrasing.", "section": "Experiments"}, {"figure_path": "1dpmeH6IHa/figures/figures_9_1.jpg", "caption": "Figure 7: Comparison of radar charts for I2EBench scores in different categories using (a) original instructions and (b) diverse instructions. The scores of all dimensions are normalized and averaged.", "description": "This figure compares the performance of different IIE models across various content categories using both original and diverse instructions. The radar charts visually represent the normalized average I2EBench scores for each model within each category (Animal, Object, Scenery, Plant, Human, Global).  This allows for a comparison of model strengths and weaknesses across different types of image editing scenarios.", "section": "Insights"}]