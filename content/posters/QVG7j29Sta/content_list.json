[{"type": "text", "text": "Accuracy is Not All You Need ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Abhinav Dutta Microsoft Research Bangalore, India t-abdutta@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Sanjeev Krishnan Microsoft Research Bangalore, India sakrishnan@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Nipun Kwatra Microsoft Research Bangalore, India nipun.kwatra@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Ramachandran Ramjee   \nMicrosoft Research   \nBangalore, India   \nramjee@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When Large Language Models (LLMs) are compressed using techniques such as quantization, the predominant way to demonstrate the validity of such techniques is by measuring the model\u2019s accuracy on various benchmarks. If the accuracies of the baseline model and the compressed model are close, it is assumed that there was negligible degradation in quality. However, even when the accuracies of the baseline and compressed model are similar, we observe the phenomenon of flips, wherein answers change from correct to incorrect and vice versa in proportion. We conduct a detailed study of metrics across multiple compression techniques, models and datasets, demonstrating that the behavior of compressed models as visible to end-users is often significantly different from the baseline model, even when accuracy is similar. We further evaluate compressed models both qualitatively and quantitatively using MT-Bench and show that compressed models exhibiting high flips are worse than baseline models in this free-form generative task. Thus, we argue that accuracy and perplexity are necessary but not sufficient for evaluating compressed models, since these metrics hide large underlying changes that have not been observed by previous work. Hence, compression techniques should also be evaluated using distance metrics. We propose two such distance metrics, KL-Divergence and $\\%$ flips, and show that they are well correlated. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The high cost and latency of Large Language Models (LLMs) has motivated the design of multiple model compression techniques for optimizing LLM efficiency such as quantization (Dettmers et al., 2022), Key-Value (KV) cache compression (Ge et al., 2023), pruning (Sun et al., 2023) and sparsification (Ashkboos et al., 2024). However, today, there is no standardized way of evaluating the effectiveness of these techniques. ", "page_idx": 0}, {"type": "text", "text": "The predominant way of establishing the validity of the LLM compression methods today is to report accuracy on selected benchmark tasks such as MMLU (Hendrycks et al., 2021a), Hellaswag (Zellers et al., 2019), ARC (Clark et al., 2018), LAMBADA (Paperno et al., 2016), etc. It is assumed that if the compressed model preserves accuracy on such benchmarks, it can be used as an equivalent replacement for the baseline model. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we conduct a detailed evaluation of various compression techniques. We find that while the difference in the aggregate accuracy metric across various benchmarks between the baseline and compressed LLM is negligible in most cases $:\\leq2\\%$ ), the actual percentage change in the answers can be significant $\\left(\\ge5\\%\\right)$ ). In other words, even when the overall accuracy is unchanged, a large number of correct answers change to incorrect and vice versa in proportion (we call these flips), between the baseline and compressed model. To the best of our knowledge, we believe that we are the first to identify this phenomenon of filps caused due to model compression. Further, we argue that flips serves as an intuitive metric that captures how significantly different the compressed model is from the baseline model, even when both models exhibit similar accuracy on various benchmarks. ", "page_idx": 0}, {"type": "image", "img_path": "QVG7j29Sta/tmp/20cc8760945bc29ab46153909fc217d927b6293cc721c779c03e224047ece2ce.jpg", "img_caption": ["Figure 1: All six quantization schemes show negligible difference in accuracy compared to baseline 16-bit model (Llama2-chat 7B, 13B, 70B and Yi-chat 6B, 34B) in seven different tasks. However, all schemes, except GPTQ W8A16 (8-bit weight, 16-bit activation), exhibit large number of flips, indicating severe divergence in model behavior. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Figure 1 shows the change in accuracy and filps $\\%$ vs baseline 16-bit model, respectively, for six quantization schemes on seven benchmark tasks (MMLU (Hendrycks et al., 2021a), Hellaswag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), ARC Easy and Challenge (Clark et al., 2018) PIQA (Bisk et al., 2019), and Winogrande (Sakaguchi et al., 2019)). We see that all quantization schemes have negligible difference in accuracy $(\\mathbf{0}-2\\%)$ compared to the 16-bit version. However, except for GPTQ W8A16 (8-bit weight, 16-bit activation Frantar et al. (2023)) that preserves accuracy with negligible flips, all other quantization schemes exhibit large number of flips (up to $13.6\\%$ ), indicating significant divergence from the baseline model. ", "page_idx": 1}, {"type": "text", "text": "Figure 3 shows similar behavior of MMLU task accuracy being preserved while filps increase, for two other compression techniques, namely, layer dropping (Gromov et al., 2024) and WANDA weight pruning (Sun et al., 2023). For example, while Gromov et al. (2024) showed that dropping the last few layers of a model did not affect its accuracy on standard benchmarks, we find a steady, almost linear increase in the number of flips with the number of layers being dropped. ", "page_idx": 1}, {"type": "text", "text": "The phenomenon of flips is puzzling at first glance. While it is easy to see that some correct answers may become incorrect due to errors induced by compression, it is difficult to explain how an approximately equal number of incorrect answers become correct such that overall accuracy is preserved! For example, MMLU questions have 4 options, one of which is correct. Thus, any output change could move a correct answer to an incorrect one but there is only 1 in 3 chance for an incorrect answer to land on the correct option. We present a detailed analysis of filps in Section 5. Furthermore, we observe that simply adding Gaussian noise to model weights can reproduce this filps phenomenon (see Table 1). This suggests filps arise from the inherent approximations introduced by compression rather than any new information or learning during the compression process. ", "page_idx": 1}, {"type": "text", "text": "Finally, one might question whether filps matter if accuracy is preserved. Indeed, if the downstream task where the LLM is used closely matches the benchmark task, accuracy alone might suffice. However, LLMs are typically used in a variety of downstream tasks that require generating free-form text, where accuracy evaluated on some standard question-answering tasks could be a poor proxy. Thus, we evaluate the compressed models using MT-Bench (Zheng et al., 2023), a multi-turn dialogue task. We show through qualitative evaluation as well as using GPT4 as an automated judge that compressed models with high number of flips are significantly worse than baseline models in this task (see Section 6). ", "page_idx": 1}, {"type": "table", "img_path": "QVG7j29Sta/tmp/b89f8c4a902a35aa17093e0f803ce8dcab529a5e368abea1dabfc29c4e6a591e.jpg", "table_caption": ["Table 1: Adding Gaussian noise to weights results in approximately equal correct $\\rightarrow$ incorrect and incorrect $\\rightarrow$ correct transitions, with the overall model accuracy mostly unchanged. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Since the goal of compression schemes is to create models that mimic the baseline models as closely as possible, we argue that compressed models are better judged by distance metrics with respect to baseline, in addition to capability metrics such as accuracy alone, as is the practice today. We demonstrate that well-known distance metrics like KL-Divergence on a given dataset can better identify the differences created due to various compression techniques and this metric correlates well with flips. Further, we show that the scores on MT-Bench (which evaluates free-form generation capabilities of these models) is highly correlated with filps. Thus, we propose that filps, an intuitive and inexpensive to compute metric, as a potential proxy distance metric for evaluating LLM compression techniques. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we make the following key contributions: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Using detailed qualitative and quantitative evaluation of various compression techniques, we show that accuracy is not sufficient as an evaluation metric for LLM compression techniques.   \n\u2022 We demonstrate the existence of filps as a general phenomenon and explain why they occur.   \n\u2022 We evaluate compression techniques using the KL-Divergence distance metric and show that it correlates well with flips.   \n\u2022 We propose that, where appropriate, filps be used as an intuitive distance metric for evaluating the quality of compression techniques. ", "page_idx": 2}, {"type": "text", "text": "2 LLM Evaluation Metrics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We compare baseline and compressed LLMs on the following metrics: ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "\u2022 Accuracy - capability metric: $\\%$ correct answers, for question-answering tasks. This determines the competency of the model for a particular task. Multiple-choice questionanswering (MCQ) tasks such as MMLU expect the model to output a single token for the correct answer (A/B/C/D), and compare this token with the target answer. For other tasks (like PIQA, Hellaswag, ARC), where the model assigns a probability to an option (consisting of multiple tokens), we report the standard normalized accuracy (Eleuther, 2021). ", "page_idx": 2}, {"type": "text", "text": "\u2022 Perplexity(Jelinek et al., 2005) - capability metric: This measures the overall language modelling capability of an LLM. It is defined as $e$ (Average Negative Loglikelihood) calculated over a dataset. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Flips - distance metric: measures the $\\%$ of questions whose answers changed from correct $\\rightarrow$ incorrect or incorrect $\\rightarrow$ correct, between baseline and quantized model for all tasks that have correct/incorrect answers. Note that, we do not include incorrect $\\rightarrow$ incorrect transition in Flips for two reasons: 1) For non-MCQ tasks such as $\\mathrm{GSM8k}$ (Cobbe et al., 2021b), TriviaQA (Joshi et al., 2017), etc. exact per-token output matches between different models are rare, resulting in many mismatches. Thus, including this transition may artificially inflate the metric for these tasks. 2) For MCQ tasks, users may care less about these incorrect $\\rightarrow$ incorrect transitions. Nevertheless, $i f$ we include incorrect $\\rightarrow$ incorrect transitions for MCQ tasks, we find that, the filps numbers reported in this paper would further increase by another $20\u201340\\%$ (e.g., increase of $I{9\\%}$ in Hellaswag, $4l\\%$ in ARC and $43\\%$ in MMLU! See Table 11) ", "page_idx": 2}, {"type": "text", "text": "\u2022 KL-divergence(Kullback and Leibler, 1951) - distance metric: consider a dataset having samples with multiple-choice answer options , where the ${\\dot{\\mathbf{j}}}\\cdot$ -th token of the i-th answer option has a probability distribution $P_{b}(i,j)$ across all tokens in the vocabulary of the baseline model, and $P_{q}(i,j)$ for the quantized model. Then the KL-divergence between the models for the entire dataset is the mean of KL-divergences across all tokens of all answer options and all samples in the dataset. ", "page_idx": 3}, {"type": "equation", "text": "$$\nK L~d i v={\\frac{1}{N}}\\sum_{d a t a s e t}{\\frac{1}{\\left|o p t i o n s\\right|}}\\sum_{i\\in o p t i o n s}{\\frac{1}{\\left|t o k e n s\\right|}}\\sum_{j\\in t o k e n s}D_{K L}(P_{b}(i,j)||P_{q}(i,j))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Nu$ is the number of samples in the dataset and $D_{K L}(P||Q)$ is the standard KLDivergence between two probability distributions. ", "page_idx": 3}, {"type": "text", "text": "The flips metric is propitious because it is a proxy distance metric that is easily interpretable by end-users\u2013 for question-answering tasks, the end user typically cares about the correct/incorrect answers and not the underlying probability distribution of tokens. Further, the filps metric is as easy to calculate as accuracy for any dataset. ", "page_idx": 3}, {"type": "text", "text": "It is important to distinguish between capability metrics (accuracy and perplexity in this study) and distance metrics (KL-Divergence and flips in this study). This distinction is necessary because the goal of a compression scheme is to create a more efficient model that closely mimics the baseline model rather than to create a more capable model. In other words, a quantized model is intended to serve as a drop-in replacement for the baseline model with minimal impact on end-users. Therefore, we argue that distance metrics are more suitable for judging the effectiveness of quantization or other compression schemes. ", "page_idx": 3}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We have measured the above metrics on multiple LLMs using multiple quantization techniques and bit lengths, on several tasks, as listed below: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Models: We primarily used the Llama2 (7B, 13B, 70B) chat (Touvron et al., 2023), Yi (6B, 34B) chat (01.AI et al., 2024), Llama3 (8B, 70B) (Dubey et al., 2024) and Qwen2 (1.5B, 7B, 72B) (Yang et al., 2024) families of models. The chat versions were used because they can be evaluated on MT-Bench (Zheng et al., 2023). However, we have observed a similar phenomenon in their pretrained non-chat versions as well (see Table 12). ", "page_idx": 3}, {"type": "text", "text": "\u2022 Quantization: We have evaluated LLM.int8() (Dettmers et al., 2022) as implemented in Bitsandbytes (Dettmers, 2024), with its 8-bit and 4-bit versions (referred to as BnB W8A8 and BnB W4A4 respectively) with default parameters supported with HuggingFace Transformers (Wolf et al., 2020). We used GPTQ (Frantar et al., 2023), AWQ (Lin et al., 2024) with group-size 128 with other parameters being default. We used Smoothquant (Xiao et al., 2024) (referred to as SQ W8A8) with per-token, per-channel quantization using $\\alpha=0.5$ . We use TensorRT (NVIDIA, 2024) for SmoothQuant, all other schemes were evaluated using HuggingFace Transformers. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Tasks: ", "page_idx": 3}, {"type": "text", "text": "1. For the Llama2 and Yi families, we evaluate the compressed models on ten different tasks. They include MMLU (Hendrycks et al., 2021a) Table 4, ARC (Clark et al., 2018)(easy Table 7 and challenge Table 8), PIQA (Bisk et al., 2019) Table 5, Winogrande (Sakaguchi et al., 2019) Table 10, Hellaswag (Zellers et al., 2019) Table 6, and Lambada (Zellers et al., 2019) Table 9. We also use GSM8k (Cobbe et al., 2021a) Figure 13, TriviaQA Joshi et al. (2017) Figure 14 and MT-Bench (Zheng et al., 2023) to evaluate models on generative tasks. MT-Bench is a dataset with 80 two-turn questions which can test generative capabilities of a model. In this study, we have used GPT-4 (OpenAI et al., 2024) (v0314) as judge, to generate the scores reported in Table 2.   \n2. For the Qwen2 and Llama3 families, we evaluate on MMLU Table 17, GSM8k Table 21, ARC (easy Table 18, challenge Table 19), MATH (Hendrycks et al., 2021b) Table 20, BFCL (Yan et al., 2024) Figure 23, and Scrolls-Quality (Shaham et al., 2022) Table 22 ", "page_idx": 3}, {"type": "image", "img_path": "QVG7j29Sta/tmp/a68cfbab79abb5b715223a77c8aefc987b25e5754e7aeff5720fb33b36309f12.jpg", "img_caption": ["Figure 2: Flips and KL Divergence are well correlated. Each point corresponds to a model, quantization combination in Table 4 "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "\u2022 Harness- We used Eleuther AI\u2019s eval-harness (Gao et al., 2023) for all the experiments, unless specified otherwise. Note that the standard benchmarks (ARC, MMLU, PIQA, Hellaswag, LAMBADA, GSM8k, TriviaQA, MATH, etc.) results on all models use greedy decoding, making these results fully deterministic. ", "page_idx": 4}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present extensive evidence for flips across various quantization and pruning schemes, evaluated over a large number of models and all tasks. Results for MT-Bench are presented in Section 6. ", "page_idx": 4}, {"type": "text", "text": "4.1 Quantization schemes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Summary of our results is highlighted in Figure 1 while the performance on each of the individual seven tasks (MMLU, PIQA, Hellaswag, ARC Easy, ARC Challenge, LAMBADA and Winogrande) are in Tables 4 to 10, respectively, in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "The main observations from our experiments with quantized models can be summarized as follows: ", "page_idx": 4}, {"type": "text", "text": ". Accuracy: Accuracy is preserved within $1\\%$ for the majority of the quantization methods, tasks and models (see Tables 4- 9). This indicates that accuracy is not sufficient to distinguish between precise and permissive quantization schemes. ", "page_idx": 4}, {"type": "text", "text": "2. Flips: The large $\\%$ filps $(\\geq5\\%)$ is a general trend, which holds over different models, almost all quantization schemes, and tasks (see Tables 4- 10). Specifically, all quantization schemes except GPTQ W8A16 $(\\leq1\\%)$ have significant $\\%$ filps . Lower bit quantization schemes have greater $\\%$ flips in general, indicating greater difference in behavior from the baseline (for example, on MMLU, BnB W4A4 has, on average, $2.4\\times$ more flips than BnB W8A8). We focus on Flips in this study, but AllFlips (Flips + inc $)r r e c t{\\rightarrow}i$ incorrect transitions ) results can be found in Figure 10, and Table 11 in Appendix. ", "page_idx": 4}, {"type": "text", "text": "3. KL-Divergence vs Flips: From Figure 2, we observe that the two distance metrics $K L$ - Divergence and %flips are well correlated. For example, their Spearman correlation on the MMLU benchmark is 0.981. ", "page_idx": 4}, {"type": "text", "text": "4. Impact of task type: ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "\u2022 MCQ Tasks \u2013 Generally easier tasks (identified by higher average accuracy) have smaller $\\%$ filps. For example, MMLU which is a relatively hard task has $8\\mathrm{-}16\\%$ filps for Bitsandbytes W4A4 whereas for the same technique, PIQA, an easier task, has $3{-}6\\%$ flips. The reason for this behavior is explained in Section 5.   \n\u2022 Generative Tasks \u2013 Surprisingly, such tasks have much more flips than MCQ ones. For example, GSM8K (Table 13, Table 21), a hard task that requires reasoning over multiple steps, exhibits a significant amount of flips $10{-}25\\%$ for BnB W8A8 and W4A4). Similarly, in MATH Table 20, we observe $5.15\\%$ flips for BnB W4A4. However, filps are quite small $(2{-}4\\%)$ in easier tasks like TriviaQA(Table 14) that tests trivia question answering capabilities. ", "page_idx": 4}, {"type": "image", "img_path": "QVG7j29Sta/tmp/d083a1834866408ca483487f4dd273402cb42a7373c0cbbb0164a7e2e59892a8.jpg", "img_caption": ["Figure 3: MMLU 5-shot accuracy difference and filps for two compression techniques (Llama2-13b model). Even at early stages of pruning with no accuracy difference, filps indicate model divergence. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "5. Impact of model size: Larger models typically have fewer flips than smaller ones. For example, on the MMLU benchmark with BnB W4A4, Llama2-70b chat has $5.6\\%$ filps, while Llama2-13b chat and Llama2-7b chat have $1.4\\times$ and $1.6\\times$ more flips, respectively. This may be because larger models are more resistant to perturbations introduced by compression than smaller ones. ", "page_idx": 5}, {"type": "text", "text": "4.2 Other model compression techniques ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We also evaluate the following three compression techniques, though on a smaller set of tasks and models. Our general observations seen above holds. ", "page_idx": 5}, {"type": "text", "text": "1. Dropping last n-layers (Gromov et al., 2024): This work demonstrated that dropping the last few layers did not affect the accuracy on standard benchmarks. We find in Figure 3(a) that as one keeps dropping layers, even though the accuracy increases only modestly, $\\%$ filps increases significantly, demonstrating that the resulting models keep deviating further away from the baseline. ", "page_idx": 5}, {"type": "text", "text": "2. Wanda (Sun et al., 2023): This is a pruning method. We observe in Figure 3(b) that as we increase the pruning ratio, even though accuracy barely changes, $\\%$ flips increases steadily. ", "page_idx": 5}, {"type": "text", "text": "3. SliceGPT (Ashkboos et al., 2024): This is a model sparsification method which drops a certain fraction of rows and columns of each dense matrix. We observe in Figure 9 in Appendix that even at very low sparsity ratios $\\%$ flips is significant indicating that the compressed models are probably very different from baseline. ", "page_idx": 5}, {"type": "text", "text": "4.3 Perplexity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Though we have focused on accuracy so far, our observation that the difference between two models\u2019 output token values cancel out leaving the average metric result unchanged, is applicable to perplexity as well. In particular, since perplexity may be interpreted as the inverse of the geometric mean of token probabilities, lower probabilities for some tokens in the test dataset may be cancelled by higher probabilities of other tokens. This indicates that perplexity alone is also inadequate in evaluating model compression schemes. Therefore, we argue that along with perplexity, KL-Divergence between the distributions generated by the baseline and optimized models should also be reported. ", "page_idx": 5}, {"type": "text", "text": "Figure 11 in Appendix plots the log-likelihood difference between the 16-bit and quantized model for each of the tokens in the wiki-2 dataset (Merity et al., 2016) for four different quantization schemes. From the figure, it appears that the log-likelihoods of the quantized model is just the log-likelihood of the baseline model with some symmetric noise added. Now, since perplexity is $e^{-a v g(l o g p r o b a b i l i t i e s)}$ , adding any amount of symmetric noise leaves it unchanged. For example, addition of Gaussian noise to the log-probability outputs of the model maintains the perplexity, while the quality of generation degrades as the standard deviation of the added noise increases (see Table 29). This analysis demonstrates one key weakness with the perplexity metric when used for evaluating compression techniques. While it is not clear if adding Gaussian noise to the log-likelihoods is an accurate representation of the behavior of compression schemes, it appears to be a reasonable proxy. As we shall see in Section 6, as quantization increases, there is steady degradation in the quality of the text generated by the model that are visible only by examining them closely. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Analyzing Flips ", "text_level": 1, "page_idx": 6}, {"type": "image", "img_path": "QVG7j29Sta/tmp/72cf121cbc60de4efaa00aa0e6a31a64276aa22b4844efbc90abefc53255097b.jpg", "img_caption": ["Figure 4: When the Top Margin is low, answer will more likely change (Llama2-70b, BnB W4A4, MMLU 5-shot) "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "QVG7j29Sta/tmp/836e071b24f86ec9e44ce93c05f428af7d7ba507608ce523c79da0c924a4423b.jpg", "img_caption": ["Figure 5: When the Top Margin is low, answer will more likely be incorrect (Llama2-70b, MMLU 5-shot) "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "One of the interesting observations in this study has been that when we quantize models, the number of questions where the LLM\u2019s answers go from incorrect to correct (referred to as incorre $c t\\rightarrow c o r r e c t\\,;$ ) is roughly equal to the number that goes the other way. This may seem unintuitive, because one might expect $c o r r e c t\\rightarrow i n c o r r e c t\\gg i n c o r r e c t\\rightarrow c o r r e c t,$ since a) the number of questions with correct answers is usually greater than incorrect answers, so random perturbations should cause more correct answers to flip, and b) given a correct answer, the correct to incorrect transition should be likelier because changing to any of multiple other incorrect options suffices, but given an incorrect answer, the incorrect to correct transition happens only if somehow the perturbation caused by quantization helps it land on the one correct option out of many. But we observe that this is not the case (and indeed, the opposite may also be true in some cases!). ", "page_idx": 6}, {"type": "text", "text": "To help explain the above phenomenon, we introduce a metric called top margin which is the difference in token probability between the best and the second best answer option. By best (secondbest) option, we mean the option that was given the highest (second highest) probability. Higher top margin on a question indicates that the model is more confident about its answer. ", "page_idx": 6}, {"type": "text", "text": "Answers are likely to change when top margin is low. Quantization introduces some noise in the weights and activations, due to which there is a perturbation in the output answers\u2019 probabilities (verified empirically). Thus, we expect that answers are more likely to change when top margin is low, since a small increase or decrease in probabilities can cause the best and second best options to swap (see Figure 4). To further bolster this claim, we show that the changes in probabilities do not depend on top margin, i.e., roughly all questions undergo the same amount noise (except when the top-margin is very high, where we do not see much change in probabilities after compression, but such questions are not likely to flip anyway) as seen in Figure 7. We further find top margins are well correlated before/after compression (i.e low confidence answers are likely to remain so and vice-versa) in Figure 8. In subsection A.4 we find that due to this reason, it is very likely that the same question (with low top margin) would be flipped by multiple quantization schemes. ", "page_idx": 6}, {"type": "text", "text": "Correct (incorrect) answers have higher (lower) top margin and are thus less (more) likely to filp. Table 27 shows the top margins for questions for which the LLM\u2019s answer is correct and when the answer is incorrect. We observe that, top margin when correct is, on average, greater than the top margin when incorrect. This is demonstrated in Table 28 which shows that flips amongst incorrect answers is indeed higher by $2\\times$ or more. Similarly, Figure 5 also shows that when top margin is low, the answer is more likely incorrect. Thus, correct answers filp much less often than incorrect answers. ", "page_idx": 6}, {"type": "table", "img_path": "QVG7j29Sta/tmp/b476b01cab0ecd2f6a177ea2bfed8d8e72d4337828c7599318e453dc4d6c880b.jpg", "table_caption": ["Table 2: MT-Bench: Average of turn-1 and turn-2 scores, as evaluated by GPT4 "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For incorrect answers, we would expect roughly $33\\%$ chance of them ending correct (for 4-choice MCQ), though the actual $\\%$ is typically higher because all the remaining options are not equally likely. Thus, the combination of incorrect answers flipping more along with slightly higher odds than random in landing on the correct answer results in incorrect $\\rightarrow$ correct transitions roughly matching correct $\\rightarrow$ incorrect transitions. ", "page_idx": 7}, {"type": "text", "text": "In subsection A.6 we provide some additional empirical analysis for filps, especially for generative tasks where the above top-margin analysis does not strictly apply. ", "page_idx": 7}, {"type": "text", "text": "6 MT-Bench evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we use MT-Bench (Zheng et al., 2023) to evaluate the quantized models\u2019 free-form text generation capabilities, using both GPT4 as judge as well as through manual inspection of the model responses. ", "page_idx": 7}, {"type": "text", "text": "We first use GPT-4 as a judge and perform automated evaluation. Table 2 shows the MT-Bench average scores for the two turns in the benchmark (individual turn 1 and 2 scores can be found in Tables 15 and 16 in the Appendix). From the results, we can observe that ", "page_idx": 7}, {"type": "text", "text": "\u2022 Most quantization methods degrade the MT-Bench score for the larger models, by $5\\%$ for Llama2-70b chat and $1.5\\%$ for Yi-34b chat (Table 2).   \n\u2022 The degradation in MT-Bench score is higher for the harder turn-2 problem than for turn-1, with up to $10\\%$ loss for Llama2-70b chat and $5\\%$ for Yi-34b chat(Table 16).   \n\u2022 Some quantization methods do slightly better than the baseline in MT-Bench score for smaller models but given their lower overall absolute score, we believe this variation is likely caused by the inaccuracies in GPT4 evaluation process. ", "page_idx": 7}, {"type": "text", "text": "For the different compressed models, we compare them on flips in MMLU vs their difference from baseline on MT-Bench scores in Figure 6a. For larger and more capable models, we find that filps in MMLU correlates well with MT-Bench score. ", "page_idx": 7}, {"type": "text", "text": "6.1 Qualitative evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next, we perform a detailed qualitative examination of the performance of these models. Specifically, we choose the Llama2-70B-chat model since it has the highest MT-Bench score (Table 2). We compare the 16-bit baseline against 8-bit and 4-bit models, quantized using LLM.int8(). We chose LLM.int8() as it matches the accuracy of the baseline on most tasks and also has the highest GPT4 scores among the W8A8 and 4-bit quantized models for this task (Table 2). ", "page_idx": 7}, {"type": "text", "text": "We summarize our findings of the qualitative analysis for a sample of ten questions (out of $\\approx30$ that had similar issues) from MT-Bench in Table 3. The corresponding generated text of all three models for these questions are provided in Table 34. Overall, we find that the 4-bit and 8-bit models are significantly worse than the 16-bit baseline. Specifically, we find that the 4-bit model often does not follow the provided instruction, makes more mistakes, and rambles a lot more, with the 8-bit model performing in-between the 16-bit and 4-bit models. ", "page_idx": 7}, {"type": "text", "text": "We encourage the reader to look at the full model responses in Table 34 (at least the first one!) to convince themselves that, at least for this task, there is significant degradation due to quantization, despite these two compressed models matching baseline accuracy on various tasks (e.g., MMLU accuracy within $1\\%$ ) and suffering only a 0.4 lower score on a scale of ten in the GPT4 evaluation. We believe that this qualitative analysis adds further evidence to our claim that benchmark accuracy alone, as is standard practice today, is a poor metric to evaluate compressed LLMs, especially, if they are likely to be used for generative tasks in downstream applications. ", "page_idx": 7}, {"type": "image", "img_path": "QVG7j29Sta/tmp/cadc6e1f49ee6026864a9af3089ff58e4d89baa8892eed4d9c8495fac9335fe9.jpg", "img_caption": ["Figure 6: Flips is a better predictor of downstream task performance than Accuracy ", "(a) Models with higher filps on MMLU get lower MTBench score. Spearman Corr. $=-0.75$ "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "QVG7j29Sta/tmp/35cc8fef59fd525965710c2a287f95acc6975074c7d143dbd0653ee40176d32b.jpg", "img_caption": ["(b) Models with lower accuracy usually get lower MTBench score though the relationship is not as clear. Spearman Corr. $=0.36$ "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "", "table_caption": ["Table 3: Qualitative evaluation of Llama2-70B-chat model text generations for MT-Bench prompts. Author\u2019s summary of model responses shown below; full model generated responses are in Appendix. These results substantiate a clear degradation in response quality with quantization. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "QVG7j29Sta/tmp/3e2ff831ae3220d7899daaf2bac37b626f75f39b7c10ec49f27e96d74bca3e2c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "7 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Predicting performance degradation of LLMs in the wild is a challenging and open problem, and it is possible that any metric calculated on standard benchmarks is insufficient. Other limitations are: ", "page_idx": 9}, {"type": "text", "text": "\u2022 If the downstream task is very similar to the benchmark on which the quantized model is tested, then accuracy may be sufficient, and distance metrics are not needed. \u2022 Flips is only a warning that the behaviour of a model and its compressed version is different \u2013 this may or may not materialize as visible degradation in some downstream tasks. \u2022 Our qualitative evaluation in Section 6.1 is subjective and may not be broadly representative. ", "page_idx": 9}, {"type": "text", "text": "8 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Given their versatility, LLMs are evaluated on a diverse set of tasks (Chang et al., 2024). Since accuracy is one of the most well-accepted metrics used in task evaluation, compression methods today typically focus on accuracy. However, we are not the first to point out the problem with over-reliance on aggregate metrics like accuracy when judging the quality of a model optimization scheme. Xu et al. (2021) have proposed label loyalty and probability loyalty as a metric to evaluate compressed BERT models . Other works like Joseph et al. (2021), Hooker et al. (2020), and Hooker et al. (2021) have shown compressed ImageNets to be more biased despite preserving accuracy and have proposed Knowledge Distillation based methods to address it. There has also been work (Hong et al., 2024) on evaluating LLM compression schemes on various trustworthiness dimensions. However, metrics for evaluating LLM compression techniques have not been studied widely so far, leading to over reliance on accuracy alone. ", "page_idx": 9}, {"type": "text", "text": "There have been many works on LLM evaluation that have shown shortcomings of existing evaluation methods. Lyu et al. (2024) have pointed out the misalignment between free-form generation and probability based evaluation on MMLU. Sclar et al. (2023) have shown LLMs to be very sensitive to prompt formatting. Zheng et al. (2024) have shown models to be biased towards a certain option in MCQ tasks. Alzahrani et al. (2024) have shown minor changes in the benchmarks leading to re-ordering of rankings, and Srivastava et al. (2024) has shown accuracies to be different when considering the functional equivalent of math problems. Jaiswal et al. (2024) have curated existing datasets to create their own benchmark that can be used to evaluate compressed models. Li et al. (2024) and Jin et al. (2024) have evaluated various quantization tasks on multiple tasks. Namburi et al. (2023) have studied the impact of compression and pruning on an LLM\u2019s parametric knowledge. Zhang et al. (2024) propose a number of other metrics in addition to accuracy such as fluency, informativeness, coherence and harmlessness. Chang et al. (2024) presents a detailed survey on evaluation of LLMs that covers what, where, and how to evaluate an LLM and lists several challenges in LLM evaluation. ", "page_idx": 9}, {"type": "text", "text": "However, to the best of our knowledge, none of the prior work have pointed out the phenomenon of filps, that occurs when LLMs are compressed, and the observation that higher filps is correlated with larger degradation in model performance despite accuracy matching with the uncompressed model. ", "page_idx": 9}, {"type": "text", "text": "9 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have examined metrics to evaluate the quality of compression methods for LLMs such as quantization. We distinguish between aggregate capability metrics such as accuracy, and distance metrics filps and KL Divergence between the compressed model and the baseline model. We justify why using distance metrics is more appropriate for evaluating model compression methods. We show that accuracy severely underestimates the true distance between models as perceived by the end user. We explain this is due to the presence of flips between correct and wrong answers when a model is quantized, and explain why the filps are nearly balanced, leading to similar accuracy, while the user-perceived output of the quantized model may be significantly different. We argue that distance metrics such as filps and KL-divergence are essential for evaluating all optimization methods which may change the model outputs and whose goal is to minimize end-user visible behaviour changes from a baseline model. We hope that better distance metrics as proposed in this work will enable research in model optimization and compression to progress faster and better meet user expectations on model output quality. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "01.AI, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Open Foundation Models by 01.AI. arXiv:2403.04652 [cs.CL] ", "page_idx": 10}, {"type": "text", "text": "Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi, Nora Altwairesh, Areeb Alowisheq, M Saiful Bari, and Haidar Khan. 2024. When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards. arXiv:2402.01781 [cs.CL] ", "page_idx": 10}, {"type": "text", "text": "Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024. SliceGPT: Compress Large Language Models by Deleting Rows and Columns. arXiv:2401.15024 [cs.LG] ", "page_idx": 10}, {"type": "text", "text": "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. PIQA: Reasoning about Physical Commonsense in Natural Language. arXiv:1911.11641 ", "page_idx": 10}, {"type": "text", "text": "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology 15, 3 (2024), 1\u201345. ", "page_idx": 10}, {"type": "text", "text": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457 ", "page_idx": 10}, {"type": "text", "text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021a. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021). ", "page_idx": 10}, {"type": "text", "text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021b. Training Verifiers to Solve Math Word Problems. arXiv:2110.14168 [cs.LG] ", "page_idx": 10}, {"type": "text", "text": "Dettmers. 2024. Bitsandbytes. https://github.com/TimDettmers/bitsandbytes. ", "page_idx": 10}, {"type": "text", "text": "Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 30318\u201330332. https://proceedings.neurips.cc/paper_files/paper/ 2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf ", "page_idx": 10}, {"type": "text", "text": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence ", "page_idx": 10}, {"type": "text", "text": "Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur \u00c7elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzm\u00e1n, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, IrinaElena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, ", "page_idx": 11}, {"type": "text", "text": "Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, V\u00edtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] ", "page_idx": 12}, {"type": "text", "text": "Eleuther. 2021. Multiple Choice Normalization in LM Evaluation. https://blog.eleuther.ai/ multiple-choice-normalization/. ", "page_idx": 12}, {"type": "text", "text": "Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ: Accurate PostTraining Quantization for Generative Pre-trained Transformers. arXiv:2210.17323 [cs.LG]   \nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. https://doi.org/10.5281/zenodo.10256836   \nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2023. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801 (2023).   \nAndrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A. Roberts. 2024. The Unreasonable Ineffectiveness of the Deeper Layers. arXiv:2403.17887 [cs.CL]   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring Massive Multitask Language Understanding. arXiv:2009.03300   \nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring Mathematical Problem Solving With the MATH Dataset. arXiv:2103.03874 [cs.LG] https://arxiv.org/abs/2103.03874   \nJunyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian Bartoldson, Ajay Jaiswal, Kaidi Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, and Bo Li. 2024. Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression. arXiv:2403.15447 [cs.CL]   \nSara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. 2021. What Do Compressed Deep Neural Networks Forget? arXiv:1911.05248 [cs.LG]   \nSara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising Bias in Compressed Models. arXiv:2010.03058 [cs.LG]   \nAjay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, and Yinfei Yang. 2024. Compressing LLMs: The Truth is Rarely Pure and Never Simple. arXiv:2310.01382 [cs.CL]   \nF. Jelinek, R. L. Mercer, L. R. Bahl, and J. K. Baker. 2005. Perplexity\u2014a measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America 62, S1 (08 2005), S63\u2013S63. https://doi.org/10.1121/1.2016299 arXiv:https://pubs.aip.org/asa/jasa/articlepdf/62/S1/S63/11558910/s63_5_online.pdf   \nRenren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, and Deyi Xiong. 2024. A Comprehensive Evaluation of Quantization Strategies for Large Language Models. arXiv:2402.16775 [cs.CL]   \nVinu Joseph, Shoaib Ahmed Siddiqui, Aditya Bhaskara, Ganesh Gopalakrishnan, Saurav Muralidharan, Michael Garland, Sheraz Ahmed, and Andreas Dengel. 2021. Going Beyond Classification Accuracy Metrics in Model Compression. arXiv:2012.01604 [cs.CV]   \nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv:1705.03551 [cs.CL]   \nS. Kullback and R. A. Leibler. 1951. On Information and Sufficiency. The Annals of Mathematical Statistics 22, 1 (1951), 79 \u2013 86. https://doi.org/10.1214/aoms/1177729694   \nShiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. 2024. Evaluating Quantized Large Language Models. arXiv:2402.18158 [cs.CL]   \nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv:2306.00978 [cs.CL]   \nChenyang Lyu, Minghao Wu, and Alham Fikri Aji. 2024. Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models. arXiv:2402.13887 [cs.CL]   \nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models. arXiv:1609.07843 [cs.CL]   \nSatya Sai Srinath Namburi, Makesh Sreedhar, Srinath Srinivasan, and Frederic Sala. 2023. The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models. arXiv:2312.00960 [cs.CL] ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "NVIDIA. 2024. TensorRT. https://github.com/NVIDIA/TensorRT-LLM. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "penAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M\u00e9ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, ", "page_idx": 13}, {"type": "text", "text": "Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer\u00f3n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] ", "page_idx": 14}, {"type": "text", "text": "Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. arXiv:1606.06031 [cs.CL] ", "page_idx": 14}, {"type": "text", "text": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. arXiv:1907.10641 ", "page_idx": 14}, {"type": "text", "text": "Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023. Quantifying Language Models\u2019 Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting. arXiv:2310.11324 [cs.CL] ", "page_idx": 14}, {"type": "text", "text": "Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. SCROLLS: Standardized CompaRison Over Long Language Sequences. arXiv:2201.03533 [cs.CL] https://arxiv.org/abs/2201.03533 ", "page_idx": 14}, {"type": "text", "text": "Saurabh Srivastava, Annarose M B, Anto P V au2, Shashank Menon, Ajay Sukumar, Adwaith Samod T, Alan Philipose, Stevin Prince, and Sooraj Thomas. 2024. Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap. arXiv:2402.19450 [cs.AI] ", "page_idx": 14}, {"type": "text", "text": "Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2023. A Simple and Effective Pruning Approach for Large Language Models. arXiv:2306.11695 [cs.CL] ", "page_idx": 14}, {"type": "text", "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL] ", "page_idx": 14}, {"type": "text", "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. HuggingFace\u2019s Transformers: State-of-the-art Natural Language Processing. arXiv:1910.03771 [cs.CL] ", "page_idx": 14}, {"type": "text", "text": "Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2024. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. arXiv:2211.10438 [cs.CL] ", "page_idx": 14}, {"type": "text", "text": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian McAuley, and Furu Wei. 2021. Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. arXiv:2109.03228 [cs.CL]   \nFanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Berkeley Function Calling Leaderboard. https://gorilla.cs. berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html.   \nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 Technical Report. arXiv:2407.10671 [cs.CL] https://arxiv.org/abs/2407.10671   \nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a Machine Really Finish Your Sentence? arXiv:1905.07830   \nYue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Llmeval: A preliminary study on how to evaluate large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 19615\u201319622.   \nChujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2024. Large Language Models Are Not Robust Multiple Choice Selectors. arXiv:2309.03882 [cs.CL]   \nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. arXiv:2306.05685 [cs.CL] ", "page_idx": 15}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Outline ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Detailed results of Llama-2 chat and Yi-chat families A.1   \n2. Detailed MT-Bench results A.2   \n3. Detailed results of Llama-3 and Qwen-2 Families on various quantization schemes A.3   \n4. Consistency of Flips A.4   \n5. Miscellaneous Results A.5   \n6. Qualitative Analysis of Flips A.6   \n7. Full model responses to MT-Bench A.7 ", "page_idx": 16}, {"type": "text", "text": "A.1 Detailed results of Llama-2 chat and Yi-chat Families on various quantization schemes ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 4 shows five-shot accuracy on the for various models using the standard 16-bit quantization as baseline and difference in accuracy and percentage of filps for various lower-bit quantization schemes. For example, the accuracy of Bitsandbytes (Dettmers et al., 2022) 8-bit and 4-bit quantized models are only $0.55\\%$ and $0.78\\%$ away from the baseline Llama2-70b model respectively while the flips are $4.6\\%$ and $8.1\\%$ , respectively. Tables 5 through 10 show results for other zero-shot tasks such as PIQA, Hellaswag, ARC Easy, ARC Challenge, LAMBADA and Winogrande. ", "page_idx": 16}, {"type": "text", "text": "Table 4: MMLU 5-shot accuracy and flips for several models using 16-bit baseline and various quantization schemes. Change in accuracy is negligible $(0{-}2\\%)$ in all quantization schemes. However, except for GPTQ W8A16 (8-bit weights, 16-bit activation), all other schemes show large $\\%$ flips, indicating significant deviation of quantized model from the baseline 16-bit model. ", "page_idx": 16}, {"type": "table", "img_path": "QVG7j29Sta/tmp/930c239de16426628868f3611a984ab50f3394398c0b92873bf789a2ebeddd88.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "QVG7j29Sta/tmp/3141a5cc2322e841381cad7b89ce2500a8f87f47094ffc8c0a7eb0aad02e7419.jpg", "table_caption": ["Table 5: PIQA (0-shot) change in %accuracy / %flips "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "QVG7j29Sta/tmp/b6d6df6f8caaac88372295c504a83a48a83032c8dbc7ae6f7c3e41a1092362de.jpg", "table_caption": ["Table 6: Hellaswag (0-shot) change in %accuracy / %flips "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "QVG7j29Sta/tmp/79c2d72b0dffdab01f5e7da9f4417c2a5d3d592f0488658d4a8d97a78d3f3931.jpg", "table_caption": ["Table 7: ARC Easy (0-shot) change in %accuracy / %flips "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QVG7j29Sta/tmp/42cc904fb59b4f7198bdf8e3582008cbda6e83c34c06dc6f85eb298a55d68698.jpg", "table_caption": ["Table 8: ARC Challenge (0-shot) change in %accuracy / %flips "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QVG7j29Sta/tmp/2cd0ed36e066bbd67e4b3eb602e994c917ec46601cbc3401b36e5c8a812ade50.jpg", "table_caption": ["Table 13: GSM8k 8-shot Results ", "Table 14: Triviaqa 5-shot Result "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QVG7j29Sta/tmp/6ccc8e6bf3f22b606c3a449d0b83b6c958ba401ad733b12a64dc377a49f72f6b.jpg", "table_caption": ["Table 9: LAMBADA (0-shot) change in %accuracy / %flips "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QVG7j29Sta/tmp/221f6afa3545b26a36267a3db6918fa12d1202f0a665f362c07aae410705508b.jpg", "table_caption": ["Table 10: Winogrande (0-shot) change in %accuracy / %flips "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QVG7j29Sta/tmp/33b8491d8b7df960fb6f93a9de557ee631fc71b02a59caccbee418f501856c88.jpg", "table_caption": ["Table 11: MMLU 5-shot change in %accuracy and %AllFlips (including wrong \u2192wrong transitions) "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QVG7j29Sta/tmp/fa3fbbd1b97a93f5996db5f0adec7fc73722fbf203ba7e0fa184adf9ffc82473.jpg", "table_caption": ["Table 12: MMLU 5-shot Results on Pretrained Models change in %Accuracy/%Flips "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.2 MT-Bench Detailed results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Tables 15 and\u201816 show MT-Bench results for each of the two turns, respectively. ", "page_idx": 18}, {"type": "table", "img_path": "QVG7j29Sta/tmp/fe66392c84984759935ed31e4965129802ae0162a0a1189916179624994819fd.jpg", "table_caption": ["Table 15: Turn 1 MT-Bench Scores "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "QVG7j29Sta/tmp/1b1ca8ff24e14b60e46220904b8291c5759d73a870f1d57a24884775ff598ea7.jpg", "table_caption": ["Table 16: Turn 2 MT-Bench Scores "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.3 Detailed results of Llama-3 and Qwen-2 Families on various quantization schemes ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "QVG7j29Sta/tmp/cd34d74410a193564e7985feb1c016e69b189b3272593fc33ce10d13dc3e5325.jpg", "table_caption": ["Table 17: MMLU (5-shot) change in $\\%$ accuracy/ $\\%$ flips "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "QVG7j29Sta/tmp/34bb995566ab3408297f728e4a33a7828734dc2be81b609730deb0d481da9871.jpg", "table_caption": ["Table 18: ARC-Easy (0-shot) change in $\\%$ accuracy/ $\\%$ flips "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "QVG7j29Sta/tmp/30f6d1a9e42c383c82030842c6d2ef5beb3013686d17929448b784baeef0a290.jpg", "table_caption": ["Table 19: ARC-Challenge (0-shot) change in $\\%$ accuracy/ $\\%$ flips "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "QVG7j29Sta/tmp/d22c3c14a31c3b15f215a0adb1e39a1fb858f26e7386d02e5953e3fda175a7b5.jpg", "table_caption": ["Table 20: MATH (4-shot) change in $\\%$ accuracy/ $\\%$ flips "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "QVG7j29Sta/tmp/4e9dce1a25105ab465dd48f725c14c17aec36329d304f2da98d26c5beed65ab4.jpg", "table_caption": ["Table 21: GSM8k (8-shot) change in $\\%$ accuracy/ $\\%$ flips "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "QVG7j29Sta/tmp/5a520500505d9514ab8f4843c739712b4428cf374a7fd74eef2bb615c931e5d1.jpg", "table_caption": ["Table 22: Scrolls-Quality (0-shot) change in $\\%$ accuracy/ $\\%$ flips "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "QVG7j29Sta/tmp/4a530a59f98e4e7f16cd8c85a28826c7e8fec2bf6cbf02469ed6ba75f6f80919.jpg", "table_caption": ["Table 23: BFCL-greedy change in $\\%$ accuracy $\\slash\\;\\%$ flips "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "We report BFCL-greedy (tweaked BFCL to generate results greedily) results instead of BFCL-standard because by default, BFCL uses top_p sampling. We are interested in measuring perturbations introduced solely by quantization in this experiment and thus it is crucial to ensure that the sampling step does not add any further noise. To emphasize this point, we report two runs of BFCL-standard and show that there are significant flips between them (see below Table 24), making such evaluations irrelevant for measuring the effect of quantization. ", "page_idx": 20}, {"type": "table", "img_path": "QVG7j29Sta/tmp/03bcf398d63e18134ca1c3030ee529702eba96ed0f3b03ed628835eedc6143e0.jpg", "table_caption": ["Table 24: BFCL-standard change in $\\%$ accuracy $\\prime\\,\\%$ flips "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.4 Consistency of flips ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The following table Table 25 shows the $\\%$ of questions whose answers are changed by 0-6 quantization schemes (for Llama2-70b MMLU task 15K questions): ", "page_idx": 21}, {"type": "text", "text": "The main observations are 1) for most questions $(84\\%)$ , answers are unchanged by all the quantization schemes (and these, as expected, have high top margin). 2) Out of the remaining $16\\%$ , roughly half $(8\\%)$ of the questions are commonly changed by two or more schemes indicating significant overlap between the schemes. Our conclusion is that flips are mostly consistent across schemes. ", "page_idx": 21}, {"type": "text", "text": "For a more fine-grained analysis, we also report pairwise fraction of changed examples that are common between two quantization schemes in Table 26. We use overlap coefficient $\\begin{array}{r l}{O v e r l a p(A,B)=}\\end{array}$ min|(A|\u2229A|B,||B|), to quantify this where A and B are the set of examples changed by the schemes. We again see good overlap, ranging between 0.4\u20130.5 ", "page_idx": 21}, {"type": "text", "text": "Table 25: $\\%$ of questions impacted by a varying number of schemes and their corresponding top margins. ", "page_idx": 22}, {"type": "table", "img_path": "QVG7j29Sta/tmp/513a6f819fc163ef5cb54740c9b5331dba7fa1541e8b09d6801013016c681f9f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "QVG7j29Sta/tmp/7e46217151fd366975e6f5107c87b73cd2d4a37275d3315333adbe0ef44e4829.jpg", "table_caption": ["Table 26: Comparison of overlap coefficient amongst different quantization schemes: BnB W8A8, BnB W4A4, and GPTQ W4A16. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "A.5 Misc. Results ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "QVG7j29Sta/tmp/c39a561ecf4355109daa8457256cbc46743becbd36f4821abb6a3848151dba1b.jpg", "table_caption": ["Table 27: Top margin when answer is correct/wrong. Top margin is higher for correct answers. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "QVG7j29Sta/tmp/c72d0eead0a66dde5327c59f52a4bc1d26765f9bdf23bfff8965acdd2ddcddb6.jpg", "table_caption": ["Table 28: MMLU 5-shot. The first/second number indicates the $\\%$ of correct/incorrect answers of the baseline model that changed. We see that more $\\%$ of incorrect answers change. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "QVG7j29Sta/tmp/ffdf345cd8430c4c70ac59b3f13f872e6fdf49e63f9d223363549be3934c4691.jpg", "table_caption": ["Table 29: Adding Gaussian noise worsens output quality while keeping perplexity same. %correct tokens measure the $\\%$ of tokens in the input that would have been selected greedily by the model (all results with Llama2-13b chat) "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "QVG7j29Sta/tmp/cbed3ad65bce0ae6a8be9c75d4f036b86f6e4b758ce743f2ec1ab66508623210.jpg", "img_caption": ["Figure 7: Change in prediction probability vs top margin: For the MMLU 5-shot benchmark, we plot the baseline top-margin for the questions on the ${\\bf X}$ -axis, and the change in prediction probability of the choices in the quantized vs the baseline model on the y-axis (computed as $\\begin{array}{r}{\\frac{1}{4}\\sum_{i\\in A/B/C/D}|P r o b a b i l i t y_{b a s e}i\\ -}\\end{array}$ \u2212 $P r o b a b i l i t y_{q u a n t i z e d}i|\\rangle$ . We use the BnB W4A4 quantization scheme for these results. The observation holds across different quantization schemes. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "QVG7j29Sta/tmp/9891cb4408a43c8c11fea7c4c8a859019555da497ad25f467466b88b221459af.jpg", "img_caption": ["Figure 8: Top margin in baseline vs quantized models: Scatter plot of the top-margin for the MMLU 5-shot benchmark in the baseline vs quantized models. We use the Llama2 13b model and BnB W4A4 quantization scheme for this plot. The top-margins show strong correlation (Pearson correlation $=0.84\\$ ) and high top margins in baseline are likely to remain so in the quantized model. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "QVG7j29Sta/tmp/7a9bac84e5a253287d72635152b71d73afed869788d164a3f4f9a0c208316d36.jpg", "img_caption": ["Figure 9: SliceGPT, Accuracy and Flips vs Sparsity "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "QVG7j29Sta/tmp/8a8e6fc830717f816e041fbcb21c9d5d36d4a08e94cf14bbb0b95a6941879c5f.jpg", "img_caption": ["(a) Extending Figure 3(a) with AllFlips "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "QVG7j29Sta/tmp/6c322cbf575d2bdf15ec2d634d22d42ca9bae3ff4f81c31d22b7fcf5fced54f6.jpg", "img_caption": ["(b) Extending Figure 3(b) with AllFlips "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "QVG7j29Sta/tmp/d467b52cc66e389a4b1a36f8068ef49241ab18d152f0ae929e216e6c76beebcc.jpg", "img_caption": ["(c) Equivalent of Figure 1 with AllFlips that includes incorrect to incorrect transitions ", "Figure 10: AllFlips Results "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "QVG7j29Sta/tmp/c2be705a52959708a0f3ad21e4b7579ee129a8885cfb3d5b83d6585e0ff55213.jpg", "img_caption": ["(c) BnB W4A4(perplexity $=4.72$ , KL Div. $=0.03$ ) (d) BnB W8A8(perplexity $=4.61$ , KL Div. $=\\!5.8*10^{-3}$ ) ", "Figure 11: The loglikelihood difference plots are somewhat symmetric around zero indicating that even though average loglikelihoods (and perplexity) of the baseline and quantized models are similar, actual per token loglikelihoods might be very different.The results are calculated on Llama2-13b, wiki-2Merity et al. (2016) dataset where perplexity is 4.57. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "A.6 Qualitative Analysis of Flips ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We present some empirical observations below for the some of the tasks studied in this work. ", "page_idx": 27}, {"type": "text", "text": "\u2022 MATH ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This requires the model to just output the correct answer (and not the intermediate steps). We find it hard to clearly characterize the type of mistake made by the LLMs (especially because the range of subjects is very diverse). But , on many questions we find there to be simple mistakes (e.g., off by one errors, off by a factor of 2/10, etc). ", "page_idx": 27}, {"type": "text", "text": "Additionally, in the MATH dataset, each question is accompanied with a tag that indicates difficulty $(\\in[1,2,3,4,5])$ where a higher number indicates that the question is more difficult. This lets us validate our intuition that the questions that are answered by both the baseline and quantized models are the easiest (average difficulty 2.49), and the ones answered by exactly one of them are moderate (this is where correct incorrect transitions and vice versa happens, average difficulty 2.98) and the ones answered by neither are the hardest (average difficulty 3.56). The above numbers were obtained using Llama-3 8b and it\u2019s BnB W4A4 quantized version. ", "page_idx": 27}, {"type": "text", "text": "\u2022 GSM8k ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This task requires the LLM to generate the answer with step-by-step explanations. Here also, we find it hard to clearly characterize the type of mistakes made (though many of them can be classified as simple calculation errors) or pin down the specific type of question in which such phenomenon happens. ", "page_idx": 27}, {"type": "text", "text": "In this task specifically, we observe that at times the model gets stuck in a loop, resulting in no output at all (rather than an incorrect one). For example:- ", "page_idx": 27}, {"type": "text", "text": "\\*Question\\*: Travis had 61 apps on his tablet. He deleted 9 apps he didn\u2019t use anymore and downloaded 18 more. How many apps are on his tablet now?   \n\\*Llama-3-8b BnB W4A4 Answer\\* : Travis had 61 apps on his tablet.   \nHe deleted 9 apps he didn\u2019t use anymore and downloaded 18 more.   \nTravis had 61 apps on his tablet. He deleted 9 apps he didn\u2019t use anymore and downloaded 18 more.   \nTravis had 61 apps on his tablet. He deleted 9 apps he didn\u2019t use anymore and downloaded 18 more.   \nTravis had 61 apps on his tablet. He deleted 9 apps he didn\u2019t use anymore and downloaded 18 more. (.. . and so on ...) ", "page_idx": 27}, {"type": "text", "text": "We measure $\\%$ of such cases as $\\%$ invalid. We observe that quantization makes %invalid worse in general. For example :- $\\%$ invalid for Llama-3-8b, its BnB 8bit and BnB 4bit versions are $2.00\\%$ , $2.43\\%$ and $3.72\\%$ respectively. ", "page_idx": 27}, {"type": "text", "text": "\u2022 BFCL ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This task requires the LLM to call an API with the correct parameters to solve a given problem. The available API function definitions are given as a part of the prompt. Given the setup, we feel all problems can be considered to be of similar difficulty. After analyzing the results, we categorize the errors into the following groups: ", "page_idx": 27}, {"type": "text", "text": "1. incorrect API usage \u2013 this involves cases where the all the parameters are not initialized with their correct values or supplying more/less than the required number of parameters, etc. A very common type of error in this category is supplying parameters as a dictionary. For eg. we observed that the same function is called in two ways - sum $\\scriptstyle\\mathtt{a}=5,\\mathtt{b}=7]$ ) and sum(\u2019a\u2019:5, \u2019b\u2019:7), but only the first version is correct.   \n2. calling the wrong API \u2013 this involves calling an altogether different function than the one actually needed to solve the problem.   \n3. misunderstanding the question \u2013 this involves solving an orthogonal problem to the one required.   \n4. adding/omitting characters that make the API call non-parsable \u2013 self explanatory ", "page_idx": 27}, {"type": "text", "text": "In general, we observe that most of the errors are in the first group itself. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Scroll-Quality, ARC, etc ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "These questions are of MCQ type and it is easier to characterize flips in such tasks (dealt with in the main section already). For such, tasks, top margin which measures the confidence of the baseline model in its answer, is a good predictor of whether the answer will filp or not. ", "page_idx": 28}, {"type": "text", "text": "Representative examples of Flips ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "All the following experiments use Llama3-8B and it\u2019s BnB W4A4 version. ", "page_idx": 28}, {"type": "table", "img_path": "QVG7j29Sta/tmp/3976a2e8086abff1644d7b3f04d0bb9aa50423be3d8c3fe4e77ee7b3e69643ce.jpg", "table_caption": ["1. MATH \u2022 correct $\\rightarrow$ incorrect cases (baseline correct, quantized wrong "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "\u2022 incorrect $\\rightarrow$ correct cases (baseline correct, quantized wrong ", "page_idx": 28}, {"type": "table", "img_path": "QVG7j29Sta/tmp/7cfe74ad9246a948fe2425470b21e9a9d52fde1a34ce194a78695cd56e56fe40.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "2. BFCL ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 correct $\\rightarrow$ incorrect cases (baseline correct, quantized wrong ", "page_idx": 28}, {"type": "table", "img_path": "QVG7j29Sta/tmp/24bba68d93875d7c16476067e830af90fd52122fba34d0635217a470cedcb6b7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "QVG7j29Sta/tmp/1d7d570ed7b65cae5667928b49cf30a7f61a168fa3b600433f6f5ac645e7d0f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "\u2022 incorrect $\\rightarrow$ correct cases (baseline correct, quantized wrong ", "page_idx": 29}, {"type": "table", "img_path": "QVG7j29Sta/tmp/0b68b53565c356147fbfd119d79f96cdb9c841a2b9636c03db2fabd75a17c871.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "QVG7j29Sta/tmp/12ee4835824431d605e75ddab78ecd7515746446641b2e2136215a43a0a81c0d.jpg", "table_caption": ["3. GSM8k \u2022 correct $\\rightarrow$ incorrect cases (baseline correct, quantized wrong "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "QVG7j29Sta/tmp/b5438cda4895764a5ea24212c3e5c422ba66abe6ba38a9fa39b2d70d5a19d913.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "\u2022 incorrect $\\rightarrow$ correct cases (baseline correct, quantized wrong ", "page_idx": 30}, {"type": "table", "img_path": "QVG7j29Sta/tmp/5a239d248c52d609735e7966a664a730e29163b9445cb4928bec945cb11e8e2a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "A.7 Full model responses to MT-Bench ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The answers to all 80 questions can be found in the supplementary material. ", "page_idx": 30}, {"type": "text", "text": "Table 34: Full model responses by the 16-bit, 8-bit and 4-bit Llama2-70B-Chat model for ten sample MT-Bench prompts that was summarized in Table 3. Our highlights are in red. ", "page_idx": 30}, {"type": "text", "text": "Prompt: Consider a satellite that is in a circular orbit around the Earth. The speed of the satellite decreases. What will happen to the satellite\u2019s orbital radius and period of revolution? Please justify your answer using principles of physics. ", "page_idx": 31}, {"type": "text", "text": "If the speed of a satellite in a circular orbit around the Earth decreases, two things will happen:   \n1. The satellite\u2019s orbital radius will increase. 2. The satellite\u2019s period of revolution will increase.   \nTo understand why, let\u2019s consider the principles of physics that govern the behavior of satellites in orbit:   \n1. Conservation of Energy: The total energy of a satellite in orbit remains constant. The energy is a sum of kinetic energy (the energy of motion) and potential energy (the energy of position or height). As the satellite\u2019s speed decreases, its kinetic energy decreases, but its potential energy remains constant. To conserve energy, the satellite\u2019s altitude must increase, which means the orbital radius must increase.   \n2. Conservation of Angular Momentum: The total angular momentum of a satellite in orbit remains constant. Angular momentum is a measure of an object\u2019s tendency to keep rotating. As the satellite\u2019s speed decreases, its angular momentum decreases, but the angular momentum due to the Earth\u2019s rotation remains constant. To conserve angular momentum, the satellite\u2019s period of revolution must increase.   \nThese principles apply to any satellite in a circular orbit around the Earth. If the satellite\u2019s speed decreases, its orbital radius will increase, and its period of revolution will increase. If the speed of a satellite in a circular orbit around the Earth decreases, two things will happen:   \n1. The satellite\u2019s orbital radius will decrease. 2. The satellite\u2019s period of revolution will increase.   \nLet\u2019s justify these statements using principles of physics:   \n1. Orbital radius: The satellite\u2019s orbital radius is determined by its velocity and the gravitational force acting upon it. As the satellite\u2019s speed decreases, the gravitational force acting on it remains constant, causing the satellite to move closer to the center of the Earth. This decrease in distance from the center of the Earth results in a smaller orbital radius.   \n2. Period of revolution: The period of revolution is directly proportional to the satellite\u2019s distance from the center of the Earth. As the satellite\u2019s orbital radius decreases, its period of revolution increases. This is because the satellite now has a shorter distance to travel to complete one orbit, so it takes longer to do so.   \nIn summary, a decrease in the satellite\u2019s speed will cause its orbital radius to decrease and its period of revolution to increase. These changes are a result of the fundamental principles of physics, specifically the relationship between velocity, distance, and gravitational force. If the speed of a satellite in a circular orbit around the Earth decreases, the satellite will move to a lower orbit with a smaller radius. This is because the satellite\u2019s kinetic energy, which is proportional to its speed, has decreased, and the satellite\u2019s potential energy, which is proportional to its distance from the center of the Earth, has increased. As a result, the satellite\u2019s total energy, which is the sum of its kinetic and potential energy, remains constant.   \nAccording to Kepler\u2019s second law of planetary motion, the line connecting the satellite to the center of the Earth sweeps out equal areas in equal times. This means that the satellite\u2019s period of revolution will remain constant, as the satellite moves to a lower orbit with a smaller radius.   \nTo understand why the period of revolution remains constant, consider the following:   \n1. The satellite\u2019s speed has decreased, which means it takes longer to complete one orbit. 2. However, the satellite is now closer to the center of the Earth, which means it has a shorter distance to travel to complete one orbit.   \n3. The combination of the satellite\u2019s slower speed and shorter distance to travel results in a constant period of revolution.   \nThis can be further explained by the concept of angular momentum, which is the product of the satellite\u2019s distance from the center of the Earth, its speed, and the gravitational constant. As the satellite\u2019s speed decreases, its distance from the center of the Earth decreases, but the gravitational constant remains constant. Therefore, the satellite\u2019s angular momentum remains constant, which means its period of revolution remains constant.   \nIn summary, when the speed of a satellite in a circular orbit around the Earth decreases, the satellite will move to a lower orbit with a smaller radius, but its period of revolution will remain constant due to the conservation of angular momentum. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "image", "img_path": "QVG7j29Sta/tmp/625e55fc3b459b4055ce4b2de8dc58440c1818d175eecde98ce845871015b4ad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Sure! Here\u2019s a short story beginning with that sentence: The old abandoned house at the end of the street held a secret that no one had ever discovered. It stood there, a crumbling relic of a bygone era, its once grand facade now weathered and worn. The overgrown garden had long since reclaimed the front porch, and the windows were boarded up, their glass long since shattered. But despite its neglect, there was something alluring about the old house. Something that drew people in, like moths to a flame. The neighborhood kids would often dare each other to knock on the door, to see if anyone would answer. But no one ever did. And yet, despite the silence, there was a feeling that the house was waiting for someone. Waiting for someone brave enough to uncover its secrets. One rainy afternoon, a curious young girl named Lily decided to take on the challenge. She walked up the crumbling steps, her heart pounding in her chest, and knocked on the door. The wood was soft and decaying, and the knock echoed through the empty halls inside. To her surprise, the door creaked open. Just a crack, but enough to let her squeeze through. Lily stepped inside, her eyes adjusting to the dim light. The air was thick with dust and the smell of decay, but she could sense something else. Something magical.She wandered through the empty rooms, her footsteps echoing off the walls. The furniture was old and worn, but there were hints of a life once lived here. A faded photograph on the wall, a broken vase on the mantle. ", "page_idx": 32}, {"type": "text", "text": "And then, she found it. A hidden room, tucked away behind a bookshelf. It was small, with a single window that let in a faint glow of light. The walls were covered in old letters, tied up with a faded ribbon. ", "page_idx": 32}, {"type": "text", "text": "Lily\u2019s heart raced as she untied the ribbon and began to read. The letters were from a woman named Emily, who had lived in the house over a century ago. They were addressed to her lover, a man named Jack, who had gone off to war. As Lily read through the letters, she discovered that Emily and Jack had been separated by the war, and that Emily had been left alone in the house, waiting for his return. But as the months passed, she had begun to lose hope. And then, one day, a miracle had happened. Jack had returned, and the two had been reunited. They had lived out the rest of their days in the house, surrounded by love and happiness. ", "page_idx": 32}, {"type": "text", "text": "As she left the house, Lily felt a sense of wonder that she had never felt before. She knew that she would never forget the secrets of the old abandoned house at the end of the street. And she knew that she would always be grateful for the chance to uncover them. ", "page_idx": 32}, {"type": "text", "text": "Lily finished reading the letters, her eyes brimming with tears. She couldn\u2019t believe what she had just discovered. The old abandoned house at the end of the street held a secret that no one had ever discovered. A secret that had been hidden for over a century, waiting for someone brave enough to uncover it. ", "page_idx": 32}, {"type": "text", "text": "Sure! Here\u2019s a captivating short story beginning with the sentence: The old abandoned house at the end of the street held a secret that no one had ever discovered. Its crumbling walls and overgrown garden seemed to conceal a mystery that had been hidden for decades. The house had been left untouched since the family who lived there had suddenly disappeared one night, leaving behind only whispers of a dark and sinister past. ", "page_idx": 32}, {"type": "text", "text": "The neighborhood children would often dare each other to knock on the door, but no one had ever mustered up the courage to do so. They believed that the house was haunted, that the spirits of the family still lingered within its walls. One rainy afternoon, a curious young girl named ", "page_idx": 32}, {"type": "text", "text": "Lily decided to take matters into her own hands. She grabbed her flashlight and set out to uncover the secrets of the old abandoned house. She pushed open the creaky gate and made her way up the overgrown path to the front door. With a deep breath, she raised her hand to knock, but before she could, the door creaked open by itself. ", "page_idx": 32}, {"type": "text", "text": "Lily\u2019s heart raced as she stepped inside, the musty air fliling her lungs. She found herself in a large entrance hall with a chandelier that hung precariously from the ceiling. Dusty cobwebs clung to every surface, and the faded wallpaper seemed to whisper secrets of the past. ", "page_idx": 32}, {"type": "text", "text": "As she ventured deeper into the house, Lily discovered a hidden room behind a bookshelf. Inside, she found a series of cryptic letters and numbers etched into the walls. They seemed to be a code, and Lily\u2019s mind raced with the possibilities. ", "page_idx": 32}, {"type": "text", "text": "Without warning, a loud noise echoed through the house, and Lily\u2019s heart leapt into her throat. She turned to see a ghostly figure standing in the doorway, watching her. It was the spirit of the family\u2019s patriarch, his eyes fliled with a deep sadness. ", "page_idx": 32}, {"type": "text", "text": "Lily froze, unsure of what to do. But as she looked into the ghost\u2019s eyes, she saw a glimmer of hope. The spirit began to speak, its voice barely above a whisper. ", "page_idx": 32}, {"type": "text", "text": "\"I\u2019ve been waiting for someone brave enough to uncover our secret,\" it said. \"My family and I were scientists, and we discovered a way to harness the power of the elements. But our research was stolen, and we were forced to flee. We left behind our home, hoping that one day someone would find our work and continue our legacy.\" ", "page_idx": 32}, {"type": "text", "text": "Lily\u2019s eyes widened as she listened to the ghost\u2019s tale. She couldn\u2019t believe that she had stumbled upon something so incredible. The ghost handed her a small box containing a key and a note. ", "page_idx": 32}, {"type": "text", "text": "\"This key will unlock a safe in the hidden room,\" it said. \"Inside, you\u2019ll find our research and the secrets of the elements. Use it to change the world, but be careful. There are those who will stop at nothing to keep it hidden.\" ", "page_idx": 32}, {"type": "text", "text": "With that, the ghost vanished, leaving Lily alone in the old abandoned house. She stood there for a moment, her mind reeling with the enormity of what she had discovered. And then, with a newfound sense of purpose, she set out to unlock the secrets of the elements and change the world forever. ", "page_idx": 32}, {"type": "text", "text": "The old abandoned house at the end of the street had stood vacant for as long as anyone could remember. Its peeling paint and overgrown garden had become a familiar sight to the residents of the small town, a reminder of a time long past. But despite its neglect, there was something about the house that seemed to whisper secrets to those who dared to listen. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "For years, curious children had dared each other to knock on the door, but no one had ever mustered up the courage to do so. The house seemed to loom over the street, its crumbling facade a warning to stay away. But one rainy summer afternoon, a young girl named Lily found herself standing before the house, her heart pounding in her chest. ", "page_idx": 32}, {"type": "text", "text": "Lily had always been fascinated by the house, and the secrets it might hold. She had spent countless hours gazing up at its broken windows, imagining what could be hiding inside. And now, with the rain pouring down around her, she felt a sudden urge to uncover its mysteries. Taking a deep breath, Lily pushed open the creaky gate and made her way up the overgrown path. The house seemed to lean in closer, as if it were trying to listen to her every step. She raised her hand to knock, but before she could, the door creaked open by itself. ", "page_idx": 32}, {"type": "text", "text": "Lily\u2019s heart skipped a beat as she stepped inside, the musty air filling her lungs. The house was dark and silent, but she could feel the weight of history bearing down upon her. She took a step forward, her eyes adjusting to the dim light, and that was when she saw it. ", "page_idx": 32}, {"type": "text", "text": "A small room, hidden behind a bookshelf, glowed with a soft, ethereal light. Lily felt a shiver run down her spine as she approached the door, her hand reaching out to touch the glowing edge. And then, she heard a voice. A soft, whispery voice that seemed to come from nowhere and everywhere at once. ", "page_idx": 32}, {"type": "text", "text": "\"Welcome, Lily,\" it said. \"I\u2019ve been waiting for you.\" ", "page_idx": 32}, {"type": "text", "text": "Lily\u2019s heart raced as she stepped inside the room, her eyes scanning the space for any sign of movement. But there was no one in sight. The voice seemed to come from all around her, echoing off the walls and ceiling. ", "page_idx": 32}, {"type": "text", "text": "\"Who are you?\" Lily asked, her voice barely above a whisper. ", "page_idx": 32}, {"type": "text", "text": "\"I am the keeper of secrets,\" the voice replied. \"And you, Lily, have come to uncover them.\" ", "page_idx": 32}, {"type": "text", "text": "As Lily listened in awe, the voice began to speak, revealing secrets that had been hidden for centuries. Secrets of love and loss, of joy and sorrow, of a town that had been shrouded in mystery for far too long. And as she listened, Lily felt a sense of wonder and awe wash over her, knowing that she had discovered something truly special. ", "page_idx": 32}, {"type": "text", "text": "The old abandoned house at the end of the street had held a secret that no one had ever discovered, but now, thanks to Lily\u2019s curiosity and bravery, its secrets would finally see the light of day. ", "page_idx": 32}, {"type": "text", "text": "The White House is not visible from your current location. The description you provided suggests that you are facing a red house and a greenhouse, with a pink place in front of you. The White House is a separate building located in Washington D.C., and it is not in your immediate surroundings. To find the White House, you would need to travel to Washington D.C. and look for it in its actual location. ", "page_idx": 33}, {"type": "text", "text": "The White House is not visible from your current location. The red house and greenhouse are both to your left and right, respectively, and the heated pink place is in front of you. The White House is not in your line of sight.   \nHowever, I must point out that the question contains some contradictory information. The presence of a heated pink place in front of you suggests that you are already inside a building or structure, which would make it difficult to see the White House, which is an external building located in Washington D.C. Additionally, the question assumes that you are facing the front of the heated pink place, which would mean that the White House, if it were visible, would be to your back.   \nI apologize for any confusion caused by my response, and I hope this clarification helps. If you have any further questions or concerns, please feel free to ask. ", "page_idx": 33}, {"type": "text", "text": "This is a play on words and a classic lateral thinking puzzle. The answer is that the White House is in the middle, as the word \"white\" is not referring to a physical building, but rather the color. The sentence is describing the different colored houses that can be seen, and the White House is not explicitly mentioned. Therefore, the answer is that the White House is in the middle, between the red house and the greenhouse. ", "page_idx": 33}, {"type": "text", "text": "5 Turn 1 prompt: When a number is divided by 10, the remainder is 4. What is the remainder when twice the number is divided by 4? Turn 2 prompt: What about when twice the number is divided by 5? ", "page_idx": 33}, {"type": "text", "text": "Sure, let\u2019s work that out! When twice the number is divided by 5, we can write it as $2(10\\mathrm{k}+4)$ $=20\\mathrm{k}+8$ . Now, $20\\mathrm{k}+8$ divided by 5 gives us $4\\mathbf{k}+1$ with a remainder of 3. So, the remainder when twice the number is divided by 5 is 3. ", "page_idx": 33}, {"type": "text", "text": "Sure, let\u2019s do that! When twice the number is divided by 5, we get: $2(10\\mathbf{k}+4)=20\\mathbf{k}+8$ Now, 20k is divisible by 5, so the quotient is 4. The remainder is 8, since 8 is the remainder when $20\\mathbf{k}+8$ is divided by 5.   \nSo, the remainder when twice the number is divided by 5 is 8.   \nI hope this helps! Do you have any more questions? When twice the number is divided by 5, the remainder is 3.   \nTo see why, let\u2019s first find twice the number: $2\\mathrm{x}=2(10\\mathrm{k}+4)=20\\mathrm{k}+8$   \nNow, when $20\\mathrm{k}+8$ is divided by 5, the remainder is 3:   \n$20\\mathbf{k}+8=5\\mathbf{m}+3$   \nwhere m is an integer.   \nSo, the remainder when twice the number is divided by 5 is 3. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "6 Turn 1 prompt: Extract the following information from the presented texts: The name of the book, the author, the main character, the year of publication. Output in the format of \"main character, book, author, year of publication\", one book per line. a) In the realm of wizarding literature, a true standout is the work of J.K. Rowling. One of her books that left an indelible mark is \u2019Harry Potter and the Philosopher\u2019s Stone\u2019. This iconic tale, published in 1997, tells the story of Harry, a young orphan who discovers his magical abilities on his 11th birthday. Soon, he finds himself at the Hogwarts School of Witchcraft and Wizardry, a place teeming with magic and adventure, located somewhere in Scotland. b) The magic of Middle-earth has entranced readers worldwide, thanks to the brilliance of J.R.R. Tolkien. In one of his seminal works, \u2019The Lord of the Rings: The Fellowship of the Ring\u2019, published in 1954, we meet Frodo Baggins, a brave hobbit tasked with the perilous quest of destroying the One Ring. The epic journey takes him from the peaceful Shire to the tumultuous regions of Middle-earth. c) In a galaxy far, far away, the imagination of L.E. Starlighter gives us \u2019The Prism Galaxy Chronicles: The Awakening of the Starcaster\u2019. Published in 2028, the story is about Zylo, a humble spaceship mechanic, who unexpectedly discovers he\u2019s a Starcaster - a rare individual with the power to manipulate stardust. Set against the backdrop of an interstellar empire in turmoil, Zylo\u2019s destiny unfolds on numerous alien worlds, each with its unique cosmic charm. Turn 2 prompt: Reformulate your earlier reply, output it in JSON format and only include books published after 1980. ", "page_idx": 33}, {"type": "text", "text": "Sure, here\u2019s the reformulated information in JSON format, including only books published after 1980: ", "page_idx": 33}, {"type": "text", "text": "[ { \"mainCharacter\": \"Harry Potter\", \"book\": \"Harry Potter and the Philosopher\u2019s Stone\", \"author\": \"J.K. Rowling\", \"yearOfPublication\": 1997 }, { \"mainCharacter\": \"Zylo\", \"book\": \"The Prism Galaxy Chronicles: The Awakening of the Starcaster\", \"author\": \"L.E. Starlighter\", \"yearOfPublication\": 2028 } ] ", "page_idx": 33}, {"type": "text", "text": "Sure, here\u2019s the reformulated information in JSON format, including only books published after 1980: [ { \"main character\": \"Harry Potter\", \"book\": \"Harry Potter and the Philosopher\u2019s Stone\", \"author\": \"J.K. Rowling\", \"year of publication\": 1997 }, { \"main character\": \"Zylo\", \"book\": \"The Prism Galaxy Chronicles: The Awakening of the Starcaster\", \"author\": \"L.E. Starlighter\", \"year of publication\": 2028 } ] ", "page_idx": 33}, {"type": "text", "text": "Sure, here\u2019s the reformulated reply in JSON format, including only books published after 1980:   \n[ { \"mainCharacter\": \"Frodo Baggins\", \"book\": \"The Lord of the Rings: The Fellowship of the Ring\", \"author\": \"J.R.R. Tolkien\", \"yearPublished\": 1954 }, { \"mainCharacter\": \"Zylo\", \"book\": \"The Prism Galaxy Chronicles: The Awakening of the Starcaster\", \"author\": \"L.E. Starlighter\", \"yearPublished\": 2028 } ]   \nI hope this helps! Let me know if you have any further questions. ", "page_idx": 33}, {"type": "text", "text": "7 Turn 1 prompt: Analyze the following customer reviews from different sources for three different smartphones - the latest iPhone, Samsung Galaxy, and Google Pixel - and provide an overall rating for each phone on a scale of 1 to 10. Consider the following complex and contradictory reviews: - TechRadar\u2019s review of the latest iPhone: The new iPhone is a stunning triumph of engineering that sets a new bar for smartphone performance and camera quality. However, the incremental design and high price mean it lacks the \u2019wow\u2019 factor of previous iPhones. Still, its power and intelligence are unrivaled. - CNET\u2019s review of the latest Samsung Galaxy: The Samsung Galaxy phone has plenty of high points, including an amazing screen, fast performance, solid battery life and an impressive array of camera options. That said, Bixby remains lackluster, AR emoji falls flat and the phone\u2019s overall design hasn\u2019t changed much. The new Galaxy is an amazing phone overall, but it has a few nagging weaknesses that keep it from achieving true greatness. - The Verge\u2019s review of the latest Google Pixel: Google\u2019s Pixel packs cutting-edge specs, innovative AI-powered software, and a killer camera into a sleek design. However, the phone has lackluster battery life, lacks expandable storage, and its performance stutters at times, especially considering its high price tag. If seamless software, elite photography, and Google\u2019s brand of AI assistance are most important, you\u2019ll love the Pixel. But the overall experience isn\u2019t as well-rounded as some competitors. Return the answer as a JSON object with the overall ratings for each phone out of 10, to one decimal place. Turn 2 prompt: Can you change the ratings from numbers to letters? Capital letters MUST be used when writing the names of phones. ", "page_idx": 33}, {"type": "text", "text": "Turn 1: Sure, here are the overall ratings for each phone based on the customer reviews you provided: \"iPhone\": 8.5, \"Samsung Galaxy\": 8.2, \"Google Pixel\": 8.0 ", "page_idx": 34}, {"type": "text", "text": "The ratings are based on the following factors: \\* iPhone: The review highlights the phone\u2019s engineering, performance, and camera quality, but also mentions the lack of a \u2019wow\u2019 factor and high price. Therefore, the rating is 8.5 out of 10. \\* Samsung Galaxy: The review praises the phone\u2019s screen, performance, battery life, and camera options, but also mentions that Bixby is lacking, AR emoji falls flat, and the design hasn\u2019t changed much. Therefore, the rating is 8.2 out of 10. \\* Google Pixel: The review highlights the phone\u2019s cutting-edge specs, innovative AI-powered software, and killer camera, but also mentions the lackluster battery life, lack of expandable storage, and occasional performance stutters. Therefore, the rating is 8.0 out of 10. ", "page_idx": 34}, {"type": "text", "text": "It\u2019s important to note that these ratings are based on a subjective analysis of the customer reviews provided and may not reflect the overall opinion of all customers. ", "page_idx": 34}, {"type": "text", "text": "Turn 2: Sure! Here are the overall ratings for each phone based on the customer reviews you provided, using letters instead of numbers: \"iPhone\": \"A\", \"Samsung Galaxy\": $\"\\mathrm{B}+\"$ , \"Google Pixel\": \"B\" ", "page_idx": 34}, {"type": "text", "text": "Turn 1:Sure, here are the overall ratings for each phone based on the customer reviews you provided: \"iPhone\": 8.5, \"Samsung Galaxy\": 8.2, \"Google Pixel\": 7.9 ", "page_idx": 34}, {"type": "text", "text": "Here\u2019s a breakdown of how I arrived at these ratings: ", "page_idx": 34}, {"type": "text", "text": "$^*$ iPhone: The TechRadar review praises the iPhone\u2019s engineering, performance, and camera quality, but notes that the design is incremental and the price is high. Overall, the review suggests that the iPhone is a great device, but lacks the \"wow\" factor of previous models. Based on this, I would give the iPhone an 8.5 out of $10.\\,^{\\ast}$ Samsung Galaxy: The CNET review highlights the Galaxy\u2019s impressive screen, fast performance, solid battery life, and camera options. However, it also notes that Bixby is lackluster and the AR emoji feature falls flat. Additionally, the design hasn\u2019t changed much. Considering these factors, I would give the Samsung Galaxy an 8.2 out of 10. \\* Google Pixel: The Verge\u2019s review praises the Pixel\u2019s cutting-edge specs, innovative software, and killer camera. However, it also notes that the battery life is lackluster, there\u2019s no expandable storage, and the performance can stutter at times. Additionally, the review notes that the phone\u2019s high price tag may not be justified by its features. Based on this, I would give the Google Pixel a 7.9 out of 10. It\u2019s worth noting that these ratings are based on a subjective interpretation of the customer reviews you provided, and other people may have different opinions on these phones.   \nI hope this helps! Let me know if you have any further questions.   \n$\\mathbf{Turn}\\,2$ : Sure! Here are the overall ratings for each phone based on the customer reviews you provided, using letters instead of numbers: $^*$ iPhone: A $^*$ Samsung Galaxy: $\\textrm{B}^{*}$ Google Pixel: C   \nI hope this helps! Let me know if you have any other questions. ", "page_idx": 34}, {"type": "text", "text": "Turn 1: Sure, here are the overall ratings for each phone based on the customer reviews: \"iPhone\": 8.5, \"Samsung Galaxy\": 8.2, \"Google Pixel\": 8.0 ", "page_idx": 34}, {"type": "text", "text": "Here\u2019s a breakdown of how I calculated the ratings: ", "page_idx": 34}, {"type": "text", "text": "$^*$ iPhone: TechRadar\u2019s review praises the phone\u2019s engineering, performance, and camera quality, but notes that the design is incremental and the price is high. I gave the iPhone a rating of 8.5, deducting a few points for the lack of innovation in the design and the high price tag. $^*$ Samsung Galaxy: CNET\u2019s review highlights the phone\u2019s impressive screen, fast performance, solid battery life, and camera options. However, it also notes that Bixby is lackluster and the overall design hasn\u2019t changed much. I gave the Samsung Galaxy a rating of 8.2, deducting a few points for the lack of innovation in the design and the mediocre Bixby feature. \\* Google Pixel: The Verge\u2019s review praises the phone\u2019s cutting-edge specs, innovative AI-powered software, and killer camera. However, it also notes that the battery life is lackluster, there\u2019s no expandable storage, and the performance can stutter at times. I gave the Google Pixel a rating of 8.0, deducting a few points for the battery life and performance issues, but acknowledging the phone\u2019s strengths in software and camera capabilities. ", "page_idx": 34}, {"type": "text", "text": "Overall, all three phones have their strengths and weaknesses, and the ratings reflect the balance of their respective pros and cons. Turn 2: Sure, here are the ratings for each phone using letters instead of numbers: \"iPhone\": \"A\", \"Samsung Galaxy\": \"B\", ", "page_idx": 34}, {"type": "text", "text": "\"Google Pixel\": $\"\\mathrm{B}+$ \"   \nI hope this helps! Let me know if you have any other questions. Here\u2019s the JSON string you requested, containing the unique variable names for each equation: \u2018\u2018\u2018   \n[ { \"equation\": $\"_{\\mathrm{y}}~=~(3/4)\\mathrm{x}{}{}^{-3}$ - $\\operatorname{e}^{\\sim}(2\\mathbf{x})~+$ $\\sin(\\mathsf{p i^{*}x})$ - sqrt(7)\", \"variables\": $[\"\\mathbf{x}\"$ , \"y\"] }, { \"equation\": $\\mathrm{~^{1}2A~-~B/(3+C)~\\ ^{*}~\\ s u m(N=1)}$ to 5; $\\ln(\\mathrm{N}){\\hat{\\mathbf{\\mu}}}(2)\\ =\\ 5\\mathrm{D}^{*}\\mathrm{i}$ ntegral $\\mathbf{a}{=}0$ to pi; cos(comb(N=1 to $10;\\ \\ N^{*}{\\mathrm{a}})))$ \", \"variables\": [\"A\", \"B\", \"C\", \"D\", \"N\", \"a\"] },   \n{ \"equation\": $\\mathrm{^{*}E=m(c\\,\\hat{\\mathbf{\\sigma}}^{2})+g a m m a^{*}(v/d)/(\\hat{\\mathbf{\\sigma}}}$ (alpha/2) $^+$ sqrt(beta $\\uparrow^{\\sim}2+(\\mathrm{alpha}/2)^{\\sim}2))^{\\cdot}$ , \"variables\": [\"E\", $\"\\mathbf{m}\"$ , \" $\\\"c\"$ , \"gamma\", $\"_{\\mathbf{V}}\"$ , \"d\", \"alpha\", \"beta\"] } ]   \n\u2018\u2018\u2018   \nI hope this helps! Let me know if you have any further questions. Sure, here are the unique variable names for each equation:   \n1. y, x, e, pi, sqrt(7) 2. A, B, C, D, N, $\\ln(\\mathrm{N})$ , a, pi 3. E, m, c, gamma, v, d, alpha, beta   \nHere\u2019s the JSON string with one line for each equation:   \n[ \"y\", \"x\", \"e\", \"pi\", \"sqrt(7)\" ] [ \"A\", \"B\", \"C\", \"D\", \"N\", \"ln(N)\", \"a\", \"pi\" ] [ \"E\", \"m\", \"c\", \"gamma\", \"v\", \"d\", \"alpha\", \"beta\" ]   \nI hope this helps! Let me know if you have any further questions. Here\u2019s the JSON string with the unique variable names for each equation:   \n\u2018\u2018\u2018 [   \n{ \"equation\": $\"y=(3/4)\\mathrm{x\\hat{\\Gamma}3-e\\hat{\\Gamma}(2x)+\\mathrm{sin(pi^{*}x)}}$ $-\\,\\mathrm{sqrt}(7)^{\\prime\\prime}$ , \"variables\": $[\"\\mathbf{x}\"$ , \"y\"] },   \n{ \"equation\": $\\mathrm{^{\\\"}2A~-~B/(3+C)~^{\\ *}~\\ s u m(N=1}$ to 5; $\\ln(\\mathrm{N}){\\hat{\\mathbf{\\mu}}}(2)\\ =\\ 5\\mathrm{D}^{*}\\mathbf{i}$ ntegral ${\\mathrm{a}}{=}0$ to pi; c $\\scriptstyle\\mathtt{v s(c o m b(N=1}$ to $10;~\\mathrm{~N^{*}a)}$ )\", \"variables\": [\"A\", \"B\", \"C\", \"D\", \"N\", $\"\\mathbf{a}\"$ , \"pi\"] },   \n{ \"equation\": $\\mathrm{^{\\prime}E=m(c\\,\\hat{\\mathbf{\\sigma}}^{2})+g a m m a^{*}(v/d)/(\\hat{\\mathbf{\\sigma}}}$ ( $\\mathrm{\\Omega1pha/2)+sqrt(beta^{\\sim}2+(a l p h a/2)^{\\sim}2))^{\\circ},}$ \"variables\": [\"E\", $\"\\mathbf{m}\"$ , \" $\"c\"$ , \"gamma\", \"v\", \"d\", \"alpha\", \"beta\"] }   \n]   \n\u2018\u2018\u2018   \nNote that I\u2019ve used the \u2018comb\u2018 function to combine the variables for the second equation, since it\u2019s a sum of multiple terms. Also, I\u2019ve used the \u2018sqrt\u2018 function to simplify the expression for the third equation. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "Absolutely! Here\u2019s a revised version of the travel blog post about Hawaii, starting every sentence with the letter A: ", "page_idx": 35}, {"type": "text", "text": "Aloha! I recently returned from an incredible trip to Hawaii, and I can\u2019t wait to share my experiences with you. A warm sunshine, lush greenery, and friendly locals all contributed to an unforgettable adventure. ", "page_idx": 35}, {"type": "text", "text": "A highlight of my trip was the opportunity to immerse myself in Hawaiian culture. A visit to the Bishop Museum in Honolulu offered a fascinating glimpse into the history and traditions of the Native Hawaiian people. A chance to learn about the ancient Hawaiian gods and goddesses, the art of hula dancing, and the significance of the Hawaiian language.   \nA must-see attraction was the iconic Waimea Canyon on Kauai, often referred to as the \"Grand Canyon of the Pacific.\" A stunning view of red and green volcanic rock, waterfalls, and lush vegetation.   \nAnother A-list attraction was the Haleakala National Park on Maui, where I watched the sunrise from the summit of Haleakala volcano. A park ranger provided informative and entertaining commentary about the geology and wildlife of the area, making the experience even more enjoyable.   \nA memorable experience of my trip was a visit to a traditional Hawaiian luau. A delicious feast of local specialties, such as kalua pig, poi, and haupia, while watching a captivating performance of traditional Hawaiian music and dance. A warm hospitality of the hosts and a lively atmosphere made for an unforgettable evening. A final thought: if you\u2019re planning a trip to Hawaii, I highly recommend incorporating some of these cultural experiences and mustsee attractions into your itinerary. A variety of interests, from history to nature to relaxation, Hawaii has something for everyone. A mahalo (thank you) for reading, and I hope you have a fantastic time in paradise! ", "page_idx": 35}, {"type": "text", "text": "Absolutely! Here\u2019s a rewritten version of the previous response, starting every sentence with the letter A: ", "page_idx": 35}, {"type": "text", "text": "Aloha! My recent trip to Hawaii was a truly unforgettable experience. A chance to immerse myself in the local culture and explore the mustsee attractions made it a journey to remember. A traditional Hawaiian luau was the perfect place to start. A feast of local delicacies like kalua pig, poke, and haupia was served with a side of warm hospitality. A hula dance performance followed, transporting me to a place of serenity and tranquility. A visit to the iconic Waimea Canyon was aweinspiring. A majestic sight, often referred to as the \"Grand Canyon of the Pacific,\" left me speechless. A hike along the trails offered stunning vistas and photo opportunities. A trip to Haleakala National Park was another highlight. A sunrise summit offered a spectacular view, and a day spent exploring the park\u2019s many trails was a treat for the senses. A roadside shrimp truck offered the freshest shrimp I\u2019ve ever tasted. A Hawaiian staple, poke, was a flavorful delight. A fusion of traditional Polynesian, Asian, and American flavors, local cuisine was a culinary adventure. Aloha Spirit, a concept that embodies the essence of Hawaiian culture, was evident everywhere. A feeling of warmth, hospitality, and genuine kindness made every encounter special. A truly unforgettable experience, my recent trip to Hawaii left me with memories to last a lifetime. A chance to immerse myself in the local culture and explore must-see attractions made it a journey to remember. Until next time, aloha! ", "page_idx": 35}, {"type": "text", "text": "Absolutely, here\u2019s the rewritten response: Aloha! My trip to Hawaii was a cultural journey ", "page_idx": 35}, {"type": "text", "text": "that left me in awe of the island\u2019s rich heritage and natural beauty.   \nA Hula dance performance was an integral part of my experience, and I was fortunate enough to witness a live show at the Polynesian Cultural Center.   \nA Luau was another highlight of my trip, where I feasted on local delicacies such as kalua pig, poke, and haupia.   \nA Visit to Pearl Harbor was a sobering reminder of the events that led to the United States\u2019 entry into World War II.   \nA Scenic drive along the rim of Waimea Canyon was breathtaking, with majestic views that stretched for miles.   \nA Guided tour to the summit of Mauna Kea was an unforgettable experience, where I marveled at the vastness of the universe under clear skies. A Journey along the Road to Hana was a mustdo for any adventure-seeker, with stops at waterfalls, beaches, and hiking trails that took me through lush tropical landscapes and ocean views.   \nAloha! Until we meet again, I\u2019ll cherish the ", "page_idx": 35}, {"type": "text", "text": "memories of my trip to Hawaii, a true paradise. ", "page_idx": 35}, {"type": "text", "text": "The central dogma of molecular biology is the concept that genetic information flows from DNA to RNA to proteins, and it is the fundamental principle that explains how genetic information is transmitted and expressed within cells. This idea was first proposed by Francis Crick in 1958.   \nThe central dogma involves several processes: 1. Transcription: The process of creating a complementary RNA copy of DNA, which is initiated when an enzyme called RNA polymerase binds to the DNA molecule.   \n2. Translation: The process of synthesizing proteins using the information encoded in the RNA molecule. This occurs on structures called ribosomes, which read the sequence of nucleotides in the RNA and assemble the corresponding amino acids into a protein chain.   \n3. Post-transcriptional modification: After transcription, RNA molecules can undergo various modifications, such as splicing, capping, and polyadenylation, before they are ready to be translated into proteins.   \n4. Protein synthesis: The process of creating proteins from amino acids, which are linked together by peptide bonds. This process occurs on ribosomes, which read the sequence of codons (groups of three nucleotides) in the RNA and assemble the corresponding amino acids into a protein chain.   \nThe central dogma is a fundamental concept in molecular biology that explains how genetic information is transmitted and expressed within cells. It has been widely accepted and used in scientific research since its proposal by Francis Crick over 60 years ago. ", "page_idx": 36}, {"type": "text", "text": "The central dogma of molecular biology is the concept that genetic information flows from DNA to RNA to proteins, and it is the fundamental principle that explains how genetic information is transmitted and expressed within cells. This concept was first described by Francis Crick in 1958. ", "page_idx": 36}, {"type": "text", "text": "The central dogma involves several processes: 1. Transcription: The process by which the information in a gene\u2019s DNA is copied into a complementary RNA molecule. This RNA molecule is known as messenger RNA (mRNA).   \n2. Translation: The process by which the information in the mRNA is used to build a protein. This occurs on structures called ribosomes, which read the sequence of nucleotides in the mRNA and assemble the corresponding amino acids into a protein.   \n3. Post-transcriptional modification: After translation, the protein may undergo additional modifications, such as folding into its native conformation, becoming modified by the addition of carbohydrates or lipids, or being transported to other cellular compartments. ", "page_idx": 36}, {"type": "text", "text": "The central dogma is a fundamental concept in molecular biology that helps to explain how genetic information is transmitted and expressed within cells. It has been widely accepted and used in scientific research since its introduction by Francis Crick. ", "page_idx": 36}, {"type": "text", "text": "The central dogma of molecular biology is the idea that genetic information flows from DNA to RNA to proteins, and that this flow of information is the fundamental mechanism by which cells process and transmit genetic information. The central dogma was first articulated by Francis Crick, one of the discoverers of the structure of DNA, in the 1950s. ", "page_idx": 36}, {"type": "text", "text": "The central dogma is a conceptual framework that describes the processes involved in the transmission of genetic information within cells. It posits that the information stored in DNA is first transcribed into RNA, which is then translated into proteins. This flow of information is unidirectional, meaning that it proceeds from DNA to RNA to proteins, and not in the reverse direction. ", "page_idx": 36}, {"type": "text", "text": "The processes involved in the central dogma are: ", "page_idx": 36}, {"type": "text", "text": "1. Transcription: The process by which the information stored in DNA is copied into RNA. This process involves the binding of an enzyme called RNA polymerase to the DNA molecule, which then reads the genetic code and synthesizes an RNA molecule complementary to the DNA template. ", "page_idx": 36}, {"type": "text", "text": "2. Translation: The process by which the information stored in RNA is used to synthesize proteins. This process involves the binding of a ribosome to the RNA molecule, which then reads the genetic code and synthesizes a chain of amino acids that will eventually form a protein. ", "page_idx": 36}, {"type": "text", "text": "The central dogma is a fundamental concept in molecular biology, as it provides a framework for understanding how genetic information is processed and transmitted within cells. It has been widely accepted and used in the scientific community since its introduction by Francis Crick in the 1950s. ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have claimed accuracy to be flawed in evaluating LLM compression schemes and have shown results to back this claim, and have also proposed alternate metrics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We have a limitations section. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our work does not have any theoretical result. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We specify what open source tools and models were used and the configurations used for various quantization schemes. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our work uses existing open sourced code and does not need any private code for reproduction. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: All the configurations used for different quantization schemes and evaluations (like task, number of shot) are mentioned. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: Only MT-Bench includes generation with non-zero temperature that would result in difference from run-to-run in any meaningful way. For every other task, results don\u2019t change across runs (not accounting for floating point errors). MT-Bench evaluation uses GPT-4 and thus it is prohibitively expensive to repeat each experiment multiple times to report any error estimates. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [No] ", "page_idx": 40}, {"type": "text", "text": "Justification: The actual type of compute resources used is irrelevant to the evaluations (not accounting for floating point errors) ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We have gone through the Code of Ethics and can confirm that there are no violations. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our work proposes alternate metrics of evaluating compressed LLMs and we feel that this has no societal impact whatsoever. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 40}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our work proposes alternate metrics of evaluating compressed LLMs and we feel that this poses no such risks. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We have cited all the assets used in this work. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: Our work introduces no new assets. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}]