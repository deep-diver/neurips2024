{"references": [{"fullname_first_author": "Tim Dettmers", "paper_title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale", "publication_date": "2022-12-01", "reason": "This paper is foundational to the work because it introduces a method for efficient 8-bit matrix multiplication in transformers, a key technique used in model compression."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "publication_date": "2023-07-18", "reason": "This paper introduces the Llama 2 family of models, which are used extensively in the experiments as baseline models for evaluating compression techniques."}, {"fullname_first_author": "Andrey Gromov", "paper_title": "The Unreasonable Ineffectiveness of the Deeper Layers", "publication_date": "2024-03-19", "reason": "This paper investigates layer dropping as a model compression technique, which is directly compared with other techniques in the current work."}, {"fullname_first_author": "Mingjie Sun", "paper_title": "Wanda: A Simple and Effective Pruning Approach for Large Language Models", "publication_date": "2023-06-01", "reason": "This paper introduces the WANDA pruning technique which is another model compression method evaluated and compared in the experiments."}, {"fullname_first_author": "Chujie Zheng", "paper_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", "publication_date": "2023-06-01", "reason": "This paper introduces the MT-Bench dataset, which is used to qualitatively and quantitatively evaluate the compressed models in a multi-turn dialogue setting."}]}