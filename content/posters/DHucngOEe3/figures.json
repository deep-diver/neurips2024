[{"figure_path": "DHucngOEe3/figures/figures_2_1.jpg", "caption": "Figure 1: An overview of MGPO. We pre-train Transformer-based policies on task-agnostic datasets, leveraging hindsight multi-goal relabeling and behavior cloning to endow policies with the capacity for modeling long-term behaviors. In unseen tasks, multi-goal Transformers adapt efficiently through prompt optimization, which searches for a sequence of goals with the aim of maximizing online returns.", "description": "This figure illustrates the MGPO (Multi-Goal Transformers with Prompt Optimization) framework.  The left side shows the pre-training phase where a multi-goal transformer is trained on a task-agnostic dataset using a technique called hindsight multi-goal relabeling. This process generates diverse goal sequences from existing trajectories.  The middle section details the pre-training of the multi-goal transformer, which learns to map goal sequences (prompts) to action sequences.  Finally, the right side depicts the online adaptation phase, where the prompt (goal sequence) is optimized using an online prompt optimization method to maximize returns in a new, unseen task.", "section": "Method"}, {"figure_path": "DHucngOEe3/figures/figures_13_1.jpg", "caption": "Figure 1: An overview of MGPO. We pre-train Transformer-based policies on task-agnostic datasets, leveraging hindsight multi-goal relabeling and behavior cloning to endow policies with the capacity for modeling long-term behaviors. In unseen tasks, multi-goal Transformers adapt efficiently through prompt optimization, which searches for a sequence of goals with the aim of maximizing online returns.", "description": "This figure illustrates the MGPO (Multi-Goal Transformers with Prompt Optimization) framework. It shows the two main stages: pre-training and online prompt optimization.  Pre-training uses a task-agnostic dataset and techniques like hindsight multi-goal relabeling and behavior cloning to train a Transformer-based policy capable of handling long-term behaviors. During online adaptation, the framework optimizes the sequence of goals (the \"prompt\") to maximize returns in the unseen task. This optimization is done through a multi-armed bandit process, leveraging returns from online trajectories to guide the selection of goal sequences.", "section": "Method"}, {"figure_path": "DHucngOEe3/figures/figures_13_2.jpg", "caption": "Figure 5: An illustration of prompts (goal sequences) and the agent's behavior in a Kitchen task. The order of goals within the prompt is crucial for successful task completion.", "description": "This figure shows two examples of how the order of goals in the prompt affects the agent's behavior and success in completing a task within the Kitchen environment.  The top row depicts a successful trial, where the agent correctly performs the actions specified by the goal sequence in the prompt. The bottom row shows a failed trial, where an incorrect ordering of goals in the prompt leads the agent to perform actions that do not complete the task successfully.  The image highlights the importance of sequential goal ordering in long-horizon tasks. ", "section": "3 Method"}, {"figure_path": "DHucngOEe3/figures/figures_14_1.jpg", "caption": "Figure 6: Performance of MGPO compared with baseline methods during online adaptation. The vertical axis indicates the task performance of the optimized policy and the horizontal axis indicates the number of online episodes. Each figure shows the average performance on all test tasks in each environment and the standard deviation across 3 random seeds.", "description": "This figure compares the performance of MGPO against several baseline methods across five different environments during online adaptation.  The x-axis represents the number of online episodes, and the y-axis shows the average performance on all test tasks within each environment. Error bars represent the standard deviation across three random seeds. The figure clearly demonstrates MGPO's superior performance and sample efficiency compared to the baselines in most environments.", "section": "4 Experiments"}, {"figure_path": "DHucngOEe3/figures/figures_14_2.jpg", "caption": "Figure 6: Performance of MGPO compared with baseline methods during online adaptation. The vertical axis indicates the task performance of the optimized policy and the horizontal axis indicates the number of online episodes. Each figure shows the average performance on all test tasks in each environment and the standard deviation across 3 random seeds.", "description": "This figure compares the online adaptation performance of MGPO against several baseline methods across five different environments (MazeRunner-15, MazeRunner-30, Kitchen, GridWorld, and Crafter). The x-axis represents the number of online episodes, and the y-axis represents the average task performance.  Error bars show standard deviations across three random seeds. The figure demonstrates MGPO's superior performance and faster convergence compared to other methods.", "section": "4 Experiments"}, {"figure_path": "DHucngOEe3/figures/figures_15_1.jpg", "caption": "Figure 8: Additional results of ablation study on the maximal prompt length K.", "description": "The figure displays additional results from an ablation study on the maximal prompt length K in the MGPO model.  It shows the performance of MGPO on MazeRunner-15 and MazeRunner-30 environments with different values for K (2, 3, 5, 10, 20, and 40).  This provides a more detailed view of how increasing prompt length affects the performance of MGPO during online adaptation.  The shaded areas represent the standard deviation across multiple runs.", "section": "B.2 Ablation Study on the Maximal Prompt Length"}, {"figure_path": "DHucngOEe3/figures/figures_16_1.jpg", "caption": "Figure 9: Visualization of prompts with different lengths and their state visitation counts in MazeRunner-15. Each image in the upper row shows a short prompt with a length of 1. Each image in the bottom row shows a long prompt with a length of 5.", "description": "This figure shows the impact of prompt length on exploration vs. exploitation.  Shorter prompts (length 1) lead to more diverse exploration of the maze, while longer prompts (length 5) guide the agent toward a more direct path to the goal, demonstrating the effect of prompt length on the agent's behavior.", "section": "4.5 Visualization and Case Study"}, {"figure_path": "DHucngOEe3/figures/figures_16_2.jpg", "caption": "Figure 3: Visualization of the optimized prompts and state visitation in the adaptation stage of MGPO-UCB. Red squares with S, T, gi represent the start positions, task goals, and the optimized prompts, respectively. We use purple to display the visitation frequency of each location, with darker shades indicating higher frequencies. In this example, initially, the agent explores the left half of the maze, aiming for the front-left task goal but is hindered by walls. In exploration, as it discovers rewarding routes to the right, MGPO-UCB adapts by sampling new prompts from these better paths. An optimized prompt is achieved after 40 episodes.", "description": "This figure visualizes how MGPO-UCB, a method for efficient online adaptation in reinforcement learning, refines its strategy over time. It shows the evolution of optimized prompts (sequences of goals) and the agent's behavior in a MazeRunner-15 environment. Initially, exploration is focused on the left side, but as rewarding paths on the right are discovered, the prompts and subsequent policy adapt accordingly. The visualization clearly illustrates how the optimized prompts guide the agent towards improved task performance, achieving an effective solution after 40 episodes.", "section": "4.5 Visualization and Case Study"}]