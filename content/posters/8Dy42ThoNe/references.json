{"references": [{"fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: Harmlessness from AI feedback", "publication_date": "2022-12-08", "reason": "This paper is highly relevant because it introduces Constitutional AI, a method for aligning LLMs with human preferences, which is directly compared to the unlearning approach in this paper."}, {"fullname_first_author": "Lucas Bourtoule", "paper_title": "Machine unlearning", "publication_date": "2021-00-00", "reason": "This paper provides foundational work on machine unlearning, a technique that is directly relevant to the LLM unlearning explored in this paper."}, {"fullname_first_author": "Paul F. Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-00-00", "reason": "This paper introduces RLHF, a widely used method for aligning LLMs, which serves as a baseline and comparison for the proposed unlearning technique in this paper."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper presents another widely used method for aligning LLMs with human feedback, providing another key comparison point for the unlearning technique proposed in this paper."}, {"fullname_first_author": "Nicholas Carlini", "paper_title": "Extracting training data from large language models", "publication_date": "2021-00-00", "reason": "This paper explores the extraction of training data from LLMs, addressing a related concern to unlearning and demonstrating the potential for unwanted information to be stored in LLMs."}]}