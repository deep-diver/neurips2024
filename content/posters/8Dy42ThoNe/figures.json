[{"figure_path": "8Dy42ThoNe/figures/figures_9_1.jpg", "caption": "Figure 1: Run time on a single NVIDIA A100-80GB GPU.", "description": "This figure shows the computation time taken by different methods for aligning LLMs.  The full RLHF approach requires significantly more time than other methods such as SFT, finetuning on remaining data, GA, and GA+Mismatch.  The GA and GA+Mismatch methods, which use only negative samples and are the focus of this paper, demonstrate the lowest computation times, achieving comparable results to the significantly more time consuming RLHF approach with only 2% of its computational time. This highlights the computational efficiency of the unlearning approach proposed in the paper.", "section": "4.5 Ablation Studies"}, {"figure_path": "8Dy42ThoNe/figures/figures_15_1.jpg", "caption": "Figure 2: Loss on unlearned samples and normal samples when we directly perform gradient ascent.", "description": "This figure shows the training loss curves for both unlearned and normal samples during the gradient ascent unlearning process. The x-axis represents the training step, and the y-axis represents the loss.  The blue line shows the loss on unlearned samples (samples the model is intended to forget), and the orange line shows the loss on normal samples (samples that the model should retain knowledge of). The significant increase in loss for unlearned samples indicates that the model is successfully forgetting the undesired information. Meanwhile, the relatively stable and low loss on normal samples demonstrates the preservation of the model's utility on desired tasks.", "section": "Ablation Studies"}]