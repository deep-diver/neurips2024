[{"type": "text", "text": "REBEL: Reinforcement Learning via Regressing Relative Rewards ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhaolin Gao1, Jonathan D. Chang2\u2217, Wenhao Zhan3, Owen Oertell1, Gokul Swamy4,   \nKiant\u00e9 Brantley5, Thorsten Joachims1, J. Andrew Bagnell4,6, Jason D. Lee3, Wen Sun1 1 Cornell University, 2 Databricks Mosaic Research, 3 Princeton University, 4 Carnegie Mellon University, 5 Harvard University, 6 Aurora Innovation ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping), and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative reward between two completions to a prompt in terms of the policy, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong performance in AlpacaEval 2.0, MTBench, and Open LLM Leaderboard. Implementation of REBEL can be found at https://github.com/ZhaolinGao/REBEL, and models trained by REBEL can be found at https://huggingface.co/Cornell-AGI. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The generality of the reinforcement learning (RL) paradigm is striking: from continuous control problems (Kalashnikov et al., 2018) to, more recently, the fine-tuning of generative models (Stiennon et al., 2022; Ouyang et al., 2022), RL has enabled concrete progress across a variety of decision-making tasks. Specifically, when it comes to fine-tuning generative models, Proximal Policy Optimization (PPO, Schulman et al. (2017)) has emerged as the de-facto RL algorithm of choice, from language models (LLMs) (Ziegler et al., 2020; Stiennon et al., 2022; Ouyang et al., 2022; Touvron et al., 2023) to generative image models (Black et al., 2023; Fan et al., 2024; Oertell et al., 2024). ", "page_idx": 0}, {"type": "image", "img_path": "yxjWAJzUyV/tmp/ecfa9df96caad99aef9491e3f76f801aa6503905f7250a4e576d504aa1df3896.jpg", "img_caption": ["Figure 1: We present REBEL: a simple and scalable RL algorithm that performs policy optimization via iteratively regressing the difference in rewards in terms of the policy, allowing us to eliminate much of the complexity (e.g. value functions, clipping) of algorithms like PPO (Schulman et al., 2017). We apply REBEL to problems in both image generation and language modeling and find that despite its conceptual and implementation-level simplicity, REBEL is able to match or sometimes outperform the performance of PPO while out-performing purely offline techniques like DPO (Rafailov et al., 2023). REBEL also achieves strong performance on common benchmarks such as AlpacaEval when fine-tuning a Llama-3-8B model. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "If we take a step back however, it is odd that we are using an algorithm designed for optimizing two-layer networks for continuous control tasks from scratch, even though we are now fine-tuning generative models with billions of parameters. In the continuous control setting, the randomly initialized neural networks and the possible stochasticity in the dynamics necessitate variance reduction through a learned value function as a baseline (Schulman et al., 2015), while clipping updates is important to limit distribution shift from iteration to iteration (Kakade and Langford, 2002). This means that when applied to generative model fine-tuning, we need to store four models in memory simultaneously (the policy, the reference policy, the critic, and the reward model), each with billions of parameters. Furthermore, we often add a KL regularization to the base model for fine-tuning, making explicit clipping unnecessary nor advisable, as pointed out by Ahmadian et al. (2024). Even outside of the generative modeling context, PPO is notorious for the wide range of performances measured, with differences being attributed to seemingly inconsequential implementation details (Henderson et al., 2019; Engstrom et al., 2020). This begs the question: are there simpler algorithms that better scale to modern RL applications? ", "page_idx": 1}, {"type": "text", "text": "Our answer is REBEL: an algorithm that reduces the problem of RL to solving a sequence of squared loss regression problems on iteratively collected datasets. Each regression problem directly uses the policy to predict the difference in rewards. This allows us to eliminate the complexity of using value functions, avoids heuristics like clipping, and scales easily to problems in both language modeling and image generation. Our key insight is that a regressor that can predict the difference in rewards between trajectories in a dataset implicitly captures an improved policy. ", "page_idx": 1}, {"type": "text", "text": "Rather than being a mere heuristic, REBEL comes with strong guarantees in theory and can be seen as a strict generalization of classical techniques (e.g., NPG) in reinforcement learning. Furthermore, REBEL cleanly incorporates offline datasets when available, can be extended to robustly handle intransitive preferences (Swamy et al., 2024), empirically out-performs techniques like PPO and DPO (Rafailov et al., 2023) in language generation, and has a faster convergence with a similar asymptotic performance in image generation. When fine-tuning a Llama-3-8B model, REBEL also demonstrates very competitive performance on AlpacaEval 2.0, MT-bench, Open LLM Leaderboard, and Arena Hard simultaneously. We begin by formalizing the preference fine-tuning setup before deriving our core algorithmic technique. ", "page_idx": 1}, {"type": "text", "text": "2 REBEL: REgression to RElative REward Based RL ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the contextual bandit formulation (Langford and Zhang, 2007) of RL which has been used to formalize the generation process of models like LLMs (Rafailov et al., 2023; Ramamurthy et al., 2022; Chang et al., 2023) and Diffusion Models (Black et al., 2023; Fan et al., 2024; Oertell et al., 2024) due to the determinism of the transitions. More explicitly, in the deterministic transition ", "page_idx": 1}, {"type": "text", "text": "Algorithm 1 REgression to RElative REward Based RL (REBEL) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "1: Input: Reward $r$ , policy class $\\Pi=\\{\\pi_{\\theta}:\\theta\\in\\Theta\\}$ , base distribution $\\mu$ , learning rate $\\eta$   \n2: Initialize policy \u03c0\u03b80.   \n3: for $t=0$ to $T-1$ do   \n4: // Base distribution $\\mu$ can either be an offline dataset or $\\pi_{t}$ .   \n5: Collect dataset $\\mathcal{D}_{t}=\\{x,y,y^{\\prime}\\}$ where $x\\sim\\rho,y\\sim\\pi_{t}(\\cdot|x),y^{\\prime}\\sim\\mu(\\cdot|x)$ .   \n6: Solve square loss regression problem:   \n$\\theta_{t+1}=\\underset{\\theta\\in\\Theta}{\\mathrm{argmin}}\\sum_{(x,y,y^{\\prime})\\in\\mathcal{D}_{t}}\\left(\\frac{1}{\\eta}\\left(\\ln\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta_{t}}(y|x)}-\\ln\\frac{\\pi_{\\theta}(y^{\\prime}|x)}{\\pi_{\\theta_{t}}(y^{\\prime}|x)}\\right)-(r(x,y)-r(x,y^{\\prime}))\\right)^{2}.$ ", "page_idx": 2}, {"type": "text", "text": "7: end for ", "page_idx": 2}, {"type": "text", "text": "setting, explicit states are not required as they are isomorphic to the sequence of actions. Furthermore, the entire sequence of actions can be considered as a single \u201carm\u201d in a bandit problem with an action space that scales exponentially in size with the horizon of the problem. ", "page_idx": 2}, {"type": "text", "text": "We denote by $(x,y)$ a (prompt, response) pair, where $x\\in\\mathscr{X}$ is the prompt and $y\\in\\mathcal{V}$ is the response (e.g. a sequence of tokens, or in general a sequence of actions). We assume access to a reward function $r(x,y)$ from which we can query for reward signals (the exact form of $r$ does not need to be known). Querying $r$ at $(x,y)$ will return a scalar $r(\\bar{x},y)$ , measuring the quality of the prompt completion. Such a reward function could be a pre-defined metric (e.g., Rouge score against human responses) or a learned model from an offline human demonstration or preference data (e.g. the RLHF paradigm (Christiano et al., 2017; Ziegler et al., 2020)), as we focus on in our experiments. ", "page_idx": 2}, {"type": "text", "text": "Denote by $\\pi\\in{\\mathcal{X}}\\mapsto\\Delta(Y)$ a policy (e.g. an LLM) that maps from a prompt $x$ to a distribution over the response space $\\boldsymbol{\\wp}$ . We use $\\rho$ to denote the distribution over prompts (i.e. initial states / contexts) $x$ and $\\bar{\\pi}_{\\theta}(y|x)$ to denote a policy with parameter $\\theta$ . At times, we interchangeably use $\\pi_{t}$ and $\\pi_{\\theta_{t}}$ when it is clear from the context. We emphasize that while we focus on the bandit formulation for notational simplicity, the algorithms proposed here can be applied to any deterministic MDP where $x$ is the initial state and the trajectory $y$ consists of the sequence of actions. ", "page_idx": 2}, {"type": "text", "text": "At each iteration of all algorithms, our goal will be to solve the following KL-constrained RL problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{t+1}=\\operatorname*{argmax}_{\\pi\\in\\Pi}\\mathbb{E}_{x,y\\sim\\pi(\\cdot|x)}r\\bigl(x,y\\bigr)-\\frac{1}{\\eta}\\mathbb{E}_{x}\\mathrm{KL}\\left(\\pi(\\cdot|x)\\vert\\vert\\pi_{t}(\\cdot|x)\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Intuitively, this can be thought of asking for the optimizer to fine-tune the policy $\\pi_{t+1}$ according to $r$ while staying close in terms of action distribution to some baseline policy $\\pi_{t}$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Deriving REBEL: REgression to RElative REward Based RL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "From Ziebart et al. (2008), we know that there exists a closed-form solution to the above minimum relative entropy problem (Eq. 2, Gr\u00fcnwald and Dawid (2004)): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\forall x,y:\\pi_{t+1}(y|x)=\\frac{\\pi_{t}(y|x)\\exp(\\eta r(x,y))}{Z(x)};\\;Z(x)=\\sum_{y}\\pi_{t}(y|x)\\exp(\\eta r(x,y)).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As observed by Rafailov et al. (2023), we can invert Eq. 3 and write the reward in terms of the policy: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\forall x,y:r(x,y)=\\frac{1}{\\eta}\\left(\\ln(Z(x))+\\ln\\left(\\frac{\\pi_{t+1}(y|x)}{\\pi_{t}(y|x)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As soon as $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ become large, we can no longer guarantee the above expression holds exactly at all $(x,y)$ and therefore need to turn our attention to choosing a policy such that Eq. 4 is approximately true. We propose using a simple square loss objective between the two sides of Eq. 4 to measure the goodness of a policy, i.e. reducing RL to a regression problem: $\\begin{array}{r}{\\Big(r(x,y)-\\frac{1}{\\eta}\\left(\\ln(Z(x))+\\ln\\left(\\frac{\\pi_{t+1}(y|x)}{\\pi_{t}(y|x)}\\right)\\right)^{2}}\\end{array}$ . Unfortunately, this loss function includes the partition function $Z(x)$ , which can be challenging to approximate over large input / output domains. ", "page_idx": 2}, {"type": "text", "text": "However, observe that $Z(x)$ only depends on $x$ and not $y$ . Thus, if we have access to paired samples, i.e. $(x,y)$ and $(x,y^{\\prime})$ , we can instead regress the difference in rewards to eliminate this term: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left((r(x,y)-r(x,y^{\\prime}))-\\frac{1}{\\eta}\\left(\\ln\\left(\\frac{\\pi_{t+1}(y|x)}{\\pi_{t}(y|x)}\\right)-\\ln\\left(\\frac{\\pi_{t+1}(y^{\\prime}|x)}{\\pi_{t}(y^{\\prime}|x)}\\right)\\right)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Of course, we need to evaluate this loss function on some distribution of samples. In particular, we propose using an on-policy dataset $D_{t}\\,=\\,\\{x,y,y^{\\prime}\\}$ with $x\\sim\\rho,y\\sim\\pi_{t}(\\cdot|x\\bar{)},y^{\\prime}\\sim\\bar{\\mu}(\\cdot|x)$ , where $\\mu$ is some base distribution. The base distribution $\\mu$ can either be a fixed offline dataset (e.g. the instruction fine-tuning dataset) or $\\pi_{t}$ itself. Thus, the choice of base distribution $\\mu$ determines whether REBEL is hybrid or fully online. Putting it all together, we arrive at our core REBEL objective in Eq. 1. Critically, observe that if we were able to perfectly solve this regression problem, we would indeed recover the optimal solution to the KL-constrained RL problem we outlined in Eq. 2. ", "page_idx": 3}, {"type": "text", "text": "3 Understanding REBEL as an Adaptive Policy Gradient ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we interpret REBEL as an adaptive policy gradient method to illuminate the relationship to past techniques. We start by introducing algorithms such as Mirror Descent, NPG, and PPO, followed by illustrating why REBEL addresses the limitations of these past algorithms. For concision, we postpone an in-depth discussion of related work to Appendix A. ", "page_idx": 3}, {"type": "text", "text": "3.1 Adaptive Gradient Algorithms for Policy Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Mirror Descent. If $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are small discrete spaces, we can use the closed-form expression for the minimum relative entropy problem (Eq. 3). This is equivalent to the classic Mirror Descent (MD) algorithm with $\\mathrm{KL}$ as the Bregman divergence. Both NPG and PPO are approximations of MD. ", "page_idx": 3}, {"type": "text", "text": "Natural Policy Gradient. When $\\boldsymbol{\\wp}$ and $\\mathcal{X}$ are large, we use a parameterized policy denoted as $\\pi_{\\theta}$ with parameter $\\theta$ . Natural Policy Gradient (NPG, Kakade (2001)) approximates the KL in Equation 2 via its second-order Taylor expansion, whose Hessian is known as the Fisher Information Matrix (FIM, Bagnell and Schneider (2003)), $F_{t}$ , i.e. $F_{t}=\\mathbb{E}_{x,y\\sim\\pi_{\\theta_{t}}(\\cdot|x)}\\left[\\nabla\\ln\\pi_{\\theta_{t}}(y|x)\\nabla\\ln\\pi_{\\theta_{t}}(y|x)^{\\top}\\right]$ . Thus, $\\mathbb{E}_{x}\\mathrm{KL}(\\pi_{\\theta}(\\cdot|x)||\\pi_{\\theta_{t}}(\\cdot|x))\\approx(\\theta-\\theta_{t})^{\\top}F_{t}(\\theta-\\theta_{t})$ . The NPG update can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{t+1}=\\theta_{t}+\\eta F_{t}^{\\dagger}\\left(\\mathbb{E}_{x,y\\sim\\pi_{\\theta_{t}}(\\cdot|x)}\\nabla\\ln\\pi_{\\theta_{t}}(y|x)r(x,y)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $F_{t}^{\\dagger}$ is pseudo-inverse of $F_{t}$ . As mentioned above, this update procedure can be understood as performing gradient updates in the local geometry induced by the Fisher information matrix, which ensures that we are taking small steps in policy space rather than in parameter space. NPG, unfortunately, does not scale to modern settings due to need of inverting the FIM at each iteration. ", "page_idx": 3}, {"type": "text", "text": "Proximal Policy Optimization. Proximal Policy Optimization (PPO, Schulman et al. (2017)) takes a more direct route than NPG and uses clipped updates ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{t+1}:=\\operatorname*{argmax}_{\\theta}\\mathbb{E}_{x,y\\sim\\pi_{\\theta_{t}}(\\cdot|x)}\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta_{t}}(y|x)};1-\\epsilon,1+\\epsilon\\right)r(x,y).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While the clipping operator can set the gradient to be zero at samples $(x,y)$ where $\\pi_{\\theta_{t+1}}(y|x)$ is much larger or smaller than $\\pi_{\\theta_{t}}(y|x)$ , it cannot actually guarantee $\\pi_{\\theta_{t+1}}$ staying close to $\\pi_{\\theta_{t}}$ , a phenomenon empirically observed in prior work (Hsu et al., 2020). Furthermore, hard clipping is not adaptive \u2013 it treats all $(x,y)$ equally and clips whenever the ratio is outside of a fixed range. In contrast, constraining the KL divergence to the prior policy allows one to vary the ratio $\\pi(y|x)/\\pi_{t}(y|x)$ at different $(x,y)$ , as long as the total KL divergence across the state space is small. Lastly, clipping reduces the effective size of a batch of training examples and thus wastes training samples. ", "page_idx": 3}, {"type": "text", "text": "3.2 Connections between REBEL and MD / NPG ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Exact REBEL is Mirror Descent. First, to build intuition, we interpret our algorithm\u2019s behavior under the assumption that the least square regression optimization returns the exact Bayes Optimal solution (i.e., our learned predictor achieves zero prediction error everywhere): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall x,y,y^{\\prime}:\\quad{\\frac{1}{\\eta}}\\left(\\ln{\\frac{\\pi_{\\theta_{t+1}}(y|x)}{\\pi_{\\theta_{t}}(y|x)}}-\\ln{\\frac{\\pi_{\\theta_{t+1}}(y^{\\prime}|x)}{\\pi_{\\theta_{t}}(y^{\\prime}|x)}}\\right)=r(x,y)-r(x,y^{\\prime})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Conditioned on Eq. 8 being true, a few lines of algebraic manipulation reveal that there must exist a function c(x) which is independent of y, such that \u2200x, y : \u03b71 l $\\begin{array}{r}{\\dot{,}\\,y:\\frac{1}{\\eta}\\ln\\frac{\\bar{\\pi_{\\theta_{t+1}}}(y|x)}{\\pi_{\\theta_{t}}(y|x)}=r(x,y)\\!+\\!c(x)}\\end{array}$ . Taking an exp on both sides and re-arrange terms, we ge $\\cdot\\forall x,y:\\pi_{\\theta_{t+1}}(y|x)\\propto\\pi_{\\theta_{t}}(y|x)\\exp\\left(\\eta r(x,y)\\right)$ . In other words, under the strong assumption that least square regression returns a point-wise accurate estimator (i.e., Eq. 8), we see the REBEL recovers the exact MD update, which gives it $(a)$ a fast $1/T$ convergence rate (Shani et al., 2020; Agarwal et al., 2021), $(b)$ conservativity, i.e., $\\begin{array}{r l}{\\operatorname*{max}_{x}\\mathrm{KL}(\\pi_{t+1}(\\cdot|x)||\\pi_{t}(\\cdot|x))}&{{}}\\end{array}$ is bounded as long as $\\operatorname*{max}_{x,y}|r(x,y)|$ is bounded, and $(c)$ monotonic policy improvement via the NPG standard analysis (Agarwal et al., 2021). ", "page_idx": 4}, {"type": "text", "text": "NPG is Approximate REBEL with Gauss-Newton Updates. We provide another interpretation of REBEL by showing that NPG (Eq. 6) can be understood as a special case of REBEL where the least square problem in Eq. 1 is approximately solved via a single iteration of the Gauss-Newton algorithm. We start by approximating our predictor $\\begin{array}{r}{\\frac{1}{\\eta}\\ln\\pi_{\\theta}(y|x)/\\bar{\\pi}_{\\theta_{t}}(y|x)}\\end{array}$ by its first order Taylor expansion at $\\begin{array}{r}{\\theta_{t}\\colon\\frac{1}{\\eta}\\left(\\ln\\pi_{\\theta}(y|x)-\\ln\\pi_{\\theta_{t}}(y|x)\\right)\\approx\\frac{1}{\\eta}\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)^{\\top}(\\theta-\\theta_{t})}\\end{array}$ , where $\\approx$ indicates that we ignore higher order terms in the expansion. Define $\\delta:=\\theta-\\theta_{t}$ and replace $\\begin{array}{r}{\\frac{1}{\\eta}\\left(\\ln\\pi_{\\theta}(y|x)-\\ln\\pi_{\\theta_{t}}(y|x)\\right)}\\end{array}$ by its first order approximation in Eq. 1. Then, we have : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\delta}\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi_{\\theta_{t}}(\\cdot|x),y^{\\prime}\\sim\\mu(\\cdot|x)}\\left(\\frac{1}{\\eta}\\left(\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)-\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y^{\\prime}|x)\\right)^{\\top}\\delta-\\left(r(x,y)-r(x,y^{\\prime})\\right)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Further simplifying notation, we denote the uniform mixture of $\\pi_{t}$ and $\\mu$ as $\\pi_{m i x}(\\cdot|x)\\;:=$ $(\\pi_{t}(\\cdot|x)\\,+\\,\\bar{\\mu}(\\cdot|\\dot{x}))\\bar{/}2$ and the Fisher information matrix $F_{t}$ averaged under said mixture as $F_{t}\\ =\\ \\mathbb{E}_{x\\sim\\rho,y\\sim\\pi_{m i x}(\\cdot|x)}\\left[\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)\\left(\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)\\right)^{\\top}\\right]$ . Solving the above least squares problem to obtain a minimum norm solution, we have the following result. ", "page_idx": 4}, {"type": "text", "text": "Claim 1. The minimum norm minimizer $\\delta^{\\star}$ of the least squares problem in Eq. 9 recovers an advantage-based NPG update: $\\delta^{\\star}:=\\eta F_{t}^{\\dagger}\\left(\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi_{m i x}(\\cdot|x)}\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)[A^{\\pi_{t}}(x,y)]\\right)$ where $F_{t}^{\\dagger}$ is pseudo-inverse of $F_{t}$ , and the advantage is defined as $A^{\\pi_{t}}(x,y):=r(x,y)-\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot|x)}r(x,y)$ . ", "page_idx": 4}, {"type": "text", "text": "The proof of this claim is deferred to Appendix B. ", "page_idx": 4}, {"type": "text", "text": "The implicit variance reduction effect of REBEL We show that regressing to relative rewards has a variance reduction effect by extending the previous derivation on REBEL with Gauss-Newton update to the setting of finite data $\\bar{\\boldsymbol{D}}\\,\\stackrel{<}{=}\\,\\{x_{n},\\}_{n},y_{n}^{\\prime}\\}_{n=1}^{N}$ . Denote the unbiased estimate of the Fisher information matrix as $\\begin{array}{r}{\\hat{F}_{t}=\\frac{1}{N}\\sum_{n=1}^{N}\\left[\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y_{n}|x_{n})\\left(\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y_{n}|x_{n})\\right)^{\\top}\\right]}\\end{array}$ and have the following claim. ", "page_idx": 4}, {"type": "text", "text": "Claim 2. The minimum norm minimizer $\\delta^{\\star}$ in Eq. 9 under finite setting has the form $\\delta^{\\star}:=$ $\\begin{array}{r}{\\hat{F}_{t}^{\\dagger}\\frac{1}{2N}\\sum_{n}\\left(\\nabla\\ln\\pi_{\\theta_{t}}(y_{n}|x_{n})(r(x_{n},y_{n})-r(x_{n},y_{n}^{\\prime}))+\\nabla\\ln\\pi_{\\theta_{t}}(y_{n}^{\\prime}|x_{n})(r(x_{n},y_{n}^{\\prime})-r(x_{n},y_{n}))\\right)}\\end{array}$ where $\\hat{F}_{t}^{\\dag}$ is pseudo-inverse of $\\hat{F}_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "The proof of this claim is deferred to Appendix C. Looking at the gradient formulation $\\nabla\\ln\\bar{\\pi}_{\\boldsymbol{\\theta}_{t}}(y_{n}|x_{n})\\left(r(x_{n},y_{n})-r(x_{n},y_{n}^{\\prime})\\right)$ in $\\delta^{\\star}$ , we see that $r(x_{n},y_{n}^{\\prime})$ serves as a baseline for variance reduction. Interestingly, this gradient formulation is similar to RLOO (REINFORCE with leave-one-out) (Kool et al., 2019). However, different from RLOO, we pre-condition this variance reduced policy gradient formulation via the Fisher information matrix, leading to better performance. ", "page_idx": 4}, {"type": "text", "text": "A REBEL With a Cause. Our algorithm REBEL addresses the limitations of NPG (scalability) and PPO (lack of conservativity or adaptivity) from above. First, unlike NPG, it does not rely on the Fisher Information Matrix at all and can easily scale to modern LLM and image generation applications, yet can be interpreted as a generalization of NPG. Second, in contrast to PPO, it doesn\u2019t have unjustified heuristics and thus enjoys strong convergence and regret guarantees just like NPG. Building on Swamy et al. (2024), we also show how to extend REBEL to preference-based settings without assuming transitivity in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the previous section, we interpret REBEL as exact MD and show its convergence by assuming that least square regression always returns a predictor that is accurate everywhere. While such an explanation is simple and has also been used in prior work (Calandriello et al., 2024; Rosset et al., 2024), point-wise out-of-distribution generalization is an extremely strong condition and is significantly beyond what a standard supervised learning method can promise. In this section, we substantially relax this condition via a reduction-based analysis: As long as we can solve the regression problems well in an in-distribution manner, REBEL can compete against any policy covered by the training data distributions. Formally, we assume the following generalization condition holds on the regressors we find. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Assumption 1 (Regression generalization bounds). Over $T$ iterations, assume that for all $t$ , we have the following for some \u03f5: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi_{t}(\\cdot\\vert x),y^{\\prime}\\sim\\mu(\\cdot\\vert x)}\\left(\\frac{1}{\\eta}\\left(\\ln\\frac{\\pi_{\\theta_{t+1}}(y\\vert x)}{\\pi_{\\theta_{t}}(y\\vert x)}-\\ln\\frac{\\pi_{\\theta_{t+1}}(y^{\\prime}\\vert x)}{\\pi_{\\theta_{t}}(y^{\\prime}\\vert x)}\\right)-(r(x,y)-r(x,y^{\\prime}))\\right)^{2}\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Detailed justifications for this assumption are provided in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "Data Coverage. Recall that the base distribution $\\mu$ can be some behavior policy, which in RLHF can be a human labeler, a supervised fine-tuned policy (SFT), or just the current learned policy (i.e., on-policy). Given a test policy $\\pi$ , we denote by $C_{\\mu\\to\\pi}$ the concentrability coefficient, i.e. ", "page_idx": 5}, {"type": "equation", "text": "$$\nC_{\\mu\\to\\pi}=\\operatorname*{max}_{x,y}\\frac{\\pi(y|x)}{\\mu(y|x)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We say $\\mu$ covers $\\pi$ if $C_{\\mu\\to\\pi}<+\\infty$ . Our goal is to bound the regret between our learned policies and an arbitrary comparator $\\pi^{*}$ (e.g. the optimal policy if it is covered by $\\mu$ ) using $\\epsilon$ and the concentrability coefficient defined in Eq. 10. The following theorem formally states the regret bound of our algorithm. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Under Assumption $I$ , after $T$ many iterations, with a proper learning rate $\\eta_{-}$ , among the learned policies $\\pi_{1},\\dots,\\pi_{T}$ , there must exist a policy $\\hat{\\pi}$ , such that: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\forall\\pi^{*}:\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi^{*}(\\cdot|x)}r(x,y)-\\mathbb{E}_{x\\sim\\rho,y\\sim\\hat{\\pi}(\\cdot|x)}r(x,y)\\le O\\left(\\sqrt{\\frac{1}{T}}+\\sqrt{C_{\\mu\\to\\pi^{*}}\\epsilon}.\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The above theorem shows a reduction from RL to supervised learning \u2014 as long as supervised learning works (i.e., $\\epsilon$ is small), then REBEL can comp\u221aete against any policy $\\pi^{*}$ that is covered by the base data distribution $\\mu$ . In the regret bound, the $1/\\sqrt{T}$ comes from Mirror Descent style update, and $C_{\\mu\\to\\pi^{*}}\\epsilon$ captures the cost of distribution shift: we train our regressors under distribution $\\pi_{t}$ and $\\mu$ , but we want the learned regressor to predict well under $\\pi^{*}$ . Si\u221amilar to the NPG analysis from Agarwal et al. (2021), we now have a slower convergence rate $1/\\sqrt{T}$ , due to the fact that we have approximation error from learning. Such an agnostic regret bound \u2014 being able to compete against any policy that is covered by training distributions \u2014 is the strongest type of agnostic learning results known in the RL literature, matching the best of what has appeared in prior policy optimization work including PSDP (Bagnell et al., 2003), CPI (Kakade and Langford, 2002), NPG (Agarwal et al., 2021), and PC-PG (Agarwal et al., 2020). While in this work we use the simplest and most intuitive definition of coverage \u2013 the density ratio-based definition in Eq. 10 \u2013 extension to more general ones such as transfer error (Agarwal et al., 2020, 2021) or concentrability coefficients that incorporate the function class (e.g., Song et al. (2023)) is straightforward. We defer the proof of the above theorem and the detailed constants that we omitted in the $O$ notation to Appendix F. We include an extension of the above analysis to the general preference setting in Appendix G. ", "page_idx": 5}, {"type": "text", "text": "Remark 1 (Discussion on the size of the response space $|\\mathcal{V}|$ and other design choices of the sampling distributions). In REBEL, when sampling a pair $(y,y^{\\prime})$ , we in default sample $y\\sim\\pi_{t}$ , i.e., we make sure at least one of them is an on-policy sample. This is to make sure that the training distribution at iteration $t$ covers $\\pi_{t}$ , which plays an essential role in avoiding a polynomial dependency on the size of the action space $|\\mathcal{V}|$ .2 On the other hand, as long as we have some off-policy distribution $\\nu_{t}$ that covers $\\pi_{t}$ for all $t_{;}$ , we can use it to sample y and pay an additional concentrability coefficient $\\mathrm{max}_{x,y,t}\\,\\pi_{t}(y|x)/\\nu_{t}(y|x)$ in the final bound. In experiments, we test the combination of the best-of-N of $\\pi_{t}$ as the base distribution $\\mu$ and the worst-of- $\\cdot N$ of $\\pi_{t}$ as the $\\nu_{t}$ . Setting $\\mu$ to be the best-of-N of $\\pi_{t}$ makes $\\mu$ cover higher quality comparator policies. Selecting $\\nu_{t}$ as the worst-of-N of $\\pi_{t}$ still ensures coverage to $\\pi_{t}$ while at the same time increasing the reward gap $r(x,y)-r(x,y^{\\prime})$ , which we find is helpful experimentally. ", "page_idx": 5}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/3c7c76f19e86927c771c0edf250936a1b9ffa2f7b035ca56e7f8edf2ff5360a2.jpg", "table_caption": ["Table 1: Results on $T L;D R$ Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our implementation of REBEL closely follows the psuedocode in Algorithm 1. In each iteration, REBEL collects a dataset $\\boldsymbol{D}_{t}\\,=\\,\\{x,y,y^{\\prime}\\}$ , where $x\\sim\\rho,y\\sim\\pi_{t}(\\cdot|x),y^{\\prime}\\sim\\mu(\\cdot|x)$ . Subsequently, REBEL optimizes the least squares regression problem in Eq. 1 through gradient descent with AdamW (Loshchilov and Hutter, 2017). We choose $\\mu\\,=\\,\\pi_{t}$ such that both $y$ and $y^{\\prime}$ are generated by the current policy. We empirically assess REBEL\u2019s performance on both natural language generation and text-guided image generation. Additional experiment details are in Appendix H. ", "page_idx": 6}, {"type": "text", "text": "5.1 Summarization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Task. We use the $T L;D R$ dataset (Stiennon et al., 2020) where $x$ is a forum post from Reddit and $y$ is a summary generated by the policy. The dataset comprises human reference summaries and preference data. We compare REBEL with baseline RL algorithms, REINFORCE (Williams, 1992) and its multi-sample extension, REINFORCE Leave-One-Out (RLOO) (Kool et al., 2019), PPO (Schulman et al., 2017), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Iterative DPO (Guo et al., 2024). Our implementation of Iterative DPO replaces our square regression objective with the DPO objective where the binary preference labels are obtained based on the reward difference. The implementation detail of the baseline methods is provided in Appendix H.1.3. Following prior work (Stiennon et al., 2020; Rafailov et al., 2023; Ahmadian et al., 2024), we train DPO on the preference dataset, while conducting online RL (RLOO, PPO, Iterative DPO, REBEL) on the human reference dataset. We include results with three different model sizes: 1.4B, 2.8B, and 6.9B based on the pre-trained models from Pythia (Biderman et al., 2023). Each model is trained from a supervised fine-tuned (SFT) model using a reward model (RM) of the same size. ", "page_idx": 6}, {"type": "text", "text": "Evaluation. We evaluate each method by its balance between reward model score and KL-divergence with the SFT policy, testing the effectiveness of the algorithm in optimizing the regularized RL objective. To evaluate the quality of the generation, we compute the winrate (Rafailov et al., 2023) against human references using GPT4 (OpenAI, 2023). The winrate is computed from a randomly sampled subset $(10\\%)$ of the test set with 600 samples. We report the average results over three seeds. ", "page_idx": 6}, {"type": "text", "text": "Quality Analysis. Table 1 presents a comparison between REBEL and baseline methods. Notably, REBEL outperforms all the baselines on RM score with 1.4B and 2.8B parameters with a slightly larger KL than PPO. In addition, REBEL achieves the highest winrate under GPT4 when evaluated against human references, indicating the benefit of regressing the relative rewards. An ablation analysis on parameter $\\eta$ is in Appendix J and the trade-off between the reward model score and KL-divergence is discussed in Appendix K. ", "page_idx": 6}, {"type": "text", "text": "Runtime & Memory Analysis. We analyze the runtime and peak memory usage for 2.8B models with REINFORCE, RLOO, PPO, DPO, Iterative DPO, and REBEL. The runtime includes both the generation time and the time required for policy updates. Both runtime and peak memory usage are measured on A6000 GPUs using the same hyperparameters detailed in Appendix H.1.5 for a batch of 512 prompts. The measurements are averaged over 100 batches. Methods are ascendingly ordered by winrate. To the right of the dashed line, PPO, RLOO $(k\\ =\\ 4)$ ), and REBEL have the highest winrates, which are comparable among them. ", "page_idx": 7}, {"type": "image", "img_path": "yxjWAJzUyV/tmp/5f2ccd8907b01ede55f218134848576c364836430c2bb37c7885697e101be32f.jpg", "img_caption": ["Figure 2: Plot of runtime and memory usage. Baselines on the left-hand side of the dashed line have lower winrates. Methods on the right-hand side of the dashed line have similar winrates to REBEL. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "While DPO and REINFORCE are more time and memory-efficient, their performance does not match up to REBEL, as shown in Table 1. RLOO $\\;k=2)$ and Iterative DPO have similar runtime and memory usage as REBEL since we set $\\mu=\\pi_{t}$ , making REBEL also generate twice per prompt. However, both methods have worse performance than REBEL. Compared to PPO and RLOO $\\lvert k=4$ ), REBEL demonstrates shorter runtimes and lower peak memory usage. PPO is slow and requires more memory since it needs to update two networks (the policy network and the value network). RLOO $[k=4]$ ) requires four generations per prompt which makes it slow and less memory efficient. In summary, compared to the two baselines (PPO and RLOO $\\!\\!\\!/k=4\\!\\!\\!/$ ) that achieve similar winrates as REBEL, REBEL is more computationally tractable and simpler to implement. ", "page_idx": 7}, {"type": "text", "text": "5.2 General Chat ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Task. We consider a general chat scenario where $x$ is a prompt from the user and $y$ is a response. We adapt the setting from Zhu et al. (2023), using OpenChat-3.5 (Wang et al., 2024) as the base model, Starling-RM-7B-alpha (Zhu et al., 2023) as the reward model, and the Nectar dataset (Zhu et al., 2023). This setup enables a direct comparison between REBEL and APA (Zhu et al., 2023) which is used to train Starling-LM-7B-alpha. ", "page_idx": 7}, {"type": "text", "text": "Evaluation. Following previous works, we use AlpacaEval 2.0 (Dubois et al., 2024), MTBench (Zheng et al., 2023), and Open LLM Leaderboard (Beeching et al., 2023) as metrics. AlpacaEval 2.0 uses prompts from AlpacaFarm (Dubois et al., 2024) to compare model responses against a reference response generated by GPT-4-Turbo. We report the winrate over the reference responses. MT-Bench consists of 80 open-ended questions on various topics. Answers are scored directly by GPT-4. Open LLM Leaderboard consists of MMLU (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), Arc (Clark et al., 2018), Winogrande (Sakaguchi et al., 2019), TruthfulQA (Lin et al., 2022), and HellaSwag (Zellers et al., 2019). The prompts of the tasks consist of zero or few-shot samples. ", "page_idx": 7}, {"type": "text", "text": "Quality Analysis. The results between models trained with REBEL and baseline methods are shown in Table 2. For MT-Bench and AlpacaEval 2.0, under the same setup, REBEL outperforms APA (Zhu et al., 2023) on both metrics, demonstrating the effectiveness of REBEL under chat setting and its superior performance over APA. For the metrics on Open LLM Leaderboard, REBEL is able to enhance the performance of GSM8K and HellaSwag and maintain the overall average as the base models. Similar values on MMLU as base models indicate that we preserve the basic capability of the pre-trained model during the RL fine-tuning process. We include a breakdown of MT-Bench in Appendix M. ", "page_idx": 7}, {"type": "text", "text": "5.2.1 Ablation: batch size and data sampling distributions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Task. In the previous section, we sample $y$ and $y^{\\prime}$ from $\\pi_{t}(\\cdot|x)$ and we use small batch size with $|\\mathcal{D}_{t}|=32$ . In this section, we investigate the alternative sampling distribution described in Remark 1. Specifically, at each iteration, we generate 5 responses from $\\pi_{t}$ for each prompt in the entire dataset (i.e., $|\\mathcal{D}_{t}|$ is the size of the entire dataset), rank them based on the reward model, and set $y$ to be the best of the five responses, and $y^{\\prime}$ to be the worst of the five responses. We perform 3 iterations for this setup with Meta-Llama-3-8B-Instruct (Meta, 2024) as the base model, ArmoRM-Llama3- ", "page_idx": 7}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/7a8c2cb59140dbadfb49c6dc4f39b67058159c4e513e11dffdbaa40798f6a412.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/fca5132ae3db0a1a73b37e595c1c460eaed133b9ccf6613aaed0649618e7a6d7.jpg", "table_caption": ["Table 2: Results on General Chat. The best-performing method for each metric is highlighted in bold. Note that the APA result is directly obtained by evaluating the Starling-LM-7B-alpha model. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Ablation Results. In this table, REBEL uses a larger batch size (the entire dataset) with the best-of-N and worst-of-N $\\mathrm{\\Delta}N=5,$ ) of $\\pi_{t}$ as the sampling distributions for generating pairs $y,y^{\\prime}$ . The best-performing method for each metric is highlighted in bold. Note that DPO is trained on the online data generated by the base model and labeled by the RM. ", "page_idx": 8}, {"type": "text", "text": "8B-v0.1 (Wang et al.) as the reward model, and the UltraFeedback dataset (Cui et al., 2023). We compare REBEL with DPO which is also trained for one epoch on the entire dataset with best-of-5 as $y_{w}$ and worst-of-5 as $y_{l}$ sampled from $\\pi_{0}$ . In other words, the training data used for the first iteration of REBEL is the same as the one we use for $\\mathrm{DPO}^{3}$ . We follow the same evaluation methods as the previous section and include Arena Hard (AH) (Li et al., 2024) in our analysis. ", "page_idx": 8}, {"type": "text", "text": "Quality Analysis. Results in Table 3 show that REBEL can significantly improve the base model\u2019s performance, especially on AlpacaEval 2.0 and Arena Hard. Compared to DPO, the model trained by REBEL with 1 iteration is better in almost all datasets, demonstrating the benefit of using the fine-grained reward gap in policy optimization over just the zero-one labels. In this large batch setting, we find that more iterations in general do not help performance. We conjecture that this is the issue of overftiting to the training dataset. A more diverse and larger dataset can potentially address this issue. ", "page_idx": 8}, {"type": "text", "text": "5.3 Image Generation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Task. We also consider the setting of image generation, where, given a consistency model (Song et al., 2023) and a target reward function, we seek to train the consistency model to output images that garner a higher reward. We use 45 common animals as generation prompts similar to Black et al. (2023); Oertell et al. (2024) and the latent consistency model (Luo et al., 2023) distillation of the Dreamshaper v7 model, a finetune of stable diffusion (Rombach et al., 2021). We compare REBEL to a clipped, policy gradient objective (Black et al., 2023; Fan et al., 2024; Oertell et al., 2024) with the aim to optimize aesthetic quality to obtain a high reward from the LAION aesthetic score predictor (Schuhmann, 2022). This baseline does not use critics or GAE for advantage estimates. However, the clipping objective is clearly motivated by PPO, and thus, we simply name this baseline as PPO. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Evaluation. We evaluate on the reward under the LAION aesthetic reward model for an equal number of reward queries/samples generated and an equal number of gradient updates. The aesthetic predictor is trained to predict human-labeled scores of images on a scale of 1 to 10. Images that tend to have the highest reward are artwork. Following Agarwal et al. (2021), we report inter-quartile means (IQM) with $95\\%$ confidence intervals (CIs) across three seeds for both REBEL and PPO. The CIs were calculated with percentile bootstrap with stratified sampling over three random seeds. ", "page_idx": 8}, {"type": "image", "img_path": "yxjWAJzUyV/tmp/c4db8f251cb069a84737de8ddbb3a46c6729e51be8d0dcced3268140f361434f.jpg", "img_caption": ["Figure 3: Learning curves as a function of reward queries to the LAION aesthetic predictor. The colored areas represent $95\\%$ CIs. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "yxjWAJzUyV/tmp/adfa44a43270965d4bcb409b2c91e8304248803c19333aa623323283f94e1848.jpg", "img_caption": ["Figure 4: Generated images using PPO and REBEL during an intermediate checkpoint. At the same number of epochs, REBEL observes a higher reward under the reward model. This can further be seen by the more diverse background of images generated from REBEL with less training time. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Quality Analysis. Figure 3 shows REBEL optimizes the consistency model faster during the beginning of training and eventually achieves a performance similar to that of PPO. For our experiments, we tuned both batch size and learning rate for our algorithms, testing batch sizes of [4, 8, 16] per GPU and learning rates $\\left[1\\mathrm{e}-4,3\\mathrm{e}-4,6\\mathrm{e}-4,1\\mathrm{e}-3\\right]$ . The main difference in implementation between PPO and REBEL is the replacement of the clipped PPO objective with our regression objective. To maximize LAION-predicted aesthetic quality, both REBEL and PPO transform a model that produces plain images into one that produces artistic drawings. We found across multiple seeds that REBEL produced lush backgrounds when compared to PPO\u2019s generations. Please see Appendix I.3 for more examples of generated images. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose REBEL, an RL algorithm that reduces the problem of RL to solving a sequence of relative reward regression problems on iteratively collected datasets. In contrast to policy gradient approaches that require additional networks and heuristics like clipping to ensure optimization stability, it suffices for REBEL to merely drive down error on a least squares problem, making it strikingly simple to implement and scale. In theory, REBEL matches the best guarantees we have for RL algorithms in the agnostic setting, while in practice, REBEL is able to match and sometimes outperform methods that are far more complex to implement or expensive to run across both language modeling and guided image generation tasks. ", "page_idx": 9}, {"type": "text", "text": "There are several open questions raised by our work. The first is whether using a loss function other than square loss (e.g. log loss or cross-entropy) could lead to better performance in practice (Farebrother et al., 2024) or tighter bounds (e.g. first-order / gap-dependent) in theory (Foster and Krishnamurthy, 2021; Wang et al., 2023, 2024). The second is whether, in the general (i.e. nonutility-based) preference setting, the coverage condition assumed in our analysis is necessary \u2013 we conjecture it is. Relatedly, it would be interesting to explore whether using preference (rather than reward) models to provide supervision for REBEL replicates the performance improvements reported by Swamy et al. (2024); Munos et al. (2023); Calandriello et al. (2024). Third, while we focus primarily on the bandit setting in the preceding sections, it would be interesting to consider the more general RL setting and explore how offline datasets can be used to improve the efficiency of policy optimization via techniques like resets (Bagnell et al., 2003; Ross and Bagnell, 2014; Swamy et al., 2023; Chang et al., 2023, 2024; Ren et al., 2024; Dice et al., 2024). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "ZG and JDC are supported by LinkedIn under the LinkedIn-Cornell Grant. GKS is supported by his family and friends. KB is supported by NSF under grant No. 2127309 to the Computing Research Association for the CIFellows Project. JDL acknowledges support of the NSF CCF 2002272, NSF IIS 2107304, and NSF CAREER Award 2144994. WS acknowledges funding from NSF IIS-2154711, NSF CAREER 2339395, DARPA LANCER: LeArning Network CybERagents, and Cornell Infosys Collaboration. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Kalashnikov, D.; Irpan, A.; Pastor, P.; Ibarz, J.; Herzog, A.; Jang, E.; Quillen, D.; Holly, E.; Kalakrishnan, M.; Vanhoucke, V.; others Scalable deep reinforcement learning for vision-based robotic manipulation. Conference on robot learning. 2018; pp 651\u2013673. ", "page_idx": 10}, {"type": "text", "text": "Stiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D. M.; Lowe, R.; Voss, C.; Radford, A.; Amodei, D.; Christiano, P. Learning to summarize from human feedback. 2022.   \nOuyang, L. et al. Training language models to follow instructions with human feedback. 2022.   \nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; Klimov, O. Proximal Policy Optimization Algorithms. 2017.   \nZiegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford, A.; Amodei, D.; Christiano, P.; Irving, G. Fine-Tuning Language Models from Human Preferences. 2020.   \nTouvron, H. et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. 2023.   \nBlack, K.; Janner, M.; Du, Y.; Kostrikov, I.; Levine, S. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301 2023,   \nFan, Y.; Watkins, O.; Du, Y.; Liu, H.; Ryu, M.; Boutilier, C.; Abbeel, P.; Ghavamzadeh, M.; Lee, K.; Lee, K. Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems 2024, 36.   \nOertell, O.; Chang, J. D.; Zhang, Y.; Brantley, K.; Sun, W. RL for Consistency Models: Faster Reward Guided Text-to-Image Generation. arXiv preprint arXiv:2404.03673 2024,   \nSchulman, J.; Moritz, P.; Levine, S.; Jordan, M.; Abbeel, P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438 2015,   \nKakade, S.; Langford, J. Approximately optimal approximate reinforcement learning. Proceedings of the Nineteenth International Conference on Machine Learning. 2002; pp 267\u2013274.   \nAhmadian, A.; Cremer, C.; Gall\u00e9, M.; Fadaee, M.; Kreutzer, J.; Pietquin, O.; \u00dcst\u00fcn, A.; Hooker, S. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs. 2024.   \nHenderson, P.; Islam, R.; Bachman, P.; Pineau, J.; Precup, D.; Meger, D. Deep Reinforcement Learning that Matters. 2019.   \nEngstrom, L.; Ilyas, A.; Santurkar, S.; Tsipras, D.; Janoos, F.; Rudolph, L.; Madry, A. Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO. 2020.   \nRafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning, C. D.; Finn, C. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. 2023.   \nSwamy, G.; Dann, C.; Kidambi, R.; Wu, Z. S.; Agarwal, A. A Minimaximalist Approach to Reinforcement Learning from Human Feedback. arXiv preprint arXiv:2401.04056 2024,   \nLangford, J.; Zhang, T. The epoch-greedy algorithm for multi-armed bandits with side information. Advances in neural information processing systems 2007, 20.   \nRamamurthy, R.; Ammanabrolu, P.; Brantley, K.; Hessel, J.; Sifa, R.; Bauckhage, C.; Hajishirzi, H.; Choi, Y. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241 2022,   \nChang, J. D.; Brantley, K.; Ramamurthy, R.; Misra, D.; Sun, W. Learning to Generate Better Than Your LLM. 2023.   \nChristiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.; Amodei, D. Deep Reinforcement Learning from Human Preferences. Advances in Neural Information Processing Systems. 2017.   \nZiebart, B. D.; Maas, A. L.; Bagnell, J. A.; Dey, A. K.; others Maximum entropy inverse reinforcement learning. Aaai. 2008; pp 1433\u20131438.   \nGr\u00fcnwald, P. D.; Dawid, A. P. Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory. 2004.   \nKakade, S. M. A Natural Policy Gradient. Advances in Neural Information Processing Systems. 2001.   \nBagnell, J. A.; Schneider, J. Covariant policy search. Proceedings of the 18th international joint conference on Artificial intelligence. 2003; pp 1019\u20131024.   \nHsu, C. C.-Y.; Mendler-D\u00fcnner, C.; Hardt, M. Revisiting Design Choices in Proximal Policy Optimization. 2020.   \nShani, L.; Efroni, Y.; Mannor, S. Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps. Proceedings of the AAAI Conference on Artificial Intelligence. 2020; pp 5668\u20135675.   \nAgarwal, A.; Kakade, S. M.; Lee, J. D.; Mahajan, G. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research 2021, 22, 1\u201376.   \nKool, W.; van Hoof, H.; Welling, M. Buy 4 REINFORCE Samples, Get a Baseline for Free! DeepRLStructPred@ICLR. 2019.   \nCalandriello, D.; Guo, D.; Munos, R.; Rowland, M.; Tang, Y.; Pires, B. A.; Richemond, P. H.; Lan, C. L.; Valko, M.; Liu, T.; others Human Alignment of Large Language Models through Online Preference Optimisation. arXiv preprint arXiv:2403.08635 2024,   \nRosset, C.; Cheng, C.-A.; Mitra, A.; Santacroce, M.; Awadallah, A.; Xie, T. Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences. arXiv preprint arXiv:2404.03715 2024,   \nBagnell, J.; Kakade, S. M.; Schneider, J.; Ng, A. Policy search by dynamic programming. Advances in neural information processing systems 2003, 16.   \nAgarwal, A.; Henaff, M.; Kakade, S.; Sun, W. Pc-pg: Policy cover directed exploration for provable policy gradient learning. Advances in neural information processing systems 2020, 33, 13399\u2013 13412.   \nSong, Y.; Zhou, Y.; Sekhari, A.; Bagnell, J. A.; Krishnamurthy, A.; Sun, W. Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient. 2023.   \nAgarwal, A.; Jiang, N.; Kakade, S. M.; Sun, W. Reinforcement learning: Theory and algorithms. 2019.   \nLoshchilov, I.; Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 2017,   \nHuang, S.; Noukhovitch, M.; Hosseini, A.; Rasul, K.; Wang, W.; Tunstall, L. The $\\mathrm{N}+$ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization. 2024.   \nStiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D.; Lowe, R.; Voss, C.; Radford, A.; Amodei, D.; Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems 2020, 33, 3008\u20133021.   \nWilliams, R. J. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Mach. Learn. 1992, 8, 229\u2013256.   \nGuo, S.; Zhang, B.; Liu, T.; Liu, T.; Khalman, M.; Llinares, F.; Rame, A.; Mesnard, T.; Zhao, Y.; Piot, B.; others Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792 2024,   \nBiderman, S.; Schoelkopf, H.; Anthony, Q. G.; Bradley, H.; O\u2019Brien, K.; Hallahan, E.; Khan, M. A.; Purohit, S.; Prashanth, U. S.; Raff, E.; others Pythia: A suite for analyzing large language models across training and scaling. International Conference on Machine Learning. 2023; pp 2397\u20132430.   \nOpenAI Gpt-4 technical report. 2023.   \nZhu, B.; Frick, E.; Wu, T.; Zhu, H.; Jiao, J. Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF. 2023.   \nWang, G.; Cheng, S.; Zhan, X.; Li, X.; Song, S.; Liu, Y. OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. 2024.   \nDubois, Y.; Galambosi, B.; Liang, P.; Hashimoto, T. B. Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators. 2024.   \nZheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.; Stoica, I. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. 2023.   \nBeeching, E.; Fourrier, C.; Habib, N.; Han, S.; Lambert, N.; Rajani, N.; Sanseviero, O.; Tunstall, L.; Wolf, T. Open LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/ open_llm_leaderboard, 2023.   \nDubois, Y.; Li, X.; Taori, R.; Zhang, T.; Gulrajani, I.; Ba, J.; Guestrin, C.; Liang, P.; Hashimoto, T. B. AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. 2024.   \nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; Steinhardt, J. Measuring Massive Multitask Language Understanding. 2021.   \nCobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; Hesse, C.; Schulman, J. Training Verifiers to Solve Math Word Problems. 2021.   \nClark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; Tafjord, O. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. 2018.   \nSakaguchi, K.; Bras, R. L.; Bhagavatula, C.; Choi, Y. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale. 2019.   \nLin, S.; Hilton, J.; Evans, O. TruthfulQA: Measuring How Models Mimic Human Falsehoods. 2022.   \nZellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; Choi, Y. HellaSwag: Can a Machine Really Finish Your Sentence? 2019.   \nZhu, B.; Sharma, H.; Frujeri, F. V.; Dong, S.; Zhu, C.; Jordan, M. I.; Jiao, J. Fine-Tuning Language Models with Advantage-Induced Policy Alignment. 2023.   \nMeta Introducing Meta Llama 3: The most capable openly available LLM to date. 2024; https: //ai.meta.com/blog/meta-llama-3/.   \nWang, H.; Xiong, W.; Xie, T.; Zhao, H.; Zhang, T. Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts. arXiv preprint arXiv:2406.12845 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Cui, G.; Yuan, L.; Ding, N.; Yao, G.; Zhu, W.; Ni, Y.; Xie, G.; Liu, Z.; Sun, M. UltraFeedback: Boosting Language Models with High-quality Feedback. 2023. ", "page_idx": 13}, {"type": "text", "text": "Song, Y.; Swamy, G.; Singh, A.; Bagnell, J. A.; Sun, W. Understanding Preference Fine-Tuning Through the Lens of Coverage. arXiv preprint arXiv:2406.01462 2024, ", "page_idx": 13}, {"type": "text", "text": "Li, T.; Chiang, W.-L.; Frick, E.; Dunlap, L.; Wu, T.; Zhu, B.; Gonzalez, J. E.; Stoica, I. From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline. 2024; https://arxiv.org/abs/2406.11939. ", "page_idx": 13}, {"type": "text", "text": "Song, Y.; Dhariwal, P.; Chen, M.; Sutskever, I. Consistency models. arXiv preprint arXiv:2303.01469 2023, ", "page_idx": 13}, {"type": "text", "text": "Luo, S.; Tan, Y.; Huang, L.; Li, J.; Zhao, H. Latent Consistency Models: Synthesizing HighResolution Images with Few-Step Inference. 2023. ", "page_idx": 13}, {"type": "text", "text": "Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; Ommer, B. High-Resolution Image Synthesis with Latent Diffusion Models. 2021. ", "page_idx": 13}, {"type": "text", "text": "Schuhmann, C. Laion aesthetics. https://laion.ai/blog/laion-aesthetics/, 2022. ", "page_idx": 13}, {"type": "text", "text": "Agarwal, R.; Schwarzer, M.; Castro, P. S.; Courville, A. C.; Bellemare, M. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems 2021, 34, 29304\u201329320. ", "page_idx": 13}, {"type": "text", "text": "Farebrother, J.; Orbay, J.; Vuong, Q.; Ta\u00efga, A. A.; Chebotar, Y.; Xiao, T.; Irpan, A.; Levine, S.; Castro, P. S.; Faust, A.; Kumar, A.; Agarwal, R. Stop Regressing: Training Value Functions via Classification for Scalable Deep RL. 2024. ", "page_idx": 13}, {"type": "text", "text": "Foster, D. J.; Krishnamurthy, A. Efficient First-Order Contextual Bandits: Prediction, Allocation, and Triangular Discrimination. 2021. ", "page_idx": 13}, {"type": "text", "text": "Wang, K.; Zhou, K.; Wu, R.; Kallus, N.; Sun, W. The benefits of being distributional: Small-loss bounds for reinforcement learning. Advances in Neural Information Processing Systems 2023, 36. ", "page_idx": 13}, {"type": "text", "text": "Wang, K.; Oertell, O.; Agarwal, A.; Kallus, N.; Sun, W. More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning. arXiv preprint arXiv:2402.07198 2024, ", "page_idx": 13}, {"type": "text", "text": "Munos, R.; Valko, M.; Calandriello, D.; Azar, M. G.; Rowland, M.; Guo, Z. D.; Tang, Y.; Geist, M.; Mesnard, T.; Michi, A.; others Nash learning from human feedback. arXiv preprint arXiv:2312.00886 2023, ", "page_idx": 13}, {"type": "text", "text": "Ross, S.; Bagnell, J. A. Reinforcement and Imitation Learning via Interactive No-Regret Learning. ArXiv 2014, abs/1406.5979. ", "page_idx": 13}, {"type": "text", "text": "Swamy, G.; Choudhury, S.; Bagnell, J. A.; Wu, Z. S. Inverse Reinforcement Learning without Reinforcement Learning. ArXiv 2023, abs/2303.14623. ", "page_idx": 13}, {"type": "text", "text": "Chang, J. D.; Shan, W.; Oertell, O.; Brantley, K.; Misra, D.; Lee, J. D.; Sun, W. Dataset Reset Policy Optimization for RLHF. arXiv preprint arXiv:2404.08495 2024, ", "page_idx": 13}, {"type": "text", "text": "Ren, J.; Swamy, G.; Wu, Z. S.; Bagnell, J. A.; Choudhury, S. Hybrid Inverse Reinforcement Learning. arXiv preprint arXiv:2402.08848 2024, ", "page_idx": 13}, {"type": "text", "text": "Dice, N. E.; Swamy, G.; Choudhury, S.; Sun, W. Efficient Inverse Reinforcement Learning without Compounding Errors. ICML 2024 Workshop on Models of Human Feedback for AI Alignment. 2024. ", "page_idx": 13}, {"type": "text", "text": "Nemirovskij, A. S.; Yudin, D. B. Problem complexity and method efficiency in optimization. 1983. ", "page_idx": 13}, {"type": "text", "text": "Konda, V.; Tsitsiklis, J. Actor-Critic Algorithms. Advances in Neural Information Processing Systems. 1999. ", "page_idx": 13}, {"type": "text", "text": "Richter, L.; Boustati, A.; N\u00fcsken, N.; Ruiz, F.; Akyildiz, O. D. VarGrad: a low-variance gradient estimator for variational inference. Advances in Neural Information Processing Systems 2020, 33, 13481\u201313492. ", "page_idx": 13}, {"type": "text", "text": "Zhu, B.; Jordan, M.; Jiao, J. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. International Conference on Machine Learning. 2023; pp 43037\u201343067.   \nSchulman, J.; Levine, S.; Abbeel, P.; Jordan, M.; Moritz, P. Trust region policy optimization. International conference on machine learning. 2015; pp 1889\u20131897.   \nPeters, J.; Schaal, S. Reinforcement learning by reward-weighted regression for operational space control. Proceedings of the 24th international conference on Machine learning. 2007; pp 745\u2013750.   \nPeng, X. B.; Kumar, A.; Zhang, G.; Levine, S. Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning. 2019.   \nZhou, Z.; Liu, J.; Yang, C.; Shao, J.; Liu, Y.; Yue, X.; Ouyang, W.; Qiao, Y. Beyond One-PreferenceFits-All Alignment: Multi-Objective Direct Preference Optimization. 2023.   \nJacq, A.; Geist, M.; Paiva, A.; Pietquin, O. Learning from a Learner. Proceedings of the 36th International Conference on Machine Learning. 2019; pp 2990\u20132999.   \nWatson, J.; Huang, S. H.; Heess, N. Coherent Soft Imitation Learning. 2023.   \nAnthropic Introducing the next generation of Claude. 2024; https://www.anthropic.com/news/ claude-3-family.   \nNakano, R. et al. WebGPT: Browser-assisted question-answering with human feedback. 2022.   \nLee, K.; Liu, H.; Ryu, M.; Watkins, O.; Du, Y.; Boutilier, C.; Abbeel, P.; Ghavamzadeh, M.; Gu, S. S. Aligning Text-to-Image Models using Human Feedback. 2023.   \nAzar, M. G.; Rowland, M.; Piot, B.; Guo, D.; Calandriello, D.; Valko, M.; Munos, R. A General Theoretical Paradigm to Understand Learning from Human Preferences. 2023.   \nEthayarajh, K.; Xu, W.; Kiela, D. Better, Cheaper, Faster LLM Alignment with KTO. 2023; https: //contextual.ai/better-cheaper-faster-llm-alignment-with-kto/.   \nLambert, N.; Pyatkin, V.; Morrison, J.; Miranda, L.; Lin, B. Y.; Chandu, K.; Dziri, N.; Kumar, S.; Zick, T.; Choi, Y.; Smith, N. A.; Hajishirzi, H. RewardBench: Evaluating Reward Models for Language Modeling. 2024.   \nTajwar, F.; Singh, A.; Sharma, A.; Rafailov, R.; Schneider, J.; Xie, T.; Ermon, S.; Finn, C.; Kumar, A. Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data. 2024.   \nRoss, S.; Gordon, G.; Bagnell, D. A reduction of imitation learning and structured prediction to no-regret online learning. Proceedings of the fourteenth international conference on artificial intelligence and statistics. 2011; pp 627\u2013635.   \nSwamy, G.; Choudhury, S.; Bagnell, J. A.; Wu, S. Of moments and matching: A game-theoretic framework for closing the imitation gap. International Conference on Machine Learning. 2021; pp 10022\u201310032.   \nXiong, W.; Dong, H.; Ye, C.; Wang, Z.; Zhong, H.; Ji, H.; Jiang, N.; Zhang, T. Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. 2024.   \nMao, X.; Li, F.-L.; Xu, H.; Zhang, W.; Luu, A. T. Don\u2019t Forget Your Reward Values: Language Model Alignment via Value-based Calibration. arXiv preprint arXiv:2402.16030 2024,   \nMunos, R. Error bounds for approximate policy iteration. ICML. 2003; pp 560\u2013567.   \nWu, Y.; Sun, Z.; Yuan, H.; Ji, K.; Yang, Y.; Gu, Q. Self-Play Preference Optimization for Language Model Alignment. arXiv preprint arXiv:2405.00675 2024,   \nBall, P. J.; Smith, L.; Kostrikov, I.; Levine, S. Efficient Online Reinforcement Learning with Offilne Data. 2023.   \nZhou, Y.; Sekhari, A.; Song, Y.; Sun, W. Offline data enhanced on-policy policy gradient with ", "page_idx": 14}, {"type": "text", "text": "provable guarantees. arXiv preprint arXiv:2311.08384 2023, ", "page_idx": 14}, {"type": "text", "text": "May, K. O. Intransitivity, utility, and the aggregation of preference patterns. Econometrica: Journal of the Econometric Society 1954, 1\u201313. ", "page_idx": 15}, {"type": "text", "text": "Tversky, A. Intransitivity of preferences. Psychological review 1969, 76, 31. ", "page_idx": 15}, {"type": "text", "text": "Gardner, M. Mathematical games. 1970; https://www.scientificamerican.com/article/ mathematical-games-1970-12/.   \nDud\u00edk, M.; Hofmann, K.; Schapire, R. E.; Slivkins, A.; Zoghi, M. Contextual dueling bandits. Conference on Learning Theory. 2015; pp 563\u2013587.   \nYe, C.; Xiong, W.; Zhang, Y.; Jiang, N.; Zhang, T. A theoretical analysis of nash learning from human feedback under general kl-regularized preference. arXiv preprint arXiv:2402.07314 2024,   \nKreweras, G. Aggregation of preference orderings. Mathematics and Social Sciences I: Proceedings of the seminars of Menthon-Saint-Bernard, France (1\u201327 July 1960) and of G\u00f6sing, Austria (3\u201327 July 1962). 1965; pp 73\u201379.   \nFishburn, P. C. Probabilistic social choice based on simple voting comparisons. The Review of Economic Studies 1984, 51, 683\u2013692.   \nKramer, G. H. On a Class of Equilibrium Conditions for Majority Rule. Econometrica 1973, 41, 285\u201397.   \nSimpson, P. B. On Defining Areas of Voter Choice: Professor Tullock on Stable Voting. The Quarterly Journal of Economics 1969, 83, 478\u2013490.   \nYue, Y.; Broder, J.; Kleinberg, R.; Joachims, T. The k-armed dueling bandits problem. Journal of Computer and System Sciences 2012, 78, 1538\u20131556.   \nWang, Y.; Liu, Q.; Jin, C. Is RLHF More Difficult than Standard RL? A Theoretical Perspective. Thirty-seventh Conference on Neural Information Processing Systems. 2023.   \nCui, Q.; Du, S. S. When are Offline Two-Player Zero-Sum Markov Games Solvable? Advances in Neural Information Processing Systems 2022, 35, 25779\u201325791.   \nZhong, H.; Xiong, W.; Tan, J.; Wang, L.; Zhang, T.; Wang, Z.; Yang, Z. Pessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets. International Conference on Machine Learning. 2022; pp 27117\u201327142.   \nCui, Q.; Du, S. S. Provably Efficient Offilne Multi-agent Reinforcement Learning via Strategy-wise Bonus. Advances in Neural Information Processing Systems. 2022; pp 11739\u201311751.   \nXiong, W.; Zhong, H.; Shi, C.; Shen, C.; Wang, L.; Zhang, T. Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game. The Eleventh International Conference on Learning Representations. 2023.   \nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; Chen, W. LoRA: Low-Rank Adaptation of Large Language Models. International Conference on Learning Representations. 2022.   \nHuang, S.; Noukhovitch, M.; Hosseini, A.; Rasul, K.; Wang, W.; Tunstall, L. The $\\mathrm{N}+$ Implementation Details of RLHF with PPO: A Case Study on TL; DR Summarization. arXiv preprint arXiv:2403.17031 2024, ", "page_idx": 15}, {"type": "text", "text": "A Detailed Discussion of Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Policy Gradients. Policy gradient (PG) methods (Nemirovskij and Yudin, 1983; Williams, 1992; Konda and Tsitsiklis, 1999; Kakade, 2001; Schulman et al., 2017) are a prominent class of RL algorithms due to their direct, gradient-based policy optimization, robustness to model misspecification (Agarwal et al., 2020), and scalability to modern AI applications from fine-tuning LLMs (Stiennon et al., 2022) to optimizing text-to-image generators (Oertell et al., 2024). ", "page_idx": 16}, {"type": "text", "text": "Broadly speaking, we can taxonomize PG methods into two families. The first family is based on REINFORCE (Williams, 1992) and often includes variance reduction techniques (Kool et al., 2019; Richter et al., 2020; Zhu et al., 2023). While prior work by Ahmadian et al. (2024) has shown that REINFORCE-based approaches can outperform more complex RL algorithms like PPO on LLM fine-tuning tasks like $T L;D R$ , we find that a properly optimized version of PPO still out-performs a REINFORCE baseline. The second family is adaptive PG techniques that precondition the policy gradient (usually with the inverse of the Fisher Information Matrix) to ensure it is covariant to re-parameterizations of the policy, which include NPG (Kakade, 2001; Bagnell and Schneider, 2003) and its practical approximations like TRPO (Schulman et al., 2015) and PPO (Schulman et al., 2017). Intuitively, the preconditioning ensures that we make small changes in terms of action distributions, rather than in terms of the actual policy parameters, leading to faster and more stable convergence. Unfortunately, computing and then inverting the Fisher Information Matrix is computationally intensive and therefore we often resort to approximations in practice, as done in TRPO. However, these approximations are still difficult to apply to large-scale generative models, necessitating even coarser approximations like PPO. In contrast, REBEL does not need any such approximations to be implemented at scale, giving us a much closer connection between theory and practice. ", "page_idx": 16}, {"type": "text", "text": "Reward Regression. The heart of REBEL is a novel reduction from RL to iterative squared loss regression. While using regression to fit either the reward (Peters and Schaal, 2007) or the value (Peng et al., 2019) targets which are then used to extract a policy have previously been explored, our method instead takes a page from DPO (Rafailov et al., 2023; Zhou et al., 2023) and inverse RL methods (Jacq et al., 2019; Watson et al., 2023) to implicitly parameterize the reward regressor in terms of the policy. This collapses the two-stage procedure of prior methods into a single step. ", "page_idx": 16}, {"type": "text", "text": "Preference Fine-Tuning (PFT) of Generative Models. RL has attracted renewed interest due to its central role in \u201caligning\u201d language models \u2013 i.e., adapting their distribution of prompt completions towards the set of responses preferred by human raters. ", "page_idx": 16}, {"type": "text", "text": "One family of techniques for PFT, often referred to as Reinforcement Learning from Human Feedback (RLHF) involves first ftiting a reward model (i.e. a classifier) to the human preference data and then using this model to provide reward values to a downstream RL algorithm (often PPO) (Christiano et al., 2017; Ziegler et al., 2020). LLMs fine-tuned by this procedure include GPT-N (OpenAI, 2023), Claude-N (Anthropic, 2024), and Llama-N (Meta, 2024). Similar approaches have proved beneficial for tasks like summarization (Stiennon et al., 2022), question answering (Nakano et al., 2022), text-to-image generation (Lee et al., 2023), and instruction following (Ouyang et al., 2022). ", "page_idx": 16}, {"type": "text", "text": "Another family of techniques for PFT essentially treats the problem as supervised learning and uses a variety of ranking loss functions. It includes DPO (Rafailov et al., 2023), IPO (Azar et al., 2023), and KTO (Ethayarajh et al., 2023). These techniques are simpler to implement as they remove components like an explicit reward model, value network, and on-policy training from the standard RLHF setup. However, recent work finds their performance to be lesser than that of on-policy methods (Lambert et al., 2024; Tajwar et al., 2024), which agrees with our findings. This is perhaps caused by their lack of interaction during training, leading to the well-known covariate shift/compounding error issue (Ross et al., 2011; Swamy et al., 2021) and the associated lower levels of performance. ", "page_idx": 16}, {"type": "text", "text": "The third family of PFT techniques combines elements from the previous two: it involves running an offilne algorithm iteratively, collecting on-policy preference feedback from either a supervisor model such as GPT4 (Rosset et al., 2024; Xiong et al., 2024; Guo et al., 2024) or from a preference model fti on human data (Calandriello et al., 2024). All of these approaches can be considered instantiations of the general SPO reduction proposed by Swamy et al. (2024), which itself can be thought of as a preference-based variant of DAgger (Ross et al., 2011). Recent work by Tajwar et al. (2024) confirms the empirical strength of these techniques which leverage additional online data. Our approach fits best into this family of techniques \u2013 we also iteratively update our model by solving a sequence of supervised learning problems over on-policy datasets. However, REBEL comes with several key differentiating factors from the prior work. Online versions of DPO or IPO (Xiong et al., 2024; Tajwar et al., 2024; Guo et al., 2024; Calandriello et al., 2024; Munos et al., 2023) essentially use a reward / preference model to generate binary win-loss labels while REBEL actually uses the output of the reward model as a regression target, taking advantage of this more nuanced feedback. In contrast to Rosset et al. (2024), algorithmically, REBEL does not use any online preference feedback from GPT4 nor does it require to generate a large number of responses per prompt, both of which are extremely expensive as reported by Rosset et al. (2024). Theoretically, we are able to prove policy performance bounds under a much weaker coverage condition. Unlike Mao et al. (2024) that regularize to the initial policy $\\pi_{0}$ during updates, we perform conservative updates by regularizing $\\pi_{t+1}$ to $\\pi_{t}$ . When doing the former, it is difficult to prove convergence or monotonic improvement as the current policy can just bounce around a ball centered at $\\pi_{0}$ , a well-known issue in the theory of approximate policy iteration (Kakade and Langford, 2002; Munos, 2003). In contrast, by incorporating the prior policy\u2019s probabilities into our regression problem, we are able to prove stronger guarantees for REBEL. When applying REBEL to the general preference setting, our algorithm shares some similarities with the concurrent work of Wu et al. (2024), which also leverages the DPO reparameterization trick to cleanly implement an Online Mirror Descent approach to computing a minimax winner via self-play. The key difference is that REBEL uses paired responses, while the algorithm of Wu et al. (2024) does not. Using paired responses, REBEL is able to cancel out the partition function and therefore does not need to resort to heuristic approximations of it (specifically, assuming the partition function is always equal to a constant). Furthermore, we can run REBEL with datasets consisting of a mixture of on-policy and off-policy data with strong guarantees, enabling hybrid training, as previously explored in the RL (Song et al., 2023; Ball et al., 2023; Zhou et al., 2023) and inverse RL (Ren et al., 2024) literature. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "B Proof of Claim $^{1}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We prove claim 1 in this section. We start from deriving the Fisher information matrix. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F_{t}:=\\displaystyle\\frac{1}{\\eta^{2}}\\mathbb{E}_{x,y\\sim\\pi_{t},y^{\\prime}\\sim\\mu}\\left(\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)-\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y^{\\prime}|x)\\right)\\left(\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)-\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y^{\\prime}|x)\\right)^{\\top}}}\\\\ {{\\mathrm{~}\\quad=\\displaystyle\\frac{2}{\\eta^{2}}\\mathbb{E}_{x,y\\sim\\pi_{m i x}}\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)^{\\top}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last equality uses the fact that cross terms from completing the square are zero. Now recall Eq. 9 which is an ordinarly least square regression problem. The minimum norm solution of the least square regression problem is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta=(\\eta/2)\\tilde{F}_{t}^{\\{}}\\left(\\mathbb{E}_{x,y\\sim\\pi_{t},y^{\\prime}\\sim\\mu}\\left(\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)-\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y^{\\prime}|x)\\right)(r(x,y)-r(x,y^{\\prime}))\\right)}\\\\ &{\\quad=(\\eta/2)\\tilde{F}_{t}^{\\{}}\\Big(\\mathbb{E}_{x,y\\sim\\pi_{t}}\\left[\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)r(x,y)\\right]+\\mathbb{E}_{x,y\\sim\\mu}\\left[\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y^{\\prime}|x)r(x,y^{\\prime})\\right]}\\\\ &{\\quad\\quad\\quad-\\mathbb{E}_{x,y\\sim\\pi_{t},y^{\\prime}\\sim\\mu}\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y^{\\prime}|x)r(x,y)\\Big)}\\\\ &{\\quad=(\\eta/2)\\tilde{F}_{t}^{\\{}}\\Big(\\mathbb{E}_{x,y\\sim\\pi_{t}}\\left[\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)\\big[r(x,y)-\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot|x)}r(x,y^{\\prime})\\right]}\\\\ &{\\quad\\quad\\quad\\quad+\\mathbb{E}_{x,y\\sim\\mu}\\left[\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)\\big[r(x,y)-\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot|x)}r(x,y^{\\prime})\\right]\\Big)}\\\\ &{\\quad=(\\eta)\\tilde{F}_{t}^{\\{}}\\left(\\mathbb{E}_{x,y\\sim\\pi_{t}+\\mu/2}\\left[\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)\\big[A^{\\pi_{t}}(x,y)\\big]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we again use the fact that $\\mathbb{E}_{y\\sim\\pi_{\\theta_{t}}(\\cdot|x)}\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)g(x)=0$ for any function $g(x)$ , and we define Advantage $A^{\\pi}(x,y):=r(x,y)\\overset{\\cdot}{-}\\mathbb{E}_{y^{\\prime}\\sim\\pi(\\cdot|x)}r(x,y^{\\prime})$ . ", "page_idx": 18}, {"type": "text", "text": "C Proof of Claim 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We prove claim 2 in this section. We start by approximating our predictor $\\begin{array}{r}{\\frac{1}{\\eta}\\ln\\pi_{\\theta}(y|x)/\\pi_{\\theta_{t}}(y|x)}\\end{array}$ by its first order Taylor expansion at $\\begin{array}{r}{\\theta_{t}\\colon\\frac{1}{\\eta}\\left(\\ln\\pi_{\\theta}(y|x)-\\ln\\pi_{\\theta_{t}}(y|x)\\right)\\approx\\frac{1}{\\eta}\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)^{\\top}(\\theta-\\theta_{t}).}\\end{array}$ where $\\approx$ indicates that we ignore higher order terms in the expansion. Setting $\\delta:=\\theta-\\theta_{t}$ and replace $\\begin{array}{r}{\\frac{1}{\\eta}\\left(\\ln\\pi_{\\theta}(y|x)-\\ln\\pi_{\\theta_{t}}(y|x)\\right)}\\end{array}$ by its first order approximation in Eq. 1, we arrive at: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\delta}\\sum_{\\substack{(x,y,y^{\\prime})\\in\\mathcal{D}_{t}}}\\left(\\frac{1}{\\eta}\\left(\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y|x)-\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y^{\\prime}|x)\\right)^{\\top}\\delta-\\left(r(x,y)-r(x,y^{\\prime})\\right)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "under finite setting. ", "page_idx": 19}, {"type": "text", "text": "Following the previous derivation, we have the unbiased estimate of Fisher information matrix under the finite setting as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{F}_{t}:=\\frac{2}{\\eta^{2}N}\\sum_{x_{n},y_{n}\\sim\\pi_{m i x}}\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y_{n}|x_{n})\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y_{n}|x_{n})^{\\top}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since Eq. 11 is an ordinarly least square regression problem. The minimum norm solution of the least square regression problem is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf\\ddot{\\rho}}=(\\eta/2)\\tilde{F}_{t}^{\\dagger}\\frac{1}{N}\\sum_{n}\\left(\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y_{n}|x_{n})-\\nabla_{\\theta}\\ln\\pi_{\\theta_{t}}(y_{n}^{\\prime}|x_{n})\\right)(r(x_{n},y_{n})-r(x_{n},y_{n}^{\\prime}))}\\ ~}\\\\ {{\\displaystyle=\\eta\\tilde{F}_{t}^{\\dagger}\\frac{1}{2N}\\sum_{n}\\left(\\nabla\\ln\\pi_{\\theta_{t}}(y_{n}|x_{n})(r(x_{n},y_{n})-r(x_{n},y_{n}^{\\prime}))+\\nabla\\ln\\pi_{\\theta_{t}}(y_{n}^{\\prime}|x_{n})(r(x_{n},y_{n}^{\\prime})-r(x_{n},y_{n})\\right)}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "D Extending REBEL to General Preferences ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the above discussion, we assume we are given access to a ground-truth reward function. However, in the generative model fine-tuning applications of RL, we often need to learn from human preferences, rather than rewards. This shift introduces a complication: not all preferences can be rationalized by an underlying utility function. In particular, intransitive preferences which are well-known to result from aggregation of different sub-populations or users evaluating different pairs of items on the basis of different features (May, 1954; Tversky, 1969; Gardner, 1970) cannot be accurately captured by a single reward model. To see this, note that if we have $a\\succ b,b\\succ c,$ , and $c\\succ a$ , it is impossible to have a reward model that simultaneously sets $\\hat{r}(a)>\\hat{r}(b),\\hat{r}(b)>\\hat{r}(c).$ , and $\\hat{r}(c)>\\hat{r}(a)$ . As we increase the space of possible choices to that of all possible prompt completions, the probability of such intransitivities sharply increases (Dud\u00edk et al., 2015), as reflected in the high levels of annotator disagreement in LLM fine-tuning datasets (Touvron et al., 2023). Thus, rather than assuming access to a reward model, in such settings, we assume access to a preference model (Munos et al., 2023; Swamy et al., 2024; Rosset et al., 2024; Ye et al., 2024). ", "page_idx": 20}, {"type": "text", "text": "D.1 A Game-Theoretic Perspective on Learning from Preferences ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "More specifically, for any tuple $(x,y,y^{\\prime})$ , we assume we have access to $\\mathcal{P}(y\\succ y^{\\prime}|x)$ : the probability that $y$ is preferred to $y^{\\prime}$ . We then define our preference model $l$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\nl(x,y,y^{\\prime})\\triangleq2\\cdot\\mathcal{P}(y\\succ y^{\\prime}|x)-1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Observe that $l(x,y,y^{\\prime})\\in[-1,1]$ is skew-symmetric, i.e., $l(x,y,y)=0$ $\\mathbf{\\Pi}_{|}=0,\\mathcal{l}(x,y,y^{\\prime})+l(x,y^{\\prime},y)=0$ for all $x\\in{\\mathcal{X}},y,y^{\\prime}\\in{\\mathcal{Y}}$ . If the learner can only receive a binary feedback $o\\in\\{0,1\\}$ indicating the preference between $y$ and $y^{\\prime}$ , we assume $o$ is sampled from a Bernoulli distribution with mean $\\bar{\\mathcal{P}}(\\bar{y^{}}\\succ y^{\\prime}|x)$ , where $o=1$ means that $y$ is preferred over $y^{\\prime}$ and 0 otherwise. ", "page_idx": 20}, {"type": "text", "text": "Given access to such a preference model, a solution concept to the preference aggregation problem with deep roots in the social choice theory literature (Kreweras, 1965; Fishburn, 1984; Kramer, 1973; Simpson, 1969) and the dueling bandit literature (Yue et al., 2012; Dud\u00edk et al., 2015) is that of a minimax winner (MW) $\\pi_{\\mathsf{M W}}$ : the Nash Equilibrium strategy of the symmetric two-player zero-sum game with $l$ as a payoff function. In particular, due to the skew-symmetric property of $l$ , Swamy et al. (2024) proved that there exists a policy $\\pi_{\\mathsf{M W}}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi(\\cdot|x),y^{\\prime}\\sim\\pi_{\\mathtt{M W}}(\\cdot|x)}\\left[l(x,y,y^{\\prime})\\right]=\\operatorname*{min}_{\\pi}\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi_{\\mathtt{M W}}(\\cdot|x),y^{\\prime}\\sim\\pi(\\cdot|x)}\\left[l(x,y,y^{\\prime})\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This implies that $(\\pi_{\\mathsf{M W}},\\pi_{\\mathsf{M W}})$ is a Nash Equilibrium (Wang et al., 2023; Munos et al., 2023; Swamy et al., 2024; Ye et al., 2024). As is standard in game solving, our objective is to obtain an $\\epsilon$ -approximate MW $\\widehat{\\pi}$ measured by the duality gap (DG): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{D G}(\\widehat{\\pi}):=\\operatorname*{max}_{\\pi}\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi(\\cdot|x),y^{\\prime}\\sim\\widehat{\\pi}(\\cdot|x)}\\left[l(x,y,y^{\\prime})\\right]-\\operatorname*{min}_{\\pi}\\mathbb{E}_{x\\sim\\rho,y\\sim\\widehat{\\pi}(\\cdot|x),y^{\\prime}\\sim\\pi(\\cdot|x)}\\left[l(x,y,y^{\\prime})\\right]\\le\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the following discussion, we will use $l(x,y,\\pi)$ to denote $\\mathbb{E}_{y^{\\prime}\\sim\\pi(\\cdot|x)}[l(x,y,y^{\\prime})]$ and $l(\\pi,\\pi^{\\prime})$ to denote $\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi(\\cdot|x),y^{\\prime}\\sim\\pi^{\\prime}(\\cdot|x)}[l(x,y,y^{\\prime})]$ for notational convenience. ", "page_idx": 20}, {"type": "text", "text": "D.2 Self-Play Preference Optimization (SPO) with REBEL as Base Learner ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We can straightforwardly extend REBEL to the general preference setting via an instantiation of the Self-Play Preference Optimization (SPO) reduction of Swamy et al. (2024). In short, Swamy et al. (2024) prove that rather than performing adversarial training, we are able to perform a simple and stable self-play procedure while retaining strong theoretical guarantees. Practically, this corresponds to sampling at leas two completions from the current policy, querying a learned preference / supervisor model on each pair, and using the winrate for each completion as its reward. We will now describe how we can adapt REBEL to this mode of feedback. ", "page_idx": 20}, {"type": "text", "text": "Assuming that we can query the preference oracle $l(x,y,y^{\\prime})$ at will, we can modify the least square objective Eq. (1) to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta_{t+1}:=\\operatorname*{argmin}_{\\theta}\\sum_{x,y,y^{\\prime},y^{\\prime\\prime}\\in\\mathcal{D}_{t}}\\left(\\frac{1}{\\eta}\\left(\\ln\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta_{t}}(y|x)}-\\ln\\frac{\\pi_{\\theta}(y^{\\prime}|x)}{\\pi_{\\theta_{t}}(y^{\\prime}|x)}\\right)-(l(x,y,y^{\\prime\\prime})-l(x,y^{\\prime},y^{\\prime\\prime}))\\right)^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $x\\,\\sim\\,\\rho,y\\,\\sim\\,\\pi_{t}(\\cdot|x),y^{\\prime\\prime}\\,\\sim\\,\\pi_{t}(\\cdot|x),y^{\\prime}\\,\\sim\\,\\mu(\\cdot|x)$ . When the exact value of $l(x,y,y^{\\prime})$ is unavailable but only a binary preference feedback $o_{y,y^{\\prime}}\\in\\dot{\\{0,1\\}}$ sampling from Bernoulli with mean $l(x,y,y^{\\prime})$ is available, we can just replace $l(x,y,\\bar{y^{\\prime\\prime}})-l(x,y^{\\prime},y^{\\prime\\prime})$ by $o_{y,y^{\\prime}}-o_{y^{\\prime},y^{\\prime\\prime}}$ . It is easy to see that the Bayes optimal of the above least square regression problem is equal to: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{y^{\\prime\\prime}\\sim\\pi_{t}(\\cdot|x)}l(x,y,y^{\\prime\\prime})-\\mathbb{E}_{y^{\\prime\\prime}\\sim\\pi_{t}(\\cdot|x)}l(x,y^{\\prime},y^{\\prime\\prime})=l(x,y,\\pi_{t})-l(x,y^{\\prime},\\pi_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Swamy et al. (2024) define an iteration-dependent reward $r_{t}(x,y)\\,:=\\,\\mathbb{E}_{y^{\\prime\\prime}\\sim\\pi_{t}(\\cdot|x)}l(x,y,y^{\\prime\\prime})\\,=$ $l(x,y,\\pi_{t})$ . Thus, the above regression problem can be understood as an extension of REBEL to the setting where the reward function changes at each iteration $t$ . Swamy et al. (2024) shows that running the exact MD (Eq. 3) with this iteration-dependent reward function $r_{t}$ leads to fast convergence to an approximate Minimax Winner, a property that we will use to provide the regret bound of REBEL in the general preference setting while accounting for nonzero mean squared error. ", "page_idx": 21}, {"type": "text", "text": "E Justification for Assumption $^{1}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Intuitively, this assumption is saying that there is a function in our class of regressors that is able to accurately fti the difference of rewards. Recall that our class of regressors is isomorphic to our policy class. Therefore, as long as our class of policies is expressive, we would expect this assumption to hold with small \u03f5. For all domains we consider, our policy class is a flexible set of generative models (e.g. Transformer-based LLMs or diffusion models). Thus, we believe it is reasonable to believe this assumption holds in practice \u2013 see Figure 6 in Appendix $\\mathrm{L}$ for empirical evidence of this point and Example 1 for more discussion. ", "page_idx": 22}, {"type": "text", "text": "More formally, the above assumption bounds the standard in-distribution generalization error (v.s. the point-wise guarantee in Eq. 8) of a well-defined supervised learning problem: least squares regression. The generalization error $\\epsilon$ captures the possible errors from the learning process for $\\theta_{t+1}$ and it could depend on the complexity of the policy class and the number of samples used in the dataset $\\mathcal{D}_{t}$ . For instance, when the the function $\\ln\\pi\\mathrm{~-~}\\!\\ln\\pi^{\\prime}$ induced by the log-difference of two policies $(\\pi,\\pi^{\\prime})$ are rich enough (e.g., policies are deep neural networks) to capture the reward difference, then $\\epsilon$ in this assumption converges to zero as we increase the number of training data. Note that while $\\epsilon$ can be small, it does not imply that the learned predictor will have a small prediction error in a point-wise manner \u2013 it almost certainly will not. ", "page_idx": 22}, {"type": "text", "text": "Example 1. One simple example is when $\\pi(y|x)\\,\\propto\\,\\exp(\\theta^{\\top}\\phi(x,y))$ for some features $\\phi(x,y)$ . In this case, $\\ln(\\pi(y|x)/\\pi_{t}(y|x))-\\ln(\\pi(y^{\\prime}|x)/\\pi_{t}(y^{\\prime}|x))=(\\theta-\\theta_{t})^{\\top}\\left(\\phi(x,y)-\\phi(x,y^{\\prime})\\right)$ , which means that our regression problem in Eq. $I$ is a classic linear regression problem. When the reward $r(x,y)$ is also linear in feature $\\phi(x,y)$ , then Eq. $I$ is a well-specified linear regression problem, and $\\epsilon$ typically scales in the rate of $O\\left(d/|\\mathcal{D}_{t}|\\right)$ with d being the dimension of feature $\\phi$ . ", "page_idx": 22}, {"type": "text", "text": "We can extend the above example to the case where $\\phi$ is the feature corresponding to some kernel, e.g., RBF kernel or even Neural Tangent Kernel, which allows us to capture the case where $\\pi$ is $a$ softmax wide neural network with the least square regression problem solved by gradient flow. The error \u03f5 again scales poly $\\mathop{}\\!\\left(d/|\\mathcal{D}_{t}|\\right)$ , where $d$ is the effective dimension of the corresponding kernel. ", "page_idx": 22}, {"type": "text", "text": "F Proof of Theorem $^{1}$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide the proof of theorem 1. For notation simplicity, throughout the proof, we denote \u03c0t for \u03c0\u03b8t, and define ft(x, y) := \u03b71 ln \u03c0t\u03c0+t(1y(|yx|)x) . ", "page_idx": 23}, {"type": "text", "text": "The following lemma shows that the learned function $f_{t}$ can predict reward $r$ well under both $\\pi_{t}$ and $\\mu$ up to terms that are $y$ -independent. ", "page_idx": 23}, {"type": "text", "text": "Lemma 1. Consider any $t~\\in~[T]$ . Define $\\Delta(x,y)~=~f_{t}(x,y)\\,-\\,r(x,y)$ . Define $\\Delta_{\\pi_{t}}(x)\\;=\\;$ $\\mathbb{E}_{y\\sim\\pi_{t}(\\cdot|x)}\\Delta(x,y)$ and $\\Delta_{\\mu}(x)\\,=\\,\\mathbb{E}_{y\\sim\\mu(\\cdot|x)}\\Delta(x,y)$ . Under assumption $I$ , for all $t$ , we have the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x,y\\sim\\pi_{t}(\\cdot|x)}\\left(f_{t}(x,y)-r(x,y)-\\Delta_{\\pi_{t}}(x)\\right)^{2}\\le\\epsilon,}\\\\ &{\\mathbb{E}_{x,y\\sim\\mu(\\cdot|x)}\\left(f_{t}(x,y)-r(x,y)-\\Delta_{\\mu}(x)\\right)^{2}\\le\\epsilon,}\\\\ &{\\mathbb{E}_{x}\\left(\\Delta_{\\pi_{t}}(x)-\\Delta_{\\mu}(x)\\right)^{2}\\le\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. From assumption 1, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{x,y_{1}\\sim\\pi_{t},y_{2}\\sim\\mu}\\left(f_{t}(x,y_{1})-\\Delta_{\\pi_{t}}(x)-r(x,y_{1})-(f_{t}(x,y_{2})-\\Delta_{\\mu}(x)-r(x,y_{2}))+\\Delta_{\\pi_{t}}(x)-\\Delta_{\\mu}(x)\\right.}\\\\ &{=\\mathbb{E}_{x,y_{1}\\sim\\pi_{t}}\\left(f_{t}(x,y_{1})-\\Delta_{\\pi_{t}}(x)-r(x,y_{1})\\right)^{2}+\\mathbb{E}_{x,y_{2}\\sim\\mu}\\left(f_{t}(x,y_{2})-\\Delta_{\\mu}(x)-r(x,y_{2})\\right)^{2}}\\\\ &{\\left.-\\left.2\\mathbb{E}_{x,y_{1}\\sim\\pi_{t},y_{2}\\sim\\mu}\\left(f_{t}(x,y_{1})-\\Delta_{\\pi_{t}}(x)-r(x,y_{1})\\right)\\left(f_{t}(x,y_{2})-\\Delta_{\\mu}(x)-r(x,y_{2})\\right)\\right.}\\\\ &{+\\left.2\\mathbb{E}_{x,y_{1}\\sim\\pi_{t}}\\left(f_{t}(x,y_{1})-\\Delta_{\\pi_{t}}(x)-r(x,y_{1})\\right)\\left(\\Delta_{\\pi_{t}}(x)-\\Delta_{\\mu}(x)\\right)}\\\\ &{-\\left.2\\mathbb{E}_{x,y_{2}\\sim\\pi_{t}}\\left(f_{t}(x,y_{2})-\\Delta_{\\mu}(x)-r(x,y_{2})\\right)\\left(\\Delta_{\\pi_{t}}(x)-\\Delta_{\\mu}(x)\\right)+\\mathbb{E}_{x}(\\Delta_{1}(x)-\\Delta_{2}(x))^{2}}\\\\ &{=\\mathbb{E}_{x,y_{1}\\sim\\pi_{t}}\\left(f_{t}(x,y_{1})-\\Delta_{\\pi_{t}}(x)-r(x,y_{1})\\right)^{2}+\\mathbb{E}_{x,y_{2}\\sim\\mu}\\left(f_{t}(x,y_{2})-\\Delta_{\\mu}(x)-r(x,y_{2})\\right)^{2}}\\\\ &{\\ \\ \\ \\ +\\mathbb{E}_{x}\\left(\\Delta_{\\pi_{t}}(x)-\\Delta_{\\mu}(x)\\right)^{2}\\le\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the above, we first complete the square, and then we only keep terms that are not necessarily zero. Since all the remaining three terms are non-negative, this concludes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "By the definition of ft, we have \u2206(x, y) = \u03b71 ln \u03c0t\u03c0+t(1y(|yx|)x) \u2212 . Taking exp on both sides, we get: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\langle x,y:\\,\\pi_{t+1}(y|x)=\\pi_{t}(y|x)\\exp\\left(\\eta(r(x,y)+\\Delta(x,y))\\right)=\\frac{\\pi_{t}(y|x)\\exp(\\eta(r(x,y)+\\Delta(x,y)-\\Delta_{\\mu}(x,y)))}{\\exp(-\\eta\\Delta_{\\mu}(x))},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Denote $g_{t}(x,y)\\;:=\\;r(x,y)\\,+\\,\\Delta(x,y)\\,-\\,\\Delta_{\\mu}(x)$ , and the advantage $A_{t}(x,y)\\;=\\;g_{t}(x,y)\\;-$ $\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot|x)}{g}_{t}(x,y^{\\prime})$ . We can rewrite the above update rule as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall x,y:\\;\\pi_{t+1}(y|x)\\propto\\pi_{t}(y|x)\\exp(\\eta A_{t}(x,y))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In other words, the algorithm can be understood as running MD on the sequence of $A_{t}$ for $t=0$ to $T-1$ . The following lemma is the standard MD regret lemma. ", "page_idx": 23}, {"type": "text", "text": "Lemma 2. Assume $\\begin{array}{r}{\\operatorname*{max}_{x,y,t}|A_{t}(x,y)|\\,\\le\\,A\\,\\in\\,\\mathbb{R}^{+}}\\end{array}$ , and $\\pi_{0}(\\cdot|x)$ is uniform over $\\boldsymbol{\\wp}$ . Then with $\\eta=\\sqrt{\\ln(|\\mathcal{V}|)/(A^{2}T)},$ , for the sequence of policies computed by REBEL, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall\\pi,x:\\;\\sum_{t=0}^{T-1}\\mathbb{E}_{y\\sim\\pi(\\cdot|x)}A_{t}(x,y)\\leq2A\\sqrt{\\ln(|\\mathcal{V}|)T}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For completeness, we provide the proof here. Start with $\\begin{array}{r l}{\\pi_{t+1}(y|x)}&{{}=}\\end{array}$ $\\pi_{t}(y|x)\\exp(\\eta A_{t}(\\bar{x},y))/Z_{t}(x)$ where $Z_{t}(x)$ is the normalization constant, taking log on both sides, and add $\\mathbb{E}_{y\\sim\\pi(\\cdot|x)}$ , we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\mathrm{KL}(\\pi(\\cdot|x)||\\pi_{t+1}(\\cdot|x))=-\\mathrm{KL}(\\pi(\\cdot|x)||\\pi_{t}(\\cdot|x))+\\eta\\mathbb{E}_{y\\sim\\pi(\\cdot|x)}A_{t}(x,y)-\\mathbb{E}_{y\\sim\\pi(\\cdot|x)}\\ln Z_{t}(x).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Rearrange terms, we get: ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\mathrm{KL}(\\pi(\\cdot|x)||\\pi_{t}(\\cdot|x))+\\mathrm{KL}(\\pi(\\cdot|x)||\\pi_{t+1}(\\cdot|x)=\\mathbb{E}_{y\\sim\\pi(\\cdot|x)}\\left[-\\eta A_{t}(x,y)+\\ln Z_{t}(x)\\right])\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $\\ln Z_{t}(x)$ , using the condition that $\\eta\\leq1/A$ , we have $\\eta A_{t}(x,y)\\leq1$ , which allows us to use the inequality $\\exp(x)\\stackrel{\\cdot}{\\leq}1+x+x^{2}$ for any $x\\leq1$ , which lead to the following inequality: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ln Z_{t}(x)=\\ln\\left(\\mathbb{E}_{y\\sim\\pi(\\cdot\\vert x)}\\exp(\\eta A_{t}(x,y))\\right)}\\\\ &{\\qquad\\qquad\\leq\\ln\\left(\\displaystyle\\sum_{y}\\pi_{t}(y\\vert x)\\left(1+\\eta A_{t}(x,y)+\\eta^{2}A_{t}(x,y)^{2}\\right)\\right)}\\\\ &{\\qquad\\qquad\\leq\\ln\\left(1+0+\\eta^{2}A^{2}\\right)\\leq\\eta^{2}A^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality uses $\\ln(1+x)\\leq x$ , and we used the fact that $\\mathbb{E}_{y\\sim\\pi_{t}(x)}A_{t}(x,y)=0$ due to the definition of advantage $A_{t}$ . Thus, we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n-\\mathrm{KL}(\\pi(\\cdot|x)||\\pi_{t}(\\cdot|x))+\\mathrm{KL}(\\pi(\\cdot|x)||\\pi_{t+1}(\\cdot|x)\\leq-\\mathbb{E}_{y\\sim\\pi(\\cdot|x)}[A_{t}(x,y)]+\\eta^{2}A^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Sum over all iterations and do the telescoping sum, we get: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\mathbb{E}_{y\\sim\\pi(\\cdot|x)}A_{t}(x,y)\\leq\\mathrm{KL}(\\pi(\\cdot|x)||\\pi_{0}(\\cdot|x))/\\eta+T\\eta A^{2}\\leq\\ln(|y|)/\\eta+T\\eta A^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "With $\\eta=\\sqrt{\\ln(|\\mathcal{V}|)/(A^{2}T)}$ , we conclude the proof. ", "page_idx": 24}, {"type": "text", "text": "With the above, now we are ready to conclude the proof of the main theorem. ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 1. Consider a comparator policy $\\pi^{*}$ . We start with the performance difference between $\\pi^{*}$ and the uniform mixture policy $\\bar{\\pi}:=\\dot{\\sum}_{t=0}^{T-1}\\pi_{t}/T$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\left(\\mathbb{E}_{x,y\\sim\\pi^{*}(\\cdot|x)}r(x,y)-\\mathbb{E}_{x,y\\sim\\pi_{t}(\\cdot|x)}r(x,y)\\right)=\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}_{x,y\\sim\\pi^{*}(\\cdot|x)}\\left(A^{\\pi_{t}}(x,y)\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we define the real advantage $A^{\\pi_{t}}(x,y):=r(x,y)-\\mathbb{E}_{y\\sim\\pi_{t}(\\cdot|x)}r(x,y)$ . Continue, we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1T\\sum_{t=0}^{T-1}\\mathbb{E}_{x,y\\sim\\pi^{*}(\\cdot\\vert x)}\\left(A^{\\pi_{t}}(x,y)\\right)}\\\\ &{\\displaystyle=\\frac1T\\sum_{t=0}^{T-1}\\mathbb{E}_{x,y\\sim\\pi^{*}(\\cdot\\vert x)}\\left(A_{t}(x,y)\\right)+\\frac1T\\sum_{t=0}^{T-1}\\mathbb{E}_{x,y\\sim\\pi^{*}(\\cdot\\vert x)}\\left(A^{\\pi_{t}}(x,y)-A_{t}(x,y)\\right)}\\\\ &{\\displaystyle\\le2A\\sqrt{\\frac{\\ln(\\vert y\\vert)}{T}}+\\frac1T\\sum_{t=0}^{T-1}\\sqrt{\\mathbb{E}_{x}\\mathbb{E}_{y\\sim\\pi^{*}(\\cdot\\vert x)}\\big(A^{\\pi_{t}}(x,y)-A_{t}(x,y)\\big)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality uses Lemma 2. We now just need to bound $\\mathbb{E}_{y\\sim\\pi^{*}(\\cdot|x)}(A^{\\pi_{t}}(x,y)\\mathrm{~-~}$ $A_{t}(x,y))^{2}$ . ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x}\\mathbb{E}_{y\\sim\\pi^{*}(\\cdot|x)}(A^{\\pi_{t}}(x,y)-A_{t}(x,y))^{2}=\\mathbb{E}_{x}\\mathbb{E}_{y\\sim\\mu(\\cdot|x)}\\frac{\\pi^{*}(y|x)}{\\mu(y|x)}(A^{\\pi_{t}}(x,y)-A_{t}(x,y))^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq C_{\\pi^{*}}\\mathbb{E}_{x,y\\sim\\mu(\\cdot|x)}(A^{\\pi_{t}}(x,y)-A_{t}(x,y))^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality uses the definition of concentrability coefficient $C_{\\pi^{*}}$ . We now bound $\\mathbb{E}_{x,y\\sim\\mu(\\cdot|x)}(A^{\\pi_{t}}(x,\\stackrel{\\cdot}{y})-\\stackrel{\\cdot}{A}_{t}(x,y))^{2}$ . Recall the definiton of $A_{t}$ from Lemma 2. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x,y\\sim\\mu(\\cdot\\vert x)}(A^{\\pi_{t}}(x,y)-A_{t}(x,y))^{2}}\\\\ &{\\ =\\mathbb{E}_{x,y\\sim\\mu(\\cdot\\vert x)}(r(x,y)-\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot\\vert x)}r(x,y^{\\prime})-g_{t}(x,y)+\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot\\vert x)}g_{t}(x,y^{\\prime}))^{2}}\\\\ &{\\ \\le2\\mathbb{E}_{x,y\\sim\\mu(\\cdot\\vert x)}\\left(r(x,y)-g_{t}(x,y)\\right)^{2}+2\\mathbb{E}_{x}\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot\\vert x)}\\left(r(x,y^{\\prime})-g_{t}(x,y^{\\prime})\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall the $g_{t}(x,y)=r(x,y)+\\Delta(x,y)-\\Delta_{\\mu}(x)$ , and from Lemma 1, we can see that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x,y\\sim\\mu(\\cdot\\vert x)}(r(x,y)-g_{t}(x,y))^{2}=\\mathbb{E}_{x,y\\sim\\mu(\\cdot\\vert x)}(\\Delta(x,y)-\\Delta_{\\mu}(x))^{2}\\leq\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $\\mathbb{E}_{x}\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot|x)}\\left(r(x,y^{\\prime})-g_{t}(x,y^{\\prime})\\right)^{2}$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x}\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot\\vert x)}\\left(r(x,y^{\\prime})-g_{t}(x,y^{\\prime})\\right)^{2}=\\mathbb{E}_{x}\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot\\vert x)}\\left(\\Delta(x,y^{\\prime})-\\Delta_{\\mu}(x)\\right)^{2}}\\\\ &{\\quad=\\mathbb{E}_{x}\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot\\vert x)}\\left(\\Delta(x,y^{\\prime})-\\Delta_{\\pi_{t}}(x)+\\Delta_{\\pi_{t}}(x)-\\Delta_{\\mu}(x)\\right)^{2}}\\\\ &{\\quad\\le2\\mathbb{E}_{x}\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot\\vert x)}\\left(\\Delta(x,y^{\\prime})-\\Delta_{\\pi_{t}}(x)\\right)^{2}+2\\mathbb{E}_{x}\\left(\\Delta_{\\pi_{t}}(x)-\\Delta_{\\mu}(x)\\right)^{2}\\le4\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality uses Lemma 1 again. This step relies on the fact that one of the samples is always on-policy, i.e., from $\\pi_{t}$ . ", "page_idx": 25}, {"type": "text", "text": "Combine things together, we can conclude that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x}\\mathbb{E}_{y\\sim\\pi^{*}(\\cdot|x)}(A^{\\pi_{t}}(x,y)-A_{t}(x,y))^{2}\\le C_{\\pi^{*}}(10\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, for the regret, we can conclude: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}_{x,y\\sim\\pi^{*}(\\cdot|x)}\\left(A^{\\pi_{t}}(x,y)\\right)\\leq2A\\sqrt{\\frac{\\ln|\\mathcal{Y}|}{T}}+\\frac{1}{T}\\sum_{t}\\sqrt{C_{\\pi^{*}}10\\epsilon}=2A\\sqrt{\\frac{\\ln|\\mathcal{Y}|}{T}}+\\sqrt{C_{\\pi^{*}}10\\epsilon}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "G Extension of analysis to General Preferences ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Extending the above analysis to the general preference case is straightforward except that it requires a stronger coverage condition. This is because we want to find a Nash Equilibrium, which requires a comparison between the learned policy against all the other policies. Results from the Markov Game literature (Cui and Du, 2022; Zhong et al., 2022; Cui and Du, 2022; Xiong et al., 2023) and Cui and Du (2022) have shown that the standard single policy coverage condition used in single-player optimization is provably not sufficient. In particular, they propose using a notion of unilateral concentrability for efficient learning, which can be defined as ", "page_idx": 26}, {"type": "equation", "text": "$$\nC_{\\mathsf{u n i},\\mu}:=\\operatorname*{max}_{\\pi,x,y,y^{\\prime\\prime}}\\frac{\\pi_{\\mathsf{M W}}(y|x)\\pi(y^{\\prime\\prime}|x)}{\\mu(y|x)\\mu(y^{\\prime\\prime}|x)},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "in the general preference setting. Notably, the above unilateral concentrability coefficient $C_{\\mathrm{uni,}\\mu}$ is equivalent to C\u00b5 := max\u03c0,x,y\u03c0\u00b5((yy||xx)) since $C_{\\mu}\\,\\,\\leq\\,\\,C_{\\mathrm{uni,}\\mu}\\,\\,\\leq\\,\\,C_{\\mu}^{2}$ . Therefore in the following discussion, we will use $C_{\\mu}$ as the coverage condition. In addition, we also assume the generalization error of the regression problem is small, ", "page_idx": 26}, {"type": "text", "text": "Assumption 2 (Regression generalization bounds for general preference). Over $T$ iterations, assume that for all $t$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{x\\sim\\rho,y\\sim\\pi_{t}(\\cdot\\,|x),y^{\\prime}\\sim\\mu(\\cdot\\,|x)}\\left(\\frac{1}{\\eta}\\left(\\ln\\frac{\\pi_{\\theta_{t+1}}(y|x)}{\\pi_{\\theta_{t}}(y|x)}-\\ln\\frac{\\pi_{\\theta_{t+1}}(y^{\\prime}|x)}{\\pi_{\\theta_{t}}(y^{\\prime}|x)}\\right)-(l(x,y,\\pi_{t})-l(x,y^{\\prime},\\pi_{t}))\\right)^{2}\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for some \u03f5. ", "page_idx": 26}, {"type": "text", "text": "Under the above coverage condition and generalization bound, we can show that REBEL is able to learn an approximate Minimax Winner: ", "page_idx": 26}, {"type": "text", "text": "Theorem 2. With assumption 2, after $T$ many iterations, with a proper learning rate $\\eta,$ , the policy $\\widehat{\\pi}=\\mathsf{U n i f}\\big(\\{\\pi_{t}\\}_{t=1}^{T}\\big)$ satisfies that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathsf{D G}(\\widehat{\\pi})\\leq O\\left(\\sqrt{\\frac{1}{T}}+\\sqrt{C_{\\mu}\\epsilon}.\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here the $O$ -notation hides problem-dependent constants that are independent of $\\epsilon,C_{\\mu},T$ . ", "page_idx": 26}, {"type": "text", "text": "Note that the coverage condition here is much stronger than the single policy coverage condition in the RL setting. We conjecture that this is the cost one has to pay by moving to the more general preference setting and leaving the investigation of the necessarily coverage condition for future work. ", "page_idx": 26}, {"type": "text", "text": "G.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Recall that $r_{t}(x,y)\\;=\\;l(x,y,\\pi_{t})$ . Let us define $\\Delta^{t}(x,y)\\;:=\\;f_{t}(x,y)\\,-\\,r_{t}(x,y)$ , $\\Delta_{\\pi_{t}}^{t}(x)\\;:=\\;$ $\\mathbb{E}_{y\\sim\\pi_{t}(\\cdot|x)}\\Delta^{t}(x,y)$ and $\\Delta_{\\mu}^{t}(x)\\ :=\\ \\mathbb{E}_{y\\sim\\mu(\\cdot|x)}\\Delta^{t}(x,y)$ . Then following the same arguments in Lemma 1, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi_{t}(\\cdot|x)}\\left[\\left(f_{t}(x,y)-r_{t}(x,y)-\\Delta_{\\pi_{t}}^{t}(x)\\right)^{2}\\right]\\le\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x\\sim\\rho,y\\sim\\mu(\\cdot|x)}\\left[\\left(f_{t}(x,y)-r_{t}(x,y)-\\Delta_{\\mu}^{t}(x)\\right)^{2}\\right]\\le\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim\\rho}\\left[\\left(\\Delta_{\\pi_{t}}^{t}(x)-\\Delta_{\\mu}^{t}(x)\\right)^{2}\\right]\\le\\epsilon.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With slight abuse of the notation, We also use $g_{t}$ and $A_{t}(x,y)$ to denote $r_{t}(x,y)\\!+\\!\\Delta^{t}(x,y)\\!-\\!\\Delta_{\\mu}^{t}(x,y)$ and $g_{t}(x,y)-\\mathbb{E}_{y^{\\prime}\\sim\\pi_{t}(\\cdot|x)}g_{t}(x,y^{\\prime})$ . Then following the same arguments in Lemma 2, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\forall\\pi,x:\\;\\sum_{t=0}^{T-1}\\mathbb{E}_{y\\sim\\pi(\\cdot|x)}A_{t}(x,y)\\leq2A\\sqrt{\\ln(|\\mathcal{V}|)T}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{\\pi}l(\\pi,\\widehat{\\pi})=\\operatorname*{max}_{\\pi}\\frac{1}{T}\\sum_{t=1}^{T}l(\\pi,\\pi_{t})}\\\\ {\\displaystyle\\qquad=\\operatorname*{max}_{\\pi}\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi(\\cdot|x)}[r_{t}(x,y)]=\\operatorname*{max}_{\\pi}\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi(\\cdot|x)}[A^{t,\\pi_{t}}(x,y)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $A^{t,\\pi_{t}}:=\\,r_{t}(x,y)\\,-\\,\\mathbb{E}_{y\\sim\\pi_{t}(\\cdot\\vert x)}[r_{t}(x,y)]$ . The last step is due to the skew symmetry of $l$ , i.e., $\\mathbb{E}_{y\\sim\\pi_{t}(\\cdot|x)}[r_{t}(x,y)]=l(x,\\pi_{t},\\pi_{t})=0$ . Then by following the same arguments in the proof of Theorem 1, with (17)(18)(19)(20), we have for any policy $\\pi$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi(\\cdot|x)}\\left(A^{t,\\pi_{t}}(x,y)\\right)\\leq2A\\sqrt{\\frac{\\ln{|y|}}{T}}+\\sqrt{10C_{\\mu\\to\\pi}\\epsilon}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}l(\\pi,\\widehat{\\pi})\\leq\\operatorname*{max}_{\\pi}\\left(2A\\sqrt{\\frac{\\ln|\\mathcal{V}|}{T}}+\\sqrt{10C_{\\mu\\to\\pi}\\epsilon}\\right)\\leq2A\\sqrt{\\frac{\\ln|\\mathcal{V}|}{T}}+\\sqrt{10C_{\\mu}\\epsilon}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that due to the skew symmetry of $l$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\pi}{\\mathrm{min}}\\,l(\\widehat{\\pi},\\pi)=\\underset{\\pi}{\\mathrm{min}}\\,\\mathbb{E}_{x\\sim\\rho,y\\sim\\widehat{\\pi}(\\cdot\\,|x),y^{\\prime}\\sim\\pi(\\cdot\\,|x)}\\,[l(x,y,y^{\\prime})]=-\\underset{\\pi}{\\mathrm{max}}\\,\\mathbb{E}_{x\\sim\\rho,y\\sim\\widehat{\\pi}(\\cdot\\,|x),y^{\\prime}\\sim\\pi(\\cdot\\,|x)}\\,[-l(x,y,y^{\\prime})]}\\\\ &{\\quad\\quad=-\\underset{\\pi}{\\mathrm{max}}\\,\\mathbb{E}_{x\\sim\\rho,y\\sim\\pi(\\cdot\\,|x),y^{\\prime}\\sim\\widehat{\\pi}(\\cdot\\,|x)}\\,[l(x,y,y^{\\prime})]=-\\underset{\\pi}{\\mathrm{max}}\\,l(\\pi,\\widehat{\\pi})\\geq-2A\\sqrt{\\frac{\\ln|\\mathcal{Y}|}{T}}-\\sqrt{10C_{\\mu}\\epsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathsf{D G}(\\widehat{\\pi})\\leq4A\\sqrt{\\frac{\\ln{|\\mathcal{V}|}}{T}}+2\\sqrt{10C_{\\mu}\\epsilon}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "H Additional Experiment Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "H.1 Summarization ", "page_idx": 28}, {"type": "text", "text": "H.1.1 Dataset Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We present dataset details in Table 4. Dataset available at https://github.com/openai/ summarize-from-feedback ", "page_idx": 28}, {"type": "text", "text": "Table 4: Dataset split, prompts, and maximum generation length for $T L;D R$ summarization ", "page_idx": 28}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/686511d4bda3ecfc689a3ca978811d293a549652924e52eaa8776b645c9f555f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "H.1.2 Model Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For SFT models, we train a Pythia 1.4B (Biderman et al., $2023)^{4}$ model for 1 epoch over the dataset with human references as labels, and use the existing fine-tuned $2.8\\mathbf{B}^{5}$ and $6.9{\\bf B}^{\\bar{6}}$ models. For reward models, we train a Pythia 1.4B parameter model for 1 epoch over the preference dataset and use the existing reward models with $2.\\bar{8}\\mathbf{B}^{7}$ and $6.9{\\mathrm{B}}^{8}$ parameters. For both REBEL and baseline methods using 1.4B and 2.8B parameters, we trained the policy and/or the critic using low-rank adapters (LoRA) (Hu et al., 2022) on top of our SFT and/or reward model respectively. For the 6.9B models, we perform full-parameter training. The 1.4B and 2.8B models are trained on 8 A6000 GPUs for one day and two days respectively. The 6.9B model is train on 8 H100 GPUs for two days. ", "page_idx": 28}, {"type": "text", "text": "H.1.3 Baseline Implementation Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For supervised fine-tuning (SFT), reward modeling training, PPO, and DPO, we follow the implementation at https://github.com/vwxyzjn/summarize_from_feedback_details (Huang et al., 2024). For iterative dpo, we implement as follows: ", "page_idx": 28}, {"type": "text", "text": "Algorithm 2 Iterative DPO   \n1: Input: Reward $r$ , policy class $\\Pi=\\{\\pi_{\\theta}\\}$ , parameter $\\beta$   \n2: Initialize policy \u03c0\u03b80.   \n3: for $t=0$ to $T-1$ do   \n4: Collect dataset $\\mathcal{D}_{t}=\\{x,y,y^{\\prime}\\}$ where $x\\sim\\rho,y\\sim\\pi_{t}(\\cdot|x),y^{\\prime}\\sim\\pi_{t}(\\cdot|x)$   \n5: Solve square loss regression problem:   \n$\\theta_{t+1}=\\operatorname*{argmin}_{\\theta}\\sum_{(x,y,y^{\\prime})\\in{\\cal D}_{t}}-\\left[\\ln\\sigma\\left((\\beta\\ln\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta_{t}}(y|x)}-\\beta\\ln\\frac{\\pi_{\\theta}(y^{\\prime}|x)}{\\pi_{\\theta_{t}}(y^{\\prime}|x)})s g n\\left(r(x,y)-r(x,y^{\\prime})\\right)\\right)\\right]\\:,$ ", "page_idx": 28}, {"type": "text", "text": "6: end for ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "where sgn is a sign function. Our implementation of iterative DPO is similar to REBEL where, at each iteration, we update with respect to $\\pi_{\\theta_{t}}$ . The major difference is that REBEL regresses toward the differences in rewards while iterative DPO only utilizes the pairwise preference signal from the rewards. ", "page_idx": 28}, {"type": "text", "text": "H.1.4 Reward Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "To ensure that $\\pi_{\\theta}$ remains close to $\\pi_{\\theta_{0}}$ , we apply an additional KL penalty to the reward: ", "page_idx": 29}, {"type": "equation", "text": "$$\nr(x,y)=R M(x,y)-\\gamma(\\ln\\pi_{\\theta_{t}}(y|x)-\\ln\\pi_{\\theta_{0}}(y|x))\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $R M(x,y)$ is score from the reward model given prompt $x$ and response $y$ . Furthermore, to ensure that the generations terminate within the maximum generation length, we penalize any generation that exceeds this length by setting $r(x,y)$ to a small fixed constant, $\\Gamma$ . ", "page_idx": 29}, {"type": "text", "text": "For $T L;D R$ summarization, we set $\\gamma=0.05$ and $\\Gamma=-1$ ", "page_idx": 29}, {"type": "text", "text": "H.1.5 Hyperparameter Details ", "text_level": 1, "page_idx": 29}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/88a593bd1b48304a4af2ababeab4b8f40c82d71ef287ab3854a20a883566cec5.jpg", "table_caption": ["Parameter setting for $T L{\\sqrt{D R}}$ summarization "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "H.1.6 Winrate Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We are using $\\mathtt{g p t-4-0613}$ checkpoint for winrate computations. Below we show the prompt for winrate evaluation and an example evaluation from GPT4. ", "page_idx": 30}, {"type": "text", "text": "Prompt for Winrate ", "text_level": 1, "page_idx": 30}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/5ead09213d80f30653803dc48f2facc786299e93e93e73159e0802bf925d2fae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Example Evaluation from GPT4 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Prompt ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "SUBREDDIT: r/AskReddit ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "TITLE: How do you get someone out of your head? ", "page_idx": 30}, {"type": "text", "text": "POST: Hi,   \n$\\Gamma\\,\\mathrm{m}\\,22$ , and I have been with my girlfriend for 5 years now. We recently moved together. We\u2019ve always loved each other intensely. ", "page_idx": 30}, {"type": "text", "text": "Problem, I recently started to have feelings for an other person (a friend). This person has had a boyfriend for now 3 years, and has absolutely no ideas. Those feelings were so strong, it was hard to hide them. After 2 months of me being distant and really sad, my girlfriend forced me to say what was bothering me. I\u2019m not a good liar, and now she knows. ", "page_idx": 30}, {"type": "text", "text": "We decided to give us a week alone, I went to my parents. ", "page_idx": 30}, {"type": "text", "text": "Now, I\u2019m completely lost. I keep on thinking about this person, and I hate that. I would like for those feelings to go away, to leave me alone. But I can\u2019t. ", "page_idx": 30}, {"type": "text", "text": "What do I do? It\u2019s been 3 months now, and I\u2019m just desperate. ", "page_idx": 30}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/534ad10807a3f808e958e191dd4dc96191dc8bef66f20d410226ffcb0fd11988.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "H.2 General Chat ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "H.2.1 Dataset Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We present dataset details in Table 5. ", "page_idx": 31}, {"type": "text", "text": "Table 5: Dataset details for General Chat ", "page_idx": 31}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/a4114326df44f20a18c8133b38e3bbd6c830f6fecf9508327e42f98dff4d7a01.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "H.2.2 Model Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "For OpenChat- $3.5^{11}$ , we only train the last four layers and keep other layers frozen. For Meta-Llama3-8B-Instruct12, we perform full-parameter training. For Starling-RM-7B-alpha13 and ArmoRM$\\mathrm{Llama3-8B-v0.1^{14}}$ , we directly use the reward scores without any normalizations. We filter out prompts that are longer than 1, 024 tokens $(2.3\\%)$ to fti the input length. OpenChat-3.5 is trained for four days, and Meta-Llama-3-8B-Instruct is train for one day on 8 H100 GPUs. ", "page_idx": 31}, {"type": "text", "text": "H.2.3 Reward Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "To ensure that $\\pi_{\\theta}$ remains close to $\\pi_{\\theta_{0}}$ , we apply an additional KL penalty to the reward: ", "page_idx": 31}, {"type": "equation", "text": "$$\nr(x,y)=R M(x,y)-\\gamma(\\ln\\pi_{\\theta_{t}}(y|x)-\\ln\\pi_{\\theta_{0}}(y|x))\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $R M(x,y)$ is score from the reward model given prompt $x$ and response $y$ . Furthermore, to ensure that the generations terminate within the maximum generation length, we penalize any generation that exceeds this length by setting $r(x,y)$ to a small fixed constant, $\\Gamma$ . ", "page_idx": 31}, {"type": "text", "text": "For the general chat experiments, we set $\\Gamma=-4$ . ", "page_idx": 31}, {"type": "text", "text": "H.2.4 Hyperparameter Details ", "text_level": 1, "page_idx": 31}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/4cda462ea6026757f068af2a1ac5295965c468e1cc7119ee3e6b207853ac9001.jpg", "table_caption": ["Parameter setting for General Chat "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "H.3 Image Generation ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "H.3.1 Dataset Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Generation prompts: cat, dog, horse, monkey, rabbit, zebra, spider, bird, sheep, deer, cow, goat, lion, tiger, bear, raccoon, fox, wolf, lizard, beetle, ant, butterfly, fish, shark, whale, dolphin, squirrel, mouse, rat, snake, turtle, frog, chicken, duck, goose, bee, pig, turkey, fly, llama, camel, bat, gorilla, hedgehog, kangaroo. ", "page_idx": 32}, {"type": "text", "text": "H.3.2 Model Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We use the latent consistency model (Luo et al., 2023) distillation of the Dreamshaper v7 model 15 for our experiments. Experiments are conducted on 4 A6000 GPUs with each run requiring 10 hours. ", "page_idx": 32}, {"type": "text", "text": "H.3.3 Hyperparameter Details ", "text_level": 1, "page_idx": 32}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/64da3e44c7567f9d37f3785059cd6ce43fd3b1ed48b7001e5e753c376a19c494.jpg", "table_caption": ["Parameter setting for Consistency Models "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/de7351bb68b25652e0e3945abb5a65d8d4e0f68fe80bddbf8a7e6616542bfb5c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "TITLE: Reddit, what kind of effect do you think that crime shows/movies might have in reducing the occurrence of thought-out, \"intelligent\" crime? ", "page_idx": 33}, {"type": "text", "text": "POST: I had this thought a few days ago when watching an episode of NCIS and they did the usual \"create a super high resolution image from a very low resolution image.\" I got to wondering how much of it is real and how much of it is fake (or, conversely, how much might be under-exaggerated). Based on these types of shows/movies (bank robbery, conning, etc.) where the bad guy nearly $100\\%$ of the time gets caught, only an idiot would think it would be a good idea to do any of this stuff, and from my [limited] knowledge of the general criminal, I\u2019m under the impression that they are mainly uneducated people in dire situations (or psychopaths/mentally unstable.. but $\\Gamma\\mathbf{m}$ thinking mostly with regards the heist type of crime \u2013 not assault or murder) that are driven to extreme lengths to make the \"easy\"/\"quick\" dollar. So I wondered, is it possible that these shows/movies that show the criminal, no matter how smart and creative, almost always getting caught could actually be (subconsciously or consciously) reducing the amount of more well thought through crime throughout the world? ", "page_idx": 33}, {"type": "text", "text": "TL;DR: Generation I had this thought recently watching NCIS episode where criminals almost always get caught, is it possible that these crime shows/movies that show the criminal almost always getting caught could actually be (subconsciously or consciously) reducing the crime? ", "page_idx": 33}, {"type": "text", "text": "Prompt ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Hi, I\u2019d like to play ice hockey. Can you explain how the game works? ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Generation ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Hockey enthusiast! I\u2019d be happy to explain the basics of ice hockey. Here\u2019s a comprehensive overview to get you started: ", "page_idx": 34}, {"type": "text", "text": "Objective: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The objective of ice hockey is to score more goals than the opposing team by using your stick to hit a puck (a rubber disc) into the opponent\u2019s goal. ", "page_idx": 34}, {"type": "text", "text": "Gameplay: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Game Structure: A standard ice hockey game consists of three periods, each lasting 20 minutes. ", "page_idx": 34}, {"type": "text", "text": "2. Teams: Each team consists of six players on the ice at a time: ", "page_idx": 34}, {"type": "text", "text": "\u2022 Forwards (3): Players who score goals and create scoring opportunities. \u2022 Defensemen (2): Players who defend their goal and prevent the opposing team from scoring. \u2022 Goalie (1): The player responsible for preventing the puck from entering the goal. ", "page_idx": 34}, {"type": "text", "text": "3. Gameplay Flow: The game starts with a face-off, where two players from opposing teams try to gain control of the puck after it\u2019s dropped onto the ice by the referee. Players can move the puck using their sticks, skates, or by carrying it. ", "page_idx": 34}, {"type": "text", "text": "4. Scoring: A goal is scored when a player hits the puck into the opponent\u2019s goal using their stick, skate, or body. ", "page_idx": 34}, {"type": "text", "text": "5. Penalties: If a player breaks the rules, they can be sent to the penalty box for a set amount of time (2-5 minutes). During this time, their team must play shorthanded (with one fewer player on the ice). ", "page_idx": 34}, {"type": "text", "text": "6. Power Play: If the opposing team scores during a penalty, the penalized team gets a power play, where they have a man advantage on the ice. ", "page_idx": 34}, {"type": "text", "text": "7. Icing: If a player shoots the puck from behind their own blue line and it crosses the opponent\u2019s goal line without being touched, it\u2019s considered icing, and the opposing team is awarded a face-off in their own end zone. ", "page_idx": 34}, {"type": "text", "text": "8. Offside: A player cannot enter the opponent\u2019s zone before the puck, or they\u2019ll be called offside. ", "page_idx": 34}, {"type": "text", "text": "Common Penalties: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 Hooking: Using the stick to hook an opponent\u2019s leg or body.   \n\u2022 Holding: Grabbing onto another player\u2019s jersey or equipment.   \n\u2022 Tripping: Using a stick, leg, or body to trip an opponent.   \n\u2022 Interference: Interfering with an opponent who doesn\u2019t have possession of the puck. ", "page_idx": 34}, {"type": "text", "text": "Equipment: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 Stick: A long, flat stick used to pass, shoot, and control the puck.   \n\u2022 Skates: Specialized ice skates designed for speed, agility, and stopping.   \n\u2022 Pads: Protective gear worn by players, including shoulder pads, elbow pads, gloves, shin guards, and hockey pants.   \n\u2022 Helmet: A protective helmet worn by all players to prevent head injuries.   \n\u2022 Goalie Gear: Specialized equipment worn by goalies, including a large pad covering their body, gloves, and a mask. ", "page_idx": 34}, {"type": "text", "text": "Basic Strategies: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 Forechecking: Applying pressure to the opponent\u2019s defensemen to gain possession of the puck.   \n\u2022 Backchecking: Pressuring the opponent\u2019s forwards to prevent them from scoring.   \n\u2022 Breakouts: Quickly moving the puck from your own end zone to the neutral zone to create scoring opportunities. ", "page_idx": 34}, {"type": "text", "text": "This is a solid foundation to get you started. As you play more, you\u2019ll learn more about the game\u2019s nuances and strategies. Have fun playing ice hockey! ", "page_idx": 34}, {"type": "text", "text": "On Reddit, Reddit Gold (also known as G\u00a8old)\u00a8 is a virtual currency that allows users to show appreciation for content creators, moderators, and other users who contribute positively to the community. When someone gives Reddit Gold to an admin (or moderator), it\u2019s often a way to express gratitude for their efforts in maintaining the community\u2019s quality, resolving issues, and keeping the discussion civil. ", "page_idx": 35}, {"type": "text", "text": "Here are some reasons why people might give Reddit Gold to admins: ", "page_idx": 35}, {"type": "text", "text": "1. Appreciation for moderation: Admins work tirelessly to ensure that discussions remain on-topic, respectful, and follow community guidelines. Giving Gold acknowledges their dedication and hard work in maintaining a positive environment. ", "page_idx": 35}, {"type": "text", "text": "2. Recognition of expertise: Admins often possess in-depth knowledge about specific topics or communities. Giving Gold shows appreciation for their expertise and willingness to share it with others. ", "page_idx": 35}, {"type": "text", "text": "3. Support for community building: Admins play a crucial role in fostering engagement, encouraging participation, and creating opportunities for users to connect with each other. Gold gifts demonstrate support for their efforts in building a strong community. ", "page_idx": 35}, {"type": "text", "text": "4. Rewarding good behavior: When admins demonstrate excellent behavior, such as resolving confilcts fairly or providing helpful guidance, users might give Gold as a way to recognize and reward their good behavior. ", "page_idx": 35}, {"type": "text", "text": "5. Boosting morale: Giving Gold can be a morale booster for admins, acknowledging their value to the community and motivating them to continue their excellent work. ", "page_idx": 35}, {"type": "text", "text": "It\u2019s worth noting that Reddit Gold is not required or expected, and admins don\u2019t receive any direct benefits from receiving Gold. However, the gesture of appreciation can go a long way in fostering a positive and supportive community. ", "page_idx": 35}, {"type": "text", "text": "I.3 Image Generations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Example Generations of REBEL ", "page_idx": 36}, {"type": "image", "img_path": "yxjWAJzUyV/tmp/34942a9ef301ed17bb9e18faca3d16043c34f2302e38bed4dc10f73c02e103fd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "yxjWAJzUyV/tmp/538d39058da00d905fa6d9938f79b5c80867d1e4baf2dbe27ea640d397c2ad66.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "J Ablation Analysis ", "text_level": 1, "page_idx": 38}, {"type": "table", "img_path": "yxjWAJzUyV/tmp/23fa8add9c5766d64ed8ce64c50774251eb9b617ab36a2fb93500c0c3065cc1d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "Table 6: REBEL ablation of the key hyperparameter $\\eta$ on summarization task and 2.8B model. The best-performing $\\eta$ for each metric is highlighted in bold. ", "page_idx": 38}, {"type": "text", "text": "Just like DPO, tuning REBEL is much more straightforward than PPO since the only hyperparameter REBEL introduced is $\\eta$ . We investigate how sensitive REBEL is to learning rate $\\eta$ in the loss. The results of ablation on summarization task and 2.8B model is shown in Table 6 with the same setting detailed in Appendix H.1.5 except for $\\eta$ . REBEL achieves the best performance when $\\eta=1$ , while increasing or decreasing $\\eta$ leads to decreased performance. Our result here indicates that $\\eta$ is an important hyperparameter that requires tuning for achieving a good performance. Setting $\\eta$ to 1.0 is a good starting point since, for all of our experiments from language modeling to image generation, $\\eta=1$ achieves the best results. ", "page_idx": 38}, {"type": "text", "text": "K Trade-off between Reward Model Score and KL-divergence ", "text_level": 1, "page_idx": 38}, {"type": "image", "img_path": "yxjWAJzUyV/tmp/db3c1657f1804973218f3171ee18d73e99e527befbcdc715f0d8f075a416c955.jpg", "img_caption": ["Figure 5: Plot of Reward vs KL-Divergence for 2.8B REBEL and PPO for summarization. We evaluate the models across the entire test set every 100 steps for 2,000 steps. Left: each point represents the average reward score and KL-divergence for a specific time step; the eclipse represents the confidence interval with 2 standard deviations. Right: we divide the KL distribution at the 2,000-step into 10 bins with equal size and average the corresponding RM scores in each bin. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "The trade-off between the reward model score and KL-divergence is shown in Figure 5. We evaluate the 2.8B REBEL and PPO every 400 gradient updates during training for 8,000 updates on summarization. The sample complexity of each update is held constant across both algorithms for fair comparison. For the left plot, each point represents the average divergence and score over the entire test set, and the eclipse represents the confidence interval with 2 standard deviations. As observed previously, PPO exhibits lower divergence, whereas REBEL shows higher divergence but is capable of achieving larger RM scores. Notably, towards the end of the training (going to the right part of the left plot), REBEL and PPO have similar KL and RM scores. For the right plot in Figure 5, we analyze a single checkpoint for each algorithm at the end of training. For each algorithm, we group every generation from the test set by its KL distribution into 10 equally sized bins and calculate the average of the corresponding RM score for each bin. We can see that REBEL achieves higher RM scores for generations with small divergence while requiring larger divergence for generations with the highest scores. ", "page_idx": 38}, {"type": "text", "text": "L Regression Loss During Training ", "text_level": 1, "page_idx": 39}, {"type": "image", "img_path": "yxjWAJzUyV/tmp/1797729e6c38847766b70dae101809d959acc49fdda927c5b3e7b764c8c467f3.jpg", "img_caption": ["Figure 6: REBEL\u2019s reward difference prediction error throughout training of our 6.9B parameter policy on the summarization task. The reward used for this task is unbounded with the range of values of the human labels in the validation set being $[-6.81,7.31]$ . We plot both the smoothed values with a moving average and the loss vales at each iteration. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 6 shows the observed loss of Eq. 1 that we observed when finetuning the 6.9B Pythia model on summarization. We see that REBEL minimizes the loss throughout training maintaining a relatively low mean squared error given that our observed rewards were mostly between $[-10,10]$ . Note that our learned reward model, however, is unbounded. ", "page_idx": 39}, {"type": "text", "text": "M Breakdown of MT-Bench ", "text_level": 1, "page_idx": 39}, {"type": "image", "img_path": "yxjWAJzUyV/tmp/57847c23222b143afba6994cc1508092123b0c64ad8fca606d0eaeb8d26bc21a.jpg", "img_caption": ["Figure 7: Breakdown of MT-Bench results over eight dimensions. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 7 shows the breakdown of MT-Bench results. REBEL (REBEL-OpenChat-3.5) outperforms both APA (Starling-LM-7B-alpha) and base (OpenChat-3.5) models on six out of eight dimensions including writing, roleplay, math, extraction, STEM, and humanities. ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The abstract and introduction (Section 1) accurately present the primary claims of the paper, aligning well with the detailed contributions and scope described in the main body. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The limitation of exact Bayes Optimal solution assumption in Section 3.2 is discussed in Section 4.   \n\u2022 Justification and limitation of Assumption 1 are discussed in Appendix E.   \n\u2022 The regret bound of REBEL is discussed in Section 4. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: All of the assumptions and claims are clearly stated and cross-referenced. ", "page_idx": 41}, {"type": "text", "text": "\u2022 Proof of Claim 1 is provided in Appendix B.   \n\u2022 Proof of Claim 2 is provided in Appendix C.   \n\u2022 Justification of Assumption 1 is provided in Appendix E.   \n\u2022 Proof of Theorem 1 is provided in Appendix F.   \n\u2022 Proof of Theorem 2 is provided in Appendix G.1. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We detail the datasets, models, hyperparameters, and evaluation metrics in Section 5 and Appendix H. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We provide an anonymized version of data and code as supplemental materials. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: We detail the datasets (splits), model pipelines, and complete sets of hyperparameters in Section 5 and Appendix H. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Our results for summarization and image generation are averages of three random seeds. Specifically we report the standard deviation across seeds for summarization and IQM with $95\\%$ confidence intervals for our image generation results. For general chat experiments, we train once to obtain the final result. This is consistent with how the community reports general chatbot evaluations on AlpacaEval 2.0 and Chatbot Arena / MTBench. We follow other works in reporting our checkpoint\u2019s results. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Resources for experiments are discussed in Appendix H. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 43}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper introduces an algorithm that could potentially be applied to any reinforcement learning task. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: Datasets and models we used in experiments are cited in Section 5 and the model/dataset cards are detailed in Appendix H. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: The code used in the paper is well documented with instructions. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]