[{"type": "text", "text": "Infinite Limits of Multi-head Transformer Dynamics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Blake Bordelon, Hamza Chaudhry, Cengiz Pehlevan John A. Paulson School of Engineering and Applied Sciences Center for Brain Science Kempner Institute for the Study of Natural and Artificial Intelligence Harvard University Cambridge, MA 02138 blake_bordelon@g.harvard.edu hchaudhry@g.harvard.edu cpehlevan@seas.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we analyze various scaling limits of the training dynamics of transformer models in the feature learning regime. We identify the set of parameterizations that admit well-defined infinite width and depth limits, allowing the attention layers to update throughout training\u2013a relevant notion of feature learning in these models. We then use tools from dynamical mean field theory (DMFT) to analyze various infinite limits (infinite key/query dimension, infinite heads, and infinite depth) which have different statistical descriptions depending on which infinite limit is taken and how attention layers are scaled. We provide numerical evidence of convergence to the limits and discuss how the parameterization qualitatively influences learned features. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Increasing the scale of transformer models has continued to improve performance of deep learning systems across many settings including computer vision [1, 2, 3, 4] and language modeling [5, 6, 7, 8, 9]. However, understanding the optimization stability and limiting behavior of these models under increases in model scale remains a core challenge. ", "page_idx": 0}, {"type": "text", "text": "One approach to scaling up systems in a stable and predictable way is to identify parameterizations of neural networks that give approximately scale-independent feature updates during training [10, 11, 12]. The mean field parameterization, commonly referred to as $\\mu\\mathrm{P}_{i}$ , is a well-known example that satisfies this property [13, 14, 15]. When such parameterizations are adopted, the learned internal representations in hidden layers of the network are very similar across model scales [16, 17], but performance tends to improve with model scale [10, 11, 12]. Further, theoretical results about their limits can often be obtained using Tensor Programs [14] or dynamical mean field theory (DMFT) techniques [15, 17]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we develop a theoretical treatment of randomly initialized transformers. We study various scaling limits of the training dynamics of these models including the infinite key/query dimension limit, the infinite head limit, and the infinite depth limit. Concretely, our contributions are the following: ", "page_idx": 0}, {"type": "text", "text": "1. We derive a DMFT for feature learning in randomly initialized transformers with key/query dimension $N$ , attention head count $\\mathcal{H}$ and depth $L$ . From the derived DMFT action, we identify large $N$ , large $\\mathcal{H}$ and large $L$ limits of the training dynamics. ", "page_idx": 0}, {"type": "text", "text": "2. We analytically show that the large key-query $N\\rightarrow\\infty$ limit requires the $\\mu\\mathrm{P}$ scaling of key/query inner product with $1/N$ , even if key/queries are reparameterized to decrease the size of their updates from gradient descent.   \n3. From the limiting equations, we show that this $N\\rightarrow\\infty$ limit causes multi-head self attention trained with stochastic gradient descent (SGD) to effectively collapse to single-head self attention since all heads follow identical dynamics.   \n4. To overcome this limitation, we analyze the infinite head $\\mathcal{H}\\to\\infty$ limit while fixing $N$ . We show there is a limiting distribution of attention variables across heads at each layer throughout training. Despite $N$ being finite, the infinite-head $\\mathcal{H}\\,\\rightarrow\\,\\infty$ limit leads to concentration of the network\u2019s output logits and learned residual stream feature kernels, giving deterministic training dynamics.   \n5. Finally, we examine large depth limits of transformers with residual branch scaling. We illustrate and discuss the tension between parameterizing a model so that it has a non-trivial kernel at initialization while maintaining feature learning within the multi-head self attention (MHSA) and multi-layer perceptron (MLP) blocks. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Hron et al. [18] studied the Neural Network Gaussian Process limit of multi-head self attention in the infinite-head $\\mathcal{H}\\to\\infty$ limit. They showed that, at initialization, there is a limiting distribution over attention matrices and that the outputs of the multi-head attention block follow a Gaussian process, establishing a connection to kernel methods. Dinan et al. [19] develop a similar theory of transformers at initialization and compute the Neural Tangent Kernel associated with this architecture as the dimensions per head $N\\to\\infty$ using a $\\frac{1}{\\sqrt{N}}$ scaling of the key-query inner product within each attention layer. One of our key theoretical results is showing that this picture of a distribution over learned attention heads persists throughout training in the feature-learning regime as $\\mathcal{H}\\to\\infty$ (though the distribution of residual stream variables generally becomes non-Gaussian). ", "page_idx": 1}, {"type": "text", "text": "Several works have analyzed the signal propagation properties of transformers at initialization at large key/query dimension $N$ and large depth $L$ [20, 21, 22, 23] including providing modifications to the standard transformer architecture [22, 24]. In this work, we pursue large depth limits of transformers by scaling the residual branch as $L^{-\\alpha_{L}}$ with $\\textstyle\\alpha_{L}\\in[\\frac{1}{2},1]$ , which has been shown to converge to a limit not only at initialization [25, 26, 27], but also throughout training in the feature learning regime [11, 12, 27]. However, we argue that in transformers that $\\alpha_{L}=1$ is preferable as it enables the attention layers to update non-negligibly as $L\\rightarrow\\infty$ . ", "page_idx": 1}, {"type": "text", "text": "Yang et al. [10] introduced the $\\mu\\mathrm{P}$ scaling for attention layers which multiplies the key/query inner product with $\\textstyle{\\frac{1}{N}}$ rather than the more commonly used \u221a1N [5]. They show empirically that this change improves stability of training and transfer of optimal hyperparameters across different values of $N$ . Vyas et al. [16] empirically found that such $\\mu\\mathrm{P}$ transformers learn attention matrices that become approximately consistent across different heads and model sizes, suggesting that models parameterized in $\\mu\\mathrm{P}$ learn similar representations across scales. ", "page_idx": 1}, {"type": "text", "text": "In addition to work on infinite width and depth limits of deep networks, there is also a non-asymptotic approach to optimizer design and scaling based on controlling the norm of weight updates [28]. This approach coincides with $\\mu\\mathrm{P}$ width-scaling when the spectral norm of the weights is used as the measure of distance [29], and can achieve hyperparameter transfer for a wide array of optimizers and initialization schemes [30? ]. ", "page_idx": 1}, {"type": "text", "text": "2 Parameterizations with Feature Learning Limits ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a transformer architecture with $L$ layers, $\\mathcal{H}$ heads per layer, and $N$ dimensional keys/ queries per head. Transformers are often defined in terms of $d_{\\mathrm{model}}\\,=\\,\\mathcal{H}d_{\\mathrm{head}}\\,=\\,\\mathcal{H}N$ which can be increased by scaling the number of heads or the dimension of each head, where $N$ is often written $d_{\\mathrm{head}}$ . Our goal is to determine the set of parameterizations that allow the attention layers to undergo non-trivial feature learning in the various $N,\\mathcal{H},L\\to\\infty$ limits. The analysis of these limits is performed with batch size and number of training steps $t$ fixed while the other architectural parameters are taken to infinity. ", "page_idx": 1}, {"type": "text", "text": "2.1 Model Scalings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The network\u2019s output is computed by a depth $L$ recursion through hidden layers $\\ell\\in[L]$ starting with the first layer $\\begin{array}{r}{h_{\\mathfrak{s}}^{1}(\\dot{\\pmb x})=\\frac{1}{\\sqrt{D}}\\dot{W}^{0}x_{\\mathfrak{s}}\\stackrel{\\cdot}{\\in}\\mathbb{R}^{\\dot{N}\\dot{\\mathcal{H}}}}\\end{array}$ where $\\mathbf{\\boldsymbol{x}}_{s}\\in\\mathbb{R}^{D}$ is the input at spatial/token position $\\pmb{\\mathfrak{s}}$ Preactivations in subsequent layers $h^{\\ell}$ are determined by a forward pass through the residual stream which contains an attention layer and a MLP layer ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ensuremath{h_{\\mathfrak{s}}}^{\\ell+1}=\\tilde{h}_{\\mathfrak{s}}^{\\ell}+\\frac{\\beta_{0}}{L^{\\alpha_{L}}}\\mathrm{MLP}\\left(\\tilde{h}_{\\mathfrak{s}}^{\\ell}\\right)\\;,\\;\\tilde{h}_{\\mathfrak{s}}^{\\ell}=h_{\\mathfrak{s}}^{\\ell}+\\frac{\\beta_{0}}{L^{\\alpha_{L}}}\\mathrm{MHSA}\\left(\\ensuremath{h^{\\ell}}\\right)_{\\mathfrak{s}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The constants $\\gamma_{0}$ and $\\beta_{0}$ control the rate of feature learning and the effective depth respectively 1. We will consider $\\begin{array}{r}{\\alpha_{L}\\in[\\frac{1}{2},1]}\\end{array}$ .2 The multi-head self attention layer (MHSA) with pre-layer-norm $_{.}^{3}$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MHSA}\\left(h^{\\ell}\\right)_{\\mathfrak{s}}=\\displaystyle\\frac{1}{\\sqrt{N\\mathcal{H}}}\\sum_{\\mathfrak{p}\\in[\\mathcal{H}]}W_{\\mathcal{O}\\mathfrak{h}}^{\\ell}v_{\\mathfrak{h}\\mathfrak{s}}^{\\ell\\sigma}\\:,\\quad v_{\\mathfrak{h}\\mathfrak{s}}^{\\ell\\sigma}=\\sum_{\\mathfrak{s}^{\\prime}\\in[S]}\\sigma_{\\mathfrak{h}^{\\prime}\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{\\ell}v_{\\mathfrak{h^{\\prime}}\\mathfrak{s^{\\prime}}}^{\\ell}}\\\\ {v_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}=\\displaystyle\\frac{1}{\\sqrt{N\\mathcal{H}}}W_{V\\mathfrak{h}}^{\\ell}\\bar{h}_{\\mathfrak{s}}^{\\ell}\\:,\\quad\\bar{h}_{\\mathfrak{s}}^{\\ell}=\\mathrm{LN}(h_{\\mathfrak{s}}^{\\ell}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\sigma_{\\mathfrak{h}}^{\\ell}\\in\\mathbb{R}^{S\\times S}$ are the attention matrices passed through a matrix-valued nonlinearity $\\sigma\\left(\\pmb{\\mathscr{A}}_{\\mathfrak{h}}^{\\ell}\\right)^{4}$ . For a sequence of length $\\boldsymbol{S}$ , the pre-attention matrix $\\mathcal{A}_{\\mathfrak{h}}^{\\ell}\\in\\mathbb{R}^{S\\times S}$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\nA_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{\\ell}=\\frac{1}{N^{\\alpha_{A}}}k_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}\\cdot q_{\\mathfrak{h}\\mathfrak{s^{\\prime}}}^{\\ell}\\,,\\;k_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}=\\frac{1}{N^{\\frac{3}{2}-\\alpha_{A}}\\sqrt{\\mathcal{H}}}W_{K\\mathfrak{h}}^{\\ell}\\bar{h}_{\\mathfrak{s}}^{\\ell}\\,,\\;q_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}=\\frac{1}{N^{\\frac{3}{2}-\\alpha_{A}}\\sqrt{\\mathcal{H}}}W_{Q\\mathfrak{h}}^{\\ell}\\bar{h}_{\\mathfrak{s}}^{\\ell}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The exponent $\\alpha_{\\mathcal{A}}$ will alter the scale of the pre-attention variables $\\mathcal{A}_{\\mathfrak{h}}^{\\ell}$ at initialization. The input matrices have shape $W_{V\\mathfrak{h}}^{\\ell}$ , $W_{K\\mathfrak{h}}^{\\ell}$ , $W_{Q\\mathfrak{h}}^{\\ell}\\in\\mathbb{R}^{N\\times N\\mathcal{H}}$ , while the output matrices have shape $W_{O\\mathfrak{h}}^{\\ell}\\in$ $\\mathbb{R}^{N\\mathcal{H}\\times N}$ . All of the weights $W_{O\\mathfrak{h^{\\prime}}}^{\\ell},W_{Q\\mathfrak{h}}^{\\ell},W_{K\\mathfrak{h}}^{\\ell}$ W \u2113Kh are initialized with \u0398(1) entries while W \u2113Kh $W_{K\\mathfrak{h}}^{\\ell},W_{Q\\mathfrak{h}}^{\\ell}$ have entries of size $\\Theta(N^{1-\\alpha,A})$ which ensures that all key and query $k,q$ vectors are $\\Theta(1)$ at initialization. The pre-attention variables $\\mathcal{A}_{\\mathfrak{h}}^{\\ell}\\in\\mathbb{R}^{S\\times S}$ at each head $\\mathfrak{h}$ are determined by key $k_{\\mathrm{h}\\thinspace s}^{\\ell}$ and query $q_{\\mathfrak{h}_{5}^{\\prime}}^{\\ell}$ inner products. The MLP layer consists of two linear layers with an element-wise nonlinearity $\\phi$ applied in between, where $W^{\\ell,2}$ , $W^{\\ell,1}\\in\\mathbb{R}^{N\\mathcal{H}\\times N\\mathcal{H}}$ are initialized with $\\Theta(1)$ entries: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{MLP}(\\tilde{h}_{\\mathrm{s}}^{\\ell})=\\frac{1}{\\sqrt{N\\mathcal{H}}}W^{\\ell,2}\\phi\\left(\\tilde{h}_{\\mathrm{s}}^{\\ell,1}\\right)\\;,\\;\\tilde{h}_{\\mathrm{s}}^{\\ell,1}=\\frac{1}{\\sqrt{N\\mathcal{H}}}W^{\\ell,1}\\tilde{h}_{\\mathrm{s}}^{\\ell}\\;,\\;\\tilde{h}_{\\mathrm{s}}^{\\ell}=\\mathrm{LN}\\left(\\tilde{h}_{\\mathrm{s}}^{\\ell}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$\\mu\\mathrm{P}$ scaling [13, 31, 14, 15] downscales the readout of the last layer compared to standard and NTK parameterization [32]. Thus, we define the output of the model as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf=\\frac{1}{\\gamma_{0}N\\mathcal{H}}\\pmb{w}^{L}\\cdot\\left(\\frac{1}{S}\\sum_{\\mathfrak{s}}h_{\\mathfrak{s}}^{L}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "5 where $h_{s}^{L}\\in\\mathbb{R}^{N\\mathcal{H}}$ are the final layer preactivations at spatial/token position ${\\mathfrak{s}}\\in[S]$ . The parameter $\\gamma_{0}$ is an additional scalar that controls the rate of change of the internal features of the network relative to the network output [33]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Learning Rate Scalings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In order to approximately preserve the size of internal feature updates, we must scale the learning rate $\\eta$ appropriately with $(N,\\mathcal{H},L)$ . However, this scaling depends on the optimizer. In Table 2, we provide the appropriate scaling of learning rates for SGD and Adam for any $\\textstyle\\alpha_{L}\\in[\\frac{1}{2},1]$ and $\\textstyle\\alpha_{{\\cal A}}\\in[\\frac{1}{2},1]$ . In what follows, we focus on the SGD scaling and theoretically analyze the $N\\rightarrow\\infty$ , $\\mathcal{H}\\rightarrow\\bar{\\infty}$ , and $L\\to\\infty$ limits of the training dynamics. We also provide in Table 2 details about what prefactor the first layer should be multiplied by and the initial weights divided by to ensure convergence to the $L\\rightarrow\\infty$ limit. Example FLAX implementations of these parameterizations for vision and language modeling transformers are provided in Appendix B. ", "page_idx": 2}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/f333fd78d2f572afdc40308735773dd5314664cb2660ee90f17961d3877614cd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Schematic representations of the transformer architecture we model. (a) The forward pass through the residual stream is an alternation of MHSA and MLP blocks scaled by $\\beta_{0}L^{-\\alpha_{L}}$ . (b) The MHSA block computes keys, queries, values, and attention variables to produce a concatenated output of dimension $d_{\\mathrm{model}}=N\\mathcal{H}$ . ", "page_idx": 3}, {"type": "table", "img_path": "p0BBKhD5aI/tmp/c65c9561f8c8cc9bf08dbcbeb943fa88a8e3a604f58913fca6ac8fdca2a91a85.jpg", "table_caption": [], "table_footnote": ["Table 1: The learning rates which should be applied to obtain the correct scale of updates for SGD or Adam optimizers. In addition, the weight variance and multiplier for the first layer may need to be rescaled (relative to eq (5)) with width/depth depending on the parameterization and optimizer. "], "page_idx": 3}, {"type": "text", "text": "Our analysis assumes that at each step $t$ of SGD or Adam a mini-batch $\\mathfrak{B}_{t}$ of size $\\Theta(1)$ is used to estimate the loss gradient. We assume that the minibatches are fixed. Further, the number of total training steps is assumed to not be scaled jointly with the model size. Therefore the analysis provided here can cover both online SGD for a fixed number of steps or full batch GD (repeating data) with a $\\Theta(1)$ sized dataset. ", "page_idx": 3}, {"type": "text", "text": "3 Infinite Limits of Learning Dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we first analyze the infinite dimension-per-head $N\\rightarrow\\infty$ limit of training. We find that for this limit, the $\\mu\\mathrm{P}$ rule of $\\alpha_{\\mathcal{A}}=1$ is necessary and show that all heads collapse to the same dynamics. To counteract this effect, we next analyze the infinite head $\\mathcal{H}\\to\\infty$ limit of the training dynamics at fixed $N,L$ , where we find a limiting distribution over attention heads. We will conclude by analyzing the statistical descriptions of various infinite depth $L\\rightarrow\\infty$ limits. 6. ", "page_idx": 3}, {"type": "text", "text": "3.1 Mean Field Theory Treatment of the Learning Dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To obtain the exact infinite limits of interest when scaling dimension-per-head $N$ , the number of heads $\\mathcal{H}$ , or the depth $L$ to infinty, we work with a tool from statistical physics known as dynamical mean field theory (DMFT). Classically, this method has been used to analyze high dimensional disordered systems such as spin glasses, random recurrent neural networks, or learning algorithms with high dimensional random data [34, 35, 36, 37, 38, 39]. Following [15, 11], we use this method to reason about the limiting dynamics of randomly initialized neural networks by tracking a set of deterministic correlation functions (feature and gradient kernels) as well as additional linear-response functions (see Appendix D). The core conceptual idea of this method is that in the infinite limit and throughout training, all neurons remain statistically independent and only interact through collective variables (feature kernels, neural network outputs, etc). Further the collective variables can be computed as averages over distribution of neurons in each hidden layer or along the residual stream. This DMFT description can be computed using a path integral method that tracks the moment generating function of the preactivations or with a dynamical cavity method (see Appendix D). ", "page_idx": 3}, {"type": "text", "text": "3.2 Scaling Dimension-Per-Head $N$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "One way of obtaining a well-defined infinite parameter limit of transformers is to take the $N\\rightarrow\\infty$ limit, where $N$ is the dimension of each head. A priori, it is unclear if there are multiple ways of scaling the key/query inner product. Concretely, it is unknown what values for the exponent $\\alpha_{\\mathcal{A}}$ are admissible for the pre-attention $\\begin{array}{r}{\\mathcal{A}\\,=\\,\\frac{1}{N^{\\alpha}\\mathcal{A}}\\pmb{k}\\cdot\\pmb q}\\end{array}$ . The keys and queries are uncorrelated at initialization which motivated the original choice of $\\alpha_{\\mathcal{A}}=\\textstyle{\\frac{1}{2}}$ [5, 18]. Yang et al. [10] assume the entries of the key and query vectors move by $\\Theta(1)$ , implying $\\alpha_{\\mathcal{A}}=1$ is necessary since the ", "page_idx": 3}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/93f33cd5cc69e980c8ae86bae2835ba935b926c4dac2eb6de31475f1a03eaa07.jpg", "img_caption": ["(a) Hyperparameter Transfer for Various $\\alpha_{\\mathcal{A}}$ (b) Attention variance across heads "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Increasing dimension-per-head $N$ with heads fixed for $\\alpha_{\\cal A}=\\{1,\\frac{1}{2}\\}$ . (a) Both $\\alpha_{\\mathcal{A}}=1$ and $\\begin{array}{r}{\\alpha_{\\mathcal{A}}=\\frac{1}{2}}\\end{array}$ exhibit similar hyperparameter transfer for vision transformers trained on CIFAR-5M over finite $\\mathbf{\\bar{\\boldsymbol{N}}}$ at $\\mathcal{H}=16$ . (b) The variance of attention variables across the different heads of a vision transformer after training for 2500 steps on CIFAR-5M. For $\\alpha_{\\mathcal{A}}=1$ the variance of attention variables decays at rate $\\mathcal{O}(N^{-\\bar{2}})$ and for $\\alpha_{\\mathcal{A}}=\\frac{1}{2}$ the variance does not decay with $N$ . ", "page_idx": 4}, {"type": "text", "text": "update to $\\pmb{k}$ is correlated to $\\pmb q$ and vice versa. However, it is possible to obtain $\\Theta(1)$ updates to the attention variable for alternative values of $\\alpha_{\\mathcal{A}}$ if we choose the change to key (also query) entries after gradient descent to be $\\delta k_{i}\\sim\\Theta(N^{-1+\\alpha_{A}})$ . We show that this scaling can approximately preserve optimal hyperparameters across $N$ in Figure 2 (a) and give similar dynamics under SGD Appendix C. However, as we show in Appendix E.1.2, any well defined $N\\rightarrow\\infty$ limit of SGD requires $\\alpha_{\\mathcal{A}}=1$ The reason is not that keys and queries become correlated, but rather that the scale of the backward pass must be controlled to ensure the dynamics remain stable (non-divergent) under SGD training. After performing two or more gradient descent steps, we demonstrate that the backpropagation signals will diverge as $N\\rightarrow\\infty$ unless initial key and query weight matrices are downscaled to have variance of order $\\Theta_{N}(1)$ . In Appendix E, we provide a DMFT analysis of the $N\\rightarrow\\infty$ limit of the transformer training dynamics. We summarize the result of that analysis informally below. ", "page_idx": 4}, {"type": "text", "text": "Result 1 (Infinite Dimension-Per-Head $N$ ) (Informal) A stable feature learning $N\\rightarrow\\infty$ limit of transformer SGD training requires taking $\\alpha_{\\mathcal{A}}=1\\,(\\mu P\\,s c a l i n g)$ , even if key/query updates are allowed to be rescaled to account for their correlation. The limiting dynamics of training are governed by the residual stream kernel $\\begin{array}{r}{H_{\\mathfrak{s}\\mathfrak{s}^{\\prime}}^{\\tilde{\\ell}}(\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime},t,t^{\\prime})=\\frac{1}{N\\mathcal{U}}h_{\\mathfrak{s}}^{\\ell}(\\mathbf{\\boldsymbol{x}},t)\\cdot\\check{\\pmb{h}}_{\\mathfrak{s}^{\\prime}}^{\\ell}(\\mathbf{\\boldsymbol{x}}^{\\prime},t^{\\prime})}\\end{array}$ and a collection of inner product kernels in each head $\\mathfrak{h}$ that concentrate as $N\\rightarrow\\infty$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})=\\displaystyle\\frac{1}{N}v_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)\\cdot v_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})\\:,\\:Q_{\\mathfrak{h}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})=\\displaystyle\\frac{1}{N}q_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)\\cdot q_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})}\\\\ &{K_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})=\\displaystyle\\frac{1}{N}k_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)\\cdot k_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})\\:,\\:A_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x,t)=\\displaystyle\\frac{1}{N}k_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)\\cdot q_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x,t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "alongside residual-stream gradient kernels and response functions in the sense of [15, 11]. The NN output logits $f({\\pmb x},t)$ evolve deterministically according to the above kernels as well as kernels for the gradient vectors $\\begin{array}{r}{{\\pmb g}^{\\ell}\\equiv\\gamma_{0}N{\\pmb\\mathcal H}\\frac{\\partial f}{\\partial{\\pmb h}^{\\ell}}}\\end{array}$ which appear in the backward pass. These variables become identical across heads such that for any h, $\\mathfrak{h^{\\prime}}\\in[\\mathcal{H}]$ $\\mathcal{C}],\\mathcal{A}_{\\mathfrak{h}^{s\\mathfrak{s}^{\\prime}}}^{\\ell}(\\pmb{x},t)=\\mathcal{A}_{\\mathfrak{h^{\\prime}}_{\\mathfrak{s}\\mathfrak{s^{\\prime}}}}^{\\ell}(\\pmb{x},t)$ . All preactivations on the residual stream and key/query/value variables within a MHSA block are statistically independent across neurons and can be described by a single scalar stochastic process ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{h_{s}^{\\ell+1}(x,t)=h_{s}^{\\ell}(x,t)+\\beta_{0}L^{-\\alpha_{L}}\\tilde{u}_{s}^{\\ell}(x,t)+\\beta_{0}L^{-\\alpha_{L}}u_{s}^{\\ell+1}(x,t)}}\\\\ {{\\mathrm{~}}}\\\\ {{\\quad\\quad+\\eta_{0}\\gamma_{0}\\beta_{0}^{2}L^{-1}\\displaystyle\\sum_{t^{\\prime}<t}\\displaystyle\\sum_{s^{\\prime}\\in\\{S\\}}\\int d x^{\\prime}\\left[\\tilde{C}_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})\\tilde{g}_{s^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})+C_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})g_{s^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})\\right]}}\\\\ {{\\mathrm{~}}}\\\\ {{k_{\\mathrm{0s}}^{\\ell}(x,t)=u_{K\\mathfrak{H s}}^{\\ell}(x,t)+\\displaystyle\\sum_{t^{\\prime}s^{\\prime}}\\int d x^{\\prime}C_{s s^{\\prime}}^{k^{\\ell}}(x,x^{\\prime},t,t^{\\prime})q_{\\mathfrak{H s}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{u}^{\\ell},u^{\\ell},u_{K\\mathfrak{h}}^{\\ell}$ are Gaussian processes with covariances $\\Phi^{\\ell,1},V^{\\ell\\sigma},H^{\\ell}$ respectively. Analogous equations hold for the queries and values. In the limit, the kernels $H_{\\mathfrak{s s^{\\prime}}}^{\\ell}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})\\ =$ $\\left\\langle h_{s}^{\\ell}({\\pmb x},t)h_{s^{\\prime}}^{\\ell}({\\pmb x}^{\\prime},t^{\\prime})\\right\\rangle$ , ${\\mathcal{A}}_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{\\ell}(\\mathbf{x},t)=\\left\\langle k_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(\\mathbf{x},t)q_{\\mathfrak{h}\\mathfrak{s^{\\prime}}}^{\\ell}(\\mathbf{x},t)\\right\\rangle$ , etc. are computed as averages $\\langle\\cdot\\rangle$ over these random variables. The deterministic kernels $C^{\\ell},\\tilde{C}^{\\ell}$ can also be expressed in terms of single site averages of residual variables and head averages of MHSA variables. The kernels $C^{\\ell},\\tilde{C}^{\\ell},C^{k^{\\ell}}$ depend on the precise mini-batches of data $\\mathfrak{B}_{t}$ presented at each step $t$ which we assume are known. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We derive this result using a Martin-Siggia-Rose path integral formalism [40] for DMFT in Appendix E. Full DMFT equations can be found in Appendix E.2. Following prior works on DMFT for infinite width feature learning, the large- $N$ limit can be straightforwardly obtained from a saddle point of the DMFT action [15, 11, 41, 17]. ", "page_idx": 5}, {"type": "text", "text": "Collapse of Attention Heads As $N\\rightarrow\\infty$ , multi-head self-attention will effectively compute the same outputs as a single-head self-attention block. We theoretically derive this effect in Appendix E.2.1 and demonstrate it empirically in Figure 2 (b). This experiment shows that in $\\alpha_{\\mathcal{A}}=1\\left(\\mu\\mathrm{P}\\right)$ transformers trained for 2500 steps on CIFAR-5M [42], the variance of attention matrices across heads decreases with $N$ . However, we note that if the scaling exponent is chosen instead as $\\begin{array}{r}{\\alpha_{\\mathcal{A}}=\\frac{1}{2}}\\end{array}$ there is non-decreasing diversity of attention variables across heads. This result is consistent with recent empirical findings that attention variables in $\\mu\\mathrm{P}$ transformers converge to the same limiting quantities at large $N$ with $\\mathcal{H}$ fixed for different initializations and also across model sizes [16]. This aspect of transformers in the large- $\\mathcal{N}$ limit is potentially undesirable as some tasks could require learning multiple attention mechanisms. Furthermore, this suggests scaling the model in this limit could increase computational cost without improving performance. To circumvent this, we explore if there exist well defined limits at finite $N$ with a diversity of attention variables across heads. ", "page_idx": 5}, {"type": "text", "text": "3.3 Scaling Number of Heads ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we take $\\mathcal{H}\\to\\infty$ with the inner dimension $N$ fixed. Rather than having all kernels concentrate, the kernel of each head of the MHSA block follows a statistically independent stochastic process. This picture was shown to hold at initialization by Hron et al. [18]. Here, using a DMFT analysis, we show that it continues to hold throughout training in the feature learning regime. ", "page_idx": 5}, {"type": "text", "text": "Result 2 (Infinite Head Limit) (Informal) The $\\mathcal{H}\\to\\infty$ limit of SGD training dynamics in a randomly initialized transformer at any key/query dimension $N$ , scaling exponents $\\alpha_{\\mathcal{A}},\\alpha_{L},$ , and any depth $L$ is governed by head-averaged kernels for pairs of input data $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}$ at training times $t,t^{\\prime}$ and spatial/token positions $\\mathfrak{s},\\mathfrak{s}^{\\prime}$ such as ", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{\\mathfrak{s s^{\\prime}}}^{\\ell,\\sigma}(\\mathbf{x},\\mathbf{x}^{\\prime},t,t^{\\prime})=\\frac{1}{N\\mathcal{H}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}v_{\\mathfrak{h}\\mathfrak{s}}^{\\ell\\sigma}(\\mathbf{x},t)\\cdot v_{\\mathfrak{h}\\mathfrak{s^{\\prime}}}^{\\ell\\sigma}(\\mathbf{x}^{\\prime},t^{\\prime})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which converge to deterministic values as $\\begin{array}{r l r}{\\mathcal{H}}&{{}\\rightarrow}&{\\infty}\\end{array}$ . The attention variables $\\{k_{\\mathfrak{h}}^{\\ell}(\\mathbf{{x},\\mathit{t}}),\\underline{{\\pmb{q}}}_{\\mathfrak{h}}^{\\ell}(\\vec{\\mathbf{x}},t),\\pmb{v}_{\\mathfrak{h}}^{\\ell}(\\mathbf{{x},\\mathit{t}}),\\mathcal{A}_{\\mathfrak{h}}^{\\ell}(\\mathbf{{x},\\mathit{t}})\\}$ within each head become statistically independent across heads and decouple in their dynamics (but not across dimensions within a head). Each residual stream neuron becomes independent and obeys a single site stochastic process analogous to Result $^{\\,l}$ , but with different kernels. ", "page_idx": 5}, {"type": "text", "text": "We derive this and the full DMFT in Appendix E.3, showing that the joint distribution of headaveraged dynamical quantities satisfies a large deviation principle and the limit can be derived as a saddle point of a DMFT action. ", "page_idx": 5}, {"type": "text", "text": "To gain intuition for this result, we first examine variables $H^{\\ell}$ and $\\mathcal{A}_{\\mathfrak{h}}^{\\ell}$ at initialization. In Figure 3, we plot the convergence of a $N=4,L=8$ vision transformer\u2019s residual stream kernel $H^{\\ell}$ to its $\\mathcal{H}\\to\\infty$ limit at rate $\\mathcal{O}(\\mathcal{H}^{-1})$ in square error, consistent with perturbative analysis near the limit [17]. Next, we plot the distribution (over heads) of $\\scriptstyle A_{\\mathfrak{h}}$ at a fixed pair of spatial/token positions for a fixed sample. This is a non-Gaussian random variable for finite $N$ , but as $N\\rightarrow\\infty$ the distribution of $\\boldsymbol{\\mathcal{A}}$ will approach a Gaussian with variance $\\Theta(N^{1-2\\alpha_{\\mathcal{A}}})$ . ", "page_idx": 5}, {"type": "text", "text": "We then investigate training dynamics as we approach the $\\mathcal{H}\\to\\infty$ limit. In Figure 4 (a) we show the test loss on CIFAR-5M as a function of the number of training iterations. The performance tends improve as $\\mathcal{H}$ increases and the model approaches its limit. In Figure 4 (b) we show that all of the models are converging in function space by measuring the squared error between finite $\\mathcal{H}$ head models and a proxy for the infinite $\\mathcal{H}$ model. Since the $\\mathcal{H}\\to\\infty$ limit is essentially uncomputable, we approximate it as the ensemble averaged predictor of the widest possible models, a technique used in prior works [16, 11]. We again see that at early time, the logits of $\\mathcal{H}$ head models converge to the limit proxy at a rate $\\mathcal{O}(\\mathcal{H}^{-1})$ , but after continued training the convergence rate weakens. This effect ", "page_idx": 5}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/d1eeacf9152f36bd8e1761f99e9ef5a5fc576727baec8432815723821e1fc26a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: The initial kernels converge as $\\mathcal{H}\\to\\infty$ and are determined by (possibly non-Gaussian) distributions of $\\mathcal{A}_{\\mathfrak{h}}^{\\ell}$ over heads in each layer. (a) Convergence of $\\begin{array}{r}{H_{\\mathfrak{s s^{\\prime}}}^{\\ell}(\\pmb{x},\\pmb{\\dot{x}}^{\\prime})=\\frac{1}{\\mathcal{H}N}\\pmb{h}_{\\mathfrak{s}}^{\\ell}(\\pmb{x})\\cdot\\pmb{h}_{\\mathfrak{s^{\\prime}}}^{\\ell}(\\pmb{x}^{\\prime})}\\end{array}$ in a $L=8,N=4$ vision transformer at initialization at rate $\\mathcal{O}(\\mathcal{H}^{-1})$ . (b) The density of $\\mathcal{A}_{\\mathfrak{h}}^{\\ell}$ entries over heads at fixed spatial location converges as $\\mathcal{H}\\to\\infty$ but is non-Gaussian for small $N$ . (c) As $N\\rightarrow\\infty$ the initial density of $\\boldsymbol{\\mathcal{A}}$ approaches a Gaussian with variance of order $\\mathcal{O}(N^{1-2\\alpha_{A}})$ . ", "page_idx": 6}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/d067d34e10cf4cd00b1b9d4d0a5414233decdbb83bf425d8dff35faa27977b8b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: Approaching the large head limit $\\mathcal{H}\\to\\infty$ in early portion of SGD dynamics for a vision transformer trained on CIFAR-5M with $(L,N)=(2,4)$ and $(\\gamma_{0},\\beta,\\alpha_{A})=(\\dot{0.}05,4,\\frac{1}{2})$ and losses averaged over 10 random inits (colored error bars are standard deviations). (a) As $\\mathcal{H}$ increases the loss and the variability over random initial seed decreases. (b) The mean square difference between output logits for $\\mathcal{H}$ head models and a proxy for the infinite head model on a held out batch of test examples. Following prior works, our proxy for the limit is the ensemble averaged outputs of the widest models [16, 11]. ", "page_idx": 6}, {"type": "text", "text": "has been observed in $\\mu\\mathrm{P}$ networks in many settings [16] and a theoretical model of this was provided in recent work which argues it arises from low-rank effects in the finite $\\mathcal{H}$ kernels [39]. ", "page_idx": 6}, {"type": "text", "text": "3.4 Infinite Depth Limits ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We next describe the infinite depth limits which depend on the choice of $\\alpha_{L}$ . Below we informally describe the main finding which again uses a DMFT formalism and is based on analyses in recent works on infinite depth networks from Bordelon et al. [11] and Yang et al. [12]. ", "page_idx": 6}, {"type": "text", "text": "Result 3 (Infinite Depth Limit) (Informal) The training dynamics for $\\mathcal{H},L\\;\\rightarrow\\;\\infty$ with $L^{-\\alpha_{L}}$ branch scaling with $\\begin{array}{r}{\\dot{\\alpha_{L}}\\in[\\frac{1}{2},1]}\\end{array}$ is described by a differential equation for residual variables $h_{\\mathfrak{s}}(\\tau,t)$ in layer time $\\begin{array}{r}{\\tau=\\operatorname*{lim}_{L\\rightarrow\\infty}\\frac{\\ell}{L}}\\end{array}$ for the residual stream ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle h_{\\mathfrak{s}}(\\tau,\\boldsymbol{x},t)=\\beta_{0}\\;\\delta_{\\alpha_{L},\\frac{1}{2}}\\int_{0}^{\\tau}d u_{\\mathfrak{s}}(\\tau^{\\prime},\\boldsymbol{x},t)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\;\\eta_{0}\\gamma_{0}\\beta_{0}^{2}\\sum_{t^{\\prime}<t}\\int d x^{\\prime}\\int_{0}^{\\tau}d\\tau^{\\prime}C_{\\mathfrak{s}\\mathfrak{s}^{\\prime}}(\\tau^{\\prime},\\boldsymbol{x},\\boldsymbol{x}^{\\prime},t,t^{\\prime})g_{\\mathfrak{s}^{\\prime}}(\\tau^{\\prime},\\boldsymbol{x}^{\\prime},t^{\\prime})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the Brownian motion term $d u_{\\mathfrak{s}}(\\tau,x,t)$ survives in the limit only if $\\begin{array}{r}{\\alpha_{L}=\\frac{1}{2}}\\end{array}$ and has covariance ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\langle d u_{s}(\\tau,x,t)d u_{s^{\\prime}}(\\tau^{\\prime},x^{\\prime},t^{\\prime})\\rangle=\\delta(\\tau-\\tau^{\\prime})d\\tau d\\tau^{\\prime}\\left[\\Phi_{s s^{\\prime}}(\\tau,x,x^{\\prime},t,t^{\\prime})+V_{s s^{\\prime}}^{\\sigma}(\\tau,x,x^{\\prime},t,t^{\\prime})\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and the deterministic kernel $C_{\\mathfrak{s s^{\\prime}}}(\\tau,\\mathbf{x},\\mathbf{x}^{\\prime},t,t^{\\prime})$ can be expressed in terms of head-averaged kernels and response functions. The weights inside each hidden MHSA layer or each MLP layer are frozen in the $L\\rightarrow\\infty$ limit unless $\\alpha_{L}=1$ . All response functions are suppressed at $L\\rightarrow\\infty$ unless $\\begin{array}{r}{\\dot{\\alpha}_{L}=\\frac{1}{2}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Below we provide a couple of short comments about this result. The proof and full DMFT is provided in Appendix E.4. ", "page_idx": 7}, {"type": "text", "text": "1. At initialization $t=0$ , the only term which contributes to the residual stream layer dynamics is the integrated Brownian motion $\\boldsymbol{\\int_{0}^{\\tau}d u(\\tau^{\\prime})}$ which survives at infinite depth for $\\begin{array}{r}{\\dot{\\alpha}_{L}=\\frac{1}{2}}\\end{array}$ . For $\\alpha_{L}=1$ this term disappears in the limit. The structure of $C(\\tau)$ is also modified by additional response functions at $\\begin{array}{r}{\\alpha_{L}=\\frac{1}{2}}\\end{array}$ [11] which we show disappear for $\\alpha_{L}=1$ .   \n2. The weights within residual blocks (including the MHSA block) can be treated as completely frozen for $\\alpha_{L}<1$ in the $L\\rightarrow\\infty$ limit, which leads to the simplified statistical description of the preactivations in those layers. However, the residual stream variables $h(\\tau)$ do still obtain $\\Theta_{L}(1)$ updates. At $\\alpha_{L}\\,=\\,1\\;$ the weights in the MHSA blocks evolve by $\\Theta_{L}(1)$ , causing additional feature evolution in the model.   \n3. A consequence of our large $N$ and large $L$ result is that the $N,L\\to\\infty$ limit with $\\begin{array}{r}{\\alpha_{L}=\\frac{1}{2}}\\end{array}$ (the parameterization studied by [11, 12]) would lead to $A_{\\mathfrak{h}^{s,s^{\\prime}}}^{\\ell}(\\mathbf{\\boldsymbol{x}},t)=0$ for all time $t$ . Thus the MHSA blocks would only involve average pooling operations over the spatial indices, despite the residual stream kernels $H^{\\ell}$ updating from feature learning. ", "page_idx": 7}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/f4187fab262bd03ca4eb890101cba1eac8bdcdf5861bea82d04ac2bb72d3c86f.jpg", "img_caption": ["Figure 5: Depth scaling in a vision transformer on CIFAR-5M with $\\begin{array}{r}{\\alpha_{L}\\in\\{\\frac{1}{2},1\\}}\\end{array}$ . (a) The key and query weights move by $1/\\sqrt{L}$ . (b) The compute scaling laws with models at fixed width $N,\\mathcal{H}$ and varying depth $L$ . At large $L$ , the $\\alpha_{L}=1$ (dashed) models perform better at fixed compute. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "First, we note in Figure 5 that the weights within each attention block freeze as $L\\rightarrow\\infty$ with $\\begin{array}{r}{\\alpha_{L}=\\frac{1}{2}}\\end{array}$ case but move at a constant scale for $\\alpha_{L}=1$ . As a consequence, the loss at large $L$ can be lower in the $\\alpha=1$ parameterization. ", "page_idx": 7}, {"type": "text", "text": "We can see some numerical evidence for the first of these effects in Figure 6 (a)-(b) where initially training at large $L$ is slower than the base model and the initial kernel appears quite different for $L\\,=\\,4$ and $L\\,=\\,64$ . The initial kernel will decrease in scale as $L\\to\\infty$ for $\\alpha_{L}\\,=\\,1$ since the preactivation vectors lose variance as we discuss in Appendix E.4, resulting in slower initial training. However, we note that the final learned feature kernels are quite similar after enough training. ", "page_idx": 7}, {"type": "text", "text": "In summary, our results indicate that the $\\alpha_{L}=1$ parameterization is the one that allows attention layers to actually be learned in the limit $L\\to\\infty$ , but that this parameterization leads to a less structured kernel at initialization. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments in Realistic Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In practice, large scale neural networks do not generally operate close to their limit. Given the costs of training large networks, one would ideally operate in a regime where there is a guarantee of consistent improvements with respect to model scale. In pursuit of this goal, we apply our theoretical findings of this paper to training language models on a larger natural language dataset, a Transformer with causal attention blocks trained on the C4 dataset [43] with Adam optimizer. As mentioned in 2.2, while our exact theoretical description of these infinite limits focus on SGD, we can implement an appropriate scaling for Adam which preserves the scale of internal feature updates. This allows us ", "page_idx": 7}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/0eb0f6e744c133f18fc874b5222140f1a1b0732a1d0030436fdf641c5b275ee5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: Initial and final representations are converging as model scale increases after one pass of training on the full CIFAR-5M with $\\mathrm{SGD+}$ momentum. The base model is a $(N,\\mathcal{H},L)=(16,16,4)$ and $\\left(\\alpha_{\\cal A},\\alpha_{\\cal L},\\beta_{0},\\gamma_{0}\\right)=\\left(1,1,4,0.1\\right)$ . (a) The test loss dynamics for one pass through CIFAR-5M. The dynamics are very similar across different head-counts $\\mathcal{H}$ but the early dynamics are changed for large depth $L$ , consistent with our theory. (b) The initial and final feature kernels after spatial pooling at the last layer of the residual stream. The initial kernel at large $L$ is quite different for $\\alpha_{\\mathcal{A}}=1$ due to suppression of Brownian motion on the forward pass, which we explain in Section 3.4. (c) The residual stream kernel across pairs of spatial positions for a single randomly chosen input sample. (d) The distribution of attention entries across heads at a fixed pair of spatial locations and data point. The initial variance of $\\boldsymbol{\\mathcal{A}}$ decreases for $\\alpha_{\\mathcal{A}}=1$ but the update is roughly consistent across $N$ . For $\\begin{array}{r}{\\alpha_{\\mathcal{A}}=\\frac{1}{2}}\\end{array}$ both initial and final distributions for $\\mathcal{A}_{\\mathfrak{h}}$ are consistent across $N$ . ", "page_idx": 8}, {"type": "text", "text": "to investigate realistic training dynamics of our LLM as we take the $N,L,\\mathcal{H}\\to\\infty$ limits. Training details are provided in Appendix F ", "page_idx": 8}, {"type": "text", "text": "In Figure 7 (a), we sweep over each of the model dimensions independently for each parameterization of $\\alpha\\bar{\\cal A}\\in\\lbrace1,\\frac{1}{2}\\rbrace$ on the left and right respectively. For fixed $N$ and $L$ , scaling $\\mathcal{H}$ provides a similar increase in performance in both parameterization and appear to start converging to a final loss around 5, with slight benefit to $\\alpha_{\\cal A}\\,=\\,\\frac{1}{2}$ . For fixed $\\mathcal{H}$ and $L$ , scaling $N$ provides a similar increase in performance to scaling heads in when $\\alpha_{\\mathcal{A}}=1$ , but a substantial increase when $\\begin{array}{r}{\\alpha_{\\mathcal{A}}=\\frac{1}{2}}\\end{array}$ . This is in line with our predictions in Section 3.2 about the benefits of diversity across attention heads. Next, for fixed $N$ and $\\mathcal{H}$ , scaling $L$ provides little to no benefit in either parameterization as predicted in Section 3.4. Finally, we inspect the sample and spatial residual stream kernels of these models before and after training and find that the kernels are identical for both $\\alpha_{\\mathcal{A}}$ , except for a slight difference for large $N$ . Furthermore, they are extremely similar for large $N$ and large $\\mathcal{H}$ . ", "page_idx": 8}, {"type": "text", "text": "Taken together, these results suggest that scaling different model dimensions do indeed have different effects on training dynamics and final performance. This provides groundwork for future large-scale experiments systematically investigating their trade-offs, thereby identifying compute-optimal scaling of realistic architectures in parameterizations with well-defined limits. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper provided analysis of the infinite head, depth and key/query dimension limits of transformer training in the feature learning regime. We showed that feature learning in $\\mu\\mathrm{P}$ multi-head transformers in the limit of $N\\rightarrow\\infty$ collapses to single-head self-attention. At finite $N$ and infinite heads $\\mathcal{H}\\to\\infty$ ", "page_idx": 8}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/a4ff0d9498ffb8f4f60f3d676808a2789de30c771989fa6271cdd7d785fcb0d4.jpg", "img_caption": ["(a) Training dynamics of LLM on C4 "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/7ad3762dc1533327bb9e95936be5e258c355c7d651bffe49dfe859b4252367de.jpg", "img_caption": ["(c) Kernels: tokens within single sample "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 7: Training dynamics and initial/final representations of decoder only language models trained on C4 converge with increasing model scale. The base model has $(N,\\mathcal{H},L)=(8,8,4)$ and $(\\alpha_{L},\\beta_{0},\\gamma_{0})=(1,4,\\bar{0}.25)$ and $\\alpha_{\\mathcal{A}}\\in\\left\\lbrace1,\\frac{1}{2}\\right\\rbrace$ . (a) Train loss dynamics after 10000 steps on C4 using Adam optimizer. The dynamics improve consistently when scaling $\\mathcal{H}$ for both values of $\\alpha_{\\mathcal{A}}$ , with slight benefit to \u03b1A = 12 . Scaling $N$ reveals a significant advantage to setting $\\begin{array}{r}{\\alpha_{{\\mathcal{A}}}=\\frac{1}{2}}\\end{array}$ . Scaling $L$ provides little improvement for either parameterization of $\\alpha_{\\mathcal{A}}$ . (b) Initial and final residual stream kernels for the final token across samples for Base, $\\mathcal{H}=128$ , $N=128$ , and $L=64$ models. The first row is at initialization. The second and third rows are after training with $\\alpha_{\\mathcal{A}}\\in\\left\\{1,\\frac{1}{2}\\right\\}$ respectively. (c) Initial and final feature kernels across pairs of tokens for a single randomly chosen input sample. Note both types of kernels are identical across $\\alpha_{\\mathcal{A}}$ except for a slight difference at large $N$ . ", "page_idx": 9}, {"type": "text", "text": "we showed that there is an alternative limit which maintains a distribution over attention heads. We discussed two different large depth limits of transformer training that reduce to differential equations in the residual layer time $\\tau$ . The depth scaling that maintains feature learning within all MHSA blocks $\\left[\\alpha_{L}\\right.=1]$ ) causes the initial kernel to lose structure from the initialization as $L\\to\\infty$ , but allows learning of the self-attention variables, whereas the depth scaling that preserves structure from initialization $\\begin{array}{r}{\\breve{(\\alpha_{L}=\\frac{1}{2})}}\\end{array}$ leads to static layers. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Directions Currently exact theoretical analysis of the limit is focused on SGD (and can be easily extended to SGD+momentum [15]) while Adam is currently only reasoned with rough scaling arguments rather than an exact theoretical description of the limit. Since Adam is most commonly used to train transformers, a theory of the limiting dynamics of Adam in Transformers would be an important future extension. In addition, while we provide an exact asymptotic description of network training, the limiting equations are compute intensive for realistic settings which is why we focus our empirical investigations on training large width networks in the appropriate parameterizations. Lastly our techniques assume that the number of training steps is fixed as the scaling parameters of interest $(N,\\mathcal{H},L)$ are taken to infinity. However, it would be important to understand learning dynamics in the regime where model size and training times are chosen to balance a compute optimal tradeoff (or perhaps even training longer than compute optimal) [8, 39, 44]. In this regime, harmful finite model-size effects become significant and comparable to the finite training horizon [10, 16, 17, 39]. Thus stress testing the ideas in this work at larger scales and longer training runs would be an important future direction of research into scaling transformer models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "BB would like to thank Alex Atanasov, Jacob Zavatone-Veth, Lorenzo Noci, Mufan Bill Li, Boris Hanin, Alex Damian, Eshaan Nichani for inspiring conversations. We would also like to thank Alex Atanasov and Jacob Zavatone-Veth for useful comments on an earlier version of this manuscript. BB is supported by a Google PhD fellowship. HC was supported by the GFSD Fellowship, Harvard GSAS Prize Fellowship, and Harvard James Mills Peirce Fellowship. CP was supported by NSF Award DMS2134157 and NSF CAREER Award IIS2239780. CP is further supported by a Sloan Research Fellowship. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. The computations in this paper were run on the FASRC Cannon cluster supported by the FAS Division of Science Research Computing Group at Harvard University. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[2] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. IEEE transactions on pattern analysis and machine intelligence, 45(1):87\u2013110, 2022.   \n[3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[4] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A jax library for computer vision research and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 21393\u201321398, 2022.   \n[5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[6] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[8] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[9] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[10] Ge Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter transfer. Advances in Neural Information Processing Systems, 34:17084\u201317097, 2021.   \n[11] Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=KZJehvRKGD.   \n[12] Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Feature learning in infinite depth neural networks. In The Twelfth International Conference on Learning Representations, 2023.   \n[13] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pages 2388\u20132464. PMLR, 2019.   \n[14] Greg Yang and Edward J Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In International Conference on Machine Learning, pages 11727\u201311737. PMLR, 2021.   \n[15] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution in wide neural networks. Advances in Neural Information Processing Systems, 35:32240\u201332256, 2022.   \n[16] Nikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, and Cengiz Pehlevan. Feature-learning networks are consistent across widths at realistic scales, 2023.   \n[17] Blake Bordelon and Cengiz Pehlevan. Dynamics of finite width kernel and prediction fluctuations in mean field neural networks. arXiv preprint arXiv:2304.03408, 2023.   \n[18] Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and ntk for deep attention networks. In International Conference on Machine Learning, pages 4376\u20134386. PMLR, 2020.   \n[19] Emily Dinan, Sho Yaida, and Susan Zhang. Effective theory of transformers at initialization, 2023.   \n[20] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning, pages 2793\u20132803. PMLR, 2021.   \n[21] Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198\u201327211, 2022.   \n[22] Bobby He and Thomas Hofmann. Simplifying transformer blocks. arXiv preprint arXiv:2311.01906, 2023.   \n[23] Aditya Cowsik, Tamra Nebabu, Xiao-Liang Qi, and Surya Ganguli. Geometric dynamics of signal propagation predict trainability of transformers, 2024.   \n[24] Lorenzo Noci, Chuning Li, Mufan Li, Bobby He, Thomas Hofmann, Chris J Maddison, and Dan Roy. The shaped transformer: Attention models in the infinite depth-and-width limit. Advances in Neural Information Processing Systems, 36, 2024.   \n[25] Soufiane Hayou. On the infinite-depth limit of finite-width neural networks. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/ forum?id $\\equiv$ RbLsYz1Az9.   \n[26] Nicola Muca Cirone, Maud Lemercier, and Cristopher Salvi. Neural signature kernels as infinite-width-depth-limits of controlled resnets. arXiv preprint arXiv:2303.17671, 2023.   \n[27] L\u00e9na\u00efc Chizat and Praneeth Netrapalli. The feature speed formula: a flexible approach to scale hyper-parameters of deep neural networks, 2024. URL https://arxiv.org/abs/2311. 18718.   \n[28] Jeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu. On the distance between two neural networks and the stability of learning. Advances in Neural Information Processing Systems, 33:21370\u201321381, 2020.   \n[29] Greg Yang, James B Simon, and Jeremy Bernstein. A spectral condition for feature learning. arXiv preprint arXiv:2310.17813, 2023.   \n[30] Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, and Yisong Yue. Automatic gradient descent: Deep learning without hyperparameters. arXiv preprint arXiv:2304.05187, 2023.   \n[31] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. Advances in neural information processing systems, 31, 2018.   \n[32] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \n[33] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. Advances in neural information processing systems, 32, 2019.   \n[34] Haim Sompolinsky and Annette Zippelius. Dynamic theory of the spin-glass phase. Physical Review Letters, 47(5):359, 1981.   \n[35] Moritz Helias and David Dahmen. Statistical field theory for neural networks, volume 970. Springer, 2020.   \n[36] Stefano Sarao Mannelli, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborova. Passed & spurious: Descent algorithms and local minima in spiked matrix-tensor models. In international conference on machine learning, pages 4333\u20134342. PMLR, 2019.   \n[37] Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborov\u00e1. Dynamical mean-field theory for stochastic gradient descent in gaussian mixture classification. Advances in Neural Information Processing Systems, 33:9540\u20139550, 2020.   \n[38] Cedric Gerbelot, Emanuele Troiani, Francesca Mignacco, Florent Krzakala, and Lenka Zdeborova. Rigorous dynamical mean field theory for stochastic gradient descent methods. arXiv preprint arXiv:2210.06591, 2022.   \n[39] Blake Bordelon, Alexander Atanasov, and Cengiz Pehlevan. A dynamical model of neural scaling laws, 2024.   \n[40] Paul Cecil Martin, ED Siggia, and HA Rose. Statistical dynamics of classical systems. Physical Review A, 8(1):423, 1973.   \n[41] Blake Bordelon and Cengiz Pehlevan. The influence of learning rule on representation dynamics in wide neural networks. arXiv preprint arXiv:2210.02157, 2022.   \n[42] Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good online learners are good offline generalizers. arXiv preprint arXiv:2010.08127, 2020.   \n[43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[44] Ibrahim M Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. Getting vit in shape: Scaling laws for compute-optimal model design. Advances in Neural Information Processing Systems, 36, 2024.   \n[45] Etai Littwin and Greg Yang. Adaptive optimization in the $\\infty$ -width limit. In The Eleventh International Conference on Learning Representations, 2022.   \n[46] Mufan Li, Mihai Nica, and Dan Roy. The neural covariance sde: Shaped infinite depth-andwidth networks at initialization. Advances in Neural Information Processing Systems, 35: 10795\u201310808, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Additional Figures ", "page_idx": 13}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/fdadef9c5e635c359ebbf87c25525830276b32d2c20c371a9004db45c65fa99d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 8: One pass training on CIFAR-5M with vision transformers with the setting of Figure 6. ", "page_idx": 13}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/82fccbaeca53a662d931b794d5af0b0030b2657fe725ce1b78844ae4290c7ebd.jpg", "img_caption": ["Figure 9: Examples of initial and learned kernels in final residual stream layer with various extrapolations of a base vision transformer model with $(\\mathcal{H},N,L)=(16,16,4)$ trained on CIFAR-5M. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/0abd27a03f9021da511817710e7957511c8f606727d7cd27cf36b93837c0b77b.jpg", "img_caption": ["Figure 10: Spatial kernels for a single test point before and after training across $\\mathcal{H},N,L$ values. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/fa501d3529cc022490baaebe98c31789f35b3ca2ef6c1f59ac8713df9ce43e99.jpg", "img_caption": ["Figure 11: Early training dynamics on CIFAR-5M in vision transformer with different dimensionper-head $N$ with heads fixed at $\\mathcal{H}=4$ for $\\alpha_{\\cal A}=\\{1,{\\textstyle{\\frac{1}{2}}}\\}$ . "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/33657e74853c87b3a47d562c5d2c2e9cb40cd27d69285fd1710ef2587984dc99.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 12: Performance of language models trained on C4 in main text Figure 7(a) as a function of compute, estimated as ${\\mathrm{FLOPs}}=6\\times{\\mathrm{Param}}$ s. The base model has size $(N,\\mathcal{H},L)=(8,8,4)$ and we examine scaling up $N,\\mathcal{H},L$ with either $\\alpha_{A}=1/2$ or $\\alpha_{\\mathcal{A}}=1$ . The $\\alpha_{\\mathcal{A}}=1$ models perform better at fixed compute for either $N$ or $\\mathcal{H}$ scaling. Increasing $L$ does not significantly increase compute in this regime since the embedding and decoding layers contribute most of the parameters. ", "page_idx": 15}, {"type": "text", "text": "B Implementations for Vision and Causal Language Modeling Transformers ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide an example FLAX implementation of the vision transformer and causal language model. We start by defining a fixed layernorm operation ", "page_idx": 16}, {"type": "text", "text": "from flax import linen as nn   \n2 import jax.numpy as jnp   \n3   \n4   \n5 class LN_Fixed(nn.Module):   \n6   \n7 eps: jnp.float32 = 1.0e-6   \n8 @nn.compact   \n9   \n10 def __call__(self , x):   \n2 features $=\\texttt{x}$ .shape [-1] # number of features   \n3 mean $=$ jnp.mean( x , axis $=$ -1 ) # mean of x   \n4 var $=$ jnp.var( x , axis $=$ -1 ) # var of x   \n5 out $=$ (x - mean[:,:,jnp.newaxis] ) / jnp.sqrt( var[:,:,jnp.   \nnewaxis] $^+$ self.eps )   \nreturn out ", "page_idx": 16}, {"type": "text", "text": "The MHSA layer is implemented as the following where scale_exp represents $\\alpha_{\\mathcal{A}}$ ", "page_idx": 16}, {"type": "text", "text": "1 # MHSA attention layer   \n2 from einops import rearrange class Attention(nn.Module): \"\"\" Multi -head Self -Attention Layer \"\"\"   \n6 scale_exp: jnp.float32 dim: int heads: int def setup(self):   \n2 self.c $=$ 1.5 - self.scale_exp # exponent for the scale factor kif_qk $=$ nn.initializers.normal(stddev $=$ self.dim $^{\\ast\\ast}$ ( self.c - 0.5) ) # possible scaling with N kif_v $=$ nn.initializers .normal(stddev $=$ 1.0 ) # O_N (1) entries # computes key , query , value self.qk_layer $=$ nn.Dense(features $=$ 2 $^*$ self.heads $^*$ self.dim , kernel_init $=$ kif_qk , use_bias $=$ False) self.v_layer $=$ nn.Dense(features $=$ self.heads $^\\ast$ self.dim , kernel_init $=$ kif_v , use_bias $=$ False) self.out_layer $=$ nn.Dense(features $=$ self.heads $^*$ self.dim , kernel_init $=$ kif_v , use_bias $=$ False) return def __call__(self ,inputs):   \n2 qk = self.qk_layer(inputs) / self.heads $^{\\ast\\ast}$ (0.5) / self.dim \\*\\*( self.c) qk $=$ rearrange( qk , \u2019b l (h d) -> b h l d\u2019 , h $=$ self.heads) # (batch , heads , loc , d ) q,k = jnp.split(qk , 2, axis $=$ -1) # gives q, k each of shape batch , heads , loc , d )   \n6 $\\texttt{v}=$ self.v_layer(inputs) / jnp.sqrt( inputs.shape [-1] )   \n8 $\\texttt{v}=$ rearrange(v, \u2019b l (h d) -> b h l d\u2019, h $=$ self.heads)   \n9 A $=$ self.dim\\*\\*(- self.scale_exp) \\* jnp.einsum(\u2019ijkl ,ijml ->ijkm \u2019 q, k) # batch x heads x loc x loc sigma_A $=$ softmax( A, axis=-1 ) ", "page_idx": 16}, {"type": "text", "text": "out $=$ jnp.einsum(\u2019ijkl ,ijlm ->ijkm \u2019, sigma_A , v) # (batch , head loc , d) out $=$ rearrange(out , \u2019b h l d -> b l (h d)\u2019) out $=$ self.out_layer(out) / jnp.sqrt( out.shape [-1] ) return out ", "page_idx": 17}, {"type": "text", "text": "32   \n33   \n34 ", "page_idx": 17}, {"type": "text", "text": "The two layer MLP block is implemented as the following with $\\phi=$ gelu nonlinearity. ", "page_idx": 17}, {"type": "text", "text": "class MLP_Block(nn.Module): \"\"\" Two Layer MLP Block \"\"\" features: int @nn.compact def __call__(self ,x): $\\texttt{N}=$ self.features kif $=$ nn.initializers .normal(stddev $\\mathbf{\\Sigma}=\\mathbf{\\Sigma}\\,\\mathbf{1}\\cdot0\\,;$ ) $\\#$ O_N (1) entries h $=$ nn.Dense(features $\\mathbf{\\Sigma}=\\mathbf{\\Sigma}\\mathbb{N}$ , kernel_init $=$ kif , use_bias $=$ False )(x) / jnp.sqrt(N) h = nn.gelu(h) h $=$ nn.Dense(features $\\mathbf{\\Sigma}=\\mathbf{\\Sigma}\\mathbb{N}$ , kernel_init $=$ kif , use_bias $=$ False )(h) / jnp.sqrt(N) return h ", "page_idx": 17}, {"type": "text", "text": "We also allow for a trainable positional encoding matrix. ", "page_idx": 17}, {"type": "text", "text": "class PositionalEncoding (nn.Module): \"\"\" Trainable Positional Encoding \"\"\" d_model : int # Hidden dimensionality of the input. max_len : int # Maximum length of a sequence to expect. scale: jnp.float32 # scale parameter for initialization def setup(self): # Create matrix of [SeqLen , HiddenDim] representing the positional encoding for max_len inputs self.pos_embedding $=$ self.param(\u2019pos_embedding \u2019, nn. initializers .normal(stddev $=$ self.scale), (1, $^{1+}$ self.max_len , self. d_model)) def __call__(self , x, train $=$ True): B,T,_ $=$ x.shape $\\texttt{x}=\\texttt{x}$ + self.pos_embedding [:,:T] / self.scale return x   \nEach residual block is implemented as the following. Below we show the $\\alpha_{L}=1$ implementation.   \n# Residual Block   \nclass ResidBlock(nn.Module): dim: int heads: int features: int L: int scale_exp: jnp.float32 = 1.0 beta: jnp.float32 = 4.0 @nn.compact def __call__(self ,x): $\\textrm{\\textbf{h}}=$ LN_Fixed ()(x) $\\textrm{\\textbf{h}}=$ Attention(dim $=$ self.dim , scale_exp $=$ self.scale_exp , heads $=$ self.heads)( h ) x = x + self.beta / self.L \\* h h = LN_Fixed ()(x) ", "page_idx": 17}, {"type": "text", "text": "h = MLP_Block(features $=$ self.features)(h) $\\textbf{\\textit{x}}=\\textbf{\\textit{x}}+$ self.beta / self.L $^\\ast$ h return x ", "page_idx": 18}, {"type": "text", "text": "17   \n18   \n19 ", "page_idx": 18}, {"type": "text", "text": "Our vision transformer model consists of an embedding layer which is applied to each patch, a positional encoding layer, $L$ residual layers each containing a MHSA and MLP block, a spatial pooling operation, and a readout. ", "page_idx": 18}, {"type": "text", "text": "class VIT(nn.Module): \"simple VIT model with \" dim: int heads: int depth: int patch_size: int scale_exp: jnp.float32 = 1.0 adam_scale: int $=\\ \\ 0\\ .\\ 0$ beta: jnp.float32 = 4.0 @nn.compact def __call__(self , x): d_model $=$ self.heads $^\\ast$ self.dim L $=$ self.depth $\\mathrm{~\\textbf~{~D~}~}=\\mathrm{~\\textbf~{~3~}~}$ # patchify images x $=$ rearrange(x, \u2019b (w p1) (h p2) c -> b (w h) (p1 p2 c)\u2019, p1 $=$ self.patch_size , p2 $=$ self.patch_size) # (batch , loc , patch_ch_dim ) kif_first $=$ nn.initializers .normal(stddev $=$ d_model \\*\\*( -0.5\\* self .adam_scale) $^\\ast$ (L/self.beta) $^{\\ast\\ast}$ (0.5 $^*$ (1.0 - self.adam_scale)) ) # O_N (1) entries kif $=$ nn.initializers .normal( stddev $=$ 1.0 ) # O_N (1) entries kif_last $=$ nn.initializers .normal(stddev $=$ (L/self.beta) $^{\\ast\\ast}$ (0.5 \\* (1-self.adam_scale) ) ) # read -in weights x $=$ (L/self.beta)\\*\\*( -0.5 \\* (1.0 - self.adam_scale))\\*d_model \\*\\*(0.5 \\* self.adam_scale) $^\\ast$ nn.Dense(features $=$ N, kernel_init $=$ kif_first , use_bias $=$ False)(x) / jnp.sqrt( D $^\\ast$ self.patch_size $**2$ ) # positional encoding ${\\tt x}=$ PositionalEncoding (d_model $=$ d_model , max_len $=$ (32// self. patch_size) $^{\\ast\\ast2}$ , scale $=$ d_model $^{\\ast\\ast}$ ( -0. $^{5\\ast}$ self.adam_scale) $^*$ (L/self. beta) $^{\\ast\\ast}$ (0.5 $^*$ (1.0- self.adam_scale)))(x) # residual stream with pre -LN for l in range(self.depth): ${\\tt x}=$ ResidBlock(dim $=$ self.dim , heads $=$ self.heads , scale_exp $=$ self.scale_exp , features $=$ d_model , bet $\\mathtt{a}=\\mathtt{s}$ elf.beta , L = L)(x) # last norm layer x = LN_Fixed ()(x) # pool over spatial dimension $\\texttt{x}=\\texttt{x}$ .mean(axis $\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\quad1$ ) # (batch , d_model) x = (L/self.beta) $^{\\ast\\ast}$ ( -0. $^{5\\ast}$ (1 - self.adam_scale)) $^\\ast$ nn.Dense( features $=~10$ , use_bias $=$ False , kernel_init $=$ kif_last)(x) / ", "page_idx": 18}, {"type": "text", "text": "d_model $^{\\ast\\ast}$ (1.0 -0.5\\* self.adam_scale) # for mean field scaling return x ", "page_idx": 18}, {"type": "text", "text": "For the causal decoder only model, we need to modify the Attention layer and also prevent pooling over spatial indices before the readout. ", "page_idx": 19}, {"type": "text", "text": "class Causal_Attention (nn.Module): scale_exp: jnp.float32 dim: int heads: int qk_ln: bool $=$ True def setup(self): self.c $=$ 1.5 - self.scale_exp # exponent for the scale factor kif_qk $=$ nn.initializers.normal(stddev $=$ self.dim $^{\\ast\\ast}$ ( self.c - 0.5) ) # possibly needs to be scaled with N kif_v $=$ nn.initializers .normal(stddev $=$ 1.0 ) # O_N (1) entries # computes key , query , value self.qk_layer $=$ nn.Dense(features $=$ 2 $^*$ self.heads $^*$ self.dim , kernel_init $=$ kif_qk , use_bias $=$ False) self.v_layer $=$ nn.Dense(features $=$ self.heads $^\\ast$ self.dim , kernel_init $=$ kif_v , use_bias $=$ False) self.out_layer $=$ nn.Dense(features $=$ self.heads $^*$ self.dim , kernel_init $=$ kif_v , use_bias $=$ False) return def __call__(self ,inputs): qk $=$ self.qk_layer(inputs) / self.heads \\*\\*(0.5) / self.dim \\*\\*( self.c) # (batch , loc , 3\\*h\\*d) qk $=$ rearrange( qk , \u2019b l (h d) -> b h l d\u2019 , h $=$ self.heads) # (batch , heads , loc , d ) q,k = jnp.split(qk , 2, axis $=$ -1) # gives q, k each of shape ( batch , heads , loc , d ) v = self.v_layer(inputs) / jnp.sqrt( inputs.shape [-1] ) v = rearrange(v, \u2019b l (h d) -> b h l d\u2019, h $=$ self.heads) A = 1.0/ self.dim $^{\\ast\\ast}$ ( self.scale_exp) $^\\ast$ jnp.einsum(\u2019ijkl ,ijml -> ijkm \u2019, q, k) # batch x heads x loc x loc exp_A $=$ jnp.einsum(\u2019ijkl ,kl ->ijkl \u2019, jnp.exp(A), jnp.tril(jnp. ones ((v.shape [2], v.shape [2])))) phi_A $=$ exp_A / exp_A.sum(axis $=$ -1)[:,:,:,jnp.newaxis] out $=$ jnp.einsum(\u2019ijkl ,ijlm ->ijkm \u2019, phi_A , v) # (batch , head , loc , d) out $=$ rearrange(out , \u2019b h l d -> b l (h d)\u2019) out $=$ self.out_layer(out) / jnp.sqrt( out.shape [-1] ) return out   \nclass LM_Transformer(nn.Module): \"\"\"A simple Decoder only transformer \"\"\" dim: int heads: int depth: int scale_exp: jnp.float32 adam_scale: int beta: jnp.float32 VOCAB_SIZE: int @nn.compact def __call_ _(self , x, train $=$ True): d_model $=$ self.heads $^\\ast$ self.dim L $=$ self.depth kif_first $=$ nn.initializers.normal(stddev $=$ d_model \\*\\*( -0.5\\*   \nself.adam_scale) $^\\ast$ (L/self.beta) $^{\\ast\\ast}$ (0.5 $^\\ast$ (1-self.adam_scale) ) ) # O(1) entries kif0 $=$ nn.initializers.normal(stddev $=\\ \\ 0\\ .\\ 0$ ) kif $=$ nn.initializers .normal(stddev $=$ 1.0) # O(1) entries kif_last $=$ nn.initializers .normal(stddev $=$ (L/self.beta)\\*\\*(0.5 $^\\ast$ (1-self.adam_scale)) $^*$ d_model $^{\\ast\\ast}$ ( -0. $^{5*}$ self.adam_scale) ) # embed the batch x sequence integers to x = (L/self.beta)\\*\\*( -0.5 \\* (1-self.adam_scale) )\\* d_model   \n\\*\\*(0.5 \\* self.adam_scale) $^\\ast$ nn.Embed(self.VOCAB_SIZE , d_model ,   \nembedding_init $=$ kif_first)(x) # batch ${\\tt x}$ seq len x N ${\\tt x}=$ PositionalEncoding (d_model $=$ d_model , scale $=$ d_model   \n\\*\\*( -0.5\\* self.adam_scale) $^*$ (L/self.beta) $^{\\ast\\ast}$ (0.5 $^\\ast$ (1- self.adam_scale   \n)) )(x) for l in range(self.depth): h = LN_Fixed ()(x) $\\textbf{\\textit{x}}=\\textbf{\\textit{x}}+$ self.beta/L $^\\ast$ Causal_Attention (dim $=$ self.dim ,   \nscale_exp $=$ self.scale_exp , heads $=$ self.heads)(h) h $=$ LN_Fixed ()(x) x = x + self.beta/L $^\\ast$ MLP_Block(features $=$ d_model)(h) x = LN_Fixed ()(x) x = (L/self.beta)\\*\\*( -0.5 \\* (1 - self.adam_scale ) ) $^\\ast$ nn.Dense   \n(features $=$ self.VOCAB_SIZE , use_bias $=$ True , kernel_init $=$ kif0)(   \n$\\begin{array}{r l r}{\\mathrm{~x~})}&{{}/}&{\\mathrm{~d~\\_mo~d~e~}1**\\left(\\mathrm{~1~.~}0\\mathrm{~-~}0\\mathrm{~.~}5*\\mathrm{~s~e~}1\\mathrm{~f~}\\cdot\\hat{\\mathrm{~}}0\\right)}\\end{array}$ adam_scale) # for mean field scaling return x ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "C Simple Heuristic Scaling Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we heuristically work out the simple scaling analysis to justify the set of parameterizations and learning rates we consider. More detailed theoretical analysis for the limit of SGD training is provided in Appendix E where we exactly characterize the $N\\rightarrow\\infty$ , $\\mathcal{H}\\to\\infty$ and $L\\rightarrow\\infty$ limits. We consider taking heads $\\mathcal{H}$ , inner dimension $N$ and depth $L$ to infinity separately and attempt to control the scale of gradients and updates. ", "page_idx": 20}, {"type": "text", "text": "C.1 Learning Rate Scalings ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We show that the correct learning rate scaling for SGD is $\\eta=\\eta_{0}N\\mathcal{H}L^{2\\alpha_{L}-1}$ . For Adam, the learning rate should be scaled as \u03b7 = \u03b70N \u22121/2H\u22121/2L\u22121+\u03b1L. ", "page_idx": 20}, {"type": "table", "img_path": "p0BBKhD5aI/tmp/3e862c16b8904e6fe891b1c8cc00f65016d3d6ef36bb3b580094d18d4feaff2f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 2: The learning rates which should be applied to obtain the correct scale of updates for SGD or Adam optimizers. In addition, the weight variance and multiplier for the first layer may need to be rescaled with depth depending on the parameterization and optimizer. ", "page_idx": 20}, {"type": "text", "text": "C.2 Heuristic Analysis of Feature Changes Under SGD ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section we consider performing a single update on a single example to all weight matrices. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta W_{O\\mathfrak{h}}^{\\ell}\\sim\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}g^{\\ell+1}v_{\\mathfrak{h}}^{\\ell\\top}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\pmb{g}^{\\ell+1}\\in\\mathbb{R}^{N\\mathcal{H}}$ and $\\pmb{v}_{\\mathfrak{h}}^{\\ell}\\in\\mathbb{R}^{N}$ have $\\Theta(1)$ entries. Thus, computing a perturbation to the forward pass we find ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\delta h^{\\ell+1}=\\delta h^{\\ell}+\\frac{1}{L^{\\alpha_{L}}\\sqrt{N\\mathcal{H}}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}\\left(\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}g^{\\ell+1}v_{\\mathfrak{h}}^{\\ell\\top}\\right)v_{\\mathfrak{h}}^{\\ell}}}\\\\ {{\\displaystyle=\\delta h^{\\ell}+\\frac{1}{L}\\left[\\frac{1}{N\\mathcal{H}}\\sum_{\\mathfrak{h}}v_{\\mathfrak{h}}^{\\ell}\\cdot v_{\\mathfrak{h}}^{\\ell}\\right]g^{\\ell+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The term in the brackets is $\\Theta(1)$ and we see that the perturbation from each layer contributes $\\Theta(L^{-1})$ .   \nAs there are $L$ layers, this will give a total change to the final layer $h^{L}$ that is $\\Theta(1)$ . ", "page_idx": 21}, {"type": "text", "text": "For the Attention variables, we note that the ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\delta W_{K\\mathfrak{h}}^{\\ell}\\sim\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}q_{\\mathfrak{h}}^{\\ell}h^{\\ell^{\\top}}\\;,\\;\\delta W_{Q\\mathfrak{h}}^{\\ell}\\sim\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}k_{\\mathfrak{h}}^{\\ell}h^{\\ell^{\\top}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ${\\bf g}_{\\mathfrak{h}},{\\pmb k}_{\\mathfrak{h}}^{\\ell}\\in\\mathbb{R}^{N}$ are the query and key for head $\\mathfrak{h}$ and $\\pmb{h}\\in\\mathbb{R}^{N\\mathcal{H}}$ is the residual stream preactivation. We can thus compute the changes to the keys and queries due to changes in their associated weights ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta k_{\\mathfrak{h}}^{\\ell}=\\frac{1}{N^{\\frac{3}{2}-\\alpha_{A}}\\sqrt{\\mathcal{H}}}\\left(\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}q_{\\mathfrak{h}}^{\\ell}h^{\\ell^{\\top}}\\right)h^{\\ell}=\\frac{1}{L^{1-\\alpha_{L}}N^{1-\\alpha_{A}}}q_{\\mathfrak{h}}^{\\ell}H^{\\ell}}\\\\ &{\\delta q_{\\mathfrak{h}}^{\\ell}=\\frac{1}{N^{\\frac{3}{2}-\\alpha_{A}}\\sqrt{\\mathcal{H}}}\\left(\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}k_{\\mathfrak{h}}^{\\ell}h^{\\ell^{\\top}}\\right)h^{\\ell}=\\frac{1}{L^{1-\\alpha_{L}}N^{1-\\alpha_{A}}}q_{\\mathfrak{h}}^{\\ell}H^{\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{H^{\\ell}=\\frac{1}{N\\mathcal{H}}\\pmb{h}^{\\ell}\\cdot\\pmb{h}^{\\ell}\\sim\\Theta(1)}\\end{array}$ . Combining these changes , we find the following update to the pre-Attention variables $\\begin{array}{r}{\\mathcal{A}_{\\mathfrak{h}}^{\\ell}=\\frac{1}{N^{\\alpha}\\mathcal{A}}k_{\\mathfrak{h}}^{\\ell}\\cdot q_{\\mathfrak{h}}^{\\ell}}\\end{array}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\delta\\mathcal{A}_{\\mathfrak{h}}^{\\ell}=\\frac{1}{L^{1-\\alpha_{L}}N}q_{\\mathfrak{h}}^{\\ell}\\cdot q_{\\mathfrak{h}}^{\\ell}H^{\\ell}+\\frac{1}{L^{1-\\alpha_{L}}N}k_{\\mathfrak{h}}^{\\ell}\\cdot k_{\\mathfrak{h}}^{\\ell}H^{\\ell}+\\frac{1}{L^{2-2\\alpha_{L}}N^{2-2\\alpha_{A}}}\\mathcal{A}_{\\mathfrak{h}}^{\\ell}(H^{\\ell})^{2}}}\\\\ {{\\qquad=\\Theta(L^{-1+\\alpha_{L}}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since $\\begin{array}{r}{\\frac{1}{N}\\pmb{k}\\cdot\\pmb{k},\\frac{1}{N}\\pmb{q}\\cdot\\pmb{q}\\sim\\Theta(1)}\\end{array}$ . This update to the attention variable due to changes in $W_{K}^{\\ell},W_{Q}^{\\ell}$ will clearly die out as $L\\rightarrow\\infty$ unless $\\alpha_{L}=1$ . ", "page_idx": 21}, {"type": "text", "text": "C.3 Heuristic Analysis of Feature Changes Under Adam ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For Adam, the gradient of each individual parameter entry is approximately normalized by its scale [45]. Thus the learning rate $\\eta$ sets the size of the updates. This is why we scale the learning rate as $\\begin{array}{r}{\\tilde{\\eta}=\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}}\\end{array}$ which gives the same scale updates to the weights as SGD ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\delta W_{O\\mathfrak{h}}^{\\ell}\\approx\\eta\\,\\pmb{g}^{\\ell+1}\\pmb{v}_{\\mathfrak{h}}^{\\ell\\top}=\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}\\pmb{g}^{\\ell+1}\\pmb{v}_{\\mathfrak{h}}^{\\ell\\top}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Again computing the correction to the forward pass we find ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\delta h^{\\ell+1}=\\delta h^{\\ell}+\\frac{1}{L^{\\alpha_{L}}\\sqrt{N\\mathcal{H}}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}\\left(\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}g^{\\ell+1}v_{\\mathfrak{h}}^{\\ell\\top}\\right)v_{\\mathfrak{h}}^{\\ell}}}\\\\ {{\\displaystyle=\\delta h^{\\ell}+\\frac{1}{L}\\left[\\frac{1}{N\\mathcal{H}}\\sum_{\\mathfrak{h}}v_{\\mathfrak{h}}^{\\ell}\\cdot v_{\\mathfrak{h}}^{\\ell}\\right]g^{\\ell+1}=\\Theta(1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly our update generates the same scale of weight updates to the key and query weight matrices ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\delta W_{K\\mathfrak{h}}^{\\ell}\\sim\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}q_{\\mathfrak{h}}^{\\ell}h^{\\ell^{\\top}}\\;,\\;\\delta W_{Q\\mathfrak{h}}^{\\ell}\\sim\\frac{1}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}k_{\\mathfrak{h}}^{\\ell}h^{\\ell^{\\top}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We can therefore follow the identical argument to identify the scale of the change to the pre-attention variables $\\delta\\mathcal{A}_{\\mathfrak{h}}^{\\ell}=\\Theta(L^{1-\\alpha_{L}})$ . ", "page_idx": 21}, {"type": "text", "text": "C.4 What Counts as Feature Learning for Attention Layers? ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Any parameterization with $\\textstyle\\alpha_{N}\\ \\in\\ [\\frac{1}{2},1]$ will cause all updates to $\\mathcal{A}_{\\mathfrak{h}}^{\\ell}$ and entries of $\\ensuremath{\\boldsymbol{h}}^{\\ell+1}$ to be $\\Theta_{N,H,L}(1)$ across finite $N$ . The entries of $\\pmb q$ and $^k$ only move by $\\Theta_{N}(1)$ if $\\alpha_{\\mathcal{A}}=1$ $\\mathrm{~l~}(\\mu\\mathrm{P}\\mathrm{~s}$ caling). However, we argue that this criterion is not strictly necessary. Rather, feature learning could alternatively be defined in terms of evolution of macroscopic variables $(H,A,f$ , etc) rather than preactivation or key/query vector entries themselves. Table 3 summarizes two example values of $\\alpha_{\\mathcal{A}}$ which are of special interest for their $N\\rightarrow\\infty$ limits. ", "page_idx": 22}, {"type": "table", "img_path": "p0BBKhD5aI/tmp/88c1c8152ff256da0d3483328f631f55b9e0713b1efdb2bd0c2354f20658c749.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 3: Two interesting choices of scaling for the attention layer exponent $\\alpha_{\\mathcal{A}}$ which give approximately constant updates to the attention matrices $\\mathcal{A}_{\\mathfrak{h}}$ . The $\\mu\\mathrm{P}$ scaling $\\alpha_{\\mathcal{A}}=1$ causes the entries of the key/query vector entries to move non-negligibly but causes all heads to be identical (and all ${\\mathcal{A}}=0$ ) at initialization. Scaling instead with $\\begin{array}{r}{\\alpha_{\\mathcal{A}}=\\frac{1}{2}}\\end{array}$ causes the $\\boldsymbol{\\mathcal{A}}$ variables to be random but still non-negligibly updated under training. ", "page_idx": 22}, {"type": "text", "text": "The choice $\\alpha_{\\mathcal{A}}=\\textstyle{\\frac{1}{2}}$ allows the variance of $\\mathcal{A}_{\\mathfrak{h}}^{\\ell}$ to be constant size as a function of $N$ while also enabling learning of these variables. We verify these scalings in Figure 13. ", "page_idx": 22}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/091d68be5453ab3f98c060313608ccc119dbb6d195dc536e2d2e181da1d5759b.jpg", "img_caption": ["(a) Change in k Entries (SGD) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/d4f426ab3b94cef724753255ece268d69a2551f676e9488e7d4f426d83a70476.jpg", "img_caption": ["(b) Change in A Entries (SGD) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/be272bcbf7276b6c1225c47988ab93d8ba27bda9801f86b2dd29077dbfb4feb5.jpg", "img_caption": ["(c) Change in k Entries (Adam) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/6c6733f3168f7f89f06087be27f6ed7c15174a0800e4b8652ef688f70edd45f8.jpg", "img_caption": ["(d) Change in $\\mathcal{A}$ Entries (Adam) "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 13: The update to (a) key $k_{\\mathrm{b}}$ entries and (b) pre-attention variables $A_{\\mathfrak{h}}$ after $t$ steps of gradient descent for scaling exponents $\\alpha_{\\cal A}\\in\\{1,{\\frac{1}{2}}\\}$ . At the first step of SGD, the updates to the keys and attention variables are suppressed due to a lack of correlation between $W_{O}$ and the gradient $\\frac{\\partial f}{\\partial\\tilde{h}}$ . After training for multiple steps, this correlation increases and non-negligible updates to the attention variables occur. (c)-(d) The same but for the Adam optimizer with our proposed parameterization. ", "page_idx": 22}, {"type": "text", "text": "D DMFT Primer and Simple Examples ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 Main Conceptual Idea of the Approach ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Dynamical mean field theory is a method that was developed in the physics of spin glasses for dealing with dynamical systems that depend on a fixed source of disorder. The disorder could be random couplings between sites in a spin glass model [34], random connections between neurons in a random recurrent neural network [35], random data drawn from a distribution [37, 39] or the random initial weights in a deep neural network [15, 11]. In our case, we are interested in the last example, where the feature learning dynamics of a randomly initialized transformer is a function of the initial weights in each layer. In what follows, we will give a primer on the main objects which typically appear in a DMFT analysis (the correlation and response functions) to illustrate the main ideas of the approach. ", "page_idx": 23}, {"type": "text", "text": "D.2 Example 1: Linear Dynamics with GOE Matrix ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we discuss and derive the DMFT equations for the simplest possible example, a linear dynamical system with a Gaussian symmetric coupling matrix. ", "page_idx": 23}, {"type": "text", "text": "In this example we show that the DMFT path integral is computing something non-trivial about the kinds of dynamics induced by a linear dynamical system with a random matrix. In this linear example, the DMFT path integral encodes spectral properties of the random matrix. ", "page_idx": 23}, {"type": "text", "text": "Let\u2019s consider the simplest possible example: $\\begin{array}{r}{\\frac{d}{d t}h_{i}(t)=\\frac{1}{\\sqrt{N}}\\sum_{j=1}^{N}W_{i j}h_{j}(t)}\\end{array}$ where $W_{i j}=W_{j i}$ is a Gaussian symmetric matrix (GOE). This matrix is fixed while the state $\\pmb{h}(t)\\in\\mathbb{R}^{N}$ evolves. The path integral appraoch would tell you that in the $N\\rightarrow\\infty$ limit, every neuron $i$ has identical statistics given by the stochastic integro-differential equation ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\partial_{t}h(t)=u(t)+\\int_{0}^{t}d s R(t,s)h(s)\\;,\\;u(t)\\sim{\\mathcal G P}(0,C(t,s))}\\\\ {\\displaystyle C(t,s)=\\left<h(t)h(s)\\right>\\;,\\;R(t,s)=\\left<\\frac{\\delta h(t)}{\\delta u(s)}\\right>}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\langle\\cdot\\rangle$ denotes an average over the random variables $u(t)$ . In this picture, the averages $\\langle\\rangle$ over the noise can also be interpreted as averages over all $N$ neurons in the system, each of which are independent. This stochastic equation can be used to close the evolution equations for the correlation $\\bar{C}(\\bar{t},s)$ and linear response function $R(t,s)$ . ", "page_idx": 23}, {"type": "text", "text": "A generic result of this path integral DMFT picture is ", "page_idx": 23}, {"type": "text", "text": "1. All neurons (all variables $h_{i}$ ) decouple statistically. The presence of all other neurons only enters through \"macroscopic\" quantities $C(t,s)$ and $R(t,s)$ known as the correlation and response functions. The distribution of these functions over random realizations satisfies a large deviations principle where the distribution over $C,R$ has the form $p(C,R)\\,\\sim$ $e^{-N S(C,R)}$ where $S$ is the DMFT action obtained from the path integral method.   \n2. Extra memory terms like $\\textstyle\\int_{0}^{t}R(t,s)h(s)$ appear which depend on the state at earlier times $s<t$ . The Markovian (deterministic) system for $p(h|W)$ becomes stochastic and nonmarkovian after marginalizing $\\begin{array}{r}{p(h)=\\int\\dot{d W}p(h|\\dot{W})p(W)}\\end{array}$ . I would argue these memory terms are not obvious apriori but are systematic to compute in this framework. ", "page_idx": 23}, {"type": "text", "text": "Since this toy example is a linear dynamical system, one can also identify a connection between the DMFT correlation and response and spectral properties of the random matrix $W$ . We note that the response has the form ", "page_idx": 23}, {"type": "equation", "text": "$$\nR(t,s)=\\frac{1}{N}\\mathrm{Tr}\\exp\\left(W(t-s)\\right)=\\int d\\lambda\\rho(\\lambda)e^{\\lambda(t-s)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\rho(\\lambda)$ is the eigenvalue density of $W$ . In fact a Fourier transform of our DMFT equation recovers the semicircle law $\\begin{array}{r}{\\rho(\\lambda)=\\frac{1}{\\pi}\\mathrm{Im}R(i\\lambda)=\\frac{1}{2\\pi}\\sqrt{[4-\\lambda^{2}]_{+}}}\\end{array}$ for the eigenvalues. ", "page_idx": 23}, {"type": "text", "text": "In general, one can think of DMFT as a more powerful version of this method that can also handle nonlinearities. ", "page_idx": 23}, {"type": "text", "text": "D.3 Example 2: Deep Linear Network Updates ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section I will try showing how this DMFT approach can give useful insights into reasoning about learning updates which are not obvious apriori. While our paper advocates for taking depth $L\\rightarrow\\infty$ in a residual network, we first thought about simply scaling depth in a standard MLP. Below we show how the proliferation of response terms gives a different predicted scaling with $L$ than if we naively disregarded response terms. ", "page_idx": 24}, {"type": "text", "text": "Consider a non-residual linear MLP network with $\\mu\\mathrm{P}/$ mean-field scaling with $L$ hidden layers with $N\\rightarrow\\infty$ . Train the model for a single step of gradient descent with learning rate $\\eta$ on a data point $(x,y)$ with $|x|^{2}=1$ and $y=1$ and output multiplier $1/\\gamma_{0}$ . The forward pass variables $h^{\\ell}(t)$ and the backward pass variables ${\\pmb g}^{\\ell}(t)$ are defined recursively as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle h^{\\ell+1}(t)=\\frac{1}{\\sqrt{N}}W^{\\ell}(t)h^{\\ell}(t)=\\frac{1}{\\sqrt{N}}W^{\\ell}(0)h^{\\ell}(t)+\\eta\\gamma_{0}\\sum_{s<t}H^{\\ell}(t,s)g^{\\ell+1}(s)}}}\\\\ {{{\\displaystyle g^{\\ell}(t)=\\frac{1}{\\sqrt{N}}W^{\\ell}(t)^{\\top}g^{\\ell+1}(t)=\\frac{1}{\\sqrt{N}}W^{\\ell}(0)^{\\top}g^{\\ell+1}(t)+\\eta\\gamma_{0}\\sum_{s<t}G^{\\ell+1}(t,s)h^{\\ell+1}(s)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, naively, one may think that $\\begin{array}{r}{\\frac{1}{\\sqrt{N}}W^{\\ell}(0)h^{\\ell}(t)}\\end{array}$ has entries that are independently Gaussian with covariance $H^{\\ell}(t,t^{\\prime})$ . However, this is incorrect and the DMFT response functions give an additional correction. ", "page_idx": 24}, {"type": "text", "text": "The DMFT Equations for this Model Following the approach of [15, 11], we find the following DMFT equations for the preactivations $h^{\\ell}$ after 1 step of training ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{h^{\\ell}(0)=u^{\\ell}(0)\\;,\\;g^{\\ell}(0)=r^{\\ell}(0)}}\\\\ {{h^{\\ell}(1)=u^{\\ell}(1)+A^{\\ell-1}(1,0)g^{\\ell}(0)+\\eta\\gamma_{0}H^{\\ell-1}(1,0)g^{\\ell}(0)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the random variables $u^{\\ell}(0),u^{\\ell}(1)$ and $r^{\\ell}(0)$ have the following covariance structure ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle u^{\\ell}(0)u^{\\ell}(0)\\right\\rangle=H^{\\ell-1}(0,0)\\;,\\;\\left\\langle u^{\\ell}(1)u^{\\ell}(0)\\right\\rangle=H^{\\ell-1}(1,0)\\;,\\;\\left\\langle u^{\\ell}(1)u^{\\ell}(1)\\right\\rangle=H^{\\ell-1}(1,1)}\\\\ &{\\left\\langle r^{\\ell}(0)r^{\\ell}(0)\\right\\rangle=G^{\\ell+1}(0,0),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and the feature kernels $H^{\\ell}(t,t^{\\prime}),G^{\\ell}(t,t^{\\prime})$ and response functions $A^{\\ell}(t,t^{\\prime})$ have the form ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{H^{\\ell}(t,t^{\\prime})=\\left\\langle h^{\\ell}(t)h^{\\ell}(t^{\\prime})\\right\\rangle\\ ,\\;G^{\\ell}(t,t^{\\prime})=\\left\\langle g^{\\ell}(t)g^{\\ell}(t^{\\prime})\\right\\rangle}}\\\\ {{\\mathrm{}}}\\\\ {{A^{\\ell}(t,t^{\\prime})=\\left\\langle\\frac{\\delta h^{\\ell}(t)}{\\delta r^{\\ell}(t^{\\prime})}\\right\\rangle}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "These recursions can be solved with the initial conditions $H^{\\ell}(0,0)\\,=\\,1$ and $G^{\\ell}(0,0)\\,=\\,1$ which implies that $H^{\\ell}(1,0)=1$ so that ", "page_idx": 24}, {"type": "equation", "text": "$$\nA^{\\ell}(1,0)=\\eta\\gamma_{0}+A^{\\ell-1}(1,0)=\\ell\\eta\\gamma_{0}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using this equation, we find ", "page_idx": 24}, {"type": "equation", "text": "$$\nH^{\\ell}(1,1)=\\left\\langle h^{\\ell}(1)h^{\\ell}(1)\\right\\rangle=H^{\\ell-1}(1,1)+\\eta^{2}\\gamma_{0}^{2}\\ell^{2}=1+\\eta^{2}\\gamma_{0}^{2}\\sum_{k=1}^{\\ell}k^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This is the DMFT prediction for the scale of the feature kernels after a step of training. ", "page_idx": 24}, {"type": "text", "text": "Neglecting the DMFT Response Gives Incorrect Depth Scalings for MLPs However, if we had neglected the DMFT response functions and approximated the dynamics as ", "page_idx": 24}, {"type": "equation", "text": "$$\nH^{\\ell}(1,1)=H^{\\ell-1}(1,1)+\\eta^{2}\\gamma_{0}^{2}\\implies H^{\\ell}(1,1)=1+\\eta^{2}\\gamma_{0}^{2}\\;\\ell\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The feature variance after $t=1$ step of gradient descent $H^{\\ell}=\\left\\langle h^{\\ell}(1)^{2}\\right\\rangle$ after $t=1$ step, the final layer ", "page_idx": 24}, {"type": "equation", "text": "$$\nH^{L}\\sim\\binom{1+\\frac{1}{3}\\eta^{2}\\gamma_{0}^{2}\\ L^{3}}{1+\\eta^{2}\\gamma_{0}^{2}\\ L}\\ \\ \\ \\ \\mathrm{DMFT~Response~Included~(Full~DMFT)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We see that without the response terms we get a completely different scaling prediction with $L$ ! ", "page_idx": 24}, {"type": "text", "text": "Scaled Residual Networks For $\\textstyle{\\frac{1}{\\sqrt{L}}}$ residual block scaling $\\begin{array}{r}{(\\alpha_{L}=\\frac{1}{2})}\\end{array}$ , the response functions are still important to accurately characterize the dynamics and contribute $\\Theta_{L}(1)$ corrections to the feature learning dynamics as $L\\rightarrow\\infty$ . However, for the $1/L$ block multiplier scaling, the response functions do not contribute in the limit. These facts are not-apriori obvious but follow from the DMFT analysis (either path integral or cavity approach). ", "page_idx": 25}, {"type": "text", "text": "E DMFT Analysis for Transformers ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we derive the limiting equations for the infinite head limit $\\mathcal{H}\\to\\infty$ of training with SGD. The results can be easily extended to SGD with momentum following the methods of [11, 15]. ", "page_idx": 25}, {"type": "text", "text": "E.1 Deriving the DMFT Action ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section we will derive the limiting equations of motion for stochastic gradient descent in the $\\mathcal{H}\\to\\infty$ limit. We start by defining the loss function which we aim to minimize ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\int d\\pmb{x}\\,p(\\pmb{x})\\,\\ell[f(\\pmb{x})]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $p(x)$ is the data distribution of interest. We note that this can be the population loss or the empirical loss on a finite collection of points. We let \u2206(x, t) \u2261 \u2212\u2202\u2202\u2113[ff((xx,,tt))] represent the error signal on datapoint $\\textbf{\\em x}$ . At each step of training $t$ a batch of examples $\\mathfrak{B}_{t}\\overset{\\cdot}{=}\\{\\pmb{x}_{1}(t),...,\\pmb{x}_{|\\mathfrak{B}_{t}|}(t)\\}$ is generated and used to estimate a gradient for SGD. In what follows, we let $\\mathbb{E}_{\\mathbf{x}\\sim\\mathfrak{B}_{t}}$ represent averages over the minibatch at time $t$ . We emphasize here that the batches $\\mathfrak{B}_{t}$ are assumed to be given or fixed and are not averaged over as random draws from $p(x)$ , but rather our expectation simply denotes the empirical mean over the minibatch at time $t$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pmb{x}\\sim\\mathfrak{B}_{t}}[\\pmb{f}(\\pmb{x})]=\\frac{1}{|\\mathfrak{B}_{t}|}\\sum_{\\pmb{x}\\in\\mathfrak{B}_{t}}f(\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We start by expressing again the forward pass equations for each layer. To make this analysis more compressed while still capturing all of the interesting aspects, we will first compute the equations of motion in the absence of MLP layers (which were analyzed in prior works, see Appendix E) and layernorm (which in the limit it will only apply a deterministic affine transformation to each of the entries of the residual stream and the backward pass gradient variables as we will show explicitly in Appendix E.6). In the absence of layernorm, our forward pass has the form ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{h}}_{\\mathfrak{s}}^{\\ell+1}(\\mathbf{\\boldsymbol{x}},t)=\\mathbf{\\boldsymbol{h}}_{\\mathfrak{s}}^{\\ell}(\\mathbf{\\boldsymbol{x}},t)+\\frac{\\beta_{0}}{L^{\\alpha_{L}}}\\mathrm{MHSA}\\left(\\boldsymbol{h}^{\\ell}(\\mathbf{\\boldsymbol{x}},t)\\right)_{\\mathfrak{s}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the MHSA layer is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{MHSA}\\left(h^{\\ell}(x,t)\\right)_{s}=\\frac{1}{\\sqrt{\\mathcal{M}}}\\displaystyle\\sum_{\\vartheta\\in[\\mathcal{M}]}\\displaystyle\\sum_{s^{\\ell}\\in[\\mathcal{A}]}\\displaystyle\\sum_{\\vartheta_{\\vartheta\\ell}^{\\ell}(x)}\\alpha_{\\vartheta^{\\ell}}^{\\ell}(x,t)\\sigma_{\\vartheta\\circ s^{\\ell}}^{\\ell}(x,t)}\\\\ &{\\sigma_{\\vartheta(x}^{\\ell}(x,t)=\\displaystyle\\frac{1}{\\sqrt{\\mathcal{N}}}W_{\\vartheta(t)}^{\\ell}(t)v_{\\vartheta_{\\ell}}^{\\ell}(x,t)}\\\\ &{v_{\\vartheta_{\\vartheta}}^{\\ell}(x,t)=\\displaystyle\\frac{1}{\\sqrt{\\mathcal{N}}\\mathcal{M}}W_{\\ell\\nu_{\\vartheta}}^{\\ell}(t)h_{s}^{\\ell}(x,t)}\\\\ &{\\sigma_{\\vartheta_{\\vartheta^{\\ell}}}^{\\ell}(x,t)=\\sigma\\left(A_{\\vartheta^{\\ell}}^{\\ell}(x,t)\\right)_{s^{\\ell}}\\cdot\\mathcal{A}_{\\vartheta\\circ s^{\\ell}}^{\\ell}(x,t)=\\frac{1}{N\\alpha_{A}^{\\ell}}k_{\\mathfrak{b}\\circ}^{\\ell}(x,t)\\cdot q_{\\mathfrak{b}^{\\ell}}^{\\ell}(x,t)}\\\\ &{k_{\\mathfrak{b}\\circ}^{\\ell}(x,t)=\\displaystyle\\frac{1}{N^{\\frac{3}{2}-\\alpha_{A}}\\sqrt{\\mathcal{M}}}W_{\\ell\\nu_{\\vartheta}}^{\\ell}(t)h_{s}^{\\ell}(x,t)}\\\\ &{q_{\\vartheta_{\\vartheta}}^{\\ell}(x,t)=\\frac{1}{N^{\\frac{3}{2}-\\alpha_{A}}\\sqrt{\\mathcal{Y}}}W_{\\vartheta\\Phi}^{\\ell}(t)h_{s}^{\\ell}(x,t)~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To compute the weight dynamics we again introduce the necessary gradient fields which we previously argued have $\\Theta(1)$ entries ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\pmb g}_{s}^{\\ell}({\\pmb x},t)\\equiv\\gamma_{0}N\\mathcal{H}\\frac{\\partial f({\\pmb x},t)}{\\partial{\\pmb h}_{s}^{\\ell}({\\pmb x},t)}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We also introduce the following intermediate quantities which are necessary to characterize the backward pass through the attention layer ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s}^{\\prime}}^{\\ell}(\\mathbf{x},t)\\equiv\\displaystyle\\frac{1}{N\\sqrt{\\mathcal{H}}}\\,g_{\\mathfrak{s}}^{\\ell+1}(\\mathbf{x},t)\\cdot\\pmb{o}_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(\\mathbf{x},t)}\\\\ &{\\dot{\\sigma}_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s}^{\\prime}\\mathfrak{s}^{\\prime\\prime}}^{\\ell}(\\mathbf{x},t)\\equiv\\displaystyle\\frac{\\partial\\sigma_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s}^{\\prime}}^{\\ell}(\\mathbf{x},t)}{\\partial\\mathcal{A}_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s}^{\\prime\\prime}}^{\\ell}(\\mathbf{x},t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We need to break up each of the weight matrices into their initial component and their update from SGD ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{O_{\\Phi}}^{\\ell}(t)=W_{O_{\\Psi}}^{\\ell}(0)+\\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{x\\sim\\mathfrak{B}_{t^{\\prime}}}\\sum_{s}\\Delta(x,t^{\\prime})\\tilde{g}_{s^{\\prime}}^{\\ell}(x,t^{\\prime})v_{\\mathfrak{h}_{s}}^{\\ell\\sigma}(x,t^{\\prime})^{\\top}}\\\\ &{V_{V\\mathfrak{h}}^{\\ell}(t)=W_{V\\mathfrak{h}}^{\\ell}(0)+\\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{x\\sim\\mathfrak{B}_{t^{\\prime}}}\\sum_{s\\textbf{s}^{\\prime}}\\Delta(x,t^{\\prime})\\sigma_{s^{\\prime}}^{\\ell}\\tilde{g}_{o\\mathfrak{h}_{s}}^{\\ell}(x,t^{\\prime})h_{s^{\\prime}}^{\\ell}(x,t^{\\prime})^{\\top}}\\\\ &{V_{K\\mathfrak{h}}^{\\ell}(t)=W_{K\\mathfrak{h}}^{\\ell}(0)+\\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{x\\sim\\mathfrak{B}_{t^{\\prime}}}\\sum_{s\\textbf{s}^{\\prime}\\in\\mathcal{V}}\\Delta(x,t^{\\prime})M_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x,t^{\\prime})\\dot{\\sigma}_{s^{\\prime}s^{\\prime\\prime}}^{\\ell}q_{\\mathfrak{h}\\mathfrak{s}^{\\prime\\prime}}^{\\ell}(x,t^{\\prime})h_{s}^{\\ell}(x,t^{\\prime})}\\\\ &{V_{Q\\mathfrak{h}}^{\\ell}(t)=W_{Q\\mathfrak{h}}^{\\ell}(0)+\\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{x\\sim\\mathfrak{B}_{t^{\\prime}}}\\sum_{s\\textbf{s}^{\\prime}\\in\\mathcal{V}}\\Delta(x,t^{\\prime})M \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can now express the residual stream as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{\\mathfrak{s}}^{\\ell+1}(x,t)=h_{\\mathfrak{s}}^{\\ell}(x,t)+\\beta_{0}L^{-\\alpha_{L}}\\,\\bar{\\chi}_{O\\mathfrak{s}}^{\\ell+1}(x,t)\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {+\\,\\eta_{0}\\gamma_{0}\\beta_{0}^{2}L^{-1}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{x^{\\prime}\\sim\\mathfrak{B}_{t^{\\prime}}}\\Delta(x^{\\prime},t^{\\prime})\\sum_{\\mathfrak{s^{\\prime}}}g_{\\mathfrak{s^{\\prime}}}^{\\ell+1}(x^{\\prime},t^{\\prime})V_{\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{\\ell\\sigma}(x,x^{\\prime},t,s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we introduced the fields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\bar{\\chi}_{O\\mathfrak{s}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{\\mathcal{H}}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{U}}\\sum_{s^{\\prime}}\\chi_{O\\mathfrak{h}\\mathfrak{s^{\\prime}}}^{\\ell}(x,t)\\sigma_{\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{\\ell}(x,t)\\;,\\;\\chi_{O\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{N}}W_{O\\mathfrak{h}}^{\\ell}(0)v_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and the kernel ", "page_idx": 26}, {"type": "equation", "text": "$$\nV_{\\mathfrak{s}\\mathfrak{s}^{\\prime}}^{\\ell\\sigma}(\\mathbf{x},\\mathbf{x}^{\\prime},t,s)=\\frac{1}{\\mathcal{H}N}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}v_{\\mathfrak{h}\\mathfrak{s}}^{\\ell\\sigma}(\\mathbf{x},t)\\cdot v_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell\\sigma}(\\mathbf{x}^{\\prime},t^{\\prime})=\\frac{1}{\\mathcal{H}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}\\sum_{\\mathfrak{s}^{\\prime\\prime}\\mathfrak{s}^{\\prime\\prime\\prime}}\\sigma_{\\mathfrak{s}\\mathfrak{s}^{\\prime\\prime}}^{\\ell}\\sigma_{\\mathfrak{s}^{\\prime}\\mathfrak{s}^{\\prime\\prime\\prime}}^{\\ell}V_{\\mathfrak{h}\\mathfrak{s}^{\\prime\\prime}\\mathfrak{s}^{\\prime\\prime\\prime}}^{\\ell}(x,\\mathbf{x}^{\\prime},t,t^{\\prime})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We see that, regardless of the choice of $\\alpha_{L}$ , the update to the residual stream due to feature learning will scale as $1/L$ which is necessary for a stable infinite depth limit [11]. To track the dynamics of the value vectors, we must also track the variables ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\chi_{V\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{N\\mathcal{H}}}W_{V\\mathfrak{h}}^{\\ell}(0)h_{\\mathfrak{s}}^{\\ell}(x,t)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We similarly find the following for the key dynamics ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{k_{\\mathrm{ps}}^{\\ell}(\\mathbf{x},t)=\\chi_{K\\mathrm{ps}}^{\\ell}(x,t)}&{\\quad\\mathrm{~o~n~}\\quad\\mathrm{f}\\quad\\frac{1}{L^{2}}}\\\\ &{\\quad+\\,\\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}\\,N^{1-\\alpha_{A}}}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{\\mathbf{x}\\sim\\mathbb{3}_{t^{\\prime}}}\\sum_{s^{\\prime}s^{\\prime\\prime}s^{\\prime\\prime\\prime}}\\Delta(x,t^{\\prime})M_{\\mathrm{fs^{\\prime}}s^{\\prime\\prime}}^{\\ell}(x,t^{\\prime})\\dot{\\sigma}_{s^{\\prime}s^{\\prime\\prime}s^{\\prime\\prime}}^{\\ell}q_{\\mathrm{fs^{\\prime\\prime\\prime}}}^{\\ell}(x,t^{\\prime})H_{\\mathrm{ss^{\\prime}}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})}&{}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(4\\pi^{2}/L^{2})}&{}\\\\ &{\\quad\\chi_{K\\mathrm{ps}}^{\\ell}(\\mathbf{x},t)=\\frac{1}{N^{\\frac{3}{2}-\\alpha_{A}}\\sqrt{\\mathcal{H}}}W_{K\\mathrm{b}}^{\\ell}(0)h_{s}^{\\ell}(x,t)}&{\\quad\\mathrm{~(4)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and an analogous update equation holds for the query dynamics $\\pmb q$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{\\mathrm{{bs}}}^{\\ell}(x,t)=\\chi_{Q\\mathrm{{bs}}}^{\\ell}(x,t)}\\\\ &{\\ +\\ \\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}N^{1-\\alpha_{A}}}\\sum_{t^{\\prime}<t}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sum_{s^{\\prime}\\sim\\mathfrak{B}_{t^{\\prime}}}\\sum_{\\tau^{\\prime}}\\Delta(x^{\\prime},t^{\\prime})M_{\\mathfrak{h}\\mathrm{s}s^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})\\dot{\\sigma}_{\\mathfrak{s}s^{\\prime}s^{\\prime\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})k_{\\mathfrak{h}s^{\\prime\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})H_{\\mathfrak{s}s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t)}\\\\ &{\\chi_{Q\\mathrm{{bs}}}^{\\ell}(x,t)=\\frac{1}{N^{\\frac{3}{2}-\\alpha_{A}}\\sqrt{\\mathcal{H}}}W_{Q\\mathrm{{b}}}^{\\ell}(0)h_{\\mathfrak{s}}^{\\ell}(x,t)}&{(42)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We see that if the residual stream is frozen so that $x_{K},x_{Q}$ are static, the keys and queries will only evolve in the $N,L\\to\\infty$ limits if $\\alpha_{L}=\\alpha_{A}=1$ as we argued in the main text. From these equations we can deduce the pre-attention values $A_{\\mathfrak{h}_{55^{\\prime}}}^{\\ell}(\\mathbf{\\boldsymbol{x}},t)$ . ", "page_idx": 27}, {"type": "text", "text": "Next, we examine the dynamics of the $M_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(\\mathbf{x},t)$ variables which are defined as ", "page_idx": 27}, {"type": "equation", "text": "$$\nM_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{\\ell}(\\pmb{x},t)=\\frac{1}{N\\sqrt{\\mathcal{H}}}\\pmb{g}_{\\mathfrak{s}}^{\\ell+1}(\\pmb{x},t)\\cdot\\pmb{o}_{\\mathfrak{h}\\mathfrak{s^{\\prime}}}^{\\ell}(\\pmb{x},t).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We can verify that $M_{\\mathfrak{h}}^{\\ell}$ are all $\\Theta_{N,\\mathcal{H}}(1)$ throughout training by expanding the dynamics of the attention output ${\\pmb{o}}_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}({\\pmb{x}},t)$ . ", "page_idx": 27}, {"type": "text", "text": "E.1.1 Backward Pass ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Next, we need to work out the recursions for the backward pass variables. After these equations have been worked out, we can isolate the dependence of the full dynamics on all of the initial weights. ", "page_idx": 27}, {"type": "text", "text": "MHSA Layer We will start by differentiating through the MHSA layer ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varphi_{i}^{\\prime}(x,t)=\\displaystyle\\sum_{\\ell^{\\prime}=1}^{\\infty}\\left(\\frac{\\partial\\bar{h}_{\\ell^{\\prime}}^{\\prime}(x,t)}{\\partial x_{\\ell}(x,t)}\\right)^{\\top}\\varphi_{i}^{\\prime+1}}\\\\ &{\\qquad=g_{i}^{\\prime+1}(x,t)+\\displaystyle\\frac{\\partial\\bar{h}_{\\ell^{\\prime}}^{\\prime}}{\\partial x^{\\prime}}\\frac{\\Big(\\frac{\\partial}{\\partial x_{\\ell}(x,t)}\\Big)^{\\top}}{\\partial\\theta_{i}^{\\prime}(x,t)}\\operatorname{MHSA}\\left(h^{\\prime}(x,t)\\right)_{\\theta^{\\prime}}^{\\top}\\varphi_{\\theta^{\\prime}}^{\\prime+1}(x,t)}\\\\ &{\\qquad=g_{i}^{\\prime+1}(x,t)+\\displaystyle\\frac{\\beta_{0}}{L^{2+\\varepsilon}\\sqrt{N T}}\\sum_{\\ell^{\\prime}=1}^{\\infty}\\sum_{\\psi}W_{\\ell^{\\prime}}^{\\prime}(\\theta)^{\\top}g_{\\theta_{i}^{\\prime}}^{\\prime}(x,t)}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{\\beta_{0}}{L^{2+\\varepsilon}\\sqrt{N T}}\\sum_{\\ell^{\\prime}=1}^{\\infty}W_{\\ell^{\\prime}}^{\\prime}(\\theta)^{\\top}K_{\\ell}^{\\prime\\prime\\prime}(x,t)+\\displaystyle\\frac{\\beta_{0}}{L^{2+\\varepsilon}\\sqrt{N T}}\\sum_{\\ell^{\\prime}=1}^{\\infty}W_{\\ell^{\\prime}}^{\\prime}(\\theta)^{\\top}q_{i}^{\\prime\\prime\\prime}(x,t)}\\\\ &{\\qquad=g_{i}^{\\prime+1}(x,t)+\\displaystyle\\frac{\\beta_{0}}{L^{2+\\varepsilon}}\\left\\{\\xi_{i^{\\prime}}^{2}(x,t)+\\xi_{i\\times}(x,t)+\\xi_{\\ell^{\\prime}}^{\\top}(x,t)\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\displaystyle\\sum_{\\ell^{\\prime}=1}^{\\infty}\\sum_{\\ell^{\\prime}=1}^{\\infty}\\sum_{\\psi}\\Big(x,t^{\\prime})G_{i^{\\prime}}^{\\prime}(x,\\theta^{\\prime})^{\\top}K_{\\ell^{\\prime}}^{\\prime}(x^{\\prime},t^{\\prime})\\Big)}\\\\ &{\\qquad=\\displaystyle\\frac{\\beta_{0}^{\\top \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we introduced the variables ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{O\\mathbb{P}\\mathfrak{H}^{\\vec{S}}}^{\\ell\\sigma}(x,t)=\\displaystyle\\sum_{\\mathfrak{H}^{\\vec{S}^{\\prime}}}\\sigma_{\\mathfrak{H}^{\\vec{S}^{\\prime}}}^{\\ell}(x,t)g_{O\\mathbb{P}\\mathfrak{H}^{\\vec{S}^{\\prime}}}^{\\ell}(x,t)}\\\\ &{k_{\\mathfrak{h}^{\\vec{S}}}^{\\ell M\\dot{\\sigma}}(x,t)=\\displaystyle\\sum_{\\mathfrak{H}^{\\vec{S}^{\\prime}\\in\\mathcal{V}^{\\prime}}}M_{\\mathfrak{h}^{\\vec{S}^{\\prime}\\in\\mathcal{V}^{\\prime}}}^{\\ell}(x,t)\\dot{\\sigma}_{\\mathfrak{h}^{\\vec{S}^{\\prime}\\in\\mathcal{V}^{\\prime\\prime}\\mathfrak{H}}}^{\\ell}(x,t)k_{\\mathfrak{h}^{\\vec{S}^{\\prime}}}^{\\ell}(x,t)}\\\\ &{q_{\\mathfrak{h}\\mathfrak{s}}^{\\ell M\\dot{\\sigma}}(x,t)=\\displaystyle\\sum_{\\mathfrak{s}^{\\prime\\prime}\\mathcal{S}^{\\prime\\prime}}M_{\\mathfrak{h}\\mathfrak{s}^{\\prime\\prime}}^{\\ell}(x,t)\\dot{\\sigma}_{\\mathfrak{h}\\mathfrak{s}^{\\prime\\prime}\\mathfrak{s}^{\\prime\\prime\\prime}}^{\\ell}(x,t)q_{\\mathfrak{h}\\mathfrak{s}^{\\prime\\prime\\prime}}^{\\ell}(x,t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and their associated kernels ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{G_{O s s^{\\prime}}^{\\ell\\sigma}(x,x^{\\prime},t,t^{\\prime})=\\displaystyle\\frac{1}{N\\mathcal{H}}\\displaystyle\\sum_{\\ell=1}^{\\mathcal{H}}g_{O\\mathbb{h}}^{\\ell\\sigma}(x,t)\\cdot g_{O\\mathbb{h}^{s^{\\prime}}}^{\\ell\\sigma}(x,t)}}\\\\ {{\\displaystyle K_{s s^{\\prime}}^{\\ell M\\dot{\\sigma}}(x,x^{\\prime},t,t^{\\prime})=\\displaystyle\\frac{1}{N\\mathcal{H}}\\displaystyle\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}k_{\\mathfrak{h}}^{\\ell M\\dot{\\sigma}}(x,t)\\cdot k_{\\mathfrak{h}^{s^{\\prime}}}^{\\ell M\\dot{\\sigma}}(x^{\\prime},t^{\\prime})}}\\\\ {{\\displaystyle Q_{s s^{\\prime}}^{\\ell M\\dot{\\sigma}}(x,x^{\\prime},t,t^{\\prime})=\\displaystyle\\frac{1}{N\\mathcal{H}}\\displaystyle\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}q_{\\mathfrak{h}}^{\\ell M\\dot{\\sigma}}(x,t)\\cdot q_{\\mathfrak{h}s^{\\prime}}^{\\ell M\\dot{\\sigma}}(x^{\\prime},t^{\\prime})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we introduced the following random fields ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\bar{\\xi}_{V\\bar{\\nu}^{\\alpha}}^{\\ell}({\\bf x},t)=\\frac{1}{\\sqrt{\\mathcal{N}}}\\displaystyle\\sum_{b\\ell\\nu^{\\prime}}\\xi_{V\\bar{\\nu}^{\\alpha}\\nu^{\\prime}}^{\\ell}({\\bf x},t)\\sigma_{b\\ell\\nu^{\\prime}}^{\\ell}({\\bf x},t)}}\\\\ {{\\displaystyle\\xi_{V\\bar{\\nu}^{\\alpha}}^{\\ell}({\\bf x},t)=\\frac{1}{\\sqrt{\\mathcal{N}}}W_{V\\bar{\\nu}^{\\alpha}\\bar{\\nu}}^{\\ell}(0)^{\\top}g_{0\\bar{\\nu}\\nu^{\\prime}}^{\\ell}({\\bf x},t)}}\\\\ {{\\displaystyle\\bar{\\xi}_{Q\\bar{\\nu}^{\\alpha}}^{\\ell}({\\bf x},t)=\\frac{1}{\\sqrt{\\mathcal{N}}}\\displaystyle\\sum_{b\\ell\\nu^{\\prime}\\omega^{\\prime}}\\xi_{Q\\bar{\\nu}\\nu^{\\prime}}^{\\ell}(x,t)\\sigma_{b\\ell\\nu^{\\prime}\\bar{\\nu}^{\\alpha}}^{\\ell}(x,t)M_{b\\bar{\\nu}^{\\alpha}\\bar{\\nu}^{\\prime}}^{\\ell}(x,t)}}\\\\ {{\\displaystyle\\xi_{V\\bar{\\nu}^{\\alpha}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{\\mathcal{N}}}W_{V\\bar{\\nu}^{\\alpha}\\bar{\\nu}}^{\\ell}(0)^{\\top}g_{0\\bar{\\nu}\\nu^{\\prime}}^{\\ell}(x,t)}}\\\\ {{\\displaystyle\\bar{\\xi}_{K\\bar{\\nu}^{\\alpha}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{\\mathcal{N}}}\\displaystyle\\sum_{b\\ell\\nu^{\\prime}\\nu^{\\prime}\\bar{\\nu}^{\\alpha}}\\xi_{K\\bar{\\nu}^{\\alpha\\prime}}^{\\ell}(x,t)\\sigma_{b\\bar{\\nu}^{\\alpha}\\nu^{\\prime}\\bar{\\nu}^{\\alpha}}^{\\ell}(x,t)M_{b\\ell\\nu^{\\prime}\\bar{\\nu}^{\\alpha}}^{\\ell}(x,t)}}\\\\ {{\\displaystyle\\xi_{K\\bar{\\nu}^{\\alpha}\\bar{\\nu}^{\\alpha}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{\\mathcal{N}}}W_{K\\bar{\\nu}^{\\alpha}\\bar{\\nu}}^{\\ell}(0)^{\\top}q_{\\bar{\\nu}\\nu^{\\prime}\\bar{\\nu}^{\\alpha}}^\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "E.1.2 Why we need $\\alpha_{\\mathcal{A}}=1$ for gradient stability ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "From the previous equation we see that regardless of the choice of $\\alpha_{\\mathcal{A}}$ we need to choose the variance of $W_{K\\mathfrak{h}}^{\\ell}(0)$ and $W_{Q\\mathfrak{h}}^{\\acute{\\ell}}(0)$ so that $\\textstyle\\frac{1}{\\sqrt{N}}W_{K\\mathfrak{h}}^{\\ell}(0)q_{\\mathfrak{h}}^{\\ell}(\\mathbf{x},t)$ has ${\\mathcal O}(1)$ entries. This means that the entries can at most $\\Theta(1)$ and not $\\Theta(N^{1-\\alpha_{A}})$ as we originally stipulated in order to obtain $\\pmb{k}$ and $\\pmb q$ with $\\Theta(1)$ entries at initialization. Thus, with this required scaling, we must have either $\\alpha_{\\mathcal{A}}=1$ and $\\Theta(1)$ variance of the weights, which leads to attention variables which are $\\Theta(N^{-1/2})$ at initialization or we choose \u03b1A = 2 and choose $k,q$ to have entries of scale $\\Theta(N^{-1/2})$ at initialization. These both lead to the same vanishing initial condition for the pre-attention variables. It thus suffices to consider $\\mu\\mathrm{P}$ scaling $\\alpha_{\\mathcal{A}}=1$ to study the $N\\rightarrow\\infty$ limit. We stress that this effect is not visible from a simple heuristic analysis of the forward pass variables after an update like we perform in Appendix C. ", "page_idx": 28}, {"type": "text", "text": "Isolating all Dependence on Initial Conditions To summarize the previous sections, we begin by collecting all of the stochastic fields which show up in the dynamics and depend on the initial weight matrices. These quantities all come in pairs since for each matrix we need to consider the forward and backward passes through the initial matrix. ", "page_idx": 28}, {"type": "text", "text": "The following variables are necessary to characterize the dynamics of the $N\\mathcal{H}$ -dimensional residual stream ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi_{O\\mathfrak{h}\\mathfrak{s}}^{\\ell}(\\mathbf{x},t)=\\frac{1}{\\sqrt{N}}W_{O\\mathfrak{h}}^{\\ell}(0)v_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)\\;,\\;\\xi_{O\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{N}}W_{O\\mathfrak{h}}^{\\ell}(0)^{\\top}g_{\\mathfrak{s}}^{\\ell+1}(x,t)}\\\\ &{\\chi_{V\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{N\\mathcal{H}}}W_{V\\mathfrak{h}}^{\\ell}(0)h_{\\mathfrak{s}}^{\\ell}(x,t)\\;,\\;\\xi_{V\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{N}}W_{V\\mathfrak{h}}^{\\ell}(0)^{\\top}g_{O\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)}\\\\ &{\\chi_{Q\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{N\\mathcal{H}}}W_{Q\\mathfrak{h}}^{\\ell}(0)h_{\\mathfrak{s}}^{\\ell}(x,t)\\;,\\;\\xi_{Q\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{N}}W_{Q\\mathfrak{h}}^{\\ell}(0)^{\\top}k_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)}\\\\ &{\\chi_{K\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{N\\mathcal{H}}}W_{K\\mathfrak{h}}^{\\ell}(0)h_{\\mathfrak{s}}^{\\ell}(x,t)\\;,\\;\\xi_{K\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)=\\frac{1}{\\sqrt{N}}W_{K\\mathfrak{h}}^{\\ell}(0)^{\\top}q_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "From these variables, we can reconstruct the entire dynamics for the $^{h,g}$ fields. We therefore study the moment generating functional for the above primitive random variables. Let ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{0}=\\{\\pmb{W}_{O\\mathfrak{h}}^{\\ell},\\pmb{W}_{V\\mathfrak{h}}^{\\ell}(0),\\pmb{W}_{K\\mathfrak{h}}^{\\ell}(0),\\pmb{W}_{Q\\mathfrak{h}}^{\\ell}(0)\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{Z[\\{j\\}]=\\mathbb{E}_{\\theta_{0}}\\exp\\left(\\sum_{\\ell\\in\\mathcal{T}_{0}}\\int d x\\left[j_{\\ell}^{x_{\\ell}^{\\ell}}(x,t)\\cdot\\chi_{\\mathcal{O}\\mathbb{H}^{\\otimes}}^{\\ell}(x,t)+j_{\\mathbb{H}^{\\otimes}}^{\\ell_{\\ell}^{\\ell}}(x,t)\\cdot\\xi_{\\mathcal{O}\\mathbb{H}^{\\otimes}}^{\\ell}(x,t)\\right]\\right)}\\quad}&{}&\\\\ &{}&{\\times\\exp\\left(\\sum_{\\ell\\in\\mathcal{T}_{0}}\\int d x\\left[j_{\\ell}^{x_{\\ell}^{\\ell}}(x,t)\\cdot\\chi_{K\\mathbb{H}^{\\otimes}}^{\\ell}(x,t)+j_{\\mathbb{H}^{\\otimes}}^{\\ell_{\\ell}^{\\ell}}(x,t)\\cdot\\xi_{K\\mathbb{H}^{\\otimes}}^{\\ell}(x,t)\\right]\\right)}\\\\ &{}&{\\times\\exp\\left(\\sum_{\\ell\\in\\mathcal{T}_{0}}\\int d x\\left[j_{\\mathbb{H}^{\\otimes}}^{x_{\\ell}^{\\ell}}(x,t)\\cdot\\chi_{\\mathcal{O}\\mathbb{H}^{\\otimes}}^{\\ell}(x,t)+j_{\\mathbb{H}^{\\otimes}}^{\\ell_{\\ell}^{\\ell}}(x,t)\\cdot\\xi_{\\mathcal{O}\\mathbb{H}^{\\otimes}}^{\\ell}(x,t)\\right]\\right)}\\\\ &{}&{\\times\\exp\\left(\\sum_{\\ell\\in\\mathcal{T}_{0}}\\int d x\\left[j_{\\mathbb{H}^{\\otimes}}^{x_{\\ell}^{\\ell}}(x,t)\\cdot\\chi_{V\\mathbb{H}^{\\otimes}}^{\\ell}(x,t)+j_{\\mathbb{H}^{\\otimes}}^{\\ell_{\\ell}^{\\ell}}(x,t)\\cdot\\xi_{V\\mathbb{H}^{\\otimes}}^{\\ell}(x,t)\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We multiply by the identity to enforce the definition of these random variables in terms of the initial weights. As an example, we would have for the first random variable $\\pmb{\\chi}_{s}^{\\ell+1}(\\pmb{x},t)$ and its corresponding pair $\\xi_{s}^{\\ell}({\\pmb x},t)$ ", "page_idx": 29}, {"type": "text", "text": "Attention Output Matrices In this section we integrate over $W_{O\\mathfrak{h}}^{\\ell},W_{K\\mathfrak{h}}^{\\ell},W_{Q\\mathfrak{h}}^{\\ell}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\bf\\Pi}_{\\mathrm{{\\tiny{R}}}_{0\\mathrm{{b}}}^{\\ell}(0)}\\exp\\left(-\\displaystyle\\frac{i}{\\sqrt{N}}\\sum_{t\\leq0}\\int{d x\\ \\mathrm{T}{\\cal W}_{{\\cal O}\\mathrm{{b}}}^{\\ell}(0)^{\\top}\\left[\\hat{\\chi}_{{\\cal O}\\mathrm{{b}}\\mathrm{s}}^{\\ell}(x,t)v_{\\mathrm{{\\tiny{b}}}}^{\\ell}(x,t)^{\\top}+\\displaystyle\\frac{1}{\\sqrt{\\mathcal{H}}}g_{\\mathrm{{s}}}^{\\ell+1}(x,t)\\hat{\\xi}_{{\\cal O}\\mathrm{{b}}}^{\\ell}(x,t)\\right.\\right.}}}\\\\ {{\\mathrm{}}}\\\\ {{\\left.{=-\\displaystyle\\frac{1}{2}\\sum_{t^{\\prime}\\leq\\mathrm{s^{\\prime}}}\\int{d x d x^{\\prime}\\hat{\\chi}_{{\\cal O}\\mathrm{{b}}}^{\\ell}(x,t)\\cdot\\hat{\\chi}_{{\\cal O}\\mathrm{{b}}\\mathrm{s}^{\\ell}}^{\\ell}(x^{\\prime},t^{\\prime})V_{\\mathrm{{\\tiny{b}}}\\mathrm{s}^{\\ell}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})}}}\\\\ {{\\mathrm{}}}\\\\ {{\\left.{=-\\displaystyle\\frac{1}{2}\\sum_{t^{\\prime}\\leq s^{\\prime}}\\int{d x d x^{\\prime}\\hat{\\xi}_{{\\cal O}\\mathrm{{b}}}^{\\ell}(x,t)\\cdot\\hat{\\xi}_{{\\cal O}\\mathrm{{b}}^{\\ell}}^{\\ell}(x^{\\prime},t^{\\prime})G_{{\\cal{s}}s^{\\prime}}^{\\ell+1}(x,x^{\\prime},t,t^{\\prime})}}}\\\\ {{\\mathrm{}}}\\\\ {{\\left.{-i\\displaystyle\\sum_{t^{\\prime}\\leq s^{\\prime}}\\int{d x d x^{\\prime}\\left[\\hat{\\xi}_{{\\cal O}\\mathrm{b}}^{\\ell}(x,t)\\cdot v_{\\mathrm{{\\tiny{b}}}\\mathrm{s}}^{\\ell}(x,t)R_{{\\mathrm{{\\tiny{b}}}}\\mathrm{s}^{\\ell+1}}^{\\ell+1}(x,x^{\\prime})\\right]}}\\end{array}\\right.}}\\\\ {{\\mathrm{}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we introduced the response function ", "page_idx": 29}, {"type": "equation", "text": "$$\nR_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{g^{\\ell+1},\\chi_{O}^{\\ell}}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})\\equiv-\\frac{i}{N\\sqrt{\\mathcal{H}}}g_{\\mathfrak{s}}^{\\ell+1}(\\pmb{x},t)\\cdot\\hat{\\chi}_{O\\mathfrak{h}\\mathfrak{s^{\\prime}}}^{\\ell}(\\pmb{x}^{\\prime},t^{\\prime}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Value Matrices Next, we average over the value matrices $W_{V\\mathfrak{h}}^{\\ell}(0)$ which gives ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{n\\mathbb{E}}_{W_{V_{\\hbar}}^{\\ell}(0)}\\exp\\left(-\\frac{i}{\\sqrt{N}}\\sum_{t\\leq}\\int{d x\\mathrm{\\T}W_{V\\hbar}^{\\ell}(0)^{\\top}\\left[\\frac{1}{\\sqrt{\\mathcal{H}}}\\hat{\\chi}_{V\\uparrow\\mathfrak{b}}^{\\ell}(x,t)h_{\\mathfrak{s}}^{\\ell}(x,t)^{\\top}+g_{0\\mathfrak{b}\\mathfrak{s}}^{\\ell}(x,t)\\hat{\\xi}_{V\\uparrow\\mathfrak{b}}^{\\ell}(x,t)\\right]^{-1}}\\right)}}\\\\ {{\\displaystyle=-\\frac{1}{2}\\sum_{t\\in^{\\prime}\\mathbb{S}^{\\ell}}\\int{d x d x^{\\prime}\\left[\\hat{\\chi}_{V\\uparrow\\mathfrak{b}}^{\\ell}(x,t)\\cdot\\hat{\\chi}_{V\\uparrow\\mathfrak{b}}^{\\ell}(x,t)H_{\\mathfrak{s}\\mathfrak{s}^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})+\\hat{\\xi}_{V\\uparrow\\mathfrak{b}\\mathfrak{s}}^{\\ell}(x,t)\\cdot\\hat{\\xi}_{V\\uparrow\\mathfrak{b}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})G_{\\mathcal{O}\\mathbb{b}\\mathfrak{s}^{\\prime}}^{\\ell}(x)\\right]}}}\\\\ {{\\displaystyle-i\\sum_{t\\in^{\\prime}\\mathbb{S}^{\\ell}}\\int{d x d x^{\\prime}\\left[\\hat{\\chi}_{V\\uparrow\\mathfrak{b}}^{\\ell}(x,t)\\cdot g_{\\mathcal{O}\\mathbb{b}\\mathfrak{s}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})R_{\\mathfrak{b}\\mathrm{s}^{\\prime}}^{h^{\\ell},\\xi_{V}^{\\ell}}(x,x^{\\prime},t,t^{\\prime})\\right]}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(5)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(5)\\times\\mathcal{O}(x^{\\prime})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Key/Query Matrices Next, we need to perform the averages involving $W_{K\\mathfrak{h}}(0)$ which has entries resulting in ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{n\\mathbb{E}}_{W_{k\\mathfrak{h}}^{\\ell}(0)}\\exp\\left(-\\frac{i}{\\sqrt{N}}\\sum_{t\\leq0}\\int d x\\,\\mathrm{T}W_{K\\mathfrak{h}}^{\\ell}(0)^{\\top}\\left[\\frac{1}{\\sqrt{\\mathcal{H}}}\\hat{\\chi}_{K\\mathfrak{h}\\mathfrak{b}}^{\\ell}(x,t)h_{\\mathfrak{s}}^{\\ell}(x,t)^{\\top}+q_{\\mathfrak{h}\\mathfrak{b}}^{\\ell}(x,t)\\hat{\\xi}_{K\\mathfrak{h}\\mathfrak{b}}^{\\ell}(x,t)^{\\top}\\right]\\right.}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}=-\\frac{1}{2}\\sum_{t\\in\\mathbb{R}^{\\prime}\\leq s^{\\prime}}\\int d x d x^{\\prime}\\left[\\hat{\\chi}_{K\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)\\cdot\\hat{\\chi}_{K\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)H_{\\mathfrak{s}\\mathfrak{s}^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})+\\hat{\\xi}_{K\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)\\cdot\\hat{\\xi}_{K\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})Q_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x^{\\prime})\\right]}}\\\\ {{\\displaystyle}-i\\sum_{t\\in\\mathbb{R}^{\\prime}\\leq s^{\\prime}}\\int d x d x^{\\prime}\\left[\\hat{\\chi}_{K\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)\\cdot q_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})H_{\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{k\\ell}(x,x^{\\prime},t,t^{\\prime})\\right]}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle\\left.\\int_{\\mathfrak{h}\\mathfrak{s}^{\\prime}\\in\\mathcal{K}}^{h^{\\ell}\\xi_{K\\mathfrak{b}}^{\\ell}}(x,x^{\\prime},t,t^{\\prime})\\equiv-\\frac{i}{N\\sqrt{\\mathcal{H}}}h_{\\mathfrak{s}}^{\\ell}(x,t)\\cdot\\hat{\\xi}_{K\\mathfrak{b}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})\\right.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We follow an identical procedure for the query matrices $W_{Q}^{\\ell}(0)$ . ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{n\\,}\\mathbb{E}_{W_{\\phi_{\\mathbb{b}}}^{\\ell}(0)}\\exp\\left(-\\frac{i}{\\sqrt{N}}\\sum_{t\\leq\\ell}\\int d x\\,\\mathrm{T}W_{Q\\mathbb{b}}^{\\ell}(0)^{\\top}\\left[\\frac{1}{\\sqrt{H}}\\hat{\\chi}_{Q\\mathbb{b}}^{\\ell}(x,t)h_{\\mathfrak{s}}^{\\ell}(x,t)^{\\top}+k_{\\mathbb{b}_{\\mathfrak{s}}}^{\\ell}(x,t)\\hat{\\xi}_{Q\\mathbb{b}\\mathfrak{s}}^{\\ell}(x,t)^{\\top}\\right]^{-1}\\!\\!\\!\\!\\!}}}\\\\ {{\\displaystyle=-\\frac{1}{2}\\sum_{t\\in\\mathbb{Z}_{\\mathbb{b}}^{\\ell}\\times\\mathcal{T}}\\int d x d x^{\\prime}\\left[\\hat{\\chi}_{Q\\mathbb{b}}^{\\ell}(x,t)\\cdot\\hat{\\chi}_{Q\\mathbb{b}}^{\\ell}(x,t)H_{\\mathfrak{s}\\nu}^{\\ell}(x,x^{\\prime},t,t^{\\prime})+\\hat{\\xi}_{Q\\mathbb{b}}^{\\ell}(x,t)\\cdot\\hat{\\xi}_{Q\\mathbb{b}^{\\ell}}^{\\ell}(x^{\\prime},t^{\\prime})K_{\\mathfrak{b}\\mathrm{s}^{\\prime}}^{\\ell}(x,t)\\right]^{-1}\\!\\!\\!\\!\\!}}\\\\ {{\\displaystyle-i\\sum_{t\\in\\mathbb{Z}_{\\mathbb{b}}^{\\ell}\\times\\mathcal{T}}\\int d x d x^{\\prime}\\left[\\hat{\\chi}_{Q\\mathbb{b}}^{\\ell}(x,t)\\cdot k_{\\mathfrak{b},\\mathfrak{s}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})H_{\\mathfrak{b}\\mathfrak{s}^{\\prime}}^{h^{\\ell},\\xi_{Q}^{\\ell}}(x,x^{\\prime},t,t^{\\prime})\\right]}}\\\\ {{\\displaystyle\\left.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Enforce Kernel Definitions After this step, we can introduce new resolutions of the identity for each of the kernels that appeared in the above computation ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1=\\int\\!\\frac{d H_{\\mathrm{ss^{\\prime}}}^{\\ell}\\left(x,x^{\\prime},t,t^{\\prime}\\right)d\\hat{H}_{\\mathrm{ss^{\\prime}}}^{\\ell}\\left(x,x^{\\prime},t,t^{\\prime}\\right)}{2\\pi i N^{-1}\\mathcal{H}^{-1}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{\\qquad\\quad\\exp\\left(\\hat{H}_{\\mathrm{ss^{\\prime}}}^{\\ell}\\!\\left(x,x^{\\prime},t,t^{\\prime}\\right)\\left[N\\mathcal{H}H_{\\mathrm{ss^{\\prime}}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})-h_{\\mathrm{s}}^{\\ell}(x,t)\\cdot h_{\\mathrm{s^{\\prime}}}^{\\ell}(x^{\\prime},t^{\\prime})\\right]\\right)}\\\\ &{1=\\int\\!\\frac{d G_{\\mathrm{ss^{\\prime}}}^{\\ell}\\left(x,x^{\\prime},t,t^{\\prime}\\right)d\\hat{G}_{\\mathrm{ss^{\\prime}}}^{\\ell}\\left(x,x^{\\prime},t,t^{\\prime}\\right)}{2\\pi i N^{-1}\\mathcal{H}^{-1}}}\\\\ &{\\qquad\\quad\\exp\\left(\\hat{G}_{\\mathrm{ss^{\\prime}}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})\\left[N\\mathcal{H}G_{\\mathrm{ss^{\\prime}}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})-g_{\\mathrm{s}}^{\\ell}(x,t)\\cdot g_{\\mathrm{s^{\\prime}}}^{\\ell}(x^{\\prime},t^{\\prime})\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This is repeated for all of the response functions which involve sums over $N\\mathcal{H}$ variables ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle1=\\int_{-\\infty}^{\\infty}\\biggl[\\!\\!\\frac{\\displaystyle\\langle R_{\\mu}^{(\\nu),\\nu;\\lambda}\\rangle\\!\\left\\langle x_{\\nu},x_{\\mu}^{\\prime},t^{\\prime}\\right\\rangle d x_{\\nu}^{\\prime}\\!\\left(x_{\\nu}^{\\prime},x_{\\nu}^{\\prime},t^{\\prime}\\right)}{\\displaystyle2\\pi\\mathrm{i}\\hbar x^{\\prime}}\\!\\!\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\quad\\quad\\exp\\left(-\\displaystyle{R_{\\mu}^{(\\nu),\\nu;\\lambda}\\!\\left(x^{\\prime},x^{\\prime},t^{\\prime}\\right)}\\left[\\!\\!\\left[N_{\\mu}^{\\mathrm{Re};\\lambda;\\,\\mu}(x,x^{\\prime},t^{\\prime})\\!+\\!\\frac{\\displaystyle\\dot{\\nu}}{\\displaystyle\\sqrt{\\hbar}}M_{\\mu}^{\\mathrm{Re};\\lambda;\\,(\\alpha,t)}\\!\\cdot\\!\\dot{x}_{\\lambda\\mu\\nu}^{\\prime}\\!\\left(x^{\\prime},t^{\\prime}\\right)\\right]\\right)\\!\\right)}\\\\ {\\displaystyle1=\\int_{-\\infty}^{\\infty}\\!\\!\\!\\!\\int_{-\\infty}^{\\infty}\\!\\!\\!\\!\\int_{2}^{\\infty}\\!\\!\\!\\!\\mathrm{d}x^{\\prime}\\!\\!\\,\\frac{\\displaystyle\\langle\\mathbf{r}^{\\prime}\\rangle\\!\\rangle\\!\\left\\langle\\mathbf{r}^{\\prime}\\right\\rangle\\!\\rangle\\!}{\\displaystyle2\\pi\\mathrm{i}\\hbar\\mathrm{\\Omega}^{-1}}\\!\\!\\!\\!}\\\\ &{\\quad\\quad\\exp\\left(-\\displaystyle{R_{\\mu}^{(\\nu),\\nu;\\lambda}\\!\\left(x^{\\prime},x^{\\prime},t^{\\prime}\\right)}\\left[\\!\\!\\left[N_{\\mu}^{\\mathrm{Re};\\lambda;\\,\\mu}(x,x^{\\prime},t^{\\prime})\\!+\\!\\frac{\\displaystyle\\dot{\\nu}}{\\displaystyle\\sqrt{\\hbar}}M_{\\mu}^{\\mathrm{Re};\\lambda;\\,\\xi}\\!\\!\\left(x^{\\prime},t^{\\prime}\\right)\\!\\right]\\right)\\!\\right)}\\\\ {\\displaystyle1=\\int_{-\\infty}^{\\infty}\\!\\!\\!\\!\\int_{-\\infty}^{\\infty}\\!\\!\\!\\!\\int_{2}^{\\infty}\\!\\!\\!\\!\\mathrm{d}x^{\\prime}\\!\\!\\,\\frac{\\displaystyle\\langle\\mathbf{r}^{\\prime}\\rangle\\!\\rangle\\!\\left\\langle\\mathbf{r}^{\\prime}\\right\\rangle\\!\\rangle\\!\\left\\langle\\mathbf{r}^{\\prime}\\right\\rangle\\!\\!\\rangle\\!}{\\displaystyle2\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "There are other kernels which are only relevant within a single head including $\\{Q_{\\mathfrak{h}}^{\\ell},K_{\\mathfrak{h}}^{\\ell},A_{\\mathfrak{h}}^{\\ell},M_{\\mathfrak{h}}^{\\ell}\\}$ . These ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{1=\\int\\!\\!\\frac{d\\hat{L}_{\\mathrm{ts^{\\prime}}}^{(2)}(x,x^{\\prime},t^{\\prime},t^{\\prime})d\\hat{Q}_{\\mathrm{ts^{\\prime}}}^{(\\pm\\nu,\\tau^{\\prime},t^{\\prime})}}{2\\pi i\\Lambda^{-1}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We now combine all of the order parameters into a large collection $Q$ which are vectorized over all layer, time, spatial and sample indices ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q=\\mathrm{Vec}\\{H^{\\ell},G^{\\ell},V^{\\ell},K^{\\ell},Q^{\\ell}\\}}\\\\ &{\\qquad\\qquad\\cup\\,\\{\\hat{H}^{\\ell},\\hat{G}^{\\ell},\\hat{V}^{\\ell},\\hat{K}^{\\ell},\\hat{Q}^{\\ell}\\}}\\\\ &{\\qquad\\qquad\\cup\\,\\{R^{v^{\\ell}\\xi_{O}^{\\ell}},R^{g^{\\ell+1},\\chi_{O}^{\\ell}},R^{k^{\\ell}\\chi_{Q}^{\\ell}},R^{q^{\\ell},\\chi_{K}^{\\ell}}\\}}\\\\ &{\\qquad\\qquad\\cup\\,\\{R^{h^{\\ell},\\xi_{V}^{\\ell}},R^{h^{\\ell},\\xi_{K}^{\\ell}},R^{h^{\\ell},\\xi_{Q}^{\\ell}},R^{h^{\\ell},\\xi_{V}^{\\ell}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "After introducing this collection of order parameters, our original MGF satisfies a large deviation principle with ", "page_idx": 31}, {"type": "equation", "text": "$$\nZ\\propto\\int\\!d{\\pmb Q}\\,\\exp\\left(N\\hbar L\\,S({\\pmb Q})\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\varepsilon:=\\frac{1}{L}\\displaystyle\\sum_{l\\in\\partial\\cap E}^{\\varepsilon\\setminus\\varepsilon}\\int d\\alpha\\alpha^{\\prime}(\\overline{{f}}_{w^{l}}^{l}(x,x^{\\prime},t^{\\prime}))\\hat{H}_{w^{l}}^{l}(x,x^{\\prime},t^{\\prime})+\\mathcal{O}_{u v}^{l}(x,x^{\\prime},t^{\\prime})\\hat{Q}_{w^{l}}^{l}(x,x^{\\prime},t^{\\prime})\\Big]}}\\\\ {{}}\\\\ {{}}\\\\ {{}}\\\\ {{}}\\\\ {{}{\\displaystyle+\\frac{1}{H\\hat{L}_{w_{1}\\to\\varepsilon_{1}}^{\\infty}}\\sum_{i=1}^{\\varepsilon}\\int d\\alpha^{\\prime}\\alpha^{\\prime}\\left[\\hat{f}_{i,w^{\\prime}}^{l}(x,x^{\\prime},t^{\\prime})\\hat{Q}_{w^{l}}^{l}(x,x^{\\prime},t^{\\prime})+\\hat{K}_{w^{\\prime}}^{l}(x,x^{\\prime},t^{\\prime})\\hat{K}_{w^{l}}^{l}(x,x^{\\prime},t^{\\prime})\\right]}}\\\\ {{}}\\\\ {{}\\displaystyle+\\frac{1}{H\\hat{L}_{w_{1}\\to\\varepsilon_{1}}^{\\infty}}\\sum_{i=1}^{\\varepsilon}\\int d\\alpha^{\\prime}\\alpha^{\\prime}\\left[\\hat{f}_{i,w^{\\prime}}^{l}(x,x^{\\prime},t^{\\prime})\\hat{V}_{i,w^{\\prime}}^{l}(x,x^{\\prime},t^{\\prime})\\right]}}\\\\ {{}}\\\\ {{}\\displaystyle+\\frac{1}{H\\hat{L}_{w_{1}\\to\\varepsilon_{1}}^{\\infty}}\\sum_{i=1}^{\\varepsilon}\\int d\\alpha\\left[\\hat{\\xi}_{i,w}^{l}(x,t)\\hat{A}_{w^{l}}^{l}(x,x^{\\prime})\\hat{\\xi}_{i-1}^{l}\\hat{Q}_{w^{l}}^{l}(x,x^{\\prime},t^{\\prime})\\right]}}\\\\ {{}}\\\\ {{}\\displaystyle-\\frac{1}{H\\hat{L}_{w_{1}\\to\\varepsilon_{1}}^{\\infty}}\\sum_{i=1}^{\\varepsilon}\\int d\\alpha^{\\prime}\\alpha^{\\prime}\\left[\\hat{f}_{i,w_{1}^{\\prime}}^{l}(x,x^{\\prime},t^{\\prime})\\hat{\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Zres ", "page_idx": 33}, {"type": "text", "text": "d\u03c7\u02c6Ohs(x, t)d\u03c7Ohs(x, t)d\u03be\u02c6V hs(x, t)d\u03beV hs(x, t)d\u03be\u02c6Khs(x, t)d\u03beKhs(x, t)d\u03be\u02c6Qhs(x, t)d\u03beQhs(x, t) ", "page_idx": 33}, {"type": "image", "img_path": "p0BBKhD5aI/tmp/60037422f9977dc2f93518e944920500315e81ea31de7ecc095ed5a414fdb290.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Though this expression is cumbersome, we will show that, since ${\\hat{H}},{\\hat{G}}$ vanish at their saddle point this MGF merely encodes the following statistical description of the fields of interest such as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\chi_{O\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)=u_{O\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)+\\displaystyle\\frac{1}{\\sqrt{\\mathcal{H}}}\\sum_{t^{\\prime}\\mathfrak{s^{\\prime}}}\\int d\\mathbf{x}^{\\prime}R_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{v^{\\ell},\\xi_{O}^{\\ell}}(x,x^{\\prime},t,t^{\\prime})g_{\\mathfrak{s^{\\prime}}}^{\\ell+1}(x^{\\prime},t^{\\prime})}}\\\\ {{\\displaystyle u_{O\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)\\sim\\mathcal{G P}\\left(0,V_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We note that $h$ only depends on $\\begin{array}{r}{\\bar{\\chi}_{O}^{\\ell}=\\frac{1}{\\sqrt{\\mathcal{H}}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}\\chi_{O\\mathfrak{h}}^{\\ell}\\sigma_{\\mathfrak{h}}^{\\ell}}\\end{array}$ so it has the form ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\bar{\\chi}_{O}^{\\ell}=\\frac{1}{\\sqrt{\\mathcal{H}}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}u_{O\\mathfrak{h}}^{\\ell}\\sigma_{\\mathfrak{h}}^{\\ell}+\\bar{R}^{v^{\\ell}\\xi_{O}^{\\ell}}g^{\\ell}\\ ,\\ \\bar{R}^{v^{\\ell}\\xi_{O}^{\\ell}}=\\frac{1}{\\mathcal{H}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}R_{\\mathfrak{h}}^{v^{\\ell},\\xi_{O}^{\\ell}}\\sigma_{\\mathfrak{h}}^{\\ell}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This fact will be important when we take the large head limit with $N$ fixed in Appendix E.3. Next, we analyze the single site MGF for the hidden MHSA layers ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{Z}_{\\operatorname*{min},\\mathbf{h}}^{i}}&{\\!=\\!\\int_{\\frac{\\prod}{\\alpha}}\\frac{d\\alpha_{22}\\sinh(x_{i}\\cos(t),d\\zeta_{\\operatorname*{max}}(x_{i},t))\\sinh(x_{i}\\cos(t),d\\zeta_{\\operatorname*{max}}(x_{i},t))}{2\\pi}\\sin(\\pi\\log(x_{i},t))}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V a r\\left(\\frac{1}{n-2}\\sum_{k=0}^{J}f\\left(d\\omega^{k}\\xi_{1}\\right)\\xi_{0}(x,t)\\xi_{1}(x,t)\\xi_{2}(x,t^{\\prime},x^{\\prime})\\right)}\\\\ &{\\approx\\log\\left(\\frac{1}{n-2}\\sum_{k=0}^{J}f\\left(d\\omega^{k}\\xi_{1}(x,t)\\xi_{0}(x,t)\\xi_{2}(x,t^{\\prime},x^{\\prime})\\right)\\right.}\\\\ &{\\left.\\times\\log\\left(\\frac{1}{n-2}\\sum_{k=0}^{J}f\\left(d\\omega^{k}\\xi_{2}(x,t)\\xi_{1}(x,t)\\xi_{2}(x,t^{\\prime},x^{\\prime})\\right)\\right.\\right.}\\\\ &{\\left.\\times\\log\\left(\\frac{1}{n-2}\\sum_{k=0}^{J}f\\left(d\\omega^{k}\\xi_{2}(x,t)\\xi_{2}(x,t)\\xi_{2}(x,t^{\\prime},x^{\\prime})\\right)\\right.\\right.}\\\\ &{\\left.\\times\\log\\left(\\frac{1}{n-2}\\sum_{k=0}^{J}f\\left(d\\omega^{k}\\xi_{2}(x,t)\\xi_{2}(x,t)+\\xi_{1}(x,t)\\xi_{2}(x,t)\\xi_{1}(x,t)\\right)\\right)}\\\\ &{\\ \\times\\log\\left(\\frac{1}{n-2}\\sum_{k=0}^{J}f\\left(d\\omega^{k}\\xi_{2}(x,t)\\xi_{2}(x,t)+\\xi_{2}(x,t)\\xi_{1}(x,t)\\xi_{2}(x,t)\\right)\\right)}\\\\ &{\\left.\\times\\log\\left(\\frac{1}{n-2}\\sum_{k=0}^{J}f\\left(d\\omega^{k}\\xi_{2}^{\\prime}\\xi_{1}(x,t)\\xi_{2}(x,t^{\\prime})\\xi_{2}(x,t)\\xi_{2}(x,t^{\\prime})\\right)\\right.}\\\\ &{\\left.\\times\\log\\left(-\\frac{1}{n-2}\\sum_{k=0}^{J}f\\left(d\\omega^{k}\\xi_{1}^{\\prime}\\xi_{2}^{\\prime}\\xi_{1}(x,t)\\xi_{2}(x,t)\\xi_{2}(x,t^{\\prime})\\right)\\right.\\right.}\\\\ &{\\left.\\times\\log\\left(-\\frac{1}{n-2}\\sum_{k=0}^{J}f\\left(d\\omega^{k}\\xi_{2}^{\\prime}\\xi_{2 \n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "E.2 Infinite $N$ (Key/Query dimension) Limit ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "First, we take the $N\\rightarrow\\infty$ limit with $\\mathcal{H},\\mathcal{L}$ fixed. This can be obtained with a simple saddle point procedure using the action in the form written in the previous section. This calculation exactly mimics prior works [15, 11] where all of the order parameters $Q$ take on their values at the saddle point $Q^{\\star}$ . ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\partial S(Q)}{\\partial Q}|_{Q^{\\star}}=0\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Saddle Point Values for Order Parameters Under this saddle point, all of the order parameters presented will concentrate and all neurons will become statistically independent. The governing equations for the order parameters $Q^{\\star}$ are given in terms of averages $\\langle\\cdot\\rangle$ over the single site densities defined by the moment generating functionals $\\mathcal{Z}$ and have the form ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\mathrm{lof}}^{\\mathrm{T}}\\left(\\tau,x,t^{\\prime},t^{\\prime}\\right)=\\delta\\left(\\tau,\\Delta_{i}^{\\mathrm{di}},\\tau\\right)\\!\\Bigg(u_{x}^{t},\\sigma_{i}^{\\prime},t^{\\prime}\\!,u_{y}^{t}\\Bigg),\\ \\mathrm{Gea}^{\\top}(\\tau,x,t^{\\prime})=\\!\\left(\\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2}\\sigma_{i}^{2}}\\Delta_{i}^{\\mathrm{di}}\\!\\!\\right)\\!,}\\\\ &{M_{\\mathrm{dif}}^{\\mathrm{T}}\\left(\\tau,x,t^{\\prime}\\right)\\!\\!\\!(u_{x}^{t},\\sigma_{i}^{\\prime},t^{\\prime}\\right)\\!\\!,\\ \\mathrm{vin}\\!\\!\\!(\\rho_{i}^{\\prime},\\sigma_{i}^{\\prime},t^{\\prime}\\!\\,u_{y}^{t})\\!\\Bigg(\\tau,\\left(x,t^{\\prime}\\right)\\!\\!\\right)\\!,}\\\\ &{C_{a v}^{\\dagger}\\left(\\tau,x,t^{\\prime}\\right)\\!\\!\\!(\\tau,x,t^{\\prime})\\!\\!\\!(\\tau,x,t^{\\prime})\\!\\!,\\ \\mathrm{vin}\\!\\!\\!(\\rho_{i}^{\\prime},\\sigma_{i}^{\\prime},t^{\\prime}\\!\\,u_{x}^{t})\\!\\Bigg(\\tau,\\left(x,t^{\\prime}\\right)\\!\\!\\right)\\!,}\\\\ &{A_{a v}^{\\dagger}\\left(\\tau,x\\right)\\!\\!\\!\\left(u_{x}^{t},\\sigma_{i}^{\\prime},t^{\\prime}\\!\\right)\\!\\!,\\ \\mathrm{vin}\\!\\!\\!(\\rho_{i}^{\\prime},\\sigma_{i}^{\\prime})\\!\\Bigg(\\tau,x,t^{\\prime}\\!\\,u_{x}^{t}\\!\\in\\!\\left(\\mathbb{R}_{a}^{T},\\sigma_{i}^{\\prime},t^{\\prime}\\right)\\!\\right)}\\\\ &{M_{\\mathrm{dif}}^{\\mathrm{TH}}\\left(\\tau,x\\right)\\!\\!\\!\\left(\\tau,x,t^{\\prime}\\right)\\!\\!\\!(\\tau,x,t^{\\prime})\\!\\!,}\\\\ &{H_{\\mathrm{bav}}^{\\mathrm{TH}}\\!\\!\\!+\\!\\!\\frac{1}{\\sigma_{i}^{2}\\sigma_{i}^{2}}\\!\\!\\!+\\!\\!\\frac{1}{\\sigma_{i}^{2}\\sigma_{i}^{2}}\\!\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Single Site Stochastic Processes Our fields of interest will obey the stochastic dynamics ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\operatorname*{det}\\{R_{j}\\}}=\\nu_{i}(x,t)+\\sqrt{\\sum_{k=1}^{N}\\sum_{p\\in\\partial\\mathcal{G}(x,t)=1}^{P}\\left(\\alpha_{k}\\nu_{i}^{(2)}(x,t)\\nu_{i k}^{(1)}(x,t^{\\prime})\\right.}\\\\ &{\\left.\\times\\int_{\\Omega}\\left(x,t^{\\prime}\\right)-\\phi^{*}\\left(y\\right)\\nu_{i k}^{(1)}\\omega_{i k}\\nu_{i k}^{(2)}(x,t^{\\prime})d x\\right)}\\\\ &{\\left.\\mathcal{U}_{i,k}\\left(x,t^{\\prime}\\right)=\\Gamma_{i k}(x,t)+\\frac{\\Gamma_{i k}^{\\prime}}{\\sqrt{2}}\\int\\mathrm{d}\\pi\\mu\\mu\\mu_{i k}^{(3)}(x,t^{\\prime})\\nu_{i k}^{(1)}(x,t^{\\prime})\\right.}\\\\ &{\\left.\\mathcal{U}_{i,k}\\left(x,t\\right)-\\phi^{*}\\left(y\\right)\\,G\\!\\left(x,t^{\\prime}\\right)\\right.}\\\\ &{\\left.\\int_{\\Omega}\\left(x,t\\right)-\\phi^{*}\\left(x,t^{\\prime}\\right)G\\!\\left(x,t^{\\prime}\\right)+\\frac{\\Gamma_{i k}^{\\prime}}{\\sqrt{2}}\\int\\mathrm{d}\\pi\\mu\\mu_{i k}^{(2)}(x,t^{\\prime})d x\\right)\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int d\\Omega\\!\\left(x,t^{\\prime}\\right)\\,}\\\\ &{\\xi_{i,k}\\left(x,t^{\\prime}\\right)=-\\xi_{i}\\left(y\\right)\\,G\\!\\left(x,t^{\\prime}\\right)\\!\\left(x,t^{\\prime}\\right)\\!\\left(x,t^{\\prime}\\right)}\\\\ &{\\left.\\mathcal{U}_{i,k}\\left(x,t\\right)-\\phi^{*}\\left(y\\right)\\,G\\!\\left(x,t^{\\prime}\\right)\\right)+\\frac{\\Gamma_{i k}^{\\prime}}{\\sqrt{2}}\\int\\mathrm{d}\\pi\\mu\\,\\alpha_{i}^{(2)}(x,t^{\\prime})G\\!\\left(x,t^{\\prime}\\right)}\\\\ &{\\left.\\mathcal{U}_{i,k}\\left(x,t\\right)-\\xi_{i}\\left(x,t^{\\prime}\\right)\\,G\\!\\left(x,t^{\\prime}\\right)+\\frac{\\Gamma_{i k}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Residual Stream The forward pass residual variables obey the following stochastic process ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\mathfrak{s}}^{\\ell+1}(x,t)=h_{\\mathfrak{s}}^{\\ell}(x,t)+\\beta_{0}L^{-\\alpha_{L}}\\bar{\\chi}_{O\\mathfrak{h}\\mathfrak{s}}^{\\ell}(x,t)}\\\\ &{\\qquad\\qquad+\\;\\eta_{0}\\gamma_{0}\\beta_{0}^{2}L^{-1}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{x^{\\prime}\\sim\\mathfrak{B}_{t^{\\prime}}}\\Delta(x^{\\prime},t^{\\prime})\\sum_{\\mathfrak{s^{\\prime}}}g_{\\mathfrak{s^{\\prime}}}^{\\ell+1}(x^{\\prime},t^{\\prime})V_{\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{\\ell\\sigma}(x,x^{\\prime},t,s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\bar{\\chi}_{O\\mathfrak{h}\\mathfrak{s}}^{\\ell}(\\pmb{x},t)=\\frac{1}{\\sqrt{\\mathcal{H}}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}\\sum_{\\mathfrak{s}^{\\prime}\\in[S]}\\sigma_{\\mathfrak{h}\\mathfrak{s}\\mathfrak{s}^{\\prime}}^{\\ell}(\\pmb{x},t)\\chi_{O\\mathfrak{h}\\mathfrak{s}^{\\prime}}^{\\ell}(\\pmb{x},t)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "the backward pass satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{\\mathfrak{s}}^{\\ell}(x,t)=g_{\\mathfrak{s}}^{\\ell+1}(x,t)+\\frac{\\beta_{0}}{L^{\\alpha_{L}}}\\left[\\bar{\\xi}_{Q\\mathfrak{s}}^{\\ell}(x,t)+\\bar{\\xi}_{K\\mathfrak{s}}^{\\ell}(x,t)+\\bar{\\xi}_{V\\mathfrak{s}}^{\\ell}(x,t)\\right]}\\\\ &{\\qquad\\qquad+\\left.\\frac{\\beta_{0}^{2}\\eta_{0}\\gamma_{0}}{L}\\sum_{t<t^{\\prime}}\\Delta_{x^{\\prime}\\sim t^{\\prime}}\\Delta(x^{\\prime},t^{\\prime})G_{O_{88^{\\prime}}^{\\ell}}^{\\ell\\sigma}(x,x^{\\prime},t,t^{\\prime})h_{\\mathfrak{s^{\\prime}}}^{\\ell}(x^{\\prime},t^{\\prime})\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\left.\\frac{\\beta_{0}^{2}\\eta_{0}\\gamma_{0}}{L}\\sum_{t<t^{\\prime}}\\mathbb{E}_{x^{\\prime}\\sim t^{\\prime}}\\Delta(x^{\\prime},t^{\\prime})\\left[K_{\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{\\ell M\\dot{\\sigma}}(x,x^{\\prime},t,t^{\\prime})+Q_{\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{\\ell M\\dot{\\sigma}}(x,x^{\\prime},t,t^{\\prime})\\right]h_{\\mathfrak{s^{\\prime}}}^{\\ell}(x^{\\prime},t^{\\prime})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The keys and queries have dynamics ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\ell}{\\mathfrak{h}^{\\mathrm{s}}}(\\mathbf{x},t)=\\chi_{K\\mathfrak{h}^{\\mathrm{p}}}^{\\ell}(x,t)}\\\\ &{\\qquad\\qquad+\\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}N^{1-\\alpha_{A}}}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{\\mathbf{x}\\sim\\mathfrak{B}_{t^{\\prime}}}\\sum_{s^{\\prime}s^{\\prime\\prime}s^{\\prime\\prime}}\\Delta(\\mathbf{x},t^{\\prime})M_{\\mathfrak{h}^{\\mathrm{s}^{\\prime}}s^{\\prime\\prime}}^{\\ell}(\\mathbf{x},t^{\\prime})\\dot{\\sigma}_{s^{\\prime}s^{\\prime\\prime}s^{\\prime\\prime}}^{\\ell}\\sigma_{\\mathfrak{h}s^{\\prime\\prime}}^{\\ell}(x,t^{\\prime})H_{s\\circ\\prime}^{\\ell}(x,x^{\\prime},t)}\\\\ &{\\qquad\\qquad+\\chi_{Q\\mathfrak{h}^{\\mathrm{s}}}^{\\ell}(x,t)}\\\\ &{\\qquad\\qquad+\\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathfrak{B}_{t^{\\prime}}}\\sum_{s^{\\prime}s^{\\prime\\prime}}\\Delta(\\mathbf{x}^{\\prime},t^{\\prime})M_{\\mathfrak{h}^{\\mathrm{s}}s^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})\\dot{\\sigma}_{s^{\\prime}s^{\\prime\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})k_{\\mathfrak{h}s^{\\prime\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})H_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "E.2.1 Multi-head Attention is Single-Head Attention as $N\\rightarrow\\infty$ ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we use the derived saddle point equations for the $N\\rightarrow\\infty$ limit and argue that they imply that all heads in the MHSA layer learn identical attention matrices and contribute the same feature updates to the residual stream. To do so, we proceed in a three step inductive argument. ", "page_idx": 37}, {"type": "text", "text": "1. First, we show that at initialization, all key, query, value and attention matrices $\\{Q_{\\mathfrak{h}},K_{\\mathfrak{h}},V_{\\mathfrak{h}},\\mathcal{A}_{\\mathfrak{h}},M_{\\mathfrak{h}}\\}$ are equal across heads.   \n2. Next, we show inductively that if these quantities $\\{Q_{\\mathfrak{h}},K_{\\mathfrak{h}},V_{\\mathfrak{h}},\\mathcal{A}_{\\mathfrak{h}},M_{\\mathfrak{h}}\\}$ are identical across heads up to some time, then that implies that the response functions $R_{\\mathfrak{h}}$ are also identical across heads up to that time.   \n3. Lastly, we show that if the response functions $R_{\\mathfrak{h}}$ are identical up to some time, then that implies that the MHSA kernels $\\{Q_{\\mathfrak{h}},K_{\\mathfrak{h}},V_{\\mathfrak{h}},\\mathcal{A}_{\\mathfrak{h}},\\mathcal{M}_{\\mathfrak{h}}\\}$ will also be identical across heads at future times. ", "page_idx": 37}, {"type": "text", "text": "First, we note that, at initialization, all of the MHSA kernels are identical across heads since ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\,\\mathfrak{h}\\in[\\mathcal{H}]\\quad Q_{\\mathfrak{h}s}^{\\ell}(x,\\pmb{x}^{\\prime},0,0)=K_{\\mathfrak{h}s}^{\\ell}(\\pmb{x},\\pmb{x}^{\\prime},0,0)=V_{\\mathfrak{h}s}^{\\ell}(\\pmb{x},\\pmb{x}^{\\prime},0,0)=H_{\\mathfrak{s}s^{\\prime}}^{\\ell}(\\pmb{x},\\pmb{x}^{\\prime},0,0)}\\\\ &{M_{\\mathfrak{h}s s^{\\prime}}^{\\ell}(\\pmb{x},0)=A_{\\mathfrak{h}s s^{\\prime}}^{\\ell}(\\pmb{x},0)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Next, we need to analyze the response functions under an inductive hypothesis on the equality of the MHSA kernels. We start by noting that all response functions are causal so we can group the response functions that arise from $\\chi_{O\\mathfrak{h}}^{\\ell}$ with the feature learning update to the residual stream, writing the following compressed equation for the forward and backward passes ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle h_{\\mathfrak{s}}^{\\ell+1}({\\bf x},t)=h_{\\mathfrak{s}}^{\\ell}({\\bf x},t)+\\frac{1}{\\sqrt{H}}\\sum_{\\mathfrak{t}=1}^{N}\\sum_{\\ell=1}^{n}u_{\\mathfrak{t}\\upsilon^{\\prime}}^{\\ell}({\\bf x},t)\\sigma_{\\mathfrak{t}\\upsilon^{\\prime}}^{\\ell}({\\bf x},t)}}\\\\ {{\\displaystyle\\qquad\\qquad+\\eta\\upsilon\\gamma_{0}\\partial_{0}^{2}L^{-1}\\sum_{\\ell<1}^{n}\\sum_{\\ell^{\\prime}=1}^{n}\\int d x^{\\prime}C_{\\mathfrak{t}\\upsilon^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})g_{\\nu^{\\prime}}^{\\ell+1}(x^{\\prime},t^{\\prime})}}\\\\ {{\\displaystyle g_{\\mathfrak{s}}^{\\ell}(x,t)=g_{\\mathfrak{s}}^{\\ell}(x,t)+\\frac{1}{\\sqrt{H}}\\sum_{\\mathfrak{t}=1}^{n}\\gamma_{\\mathfrak{t}\\upsilon^{\\prime}}^{\\ell}(x,t)\\sigma_{\\mathfrak{t}\\upsilon^{\\prime}}^{\\ell}({\\bf x},t)}}\\\\ {{\\displaystyle\\qquad\\qquad+\\frac{1}{\\sqrt{H}}\\sum_{\\mathfrak{t}=1}^{n}r_{\\mathfrak{d}\\upsilon^{\\prime}}^{\\ell}(x,t^{\\prime})\\sigma_{\\mathfrak{t}\\upsilon^{\\prime}\\upsilon^{\\prime}}^{\\ell}(x,t)M_{\\mathfrak{t}\\upsilon^{\\prime}\\upsilon^{\\prime}}^{\\ell}(x,t)}}\\\\ {{\\displaystyle\\qquad\\qquad+\\frac{1}{\\sqrt{H}}\\sum_{\\mathfrak{t}=1}^{n}r_{\\mathfrak{d}\\upsilon^{\\prime}}^{\\ell}(x,t)\\sigma_{\\mathfrak{t}\\upsilon^{\\prime}\\upsilon^{\\prime}}^{\\ell}(x,t)M_{\\mathfrak{t}\\upsilon^{\\prime}\\upsilon^{\\prime}}^{\\ell}(x,t)}}\\\\ {{\\displaystyle\\qquad+\\eta\\upsilon\\gamma_{0}\\partial_{0}^{2}L^{-1}\\sum_{\\ell=1}^{n}\\sum_{\\ell^{\\prime}=1}^{n}\\int d x^{\\prime}C_{\\mathfrak{t}\\upsilon^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})h_{\\mathfrak{t}\\upsilon^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $C^{h^{\\ell}}$ and $C^{g^{\\ell}}$ only involve deterministic head-averaged kernels and thus do not carry a $\\mathfrak{h}$ index. We now derive useful response function identities ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\sqrt{\\mathscr{H}}\\displaystyle\\frac{\\delta h_{\\ell}^{\\ell}(x,t)}{\\delta u_{{\\cal O}\\cup\\left\\langle{\\bf x^{\\prime}},t^{\\prime}\\right\\rangle}}=\\Theta(\\ell-\\ell^{\\prime})\\delta(t-t^{\\prime})\\delta(x-x^{\\prime}){\\sigma_{\\mathrm{bs}^{\\prime}}^{\\ell}}(x,t)}}\\\\ {{\\displaystyle\\ +\\,\\eta_{0}\\gamma_{0}\\beta_{0}^{2}L^{-1}\\displaystyle\\sum_{k=1}^{\\ell}\\sum_{t^{\\prime\\prime}<t}\\sum_{s^{\\prime\\prime}}\\int d x^{\\prime\\prime}C_{5s^{\\prime}}^{h^{k}}(x,x^{\\prime\\prime},t,t^{\\prime\\prime})\\left(\\sqrt{\\mathscr{H}}\\displaystyle\\frac{\\delta g_{9^{\\prime}}^{k+1}(x^{\\prime\\prime},t^{\\prime\\prime})}{\\delta u_{{\\cal O}\\cup\\left\\{{\\bf y^{\\prime}},t^{\\prime}\\right\\}}^{\\ell}(x^{\\prime},t^{\\prime})}\\right)}}\\\\ {{\\displaystyle\\sqrt{\\mathscr{H}}\\displaystyle\\frac{\\delta g_{8^{\\prime\\prime}}^{\\ell}(x^{\\prime\\prime},t^{\\prime\\prime})}{\\delta u_{{\\cal O}\\cup\\left\\{{\\bf y^{\\prime}},t^{\\prime}\\right\\}}^{\\ell}(x^{\\prime},t^{\\prime})}=\\eta_{0}\\gamma_{0}\\beta_{0}^{2}L^{-1}\\displaystyle\\sum_{k=\\ell}^{L}\\sum_{t^{\\prime\\prime}<t}\\sum_{s^{\\prime\\prime}}\\int d x^{\\prime\\prime}C_{5s^{\\prime\\prime}}^{g^{k}}(x,x^{\\prime\\prime},t,t^{\\prime\\prime})\\left(\\sqrt{\\mathscr{H}}\\displaystyle\\frac{\\delta h_{\\ell^{\\prime}}^{k}(x^{\\prime\\prime},t^{\\prime\\prime})}{\\delta u_{{\\cal O}\\cup\\left\\{{\\bf y^{\\prime}},t^{\\prime}\\right\\}}^{\\ell}(x^{\\prime},t^{\\prime})}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "These equations give the needed response function $R^{g^{\\ell+1}\\chi_{O}^{\\ell}}$ . From these equations we immediately see that if $\\sigma_{\\mathfrak{h}^{\\mathfrak{s}\\mathfrak{s}^{\\prime}}}^{\\ell}(\\mathbf{x},t)=\\sigma_{\\mathfrak{h}^{\\prime}{\\mathfrak{s}\\mathfrak{s}^{\\prime}}}^{\\ell}(\\mathbf{x},t)$ (which holds under our inductive hypothesis) then $R_{\\mathfrak{h}}^{g^{\\ell+1}\\chi_{O}^{\\ell}}=$ Rgh\u2032\u2113+1\u03c7\u2113O. This same argument is repeated for all other response functions that are computed as derivatives of residual stream variables. We have thus found that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\forall\\mathfrak{h},\\mathfrak{h}^{\\prime}\\in[\\mathcal{H}]\\ ,\\ R_{\\mathfrak{h}}^{g^{\\ell+1}\\chi_{O}^{\\ell}}=R_{\\mathfrak{h}^{\\prime}}^{g^{\\ell+1}\\chi_{O}^{\\ell}}\\ ,\\ R_{\\mathfrak{h}}^{h^{\\ell}\\xi_{V}^{\\ell}}=R_{\\mathfrak{h}^{\\prime}}^{h^{\\ell}\\xi_{V}^{\\ell}}\\ ,\\ R_{\\mathfrak{h}}^{h^{\\ell}\\xi_{K}^{\\ell}}=R_{\\mathfrak{h}^{\\prime}}^{h^{\\ell}\\xi_{K}^{\\ell}}\\ ,\\ R_{\\mathfrak{h}}^{h^{\\ell}\\xi_{Q}^{\\ell}}=R_{\\mathfrak{h}^{\\prime}}^{h^{\\ell}\\xi_{Q}^{\\ell}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Now, we can analyze the dynamics of the keys, queries and values within a head using the above property and the original inductive hypothesis that ${\\mathcal{A}}_{\\mathfrak{h}}={\\mathcal{A}}_{\\mathfrak{h^{\\prime}}},M_{\\mathfrak{h}}=M_{\\mathfrak{h^{\\prime}}}...$ for times less than $t$ . We will now prove that this implies that these variables will remain the same. We start by examining the keys and queries which have the form ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{k_{\\mathrm{ps}}^{\\ell}(\\pmb{x},t)=u_{K\\mathfrak{h}\\mathfrak{s}}^{\\ell}(\\pmb{x},t)+\\displaystyle\\sum_{t^{\\prime}<t}\\sum_{\\mathfrak{s^{\\prime}}}\\int d\\pmb{x}^{\\prime}C_{\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{k^{\\ell}}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})q_{\\mathfrak{h}\\mathfrak{s^{\\prime}}}^{\\ell}(\\pmb{x}^{\\prime},t^{\\prime})}}\\\\ {{q_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(\\pmb{x},t)=u_{Q\\mathfrak{h}\\mathfrak{s}}^{\\ell}(\\pmb{x},t)+\\displaystyle\\sum_{t^{\\prime}<t}\\sum_{\\mathfrak{s^{\\prime}}}\\int d\\pmb{x}^{\\prime}C_{\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{q^{\\ell}}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})k_{\\mathfrak{h}\\mathfrak{s^{\\prime}}}^{\\ell}(\\pmb{x}^{\\prime},t^{\\prime})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $C_{{\\mathfrak{s s}}^{\\prime}}^{k^{\\ell}}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})$ and $C_{{\\mathfrak{s s}}^{\\prime}}^{q^{\\ell}}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})$ are two operators that do not carry a $\\mathfrak{h}$ (are the same across all heads). These relations can be viewed as a linear system of equations. We let $k_{\\mathfrak{h}}^{\\ell}\\ =$ $\\mathrm{Vec}\\{k_{\\mathfrak{h}\\mathfrak{s}}^{\\ell}(\\mathbf{x},t)\\}_{\\mathfrak{s}\\mathbf{x}t}$ and analogously $C^{k^{\\ell}}=\\operatorname{Mat}\\{C_{\\mathfrak{s}\\mathfrak{s^{\\prime}}}^{k^{\\ell}}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})\\}$ as in the calculations of Bordelon and Pehlevan [15, 41]. Using this shorthand, we can express the key/query and attention kernels as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k_{\\mathrm{t}}^{\\ell}=\\left[I-C^{k^{\\ell}}C^{q^{\\ell}}\\right]^{-1}\\left[u_{K\\setminus\\ell}^{\\ell}+C^{k^{\\ell}}u_{Q\\oplus}^{\\ell}\\right]\\ ,\\ q_{\\ell}^{\\ell}=\\left[I-C^{q^{\\ell}}C^{k^{\\ell}}\\right]^{-1}\\left[u_{Q\\oplus}^{\\ell}+C^{q^{\\ell}}u_{K\\uplus}^{\\ell}\\right]}\\\\ &{K_{\\ell}^{\\ell}=\\left[I-C^{k^{\\ell}}C^{q^{\\ell}}\\right]^{-1}\\left[H^{\\ell}+C^{k^{\\ell}}H^{\\ell}C^{k^{\\ell}\\top}\\right]\\left[I-C^{k^{\\ell}}C^{q^{\\ell}}\\right]^{-1\\top}}\\\\ &{Q_{\\ell}^{\\ell}=\\left[I-C^{q^{\\ell}}C^{k^{\\ell}}\\right]^{-1}\\left[H^{\\ell}+C^{q^{\\ell}}H^{\\ell}C^{q^{\\ell}\\top}\\right]\\left[I-C^{q^{\\ell}}C^{k^{\\ell}}\\right]^{-1\\top}}\\\\ &{A_{\\ell}^{\\ell}=\\left[I-C^{k^{\\ell}}C^{q^{\\ell}}\\right]^{-1}C^{k^{\\ell}}H^{\\ell}C^{q^{\\ell}\\top}\\left[I-C^{q^{\\ell}}C^{k^{\\ell}}\\right]^{-1\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We thus see that the final kernels $K_{\\mathfrak{h}}^{\\ell},Q_{\\mathfrak{h}}^{\\ell},\\mathcal{A}_{\\mathfrak{h}}^{\\ell}$ are all identical across heads. An identical argument can be carried out for the value kernel $V_{\\mathfrak{h}}^{\\ell}$ and the $M_{\\mathfrak{h}}^{\\ell}$ order parameter. ", "page_idx": 38}, {"type": "text", "text": "E.3 Infinite $\\mathcal{H}$ Limit ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this section, we compute the infinite head limit with $N,L$ fixed. This limit is more technically involved than the $N\\rightarrow\\infty$ limit which required only a simple saddle point of the full DMFT action over all kernels. At finite $N$ we cannot use this technique since the kernels within MHSA blocks are random variables. However, as was shown in Bordelon and Pehlevan [17], the DMFT action still contains the necessary information to characterize the distribution over order parameters at finite $N$ . In the case of transformers with infinitely many heads, a subset of the order parameters introduced in the previous section will still concentrate as $\\mathcal{H}\\to\\infty$ including ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle H_{\\mathrm{ss^{\\prime}}}^{\\ell}({\\bf x},{\\bf x}^{\\prime},t,t^{\\prime})=\\frac{1}{N\\mathcal{H}}h_{s}^{\\ell}({\\bf x},t)\\cdot h_{s}^{\\ell}({\\bf x},t)}\\ ~}\\\\ {{\\displaystyle G_{\\mathrm{ss^{\\prime}}}^{\\ell}({\\bf x},{\\bf x}^{\\prime},t,t^{\\prime})=\\frac{1}{N\\mathcal{H}}g_{s}^{\\ell}({\\bf x},t)\\cdot g_{s}^{\\ell}({\\bf x},t)}\\ ~}\\\\ {{\\displaystyle V_{\\mathrm{ss^{\\prime}}}^{\\ell\\sigma}({\\bf x},{\\bf x}^{\\prime},t,t^{\\prime})=\\frac{1}{\\mathcal{H}}\\sum_{\\mathfrak{h}=1}^{\\varkappa}\\sum_{s^{\\prime\\prime}\\in\\mathcal{V}^{\\prime\\prime}}V_{\\mathfrak{h}^{\\mathrm{s^{\\prime\\prime}}}s^{\\prime\\prime\\prime}}^{\\ell\\sigma}({\\boldsymbol x},{\\boldsymbol x}^{\\prime},t,t^{\\prime})\\sigma_{\\mathfrak{h}s^{\\prime\\prime}}^{\\ell}({\\boldsymbol x},t)\\sigma_{\\mathfrak{h}^{\\mathrm{s}s^{\\prime\\prime\\prime}}}^{\\ell}({\\boldsymbol x}^{\\prime},t^{\\prime})}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and many more correlation and response functions. We will call the full collection of all the necessary head-averaged order parameters $Q_{\\mathrm{global}}$ . Further, not all of the stochastic fields will be relevant to characterize the residual stream. Specifically, only head-averaged fields $\\bar{\\chi}_{O}^{\\ell},\\bar{\\xi}_{V}^{\\ell},\\bar{\\xi}_{K}^{\\ell},\\bar{\\xi}_{Q}^{\\ell}$ are relevant. For example, the first of these is defined as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\chi}_{O s}^{\\ell}(x,t)=\\displaystyle\\frac{1}{\\sqrt{\\mathcal{H}}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}\\sum_{s^{\\prime}\\in[S]}\\sigma_{\\mathfrak{h}s^{\\prime}}^{\\ell}(x,t)\\chi_{O\\mathfrak{h}s^{\\prime}}^{\\ell}(x,t)}\\\\ &{=\\displaystyle\\frac{1}{\\sqrt{\\mathcal{H}}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}\\sum_{s^{\\prime}\\in[S]}\\sigma_{\\mathfrak{h}s^{\\prime}}^{\\ell}(x,t)\\left[u_{O\\mathfrak{h}s^{\\prime}}^{\\ell}(x,t)+\\frac{1}{\\sqrt{\\mathcal{H}}}\\sum_{t^{\\prime}s^{\\prime\\prime}}\\int d x^{\\prime}R_{\\mathfrak{h}s^{\\prime}s^{\\prime\\prime}}^{v^{\\ell},\\xi_{O}^{\\ell}}(x,x^{\\prime},t,t^{\\prime})g_{s^{\\prime\\prime}}^{\\ell+1}(x^{\\prime},t^{\\prime})\\right]}\\\\ &{=\\bar{u}_{O s^{\\prime}}^{\\ell}(x,t)+\\sum_{t^{\\prime}s^{\\prime}}\\int d x^{\\prime}\\bar{R}_{s^{\\prime}s^{\\prime}}^{v^{\\ell}\\xi_{O}^{\\sigma}}(x,x^{\\prime},t,t^{\\prime})g_{s^{\\prime}}^{\\ell+1}(x^{\\prime},t^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We note that the above equation is true at any value of $N$ but the covariance of $u_{\\mathfrak{h}}^{\\ell}$ and the response functions Rh $R_{\\mathfrak{h}}^{v^{\\ell}\\xi_{O}^{\\ell}}$ are random variables [17]. However, the residual stream only depends upon collective, head-averaged variables ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\bar{u}_{O\\mathfrak{s}^{\\prime}}^{\\ell}(\\pmb{x},t)=\\displaystyle\\frac{1}{\\sqrt{\\mathcal{H}}}\\sum_{\\mathfrak{b}=1}^{\\mathcal{H}}\\sum_{\\mathfrak{s}^{\\prime}\\in[S]}\\sigma_{\\mathfrak{b}\\mathfrak{s}^{\\prime}}^{\\ell}u_{O\\mathfrak{b}\\mathfrak{s}^{\\prime}}^{\\ell}(\\pmb{x},t)}}\\\\ {{\\bar{R}_{\\mathfrak{s}\\mathfrak{s}^{\\prime}}^{v^{\\ell}\\xi_{O}^{\\ell}\\sigma}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})=\\displaystyle\\frac{1}{\\mathcal{H}}\\sum_{\\mathfrak{b}=1}^{\\mathcal{H}}\\sum_{\\mathfrak{s}^{\\prime\\prime}}\\sigma_{\\mathfrak{b}\\mathfrak{s}^{\\prime\\prime}}^{\\ell}(\\pmb{x},t)R_{\\mathfrak{b}\\mathfrak{s}^{\\prime\\prime}\\mathfrak{s}^{\\prime}}^{v^{\\ell},\\xi_{O}^{\\ell}}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The intuition behind the large $\\mathcal{H}$ limit is that, even though $\\sigma_{\\mathfrak{h}}^{\\ell}$ and $R_{\\mathfrak{h}}^{v^{\\ell}\\xi_{O}^{\\ell}}$ are random variables, there should be a central limit theorem for $\\bar{u}_{O}^{\\ell}$ and a law of large numbers for $\\bar{R}_{\\mathfrak{s s^{\\prime}}}^{v^{\\ell}\\xi_{O}^{\\ell}\\sigma}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})$ ", "page_idx": 39}, {"type": "text", "text": "E.3.1 Partitioning Order Parameters ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Based on the intuition developed in the previous section, we now derive an alternative DMFT action by tracking the moment generating functional for the head-averaged random fields that occur on the residual stream \u03c7\u00af\u2113O, \u03be\u00af\u2113V , \u03be\u00af\u2113K, \u03be\u00af\u2113Q . To characterize this joint distribution, we must also of course keep track of the random fields within the MHSA blocks such as {\u03c7\u2113Qh, \u03c7\u2113Kh, \u03c7\u2113V h, \u03be\u2113Oh}h\u2208[H]. Repeating the path integral setup of the previous section, we need to performing averages over all initial weights such as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathfrak{n}\\mathbb{E}_{\\{W_{0}^{\\ell}(0)\\}}\\exp\\left(-\\frac{i}{\\sqrt{N\\mathcal{H}}}\\sum_{\\mathfrak{h}=1}^{\\mathcal{H}}\\mathrm{Tr}W_{\\mathcal{O}_{\\mathfrak{h}}}^{\\ell}(0)^{\\top}\\sum_{t\\leq s}\\int d x\\left[\\hat{\\chi}_{\\mathcal{O}_{\\mathfrak{h}}}^{\\ell}(x,t)v_{\\mathfrak{h}_{\\mathfrak{s}}}^{\\ell}(x,t)^{\\top}+g_{\\mathfrak{s}}^{\\ell+1}(x,t)\\hat{\\xi}_{\\mathcal{O}_{\\mathfrak{h}}}^{\\ell}(x,t)\\right]\\right)}\\\\ {\\displaystyle}&{\\,\\,=-\\frac{1}{2}\\sum_{t^{\\ell}\\in\\mathcal{S}^{\\prime}}\\int d x d x^{\\prime}\\hat{\\chi}_{\\mathcal{O}_{\\mathfrak{s}}}^{\\ell}(x,t)\\cdot\\hat{\\chi}_{\\mathcal{O}^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})V_{\\mathfrak{s}^{\\prime}}^{\\ell\\sigma}(x,x^{\\prime},t,t^{\\prime})}\\\\ {\\displaystyle}&{\\,\\,\\,-\\frac{1}{2}\\sum_{\\mathfrak{h}\\in\\mathbb{R}}\\sum_{t^{\\ell}\\in\\mathcal{S}^{\\prime}}\\int d x d x^{\\prime}\\hat{\\xi}_{\\mathcal{O}_{\\mathfrak{h}}\\mathfrak{s}}^{\\ell}(x,t)\\cdot\\hat{\\xi}_{\\mathcal{O}_{\\mathfrak{h}}\\mathfrak{s^{\\prime}}}^{\\ell}(x^{\\prime},t^{\\prime})G_{\\mathfrak{s}^{\\prime}}^{\\ell+1}(x,x^{\\prime},t,t^{\\prime})}\\\\ {\\displaystyle}&{\\,\\,\\,-\\frac{1}{N\\mathcal{H}}\\sum_{t^{\\prime}\\in\\mathcal{S}^{\\prime}}\\int d x d x^{\\prime}\\left(\\hat{\\chi}_{\\mathcal{O}_{\\mathfrak{s}}}^{\\ell}(x,t)\\cdot g_{\\mathfrak{s^{\\prime}}}^{\\ell+1}(x^{\\prime},t^{\\prime})\\right)\\left(\\sum_{\\mathfrak{h}\\in[\\mathcal{H}]}v_{\\mathfrak{s}}^{\\ell\\sigma}(x,t)\\cdot\\hat{\\xi}_{\\mathcal{O}_{\\mathfrak{h}^{\\sigma}}}^{\\ell}(x^{\\prime},t^{\\prime})\\right)_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "It is clear that from this integral that the relevant self-averaging order parameters are ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V_{s s^{\\prime}}^{\\ell\\sigma}({\\pmb x},{\\pmb x}^{\\prime},t,t^{\\prime})=\\frac{1}{N\\hbar}\\sum_{\\mathfrak{p}=1}^{\\mathcal{H}}v_{\\mathfrak{p}s}^{\\ell\\sigma}({\\pmb x},t)\\cdot v_{\\mathfrak{h}s^{\\prime}}^{\\ell\\sigma}({\\pmb x}^{\\prime},t^{\\prime})}}\\\\ {{\\displaystyle G_{s s^{\\prime}}^{\\ell}({\\pmb x},{\\pmb x}^{\\prime},t,t^{\\prime})=\\frac{1}{N\\hbar}\\sum_{\\mathfrak{p}=1}^{\\mathcal{H}}g_{s}^{\\ell}({\\pmb x},t)\\cdot g_{s^{\\prime}}^{\\ell}({\\pmb x}^{\\prime},t^{\\prime})}}\\\\ {{\\displaystyle R_{s s^{\\prime}}^{g^{\\ell}\\tilde{\\chi}^{0}}({\\pmb x},{\\pmb x}^{\\prime},t,t^{\\prime})=-\\frac{i}{N\\hbar}g_{s}^{\\ell+1}({\\pmb x},t)\\cdot\\hat{\\dot{\\chi}}_{O s^{\\prime}}^{\\ell}({\\pmb x}^{\\prime},t^{\\prime})}}\\\\ {{\\displaystyle\\bar{R}_{s s^{\\prime}}^{v^{\\ell}\\ell\\tilde{\\chi}^{0}}({\\pmb x},{\\pmb x}^{\\prime},t,t^{\\prime})=-\\frac{i}{N\\hbar}\\sum_{\\mathfrak{p}=1}^{\\mathcal{H}}v_{s}^{\\ell\\sigma}({\\pmb x},t)\\cdot\\hat{\\xi}_{O s^{\\prime}}^{\\ell}({\\pmb x}^{\\prime},t^{\\prime})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We repeat this same procedure for all collections of weights and arrive at the following set of order parameters ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\mathrm{global}}=\\mathrm{Vec}\\{H^{\\ell},G^{\\ell},V^{\\ell\\sigma},K^{\\ell M\\dot{\\sigma}},Q^{\\ell M\\dot{\\sigma}},G_{O}^{\\ell\\sigma}\\}}\\\\ &{\\qquad\\qquad\\quad\\cup\\,\\{\\hat{H}^{\\ell},\\hat{G}^{\\ell},\\hat{V}^{\\ell\\sigma},\\hat{K}^{\\ell M\\dot{\\sigma}},\\hat{Q}^{\\ell M\\dot{\\sigma}},\\hat{G}_{O}^{\\ell\\sigma}\\}}\\\\ &{\\qquad\\qquad\\quad\\cup\\,\\{R^{g^{\\ell}\\bar{\\chi}_{O}},R^{h^{\\ell}\\bar{\\xi}_{V}},R^{h^{\\ell}\\bar{\\xi}_{K}},R^{h^{\\ell}\\bar{\\xi}_{Q}}\\}}\\\\ &{\\qquad\\qquad\\quad\\cup\\,\\{\\bar{R}^{v^{\\ell}\\xi_{O}^{\\ell}},\\bar{R}^{g_{O}^{\\ell}\\chi_{V}^{\\ell}},\\bar{R}^{q^{\\ell}\\chi_{K}^{\\ell}},\\bar{R}^{k^{\\ell}\\chi_{Q}^{\\ell}}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We expect that these order parameters will be self-averaging since they involve averages over $\\mathcal{H}$ variables. However, the other variables we introduced $\\{A_{0},Q_{0},K_{0},V_{0}\\}$ will not concentrate at finite $N$ and will instead behave as random variables. ", "page_idx": 40}, {"type": "text", "text": "After introducing these order parameters we find that the moment generating functional has the form ", "page_idx": 40}, {"type": "equation", "text": "$$\nZ=\\int d Q_{\\mathrm{global}}\\exp\\left(N\\mathcal{H}L\\;S(Q_{\\mathrm{global}})\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $S$ has the form ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{S=\\frac{1}{L}\\displaystyle\\sum_{c=1}^{L}\\sum_{\\tau=\\nu_{s}=\\nu_{s}}^{f}\\int d x d x^{\\prime}[H_{s\\nu}^{\\prime}(x,x^{\\prime},t,t^{\\prime})\\hat{H}_{s\\nu}^{\\prime\\prime}(x,x^{\\prime},t,t^{\\prime})+G_{\\nu\\nu}^{\\prime}(x,x^{\\prime},t,t^{\\prime})\\hat{G}_{\\nu\\nu}^{\\prime}(x,x^{\\prime},t,t^{\\prime})]}\\\\ &{\\quad+\\displaystyle\\frac{1}{L}\\displaystyle\\sum_{c=1}^{L}\\sum_{\\nu=\\nu_{s}=\\nu_{s}}^{f}\\int d x d x^{\\prime}[V_{\\nu\\nu}^{\\prime\\alpha}(x,x^{\\prime},t,t^{\\prime})\\hat{V}_{\\nu\\nu}^{\\prime\\alpha}(x,x^{\\prime},t,t^{\\prime})+K_{\\nu\\nu}^{\\prime}(x,x^{\\prime},t,t^{\\prime})\\hat{K}_{\\nu\\nu}^{\\prime\\alpha\\beta}(x,x^{\\prime},t,t^{\\prime})}\\\\ &{\\quad+\\displaystyle\\frac{1}{L}\\displaystyle\\sum_{c=1\\nu_{s}=\\nu_{s}}^{L}\\int d x d x^{\\prime}[G_{\\nu\\nu}^{\\prime\\alpha}(x,x^{\\prime},t,t^{\\prime})\\hat{G}_{\\nu\\nu}^{\\prime\\alpha}(x,x^{\\prime},t,t^{\\prime})+Q_{\\nu\\nu}^{\\prime\\alpha\\beta\\nu}(x,x^{\\prime},t,t^{\\prime})\\hat{Q}_{\\nu\\nu}^{\\prime\\alpha\\beta}(x,x^{\\prime},t,t^{\\prime})}\\\\ &{\\quad-\\displaystyle\\frac{1}{L}\\displaystyle\\sum_{c=1\\nu_{s}=\\nu_{s}}^{L}\\int d x d x^{\\prime}[H_{s\\nu}^{\\prime\\alpha}(x,x^{\\prime},t,t^{\\prime})\\hat{H}_{\\nu\\nu}^{\\prime\\alpha}(x^{\\prime},x^{\\prime},t^{\\prime})+R_{\\nu\\nu}^{\\prime\\alpha\\beta\\nu}(x,x^{\\prime},t,t^{\\prime})\\hat{H}_{\\nu\\nu}^{\\prime\\beta}(x^{\\prime},x^{\\prime})}\\\\ &{\\quad-\\displaystyle\\frac{1}{L}\\displaystyle\\sum_{c=1\\nu_{s}}^{L}\\displaystyle\\sum_{\\nu=\\nu_{s}+\\nu_{s}}^{f}\\int d x d x^{\\prime}[\\hat{H}_\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{=}}\\\\ {{\\displaystyle{\\int}\\displaystyle{\\int}\\displaystyle{\\frac{\\hat{d}\\hat{\\omega}_{r}^{(1)}({\\bf x},t){\\hat{\\omega}}_{r}^{(1)}\\hat{d}({\\bf x},t){\\hat{\\omega}}_{r}^{(1)}\\hat{d}({\\bf x},t)-\\hat{\\omega}_{r}^{(1)}\\hat{d}({\\bf x},t)\\frac{\\hat{d}({\\bf x},t){\\hat{\\omega}}_{r}^{(1)}({\\bf x},t)+\\hat{\\omega}_{r}^{(1)}({\\bf x},t){\\hat{\\omega}}_{r}^{(1)}({\\bf x},t){\\hat{\\bf z}}_{r}^{(1)}({\\bf x},t)}{2\\pi}}}\\\\ {{\\displaystyle{\\exp}\\left(-\\sum_{s=0}^{\\infty}\\int\\mathrm{d}s\\omega t^{\\prime}\\left[\\hat{\\omega}_{s}({\\bf x},t)\\hat{\\omega}_{s}^{(\\prime)}({\\bf x},t^{\\prime})\\hat{d}({\\bf x}^{\\prime},t^{\\prime})+\\hat{\\omega}_{s}^{(\\prime)}({\\bf x},t)\\hat{\\omega}_{s}^{(\\prime)}({\\bf x}^{\\prime},t^{\\prime})\\hat{d}({\\bf x}^{\\prime},t^{\\prime})\\right]\\right)}~}\\\\ {{\\displaystyle{\\exp}\\left(-\\frac{1}{2}\\sum_{s=0}^{\\infty}\\int\\mathrm{d}s\\omega t^{\\prime}\\left[\\hat{\\omega}_{s}({\\bf x},t)\\hat{\\omega}_{s}^{(\\prime)}({\\bf x}^{\\prime},t^{\\prime})\\hat{\\omega}_{s}^{(\\prime)}({\\bf x},t^{\\prime})+\\hat{\\omega}_{s}({\\bf x},t)\\hat{\\omega}_{s}^{(\\prime)}({\\bf x}^{\\prime},t^{\\prime})\\hat{d}({\\bf x}^{\\prime},t)\\right]\\right)~}}\\\\ {{\\displaystyle{\\exp}\\left(-\\frac{1}{2}\\sum_{s=0}^{\\infty}\\int\\mathrm{d}s\\omega t^{\\prime}\\left[\\hat{\\omega}_{s}({\\bf x},t)\\hat{\\omega}_{s}^{(\\prime)}({\\bf x}^{\\prime},t^{\\prime})\\hat{\\omega}_{s}^{(\\prime)}({\\bf x},t,{\\bf x}^{\\prime},t^{\\prime})+\\hat{\\omega}_{s}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We can express the MHSA single-head partition functions $\\mathcal{Z}_{\\mathrm{MHSA}}$ in terms of the remaining order parameters within each head that will no longer concentrate at finite $N$ ", "page_idx": 41}, {"type": "equation", "text": "$$\nQ_{\\mathrm{MHSA}}^{\\ell}=\\{\\mathcal{A}^{\\ell},M^{\\ell},Q^{\\ell},K^{\\ell},V^{\\ell},G_{O}^{\\ell},\\hat{A}^{\\ell},\\hat{M}^{\\ell},\\hat{Q}^{\\ell},\\hat{K}^{\\ell},\\hat{V}^{\\ell},\\hat{G}_{O}^{\\ell}\\}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "After introducing these order parameters, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{Z}_{\\mathrm{MHSA}}=\\int d Q_{\\mathrm{MHSA}}^{\\ell}\\exp{\\big(N S_{\\mathrm{MHSA}}(Q_{\\mathrm{MHSA}}^{\\ell})\\big)}}}\\\\ &{}&{S_{\\mathrm{MHSA}}=\\displaystyle\\sum_{t t^{\\prime}s s^{\\prime}}\\int d x d x^{\\prime}[Q_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})\\hat{Q}_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})+K_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})\\hat{K}_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})]}\\\\ &{}&{+\\displaystyle\\sum_{t t^{\\prime}s s^{\\prime}}\\int d x d x^{\\prime}[V_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})\\hat{V}_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})+G_{O s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})\\hat{G}_{O s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})]}\\\\ &{}&{+\\displaystyle\\sum_{t s s^{\\prime}}\\int d x[A_{s s^{\\prime}}^{\\ell}(x,t)\\hat{A}_{s s^{\\prime}}^{\\ell}(x,t)+M_{s s^{\\prime}}^{\\ell}(x,t)\\hat{M}_{s s^{\\prime}}^{\\ell}(x,t)]+\\ln{\\mathcal{Z}_{d k v}^{\\ell}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Zq\u2113kv =   d\u03be\u02c6\u2113Os(x, t)d\u03be\u2113Os(x, t)d\u03c7\u02c6\u2113V s(x, t)d\u03c7\u2113V s(x, t)d\u03c7\u02c6\u2113Ks(x, t)d\u03c7\u2113Ks(x, t)d\u03c7\u02c6Qs(x, t)d\u03c7Qs(x, t) ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\Delta t}{\\Delta t}}\\\\ &{:=}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\left(d\\omega\\left(x,y\\right)\\right)d\\omega^{\\prime}\\left(x,t\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,x^{\\prime},t\\right)c(x,y)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,y)\\left(d\\omega^{\\prime}\\left(x,y\\right)\\right)\\right)\\right)}\\\\ &{\\omega\\left(-\\sum_{m}f\\left(d\\omega k_{m}^{\\prime}\\left(x,y\\right)\\left(-d\\omega^{\\prime}\\left(x,y\\right)\\right)\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m^{*}\\left(\\frac{1}{r}\\displaystyle\\sum_{s=0}^{\\infty}\\int d a\\delta u^{*}\\delta(a,t)\\delta(a^{*},t)\\delta(a^{*},t^{*})\\delta(a^{*},t^{*}),\\alpha^{*}\\right)}\\\\ &{m^{*}\\left(-\\frac{1}{r}\\displaystyle\\sum_{s=0}^{\\infty}f d a\\delta(a^{*},t_{r})\\delta(a,t)\\delta(a^{*},t^{*})\\delta(a^{*},t^{*})\\right)}\\\\ &{m^{*}\\left(-\\frac{1}{r}\\displaystyle\\sum_{s=0}^{\\infty}f d a\\delta(a,t_{r})\\delta(a,t_{r})\\delta(a,t^{*},t^{*})\\right)}\\\\ &{m^{*}\\left(-\\frac{1}{r}\\displaystyle\\sum_{s=0}^{\\infty}f d a\\delta(a^{*},t_{r})\\delta(a,t_{r})\\delta(a^{*},t^{*})\\delta(a^{*},t^{*})\\right)}\\\\ &{m^{*}\\left(-\\frac{1}{r}\\displaystyle\\sum_{s=0}^{\\infty}f d a\\delta(a^{*},t_{r})\\delta(a,t_{r})\\delta(a^{*},t_{r})\\delta(a^{*},t^{*})\\right)}\\\\ &{m^{*}\\left(\\frac{1}{r}\\displaystyle\\sum_{s=0}^{\\infty}f d a\\delta(a,t_{r})\\delta(a,t_{r})\\delta(a,t_{r})\\delta(a,t^{*})\\delta(a^{*},t_{r})\\right)}\\\\ &{m^{*}\\left(\\frac{1}{r}\\displaystyle\\sum_{s=0}^{\\infty}f d a\\delta(a,t_{r})\\delta(a,t_{r})+\\frac{1}{r}\\delta(a,t_{r})\\delta(a,t_{r})\\delta(a^{*},t_{r})\\right)}\\\\ &{m^{*}\\left(-\\frac{1}{r}\\displaystyle\\sum_{s=0}^{\\infty}f d a\\delta(a,t_{r})\\delta^{2}\\delta(a,t_{r})\\delta(a,t_{r})\\delta(a^{*},t_{r})\\right)}\\\\ &{m^{*}\\left(-\\frac{1}{r}\\displaystyle\\sum_{s=0}^{\\infty}f d a\\delta^{*}\\delta(a,t_{r})\\delta(a,t_{r})\\delta(a,t_{r})\\delta(a^{*},t_{r})\\right)}\\\\ &{m^{*}\\left(-\\frac{1}{r}\\displaystyle\\sum_\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The saddle point equations for this limit are computed as derivatives with respect to $Q_{\\mathrm{global}}\\ o n l y$ , reflecting that head-averages will converge as $\\mathcal{H}\\to\\infty$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n{\\frac{\\partial S}{\\partial Q_{\\mathrm{global}}}}=0.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The final saddle point equations are given in terms of averages over the distribution of heads defined by $\\mathcal{Z}_{\\mathrm{MHSA}}$ which we denote as $\\left\\langle\\cdot\\right\\rangle_{\\mathrm{MHSA}}$ as well as averages over the residual stream which we denote as $\\langle\\cdot\\rangle$ . ", "page_idx": 43}, {"type": "text", "text": "These equations give the following (we suppress the sequence indices to simplify the final expressions) ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H^{(1)}(x,y,t,z)=(k,\\pi_{c}^{(2)},t,y(\\pi_{c}^{(3)},\\tau_{c}^{(4)},\\tau_{c}^{(4)},\\tau_{c}^{(5)},\\tau_{c}^{(6)},\\tau_{c}^{(7)},\\tau_{c}^{(8)},\\tau_{c}^{(8)},\\tau_{c}^{(9)},\\tau_{c}^{(1)},\\tau_{c}^{(1)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(3)},\\tau_{c}^{(3)},\\tau_{c}^{(3)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(3)},\\tau_{c}^{(3)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(3)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(3)},\\tau_{c}^{(1)},\\tau_{c}^{(3)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(3)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(3)},\\tau_{c}^{(1)},\\tau_{c}^{(3)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(3)},\\tau_{c}^{(1)},\\tau_{c}^{(2)},\\tau_{c}^{(1)},\\tau_{c}^{(3)},\\tau_{c}^{ \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Residual Stream Dynamics The residual stream satisfies the following single-site dynamics ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\iota_{s}^{\\ell+1}({\\bf x},t)=h_{s}^{\\ell}({\\bf x},t)+\\displaystyle\\frac{\\beta_{0}}{L^{\\alpha}\\pi^{\\ell}}\\bar{u}_{\\alpha}^{\\ell}({\\bf x},t)+\\displaystyle\\frac{\\beta_{0}}{L^{\\alpha}\\;r^{\\ell\\;\\ell}}\\displaystyle\\int d x^{\\prime}\\bar{R}_{s\\alpha}^{\\ell\\;\\ell}(x,x^{\\prime},t,t^{\\prime})g_{s}^{\\ell+1}(x^{\\prime},t^{\\prime})}}\\\\ {{+\\displaystyle\\frac{\\eta_{0}\\gamma_{0}\\beta_{0}^{2}}{L}\\sum_{t^{\\prime}<t}\\mathbb{E}_{x^{\\prime}\\sim\\mathbb{R}_{\\ell^{\\prime}}}V_{s\\alpha}^{\\ell\\;\\ell}(x,x^{\\prime},t,t^{\\prime})g_{s}^{\\ell+1}(x^{\\prime},t^{\\prime})\\;,\\;\\bar{u}_{\\alpha}^{\\ell}({\\bf x},t)\\sim\\mathcal{F P}(0,V^{\\ell\\;\\ell})}}\\\\ {{}}\\\\ {{g_{\\;\\theta}^{\\ell}(x,t)=g_{s}^{\\ell+1}(x,t)+\\displaystyle\\frac{\\beta_{0}}{L^{\\alpha}\\;r^{\\ell}}\\left[\\bar{R}_{s\\alpha}^{\\ell}(x,t)+\\bar{r}_{k\\ast}^{\\ell}(x,t)+\\bar{r}_{\\ Q_{\\alpha}}^{\\ell}(x,t)\\right]}}\\\\ {{+\\displaystyle\\frac{\\beta_{0}}{L^{\\alpha}\\;r^{\\ell}\\;\\sqrt{\\;\\ell}}\\displaystyle\\int d x^{\\prime}\\big[\\bar{R}_{s\\omega}^{\\ell}(x,x^{\\prime},t^{\\prime})+\\bar{R}_{s\\alpha}^{\\ell\\times\\ell}(x,x^{\\prime},t,t^{\\prime})+\\bar{R}_{s\\alpha}^{\\ell^{\\prime}\\times\\ell}(x,x^{\\prime},t,t^{\\prime})]h_{s}^{\\ell}(x^{\\prime},t^{\\prime})}}\\\\ {{+\\displaystyle\\frac{\\eta_{0}\\gamma_{0}\\beta_{0}^{2}}{L}\\sum_{t^{\\prime}<t}\\mathbb{E}_{x^{\\prime}\\sim\\mathbb{R}_{\\ell^{\\prime}}}[G_{\\alpha\\omega^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})+K_{\\alpha\\omega^{\\prime}}^{\\ell M\\\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This matches the result provided in the main text which introduces a compressed ", "page_idx": 44}, {"type": "equation", "text": "$$\nC_{\\mathrm{ss^{\\prime}}}^{\\ell}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})=\\frac{1}{\\eta_{0}\\gamma_{0}\\beta_{0}}\\bar{R}_{\\mathrm{ss^{\\prime}}}^{v^{\\ell}\\xi_{0}^{\\ell}}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})+p_{t^{\\prime}}(\\pmb{x}^{\\prime})\\Delta(\\pmb{x}^{\\prime},t^{\\prime})V_{\\mathrm{ss^{\\prime}}}^{\\ell\\sigma}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $\\begin{array}{r}{p_{t}({\\pmb x})=\\frac{1}{|\\mathfrak{B}_{t}|}\\sum_{{\\pmb x}^{\\prime}\\in\\mathfrak{B}_{t}}\\delta({\\pmb x}-{\\pmb x}^{\\prime})}\\end{array}$ denotes the uniform distribution over the batch $\\mathfrak{B}_{t}$ . ", "page_idx": 44}, {"type": "text", "text": "E.4 Infinite $L$ Limits ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this section, we discuss the two large $L$ limits. This can be derived formally in two distinct ways. First, one could start with the initial For this section, it suffices to reason about the scale of the Gaussian noise which appears in the residual stream and the contribution from the response functions. ", "page_idx": 44}, {"type": "text", "text": "E.4.1 Basic Intuition ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Deriving the infinite depth limits To gain intuition for the large $\\mathcal{H},L\\to\\infty$ limit, we use the fact that the random variables $\\bar{\\chi^{\\ell}}=u^{\\ell}+R^{\\ell}\\bar{g}^{\\ell}$ decompose into a Gaussian $u^{\\ell}$ which are uncorrelated across layers and a linear response $R^{\\ell}$ are response functions. This implies that ", "page_idx": 44}, {"type": "equation", "text": "$$\nh^{L}=\\frac{\\beta_{0}}{L^{\\alpha_{L}}}\\sum_{k=1}^{L}u^{k}+\\frac{\\beta_{0}}{L^{\\alpha_{L}}}\\sum_{k=1}^{L}R^{k}g^{k}+\\frac{\\eta_{0}\\gamma_{0}\\beta_{0}^{2}}{L}\\sum_{k=1}^{L}V^{k\\sigma}g^{k}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We first note that the sum of the Gaussians is a zero-mean random variable with standard deviation ", "page_idx": 44}, {"type": "equation", "text": "$$\n{\\frac{1}{L^{\\alpha}}}\\sum_{k=1}^{L}u^{k}\\sim{\\mathcal{O}}(L^{{\\frac{1}{2}}-\\alpha_{L}})\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Thus, this integrated random variable will vanish unless $\\begin{array}{r}{\\alpha_{L}=\\frac{1}{2}}\\end{array}$ . ", "page_idx": 44}, {"type": "text", "text": "Next, we can investigate the scale of the residual stream response functions. For instance ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial h^{\\ell}}{\\partial u^{k}}=\\mathcal{O}\\left(L^{-\\alpha_{L}}\\right)\\,\\,,\\,\\,\\frac{\\partial h^{\\ell}}{\\partial r^{k}}=\\mathcal{O}\\left(L^{-\\alpha_{L}}\\right)\\,\\,,\\,\\,\\frac{\\partial g^{\\ell}}{\\partial u^{k}}=\\mathcal{O}\\left(L^{-\\alpha_{L}}\\right)\\,\\,,\\,\\,\\frac{\\partial g^{\\ell}}{\\partial r^{k}}=\\mathcal{O}\\left(L^{-\\alpha_{L}}\\right)\\,}\\\\ &{\\displaystyle\\frac{1}{L^{\\alpha_{L}}}\\sum_{k=1}^{\\ell}R^{k}g^{k}=\\mathcal{O}\\left(L^{1-2\\alpha_{L}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "As a consequence, we see that the effect of the Gaussian and linear response terms will vanish as $L\\rightarrow\\infty$ provided that $\\alpha_{L}>\\frac{1}{2}$ . We will consider first, the case where $\\alpha=1$ which gives an ODE like limit for the residual updates before moving onto the more involved $\\begin{array}{r}{\\alpha_{L}=\\frac{1}{2}}\\end{array}$ case. ", "page_idx": 44}, {"type": "text", "text": "To formally take the $L\\rightarrow\\infty$ limit, we redefine all of the preactivation fields and kernels in terms of layer time $\\tau$ defined as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\tau=\\operatorname*{lim}_{L\\to\\infty}\\frac{\\ell}{L}\\in[0,1].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For example, the residual kernels are defined as ", "page_idx": 44}, {"type": "equation", "text": "$$\nH_{^{55^{\\prime}}}(\\tau,{\\pmb x},{\\pmb x}^{\\prime},t,t^{\\prime})\\equiv\\operatorname*{lim}_{L\\rightarrow\\infty}H_{^{55^{\\prime}}}^{L\\tau}({\\pmb x},{\\pmb x}^{\\prime},t,t^{\\prime}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The finite difference equations for the residual updates $\\begin{array}{r}{L(h^{\\ell+1}-h^{\\ell})\\sim\\frac{\\partial}{\\partial\\tau}h(\\tau)}\\end{array}$ become differential updates (either SDE-like or ODE-like depending on $\\alpha_{L}$ ) [46, 11]. ", "page_idx": 44}, {"type": "text", "text": "E.4.2 ODE Limit $\\alpha_{L}=1$ ", "text_level": 1, "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{\\tau}h_{s}(\\tau,\\mathbf{x},t)=\\eta_{0}\\gamma_{0}\\beta_{0}^{2}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathbb{B}_{t}}\\Delta(\\mathbf{x}^{\\prime},t^{\\prime})V_{s s^{\\prime}}^{\\sigma}(\\tau,x,\\mathbf{x}^{\\prime},t,t^{\\prime})g_{s^{\\prime}}(\\tau^{\\prime},\\mathbf{x}^{\\prime},t^{\\prime})}\\\\ &{\\phantom{\\eta_{\\tau}h_{s}(\\tau,\\mathbf{x},t)=\\eta_{0}\\gamma_{0}\\beta_{0}^{2}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathbb{B}_{t}}\\Delta(\\mathbf{x}^{\\prime},t^{\\prime})[G_{s s^{\\prime}}^{\\sigma}(\\tau,x,\\mathbf{x}^{\\prime},t,t^{\\prime})+{K}_{s s^{\\prime}}^{M\\dot{\\sigma}}(\\tau,\\mathbf{x},x^{\\prime},t,t^{\\prime})]g_{s^{\\prime}}(\\tau^{\\prime},x,\\mathbf{x}^{\\prime},t^{\\prime})}\\\\ &{\\phantom{\\eta_{\\tau}h_{s}(\\tau,x,t)=\\eta_{0}\\gamma_{0}\\beta_{0}^{2}\\displaystyle\\sum_{t^{\\prime}<t}\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathbb{B}_{t}}\\Delta(\\mathbf{x}^{\\prime},t^{\\prime})[Q_{s s^{\\prime}}^{M\\dot{\\sigma}}(\\tau,\\mathbf{x},x^{\\prime},t,t^{\\prime})]g_{s^{\\prime}}(\\tau^{\\prime},x^{\\prime},t^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "E.4.3 SDE Limit $\\begin{array}{r}{\\alpha_{L}=\\frac{1}{2}}\\end{array}$ ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "This $\\begin{array}{r}{\\alpha_{L}=\\frac{1}{2}}\\end{array}$ limit is more technically involved since neither the Gaussian terms from the DMFT nor the response functions vanish. For the Gaussian terms, we note that the sums of the independent Gaussians are all multiplied by $\\textstyle{\\frac{1}{\\sqrt{L}}}\\sim{\\sqrt{d\\tau}}$ , which can be interpreted as integrated Brownian motion in the limit ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{lim}_{L\\rightarrow\\infty}\\frac{1}{\\sqrt{L}}\\sum_{k=1}^{\\ell}u^{k}\\rightarrow\\int_{0}^{\\tau}d u(\\tau^{\\prime})}\\\\ {\\langle d u(\\tau)d u(\\tau^{\\prime})\\rangle=V^{\\sigma}(\\tau)\\delta(\\tau-\\tau^{\\prime})d\\tau d\\tau^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Following the derivation of Bordelon et al. [11], which maintains the exact dependence on the full integrated response and provides the result as an integrated SDE ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle h_{\\mathfrak{s}}(\\tau,\\boldsymbol{x},t)=\\beta_{0}\\int_{0}^{\\tau}d\\bar{u}_{\\mathfrak{s}}(\\tau^{\\prime}\\mathbf{x},t)+\\eta_{0}\\gamma_{0}\\beta_{0}^{2}\\sum_{t^{\\prime}<t}\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathfrak{B}_{t^{\\prime}}}\\int_{0}^{\\tau}d\\tau^{\\prime}C_{\\mathfrak{s}\\mathrm{s}^{\\prime}}(\\tau,\\boldsymbol{x},\\boldsymbol{x}^{\\prime},t,t^{\\prime})g_{\\mathrm{s}^{\\prime}}(\\tau^{\\prime},\\boldsymbol{x}^{\\prime},t^{\\prime})\\,,}\\\\ {\\displaystyle C_{\\mathfrak{s}\\mathrm{s}^{\\prime}}(\\tau,\\boldsymbol{x},\\boldsymbol{x}^{\\prime},t,t^{\\prime})=\\frac{1}{\\eta_{0}\\gamma_{0}\\beta_{0}}\\bar{R}_{\\mathfrak{s}\\mathrm{s}^{\\prime}}^{v\\xi_{0}}(\\tau,\\boldsymbol{x},\\boldsymbol{x}^{\\prime},t,t^{\\prime})+p_{t^{\\prime}}(\\mathbf{x}^{\\prime})\\Delta(\\mathbf{x}^{\\prime},t^{\\prime})V_{\\mathfrak{s}\\mathrm{s}^{\\prime}}^{\\sigma}(\\tau,\\boldsymbol{x},\\boldsymbol{x}^{\\prime},t,t^{\\prime}).\\qquad\\mathrm{~(9)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Combining the forward pass equations from the previous two subsection recovers the Result 3 of the main text. This is combined with a complementary equation for the backward pass. ", "page_idx": 45}, {"type": "text", "text": "E.5 Effect of MLP Layers ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Adding the MLP block to the residual stream can also be easily handled with the methods of the preceeding sections. The forward pass equations in this case take the form ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{h}_{\\mathfrak{s}}^{\\ell}(\\mathbf{x},t)=h_{\\mathfrak{s}}^{\\ell}(\\mathbf{x},t)+\\displaystyle\\frac{\\beta_{0}}{L^{\\alpha_{L}}}\\mathrm{MHSA}\\left(h^{\\ell}(\\mathbf{x},t)\\right)_{\\mathfrak{s}}}\\\\ &{h^{\\ell+1}(\\pmb{x},t)=\\tilde{h}_{\\mathfrak{s}}^{\\ell}(\\pmb{x},t)+\\displaystyle\\frac{\\beta_{0}}{L^{\\alpha_{L}}}\\mathrm{MLP}\\left(\\tilde{h}_{\\mathfrak{s}}^{\\ell}(\\pmb{x},t)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the MLP layer is ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{MLP}(\\tilde{h}_{\\mathfrak{s}}^{\\ell})=\\frac{1}{\\sqrt{N\\mathcal{H}}}W^{\\ell,2}\\phi\\left(\\tilde{h}_{\\mathfrak{s}}^{\\ell,1}\\right)\\ ,\\ \\tilde{h}^{\\ell,1}=\\frac{1}{\\sqrt{N\\mathcal{H}}}W^{\\ell,1}\\tilde{h}_{\\mathfrak{s}}^{\\ell}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "The following gradient fields are necessary ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\pmb g}_{\\mathfrak{s}}^{\\ell}({\\pmb x},t)\\equiv\\gamma_{0}N\\mathcal{H}\\frac{\\partial f({\\pmb x},t)}{\\partial{\\pmb h}_{\\mathfrak{s}}^{\\ell}({\\pmb x},t)}\\;,\\;\\tilde{g}_{\\mathfrak{s}}^{\\ell}({\\pmb x},t)\\equiv\\gamma_{0}N\\mathcal{H}\\frac{\\partial f({\\pmb x},t)}{\\partial\\tilde{h}_{\\mathfrak{s}}^{\\ell}({\\pmb x},t)}}\\\\ {{\\tilde{g}_{\\mathfrak{s}}^{\\ell,1}}({\\pmb x},t)\\equiv\\gamma_{0}N\\mathcal{H}\\frac{\\partial f({\\pmb x},t)}{\\partial\\tilde{h}_{\\mathfrak{s}}^{\\ell,1}({\\pmb x},t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{W^{\\ell,2}(t)=W^{\\ell,2}(0)+\\displaystyle\\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}\\sum_{t^{\\prime}<t}\\mathbb{E}_{x\\sim\\mathfrak{B}_{t^{\\prime}}}\\sum_{s}\\Delta(x,t^{\\prime})g_{s}^{\\ell+1}(x,t^{\\prime})\\phi(\\widetilde{h}_{s}^{\\ell,1}(x,t^{\\prime}))^{\\top}\\;,}}\\\\ {{W^{\\ell,1}(t)=W^{\\ell,1}(0)+\\displaystyle\\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}\\sqrt{N\\mathcal{H}}}\\sum_{t^{\\prime}<t}\\mathbb{E}_{x\\sim\\mathfrak{B}_{t^{\\prime}}}\\sum_{s}\\Delta(x,t^{\\prime})\\widetilde{g}_{s}^{\\ell,1}(x,t^{\\prime})\\bar{h}_{s}^{\\ell}(x,t^{\\prime})^{\\top}\\qquad(\\mathrm{0leqi\\leqn~0~\\alpha~0~\\alpha~0~\\alpha~}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "The MLP hidden layer dynamics is much simpler to characterize and resembles the structure analyzed in prior works on infinite width networks [15]. ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\tilde{h}_{\\mathrm{s}}^{\\ell,1}(x,t)=\\tilde{\\chi}_{\\mathrm{s}}^{\\ell,1}(x,t)+\\frac{\\beta_{0}\\eta_{0}\\gamma_{0}}{L^{1-\\alpha_{L}}}\\sum_{t^{\\prime}<t}\\mathbb{E}_{x\\sim\\mathfrak{B}_{t^{\\prime}}}\\sum_{\\mathfrak{s}}\\Delta(x,t^{\\prime})\\tilde{g}_{\\mathrm{s}}^{\\ell,1}(x,t^{\\prime})H_{\\mathrm{s}\\mathrm{s^{\\prime}}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Again, we see that the inner dynamics for $\\tilde{h}^{\\ell,1}$ due to the weight updates in this layer scale as $L^{\\bar{-}1+\\alpha_{L}}$ , suggesting the need to choose $\\alpha_{L}=1$ if we desire this hidden layer to contribute to the representational updates. ", "page_idx": 45}, {"type": "text", "text": "MLP Layer Gradients For the MLP layer we have the simpler backpropagation equations ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{g}_{\\mathfrak{s}}^{\\ell,1}(\\pmb{x},t)=\\left(\\frac{\\partial h_{\\mathfrak{s}}^{\\ell+1}(\\pmb{x},t)}{\\partial\\tilde{h}_{\\mathfrak{s}}^{\\ell,1}(\\pmb{x},t)}\\right)^{\\top}g_{\\mathfrak{s}}^{\\ell+1}(\\pmb{x},t)}\\\\ &{\\qquad\\qquad=\\frac{\\beta_{0}}{L_{L}^{\\alpha}}\\dot{\\phi}(\\tilde{h}_{\\mathfrak{s}}^{\\ell,1}(\\pmb{x},t))\\odot\\left[\\frac{1}{\\sqrt{N\\mathcal{H}}}W^{\\ell,2}(t)^{\\top}g_{\\mathfrak{s}}^{\\ell+1}(\\pmb{x},t)\\right]}\\\\ &{\\qquad\\qquad\\tilde{g}_{\\mathfrak{s}}^{\\ell}(\\pmb{x},t)=\\frac{1}{\\sqrt{N\\mathcal{H}}}W^{\\ell,1}(t)^{\\top}\\tilde{g}_{\\mathfrak{s}}^{\\ell,1}(\\pmb{x},t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The components of these fields that depend on initial conditions are ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\xi}_{s}^{\\ell,1}(\\pmb{x},t)=\\frac{1}{\\sqrt{N\\mathcal{H}}}\\pmb{W}^{\\ell,2}(0)^{\\top}\\pmb{g}_{s}^{\\ell+1}(\\pmb{x},t)}\\\\ &{\\pmb{\\xi}_{s}^{\\ell}(\\pmb{x},t)=\\frac{1}{\\sqrt{N\\mathcal{H}}}\\pmb{W}^{\\ell,1}(0)^{\\top}\\pmb{g}_{s}^{\\ell,1}(\\pmb{x},t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "MLP Matrices After utilizing these resolutions of the identity for all ${\\mathfrak{s}},x,t$ , we can integrate over the weights $W^{\\ell,2}(0)$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathrm{\\Delta}\\ n\\mathbb{E}_{W^{\\ell,2}(0)}\\exp\\left(-\\displaystyle\\frac{i}{\\sqrt{N\\mathcal{H}}}\\sum_{t\\ s}\\int d x\\mathrm{\\normalfont~TiW}^{\\ell,2}(0)^{\\top}\\left[\\hat{{\\chi}}_{s}^{\\ell+1}(x,t)\\phi(\\tilde{h}_{s}^{\\ell,1}(x,t))^{\\top}+g_{s}^{\\ell+1}(x,t)\\hat{\\xi}_{s}^{\\ell,1}(x,t)\\right]^{2}\\right)}}\\\\ {{=-\\displaystyle\\frac{1}{2}\\sum_{t^{\\prime}\\ s\\ s^{\\prime}}\\int d x d x^{\\prime}\\left[\\hat{{\\chi}}_{s}^{\\ell+1}(x,t)\\cdot\\hat{{\\chi}}_{s^{\\prime}}^{\\ell+1}(x^{\\prime},t^{\\prime})\\Phi_{s\\ s^{\\prime}}^{\\ell,1}(x,x^{\\prime},t,t^{\\prime})+\\hat{\\xi}_{s}^{\\ell,1}(x,t)\\cdot\\hat{\\xi}_{s^{\\prime}}^{\\ell,1}(x^{\\prime},t^{\\prime})G_{s s^{\\prime}}^{\\ell+1}(x,x^{\\prime},t)\\right]}}\\\\ {{-\\displaystyle\\ i\\sum_{t t^{\\prime}s\\ s^{\\prime}}\\int d x d x^{\\prime}\\left[\\hat{{\\chi}}_{s}^{\\ell+1}(x,t)\\cdot g_{s^{\\prime}}^{\\ell+1}(x^{\\prime},t^{\\prime})R_{s\\ s^{\\prime}}^{\\ell,1}(x,x^{\\prime},t,t^{\\prime})\\right]}}&{{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(106)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where we introduced the response function ", "page_idx": 46}, {"type": "equation", "text": "$$\nR_{s s^{\\prime}}^{\\ell,1}(\\pmb{x},\\pmb{x}^{\\prime},t,t^{\\prime})\\equiv-\\frac{i}{N\\mathcal{H}}\\phi\\left(\\tilde{\\pmb{h}}_{s}^{\\ell,1}(\\pmb{x},t)\\right)\\cdot\\hat{\\pmb{\\xi}}_{s^{\\prime}}^{\\ell,1}(\\pmb{x}^{\\prime},t^{\\prime})\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We can perform an identical step to integrate over $W^{\\ell,1}(0)$ . This gives us ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{n\\,}\\mathbb{E}_{W^{\\ell,1}(0)}\\exp\\left(-\\frac{i}{\\sqrt{N\\mathcal{H}}}\\sum_{t\\in}\\int d x\\mathrm{\\,Tr}W^{\\ell,1}(0)^{\\top}\\left[\\hat{\\tilde{X}}_{s}^{\\ell,1}(x,t)\\tilde{h}_{s}^{\\ell}(x,t)^{\\top}+g_{s}^{\\ell,1}(x,t)\\hat{\\xi}_{s}^{\\ell}(x,t)^{\\top}\\right]\\right)}}\\\\ {{\\displaystyle=-\\frac{1}{2}\\sum_{t^{\\prime}s^{\\prime}}\\int d x d x^{\\prime}\\left[\\hat{\\tilde{X}}_{s}^{\\ell,1}(x,t)\\cdot\\hat{\\tilde{\\chi}}_{s^{\\prime}}^{\\ell,1}(x^{\\prime},t^{\\prime})\\tilde{H}_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})+\\hat{\\xi}_{s}^{\\ell}(x,t)\\cdot\\hat{\\xi}_{s^{\\prime}}^{\\ell}(x^{\\prime},t^{\\prime})G_{s s^{\\prime}}^{\\ell,1}(x,x^{\\prime},t,t^{\\prime})\\right]}}\\\\ {{\\displaystyle-\\,i\\sum_{t t^{\\prime}s^{\\prime}}\\int d x d x^{\\prime}\\left[\\hat{\\chi}_{s}^{\\ell,1}(x,t)\\cdot g_{s^{\\prime}}^{\\ell+1}(x^{\\prime},t^{\\prime})\\tilde{H}_{s s^{\\prime}}^{\\ell}(x,x^{\\prime},t,t^{\\prime})\\right]}}&{{\\displaystyle(108)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where we introduced ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\tilde{R}_{\\mathfrak{s s}^{\\prime}}^{\\ell}(\\mathbf{x},\\mathbf{x}^{\\prime},t,t^{\\prime})=-\\frac{i}{N\\mathcal{H}}\\tilde{h}_{\\mathfrak{s}}^{\\ell}(\\mathbf{x},t)\\cdot\\hat{\\xi}_{\\mathfrak{h}}^{\\ell}(\\mathbf{x}^{\\prime},t^{\\prime})\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "E.6 Effect of Layer Norm on the Limiting Process ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Layernorm The derivative of layer-norm $\\frac{\\partial\\bar{h}_{\\hat{s}^{\\prime}}^{\\ell}}{\\partial h_{\\hat{s}^{\\prime}}^{\\ell\\top}}$ acts as the following in the large $\\mathcal{H}$ limit ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{\\partial\\bar{h}}{\\partial h^{\\top}}=\\frac{1}{\\sqrt{\\sigma^{2}+\\epsilon}}\\left(I-\\frac{1}{N\\mathcal{H}}{\\bf11^{\\top}}\\right)-\\frac{1}{N\\mathcal{H}}\\frac{1}{(\\sigma^{2}+\\epsilon)^{3/2}}\\left[h-\\mu{\\bf1}\\right]\\left[h-\\mu{\\bf1}\\right]^{\\top}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "H \u2192\u221e \u00b5 =N1H Ionv etrh er alinmdiot mo fi itohnes .v aWriea tbhleuss oannsdi $\\begin{array}{r}{\\sigma^{2}=\\frac{1}{N\\mathcal{H}}|h-\\mu1|^{2}}\\end{array}$ owf ivlle cbteocros mace t doent egrrmaidniiesntitcs ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left(\\frac{\\partial\\bar{h}}{\\partial h^{\\top}}\\right)^{\\top}g=\\frac{1}{\\sqrt{\\sigma^{2}+\\epsilon}}\\left(g-1\\mu_{g}\\right)-\\frac{1}{(\\sigma^{2}+\\epsilon)^{3/2}}\\left[h-\\mu\\mathbf{1}\\right]\\left(\\frac{1}{N\\mathcal{H}}\\left[h-\\mu\\mathbf{1}\\right]^{\\top}g\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Each of these operations will lead to one inner product that will be self averaging $\\begin{array}{r}{\\mu_{g}=\\frac{1}{N\\mathcal{H}}{\\bf1}\\cdot{\\bf g}}\\end{array}$ or $\\begin{array}{r}{\\left(\\frac{1}{N{\\mathcal{H}}}\\left[h-\\mu\\mathbf{1}\\right]^{\\top}g\\right)}\\end{array}$ . Thus this operation will not alter the backward pass in terms of scaling. ", "page_idx": 46}, {"type": "text", "text": "F Compute Resources and Experimental Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Each of the experimental runs performed in this paper were all performed on single NVIDIA H100 GPU. Each run of the full CIFAR-5M took anywhere from 5 minutes to 1 hour depending on model size. Each run of the C4 training took anywhere from 1 hour to 6 hours depending on model size and total amount of training steps. ", "page_idx": 47}, {"type": "text", "text": "The language model trained on C4 used a context length of 256 and the GPT-tokenizer from Huggingface. Sequences that were too short were concatenated with other sentences to reach the full context length rather than padding the end of the sequence. We use trainable positional encodings and separate embedding and decoding parameters as implemented in Appendix F. ", "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 48}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 48}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 48}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 48}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 48}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 48}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 48}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: We claim to analyze various limits of transformer training and provide theoretical and empirical results to that effect. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We provide a limitations and future directions section in our conclusion. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 49}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: We provide derivations of all of our results at the level of rigor of physics calculations. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We provide example FLAX code which shows how our models are implemented. We also mention the datasets and hyperparameters used.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: We provide code in the uploaded supplementary material. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 50}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 51}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We describe the number of ensembles and general experimental details (SGD vs Adam, depth, width, head number etc) in all of our experiments. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 51}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We provide errorbars wherever appropriate, usually averaging and measuring standard deviation over different initializations. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We provide a section mentioning the compute resources used. All experiments can be performed on a single Nvidia A100 or H100 GPU. The details on run times for each experiment are provided in Appendix F. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 52}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We are conforming to the code of ethics and preserve our anonymity. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 52}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: This work is of a theoretical nature and is about general scientific understanding of transformer models. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 52}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: We are not releasing any data or models. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 53}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We cite the papers that introduce the common crawl (C4) and the CIFAR-5M datasets. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 53}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: No new assets are introduced in this paper. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 53}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: We do not perform any crowdsourcing experiments or research involving human subjects. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 54}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: There are no study participants in our paper. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 54}]