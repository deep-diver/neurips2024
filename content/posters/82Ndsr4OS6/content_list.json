[{"type": "text", "text": "Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Honghao Wei Washington State University honghao.wei@wsu.edu ", "page_idx": 0}, {"type": "text", "text": "Xiyue Peng ShanghaiTech University pengxy2024@shanghaitech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Arnob Ghosh New Jersey Institute of Technology arnob.ghosh@njit.edu ", "page_idx": 0}, {"type": "text", "text": "Xin Liu ShanghaiTech University liuxin7@shanghaitech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose WSAC (Weighted Safe Actor-Critic), a novel algorithm for Safe Offilne Reinforcement Learning (RL) under functional approximation, which can robustly optimize policies to improve upon an arbitrary reference policy with limited data coverage. WSAC is designed as a two-player Stackelberg game to optimize a refined objective function. The actor optimizes the policy against two adversarially trained value critics with small importance-weighted Bellman errors, which focus on scenarios where the actor\u2019s performance is inferior to the reference policy. In theory, we demonstrate that when the actor employs a no-regret optimization oracle, WSAC achieves a number of guarantees: $(i)$ For the first time in the safe offilne RL setting, we establish that WSAC can produce a policy that outperforms any reference policy while maintaining the same level of safety, which is critical to designing a safe algorithm for offline RL. $(i i)$ WSAC achieves the optimal statistical convergence rate of $1/\\sqrt{N}$ to the reference policy, where $N$ is the size of the offilne dataset. $(i i i)$ We theoretically show that WSAC guarantees a safe policy improvement across a broad range of hyperparameters that control the degree of pessimism, indicating its practical robustness. Additionally, we offer a practical version of WSAC and compare it with existing state-of-the-art safe offline RL algorithms in several continuous control environments. WSAC outperforms all baselines across a range of tasks, supporting the theoretical results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online safe reinforcement learning (RL) has found successful applications in various domains, such as autonomous driving (Isele et al., 2018), recommender systems (Chow et al., 2017), and robotics (Achiam et al., 2017). It enables the learning of safe policies effectively while satisfying certain safety constraints, including collision avoidance, budget adherence, and reliability. However, collecting diverse interaction data can be extremely costly and infeasible in many real-world applications, and this challenge becomes even more critical in scenarios where risky behavior cannot be tolerated. Given the inherently risk-sensitive nature of these safety-related tasks, data collection becomes feasible only when employing behavior policies satisfies all the safety requirements. ", "page_idx": 0}, {"type": "text", "text": "To overcome the limitations imposed by interactive data collection, offilne RL algorithms are designed to learn a policy from an available dataset collected from historical experiences by some behavior policy, which may differ from the policy we aim to learn. A desirable property of an effective offilne algorithm is the assurance of robust policy improvement (RPI), which guarantees that a learned policy is always at least as good as the baseline behavior policies (Fujimoto et al., 2019; Laroche et al., 2019; ", "page_idx": 0}, {"type": "text", "text": "Kumar et al., 2019; Siegel et al., 2020; Chen et al., 2022a; Zhu et al., 2023; Bhardwaj et al., 2024). We extend the property of RPI to offilne safe RL called safe robust policy improvement (SRPI), which indicates the improvement should be safe as well. This is particularly important in offilne safe RL. For example, in autonomous driving, an expert human driver operates the vehicle to collect a diverse dataset under various road and weather conditions, serving as the behavior policy. This policy is considered both effective and safe, as it demonstrates proficient human driving behavior while adhering to all traffic laws and other safety constraints. Achieving a policy that upholds the SRPI characteristic with such a dataset can significantly mitigate the likelihood of potential collisions and other safety concerns. ", "page_idx": 1}, {"type": "text", "text": "In offilne RL, we represent the state-action occupancy distribution of policy $\\pi$ over the dataset distribution $\\mu$ using the ratio $w^{\\pi}=d^{\\pi}/\\mu$ . A commonly required assumption is that the $\\ell_{\\infty}$ concentrability $C_{\\ell_{\\infty}}^{\\pi}$ is bounded, which is defined as the infinite norm of $w^{\\pi}$ for all policies (Liu et al., 2019; Chen and Jiang, 2019; Wang et al., 2019; Liao et al., 2022; Zhang et al., 2020). A stronger assumption requires a uniform lower bound on $\\left[\\mu(a|s)\\right]$ (Xie and Jiang, 2021). However, such all-policy concentrability assumptions are difficult to satisfy in practice, particularly for offline safe RL, as they essentially require the offline dataset to have good coverage of all unsafe stateaction pairs. To address the full coverage requirement, other works (Rashidinejad et al., 2021; Zhan et al., 2022; Chen and Jiang, 2022; Xie et al., 2021; Uehara and Sun, 2021) adapt conservative algorithms by employing the principle of pessimism in the face of uncertainty, reducing the assumption to the best covered policy (or optimal policy) concerning $\\ell_{\\infty}$ concentrability. Zhu et al. (2023) introduce $\\ell_{2}$ concentrability to further relax the assumption, indicating that $\\ell_{\\infty}$ concentrability is always an upper bound of $\\ell_{2}$ concentrability (see Table 1 for detailed comparisons with previous works). While provable guarantees are obtained using single policy concentrability for unconstrained MDP as Table 1 suggests for the safe RL setting, all the existing studies (Hong et al., 2024; Le et al., 2019) still require the coverage on all the policies. Further, as Table 1 suggests, the above papers do not guarantee robust safe policy improvement. Our main contributions are summarized below: ", "page_idx": 1}, {"type": "image", "img_path": "82Ndsr4OS6/tmp/7ab2d96180731ae81a66239d4b373975cb87658b0d8add431ea6b998daa80e0c.jpg", "img_caption": ["Figure 1: Comparison between WSAC and the behavior policy in the tabular case. The behavior policy is a mixture of the optimal policy and a random policy, with the mixture percentage representing the proportion of the optimal policy. The cost threshold is set to 0.1. We observe that WSAC consistently ensures a safely improved policy across various scenarios, even when the behavior policy is not safe. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1. We pr\u221aove that our algorithm, which uses average Bellman error, enjoys an optimal statistical rate of $1/\\sqrt{N}$ under partial data coverage assumption. This is the first work that achieves such a result using only single-policy $\\ell_{2}$ concentrability. ", "page_idx": 1}, {"type": "text", "text": "2. We propose a novel offline safe RL algorithm, called Weighted Safe Actor-Critic (WSAC), which can robustly learn policies that improve upon any behavior policy with controlled relative pessimism. We prove that under standard function approximation assumptions, when the actor incorporates a no-regret policy optimization oracle, WSAC outputs a policy that never degrades the performance of a reference policy (including the behavior policy) for a range of hyperparameters (defined later). This is the first work that provably demonstrates the property of SRPI in offline safe RL setting.   \n3. We point out that primal-dual-based approaches Hong et al. (2024) may require all-policy concentrability assumption. Thus, unlike, the primal-dual-based appraoch, we propose a novel rectified penalty-based approach to obtain results using single-policy concentrability. Thus, we need novel analysis techniques to prove results.   \n4. Furthermore, we provide a practical implementation of WSAC following a two-timescale actorcritic framework using adversarial frameworks similar to Cheng et al. (2022); Zhu et al. (2023), and test it on several continuous control environments in the offilne safe RL benchmark (Liu et al., 2023a). WSAC outperforms all other state-of-the-art baselines, validating the property of a safe policy improvement. ", "page_idx": 1}, {"type": "text", "text": "Table 1: Comparison of algorithms for offline RL (safe RL) with function approximation. The parameters $C_{\\ell_{2}}^{\\bar{\\pi}},C_{\\ell_{\\infty}}^{\\pi},C_{B e l l m a n}^{\\pi}$ refer to different types of concentrabilities, it always hold $C_{\\ell_{2}}^{\\pi}\\leq$ $C_{\\ell_{\\infty}}^{\\pi}$ and under certain condition $C_{\\ell_{2}}^{\\pi}\\leq C_{B e l l m a n}^{\\pi}$ , detailed definitions and more discussions can be found in Section 3.3. ", "page_idx": 2}, {"type": "table", "img_path": "82Ndsr4OS6/tmp/bbd88440528437c4767d33a122b222b1128e2c115d099ccf46bb4f721c40258f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Offilne safe RL: Deep offilne safe RL algorithms (Lee et al., 2022; Liu et al., 2023b; Xu et al., 2022; Chen et al., 2021; Zheng et al., 2024) have shown strong empirical performance but lack theoretical guarantees. To the best of our knowledge, the investigation of policy improvement properties in offline safe RL is relatively rare in the state-of-the-art offline RL literature. $\\mathrm{Wu}$ et al. (2021) focus on the offline constrained multi-objective Markov Decision Process (CMOMDP) and demonstrate that an optimal policy can be learned when there is sufficient data coverage. However, although they show that CMDP problems can be formulated as CMOMDP problems, they assume a linear kernel CMOMDP in their paper, whereas our consideration extends to a more general function approximation setting. Le et al. (2019) propose a model-based primal-dual-type algorithm with deviation control for offilne safe RL in the tabular setting. With prior knowledge of the slackness in Slater\u2019s condition and a constant on the concentrability coefficient, an $(\\epsilon,\\delta)$ -PAC error is achievable when the number of data samples $N$ is large enough $\\mathit{\\check{N}}=\\tilde{\\mathcal{O}}(1/\\epsilon^{2}))$ . These assumptions make the algorithm impractical, and their computational complexity is much higher than ours. Additionally, we consider a more practical, model-free function approximation setting. In another concurrent work (Hong et al., 2024), a primal-dual critic algorithm is proposed for offline-constrained RL settings with general function approximation. However, their algorithm requires $\\ell_{2}$ concentrability for all policies, which is not practical as discussed. The reason is that the dual variable optimization in their primal-dual design requires an accurate estimation of all the policies used in each episode, which necessitates coverage over all policies. Moreover, they cannot guarantee the property of SRPI. Moreover, their algorithm requires an additional offline policy evaluation (OPE) oracle for policy evaluation, making the algorithm less efficient. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Constrained Markov Decision Process ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a Constrained Markov Decision Process (CMDP) $\\mathcal{M}$ , denoted by $(S,{\\mathcal{A}},{\\mathcal{P}},R,C,\\gamma,\\rho)$ . $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space, $\\mathcal{P}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathcal{S})$ is the transition kernel, where $\\Delta(\\cdot)$ is a probability simplex, $R:S\\times A\\to[0,1]$ is the reward function, $C:\\mathcal{S\\times A}\\to[-1,1]$ is the cost function, $\\gamma\\in[0,1)$ is the discount factor and $\\rho:S\\rightarrow[0,1]$ is the initial state distribution. We assume $\\boldsymbol{\\mathcal{A}}$ is finite while allowing $\\boldsymbol{S}$ to be arbitrarily complex. We use $\\pi:S\\to\\Delta(A)$ to denote a stationary policy, which specifies a distribution over actions for each state. At each time, the agent observes a state $s_{t}\\in\\mathcal S$ , takes an action $a_{t}\\in\\mathcal{S}$ according to a policy $\\pi$ , receives a reward $r_{t}$ and a cost $c_{t}$ , where $r_{t}=R(s_{t},a_{t}),c_{t}=C(s_{t},a_{t})$ . Then the CMDP moves to the next state $s_{t+1}$ based on the transition kernel $\\mathcal{P}(\\cdot|s_{t},a_{t})$ . Given a policy $\\pi$ , we use $V_{r}^{\\pi}(s)$ and $V_{c}^{\\pi}(s)$ to denote the expected discounted return and the expected cumulative discounted cost of $\\pi$ , starting from state $s$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V_{r}^{\\pi}(s):=\\mathbb{E}[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}|s_{0}=s,a_{t}\\sim\\pi(\\cdot|s_{t})]}}\\\\ {{\\displaystyle V_{c}^{\\pi}(s):=\\mathbb{E}[\\sum_{t=0}^{\\infty}\\gamma^{t}c_{t}|s_{0}=s,a_{t}\\sim\\pi(\\cdot|s_{t})].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Accordingly, we also define the $Q-$ value function of a policy $\\pi$ for the reward and cost as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{Q_{r}^{\\pi}(s,a):=}\\mathbb{E}[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}|(s_{0},a_{0})=(s,a),a_{t}\\sim\\pi(\\cdot|s_{t})]}}\\\\ {{\\displaystyle{Q_{c}^{\\pi}(s,a):=}\\mathbb{E}[\\sum_{t=0}^{\\infty}\\gamma^{t}c_{t}|(s_{0},a_{0})=(s,a),a_{t}\\sim\\pi(\\cdot|s_{t})],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "respectively. As rewards and costs are bounded, we have that $\\begin{array}{r}{0\\leq Q_{r}^{\\pi}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ , and $\\begin{array}{r}{-{\\frac{1}{1-\\gamma}}\\leq Q_{c}^{\\pi}\\leq}\\end{array}$ $\\textstyle{\\frac{1}{1-\\gamma}}$ . We let $\\begin{array}{r}{V_{\\mathrm{max}}=\\frac{1}{1-\\gamma}}\\end{array}$ to simplify the notation. We further write ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ_{r}(\\pi):=(1-\\gamma)\\mathbb{E}_{s\\sim\\rho}[V_{r}^{\\pi}(s)],\\quad J_{c}(\\pi):=(1-\\gamma)\\mathbb{E}_{s\\sim\\rho}[V_{c}^{\\pi}(s)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "to represent the normalized average reward/cost value of policy $\\pi$ . In addition, we use $d^{\\pi}(s,a)$ to denote the normalized and discounted state-action occupancy measure of the policy $\\pi$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nd^{\\pi}(s,a):=(1-\\gamma)\\mathbb{E}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{1}(s_{t}=s,a_{t}=a)|a_{t}\\sim\\pi(\\cdot|s_{t})],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbb{1}(\\cdot)$ is the indicator function. We also use $\\begin{array}{r}{d^{\\pi}(s)=\\sum_{a\\in\\mathcal{A}}d^{\\pi}(s,a)}\\end{array}$ to denote the discounted state occupancy and we use $\\mathbb{E}_{\\pi}$ as a shorthand of $\\mathbb{E}_{(s,a)\\sim d^{\\pi}}[\\cdot]$ or $\\mathbb{E}_{s\\sim d^{\\pi}}[\\cdot]$ to denote the expectation with respect to $d^{\\pi}$ . Thus The objective in safe RL for an agent is to find a policy such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi\\in\\arg\\operatorname*{max}\\;J_{r}(\\pi)\\qquad\\mathrm{s.t.}\\;J_{c}(\\pi)\\leq0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark 3.1. For ease of exposition, this paper exclusively focuses on a single constraint. However, it is readily extendable to accommodate multiple constraints. ", "page_idx": 3}, {"type": "text", "text": "3.2 Function Approximation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In complex environments, the state space $\\boldsymbol{S}$ is usually very large or even infinite. We assume access to a policy class $\\Pi\\subseteq(S\\to\\Delta(A))$ consisting of all candidate policies from which we can search for good policies. We also assume access to a value function class $\\mathcal{F}\\subseteq(S\\times\\mathcal{A}\\to[0,V_{\\mathrm{max}}])$ to model the reward $Q-$ functions, and ${\\mathcal{G}}\\subseteq(S\\times A\\to[-V_{\\operatorname*{max}},V_{\\operatorname*{max}}])$ to model the cost $Q.$ \u2212functions of candidate policies. We further assume access to a function class $\\mathcal{W}\\in\\{w:S\\times A\\rightarrow[0,B_{w}]\\}$ that represents marginalized importance weights with respect to the offilne data distribution. Without loss of generality, we assume that the all-one function is contained in $\\mathcal{W}$ . ", "page_idx": 3}, {"type": "text", "text": "For a given policy $\\pi\\in\\Pi$ , we denote $\\begin{array}{r}{f(s^{\\prime},\\pi):=\\sum_{a^{\\prime}}\\pi(a^{\\prime}|s^{\\prime})f(s^{\\prime},a^{\\prime})}\\end{array}$ for any $s\\in S$ . The Bellman operator $\\mathcal{T}_{r}^{\\pi}:\\mathbb{R}^{S\\times A}\\rightarrow\\mathbb{R}^{S\\times A}$ for the reward is  defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\mathcal{T}_{r}^{\\pi}f)(s,a):=R(s,a)+\\gamma\\mathbb{E}_{\\mathcal{P}(s^{\\prime}\\mid s,a)}[f(s^{\\prime},\\pi)],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The Bellman operator $\\mathcal{T}_{c}^{\\pi}:\\mathbb{R}^{S\\times A}\\rightarrow\\mathbb{R}^{S\\times A}$ for the cost is ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\mathcal{T}_{c}^{\\pi}f)(s,a):=C(s,a)+\\gamma\\mathbb{E}_{\\mathcal{P}(s^{\\prime}\\mid s,a)}[f(s^{\\prime},\\pi)].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let $\\|\\cdot\\|_{2,\\mu}\\,:=\\,\\sqrt{\\mathbb{E}_{\\mu}[(\\cdot)^{2}]}$ denote the Euclidean norm weighted by distribution $\\mu$ . We make the following standard assumptions in offilne RL setting (Xie et al., 2021; Cheng et al., 2022; Zhu et al., 2023) on the representation power of the function classes: ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.2 (Approximate Realizability). Assume there exists $\\epsilon_{1}~\\geq~0$ , s.t. for any given policy $\\pi\\in\\Pi$ , we have $\\operatorname*{min}_{f\\in{\\mathcal{F}}}$ maxadmissible \u03bd $\\|\\b{f}-T_{r}^{\\pi}\\b{f}\\|_{2,\\nu}^{2}\\leq\\epsilon_{1}$ , and $\\mathrm{min}_{f\\in\\mathcal{F}}\\,\\mathrm{max}_{\\mathrm{admissible}\\;\\nu}\\;\\|f-$ $T_{c}^{\\pi}f\\|_{2,\\nu}^{2}\\,\\leq\\,\\epsilon_{1}$ , where $\\nu$ is the state-action distribution of any admissible policy such that $\\nu\\in$ $\\{d^{\\pi},\\forall\\pi\\in\\Pi\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2 assumes that for any policy $\\pi\\,\\in\\,\\Pi$ , $Q_{r}^{\\pi}$ and $Q_{c}^{\\pi}$ are approximately realizable in $\\mathcal{F}$ and $\\mathcal{G}$ . When $\\epsilon_{1}$ is small for all admissible $\\nu$ , we have $f_{r}\\,\\approx\\,Q_{r}^{\\pi}$ , and $f_{c}\\approx Q_{c}^{\\pi}$ . In particular, when $\\epsilon_{1}=0$ , we have $Q_{r}^{\\pi}\\in\\mathcal{F},Q_{c}^{\\pi}\\in\\mathcal{F}$ for any policy $\\pi\\in\\Pi$ . Note that we do not need Bellman completeness assumption Cheng et al. (2022). ", "page_idx": 4}, {"type": "text", "text": "3.3 Offline RL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In offline RL, we assume that the available offline data $\\mathcal{D}=\\{(s_{i},a_{i},r_{i},c_{i},s_{i}^{\\prime})\\}_{i=1}^{N}$ consists of $N$ samples. Samples are i.i.d. (which are common assumptions in unconstrained Cheng et al. (2022), as well as constrained setting Hong et al. (2024)), and the distribution of each tuple $(s,a,r,c,s^{\\prime})$ is specified by a distribution $\\bar{\\mu}\\in\\bar{\\Delta(S\\times A)}$ , which is also the discounted visitation probability of a behavior policy (also denoted by $\\mu$ for simplicity). In particular, $(s,a)\\sim\\mu,r=R(s,a),c=$ $C(s,a),s^{\\prime}\\stackrel{}{\\sim}\\mathcal{P}(\\cdot|s,a)$ . We use $a\\sim\\mu(\\cdot|s)$ , to denote that the action is drawn using the behavior policy and $(s,a,s^{\\prime})\\sim\\mu$ to denote that $(s,a)\\sim\\mu$ , and $s^{\\prime}\\sim\\mathcal{P}(\\cdot|s,a)$ . ", "page_idx": 4}, {"type": "text", "text": "For a given policy $\\pi$ , we define the marginalized importance weights $\\begin{array}{r}{w^{\\pi}(s,a):=\\frac{d^{\\pi}(s,a)}{\\mu(s,a)}}\\end{array}$ which is the ratio between the discounted state-action occupancy of $\\pi$ and the data distribution $\\mu$ . This ratio can be used to measure the concentrability of the data coverage (Xie and Jiang, 2020; Zhan et al., 2022; Rashidinejad et al., 2022; Ozdaglar et al., 2023; Lee et al., 2021). ", "page_idx": 4}, {"type": "text", "text": "In this paper we study offilne RL with access to a dataset with limited coverage. The coverage of a policy $\\pi$ is the dataset can be measured by the weighted $\\ell_{2}$ single policy concentrability coefficient (Zhu et al., 2023; Yin and Wang, 2021; Uehara et al., 2024; Hong et al., 2024): ", "page_idx": 4}, {"type": "text", "text": "Definition 3.3 $\\ell_{2}$ Concentrability). Given a policy $\\pi$ , define $C_{\\ell_{2}}^{\\pi}=\\|w^{\\pi}\\|_{2,\\mu}=\\|d^{\\pi}/\\mu\\|_{2,\\mu}$ . Remark 3.4. The definition here is much weaker than the all policy concentrability used in offline RL (Chen and Jiang, 2019) and safe offilne RL (Le et al., 2019; Hong et al., 2024), which requires the ratio $\\displaystyle\\frac{d^{\\pi}(s,a)}{\\mu(s,a)}$ to be bounded for all $s\\in S$ and $a\\in A$ and all policies $\\pi$ . In particular, the all-policy concentrability assumption essentially requires the dataset to have full coverage of all policies ((nearly all the state action pairs). This requirement is often violated in practical scenarios. This requirement is even impossible to meet in safe offline RL because it would require collecting data from every dangerous state and actions, which clearly is impractical. ", "page_idx": 4}, {"type": "text", "text": "In the following lemma, we compare two variants of single-policy concentrability definition with the $\\ell_{2}$ defined in Definition 3.3. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 (Restate Proposition 2.1 in Zhu et al. (2023)). Define the $\\ell_{\\infty}$ single policy concentrability (Rashidinejad et al., 2021) as $C_{\\ell_{\\infty}}^{\\pi}\\,=\\,\\|d^{\\pi}/\\mu\\|_{\\infty}$ and the Bellman-consistent single-policy concentrability (Xie et al., 2021) as CB\u03c0ellman = maxf\u2208F 2,\u00b5 \u2225\u2225ff\u2212\u2212TT  \u03c0\u03c0ff\u2225\u2225222,d\u03c0 (T could be Tr or Tc in our setting) Then, it always holds $(C_{\\ell_{2}}^{\\pi})^{2}\\leq C_{\\ell_{\\infty}}^{\\pi},C_{\\ell_{2}}^{\\pi}\\leq C_{\\ell_{\\infty}}^{\\pi}$ and there exist offilne $R L$ instances where $(C_{\\ell_{2}}^{\\pi})^{2}\\leq C_{B e l l m a n}^{\\pi}$ , $C_{\\ell_{2}}^{\\pi}\\leq C_{B e l l m a n}^{\\pi}$ . Rem2ark 3.5. It is easy 2to observe that the $\\ell_{2}$ variant is bounded by $\\ell_{\\infty}$ and $C_{B e l l m a n}^{\\pi}$ under some c\u221aases. There is an example (Example 1) in Zhu et al. (2023) showing that $C_{\\ell_{2}}^{\\pi}$ is bounded by a constant $\\sqrt{2}$ while $C_{\\ell_{\\infty}}^{\\pi}$ could be arbitrarily large. For the case when the function class $\\mathcal{F}$ is highly expressive, $C_{B e l l m a n}^{\\pi}$ could be close to $C_{\\ell_{\\infty}}^{\\pi}$ and thus possibly larger than $C_{\\ell_{2}}^{\\pi}$ . Intuitively, $C_{\\ell_{2}}^{\\pi}$ implies that only $\\mathbb{E}_{d^{\\pi}}[w^{\\pi}(s,a)]$ is bounded, rather, $w^{\\pi}(s,a)$ is bounded for all $(s,a)$ in $\\ell_{\\infty}$ concentrability bound. Given the definition of the concentrability, we make the following assumption on the weight function class $\\mathcal{W}$ and a single-policy realizability: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Assumption 3.6 (Boundedness of $\\mathcal{W}$ in $\\ell_{2}$ norm). For all $w\\in\\mathcal{W}$ , assume that $\\|w\\|_{2,\\mu}\\leq C_{\\ell_{2}}^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.7 (Single-policy realizability of $w^{\\pi}$ ). For some policy $\\pi$ that we would like to compete with, assume that $w^{\\pi}\\in\\mathcal{W}$ . ", "page_idx": 4}, {"type": "text", "text": "In this paper, we want to study the robust policy improvement on any reference policy, then we assume that we are provided a reference policy $\\pi_{\\mathrm{ref}}$ . Note that in many applications (e.g., scheduling, networking) we indeed have a reference policy. We want that while applying a sophisticated RL policy it should do better and be safe as well. This is one of the main motivations behind this assumption. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Assumption 3.8 (Reference Policy). We assume access to a reference policy $\\pi_{\\mathrm{ref}}\\in\\Pi$ , which can be queried at any state. ", "page_idx": 5}, {"type": "text", "text": "In many applications such as networking, scheduling, and control problems, there are existing good enough reference policies. In these cases, a robust and safe policy improvement over these reference policies has practical value. If $\\pi_{\\mathrm{ref}}$ is not provided, we can simply run a behavior cloning on the offilne data to extract the behavior policy as $\\pi_{\\mathrm{ref}}$ accurately, as long as the size of the offilne data set is large enough. More discussion can be found in Section C in the Appendix. ", "page_idx": 5}, {"type": "text", "text": "4 Actor-Critic with Importance Weighted Bellman Error ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our algorithm design builds upon the constrained actor-critic method, in which we iteratively optimize a policy and improve the policy based on the evaluation of reward and cost. Consider the following actor-critic approach for solving the optimization problem (5): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\pi}^{*}\\in\\arg\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\,f_{r}^{\\pi}\\big(s_{0},\\pi\\big)\\quad s.t.\\quad f_{c}^{\\pi}\\big(s_{0},\\pi\\big)\\leq0}\\\\ &{f_{r}^{\\pi}\\in\\arg\\operatorname*{min}_{f\\in\\mathcal{F}}\\mathbb{E}_{\\mu}\\big[\\big((f-\\mathcal{T}_{r}f)(s,a)\\big)^{2}\\big],\\quad f_{c}^{\\pi}\\in\\arg\\operatorname*{min}_{f\\in\\mathcal{G}}\\mathbb{E}_{\\mu}\\big[\\big((f-\\mathcal{T}_{c}f)(s,a)\\big)^{2}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we assume that $s_{0}$ is a fixed initial state, and $\\begin{array}{r}{f_{r}(s,\\pi)\\,=\\,\\sum_{a\\in{\\cal{A}}}\\pi(a|s)f_{r}(s,a),f_{c}(s,\\pi)\\,=\\,}\\end{array}$ $\\textstyle\\sum_{a\\in{\\mathcal{A}}}\\pi(a|s)f_{c}(s,a)$ . The policy is optimized by maximizing the r eward $q$ function $f_{r}$ while ensuring that $f_{c}$ satisfies the constraint, and the two functions are trained by minimizing the Bellman error. However, this formulation has several disadvantages. 1) It cannot handle insufficient data coverage, which may fail to provide an accurate estimation of the policy for unseen states and actions. 2)It cannot guarantee robust policy improvement. 3) The actor training step is computationally intractable especially when the policy space is extremely large. ", "page_idx": 5}, {"type": "text", "text": "To address the insufficient data coverage issue, as mentioned in Xie et al. (2021) the critic can include a Bellman-consistent pessimistic evaluation of $\\pi$ , which selects the most pessimistic function that approximately satisfies the Bellman equation, which is called absolute pessimism. Then later as indicated by Cheng et al. (2022), instead of using an absolute pessimism, a relative pessimism approach by considering competing to the behavior policy can obtain a robust improvement over the behavior policy. However, this kind of approach can only a\u221achieve a suboptimal statistical rate of $N^{1/3}$ , and fails to achieve the optimal statistical rate of $1/\\sqrt{N}$ , then later a weighted average Bellman error (Uehara et al., 2020; Xie and Jiang, 2020; Zhu et al., 2023) could be treated as one possible solution for improving the order. We remark here that all the discussions here are for the traditional unconstrained offilne RL. Regarding safety, no existing efficient algorithms in safe offline RL have theoretically demonstrated the property of robust policy improvement with optimal statistical rate. ", "page_idx": 5}, {"type": "text", "text": "Can Primal-dual based approaches achieve result using only single policy coverability?: The most commonly used approach for addressing safe RL problems is primal-dual optimization (Efroni et al., 2020; Altman, 1999). As shown in current offline safe RL literature (Hong et al., 2024; Le et al., 2019), the policy can be optimized by maximizing a new unconstrained \u201creward\" $Q-$ function $f_{r}^{\\pi}(s_{0},\\pi)-\\lambda f_{c}^{\\pi}(s_{0},\\pi)$ where $\\lambda$ is a dual variable. Then, the dual-variable can be tuned by taking gradient descent step. As we discussed in the introduction, all these require all policy concentrability which is not practical especially for safe RL. Important question is whether all policy concentrability assumption can be relaxed. Note that primal-dual algorithm relies on solving the min-max problem $\\mathrm{min}_{\\lambda}\\,\\mathrm{\\bar{m}a x}_{\\pi}\\,f_{r}^{\\pi}(s_{0},\\pi)\\,-\\,\\lambda f_{c}^{\\pi}(s_{0},\\pi)$ . Recent result (Cui and Du, 2022) shows that single policy concentrability assumption is not enough for offline min-max game. Hence, we conjecture that using the primal-dual method we can not relax the all policy concentrability assumption. Intuitively, the primal-dual based method (Hong et al., 2024) rely on bounding the regret in dual domain $\\begin{array}{r}{\\sum_{k}\\bar{(}\\lambda_{k}-\\lambda^{*})(f_{c}^{\\pi_{k}}-0)}\\end{array}$ , hence, all the policies $\\{\\pi_{k}\\}_{k=1}^{K}$ encountered throughout the iteration must be supported by the dataset to evaluate the dual value $\\bar{\\lambda}^{*}\\bar{(f_{c}^{\\pi_{k}}-0)}$ where $\\lambda^{*}$ is the optimal dual value. ", "page_idx": 5}, {"type": "text", "text": "Our novelty: In contrast, we propose an aggression-limited objective function $f_{r}(s_{0},\\pi)-\\lambda~.$ \u00b7 $[f_{c}(s_{0},\\pi)]_{+}$ to control aggressive policies, where $\\{\\cdot\\}_{+}\\;:=\\;\\operatorname*{max}\\{\\cdot,0\\}$ . The high-level intuition behind this aggression-limited objective function is that by appropriately selecting a $\\lambda$ (usually large enough), we penalize all the policies that are not safe. As a result, the policy that maximizes the objective function is the optimal safe policy. This formulation is fundamentally different from the traditional primal-dual approach as it does not require dual-variable tuning, and thus, does not require all policy concentrability. In particular, we only need to bound the primal domain regret which can be done as long as the reference policy is covered by the dataset similar to the unconstrained setup. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Combining all the previous ideas together provides the design of our main algorithm named WSAC (Weighted Safe Actor-Critic). In Section 5, we will provide theoretical guarantees of WSAC and discuss its advantages over existing approaches in offilne safe RL. WSAC aims to solve the following optimization problem: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\pi}^{*}\\in\\underset{\\pi\\in\\Pi}{\\arg\\operatorname*{max}}\\,\\mathcal{L}_{\\mu}(\\pi,f_{r}^{\\pi})-\\lambda\\{\\mathcal{L}_{\\mu}(\\pi,f_{c}^{\\pi})\\}_{+}}\\\\ &{f_{r}^{\\pi}\\in\\underset{f_{r}\\in\\mathcal{F}}{\\arg\\operatorname*{min}}\\,\\mathcal{L}_{\\mu}(\\pi,f_{r})+\\beta\\mathcal{E}_{\\mu}(\\pi,f_{r}),\\quad f_{c}^{\\pi}\\in\\underset{f_{c}\\in\\mathcal{G}}{\\arg\\operatorname*{min}}\\,-\\lambda\\mathcal{L}_{\\mu}(\\pi,f_{c})+\\beta\\hat{\\mathcal{E}}_{\\mu}(\\pi,f_{c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{L}_{\\mu}(\\pi,f)~~:=~~\\mathbb{E}_{\\mu}[f(s,\\pi)~-~f(s,a)]$ , and $\\begin{array}{r l r}{\\mathcal{E}_{\\mu}(\\pi,f)}&{{}:=}&{\\operatorname*{max}_{w\\in\\mathcal{W}}|\\mathbb{E}_{\\mu}[w(s,a)((f~-}\\end{array}$ $\\begin{array}{r}{T_{r}^{\\pi}f)(s,a))\\|,\\hat{\\mathcal{E}}_{\\boldsymbol{\\mu}}(\\pi,f)\\,:=\\,\\operatorname*{max}_{w\\in\\mathcal{W}}|\\mathbb{E}_{\\boldsymbol{\\mu}}[w(s,a)((f-T_{c}^{\\pi}f)(s,a))]|}\\end{array}$ . This formulation can also be treated as a Stackelberg game (Von Stackelberg, 2010) or bilevel optimization problem. We penalize the objective function only when the approximate cost $Q$ -function $f_{c}^{\\pi}$ of the policy $\\pi$ is more perilous than the behavior policy $(f_{c}^{\\pi}(s,\\pi)\\geq f_{c}^{\\pi}(s,a))$ forcing our policy to be as safe as the behavior policy. Maximization over $w$ in for training the two critics can ensure that the Bellman error is small when averaged over measure $\\mu\\cdot w$ for any $w\\in\\mathscr{W}$ , which turns out to be sufficient to control the suboptimality of the learned policy. ", "page_idx": 6}, {"type": "text", "text": "In the following theorem, we show that the solution of the optimization problem (6) is not worse than the behavior policy $\\mu$ in both performance and safety for any $\\beta\\geq0,\\lambda>0$ than the policy $\\mu$ under Assumption 3.2 with $\\epsilon_{1}=0$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. Assume that Assumption 3.2 holds with $\\epsilon_{1}=0$ , and the behavior policy $\\mu\\in\\Pi$ , then for any $\\beta\\geq0,\\lambda>0$ we have $J_{r}(\\bar{\\hat{\\pi}}^{*})\\geq J_{r}(\\mu)$ , and $\\{J_{c}(\\hat{\\pi}^{*})\\}_{+}\\leq\\{J_{c}(\\mu)\\}_{+}+\\frac{1}{\\lambda}$ . ", "page_idx": 6}, {"type": "text", "text": "The result in Theorem 4.1 shows that by selecting $\\lambda$ large enough, for any $\\beta\\geq0$ , the solution can achieve better performance than the behavior policy while maintaining safety that is arbitrarily close to that of the behavior policy. The Theorem verifies the design of our framework which has the potential to have a robust safe improvement. ", "page_idx": 6}, {"type": "text", "text": "In the next section, we will introduce our main algorithm WSAC and provide its theoretical guarantees. ", "page_idx": 6}, {"type": "text", "text": "5 Theoretical Analysis of WSAC ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Main Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present the theoretical version of our new model-free offline safe RL algorithm WSAC. Since we only have access to a dataset $\\mathcal{D}$ instead of the data distribution. WSAC sovles an empirical version of (6): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\pi}\\in\\underset{\\pi\\in\\Pi}{\\arg\\operatorname*{max}}\\;\\mathcal{L}_{\\mathcal{D}}(\\pi,f_{r}^{\\pi})-\\lambda\\{\\mathcal{L}_{\\mathcal{D}}(\\pi,f_{c}^{\\pi})\\}_{+}}\\\\ &{f_{r}^{\\pi}\\in\\underset{f_{r}\\in\\mathcal{F}}{\\arg\\operatorname*{min}}\\;\\mathcal{L}_{\\mathcal{D}}(\\pi,f_{r})+\\beta\\mathcal{E}_{\\mathcal{D}}(\\pi,f_{r}),\\quad f_{c}^{\\pi}\\in\\underset{f_{c}\\in\\mathcal{G}}{\\arg\\operatorname*{min}}\\;-\\lambda\\mathcal{L}_{\\mathcal{D}}(\\pi,f_{c})+\\beta\\hat{\\mathcal{E}}_{\\mathcal{D}}(\\pi,f_{c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathcal{D}}(\\pi,f):=\\!\\!\\mathbb{E}_{\\mathcal{D}}[f(s,\\pi)-f(s,a)]}\\\\ &{\\mathcal{E}_{\\mathcal{D}}(\\pi,f):=\\displaystyle\\operatorname*{max}_{w\\in\\mathcal{W}}\\big|\\mathbb{E}_{\\mathcal{D}}[w(s,a)(f(s,a)-r-\\gamma f(s^{\\prime},\\pi))]\\big|}\\\\ &{\\hat{\\mathcal{E}}_{\\mathcal{D}}(\\pi,f):=\\displaystyle\\operatorname*{max}_{w\\in\\mathcal{W}}\\big|\\mathbb{E}_{\\mathcal{D}}[w(s,a)(f(s,a)-c-\\gamma f(s^{\\prime},\\pi))]\\big|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As shown in Algorithm 1, at each iteration, WSAC selects $f_{r}^{k}$ maximally pessimistic and $f_{c}^{k}$ maximally optimistic for the current policy $\\pi_{k}$ with a weighted regularization on the estimated Bellman error for reward and cost, respectively (Line 4 and 6) to address the worse cases within reasonable range. In order to achieve a safe robust policy improvement, the actor then applies a no-regret policy optimization oracle to update the policy $\\pi_{k+1}$ by optimizing the aggressionlimited objective function compared with the reference policy (Line 7) $f_{r}^{k}(s,a)\\stackrel{\\bullet}{-}\\lambda\\{\\breve{f}_{c}^{k}(s,a)\\stackrel{}{-}$ ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Weighted Safe Actor-Critic (WSAC) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1: Input: Batch data $\\mathcal{D}$ , coefficient $\\beta,\\lambda$ . Value function classes ${\\mathcal{F}},{\\mathcal{G}}$ , importance weight function class $\\mathcal{W}$ , Initialize policy $\\pi_{1}$ randomly. Any reference policy $\\pi_{\\mathrm{ref}}$ . No-regret policy optimization oracle PO (Definition 5.1). ", "page_idx": 7}, {"type": "text", "text": "2: for $k=1,2,\\ldots,K$ do   \n3: Obtain the reward state-action value function estimation of $\\pi_{k}$ :   \n4: $\\begin{array}{r}{f_{r}^{k}\\gets\\arg\\operatorname*{min}_{f_{r}\\in\\mathcal{F}}\\mathcal{L}_{\\mathcal{D}}(\\pi_{k},f_{r})+\\beta\\mathcal{E}_{\\mathcal{D}}(\\pi_{k},f_{r})}\\end{array}$   \n5: Obtain the cost state-action value function estimation of $\\pi_{k}$ :   \n6: $\\begin{array}{r}{f_{c}^{k}\\gets\\arg\\operatorname*{min}_{f_{c}\\in\\mathcal{G}}-\\lambda\\mathcal{L}_{\\mathcal{D}}(\\pi_{k},f_{c})+\\beta\\hat{\\mathcal{E}}_{\\mathcal{D}}(\\pi_{k},f_{c})}\\end{array}$   \n7: Update policy: $\\boldsymbol{\\pi}_{k+1}\\leftarrow\\mathbf{PO}(\\pi_{k},f_{r}^{k}(s,a)-\\lambda\\{f_{c}^{k}(s,a)-f_{c}^{k}(s,\\pi_{\\mathrm{ref}})\\}_{+},\\mathcal{D}).$ // $\\mathcal{L}_{D},\\mathcal{E}_{D},\\hat{\\mathcal{E}}_{D}$   \nare defined in (5.1) ", "page_idx": 7}, {"type": "text", "text": "8: end for ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "// Uniformly mix \u03c01, . . . , \u03c0K ", "page_idx": 7}, {"type": "text", "text": "$\\overline{{f_{c}^{k}(s,\\pi_{\\mathrm{ref}})\\}_{+}}$ . Our algorithm is very computationally efficient and tractable compared with existing approaches (Hong et al., 2024; Le et al., 2019), since we do not need another inner loop for optimizing the dual variable with an additional online algorithm or offline policy evaluation oracle. The policy improvement process relies on a no-regret policy optimization oracle, a technique commonly employed in offline RL literature (Zhu et al., 2023; Cheng et al., 2022; Hong et al., 2024; Zhu et al., 2023). Extensive literature exists on such methodologies. For instance, approaches like soft policy iteration (Pirotta et al., 2013) and algorithms based on natural policy gradients (Kakade, 2001; Agarwal et al., 2021) can function as effective no-regret policy optimization oracles. We now formally define the oracle: ", "page_idx": 7}, {"type": "text", "text": "Definition 5.1 (No-regret policy optimization oracle). An algorithm PO is called a no-regret policy optimization oracle if for any sequence of functions $f^{1},\\ldots,\\bar{f}^{K}$ with $f^{k}:S\\times A\\to[0,\\bar{V_{\\operatorname*{max}}}],\\bar{\\forall}k\\ \\bar{\\in}$ $[K]$ . The policies $\\pi_{1},\\dots,\\pi_{K}$ produced by the oracle PO satisfy that for any policy $\\pi\\in\\Pi$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\epsilon_{o p t}^{\\pi}\\triangleq\\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{E}_{\\pi}[f^{k}(s,\\pi)-f^{k}(s,\\pi_{k})]=o(1)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "There indeed exist many methods that can serve as the no-regret oracle, for example, the mirrordescent approach (Geist et al., 2019) or the natural policy gradient approach (Kakade, 2001) of the form \u03c0k+1(a|s) \u221d\u03c0k(a|s) exp(\u03b7f k(s, a)) with \u03b7 = 2lVo g2 |A|K (Even-Dar et al., 2009; Agarwal et al., 2021). In the following define $\\epsilon_{o p t}^{\\pi}$ as the error generated from the oracle PO by considering $f_{r}^{k}(s,a)-\\lambda\\{f_{c}^{k}(s,a)-f_{c}^{k}(s,\\pi)\\}_{+}$ as the sequence of functions in Definition 5.1, then we have the following guarantee. ", "page_idx": 7}, {"type": "text", "text": "Lemma 2. Applying a no-regret oracle PO for $K$ episodes with $(f_{r}^{k}(s,a)\\!-\\!\\lambda\\{f_{c}^{k}(s,a)\\!-\\!f_{c}^{k}(s,\\pi)\\}_{+})$ for an arbitrary policy $\\pi$ , can guarantee ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{E}_{\\boldsymbol\\pi}[f_{r}^{k}(s,\\boldsymbol\\pi)-f_{r}^{k}(s,\\boldsymbol\\pi_{k})]\\leq\\epsilon_{o p t}^{\\pi}}\\\\ &{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{E}_{\\boldsymbol\\pi}[\\{f_{c}^{k}(s,\\boldsymbol\\pi_{k})-f_{c}^{k}(s,\\boldsymbol\\pi)\\}_{+}]\\leq\\epsilon_{o p t}^{\\pi}+\\frac{V_{\\operatorname*{max}}}{\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma 2 establishes that the policy outputted by PO with considering the aggression-limited \u201creward\" can have a strong guarantee on the performance of both reward and cost when $\\lambda$ is large enough., which is comparable with any competitor policy. This requirement is critical to achieving the performance guarantee of our algorithm and the safe and robust policy improvement. The detailed proof is deferred to Appendix B.2 due to page limit. ", "page_idx": 7}, {"type": "text", "text": "5.2 Theoretical Guarantees ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We are now ready to provide the theoretical guarantees of WSAC Algorithm 1. The complete proof is deferred to Appendix B.3. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2 (Main Theorem). Under Assumptions 3.2 and 3.6, let the reference policy $\\pi_{r e f}\\in\\Pi$ in Algorithm 1 be any policy satisfying Assumption 3.7, then with probability at least $1-\\delta$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{J_{r}(\\pi_{r e f})-J_{r}(\\bar{\\pi})\\leq\\mathcal{O}\\bigg(\\epsilon_{s t a t}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}\\bigg)+\\epsilon_{o p t}^{\\pi_{r e f}}}}\\\\ {{J_{c}(\\bar{\\pi})-J_{c}(\\pi_{r e f})\\leq\\mathcal{O}\\bigg(\\epsilon_{s t a t}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}\\bigg)+\\epsilon_{o p t}^{\\pi_{r e f}}+\\frac{V_{\\operatorname*{max}}}{\\lambda},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "$\\begin{array}{r}{\\epsilon_{s t a t}:=V_{\\operatorname*{max}}C_{\\ell_{2}}^{*}\\sqrt{\\frac{\\log(|\\mathcal{F}||\\Pi||W|/\\delta)}{N}}+\\frac{V_{\\operatorname*{max}}B_{w}\\log(|\\mathcal{F}||\\Pi||W|/\\delta)}{N}}\\end{array}$ , and $\\bar{\\pi}$ is the policy returned by Algorithm 1 with $\\beta=2$ and $\\pi_{r e f}$ as input. ", "page_idx": 8}, {"type": "text", "text": "Remark 5.3. When $\\epsilon_{1}=0$ , i.e., no model misspecification, which states that the true value function belongs to the function class being used to approximate it (the function class is right enough), let $\\pi_{\\mathrm{ref}}$ be the optimal policy, the results in Theorem 5.2 achieve an optimal dependence statistical rate of $\\frac{1}{\\sqrt{N}}$ (for large $k$ ), which matches the best existing results. Our algorithm is both statistically optimal and computationally efficient with only single-policy assumption rather relying much stronger assumptions of all policy concentrability Hong et al. (2024); Le et al. (2019). Hence, if the behavior policy or the reference policy is safe, our result indicates that the policy returned by our algorithm will also be safe (nearly). Such a guarantee was missing in the existing literature. ", "page_idx": 8}, {"type": "text", "text": "Remark 5.4. We also do not need a completeness assumption,which requires that for any $f\\in\\mathcal F$ or $\\mathcal{G}$ and $\\pi\\in\\Pi$ , it approximately holds that $\\mathcal{T}_{r}f\\in\\mathcal{F},\\mathcal{T}_{c}f\\in\\mathcal{F}$ as required in Xie et al. (2021); Chen et al. (2022b). They need this assumption to address over-estimation issues caused by the $\\ell_{2}$ square Bellman error, but our algorithm can get rid of the strong assumption by using a weighted Bellman error which is a simple and unbiased estimator. ", "page_idx": 8}, {"type": "text", "text": "Remark 5.5. Our algorithm can compete with any reference policy $\\pi_{\\mathrm{ref}}\\in\\Pi$ as long as $w^{\\pi_{\\mathrm{ref}}}=d^{\\pi_{\\mathrm{ref}}}/\\mu$ is contained in $\\mathcal{W}$ . The importance ratio of the behavior policy is $w^{\\mu}=d^{\\mu}/\\mu=\\mu/\\mu=1$ which always satisfies this condition, implying that our algorithm can have a safe robust policy improvement (in Theorem 5.6 discussed below). ", "page_idx": 8}, {"type": "text", "text": "5.3 A Safe Robust Policy Improvement ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A Robust policy improvement (RPI)(Cheng et al., 2022; Zhu et al., 2023; Bhardwaj et al., 2024) refers to the property of an offilne RL algorithm that the offilne algorithm can learn to improve over the behavior policy, using a wide range of hyperparameters. In this paper, we introduce the property of Safe Robust policy improvement (SRPI) such that the offilne algorithm can learn to improve over the behavior policy in both return and safety, using a wide range of\u221a hyperparameters. In the following Theorem 5.6 we show that as long as the hyperparameter $\\beta=o(\\sqrt{N})$ , our algorithm can, with high probability, produce a policy with vanishing suboptimality compared to the behavior policy. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.6 (SRPI). Under Assumptions 3.2 and 3.6, then with probability at least $1-\\delta$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{r}(\\mu)-J_{r}(\\bar{\\pi})\\leq\\mathcal{O}\\biggr(\\epsilon_{s t a t}^{\\pi}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}\\biggr)+\\epsilon_{o p t}^{\\mu}}\\\\ &{J_{c}(\\bar{\\pi})-J_{c}(\\mu)\\leq\\mathcal{O}\\biggr(\\epsilon_{s t a t}^{\\pi}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}\\biggr)+\\epsilon_{o p t}^{\\mu}+\\frac{V_{\\operatorname*{max}}}{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "$\\begin{array}{r}{\\epsilon_{s t a t}:=V_{\\operatorname*{max}}C_{\\ell_{2}}^{*}\\sqrt{\\frac{\\log(|\\mathcal{F}||\\Pi||W|/\\delta)}{N}}+\\frac{V_{\\operatorname*{max}}B_{w}\\log(|\\mathcal{F}||\\Pi||W|/\\delta)}{N}}\\end{array}$ VmaxBw log(|F||\u03a0||W |/\u03b4), and \u03c0\u00af is the policy returned by Algorithm $^{\\,I}$ with $\\beta\\geq0$ and $\\mu$ as input. ", "page_idx": 8}, {"type": "text", "text": "The detailed proofs are deferred to Appendix B.4. ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.1 WSAC-Practical Implementation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We introduce a deep RL implementation of WSAC in Algorithm 2 (in Appendix), following the key structure of its theoretical version (Algorithm 1). The reward, cost $Q$ \u2212functions $f_{r},f_{c}$ and the policy network $\\pi$ are all parameterized by neural networks. The critic losses (line 4) $l_{r e w a r d}(f_{r})$ and ", "page_idx": 8}, {"type": "table", "img_path": "82Ndsr4OS6/tmp/0029b3a79f12587c9bb5a3fc8c3e7ddcb3b9b14de880f5bcf12a15247babcfd1.jpg", "table_caption": ["Table 2: The normalized reward and cost of WSAC and other baselines. The Average line shows the average situation in various environments. The cost threshold is 1. Gray: Unsafe agent whose normalized cost is greater than 1. Blue: Safe agent with best performance "], "table_footnote": ["$l_{c o s t}(f_{c})$ are calculated based on the principles of Algorithm 1, on the minibatch dataset. Optimizing the actor aims to achieve a no-regret optimization oracle, we use a gradient based update on the actor loss (line 5) $l_{a c t o r}(\\pi)$ . In the implementation we use adaptive gradient descent algorithm ADAM (Kingma and Ba, 2015) for updating two critic networks and the actor network. Algorithm follows standard two-timescale first-order algorithms (Fujimoto et al., 2018; Haarnoja et al., 2018) with a fast learning rate $\\eta_{f a s t}$ on update critic networks and a slow learning rate $\\eta_{s l o w}$ for updating the actor. "], "page_idx": 9}, {"type": "text", "text": "6.2 Simulations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present a scalable deep RL version of WSAC in Algorithm 2, following the principles of Algorithm 1. We evaluate WSAC and consider Behavior Cloning (BC), safe Behavior Cloning (Safe-BC), BatchConstrained deep Q-learning with Lagrangian PID (BCQL) Fujimoto et al. (2019); Stooke et al. (2020) , bootstrapping error accumulation reduction with Lagrangian PID (BEARL) Kumar et al. (2019); Stooke et al. (2020), Constraints Penalized Q-learning (CPQ) Xu et al. (2022) and one of the state-of-the-art algorithms, COptiDICE (Lee et al., 2022) as baselines. ", "page_idx": 9}, {"type": "text", "text": "We study several representative environments and focus on presenting \u201cBallCircle\u201d. In BallCircle, it requires the ball on a circle in a clockwise direction without leaving the safety zone defined by the boundaries as proposed by Achiam et al. (2017). The ball is a spherical-shaped agent which can freely move on the xy-plane. The reward is dense and increases by the car\u2019s velocity and by the proximity towards the boundary of the circle. The cost is incurred if the agent leaves the safety zone defined by the boundaries. ", "page_idx": 9}, {"type": "text", "text": "We use the offline dataset from Liu et al. (2019), where the corresponding expert policy are used to interact with the environments and collect the data. To better illustrate the results, we normalize the reward and cost. Our simulation results are reported in Table 2, we observe that WSAC can guarantee that all the final agents are safe, which is most critical in safe RL literature. Even in challenging environments such as PointButton, which most baselines fail to learn safe policies. WSAC has the best results in 3 of the environments. Moreover, WSAC outperforms all the baselines in terms of the average performance, demonstrating its ability to learn a safe policy by leveraging an offline dataset. The simulation results verify our theoretical findings. We also compared WSAC with all the baselines in the case where the cost limits are different, WSAC still outperforms all the other baselines and ensures a safe policy. We further include simulations to investigate the contribution of each component of our algorithm, including the weighted Bellman regularizer, the aggression-limited objective, and the no-regret policy optimization which together guarantee the theoretical results. More details and discussions are deferred to the Appendix D due to page limit. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we explore the problem of offilne Safe-RL with a single policy data coverage assumption. We propose a novel algorithm, WSAC, which, for the first time, is proven to guarantee the property of safe robust policy improvement. WSAC is able to outperform any reference policy, including the behavior policy, while maintaining the same level of safety across a broad range of hyperparameters. Our simulation results demonstrate that WSAC outperforms existing state-of-the-art offilne safe-RL algorithms. Interesting future work includes combining WSAC with online exploration with safety guarantees and extending the approach to multi-agent settings to handle coupled constraints. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017). Constrained policy optimization. In Int. Conf. Machine Learning (ICML), volume 70, pages 22\u201331. JMLR. ", "page_idx": 10}, {"type": "text", "text": "Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G. (2021). On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1\u201376. ", "page_idx": 10}, {"type": "text", "text": "Altman, E. (1999). Constrained Markov decision processes, volume 7. CRC Press. ", "page_idx": 10}, {"type": "text", "text": "Bhardwaj, M., Xie, T., Boots, B., Jiang, N., and Cheng, C.-A. (2024). Adversarial model for offilne reinforcement learning. Advances in Neural Information Processing Systems, 36.   \nChen, F., Zhang, J., and Wen, Z. (2022a). A near-optimal primal-dual method for off-policy learning in cmdp. In Advances Neural Information Processing Systems (NeurIPS), volume 35, pages 10521\u201310532.   \nChen, J. and Jiang, N. (2019). Information-theoretic considerations in batch reinforcement learning. In Int. Conf. Machine Learning (ICML), pages 1042\u20131051. PMLR.   \nChen, J. and Jiang, N. (2022). Offilne reinforcement learning under value and density-ratio realizability: the power of gaps. In Uncertainty in Artificial Intelligence, pages 378\u2013388. PMLR.   \nChen, L., Jain, R., and Luo, H. (2022b). Learning infinite-horizon average-reward markov decision process with constraints. In Int. Conf. Machine Learning (ICML), pages 3246\u20133270. PMLR.   \nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. (2021). Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097.   \nCheng, C.-A., Kolobov, A., and Agarwal, A. (2020). Policy improvement via imitation of multiple oracles. In Advances Neural Information Processing Systems (NeurIPS), volume 33, pages 5587\u2013 5598.   \nCheng, C.-A., Xie, T., Jiang, N., and Agarwal, A. (2022). Adversarially trained actor critic for offilne reinforcement learning. In International Conference on Machine Learning, pages 3852\u20133878. PMLR.   \nChow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M. (2017). Risk-constrained reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18(1):6070\u20136120.   \nCui, Q. and Du, S. S. (2022). When are offline two-player zero-sum markov games solvable? Advances in Neural Information Processing Systems, 35:25779\u201325791.   \nEfroni, Y., Mannor, S., and Pirotta, M. (2020). Exploration-exploitation in constrained MDPs. arXiv preprint arXiv:2003.02189.   \nEven-Dar, E., Kakade, S. M., and Mansour, Y. (2009). Online markov decision processes. Mathematics of Operations Research, 34(3):726\u2013736.   \nFujimoto, S., Meger, D., and Precup, D. (2019). Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pages 2052\u20132062. PMLR.   \nFujimoto, S., van Hoof, H., and Meger, D. (2018). Addressing function approximation error in actor-critic methods. In Int. Conf. Machine Learning (ICML), pages 1582\u20131591.   \nGeist, M., Scherrer, B., and Pietquin, O. (2019). A theory of regularized markov decision processes. In International Conference on Machine Learning, pages 2160\u20132169. PMLR.   \nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft Actor-Critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Int. Conf. Machine Learning (ICML), pages 1861\u20131870.   \nHong, K., Li, Y., and Tewari, A. (2024). A primal-dual-critic algorithm for offline constrained reinforcement learning. In Int. Conf. Artificial Intelligence and Statistics (AISTATS), pages 280\u2013 288. PMLR.   \nIsele, D., Nakhaei, A., and Fujimura, K. (2018). Safe reinforcement learning on autonomous vehicles. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1\u20136. IEEE.   \nKakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning. In Int. Conf. Machine Learning (ICML), pages 267\u2013274.   \nKakade, S. M. (2001). A natural policy gradient. In Advances Neural Information Processing Systems (NeurIPS).   \nKingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In Bengio, Y. and LeCun, Y., editors, Int. Conf. on Learning Representations (ICLR).   \nKumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. (2019). Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32.   \nKumar, A., Hong, J., Singh, A., and Levine, S. (2022). Should i run offilne reinforcement learning or behavioral cloning? In Int. Conf. on Learning Representations (ICLR).   \nLaroche, R., Trichelair, P., and Des Combes, R. T. (2019). Safe policy improvement with baseline bootstrapping. In International conference on machine learning, pages 3652\u20133661. PMLR.   \nLe, H., Voloshin, C., and Yue, Y. (2019). Batch policy learning under constraints. In International Conference on Machine Learning, pages 3703\u20133712. PMLR.   \nLee, J., Jeon, W., Lee, B., Pineau, J., and Kim, K.-E. (2021). Optidice: Offline policy optimization via stationary distribution correction estimation. In Meila, M. and Zhang, T., editors, Int. Conf. Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pages 6120\u20136130. PMLR.   \nLee, J., Paduraru, C., Mankowitz, D. J., Heess, N., Precup, D., Kim, K.-E., and Guez, A. (2022). Coptidice: Offline constrained reinforcement learning via stationary distribution correction estimation. arXiv preprint arXiv:2204.08957.   \nLiao, P., Qi, Z., Wan, R., Klasnja, P., and Murphy, S. A. (2022). Batch policy learning in average reward markov decision processes. Annals of statistics, 50(6):3364.   \nLiu, B., Cai, Q., Yang, Z., and Wang, Z. (2019). Neural trust region/proximal policy optimization attains globally optimal policy. Advances in neural information processing systems, 32.   \nLiu, Z., Guo, Z., Lin, H., Yao, Y., Zhu, J., Cen, Z., Hu, H., Yu, W., Zhang, T., Tan, J., et al. (2023a). Datasets and benchmarks for offilne safe reinforcement learning. arXiv preprint arXiv:2306.09303.   \nLiu, Z., Guo, Z., Yao, Y., Cen, Z., Yu, W., Zhang, T., and Zhao, D. (2023b). Constrained decision transformer for offline safe reinforcement learning. In International Conference on Machine Learning, pages 21611\u201321630. PMLR.   \nOzdaglar, A. E., Pattathil, S., Zhang, J., and Zhang, K. (2023). Revisiting the linear-programming framework for offline rl with general function approximation. In International Conference on Machine Learning, pages 26769\u201326791. PMLR.   \nPirotta, M., Restelli, M., Pecorino, A., and Calandriello, D. (2013). Safe policy iteration. In Int. Conf. Machine Learning (ICML), pages 307\u2013315. PMLR.   \nRajaraman, N., Yang, L., Jiao, J., and Ramchandran, K. (2020). Toward the fundamental limits of imitation learning. Advances Neural Information Processing Systems (NeurIPS), 33:2914\u20132924.   \nRashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021). Bridging offline reinforcement learning and imitation learning: A tale of pessimism. In Advances Neural Information Processing Systems (NeurIPS), volume 34, pages 11702\u201311716.   \nRashidinejad, P., Zhu, H., Yang, K., Russell, S., and Jiao, J. (2022). Optimal conservative offline rl with general function approximation via augmented lagrangian. arXiv preprint arXiv:2211.00716.   \nSiegel, N. Y., Springenberg, J. T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner, R., Heess, N., and Riedmiller, M. (2020). Keep doing what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint arXiv:2002.08396.   \nStooke, A., Achiam, J., and Abbeel, P. (2020). Responsive safety in reinforcement learning by pid lagrangian methods. In Int. Conf. Machine Learning (ICML), pages 9133\u20139143. PMLR.   \nUehara, M., Huang, J., and Jiang, N. (2020). Minimax weight and q-function learning for off-policy evaluation. In International Conference on Machine Learning, pages 9659\u20139668. PMLR.   \nUehara, M., Kallus, N., Lee, J. D., and Sun, W. (2024). Offline minimax soft-q-learning under realizability and partial coverage. Advances in Neural Information Processing Systems, 36.   \nUehara, M. and Sun, W. (2021). Pessimistic model-based offilne reinforcement learning under partial coverage. arXiv preprint arXiv:2107.06226.   \nVon Stackelberg, H. (2010). Market structure and equilibrium. Springer Science & Business Media.   \nWang, L., Cai, Q., Yang, Z., and Wang, Z. (2019). Neural policy gradient methods: Global optimality and rates of convergence. arXiv preprint arXiv:1909.01150.   \nWu, R., Zhang, Y., Yang, Z., and Wang, Z. (2021). Offilne constrained multi-objective reinforcement learning via pessimistic dual value iteration. In Advances Neural Information Processing Systems (NeurIPS), volume 34, pages 25439\u201325451.   \nXie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021). Bellman-consistent pessimism for offilne reinforcement learning. Advances in neural information processing systems, 34:6683\u2013 6694.   \nXie, T. and Jiang, N. (2020). $\\mathrm{Q^{*}}$ approximation schemes for batch reinforcement learning: A theoretical comparison. In Conference on Uncertainty in Artificial Intelligence, pages 550\u2013559. PMLR.   \nXie, T. and Jiang, N. (2021). Batch value-function approximation with only realizability. In Int. Conf. Machine Learning (ICML), pages 11404\u201311413. PMLR.   \nXu, H., Zhan, X., and Zhu, X. (2022). Constraints penalized q-learning for safe offilne reinforcement learning. In AAAI Conf. Artificial Intelligence, volume 36, pages 8753\u20138760.   \nYin, M. and Wang, Y.-X. (2021). Towards instance-optimal offline reinforcement learning with pessimism. Advances in neural information processing systems, 34:4065\u20134078.   \nZhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J. (2022). Offline reinforcement learning with realizability and single-policy concentrability. In Proc. Conf. Learning Theory (COLT), pages 2730\u20132775. PMLR.   \nZhang, J., Koppel, A., Bedi, A. S., Szepesvari, C., and Wang, M. (2020). Variational policy gradient method for reinforcement learning with general utilities. Advances in Neural Information Processing Systems, 33:4572\u20134583.   \nZheng, Y., Li, J., Yu, D., Yang, Y., Li, S. E., Zhan, X., and Liu, J. (2024). Safe offline reinforcement learning with feasibility-guided diffusion model. arXiv preprint arXiv:2401.10700.   \nZhu, H., Rashidinejad, P., and Jiao, J. (2023). Importance weighted actor-critic for optimal conservative offline reinforcement learning. arXiv preprint arXiv:2301.12714. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Auxiliary Lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the following, we first provide several lemmas which are useful for proving our main results. Lemma 3. With probability at least $1-\\delta,$ , for any $f_{r}\\in\\mathcal{F},f_{c}\\in\\mathcal{G},\\pi\\in\\Pi$ and $w\\in\\mathcal{W}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\vert\\mathbb{E}_{\\mu}[(f_{r}-\\mathcal T_{r}^{\\pi}f)w]\\vert-\\Big|\\frac{1}{N}\\sum_{(s,a,r,s^{\\prime})}w(s,a)(f_{r}(s,a)-r-\\gamma f_{r}(s^{\\prime},\\pi))\\Big|\\Big|}\\\\ &{\\le\\!O\\Big(V_{\\operatorname*{max}}\\sqrt{\\frac{\\log(|\\mathcal{F}||\\Pi||\\vert\\mathcal{W}\\vert/\\delta)}{N}}+\\frac{V_{\\operatorname*{max}}B_{w}\\log(|\\mathcal{F}||\\Pi|\\vert\\mathcal{W}\\vert/\\delta)}{N}\\Big)}\\\\ &{\\ \\ \\Big|\\vert\\mathbb{E}_{\\mu}[(f_{c}-\\mathcal T_{c}^{\\pi}f)w]\\vert-\\Big|\\frac{1}{N}\\sum_{(s,a,r,s^{\\prime})}w(s,a)(f_{c}(s,a)-c-\\gamma f_{c}(s^{\\prime},\\pi))\\Big|\\Big|}\\\\ &{\\le\\!O\\Big(V_{\\operatorname*{max}}\\sqrt{\\frac{\\log(|\\mathcal{G}||\\Pi|\\vert\\vert\\mathcal{W}\\vert/\\delta)}{N}}+\\frac{V_{\\operatorname*{max}}B_{w}\\log(|\\mathcal{G}||\\Pi|\\vert\\vert\\mathcal{W}\\vert/\\delta)}{N}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The proofs can be found in Lemma 4 in Zhu et al. (2023). ", "page_idx": 13}, {"type": "text", "text": "Lemma 4. With probability at least $1-2\\delta_{\\mathrm{i}}$ for any $f_{r}\\in\\mathcal{F},f_{c}\\in\\mathcal{G}$ and $\\pi\\in\\Pi$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{E}_{\\mu}(\\pi,f_{r})-\\mathcal{E}_{\\mathcal{D}}(\\pi,f_{r})|\\leq\\epsilon_{s t a t}}\\\\ &{|\\mathcal{E}_{\\mu}(\\pi,f_{c})-\\mathcal{E}_{\\mathcal{D}}(\\pi,f_{c})|\\leq\\epsilon_{s t a t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$\\begin{array}{r}{\\epsilon_{s t a t}:=V_{\\operatorname*{max}}C_{\\ell_{2}}^{*}\\sqrt{\\frac{\\log(|\\mathcal{F}||\\mathcal{G}||\\Pi||W|/\\delta)}{N}}+\\frac{V_{\\operatorname*{max}}B_{w}\\log(|\\mathcal{F}||\\mathcal{G}||\\Pi||W|/\\delta)}{N}.}\\end{array}$ ", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Condition on the high probability event in Lemma 3,for any $f_{r}\\in\\mathcal{F},f_{c}\\in\\mathcal{G},\\pi\\in\\Pi$ , define $w_{\\pi,f}^{*}=\\arg\\operatorname*{max}_{w\\in\\mathcal{W}}\\mathcal{E}_{\\mu}(\\pi,f_{r})=\\arg\\operatorname*{max}_{w\\in\\mathcal{W}}\\left|\\mathbb{E}_{\\mu}[w(s,a)(f_{r}-\\mathcal{T}_{r}^{\\pi}f_{r})(s,a)]\\right|$ ", "page_idx": 13}, {"type": "text", "text": "and define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{w}_{\\pi,f_{r}}=\\arg\\operatorname*{max}_{w\\in\\mathcal{W}}\\mathcal{E}_{\\mathcal{D}}(\\pi,f_{r})=\\arg\\operatorname*{max}_{w\\in\\mathcal{W}}|\\frac{1}{N}\\sum_{(s,a,r,s^{\\prime})\\in\\mathcal{D}}w(s,a)(f_{r}(s,a)-r-\\gamma f_{r}(s^{\\prime},\\pi))|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then we can have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{T}_{\\mu}(\\pi,f_{r})-\\mathcal{E}_{\\mathcal{D}}(\\pi,f_{r})}\\\\ &{=\\lvert\\mathbb{E}_{\\mu}[w_{\\pi,f_{r}}^{*}(s,a)(f_{r}-\\mathcal{T}_{r}^{\\pi}f_{r})(s,a)]\\rvert-\\bigg\\lvert\\frac{1}{N}\\sum_{(s,a,r,s^{\\prime})}\\hat{w}_{\\pi,f}(s,a)(f_{r}(s,a)-r-\\gamma f_{r}^{\\prime}(s^{\\prime},\\pi))\\bigg\\rvert}\\\\ &{=\\lvert\\mathbb{E}_{\\mu}[w_{\\pi,f_{r}}^{*}(s,a)(f_{r}-\\mathcal{T}_{r}^{\\pi}f_{r})(s,a)]\\rvert-\\lvert\\mathbb{E}_{\\mu}[\\hat{w}_{\\pi,f_{r}}(s,a)(f_{r}-\\mathcal{T}_{r}^{\\pi}f_{r})(s,a)]\\rvert}\\\\ &{\\quad+\\left\\lvert\\mathbb{E}_{\\mu}[\\hat{w}_{\\pi,f_{r}}(s,a)(f_{r}-\\mathcal{T}_{r}^{\\pi}f_{r})(s,a)]\\right\\rvert-\\bigg\\lvert\\frac{1}{N}\\sum_{(s,a,r,s^{\\prime})}\\hat{w}_{\\pi,f}(s,a)(f_{r}(s,a)-r-\\gamma f_{r}^{\\prime}(s^{\\prime},\\pi))\\bigg\\rvert}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$\\geq0-\\epsilon_{s t a t}=-\\epsilon_{s t a t}$ ", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the inequality is true by using the definition of $w_{\\pi,f_{r}}^{*}$ and Lemma 3. Thus ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{E}_{\\mu}(\\pi,f_{r})-\\epsilon_{\\mathcal{D}}(\\pi,f_{r})}\\\\ &{=\\lvert\\mathbb{E}_{\\mu}[w_{\\pi,f_{r}}^{*}(s,a)(f_{r}-{\\mathcal T}_{r}^{\\pi}f_{r})(s,a)]\\rvert-\\bigg\\lvert\\frac{1}{N}\\displaystyle\\sum_{(s,a,r,s^{\\prime})}w_{\\pi,f_{r}}^{*}(s,a)(f_{r}(s,a)-r-\\gamma f_{r}^{\\prime}(s^{\\prime},\\pi))\\bigg\\rvert}\\\\ &{+\\bigg\\lvert\\frac{1}{N}\\displaystyle\\sum_{(s,a,r,s^{\\prime})}w_{\\pi,f_{r}}^{*}(s,a)(f_{r}(s,a)-r-\\gamma f_{r}^{\\prime}(s^{\\prime},\\pi))\\bigg\\rvert}\\\\ &{\\quad-\\left\\lvert\\frac{1}{N}\\displaystyle\\sum_{(s,a,r,s^{\\prime})}\\hat{w}_{\\pi,f_{r}}(s,a)(f_{r}(s,a)-r-\\gamma f_{r}^{\\prime}(s^{\\prime},\\pi))\\right\\rvert}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The proof for the case $|\\mathcal{E}_{\\mu}(\\pi,f_{c})-\\mathcal{E}_{\\mathcal{D}}(\\pi,f_{c})|\\leq\\epsilon_{s t a t}$ is similar. ", "page_idx": 13}, {"type": "text", "text": "Lemma 5. (Empirical weighted average Bellman Error) With probability at least $1-2\\delta$ , for any $\\pi\\in\\Pi$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathcal{E}}_{\\mathcal{D}}(\\pi,f_{r}^{\\pi})\\leq{C}_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}+{\\epsilon_{s t a t}}}\\\\ {{\\mathcal{E}}_{\\mathcal{D}}(\\pi,f_{c}^{\\pi})\\leq{C}_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}+{\\epsilon_{s t a t}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{r}^{\\pi}:=\\arg\\operatorname*{min}_{a d m i s s i b l e}\\|f_{r}-\\mathcal{T}_{r}^{\\pi}f_{r}\\|_{2,\\nu}^{2},\\forall\\pi\\in\\Pi}\\\\ &{f_{c}^{\\pi}:=\\arg\\operatorname*{min}_{a d m i s s i b l e}\\|f_{c}-\\mathcal{T}_{c}^{\\pi}f_{c}\\|_{2,\\nu}^{2},\\forall\\pi\\in\\Pi.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Condition on the high probability event in Lemma 4, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phantom{\\leq}\\mathcal{E}_{\\mu}(\\pi,f_{r}^{\\pi})=\\displaystyle\\operatorname*{max}_{w\\in\\mathcal{W}}|\\mathbb{E}_{\\mu}[w(s,a)((f-T_{r}^{\\pi}f_{r}^{\\pi})(s,a))]|}\\\\ &{\\leq\\!\\!\\mathcal{E}_{\\mu}(\\pi,f_{r}^{\\pi})=\\displaystyle\\operatorname*{max}_{w\\in\\mathcal{W}}||\\!|w||_{2,\\mu}||f_{\\pi}-T_{r}^{\\pi}f)(s,a))|\\!|_{2,\\mu}}\\\\ &{\\leq\\!C_{\\ell_{2}^{\\ast}}\\!\\sqrt{\\epsilon_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first inequality is true because of Cauchy-Schwarz inequality and the second inequality comes from the definition of $f_{r}^{\\pi}$ and Assumption 3.2, thus we can obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathcal{D}}(\\pi,f_{r}^{\\pi})\\leq\\mathcal{E}_{\\mu}(\\pi,f_{r}^{\\pi})+\\epsilon_{s t a t}\\leq C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}+\\epsilon_{s t a t}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Following a similar proof we can have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathcal{E}}_{\\mathcal{D}}(\\pi,f_{c}^{\\pi})\\le\\mathcal{E}_{\\mu}(\\pi,f_{c}^{\\pi})+\\epsilon_{s t a t}\\le C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}+\\epsilon_{s t a t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 6. (Performance difference decomposition, restate of Lemma 12 in Cheng et al. (2022)) For an arbitrary policy $\\pi,\\hat{\\pi}\\in\\Pi$ , and $f$ be an arbitrary function over $s\\times A$ . Then we have, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad J_{\\circ}(\\pi)-J_{\\circ}(\\hat{\\pi})}\\\\ &{=\\!\\mathbb{E}_{\\mu}\\big[\\big(f-\\mathcal{T}_{\\circ}^{\\hat{\\pi}}\\big)(s,a)\\big]+\\mathbb{E}_{\\pi}\\big[\\big(\\mathcal{T}_{\\circ}^{\\hat{\\pi}}f-f\\big)(s,a)\\big]+\\mathbb{E}_{\\pi}\\big[f(s,\\pi)-f(s,\\hat{\\pi})\\big]+\\mathcal{L}_{\\mu}(\\hat{\\pi},f)-\\mathcal{L}_{\\mu}(\\hat{\\pi},Q_{\\circ}^{\\hat{\\pi}}f-f)\\big(s,\\hat{\\pi}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\diamond:=r\\,o r\\,c$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. We prove the case when $\\diamond\\ :=\\ r$ , the other case is identical. Let $R^{f,{\\hat{\\pi}}}(s,a)\\,:=\\,f(s,a)\\,-$ $\\gamma\\mathbb{E}_{s^{\\prime}|(s,a)}[\\bar{f}(s^{\\prime},\\hat{\\pi})]$ be a virtual reward function for given $f$ and $\\hat{\\pi}$ . According to performance difference lemma (Kakade and Langford, 2002), We first have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(J_{r}(\\hat{\\pi})-J_{r}(\\mu)\\right)=\\!\\!\\mathcal{L}_{\\mu}(\\hat{\\pi},Q_{r}^{\\hat{\\pi}})}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!(\\Delta(\\hat{\\pi}):=\\mathcal{L}_{\\mu}(\\hat{\\pi},Q_{r}^{\\hat{\\pi}})-\\mathcal{L}_{\\mu}(\\hat{\\pi},f))}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last equality is true because that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{R^{f,\\hat{\\pi}}}^{\\pi}(s,a)=(T_{R^{f,\\hat{\\pi}}}^{\\pi}f)(s,a)=R^{f,\\hat{\\pi}}+\\gamma\\mathbb{E}_{s^{\\prime}|(s,a)}[f(s^{\\prime},\\hat{\\pi})]=f(s,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(J_{r}(\\pi)-J_{r}(\\hat{\\pi}))=\\!(J_{r}(\\pi)-J_{r}(\\mu)-(J_{r}(\\hat{\\pi})-J_{r}(\\mu))}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\!(J_{r}(\\pi)-f(d_{0},\\hat{\\pi}))+\\biggl(\\mathbb{E}_{\\mu}[R^{\\hat{\\pi},f}(s,a)]-J_{r}(\\mu)\\biggr)-\\Delta(\\hat{\\pi}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the first term, we have ", "page_idx": 15}, {"type": "text", "text": "nitial state) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{(J_{r}(\\pi)-f(d_{0},\\hat{\\pi}))=(J_{r}(\\pi)-f(s_{0},\\hat{\\pi}))}}&{}&{\\mathrm{(deterministic~in~}}\\\\ &{}&{=J_{r}(\\pi)-\\mathbb{E}_{d^{\\pi}}[R^{\\hat{\\pi},f}(s,a)]+\\mathbb{E}_{d^{\\pi}}[R^{\\hat{\\pi},f}(s,a)]-f(s_{0},\\hat{\\pi})}\\\\ &{}&{=\\mathbb{E}_{d^{\\pi}}[R(s,a)-R^{\\hat{\\pi},f}(s,a)]+\\mathbb{E}_{d^{\\pi}}[f(s,\\pi)-f(s,\\hat{\\pi})]}\\\\ &{}&{=\\mathbb{E}_{d^{\\pi}}[(\\mathcal T_{r}^{\\hat{\\pi}}f-f)(s,a)]+\\mathbb{E}_{d^{\\pi}}[f(s,\\pi)-f(s,\\hat{\\pi})],~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second equality is true because ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{d^{\\pi}}[R^{\\#,f}(s,a)]-f(s_{0},\\hat{n})}\\\\ &{=\\mathbb{E}_{d^{\\pi}}\\left[f(s,a)-\\gamma\\mathbb{E}_{s^{\\pi}|(s,a)}[f(s^{\\prime},\\hat{\\pi})]\\right]-f(s_{0},\\hat{\\pi})}\\\\ &{=\\mathbb{E}_{d^{\\pi}}\\left[f(s,\\pi)\\right]-\\displaystyle\\sum_{s}\\sum_{t=1}^{\\infty}\\gamma^{t}\\,\\mathrm{Pr}(s_{t}=s|s_{0}\\sim d_{0},\\pi)f(s,\\hat{\\pi}(s))-f(s_{0},\\hat{\\pi})}\\\\ &{=\\mathbb{E}_{d^{\\pi}}\\left[f(s,\\pi)\\right]-\\displaystyle\\sum_{s}\\sum_{t=0}^{\\infty}\\gamma^{t}\\,\\mathrm{Pr}\\big(s_{t}=s|s_{0}\\sim d_{0},\\pi)f(s,\\hat{\\pi}(s))}\\\\ &{=\\mathbb{E}_{d^{\\pi}}\\left[f(s,\\pi)\\right]-\\displaystyle\\sum_{s,a}\\sum_{t=0}^{\\infty}\\gamma^{t}\\,\\mathrm{Pr}\\big(s_{t}=s,a_{t}=a|s_{0}\\sim d_{0},\\pi\\big)f(s,\\hat{\\pi}(s))}\\\\ &{=\\mathbb{E}_{d^{\\pi}}\\left[f(s,\\pi)\\right]-\\displaystyle\\sum_{s,a}\\sum_{t=0}^{\\infty}\\gamma^{t}\\,\\mathrm{Pr}\\big(s_{t}=s,a_{t}=a|s_{0}\\sim d_{0},\\pi\\big)f(s,\\hat{\\pi}(s))}\\\\ &{=\\mathbb{E}_{d^{\\pi}}\\left[f(s,\\pi)\\right]-\\displaystyle\\sum_{s,a}f(s,\\hat{\\pi})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the second term we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\mathbb{E}_{\\mu}[R^{\\hat{\\pi},f}(s,a)]-J_{r}(\\mu)}\\\\ &{{=}\\mathbb{E}_{\\mu}[R^{\\hat{\\pi},f}(s,a)-R(s,a)]}\\\\ &{{=}\\mathbb{E}_{\\mu}[(f-T_{r}^{\\hat{\\pi}}f)(s,a)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore plugging 26 and (27) into Eq. (25), we have ", "page_idx": 15}, {"type": "text", "text": "$J_{r}(\\pi)-J_{r}(\\hat{\\pi})$ $=\\!\\mathbb{E}_{\\mu}\\big[\\big(f-T_{r}^{\\hat{\\pi}}\\big)(s,a)\\big]+\\mathbb{E}_{\\pi}\\big[\\big(T_{r}^{\\hat{\\pi}}f-f\\big)(s,a)\\big]+\\mathbb{E}_{\\pi}\\big[f(s,\\pi)-f(s,\\hat{\\pi})\\big]+\\mathcal{L}_{\\mu}(\\hat{\\pi},f)-\\mathcal{L}_{\\mu}(\\hat{\\pi},Q_{r}^{\\hat{\\pi}})\\big]$ ). The proof is completed. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma 7. With probability at least $1-2\\delta$ , for any $f_{r}\\in\\mathcal{F},f_{c}\\in\\mathcal{G}$ , and $\\pi\\in\\Pi$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{L}_{\\mu}(\\pi,f_{r})-\\mathcal{L}_{\\mathcal{D}}(\\pi,f_{r})|\\leq\\epsilon_{s t a t}}\\\\ &{\\qquad\\qquad\\qquad\\left|\\mathcal{L}_{\\mu}(\\pi,f_{c})-\\mathcal{L}_{\\mathcal{D}}(\\pi,f_{c})\\right|\\leq\\epsilon_{s t a t}}\\\\ &{\\epsilon_{s t a t}:=V_{\\operatorname*{max}}C_{\\ell_{2}}^{*}\\sqrt{\\frac{\\log(|\\mathcal{F}||\\mathcal{G}||\\Pi||W|/\\delta)}{N}}+\\frac{V_{\\operatorname*{max}}B_{w}\\log(|\\mathcal{F}||\\mathcal{G}||\\Pi||W|/\\delta)}{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Recall that $\\mathbb{E}_{\\mu}[\\mathcal{L}_{\\mathcal{D}}(\\pi,f_{r})]=\\mathcal{L}_{\\mu}(\\pi,f)$ and $\\left|f_{r}(s,\\pi)-f_{r}(s,a)\\right|\\le V_{\\mathrm{max}}$ . For any $f_{r}\\in\\mathcal{F}$ , policy $\\pi\\in\\Pi$ , applying a Hoeffding\u2019s inequality and a union bound we can obtain with probability $1-\\delta$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathcal{L}_{\\mu}(\\pi,f_{r})-\\mathcal{L}_{\\mathcal{D}}(\\pi,f_{r})|\\leq\\mathcal{O}\\bigg(V_{\\mathrm{max}}\\sqrt{\\frac{\\log(|\\mathcal{F}||\\Pi|/\\delta)}{N}}\\bigg)\\leq\\epsilon_{s t a t}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The inequality for proving the $f_{c},\\pi$ is the same. ", "page_idx": 15}, {"type": "text", "text": "B Missing Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. According to the performance difference lemma (Kakade and Langford, 2002), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(J_{r}(\\pi)-J_{r}(\\mu))-\\lambda\\{J_{c}(\\pi)-J_{c}(\\mu)\\}_{+}}\\\\ &{=\\!\\mathcal{L}_{\\mu}(\\pi,Q_{r}^{\\pi})-\\lambda\\{\\mathcal{L}_{\\mu}(\\pi,Q_{c}^{\\pi})\\}_{+}}\\\\ &{=\\!\\mathcal{L}_{\\mu}(\\pi,Q_{r}^{\\pi})+\\beta\\mathcal{E}_{\\mu}(\\pi,Q_{r}^{\\pi})-\\lambda\\{\\mathcal{L}_{\\mu}(\\pi,Q_{c}^{\\pi})\\}_{+}+\\beta\\hat{\\mathcal{E}}_{\\mu}(\\pi,Q_{c}^{\\pi})}\\\\ &{\\geq\\!\\mathcal{L}_{\\mu}(\\pi,f_{r}^{\\pi})+\\beta\\mathcal{E}_{\\mu}(\\pi,f_{r}^{\\pi})-\\lambda\\{\\mathcal{L}_{\\mu}(\\pi,f_{c}^{\\pi})\\}_{+}+\\beta\\hat{\\mathcal{E}}_{\\mu}(\\pi,f_{c}^{\\pi})}\\\\ &{\\geq\\!\\mathcal{L}_{\\mu}(\\pi,f_{r}^{\\pi})-\\lambda\\{\\mathcal{L}_{\\mu}(\\pi,f_{c}^{\\pi})\\}_{+},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second equality is true because $\\mathcal{E}_{\\mu}(\\pi,Q_{r}^{\\pi})=\\hat{\\mathcal{E}}_{\\mu}(\\pi,Q_{c}^{\\pi})=0$ by Assumption 3.2, and the first inequality comes from the selection of $f_{r}^{\\pi}$ and $f_{c}^{\\pi}$ in optimization (6). ", "page_idx": 16}, {"type": "text", "text": "Therefore, we can obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{r}(\\hat{\\pi}^{*})-J_{r}(\\mu)\\geq\\!\\left(\\mathcal{L}_{\\mu}(\\hat{\\pi}^{*},f_{r}^{\\hat{\\pi}^{*}})-\\lambda\\{\\mathcal{L}_{\\mu}(\\hat{\\pi}^{*},f_{c}^{\\hat{\\pi}^{*}})\\}_{+}\\right)+\\lambda\\{J_{c}(\\hat{\\pi}^{*})-J_{c}(\\mu)\\}_{+}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\!\\left(\\mathcal{L}_{\\mu}(\\mu,f_{r}^{\\mu})-\\lambda\\{\\mathcal{L}_{\\mu}(\\mu,f_{c}^{\\mu})\\}_{+}\\right)+\\lambda\\{J_{c}(\\hat{\\pi}^{*})-J_{c}(\\mu)\\}_{+}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\lambda\\{J_{c}(\\hat{\\pi}^{*})-J_{c}(\\mu)\\}_{+}\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\{J_{c}(\\hat{\\pi}^{*})\\}_{+}-\\{J_{c}(\\mu)\\}_{+}\\leq\\{J_{c}(\\hat{\\pi}^{*})-J_{c}(\\mu)\\}_{+}\\leq\\frac{1}{\\lambda}\\big(J_{r}(\\hat{\\pi}^{*})-J_{r}(\\mu)\\big)\\leq\\frac{1}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Denote $\\pi_{\\mathrm{ref}}$ as $\\pi$ . First according to the definition for the no-regret oracle 5.1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac1{K}\\sum_{k=1}^{K}\\mathbb E_{\\boldsymbol\\pi}[f_{r}^{k}(s,\\boldsymbol\\pi)-f_{r}^{k}(s,\\boldsymbol\\pi_{k})-\\lambda\\{f_{c}^{k}(s,\\boldsymbol\\pi)-f_{c}^{k}(s,\\boldsymbol\\pi)\\}_{+}}}\\\\ {{+\\lambda\\{f_{c}^{k}(s,\\boldsymbol\\pi_{k})-f_{c}^{k}(s,\\boldsymbol\\pi)\\}_{+}]\\leq\\epsilon_{o p t}^{\\pi}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{E}_{\\pi}[f_{r}^{k}(s,\\pi)-f_{r}^{k}(s,\\pi_{k})]}\\\\ &{\\leq\\!\\epsilon_{o p t}^{\\pi}+\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{E}_{\\pi}[\\lambda\\{f_{c}^{k}(s,\\pi)-f_{c}^{k}(s,\\pi)\\}_{+}-\\lambda\\{f_{c}^{k}(s,\\pi_{k})-f_{c}^{k}(s,\\pi)\\}_{+}]\\leq\\epsilon_{o p t}^{\\pi},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{E}_{\\pi}[\\{f_{c}^{k}(s,\\pi_{k})-f_{c}^{k}(s,\\pi)\\}_{+}]\\le\\epsilon_{o p t}^{\\pi}-\\frac{1}{\\lambda K}\\sum_{k=1}^{K}\\mathbb{E}_{\\pi}[f_{r}^{k}(s,\\pi)-f_{r}^{k}(s,\\pi_{k})]\\le\\epsilon_{o p t}^{\\pi}+\\frac{V_{\\operatorname*{max}}}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We finish the proof. ", "page_idx": 16}, {"type": "text", "text": "B.3 Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem (Restate of Theorem 5.2). Under Assumptions 3.2 and 3.6, let the reference policy $\\pi_{r e f}\\in\\Pi$ be any policy satisfying Assumption 3.7, then with probability at least $1-\\delta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{J_{r}(\\pi_{r e f})-J_{r}(\\bar{\\pi})\\leq\\mathcal{O}\\bigg(\\epsilon_{s t a t}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}\\bigg)+\\epsilon_{o p t}^{\\pi}}}\\\\ {{J_{c}(\\bar{\\pi})-J_{c}(\\pi_{r e f})\\leq\\mathcal{O}\\bigg(\\epsilon_{s t a t}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}\\bigg)+\\epsilon_{o p t}^{\\pi}+\\frac{V_{\\operatorname*{max}}}{\\lambda},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where log(|F||G||\u03a0||W |/\u03b4) + VmaxBw log(|F||G||\u03a0||W |/\u03b4), and \u03c0\u00af is the policy returned by Algorithm $^{\\,l}$ with $\\beta=2$ and $\\pi_{r e f}$ as input. ", "page_idx": 16}, {"type": "text", "text": "Proof. Denote $\\pi_{\\mathrm{ref}}$ as $\\pi$ . According to the definition of $\\bar{\\pi}$ , and Lemma 6 we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{J_{r}(\\pi)-J_{r}(\\bar{\\pi})=\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}(J_{r}(\\pi)-J_{r}(\\pi_{k}))}}\\\\ {{=\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\left(\\underbrace{\\mathbb{E}_{\\mu}\\left[f_{r}^{k}-\\mathcal{T}_{r}^{\\pi_{k}}f_{r}^{k}\\right]}_{\\mathrm{(l)}}+\\underbrace{\\mathbb{E}_{\\pi}[\\mathcal{T}_{r}^{\\pi_{k}}f_{r}^{k}-f_{r}^{k}]}_{\\mathrm{(lI)}}\\right.}}\\\\ {{\\displaystyle\\left.+\\underbrace{\\mathbb{E}_{\\pi}[f_{r}^{k}(s,\\pi)-f_{r}^{k}(s,\\pi_{k})]}_{\\mathrm{(II)}}+\\underbrace{\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{k})-\\mathcal{L}_{\\mu}(\\pi_{k},Q^{\\pi_{k}})}_{\\mathrm{(IV)}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Condition on the high probability event in , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\mathrm{I})+(\\mathrm{II})\\leq2\\mathcal{E}_{\\mu}(\\pi_{k},f_{r}^{k})\\leq2\\mathcal{E}_{\\mathcal{D}}(\\pi_{k},f_{r}^{k})+2\\epsilon_{s t a t}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "According to a similar argument as that in the Lemma 13 in Cheng et al. (2022), we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\mathcal{L}_{\\mu}(\\pi_{k},Q_{r}^{\\pi_{k}})-\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{\\pi_{k}})|}\\\\ &{=\\!|\\mathbb{E}_{\\mu}[Q_{r}^{\\pi_{k}}(s,\\pi_{k})-Q_{r}^{\\pi_{k}}(s,a)]-\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{\\pi_{k}})|}\\\\ &{=\\!|(J_{r}(\\pi_{k})-J_{r}(\\mu))-\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{\\pi_{k}})|}\\\\ &{=\\!|(f_{r}^{\\pi_{k}}(s_{0},\\pi_{k})-J_{r}(\\mu))+(J_{r}(\\pi_{k})-f_{r}^{\\pi_{k}}(s_{0},\\pi_{k}))-\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{\\pi_{k}})|}\\\\ &{=\\!|\\mathbb{E}_{\\mu}[f_{r}^{\\pi_{k}}(s,\\pi_{k})-(T_{r}^{\\pi_{k}}f_{r}^{\\pi_{k}})(s,a)]+\\mathbb{E}_{d^{\\pi_{k}}}[(T_{r}^{\\pi_{k}}f_{r}^{\\pi_{k}})(s,a)-f_{r}^{\\pi_{k}}(s,a)]-\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{\\pi_{k}})|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "(by the extension of performance difference lemma (Lemma 1 in Cheng et al. (2020))) $\\begin{array}{r l}&{=\\lvert\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{\\pi_{k}})+\\mathbb{E}_{\\mu}[f_{r}^{\\pi_{k}}(s,a)-(\\mathcal{T}_{r}^{\\pi_{k}}f_{r}^{\\pi_{k}})(s,a)]+\\mathbb{E}_{d^{\\pi_{k}}}[(\\mathcal{T}_{r}^{\\pi_{k}}f_{r}^{\\pi_{k}})(s,a)-f_{r}^{\\pi_{k}}(s,a)]-\\mathcal{L}_{\\mu}(\\pi_{k})}\\\\ &{\\le\\lVert f_{r}^{\\pi_{k}}(s,a)-(\\mathcal{T}_{r}^{\\pi_{k}}f_{r}^{\\pi_{k}})(s,a)\\rVert_{2,\\mu}+\\lVert(\\mathcal{T}_{r}^{\\pi_{k}}f_{r}^{\\pi_{k}})(s,a)-f_{r}^{\\pi_{k}}(s,a)\\rVert_{2,d^{\\pi_{k}}}}\\end{array}$ , f r\u03c0 k)| \u2264O( \u03f51), (41) ", "page_idx": 17}, {"type": "text", "text": "where $f_{r}^{\\pi}:=\\arg\\operatorname*{min}$ sup $\\|f_{r}-T_{r}^{\\pi}f_{r}\\|_{2,\\nu}^{2},\\forall\\pi\\in\\Pi$ . By using Lemma 7, we have $f_{r}\\!\\in\\!\\mathcal{F}$ admissible $\\nu$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{k})-\\mathcal{L}_{\\mathcal{D}}(\\pi_{k},f_{r}^{k})|+|\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{\\pi_{k}})-\\mathcal{L}_{\\mathcal{D}}(\\pi_{k},f_{r}^{\\pi_{k}})|\\leq\\mathcal{O}(\\epsilon_{s t a t}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathrm{I})+(\\mathrm{II})+(\\mathrm{IV})\\leq\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{k})+2\\mathcal{E}_{\\mu}(\\pi_{k},f_{r}^{k})+2\\epsilon_{s t a t}-\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{\\pi_{k}})+\\mathcal{O}(\\sqrt{\\epsilon_{1}})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathcal{L}_{\\mathcal{D}}(\\pi_{k},f_{r}^{k})+2\\mathcal{E}_{\\mathcal{D}}(\\pi_{k},f_{r}^{k})+\\mathcal{O}(\\epsilon_{s t a t})-\\mathcal{L}_{\\mathcal{D}}(\\pi_{k},f_{r}^{\\pi_{k}})+\\mathcal{O}(\\sqrt{\\epsilon_{1}})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathcal{L}_{\\mathcal{D}}(\\pi_{k},f_{r}^{\\pi_{k}})+2\\mathcal{E}_{\\mathcal{D}}(\\pi_{k},f_{r}^{\\pi_{k}})+\\mathcal{O}(\\epsilon_{s t a t})-\\mathcal{L}_{\\mathcal{D}}(\\pi_{k},f_{r}^{\\pi_{k}})+\\mathcal{O}(\\sqrt{\\epsilon_{1}})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathcal{O}(\\epsilon_{s t a t}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the third inequality holds by the selection of $f_{r}^{k}$ , and the last inequality holds by Lemma 5. Therefore by using Lemma B.2 we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\nJ_{r}(\\pi)-J_{r}(\\bar{\\pi})\\leq\\mathcal{O}(\\epsilon_{s t a t}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}})+\\epsilon_{o p t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Following a similar argument, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\nJ_{c}(\\bar{\\pi})-J_{c}(\\pi)=\\frac{1}{K}\\sum_{k=1}^{K}(J_{c}(\\pi_{k})-J_{c}(\\pi))\\leq\\mathcal{O}(\\epsilon_{s t a t}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}})+\\epsilon_{o p t}^{\\pi}+\\frac{V_{m a x}}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.4 Proof of Theorem 5.6 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem (Restate of Theorem 5.2). Under Assumptions 3.2 and 3.6, let the reference policy $\\pi_{r e f}\\in\\Pi$ be any policy satisfying Assumption 3.7, then with probability at least $1-\\delta$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{r}(\\mu)-J_{r}(\\bar{\\pi})\\leq\\mathcal{O}\\biggr(\\epsilon_{s t a t}^{\\pi}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}\\biggr)+\\epsilon_{o p t}^{\\mu}}\\\\ &{J_{c}(\\bar{\\pi})-J_{c}(\\mu)\\leq\\mathcal{O}\\biggr(\\epsilon_{s t a t}^{\\pi}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}\\biggr)+\\epsilon_{o p t}^{\\mu}+\\frac{V_{\\operatorname*{max}}}{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r}{\\epsilon_{s t a t}:=V_{\\operatorname*{max}}C_{\\ell_{2}}^{*}\\sqrt{\\frac{\\log(|\\mathcal{F}||\\Pi||W|/\\delta)}{N}}+\\frac{V_{\\operatorname*{max}}B_{w}\\log(|\\mathcal{F}||\\Pi||W|/\\delta)}{N}}\\end{array}$ VmaxBw log(|F||\u03a0||W |/\u03b4), and \u03c0\u00af is the policy returned by Algorithm $^{\\,I}$ with $\\beta\\geq0$ and $\\mu$ as input. ", "page_idx": 18}, {"type": "text", "text": "Proof. Following a similar proof in Theorem 5.2. But when the reference policy is the behavior policy, we have $\\bar{(\\mathbf{I})}+\\bar{(\\mathbf{II})}=\\bar{0}$ . Therefore we have have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\mathrm{IV})=\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{k})-\\mathcal{L}_{\\mu}(\\pi_{k},Q^{\\pi_{k}})}\\\\ &{\\le\\!\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{k})-\\mathcal{L}_{\\mu}(\\pi_{k},Q^{\\pi_{k}})+\\beta\\mathcal{E}_{\\mathcal{D}}(\\pi_{k},f_{r}^{k})}\\\\ &{\\le\\!\\mathcal{L}_{\\mu}(\\pi_{k},f_{r}^{k})-\\mathcal{L}_{\\mu}(\\pi_{k},Q^{\\pi_{k}})+\\beta\\mathcal{E}_{\\mathcal{D}}(\\pi_{k},f_{r}^{k})-\\beta\\mathcal{E}_{\\mathcal{D}}(\\pi,f_{\\pi_{k}})+\\beta C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}+\\beta\\epsilon_{s t a t}\\quad\\mathrm{(Lemma~5)}}\\\\ &{\\le\\!\\mathcal{L}_{\\mathcal{D}}(\\pi_{k},f_{r}^{k})+b e t a\\mathcal{E}_{\\mathcal{D}}(\\pi_{k},f_{r}^{k})-\\mathcal{L}_{\\mathcal{D}}(\\pi_{k},f_{r}^{\\pi_{k}})-\\beta\\mathcal{E}_{\\mathcal{D}}(\\pi,f_{\\pi_{k}})+(\\beta+1)(\\epsilon_{s t a t}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}})}\\\\ &{\\le\\!(\\beta+1)(\\epsilon_{s t a t}+C_{\\ell_{2}}^{*}\\sqrt{\\epsilon_{1}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C Discussion on obtaining the behavior policy ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To extract the behavior policy when it is not provided, we can simply run behavior cloning on the offline data. In particular, we can estimate the learned behavior policy $\\hat{\\pi}_{\\mu}$ as follows: $\\forall s\\in$ D, \u03c0\u02c6\u00b5(a|s) \u2190 nn((s,sa)) , and $\\begin{array}{r}{\\forall s\\,\\notin\\,\\mathcal{D},\\hat{\\pi}_{\\mu}(a|s)\\,\\gets\\,\\frac{1}{|A|}}\\end{array}$ , where $n(s,a)$ is the number of times $(s,a)$ appears in the offline dataset $\\mathcal{D}$ . Essentially, the estimated BC policy matches the empirical behavior policy on states in the offline dataset and takes uniform random actions outside the support of the dataset. It is easy to show that the gap between the learned policy $\\hat{\\pi}_{\\mu}$ and the behavior policy $\\pi_{\\mu}$ is upper bounded by $\\mathcal{O}(\\operatorname*{min}\\{1,|S|/\\bar{N}\\})$ (Kumar et al., 2022; Rajaraman et al., 2020). We can have a very accurate estimate as long as the size of the dataset is large enough. ", "page_idx": 18}, {"type": "text", "text": "D Expermintal Supplement ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Practical Algorithm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The practical version of our algorithm WSAC is shown in Algorithm 2. ", "page_idx": 18}, {"type": "text", "text": "Algorithm 2 WSAC - Practical Version   \n1: Input: Batch data $\\mathcal{D}$ , policy network $\\pi$ , network for the reward critic $f_{r}$ , network for the cost critic $f_{c},\\beta>0,\\lambda>0$ .   \n2: for $k=1,2,\\ldots,K$ do   \n3: Sample minibatch $\\mathcal{D}_{\\mathrm{mini}}$ from the dataset $\\mathcal{D}$ .   \n4: Update Critic Networks: $\\begin{array}{r l}&{l_{\\mathrm{reward}}(f_{r}):=\\mathcal{L}_{\\mathcal{D}_{\\mathrm{mini}}}(\\pi,f_{r})+\\beta\\mathcal{E}_{\\mathcal{D}_{\\mathrm{mini}}}(\\pi,f_{r}),}\\\\ &{\\qquad\\quad\\;\\;f_{r}\\gets\\mathrm{ADAM}(f_{r}-\\eta_{\\mathrm{fast}}\\nabla l_{\\mathrm{reward}}(f_{r})),}\\\\ &{l_{\\mathrm{cost}}(f_{c}):=-\\lambda\\mathcal{L}_{\\mathcal{D}_{\\mathrm{mini}}}(\\pi,f_{c})+\\beta\\mathcal{E}_{\\mathcal{D}_{\\mathrm{mini}}}(\\pi,f_{c}),}\\\\ &{\\qquad\\quad\\;\\;f_{c}\\gets\\mathrm{ADAM}(f_{c}-\\eta_{\\mathrm{fast}}\\nabla l_{\\mathrm{cost}}(f_{c})).}\\end{array}$   \n5: Update Policy Network: $\\begin{array}{r l}&{l_{\\mathrm{actor}}(\\pi):=-\\mathcal{L}_{\\mathrm{mini}}(\\pi,f_{r})+\\lambda\\{\\mathcal{L}_{\\mathrm{mini}}(\\pi,f_{c})\\}_{+},}\\\\ &{\\quad\\quad\\quad\\pi\\gets\\mathrm{ADAM}(\\pi-\\eta_{\\mathrm{slow}}\\nabla l_{\\mathrm{actor}}(\\pi)).}\\end{array}$   \n6: end for   \n7: Output: \u03c0 ", "page_idx": 18}, {"type": "text", "text": "D.2 Environments Description ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Besides the \u201cBallCircle\" environment, we also study several representative environments as follows.   \nAll of them are shown in Figure 2 and their offline dataset is from Liu et al. (2023a). ", "page_idx": 18}, {"type": "image", "img_path": "82Ndsr4OS6/tmp/d80216238243aa5002fc04dca3f8d0ca3966a4a4c6cc6cb20cb593589b12628c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 2: BallCircle and CarCircle (left), PointButton (medium), PointPush(right) . ", "page_idx": 19}, {"type": "text", "text": "\u2022 CarCircle: This environment requires the car to move on a circle in a clockwise direction within the safety zone defined by the boundaries. The car is a four-wheeled agent based on MIT\u2019s race car. The reward is dense and increases by the car\u2019s velocity and by the proximity towards the boundary of the circle and the cost is incurred if the agent leaves the safety zone defined by the two yellow boundaries, which are the same as \"CarCircle\". ", "page_idx": 19}, {"type": "text", "text": "\u2022 PointButton: This environment requires the point to navigate to the goal button location and touch the right goal button while avoiding more gremlins and hazards. The point has two actuators, one for rotation and the other for forward/backward movement. The reward consists of two parts, indicating the distance between the agent and the goal and if the agent reaches the goal button and touches it. The cost will be incurred if the agent enters the hazardous areas, contacts the gremlins, or presses the wrong button. ", "page_idx": 19}, {"type": "text", "text": "\u2022 PointPush: This environment requires the point to push a box to reach the goal while circumventing hazards and pillars. The objects are in 2D planes and the point is the same as \"PointButton\". It has a small square in front of it, which makes it easier to determine the orientation visually and also helps point push the box. ", "page_idx": 19}, {"type": "text", "text": "D.3 Implementation Details and Experimental settings ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We run all the experiments with NVIDIA GeForce RTX 3080 Ti 8\u2212Core Processor. The normalized reward and cost are summarized as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{R_{n o r m a l i z e d}=\\displaystyle\\frac{R_{\\pi}-r_{m i n}({\\cal M})}{r_{m a x}({\\cal M})-r_{m i n}({\\cal M})}}}\\\\ {{\\displaystyle C_{n o r m a l i z e d}=\\displaystyle\\frac{C_{\\pi}+\\epsilon}{\\kappa+\\epsilon},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $r(\\mathcal{M})$ is the empirical reward for task $\\mathcal{M}$ , $\\kappa$ is the cost threshold, $\\epsilon$ is a small number to ensure numerical stability. Thus any normalized cost below 1 is considered as safe. We use $R_{\\pi}$ and $C_{\\pi}$ to dentoe the cumulative rewards and cost for the evaluated policy, respectively. The parameters of $r_{m i n}(\\mathcal{M})$ , $r_{m a x}(M)$ and $\\kappa$ are environment-dependent constants and the specific values can be found in the Appendix D. We remark that the normalized reward and cost only used for demonstrating the performance purpose and are not used in the training process. The detailed value of the reward and costs can be found in Table 3. To mitigate the risk of unsafe scenarios, we introduce a hyperparameter ", "page_idx": 19}, {"type": "table", "img_path": "82Ndsr4OS6/tmp/4f2adc2e392c89781592c8e68bc5fb52ddefb996bd852386f201b2743f93a2e3.jpg", "table_caption": ["Table 3: Hyperparameters of WSAC "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "$U B_{Q_{C}}$ to the cost $Q$ -function as an overestimation when calculating the actor loss. We use two separate $\\beta_{r},\\beta_{c}$ for reward and cost $Q$ functions to make the algorithm more flexible. ", "page_idx": 19}, {"type": "image", "img_path": "82Ndsr4OS6/tmp/d3a1472aa7a48f8965305a492df9f0526309ab1f8afca0352476960b3d0ed2e0.jpg", "img_caption": ["(d) PointPush "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 3: The moving average of evaluation results is recorded every 500 training steps, with each result representing the average over 20 evaluation episodes and three random seeds. A cost threshold 1 is applied, with any normalized cost below 1 considered safe. ", "page_idx": 20}, {"type": "text", "text": "We use different $\\beta$ for the reward and cost critic networks and different $U B_{Q_{C}}$ for the actor-network to make the adversarial training more stable. We also let the key parameter $\\lambda$ within a certain range balance reward and cost during the training process. Their values are shown in Table 3. In experiments, we take $\\mathcal{W}=\\{0,C_{\\infty}\\}$ for computation effective. Then we can reduce $\\mathcal{E}_{D}(\\pi,f)$ to $C_{\\infty}\\mathbb{E}_{\\mathcal{D}}[(f(s,a)-r-\\gamma f(s^{\\prime},\\pi))^{2}]$ and reduce $\\hat{\\mathcal{E}}_{D}(\\pi,f)$ to $C_{\\infty}\\mathbb{E}_{\\mathcal{D}}[(f(s,a)-c-\\gamma f(s^{\\prime},\\pi))^{2}]$ . In this case, $C_{\\infty}$ can be considered as a part of the hyperparameter $\\beta_{r}(\\beta_{c})$ . ", "page_idx": 21}, {"type": "text", "text": "D.4 Experimental results details and supplements ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The evaluation performances of the agents in each environment after 30000 update steps of training are shown in Table 2, and the performance of average rewards and costs are shown in Figure 3. From the results, we observe that WSAC achieves a best reward performance with significantly lowest costs against all the baselines. It suggests WSAC can establish a safe and efficient policy and achieve a steady improvement by leveraging the offline dataset. ", "page_idx": 21}, {"type": "text", "text": "D.5 Simulations under different cost limits ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To further evaluate the performance of our algorithm under varying situations. We further compare our algorithm with baselines under varying cost limits, we report the average performance of our method and other baselines in Table 4. Specifically, cost limits of [10, 20, 40] are used for the BallCircle and CarCircle environments, and [20, 40, 80] for the PointButton and PointPush environments, following the standard setup outlined by Liu et al. (2023a). Our results demonstrate that WSAC maintains safety across all environments, and its performance is either comparable to or superior to the best baseline in each case. These suggest that WSAC is well-suited for adapting to tasks of varying difficulty. ", "page_idx": 21}, {"type": "text", "text": "Table 4: The normalized reward and cost of WSAC and other baselines for different cost limits. Each value is averaged over 3 distinct cost limits, 20 evaluation episodes, and 3 random seeds. The Average line shows the average situation in various environments. The cost threshold is 1. Gray: Unsafe agent whose normalized cost is greater than 1. Blue: Safe agent with best performance. The performance of all the baselines is reported according to the results in Liu et al. (2023a). ", "page_idx": 21}, {"type": "table", "img_path": "82Ndsr4OS6/tmp/e6c719ee7a3b2e2755a2546b848056cba2c8256b98810791cefe66c038e7a045.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.6 Ablation studies ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To investigate the contribution of each component of our algorithm, including the weighted Bellman regularizer, the aggression-limited objective, and the no-regret policy optimization (which together guarantee our theoretical results), we performed an ablation study in the tabular setting. The results, presented in Table 5, indicate that the weighted Bellman regularization ensures the safety of the algorithm, while the aggression-limited objective fine-tunes the algorithm to achieve higher rewards without compromising safety. ", "page_idx": 21}, {"type": "table", "img_path": "82Ndsr4OS6/tmp/0811d0e1ad90cd0705196ba60c393a62a1499b8be0f870eb4f133f88c971af08.jpg", "table_caption": ["Table 5: Ablation study under tabular case (cost limit is 0.1) over 10 repeat experiments "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.7 Sensitivity Analysis of Hyper-Parameters ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We provide the rewards and costs under different sets of $\\beta_{r}~=~\\beta_{c}~\\in~\\{1,0.5,0.05\\}$ and $\\lambda\\ \\in$ $\\{[0,\\dot{1}],[0,2],[1,2]\\}$ (since our $\\lambda$ only increases, the closed interval here represents the initial value ", "page_idx": 21}, {"type": "image", "img_path": "82Ndsr4OS6/tmp/e2c15e407deb24bba2f6f2c6dd56e8edad3b7daaaeea588430f79f8570219308.jpg", "img_caption": ["Figure 4: Sensitivity Analysis of Hyperparameters in the Tabular Case. The left figure illustrates tests conducted with various $\\beta$ values (For the sake of discussion, we denote $\\beta=\\beta_{r}=\\beta_{c})$ with $\\lambda=[0,2]$ , while the right figure presents tests across different ranges of $\\lambda$ with $\\beta_{r}=\\beta_{c}=2.0$ . and the upper bound of $\\lambda$ ) to demonstrate the robustness of our approach in the tabular setting in Figure 4. We can observe that the performance is almost the same under different sets of parameters and different qualities of behavior policies. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 23}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 23}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 23}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 23}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 23}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The claims made in Abstract and Intriduction reflect the paper\u2019s contributions. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The limitations have been properly discussed in Conclusion and in Section 4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All the assumptions have been properly stated. The complete proofs are in Appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our experimental results are reproducible. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide the data and code. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 25}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The Experimental setting and details are in Appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In the experiment, we used multiple random seeds to ensure the statistical significance of the results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We specified the computational resources we used in the Appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper is theoretical in nature, and it has been conducted with the NeurIPS code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper has tremendous positive societal impact as it develops RL algorithms with provable safety guarantee using offilne data. Such a guarantee is essential for practical implementation of RL algorithms. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]